Pull,Path,Diff_hunk,Comment
351,engine/src/main/java/com/datatorrent/stram/LocalModeImpl.java,"@@ -54,6 +55,53 @@ public DAG cloneDAG() throws Exception
   }
 
   @Override
+  public LocalAppHandle launchApp(StreamingApplication application, Configuration configuration, Attribute.AttributeMap
+      launchAttributes) throws LaunchException
+  {
+    try {
+      //application.populateDAG(getDAG(), configuration == null ? new Configuration(false) : configuration);","[{'comment': 'remove commented code.\n', 'commenter': 'tushargosavi'}, {'comment': 'Done\n', 'commenter': 'pramodin'}]"
351,api/src/main/java/com/datatorrent/api/LocalMode.java,"@@ -125,9 +137,13 @@ public static void runApp(StreamingApplication app, int runMillis)
   public static void runApp(StreamingApplication app, Configuration configuration, int runMillis)
   {
     LocalMode lma = newInstance();
-    app.populateDAG(lma.getDAG(), configuration == null ? new Configuration(false) : configuration);
-    LocalMode.Controller lc = lma.getController();
-    lc.run(runMillis);
+    Attribute.AttributeMap launchAttributes = new Attribute.AttributeMap.DefaultAttributeMap();
+    launchAttributes.put(RUN_MILLIS, (long)runMillis);
+    try {
+      lma.launchApp(app, configuration, launchAttributes);
+    } catch (LaunchException e) {
+      DTThrowable.rethrow(e);","[{'comment': 'DTThowable has been deprecated\n', 'commenter': 'davidyan74'}, {'comment': 'No need for it anyways if we do away with the checked exception.\n', 'commenter': 'tweise'}]"
351,api/src/main/java/com/datatorrent/api/ClusterMode.java,"@@ -0,0 +1,67 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.api;
+
+import org.apache.hadoop.conf.Configuration;
+
+/**
+ *
+ */
+public abstract class ClusterMode<H extends Launcher.AppHandle> extends Launcher<H>
+{
+
+  public static final Attribute<String> LIB_JARS = new Attribute<String>(new StringCodec.String2String());","[{'comment': 'Maybe use the diamond operator in Java 7?\n', 'commenter': 'davidyan74'}]"
351,api/src/main/java/com/datatorrent/api/Attribute.java,"@@ -207,6 +209,16 @@ public DefaultAttributeMap clone() throws CloneNotSupportedException
       }
 
       @Override
+      public <T> T getValue(Attribute<T> key)","[{'comment': 'Any reason why Context.getValue cannot be used?\n', 'commenter': 'davidyan74'}]"
351,api/src/main/java/com/datatorrent/api/ClusterMode.java,"@@ -0,0 +1,67 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.api;
+
+import org.apache.hadoop.conf.Configuration;
+
+/**
+ *
+ */
+public abstract class ClusterMode<H extends Launcher.AppHandle> extends Launcher<H>
+{
+
+  public static final Attribute<String> LIB_JARS = new Attribute<String>(new StringCodec.String2String());
+  public static final Attribute<String> ORIGINAL_APP_ID = new Attribute<String>(new StringCodec.String2String());
+  public static final Attribute<String> QUEUE_NAME = new Attribute<String>(new StringCodec.String2String());","[{'comment': ""@PramodSSImmaneni : I have following feedback on this\n1. Attribute is used to define variables that control Platform behavior. These defined here don't control platform behavior\n2. When user launches app using cli, user can pass these as command line options but they are not set as attributes but now with cluster mode implementation you are asking user to set them as attributes. There is discrepancy here.\n3. Using cli, user can also set more options which is not reflected in this.\n"", 'commenter': 'gauravgopi123'}, {'comment': '@gauravgopi123 I am using the Attribute object as a utility here. It provides the functionality needed like default values based on type.\n', 'commenter': 'pramodin'}, {'comment': '@PramodSSImmaneni : IMO using attributes here is different from how these values are passed on cli... If they were attributes I should not able to set them using configuration file which this is not..\n', 'commenter': 'gauravgopi123'}, {'comment': 'Do not view them as attributes, they are simply parameters that need to be passed in and they can be different types such as ints, strings, boolean are other objects, attribute object neatly fits that bill  to pass parameters of different types. I can simply extend attribute and call it parameter and it would give me the same functionality.\n', 'commenter': 'pramodin'}, {'comment': 'I think extending it and calling it parameter would be good. By doing that you can also restrict the set of parameters that user can set.\n', 'commenter': 'gauravgopi123'}]"
351,api/src/main/java/com/datatorrent/api/LocalMode.java,"@@ -18,34 +18,46 @@
  */
 package com.datatorrent.api;
 
-import java.util.Iterator;
-import java.util.ServiceLoader;
-
 import org.apache.hadoop.conf.Configuration;
 
+import com.datatorrent.netlet.util.DTThrowable;
+
 /**
  * Local mode execution for single application
  *
  * @since 0.3.2
  */
-public abstract class LocalMode
+public abstract class LocalMode<H extends Launcher.AppHandle> extends Launcher<H>
 {
 
+  public static final Attribute<Long> RUN_MILLIS = new Attribute<Long>((Long)null);
+  public static final Attribute<Boolean> RUN_ASYNC = new Attribute<Boolean>(false);","[{'comment': 'Same comment here as before on attributes\n', 'commenter': 'gauravgopi123'}]"
351,api/src/main/java/com/datatorrent/api/Attribute.java,"@@ -146,6 +146,8 @@ public String toString()
 
     Set<Map.Entry<Attribute<?>, Object>> entrySet();
 
+    <T> T getValue(Attribute<T> key);","[{'comment': 'why do you need this? There is get(Attribute`<T>` key) already..\n', 'commenter': 'gauravgopi123'}]"
351,api/src/main/java/com/datatorrent/api/ClusterMode.java,"@@ -0,0 +1,67 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.api;
+
+import org.apache.hadoop.conf.Configuration;
+
+/**
+ *
+ */
+public abstract class ClusterMode<H extends Launcher.AppHandle> extends Launcher<H>","[{'comment': 'Would ""YarnAppLauncher"" be a more appropriate name? IMO what is today ""LocalMode"" should maybe be renamed to ""EmbeddedAppLauncher""? \n', 'commenter': 'tweise'}, {'comment': 'AppLauncher is a good idea..\n', 'commenter': 'pramodin'}]"
351,api/src/main/java/com/datatorrent/api/Launcher.java,"@@ -0,0 +1,111 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.api;
+
+import java.util.Iterator;
+import java.util.ServiceLoader;
+
+import org.apache.hadoop.conf.Configuration;
+
+/**
+ *
+ */
+public abstract class Launcher<H extends Launcher.AppHandle>
+{
+
+  public enum LaunchMode
+  {
+    LOCAL, CLUSTER","[{'comment': 'EMBEDDED, YARN\n', 'commenter': 'tweise'}, {'comment': 'The CLUSTER one could remain as is in case we add support for other distributed systems such as mesos. The factory can return the right *AppLauncher based on the environment in CLUSTER mode? What do you think?\n', 'commenter': 'pramodin'}]"
351,api/src/main/java/com/datatorrent/api/Launcher.java,"@@ -0,0 +1,111 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.api;
+
+import java.util.Iterator;
+import java.util.ServiceLoader;
+
+import org.apache.hadoop.conf.Configuration;
+
+/**
+ *
+ */
+public abstract class Launcher<H extends Launcher.AppHandle>
+{
+
+  public enum LaunchMode
+  {
+    LOCAL, CLUSTER
+  }
+
+  public interface AppHandle {}
+
+  /**
+   * Get the launcher instance
+   *
+   * @param launchMode - The launch mode to use
+   *
+   * @return The launcher
+   */
+  public static Launcher<?> getLauncher(LaunchMode launchMode)
+  {
+    Launcher launcher;
+    if (launchMode == LaunchMode.LOCAL) {
+      launcher = LocalMode.newInstance();
+    } else {
+      launcher = ClusterMode.newInstance();
+    }
+    return launcher;
+  }
+
+  /**
+   * Launch application with configuration
+   *
+   * @param application  - Application to be run
+   * @param configuration - Application Configuration
+   *
+   * @return The application handle
+   */
+  public H launchApp(StreamingApplication application, Configuration configuration) throws LaunchException
+  {
+    return launchApp(application, configuration, null);
+  }
+
+  /**
+   * Launch application with configuration and launch parameters
+   *
+   * @param application  - Application to be run
+   * @param configuration - Application Configuration
+   * @param launchAttributes - Launch Parameters
+   *
+   * @return The application handle
+   */
+  public abstract H launchApp(StreamingApplication application, Configuration configuration, Attribute.AttributeMap launchAttributes) throws LaunchException;
+
+  /**
+   * Shutdown the application
+   *
+   * @param app
+   */
+  public abstract void shutdownApp(H app);
+
+  protected static <T> T loadService(Class<T> clazz)
+  {
+    ServiceLoader<T> loader = ServiceLoader.load(clazz);
+    Iterator<T> impl = loader.iterator();
+    if (!impl.hasNext()) {
+      throw new RuntimeException(""No implementation for "" + clazz);
+    }
+    return impl.next();
+  }
+
+  public static class LaunchException extends Exception","[{'comment': 'Why do we need this, and why check exception?\n', 'commenter': 'tweise'}, {'comment': 'During app launch, the implementation methods such as prepareDAG throw plain Exception. Do you suggest to throw plain exception from launchApp or wrap it up with an unchecked exception. The latter might lead to the developer not preparing for errors during launch. If not throwing unchecked exception, why not throw a named exception which we could customize in the future with relevant information pertaining to launch failure.\n', 'commenter': 'pramodin'}]"
351,api/src/main/java/com/datatorrent/api/Launcher.java,"@@ -0,0 +1,111 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.api;
+
+import java.util.Iterator;
+import java.util.ServiceLoader;
+
+import org.apache.hadoop.conf.Configuration;
+
+/**
+ *
+ */
+public abstract class Launcher<H extends Launcher.AppHandle>
+{
+
+  public enum LaunchMode
+  {
+    LOCAL, CLUSTER
+  }
+
+  public interface AppHandle {}
+
+  /**
+   * Get the launcher instance
+   *
+   * @param launchMode - The launch mode to use
+   *
+   * @return The launcher
+   */
+  public static Launcher<?> getLauncher(LaunchMode launchMode)
+  {
+    Launcher launcher;
+    if (launchMode == LaunchMode.LOCAL) {
+      launcher = LocalMode.newInstance();
+    } else {
+      launcher = ClusterMode.newInstance();
+    }
+    return launcher;
+  }
+
+  /**
+   * Launch application with configuration
+   *
+   * @param application  - Application to be run
+   * @param configuration - Application Configuration
+   *
+   * @return The application handle
+   */
+  public H launchApp(StreamingApplication application, Configuration configuration) throws LaunchException
+  {
+    return launchApp(application, configuration, null);
+  }
+
+  /**
+   * Launch application with configuration and launch parameters
+   *
+   * @param application  - Application to be run
+   * @param configuration - Application Configuration
+   * @param launchAttributes - Launch Parameters
+   *
+   * @return The application handle
+   */
+  public abstract H launchApp(StreamingApplication application, Configuration configuration, Attribute.AttributeMap launchAttributes) throws LaunchException;
+
+  /**
+   * Shutdown the application
+   *
+   * @param app
+   */
+  public abstract void shutdownApp(H app);","[{'comment': 'This could explain the behavior. Shutdown now, forced, etc.?\n', 'commenter': 'tweise'}]"
351,api/src/main/java/com/datatorrent/api/Launcher.java,"@@ -0,0 +1,111 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.api;
+
+import java.util.Iterator;
+import java.util.ServiceLoader;
+
+import org.apache.hadoop.conf.Configuration;
+
+/**
+ *
+ */
+public abstract class Launcher<H extends Launcher.AppHandle>
+{
+
+  public enum LaunchMode
+  {
+    LOCAL, CLUSTER
+  }
+
+  public interface AppHandle {}
+
+  /**
+   * Get the launcher instance
+   *
+   * @param launchMode - The launch mode to use
+   *
+   * @return The launcher
+   */
+  public static Launcher<?> getLauncher(LaunchMode launchMode)
+  {
+    Launcher launcher;
+    if (launchMode == LaunchMode.LOCAL) {
+      launcher = LocalMode.newInstance();
+    } else {
+      launcher = ClusterMode.newInstance();
+    }
+    return launcher;
+  }
+
+  /**
+   * Launch application with configuration
+   *
+   * @param application  - Application to be run
+   * @param configuration - Application Configuration
+   *
+   * @return The application handle
+   */
+  public H launchApp(StreamingApplication application, Configuration configuration) throws LaunchException
+  {
+    return launchApp(application, configuration, null);
+  }
+
+  /**
+   * Launch application with configuration and launch parameters","[{'comment': 'more documentation\n', 'commenter': 'tweise'}]"
351,api/src/main/java/com/datatorrent/api/LocalMode.java,"@@ -18,34 +18,46 @@
  */
 package com.datatorrent.api;
 
-import java.util.Iterator;
-import java.util.ServiceLoader;
-
 import org.apache.hadoop.conf.Configuration;
 
+import com.datatorrent.netlet.util.DTThrowable;","[{'comment': 'remove this\n', 'commenter': 'tweise'}]"
351,engine/src/main/java/com/datatorrent/stram/ClusterModeImpl.java,"@@ -0,0 +1,100 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.stram;","[{'comment': 'New code should be added to org.apache.apex.engine packages.\n', 'commenter': 'tweise'}, {'comment': 'Sounds good. What about api? Should Launcher go into org.apache.apex.engine as well?\n', 'commenter': 'pramodin'}]"
351,engine/src/main/java/com/datatorrent/stram/ClusterModeImpl.java,"@@ -0,0 +1,100 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.stram;
+
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.yarn.api.records.ApplicationId;
+
+import com.datatorrent.api.Attribute;
+import com.datatorrent.api.ClusterMode;
+import com.datatorrent.api.StreamingApplication;
+import com.datatorrent.stram.client.StramAppLauncher;
+import com.datatorrent.stram.plan.logical.LogicalPlan;
+import com.datatorrent.stram.plan.logical.LogicalPlanConfiguration;
+import com.datatorrent.stram.util.StreamingAppFactory;
+
+/**
+ *
+ */
+public class ClusterModeImpl extends ClusterMode<ClusterAppHandle>","[{'comment': 'Again, there are many clusters..\n', 'commenter': 'tweise'}]"
351,engine/src/main/java/com/datatorrent/stram/ClusterModeImpl.java,"@@ -0,0 +1,100 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.stram;
+
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.yarn.api.records.ApplicationId;
+
+import com.datatorrent.api.Attribute;
+import com.datatorrent.api.ClusterMode;
+import com.datatorrent.api.StreamingApplication;
+import com.datatorrent.stram.client.StramAppLauncher;
+import com.datatorrent.stram.plan.logical.LogicalPlan;
+import com.datatorrent.stram.plan.logical.LogicalPlanConfiguration;
+import com.datatorrent.stram.util.StreamingAppFactory;
+
+/**
+ *
+ */
+public class ClusterModeImpl extends ClusterMode<ClusterAppHandle>
+{
+
+  private static final Map<Attribute<?>, String> propMapping = new HashMap<>();
+
+  static {
+    propMapping.put(ClusterMode.LIB_JARS, StramAppLauncher.LIBJARS_CONF_KEY_NAME);
+    propMapping.put(ClusterMode.ORIGINAL_APP_ID, StramAppLauncher.ORIGINAL_APP_ID);
+    propMapping.put(ClusterMode.QUEUE_NAME, StramAppLauncher.QUEUE_NAME);
+  }
+
+  public ClusterAppHandle launchApp(final StreamingApplication app, Configuration conf, Attribute.AttributeMap launchAttributes) throws LaunchException
+  {
+    if (launchAttributes != null) {
+      for (Map.Entry<Attribute<?>, Object> entry : launchAttributes.entrySet()) {
+        String property = propMapping.get(entry.getKey());
+        if (property != null) {
+          setConfiguration(conf, property, entry.getValue());
+        }
+      }
+    }
+    try {
+      String name = app.getClass().getName();
+      StramAppLauncher appLauncher = new StramAppLauncher(name, conf);
+      appLauncher.loadDependencies();
+      StreamingAppFactory appFactory = new StreamingAppFactory(name, app.getClass())
+      {
+        @Override
+        public LogicalPlan createApp(LogicalPlanConfiguration planConfig)
+        {
+          return super.createApp(app, planConfig);
+        }
+      };
+      ApplicationId appId = appLauncher.launchApp(appFactory);
+      return new ClusterAppHandle(appId);
+    } catch (Exception ex) {
+      throw new LaunchException(ex);
+    }
+  }
+
+  @Override
+  public void shutdownApp(ClusterAppHandle app)
+  {
+    // TODO","[{'comment': ""if something isn't implemented, it should throw UnsupportedOperation\n"", 'commenter': 'tweise'}]"
351,engine/src/main/java/com/datatorrent/stram/LocalAppHandle.java,"@@ -0,0 +1,35 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.stram;
+
+import com.datatorrent.api.Launcher;
+import com.datatorrent.api.LocalMode;
+
+/**
+ *
+ */
+public class LocalAppHandle implements Launcher.AppHandle","[{'comment': 'EmbeddedApp\n', 'commenter': 'tweise'}, {'comment': 'Can this not be a nested class?\n', 'commenter': 'tweise'}, {'comment': 'Wanted to do that but the LocalModeImpl (EmbeddedAppLauncherImpl) is using the handle in the class declaration and the compiler does not like that to be a nested class\n', 'commenter': 'pramodin'}]"
351,engine/src/main/java/com/datatorrent/stram/util/StreamingAppFactory.java,"@@ -0,0 +1,64 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.stram.util;","[{'comment': 'package\n', 'commenter': 'tweise'}]"
391,engine/src/test/java/com/datatorrent/stram/cli/ApexCliTest.java,"@@ -195,6 +195,7 @@ public void testLaunchAppPackagePrecedenceWithConfigPackage() throws Exception
     Assert.assertEquals(""app-package-config"", props.get(""dt.test.3""));
     Assert.assertEquals(""user-home-config"", props.get(""dt.test.4""));
     Assert.assertEquals(""package-default"", props.get(""dt.test.5""));
+    Assert.assertEquals(""fromConfigPackage"", props.get(""dt.test.7""));","[{'comment': 'Please try to use the same convention for the property value in this test.\n', 'commenter': 'davidyan74'}]"
391,engine/src/test/resources/testAppPackage/mydtapp/src/main/resources/META-INF/properties.xml,"@@ -45,6 +45,10 @@
     <value>package-default</value>
   </property>
   <property>
+    <name>dt.test.7</name>
+    <value>fromAppPackage</value>","[{'comment': 'There is no test that verifies this value\n', 'commenter': 'davidyan74'}]"
391,engine/src/test/resources/testConfigPackage/testConfigPackageSrc/META-INF/properties.xml,"@@ -48,5 +48,9 @@
     <name>dt.test.2</name>
     <value>config-package</value>
   </property>
+  <property>
+    <name>dt.test.7</name>","[{'comment': 'What is the purpose of this property? It seems to serve the same purpose as dt.test.2?\n', 'commenter': 'davidyan74'}]"
391,engine/src/main/java/com/datatorrent/stram/client/AppPackage.java,"@@ -404,6 +407,8 @@ public void processAppDirectory(boolean skipJars)
         LOG.warn(""Ignoring file {} with unknown extension in app directory"", entry.getName());
       }
     }
+
+    processAppProperties();","[{'comment': 'Does moving the processAppProperties here mean that the properties specified in the app json file or the app properties files would be overwritten by the properties-\\* xml files?\n', 'commenter': 'davidyan74'}, {'comment': 'Ignore the above comment.\n', 'commenter': 'davidyan74'}]"
391,engine/src/test/java/com/datatorrent/stram/cli/ApexCliTest.java,"@@ -206,6 +206,22 @@ public void testLaunchAppPackagePrecedenceWithConfigPackage() throws Exception
   }
 
   @Test
+  public void testLaunchAppPackagePrecedenceWithConfigPackageApps() throws Exception
+  {
+    // set launch command options
+    ApexCli.LaunchCommandLineInfo commandLineInfo = ApexCli
+        .getLaunchCommandLineInfo(new String[]{""-D"", ""dt.test.1=launch-define"", ""-apconf"", ""my-app-conf1.xml"", ""-conf"", configFile.getAbsolutePath(), ""-useConfigApps"", ""exclusive""});
+    // process and look at launch config","[{'comment': '@davidyan74 I added _-apConf_ and updated the tests.\n', 'commenter': 'sandeshh'}]"
395,engine/src/test/java/com/datatorrent/stram/StreamingContainerManagerTest.java,"@@ -322,10 +322,21 @@ public void testStaticPartitioning()
     }
 
     // unifier
-    List<PTOperator> o2Unifiers = plan.getMergeOperators(dag.getMeta(node2));
-    Assert.assertEquals(""number unifiers"", 1, o2Unifiers.size());
-    List<OperatorDeployInfo> cUnifier = getDeployInfo(dnm.getContainerAgent(o2Unifiers.get(0).getContainer().getExternalId()));
-    Assert.assertEquals(""number operators "" + cUnifier, 1, cUnifier.size());
+    PTContainer containerWithUnifier;","[{'comment': 'How about getting the container of the downstream operator instead?\n', 'commenter': 'tweise'}, {'comment': 'Done.\n', 'commenter': 'sandeshh'}]"
395,engine/src/test/java/com/datatorrent/stram/StreamingContainerManagerTest.java,"@@ -764,15 +775,20 @@ public void testOperatorShutdown()
     PTOperator o3p1 = physicalPlan.getOperators(dag.getMeta(o3)).get(0);
     MockContainer mc4 = mockContainers.get(o3p1.getContainer());
     MockOperatorStats o3p1mos = mc4.stats(o3p1.getId());
-    o3p1mos.currentWindowId(1).checkpointWindowId(1).deployState(DeployState.ACTIVE);
-    mc4.sendHeartbeat();
+    PTOperator unifier = null;","[{'comment': '```\nPTOperator unifier = o3p1.upstreamMerge.values().iterator().next();\n```\n', 'commenter': 'tweise'}, {'comment': 'Done.\n', 'commenter': 'sandeshh'}]"
395,engine/src/test/java/com/datatorrent/stram/plan/physical/PhysicalPlanTest.java,"@@ -668,7 +671,8 @@ public void testRepartitioningScaleDownSinglePartition()
     Assert.assertSame("""", p1Doper.getOperatorMeta(), o1Meta.getMeta(o1.output).getUnifierMeta());
     Assert.assertTrue(""unifier "", p1Doper.isUnifier());
 
-    Collection<PTOperator> o1Unifiers = plan.getMergeOperators(o1Meta);
+    Collection<PTOperator> o1Unifiers = getUnifierStartingWithName(plan, o1Meta.getName());","[{'comment': 'No point having this here. You could add assert above for\n\n```\no1p1.getOutputs().get(0).sinks.size()\n```\n', 'commenter': 'tweise'}, {'comment': 'Done.\n', 'commenter': 'sandeshh'}]"
395,engine/src/main/java/com/datatorrent/stram/plan/physical/StreamMapping.java,"@@ -307,6 +307,11 @@ private void redoMapping()
           } else {
             // MxN partitioning: unifier per downstream partition
             LOG.debug(""MxN unifier for {} {} {}"", new Object[] {doperEntry.first, doperEntry.second.getPortName(), pks});
+
+            if (pks != null && pks.mask == 0) {","[{'comment': 'Maybe move that down so it matches previous pattern?\n', 'commenter': 'tweise'}]"
396,engine/src/main/java/com/datatorrent/stram/util/AbstractWritableAdapter.java,"@@ -56,6 +63,9 @@ public void readFields(DataInput arg0) throws IOException
       ois.close();
     }
     catch (Exception e) {
+      final Path path = Files.createTempFile(""dt-"", "".ser"");
+      logger.error(""Failed to de-serialize {}. Writing raw data to {}."", this.getClass().getName(), path, e);","[{'comment': 'This may cause security concerns. Consider using an additional flag when logging the data.\n', 'commenter': 'pramodin'}, {'comment': ""The temp file is in the user private container directory, so what's the security concern?\n"", 'commenter': 'tweise'}, {'comment': 'There are different security requirements for when data touches disk than when it is in motion. Companies may have policies that any data being stored on disk needs to be encrypted and this would not pass that.\n', 'commenter': 'pramodin'}, {'comment': 'That would be a much larger discussion that needs to consider all cases where data is written to disk and IMO does not belong into this PR. \n', 'commenter': 'tweise'}, {'comment': 'Agree with @tweise, the same should apply to checkpoints.\n', 'commenter': 'vrozov'}, {'comment': ""The user will not know that this data will be logged, unlike checkpoints (where he can control how data gets saved), but it's ok to address this in a broader context. It would be good to keep track of this in some document, all the points where we touch disk, buffer spooling, tuple recording etc.\n"", 'commenter': 'pramodin'}, {'comment': 'Take this discussion to a separate thread, maybe open a JIRA? Data can also be saved through buffer server spillover and elsewhere.\n', 'commenter': 'tweise'}, {'comment': 'Like I said ""it\'s ok to address this in a broader context""\n', 'commenter': 'pramodin'}]"
396,engine/src/main/java/com/datatorrent/stram/util/AbstractWritableAdapter.java,"@@ -56,6 +63,9 @@ public void readFields(DataInput arg0) throws IOException
       ois.close();
     }
     catch (Exception e) {
+      final Path path = Files.createTempFile(""dt-"", "".ser"");","[{'comment': 'Please change the prefix to something else, maybe ""apex-rpc-dump-"" ?\n', 'commenter': 'tweise'}]"
400,engine/src/main/java/com/datatorrent/stram/webapp/ContainerInfo.java,"@@ -43,6 +45,7 @@
   public long lastHeartbeat;
   @RecordField(type = ""stats"")
   public int numOperators;
+  public Map<Integer, String> operatorsIdandName;","[{'comment': 'How about just call this `operators`?\n', 'commenter': 'davidyan74'}, {'comment': 'Yes, that is better. Done.\n', 'commenter': 'sandeshh'}]"
400,engine/src/main/java/com/datatorrent/stram/StreamingContainerAgent.java,"@@ -450,6 +451,12 @@ public ContainerInfo getContainerInfo()
     ci.state = container.getState().name();
     ci.jvmName = this.jvmName;
     ci.numOperators = container.getOperators().size();
+    ci.operatorsIdandName = new HashMap<>();","[{'comment': 'Make this a TreeMap so that the output of the json will be sorted.\n', 'commenter': 'davidyan74'}, {'comment': 'Done.\n', 'commenter': 'sandeshh'}]"
401,engine/src/main/java/com/datatorrent/stram/engine/WindowGenerator.java,"@@ -282,7 +282,7 @@ public static long getAheadWindowId(long windowId, long firstWindowMillis, long
    *
    * @param windowIdA
    * @param windowIdB
-   * @param firstWindowMillis
+   * @param firstWindowMillis TODO: This parameter actually does not matter. Should remove it.","[{'comment': 'Then why not remove it?\n', 'commenter': 'tweise'}, {'comment': ""Wouldn't that break backward compatibility?\n"", 'commenter': 'davidyan74'}, {'comment': 'This is engine internal.\n', 'commenter': 'tweise'}]"
401,engine/src/main/java/com/datatorrent/stram/StreamingContainerManager.java,"@@ -1828,6 +1796,104 @@ public void run()
     return rsp;
   }
 
+  static class UpdateOperatorLatencyContext
+  {
+    StreamingContainerManager scm;
+
+    UpdateOperatorLatencyContext()
+    {
+    }
+
+    UpdateOperatorLatencyContext(StreamingContainerManager scm)
+    {
+      this.scm = scm;
+    }
+
+    long getRPCLatency(PTOperator oper)
+    {
+      MovingAverageLong rpcLatency = this.scm.rpcLatencies.get(oper.getContainer().getExternalId());
+      return rpcLatency == null ? 0 : rpcLatency.getAvg();
+    }
+
+    boolean endWindowStatsExists(long windowId)
+    {
+      return scm.endWindowStatsOperatorMap.containsKey(windowId);
+    }
+
+    long getEndWindowEmitTimestamp(long windowId, PTOperator oper)
+    {
+      Map<Integer, EndWindowStats> endWindowStatsMap = scm.endWindowStatsOperatorMap.get(windowId);
+      if (endWindowStatsMap == null) {
+        return -1;
+      }
+      EndWindowStats ews = endWindowStatsMap.get(oper.getId());
+      if (ews == null) {
+        return -1;
+      }
+      return ews.emitTimestamp;
+    }
+  }
+
+  public long updateOperatorLatency(PTOperator oper, UpdateOperatorLatencyContext ctx)","[{'comment': '@davidyan74 I was hoping to move this out of StreamingContainerManager to a separate class. \n', 'commenter': 'tweise'}, {'comment': 'I think we should keep it the way it is for the scope of this PR since it is consistent with the existing checkpoint-related code  in StreamingContainerManager. We can open a separate ticket to refactor the code to make this class smaller though.\n', 'commenter': 'davidyan74'}]"
401,engine/src/main/java/com/datatorrent/stram/StreamingContainerManager.java,"@@ -1828,6 +1796,104 @@ public void run()
     return rsp;
   }
 
+  static class UpdateOperatorLatencyContext
+  {
+    StreamingContainerManager scm;","[{'comment': 'Why depend on scm for the context? It needs rpcLatencies, endWindowStatsOperatorMap and a few other things but not everything else. We should try to decompose this monolithic class into pieces that can be developed and tested independently. For the latency test, I would expect that no instance of scm is needed.\n', 'commenter': 'tweise'}, {'comment': 'done\n', 'commenter': 'davidyan74'}]"
402,engine/src/main/java/com/datatorrent/stram/plan/physical/PhysicalPlan.java,"@@ -1264,12 +1264,12 @@ public Operator loadOperator(PTOperator oper)
    */
   private Checkpoint getActivationCheckpoint(PTOperator oper)
   {
-    if (oper.recoveryCheckpoint == null && oper.checkpoints.isEmpty()) {
+    if ((oper.recoveryCheckpoint == null || oper.recoveryCheckpoint == Checkpoint.INITIAL_CHECKPOINT) && oper.checkpoints.isEmpty()) {","[{'comment': 'It was supposed to be noop for an already initialized operator. Why is recoveryCheckpoint assigned in your case? There is the assumption in initPartitioning that does not seem correct.\n', 'commenter': 'tweise'}, {'comment': 'When new operator is added to DAG its get added through addLogicalOperator, which\ncalls initParitioning with INITIAL_CHECKPOINT , which calls addPTOperator which\nsets recoveryCheckpoint to INITIAL_CHECKPOINT. It happens during new DAG start also,\nall operators recoveryCheckpoint is set to INITIAL_CHECKPOINT.\n\nonly in case of newly added unifiers, recoveryCheckpoint is set to null, hence getActivationCheckpoint returns correct windowId, and initCheckpoint is called again with new windowId.\n', 'commenter': 'tushargosavi'}, {'comment': 'I believe this check should be removed and instead null passed to addPTOperator. Please verify.\n\n```\n+++ b/engine/src/main/java/com/datatorrent/stram/plan/physical/PhysicalPlan.java\n@@ -781,7 +781,7 @@ public class PhysicalPlan implements Serializable\n     // create operator instance per partition\n     Map<Integer, Partition<Operator>> operatorIdToPartition = Maps.newHashMapWithExpectedSize(partitions.size());\n     for (Partition<Operator> partition : partitions) {\n-      PTOperator p = addPTOperator(m, partition, Checkpoint.INITIAL_CHECKPOINT);\n+      PTOperator p = addPTOperator(m, partition, null);\n       operatorIdToPartition.put(p.getId(), partition);\n     }\n\n@@ -1264,7 +1264,7 @@ public class PhysicalPlan implements Serializable\n    */\n   private Checkpoint getActivationCheckpoint(PTOperator oper)\n   {\n-    if ((oper.recoveryCheckpoint == null || oper.recoveryCheckpoint == Checkpoint.INITIAL_CHECKPOINT) && oper.checkpoints.isEmpty()) {\n+    if (oper.recoveryCheckpoint == null && oper.checkpoints.isEmpty()) {\n       Checkpoint activationCheckpoint = Checkpoint.INITIAL_CHECKPOINT;\n       for (PTInput input : oper.inputs) {\n         PTOperator sourceOper = input.source.source;\n```\n', 'commenter': 'tweise'}]"
402,engine/src/test/java/com/datatorrent/stram/LogicalPlanModificationTest.java,"@@ -408,4 +409,47 @@ public void testExecutionManagerWithAsyncStorageAgent() throws Exception
     testExecutionManager(new AsyncFSStorageAgent(testMeta.getPath(), null));
   }
 
+  @Test
+  public void testNewOpearatorRecoveryWindowIds()","[{'comment': 'spelling\n', 'commenter': 'tweise'}]"
402,engine/src/test/java/com/datatorrent/stram/LogicalPlanModificationTest.java,"@@ -408,4 +409,47 @@ public void testExecutionManagerWithAsyncStorageAgent() throws Exception
     testExecutionManager(new AsyncFSStorageAgent(testMeta.getPath(), null));
   }
 
+  @Test
+  public void testNewOpearatorRecoveryWindowIds()
+  {
+    GenericTestOperator o1 = dag.addOperator(""o1"", GenericTestOperator.class);
+
+    TestPlanContext ctx = new TestPlanContext();
+    dag.setAttribute(OperatorContext.STORAGE_AGENT, ctx);
+    PhysicalPlan plan = new PhysicalPlan(dag, ctx);
+    ctx.deploy.clear();
+    ctx.undeploy.clear();
+
+    LogicalPlan.OperatorMeta o1Meta = dag.getMeta(o1);
+    List<PTOperator> o1Partitions = plan.getOperators(o1Meta);
+    Assert.assertEquals(""partitions "" + o1Partitions, 1, o1Partitions.size());
+    PhysicalPlanTest.setActivationCheckpoint(o1Partitions.get(0), 10);
+    Assert.assertEquals(""containers"", 1, plan.getContainers().size());
+
+    PlanModifier pm = new PlanModifier(plan);
+    GenericTestOperator o2 = new GenericTestOperator();
+    GenericTestOperator o3 = new GenericTestOperator();
+    pm.addOperator(""o2"", o2);
+    pm.addOperator(""o3"", o3);
+    pm.addStream(""s1"", o1.outport1, o2.inport2);
+    pm.addStream(""s2"", o2.outport1, o3.inport1);
+
+    Assert.assertEquals(""undeploy "" + ctx.undeploy, 0, ctx.undeploy.size());
+    Assert.assertEquals(""deploy "" + ctx.deploy, 0, ctx.deploy.size());
+
+    pm.applyChanges(ctx);
+
+    Assert.assertEquals(""containers post change"", 3, plan.getContainers().size());
+
+    Assert.assertEquals(""undeploy "" + ctx.undeploy, 1, ctx.undeploy.size());","[{'comment': 'How are these related to this test?\n', 'commenter': 'tweise'}, {'comment': 'removed these\n', 'commenter': 'tushargosavi'}]"
402,engine/src/test/java/com/datatorrent/stram/LogicalPlanModificationTest.java,"@@ -408,4 +409,47 @@ public void testExecutionManagerWithAsyncStorageAgent() throws Exception
     testExecutionManager(new AsyncFSStorageAgent(testMeta.getPath(), null));
   }
 
+  @Test
+  public void testNewOpearatorRecoveryWindowIds()
+  {
+    GenericTestOperator o1 = dag.addOperator(""o1"", GenericTestOperator.class);
+
+    TestPlanContext ctx = new TestPlanContext();
+    dag.setAttribute(OperatorContext.STORAGE_AGENT, ctx);
+    PhysicalPlan plan = new PhysicalPlan(dag, ctx);
+    ctx.deploy.clear();
+    ctx.undeploy.clear();
+
+    LogicalPlan.OperatorMeta o1Meta = dag.getMeta(o1);
+    List<PTOperator> o1Partitions = plan.getOperators(o1Meta);
+    Assert.assertEquals(""partitions "" + o1Partitions, 1, o1Partitions.size());
+    PhysicalPlanTest.setActivationCheckpoint(o1Partitions.get(0), 10);
+    Assert.assertEquals(""containers"", 1, plan.getContainers().size());
+
+    PlanModifier pm = new PlanModifier(plan);
+    GenericTestOperator o2 = new GenericTestOperator();
+    GenericTestOperator o3 = new GenericTestOperator();
+    pm.addOperator(""o2"", o2);
+    pm.addOperator(""o3"", o3);
+    pm.addStream(""s1"", o1.outport1, o2.inport2);
+    pm.addStream(""s2"", o2.outport1, o3.inport1);
+
+    Assert.assertEquals(""undeploy "" + ctx.undeploy, 0, ctx.undeploy.size());
+    Assert.assertEquals(""deploy "" + ctx.deploy, 0, ctx.deploy.size());
+
+    pm.applyChanges(ctx);
+
+    Assert.assertEquals(""containers post change"", 3, plan.getContainers().size());
+
+    Assert.assertEquals(""undeploy "" + ctx.undeploy, 1, ctx.undeploy.size());
+    Assert.assertEquals(""deploy "" + ctx.deploy, 3, ctx.deploy.size());
+
+    LogicalPlan.OperatorMeta o2Meta = plan.getLogicalPlan().getMeta(o2);
+    List<PTOperator> o2Partitions = plan.getOperators(o2Meta);
+    Assert.assertEquals(""unifier activation checkpoint "" + o2Meta, 10, o2Partitions.get(0).getRecoveryCheckpoint().windowId);","[{'comment': 'unifier?\n', 'commenter': 'tweise'}]"
402,engine/src/main/java/com/datatorrent/stram/plan/physical/PhysicalPlan.java,"@@ -1268,10 +1268,11 @@ private Checkpoint getActivationCheckpoint(PTOperator oper)
       Checkpoint activationCheckpoint = Checkpoint.INITIAL_CHECKPOINT;
       for (PTInput input : oper.inputs) {
         PTOperator sourceOper = input.source.source;
+        Checkpoint checkPoint = sourceOper.recoveryCheckpoint;","[{'comment': 'minor: can you change the variable name to checkpoint\n', 'commenter': 'tweise'}, {'comment': 'renamed variable name to checkpoint\n', 'commenter': 'tushargosavi'}]"
405,engine/src/test/java/com/datatorrent/stram/engine/GenericNodeTest.java,"@@ -393,6 +401,119 @@ public void run()
   }
 
   @Test
+  public void testBufferServerSubscriberActivationBeforeOperator() throws InterruptedException, IOException
+  {
+    int bufferServerPort = 0;","[{'comment': 'delay this declaration till it can be initialized\n', 'commenter': 'vrozov'}, {'comment': 'done\n', 'commenter': 'sandeshh'}]"
405,engine/src/test/java/com/datatorrent/stram/engine/GenericNodeTest.java,"@@ -393,6 +401,119 @@ public void run()
   }
 
   @Test
+  public void testBufferServerSubscriberActivationBeforeOperator() throws InterruptedException, IOException
+  {
+    int bufferServerPort = 0;
+    Server bufferServer = null;","[{'comment': 'delay declaration till it can be initialized\n', 'commenter': 'vrozov'}, {'comment': 'done\n', 'commenter': 'sandeshh'}]"
405,engine/src/test/java/com/datatorrent/stram/engine/GenericNodeTest.java,"@@ -393,6 +401,119 @@ public void run()
   }
 
   @Test
+  public void testBufferServerSubscriberActivationBeforeOperator() throws InterruptedException, IOException
+  {
+    int bufferServerPort = 0;
+    Server bufferServer = null;
+
+    final String streamName = ""streamName"";
+    final String upstreamNodeId = ""upstreamNodeId"";
+    final String  downstreamNodeId = ""downStreamNodeId"";
+
+    StreamContext issContext;
+    StreamContext ossContext;
+    BufferServerSubscriber iss;
+    BufferServerPublisher oss;
+
+    EventLoop eventloop = DefaultEventLoop.createEventLoop(""StreamTestEventLoop"");
+
+    ((DefaultEventLoop)eventloop).start();
+    bufferServer = new Server(0); // find random port
+    InetSocketAddress bindAddr = bufferServer.run(eventloop);
+    bufferServerPort = bindAddr.getPort();","[{'comment': 'final int bufferServerPort = bufferServer.run(eventloop).getPort();\n', 'commenter': 'vrozov'}, {'comment': 'done\n', 'commenter': 'sandeshh'}]"
405,engine/src/test/java/com/datatorrent/stram/engine/GenericNodeTest.java,"@@ -393,6 +401,119 @@ public void run()
   }
 
   @Test
+  public void testBufferServerSubscriberActivationBeforeOperator() throws InterruptedException, IOException
+  {
+    int bufferServerPort = 0;
+    Server bufferServer = null;
+
+    final String streamName = ""streamName"";
+    final String upstreamNodeId = ""upstreamNodeId"";
+    final String  downstreamNodeId = ""downStreamNodeId"";
+
+    StreamContext issContext;
+    StreamContext ossContext;
+    BufferServerSubscriber iss;
+    BufferServerPublisher oss;
+
+    EventLoop eventloop = DefaultEventLoop.createEventLoop(""StreamTestEventLoop"");
+
+    ((DefaultEventLoop)eventloop).start();
+    bufferServer = new Server(0); // find random port
+    InetSocketAddress bindAddr = bufferServer.run(eventloop);
+    bufferServerPort = bindAddr.getPort();
+
+    final StreamCodec<Object> serde = new DefaultStatefulStreamCodec<Object>();
+    final ArrayList<Object> list = new ArrayList<Object>();
+
+    GenericOperator go = new GenericOperator();
+    final GenericNode gn = new GenericNode(go, new com.datatorrent.stram.engine.OperatorContext(0, ""operator"",","[{'comment': 'Please check if OperatorContext is required or it can be null\n', 'commenter': 'vrozov'}, {'comment': 'The context cannot be null, in the _Node_ activation values are read from context.\n', 'commenter': 'sandeshh'}]"
405,engine/src/test/java/com/datatorrent/stram/engine/GenericNodeTest.java,"@@ -393,6 +401,119 @@ public void run()
   }
 
   @Test
+  public void testBufferServerSubscriberActivationBeforeOperator() throws InterruptedException, IOException
+  {
+    int bufferServerPort = 0;
+    Server bufferServer = null;
+
+    final String streamName = ""streamName"";
+    final String upstreamNodeId = ""upstreamNodeId"";
+    final String  downstreamNodeId = ""downStreamNodeId"";
+
+    StreamContext issContext;
+    StreamContext ossContext;
+    BufferServerSubscriber iss;
+    BufferServerPublisher oss;
+
+    EventLoop eventloop = DefaultEventLoop.createEventLoop(""StreamTestEventLoop"");
+
+    ((DefaultEventLoop)eventloop).start();
+    bufferServer = new Server(0); // find random port
+    InetSocketAddress bindAddr = bufferServer.run(eventloop);
+    bufferServerPort = bindAddr.getPort();
+
+    final StreamCodec<Object> serde = new DefaultStatefulStreamCodec<Object>();
+    final ArrayList<Object> list = new ArrayList<Object>();
+
+    GenericOperator go = new GenericOperator();
+    final GenericNode gn = new GenericNode(go, new com.datatorrent.stram.engine.OperatorContext(0, ""operator"",
+        new DefaultAttributeMap(), null));
+    gn.setId(1);
+
+    Sink<Object> output = new Sink<Object>()
+    {
+      @Override
+      public void put(Object tuple)
+      {
+        list.add(tuple);
+      }
+
+      @Override
+      public int getCount(boolean reset)
+      {
+        return 0;
+      }
+    };
+
+    issContext = new StreamContext(streamName);
+    issContext.setSourceId(upstreamNodeId);
+    issContext.setSinkId(downstreamNodeId);
+    issContext.setFinishedWindowId(-1);
+    issContext.setBufferServerAddress(InetSocketAddress.createUnresolved(""localhost"", bufferServerPort));","[{'comment': 'Why createUnresolved?\n', 'commenter': 'vrozov'}, {'comment': 'Done. Using ""localhost"" to create the socket address.\n', 'commenter': 'sandeshh'}]"
405,engine/src/test/java/com/datatorrent/stram/engine/GenericNodeTest.java,"@@ -393,6 +401,119 @@ public void run()
   }
 
   @Test
+  public void testBufferServerSubscriberActivationBeforeOperator() throws InterruptedException, IOException
+  {
+    int bufferServerPort = 0;
+    Server bufferServer = null;
+
+    final String streamName = ""streamName"";
+    final String upstreamNodeId = ""upstreamNodeId"";
+    final String  downstreamNodeId = ""downStreamNodeId"";
+
+    StreamContext issContext;
+    StreamContext ossContext;
+    BufferServerSubscriber iss;
+    BufferServerPublisher oss;
+
+    EventLoop eventloop = DefaultEventLoop.createEventLoop(""StreamTestEventLoop"");
+
+    ((DefaultEventLoop)eventloop).start();
+    bufferServer = new Server(0); // find random port
+    InetSocketAddress bindAddr = bufferServer.run(eventloop);
+    bufferServerPort = bindAddr.getPort();
+
+    final StreamCodec<Object> serde = new DefaultStatefulStreamCodec<Object>();
+    final ArrayList<Object> list = new ArrayList<Object>();
+
+    GenericOperator go = new GenericOperator();
+    final GenericNode gn = new GenericNode(go, new com.datatorrent.stram.engine.OperatorContext(0, ""operator"",
+        new DefaultAttributeMap(), null));
+    gn.setId(1);
+
+    Sink<Object> output = new Sink<Object>()
+    {
+      @Override
+      public void put(Object tuple)
+      {
+        list.add(tuple);
+      }
+
+      @Override
+      public int getCount(boolean reset)
+      {
+        return 0;
+      }
+    };
+
+    issContext = new StreamContext(streamName);
+    issContext.setSourceId(upstreamNodeId);
+    issContext.setSinkId(downstreamNodeId);
+    issContext.setFinishedWindowId(-1);
+    issContext.setBufferServerAddress(InetSocketAddress.createUnresolved(""localhost"", bufferServerPort));
+    issContext.put(StreamContext.CODEC, serde);
+    issContext.put(StreamContext.EVENT_LOOP, eventloop);
+
+    iss = new BufferServerSubscriber(downstreamNodeId, 1024);
+    iss.setup(issContext);
+    iss.activate(issContext);
+
+    ossContext = new StreamContext(streamName);
+    ossContext.setSourceId(upstreamNodeId);
+    ossContext.setSinkId(downstreamNodeId);
+    ossContext.setBufferServerAddress(InetSocketAddress.createUnresolved(""localhost"", bufferServerPort));
+    ossContext.put(StreamContext.CODEC, serde);
+    ossContext.put(StreamContext.EVENT_LOOP, eventloop);
+
+    oss = new BufferServerPublisher(upstreamNodeId, 1024);
+    oss.setup(ossContext);
+    oss.activate(ossContext);
+
+    gn.connectInputPort(""ip1"", iss.acquireReservoir(""testReservoir"", 1));
+    gn.connectOutputPort(""op"", output);
+    gn.firstWindowMillis = 0;
+    gn.windowWidthMillis = 100;
+
+    Tuple beginWindow1 = new Tuple(MessageType.BEGIN_WINDOW, 0x1L);
+    Tuple endWindow1 = new EndWindowTuple(0x1L);
+    Tuple beginWindow2 = new Tuple(MessageType.BEGIN_WINDOW, 0x2L);
+    Tuple endWindow2 = new EndWindowTuple(0x2L);
+    Tuple beginWindow3 = new Tuple(MessageType.BEGIN_WINDOW, 0x3L);
+    Tuple endWindow3 = new EndWindowTuple(0x3L);
+    EndStreamTuple est = new EndStreamTuple(0L);
+
+    oss.put(beginWindow1);
+    oss.put(endWindow1);
+    oss.put(beginWindow2);
+    oss.put(endWindow2);
+    oss.put(beginWindow3);
+    oss.put(endWindow3);
+    oss.put(est);","[{'comment': 'The purpose of the test is to check that BufferServerSubscriber can be activated and process tuples without a loss even if it is activated before the operator is setup and starts processing tuples. There is no guarantee that Publisher actually delivers tuples to the buffer server and buffer server sends them to the Subscriber in the test.\n', 'commenter': 'vrozov'}]"
405,engine/src/test/java/com/datatorrent/stram/engine/GenericNodeTest.java,"@@ -393,6 +404,127 @@ public void run()
   }
 
   @Test
+  public void testBufferServerSubscriberActivationBeforeOperator() throws InterruptedException, IOException
+  {
+    final String streamName = ""streamName"";
+    final String upstreamNodeId = ""upstreamNodeId"";
+    final String  downstreamNodeId = ""downStreamNodeId"";
+
+    EventLoop eventloop = DefaultEventLoop.createEventLoop(""StreamTestEventLoop"");
+
+    ((DefaultEventLoop)eventloop).start();
+    final Server bufferServer = new Server(0); // find random port
+    final int bufferServerPort = bufferServer.run(eventloop).getPort();
+
+    final StreamCodec<Object> serde = new DefaultStatefulStreamCodec<Object>();
+    final BlockingQueue<Object> tuples = new ArrayBlockingQueue<Object>(10);
+
+    GenericTestOperator go = new GenericTestOperator();
+    final GenericNode gn = new GenericNode(go, new com.datatorrent.stram.engine.OperatorContext(0, ""operator"",
+        new DefaultAttributeMap(), null));
+    gn.setId(1);
+
+    Sink<Object> output = new Sink<Object>()
+    {
+      @Override
+      public void put(Object tuple)
+      {
+        tuples.add(tuple);
+      }
+
+      @Override
+      public int getCount(boolean reset)
+      {
+        return 0;
+      }
+    };
+
+    InetSocketAddress socketAddress = new InetSocketAddress(""localhost"", bufferServerPort);
+
+    StreamContext issContext = new StreamContext(streamName);
+    issContext.setSourceId(upstreamNodeId);
+    issContext.setSinkId(downstreamNodeId);
+    issContext.setFinishedWindowId(-1);
+    issContext.setBufferServerAddress(socketAddress);
+    issContext.put(StreamContext.CODEC, serde);
+    issContext.put(StreamContext.EVENT_LOOP, eventloop);
+
+    StreamContext ossContext = new StreamContext(streamName);
+    ossContext.setSourceId(upstreamNodeId);
+    ossContext.setSinkId(downstreamNodeId);
+    ossContext.setBufferServerAddress(socketAddress);
+    ossContext.put(StreamContext.CODEC, serde);
+    ossContext.put(StreamContext.EVENT_LOOP, eventloop);
+
+    BufferServerPublisher oss = new BufferServerPublisher(upstreamNodeId, 1024);
+    oss.setup(ossContext);
+    oss.activate(ossContext);
+
+    oss.put(new Tuple(MessageType.BEGIN_WINDOW, 0x1L));
+    byte[] buff = PayloadTuple.getSerializedTuple(0, 1);
+    buff[buff.length - 1] = (byte)1;
+    oss.put(buff);
+    oss.put(new EndWindowTuple(0x1L));
+    oss.put(new Tuple(MessageType.BEGIN_WINDOW, 0x2L));
+    buff = PayloadTuple.getSerializedTuple(0, 1);
+    buff[buff.length - 1] = (byte)2;
+    oss.put(buff);
+    oss.put(new EndWindowTuple(0x2L));
+    oss.put(new Tuple(MessageType.BEGIN_WINDOW, 0x3L));
+    buff = PayloadTuple.getSerializedTuple(0, 1);
+    buff[buff.length - 1] = (byte)3;
+    oss.put(buff);
+
+    oss.put(new EndWindowTuple(0x3L));
+    oss.put(new EndStreamTuple(0L));
+
+    BufferServerSubscriber iss = new BufferServerSubscriber(downstreamNodeId, 1024);
+    iss.setup(issContext);
+
+    gn.connectInputPort(GenericTestOperator.IPORT1, iss.acquireReservoir(""testReservoir"", 10));
+    gn.connectOutputPort(GenericTestOperator.OPORT1, output);
+
+    SweepableReservoir tupleWait = iss.acquireReservoir(""testReservoir2"", 10);
+
+    iss.activate(issContext);
+
+    while (tupleWait.sweep() != null) {","[{'comment': 'Should the condition be reversed?\n', 'commenter': 'vrozov'}, {'comment': 'Yes, will change it.\n', 'commenter': 'sandeshh'}]"
410,api/src/main/java/com/datatorrent/api/DAG.java,"@@ -288,4 +288,23 @@
   {
 
   }
+
+  /**
+   * This class provides functionality to extend existing DAG. The type of extensions allowed are
+   *
+   * <li>Add a new operator</li>
+   * <li>Remove an existing operator</li>
+   * <li>Add an operator to stream</li>
+   * <li>Add an stream between existing operator and new opeartor</li>
+   */
+  interface DAGChangeSet extends DAG","[{'comment': 'Why does it extend DAG?\n', 'commenter': 'tweise'}, {'comment': 'My idea was to use the same api to specify dag changes with\naddition of new methods to change existing DAG. This also\nallows running existing application as a subdag, for\nexample following application run three applications one after\nanother, individual applications can be run separately too\n\nhttps://github.com/tushargosavi/apex-dynamic-scheduling/blob/master/src/main/java/com/datatorrent/wordcount/lindag/DagSchedulingApp.java#L35\n', 'commenter': 'tushargosavi'}]"
410,api/src/main/java/com/datatorrent/api/StatsListener.java,"@@ -115,6 +117,55 @@
     List<OperatorResponse> getOperatorResponse();
   }
 
+  /**
+   * An interface to the DAG. Stats listener can get information about
+   * operator or other elements in the DAG through this interface. currerntly","[{'comment': 'check all doc blocks for spelling\n', 'commenter': 'tweise'}, {'comment': 'done\n', 'commenter': 'tushargosavi'}]"
410,api/src/main/java/com/datatorrent/api/StatsListener.java,"@@ -115,6 +117,55 @@
     List<OperatorResponse> getOperatorResponse();
   }
 
+  /**
+   * An interface to the DAG. Stats listener can get information about
+   * operator or other elements in the DAG through this interface. currerntly
+   * we only provide method to extract the operator name based on the physical
+   * id of the operator. In future more methods can be added.
+   *
+   */
+  interface StatsListenerContext","[{'comment': 'How does this converge with existing processStats?\n', 'commenter': 'tweise'}, {'comment': 'It will be additional interface between DAG and statlistener. It can be used to get additional information about the operator/dag when processStats is called. currently processStats BatchedOperatorStats argument is very limited, in case of shared stats listener it is difficult to know for which operator processStats is being called. In presence of context stats listener can get operator name from operator id to determine the operator. Also it can provide additional interaction between engine and statslistener such as requesting dag change or shutting down the application.\n', 'commenter': 'tushargosavi'}]"
410,api/src/main/java/com/datatorrent/api/StatsListener.java,"@@ -115,6 +117,55 @@
     List<OperatorResponse> getOperatorResponse();
   }
 
+  /**
+   * An interface to the DAG. Stats listener can get information about
+   * operator or other elements in the DAG through this interface. currerntly
+   * we only provide method to extract the operator name based on the physical
+   * id of the operator. In future more methods can be added.
+   *
+   */
+  interface StatsListenerContext
+  {
+    /**
+     * Return name of the operator given its id. Returns null if operator not found
+     * in the DAG.
+     * @param id Operator id
+     * @return name of the operator.
+     */
+    String getOperatorName(int id);
+
+    /**
+     * Create an instance of DAGChangeSet, which will be used by statsListener to submit
+     * dag modifications through {@link StatsListenerContext#submitDagChange(com.datatorrent.api.DAG.DAGChangeSet)}
+     * @return
+     */
+    DAGChangeSet createDAG();
+
+    /**
+     * Submit DAG modification request to the engine. After successful validation of
+     * new DAG, a future object is returned. StatListeners can use this future object
+     * to check the state of request. {@link FutureTask#get()} will throw an exception
+     * if any exception is thrown while DAG modifications.
+     *
+     * If an existing DAG modification is pending, then null is returned. in this case
+     * statsListener can submit the request again on next invocation.
+     *
+     * @param dagchanges The new modifications to logical dag.
+     * @return Future object to check state of the request.
+     */
+    FutureTask<Object> submitDagChange(DAGChangeSet dagchanges) throws IOException, ClassNotFoundException;
+  }
+
+  /**
+   * If StatsListener implements ContextAwareStatsListener interface, then engine will
+   * provide a reference to StatsListenerContext using which listener can examine
+   * current state of the DAG.
+   */
+  interface ContextAwareStatsListener
+  {
+    void setContext(StatsListenerContext context);","[{'comment': 'Any reason to not provide this as argument to processStats?\n', 'commenter': 'tweise'}, {'comment': 'Adding a new argument will break binary compatibility of existing stats listeners. hence went for this approach. This could be implemented with\n- Providing context through BatchedOperatorStats.\n- Adding a new interface extending StatsListener which defines processStats with additional argument, for this listeners new method will be called by platform. please suggest how can we implement this?\n', 'commenter': 'tushargosavi'}, {'comment': 'I have changed the processStats to accept additional argument by extending the interface to maintain backward compatibility. But this solution require us to implement two processStats functional one from old interface and one from new interface.\n', 'commenter': 'tushargosavi'}]"
410,engine/src/main/java/com/datatorrent/stram/StreamingContainerManager.java,"@@ -2904,6 +2905,8 @@ public LogicalPlan getLogicalPlan()
     return future;
   }
 
+","[{'comment': 'remove extra modifications\n', 'commenter': 'tweise'}, {'comment': 'done\n', 'commenter': 'tushargosavi'}]"
410,engine/src/main/java/com/datatorrent/stram/StreamingContainerManager.java,"@@ -2918,13 +2921,7 @@ public Object call() throws Exception
     {
       // clone logical plan, for dry run and validation
       LOG.info(""Begin plan changes: {}"", requests);
-      LogicalPlan lp = plan.getLogicalPlan();
-      ByteArrayOutputStream bos = new ByteArrayOutputStream();
-      LogicalPlan.write(lp, bos);
-      bos.flush();
-      ByteArrayInputStream bis = new ByteArrayInputStream(bos.toByteArray());
-      lp = LogicalPlan.read(bis);
-
+      LogicalPlan lp = plan.cloneLogicalPlan();","[{'comment': 'SerializationUtils.clone(...) ?\n', 'commenter': 'tweise'}, {'comment': 'done\n', 'commenter': 'tushargosavi'}]"
410,engine/src/main/java/com/datatorrent/stram/StreamingContainerManager.java,"@@ -3017,6 +3014,7 @@ public static StreamingContainerManager getInstance(RecoveryHandler rh, LogicalP
 
         // restore checkpoint info
         plan.syncCheckpoints(scm.vars.windowStartMillis, scm.clock.getTime());
+        plan.setContextAfterDAGRecovery();","[{'comment': 'This would not be needed when passing context with processStats.\n', 'commenter': 'tweise'}, {'comment': 'with additional parameter it is not needed. But let me know which is the preferred way?\n', 'commenter': 'tushargosavi'}]"
410,engine/src/main/java/com/datatorrent/stram/StreamingContainerManager.java,"@@ -3208,10 +3206,50 @@ public Object call() throws Exception
     return null;
   }
 
+  private class ChangeDAGRunnable implements Callable<Object>
+  {
+    private final transient DAGChangeSetImpl request;
+
+    public ChangeDAGRunnable(DAG.DAGChangeSet changes)
+    {
+      request = (DAGChangeSetImpl)changes;
+    }
+
+    @Override
+    public Object call() throws Exception
+    {
+      if (request == null) {
+        return null;
+      }
+
+      try {
+        LOG.info(""starting plan change "");
+        LogicalPlan lp = plan.cloneLogicalPlan();
+        PlanModifier pm = new PlanModifier(lp);
+        pm.applyDagChangeSet(request);
+        lp.validate();
+
+        LOG.info(""validated logical plan, now making actual change"");
+        pm = new PlanModifier(plan);
+        pm.applyDagChangeSet(request);
+        LOG.info(""plan change done"");
+        return null;
+      } finally {
+        plan.clearPendingChangeRequest();","[{'comment': 'why is this needed?\n', 'commenter': 'tweise'}, {'comment': 'At a time only one pending dag change request will be active. once the dag change request is processed successfully or unsuccessfully, it will be cleared so that new request can be submitted from statsListener. Stats listener has access to the feature through with it can check the status of last submitted dag change request.\n', 'commenter': 'tushargosavi'}]"
410,engine/src/main/java/com/datatorrent/stram/plan/logical/mod/DAGChangeSetImpl.java,"@@ -0,0 +1,269 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.stram.plan.logical.mod;
+
+import java.util.ArrayList;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.Stack;
+
+import com.google.common.collect.Maps;
+
+import com.datatorrent.api.DAG;
+import com.datatorrent.api.DAG.DAGChangeSet;
+import com.datatorrent.api.Operator;
+import com.datatorrent.stram.plan.logical.LogicalPlan;
+
+/**
+ * An implementation for DAGChangeSet. Instance of this object will be provided
+ * to StatsListener through context, stat listener can use this object to modify
+ * existing DAG and return modified DAG to engine.
+ */
+public class DAGChangeSetImpl extends LogicalPlan implements DAGChangeSet","[{'comment': 'Why is the change set not separate from DAG. Exsisting plan modification from the CLI works without such extension.\n', 'commenter': 'tweise'}, {'comment': 'Existing plan modification use different api to change the dag. As I am using same API with few additions (for example addSink to existing stream/remove operator) to specify DAG change, I have extended current DAG implementation (LogicalPlan) to support additional methods.\n', 'commenter': 'tushargosavi'}]"
410,engine/src/main/java/com/datatorrent/stram/plan/physical/PhysicalPlan.java,"@@ -1898,4 +1967,87 @@ public Integer getStreamCodecIdentifier(StreamCodec<?> streamCodecInfo)
       return null;
     }
   }
+
+  @VisibleForTesting
+  public LogicalPlan cloneLogicalPlan() throws IOException, ClassNotFoundException
+  {
+    LogicalPlan lp = getLogicalPlan();
+    ByteArrayOutputStream bos = new ByteArrayOutputStream();
+    LogicalPlan.write(lp, bos);
+    bos.flush();
+    ByteArrayInputStream bis = new ByteArrayInputStream(bos.toByteArray());
+    lp = LogicalPlan.read(bis);
+    return lp;
+  }
+
+
+  public void clearPendingChangeRequest()","[{'comment': 'Not clear why it is needed. Generally public methods should have javadoc.\n', 'commenter': 'tweise'}]"
410,engine/src/test/java/com/datatorrent/stram/engine/InputOperatorTest.java,"@@ -51,10 +52,13 @@
 
   public static class EvenOddIntegerGeneratorInputOperator implements InputOperator, com.datatorrent.api.Operator.ActivationListener<OperatorContext>
   {
+    @OutputPortFieldAnnotation(optional = true)","[{'comment': 'Is this still needed?', 'commenter': 'tweise'}]"
410,api/src/main/java/com/datatorrent/api/DAG.java,"@@ -288,4 +288,23 @@
   {
 
   }
+
+  /**
+   * This class provides functionality to extend existing DAG. The type of extensions allowed are
+   *
+   * <li>Add a new operator</li>
+   * <li>Remove an existing operator</li>
+   * <li>Add an operator to stream</li>
+   * <li>Add an stream between existing operator and new operator</li>
+   */
+  interface DAGChangeSet extends DAG
+  {
+    void removeOperator(String name);
+
+    void removeStream(String name);
+
+    StreamMeta extendStream(String id, Operator.InputPort... ports);
+
+    StreamMeta addStream(String id, String operatorName, String portName, Operator.InputPort... ports);","[{'comment': ""What's the reason to not specify the source as port object?"", 'commenter': 'tweise'}]"
410,api/src/main/java/com/datatorrent/api/DAG.java,"@@ -288,4 +288,23 @@
   {
 
   }
+
+  /**
+   * This class provides functionality to extend existing DAG. The type of extensions allowed are
+   *
+   * <li>Add a new operator</li>","[{'comment': 'Remove ""a"" and ""an""', 'commenter': 'tweise'}]"
410,engine/src/main/java/com/datatorrent/stram/StreamingContainerManager.java,"@@ -3199,13 +3194,40 @@ public Object call() throws Exception
     }
   }
 
-  @VisibleForTesting
-  protected Collection<Pair<Long, Map<String, Object>>> getLogicalMetrics(String operatorName)
+  private class ChangeDAGRunnable implements Callable<Object>
   {
-    if (logicalMetrics.get(operatorName) != null) {
-      return Collections.unmodifiableCollection(logicalMetrics.get(operatorName));
+    private final transient DAGChangeSetImpl request;
+
+    public ChangeDAGRunnable(DAG.DAGChangeSet changes)
+    {
+      request = (DAGChangeSetImpl)changes;
+    }
+
+    @Override
+    public Object call() throws Exception
+    {
+      if (request == null) {
+        return null;
+      }
+
+      try {
+        LOG.info(""starting plan change "");","[{'comment': '""logical plan change""', 'commenter': 'tweise'}]"
410,engine/src/main/java/com/datatorrent/stram/StreamingContainerManager.java,"@@ -3199,13 +3194,40 @@ public Object call() throws Exception
     }
   }
 
-  @VisibleForTesting
-  protected Collection<Pair<Long, Map<String, Object>>> getLogicalMetrics(String operatorName)
+  private class ChangeDAGRunnable implements Callable<Object>
   {
-    if (logicalMetrics.get(operatorName) != null) {
-      return Collections.unmodifiableCollection(logicalMetrics.get(operatorName));
+    private final transient DAGChangeSetImpl request;
+
+    public ChangeDAGRunnable(DAG.DAGChangeSet changes)
+    {
+      request = (DAGChangeSetImpl)changes;
+    }
+
+    @Override
+    public Object call() throws Exception
+    {
+      if (request == null) {
+        return null;
+      }
+
+      try {
+        LOG.info(""starting plan change "");
+        LogicalPlan lp = (LogicalPlan)SerializationUtils.clone(plan.getLogicalPlan());
+        PlanModifier pm = new PlanModifier(lp);
+        pm.applyDagChangeSet(request);
+        lp.validate();
+
+        LOG.info(""validated logical plan, now making actual change"");
+        pm = new PlanModifier(plan);
+        pm.applyDagChangeSet(request);
+        LOG.info(""plan change done"");
+        return null;
+      } finally {
+        /* At a time only one request can be active, once request is processed with or without error,
+         * clear the pendingDagChagneRequest so that new request can be submitted from the StatsListenerContext. */","[{'comment': 'spelling', 'commenter': 'tweise'}]"
410,engine/src/main/java/com/datatorrent/stram/plan/physical/PhysicalPlan.java,"@@ -1523,19 +1544,33 @@ public LogicalPlan getLogicalPlan()
    */
   public List<PTOperator> getOperators(OperatorMeta logicalOperator)
   {
+    /* During dynamic plan change, operators are added in logical plan first and then
+       physical plan is changed. There is a race, when REST api try to get information
+       about an operator which is not yet added in physical plan causes
+       NullPointerException here. null return value is handled at caller appropriately.
+     */
+    if (this.logicalToPTOperator.get(logicalOperator) == null) {
+      return null;
+    }
     return this.logicalToPTOperator.get(logicalOperator).partitions;
   }
 
   public Collection<PTOperator> getAllOperators(OperatorMeta logicalOperator)
   {
+    if (this.logicalToPTOperator.get(logicalOperator) == null) {
+      return null;
+    }
     return this.logicalToPTOperator.get(logicalOperator).getAllOperators();
   }
 
   public List<PTOperator> getLeafOperators()
   {
     List<PTOperator> operators = new ArrayList<>();
     for (OperatorMeta opMeta : dag.getLeafOperators()) {
-      operators.addAll(getAllOperators(opMeta));
+      Collection<PTOperator> allOpers = getAllOperators(opMeta);","[{'comment': 'why this?', 'commenter': 'tweise'}]"
410,engine/src/main/java/com/datatorrent/stram/plan/physical/PhysicalPlan.java,"@@ -1675,10 +1710,36 @@ public final void addLogicalOperator(OperatorMeta om)
     } else {
       initPartitioning(pnodes, 0);
     }
+    updateUpstreamsOutputMappings(om, null);","[{'comment': 'Spelling. Also not clear why it is needed. Is it to fix an existing bug or is it for newly introduced functionality?', 'commenter': 'tweise'}]"
410,engine/src/main/java/com/datatorrent/stram/plan/physical/PhysicalPlan.java,"@@ -1794,7 +1855,12 @@ public void setAvailableResources(int memoryMB)
   public void onStatusUpdate(PTOperator oper)
   {
     for (StatsListener l : oper.statsListeners) {
-      final StatsListener.Response rsp = l.processStats(oper.stats);
+      final StatsListener.Response rsp;
+      if (l instanceof StatsListener.StatsListenerWithContext) {","[{'comment': 'Would it be possible to internally handle all listeners through the new interface and adapt the old interface through adapter? We should also deprecate the old interface. ', 'commenter': 'tweise'}]"
410,engine/src/main/java/com/datatorrent/stram/plan/physical/PlanModifier.java,"@@ -280,4 +287,121 @@ public void applyChanges(PhysicalPlan.PlanContext physicalPlanContext)
     physicalPlan.deployChanges();
   }
 
+  public void applyDagChangeSet(DAG.DAGChangeSet dagChanges)
+  {
+    DAGChangeSetImpl dag = (DAGChangeSetImpl)dagChanges;
+    List<OperatorMeta> orderedOperators = dag.getOperatorsInOrder();
+    for (OperatorMeta om : orderedOperators) {
+      LOG.info(""Adding operator {}"", om.getName());
+      logicalPlan.addOperator(om.getName(), om.getOperator());
+      OperatorMeta newMeta = logicalPlan.getMeta(om.getOperator());
+      newMeta.copyAttributesFrom(om);
+    }
+
+    for (StreamMeta streamMeta : dag.getAllStreams()) {
+      LOG.info(""Adding stream {}"", streamMeta.getName());
+      StreamMeta sm = logicalPlan.getStream(streamMeta.getName());
+      if (sm == null) {
+        sm = logicalPlan.addStream(streamMeta.getName());
+      }
+      sm.setSource(streamMeta.getSource().getPortObject());
+      for (InputPortMeta sink : streamMeta.getSinks()) {
+        sm.addSink(sink.getPortObject());
+      }
+      sm.setLocality(streamMeta.getLocality());
+    }
+
+    /**
+     * Add streams which are exended
+     */
+    for (DAG.StreamMeta sm : dag.getExtendStreams().values()) {
+      LOG.info(""Extending streams {}"", sm.getName());
+      StreamMeta origStreamMeta = null;
+      Collection<InputPort> inputPorts = null;
+
+      if (sm instanceof DAGChangeSetImpl.StreamExtendBySource) {
+        DAGChangeSetImpl.StreamExtendBySource sm1 = (DAGChangeSetImpl.StreamExtendBySource)sm;
+        LOG.debug(""process StreamExtendBySource name {} operator {} port {}"", sm1.getName(), sm1.getOperatorName(), sm1.getPortName());
+        origStreamMeta = getSteramBySource(sm1.getOperatorName(), sm1.getPortName());
+        // If stream is not present in original dagChanges, then add it.
+        if (origStreamMeta == null) {
+          origStreamMeta = logicalPlan.addStream(sm.getName());
+          origStreamMeta.setSource(getPortObject(sm1.getOperatorName(), sm1.getPortName()));
+        }
+        inputPorts = sm1.getSinkPorts();
+      }
+
+      if (sm instanceof DAGChangeSetImpl.ExtendStreamMeta) {
+        DAGChangeSetImpl.ExtendStreamMeta esm = (DAGChangeSetImpl.ExtendStreamMeta)sm;
+        origStreamMeta = logicalPlan.getStream(esm.getName());
+        if (sm == null) {
+          throw new RuntimeException(""The stream to extend does not exists in original DAG"");
+        }
+        inputPorts = esm.getSinkPorts();
+      }
+
+      if (origStreamMeta != null && inputPorts != null) {
+        LOG.debug(""populating stream {} with sinks {}"", sm.getName(), inputPorts.size());
+        for (InputPort port : inputPorts) {
+          origStreamMeta.addSink(port);
+        }
+      }
+    }
+
+    if (physicalPlan != null) {
+
+      // start physical plan change
+      for (OperatorMeta om : orderedOperators) {
+        OperatorMeta newMeta = logicalPlan.getOperatorMeta(om.getName());
+        physicalPlan.addLogicalOperator(newMeta);
+      }
+
+      for (StreamMeta streamMeta : logicalPlan.getAllStreams()) {
+        for (InputPortMeta ipm : streamMeta.getSinks()) {
+          physicalPlan.connectInput(ipm);
+        }
+      }
+    }
+
+    for (String name : dag.getRemovedStreams()) {
+      LOG.info(""Removing stream {}"", name);
+      removeStream(name);
+    }
+
+    for (String name : dag.getRemovedOperators()) {
+      LOG.info(""Removing operator {}"", name);
+      removeOperator(name);
+    }
+
+    if (physicalPlan != null) {
+      physicalPlan.deployChanges();
+    }
+  }
+
+  StreamMeta getSteramBySource(String operatorName, String portName)","[{'comment': 'spelling', 'commenter': 'tweise'}]"
410,engine/src/test/java/com/datatorrent/stram/engine/InputOperatorTest.java,"@@ -51,10 +52,13 @@
 
   public static class EvenOddIntegerGeneratorInputOperator implements InputOperator, com.datatorrent.api.Operator.ActivationListener<OperatorContext>
   {
+    @OutputPortFieldAnnotation(optional = true)
     public final transient DefaultOutputPort<Integer> even = new DefaultOutputPort<Integer>();
+    @OutputPortFieldAnnotation(optional = true)","[{'comment': 'ditto', 'commenter': 'tweise'}]"
411,api/src/main/java/com/datatorrent/api/DefaultOutputPort.java,"@@ -32,6 +32,8 @@
 public class DefaultOutputPort<T> implements Operator.OutputPort<T>
 {
   private transient Sink<Object> sink;
+  private transient Thread operatorThread;
+  public static final String DT_DISABLE_CHECK = ""apex.port.emit.check.disable"";","[{'comment': 'Please change to ""com.datatorrent.api.DefaultOutputPort.thread.check.disable"" or ""org.apache.apex..."". It iwll be good to avoid using DT prefix in the name.\n', 'commenter': 'vrozov'}, {'comment': 'Done\n', 'commenter': 'sanjaypujare'}]"
411,api/src/main/java/com/datatorrent/api/DefaultOutputPort.java,"@@ -48,6 +50,12 @@ public DefaultOutputPort()
    */
   public void emit(T tuple)
   {
+    // operatorThread could be null if setup() never got called.
+    if (operatorThread != null && Thread.currentThread() != operatorThread) {
+      // only under certain modes: enforce this
+      throw new IllegalStateException(""emit() not called from operator thread!, currentThread="" +","[{'comment': '```\nthrow new IllegalStateException(""Current thread "" + Thread.currentThread().getName() + "" is different from the operator thread "" + operatorThread.getName());\n```\n', 'commenter': 'vrozov'}, {'comment': 'Done\n', 'commenter': 'sanjaypujare'}]"
411,api/src/test/java/com/datatorrent/api/DefaultOuputPortTest.java,"@@ -0,0 +1,122 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.api;
+
+import org.junit.Assert;
+import org.junit.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+
+/**
+ *
+ */
+public class DefaultOuputPortTest
+{
+
+  DefaultOutputPort<Object> port;
+
+  /**
+   * Same thread for setup() and emit()
+   */
+  @Test
+  public void testSameThreadForSetupAndEmit()
+  {
+    port = new DefaultOutputPort<Object>();
+    port.setup(null);
+    port.emit(null);
+    // if it comes here it passes","[{'comment': 'It relies on the DefaultOutputPort.sink to be initialized to BLACKHOLE. It will be better to have custom Sink implementation.\n', 'commenter': 'vrozov'}, {'comment': 'Done.\n', 'commenter': 'sanjaypujare'}]"
411,api/src/test/java/com/datatorrent/api/DefaultOuputPortTest.java,"@@ -0,0 +1,122 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.api;
+
+import org.junit.Assert;
+import org.junit.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+
+/**
+ *
+ */
+public class DefaultOuputPortTest
+{
+
+  DefaultOutputPort<Object> port;
+
+  /**
+   * Same thread for setup() and emit()
+   */
+  @Test
+  public void testSameThreadForSetupAndEmit()
+  {
+    port = new DefaultOutputPort<Object>();
+    port.setup(null);
+    port.emit(null);
+    // if it comes here it passes
+  }
+
+  /**
+   * setup() not called : null thread object should not cause exception
+   */
+  @Test
+  public void testSetupNotCalledAndEmit()
+  {
+    port = new DefaultOutputPort<Object>();
+    port.emit(null);
+    // if it comes here it passes
+  }
+
+  boolean pass = false;
+
+  /**
+   * Different thread for setup() and emit()
+   * @throws InterruptedException
+   */
+  @Test
+  public void testDifferentThreadForSetupAndEmit() throws InterruptedException
+  {
+    System.clearProperty(DefaultOutputPort.DT_DISABLE_CHECK);  // do not suppress the check
+    pass = false;
+    port = new DefaultOutputPort<Object>();
+    port.setup(null);
+    Thread thread = new Thread(new Runnable()","[{'comment': 'Runnable is not needed here.\n', 'commenter': 'vrozov'}, {'comment': 'Done\n', 'commenter': 'sanjaypujare'}]"
411,api/src/test/java/com/datatorrent/api/DefaultOuputPortTest.java,"@@ -0,0 +1,122 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.api;
+
+import org.junit.Assert;
+import org.junit.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+
+/**
+ *
+ */
+public class DefaultOuputPortTest
+{
+
+  DefaultOutputPort<Object> port;
+
+  /**
+   * Same thread for setup() and emit()
+   */
+  @Test
+  public void testSameThreadForSetupAndEmit()
+  {
+    port = new DefaultOutputPort<Object>();
+    port.setup(null);
+    port.emit(null);
+    // if it comes here it passes
+  }
+
+  /**
+   * setup() not called : null thread object should not cause exception
+   */
+  @Test
+  public void testSetupNotCalledAndEmit()
+  {
+    port = new DefaultOutputPort<Object>();
+    port.emit(null);
+    // if it comes here it passes
+  }
+
+  boolean pass = false;
+
+  /**
+   * Different thread for setup() and emit()
+   * @throws InterruptedException
+   */
+  @Test
+  public void testDifferentThreadForSetupAndEmit() throws InterruptedException
+  {
+    System.clearProperty(DefaultOutputPort.DT_DISABLE_CHECK);  // do not suppress the check
+    pass = false;
+    port = new DefaultOutputPort<Object>();
+    port.setup(null);
+    Thread thread = new Thread(new Runnable()
+    {
+
+      @Override
+      public void run()
+      {
+        try {
+          port.emit(null);
+          Assert.fail(""No exception thrown!"");","[{'comment': 'The Assert has no impact.\n', 'commenter': 'vrozov'}, {'comment': 'Done\n', 'commenter': 'sanjaypujare'}]"
411,api/src/test/java/com/datatorrent/api/DefaultOuputPortTest.java,"@@ -0,0 +1,122 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.api;
+
+import org.junit.Assert;
+import org.junit.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+
+/**
+ *
+ */
+public class DefaultOuputPortTest
+{
+
+  DefaultOutputPort<Object> port;
+
+  /**
+   * Same thread for setup() and emit()
+   */
+  @Test
+  public void testSameThreadForSetupAndEmit()
+  {
+    port = new DefaultOutputPort<Object>();","[{'comment': 'Consider initializing port in setup().\n', 'commenter': 'vrozov'}, {'comment': 'Done. (port set in @Before method)\n', 'commenter': 'sanjaypujare'}]"
411,api/src/test/java/com/datatorrent/api/DefaultOuputPortTest.java,"@@ -0,0 +1,122 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.api;
+
+import org.junit.Assert;
+import org.junit.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+
+/**
+ *
+ */
+public class DefaultOuputPortTest
+{
+
+  DefaultOutputPort<Object> port;
+
+  /**
+   * Same thread for setup() and emit()
+   */
+  @Test
+  public void testSameThreadForSetupAndEmit()
+  {
+    port = new DefaultOutputPort<Object>();
+    port.setup(null);
+    port.emit(null);
+    // if it comes here it passes
+  }
+
+  /**
+   * setup() not called : null thread object should not cause exception
+   */
+  @Test
+  public void testSetupNotCalledAndEmit()
+  {
+    port = new DefaultOutputPort<Object>();
+    port.emit(null);
+    // if it comes here it passes
+  }
+
+  boolean pass = false;
+
+  /**
+   * Different thread for setup() and emit()
+   * @throws InterruptedException
+   */
+  @Test
+  public void testDifferentThreadForSetupAndEmit() throws InterruptedException
+  {
+    System.clearProperty(DefaultOutputPort.DT_DISABLE_CHECK);  // do not suppress the check
+    pass = false;
+    port = new DefaultOutputPort<Object>();
+    port.setup(null);
+    Thread thread = new Thread(new Runnable()
+    {
+
+      @Override
+      public void run()
+      {
+        try {
+          port.emit(null);
+          Assert.fail(""No exception thrown!"");
+        } catch (IllegalStateException ise) {
+          pass = ise.getMessage().startsWith(""emit() not called from operator thread!"");
+        }
+      }
+    });
+    thread.start();
+    thread.join();
+    Assert.assertTrue(""same thread check didn't take place!"", pass);
+  }
+
+  /**
+   * Different thread for setup() and emit() but suppress check property set
+   * @throws InterruptedException
+   */
+  @Test
+  public void testDifferentThreadForSetupAndEmit_CheckSuppressed() throws InterruptedException
+  {
+    System.setProperty(DefaultOutputPort.DT_DISABLE_CHECK, ""true"");  // suppress the check
+    pass = false;
+    port = new DefaultOutputPort<Object>();
+    port.setup(null);
+    Thread thread = new Thread(new Runnable()
+    {
+
+      @Override
+      public void run()
+      {
+        try {
+          port.emit(null);","[{'comment': 'It will be better to rely on custom Sink implementation to check for emit(). This will avoid introducing `pass` that needs to be declared volatile.\n', 'commenter': 'vrozov'}, {'comment': 'Done. (pass is still used to check that the exception is not thrown)\n', 'commenter': 'sanjaypujare'}]"
411,api/src/test/java/com/datatorrent/api/DefaultOuputPortTest.java,"@@ -0,0 +1,142 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.api;
+
+import org.junit.Assert;
+import org.junit.Before;
+import org.junit.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+
+/**
+ *
+ */
+public class DefaultOuputPortTest
+{
+
+  DefaultOutputPort<Object> port;
+  Sink<Object> sink;
+
+  @Before
+  public void setupTest()
+  {
+    port = new DefaultOutputPort<Object>();
+    sink = new Sink<Object>()
+    {
+      private int count = 0;
+
+      @Override
+      public void put(Object tuple)
+      {
+        count++;
+      }
+
+      @Override
+      public int getCount(boolean reset)
+      {
+        return count;
+      }
+    };
+    port.setSink(sink);
+  }
+
+  /**
+   * Same thread for setup() and emit()
+   */
+  @Test
+  public void testSameThreadForSetupAndEmit()
+  {
+    port.setup(null);
+    port.emit(null);
+    // if it comes here it passes","[{'comment': 'assert on sink.getCount().\n', 'commenter': 'vrozov'}]"
411,api/src/test/java/com/datatorrent/api/DefaultOuputPortTest.java,"@@ -0,0 +1,142 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.api;
+
+import org.junit.Assert;
+import org.junit.Before;
+import org.junit.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+
+/**
+ *
+ */
+public class DefaultOuputPortTest
+{
+
+  DefaultOutputPort<Object> port;
+  Sink<Object> sink;
+
+  @Before
+  public void setupTest()
+  {
+    port = new DefaultOutputPort<Object>();
+    sink = new Sink<Object>()
+    {
+      private int count = 0;
+
+      @Override
+      public void put(Object tuple)
+      {
+        count++;
+      }
+
+      @Override
+      public int getCount(boolean reset)
+      {
+        return count;
+      }
+    };
+    port.setSink(sink);
+  }
+
+  /**
+   * Same thread for setup() and emit()
+   */
+  @Test
+  public void testSameThreadForSetupAndEmit()
+  {
+    port.setup(null);
+    port.emit(null);
+    // if it comes here it passes
+  }
+
+  /**
+   * setup() not called : null thread object should not cause exception
+   */
+  @Test
+  public void testSetupNotCalledAndEmit()
+  {
+    port.emit(null);
+    // if it comes here it passes","[{'comment': 'assert on sink.getCount().\n', 'commenter': 'vrozov'}]"
411,api/src/test/java/com/datatorrent/api/DefaultOuputPortTest.java,"@@ -0,0 +1,142 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.api;
+
+import org.junit.Assert;
+import org.junit.Before;
+import org.junit.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+
+/**
+ *
+ */
+public class DefaultOuputPortTest
+{
+
+  DefaultOutputPort<Object> port;
+  Sink<Object> sink;
+
+  @Before
+  public void setupTest()
+  {
+    port = new DefaultOutputPort<Object>();
+    sink = new Sink<Object>()
+    {
+      private int count = 0;","[{'comment': 'volatile\n', 'commenter': 'vrozov'}]"
411,api/src/test/java/com/datatorrent/api/DefaultOuputPortTest.java,"@@ -0,0 +1,142 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.api;
+
+import org.junit.Assert;
+import org.junit.Before;
+import org.junit.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+
+/**
+ *
+ */
+public class DefaultOuputPortTest
+{
+
+  DefaultOutputPort<Object> port;
+  Sink<Object> sink;
+
+  @Before
+  public void setupTest()
+  {
+    port = new DefaultOutputPort<Object>();
+    sink = new Sink<Object>()
+    {
+      private int count = 0;
+
+      @Override
+      public void put(Object tuple)
+      {
+        count++;
+      }
+
+      @Override
+      public int getCount(boolean reset)
+      {
+        return count;
+      }
+    };
+    port.setSink(sink);
+  }
+
+  /**
+   * Same thread for setup() and emit()
+   */
+  @Test
+  public void testSameThreadForSetupAndEmit()
+  {
+    port.setup(null);
+    port.emit(null);
+    // if it comes here it passes
+  }
+
+  /**
+   * setup() not called : null thread object should not cause exception
+   */
+  @Test
+  public void testSetupNotCalledAndEmit()
+  {
+    port.emit(null);
+    // if it comes here it passes
+  }
+
+  boolean pass = false;
+
+  /**
+   * Different thread for setup() and emit()
+   * @throws InterruptedException
+   */
+  @Test
+  public void testDifferentThreadForSetupAndEmit() throws InterruptedException
+  {
+    System.clearProperty(DefaultOutputPort.THREAD_AFFINITY_DISABLE_CHECK);  // do not suppress the check
+    pass = false;
+    port.setup(null);
+    Thread thread = new Thread(""test-thread-xyz"")
+    {
+
+      @Override
+      public void run()
+      {
+        try {
+          port.emit(null);
+        } catch (IllegalStateException ise) {
+          pass = ise.getMessage().startsWith(""Current thread test-thread-xyz is different from the operator thread "");
+        }
+      }
+    };
+    thread.start();
+    thread.join();
+    Assert.assertTrue(""same thread check didn't take place!"", pass);","[{'comment': 'assert on sink.getCount().\n', 'commenter': 'vrozov'}]"
411,api/src/test/java/com/datatorrent/api/DefaultOuputPortTest.java,"@@ -0,0 +1,142 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.api;
+
+import org.junit.Assert;
+import org.junit.Before;
+import org.junit.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+
+/**
+ *
+ */
+public class DefaultOuputPortTest
+{
+
+  DefaultOutputPort<Object> port;
+  Sink<Object> sink;
+
+  @Before
+  public void setupTest()
+  {
+    port = new DefaultOutputPort<Object>();
+    sink = new Sink<Object>()
+    {
+      private int count = 0;
+
+      @Override
+      public void put(Object tuple)
+      {
+        count++;
+      }
+
+      @Override
+      public int getCount(boolean reset)
+      {
+        return count;
+      }
+    };
+    port.setSink(sink);
+  }
+
+  /**
+   * Same thread for setup() and emit()
+   */
+  @Test
+  public void testSameThreadForSetupAndEmit()
+  {
+    port.setup(null);
+    port.emit(null);
+    // if it comes here it passes
+  }
+
+  /**
+   * setup() not called : null thread object should not cause exception
+   */
+  @Test
+  public void testSetupNotCalledAndEmit()
+  {
+    port.emit(null);
+    // if it comes here it passes
+  }
+
+  boolean pass = false;","[{'comment': 'volatile\n', 'commenter': 'vrozov'}]"
421,apex-app-archetype/src/main/resources/META-INF/maven/archetype-metadata.xml,"@@ -50,6 +50,12 @@
       </includes>
     </fileSet>
     <fileSet filtered=""true"" encoding=""UTF-8"">
+      <directory>src/main/resources</directory>
+      <includes>
+        <include>**/log4j.props</include>","[{'comment': ""Why can't we name the file log4j.properties i.e. with the standard suffix?"", 'commenter': 'sanjaypujare'}, {'comment': 'why do we not use the de facto standard name log4j.properties?', 'commenter': 'davidyan74'}, {'comment': 'When I named it as log4j.properties I was facing issue, and always hadoop log4j.properties was getting picked up. After your and david\'s comment, I tried updating classpath to include ""current"" folder first and it worked. \r\n\r\nSo I will update PR to use log4j.properties and update classpath. @davidyan74  please check this and let me know if updating classpath might have any other side-effects.', 'commenter': 'DT-Priyanka'}, {'comment': 'please check my previous comment.', 'commenter': 'DT-Priyanka'}, {'comment': 'Sorry for confusion, but when I renamed the file to log4j.properties everything worked well with Apex. For further testing I created datatorrent-rts installer. Now event the ""apex"" script tries to use new log4j.properties file. Previously it used to use default log4j.properties shipped by hadoop and use CONSOLE appender as per defaults there. But now as it picks our new log4j.properties it tries to load APEXRFA appender and tries to write in some log file which is throwing exception.\r\n\r\nFor clarity and simplicity I believe better we have different log file name may be like apexlog4j.properties. @davidyan74 @sanjaypujare  please comment on this issue.\r\n\r\nI will change log4j name again once I hear back your comments.', 'commenter': 'DT-Priyanka'}, {'comment': 'Hmmm, I would have still preferred to use log4j.properties as that is the standard. But looking at the issues you faced I am okay to use apexlog4j.properties.', 'commenter': 'sanjaypujare'}]"
421,apex-app-archetype/src/main/resources/META-INF/maven/archetype.xml,"@@ -30,6 +30,9 @@
   <resources>
     <resource>src/main/resources/META-INF/*</resource>
   </resources>
+  <resources>
+    <resource>src/main/resources/log4j.props</resource>","[{'comment': 'Same as above - why not name it log4j.properties?', 'commenter': 'sanjaypujare'}]"
421,apex-app-archetype/src/main/resources/archetype-resources/src/main/resources/META-INF/properties.xml,"@@ -13,6 +13,10 @@
   </property>
   -->
   <property>
+    <name>dt.attr.CONTAINER_JVM_OPTIONS</name>
+    <value>-Dlog4j.configuration=log4j.props</value>
+  </property>
+  <property>","[{'comment': ""If you had named the file log4j.properties, you wouldn't need to pass this option to the JVM, isn't that right? I am trying to understand if there is a reason to use a non-standard name and then an additional option because of that."", 'commenter': 'sanjaypujare'}, {'comment': 'right, changing to log4j.properties. Need to make classpath changes to use name log4j.properties', 'commenter': 'DT-Priyanka'}]"
421,apex-app-archetype/src/main/resources/archetype-resources/src/main/resources/log4j.props,"@@ -0,0 +1,278 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# ""License""); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+# Define some default values that can be overridden by system properties
+hadoop.root.logger=INFO,console
+hadoop.log.dir=.
+hadoop.log.file=hadoop.log
+
+# Define the root logger to the system property ""hadoop.root.logger"".
+log4j.rootLogger=INFO,APEXRFA, EventCounter
+
+# Logging Threshold
+log4j.threshold=ALL
+
+# Null Appender
+log4j.appender.NullAppender=org.apache.log4j.varia.NullAppender
+
+#
+# Rolling File Appender - cap space usage at 5gb.
+#
+hadoop.log.maxfilesize=256MB
+hadoop.log.maxbackupindex=20
+log4j.appender.RFA=org.apache.log4j.RollingFileAppender
+log4j.appender.RFA.File=${hadoop.log.dir}/${hadoop.log.file}
+
+log4j.appender.RFA.MaxFileSize=${hadoop.log.maxfilesize}
+log4j.appender.RFA.MaxBackupIndex=${hadoop.log.maxbackupindex}
+
+log4j.appender.RFA.layout=org.apache.log4j.PatternLayout
+
+# Pattern format: Date LogLevel LoggerName LogMessage
+log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
+# Debugging Pattern format
+#log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n
+
+#
+# APEX Rolling File Appender
+#
+log4j.appender.APEXRFA=org.apache.apex.log.appender.ApexRollingFileAppender
+log4j.appender.APEXRFA.File=${hadoop.log.dir}/${hadoop.log.file}
+
+log4j.appender.APEXRFA.MaxFileSize=${hadoop.log.maxfilesize}
+log4j.appender.APEXRFA.MaxBackupIndex=${hadoop.log.maxbackupindex}
+
+log4j.appender.APEXRFA.layout=org.apache.log4j.PatternLayout
+
+# Pattern format: Date LogLevel LoggerName LogMessage
+log4j.appender.APEXRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
+
+#
+# Daily Rolling File Appender
+#
+
+log4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender
+log4j.appender.DRFA.File=${hadoop.log.dir}/${hadoop.log.file}
+
+# Rollver at midnight
+log4j.appender.DRFA.DatePattern=.yyyy-MM-dd
+
+# 30-day backup
+#log4j.appender.DRFA.MaxBackupIndex=30
+log4j.appender.DRFA.layout=org.apache.log4j.PatternLayout
+
+# Pattern format: Date LogLevel LoggerName LogMessage
+log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
+# Debugging Pattern format
+#log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n
+
+
+#
+# console
+# Add ""console"" to rootlogger above if you want to use this 
+#
+
+log4j.appender.console=org.apache.log4j.ConsoleAppender
+log4j.appender.console.target=System.err
+log4j.appender.console.layout=org.apache.log4j.PatternLayout
+log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n
+
+#
+# TaskLog Appender
+#
+
+#Default values
+hadoop.tasklog.taskid=null
+hadoop.tasklog.iscleanup=false
+hadoop.tasklog.noKeepSplits=4
+hadoop.tasklog.totalLogFileSize=100
+hadoop.tasklog.purgeLogSplits=true
+hadoop.tasklog.logsRetainHours=12
+
+log4j.appender.TLA=org.apache.hadoop.mapred.TaskLogAppender
+log4j.appender.TLA.taskId=${hadoop.tasklog.taskid}
+log4j.appender.TLA.isCleanup=${hadoop.tasklog.iscleanup}
+log4j.appender.TLA.totalLogFileSize=${hadoop.tasklog.totalLogFileSize}
+
+log4j.appender.TLA.layout=org.apache.log4j.PatternLayout
+log4j.appender.TLA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
+
+#
+# HDFS block state change log from block manager
+#
+# Uncomment the following to suppress normal block state change
+# messages from BlockManager in NameNode.
+#log4j.logger.BlockStateChange=WARN
+
+#
+#Security appender
+#
+hadoop.security.logger=INFO,NullAppender
+hadoop.security.log.maxfilesize=256MB
+hadoop.security.log.maxbackupindex=20
+log4j.category.SecurityLogger=${hadoop.security.logger}
+hadoop.security.log.file=SecurityAuth-${user.name}.audit
+log4j.appender.RFAS=org.apache.log4j.RollingFileAppender 
+log4j.appender.RFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}
+log4j.appender.RFAS.layout=org.apache.log4j.PatternLayout
+log4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
+log4j.appender.RFAS.MaxFileSize=${hadoop.security.log.maxfilesize}
+log4j.appender.RFAS.MaxBackupIndex=${hadoop.security.log.maxbackupindex}
+
+#
+# Daily Rolling Security appender
+#
+log4j.appender.DRFAS=org.apache.log4j.DailyRollingFileAppender 
+log4j.appender.DRFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}
+log4j.appender.DRFAS.layout=org.apache.log4j.PatternLayout
+log4j.appender.DRFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
+log4j.appender.DRFAS.DatePattern=.yyyy-MM-dd
+
+#
+# hadoop configuration logging
+#
+
+# Uncomment the following line to turn off configuration deprecation warnings.
+# log4j.logger.org.apache.hadoop.conf.Configuration.deprecation=WARN
+
+#
+# hdfs audit logging
+#
+hdfs.audit.logger=INFO,NullAppender
+hdfs.audit.log.maxfilesize=256MB
+hdfs.audit.log.maxbackupindex=20
+log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=${hdfs.audit.logger}
+log4j.additivity.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=false
+log4j.appender.RFAAUDIT=org.apache.log4j.RollingFileAppender
+log4j.appender.RFAAUDIT.File=${hadoop.log.dir}/hdfs-audit.log
+log4j.appender.RFAAUDIT.layout=org.apache.log4j.PatternLayout
+log4j.appender.RFAAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
+log4j.appender.RFAAUDIT.MaxFileSize=${hdfs.audit.log.maxfilesize}
+log4j.appender.RFAAUDIT.MaxBackupIndex=${hdfs.audit.log.maxbackupindex}
+
+#
+# mapred audit logging
+#
+mapred.audit.logger=INFO,NullAppender
+mapred.audit.log.maxfilesize=256MB
+mapred.audit.log.maxbackupindex=20
+log4j.logger.org.apache.hadoop.mapred.AuditLogger=${mapred.audit.logger}
+log4j.additivity.org.apache.hadoop.mapred.AuditLogger=false
+log4j.appender.MRAUDIT=org.apache.log4j.RollingFileAppender
+log4j.appender.MRAUDIT.File=${hadoop.log.dir}/mapred-audit.log
+log4j.appender.MRAUDIT.layout=org.apache.log4j.PatternLayout
+log4j.appender.MRAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
+log4j.appender.MRAUDIT.MaxFileSize=${mapred.audit.log.maxfilesize}
+log4j.appender.MRAUDIT.MaxBackupIndex=${mapred.audit.log.maxbackupindex}
+
+# Custom Logging levels
+
+#log4j.logger.org.apache.hadoop.mapred.JobTracker=DEBUG
+#log4j.logger.org.apache.hadoop.mapred.TaskTracker=DEBUG
+#log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=DEBUG
+
+# Jets3t library
+log4j.logger.org.jets3t.service.impl.rest.httpclient.RestS3Service=ERROR
+
+#
+# Event Counter Appender
+# Sends counts of logging messages at different severity levels to Hadoop Metrics.
+#
+log4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter
+
+#
+# Job Summary Appender 
+#
+# Use following logger to send summary to separate file defined by 
+# hadoop.mapreduce.jobsummary.log.file :
+# hadoop.mapreduce.jobsummary.logger=INFO,JSA
+# 
+hadoop.mapreduce.jobsummary.logger=${hadoop.root.logger}
+hadoop.mapreduce.jobsummary.log.file=hadoop-mapreduce.jobsummary.log
+hadoop.mapreduce.jobsummary.log.maxfilesize=256MB
+hadoop.mapreduce.jobsummary.log.maxbackupindex=20
+log4j.appender.JSA=org.apache.log4j.RollingFileAppender
+log4j.appender.JSA.File=${hadoop.log.dir}/${hadoop.mapreduce.jobsummary.log.file}
+log4j.appender.JSA.MaxFileSize=${hadoop.mapreduce.jobsummary.log.maxfilesize}
+log4j.appender.JSA.MaxBackupIndex=${hadoop.mapreduce.jobsummary.log.maxbackupindex}
+log4j.appender.JSA.layout=org.apache.log4j.PatternLayout
+log4j.appender.JSA.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n
+log4j.logger.org.apache.hadoop.mapred.JobInProgress$JobSummary=${hadoop.mapreduce.jobsummary.logger}
+log4j.additivity.org.apache.hadoop.mapred.JobInProgress$JobSummary=false
+
+#
+# Yarn ResourceManager Application Summary Log 
+#
+# Set the ResourceManager summary log filename
+yarn.server.resourcemanager.appsummary.log.file=rm-appsummary.log
+# Set the ResourceManager summary log level and appender
+yarn.server.resourcemanager.appsummary.logger=${hadoop.root.logger}
+#yarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY","[{'comment': 'Why have both the commented and uncommented lines?', 'commenter': 'sanjaypujare'}, {'comment': 'I have copied this file from hadoop conf and just added our APEXRFA appender to it. Not updating any other appender settings.\r\nDo you think we should update/remove other appender settings?', 'commenter': 'DT-Priyanka'}]"
421,apex-conf-archetype/src/main/resources/archetype-resources/src/main/resources/log4j.props,"@@ -0,0 +1,278 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# ""License""); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+# Define some default values that can be overridden by system properties
+hadoop.root.logger=INFO,console
+hadoop.log.dir=.
+hadoop.log.file=hadoop.log
+
+# Define the root logger to the system property ""hadoop.root.logger"".
+log4j.rootLogger=INFO,APEXRFA, EventCounter
+
+# Logging Threshold
+log4j.threshold=ALL
+
+# Null Appender
+log4j.appender.NullAppender=org.apache.log4j.varia.NullAppender
+
+#
+# Rolling File Appender - cap space usage at 5gb.
+#
+hadoop.log.maxfilesize=256MB
+hadoop.log.maxbackupindex=20
+log4j.appender.RFA=org.apache.log4j.RollingFileAppender
+log4j.appender.RFA.File=${hadoop.log.dir}/${hadoop.log.file}
+
+log4j.appender.RFA.MaxFileSize=${hadoop.log.maxfilesize}
+log4j.appender.RFA.MaxBackupIndex=${hadoop.log.maxbackupindex}
+
+log4j.appender.RFA.layout=org.apache.log4j.PatternLayout
+
+# Pattern format: Date LogLevel LoggerName LogMessage
+log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
+# Debugging Pattern format
+#log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n
+
+#
+# APEX Rolling File Appender
+#
+log4j.appender.APEXRFA=org.apache.apex.log.appender.ApexRollingFileAppender
+log4j.appender.APEXRFA.File=${hadoop.log.dir}/${hadoop.log.file}
+
+log4j.appender.APEXRFA.MaxFileSize=${hadoop.log.maxfilesize}
+log4j.appender.APEXRFA.MaxBackupIndex=${hadoop.log.maxbackupindex}
+
+log4j.appender.APEXRFA.layout=org.apache.log4j.PatternLayout
+
+# Pattern format: Date LogLevel LoggerName LogMessage
+log4j.appender.APEXRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
+
+#
+# Daily Rolling File Appender
+#
+
+log4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender
+log4j.appender.DRFA.File=${hadoop.log.dir}/${hadoop.log.file}
+
+# Rollver at midnight
+log4j.appender.DRFA.DatePattern=.yyyy-MM-dd
+
+# 30-day backup
+#log4j.appender.DRFA.MaxBackupIndex=30
+log4j.appender.DRFA.layout=org.apache.log4j.PatternLayout
+
+# Pattern format: Date LogLevel LoggerName LogMessage
+log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
+# Debugging Pattern format
+#log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n
+
+
+#
+# console
+# Add ""console"" to rootlogger above if you want to use this 
+#
+
+log4j.appender.console=org.apache.log4j.ConsoleAppender
+log4j.appender.console.target=System.err
+log4j.appender.console.layout=org.apache.log4j.PatternLayout
+log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n
+
+#
+# TaskLog Appender
+#
+
+#Default values
+hadoop.tasklog.taskid=null
+hadoop.tasklog.iscleanup=false
+hadoop.tasklog.noKeepSplits=4
+hadoop.tasklog.totalLogFileSize=100
+hadoop.tasklog.purgeLogSplits=true
+hadoop.tasklog.logsRetainHours=12
+
+log4j.appender.TLA=org.apache.hadoop.mapred.TaskLogAppender
+log4j.appender.TLA.taskId=${hadoop.tasklog.taskid}
+log4j.appender.TLA.isCleanup=${hadoop.tasklog.iscleanup}
+log4j.appender.TLA.totalLogFileSize=${hadoop.tasklog.totalLogFileSize}
+
+log4j.appender.TLA.layout=org.apache.log4j.PatternLayout
+log4j.appender.TLA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
+
+#
+# HDFS block state change log from block manager
+#
+# Uncomment the following to suppress normal block state change
+# messages from BlockManager in NameNode.
+#log4j.logger.BlockStateChange=WARN
+
+#
+#Security appender
+#
+hadoop.security.logger=INFO,NullAppender
+hadoop.security.log.maxfilesize=256MB
+hadoop.security.log.maxbackupindex=20
+log4j.category.SecurityLogger=${hadoop.security.logger}
+hadoop.security.log.file=SecurityAuth-${user.name}.audit
+log4j.appender.RFAS=org.apache.log4j.RollingFileAppender 
+log4j.appender.RFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}
+log4j.appender.RFAS.layout=org.apache.log4j.PatternLayout
+log4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
+log4j.appender.RFAS.MaxFileSize=${hadoop.security.log.maxfilesize}
+log4j.appender.RFAS.MaxBackupIndex=${hadoop.security.log.maxbackupindex}
+
+#
+# Daily Rolling Security appender
+#
+log4j.appender.DRFAS=org.apache.log4j.DailyRollingFileAppender 
+log4j.appender.DRFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}
+log4j.appender.DRFAS.layout=org.apache.log4j.PatternLayout
+log4j.appender.DRFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
+log4j.appender.DRFAS.DatePattern=.yyyy-MM-dd
+
+#
+# hadoop configuration logging
+#
+
+# Uncomment the following line to turn off configuration deprecation warnings.
+# log4j.logger.org.apache.hadoop.conf.Configuration.deprecation=WARN
+
+#
+# hdfs audit logging
+#
+hdfs.audit.logger=INFO,NullAppender
+hdfs.audit.log.maxfilesize=256MB
+hdfs.audit.log.maxbackupindex=20
+log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=${hdfs.audit.logger}
+log4j.additivity.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=false
+log4j.appender.RFAAUDIT=org.apache.log4j.RollingFileAppender
+log4j.appender.RFAAUDIT.File=${hadoop.log.dir}/hdfs-audit.log
+log4j.appender.RFAAUDIT.layout=org.apache.log4j.PatternLayout
+log4j.appender.RFAAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
+log4j.appender.RFAAUDIT.MaxFileSize=${hdfs.audit.log.maxfilesize}
+log4j.appender.RFAAUDIT.MaxBackupIndex=${hdfs.audit.log.maxbackupindex}
+
+#
+# mapred audit logging
+#
+mapred.audit.logger=INFO,NullAppender
+mapred.audit.log.maxfilesize=256MB
+mapred.audit.log.maxbackupindex=20
+log4j.logger.org.apache.hadoop.mapred.AuditLogger=${mapred.audit.logger}
+log4j.additivity.org.apache.hadoop.mapred.AuditLogger=false
+log4j.appender.MRAUDIT=org.apache.log4j.RollingFileAppender
+log4j.appender.MRAUDIT.File=${hadoop.log.dir}/mapred-audit.log
+log4j.appender.MRAUDIT.layout=org.apache.log4j.PatternLayout
+log4j.appender.MRAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
+log4j.appender.MRAUDIT.MaxFileSize=${mapred.audit.log.maxfilesize}
+log4j.appender.MRAUDIT.MaxBackupIndex=${mapred.audit.log.maxbackupindex}
+
+# Custom Logging levels
+
+#log4j.logger.org.apache.hadoop.mapred.JobTracker=DEBUG
+#log4j.logger.org.apache.hadoop.mapred.TaskTracker=DEBUG
+#log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=DEBUG
+
+# Jets3t library
+log4j.logger.org.jets3t.service.impl.rest.httpclient.RestS3Service=ERROR
+
+#
+# Event Counter Appender
+# Sends counts of logging messages at different severity levels to Hadoop Metrics.
+#
+log4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter
+
+#
+# Job Summary Appender 
+#
+# Use following logger to send summary to separate file defined by 
+# hadoop.mapreduce.jobsummary.log.file :
+# hadoop.mapreduce.jobsummary.logger=INFO,JSA
+# 
+hadoop.mapreduce.jobsummary.logger=${hadoop.root.logger}
+hadoop.mapreduce.jobsummary.log.file=hadoop-mapreduce.jobsummary.log
+hadoop.mapreduce.jobsummary.log.maxfilesize=256MB
+hadoop.mapreduce.jobsummary.log.maxbackupindex=20
+log4j.appender.JSA=org.apache.log4j.RollingFileAppender
+log4j.appender.JSA.File=${hadoop.log.dir}/${hadoop.mapreduce.jobsummary.log.file}
+log4j.appender.JSA.MaxFileSize=${hadoop.mapreduce.jobsummary.log.maxfilesize}
+log4j.appender.JSA.MaxBackupIndex=${hadoop.mapreduce.jobsummary.log.maxbackupindex}
+log4j.appender.JSA.layout=org.apache.log4j.PatternLayout
+log4j.appender.JSA.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n
+log4j.logger.org.apache.hadoop.mapred.JobInProgress$JobSummary=${hadoop.mapreduce.jobsummary.logger}
+log4j.additivity.org.apache.hadoop.mapred.JobInProgress$JobSummary=false
+
+#
+# Yarn ResourceManager Application Summary Log 
+#
+# Set the ResourceManager summary log filename
+yarn.server.resourcemanager.appsummary.log.file=rm-appsummary.log
+# Set the ResourceManager summary log level and appender
+yarn.server.resourcemanager.appsummary.logger=${hadoop.root.logger}
+#yarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY
+
+# To enable AppSummaryLogging for the RM, 
+# set yarn.server.resourcemanager.appsummary.logger to 
+# <LEVEL>,RMSUMMARY in hadoop-env.sh
+
+# Appender for ResourceManager Application Summary Log
+# Requires the following properties to be set
+#    - hadoop.log.dir (Hadoop Log directory)
+#    - yarn.server.resourcemanager.appsummary.log.file (resource manager app summary log filename)
+#    - yarn.server.resourcemanager.appsummary.logger (resource manager app summary log level and appender)
+
+log4j.logger.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=${yarn.server.resourcemanager.appsummary.logger}
+log4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=false
+log4j.appender.RMSUMMARY=org.apache.log4j.RollingFileAppender
+log4j.appender.RMSUMMARY.File=${hadoop.log.dir}/${yarn.server.resourcemanager.appsummary.log.file}
+log4j.appender.RMSUMMARY.MaxFileSize=256MB
+log4j.appender.RMSUMMARY.MaxBackupIndex=20
+log4j.appender.RMSUMMARY.layout=org.apache.log4j.PatternLayout
+log4j.appender.RMSUMMARY.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
+
+# HS audit log configs
+#mapreduce.hs.audit.logger=INFO,HSAUDIT
+#log4j.logger.org.apache.hadoop.mapreduce.v2.hs.HSAuditLogger=${mapreduce.hs.audit.logger}
+#log4j.additivity.org.apache.hadoop.mapreduce.v2.hs.HSAuditLogger=false
+#log4j.appender.HSAUDIT=org.apache.log4j.DailyRollingFileAppender
+#log4j.appender.HSAUDIT.File=${hadoop.log.dir}/hs-audit.log
+#log4j.appender.HSAUDIT.layout=org.apache.log4j.PatternLayout
+#log4j.appender.HSAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
+#log4j.appender.HSAUDIT.DatePattern=.yyyy-MM-dd
+
+# Http Server Request Logs
+#log4j.logger.http.requests.namenode=INFO,namenoderequestlog
+#log4j.appender.namenoderequestlog=org.apache.hadoop.http.HttpRequestLogAppender
+#log4j.appender.namenoderequestlog.Filename=${hadoop.log.dir}/jetty-namenode-yyyy_mm_dd.log
+#log4j.appender.namenoderequestlog.RetainDays=3
+
+#log4j.logger.http.requests.datanode=INFO,datanoderequestlog
+#log4j.appender.datanoderequestlog=org.apache.hadoop.http.HttpRequestLogAppender
+#log4j.appender.datanoderequestlog.Filename=${hadoop.log.dir}/jetty-datanode-yyyy_mm_dd.log
+#log4j.appender.datanoderequestlog.RetainDays=3
+
+#log4j.logger.http.requests.resourcemanager=INFO,resourcemanagerrequestlog
+#log4j.appender.resourcemanagerrequestlog=org.apache.hadoop.http.HttpRequestLogAppender
+#log4j.appender.resourcemanagerrequestlog.Filename=${hadoop.log.dir}/jetty-resourcemanager-yyyy_mm_dd.log
+#log4j.appender.resourcemanagerrequestlog.RetainDays=3
+
+#log4j.logger.http.requests.jobhistory=INFO,jobhistoryrequestlog
+#log4j.appender.jobhistoryrequestlog=org.apache.hadoop.http.HttpRequestLogAppender
+#log4j.appender.jobhistoryrequestlog.Filename=${hadoop.log.dir}/jetty-jobhistory-yyyy_mm_dd.log
+#log4j.appender.jobhistoryrequestlog.RetainDays=3
+
+#log4j.logger.http.requests.nodemanager=INFO,nodemanagerrequestlog
+#log4j.appender.nodemanagerrequestlog=org.apache.hadoop.http.HttpRequestLogAppender
+#log4j.appender.nodemanagerrequestlog.Filename=${hadoop.log.dir}/jetty-nodemanager-yyyy_mm_dd.log
+#log4j.appender.nodemanagerrequestlog.RetainDays=3","[{'comment': 'Do we want to keep the commented content around? What would be the reason for that?', 'commenter': 'sanjaypujare'}]"
421,docs/application_packages.md,"@@ -451,7 +451,7 @@ Application Package needs the classpath in a specific order.
 ### Logging configuration
 
 Just like other Java projects, you can change the logging configuration
-by having your log4j.properties under src/main/resources.  For example,
+by updating your log4j.props under src/main/resources.  For example,
 if you have the following in src/main/resources/log4j.properties:","[{'comment': 'This one says log4j.properties although the file was log4j.props. In any case I prefer properties to props as stated before.', 'commenter': 'sanjaypujare'}]"
421,docs/application_packages.md,"@@ -464,8 +464,8 @@ if you have the following in src/main/resources/log4j.properties:
 The root loggers level is set to WARN and the output is set to the console (stdout).
 
 Note that by default from project created from the maven archetype,
-there is already a log4j.properties file under src/test/resources and
-that file is only used for the unit test.
+there is already a log4j.props file under src/main/resources that can be modified or replaced.
+Also there is log4j.properties file under src/test/resources and that file is only used for the unit test.","[{'comment': '""Also there is a log4j.properties file under src/test/resources and that file is only used for unit tests.""', 'commenter': 'sanjaypujare'}]"
421,engine/src/main/java/com/datatorrent/stram/LaunchContainerRunnable.java,"@@ -172,6 +173,18 @@ public void run()
         if (archives != null) {
           addFilesToLocalResources(LocalResourceType.ARCHIVE, archives, localResources, fs);
         }
+        String configuredAppPath = dag.getValue(LogicalPlan.APPLICATION_PATH);
+        Path log4jPath = new Path(configuredAppPath + File.separator + StramClient.APEX_LOG4J_PROPS_FILE);","[{'comment': ""If the user changes the JVM option (-Dlog4j.configuration=log4j.props) using a different name, how would this code use that value? Aren't you using the hardcoded log4j.props name?"", 'commenter': 'sanjaypujare'}, {'comment': ""changing name to log4j. properties. Still user won't get freedom to rename file present in conf dir.\r\nIf he really wants to rename, he can add renamed properties file to application jar and add -Dlog4j.configuration option to properties and it should work."", 'commenter': 'DT-Priyanka'}]"
421,engine/src/main/java/com/datatorrent/stram/StramClient.java,"@@ -479,10 +481,12 @@ public void startApplication() throws YarnException, IOException
         dag.setAttribute(LogicalPlan.CONTAINER_OPTS_CONFIGURATOR, new BasicContainerOptConfigurator());
       }
 
+      log4jPropFile = StramClientUtils.getConfigDir() + File.separator + APEX_LOG4J_PROPS_FILE;","[{'comment': ""As mentioned before this won't work if user changes the value in -Dlog4j.configuration=log4j.props, correct?"", 'commenter': 'sanjaypujare'}]"
421,engine/src/main/java/com/datatorrent/stram/api/StreamingContainerUmbilicalProtocol.java,"@@ -413,7 +413,7 @@ public String toString()
    * @param operators
    * @param msg
    */
-  void reportError(String containerId, int[] operators, String msg);
+  void reportError(String containerId, int[] operators, String string, String logFileName, long logFileOffset);","[{'comment': 'where is the logic that if the new rollingFileAppender is not used, do not emit the filename and offset in the events?', 'commenter': 'sanjaypujare'}, {'comment': 'I am returning default values as filename=null and offset=0, if right appender is not used.\r\nEven if I skip to emit filename, the ContainerErrorEvent and OperatorErrorEvent class beans are updated to have getter and setter for filename and offset. Due to which those fields will be read (check FSEventRecorder ""writeEvent"" function.). It uses BeanUtils.describe function and hence calls all getter.\r\nSo I decided to add default values than skipping it.', 'commenter': 'DT-Priyanka'}]"
421,engine/src/main/java/org/apache/apex/log/appender/ApexRollingFileAppender.java,"@@ -0,0 +1,142 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.apex.log.appender;
+
+import java.io.BufferedWriter;
+import java.io.File;
+import java.io.FileNotFoundException;
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.io.InterruptedIOException;
+import java.io.Writer;
+import java.nio.file.Files;
+import java.nio.file.Paths;
+
+import org.apache.log4j.RollingFileAppender;
+import org.apache.log4j.helpers.CountingQuietWriter;
+import org.apache.log4j.helpers.LogLog;
+
+public class ApexRollingFileAppender extends RollingFileAppender
+{
+  private int currentFileIndex = 0;
+  private String currentFileName = """";
+
+  public synchronized void setFile(String fileName, boolean append, boolean bufferedIO, int bufferSize)
+      throws IOException
+  {
+    currentFileName = fileName + ""."" + currentFileIndex;
+    LogLog.debug(""setFile called: "" + currentFileName + "", "" + append);
+
+    // It does not make sense to have immediate flush and bufferedIO.
+    if (bufferedIO) {
+      setImmediateFlush(false);
+    }
+
+    reset();
+    FileOutputStream ostream = null;
+    try {
+      //   attempt to create file
+      ostream = new FileOutputStream(currentFileName, append);
+    } catch (FileNotFoundException ex) {
+      //   if parent directory does not exist then
+      //      attempt to create it and try to create file
+      //      see bug 9150
+      String parentName = new File(currentFileName).getParent();
+      if (parentName != null) {
+        File parentDir = new File(parentName);
+        if (!parentDir.exists() && parentDir.mkdirs()) {
+          ostream = new FileOutputStream(currentFileName, append);
+        } else {
+          throw ex;
+        }
+      } else {
+        throw ex;
+      }
+    }
+    Writer fw = createWriter(ostream);
+    if (bufferedIO) {
+      fw = new BufferedWriter(fw, bufferSize);
+    }
+
+    if (new File(fileName).exists()) {
+      Files.delete(Paths.get(fileName));
+    }
+    Files.createSymbolicLink(Paths.get(fileName), Paths.get(currentFileName));
+    this.setQWForFiles(fw);
+    this.fileName = fileName;
+    this.fileAppend = append;
+    this.bufferedIO = bufferedIO;
+    this.bufferSize = bufferSize;
+    writeHeader();
+    LogLog.debug(""setFile ended"");
+  }
+
+  @Override
+  public void rollOver()
+  {
+    File file;","[{'comment': ""Why does this file variable need to be declared here when it's only used within the if (maxBackupIndex > 0)"", 'commenter': 'davidyan74'}, {'comment': 'moving declaration.', 'commenter': 'DT-Priyanka'}]"
421,apex-conf-archetype/src/main/resources/archetype-resources/src/main/resources/log4j.properties,"@@ -0,0 +1,278 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# ""License""); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+# Define some default values that can be overridden by system properties
+hadoop.root.logger=INFO,console
+hadoop.log.dir=.
+hadoop.log.file=hadoop.log
+
+# Define the root logger to the system property ""hadoop.root.logger"".
+log4j.rootLogger=INFO,APEXRFA, EventCounter
+
+# Logging Threshold
+log4j.threshold=ALL
+
+# Null Appender
+log4j.appender.NullAppender=org.apache.log4j.varia.NullAppender
+
+#
+# Rolling File Appender - cap space usage at 5gb.
+#
+hadoop.log.maxfilesize=256MB
+hadoop.log.maxbackupindex=20
+log4j.appender.RFA=org.apache.log4j.RollingFileAppender
+log4j.appender.RFA.File=${hadoop.log.dir}/${hadoop.log.file}
+
+log4j.appender.RFA.MaxFileSize=${hadoop.log.maxfilesize}
+log4j.appender.RFA.MaxBackupIndex=${hadoop.log.maxbackupindex}
+
+log4j.appender.RFA.layout=org.apache.log4j.PatternLayout
+
+# Pattern format: Date LogLevel LoggerName LogMessage
+log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
+# Debugging Pattern format
+#log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n
+
+#
+# APEX Rolling File Appender
+#
+log4j.appender.APEXRFA=org.apache.apex.log.appender.ApexRollingFileAppender
+log4j.appender.APEXRFA.File=${hadoop.log.dir}/${hadoop.log.file}
+
+log4j.appender.APEXRFA.MaxFileSize=${hadoop.log.maxfilesize}
+log4j.appender.APEXRFA.MaxBackupIndex=${hadoop.log.maxbackupindex}
+
+log4j.appender.APEXRFA.layout=org.apache.log4j.PatternLayout
+
+# Pattern format: Date LogLevel LoggerName LogMessage
+log4j.appender.APEXRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
+","[{'comment': ""It would be good to point out as comments that you're taking the default log4j.properties from hadoop and add the apex specific entries to it to achieve what goal, and also have the apex specific entries separate from the rest of the file."", 'commenter': 'davidyan74'}, {'comment': 'yes I would do that.', 'commenter': 'DT-Priyanka'}]"
421,engine/src/main/java/com/datatorrent/stram/util/LoggerUtil.java,"@@ -275,4 +278,21 @@ public static synchronized void changeLoggersLevel(@Nonnull Map<String, String>
     }
     return ImmutableMap.copyOf(matchedClasses);
   }
+
+  public static String getLogFileName()
+  {
+    ApexRollingFileAppender apexAppender = (ApexRollingFileAppender)LogManager.getRootLogger().getAppender(","[{'comment': 'could this throw ClassCastException?', 'commenter': 'davidyan74'}, {'comment': 'It shouldn\'t as I am looking for appender with name ""APEXRFA"". Still I would add a check for safety.', 'commenter': 'DT-Priyanka'}]"
421,engine/src/main/java/org/apache/apex/log/appender/ApexRollingFileAppender.java,"@@ -0,0 +1,174 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.apex.log.appender;
+
+import java.io.BufferedWriter;
+import java.io.File;
+import java.io.FileNotFoundException;
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.io.InterruptedIOException;
+import java.io.Writer;
+import java.nio.file.Files;
+import java.nio.file.Paths;
+
+import org.apache.log4j.RollingFileAppender;
+import org.apache.log4j.helpers.CountingQuietWriter;
+import org.apache.log4j.helpers.LogLog;
+import org.apache.log4j.spi.LoggingEvent;
+
+/**
+ * ApexRollingFileAppender extends {@link RollingFileAppender}
+ * This appender doesn't keep renaming files when file reach max size.
+ * The appender starts with <filename>.0 file when file reaches max size,
+ * it creates next file <filename>.1 and so on till <filename>.MaxBackupIndex.
+ * After that it will delete <filename>.0 and start creating log files in that
+ * order again.
+ * To identify current file, it creates <filename> symlink to current file.
+ */
+public class ApexRollingFileAppender extends RollingFileAppender
+{
+  private int currentFileIndex = 0;
+  private String currentFileName = """";
+  private long lastExceptionOffset;
+
+  public synchronized void setFile(String fileName, boolean append, boolean bufferedIO, int bufferSize)
+      throws IOException
+  {
+    currentFileName = fileName + ""."" + currentFileIndex;
+    LogLog.debug(""setFile called: "" + currentFileName + "", "" + append);
+
+    // It does not make sense to have immediate flush and bufferedIO.
+    if (bufferedIO) {
+      setImmediateFlush(false);
+    }
+
+    reset();
+    FileOutputStream ostream = null;
+    try {
+      //   attempt to create file
+      ostream = new FileOutputStream(currentFileName, append);
+    } catch (FileNotFoundException ex) {
+      //   if parent directory does not exist then
+      //      attempt to create it and try to create file
+      //      see bug 9150
+      String parentName = new File(currentFileName).getParent();
+      if (parentName != null) {
+        File parentDir = new File(parentName);
+        if (!parentDir.exists() && parentDir.mkdirs()) {
+          ostream = new FileOutputStream(currentFileName, append);
+        } else {
+          throw ex;
+        }
+      } else {
+        throw ex;
+      }
+    }
+    Writer fw = createWriter(ostream);
+    if (bufferedIO) {
+      fw = new BufferedWriter(fw, bufferSize);
+    }
+
+    if (new File(fileName).exists()) {
+      Files.delete(Paths.get(fileName));
+    }
+    Files.createSymbolicLink(Paths.get(fileName), Paths.get(currentFileName));
+    this.setQWForFiles(fw);
+    this.fileName = fileName;
+    this.fileAppend = append;
+    this.bufferedIO = bufferedIO;
+    this.bufferSize = bufferSize;
+    writeHeader();
+    LogLog.debug(""setFile ended"");
+  }
+
+  @Override
+  public void rollOver()
+  {
+    if (qw != null) {
+      long size = ((CountingQuietWriter)qw).getCount();
+      LogLog.debug(""rolling over count="" + size);
+    }
+    LogLog.debug(""maxBackupIndex="" + maxBackupIndex);
+
+    boolean deleteSucceeded = true;
+    int nextFileIndex = (currentFileIndex + 1) % maxBackupIndex;
+    String nextFileName = fileName + '.' + nextFileIndex;","[{'comment': 'This is going to be confusing because the filename is going to be apex.log.1, apex.log.2, and so on.\r\nI would imagine, most people, including myself, would make assumption about this notation and quickly assume the standard log rotation.', 'commenter': 'davidyan74'}, {'comment': ""Yes this is confusing, I had raised that point at time of proposing this design. And that's why we though we would have a soft link to point to current file. If we wish you have fixed log file names we have to change logger behavior to something, and provided that something is different from standard log rotation, this is bound to be confusing."", 'commenter': 'DT-Priyanka'}]"
421,engine/src/main/java/org/apache/apex/log/appender/ApexRollingFileAppender.java,"@@ -0,0 +1,174 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.apex.log.appender;
+
+import java.io.BufferedWriter;
+import java.io.File;
+import java.io.FileNotFoundException;
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.io.InterruptedIOException;
+import java.io.Writer;
+import java.nio.file.Files;
+import java.nio.file.Paths;
+
+import org.apache.log4j.RollingFileAppender;
+import org.apache.log4j.helpers.CountingQuietWriter;
+import org.apache.log4j.helpers.LogLog;
+import org.apache.log4j.spi.LoggingEvent;
+
+/**
+ * ApexRollingFileAppender extends {@link RollingFileAppender}
+ * This appender doesn't keep renaming files when file reach max size.
+ * The appender starts with <filename>.0 file when file reaches max size,
+ * it creates next file <filename>.1 and so on till <filename>.MaxBackupIndex.
+ * After that it will delete <filename>.0 and start creating log files in that
+ * order again.
+ * To identify current file, it creates <filename> symlink to current file.
+ */
+public class ApexRollingFileAppender extends RollingFileAppender","[{'comment': ""I discussed with @vrozov briefly about this, and we think there should be an interface that has two methods. E.g.:\r\n```java\r\ninterface LogLocationProvider\r\n{\r\n  String getCurrentFileName();\r\n  long getCurrentFileOffset();\r\n}\r\n```\r\nand have ApexRollingFileAppender implement this interface so that it's possible for developers to implement this interface to have their own file appender that works with this feature. And the code that says `appender instanceof ApexRollingFileAppender` would become `appender instanceof LogLocationProvider`\r\n\r\nNote that I'm not using the name getLastExceptionOffset because it's really the current offset of the log file, and has nothing to do with exceptions."", 'commenter': 'davidyan74'}]"
421,apex-app-archetype/src/main/resources/META-INF/maven/archetype-metadata.xml,"@@ -50,6 +50,12 @@
       </includes>
     </fileSet>
     <fileSet filtered=""true"" encoding=""UTF-8"">
+      <directory>src/main/resources</directory>
+      <includes>
+        <include>**/apexlog4j.properties</include>","[{'comment': ""Again, please investigate whether it's possible to retain the log4j.properties default name to achieve the goal."", 'commenter': 'davidyan74'}, {'comment': 'I will give it a try again.', 'commenter': 'DT-Priyanka'}]"
432,engine/src/main/java/com/datatorrent/stram/util/LoggerUtil.java,"@@ -278,4 +280,37 @@ public static synchronized void changeLoggersLevel(@Nonnull Map<String, String>
     }
     return ImmutableMap.copyOf(matchedClasses);
   }
+
+  /**
+   * Returns logger log file name
+   * @return logFileName
+   */
+  public static String getLogFileName()
+  {
+    Enumeration e = LogManager.getRootLogger().getAllAppenders();
+    while (e.hasMoreElements()) {
+      Appender appender = (Appender)e.nextElement();
+      if (appender instanceof FileAppender) {
+        return ((FileAppender)appender).getFile();","[{'comment': 'What if there is more than one FileAppender?', 'commenter': 'vrozov'}, {'comment': 'We have hardcoded ""RFA"" in our code (check classes StramClient and LaunchContainerRunnable), is it a good idea to look for ""RFA"" specifically? If RFA isn\'t found we should return ""null"".', 'commenter': 'DT-Priyanka'}, {'comment': 'Apex supports custom log4j.properties that may have multiple FileAppenders and may not use ${hadoop.root.logger} as a root logger. It will be good to support custom log4j.properties or at minimum have an ability to add such support in the future without changing API.', 'commenter': 'vrozov'}, {'comment': 'I would say, let\'s keep it configurable for now? \r\nAs there could be multiple file Appenders, it\'s not straightforward to figure out which one is being used for current exception. log4j can be configured to use different appenders for different classes. For the V1 of feature, we can keep it configurable and give default value as ""RFA"". We can further extend this feature to find out the appender for class logging the exception and then fetch the file name.', 'commenter': 'DT-Priyanka'}]"
432,engine/src/main/java/com/datatorrent/stram/util/LoggerUtil.java,"@@ -278,4 +284,63 @@ public static synchronized void changeLoggersLevel(@Nonnull Map<String, String>
     }
     return ImmutableMap.copyOf(matchedClasses);
   }
+
+  /**
+   * Returns logger log file name
+   * @return logFileName
+   */
+  @Evolving
+  public static String getLogFileName()
+  {
+    Set<FileAppender> fileAppenders = getRootFileAppenders();
+    //skip fetching log file name if we have multiple file Appenders
+    if (fileAppenders.size() == 1) {
+      FileAppender fileAppender = fileAppenders.iterator().next();
+      if (isErrorLevelEnable(fileAppender)) {
+        return fileAppender.getFile();
+      }
+    }
+    return null;
+  }
+
+  /**
+   * Returns offset of current log file.
+   * @return logFileOffset
+   */
+  @Evolving
+  public static long getLogFileOffset()
+  {
+    Set<FileAppender> fileAppenders = getRootFileAppenders();
+    //skip fetching log file offset if we have multiple file Appenders
+    if (fileAppenders.size() == 1) {
+      FileAppender fileAppender = fileAppenders.iterator().next();
+      if (isErrorLevelEnable(fileAppender)) {
+        File logFile = new File(fileAppender.getFile());
+        return logFile.length();
+      }
+    }
+    return 0;
+  }
+
+  private static Set<FileAppender> getRootFileAppenders()","[{'comment': 'Why is the call necessary, should not it always return the same set of FileAppenders? Please introduce static Set<FileAppender> or better static FileAppender and initialize it in the static block. It should be initialized to a single FileAppender or null.', 'commenter': 'vrozov'}, {'comment': 'sounds good.', 'commenter': 'DT-Priyanka'}]"
432,engine/src/main/java/com/datatorrent/stram/util/LoggerUtil.java,"@@ -278,4 +284,63 @@ public static synchronized void changeLoggersLevel(@Nonnull Map<String, String>
     }
     return ImmutableMap.copyOf(matchedClasses);
   }
+
+  /**
+   * Returns logger log file name
+   * @return logFileName
+   */
+  @Evolving
+  public static String getLogFileName()
+  {
+    Set<FileAppender> fileAppenders = getRootFileAppenders();
+    //skip fetching log file name if we have multiple file Appenders
+    if (fileAppenders.size() == 1) {
+      FileAppender fileAppender = fileAppenders.iterator().next();
+      if (isErrorLevelEnable(fileAppender)) {
+        return fileAppender.getFile();
+      }
+    }
+    return null;
+  }
+
+  /**
+   * Returns offset of current log file.
+   * @return logFileOffset
+   */
+  @Evolving
+  public static long getLogFileOffset()
+  {
+    Set<FileAppender> fileAppenders = getRootFileAppenders();
+    //skip fetching log file offset if we have multiple file Appenders
+    if (fileAppenders.size() == 1) {
+      FileAppender fileAppender = fileAppenders.iterator().next();
+      if (isErrorLevelEnable(fileAppender)) {
+        File logFile = new File(fileAppender.getFile());
+        return logFile.length();
+      }
+    }
+    return 0;
+  }
+
+  private static Set<FileAppender> getRootFileAppenders()
+  {
+    Enumeration e = LogManager.getRootLogger().getAllAppenders();
+    Set<FileAppender> fileAppenders = new HashSet<>();
+    while (e.hasMoreElements()) {
+      Appender appender = (Appender)e.nextElement();
+      if (appender instanceof FileAppender) {
+        fileAppenders.add((FileAppender)appender);
+      }
+    }
+    return fileAppenders;
+  }
+
+  private static boolean isErrorLevelEnable(FileAppender fileAppender)","[{'comment': 'The threshold of a file appender is static. Evaluation of a FileAppender threshold should be done only once during iteration over file appenders. Logging in v1 should be enabled only when there is exactly one FileAppender with threshold greater or equal to ERROR.', 'commenter': 'vrozov'}, {'comment': 'I was little skeptical, if I put changes related to finding appenders and threshold in static code block will be be okay? If logger is initialized before that?', 'commenter': 'DT-Priyanka'}, {'comment': 'done', 'commenter': 'DT-Priyanka'}]"
432,engine/src/main/java/com/datatorrent/stram/util/LoggerUtil.java,"@@ -278,4 +284,63 @@ public static synchronized void changeLoggersLevel(@Nonnull Map<String, String>
     }
     return ImmutableMap.copyOf(matchedClasses);
   }
+
+  /**
+   * Returns logger log file name
+   * @return logFileName
+   */
+  @Evolving
+  public static String getLogFileName()","[{'comment': 'getLogFileName() and getLogFileOffset() duplicate 90% of the code. It will be better to have a single function that returns File and caller uses File object to get the name and the offset or introduce object with the name and the offset that that single function returns. Note that name and offset describe the same log file, they do not exist separately.', 'commenter': 'vrozov'}, {'comment': 'I will better introduce a object so that we fetch name and offset at same time.', 'commenter': 'DT-Priyanka'}]"
432,engine/src/main/java/com/datatorrent/stram/util/LoggerUtil.java,"@@ -278,4 +284,63 @@ public static synchronized void changeLoggersLevel(@Nonnull Map<String, String>
     }
     return ImmutableMap.copyOf(matchedClasses);
   }
+
+  /**
+   * Returns logger log file name
+   * @return logFileName
+   */
+  @Evolving
+  public static String getLogFileName()
+  {
+    Set<FileAppender> fileAppenders = getRootFileAppenders();
+    //skip fetching log file name if we have multiple file Appenders
+    if (fileAppenders.size() == 1) {
+      FileAppender fileAppender = fileAppenders.iterator().next();
+      if (isErrorLevelEnable(fileAppender)) {
+        return fileAppender.getFile();
+      }
+    }
+    return null;
+  }
+
+  /**
+   * Returns offset of current log file.
+   * @return logFileOffset
+   */
+  @Evolving
+  public static long getLogFileOffset()
+  {
+    Set<FileAppender> fileAppenders = getRootFileAppenders();
+    //skip fetching log file offset if we have multiple file Appenders
+    if (fileAppenders.size() == 1) {
+      FileAppender fileAppender = fileAppenders.iterator().next();
+      if (isErrorLevelEnable(fileAppender)) {
+        File logFile = new File(fileAppender.getFile());
+        return logFile.length();
+      }
+    }
+    return 0;","[{'comment': '0 is a valid offset, it should be -1, but I would prefer to use either File or separate object and in this case, it will be sufficient to return null (please see my previous comment).', 'commenter': 'vrozov'}, {'comment': 'I had started with ""-1"", but as some ""api"" detect ""-1"" as EOF, I had seen some issues. I had tried ti out with original PR long time back, don\'t remember what was exact problem I was facing.', 'commenter': 'DT-Priyanka'}]"
432,engine/src/main/java/com/datatorrent/stram/util/LoggerUtil.java,"@@ -278,4 +284,63 @@ public static synchronized void changeLoggersLevel(@Nonnull Map<String, String>
     }
     return ImmutableMap.copyOf(matchedClasses);
   }
+
+  /**
+   * Returns logger log file name
+   * @return logFileName
+   */
+  @Evolving
+  public static String getLogFileName()
+  {
+    Set<FileAppender> fileAppenders = getRootFileAppenders();
+    //skip fetching log file name if we have multiple file Appenders
+    if (fileAppenders.size() == 1) {
+      FileAppender fileAppender = fileAppenders.iterator().next();
+      if (isErrorLevelEnable(fileAppender)) {
+        return fileAppender.getFile();
+      }
+    }
+    return null;
+  }
+
+  /**
+   * Returns offset of current log file.
+   * @return logFileOffset
+   */
+  @Evolving
+  public static long getLogFileOffset()
+  {
+    Set<FileAppender> fileAppenders = getRootFileAppenders();
+    //skip fetching log file offset if we have multiple file Appenders
+    if (fileAppenders.size() == 1) {
+      FileAppender fileAppender = fileAppenders.iterator().next();
+      if (isErrorLevelEnable(fileAppender)) {
+        File logFile = new File(fileAppender.getFile());
+        return logFile.length();","[{'comment': 'What if immediateFlush is set to false?', 'commenter': 'vrozov'}, {'comment': 'In that case we might end up either giving offset which is little before the actual log entry or our log entry never gets written to file, as container crashes immediately after logging.\r\n\r\nShould we disable our feature if immediateFlush == false ?', 'commenter': 'DT-Priyanka'}, {'comment': 'Please research ability to flush', 'commenter': 'vrozov'}, {'comment': 'Only option I see to always enable ability to flush is to override shouldFlush and return true always. But for this, we will need to extend FileAppender and introduce a custom appender which isn\'t very good solution.\r\nAs the flag ""immediateFlush"" can be changed from config file, as of now there is no 100% gaurentee of keeping it enable all times (by default it\'s enable)\r\n\r\nWe can skip feature if flag is false, but again as we are giving approx offset before exception, I think we need not disable feature for ""immediateFlush = false"". Let me know what you think.', 'commenter': 'DT-Priyanka'}, {'comment': 'My take is to disable the feature if there is no way to flush when immediateFlush is false. A few last entries including the error event may not be present in the log file.', 'commenter': 'vrozov'}]"
432,engine/src/main/java/com/datatorrent/stram/api/StreamingContainerUmbilicalProtocol.java,"@@ -412,8 +412,10 @@ public String toString()
    * @param containerId
    * @param operators
    * @param msg
+   * @param logFileName
+   * @param logFileOffset
    */
-  void reportError(String containerId, int[] operators, String msg);
+  void reportError(String containerId, int[] operators, String string, String logFileName, long logFileOffset);","[{'comment': 'msg, name and offset should be encapsulated into object. name and offset do not exist without the msg.', 'commenter': 'vrozov'}, {'comment': 'as msg do exist without name, offset (in cases when we have multiple appenders etc) I am keeping LogFileInformation separately.', 'commenter': 'DT-Priyanka'}]"
432,engine/src/test/resources/log4j.properties,"@@ -17,7 +17,7 @@
 # under the License.
 #
 
-log4j.rootLogger=INFO,CONSOLE
+log4j.rootLogger=INFO,CONSOLE,malhar","[{'comment': 'The change impacts all apex core unit tests. ', 'commenter': 'vrozov'}, {'comment': 'Yes, it adds extra logger, which should not break anything, this is extra logger. I will still try to see if I can use custom properties file for our particular test.', 'commenter': 'DT-Priyanka'}]"
432,engine/src/test/resources/log4j.properties,"@@ -26,7 +26,7 @@ log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} [%t] %-5p %c{2} %M -
 log4j.appender.malhar=org.apache.log4j.RollingFileAppender
 log4j.appender.malhar.layout=org.apache.log4j.PatternLayout
 log4j.appender.malhar.layout.ConversionPattern=%d{ISO8601} [%t] %-5p %c{2} %M - %m%n
-#log4j.appender.malhar.File=/tmp/app.log
+log4j.appender.malhar.File=/tmp/app.log","[{'comment': 'All logging in unit tests should be done to the target directory.', 'commenter': 'vrozov'}]"
432,engine/src/main/java/com/datatorrent/stram/api/StramEvent.java,"@@ -420,12 +420,22 @@ public void setRequest(LogicalPlanRequest request)
   {
     private String containerId;
     private String errorMessage;
+    private String logFileName;","[{'comment': 'LogFileInfo?', 'commenter': 'vrozov'}, {'comment': ""I want to avoid using objects here. If you check FSEventRecorder. it uses BeanUtils to read properties of the Event object as key value pairs in map. If we use objects inside StramEvent, I think the conversion of event to key-value pair map will have to change. So won't use LogInformation object here."", 'commenter': 'DT-Priyanka'}]"
432,engine/src/main/java/com/datatorrent/stram/StramLocalCluster.java,"@@ -33,6 +33,8 @@
 
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
+
+import org.apache.apex.log.LogFileInformation;","[{'comment': 'Add empty line after org.apache.apex', 'commenter': 'vrozov'}]"
432,engine/src/main/java/com/datatorrent/stram/api/StreamingContainerUmbilicalProtocol.java,"@@ -23,6 +23,7 @@
 import java.util.ArrayList;
 import java.util.List;
 
+import org.apache.apex.log.LogFileInformation;","[{'comment': 'Add empty line after org.apache.apex', 'commenter': 'vrozov'}]"
432,engine/src/main/java/com/datatorrent/stram/util/LoggerUtil.java,"@@ -278,4 +288,63 @@ public static synchronized void changeLoggersLevel(@Nonnull Map<String, String>
     }
     return ImmutableMap.copyOf(matchedClasses);
   }
+
+  /**
+   * Returns logger log file {@link LogFileInformation}
+   * @return logFileInformation
+   */
+  @Evolving
+  public static LogFileInformation getLogFileInformation()
+  {
+    if (shouldFetchLogFileInformation()) {
+      File logFile = new File(rootFileAppender.getFile());
+      LogFileInformation logFileInfo = new LogFileInformation(rootFileAppender.getFile(), logFile.length());
+      return logFileInfo;
+    }
+    return new LogFileInformation(null, 0);
+  }
+
+  private static FileAppender getRootFileAppender()
+  {
+    Enumeration e = LogManager.getRootLogger().getAllAppenders();
+    Set<FileAppender> fileAppenders = new HashSet<>();","[{'comment': 'Why is the Set necessary? It should be sufficient to have just FileAppender.', 'commenter': 'vrozov'}]"
432,engine/src/main/java/com/datatorrent/stram/engine/StreamingContainer.java,"@@ -313,15 +314,19 @@ public static void main(String[] args) throws Throwable
         stramChild.teardown();
       }
     } catch (Error error) {
+      //fetch logFileInfo before logging exception, to get offset before exception
+      LogFileInformation logFileInfo = LoggerUtil.getLogFileInformation();","[{'comment': 'Check logger level.', 'commenter': 'vrozov'}]"
432,engine/src/main/java/com/datatorrent/stram/util/LoggerUtil.java,"@@ -278,4 +288,63 @@ public static synchronized void changeLoggersLevel(@Nonnull Map<String, String>
     }
     return ImmutableMap.copyOf(matchedClasses);
   }
+
+  /**
+   * Returns logger log file {@link LogFileInformation}
+   * @return logFileInformation
+   */
+  @Evolving
+  public static LogFileInformation getLogFileInformation()
+  {
+    if (shouldFetchLogFileInformation()) {
+      File logFile = new File(rootFileAppender.getFile());
+      LogFileInformation logFileInfo = new LogFileInformation(rootFileAppender.getFile(), logFile.length());
+      return logFileInfo;
+    }
+    return new LogFileInformation(null, 0);
+  }
+
+  private static FileAppender getRootFileAppender()
+  {
+    Enumeration e = LogManager.getRootLogger().getAllAppenders();
+    Set<FileAppender> fileAppenders = new HashSet<>();
+    while (e.hasMoreElements()) {
+      Appender appender = (Appender)e.nextElement();
+      if (appender instanceof FileAppender) {
+        fileAppenders.add((FileAppender)appender);","[{'comment': 'Check FileAppender log level here. There should be one (or at least one?) FileAppender with ERROR level logging threshold.', 'commenter': 'vrozov'}]"
440,api/src/main/java/com/datatorrent/api/ControlSink.java,"@@ -0,0 +1,77 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.api;
+
+/**
+ * A {@link Sink} which supports adding control tuples
+ * Additionally allows to set and retrieve propogation information for control tuples
+ */
+public interface ControlSink<T> extends Sink<T>
+{
+  public static final ControlSink<Object> BLACKHOLE = new ControlSink<Object>()
+  {
+    @Override
+    public void put(Object tuple)
+    {
+    }
+
+    @Override
+    public void putControl(Object payload)
+    {
+    }
+
+    @Override
+    public int getCount(boolean reset)
+    {
+      return 0;
+    }
+
+    @Override
+    public boolean isPropogateControlTuples()","[{'comment': 'The word is ""propagate"".', 'commenter': 'davidyan74'}]"
440,api/src/main/java/com/datatorrent/api/ControlSink.java,"@@ -0,0 +1,77 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.api;
+
+/**
+ * A {@link Sink} which supports adding control tuples
+ * Additionally allows to set and retrieve propogation information for control tuples
+ */
+public interface ControlSink<T> extends Sink<T>","[{'comment': 'CustomControlTupleEnabledSink? ControlSink is too generic.', 'commenter': 'vrozov'}, {'comment': 'Done. Also renamed the DefaultControlSink to DefaultCustomControlTupleEnabledSink.', 'commenter': 'bhupeshchawda'}]"
440,api/src/main/java/com/datatorrent/api/ControlSink.java,"@@ -0,0 +1,77 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.api;
+
+/**
+ * A {@link Sink} which supports adding control tuples
+ * Additionally allows to set and retrieve propogation information for control tuples
+ */
+public interface ControlSink<T> extends Sink<T>
+{
+  public static final ControlSink<Object> BLACKHOLE = new ControlSink<Object>()","[{'comment': 'Change DefaultInputPort to use ControlSink.BLACKHOLE and remove Sink.BLACKHOLE.', 'commenter': 'vrozov'}, {'comment': 'Done', 'commenter': 'bhupeshchawda'}]"
440,api/src/main/java/com/datatorrent/api/DefaultOutputPort.java,"@@ -37,8 +37,8 @@
   public static final String THREAD_AFFINITY_DISABLE_CHECK = ""com.datatorrent.api.DefaultOutputPort.thread.check.disable"";
   private static final Logger logger = LoggerFactory.getLogger(DefaultOutputPort.class);
 
-  private transient Sink<Object> sink;
-  private transient Thread operatorThread;
+  protected transient Sink<Object> sink;
+  protected transient Thread operatorThread;","[{'comment': 'private', 'commenter': 'vrozov'}, {'comment': 'Done', 'commenter': 'bhupeshchawda'}]"
440,api/src/main/java/com/datatorrent/api/DefaultOutputPort.java,"@@ -55,13 +55,20 @@ public DefaultOutputPort()
    */
   public void emit(T tuple)
   {
+    if (verifySameThread()) {
+      getSink().put(tuple);","[{'comment': 'why sink->getSink()?', 'commenter': 'vrozov'}, {'comment': 'Removed. Not needed after sink in ControlAwareDefaultOutputPort was removed. ', 'commenter': 'bhupeshchawda'}]"
440,api/src/main/java/com/datatorrent/api/DefaultOutputPort.java,"@@ -55,13 +55,20 @@ public DefaultOutputPort()
    */
   public void emit(T tuple)
   {
+    if (verifySameThread()) {
+      getSink().put(tuple);
+    }
+  }
+
+  public boolean verifySameThread()","[{'comment': 'protected void check/assert/verifyOperatorThread', 'commenter': 'vrozov'}, {'comment': 'Done', 'commenter': 'bhupeshchawda'}]"
440,api/src/main/java/com/datatorrent/api/DefaultOutputPort.java,"@@ -113,4 +120,8 @@ public void teardown()
   {
   }
 
+  public Sink<Object> getSink()","[{'comment': 'Can it be protected?', 'commenter': 'vrozov'}, {'comment': 'yes, done', 'commenter': 'bhupeshchawda'}]"
440,api/src/main/java/org/apache/apex/api/ControlTuple.java,"@@ -0,0 +1,27 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.apex.api;
+
+/**
+ * An interface for Custom Control Tuples which can be generated by the user
+ * Any user generated control tuple must implement this interface
+ */
+public interface ControlTuple","[{'comment': 'CustomControlTuple or UserDefinedControlTuple?', 'commenter': 'vrozov'}, {'comment': 'Renamed to UserDefinedControlTuple.', 'commenter': 'bhupeshchawda'}]"
440,api/src/main/java/org/apache/apex/api/ControlAwareDefaultOutputPort.java,"@@ -0,0 +1,73 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.apex.api;
+
+import com.datatorrent.api.ControlSink;
+import com.datatorrent.api.DefaultOutputPort;
+import com.datatorrent.api.Sink;
+
+/**
+ * Default abstract implementation for OutputPort which is capable of emitting custom control tuples
+ * the {@link #emitControl(ControlTuple)} method can be used to emit control tuples onto this output port
+ * Additionally this also allows setting whether or not to enable this port to propagate control tuples
+ */
+public class ControlAwareDefaultOutputPort<T> extends DefaultOutputPort<T>
+{
+  private ControlSink<Object> sink;","[{'comment': 'There should be only one sink.', 'commenter': 'vrozov'}, {'comment': 'Yes, removed.', 'commenter': 'bhupeshchawda'}]"
440,engine/src/main/java/com/datatorrent/stram/engine/DefaultControlSink.java,"@@ -0,0 +1,58 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.stram.engine;
+
+import com.datatorrent.api.ControlSink;
+import com.datatorrent.stram.tuple.CustomControlTuple;
+
+/**
+ * A default implementation for {@link ControlSink}
+ */
+public abstract class DefaultControlSink<T> implements ControlSink<T>
+{
+  private boolean propagateControlTuples = true; // default
+
+  /**
+   * {@inheritDoc}
+   */
+  @Override
+  public void putControl(Object payload)","[{'comment': 'Object->ControlTuple', 'commenter': 'vrozov'}, {'comment': 'Yes, done', 'commenter': 'bhupeshchawda'}]"
440,api/src/main/java/com/datatorrent/api/CustomControlTupleEnabledSink.java,"@@ -0,0 +1,55 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.api;
+
+import org.apache.apex.api.UserDefinedControlTuple;
+
+/**
+ * A {@link Sink} which supports adding control tuples
+ * Additionally allows to set and retrieve propogation information for control tuples
+ */
+public interface CustomControlTupleEnabledSink<T> extends Sink<T>","[{'comment': 'can you rename it to ControlTupleEnabledSink', 'commenter': 'tushargosavi'}]"
440,api/src/main/java/org/apache/apex/api/ControlAwareDefaultInputPort.java,"@@ -0,0 +1,42 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.apex.api;
+
+import com.datatorrent.api.CustomControlTupleEnabledSink;
+import com.datatorrent.api.DefaultInputPort;
+
+/**
+ * Default abstract implementation for InputPort which is capable of processing custom control tuples
+ */
+public abstract class ControlAwareDefaultInputPort<T> extends DefaultInputPort<T> implements CustomControlTupleEnabledSink<T>
+{
+  @Override
+  public boolean putControl(UserDefinedControlTuple payload)
+  {
+    count++;","[{'comment': 'Do you want to increment it for control tuple as well? or can we maitain separate count for control tuples.', 'commenter': 'tushargosavi'}]"
440,api/src/main/java/org/apache/apex/api/UserDefinedControlTuple.java,"@@ -0,0 +1,35 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.apex.api;
+
+/**
+ * An interface for Custom Control Tuples which can be generated by the user
+ * Any user generated control tuple must implement this interface
+ */
+public interface UserDefinedControlTuple
+{
+  enum DeliveryType
+  {
+    IMMEDIATE,","[{'comment': 'Document the behaviour for delivery type.', 'commenter': 'tushargosavi'}]"
440,engine/src/main/java/com/datatorrent/stram/engine/GenericNode.java,"@@ -362,6 +414,61 @@ public final void run()
                 }
                 break;
 
+              case CUSTOM_CONTROL:
+                activePort.remove();
+                /* All custom control tuples are expected to be arriving in the current window only.*/
+                /* Buffer control tuples until end of the window */
+                CustomControlTuple cct = (CustomControlTuple)t;
+                UserDefinedControlTuple udct = (UserDefinedControlTuple)cct.getUserObject();
+
+                // Handle Immediate Delivery Control Tuples
+                if (udct.getDeliveryType().equals(UserDefinedControlTuple.DeliveryType.IMMEDIATE)) {
+                  if (!isDuplicate(immediateDeliveryTuples.get(activePort), cct)) {
+                    // Forward immediately
+                    if (reservoirPortMap.isEmpty()) {
+                      populateReservoirInputPortMap();
+                    }
+
+                    for (Entry<SweepableReservoir,Sink> reservoirSinkPair: reservoirPortMap.entrySet()) {
+                        if (reservoirSinkPair.getValue() instanceof ControlAwareDefaultInputPort) {
+                          CustomControlTupleEnabledSink sink = (CustomControlTupleEnabledSink)reservoirSinkPair.getValue();
+                          if (!sink.putControl((UserDefinedControlTuple)((CustomControlTuple)cct).getUserObject())) {
+                            if (!delay) {
+                              for (int s = sinks.length; s-- > 0; ) {
+                                sinks[s].put(cct);
+                              }
+                              controlTupleCount++;
+                            }
+                          } else {
+                            // do nothing
+                          }
+                        } else {
+                          if (!delay) {
+                            for (int s = sinks.length; s-- > 0; ) {
+                              sinks[s].put(cct);
+                            }
+                            controlTupleCount++;","[{'comment': 'can you extract sending tuple on all ports in a method.', 'commenter': 'tushargosavi'}]"
440,engine/src/main/java/com/datatorrent/stram/engine/OiONode.java,"@@ -82,10 +93,46 @@ public void put(Tuple t)
         case END_WINDOW:
           endWindowDequeueTimes.put(reservoir, System.currentTimeMillis());
           if (--expectingEndWindows == 0) {
+
+            /* process custom control tuples here */
+
+            if (customControlTuples.get(currentWindowId) != null) {
+              for (CustomControlTuple cct : customControlTuples.get(currentWindowId)) {
+                if (((OiOStream.OiOReservoir)reservoir).getSink() instanceof CustomControlTupleEnabledSink) {
+                  if (!((CustomControlTupleEnabledSink)((OiOStream.OiOReservoir)reservoir)
+                      .getSink())
+                      .putControl((UserDefinedControlTuple)cct.getUserObject())) {
+
+                    // Operator will not handle. Propagate.
+                    for (int s = sinks.length; s-- > 0; ) {
+                      sinks[s].put(cct);
+                    }
+                    controlTupleCount++;
+                  }
+                } else {
+                  // Operator will not handle. Propagate.
+                  for (int s = sinks.length; s-- > 0; ) {
+                    sinks[s].put(cct);
+                  }
+                  controlTupleCount++;
+                }
+              }
+            }","[{'comment': 'consider delivery type.', 'commenter': 'tushargosavi'}]"
440,engine/src/main/java/com/datatorrent/stram/engine/OiONode.java,"@@ -82,10 +93,46 @@ public void put(Tuple t)
         case END_WINDOW:
           endWindowDequeueTimes.put(reservoir, System.currentTimeMillis());
           if (--expectingEndWindows == 0) {
+
+            /* process custom control tuples here */
+
+            if (customControlTuples.get(currentWindowId) != null) {
+              for (CustomControlTuple cct : customControlTuples.get(currentWindowId)) {
+                if (((OiOStream.OiOReservoir)reservoir).getSink() instanceof CustomControlTupleEnabledSink) {
+                  if (!((CustomControlTupleEnabledSink)((OiOStream.OiOReservoir)reservoir)
+                      .getSink())","[{'comment': 'reference to sink can be stored instread of doing (OiOStream.OiOReservoir)reservoir).getSink()', 'commenter': 'tushargosavi'}]"
440,engine/src/main/java/com/datatorrent/stram/stream/BufferServerPublisher.java,"@@ -61,6 +64,7 @@
   private EventLoop eventloop;
   private int count;
   private StatefulStreamCodec<Object> statefulSerde;
+  private boolean propagate = true;","[{'comment': 'unused variable.', 'commenter': 'tushargosavi'}]"
440,engine/src/main/java/com/datatorrent/stram/engine/Stream.java,"@@ -32,7 +33,7 @@
 /*
  * Provides basic interface for a stream object. Stram, StramChild work via this interface
  */
-public interface Stream extends Component<StreamContext>, ActivationListener<StreamContext>, Sink<Object>
+public interface Stream extends Component<StreamContext>, ActivationListener<StreamContext>, Sink<Object>, CustomControlTupleEnabledSink<Object>","[{'comment': 'you can remove Sink<Object> as CustomControlEnabledSink extends from it.', 'commenter': 'tushargosavi'}]"
440,engine/src/main/java/com/datatorrent/stram/tuple/CustomControlTuple.java,"@@ -0,0 +1,59 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.stram.tuple;
+
+import java.util.UUID;
+
+import com.esotericsoftware.kryo.serializers.FieldSerializer;
+import com.esotericsoftware.kryo.serializers.JavaSerializer;
+
+import com.datatorrent.bufferserver.packet.MessageType;
+
+/**
+ * An implementation for @{@link Tuple} which can be generated by the user
+ * Acts as the wrapper for the user payload
+ */
+public class CustomControlTuple extends Tuple
+{
+  private Object userObject;","[{'comment': 'make it final', 'commenter': 'tushargosavi'}]"
440,engine/src/main/java/com/datatorrent/stram/tuple/CustomControlTuple.java,"@@ -0,0 +1,59 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.stram.tuple;
+
+import java.util.UUID;
+
+import com.esotericsoftware.kryo.serializers.FieldSerializer;
+import com.esotericsoftware.kryo.serializers.JavaSerializer;
+
+import com.datatorrent.bufferserver.packet.MessageType;
+
+/**
+ * An implementation for @{@link Tuple} which can be generated by the user
+ * Acts as the wrapper for the user payload
+ */
+public class CustomControlTuple extends Tuple
+{
+  private Object userObject;
+  @FieldSerializer.Bind(JavaSerializer.class)
+  private UUID uid;","[{'comment': 'make it final', 'commenter': 'tushargosavi'}]"
440,engine/src/test/java/com/datatorrent/stram/engine/GenericNodeTest.java,"@@ -609,6 +611,143 @@ public void run()
   }
 
   @Test
+  public void testCustomControlTuplesOrder() throws InterruptedException
+  {
+    long maxSleep = 5000000;
+    long sleeptime = 25L;
+    GenericOperator go = new GenericOperator();
+    final GenericNode gn = new GenericNode(go, new com.datatorrent.stram.engine.OperatorContext(0, ""operator"",
+        new DefaultAttributeMap(), null));
+    gn.setId(1);
+    AbstractReservoir reservoir1 = AbstractReservoir.newReservoir(""ip1Res"", 1024);
+
+    gn.connectInputPort(""ip1"", reservoir1);
+    TestSink testSink = new TestSink();
+    gn.connectOutputPort(""op"", testSink);
+    gn.firstWindowMillis = 0;
+    gn.windowWidthMillis = 100;
+
+    final AtomicBoolean ab = new AtomicBoolean(false);
+    Thread t = new Thread()
+    {
+      @Override
+      public void run()
+      {
+        ab.set(true);
+        gn.activate();
+        gn.run();
+        gn.deactivate();
+      }
+    };
+    t.start();
+
+    long interval = 0;
+    do {
+      Thread.sleep(sleeptime);
+      interval += sleeptime;
+    } while ((ab.get() == false) && (interval < maxSleep));
+
+    int controlTupleCount = gn.controlTupleCount;
+    Tuple beginWindow = new Tuple(MessageType.BEGIN_WINDOW, 0x1L);
+    reservoir1.add(beginWindow);
+
+    interval = 0;
+    do {
+      Thread.sleep(sleeptime);
+      interval += sleeptime;
+    } while ((gn.controlTupleCount == controlTupleCount) && (interval < maxSleep));
+    controlTupleCount = gn.controlTupleCount;
+
+    CustomControlTuple t1 = new CustomControlTuple(new CustomControlTupleTest.TestControlTuple(1, false));
+    CustomControlTuple t2 = new CustomControlTuple(new CustomControlTupleTest.TestControlTuple(2, true));
+    CustomControlTuple t3 = new CustomControlTuple(new CustomControlTupleTest.TestControlTuple(3, false));
+    CustomControlTuple t4 = new CustomControlTuple(new CustomControlTupleTest.TestControlTuple(4, true));
+    reservoir1.add(t1);
+    reservoir1.add(t2);
+    reservoir1.add(t3);
+    reservoir1.add(t4);
+
+    interval = 0;
+    do {
+      Thread.sleep(sleeptime);
+      interval += sleeptime;
+    } while ((gn.controlTupleCount == controlTupleCount) && (interval < maxSleep));
+
+    Assert.assertTrue(""Custom control tuples emitted immediately"", testSink.getResultCount() == 3);
+
+    controlTupleCount = gn.controlTupleCount;
+    Tuple endWindow = new Tuple(MessageType.END_WINDOW, 0x1L);
+    reservoir1.add(endWindow);
+
+    interval = 0;
+    do {
+      Thread.sleep(sleeptime);
+      interval += sleeptime;
+    } while ((gn.controlTupleCount == controlTupleCount) && (interval < maxSleep));
+
+    gn.shutdown();
+    t.join();
+
+    Assert.assertTrue(""Total control tuples"", testSink.getResultCount() == 6);
+
+    long expected = 0;
+    for (Object o: testSink.collectedTuples) {
+      if (o instanceof CustomControlTuple) {
+        expected++;
+      }
+    }
+    Assert.assertTrue(""Number of Custom control tuples"", expected == 4);
+  }","[{'comment': 'Add similar tests for OiO node.', 'commenter': 'tushargosavi'}]"
445,bufferserver/src/main/java/com/datatorrent/bufferserver/internal/DataList.java,"@@ -822,10 +831,10 @@ public void run()
       };
     }
 
-    protected void release(boolean wait)
+    protected void release(boolean wait, boolean writer)
     {
       final int refCount = this.refCount.decrementAndGet();
-      if (refCount == 0 && storage != null) {
+      if (refCount == 0 && storage != null && canRelease(writer)) {","[{'comment': 'How about having the check in *switchToNewBufferOrSuspendRead* ? Then we can have *backPressureCheck* method, without any code changes to release method.', 'commenter': 'sandeshh'}, {'comment': 'Functionally the responsibility of the method is to check if the block can be released (the refCount check) and release it so why not here. Additionally, the release would be triggered by the subscriber not the publisher.', 'commenter': 'pramodin'}, {'comment': ""DataList's memory manager should not be responsible for backpressure. It should be the responsibility of the BufferServer, this will allow us to change the policy of backpressure easily from Block granularity to Window/tuple granularity.\r\n\r\nInstead of waiting till the end and blocking, adaptive sleep per tuple/window may be a good alternative, as it can quickly reduce the numbers of tuple generated per window."", 'commenter': 'sandeshh'}, {'comment': ""I don't think you can make a blanket statement like that. DataList for practical purposes is a queue and queue implementations enforce blocking all the time, think ArrayBlockingQueue.\r\n \r\nWhile there may be opportunity to implement more advanced flow control with the queues inside the queue or using pluggable mechanism, this implementation aims to provide a default control mechanism where none is present today."", 'commenter': 'pramodin'}, {'comment': ""This is a back pressure implementation with additional performance gain block eviction policy: blocks not yet consumed by all Subscribers can't be spooled. In general, back pressure may be implemented without such assumption. It can define a number of blocks or Apex platform windows that Publisher may be ahead from all Subscribers. The number of blocks may be larger than the total number of blocks allowed to be allocated and some blocks may be spooled to disk. It may be good to have this reflected in the attribute name."", 'commenter': 'vrozov'}, {'comment': '@sandeshh regarding your second point, what you are suggesting should probably happen at the input operator because we need to maintain window data consistency. It would need to happen outside the buffer server but using buffer server stats.', 'commenter': 'pramodin'}, {'comment': '@vrozov I think you make some good points but those could be covered by separate attributes like number of blocks when to start apply pressure or as sandesh suggested a pluggable policy and number of blocks being a property of one such policy. In fact, I might be able to add the ability to configure number of blocks (taking spooling into account) trivially.', 'commenter': 'pramodin'}]"
445,api/src/main/java/com/datatorrent/api/Context.java,"@@ -402,6 +402,10 @@
      */
     Attribute<Boolean> BUFFER_SPOOLING = new Attribute<Boolean>(true);
     /**
+     * Whether to apply back pressure and stall the upstream operator when the downstream operator is slow.
+     */
+    Attribute<Boolean> BUFFER_BACK_PRESSURE = new Attribute<Boolean>(true);","[{'comment': 'Why is it necessary to have an attribute for this. Back pressure should work automatically.', 'commenter': 'tweise'}, {'comment': ""I have been thinking about this as well. I went with this becaue of two reasons.\r\n\r\n1. There may be scenarios where the input source is bursty and also doesn't have long memory, so large amount of data arrives for a period of time but only remains in the input source for a short period of time after that, so it has to be read out of the input source quickly. In that case it would be better to disable the back pressure and have the data be in our dag and spool it in the input operator for fault tolerance. Because of the bursty nature the output operator will catch up with the input.\r\n\r\n2. There may be some unintended issue caused by this fix that we don't see in all our collective wisdom today. At least it will give an opportunity for the user to go back to old behavior."", 'commenter': 'pramodin'}, {'comment': ""The spooled buffer isn't durable, it does not survive a container restart and does not contribute to fault-tolerance. The input operator has to be able to replay, either from the source or from its own backup store / WAL. How does publisher blocking or not affect that?\r\n"", 'commenter': 'tweise'}, {'comment': 'I do see a use case where an operator may want to burst output, for example in a case of an aggregation: an operator waits to emit the result till the aggregation is done and while waiting, it may accumulate tuples in memory. Once the aggregation is done, the operator emits a large chunk of data that can and should be spooled to disk and later it will be consumed by subscribers. At the same time, the operator is ready to accumulate new tuples in memory for the next aggregation.\r\n\r\nPutting the attribute on the DAG assumes that it is intended for preserving backward compatibility with the old behavior. To handle burst output, it should be an attribute of the stream (output port).', 'commenter': 'vrozov'}, {'comment': '@tweise For case 1 of bursty input source and limited retention period. I did mean WAL when I said ""spool it in the input operator"", probably shouldn\'t have used the word spool because of its use in buffer server context. We may not want to block input operator because the input source has limited retention period, so we want the input operator to read in the data and bring it into the application and not get blocked. For example a folder where files are dropped in as a burst and cleaned up soon after so the data has to be brought into the application even if the rest of the dag takes time to process it and store it.', 'commenter': 'pramodin'}, {'comment': ""@vrozov I don't quite understand your example. If it is aggregation won't the operator emit less data. Do you mean like accumulation into a big collection as opposed to aggregation? Also, having back pressure doesn't mean no spooling, data can still get spooled once the subscribers have read the data. \r\n\r\nIf we have back pressure as a per port setting, the first operator in the graph traversing from the input, that has the setting turned on will get blocked and its upstream operators will move forward and continue to process data. I don't see a big benefit to this as opposed to turning it off at a DAG level as the application would be pulling data in from the input source anyway."", 'commenter': 'pramodin'}, {'comment': 'I also think it should be an attribute on the output port to provide full flexibility. If at all it will be used for a very specific reason (such as dealing with subscribers that have different throughput pattern) and the user will understand the effect it has.', 'commenter': 'tweise'}, {'comment': 'Per port will not work. There is only a single thread running the operator so if there is back pressure from one port blocking the operator writing to the port, it cannot write to another port.\r\n\r\nIt can be an operator level setting and I can implement it, but I am not able to see a big benefit over enabling/disabling at the app level as some operators will be able to go ahead but some will wait, overall not achieving any bigger purpose.\r\n\r\nI see the advantage in having the ability to enable/disable it at DAG level, because by disabling it we will be able to bring data into the DAG asap which may be needed in some scenarios where input sources cannot hold data for long period of time.', 'commenter': 'pramodin'}, {'comment': '@PramodSSImmaneni It depends on an aggregation function. Aggregation does not always mean reduce. Independently, whether it is a simple accumulation or additional reduce, in my use case data may be accumulated in an operator for a long period of time leading to a large output size even after optional reduce.\r\n\r\nIn a DAG that reads data from Kafka, parses it, aggregates and continue processing, the back pressure needs to be enabled between Kafka and the aggregate operator but be disabled for the aggregate operator output.', 'commenter': 'vrozov'}, {'comment': 'Why would it need to be disabled for aggregate output?', 'commenter': 'pramodin'}, {'comment': '@PramodSSImmaneni it needs to be disabled for the aggregate operator to be able to emit all aggregates and let the aggregate operator to free up memory and start consuming from Kafka. Operators downstream from the aggregate can consume data from the aggregate operator buffer server at their own pace. In this example, buffer server that is co-located in the same container as the aggregate operator works like Kafka.', 'commenter': 'vrozov'}, {'comment': 'It should be OK to have per port attribute. In a case back pressure is enabled only for a subset of an operator output ports, the operator will proceed at the speed of the slowest port that in this case will most likely be the port where the back pressure is enabled. It is similar to the case where an operator may have two ports/streams and one stream is container local with a small queue size. Assuming that the downstream operators (both local and remote) process tuples at the same rate, the operator will be blocked emitting to inline stream (due to small size).', 'commenter': 'vrozov'}]"
445,bufferserver/src/main/java/com/datatorrent/bufferserver/internal/DataList.java,"@@ -845,6 +854,30 @@ protected void release(boolean wait)
       }
     }
 
+    private boolean canRelease(boolean writer)","[{'comment': 'private boolean isSpoolAllowed(boolean isPublisher)? ', 'commenter': 'vrozov'}, {'comment': 'I think the caller makes the decision to do the spooling. This method is just saying the block can be released because it is not needed so I am thinking the general thought around the method name is fine.', 'commenter': 'pramodin'}, {'comment': ""'release' is a two step process. It includes decrementing reference count and if resulting reference count is zero and spooling is enabled, deciding based on eviction policy whether the block should be also spooled to disk. The first part (decrementing reference count) is done always. `canRelease` determines whether the block that is released can be also spooled to disk. "", 'commenter': 'vrozov'}, {'comment': 'canEvict() or isEvictable()?', 'commenter': 'vrozov'}, {'comment': 'The calling method is called release hence went with canRelease. I am fine either way. ', 'commenter': 'pramodin'}, {'comment': 'For a method named `canRelease()` I would expect the following sequence:\r\n```\r\nif (canRelease()) {\r\n  release();\r\n}\r\n```\r\nThis method returns true if block can be evicted from memory.', 'commenter': 'vrozov'}, {'comment': 'What do you think about renaming release() to checkRelease() instead? If not, I can change canRelease() it to canEvict()?', 'commenter': 'pramodin'}]"
445,bufferserver/src/test/java/com/datatorrent/bufferserver/server/ServerTest.java,"@@ -70,7 +70,7 @@ public static void setupServerAndClients() throws Exception
     eventloopServer.start();
     eventloopClient.start();
 
-    instance = new Server(0, 4096,8);
+    instance = new Server(0, 4096, 8);","[{'comment': 'IMO, we should either enforce space after comma in codestyle check (it is not supported by the version of checkstyle Apex uses) or avoid format changes.', 'commenter': 'vrozov'}, {'comment': 'What you say makes sense but this is an oversight where there is space after one argument and no space after another in the same line. I had originally touched this line and hence made the change.', 'commenter': 'pramodin'}]"
445,api/src/main/java/com/datatorrent/api/Context.java,"@@ -181,6 +181,12 @@
      */
     Attribute<Class<?>> TUPLE_CLASS = new Attribute<>(Class2String.getInstance());
 
+    /**","[{'comment': 'Please remove doc and the commented attribute', 'commenter': 'vrozov'}, {'comment': ""It's in PortContext not DAGContext. It can stay as a placeholder to indicate that the plan is to go stream level. I can add a TODO or I can remove it. Let me know."", 'commenter': 'pramodin'}]"
448,engine/src/main/java/com/datatorrent/stram/RecoverableRpcProxy.java,"@@ -68,98 +68,111 @@
   private static final int RPC_TIMEOUT_DEFAULT = 5000;
 
   private final Configuration conf;
-  private final String appPath;
   private StreamingContainerUmbilicalProtocol umbilical;
   private String lastConnectURI;
-  private long lastCompletedCallTms;
-  private long retryTimeoutMillis = Long.getLong(RETRY_TIMEOUT, RETRY_TIMEOUT_DEFAULT);
-  private long retryDelayMillis = Long.getLong(RETRY_DELAY, RETRY_DELAY_DEFAULT);
-  private int rpcTimeout = Integer.getInteger(RPC_TIMEOUT, RPC_TIMEOUT_DEFAULT);
-
-  public RecoverableRpcProxy(String appPath, Configuration conf) throws IOException
+  private long retryTimeoutMillis;
+  private long retryDelayMillis;
+  private int rpcTimeout;
+  private final UserGroupInformation currentUser;
+  private final SocketFactory defaultSocketFactory;
+  private final FSRecoveryHandler fsRecoveryHandler;
+
+  public RecoverableRpcProxy(String appPath, Configuration conf)
   {
     this.conf = conf;
-    this.appPath = appPath;
-    connect();
+    try {
+      currentUser = UserGroupInformation.getCurrentUser();
+      defaultSocketFactory = NetUtils.getDefaultSocketFactory(conf);
+      fsRecoveryHandler = new FSRecoveryHandler(appPath, conf);
+      connect(0);
+    } catch (IOException e) {
+      LOG.error(""Fail to create RecoverableRpcProxy"", e);
+      throw new RuntimeException(e);
+    }
   }
 
-  private void connect() throws IOException
+  private long connect(long timeMillis) throws IOException","[{'comment': ""timeMillis isn't used anywhere?\r\n"", 'commenter': 'tweise'}, {'comment': 'timeMillis is the return value of the connect() in case connect URL was not changed. It is reset on line 130 in the case of updated URL.', 'commenter': 'vrozov'}]"
448,engine/src/main/java/com/datatorrent/stram/RecoverableRpcProxy.java,"@@ -68,98 +68,111 @@
   private static final int RPC_TIMEOUT_DEFAULT = 5000;
 
   private final Configuration conf;
-  private final String appPath;
   private StreamingContainerUmbilicalProtocol umbilical;
   private String lastConnectURI;
-  private long lastCompletedCallTms;
-  private long retryTimeoutMillis = Long.getLong(RETRY_TIMEOUT, RETRY_TIMEOUT_DEFAULT);
-  private long retryDelayMillis = Long.getLong(RETRY_DELAY, RETRY_DELAY_DEFAULT);
-  private int rpcTimeout = Integer.getInteger(RPC_TIMEOUT, RPC_TIMEOUT_DEFAULT);
-
-  public RecoverableRpcProxy(String appPath, Configuration conf) throws IOException
+  private long retryTimeoutMillis;","[{'comment': 'Why are the defaults removed and then set elsewhere, how is it related to this issue?\r\n', 'commenter': 'tweise'}, {'comment': 'In case URL does not have one of default values specified, that value should be reset to the default.', 'commenter': 'vrozov'}]"
448,engine/src/main/java/com/datatorrent/stram/StreamingContainerParent.java,"@@ -208,20 +212,15 @@ public ContainerHeartbeatResponse processHeartbeat(ContainerHeartbeat msg)
     //LOG.debug(""RPC latency from child container {} is {} ms (according to system clocks)"", msg.getContainerId(),
     // now - msg.sentTms);
     dagManager.updateRPCLatency(msg.getContainerId(), now - msg.sentTms);
-    try {
-      final ContainerHeartbeat fmsg = msg;
-      return SecureExecutor.execute(new SecureExecutor.WorkLoad<ContainerHeartbeatResponse>()
+    final ContainerHeartbeat fmsg = msg;
+    return SecureExecutor.execute(new SecureExecutor.WorkLoad<ContainerHeartbeatResponse>()
+    {
+      @Override
+      public ContainerHeartbeatResponse run()
       {
-        @Override
-        public ContainerHeartbeatResponse run()
-        {
-          return dagManager.processHeartbeat(fmsg);
-        }
-      });
-    } catch (IOException ex) {
-      LOG.error(""Error processing heartbeat"", ex);","[{'comment': 'I suppose this was logged to that we have visibility in the AM log. What happens now, what is the benefit of propagating IOException?', 'commenter': 'tweise'}, {'comment': 'Before the exception was swallowed and `null` was returned, causing NullPointerException on the RPC client/container side. It now propagates the exception to the RPC client, so standard mechanism for RPC timeout and recovery is invoked.', 'commenter': 'vrozov'}, {'comment': ""That's because the recovery wasn't intended for internal errors but for connectivity issues including when the AM moves to new location. There is also an unnecessary assignment on line 215: final ContainerHeartbeat fmsg = msg; - can you also remove that while at it?"", 'commenter': 'tweise'}]"
448,engine/src/main/java/com/datatorrent/stram/engine/StreamingContainer.java,"@@ -1418,19 +1419,32 @@ public void run()
               logger.error(""Voluntary container termination due to an error in operator {}."", currentdi, error);
               operators = new int[]{currentdi.id};
             }
-            umbilical.reportError(containerId, operators, ""Voluntary container termination due to an error. "" + ExceptionUtils.getStackTrace(error));
-            System.exit(1);
+            try {
+              umbilical.reportError(containerId, operators, ""Voluntary container termination due to an error. "" + ExceptionUtils.getStackTrace(error));
+            } catch (Exception e) {
+              logger.debug(""Fail to log"", e);
+            } finally {
+              System.exit(1);
+            }
           } catch (Exception ex) {
             if (currentdi == null) {
               failedNodes.add(ndi.id);
               logger.error(""Operator set {} stopped running due to an exception."", setOperators, ex);
               int[] operators = new int[]{ndi.id};
-              umbilical.reportError(containerId, operators, ""Stopped running due to an exception. "" + ExceptionUtils.getStackTrace(ex));
+              try {
+                umbilical.reportError(containerId, operators, ""Stopped running due to an exception. "" + ExceptionUtils.getStackTrace(ex));
+              } catch (Exception e) {","[{'comment': ""Doesn't this change the flow where before exception would propagate and now it is swallowed?"", 'commenter': 'tweise'}, {'comment': 'The catch is in main(), there is no need to propagate it. Otherwise, it would propagate to the default exception handler that writes the exception to stderr.', 'commenter': 'vrozov'}, {'comment': 'I must be missing something. This is in activate, which is called from deploy.', 'commenter': 'tweise'}, {'comment': 'Oops, looked at the wrong place. It is in the run() of an operator thread created inside activate(). Before exception would become an unhandled exception and it would be handled by the ThreadGroup.uncaughtException() that writes exception stack trace to stderr.', 'commenter': 'vrozov'}]"
463,engine/src/main/java/com/datatorrent/stram/client/AppPackage.java,"@@ -329,6 +330,13 @@ public void processAppDirectory(boolean skipJars)
 
     Configuration config = new Configuration();
 
+    // load configuration with default properties
+    if (MapUtils.isNotEmpty(defaultProperties)) {
+      for (String key : defaultProperties.keySet()) {","[{'comment': 'Please use .entrySet() instead of .keySet()', 'commenter': 'sandeshh'}, {'comment': '@sandeshh updated code as per comments . Will add unit tests in couple of days and will inform you to review tests at that time.', 'commenter': 'patilvikram'}]"
463,engine/src/main/java/com/datatorrent/stram/client/AppPackage.java,"@@ -329,6 +330,13 @@ public void processAppDirectory(boolean skipJars)
 
     Configuration config = new Configuration();
 
+    // load configuration with default properties
+    if (MapUtils.isNotEmpty(defaultProperties)) {","[{'comment': ""The check is not needed as defaultProperties can't be null and it is OK to iterate over empty Map."", 'commenter': 'vrozov'}]"
463,engine/src/main/java/com/datatorrent/stram/client/AppPackage.java,"@@ -329,6 +329,11 @@ public void processAppDirectory(boolean skipJars)
 
     Configuration config = new Configuration();
 
+    // load configuration with default properties","[{'comment': 'Comment is not required here.', 'commenter': 'sandeshh'}]"
463,.idea/codeStyleSettings.xml,"@@ -43,6 +43,9 @@
         <JavaCodeStyleSettings>
           <option name=""CLASS_NAMES_IN_JAVADOC"" value=""3"" />
         </JavaCodeStyleSettings>
+        <MarkdownNavigatorCodeStyleSettings>","[{'comment': 'This should not be part of your change.', 'commenter': 'sandeshh'}]"
463,engine/src/test/java/com/datatorrent/stram/client/AppPackageTest.java,"@@ -105,7 +105,13 @@ public void testAppPackage() throws Exception
 
     Assert.assertEquals(""mydtapp-1.0-SNAPSHOT.jar"", apps.get(""MyFirstApplication"").getString(""file""));
 
+    Assert.assertNotNull(apps.get(""MyFirstApplication"").get(""dag""));
     JSONObject dag = apps.get(""MyFirstApplication"").getJSONObject(""dag"");
+
+    String errorStackTrace = apps.get(""MyFirstApplication"").getString(""errorStackTrace"");
+    Assert.assertEquals(""null"",errorStackTrace);","[{'comment': 'redundant assert.', 'commenter': 'sandeshh'}]"
463,engine/src/main/java/com/datatorrent/stram/client/AppPackage.java,"@@ -7,7 +7,7 @@
  * ""License""); you may not use this file except in compliance
  * with the License.  You may obtain a copy of the License at
  *
- *   http://www.apache.org/licenses/LICENSE-2.0
+ * http://www.apache.org/licenses/LICENSE-2.0","[{'comment': 'Why is it necessary?', 'commenter': 'vrozov'}, {'comment': 'Why is it necessary?', 'commenter': 'vrozov'}, {'comment': '@vrozov  Fixed this one', 'commenter': 'patilvikram'}, {'comment': 'Fixed', 'commenter': 'patilvikram'}]"
463,engine/src/main/java/com/datatorrent/stram/client/AppPackage.java,"@@ -120,7 +119,8 @@ public AppPackage(File file) throws IOException, ZipException
    * If app directory is to be processed, there may be resource leak in the class loader. Only pass true for short-lived
    * applications
    *
-   * If contentFolder is not null, it will try to create the contentFolder, file will be retained on disk after App Package is closed
+   * If contentFolder is not null, it will try to create the contentFolder, file will be retained on disk after App","[{'comment': 'Format changes should be avoided', 'commenter': 'vrozov'}, {'comment': 'Fixed', 'commenter': 'patilvikram'}]"
463,engine/src/main/java/com/datatorrent/stram/client/AppPackage.java,"@@ -155,7 +155,8 @@ public AppPackage(File file, File contentFolder, boolean processAppDirectory) th
     appPackageDescription = attr.getValue(ATTRIBUTE_DT_APP_PACKAGE_DESCRIPTION);
     String classPathString = attr.getValue(ATTRIBUTE_CLASS_PATH);
     if (appPackageName == null || appPackageVersion == null || classPathString == null) {
-      throw new IOException(""Not a valid app package.  App Package Name or Version or Class-Path is missing from MANIFEST.MF"");
+      throw new IOException(""Not a valid app package.  App Package Name or Version or Class-Path is missing from "" +","[{'comment': 'Format changes should be avoided', 'commenter': 'vrozov'}, {'comment': 'Fixed', 'commenter': 'patilvikram'}]"
463,engine/src/main/scripts/apex,"@@ -91,7 +91,7 @@ if [ -n ""$DT_CLASSPATH"" ]; then
     export HADOOP_CLASSPATH=""$HADOOP_CLASSPATH:$DT_CLASSPATH""
   fi
 fi
-
+echo $DT_CLASSPATH","[{'comment': '?', 'commenter': 'vrozov'}, {'comment': 'Fixed', 'commenter': 'patilvikram'}]"
463,engine/src/test/resources/testAppPackage/mydtapp/src/main/java/com/example/mydtapp/Application.java,"@@ -20,6 +20,8 @@
 
 import org.apache.hadoop.conf.Configuration;
 
+import com.esotericsoftware.kryo.NotNull;","[{'comment': 'Where is it used?', 'commenter': 'vrozov'}, {'comment': 'Removed', 'commenter': 'patilvikram'}]"
463,engine/src/test/resources/testAppPackage/mydtapp/src/main/java/com/example/mydtapp/Application.java,"@@ -29,6 +31,7 @@
 public class Application implements StreamingApplication
 {
 
+","[{'comment': '?', 'commenter': 'vrozov'}, {'comment': 'Removed extra new line character.', 'commenter': 'patilvikram'}]"
463,engine/src/test/java/com/datatorrent/stram/client/AppPackageTest.java,"@@ -139,4 +144,25 @@ public void testAppLevelRequiredProperties()
     }
     Assert.fail(""Should consist of an app called MyFirstApplication"");
   }
+
+  @Test
+  public void testDefaultProperties()
+  {
+
+    Map<String, String> defaultProperties = ap.getDefaultProperties();
+    Assert.assertEquals(7, defaultProperties.size());
+
+    Map<String, String> defaultPropertiesValidationMap = new HashMap<String, String>();
+    defaultPropertiesValidationMap.put(""dt.test.1"", ""package-default"");","[{'comment': 'You can directly do the assert without putting key-val into map.', 'commenter': 'sandeshh'}, {'comment': 'Fixed as per comment', 'commenter': 'patilvikram'}, {'comment': '@sandeshh  Fixed as per your comment', 'commenter': 'patilvikram'}]"
463,engine/src/test/resources/testAppPackage/mydtapp/src/main/java/com/example/mydtapp/Application.java,"@@ -37,6 +37,8 @@ public void populateDAG(DAG dag, Configuration conf)
 
     RandomNumberGenerator rand = dag.addOperator(""rand"", RandomNumberGenerator.class);
     StdoutOperator stdout = dag.addOperator(""stdout"", new StdoutOperator());
+    // This module will be added to dag for testing purpose but will not be connected in a dag.
+    dag.addModule(""testModule"", com.example.mydtapp.TestModule.class);","[{'comment': 'why this change is required? Tests run without this change as well.', 'commenter': 'sandeshh'}, {'comment': '@sandeshh This application is a part of the setup for tests in AppPackageTest. If the code has the module as part of DAG and fix is not provided, tests will fail. But even if I remove the module from tests and with a fix, tests will run as they supposed to. So the module is required to make sure that properties are made available to Custom Modules. \r\n\r\n', 'commenter': 'patilvikram'}, {'comment': 'Operators are more common than Modules, can you also add a check for operators?', 'commenter': 'sandeshh'}]"
463,engine/src/test/resources/testAppPackage/mydtapp/src/main/java/com/example/mydtapp/StdoutOperator.java,"@@ -18,6 +18,7 @@
  */
 package com.example.mydtapp;
 
+import com.esotericsoftware.kryo.NotNull;","[{'comment': 'Where is the import used? Apex does not use Kryo for NotNull annotations.', 'commenter': 'vrozov'}, {'comment': 'Corrected it with the use of import javax.validation.constraints.NotNull;', 'commenter': 'patilvikram'}]"
463,engine/src/test/resources/testAppPackage/mydtapp/src/main/java/com/example/mydtapp/TestModule.java,"@@ -0,0 +1,54 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.example.mydtapp;
+
+import org.apache.hadoop.conf.Configuration;
+import com.esotericsoftware.kryo.NotNull;
+import com.datatorrent.api.DAG;
+import com.datatorrent.api.Module;
+
+/**
+ * This is a simple operator that emits random number
+ */
+
+public class TestModule implements Module
+{
+  @NotNull
+  private String testInput;
+
+  public final transient ProxyOutputPort<String> outputs = new ProxyOutputPort<String>();
+
+  public final transient ProxyInputPort<String> inputs = new ProxyInputPort<String>();
+
+  @Override
+  public void populateDAG(DAG dag, Configuration configuration)
+  {
+    testInput.length();","[{'comment': 'What is the purpose of this operation?', 'commenter': 'vrozov'}, {'comment': 'This throws NullPointerException if testInput is not properly initialized. Fix provided in this pull request indirectly ensures properties mentioned in properties.xml are initialized correctly.', 'commenter': 'patilvikram'}]"
464,engine/src/main/java/com/datatorrent/stram/client/DTConfiguration.java,"@@ -99,6 +102,11 @@ public ConfigException(String message)
     return result.entrySet().iterator();
   }
 
+  public Map<String, ValueEntry> getMap()","[{'comment': 'There is no need for getMap()', 'commenter': 'sandeshh'}, {'comment': 'I am using it in AppPackage.java', 'commenter': 'ajaygit158-zz'}, {'comment': 'DTConfiguration exposes iterators, that should be used. Map is an internal representation which should not be exposed.', 'commenter': 'sandeshh'}]"
464,engine/src/main/java/com/datatorrent/stram/client/AppPackage.java,"@@ -426,21 +427,22 @@ private void processPropertiesXml(File file, AppInfo app)
     DTConfiguration config = new DTConfiguration();
     try {
       config.loadFile(file);
-      for (Map.Entry<String, String> entry : config) {
+      for (Map.Entry<String, ValueEntry> entry : config.getMap().entrySet()) {
         String key = entry.getKey();
-        String value = entry.getValue();
-        if (value == null) {
+        ValueEntry valueEntry = entry.getValue();
+        LOG.info(""Properties: {} {} {}"", key, valueEntry.value, valueEntry.description);","[{'comment': 'LOG.debug?', 'commenter': 'sandeshh'}, {'comment': 'Why is the logging necessary, especially for every Map.Entry in the Set?', 'commenter': 'vrozov'}, {'comment': 'True. Removed the log', 'commenter': 'ajaygit158-zz'}]"
464,engine/src/main/java/com/datatorrent/stram/cli/ApexCli.java,"@@ -3448,13 +3449,25 @@ public void execute(String[] args, ConsoleReader reader) throws Exception
     public void execute(String[] args, ConsoleReader reader) throws Exception
     {
       try (AppPackage ap = newAppPackageInstance(new File(expandFileName(args[1], true)))) {
+        //Ignore all attribute descriptions since attributes are platform dependent and not app-package dependent.","[{'comment': 'Do we even need to filter these values? What if the user wants to have a custom description, as to why they are setting that particular attribute?', 'commenter': 'sandeshh'}, {'comment': '@sandeshh Please see discussion in JIRA.', 'commenter': 'vrozov'}]"
464,engine/src/main/java/com/datatorrent/stram/client/DTConfiguration.java,"@@ -73,7 +74,9 @@
   public static class ValueEntry","[{'comment': 'Please make this private. Introduce PropertyInfo class in AppPackage, with value and description.\r\n\r\nYou can populate the description field from DtConfiguration.getDescription method', 'commenter': 'sandeshh'}]"
464,engine/src/main/java/com/datatorrent/stram/client/DTConfiguration.java,"@@ -70,10 +71,12 @@
   private final Map<String, ValueEntry> map = new LinkedHashMap<>();
   private static final Logger LOG = LoggerFactory.getLogger(DTConfiguration.class);
 
-  public static class ValueEntry
+  private static class ValueEntry
   {
     public String value;
+    @JsonIgnore","[{'comment': 'Is it still necessary?', 'commenter': 'vrozov'}, {'comment': 'Forgot to remove. Removed this in updated PR.', 'commenter': 'ajaygit158-zz'}]"
464,engine/src/main/java/com/datatorrent/stram/client/AppPackage.java,"@@ -426,7 +452,10 @@ private void processPropertiesXml(File file, AppInfo app)
       config.loadFile(file);
       for (Map.Entry<String, String> entry : config) {
         String key = entry.getKey();
+        PropertyInfo propertyInfo = new PropertyInfo();","[{'comment': 'It is not necessary to construct propertyInfo when `value == null`, move to line 466.', 'commenter': 'vrozov'}, {'comment': 'changed', 'commenter': 'ajaygit158-zz'}]"
464,engine/src/main/java/com/datatorrent/stram/client/AppPackage.java,"@@ -109,6 +109,32 @@ public AppInfo(String name, String file, String type)
 
   }
 
+  public static class PropertyInfo
+  {
+    private String value;","[{'comment': 'Both value and description should be final.', 'commenter': 'vrozov'}, {'comment': 'Changed', 'commenter': 'ajaygit158-zz'}]"
464,engine/src/main/java/com/datatorrent/stram/cli/ApexCli.java,"@@ -3474,13 +3475,25 @@ public void execute(String[] args, ConsoleReader reader) throws Exception
     public void execute(String[] args, ConsoleReader reader) throws Exception
     {
       try (AppPackage ap = newAppPackageInstance(new File(expandFileName(args[1], true)))) {
+        //Ignore all attribute descriptions since attributes are platform dependent and not app-package dependent.
+        for (Map.Entry<String, PropertyInfo> entry : ap.getDefaultProperties().entrySet()) {
+          if (entry.getKey().contains("".attr."")) {","[{'comment': ""While creating the PropertyInfo don't load the Description for attributes. So you don't need both the for loops."", 'commenter': 'sandeshh'}, {'comment': 'Updated the code for this change.', 'commenter': 'ajaygit158-zz'}]"
464,engine/src/main/java/com/datatorrent/stram/client/AppPackage.java,"@@ -435,10 +464,10 @@ private void processPropertiesXml(File file, AppInfo app)
           }
         } else {
           if (app == null) {
-            defaultProperties.put(key, value);
+            defaultProperties.put(key, propertyInfo);","[{'comment': 'While creating Property make description null for attributes', 'commenter': 'sandeshh'}]"
466,engine/src/test/java/com/datatorrent/stram/plan/logical/LogicalPlanConfigurationTest.java,"@@ -600,6 +609,104 @@ public void populateDAG(DAG dag, Configuration conf)
   }
 
   @Test
+  @SuppressWarnings({""UnnecessaryBoxing"", ""AssertEqualsBetweenInconvertibleTypes""})
+  public void testModuleUnifierLevelAttributes()
+  {
+    class DummyOperator extends BaseOperator
+    {
+      int prop;","[{'comment': 'Where is `prop` used?', 'commenter': 'vrozov'}, {'comment': ""It was used earlier while testing. I've got rid of it."", 'commenter': 'deepak-narkhede'}]"
466,engine/src/test/java/com/datatorrent/stram/plan/logical/LogicalPlanConfigurationTest.java,"@@ -600,6 +609,104 @@ public void populateDAG(DAG dag, Configuration conf)
   }
 
   @Test
+  @SuppressWarnings({""UnnecessaryBoxing"", ""AssertEqualsBetweenInconvertibleTypes""})
+  public void testModuleUnifierLevelAttributes()
+  {
+    class DummyOperator extends BaseOperator
+    {
+      int prop;
+
+      public transient DefaultInputPort<Integer> input = new DefaultInputPort<Integer>()
+      {
+        @Override
+        public void process(Integer tuple)
+        {
+          LOG.debug(tuple.intValue() + "" processed"");","[{'comment': 'Use smart logging', 'commenter': 'vrozov'}, {'comment': ""Done. Removed the log as it won't be used further."", 'commenter': 'deepak-narkhede'}]"
466,engine/src/test/java/com/datatorrent/stram/plan/logical/LogicalPlanConfigurationTest.java,"@@ -600,6 +609,104 @@ public void populateDAG(DAG dag, Configuration conf)
   }
 
   @Test
+  @SuppressWarnings({""UnnecessaryBoxing"", ""AssertEqualsBetweenInconvertibleTypes""})
+  public void testModuleUnifierLevelAttributes()
+  {
+    class DummyOperator extends BaseOperator
+    {
+      int prop;
+
+      public transient DefaultInputPort<Integer> input = new DefaultInputPort<Integer>()
+      {
+        @Override
+        public void process(Integer tuple)
+        {
+          LOG.debug(tuple.intValue() + "" processed"");
+          output.emit(tuple);
+        }
+      };
+      public transient DefaultOutputPort<Integer> output = new DefaultOutputPort<Integer>();
+    }
+
+    class DummyOutputOperator extends BaseOperator
+    {
+      int prop;","[{'comment': 'Where is `prop` used?', 'commenter': 'vrozov'}, {'comment': 'Same reason. Done.', 'commenter': 'deepak-narkhede'}]"
466,engine/src/test/java/com/datatorrent/stram/plan/logical/LogicalPlanConfigurationTest.java,"@@ -600,6 +609,104 @@ public void populateDAG(DAG dag, Configuration conf)
   }
 
   @Test
+  @SuppressWarnings({""UnnecessaryBoxing"", ""AssertEqualsBetweenInconvertibleTypes""})
+  public void testModuleUnifierLevelAttributes()
+  {
+    class DummyOperator extends BaseOperator
+    {
+      int prop;
+
+      public transient DefaultInputPort<Integer> input = new DefaultInputPort<Integer>()
+      {
+        @Override
+        public void process(Integer tuple)
+        {
+          LOG.debug(tuple.intValue() + "" processed"");
+          output.emit(tuple);
+        }
+      };
+      public transient DefaultOutputPort<Integer> output = new DefaultOutputPort<Integer>();
+    }
+
+    class DummyOutputOperator extends BaseOperator","[{'comment': 'Why is `DummyOperator` not sufficient for the test?', 'commenter': 'vrozov'}, {'comment': 'Yes, It is sufficient. Initially had more logic for testing purpose hence had created new output operator. Changed the code to use only DummyOperator.', 'commenter': 'deepak-narkhede'}]"
466,engine/src/test/java/com/datatorrent/stram/plan/logical/LogicalPlanConfigurationTest.java,"@@ -600,6 +609,104 @@ public void populateDAG(DAG dag, Configuration conf)
   }
 
   @Test
+  @SuppressWarnings({""UnnecessaryBoxing"", ""AssertEqualsBetweenInconvertibleTypes""})
+  public void testModuleUnifierLevelAttributes()
+  {
+    class DummyOperator extends BaseOperator
+    {
+      int prop;
+
+      public transient DefaultInputPort<Integer> input = new DefaultInputPort<Integer>()
+      {
+        @Override
+        public void process(Integer tuple)
+        {
+          LOG.debug(tuple.intValue() + "" processed"");
+          output.emit(tuple);
+        }
+      };
+      public transient DefaultOutputPort<Integer> output = new DefaultOutputPort<Integer>();
+    }
+
+    class DummyOutputOperator extends BaseOperator
+    {
+      int prop;
+
+      public transient DefaultInputPort<Integer> input = new DefaultInputPort<Integer>()
+      {
+        @Override
+        public void process(Integer tuple)
+        {
+          LOG.debug(tuple.intValue() + "" processed"");
+        }
+      };
+    }
+
+    class TestUnifierAttributeModule implements Module
+    {
+      public transient ProxyInputPort<Integer> moduleInput = new ProxyInputPort<Integer>();","[{'comment': 'use 1.7 type inference in generics', 'commenter': 'vrozov'}, {'comment': 'Done.', 'commenter': 'deepak-narkhede'}]"
466,engine/src/test/java/com/datatorrent/stram/plan/logical/LogicalPlanConfigurationTest.java,"@@ -600,6 +609,104 @@ public void populateDAG(DAG dag, Configuration conf)
   }
 
   @Test
+  @SuppressWarnings({""UnnecessaryBoxing"", ""AssertEqualsBetweenInconvertibleTypes""})
+  public void testModuleUnifierLevelAttributes()
+  {
+    class DummyOperator extends BaseOperator
+    {
+      int prop;
+
+      public transient DefaultInputPort<Integer> input = new DefaultInputPort<Integer>()
+      {
+        @Override
+        public void process(Integer tuple)
+        {
+          LOG.debug(tuple.intValue() + "" processed"");
+          output.emit(tuple);
+        }
+      };
+      public transient DefaultOutputPort<Integer> output = new DefaultOutputPort<Integer>();
+    }
+
+    class DummyOutputOperator extends BaseOperator
+    {
+      int prop;
+
+      public transient DefaultInputPort<Integer> input = new DefaultInputPort<Integer>()
+      {
+        @Override
+        public void process(Integer tuple)
+        {
+          LOG.debug(tuple.intValue() + "" processed"");
+        }
+      };
+    }
+
+    class TestUnifierAttributeModule implements Module
+    {
+      public transient ProxyInputPort<Integer> moduleInput = new ProxyInputPort<Integer>();
+      public transient ProxyOutputPort<Integer> moduleOutput = new Module.ProxyOutputPort<Integer>();
+
+      @Override
+      public void populateDAG(DAG dag, Configuration conf)
+      {
+        DummyOperator dummyOperator = dag.addOperator(""DummyOperator"", new DummyOperator());
+        dag.setOperatorAttribute(dummyOperator, Context.OperatorContext.PARTITIONER, new StatelessPartitioner<DummyOperator>(3));","[{'comment': 'Shorten to `OperatorContext.PARTITIONER`', 'commenter': 'vrozov'}, {'comment': 'Done.', 'commenter': 'deepak-narkhede'}]"
466,engine/src/test/java/com/datatorrent/stram/plan/logical/LogicalPlanConfigurationTest.java,"@@ -600,6 +609,104 @@ public void populateDAG(DAG dag, Configuration conf)
   }
 
   @Test
+  @SuppressWarnings({""UnnecessaryBoxing"", ""AssertEqualsBetweenInconvertibleTypes""})
+  public void testModuleUnifierLevelAttributes()
+  {
+    class DummyOperator extends BaseOperator
+    {
+      int prop;
+
+      public transient DefaultInputPort<Integer> input = new DefaultInputPort<Integer>()
+      {
+        @Override
+        public void process(Integer tuple)
+        {
+          LOG.debug(tuple.intValue() + "" processed"");
+          output.emit(tuple);
+        }
+      };
+      public transient DefaultOutputPort<Integer> output = new DefaultOutputPort<Integer>();
+    }
+
+    class DummyOutputOperator extends BaseOperator
+    {
+      int prop;
+
+      public transient DefaultInputPort<Integer> input = new DefaultInputPort<Integer>()
+      {
+        @Override
+        public void process(Integer tuple)
+        {
+          LOG.debug(tuple.intValue() + "" processed"");
+        }
+      };
+    }
+
+    class TestUnifierAttributeModule implements Module
+    {
+      public transient ProxyInputPort<Integer> moduleInput = new ProxyInputPort<Integer>();
+      public transient ProxyOutputPort<Integer> moduleOutput = new Module.ProxyOutputPort<Integer>();
+
+      @Override
+      public void populateDAG(DAG dag, Configuration conf)
+      {
+        DummyOperator dummyOperator = dag.addOperator(""DummyOperator"", new DummyOperator());
+        dag.setOperatorAttribute(dummyOperator, Context.OperatorContext.PARTITIONER, new StatelessPartitioner<DummyOperator>(3));
+        dag.setUnifierAttribute(dummyOperator.output, OperatorContext.TIMEOUT_WINDOW_COUNT, 2);
+        moduleInput.set(dummyOperator.input);
+        moduleOutput.set(dummyOperator.output);
+      }
+    }
+
+    StreamingApplication app = new StreamingApplication()
+    {
+      @Override
+      public void populateDAG(DAG dag, Configuration conf)
+      {
+        Module m1 = dag.addModule(""TestModule"", new TestUnifierAttributeModule());","[{'comment': 'usually `m1` means that there should be `m2`. Please be consistent in naming variables. Change `Module` to `TestUnifierAttributeModule` as it is casted to `TestUnifierAttributeModule` anyway.', 'commenter': 'vrozov'}, {'comment': 'Done.', 'commenter': 'deepak-narkhede'}]"
466,engine/src/test/java/com/datatorrent/stram/plan/logical/LogicalPlanConfigurationTest.java,"@@ -600,6 +609,104 @@ public void populateDAG(DAG dag, Configuration conf)
   }
 
   @Test
+  @SuppressWarnings({""UnnecessaryBoxing"", ""AssertEqualsBetweenInconvertibleTypes""})
+  public void testModuleUnifierLevelAttributes()
+  {
+    class DummyOperator extends BaseOperator
+    {
+      int prop;
+
+      public transient DefaultInputPort<Integer> input = new DefaultInputPort<Integer>()
+      {
+        @Override
+        public void process(Integer tuple)
+        {
+          LOG.debug(tuple.intValue() + "" processed"");
+          output.emit(tuple);
+        }
+      };
+      public transient DefaultOutputPort<Integer> output = new DefaultOutputPort<Integer>();
+    }
+
+    class DummyOutputOperator extends BaseOperator
+    {
+      int prop;
+
+      public transient DefaultInputPort<Integer> input = new DefaultInputPort<Integer>()
+      {
+        @Override
+        public void process(Integer tuple)
+        {
+          LOG.debug(tuple.intValue() + "" processed"");
+        }
+      };
+    }
+
+    class TestUnifierAttributeModule implements Module
+    {
+      public transient ProxyInputPort<Integer> moduleInput = new ProxyInputPort<Integer>();
+      public transient ProxyOutputPort<Integer> moduleOutput = new Module.ProxyOutputPort<Integer>();
+
+      @Override
+      public void populateDAG(DAG dag, Configuration conf)
+      {
+        DummyOperator dummyOperator = dag.addOperator(""DummyOperator"", new DummyOperator());
+        dag.setOperatorAttribute(dummyOperator, Context.OperatorContext.PARTITIONER, new StatelessPartitioner<DummyOperator>(3));
+        dag.setUnifierAttribute(dummyOperator.output, OperatorContext.TIMEOUT_WINDOW_COUNT, 2);
+        moduleInput.set(dummyOperator.input);
+        moduleOutput.set(dummyOperator.output);
+      }
+    }
+
+    StreamingApplication app = new StreamingApplication()
+    {
+      @Override
+      public void populateDAG(DAG dag, Configuration conf)
+      {
+        Module m1 = dag.addModule(""TestModule"", new TestUnifierAttributeModule());
+        DummyOutputOperator dummyOutputOperator = dag.addOperator(""DummyOutputOperator"", new DummyOutputOperator());
+        dag.addStream(""Module To Operator"", ((TestUnifierAttributeModule)m1).moduleOutput, dummyOutputOperator.input);
+      }
+    };
+
+    String appName = ""UnifierApp"";
+    LogicalPlanConfiguration dagBuilder = new LogicalPlanConfiguration(new Configuration(false));
+    LogicalPlan dag = new LogicalPlan();
+    dag.setAttribute(Context.OperatorContext.STORAGE_AGENT, new MockStorageAgent());
+    dagBuilder.prepareDAG(dag, app, appName);
+    LogicalPlan.OperatorMeta ometa = dag.getOperatorMeta(""TestModule$DummyOperator"");
+    LogicalPlan.OperatorMeta om = null;
+    for (Map.Entry<LogicalPlan.OutputPortMeta, LogicalPlan.StreamMeta> entry : ometa.getOutputStreams().entrySet()) {
+      if (entry.getKey().getPortName().equals(""output"")) {
+        om = entry.getKey().getUnifierMeta();
+      }
+    }
+
+    /*
+     * Verify the attribute value after preparing DAG.
+     */
+    Assert.assertNotNull(om);
+    Assert.assertEquals("""", Integer.valueOf(2), om.getValue(Context.OperatorContext.TIMEOUT_WINDOW_COUNT));","[{'comment': 'Why is boxing necessary? If message is empty, why not to use Assert.assertEquals() that does not require message?', 'commenter': 'vrozov'}, {'comment': 'Added valid messages.', 'commenter': 'deepak-narkhede'}]"
466,engine/src/test/java/com/datatorrent/stram/plan/logical/LogicalPlanConfigurationTest.java,"@@ -600,6 +609,104 @@ public void populateDAG(DAG dag, Configuration conf)
   }
 
   @Test
+  @SuppressWarnings({""UnnecessaryBoxing"", ""AssertEqualsBetweenInconvertibleTypes""})
+  public void testModuleUnifierLevelAttributes()
+  {
+    class DummyOperator extends BaseOperator
+    {
+      int prop;
+
+      public transient DefaultInputPort<Integer> input = new DefaultInputPort<Integer>()
+      {
+        @Override
+        public void process(Integer tuple)
+        {
+          LOG.debug(tuple.intValue() + "" processed"");
+          output.emit(tuple);
+        }
+      };
+      public transient DefaultOutputPort<Integer> output = new DefaultOutputPort<Integer>();
+    }
+
+    class DummyOutputOperator extends BaseOperator
+    {
+      int prop;
+
+      public transient DefaultInputPort<Integer> input = new DefaultInputPort<Integer>()
+      {
+        @Override
+        public void process(Integer tuple)
+        {
+          LOG.debug(tuple.intValue() + "" processed"");
+        }
+      };
+    }
+
+    class TestUnifierAttributeModule implements Module
+    {
+      public transient ProxyInputPort<Integer> moduleInput = new ProxyInputPort<Integer>();
+      public transient ProxyOutputPort<Integer> moduleOutput = new Module.ProxyOutputPort<Integer>();
+
+      @Override
+      public void populateDAG(DAG dag, Configuration conf)
+      {
+        DummyOperator dummyOperator = dag.addOperator(""DummyOperator"", new DummyOperator());
+        dag.setOperatorAttribute(dummyOperator, Context.OperatorContext.PARTITIONER, new StatelessPartitioner<DummyOperator>(3));
+        dag.setUnifierAttribute(dummyOperator.output, OperatorContext.TIMEOUT_WINDOW_COUNT, 2);
+        moduleInput.set(dummyOperator.input);
+        moduleOutput.set(dummyOperator.output);
+      }
+    }
+
+    StreamingApplication app = new StreamingApplication()
+    {
+      @Override
+      public void populateDAG(DAG dag, Configuration conf)
+      {
+        Module m1 = dag.addModule(""TestModule"", new TestUnifierAttributeModule());
+        DummyOutputOperator dummyOutputOperator = dag.addOperator(""DummyOutputOperator"", new DummyOutputOperator());
+        dag.addStream(""Module To Operator"", ((TestUnifierAttributeModule)m1).moduleOutput, dummyOutputOperator.input);
+      }
+    };
+
+    String appName = ""UnifierApp"";
+    LogicalPlanConfiguration dagBuilder = new LogicalPlanConfiguration(new Configuration(false));
+    LogicalPlan dag = new LogicalPlan();
+    dag.setAttribute(Context.OperatorContext.STORAGE_AGENT, new MockStorageAgent());
+    dagBuilder.prepareDAG(dag, app, appName);
+    LogicalPlan.OperatorMeta ometa = dag.getOperatorMeta(""TestModule$DummyOperator"");
+    LogicalPlan.OperatorMeta om = null;
+    for (Map.Entry<LogicalPlan.OutputPortMeta, LogicalPlan.StreamMeta> entry : ometa.getOutputStreams().entrySet()) {","[{'comment': 'Why is entrySet() necessary? Only the key is used. Why not to assert inside the iteration?', 'commenter': 'vrozov'}, {'comment': 'Initially had used the value too to verify the id from StreamMeta. Hence entrySet was used. Changed it to use only keyset. Also assert is added inside the iteration.', 'commenter': 'deepak-narkhede'}]"
475,engine/src/main/java/com/datatorrent/stram/api/plugin/ApexPlugin.java,"@@ -0,0 +1,56 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.stram.api.plugin;","[{'comment': 'How about making it org.apache.apex.api', 'commenter': 'pramodin'}, {'comment': 'If we make it part of top level API then other internal structures from engine needs to be moved into this part. this could cause leaking implementation into api.\r\n\r\nwe could move it part of org.apache.stram.api inside stram.', 'commenter': 'tushargosavi'}]"
475,engine/src/main/java/com/datatorrent/stram/StreamingContainerManager.java,"@@ -799,6 +807,10 @@ public void monitorHeartbeat()
     processEvents();
 
     committedWindowId = updateCheckpoints(false);
+    if (lastCommittedWindowId != committedWindowId) {","[{'comment': ""There is a merge conflict here. Also is this being added so that multiple calls don't happen for the same committed window id."", 'commenter': 'pramodin'}, {'comment': ""yes, this is added so that multiple call don't happen for same committed windowId."", 'commenter': 'tushargosavi'}, {'comment': 'Can it happen that multiple checkpoints are committed as the latest checkpoint window?', 'commenter': 'bhupeshchawda'}]"
475,engine/src/main/java/com/datatorrent/stram/StreamingContainerManager.java,"@@ -2386,6 +2399,9 @@ public void deploy(Set<PTContainer> releaseContainers, Collection<PTOperator> un
   @Override
   public void recordEventAsync(StramEvent ev)
   {
+    if (apexPluginManager != null) {","[{'comment': 'Can the plugin manager listen to the event bus, get the event from the bus and use it? It might be able to get access to other types of events as well by listening to the bus.', 'commenter': 'pramodin'}, {'comment': 'Currently no. Plugin framework is itself a message bus. we could provide a hook to forward messages received on the bus to our plugin infrastructure. But currently only events are sent on the bus are events and they are already handled by plugin framework. In-fact the eventrecorder could be made as a plugin which needs this eventBus.', 'commenter': 'tushargosavi'}]"
475,engine/src/main/java/com/datatorrent/stram/api/plugin/PluginManager.java,"@@ -0,0 +1,52 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.stram.api.plugin;
+
+import org.apache.hadoop.classification.InterfaceStability;
+
+import com.datatorrent.stram.api.StramEvent;
+import com.datatorrent.stram.api.StreamingContainerUmbilicalProtocol;
+
+/**
+ * PluginManager provides and interface between Stram and ApexPlugin. It dispatches events from Stram
+ * to the interested @{link ApexPlugin}. It provides functionality for registration of callback handler
+ * for events.
+ *
+ * The actual dispatching is left to the implementation.
+ */
+@InterfaceStability.Evolving
+public interface PluginManager","[{'comment': 'Can we differentiate this from ApexPluginManager, which is not a PluginManager? Little confusing..', 'commenter': 'bhupeshchawda'}, {'comment': 'changed code structure a bit.', 'commenter': 'tushargosavi'}]"
475,engine/src/main/java/com/datatorrent/stram/plugin/ApexPluginManager.java,"@@ -0,0 +1,28 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.stram.plugin;
+
+import org.apache.hadoop.service.Service;
+
+import com.datatorrent.stram.api.plugin.PluginManager;
+
+public interface ApexPluginManager extends Service","[{'comment': 'ApexPluginDispatcher?', 'commenter': 'bhupeshchawda'}, {'comment': 'renamed the class accordingly', 'commenter': 'tushargosavi'}]"
475,engine/src/main/java/com/datatorrent/stram/plugin/AbstractApexPluginManager.java,"@@ -0,0 +1,190 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.stram.plugin;
+
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.net.URI;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FileContext;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.service.AbstractService;
+import org.apache.hadoop.yarn.conf.YarnConfiguration;
+
+import com.google.common.collect.Lists;
+
+import com.datatorrent.stram.StramAppContext;
+import com.datatorrent.stram.StreamingContainerManager;
+import com.datatorrent.stram.api.plugin.ApexPlugin;
+import com.datatorrent.stram.api.plugin.PluginContext;
+import com.datatorrent.stram.api.plugin.PluginLocator;
+import com.datatorrent.stram.api.plugin.PluginManager;
+import com.datatorrent.stram.plan.logical.LogicalPlan;
+
+/**
+ * A default implementation for ApexPluginManager. It handler common tasks such as per handler
+ * registration. actual dispatching is left for classes extending from it.
+ */
+public abstract class AbstractApexPluginManager extends AbstractService implements ApexPluginManager","[{'comment': 'Evolving?', 'commenter': 'bhupeshchawda'}, {'comment': 'This is internal implementation and not part of API.', 'commenter': 'tushargosavi'}]"
475,engine/src/main/java/com/datatorrent/stram/plugin/AbstractApexPluginManager.java,"@@ -0,0 +1,190 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.stram.plugin;
+
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.net.URI;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FileContext;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.service.AbstractService;
+import org.apache.hadoop.yarn.conf.YarnConfiguration;
+
+import com.google.common.collect.Lists;
+
+import com.datatorrent.stram.StramAppContext;
+import com.datatorrent.stram.StreamingContainerManager;
+import com.datatorrent.stram.api.plugin.ApexPlugin;
+import com.datatorrent.stram.api.plugin.PluginContext;
+import com.datatorrent.stram.api.plugin.PluginLocator;
+import com.datatorrent.stram.api.plugin.PluginManager;
+import com.datatorrent.stram.plan.logical.LogicalPlan;
+
+/**
+ * A default implementation for ApexPluginManager. It handler common tasks such as per handler
+ * registration. actual dispatching is left for classes extending from it.
+ */
+public abstract class AbstractApexPluginManager extends AbstractService implements ApexPluginManager
+{
+  private static final Logger LOG = LoggerFactory.getLogger(AbstractApexPluginManager.class);
+  protected final Collection<ApexPlugin> plugins = Lists.newArrayList();
+  protected final StramAppContext appContext;
+  protected final StreamingContainerManager dmgr;
+  private final PluginLocator locator;
+  protected transient Configuration launchConfig;
+  protected FileContext fileContext;
+  protected final Map<ApexPlugin, PluginInfo> pluginInfoMap = new HashMap<>();
+  protected PluginContext pluginContext;
+
+  public AbstractApexPluginManager(PluginLocator locator, StramAppContext context, StreamingContainerManager dmgr)
+  {
+    super(AbstractApexPluginManager.class.getName());
+    this.locator = locator;
+    this.appContext = context;
+    this.dmgr = dmgr;
+    LOG.info(""Creating appex service "");
+  }
+
+  private Configuration readLaunchConfiguration() throws IOException
+  {
+    try {
+      LOG.info(""Reading launch configuration file "");
+      Path appPath = new Path(appContext.getApplicationPath());
+      URI uri = appPath.toUri();
+      Configuration config = new YarnConfiguration();
+      fileContext = uri.getScheme() == null ? FileContext.getFileContext(config) : FileContext.getFileContext(uri, config);
+      FSDataInputStream is = fileContext.open(new Path(appPath, LogicalPlan.LAUNCH_CONFIG_FILE_NAME));
+      config.addResource(is);
+      LOG.info(""Read launch configuration"");
+      return config;
+    } catch (FileNotFoundException ex) {
+      return new Configuration();
+    }
+  }
+
+  @Override
+  protected void serviceInit(Configuration conf) throws Exception
+  {
+    super.serviceInit(conf);
+    this.launchConfig = readLaunchConfiguration();
+    pluginContext = new DefaultPluginContextImpl(appContext, dmgr, readLaunchConfiguration());
+    if (locator != null) {
+      Collection<ApexPlugin> plugins = locator.discoverPlugins();
+      if (plugins != null) {
+        this.plugins.addAll(plugins);
+      }
+    }
+
+    super.serviceStart();
+    for (ApexPlugin plugin : plugins) {
+      plugin.init(new PluginManagerImpl(plugin));
+    }
+  }
+
+  /**
+   * Keeps information about plugin and its registrations. Dispatcher use this
+   * information while delivering events to plugin.
+   */
+  class PluginInfo
+  {
+    final ApexPlugin plugin;
+    final RegistrationMap registrations = new RegistrationMap();
+    //final Map<PluginManager.RegistrationType<?>, List<PluginManager.Handler<?>>> registrations = new HashMap<>();
+    public PluginInfo(ApexPlugin plugin)
+    {
+      this.plugin = plugin;
+    }
+  }
+
+  PluginInfo getPluginInfo(ApexPlugin plugin)
+  {
+    PluginInfo pInfo = pluginInfoMap.get(plugin);
+    if (pInfo == null) {
+      pInfo = new PluginInfo(plugin);
+      pluginInfoMap.put(plugin, pInfo);
+    }
+    return pInfo;
+  }
+
+  public <T> void register(PluginManager.RegistrationType<T> type, PluginManager.Handler<T> handler, ApexPlugin owner)
+  {
+    PluginInfo pInfo = getPluginInfo(owner);
+    pInfo.registrations.put(type, handler);
+  }
+
+  /**
+   * A wrapper PluginManager to track registration from a plugin. with this plugin
+   * don't need to pass explicit owner argument during registration.
+   */
+  class PluginManagerImpl implements PluginManager
+  {
+    private final ApexPlugin owner;
+
+    PluginManagerImpl(ApexPlugin plugin)
+    {
+      this.owner = plugin;
+    }
+
+    @Override
+    public <T> void register(RegistrationType<T> type, Handler<T> handler)
+    {
+      AbstractApexPluginManager.this.register(type, handler, owner);
+    }
+
+    @Override
+    public PluginContext getPluginContext()
+    {
+      return AbstractApexPluginManager.this.pluginContext;
+    }
+  }
+
+  /**
+   * A Map which keep track for registration from ApexPlugins.
+   */
+  class RegistrationMap","[{'comment': 'Is this map needed?', 'commenter': 'bhupeshchawda'}, {'comment': 'removed this map. made this part of code inline', 'commenter': 'tushargosavi'}]"
475,engine/src/main/java/com/datatorrent/stram/StreamingContainerManager.java,"@@ -432,6 +439,7 @@ private StreamingContainerManager(CheckpointState checkpointedState, boolean ena
     init(enableEventRecording);
   }
 
+","[{'comment': 'extra line', 'commenter': 'bhupeshchawda'}, {'comment': 'removed extra line.', 'commenter': 'tushargosavi'}]"
475,engine/src/main/java/com/datatorrent/stram/plugin/AbstractApexPluginManager.java,"@@ -0,0 +1,190 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.stram.plugin;
+
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.net.URI;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FileContext;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.service.AbstractService;
+import org.apache.hadoop.yarn.conf.YarnConfiguration;
+
+import com.google.common.collect.Lists;
+
+import com.datatorrent.stram.StramAppContext;
+import com.datatorrent.stram.StreamingContainerManager;
+import com.datatorrent.stram.api.plugin.ApexPlugin;
+import com.datatorrent.stram.api.plugin.PluginContext;
+import com.datatorrent.stram.api.plugin.PluginLocator;
+import com.datatorrent.stram.api.plugin.PluginManager;
+import com.datatorrent.stram.plan.logical.LogicalPlan;
+
+/**
+ * A default implementation for ApexPluginManager. It handler common tasks such as per handler
+ * registration. actual dispatching is left for classes extending from it.
+ */
+public abstract class AbstractApexPluginManager extends AbstractService implements ApexPluginManager
+{
+  private static final Logger LOG = LoggerFactory.getLogger(AbstractApexPluginManager.class);
+  protected final Collection<ApexPlugin> plugins = Lists.newArrayList();
+  protected final StramAppContext appContext;
+  protected final StreamingContainerManager dmgr;
+  private final PluginLocator locator;
+  protected transient Configuration launchConfig;
+  protected FileContext fileContext;
+  protected final Map<ApexPlugin, PluginInfo> pluginInfoMap = new HashMap<>();
+  protected PluginContext pluginContext;
+
+  public AbstractApexPluginManager(PluginLocator locator, StramAppContext context, StreamingContainerManager dmgr)
+  {
+    super(AbstractApexPluginManager.class.getName());
+    this.locator = locator;
+    this.appContext = context;
+    this.dmgr = dmgr;
+    LOG.info(""Creating appex service "");
+  }
+
+  private Configuration readLaunchConfiguration() throws IOException
+  {
+    try {
+      LOG.info(""Reading launch configuration file "");
+      Path appPath = new Path(appContext.getApplicationPath());
+      URI uri = appPath.toUri();
+      Configuration config = new YarnConfiguration();
+      fileContext = uri.getScheme() == null ? FileContext.getFileContext(config) : FileContext.getFileContext(uri, config);
+      FSDataInputStream is = fileContext.open(new Path(appPath, LogicalPlan.LAUNCH_CONFIG_FILE_NAME));
+      config.addResource(is);
+      LOG.info(""Read launch configuration"");
+      return config;
+    } catch (FileNotFoundException ex) {
+      return new Configuration();
+    }
+  }
+
+  @Override
+  protected void serviceInit(Configuration conf) throws Exception
+  {
+    super.serviceInit(conf);
+    this.launchConfig = readLaunchConfiguration();
+    pluginContext = new DefaultPluginContextImpl(appContext, dmgr, readLaunchConfiguration());
+    if (locator != null) {
+      Collection<ApexPlugin> plugins = locator.discoverPlugins();
+      if (plugins != null) {
+        this.plugins.addAll(plugins);
+      }
+    }
+
+    super.serviceStart();
+    for (ApexPlugin plugin : plugins) {
+      plugin.init(new PluginManagerImpl(plugin));
+    }
+  }
+
+  /**
+   * Keeps information about plugin and its registrations. Dispatcher use this
+   * information while delivering events to plugin.
+   */
+  class PluginInfo
+  {
+    final ApexPlugin plugin;
+    final RegistrationMap registrations = new RegistrationMap();
+    //final Map<PluginManager.RegistrationType<?>, List<PluginManager.Handler<?>>> registrations = new HashMap<>();","[{'comment': 'Remove commented line', 'commenter': 'bhupeshchawda'}, {'comment': 'removed ', 'commenter': 'tushargosavi'}]"
475,engine/src/main/java/com/datatorrent/stram/plugin/AbstractApexPluginManager.java,"@@ -0,0 +1,190 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.stram.plugin;
+
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.net.URI;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FileContext;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.service.AbstractService;
+import org.apache.hadoop.yarn.conf.YarnConfiguration;
+
+import com.google.common.collect.Lists;
+
+import com.datatorrent.stram.StramAppContext;
+import com.datatorrent.stram.StreamingContainerManager;
+import com.datatorrent.stram.api.plugin.ApexPlugin;
+import com.datatorrent.stram.api.plugin.PluginContext;
+import com.datatorrent.stram.api.plugin.PluginLocator;
+import com.datatorrent.stram.api.plugin.PluginManager;
+import com.datatorrent.stram.plan.logical.LogicalPlan;
+
+/**
+ * A default implementation for ApexPluginManager. It handler common tasks such as per handler
+ * registration. actual dispatching is left for classes extending from it.
+ */
+public abstract class AbstractApexPluginManager extends AbstractService implements ApexPluginManager
+{
+  private static final Logger LOG = LoggerFactory.getLogger(AbstractApexPluginManager.class);
+  protected final Collection<ApexPlugin> plugins = Lists.newArrayList();
+  protected final StramAppContext appContext;
+  protected final StreamingContainerManager dmgr;
+  private final PluginLocator locator;
+  protected transient Configuration launchConfig;
+  protected FileContext fileContext;
+  protected final Map<ApexPlugin, PluginInfo> pluginInfoMap = new HashMap<>();
+  protected PluginContext pluginContext;
+
+  public AbstractApexPluginManager(PluginLocator locator, StramAppContext context, StreamingContainerManager dmgr)
+  {
+    super(AbstractApexPluginManager.class.getName());
+    this.locator = locator;
+    this.appContext = context;
+    this.dmgr = dmgr;
+    LOG.info(""Creating appex service "");","[{'comment': 'appex => apex', 'commenter': 'bhupeshchawda'}, {'comment': 'done', 'commenter': 'tushargosavi'}]"
475,engine/src/main/java/com/datatorrent/stram/plugin/AbstractApexPluginManager.java,"@@ -0,0 +1,190 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.stram.plugin;
+
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.net.URI;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FileContext;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.service.AbstractService;
+import org.apache.hadoop.yarn.conf.YarnConfiguration;
+
+import com.google.common.collect.Lists;
+
+import com.datatorrent.stram.StramAppContext;
+import com.datatorrent.stram.StreamingContainerManager;
+import com.datatorrent.stram.api.plugin.ApexPlugin;
+import com.datatorrent.stram.api.plugin.PluginContext;
+import com.datatorrent.stram.api.plugin.PluginLocator;
+import com.datatorrent.stram.api.plugin.PluginManager;
+import com.datatorrent.stram.plan.logical.LogicalPlan;
+
+/**
+ * A default implementation for ApexPluginManager. It handler common tasks such as per handler
+ * registration. actual dispatching is left for classes extending from it.
+ */
+public abstract class AbstractApexPluginManager extends AbstractService implements ApexPluginManager
+{
+  private static final Logger LOG = LoggerFactory.getLogger(AbstractApexPluginManager.class);
+  protected final Collection<ApexPlugin> plugins = Lists.newArrayList();
+  protected final StramAppContext appContext;
+  protected final StreamingContainerManager dmgr;
+  private final PluginLocator locator;
+  protected transient Configuration launchConfig;
+  protected FileContext fileContext;
+  protected final Map<ApexPlugin, PluginInfo> pluginInfoMap = new HashMap<>();
+  protected PluginContext pluginContext;
+
+  public AbstractApexPluginManager(PluginLocator locator, StramAppContext context, StreamingContainerManager dmgr)
+  {
+    super(AbstractApexPluginManager.class.getName());
+    this.locator = locator;
+    this.appContext = context;
+    this.dmgr = dmgr;
+    LOG.info(""Creating appex service "");
+  }
+
+  private Configuration readLaunchConfiguration() throws IOException
+  {
+    try {
+      LOG.info(""Reading launch configuration file "");
+      Path appPath = new Path(appContext.getApplicationPath());
+      URI uri = appPath.toUri();
+      Configuration config = new YarnConfiguration();
+      fileContext = uri.getScheme() == null ? FileContext.getFileContext(config) : FileContext.getFileContext(uri, config);
+      FSDataInputStream is = fileContext.open(new Path(appPath, LogicalPlan.LAUNCH_CONFIG_FILE_NAME));
+      config.addResource(is);
+      LOG.info(""Read launch configuration"");
+      return config;
+    } catch (FileNotFoundException ex) {
+      return new Configuration();","[{'comment': 'LOG.warn(""Config not found"") ?', 'commenter': 'bhupeshchawda'}, {'comment': 'added the log message', 'commenter': 'tushargosavi'}]"
475,engine/src/main/java/com/datatorrent/stram/plugin/AbstractApexPluginManager.java,"@@ -0,0 +1,190 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.stram.plugin;
+
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.net.URI;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FileContext;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.service.AbstractService;
+import org.apache.hadoop.yarn.conf.YarnConfiguration;
+
+import com.google.common.collect.Lists;
+
+import com.datatorrent.stram.StramAppContext;
+import com.datatorrent.stram.StreamingContainerManager;
+import com.datatorrent.stram.api.plugin.ApexPlugin;
+import com.datatorrent.stram.api.plugin.PluginContext;
+import com.datatorrent.stram.api.plugin.PluginLocator;
+import com.datatorrent.stram.api.plugin.PluginManager;
+import com.datatorrent.stram.plan.logical.LogicalPlan;
+
+/**
+ * A default implementation for ApexPluginManager. It handler common tasks such as per handler
+ * registration. actual dispatching is left for classes extending from it.
+ */
+public abstract class AbstractApexPluginManager extends AbstractService implements ApexPluginManager
+{
+  private static final Logger LOG = LoggerFactory.getLogger(AbstractApexPluginManager.class);
+  protected final Collection<ApexPlugin> plugins = Lists.newArrayList();
+  protected final StramAppContext appContext;
+  protected final StreamingContainerManager dmgr;
+  private final PluginLocator locator;
+  protected transient Configuration launchConfig;
+  protected FileContext fileContext;
+  protected final Map<ApexPlugin, PluginInfo> pluginInfoMap = new HashMap<>();
+  protected PluginContext pluginContext;
+
+  public AbstractApexPluginManager(PluginLocator locator, StramAppContext context, StreamingContainerManager dmgr)
+  {
+    super(AbstractApexPluginManager.class.getName());
+    this.locator = locator;
+    this.appContext = context;
+    this.dmgr = dmgr;
+    LOG.info(""Creating appex service "");
+  }
+
+  private Configuration readLaunchConfiguration() throws IOException
+  {
+    try {
+      LOG.info(""Reading launch configuration file "");
+      Path appPath = new Path(appContext.getApplicationPath());
+      URI uri = appPath.toUri();
+      Configuration config = new YarnConfiguration();
+      fileContext = uri.getScheme() == null ? FileContext.getFileContext(config) : FileContext.getFileContext(uri, config);
+      FSDataInputStream is = fileContext.open(new Path(appPath, LogicalPlan.LAUNCH_CONFIG_FILE_NAME));
+      config.addResource(is);
+      LOG.info(""Read launch configuration"");
+      return config;
+    } catch (FileNotFoundException ex) {
+      return new Configuration();
+    }
+  }
+
+  @Override
+  protected void serviceInit(Configuration conf) throws Exception
+  {
+    super.serviceInit(conf);
+    this.launchConfig = readLaunchConfiguration();
+    pluginContext = new DefaultPluginContextImpl(appContext, dmgr, readLaunchConfiguration());","[{'comment': 'readLaunchConfiguration() called twice... use this.launchConfig?', 'commenter': 'bhupeshchawda'}, {'comment': 'good catch, removed an extra call.', 'commenter': 'tushargosavi'}]"
475,engine/src/test/resources/META-INF/services/com.datatorrent.stram.api.plugin.ApexPlugin,"@@ -0,0 +1,19 @@
+##
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# ""License""); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+#
+com.datatorrent.stram.plugin.DebugPlugin","[{'comment': 'New line', 'commenter': 'bhupeshchawda'}, {'comment': 'added new line', 'commenter': 'tushargosavi'}]"
475,engine/src/main/java/com/datatorrent/stram/StreamingAppMasterService.java,"@@ -575,10 +581,20 @@ protected void serviceInit(Configuration conf) throws Exception
       this.appDataPushAgent = new AppDataPushAgent(dnmgr, appContext);
       addService(this.appDataPushAgent);
     }
-    // initialize all services added above
+    initApexPluginDispatcher();
+
+    // Initialize all services added above
     super.serviceInit(conf);
   }
 
+  private void initApexPluginDispatcher()
+  {
+    PluginLocator locator = new ServiceLoaderBasedPluginLocator();","[{'comment': 'How about making this configurable? A property can be used to specify the locator to use, default could be service based. But another option could be property based that specifies list of plugin classes, see comment below on replacing static with property based locator. This sub-framework can be used by dag side as well.', 'commenter': 'pramodin'}]"
475,engine/src/main/java/org/apache/apex/engine/plugin/AbstractApexPluginDispatcher.java,"@@ -0,0 +1,189 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.apex.engine.plugin;
+
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.net.URI;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import org.apache.apex.stram.api.plugin.ApexPlugin;
+import org.apache.apex.stram.api.plugin.PluginContext;
+import org.apache.apex.stram.api.plugin.PluginLocator;
+import org.apache.apex.stram.api.plugin.PluginManager;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FileContext;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.service.AbstractService;
+import org.apache.hadoop.yarn.conf.YarnConfiguration;
+
+import com.google.common.collect.Lists;
+
+import com.datatorrent.stram.StramAppContext;
+import com.datatorrent.stram.StreamingContainerManager;
+import com.datatorrent.stram.plan.logical.LogicalPlan;
+
+/**
+ * A default implementation for ApexPluginDispatcher. It handler common tasks such as per handler
+ * registration. actual dispatching is left for classes extending from it.
+ */
+public abstract class AbstractApexPluginDispatcher extends AbstractService implements ApexPluginDispatcher
+{
+  private static final Logger LOG = LoggerFactory.getLogger(AbstractApexPluginDispatcher.class);
+  protected final Collection<ApexPlugin> plugins = Lists.newArrayList();
+  protected final StramAppContext appContext;
+  protected final StreamingContainerManager dmgr;
+  private final PluginLocator locator;
+  protected transient Configuration launchConfig;
+  protected FileContext fileContext;
+  protected final Map<ApexPlugin, PluginInfo> pluginInfoMap = new HashMap<>();
+  protected PluginContext pluginContext;
+
+  public AbstractApexPluginDispatcher(PluginLocator locator, StramAppContext context, StreamingContainerManager dmgr)
+  {
+    super(AbstractApexPluginDispatcher.class.getName());
+    this.locator = locator;
+    this.appContext = context;
+    this.dmgr = dmgr;
+    LOG.info(""Creating apex service "");","[{'comment': 'debug level', 'commenter': 'pramodin'}]"
475,engine/src/main/java/org/apache/apex/engine/plugin/AbstractApexPluginDispatcher.java,"@@ -0,0 +1,189 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.apex.engine.plugin;
+
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.net.URI;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import org.apache.apex.stram.api.plugin.ApexPlugin;
+import org.apache.apex.stram.api.plugin.PluginContext;
+import org.apache.apex.stram.api.plugin.PluginLocator;
+import org.apache.apex.stram.api.plugin.PluginManager;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FileContext;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.service.AbstractService;
+import org.apache.hadoop.yarn.conf.YarnConfiguration;
+
+import com.google.common.collect.Lists;
+
+import com.datatorrent.stram.StramAppContext;
+import com.datatorrent.stram.StreamingContainerManager;
+import com.datatorrent.stram.plan.logical.LogicalPlan;
+
+/**
+ * A default implementation for ApexPluginDispatcher. It handler common tasks such as per handler
+ * registration. actual dispatching is left for classes extending from it.
+ */
+public abstract class AbstractApexPluginDispatcher extends AbstractService implements ApexPluginDispatcher
+{
+  private static final Logger LOG = LoggerFactory.getLogger(AbstractApexPluginDispatcher.class);
+  protected final Collection<ApexPlugin> plugins = Lists.newArrayList();
+  protected final StramAppContext appContext;
+  protected final StreamingContainerManager dmgr;
+  private final PluginLocator locator;
+  protected transient Configuration launchConfig;
+  protected FileContext fileContext;
+  protected final Map<ApexPlugin, PluginInfo> pluginInfoMap = new HashMap<>();
+  protected PluginContext pluginContext;
+
+  public AbstractApexPluginDispatcher(PluginLocator locator, StramAppContext context, StreamingContainerManager dmgr)
+  {
+    super(AbstractApexPluginDispatcher.class.getName());
+    this.locator = locator;
+    this.appContext = context;
+    this.dmgr = dmgr;
+    LOG.info(""Creating apex service "");
+  }
+
+  private Configuration readLaunchConfiguration() throws IOException
+  {
+    Path appPath = new Path(appContext.getApplicationPath());
+    Path  configFilePath = new Path(appPath, LogicalPlan.LAUNCH_CONFIG_FILE_NAME);
+    try {
+      LOG.info(""Reading launch configuration file "");","[{'comment': 'debug level', 'commenter': 'pramodin'}]"
476,engine/src/main/java/com/datatorrent/stram/plan/physical/PhysicalPlan.java,"@@ -1892,4 +1947,22 @@ public Integer getStreamCodecIdentifier(StreamCodec<?> streamCodecInfo)
       return null;
     }
   }
+
+  /**
+   * A interface object for the statsListener to access the DAG, it provides methods","[{'comment': 'An interface for the `{@link StatsListener}` ... Please fix spelling and formatting of other Javadoc comments as well.', 'commenter': 'tweise'}]"
476,engine/src/main/java/com/datatorrent/stram/plan/physical/PhysicalPlan.java,"@@ -205,6 +211,53 @@ public Response processStats(BatchedOperatorStats stats)
     {
       return ((StatsListener)om.getOperator()).processStats(stats);
     }
+
+    @Override
+    public Response processStats(BatchedOperatorStats stats, StatsListenerContext context)
+    {
+      StatsListener listener = (StatsListener)om.getOperator();
+      if (listener instanceof StatsListenerWithContext) {
+        return ((StatsListenerWithContext)listener).processStats(stats, context);
+      } else {
+        return processStats(stats);
+      }
+    }
+  }
+
+  /**
+   * Adapter for handling deprecated StatsListener. This implementation calls {@see StatsListener.processStats(BatchedOperatorStats)}","[{'comment': 'The correct formatting would be `{@link StatsListener#processStats(BatchedOperatorStats)}`', 'commenter': 'tweise'}, {'comment': 'will do it', 'commenter': 'tushargosavi'}]"
476,engine/src/test/java/com/datatorrent/stram/plan/physical/PhysicalPlanTest.java,"@@ -2246,4 +2253,97 @@ public void testParallelPartitionForSlidingWindow()
     PhysicalPlan plan = new PhysicalPlan(dag, new TestPlanContext());
     Assert.assertEquals(""number of containers"", 7, plan.getContainers().size());
   }
+
+  static class NoOpStatsListener implements StatsListener","[{'comment': 'could this be done with mockito?', 'commenter': 'tweise'}, {'comment': 'I will check with Mockito.', 'commenter': 'tushargosavi'}, {'comment': 'I have use Mockito to mock the interface in the tests, and updated the pull request ', 'commenter': 'tushargosavi'}]"
478,engine/src/test/java/com/datatorrent/stram/plan/logical/LogicalPlanConfigurationTest.java,"@@ -614,44 +614,27 @@ public void testModuleUnifierLevelAttributes()
   {","[{'comment': 'Why is @SuppressWarnings({""UnnecessaryBoxing"", ""AssertEqualsBetweenInconvertibleTypes""}) necessary?', 'commenter': 'vrozov'}, {'comment': 'Done.', 'commenter': 'deepak-narkhede'}]"
478,engine/src/test/java/com/datatorrent/stram/plan/logical/LogicalPlanConfigurationTest.java,"@@ -614,44 +614,27 @@ public void testModuleUnifierLevelAttributes()
   {
     class DummyOperator extends BaseOperator
     {
-      int prop;
-
       public transient DefaultInputPort<Integer> input = new DefaultInputPort<Integer>()
       {
         @Override
         public void process(Integer tuple)
         {
-          LOG.debug(tuple.intValue() + "" processed"");
           output.emit(tuple);
         }
       };
-      public transient DefaultOutputPort<Integer> output = new DefaultOutputPort<Integer>();
-    }
-
-    class DummyOutputOperator extends BaseOperator
-    {
-      int prop;
-
-      public transient DefaultInputPort<Integer> input = new DefaultInputPort<Integer>()
-      {
-        @Override
-        public void process(Integer tuple)
-        {
-          LOG.debug(tuple.intValue() + "" processed"");
-        }
-      };
+      public transient DefaultOutputPort<Integer> output = new DefaultOutputPort<>();
     }
 
     class TestUnifierAttributeModule implements Module
     {
-      public transient ProxyInputPort<Integer> moduleInput = new ProxyInputPort<Integer>();
-      public transient ProxyOutputPort<Integer> moduleOutput = new Module.ProxyOutputPort<Integer>();
+      public transient ProxyInputPort<Integer> moduleInput = new ProxyInputPort<>();
+      public transient ProxyOutputPort<Integer> moduleOutput = new Module.ProxyOutputPort<>();","[{'comment': 'Why is Module required here?', 'commenter': 'vrozov'}, {'comment': 'Done.', 'commenter': 'deepak-narkhede'}]"
478,engine/src/test/java/com/datatorrent/stram/plan/logical/LogicalPlanConfigurationTest.java,"@@ -676,18 +659,22 @@ public void populateDAG(DAG dag, Configuration conf)
     dagBuilder.prepareDAG(dag, app, appName);
     LogicalPlan.OperatorMeta ometa = dag.getOperatorMeta(""TestModule$DummyOperator"");","[{'comment': 'Why is LogicalPlan required? OperatorMeta is already imported.', 'commenter': 'vrozov'}, {'comment': 'Done.', 'commenter': 'deepak-narkhede'}]"
478,engine/src/test/java/com/datatorrent/stram/plan/logical/LogicalPlanConfigurationTest.java,"@@ -676,18 +659,22 @@ public void populateDAG(DAG dag, Configuration conf)
     dagBuilder.prepareDAG(dag, app, appName);","[{'comment': 'Why is Context necessary? Please be consistent and try to shorten the code. Change MockStorageAgent to Mockito.mock(StorageAgent.class).', 'commenter': 'vrozov'}, {'comment': 'Done.', 'commenter': 'deepak-narkhede'}]"
478,engine/src/test/java/com/datatorrent/stram/plan/logical/LogicalPlanConfigurationTest.java,"@@ -676,18 +659,22 @@ public void populateDAG(DAG dag, Configuration conf)
     dagBuilder.prepareDAG(dag, app, appName);
     LogicalPlan.OperatorMeta ometa = dag.getOperatorMeta(""TestModule$DummyOperator"");
     LogicalPlan.OperatorMeta om = null;","[{'comment': 'Move declaration inside the if statement inside the loop.', 'commenter': 'vrozov'}, {'comment': 'Done.', 'commenter': 'deepak-narkhede'}]"
478,engine/src/test/java/com/datatorrent/stram/plan/logical/LogicalPlanConfigurationTest.java,"@@ -676,18 +659,22 @@ public void populateDAG(DAG dag, Configuration conf)
     dagBuilder.prepareDAG(dag, app, appName);
     LogicalPlan.OperatorMeta ometa = dag.getOperatorMeta(""TestModule$DummyOperator"");
     LogicalPlan.OperatorMeta om = null;
-    for (Map.Entry<LogicalPlan.OutputPortMeta, LogicalPlan.StreamMeta> entry : ometa.getOutputStreams().entrySet()) {
-      if (entry.getKey().getPortName().equals(""output"")) {
-        om = entry.getKey().getUnifierMeta();
-      }
-    }
 
     /*
      * Verify the attribute value after preparing DAG.
      */
-    Assert.assertNotNull(om);
-    Assert.assertEquals("""", Integer.valueOf(2), om.getValue(Context.OperatorContext.TIMEOUT_WINDOW_COUNT));
+    for (OutputPortMeta portMeta : ometa.getOutputStreams().keySet()) {
+      if (portMeta.getPortName().equals(""output"")) {
+        om = portMeta.getUnifierMeta();
+        Assert.assertNotNull(om);
+        Assert.assertEquals(""Unifier attribute value incorrect after logical plan creation."",
+            Integer.valueOf(2), om.getValue(Context.OperatorContext.TIMEOUT_WINDOW_COUNT));","[{'comment': 'Why is boxing necessary?', 'commenter': 'vrozov'}, {'comment': 'As attribute was Integer object so used (object, object), hence boxing is required for integer value 2. Also preferred over (long, long ) avoiding casting. I thought autoboxing would work but IDE was throws errors. Hence boxing was used.', 'commenter': 'deepak-narkhede'}, {'comment': 'Why is boxing preferable over casting? Please avoid boxing.', 'commenter': 'vrozov'}]"
478,engine/src/test/java/com/datatorrent/stram/plan/logical/LogicalPlanConfigurationTest.java,"@@ -676,18 +659,22 @@ public void populateDAG(DAG dag, Configuration conf)
     dagBuilder.prepareDAG(dag, app, appName);
     LogicalPlan.OperatorMeta ometa = dag.getOperatorMeta(""TestModule$DummyOperator"");
     LogicalPlan.OperatorMeta om = null;
-    for (Map.Entry<LogicalPlan.OutputPortMeta, LogicalPlan.StreamMeta> entry : ometa.getOutputStreams().entrySet()) {
-      if (entry.getKey().getPortName().equals(""output"")) {
-        om = entry.getKey().getUnifierMeta();
-      }
-    }
 
     /*
      * Verify the attribute value after preparing DAG.
      */
-    Assert.assertNotNull(om);
-    Assert.assertEquals("""", Integer.valueOf(2), om.getValue(Context.OperatorContext.TIMEOUT_WINDOW_COUNT));
+    for (OutputPortMeta portMeta : ometa.getOutputStreams().keySet()) {
+      if (portMeta.getPortName().equals(""output"")) {
+        om = portMeta.getUnifierMeta();
+        Assert.assertNotNull(om);","[{'comment': 'Why is assertNotNull necessary? om will be used in the next statement and test will fail if it is null anyway.', 'commenter': 'vrozov'}, {'comment': 'It was added to have precise check if unifierMeta is null. I agree it would be detected on next statement but with extra stack of null pointer exceptions rather than intended check is for unifier count. Removed it.', 'commenter': 'deepak-narkhede'}]"
483,engine/src/main/java/com/datatorrent/stram/StreamingAppMasterService.java,"@@ -1068,12 +1071,40 @@ private void finishApplication(FinalApplicationStatus finalStatus) throws YarnEx
    * Check for containers that were allocated in a previous attempt.
    * If the containers are still alive, wait for them to check in via heartbeat.
    */
-  private void previouslyAllocatedContainers(List<Container> containers)
+  private void previouslyAllocatedContainers(List<Container> containersListByYarn, Collection<ContainerId> releasedContainers)
   {
-    for (Container container : containers) {
-      this.allocatedContainers.put(container.getId().toString(), new AllocatedContainer(container));
-      //check the status
-      nmClient.getContainerStatusAsync(container.getId(), container.getNodeId());
+    if (containersListByYarn.size() != 0) {
+      Map<String, Container> fromYarn = new HashMap<>();
+      Set<String> fromStreamingContainerManager = new HashSet<>();
+
+      for (Container container : containersListByYarn) {
+        fromYarn.put(container.getId().toString(), container);
+      }
+
+      for (PTContainer ptContainer : dnmgr.getPhysicalPlan().getContainers()) {
+        fromStreamingContainerManager.add(ptContainer.getExternalId());
+      }
+
+      for (PTContainer ptContainer : dnmgr.getPhysicalPlan().getContainers()) {
+        String containerId = ptContainer.getExternalId();
+        if (fromYarn.containsKey(containerId)) {","[{'comment': 'it is a double get().', 'commenter': 'vrozov'}, {'comment': 'Fixed.', 'commenter': 'sandeshh'}]"
483,engine/src/main/java/com/datatorrent/stram/StreamingAppMasterService.java,"@@ -1068,12 +1071,40 @@ private void finishApplication(FinalApplicationStatus finalStatus) throws YarnEx
    * Check for containers that were allocated in a previous attempt.
    * If the containers are still alive, wait for them to check in via heartbeat.
    */
-  private void previouslyAllocatedContainers(List<Container> containers)
+  private void previouslyAllocatedContainers(List<Container> containersListByYarn, Collection<ContainerId> releasedContainers)
   {
-    for (Container container : containers) {
-      this.allocatedContainers.put(container.getId().toString(), new AllocatedContainer(container));
-      //check the status
-      nmClient.getContainerStatusAsync(container.getId(), container.getNodeId());
+    if (containersListByYarn.size() != 0) {
+      Map<String, Container> fromYarn = new HashMap<>();
+      Set<String> fromStreamingContainerManager = new HashSet<>();
+
+      for (Container container : containersListByYarn) {
+        fromYarn.put(container.getId().toString(), container);
+      }
+
+      for (PTContainer ptContainer : dnmgr.getPhysicalPlan().getContainers()) {
+        fromStreamingContainerManager.add(ptContainer.getExternalId());
+      }
+
+      for (PTContainer ptContainer : dnmgr.getPhysicalPlan().getContainers()) {
+        String containerId = ptContainer.getExternalId();
+        Container container;","[{'comment': 'Move assignment to the declaration.', 'commenter': 'vrozov'}, {'comment': 'Done.', 'commenter': 'sandeshh'}]"
483,engine/src/main/java/com/datatorrent/stram/StreamingAppMasterService.java,"@@ -1068,12 +1071,42 @@ private void finishApplication(FinalApplicationStatus finalStatus) throws YarnEx
    * Check for containers that were allocated in a previous attempt.
    * If the containers are still alive, wait for them to check in via heartbeat.
    */
-  private void previouslyAllocatedContainers(List<Container> containers)
+  private void previouslyAllocatedContainers(List<Container> containersListByYarn, Collection<ContainerId> releasedContainers)
   {
-    for (Container container : containers) {
-      this.allocatedContainers.put(container.getId().toString(), new AllocatedContainer(container));
-      //check the status
-      nmClient.getContainerStatusAsync(container.getId(), container.getNodeId());
+    if (containersListByYarn.size() != 0) {
+      Map<String, Container> fromYarn = new HashMap<>();
+      Set<String> fromStreamingContainerManager = new HashSet<>();
+
+      for (Container container : containersListByYarn) {
+        fromYarn.put(container.getId().toString(), container);
+      }
+
+      for (PTContainer ptContainer : dnmgr.getPhysicalPlan().getContainers()) {
+        fromStreamingContainerManager.add(ptContainer.getExternalId());
+      }
+
+      for (PTContainer ptContainer : dnmgr.getPhysicalPlan().getContainers()) {
+
+        String containerId = ptContainer.getExternalId();
+        Container container = fromYarn.get(containerId);
+
+        if (container != null) {
+          allocatedContainers.put(containerId, new AllocatedContainer(container));
+          nmClient.getContainerStatusAsync(container.getId(), container.getNodeId());","[{'comment': 'The container is in the list that Yarn provided. Is it still necessary to request the status?', 'commenter': 'vrozov'}, {'comment': 'Not strictly necessary, added as an extra check.', 'commenter': 'sandeshh'}]"
483,engine/src/main/java/com/datatorrent/stram/StreamingAppMasterService.java,"@@ -1068,12 +1071,48 @@ private void finishApplication(FinalApplicationStatus finalStatus) throws YarnEx
    * Check for containers that were allocated in a previous attempt.
    * If the containers are still alive, wait for them to check in via heartbeat.
    */
-  private void previouslyAllocatedContainers(List<Container> containers)
+  private void previouslyAllocatedContainers(List<Container> containersListByYarn, Collection<ContainerId> releasedContainers)
   {
-    for (Container container : containers) {
-      this.allocatedContainers.put(container.getId().toString(), new AllocatedContainer(container));
-      //check the status
-      nmClient.getContainerStatusAsync(container.getId(), container.getNodeId());
+    LOG.info(""Finding information about the previously allocated containers."");","[{'comment': 'debug?', 'commenter': 'vrozov'}, {'comment': 'All these logging can be at Info level, as they are one time for life of AppMaster', 'commenter': 'sandeshh'}, {'comment': 'What info does it provide?', 'commenter': 'vrozov'}]"
483,engine/src/main/java/com/datatorrent/stram/StreamingAppMasterService.java,"@@ -1068,12 +1071,48 @@ private void finishApplication(FinalApplicationStatus finalStatus) throws YarnEx
    * Check for containers that were allocated in a previous attempt.
    * If the containers are still alive, wait for them to check in via heartbeat.
    */
-  private void previouslyAllocatedContainers(List<Container> containers)
+  private void previouslyAllocatedContainers(List<Container> containersListByYarn, Collection<ContainerId> releasedContainers)
   {
-    for (Container container : containers) {
-      this.allocatedContainers.put(container.getId().toString(), new AllocatedContainer(container));
-      //check the status
-      nmClient.getContainerStatusAsync(container.getId(), container.getNodeId());
+    LOG.info(""Finding information about the previously allocated containers."");
+
+    if (containersListByYarn.size() != 0) {
+
+      LOG.info(""YARN has the list of {} running containers."", containersListByYarn.size());","[{'comment': 'Why two log entries? IMO, none of them are necessary at least at info level. Why not to log all containers in debug?', 'commenter': 'vrozov'}, {'comment': 'Done.', 'commenter': 'sandeshh'}]"
483,engine/src/main/java/com/datatorrent/stram/StreamingAppMasterService.java,"@@ -1068,12 +1071,48 @@ private void finishApplication(FinalApplicationStatus finalStatus) throws YarnEx
    * Check for containers that were allocated in a previous attempt.
    * If the containers are still alive, wait for them to check in via heartbeat.
    */
-  private void previouslyAllocatedContainers(List<Container> containers)
+  private void previouslyAllocatedContainers(List<Container> containersListByYarn, Collection<ContainerId> releasedContainers)
   {
-    for (Container container : containers) {
-      this.allocatedContainers.put(container.getId().toString(), new AllocatedContainer(container));
-      //check the status
-      nmClient.getContainerStatusAsync(container.getId(), container.getNodeId());
+    LOG.info(""Finding information about the previously allocated containers."");
+
+    if (containersListByYarn.size() != 0) {
+
+      LOG.info(""YARN has the list of {} running containers."", containersListByYarn.size());
+      LOG.info(""Number of containers from StreamingContainerManager is {}"", dnmgr.getPhysicalPlan().getContainers().size());
+
+      Map<String, Container> fromYarn = new HashMap<>();
+      Set<String> fromStreamingContainerManager = new HashSet<>();
+
+      for (Container container : containersListByYarn) {
+        fromYarn.put(container.getId().toString(), container);
+      }
+
+      for (PTContainer ptContainer : dnmgr.getPhysicalPlan().getContainers()) {
+        fromStreamingContainerManager.add(ptContainer.getExternalId());
+      }
+
+      for (PTContainer ptContainer : dnmgr.getPhysicalPlan().getContainers()) {
+
+        String containerId = ptContainer.getExternalId();
+        Container container = fromYarn.get(containerId);
+
+        if (container != null) {
+          allocatedContainers.put(containerId, new AllocatedContainer(container));
+          fromYarn.remove(containerId);
+          fromStreamingContainerManager.remove(containerId);
+        }
+      }
+
+      for (Container container : fromYarn.values()) {
+        releasedContainers.add(container.getId());","[{'comment': 'IMO, this is what needs to be logged at INFO level. There is Yarn container allocated that does not have a corresponding container in the physical plan.', 'commenter': 'vrozov'}, {'comment': 'Done', 'commenter': 'sandeshh'}]"
483,engine/src/main/java/com/datatorrent/stram/StreamingAppMasterService.java,"@@ -1068,12 +1071,48 @@ private void finishApplication(FinalApplicationStatus finalStatus) throws YarnEx
    * Check for containers that were allocated in a previous attempt.
    * If the containers are still alive, wait for them to check in via heartbeat.
    */
-  private void previouslyAllocatedContainers(List<Container> containers)
+  private void previouslyAllocatedContainers(List<Container> containersListByYarn, Collection<ContainerId> releasedContainers)
   {
-    for (Container container : containers) {
-      this.allocatedContainers.put(container.getId().toString(), new AllocatedContainer(container));
-      //check the status
-      nmClient.getContainerStatusAsync(container.getId(), container.getNodeId());
+    LOG.info(""Finding information about the previously allocated containers."");
+
+    if (containersListByYarn.size() != 0) {
+
+      LOG.info(""YARN has the list of {} running containers."", containersListByYarn.size());
+      LOG.info(""Number of containers from StreamingContainerManager is {}"", dnmgr.getPhysicalPlan().getContainers().size());
+
+      Map<String, Container> fromYarn = new HashMap<>();
+      Set<String> fromStreamingContainerManager = new HashSet<>();
+
+      for (Container container : containersListByYarn) {
+        fromYarn.put(container.getId().toString(), container);
+      }
+
+      for (PTContainer ptContainer : dnmgr.getPhysicalPlan().getContainers()) {
+        fromStreamingContainerManager.add(ptContainer.getExternalId());
+      }
+
+      for (PTContainer ptContainer : dnmgr.getPhysicalPlan().getContainers()) {
+
+        String containerId = ptContainer.getExternalId();
+        Container container = fromYarn.get(containerId);
+
+        if (container != null) {
+          allocatedContainers.put(containerId, new AllocatedContainer(container));
+          fromYarn.remove(containerId);
+          fromStreamingContainerManager.remove(containerId);
+        }
+      }
+
+      for (Container container : fromYarn.values()) {
+        releasedContainers.add(container.getId());
+      }
+
+      for (String containerId : fromStreamingContainerManager) {
+        dnmgr.scheduleContainerRestart(containerId);","[{'comment': 'Another candidate for INFO level logging.', 'commenter': 'vrozov'}, {'comment': 'Done.', 'commenter': 'sandeshh'}]"
483,engine/src/main/java/com/datatorrent/stram/StreamingContainerManager.java,"@@ -2294,6 +2294,28 @@ private void requestContainer(PTContainer c)
     }
   }
 
+  public void deployAfterRestart()
+  {
+    if (restarted) {
+      LOG.info(""App will be deployed from the previous checkpoint."");","[{'comment': '""Redeploying application from the previous checkpoint""? Can you provide more metadata regarding the checkpoint, for example a timestamp?', 'commenter': 'vrozov'}, {'comment': 'Removing the log, as most of the information will be logged anyways, there is nothing new here.', 'commenter': 'sandeshh'}]"
483,engine/src/main/java/com/datatorrent/stram/StreamingAppMasterService.java,"@@ -1068,12 +1071,54 @@ private void finishApplication(FinalApplicationStatus finalStatus) throws YarnEx
    * Check for containers that were allocated in a previous attempt.
    * If the containers are still alive, wait for them to check in via heartbeat.
    */
-  private void previouslyAllocatedContainers(List<Container> containers)
+  private void previouslyAllocatedContainers(List<Container> containersListByYarn, Collection<ContainerId> releasedContainers)
   {
-    for (Container container : containers) {
-      this.allocatedContainers.put(container.getId().toString(), new AllocatedContainer(container));
-      //check the status
-      nmClient.getContainerStatusAsync(container.getId(), container.getNodeId());
+    LOG.info(""Finding information about the previously allocated containers."");
+
+    if (containersListByYarn.size() != 0) {
+      
+      Map<String, Container> fromYarn = new HashMap<>();
+      Set<String> fromStreamingContainerManager = new HashSet<>();
+
+      LOG.debug(""Containers list by YARN - "");","[{'comment': 'Why not to log all containers here?', 'commenter': 'vrozov'}, {'comment': 'Done.', 'commenter': 'sandeshh'}]"
483,engine/src/main/java/com/datatorrent/stram/StreamingAppMasterService.java,"@@ -1089,13 +1090,57 @@ private void finishApplication(FinalApplicationStatus finalStatus) throws YarnEx
    * Check for containers that were allocated in a previous attempt.
    * If the containers are still alive, wait for them to check in via heartbeat.
    */
-  private void previouslyAllocatedContainers(List<Container> containers)
+  private List<ContainerId> previouslyAllocatedContainers(List<Container> containersListByYarn)
   {
-    for (Container container : containers) {
-      this.allocatedContainers.put(container.getId().toString(), new AllocatedContainer(container));
-      //check the status
-      nmClient.getContainerStatusAsync(container.getId(), container.getNodeId());
+    List<ContainerId> containersToRelease = new ArrayList<>();
+
+    if (containersListByYarn.size() != 0) {
+
+      Map<String, Container> fromYarn = new HashMap<>();
+      Set<String> fromStreamingContainerManager = new HashSet<>();
+
+      LOG.debug(""Containers list by YARN - {}"", containersListByYarn);
+      LOG.debug(""Containers list by Streaming Container Manger - {}"", dnmgr.getPhysicalPlan().getContainers());
+
+      for (Container container : containersListByYarn) {
+        fromYarn.put(container.getId().toString(), container);
+      }
+
+      for (PTContainer ptContainer : dnmgr.getPhysicalPlan().getContainers()) {
+        // SCM starts the container without external ID.
+        if (ptContainer.getExternalId() != null) {
+          fromStreamingContainerManager.add(ptContainer.getExternalId());
+        }
+      }
+
+      for (PTContainer ptContainer : dnmgr.getPhysicalPlan().getContainers()) {
+
+        String containerId = ptContainer.getExternalId();","[{'comment': 'It is possible that containerId is `null` here as well. It will be better to handle containers with ExternalId `null` in this loop only.', 'commenter': 'vrozov'}, {'comment': 'Agreed. Done.', 'commenter': 'sandeshh'}]"
486,engine/src/main/java/com/datatorrent/stram/StreamingContainerManager.java,"@@ -1180,6 +1188,9 @@ public void removeContainerAgent(String containerId)
   {
     LOG.debug(""Removing container agent {}"", containerId);
     StreamingContainerAgent containerAgent = containers.remove(containerId);
+    if (containerHeartBeatMissSet.contains(containerId)) {","[{'comment': 'Why is containerHeartBeatMissSet.contains() call necessary?', 'commenter': 'vrozov'}]"
486,engine/src/main/java/com/datatorrent/stram/StreamingContainerManager.java,"@@ -217,6 +217,7 @@
   protected String shutdownDiagnosticsMessage = """";
   private long lastResourceRequest = 0;
   private final Map<String, StreamingContainerAgent> containers = new ConcurrentHashMap<>();
+  private final Set<String> containerHeartBeatMissSet = Collections.newSetFromMap(new ConcurrentHashMap<String, Boolean>());","[{'comment': 'Why is the Set necessary? Will not it be easier to track that the container time outed on the container itself?  ', 'commenter': 'vrozov'}, {'comment': ""Yes it will be easier, but when I was testing it I observed multiple container error events for the heartbeat miss of the same container , although this wasn't wrong but according to me only one container error event should be raised for this scenario and hence I have used a Set to assure if the container has already raised this error event it should not raise it again."", 'commenter': 'KapoorHitesh'}, {'comment': 'What is the significance of the Set or a Collection of containers that missed the heartbeat? Can you introduce boolean on StreamingContainerAgent that identifies containers that missed the heartbeat? Another option will be to rely on the lastHeartBeatMillis. Update it to -1 (check what will be a good option) when the container misses heartbeat for the first time.', 'commenter': 'vrozov'}]"
486,engine/src/main/java/com/datatorrent/stram/StreamingContainerManager.java,"@@ -784,10 +784,15 @@ public void monitorHeartbeat(boolean waitForRecovery)
             containerStopRequests.put(c.getExternalId(), c.getExternalId());
           }
         } else {
-          if (currentTms - sca.lastHeartbeatMillis > this.vars.heartbeatTimeoutMillis) {
+          if (sca.lastHeartbeatMillis != -1 && (currentTms - sca.lastHeartbeatMillis > this.vars.heartbeatTimeoutMillis)) {
             if (!isApplicationIdle()) {
               // request stop (kill) as process may still be hanging around (would have been detected by Yarn otherwise)
-              LOG.info(""Container {}@{} heartbeat timeout ({} ms)."", c.getExternalId(), c.host, currentTms - sca.lastHeartbeatMillis);
+              String info = String.format(""Container %s@%s heartbeat timeout (%d%n ms)."", c.getExternalId(), c.host, currentTms - sca.lastHeartbeatMillis);
+              LOG.info(info);","[{'comment': 'While you are looking at this code, please change to LOG.warn or LOG.error, it should not be info level. The same applies to line 783.', 'commenter': 'vrozov'}]"
486,engine/src/main/java/com/datatorrent/stram/StreamingContainerManager.java,"@@ -784,10 +784,15 @@ public void monitorHeartbeat(boolean waitForRecovery)
             containerStopRequests.put(c.getExternalId(), c.getExternalId());
           }
         } else {
-          if (currentTms - sca.lastHeartbeatMillis > this.vars.heartbeatTimeoutMillis) {
+          if (sca.lastHeartbeatMillis != -1 && (currentTms - sca.lastHeartbeatMillis > this.vars.heartbeatTimeoutMillis)) {","[{'comment': 'Before, Stram will request Yarn to kill a container each time it detects heartbeat timeout. Can we keep the original behavior? Are there any pros of sending only one? If not, please keep the original behavior and limit check `sca.lastHeartbeatMillis != -1` only to avoid multiple StramEvents.', 'commenter': 'vrozov'}, {'comment': 'Thank you for the review comment. What do you think about the repetitive log message? According to me we should only have one log message (like we are having after these code changes).\r\n', 'commenter': 'KapoorHitesh'}, {'comment': 'I agree, there should be only one StramEvent raised, but there may be multiple attempts to request Yarn to kill the container.', 'commenter': 'vrozov'}]"
486,engine/src/main/java/com/datatorrent/stram/StreamingContainerManager.java,"@@ -784,10 +784,15 @@ public void monitorHeartbeat(boolean waitForRecovery)
             containerStopRequests.put(c.getExternalId(), c.getExternalId());
           }
         } else {
-          if (currentTms - sca.lastHeartbeatMillis > this.vars.heartbeatTimeoutMillis) {
+          if (sca.lastHeartbeatMillis != -1 && (currentTms - sca.lastHeartbeatMillis > this.vars.heartbeatTimeoutMillis)) {
             if (!isApplicationIdle()) {","[{'comment': '@tweise @PramodSSImmaneni  This does not look right to me. Should not a container be killed even when an application is idle when the container misses a heartbeat?', 'commenter': 'vrozov'}, {'comment': 'I am not sure but according to me when the application starts at that time some operators may not be deployed instantly and may be waiting for some resources may be at that time container may miss a heartbeat but should not be killed. \r\nI need to check this theory will get back to you on this.', 'commenter': 'KapoorHitesh'}, {'comment': 'This is the question on `isApplicationIdle()` for @tweise and @PramodSSImmaneni, not directly related to the PR.', 'commenter': 'vrozov'}]"
486,engine/src/main/java/com/datatorrent/stram/StreamingContainerManager.java,"@@ -780,14 +780,22 @@ public void monitorHeartbeat(boolean waitForRecovery)
           //LOG.debug(""{} {} {}"", c.getExternalId(), currentTms - sca.createdMillis, this.vars.heartbeatTimeoutMillis);
           // container allocated but process was either not launched or is not able to phone home
           if (currentTms - sca.createdMillis > 2 * this.vars.heartbeatTimeoutMillis) {
-            LOG.info(""Container {}@{} startup timeout ({} ms)."", c.getExternalId(), c.host, currentTms - sca.createdMillis);
+            LOG.error(""Container {}@{} startup timeout ({} ms)."", c.getExternalId(), c.host, currentTms - sca.createdMillis);","[{'comment': '@Hitesh-Scorpio @vrozov this is incorrect use of logging and I want to discuss this before it spreads further. That there can be problems with worker containers is an expected scenario in a distributed system and as we can see Apex handles the condition.  ERROR should be reserved for problems internal to Apex that cannot be handled by the system. Please undo this change.', 'commenter': 'tweise'}]"
486,engine/src/main/java/com/datatorrent/stram/StreamingContainerManager.java,"@@ -780,14 +780,22 @@ public void monitorHeartbeat(boolean waitForRecovery)
           //LOG.debug(""{} {} {}"", c.getExternalId(), currentTms - sca.createdMillis, this.vars.heartbeatTimeoutMillis);
           // container allocated but process was either not launched or is not able to phone home
           if (currentTms - sca.createdMillis > 2 * this.vars.heartbeatTimeoutMillis) {
-            LOG.info(""Container {}@{} startup timeout ({} ms)."", c.getExternalId(), c.host, currentTms - sca.createdMillis);
+            LOG.error(""Container {}@{} startup timeout ({} ms)."", c.getExternalId(), c.host, currentTms - sca.createdMillis);
             containerStopRequests.put(c.getExternalId(), c.getExternalId());
           }
         } else {
           if (currentTms - sca.lastHeartbeatMillis > this.vars.heartbeatTimeoutMillis) {
             if (!isApplicationIdle()) {
+              // Check if the heartbeat for this agent has already been missed to raise the StramEvent only once
+              if (sca.lastHeartbeatMillis != -1) {
+                String info = String.format(""Container %s@%s heartbeat timeout (%d%n ms)."", c.getExternalId(), c.host, currentTms - sca.lastHeartbeatMillis);
+                LOG.error(info);","[{'comment': 'As the variable says, this is INFO.', 'commenter': 'tweise'}, {'comment': 'Reverted it back to info. ', 'commenter': 'KapoorHitesh'}]"
497,engine/src/main/java/com/datatorrent/stram/StreamingContainerManager.java,"@@ -2161,6 +2177,16 @@ public void updateRecoveryCheckpoints(PTOperator operator, UpdateCheckpointsCont
 
   }
 
+  private boolean isDelayPartOfGroup(Set<OperatorMeta> opers)","[{'comment': 'static?', 'commenter': 'vrozov'}]"
497,engine/src/main/java/com/datatorrent/stram/StreamingContainerManager.java,"@@ -2068,6 +2068,22 @@ public void updateRecoveryCheckpoints(PTOperator operator, UpdateCheckpointsCont
     if (checkpointGroup.size() > 1) {
       for (OperatorMeta om : checkpointGroup) {
         Collection<PTOperator> operators = plan.getAllOperators(om);
+        // Include unifiers in group if delay operator is present
+        // Do only if group contains a delay operator
+        if (isDelayPartOfGroup(checkpointGroup)) {","[{'comment': 'the outcome of the `isDelayPartOfGroup` does not seem to depend on `om`, why it is called inside the iteration? Was the intention to call it with operators?', 'commenter': 'vrozov'}, {'comment': 'Until this point, the contents of `operators` may be incorrect. If delay operator is present, likely any unifiers reachable from any of the `operators` need to be part of the group as well. In other words, if delay is part of the group, then  `operators` might change. ', 'commenter': 'bhupeshchawda'}]"
497,engine/src/main/java/com/datatorrent/stram/StreamingContainerManager.java,"@@ -2068,6 +2068,22 @@ public void updateRecoveryCheckpoints(PTOperator operator, UpdateCheckpointsCont
     if (checkpointGroup.size() > 1) {
       for (OperatorMeta om : checkpointGroup) {
         Collection<PTOperator> operators = plan.getAllOperators(om);
+        // Include unifiers in group if delay operator is present
+        // Do only if group contains a delay operator
+        if (isDelayPartOfGroup(checkpointGroup)) {
+          Set<PTOperator> unifiers = new HashSet<>();","[{'comment': 'Why is `isDelayPartOfGroup` a separate method and this part is not?', 'commenter': 'vrozov'}, {'comment': 'True. I can put all the logic in a separate method and return a set of unifier operators which can be appended to `operators`. Will do that.', 'commenter': 'bhupeshchawda'}]"
497,engine/src/main/java/com/datatorrent/stram/StreamingContainerManager.java,"@@ -2068,6 +2068,22 @@ public void updateRecoveryCheckpoints(PTOperator operator, UpdateCheckpointsCont
     if (checkpointGroup.size() > 1) {
       for (OperatorMeta om : checkpointGroup) {
         Collection<PTOperator> operators = plan.getAllOperators(om);
+        // Include unifiers in group if delay operator is present
+        // Do only if group contains a delay operator","[{'comment': 'Why only when delay operator is present?', 'commenter': 'tweise'}, {'comment': 'I saw that when delay is part of the group, the unifiers create a separate checkpoint group even though the upstream and downstream operators are in the same group which seems to be incorrect. In other cases, each of the operators form a separate group and the problem does not arise in that case.', 'commenter': 'bhupeshchawda'}, {'comment': ""I think we should discuss why unifiers should not always be in the same group? Let's do that before getting into which code goes where."", 'commenter': 'tweise'}, {'comment': 'IMO if we have a dag like A -> B, where A is partitioned and A and B are in the same group, then the unifier should also be in the same group. Is this understanding correct?', 'commenter': 'bhupeshchawda'}, {'comment': 'Yes, I think that should be the case. The unifier belongs to the same logical operator.', 'commenter': 'tweise'}]"
497,engine/src/main/java/com/datatorrent/stram/StreamingContainerManager.java,"@@ -2174,6 +2178,33 @@ public void updateRecoveryCheckpoints(PTOperator operator, UpdateCheckpointsCont
 
   }
 
+  private Collection<PTOperator> getUnifiersInCheckpointGroup(Set<OperatorMeta> checkpointGroup, Collection<PTOperator> operators)
+  {
+    boolean isDelayPartOfGroup = false;
+    for (OperatorMeta om : checkpointGroup) {
+      if (om.getOperator() instanceof Operator.DelayOperator) {
+        isDelayPartOfGroup = true;
+      }
+    }
+    // Include unifiers in group if delay operator is present
+    // Do only if group contains a delay operator
+    if (isDelayPartOfGroup) {","[{'comment': 'I thought the result of the discussion was to always include the unifiers. So there is no need to check for a delay operator.', 'commenter': 'tweise'}, {'comment': 'Sorry, I misunderstood your comment earlier. In absence of delay, what is the problem if unifier is part of a separate checkpoint group? This is what happens currently and seems to be working okay.', 'commenter': 'bhupeshchawda'}, {'comment': 'I think that when logical operators are grouped for checkpointing, then the associated unifiers should also be covered. I would consider that as an omission in the current code. Also, if we agree that this is how it should behave, you can remove the delay checking logic and simply always include unifiers.', 'commenter': 'tweise'}, {'comment': 'Agreed. Updated the logic', 'commenter': 'bhupeshchawda'}]"
497,engine/src/main/java/com/datatorrent/stram/StreamingContainerManager.java,"@@ -2081,6 +2081,10 @@ public void updateRecoveryCheckpoints(PTOperator operator, UpdateCheckpointsCont
     if (checkpointGroup.size() > 1) {
       for (OperatorMeta om : checkpointGroup) {
         Collection<PTOperator> operators = plan.getAllOperators(om);","[{'comment': 'This is supposed to return the unifiers also, why are they missing?', 'commenter': 'tweise'}, {'comment': 'Yes, it seems like it should include unifiers as well.. let me look into it..', 'commenter': 'bhupeshchawda'}, {'comment': 'It would be good to understand that, since it would make the extra code to find unifiers here unnecessary. It is possible that this is a result of changing how single unifiers are mapped in the physical plan in an earlier release. If so we should check if getAllOperators needs to be modified.', 'commenter': 'tweise'}, {'comment': 'Seems like only cascading and sliding unifiers are returned. Even the documentation says that MxN unifiers will not be returned.\r\n```\r\n    /**\r\n     * Return all partitions and unifiers, except MxN unifiers\r\n     * @return\r\n     */\r\n    private Collection<PTOperator> getAllOperators()\r\n    {\r\n      Collection<PTOperator> c = new ArrayList<>(partitions.size() + 1);\r\n      c.addAll(partitions);\r\n      for (StreamMapping ug : outputStreams.values()) {\r\n        ug.addTo(c);\r\n      }\r\n      return c;\r\n    }\r\n```  ', 'commenter': 'bhupeshchawda'}, {'comment': ""Yes, the documentation isn't correct though, this applies not just to a shuffle, but even to a single downstream operator (that was the change I referred to)."", 'commenter': 'tweise'}]"
497,engine/src/main/java/com/datatorrent/stram/StreamingContainerManager.java,"@@ -2082,6 +2082,10 @@ public void updateRecoveryCheckpoints(PTOperator operator, UpdateCheckpointsCont
     if (checkpointGroup.size() > 1) {
       for (OperatorMeta om : checkpointGroup) {
         Collection<PTOperator> operators = plan.getAllOperators(om);
+        Collection<PTOperator> unifiers = getUnifiersInCheckpointGroup(operators);
+        if (unifiers != null) {","[{'comment': 'How can this be null?', 'commenter': 'tweise'}, {'comment': 'Remove the redundant null check.', 'commenter': 'tweise'}, {'comment': 'removed', 'commenter': 'bhupeshchawda'}]"
497,engine/src/main/java/com/datatorrent/stram/StreamingContainerManager.java,"@@ -2175,6 +2179,22 @@ public void updateRecoveryCheckpoints(PTOperator operator, UpdateCheckpointsCont
 
   }
 
+  private Collection<PTOperator> getUnifiersInCheckpointGroup(Collection<PTOperator> operators)","[{'comment': 'make the method static', 'commenter': 'tweise'}, {'comment': 'done', 'commenter': 'bhupeshchawda'}]"
497,engine/src/test/java/com/datatorrent/stram/CheckpointTest.java,"@@ -282,6 +286,57 @@ public void testUpdateRecoveryCheckpoint() throws Exception
   }
 
   @Test
+  public void testUpdateRecoveryCheckpointWithCycle() throws Exception
+  {
+    Clock clock = new SystemClock();
+
+    dag.setAttribute(com.datatorrent.api.Context.OperatorContext.STORAGE_AGENT, new MemoryStorageAgent());
+
+    // Simulate a DAG with a loop which has a unifier operator
+    TestGeneratorInputOperator o1 = dag.addOperator(""o1"", TestGeneratorInputOperator.class);
+    GenericTestOperator o2 = dag.addOperator(""o2"", GenericTestOperator.class);
+    GenericTestOperator o3 = dag.addOperator(""o3"", GenericTestOperator.class);
+    GenericTestOperator o4 = dag.addOperator(""o4"", GenericTestOperator.class);
+    DefaultDelayOperator d = dag.addOperator(""d"", DefaultDelayOperator.class);
+
+    dag.addStream(""o1.output1"", o1.outport, o2.inport1);
+    dag.addStream(""o2.output1"", o2.outport1, o3.inport1);
+    dag.addStream(""o3.output1"", o3.outport1, o4.inport1);
+    dag.addStream(""o4.output1"", o4.outport1, d.input);
+    dag.addStream(""d.output"", d.output, o2.inport2);
+    dag.setOperatorAttribute(o3, Context.OperatorContext.PARTITIONER, new StatelessPartitioner<Operator>(2));","[{'comment': 'Please avoid explicit type argument.', 'commenter': 'vrozov'}, {'comment': '@vrozov will fix this in a new PR..', 'commenter': 'bhupeshchawda'}]"
500,engine/src/main/java/com/datatorrent/stram/cli/ApexCli.java,"@@ -3070,7 +3071,7 @@ public void execute(String[] args, ConsoleReader reader) throws Exception
             }
 
             arr.put(oper);
-          } catch (Exception | NoClassDefFoundError ex) {
+          } catch (Exception | NoClassDefFoundError | UnsupportedClassVersionError ex) {","[{'comment': 'Why add UnsupportedClassVersionError to the list if none of your code is throwing it?', 'commenter': 'sanjaypujare'}, {'comment': 'get-app-package-operators will throw this exception and kill the apexCli. Catching it there will still output correctly compiled operators.', 'commenter': 'oliverwnk'}]"
500,engine/src/main/java/com/datatorrent/stram/client/AppPackage.java,"@@ -153,10 +155,12 @@ public AppPackage(File file, File contentFolder, boolean processAppDirectory) th
     dtEngineVersion = attr.getValue(ATTRIBUTE_DT_ENGINE_VERSION);
     appPackageDisplayName = attr.getValue(ATTRIBUTE_DT_APP_PACKAGE_DISPLAY_NAME);
     appPackageDescription = attr.getValue(ATTRIBUTE_DT_APP_PACKAGE_DESCRIPTION);
+    buildJdkVersion = attr.getValue(ATTRIBUTE_BUILD_JDK_VERSION);
     String classPathString = attr.getValue(ATTRIBUTE_CLASS_PATH);
     if (appPackageName == null || appPackageVersion == null || classPathString == null) {
       throw new IOException(""Not a valid app package.  App Package Name or Version or Class-Path is missing from MANIFEST.MF"");
     }
+","[{'comment': 'Remove the extra empty line', 'commenter': 'sanjaypujare'}]"
500,engine/src/main/java/com/datatorrent/stram/cli/ApexCli.java,"@@ -3516,6 +3517,16 @@ private void checkPlatformCompatible(AppPackage ap)
     }
   }
 
+  private void checkJavaVersionCompatible(AppPackage ap)
+  {
+    String buildJdkVersion = ap.getBuildJdkVersion();
+    String systemVersion = System.getProperty(""java.version"");
+    if (VersionInfo.compare(buildJdkVersion, systemVersion) > 0) {","[{'comment': ""I wasn't sure about comparing the Build-jdk version with the current Java version (the one running the Apex cli). Note that one can use Build-jdk version 1.8 to generate class files of version 1.7 (class version major 51) and if there are no other issues it should ideally run. But this check will catch such cases, preventing running of such packages. However the Java runtime libraries could still present a problem so in my opinion this check is good for now."", 'commenter': 'sanjaypujare'}, {'comment': ""Currently you are calling this from the LaunchCommand.execute() so throwing an exception makes sense. But we can (and should) also enhance the GetAppPackageInfoCommand to display the Build-jdk value for user's benefit. In that case you may want to refactor this function to just return the 2 values (when called from GetAppPackageInfoCommand) or throw an exception (like here when called from  LunchCommand)."", 'commenter': 'sanjaypujare'}, {'comment': 'GetAppPackageInfoCommand prints all values of a JsonSerialization of the AppPackage class. Since I added buildJdk as a member in AppPackage it will already print it.', 'commenter': 'oliverwnk'}, {'comment': '@oliverwnk The check for Build-jdk is not good. As @sanjaypujare pointed out, it is possible to build an application package on a machine where JDK 1.8 is installed and generate 1.7 or 1.6 compatible binaries. The check is way too restrictive.', 'commenter': 'vrozov'}, {'comment': '@oliverwnk @vrozov In that case the only way to implement this check is to check each and every class file in the APA (i.e. each class in each jar of the APA) and compare the class version against that the current Java supports. If there is anything newer then generate an error', 'commenter': 'sanjaypujare'}, {'comment': ""I don't see why checking every class is necessary. An application package may contain classes that are not used or used only if JVM version supports them. Additionally Apex client JVM may not match JVMs on the cluster. It should be sufficient to catch UnsupportedClassVersionError."", 'commenter': 'vrozov'}]"
501,engine/src/main/java/com/datatorrent/stram/plan/logical/LogicalPlan.java,"@@ -1248,22 +1248,25 @@ public int hashCode()
   @Override
   public <T extends Operator> T addOperator(String name, T operator)
   {
-    if (operators.containsKey(name)) {
-      if (operators.get(name).operator == operator) {
-        return operator;
+    if ( name != null && !name.isEmpty()) {","[{'comment': 'Check name and throw an exception. The rest of the method should not be affected.', 'commenter': 'vrozov'}]"
501,engine/src/main/java/com/datatorrent/stram/plan/logical/LogicalPlan.java,"@@ -1248,6 +1248,9 @@ public int hashCode()
   @Override
   public <T extends Operator> T addOperator(String name, T operator)","[{'comment': 'Annotate `name` with `@Nonnull` (javax.annotation.Nonnull) both in the interface declaration and implementation. ', 'commenter': 'vrozov'}]"
501,engine/src/main/java/com/datatorrent/stram/plan/logical/LogicalPlan.java,"@@ -1248,6 +1248,9 @@ public int hashCode()
   @Override
   public <T extends Operator> T addOperator(String name, T operator)
   {
+    if (name == null || name.isEmpty()) {","[{'comment': 'Use `checkArgument()` (statically import `com.google.common.base.Preconditions.checkArgument`) and `isNullOrEmpty()` (statically import `com.google.common.base.Strings.isNullOrEmpty`). ', 'commenter': 'vrozov'}]"
501,engine/src/main/java/com/datatorrent/stram/plan/logical/LogicalPlan.java,"@@ -1248,6 +1248,9 @@ public int hashCode()
   @Override
   public <T extends Operator> T addOperator(String name, T operator)
   {
+    if (name == null || name.isEmpty()) {
+      throw new IllegalArgumentException(""name argument is null or empty"");","[{'comment': 'Be consistent with other `IllegalArgumentException`: it should either be ""operator id is null or empty"" or change ""id"" to ""name"" in other places too (I would prefer changing ""id"" to ""name"").', 'commenter': 'vrozov'}]"
501,engine/src/test/java/com/datatorrent/stram/plan/logical/LogicalPlanTest.java,"@@ -502,6 +506,66 @@ public void testOperatorAnnotation()
     }
   }
 
+  class testModule implements Module","[{'comment': 'private static? Can you use an anonymous class instead or Mock it?', 'commenter': 'vrozov'}]"
501,engine/src/test/java/com/datatorrent/stram/plan/logical/LogicalPlanTest.java,"@@ -502,6 +506,66 @@ public void testOperatorAnnotation()
     }
   }
 
+  class testModule implements Module
+  {
+    @Override
+    public void populateDAG(DAG dag, Configuration conf)
+    {
+    }
+  }
+
+  @Test
+  public void testEmptyNameArgument()
+  {
+    LogicalPlan dag = new LogicalPlan();","[{'comment': 'Move LogicalPlan creation to `@Setup`', 'commenter': 'vrozov'}]"
501,engine/src/test/java/com/datatorrent/stram/plan/logical/LogicalPlanTest.java,"@@ -502,6 +506,66 @@ public void testOperatorAnnotation()
     }
   }
 
+  class testModule implements Module
+  {
+    @Override
+    public void populateDAG(DAG dag, Configuration conf)
+    {
+    }
+  }
+
+  @Test
+  public void testEmptyNameArgument()
+  {
+    LogicalPlan dag = new LogicalPlan();
+
+    //test addOperator()
+    try {
+      dag.addOperator("""", BaseOperator.class);
+      Assert.fail(""should raise name argument is null or empty"");
+    } catch (IllegalArgumentException e) {","[{'comment': 'Use `@Test(expected = IllegalArgumentException.class)`\r\n', 'commenter': 'vrozov'}]"
501,engine/src/main/java/com/datatorrent/stram/plan/logical/LogicalPlan.java,"@@ -72,6 +73,9 @@
 
 import com.google.common.collect.Sets;
 
+import static com.google.common.base.Preconditions.checkArgument;","[{'comment': 'Please move static imports to the end of the list.', 'commenter': 'vrozov'}]"
501,engine/src/test/java/com/datatorrent/stram/plan/logical/LogicalPlanTest.java,"@@ -88,12 +93,17 @@
 
 public class LogicalPlanTest
 {
+  LogicalPlan dag;","[{'comment': 'private', 'commenter': 'vrozov'}]"
501,engine/src/main/java/com/datatorrent/stram/plan/logical/LogicalPlan.java,"@@ -1347,16 +1353,19 @@ private void setParent(ModuleMeta meta)
   }
 
   @Override
-  public <T extends Module> T addModule(String name, T module)
+  public <T extends Module> T addModule(@Nonnull String name, T module)
   {
+    if (name == null || name.isEmpty()) {","[{'comment': 'use checkArgument()', 'commenter': 'vrozov'}]"
501,engine/src/main/java/com/datatorrent/stram/plan/logical/LogicalPlan.java,"@@ -1399,8 +1408,11 @@ public void removeOperator(Operator operator)
   }
 
   @Override
-  public StreamMeta addStream(String id)
+  public StreamMeta addStream(@Nonnull String id)
   {
+    if (id == null || id.isEmpty()) {","[{'comment': 'checkArgument', 'commenter': 'vrozov'}]"
520,api/src/main/java/com/datatorrent/api/Context.java,"@@ -531,6 +531,11 @@
      */
     Attribute<String> LIBRARY_JARS = new Attribute<>(String2String.getInstance());
 
+    /**
+     * Configuration file for custom SSL keystore to override ssl-server.xml
+     */
+    Attribute<String> CUSTOM_SSL_SERVER_CONFIG = new Attribute<>(String2String.getInstance());","[{'comment': ""There isn't anything specific to ssl here isn't it. It is custom config that is being passed to stram webservice for initialization. There is a property for stram web service authentication (STRAM_HTTP_AUTHENTICATION). I suggest this property follow the same format, maybe STRAM_HTTP_CUSTOM_CONFIG? I understand it is a little bit of a misnomer because this is primarily for https but I think it is better to be consistent for the user. In the javadoc you can include information such as this config can be used to configure ssl parameters and also that this file needs to be present on all the nodes."", 'commenter': 'pramodin'}, {'comment': 'Yes, you are right although we are doing this only to get custom SSL properties in. I am okay with the name you are suggesting (while we are trying to finalize the change on the Yarn side).', 'commenter': 'sanjaypujare'}]"
520,api/src/main/java/com/datatorrent/api/Context.java,"@@ -531,6 +531,12 @@
      */
     Attribute<String> LIBRARY_JARS = new Attribute<>(String2String.getInstance());
 
+    /**
+     * This configuration file can be used to configure ssl parameters over-riding the default Yarn ssl-server.xml","[{'comment': 'Since this is any custom config, can you adjust the javadoc to the same. You can add additional documentation that it can be useful to set ssl parameters etc.', 'commenter': 'pramodin'}]"
530,engine/src/main/java/com/datatorrent/stram/security/ACLManager.java,"@@ -0,0 +1,67 @@
+/**","[{'comment': 'new code needs to go into org.apache.apex packages', 'commenter': 'tweise'}]"
530,engine/src/main/java/com/datatorrent/stram/StramClient.java,"@@ -37,7 +37,6 @@
 import org.slf4j.LoggerFactory;
 
 import org.apache.apex.common.util.JarHelper;
-","[{'comment': 'Avoid format changes', 'commenter': 'vrozov'}]"
530,engine/src/main/java/com/datatorrent/stram/security/ACLManager.java,"@@ -0,0 +1,67 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.stram.security;
+
+import java.io.IOException;
+import java.util.Map;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.yarn.api.records.ApplicationAccessType;
+import org.apache.hadoop.yarn.api.records.ContainerLaunchContext;
+import org.apache.hadoop.yarn.conf.YarnConfiguration;
+
+import com.google.common.collect.Maps;
+
+/**
+ *
+ */
+public class ACLManager
+{
+
+  private static final Logger logger = LoggerFactory.getLogger(ACLManager.class);
+
+  public static void setupUserACLs(ContainerLaunchContext launchContext, String userName, Configuration conf) throws IOException
+  {
+    logger.debug(""Setup login acls {}"", userName);
+    if (areAclsRequired(conf)) {
+      logger.debug(""Configuring ACLs for {}"", userName);
+      Map<ApplicationAccessType, String> acls = Maps.newHashMap();
+      acls.put(ApplicationAccessType.VIEW_APP, userName);
+      acls.put(ApplicationAccessType.MODIFY_APP, userName);
+      launchContext.setApplicationACLs(acls);
+    }
+  }
+
+  public static boolean areAclsRequired(Configuration conf)","[{'comment': 'Should this be named `areACLsRequired` ? We should be consistent in ACL vs Acl in identifier names', 'commenter': 'sanjaypujare'}]"
534,engine/src/main/java/com/datatorrent/stram/StreamingContainerManager.java,"@@ -485,11 +488,12 @@ public final ContainerInfo getAppMasterContainerInfo()
         String nodeHttpAddress = nmHost + "":"" + nmHttpPort;
         if (allocatedMemoryMB == 0) {
           String url = ConfigUtils.getSchemePrefix(conf) + nodeHttpAddress + ""/ws/v1/node/containers/"" + ci.id;
-          WebServicesClient webServicesClient = new WebServicesClient();
-          try {
-            String content = webServicesClient.process(url, String.class, new WebServicesClient.GetWebServicesHandler<String>());
-            JSONObject json = new JSONObject(content);
-            int totalMemoryNeededMB = json.getJSONObject(""container"").getInt(""totalMemoryNeededMB"");
+          try (YarnClient rmClient = new YarnClientImpl()) {","[{'comment': 'Use YarnClient.createYarnClient()', 'commenter': 'vrozov'}, {'comment': 'Looks like @devtagare is not working on this. I will make the change as I am familiar with this issue. I will add my commit on top so his pull request will also be addressed when merged.', 'commenter': 'pramodin'}]"
543,engine/src/main/java/com/datatorrent/stram/StreamingAppMasterService.java,"@@ -870,6 +873,21 @@ private void execute() throws YarnException, IOException
       }
 
       numRequestedContainers += containerRequests.size() - removedContainerRequests.size();
+
+      for (String containerId : dnmgr.containerStopRequests.values()) {
+        Long pendingSince = pendingContainersToKill.get(containerId);
+        if (pendingSince == null) {
+          pendingContainersToKill.put(containerId, System.currentTimeMillis());
+        }
+      }
+
+      for (Map.Entry<String, Long> pendingToKill : pendingContainersToKill.entrySet()) {
+        if (System.currentTimeMillis() - pendingToKill.getValue() > 30 * 1000) {","[{'comment': 'Instead of hardcoding 30 * 1000 can we make it configurable or at least define a const variable?', 'commenter': 'sanjaypujare'}, {'comment': 'Done.', 'commenter': 'sandeshh'}]"
543,engine/src/main/java/com/datatorrent/stram/StreamingAppMasterService.java,"@@ -870,6 +873,21 @@ private void execute() throws YarnException, IOException
       }
 
       numRequestedContainers += containerRequests.size() - removedContainerRequests.size();
+","[{'comment': 'It might be better to populate pendingContainersToKill inside sendContainerAskToRM() where the last loop goes thru all of dnmgr.containerStopRequests and optionally issues a stopContainerAsync. At that point we can add the current containerIdStr  entry to pendingContainersToKill with the current-time. The logic is in one place so easier to understand (also possibly synchronized on containerStopRequests to prevent race conditions).', 'commenter': 'sanjaypujare'}, {'comment': 'An alternative (more modular/OO) implementation could be to move this logic into the existing NMCallbackHandler class. Save the reference to NMCallbackHandler in StreamingAppMasterService (or get it from nmClient.getCallbackHandler) and move pendingContainersToKill in there. When NMCallbackHandler.onContainerStopped(ContainerId) is called remove the containerId from pendingContainersToKill and then only examine that list for containers that have been pending there for a long time.', 'commenter': 'sanjaypujare'}, {'comment': ""Regarding \r\n1. sendContainerAskToRM is already overloaded, as kill request is sent to NM and not to RM, so adding more things to it will only complicate it further.\r\n2. We can't move this logic to NMCallbackHandler. This PR is because onContainerStopped may not have been called."", 'commenter': 'sandeshh'}, {'comment': '1. What do you mean overloaded? If the name is misleading we can rename it (it is private) to indicate what it is supposed to be doing including the new code in this PR suggested by me.\r\n\r\n2. The end result is the same. When onContainerStopped is called the contaierIds are removed from pendingContainersToKill and if it is not called (the case you are saying) we will detect the containers that have been in pendingContainersToKill for too long (more than NODE_MANAGER_KILL_CONTAINER_TIMEOUT).\r\n\r\nI suggest you reconsider', 'commenter': 'sanjaypujare'}]"
543,engine/src/main/java/com/datatorrent/stram/StreamingAppMasterService.java,"@@ -1203,17 +1209,33 @@ private AllocateResponse sendContainerAskToRM(List<ContainerRequest> containerRe
       amRmClient.releaseAssignedContainer(containerId);
     }
 
+    return amRmClient.allocate(0);
+  }
+
+  private void processContainerStopRequests()
+  {
     for (String containerIdStr : dnmgr.containerStopRequests.values()) {
       AllocatedContainer allocatedContainer = this.allocatedContainers.get(containerIdStr);
       if (allocatedContainer != null && !allocatedContainer.stopRequested) {
         nmClient.stopContainerAsync(allocatedContainer.container.getId(), allocatedContainer.container.getNodeId());
         LOG.info(""Requested stop container {}"", containerIdStr);
         allocatedContainer.stopRequested = true;
       }
+
+      Long pendingSince = pendingContainersToKill.get(containerIdStr);","[{'comment': ""Is a separate logic block needed for this? Can't the addition to the pendingContainersToKill go into the previous if block."", 'commenter': 'pramodin'}, {'comment': 'No need for a separate block, rolling this up.', 'commenter': 'sandeshh'}]"
543,engine/src/main/java/com/datatorrent/stram/StreamingAppMasterService.java,"@@ -1203,17 +1209,33 @@ private AllocateResponse sendContainerAskToRM(List<ContainerRequest> containerRe
       amRmClient.releaseAssignedContainer(containerId);
     }
 
+    return amRmClient.allocate(0);
+  }
+
+  private void processContainerStopRequests()","[{'comment': 'Is there a specific need for refactoring this logic out of sendContainerAskToRM into a separate method. The sendContainerAskToRM method already seems to be handling all the different kinds of container requests.', 'commenter': 'pramodin'}, {'comment': ""Method name was misleading, it was sending a request to both RM and NM. Now it is clearly separated and this also addresses @sanjaypujare's code review comments."", 'commenter': 'sandeshh'}, {'comment': 'How about calling it processPendingContainers, see comment below on the method format.', 'commenter': 'pramodin'}, {'comment': 'Done.', 'commenter': 'sandeshh'}]"
543,engine/src/main/java/com/datatorrent/stram/StreamingAppMasterService.java,"@@ -138,6 +139,7 @@
    * This should be replaced when a constant is defined there
    */
   private static final String SSL_SERVER_KEYSTORE_LOCATION = ""ssl.server.keystore.location"";
+  private static final int NODE_MANAGER_KILL_CONTAINER_TIMEOUT = 30 * 1000;","[{'comment': 'Can you make it configurable by a system property. See bufferserver.server.Server.BACK_PRESSURE_ENABLED', 'commenter': 'pramodin'}, {'comment': 'done.', 'commenter': 'sandeshh'}]"
543,engine/src/main/java/com/datatorrent/stram/StreamingAppMasterService.java,"@@ -171,6 +173,8 @@
   private AppDataPushAgent appDataPushAgent;
   private ApexPluginDispatcher apexPluginDispatcher;
 
+  private Map<String, Long> pendingContainersToKill = new ConcurrentHashMap<>();","[{'comment': 'Extra space before this line', 'commenter': 'pramodin'}, {'comment': 'done.', 'commenter': 'sandeshh'}]"
543,engine/src/main/java/com/datatorrent/stram/StreamingAppMasterService.java,"@@ -1203,17 +1209,29 @@ private AllocateResponse sendContainerAskToRM(List<ContainerRequest> containerRe
       amRmClient.releaseAssignedContainer(containerId);
     }
 
+    return amRmClient.allocate(0);
+  }
+
+  private void processContainerStopRequests()
+  {
     for (String containerIdStr : dnmgr.containerStopRequests.values()) {","[{'comment': ""Why can't the first for section remain in sendContainerAskToRM as that method is dealing with sending of all requests to RM."", 'commenter': 'pramodin'}]"
543,engine/src/main/java/com/datatorrent/stram/StreamingAppMasterService.java,"@@ -1203,17 +1209,29 @@ private AllocateResponse sendContainerAskToRM(List<ContainerRequest> containerRe
       amRmClient.releaseAssignedContainer(containerId);
     }
 
+    return amRmClient.allocate(0);
+  }
+
+  private void processContainerStopRequests()
+  {
     for (String containerIdStr : dnmgr.containerStopRequests.values()) {
       AllocatedContainer allocatedContainer = this.allocatedContainers.get(containerIdStr);
       if (allocatedContainer != null && !allocatedContainer.stopRequested) {
         nmClient.stopContainerAsync(allocatedContainer.container.getId(), allocatedContainer.container.getNodeId());
         LOG.info(""Requested stop container {}"", containerIdStr);
         allocatedContainer.stopRequested = true;
+        pendingContainersToKill.put(containerIdStr, System.currentTimeMillis());
       }
+
       dnmgr.containerStopRequests.remove(containerIdStr);
     }
 
-    return amRmClient.allocate(0);
+    for (Map.Entry<String, Long> pendingToKill : pendingContainersToKill.entrySet()) {
+      if (System.currentTimeMillis() - pendingToKill.getValue() > NODE_MANAGER_KILL_CONTAINER_TIMEOUT) {","[{'comment': 'greater or equal', 'commenter': 'pramodin'}, {'comment': 'Done.', 'commenter': 'sandeshh'}]"
543,engine/src/main/java/com/datatorrent/stram/StreamingAppMasterService.java,"@@ -138,6 +139,7 @@
    * This should be replaced when a constant is defined there
    */
   private static final String SSL_SERVER_KEYSTORE_LOCATION = ""ssl.server.keystore.location"";
+  private static int NODE_MANAGER_KILL_CONTAINER_TIMEOUT;","[{'comment': 'How about a more appropriate name as this timeout is not a Node Manager setting but rather a fallback setting. You could just call it REMOVE_CONTAINER_TIMEOUT (after the RM call removeContainerRequest)', 'commenter': 'pramodin'}, {'comment': 'done.', 'commenter': 'sandeshh'}]"
543,engine/src/main/java/com/datatorrent/stram/StreamingAppMasterService.java,"@@ -170,11 +172,20 @@
   private StramDelegationTokenManager delegationTokenManager = null;
   private AppDataPushAgent appDataPushAgent;
   private ApexPluginDispatcher apexPluginDispatcher;
+  private Map<String, Long> pendingContainersToKill = new ConcurrentHashMap<>();
 
   public StreamingAppMasterService(ApplicationAttemptId appAttemptID)
   {
     super(StreamingAppMasterService.class.getName());
     this.appAttemptID = appAttemptID;
+
+    Integer nodeManagerKillTimeout = Integer.getInteger(""org.apache.apex.nodemanager.kill.timeout"");","[{'comment': 'REMOVE_CONTAINER_TIMEOUT = Integer.getInteger(""org.apache.apex.nodemanager.kill.timeout"", 30 * 1000);', 'commenter': 'vrozov'}]"
543,engine/src/main/java/com/datatorrent/stram/StreamingAppMasterService.java,"@@ -1209,10 +1219,18 @@ private AllocateResponse sendContainerAskToRM(List<ContainerRequest> containerRe
         nmClient.stopContainerAsync(allocatedContainer.container.getId(), allocatedContainer.container.getNodeId());
         LOG.info(""Requested stop container {}"", containerIdStr);
         allocatedContainer.stopRequested = true;
+        pendingContainersToKill.put(containerIdStr, System.currentTimeMillis());
       }
       dnmgr.containerStopRequests.remove(containerIdStr);
     }
 
+    for (Map.Entry<String, Long> pendingToKill : pendingContainersToKill.entrySet()) {
+      if (System.currentTimeMillis() - pendingToKill.getValue() >= REMOVE_CONTAINER_TIMEOUT) {
+        LOG.info(""Timeout happened for NodeManager kill container request, recovering the container {} without waiting"", pendingToKill.getKey());
+        recoverContainer(pendingToKill.getKey());","[{'comment': 'Is not it subject to ConcurrentModificationException?', 'commenter': 'vrozov'}]"
543,engine/src/main/java/com/datatorrent/stram/StreamingAppMasterService.java,"@@ -1266,26 +1286,35 @@ public void onStopContainerError(ContainerId containerId, Throwable t)
       recoverContainer(containerId);
     }
 
+    @Override
+    public void onContainerStarted(ContainerId containerId, Map<String, ByteBuffer> allServiceResponse)","[{'comment': 'The order of the methods has changed', 'commenter': 'pramodin'}, {'comment': 'fixed.', 'commenter': 'sandeshh'}]"
543,engine/src/main/java/com/datatorrent/stram/StreamingAppMasterService.java,"@@ -1205,14 +1224,21 @@ private AllocateResponse sendContainerAskToRM(List<ContainerRequest> containerRe
 
     for (String containerIdStr : dnmgr.containerStopRequests.values()) {
       AllocatedContainer allocatedContainer = this.allocatedContainers.get(containerIdStr);
-      if (allocatedContainer != null && !allocatedContainer.stopRequested) {
+      if (allocatedContainer != null && allocatedContainer.stopRequested != -1) {","[{'comment': 'encapsulate logic allocatedContainer.stopRequested != -1 into AllocatedContainer:`allocatedContainer.isStopRequested()`', 'commenter': 'vrozov'}, {'comment': 'done.', 'commenter': 'sandeshh'}]"
543,engine/src/main/java/com/datatorrent/stram/StreamingAppMasterService.java,"@@ -1205,14 +1224,21 @@ private AllocateResponse sendContainerAskToRM(List<ContainerRequest> containerRe
 
     for (String containerIdStr : dnmgr.containerStopRequests.values()) {
       AllocatedContainer allocatedContainer = this.allocatedContainers.get(containerIdStr);
-      if (allocatedContainer != null && !allocatedContainer.stopRequested) {
+      if (allocatedContainer != null && allocatedContainer.stopRequested != -1) {
         nmClient.stopContainerAsync(allocatedContainer.container.getId(), allocatedContainer.container.getNodeId());
         LOG.info(""Requested stop container {}"", containerIdStr);
-        allocatedContainer.stopRequested = true;
+        allocatedContainer.stopRequested = System.currentTimeMillis();","[{'comment': ""move to AllocatedContainer: `allocatedContainer.stop()'"", 'commenter': 'vrozov'}]"
543,engine/src/main/java/com/datatorrent/stram/StreamingAppMasterService.java,"@@ -1205,14 +1224,21 @@ private AllocateResponse sendContainerAskToRM(List<ContainerRequest> containerRe
 
     for (String containerIdStr : dnmgr.containerStopRequests.values()) {
       AllocatedContainer allocatedContainer = this.allocatedContainers.get(containerIdStr);
-      if (allocatedContainer != null && !allocatedContainer.stopRequested) {
+      if (allocatedContainer != null && allocatedContainer.stopRequested != -1) {
         nmClient.stopContainerAsync(allocatedContainer.container.getId(), allocatedContainer.container.getNodeId());
         LOG.info(""Requested stop container {}"", containerIdStr);
-        allocatedContainer.stopRequested = true;
+        allocatedContainer.stopRequested = System.currentTimeMillis();
       }
       dnmgr.containerStopRequests.remove(containerIdStr);
     }
 
+    for (Map.Entry<String, AllocatedContainer> entry : allocatedContainers.entrySet()) {
+      if (System.currentTimeMillis() - entry.getValue().stopRequested >= REMOVE_CONTAINER_TIMEOUT) {","[{'comment': 'The same as other comments. Also, move REMOVE_CONTAINER_TIMEOUT.', 'commenter': 'vrozov'}, {'comment': 'done.', 'commenter': 'sandeshh'}]"
547,engine/src/main/java/com/datatorrent/stram/client/StramClientUtils.java,"@@ -118,6 +118,8 @@
   public static final long DELEGATION_TOKEN_MAX_LIFETIME_DEFAULT = 7 * 24 * 60 * 60 * 1000;
   public static final String TOKEN_REFRESH_PRINCIPAL = StramUserLogin.DT_AUTH_PREFIX + ""token.refresh.principal"";
   public static final String TOKEN_REFRESH_KEYTAB = StramUserLogin.DT_AUTH_PREFIX + ""token.refresh.keytab"";
+  public static final String CUSTOM_SSL_KEYSTORE_CONFIG = StreamingApplication.APEX_PREFIX + ""ssl.keystore.config"";","[{'comment': 'IMO, it will be very confusing that some settings use ""dt"" prefix and others use ""apex"" prefix. I would suggest that both ""dt"" and ""apex"" are acceptable with ""dt"" being a deprecated prefix similar to what was done to attributes. In the next major release ""dt"" can be removed. If this is acceptable to everyone, this PR should use ""dt"" prefix and deprecation of ""dt"" should be handled in a separate PR.\r\n\r\nI also do not understand why the setting is necessary and how it is different from STRAM_HTTP_CUSTOM_CONFIG attribute. Should not it be possible to specify different keystore for different applications? How will it be covered if CUSTOM_SSL_KEYSTORE_CONFIG is not an attribute?', 'commenter': 'vrozov'}, {'comment': 'It was @tweise \'s suggestion to use apex instead of dt so I would like to get his response. Also @vrozov the last line in your 1st para says ""...this PR should use ""dt"" prefix ..."" . Did you mean to say ""...this PR should _also_ use ""dt"" prefix..."" .', 'commenter': 'sanjaypujare'}, {'comment': ""Regarding STRAM_HTTP_CUSTOM_CONFIG this was added in APEXCORE-711 and serves a slightly different purpose. If the SSL keystore files are already present on the app master node (which means they don't need to be copied/deployed to the target app master node) then APEXCORE-711 functionality is sufficient and you just need to use STRAM_HTTP_CUSTOM_CONFIG attribute.\r\n\r\nWhereas this JIRA is needed if the SSL keystore is not present on the target app master node so the user needs the Stram client to package it and include it in the app package to be deployed. Let me know if you need more info - you can also read the description in the JIRA"", 'commenter': 'sanjaypujare'}, {'comment': ""@sanjaypujare it is correct that I'm not OK to add new keys with dt. prefix to the codebase. I suggested to move the existing keys to apex. as part of the same PR and that's what I would do. If you prefer to have a separate PR that is up to you. What is the proposal for naming of the keys? I also think there is a need to expand javadoc."", 'commenter': 'tweise'}, {'comment': 'I would like to keep this PR restricted to what the JIRA expects. So my proposal for this PR is as follows:\r\n\r\nSupport both dt.ssl.keystore.config and apex.ssl.keystore.config with the understanding that dt.ssl.keystore.config is deprecated and will be removed in the next major release (4.x).\r\n\r\nEnhance Javadocs for the new members added in this PR with proper explanation about usage and deprecation.\r\n\r\nRegarding naming of the keys I am awaiting a response from @PramodSSImmaneni  to make sure we have an agreement on the naming.', 'commenter': 'sanjaypujare'}, {'comment': '@tweise I would prefer to handle deprecation of ""dt"" prefix as part of a separate PR as the scope of the change is way beyond this PR. In addition, I would focus on the user experience and IMO, it will be quite confusing to have some settings that start with ""dt"" and others that start with ""apex"". ', 'commenter': 'vrozov'}, {'comment': '@sanjaypujare Please see my comment regarding attribute vs dt-site.xml setting. Why one is DAG level attribute and another a setting? Why is it OK to blindly overwrite STRAM_HTTP_CUSTOM_CONFIG attribute value if it was set when  CUSTOM_SSL_KEYSTORE_CONFIG is present? Can server vs client configuration be handled by a separate attribute/setting, but in a way that is less confusing to end users?', 'commenter': 'vrozov'}, {'comment': '@vrozov  STRAM_HTTP_CUSTOM_CONFIG is an attribute because only the StreamingAppMasterService uses it and that is the most common way to pass values (attributes) to it.\r\n\r\nCUSTOM_SSL_KEYSTORE_CONFIG is a setting because the Apex CLI (StramClient) implements it and the most common way to pass settings to it are via dt-site.xml or thru -D option on the launch command.\r\n\r\nThe code in this JIRA is going to use the feature added in APEXCORE-711. We had designed it to be like this based on user+developer discussion captured in http://apache-apex-developers-list.78491.x6.nabble.com/Enhancement-to-support-custom-SSL-configuration-td14335.html . So there is no question of the client setting blindly overwriting the server setting', 'commenter': 'sanjaypujare'}, {'comment': ""@sanjaypujare It is implementation difference, from logical usage I don't see any difference, so I think that the implementation will be confusing to end users.\r\n\r\nI don't see anything in the discussion on dev@apex related to the attributes/settings to be used, so the implementation was not discussed and/or approved by the community."", 'commenter': 'vrozov'}, {'comment': ""@vrozov  there is a difference as I explained earlier.  The CUSTOM_SSL_KEYSTORE_CONFIG  setting used in this JIRA is only processed by the Apex CLI (Stram client) and so it doesn't need to be an attribute that any code on the App Master side needs to be aware of.\r\n\r\nAs far as usability or user confusion goes, I think a setting/property is preferable to an attribute from usability point of view so @devtagare decided to use a setting here and I agree with that decision and personally I don't believe it is confusing."", 'commenter': 'sanjaypujare'}, {'comment': ""@sanjaypujare\r\n- The CUSTOM_SSL_KEYSTORE_CONFIG setting is only used during application start, so it is not required for the Apex client itself. \r\n- I don't see why setting/property is preferable over an attribute. Attributes support per application/operator/port specification, while settings do not.\r\n- Why is it OK to blindly overwrite STRAM_HTTP_CUSTOM_CONFIG?"", 'commenter': 'vrozov'}, {'comment': 'Looks like these are not used anymore with the new changes. Could you remove them?', 'commenter': 'pramodin'}]"
547,api/src/main/java/com/datatorrent/api/Context.java,"@@ -536,11 +536,10 @@
     Attribute<String> LIBRARY_JARS = new Attribute<>(String2String.getInstance());
 
     /**
-     * This configuration file can be used to over-ride the default Yarn configuration.
-     * For example, this can be used to provide custom SSL parameters in the configuration.
-     * Note that this file needs to be present on the node.
+     * SSL Configuration string property. Currently used to specify SSL parameters for","[{'comment': 'For the second part of the statement. How about just saying Used to specify SSL configuration for stram web services.', 'commenter': 'pramodin'}, {'comment': 'Configuration => configuration. Agreed there is no need for ""Currently"" and similar expansions.', 'commenter': 'tweise'}]"
547,api/src/main/java/com/datatorrent/api/SSLConfig.java,"@@ -0,0 +1,82 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.api;
+
+import java.io.Serializable;
+
+/**
+ *
+ */
+public class SSLConfig implements Serializable
+{
+
+  private static final long serialVersionUID = -3491488868092056793L;
+  private String keyStorePath;
+  private String keyStorePassword;
+  private String keyStoreKeyPassword;
+  private String nodeLocalConfigPath;","[{'comment': 'How about just configPath. Also can you add some documentation for all these parameters.', 'commenter': 'pramodin'}]"
547,api/src/main/java/com/datatorrent/api/SSLConfig.java,"@@ -0,0 +1,82 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.api;","[{'comment': 'org.apache.apex.api', 'commenter': 'pramodin'}]"
547,engine/src/main/java/com/datatorrent/stram/StramClient.java,"@@ -42,6 +42,7 @@
 import org.apache.commons.io.IOUtils;
 import org.apache.commons.lang.ArrayUtils;
 import org.apache.commons.lang.StringUtils;
+import org.apache.directory.api.util.Strings;","[{'comment': 'Are there methods in StringUtils that you can use for your need?', 'commenter': 'pramodin'}]"
547,engine/src/main/java/com/datatorrent/stram/StramClient.java,"@@ -644,6 +647,37 @@ public void startApplication() throws YarnException, IOException
     }
   }
 
+  /**
+   * Process the SSLConfig if present in the DAG's attributes
+   *
+   * @param sslConfig  SSLConfig object derived from SSL_CONFIG attribute
+   * @param fs    HDFS file system object
+   * @param appPath    application path for the current application
+   * @param localResources  Local resources to modify
+   * @throws IOException
+   */
+  private void processSSLConfig(SSLConfig sslConfig, FileSystem fs, Path appPath, Map<String, LocalResource> localResources) throws IOException
+  {
+    if (sslConfig != null) {
+      String nodeLocalConfig = sslConfig.getNodeLocalConfigPath();
+
+      if (Strings.isNotEmpty(nodeLocalConfig)) {
+        // all others should be empty
+        if (Strings.isNotEmpty(sslConfig.getKeyStorePath()) || Strings.isNotEmpty(sslConfig.getKeyStorePassword())
+            || Strings.isNotEmpty(sslConfig.getKeyStoreKeyPassword())) {
+          throw new IllegalArgumentException(""Cannot specify both nodeLocalConfigPath and other parameters in "" + sslConfig);
+        }
+        // pass thru: Stram will implement read the node local config file to implement SSL
+      } else {
+        // need to package and copy the keyStore file
+        String keystorePath = sslConfig.getKeyStorePath();","[{'comment': 'Do we need to do any property validation like above?', 'commenter': 'pramodin'}]"
547,engine/src/main/java/com/datatorrent/stram/StramClient.java,"@@ -644,6 +647,37 @@ public void startApplication() throws YarnException, IOException
     }
   }
 
+  /**
+   * Process the SSLConfig if present in the DAG's attributes
+   *
+   * @param sslConfig  SSLConfig object derived from SSL_CONFIG attribute
+   * @param fs    HDFS file system object
+   * @param appPath    application path for the current application
+   * @param localResources  Local resources to modify
+   * @throws IOException
+   */
+  private void processSSLConfig(SSLConfig sslConfig, FileSystem fs, Path appPath, Map<String, LocalResource> localResources) throws IOException
+  {
+    if (sslConfig != null) {
+      String nodeLocalConfig = sslConfig.getNodeLocalConfigPath();
+
+      if (Strings.isNotEmpty(nodeLocalConfig)) {
+        // all others should be empty
+        if (Strings.isNotEmpty(sslConfig.getKeyStorePath()) || Strings.isNotEmpty(sslConfig.getKeyStorePassword())
+            || Strings.isNotEmpty(sslConfig.getKeyStoreKeyPassword())) {
+          throw new IllegalArgumentException(""Cannot specify both nodeLocalConfigPath and other parameters in "" + sslConfig);
+        }
+        // pass thru: Stram will implement read the node local config file to implement SSL
+      } else {
+        // need to package and copy the keyStore file
+        String keystorePath = sslConfig.getKeyStorePath();
+        String[] sslFileArray = {keystorePath};
+        String sslFileNames = copyFromLocal(fs, appPath, sslFileArray);
+        LaunchContainerRunnable.addFilesToLocalResources(LocalResourceType.ARCHIVE, sslFileNames, localResources, fs);","[{'comment': 'Should this be ARCHIVE or FILE?', 'commenter': 'pramodin'}]"
547,engine/src/main/java/com/datatorrent/stram/StramClient.java,"@@ -726,4 +760,5 @@ public void setFiles(String files)
   {
     this.files = files;
   }
+","[{'comment': 'Extraneous formatting?', 'commenter': 'pramodin'}]"
547,engine/src/main/java/com/datatorrent/stram/StramClient.java,"@@ -454,6 +456,7 @@ public void startApplication() throws YarnException, IOException
         appPath = new Path(configuredAppPath);
       }
       String libJarsCsv = copyFromLocal(fs, appPath, localJarFiles.toArray(new String[]{}));
+      processSSLConfig(dag.getValue(Context.DAGContext.SSL_CONFIG), fs, appPath, localResources);","[{'comment': ""This is about setting up SSL resources if specified isn't it. How about calling it something like prepareSSLResources or setupSSLResources. Also, consider whether we need to breakout this into a separate method as this is not in line with the pattern in this method where other types of resources are being handled inline in the method."", 'commenter': 'pramodin'}, {'comment': 'I am -1 on inline, it should not be done in the first place. +1 for setupSSLResources.', 'commenter': 'vrozov'}]"
547,engine/src/main/java/com/datatorrent/stram/StreamingAppMasterService.java,"@@ -649,6 +650,32 @@ protected void serviceStart() throws Exception
     }
   }
 
+  /**
+   * Modify the config object by adding SSL related parameters in a resource as the case may be so the WebApp uses
+   * correct SSL params
+   *
+   * @param config  Configuration to be modified
+   */
+  private void addSSLConfigResource(Configuration config)
+  {
+    SSLConfig sslConfig = dag.getValue(Context.DAGContext.SSL_CONFIG);
+    if (sslConfig != null) {
+      String nodeLocalConfig = sslConfig.getNodeLocalConfigPath();
+      if (StringUtils.isNotEmpty(nodeLocalConfig)) {
+        config.addResource(new Path(nodeLocalConfig));
+      } else {
+        // create a configuration object and add it as a resource
+        Configuration sslConfigResource = new Configuration(false);
+        String keyStorePath = sslConfig.getKeyStorePath();  // need to strip out the root
+        keyStorePath = (new File(keyStorePath)).getName();
+        sslConfigResource.set(""ssl.server.keystore.location"", new File(keyStorePath).getName(), Context.DAGContext.SSL_CONFIG.getLongName());","[{'comment': 'Can you use hadoop constants for the ssl parameters?', 'commenter': 'pramodin'}]"
547,engine/src/main/java/com/datatorrent/stram/StreamingAppMasterService.java,"@@ -649,6 +650,32 @@ protected void serviceStart() throws Exception
     }
   }
 
+  /**
+   * Modify the config object by adding SSL related parameters in a resource as the case may be so the WebApp uses
+   * correct SSL params
+   *
+   * @param config  Configuration to be modified
+   */
+  private void addSSLConfigResource(Configuration config)
+  {
+    SSLConfig sslConfig = dag.getValue(Context.DAGContext.SSL_CONFIG);
+    if (sslConfig != null) {
+      String nodeLocalConfig = sslConfig.getNodeLocalConfigPath();
+      if (StringUtils.isNotEmpty(nodeLocalConfig)) {
+        config.addResource(new Path(nodeLocalConfig));
+      } else {
+        // create a configuration object and add it as a resource
+        Configuration sslConfigResource = new Configuration(false);
+        String keyStorePath = sslConfig.getKeyStorePath();  // need to strip out the root
+        keyStorePath = (new File(keyStorePath)).getName();","[{'comment': 'Is there a way to pass this path or name in a direct way from the client as opposed to deriving it from the client path. The logic is repeated in two places, the client and here and will need to kept in sync.', 'commenter': 'pramodin'}]"
547,engine/src/main/java/com/datatorrent/stram/client/StramClientUtils.java,"@@ -118,6 +118,8 @@
   public static final long DELEGATION_TOKEN_MAX_LIFETIME_DEFAULT = 7 * 24 * 60 * 60 * 1000;
   public static final String TOKEN_REFRESH_PRINCIPAL = StramUserLogin.DT_AUTH_PREFIX + ""token.refresh.principal"";
   public static final String TOKEN_REFRESH_KEYTAB = StramUserLogin.DT_AUTH_PREFIX + ""token.refresh.keytab"";
+  public static final String CUSTOM_SSL_KEYSTORE_CONFIG = StreamingApplication.APEX_PREFIX + ""ssl.keystore.config"";
+  public static final String CUSTOM_SSL_KEYSTORE_PATH = StreamingApplication.APEX_PREFIX + ""ssl.keystore.path"";","[{'comment': 'How it will be used?', 'commenter': 'vrozov'}]"
547,api/src/main/java/com/datatorrent/api/SSLConfig.java,"@@ -0,0 +1,82 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.api;
+
+import java.io.Serializable;
+
+/**","[{'comment': 'Please provide java doc', 'commenter': 'vrozov'}]"
547,api/src/main/java/com/datatorrent/api/SSLConfig.java,"@@ -0,0 +1,82 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.api;
+
+import java.io.Serializable;
+
+/**
+ *
+ */
+public class SSLConfig implements Serializable","[{'comment': 'Does it need to be a top level class? Can it be moved to Context as public static class?', 'commenter': 'vrozov'}, {'comment': 'Evolving?', 'commenter': 'vrozov'}]"
547,api/src/main/java/com/datatorrent/api/SSLConfig.java,"@@ -0,0 +1,82 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.api;
+
+import java.io.Serializable;
+
+/**
+ *
+ */
+public class SSLConfig implements Serializable
+{
+
+  private static final long serialVersionUID = -3491488868092056793L;
+  private String keyStorePath;
+  private String keyStorePassword;
+  private String keyStoreKeyPassword;
+  private String nodeLocalConfigPath;
+
+  public String getKeyStorePath()
+  {
+    return keyStorePath;
+  }
+
+  public void setKeyStorePath(String keyStorePath)
+  {
+    this.keyStorePath = keyStorePath;
+  }
+
+  public String getKeyStorePassword()
+  {
+    return keyStorePassword;
+  }
+
+  public void setKeyStorePassword(String keyStorePassword)
+  {
+    this.keyStorePassword = keyStorePassword;
+  }
+
+  public String getKeyStoreKeyPassword()
+  {
+    return keyStoreKeyPassword;
+  }
+
+  public void setKeyStoreKeyPassword(String keyStoreKeyPassword)
+  {
+    this.keyStoreKeyPassword = keyStoreKeyPassword;
+  }
+
+  public String getNodeLocalConfigPath()
+  {
+    return nodeLocalConfigPath;
+  }
+
+  public void setNodeLocalConfigPath(String nodeLocalPath)
+  {
+    this.nodeLocalConfigPath = nodeLocalPath;
+  }
+
+  @Override
+  public String toString()
+  {
+    return ""SSLConfig [keyStorePath="" + keyStorePath + "", keyStorePassword="" + keyStorePassword","[{'comment': 'security violation.', 'commenter': 'vrozov'}]"
547,engine/src/main/java/com/datatorrent/stram/StramClient.java,"@@ -644,6 +647,37 @@ public void startApplication() throws YarnException, IOException
     }
   }
 
+  /**
+   * Process the SSLConfig if present in the DAG's attributes
+   *
+   * @param sslConfig  SSLConfig object derived from SSL_CONFIG attribute
+   * @param fs    HDFS file system object
+   * @param appPath    application path for the current application
+   * @param localResources  Local resources to modify
+   * @throws IOException
+   */
+  private void processSSLConfig(SSLConfig sslConfig, FileSystem fs, Path appPath, Map<String, LocalResource> localResources) throws IOException
+  {
+    if (sslConfig != null) {
+      String nodeLocalConfig = sslConfig.getNodeLocalConfigPath();
+
+      if (Strings.isNotEmpty(nodeLocalConfig)) {","[{'comment': 'Why Strings?', 'commenter': 'vrozov'}]"
554,engine/src/main/java/org/apache/apex/engine/GroupingManager.java,"@@ -16,16 +16,17 @@
  * specific language governing permissions and limitations
  * under the License.
  */
-package org.apache.apex.stram;
+package org.apache.apex.engine;","[{'comment': 'It will be better to introduce a package (like org.apache.apex.engine.events.grouping) for classes related to the functionality.', 'commenter': 'vrozov'}]"
554,engine/src/main/java/org/apache/apex/engine/GroupingManager.java,"@@ -39,7 +40,7 @@
 public class GroupingManager
 {
   private static final GroupingManager groupingManager = new GroupingManager();
-  private Map<String, GroupingRequest> groupingRequests = Maps.newHashMap();
+  private Map<String, GroupingRequest> groupingRequests = Maps.newConcurrentMap();","[{'comment': 'Is concurrent necessary? If yes, the pattern used in addOrModifyGroupingRequest is subject to a data loss.', 'commenter': 'vrozov'}]"
554,engine/src/main/java/org/apache/apex/engine/GroupingManager.java,"@@ -153,14 +157,15 @@ public boolean removeOperatorFromGroupingRequest(int operatorId)
    */
   public void removeProcessedGroupingRequests()
   {
-    for (Entry<String, GroupingRequest> request : groupingRequests.entrySet()) {
+    Iterator<Entry<String, GroupingRequest>> itr = groupingRequests.entrySet().iterator();
+    while (itr.hasNext()) {
+      Entry<String, GroupingRequest> request = itr.next();
       if (request.getValue().getOperatorsToDeploy().size() == 0
           && request.getValue().getOperatorsToUndeploy().size() == 0) {
         LOG.info(""Removing for :"" + request.getKey());","[{'comment': 'Use sl4j logging. Does it need to be info level?', 'commenter': 'vrozov'}]"
554,engine/src/main/java/org/apache/apex/engine/GroupingManager.java,"@@ -153,14 +157,15 @@ public boolean removeOperatorFromGroupingRequest(int operatorId)
    */
   public void removeProcessedGroupingRequests()
   {
-    for (Entry<String, GroupingRequest> request : groupingRequests.entrySet()) {
+    Iterator<Entry<String, GroupingRequest>> itr = groupingRequests.entrySet().iterator();
+    while (itr.hasNext()) {
+      Entry<String, GroupingRequest> request = itr.next();
       if (request.getValue().getOperatorsToDeploy().size() == 0","[{'comment': 'Introduce isDone() or isComplete() into the GroupingRequest.', 'commenter': 'vrozov'}]"
554,engine/src/main/java/org/apache/apex/engine/events/grouping/GroupingRequest.java,"@@ -129,6 +129,18 @@ public void addAffectedContainer(String containerId)
   }
 
   /**
+   * Checks if request is processed
+   * @return isProcessed
+   */
+  public boolean isProcessed()
+  {
+    if (getOperatorsToDeploy().size() == 0 && getOperatorsToUndeploy().size() == 0) {","[{'comment': 'return operatorsToDeploy.isEmpty() && operatorsToUndeploy.isEmpty();', 'commenter': 'vrozov'}]"
554,engine/src/main/java/org/apache/apex/engine/events/grouping/GroupingManager.java,"@@ -153,14 +157,14 @@ public boolean removeOperatorFromGroupingRequest(int operatorId)
    */
   public void removeProcessedGroupingRequests()
   {
-    for (Entry<String, GroupingRequest> request : groupingRequests.entrySet()) {
-      if (request.getValue().getOperatorsToDeploy().size() == 0
-          && request.getValue().getOperatorsToUndeploy().size() == 0) {
-        LOG.info(""Removing for :"" + request.getKey());
-        groupingRequests.remove(request.getKey());
+    Iterator<Entry<String, GroupingRequest>> itr = groupingRequests.entrySet().iterator();
+    while (itr.hasNext()) {
+      Entry<String, GroupingRequest> request = itr.next();
+      if (request.getValue().isProcessed()) {
+        LOG.debug(""Removing Grouping request for :"" + request.getKey());","[{'comment': 'Use sl4j', 'commenter': 'vrozov'}, {'comment': '@vrozov  can you please help me understand what you mean by sl4j. I have used slf4j logger here.', 'commenter': 'DT-Priyanka'}, {'comment': 'Use {} parameters instead of string concat. It is a basic thing that all contributors should be aware of.', 'commenter': 'tweise'}]"
554,engine/src/main/java/org/apache/apex/engine/events/grouping/GroupingManager.java,"@@ -153,14 +157,14 @@ public boolean removeOperatorFromGroupingRequest(int operatorId)
    */
   public void removeProcessedGroupingRequests()
   {
-    for (Entry<String, GroupingRequest> request : groupingRequests.entrySet()) {
-      if (request.getValue().getOperatorsToDeploy().size() == 0
-          && request.getValue().getOperatorsToUndeploy().size() == 0) {
-        LOG.info(""Removing for :"" + request.getKey());
-        groupingRequests.remove(request.getKey());
+    Iterator<Entry<String, GroupingRequest>> itr = groupingRequests.entrySet().iterator();
+    while (itr.hasNext()) {
+      Entry<String, GroupingRequest> request = itr.next();","[{'comment': 'please rename to entry. The actual request is itr.next().getValue();', 'commenter': 'vrozov'}]"
554,engine/src/main/java/org/apache/apex/engine/events/grouping/GroupingRequest.java,"@@ -129,6 +129,18 @@ public void addAffectedContainer(String containerId)
   }
 
   /**
+   * Checks if request is processed
+   * @return isProcessed
+   */
+  public boolean isProcessed()
+  {
+    if (getOperatorsToDeploy().isEmpty() && getOperatorsToUndeploy().isEmpty()) {","[{'comment': 'Please see my original comment. The result of isProcessed() is the value of operatorsToDeploy.isEmpty() && operatorsToUndeploy.isEmpty(). There is no need to extra if', 'commenter': 'vrozov'}]"
555,common/src/main/java/org/apache/apex/common/util/JarHelper.java,"@@ -186,4 +190,62 @@ private void addEntry(File file, String name) throws IOException
       jos.closeEntry();
     }
   }
+
+  /**
+   * Returns a full path to the jar-file that contains the given class and all full paths to dependent jar-files
+   * that are defined in the property ""apex-dependencies"" of the manifest of the root jar-file
+   * @param classPath Class path
+   * @return List of jar-files
+   */
+  public static List<String> getApexDependentJarsByClass(String classPath)
+  {
+    try {
+      return getApexDependentJarsByClass(Thread.currentThread().getContextClassLoader().loadClass(classPath));
+    } catch (ClassNotFoundException ex) {
+      logger.error(""Cannot find the class {}"", classPath, ex);
+      throw new RuntimeException(""Cannot find the class "" + classPath, ex);
+    }
+  }
+
+  /**
+   * Returns a full path to the jar-file that contains the given class and all full paths to dependent jar-files
+   * that are defined in the property ""apex-dependencies"" of the manifest of the root jar-file
+   * @param clazz Class
+   * @return List of jar-files
+   */
+  public static List<String> getApexDependentJarsByClass(Class clazz)
+  {
+    List<String> list = new LinkedList<>();
+
+    try {
+      URLConnection conn = clazz.getResource(clazz.getSimpleName() + "".class"").openConnection();","[{'comment': 'Please use JarHelper.getJar().', 'commenter': 'vrozov'}, {'comment': 'Unfortunately I cannot use the implementation of the method JarHelper.getJar(). The method getJar() creates jar file if the method parameter Java class does not belong to any jar-file. And it does not satisfy to the PR requirements (please talk to @PramodSSImmaneni about the PR requirements).\r\n\r\nLogically the implementation of the method getJar() is equal to the following 2 lines of Java code:\r\n\r\n    URLConnection conn = clazz.getResource(clazz.getSimpleName() + "".class"").openConnection();\r\n    return conn instanceof JarURLConnection ? ((JarURLConnection)conn).getJarFileURL().getFile() : null;\r\n\r\nThe method getJar() returns a name of jar-file as a string. But the implementation of the method getApexDependentJarsByClass() should continue to work with URLConnection object to retrieve the manifest properties.\r\n\r\nI think the code will look very ugly, if after getting of the jar-file name we need to recreate the URLConnection object from string again. It does not make sense.\r\n\r\nAlso in order to use the method JarHelper.getJar() the code should create an instance of the class JarHelper. But logically the method getJar() should be static , because it does not depend on any environment. (The implementation of the method contains caching of jar-name that is not used by the implementation of the method getApexDependentJarsByClass()).\r\n', 'commenter': 'sgolovko'}, {'comment': 'There is no requirement not to create a jar file if getJar() is called for a class that is located on the local file system and does not belong to an existing jar file. I don\'t see why such requirement will exist and why for certain classes jar should be created on the fly and some classes require pre-existing jars. The behavior must be consistent across all classes that Apex loads.\r\n\r\nLoading of the manifest file should be added to the getJar(). Ability to specify extra dependencies using ""apex-dependencies"" (or Class-Path) should not be limited to a specific jars or classes. It needs to be a generic feature available to all classes that Apex loads.\r\n\r\nThere is no need to make getJar() static. There is only one instance of JarHelper created.', 'commenter': 'vrozov'}]"
555,common/src/main/java/org/apache/apex/common/util/JarHelper.java,"@@ -186,4 +190,62 @@ private void addEntry(File file, String name) throws IOException
       jos.closeEntry();
     }
   }
+
+  /**
+   * Returns a full path to the jar-file that contains the given class and all full paths to dependent jar-files
+   * that are defined in the property ""apex-dependencies"" of the manifest of the root jar-file
+   * @param classPath Class path
+   * @return List of jar-files
+   */
+  public static List<String> getApexDependentJarsByClass(String classPath)
+  {
+    try {
+      return getApexDependentJarsByClass(Thread.currentThread().getContextClassLoader().loadClass(classPath));
+    } catch (ClassNotFoundException ex) {
+      logger.error(""Cannot find the class {}"", classPath, ex);
+      throw new RuntimeException(""Cannot find the class "" + classPath, ex);
+    }
+  }
+
+  /**
+   * Returns a full path to the jar-file that contains the given class and all full paths to dependent jar-files
+   * that are defined in the property ""apex-dependencies"" of the manifest of the root jar-file
+   * @param clazz Class
+   * @return List of jar-files
+   */
+  public static List<String> getApexDependentJarsByClass(Class clazz)
+  {
+    List<String> list = new LinkedList<>();
+
+    try {
+      URLConnection conn = clazz.getResource(clazz.getSimpleName() + "".class"").openConnection();
+      if (conn instanceof JarURLConnection) {
+        String jarPath = ((JarURLConnection)conn).getJarFileURL().getFile();
+        list.add(jarPath);
+        logger.debug(""Jar-file {} that contains the class {} was added"", jarPath, clazz.getCanonicalName());
+        String value = ((JarURLConnection)conn).getMainAttributes().getValue(""apex-dependencies"");
+
+        if (!StringUtils.isEmpty(value)) {
+          String folderPath = new File(jarPath).getParent();
+          for (String jarFile : value.split("","")) {
+            String file = folderPath + File.separator + jarFile;","[{'comment': '1. This logic seems a little hard-bound. So basically its expected that user should put the dependent jars in the same location as the plugin jar. This may not be true. \r\n\r\n2 This adds a question related to what should be the path in ""apex-dependencies""?? Relative? absolute? just jar name?\r\n\r\n3. Can you please check if there is enough control in maven plugin to have ""apex-dependencies"" comma seperated?', 'commenter': 'chinmaykolhatkar'}, {'comment': 'According to the agreement, the dependent jar-file should be located in the folder or subfolders that contains the parent jar-file. And the path of the dependent jar-files must be relative.\r\n\r\nMaven allows a user to add any attributes that should be included into the jar manifest. And the attribute values can have commas inside the values. ', 'commenter': 'sgolovko'}]"
555,engine/src/main/java/com/datatorrent/stram/StramClient.java,"@@ -338,7 +344,7 @@ public void startApplication() throws YarnException, IOException
       throw new IllegalStateException(applicationType + "" is not a valid application type."");
     }
 
-    LinkedHashSet<String> localJarFiles = findJars(dag, defaultClasses);","[{'comment': 'Why this change? If this is not related to this PR, please undo it.', 'commenter': 'chinmaykolhatkar'}, {'comment': 'Please see the comments above.', 'commenter': 'sgolovko'}]"
555,engine/src/main/java/com/datatorrent/stram/StramClient.java,"@@ -179,7 +179,7 @@ public void stop()
     yarnClient.stop();
   }
 
-  public static LinkedHashSet<String> findJars(LogicalPlan dag, Class<?>[] defaultClasses)
+  public LinkedHashSet<String> findJars(Class<?>[] defaultClasses)","[{'comment': 'Why this change? If this is not related to this PR, please undo it.', 'commenter': 'chinmaykolhatkar'}, {'comment': 'Logically the method findJars() is an internal method of the class  StramClient. The PR implementation has to use an extra private property of the class StramClient: Configuration conf. So I think it is much better to change the method to dynamic and exclude dag and configuration from the method signature.', 'commenter': 'sgolovko'}]"
555,common/src/main/java/org/apache/apex/common/util/JarHelper.java,"@@ -186,4 +190,62 @@ private void addEntry(File file, String name) throws IOException
       jos.closeEntry();
     }
   }
+
+  /**
+   * Returns a full path to the jar-file that contains the given class and all full paths to dependent jar-files
+   * that are defined in the property ""apex-dependencies"" of the manifest of the root jar-file
+   * @param classPath Class path
+   * @return List of jar-files
+   */
+  public static List<String> getApexDependentJarsByClass(String classPath)
+  {
+    try {
+      return getApexDependentJarsByClass(Thread.currentThread().getContextClassLoader().loadClass(classPath));
+    } catch (ClassNotFoundException ex) {
+      logger.error(""Cannot find the class {}"", classPath, ex);
+      throw new RuntimeException(""Cannot find the class "" + classPath, ex);
+    }
+  }
+
+  /**
+   * Returns a full path to the jar-file that contains the given class and all full paths to dependent jar-files
+   * that are defined in the property ""apex-dependencies"" of the manifest of the root jar-file
+   * @param clazz Class
+   * @return List of jar-files
+   */
+  public static List<String> getApexDependentJarsByClass(Class clazz)
+  {
+    List<String> list = new LinkedList<>();
+
+    try {
+      URLConnection conn = clazz.getResource(clazz.getSimpleName() + "".class"").openConnection();
+      if (conn instanceof JarURLConnection) {
+        String jarPath = ((JarURLConnection)conn).getJarFileURL().getFile();
+        list.add(jarPath);
+        logger.debug(""Jar-file {} that contains the class {} was added"", jarPath, clazz.getCanonicalName());
+        String value = ((JarURLConnection)conn).getMainAttributes().getValue(""apex-dependencies"");","[{'comment': 'Should this be `conn.getManifest().getMainAttributes()`?\r\nAlso, instead of `apex-dependencies` can we keep the attribute name as `Class-Path` which is usually the default?', 'commenter': 'bhupeshchawda'}, {'comment': 'The implementation of the method JarURLConnection.getMainAttributes() returns the manifest attributes.\r\n\r\nUnfortunately we cannot use the manifest attribute Class-Path, because the purpose of the new property to deploy the corresponded jar-files. But the attribute Class-Path points to the used jars. For instance, you can look at the content of the attribute Class-Path in apex jar-files to see how many jars are included by Maven.', 'commenter': 'sgolovko'}]"
555,common/src/main/java/org/apache/apex/common/util/JarHelper.java,"@@ -186,4 +190,62 @@ private void addEntry(File file, String name) throws IOException
       jos.closeEntry();
     }
   }
+
+  /**
+   * Returns a full path to the jar-file that contains the given class and all full paths to dependent jar-files
+   * that are defined in the property ""apex-dependencies"" of the manifest of the root jar-file
+   * @param classPath Class path
+   * @return List of jar-files
+   */
+  public static List<String> getApexDependentJarsByClass(String classPath)
+  {
+    try {
+      return getApexDependentJarsByClass(Thread.currentThread().getContextClassLoader().loadClass(classPath));
+    } catch (ClassNotFoundException ex) {
+      logger.error(""Cannot find the class {}"", classPath, ex);
+      throw new RuntimeException(""Cannot find the class "" + classPath, ex);
+    }
+  }
+
+  /**
+   * Returns a full path to the jar-file that contains the given class and all full paths to dependent jar-files
+   * that are defined in the property ""apex-dependencies"" of the manifest of the root jar-file
+   * @param clazz Class
+   * @return List of jar-files
+   */
+  public static List<String> getApexDependentJarsByClass(Class clazz)
+  {
+    List<String> list = new LinkedList<>();
+
+    try {
+      URLConnection conn = clazz.getResource(clazz.getSimpleName() + "".class"").openConnection();
+      if (conn instanceof JarURLConnection) {
+        String jarPath = ((JarURLConnection)conn).getJarFileURL().getFile();
+        list.add(jarPath);
+        logger.debug(""Jar-file {} that contains the class {} was added"", jarPath, clazz.getCanonicalName());
+        String value = ((JarURLConnection)conn).getMainAttributes().getValue(""apex-dependencies"");
+
+        if (!StringUtils.isEmpty(value)) {
+          String folderPath = new File(jarPath).getParent();
+          for (String jarFile : value.split("","")) {","[{'comment': 'the classpath entries would be space separated if we go with the default..', 'commenter': 'bhupeshchawda'}, {'comment': 'We are not going to reuse the attribute Class-Path.', 'commenter': 'sgolovko'}]"
555,engine/src/main/java/com/datatorrent/stram/StreamingAppMasterService.java,"@@ -604,6 +604,8 @@ protected void serviceInit(Configuration conf) throws Exception
   }
 
   public static final String PLUGINS_CONF_KEY = ""apex.plugin.stram.plugins"";","[{'comment': '@tushargosavi @PramodSSImmaneni @chinmaykolhatkar Please rename to ""apex.engine.plugins"". My concern is using plugin twice and referring to stram.', 'commenter': 'vrozov'}, {'comment': ""Additionally, is it necessary to have a separate entry for setup and engine plugins? Can't platform distinguish them based on interfaces the class implements."", 'commenter': 'vrozov'}]"
555,common/src/main/java/org/apache/apex/common/util/JarHelper.java,"@@ -70,41 +74,108 @@ public static String createJar(String prefix, File dir, boolean deleteOnExit) th
 
   public String getJar(Class<?> jarClass)
   {
-    String jar = null;
-    final CodeSource codeSource = jarClass.getProtectionDomain().getCodeSource();
-    if (codeSource != null) {
-      URL location = codeSource.getLocation();
-      jar = sourceToJar.get(location);
-      if (jar == null) {
-        // don't create jar file from folders multiple times
-        if (""jar"".equals(location.getProtocol())) {
-          try {
-            location = ((JarURLConnection)location.openConnection()).getJarFileURL();
-          } catch (IOException e) {
-            throw new AssertionError(""Cannot resolve jar file for "" + jarClass, e);
-          }
-        }
-        if (""file"".equals(location.getProtocol())) {
-          jar = location.getFile();
-          final File dir = new File(jar);
-          if (dir.isDirectory()) {
-            try {
-              jar = createJar(""apex-"", dir, false);
-            } catch (IOException e) {
-              throw new AssertionError(""Cannot resolve jar file for "" + jarClass + "". URL "" + location, e);
-            }
+    List<String> list = getJars(jarClass, true, false);
+    if (list == null) {
+      return null;
+    } else  if (list.isEmpty()) {
+      throw new AssertionError(""Cannot resolve jar file for "" + jarClass);
+    }
+    return list.get(0);
+  }
+
+  /**
+   * Returns a full path to the jar-file that contains the given class and all full paths to dependent jar-files
+   * that are defined in the property ""apex-dependencies"" of the manifest of the root jar-file.
+   * If the class is an independent file the method makes jar file from the folder that contains the class
+   * @param classPath Class path
+   * @param makeJarFromFolder True if the method should make jar from folder that contains the independent class
+   * @param addJarDependencies True if the method should include dependent jar files
+   * @return List of names of the jar-files
+   */
+  public List<String> getJars(String classPath, boolean makeJarFromFolder, boolean addJarDependencies)
+  {
+    try {
+      return getJars(Thread.currentThread()
+          .getContextClassLoader().loadClass(classPath), makeJarFromFolder, addJarDependencies);
+    } catch (ClassNotFoundException ex) {
+      logger.error(""Cannot find the class {}"", classPath, ex);
+      throw new RuntimeException(""Cannot find the class "" + classPath, ex);
+    }
+  }
+
+  /**
+   * Returns a full path to the jar-file that contains the given class and all full paths to dependent jar-files
+   * that are defined in the property ""apex-dependencies"" of the manifest of the root jar-file.
+   * If the class is an independent file the method makes jar file from the folder that contains the class
+   * @param clazz Class
+   * @param makeJarFromFolder True if the method should make jar from folder that contains the independent class
+   * @param addJarDependencies True if the method should include dependent jar files
+   * @return List of names of the jar-files
+   */
+  public List<String> getJars(Class<?> clazz, boolean makeJarFromFolder, boolean addJarDependencies)
+  {
+    CodeSource codeSource = clazz.getProtectionDomain().getCodeSource();
+    if (codeSource == null) {
+      return null;
+    }
+
+    List<String> list = new LinkedList<>();
+    URL location  =  codeSource.getLocation();
+    String jarPath = sourceToJar.get(location);
+
+    if (jarPath == null) {
+      try {
+        URLConnection conn = clazz.getResource(clazz.getSimpleName() + "".class"").openConnection();","[{'comment': 'Why is this necessary? Why does the class need to reload itself?', 'commenter': 'vrozov'}, {'comment': 'Done.', 'commenter': 'sgolovko'}]"
555,common/src/main/java/org/apache/apex/common/util/JarHelper.java,"@@ -70,41 +74,108 @@ public static String createJar(String prefix, File dir, boolean deleteOnExit) th
 
   public String getJar(Class<?> jarClass)
   {
-    String jar = null;
-    final CodeSource codeSource = jarClass.getProtectionDomain().getCodeSource();
-    if (codeSource != null) {
-      URL location = codeSource.getLocation();
-      jar = sourceToJar.get(location);
-      if (jar == null) {
-        // don't create jar file from folders multiple times
-        if (""jar"".equals(location.getProtocol())) {
-          try {
-            location = ((JarURLConnection)location.openConnection()).getJarFileURL();
-          } catch (IOException e) {
-            throw new AssertionError(""Cannot resolve jar file for "" + jarClass, e);
-          }
-        }
-        if (""file"".equals(location.getProtocol())) {
-          jar = location.getFile();
-          final File dir = new File(jar);
-          if (dir.isDirectory()) {
-            try {
-              jar = createJar(""apex-"", dir, false);
-            } catch (IOException e) {
-              throw new AssertionError(""Cannot resolve jar file for "" + jarClass + "". URL "" + location, e);
-            }
+    List<String> list = getJars(jarClass, true, false);
+    if (list == null) {
+      return null;
+    } else  if (list.isEmpty()) {
+      throw new AssertionError(""Cannot resolve jar file for "" + jarClass);
+    }
+    return list.get(0);
+  }
+
+  /**
+   * Returns a full path to the jar-file that contains the given class and all full paths to dependent jar-files
+   * that are defined in the property ""apex-dependencies"" of the manifest of the root jar-file.
+   * If the class is an independent file the method makes jar file from the folder that contains the class
+   * @param classPath Class path
+   * @param makeJarFromFolder True if the method should make jar from folder that contains the independent class
+   * @param addJarDependencies True if the method should include dependent jar files
+   * @return List of names of the jar-files
+   */
+  public List<String> getJars(String classPath, boolean makeJarFromFolder, boolean addJarDependencies)
+  {
+    try {
+      return getJars(Thread.currentThread()
+          .getContextClassLoader().loadClass(classPath), makeJarFromFolder, addJarDependencies);
+    } catch (ClassNotFoundException ex) {
+      logger.error(""Cannot find the class {}"", classPath, ex);
+      throw new RuntimeException(""Cannot find the class "" + classPath, ex);
+    }
+  }
+
+  /**
+   * Returns a full path to the jar-file that contains the given class and all full paths to dependent jar-files
+   * that are defined in the property ""apex-dependencies"" of the manifest of the root jar-file.
+   * If the class is an independent file the method makes jar file from the folder that contains the class
+   * @param clazz Class
+   * @param makeJarFromFolder True if the method should make jar from folder that contains the independent class
+   * @param addJarDependencies True if the method should include dependent jar files
+   * @return List of names of the jar-files
+   */
+  public List<String> getJars(Class<?> clazz, boolean makeJarFromFolder, boolean addJarDependencies)
+  {
+    CodeSource codeSource = clazz.getProtectionDomain().getCodeSource();","[{'comment': 'Please keep the original code unless there is a reason to change it.', 'commenter': 'vrozov'}, {'comment': 'Done.', 'commenter': 'sgolovko'}]"
555,engine/src/main/java/com/datatorrent/stram/StramClient.java,"@@ -223,8 +223,14 @@ public void stop()
       localJarFiles.addAll(Arrays.asList(libJars));
     }
 
-    LOG.info(""Local jar file dependencies: "" + localJarFiles);
+    String pluginClassesPaths = conf.get(StreamingAppMasterService.PLUGINS_CONF_KEY);
+    if (!StringUtils.isEmpty(pluginClassesPaths)) {
+      for (String pluginClassPath : StringUtils.splitByWholeSeparator(pluginClassesPaths, StreamingAppMasterService.PLUGINS_CONF_SEP)) {
+        localJarFiles.addAll(jarHelper.getJars(pluginClassPath, true, true));
+      }
+    }
 
+    LOG.info(""Local jar file dependencies: "" + localJarFiles);","[{'comment': 'Use sl4j logging', 'commenter': 'vrozov'}, {'comment': ""I didn't update this line. But anyway it is fixed."", 'commenter': 'sgolovko'}]"
555,common/src/main/java/org/apache/apex/common/util/JarHelper.java,"@@ -70,41 +74,108 @@ public static String createJar(String prefix, File dir, boolean deleteOnExit) th
 
   public String getJar(Class<?> jarClass)
   {
-    String jar = null;
-    final CodeSource codeSource = jarClass.getProtectionDomain().getCodeSource();
-    if (codeSource != null) {
-      URL location = codeSource.getLocation();
-      jar = sourceToJar.get(location);
-      if (jar == null) {
-        // don't create jar file from folders multiple times
-        if (""jar"".equals(location.getProtocol())) {
-          try {
-            location = ((JarURLConnection)location.openConnection()).getJarFileURL();
-          } catch (IOException e) {
-            throw new AssertionError(""Cannot resolve jar file for "" + jarClass, e);
-          }
-        }
-        if (""file"".equals(location.getProtocol())) {
-          jar = location.getFile();
-          final File dir = new File(jar);
-          if (dir.isDirectory()) {
-            try {
-              jar = createJar(""apex-"", dir, false);
-            } catch (IOException e) {
-              throw new AssertionError(""Cannot resolve jar file for "" + jarClass + "". URL "" + location, e);
-            }
+    List<String> list = getJars(jarClass, true, false);
+    if (list == null) {
+      return null;
+    } else  if (list.isEmpty()) {
+      throw new AssertionError(""Cannot resolve jar file for "" + jarClass);
+    }
+    return list.get(0);
+  }
+
+  /**
+   * Returns a full path to the jar-file that contains the given class and all full paths to dependent jar-files
+   * that are defined in the property ""apex-dependencies"" of the manifest of the root jar-file.
+   * If the class is an independent file the method makes jar file from the folder that contains the class
+   * @param classPath Class path
+   * @param makeJarFromFolder True if the method should make jar from folder that contains the independent class
+   * @param addJarDependencies True if the method should include dependent jar files
+   * @return List of names of the jar-files
+   */
+  public List<String> getJars(String classPath, boolean makeJarFromFolder, boolean addJarDependencies)","[{'comment': 'Finding a class by its name is not the functionality provided by getJars(). It should be outside of getJars().', 'commenter': 'vrozov'}, {'comment': 'Done.', 'commenter': 'sgolovko'}]"
555,engine/src/main/java/com/datatorrent/stram/StramClient.java,"@@ -223,8 +223,14 @@ public void stop()
       localJarFiles.addAll(Arrays.asList(libJars));
     }
 
-    LOG.info(""Local jar file dependencies: "" + localJarFiles);
+    String pluginClassesPaths = conf.get(StreamingAppMasterService.PLUGINS_CONF_KEY);
+    if (!StringUtils.isEmpty(pluginClassesPaths)) {
+      for (String pluginClassPath : StringUtils.splitByWholeSeparator(pluginClassesPaths, StreamingAppMasterService.PLUGINS_CONF_SEP)) {
+        localJarFiles.addAll(jarHelper.getJars(pluginClassPath, true, true));","[{'comment': 'There must not be any difference in behavior between plugin classes and any other classes. In all cases it should use getJars(class, true, true) and please make it the default behavior.', 'commenter': 'vrozov'}, {'comment': 'Done.', 'commenter': 'sgolovko'}]"
555,common/src/main/java/org/apache/apex/common/util/JarHelper.java,"@@ -70,41 +74,108 @@ public static String createJar(String prefix, File dir, boolean deleteOnExit) th
 
   public String getJar(Class<?> jarClass)
   {
-    String jar = null;
-    final CodeSource codeSource = jarClass.getProtectionDomain().getCodeSource();
-    if (codeSource != null) {
-      URL location = codeSource.getLocation();
-      jar = sourceToJar.get(location);
-      if (jar == null) {
-        // don't create jar file from folders multiple times
-        if (""jar"".equals(location.getProtocol())) {
-          try {
-            location = ((JarURLConnection)location.openConnection()).getJarFileURL();
-          } catch (IOException e) {
-            throw new AssertionError(""Cannot resolve jar file for "" + jarClass, e);
-          }
-        }
-        if (""file"".equals(location.getProtocol())) {
-          jar = location.getFile();
-          final File dir = new File(jar);
-          if (dir.isDirectory()) {
-            try {
-              jar = createJar(""apex-"", dir, false);
-            } catch (IOException e) {
-              throw new AssertionError(""Cannot resolve jar file for "" + jarClass + "". URL "" + location, e);
-            }
+    List<String> list = getJars(jarClass, true, false);
+    if (list == null) {
+      return null;
+    } else  if (list.isEmpty()) {
+      throw new AssertionError(""Cannot resolve jar file for "" + jarClass);
+    }
+    return list.get(0);
+  }
+
+  /**
+   * Returns a full path to the jar-file that contains the given class and all full paths to dependent jar-files
+   * that are defined in the property ""apex-dependencies"" of the manifest of the root jar-file.
+   * If the class is an independent file the method makes jar file from the folder that contains the class
+   * @param classPath Class path
+   * @param makeJarFromFolder True if the method should make jar from folder that contains the independent class
+   * @param addJarDependencies True if the method should include dependent jar files
+   * @return List of names of the jar-files
+   */
+  public List<String> getJars(String classPath, boolean makeJarFromFolder, boolean addJarDependencies)
+  {
+    try {
+      return getJars(Thread.currentThread()
+          .getContextClassLoader().loadClass(classPath), makeJarFromFolder, addJarDependencies);
+    } catch (ClassNotFoundException ex) {
+      logger.error(""Cannot find the class {}"", classPath, ex);
+      throw new RuntimeException(""Cannot find the class "" + classPath, ex);
+    }
+  }
+
+  /**
+   * Returns a full path to the jar-file that contains the given class and all full paths to dependent jar-files
+   * that are defined in the property ""apex-dependencies"" of the manifest of the root jar-file.
+   * If the class is an independent file the method makes jar file from the folder that contains the class
+   * @param clazz Class
+   * @param makeJarFromFolder True if the method should make jar from folder that contains the independent class
+   * @param addJarDependencies True if the method should include dependent jar files
+   * @return List of names of the jar-files
+   */
+  public List<String> getJars(Class<?> clazz, boolean makeJarFromFolder, boolean addJarDependencies)
+  {
+    CodeSource codeSource = clazz.getProtectionDomain().getCodeSource();
+    if (codeSource == null) {
+      return null;
+    }
+
+    List<String> list = new LinkedList<>();
+    URL location  =  codeSource.getLocation();
+    String jarPath = sourceToJar.get(location);
+
+    if (jarPath == null) {
+      try {
+        URLConnection conn = clazz.getResource(clazz.getSimpleName() + "".class"").openConnection();
+
+        if (conn instanceof JarURLConnection) {
+          jarPath = ((JarURLConnection)conn).getJarFileURL().getFile();
+          if (addJarDependencies) {
+            getDependentJarsFromManifest((JarURLConnection)conn, jarPath, list);
+            // add the location of the jar-file to cache only if the dependent jars were added to the jar list
+            sourceToJar.put(location, jarPath);
           }
+        } else if (makeJarFromFolder && (""file"".equals(location.getProtocol()))) {
+          jarPath = createJar(""apex-"", new File(conn.getURL().getFile()).getParentFile(), false);
+          sourceToJar.put(location, jarPath);
         } else {
-          throw new AssertionError(""Cannot resolve jar file for "" + jarClass + "". URL "" + location);
+          logger.warn(""The class {} was loaded from incorrect url connection {}"", clazz.getCanonicalName(), conn);
+          return list;
         }
-        sourceToJar.put(location, jar);
-        logger.debug(""added sourceLocation {} as {}"", location, jar);
+      } catch (IOException ex) {
+        String className = clazz.getCanonicalName();
+        logger.error(""Cannot resolve jar file for the class {}"", className, ex);
+        throw new RuntimeException(""Cannot resolve jar file for the class "" + className, ex);
       }
-      if (jar == null) {
-        throw new AssertionError(""Cannot resolve jar file for "" + jarClass);
+    }
+
+    list.add(jarPath);
+    logger.debug(""added sourceLocation {} as {}"", location, jarPath);
+
+    return list;
+  }
+
+  /**
+   * Gets dependent jar-files from manifest
+   * @param conn Jar connection
+   * @param jarPath path to jar file
+   * @param list List of target jar-files
+   * @throws IOException
+   */
+  private void getDependentJarsFromManifest(JarURLConnection conn, String jarPath, List<String> list) throws IOException
+  {
+    String value = ((JarURLConnection)conn).getMainAttributes().getValue(""apex-dependencies"");","[{'comment': 'define ""apex-dependencies"" as public static final String.\r\ncast to (JarURLConnection) is not necessary\r\n', 'commenter': 'vrozov'}, {'comment': 'Done.', 'commenter': 'sgolovko'}]"
555,common/src/main/java/org/apache/apex/common/util/JarHelper.java,"@@ -70,41 +74,108 @@ public static String createJar(String prefix, File dir, boolean deleteOnExit) th
 
   public String getJar(Class<?> jarClass)
   {
-    String jar = null;
-    final CodeSource codeSource = jarClass.getProtectionDomain().getCodeSource();
-    if (codeSource != null) {
-      URL location = codeSource.getLocation();
-      jar = sourceToJar.get(location);
-      if (jar == null) {
-        // don't create jar file from folders multiple times
-        if (""jar"".equals(location.getProtocol())) {
-          try {
-            location = ((JarURLConnection)location.openConnection()).getJarFileURL();
-          } catch (IOException e) {
-            throw new AssertionError(""Cannot resolve jar file for "" + jarClass, e);
-          }
-        }
-        if (""file"".equals(location.getProtocol())) {
-          jar = location.getFile();
-          final File dir = new File(jar);
-          if (dir.isDirectory()) {
-            try {
-              jar = createJar(""apex-"", dir, false);
-            } catch (IOException e) {
-              throw new AssertionError(""Cannot resolve jar file for "" + jarClass + "". URL "" + location, e);
-            }
+    List<String> list = getJars(jarClass, true, false);
+    if (list == null) {
+      return null;
+    } else  if (list.isEmpty()) {
+      throw new AssertionError(""Cannot resolve jar file for "" + jarClass);
+    }
+    return list.get(0);
+  }
+
+  /**
+   * Returns a full path to the jar-file that contains the given class and all full paths to dependent jar-files
+   * that are defined in the property ""apex-dependencies"" of the manifest of the root jar-file.
+   * If the class is an independent file the method makes jar file from the folder that contains the class
+   * @param classPath Class path
+   * @param makeJarFromFolder True if the method should make jar from folder that contains the independent class
+   * @param addJarDependencies True if the method should include dependent jar files
+   * @return List of names of the jar-files
+   */
+  public List<String> getJars(String classPath, boolean makeJarFromFolder, boolean addJarDependencies)
+  {
+    try {
+      return getJars(Thread.currentThread()
+          .getContextClassLoader().loadClass(classPath), makeJarFromFolder, addJarDependencies);
+    } catch (ClassNotFoundException ex) {
+      logger.error(""Cannot find the class {}"", classPath, ex);
+      throw new RuntimeException(""Cannot find the class "" + classPath, ex);
+    }
+  }
+
+  /**
+   * Returns a full path to the jar-file that contains the given class and all full paths to dependent jar-files
+   * that are defined in the property ""apex-dependencies"" of the manifest of the root jar-file.
+   * If the class is an independent file the method makes jar file from the folder that contains the class
+   * @param clazz Class
+   * @param makeJarFromFolder True if the method should make jar from folder that contains the independent class
+   * @param addJarDependencies True if the method should include dependent jar files
+   * @return List of names of the jar-files
+   */
+  public List<String> getJars(Class<?> clazz, boolean makeJarFromFolder, boolean addJarDependencies)
+  {
+    CodeSource codeSource = clazz.getProtectionDomain().getCodeSource();
+    if (codeSource == null) {
+      return null;
+    }
+
+    List<String> list = new LinkedList<>();
+    URL location  =  codeSource.getLocation();
+    String jarPath = sourceToJar.get(location);
+
+    if (jarPath == null) {
+      try {
+        URLConnection conn = clazz.getResource(clazz.getSimpleName() + "".class"").openConnection();
+
+        if (conn instanceof JarURLConnection) {
+          jarPath = ((JarURLConnection)conn).getJarFileURL().getFile();
+          if (addJarDependencies) {
+            getDependentJarsFromManifest((JarURLConnection)conn, jarPath, list);
+            // add the location of the jar-file to cache only if the dependent jars were added to the jar list
+            sourceToJar.put(location, jarPath);
           }
+        } else if (makeJarFromFolder && (""file"".equals(location.getProtocol()))) {
+          jarPath = createJar(""apex-"", new File(conn.getURL().getFile()).getParentFile(), false);
+          sourceToJar.put(location, jarPath);
         } else {
-          throw new AssertionError(""Cannot resolve jar file for "" + jarClass + "". URL "" + location);
+          logger.warn(""The class {} was loaded from incorrect url connection {}"", clazz.getCanonicalName(), conn);
+          return list;
         }
-        sourceToJar.put(location, jar);
-        logger.debug(""added sourceLocation {} as {}"", location, jar);
+      } catch (IOException ex) {
+        String className = clazz.getCanonicalName();
+        logger.error(""Cannot resolve jar file for the class {}"", className, ex);
+        throw new RuntimeException(""Cannot resolve jar file for the class "" + className, ex);
       }
-      if (jar == null) {
-        throw new AssertionError(""Cannot resolve jar file for "" + jarClass);
+    }
+
+    list.add(jarPath);
+    logger.debug(""added sourceLocation {} as {}"", location, jarPath);
+
+    return list;
+  }
+
+  /**
+   * Gets dependent jar-files from manifest
+   * @param conn Jar connection
+   * @param jarPath path to jar file
+   * @param list List of target jar-files
+   * @throws IOException
+   */
+  private void getDependentJarsFromManifest(JarURLConnection conn, String jarPath, List<String> list) throws IOException","[{'comment': 'Please consider using JarFile instead of JarURLConnection\r\nI would expect the getDependentJarsFromManifest to return a list of the dependent jars, instead of accepting a list.', 'commenter': 'vrozov'}, {'comment': 'Done.', 'commenter': 'sgolovko'}]"
555,common/src/main/java/org/apache/apex/common/util/JarHelper.java,"@@ -107,6 +114,73 @@ public String getJar(Class<?> jarClass)
     return jar;
   }
 
+  public String getJar(Class<?> jarClass)
+  {
+    return getJar(jarClass, true);
+  }
+
+  /**
+   * Returns a full path to the jar-file that contains the given class and all full paths to dependent jar-files
+   * that are defined in the property ""apex-dependencies"" of the manifest of the root jar-file.
+   * If the class is an independent file the method makes jar file from the folder that contains the class
+   * @param jarClass Class
+   * @param makeJarFromFolder True if the method should make jar from folder that contains the independent class
+   * @param addJarDependencies True if the method should include dependent jar files
+   * @return List of names of the jar-files
+   */
+  public List<String> getJars(Class<?> jarClass, boolean makeJarFromFolder, boolean addJarDependencies)","[{'comment': 'Should it be a Set? Duplicates should not be allowed.', 'commenter': 'vrozov'}]"
558,engine/src/main/java/com/datatorrent/stram/debug/TupleRecorder.java,"@@ -83,27 +85,9 @@
   // If there are errors processing tuples, don't log an error for every tuple as it could overwhelm the logs.
   // The property specifies the minumum number of tuples between two consecutive error log statements. Set it to zero to
   // log every tuple error
-  private static long ERROR_LOG_GAP;
+  private static long ERROR_LOG_GAP = SystemPropertyReader.getLong(""org.apache.apex.stram.tupleRecorder.errorLogGap"", 10000L, 0, Integer.MAX_VALUE);","[{'comment': 'Because of using maxValue of Integer.MAX_VALUE the behavior now has changed. The old code only disallowed  values < 0 but with this even values > Integer.MAX_VALUE are disallowed. It may be intentional and a good check to have (but I doubt that is the case for the error-log-gap number)', 'commenter': 'sanjaypujare'}, {'comment': 'Will make it Long.MAX_VALUE', 'commenter': 'sandeshh'}]"
558,common/src/main/java/org/apache/apex/common/util/SystemPropertyReader.java,"@@ -0,0 +1,48 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.apex.common.util;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class SystemPropertyReader
+{
+  public static long getLong(String propertyName, long defaultValue, long minValue, long maxValue)
+  {
+    String property = System.getProperty(propertyName);
+    long result = defaultValue;
+    if (property != null) {
+      try {
+        long value = Long.decode(property);
+        if (value <= minValue || value >= maxValue) {","[{'comment': 'This has introduced a regression. Caller calls with minValue of 0 which is disallowed because of `value <= minValue`. \r\n\r\nIn any case, since we are creating a Util class/method we should have Javadocs to describe the method and params and in this case whether the min/max values are inclusive/exclusive.', 'commenter': 'sanjaypujare'}, {'comment': 'adding Javadoc and make the range as inclusive.', 'commenter': 'sandeshh'}, {'comment': 'Would removing the equality from the condition solve this', 'commenter': 'pramodin'}, {'comment': '@PramodSSImmaneni yes, I have made that change.', 'commenter': 'sandeshh'}]"
558,engine/src/main/java/com/datatorrent/stram/debug/TupleRecorder.java,"@@ -83,27 +85,9 @@
   // If there are errors processing tuples, don't log an error for every tuple as it could overwhelm the logs.
   // The property specifies the minumum number of tuples between two consecutive error log statements. Set it to zero to
   // log every tuple error","[{'comment': ""With this code change `0` won't actually work, right?"", 'commenter': 'sanjaypujare'}, {'comment': 'Addressed it.', 'commenter': 'sandeshh'}]"
558,common/src/main/java/org/apache/apex/common/util/SystemPropertyReader.java,"@@ -0,0 +1,48 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.apex.common.util;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class SystemPropertyReader","[{'comment': ""annotate with `@Evolving`. For future extensions, I'd recommend naming the class more generically so we can add more System utility methods later if necessary."", 'commenter': 'vrozov'}, {'comment': 'ok', 'commenter': 'sandeshh'}]"
558,common/src/main/java/org/apache/apex/common/util/SystemPropertyReader.java,"@@ -0,0 +1,56 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.apex.common.util;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class SystemPropertyReader
+{
+  /**
+   * Reading system property as long value.
+   * @param propertyName Name of the system property
+   * @param defaultValue Default value to return in case of an error, out of range etc.
+   * @param minValue minimum valid value
+   * @param maxValue maximum valid value
+   * @return returns the value if it is between min and max value(inclusive), otherwise default value is returned.
+   */
+  public static long getLong(String propertyName, long defaultValue, long minValue, long maxValue)
+  {
+    String property = System.getProperty(propertyName);
+    long result = defaultValue;
+    if (property != null) {
+      try {
+        long value = Long.decode(property);
+        if (value < minValue || value > maxValue) {
+          logger.warn(""Property {} is outside the range, setting to default"", propertyName);","[{'comment': '""Property {} value {} is outside the range [{},{}], setting to default {}""?', 'commenter': 'vrozov'}, {'comment': 'Done.', 'commenter': 'sandeshh'}]"
558,common/src/main/java/org/apache/apex/common/util/Helper.java,"@@ -0,0 +1,59 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.apex.common.util;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import org.apache.hadoop.classification.InterfaceStability;","[{'comment': ""optionally 'import org.apache.hadoop.classification.InterfaceStability.Evolving;'"", 'commenter': 'vrozov'}, {'comment': 'Done.', 'commenter': 'sandeshh'}]"
558,common/src/main/java/org/apache/apex/common/util/SystemHelper.java,"@@ -0,0 +1,57 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.apex.common.util;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+@org.apache.hadoop.classification.InterfaceStability.Evolving","[{'comment': 'Please use (static) import. This is the least readable and reusable notation.', 'commenter': 'vrozov'}, {'comment': 'done.', 'commenter': 'sandeshh'}]"
558,common/src/main/java/org/apache/apex/common/util/SystemHelper.java,"@@ -0,0 +1,58 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.apex.common.util;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import org.apache.hadoop.classification.InterfaceStability.Evolving;
+
+@Evolving
+public class SystemHelper
+{
+  /**
+   * Reading system property as long value.
+   * @param propertyName Name of the system property
+   * @param defaultValue Default value to return in case of an error, out of range etc.
+   * @param minValue minimum valid value
+   * @param maxValue maximum valid value
+   * @return returns the value if it is between min and max value(inclusive), otherwise default value is returned.
+   */
+  public static long getSystemPropertyAsLong(String propertyName, long defaultValue, long minValue, long maxValue)
+  {
+    String property = System.getProperty(propertyName);
+    long result = defaultValue;
+    if (property != null) {
+      try {
+        long value = Long.decode(property);
+        if (value < minValue || value > maxValue) {
+          logger.warn(""Property {} is outside the range [{},{}], setting to default {}"", propertyName, minValue, maxValue, defaultValue);
+        } else {
+          result = value;
+        }
+      } catch (Exception ex) {
+        logger.warn(""Unable to parse the property {}, because of the exception {}, setting to default {}"", propertyName, ex, defaultValue);
+      }
+    }
+    logger.debug(""System property {}'s value is {}"", propertyName, result);","[{'comment': 'why does this need logging?', 'commenter': 'tweise'}, {'comment': 'This code is more of refactoring, logging was already present.', 'commenter': 'sandeshh'}]"
558,common/src/main/java/org/apache/apex/common/util/SystemHelper.java,"@@ -0,0 +1,58 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.apex.common.util;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import org.apache.hadoop.classification.InterfaceStability.Evolving;
+
+@Evolving
+public class SystemHelper
+{
+  /**
+   * Reading system property as long value.
+   * @param propertyName Name of the system property
+   * @param defaultValue Default value to return in case of an error, out of range etc.
+   * @param minValue minimum valid value
+   * @param maxValue maximum valid value
+   * @return returns the value if it is between min and max value(inclusive), otherwise default value is returned.
+   */
+  public static long getSystemPropertyAsLong(String propertyName, long defaultValue, long minValue, long maxValue)
+  {
+    String property = System.getProperty(propertyName);
+    long result = defaultValue;
+    if (property != null) {
+      try {
+        long value = Long.decode(property);
+        if (value < minValue || value > maxValue) {
+          logger.warn(""Property {} is outside the range [{},{}], setting to default {}"", propertyName, minValue, maxValue, defaultValue);
+        } else {
+          result = value;
+        }
+      } catch (Exception ex) {
+        logger.warn(""Unable to parse the property {}, because of the exception {}, setting to default {}"", propertyName, ex, defaultValue);","[{'comment': 'logger.warn(""Can\'t convert property {} value {} to a long, using default {}"", property Name, property, defaultValue, ex);', 'commenter': 'vrozov'}, {'comment': 'Done.', 'commenter': 'sandeshh'}]"
558,common/src/main/java/org/apache/apex/common/util/SystemHelper.java,"@@ -0,0 +1,58 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.apex.common.util;","[{'comment': 'Do we envision the class to be used by users? If it is for internal use, can we find a better solution for this. Can this, for example go into bufferserver or worst case a separate module for internal common.', 'commenter': 'pramodin'}, {'comment': '@sandeshh Any thoughts on this? Any comments from other reviewers?', 'commenter': 'pramodin'}, {'comment': 'I think this looks useful enough to be usable by (external) users so I am okay with the current package and the class name of PropertiesHelper', 'commenter': 'sanjaypujare'}, {'comment': ""Bufferserver doesn't have a use for this method. I am also in favor of leaving it as is. "", 'commenter': 'sandeshh'}, {'comment': 'If it is only needed in engine, then why is it not there? Is it useful for Apex users? In general we should leave api/common confined to things that users realistically need.', 'commenter': 'tweise'}, {'comment': 'It may be useful for other classes in common or buffer server. There are already classes like NameableThreadFactory in common that are used both by the engine and the buffer server. ', 'commenter': 'vrozov'}]"
558,common/src/main/java/org/apache/apex/common/util/SystemHelper.java,"@@ -0,0 +1,58 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.apex.common.util;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import org.apache.hadoop.classification.InterfaceStability.Evolving;
+
+@Evolving
+public class SystemHelper","[{'comment': 'I would suggest going with a more restrictive scope like PropertiesHelper unless there are other System helper utilities you have in mind.', 'commenter': 'pramodin'}, {'comment': 'Done.', 'commenter': 'sandeshh'}]"
558,common/src/main/java/org/apache/apex/common/util/PropertiesHelper.java,"@@ -0,0 +1,58 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.apex.common.util;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import org.apache.hadoop.classification.InterfaceStability.Evolving;
+
+@Evolving
+public class PropertiesHelper
+{
+  /**
+   * Reading system property as long value.
+   * @param propertyName Name of the system property
+   * @param defaultValue Default value to return in case of an error, out of range etc.
+   * @param minValue minimum valid value
+   * @param maxValue maximum valid value
+   * @return returns the value if it is between min and max value(inclusive), otherwise default value is returned.
+   */
+  public static long getSystemPropertyAsLong(String propertyName, long defaultValue, long minValue, long maxValue)","[{'comment': 'With the reduced scope of the class (rename to PropertiesHelper), can this method be renamed to getLong() as well?', 'commenter': 'vrozov'}, {'comment': '+1 to renaming getLong() or preferably getAsLong() so the usage will be\r\n\r\n```\r\nPropertiesHelper.getAsLong(propertyName, ....)\r\n```\r\n\r\nwhich looks nice.\r\n', 'commenter': 'sanjaypujare'}]"
564,engine/src/main/java/com/datatorrent/stram/cli/ApexCli.java,"@@ -2178,6 +2178,19 @@ public void execute(String[] args, ConsoleReader reader) throws Exception
 
   }
 
+  private ApplicationReport findApplicationReportFromAppNameOrId(String appNameOrId)
+  {
+    ApplicationReport app = null;
+
+    if (getApplication(appNameOrId) != null) {","[{'comment': 'app = getApplication(appNameOrId)\r\nif (app == null) {\r\n     app = getApplicationByName(appNameOrId)\r\n}\r\n\r\nTo avoid calling getApplication and getApplicationByName twice.', 'commenter': 'tushargosavi'}]"
564,engine/src/main/java/com/datatorrent/stram/cli/ApexCli.java,"@@ -2178,6 +2178,19 @@ public void execute(String[] args, ConsoleReader reader) throws Exception
 
   }
 
+  private ApplicationReport findApplicationReportFromAppNameOrId(String appNameOrId)","[{'comment': 'this function could be used in KillAppCommand#execute too', 'commenter': 'tushargosavi'}]"
564,engine/src/main/java/com/datatorrent/stram/cli/ApexCli.java,"@@ -738,7 +738,7 @@ void printUsage(String cmd)
         ""Kill a container""));
     connectedCommands.put(""shutdown-app"", new CommandSpec(new ShutdownAppCommand(),
         null,
-        new Arg[]{new VarArg(""app-id"")},
+        new Arg[]{new VarArg(""app-id/app-name"")},","[{'comment': 'Similar change is needed at\r\n     globalCommands.put(""shutdown-app"", new CommandSpec(new ShutdownAppCommand(),\r\n         new Arg[]{new Arg(""app-id"")},\r\n         new Arg[]{new VarArg(""app-id"")},\r\n\r\nline number 634', 'commenter': 'tushargosavi'}]"
564,engine/src/main/java/com/datatorrent/stram/cli/ApexCli.java,"@@ -2178,6 +2178,15 @@ public void execute(String[] args, ConsoleReader reader) throws Exception
 
   }
 
+  private ApplicationReport findApplicationReportFromAppNameOrId(String appNameOrId)
+  {
+    ApplicationReport app = getApplication(appNameOrId);
+    if (app == null) {
+      app = getApplicationByName(appNameOrId);
+    }
+    return app;","[{'comment': 'All the callers throw the identical exception when the returned app is null. Can we just throw that exception here and the caller can always assume app is non-null?\r\n\r\nThinking further, the current behavior - even with my proposed suggestion - fails the whole command if a single app-id/app-name is not found. Do we want to keep that behavior or modify it to only ignore the invalid app-id/app-name', 'commenter': 'sanjaypujare'}, {'comment': 'The behavior seems to be inconsistent between different CL tools. I just checked  `docker start valid_container_name invalid_container_name`: this would start valid_container but not invalid_container. Then again when using `apt-get install valid_package_name invalid_package_name` it just fails completely, installing neither of those packages. \r\n', 'commenter': 'florianschmidt1994'}, {'comment': 'Making `findApplicationReportFromAppNameOrId()` throw a CLIException could \r\n- on one hand clean up the code so we don\'t need to rewrite it every time\r\n- on the other hand it would make it inconsistent with getApplication and getApplicationByName, where no exception is thrown. \r\n\r\nAlso if we leave the exception out of the method, we would still have the possibility of implementing the logic for ""fail all vs. fail only for invalid appName"" in the respective commands, which we do not have if we move it into the method', 'commenter': 'florianschmidt1994'}, {'comment': 'Ok', 'commenter': 'sanjaypujare'}, {'comment': 'IMO, it should be the best effort attempt. If application id or name is not found, the error needs to be reported, but it is necessary to continue to the next id/name. At the end, command should report all applications that were shutdown.', 'commenter': 'vrozov'}]"
564,engine/src/main/java/com/datatorrent/stram/cli/ApexCli.java,"@@ -2193,9 +2202,11 @@ public void execute(String[] args, ConsoleReader reader) throws Exception
       } else {
         apps = new ApplicationReport[args.length - 1];
         for (int i = 1; i < args.length; i++) {
-          apps[i - 1] = getApplication(args[i]);
-          if (apps[i - 1] == null) {
-            throw new CliException(""Streaming application with id "" + args[i] + "" is not found."");
+          ApplicationReport ap = findApplicationReportFromAppNameOrId(args[i]);
+          if (ap == null) {
+            throw new CliException(""Streaming application with id or name "" + args[i] + "" is not found."");","[{'comment': 'Pls see my comment above about failing the command for the whole list when a single app is not found.', 'commenter': 'sanjaypujare'}]"
564,engine/src/main/java/com/datatorrent/stram/cli/ApexCli.java,"@@ -632,8 +632,8 @@ void printUsage(String cmd)
         new Arg[]{new FileArg(""jar-file/json-file/properties-file/app-package-file-path/app-package-file-uri""), new Arg(""matching-app-name"")},
         ""Launch an app"", LAUNCH_OPTIONS.options));
     globalCommands.put(""shutdown-app"", new CommandSpec(new ShutdownAppCommand(),
-        new Arg[]{new Arg(""app-id"")},
-        new Arg[]{new VarArg(""app-id"")},
+        new Arg[]{new Arg(""app-id/app-name"")},
+        new Arg[]{new VarArg(""app-id/app-name"")},
         ""Shutdown an app""));","[{'comment': '=> ""Shutdown app or apps""\r\n', 'commenter': 'sanjaypujare'}]"
564,engine/src/main/java/com/datatorrent/stram/cli/ApexCli.java,"@@ -632,9 +632,9 @@ void printUsage(String cmd)
         new Arg[]{new FileArg(""jar-file/json-file/properties-file/app-package-file-path/app-package-file-uri""), new Arg(""matching-app-name"")},
         ""Launch an app"", LAUNCH_OPTIONS.options));
     globalCommands.put(""shutdown-app"", new CommandSpec(new ShutdownAppCommand(),
-        new Arg[]{new Arg(""app-id"")},
-        new Arg[]{new VarArg(""app-id"")},
-        ""Shutdown an app""));
+        new Arg[]{new Arg(""app-id/app-name"")},
+        new Arg[]{new VarArg(""app-id/app-name"")},
+        ""Shutdown app or apps""));","[{'comment': 'Shutdown application(s) by id or name?', 'commenter': 'vrozov'}]"
564,engine/src/main/java/com/datatorrent/stram/cli/ApexCli.java,"@@ -2178,49 +2180,80 @@ public void execute(String[] args, ConsoleReader reader) throws Exception
 
   }
 
+  private ApplicationReport findApplicationReportFromAppNameOrId(String appNameOrId)
+  {
+    ApplicationReport app = getApplication(appNameOrId);
+    if (app == null) {
+      app = getApplicationByName(appNameOrId);
+    }
+    return app;
+  }
+
   private class ShutdownAppCommand implements Command
   {
     @Override
     public void execute(String[] args, ConsoleReader reader) throws Exception
     {
-      ApplicationReport[] apps;
+      Map<String, ApplicationReport> appIdReports = new LinkedHashMap<>();
+
       if (args.length == 1) {
         if (currentApp == null) {
           throw new CliException(""No application selected"");
         } else {
-          apps = new ApplicationReport[]{currentApp};
+          appIdReports.put(currentApp.getApplicationId().toString(), currentApp);
         }
       } else {
-        apps = new ApplicationReport[args.length - 1];
-        for (int i = 1; i < args.length; i++) {
-          apps[i - 1] = getApplication(args[i]);
-          if (apps[i - 1] == null) {
-            throw new CliException(""Streaming application with id "" + args[i] + "" is not found."");
-          }
+        String[] appNamesOrIds = Arrays.copyOfRange(args, 1, args.length);","[{'comment': 'Do we want to check for duplicates? ', 'commenter': 'sanjaypujare'}, {'comment': ""I'd vote for going with the way it currently behaves, which is shutting down an app if it is found, and if it is specified as argument twice: printing an error message on the second time, because the app is already shut down. "", 'commenter': 'florianschmidt1994'}, {'comment': 'ok', 'commenter': 'sanjaypujare'}]"
564,engine/src/main/java/com/datatorrent/stram/cli/ApexCli.java,"@@ -2324,17 +2357,11 @@ public void execute(String[] args, ConsoleReader reader) throws Exception
       int i = 0;
       try {
         while (++i < args.length) {
-          app = getApplication(args[i]);
+          app = findApplicationReportFromAppNameOrId(args[i]);
           if (app == null) {
-
-            /*
-             * try once again with application name type.
-             */
-            app = getApplicationByName(args[i]);
-            if (app == null) {
-              throw new CliException(""Streaming application with id or name "" + args[i] + "" is not found."");
-            }
+            throw new CliException(""Streaming application with id or name "" + args[i] + "" is not found."");","[{'comment': 'Why do we throw exception and stop processing the kill command as opposed to what we decided to do for the shutdown command e.g. continue processing the remaining arguments? In that case we can also achieve some code reuse between shutdown and kill commands - building the app report list etc can be common code.', 'commenter': 'sanjaypujare'}]"
564,engine/src/main/java/com/datatorrent/stram/cli/ApexCli.java,"@@ -2324,17 +2357,11 @@ public void execute(String[] args, ConsoleReader reader) throws Exception
       int i = 0;
       try {
         while (++i < args.length) {
-          app = getApplication(args[i]);
+          app = findApplicationReportFromAppNameOrId(args[i]);
           if (app == null) {
-
-            /*
-             * try once again with application name type.
-             */
-            app = getApplicationByName(args[i]);
-            if (app == null) {
-              throw new CliException(""Streaming application with id or name "" + args[i] + "" is not found."");
-            }
+            throw new CliException(""Streaming application with id or name "" + args[i] + "" is not found."");
           }
+
           yarnClient.killApplication(app.getApplicationId());","[{'comment': 'Same as above - if an exception is thrown the processing stops.', 'commenter': 'sanjaypujare'}]"
564,engine/src/test/java/com/datatorrent/stram/cli/ApexCliShutdownCommandTest.java,"@@ -0,0 +1,142 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.stram.cli;
+
+import java.io.ByteArrayOutputStream;
+import java.io.PrintStream;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Set;
+
+import org.codehaus.jettison.json.JSONObject;
+import org.junit.Assert;
+import org.junit.Rule;
+import org.junit.Test;
+import org.mockito.Mockito;
+import org.powermock.core.classloader.annotations.PowerMockIgnore;
+import org.powermock.core.classloader.annotations.PrepareForTest;
+import org.powermock.modules.junit4.rule.PowerMockRule;
+import org.powermock.reflect.Whitebox;
+
+import org.apache.hadoop.yarn.api.records.ApplicationId;
+import org.apache.hadoop.yarn.api.records.ApplicationReport;
+import org.apache.hadoop.yarn.api.records.FinalApplicationStatus;
+import org.apache.hadoop.yarn.api.records.YarnApplicationState;
+import org.apache.hadoop.yarn.client.api.YarnClient;
+
+import com.datatorrent.stram.client.StramAgent;
+import com.datatorrent.stram.util.WebServicesClient;
+
+import jline.console.ConsoleReader;
+
+import static org.powermock.api.mockito.PowerMockito.mock;
+import static org.powermock.api.mockito.PowerMockito.when;
+import static org.powermock.api.support.membermodification.MemberMatcher.constructor;
+import static org.powermock.api.support.membermodification.MemberModifier.suppress;
+
+/**
+ *
+ */
+@PowerMockIgnore({
+    ""javax.net.ssl.*"",
+    ""org.apache.log4j.*""
+    })
+@PrepareForTest({YarnClient.class, ApexCli.class, StramAgent.class})
+public class ApexCliShutdownCommandTest
+{
+
+  @Rule
+  public PowerMockRule powerMockRule = new PowerMockRule();
+
+  private ApplicationReport mockRunningApplicationReport(String appId, String appName)
+  {
+    ApplicationReport app = mock(ApplicationReport.class);
+    ApplicationId applicationId = mock(ApplicationId.class);
+
+    when(applicationId.toString()).thenReturn(appId);
+    when(app.getApplicationId()).thenReturn(applicationId);
+
+    when(app.getName()).thenReturn(appName);
+
+    when(app.getYarnApplicationState()).thenReturn(YarnApplicationState.RUNNING);
+    when(app.getFinalApplicationStatus()).thenReturn(FinalApplicationStatus.UNDEFINED);
+
+    when(app.getTrackingUrl()).thenReturn(""http://example.com"");
+
+    return app;
+  }
+
+  @Test
+  public void shutdownAppCommandUsesBestEffortApproach() throws Exception
+  {
+    // Given a cli and two running apps
+    ApexCli cliUnderTest = new ApexCli();
+    StramAgent stramAgent = mock(StramAgent.class);
+    YarnClient yarnClient = mock(YarnClient.class);
+
+    Whitebox.setInternalState(cliUnderTest, ""stramAgent"", stramAgent);
+    Whitebox.setInternalState(cliUnderTest, ""yarnClient"", yarnClient);
+    Whitebox.setInternalState(cliUnderTest, ""consolePresent"", true);
+
+    suppress(constructor(WebServicesClient.class, new Class[0]));
+
+    List<ApplicationReport> runningApplications = new ArrayList<>();
+
+    ApplicationReport app1 = mockRunningApplicationReport(""application-id-1"", ""app1"");
+    ApplicationReport app2 = mockRunningApplicationReport(""application-id-2"", ""app2"");
+
+    runningApplications.add(app1);
+    runningApplications.add(app2);
+
+    when(yarnClient.getApplications(Mockito.any(Set.class))).thenReturn(runningApplications);
+    when(stramAgent.issueStramWebRequest(
+        Mockito.any(WebServicesClient.class),
+        Mockito.anyString(),
+        Mockito.any(StramAgent.StramUriSpec.class),
+        Mockito.any(WebServicesClient.WebServicesHandler.class)))
+        .thenReturn(new JSONObject());
+
+    final ByteArrayOutputStream stdOut = new ByteArrayOutputStream();
+    final ByteArrayOutputStream stdErr = new ByteArrayOutputStream();
+
+    PrintStream beforeOut = System.out;
+    PrintStream beforeErr = System.err;
+
+    System.setOut(new PrintStream(stdOut));
+    System.setErr(new PrintStream(stdErr));
+
+    // When processing the shutdown command for two valid and one invalid appNames
+    String shutdownAppsCommand = ""shutdown-app app1 notExisting app2"";
+    cliUnderTest.processLine(shutdownAppsCommand, new ConsoleReader(), true);
+","[{'comment': ""Wouldn't it be a good idea to flush the current System.out and System.err (or close them) to ensure you get complete/consistent output in your stdOut and stdErr?"", 'commenter': 'sanjaypujare'}, {'comment': 'New squashed version uses autoflush on PrintStream now', 'commenter': 'florianschmidt1994'}]"
564,engine/src/test/java/com/datatorrent/stram/cli/ApexCliShutdownCommandTest.java,"@@ -0,0 +1,142 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.stram.cli;
+
+import java.io.ByteArrayOutputStream;
+import java.io.PrintStream;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Set;
+
+import org.codehaus.jettison.json.JSONObject;
+import org.junit.Assert;
+import org.junit.Rule;
+import org.junit.Test;
+import org.mockito.Mockito;
+import org.powermock.core.classloader.annotations.PowerMockIgnore;
+import org.powermock.core.classloader.annotations.PrepareForTest;
+import org.powermock.modules.junit4.rule.PowerMockRule;
+import org.powermock.reflect.Whitebox;
+
+import org.apache.hadoop.yarn.api.records.ApplicationId;
+import org.apache.hadoop.yarn.api.records.ApplicationReport;
+import org.apache.hadoop.yarn.api.records.FinalApplicationStatus;
+import org.apache.hadoop.yarn.api.records.YarnApplicationState;
+import org.apache.hadoop.yarn.client.api.YarnClient;
+
+import com.datatorrent.stram.client.StramAgent;
+import com.datatorrent.stram.util.WebServicesClient;
+
+import jline.console.ConsoleReader;
+
+import static org.powermock.api.mockito.PowerMockito.mock;
+import static org.powermock.api.mockito.PowerMockito.when;
+import static org.powermock.api.support.membermodification.MemberMatcher.constructor;
+import static org.powermock.api.support.membermodification.MemberModifier.suppress;
+
+/**
+ *
+ */
+@PowerMockIgnore({
+    ""javax.net.ssl.*"",
+    ""org.apache.log4j.*""
+    })
+@PrepareForTest({YarnClient.class, ApexCli.class, StramAgent.class})
+public class ApexCliShutdownCommandTest","[{'comment': 'How about adding a test for the kill command as well? You can add it here and rename the class .', 'commenter': 'sanjaypujare'}, {'comment': 'Added a comment below on how to go from here', 'commenter': 'florianschmidt1994'}]"
564,engine/src/main/java/com/datatorrent/stram/cli/ApexCli.java,"@@ -2178,49 +2180,85 @@ public void execute(String[] args, ConsoleReader reader) throws Exception
 
   }
 
+  private ApplicationReport findApplicationReportFromAppNameOrId(String appNameOrId)
+  {
+    ApplicationReport app = getApplication(appNameOrId);
+    if (app == null) {
+      app = getApplicationByName(appNameOrId);
+    }
+    return app;
+  }
+
   private class ShutdownAppCommand implements Command
   {
     @Override
     public void execute(String[] args, ConsoleReader reader) throws Exception
     {
-      ApplicationReport[] apps;
+      Map<String, ApplicationReport> appIdReports = new LinkedHashMap<>();
+
       if (args.length == 1) {
         if (currentApp == null) {
           throw new CliException(""No application selected"");
         } else {
-          apps = new ApplicationReport[]{currentApp};
+          appIdReports.put(currentApp.getApplicationId().toString(), currentApp);
         }
       } else {
-        apps = new ApplicationReport[args.length - 1];
-        for (int i = 1; i < args.length; i++) {
-          apps[i - 1] = getApplication(args[i]);
-          if (apps[i - 1] == null) {
-            throw new CliException(""Streaming application with id "" + args[i] + "" is not found."");
-          }
+        String[] appNamesOrIds = Arrays.copyOfRange(args, 1, args.length);
+
+        for (String appNameOrId : appNamesOrIds) {
+          ApplicationReport ap = findApplicationReportFromAppNameOrId(appNameOrId);
+          appIdReports.put(appNameOrId, ap);
         }
       }
 
-      for (ApplicationReport app : apps) {
-        try {
-          JSONObject response = getResource(new StramAgent.StramUriSpec().path(StramWebServices.PATH_SHUTDOWN), app, new WebServicesClient.WebServicesHandler<JSONObject>()
-          {
-            @Override
-            public JSONObject process(WebResource.Builder webResource, Class<JSONObject> clazz)
-            {
-              return webResource.accept(MediaType.APPLICATION_JSON).post(clazz, new JSONObject());
-            }
+      for (Map.Entry<String, ApplicationReport> entry : appIdReports.entrySet()) {
+        String appNameOrId = entry.getKey();
+        ApplicationReport app = entry.getValue();
 
-          });
-          if (consolePresent) {
-            System.out.println(""Shutdown requested: "" + response);
+        shutdownApp(appNameOrId, app);
+      }
+    }
+
+    private void shutdownApp(String appNameOrId, ApplicationReport app)
+    {
+      if (app == null) {
+        String errMessage = ""Failed to request shutdown for app %s: Application with id or name %s not found%n"";
+        System.err.printf(errMessage, appNameOrId, appNameOrId);
+        return;
+      }
+
+      try {
+        JSONObject response = sendShutdownRequest(app);
+        if (consolePresent) {
+          System.out.printf(""Shutdown of app %s requested: %s%n"", app.getApplicationId().toString(), response);
+        }
+      } catch (Exception e) {
+        String errMessage = ""Failed to request shutdown for app %s: %s%n"";
+        System.err.printf(errMessage, app.getApplicationId().toString(), e.getMessage());
+      } finally {
+        if (currentApp != null) {
+          if (app.getApplicationId().equals(currentApp.getApplicationId())) {
+            currentApp = null;","[{'comment': ""In doing this you are assuming that shutdown succeeded or will succeed. If shutdown doesn't succeed why set currentApp to null?"", 'commenter': 'sanjaypujare'}]"
565,engine/src/main/java/com/datatorrent/stram/StreamingAppMasterService.java,"@@ -1011,7 +1012,7 @@ private void execute() throws YarnException, IOException
             numCompletedContainers.incrementAndGet();
             LOG.info(""Container completed successfully."" + "", containerId="" + containerStatus.getContainerId());
             // Reset counter for node failure, if exists
-            String hostname = allocatedContainer.container.getNodeId().getHost();
+            String hostname = allocatedContainer.host;","[{'comment': 'Can allocatedContainer be null, so we should check before de-referencing it? Ideally it should not when exitStatus == 0, but may we can prevent another NPE with a check?', 'commenter': 'sanjaypujare'}, {'comment': 'Yes, it looks like it can be `null` when exitStatus is 0. It will be good to understand how a container can exit with the exit status 0 without being added to the `allocatedContainers` or was it already removed from the list.', 'commenter': 'vrozov'}]"
565,engine/src/main/java/com/datatorrent/stram/StreamingAppMasterService.java,"@@ -1253,10 +1254,14 @@ public void run()
     private final Container container;
     private boolean stopRequested;
     private Token<StramDelegationTokenIdentifier> delegationToken;
+    private final NodeId nodeId;
+    private final String host;
 
     private AllocatedContainer(Container c)
     {
       container = c;
+      nodeId = c.getNodeId();
+      host = nodeId.getHost();","[{'comment': 'Is there a need to cache host if nodeId is cached i.e. do we know `nodeId.getHost()` can be null later because of the same protocol buffer issue?', 'commenter': 'sanjaypujare'}, {'comment': 'Can getNodeId() return null here as well? Looking at Hadoop Container (and NodeId) Impl classes there are 2 variations: proto and builder. In the proto mode, nodeId etc are all non-null. If the same container object is used to send a request to Yarn and if any setter is called then the object switches to builder mode in which case nodeId will be null. So this bug might indicate possibly incorrect usage of objects in the alloatedContainers map.', 'commenter': 'sanjaypujare'}, {'comment': ""getNodeId() returning null should be handled as a separate JIRA and a separate PR. For this PR, I'd suggest focusing on a known issue (NPE in StreamingAppMasterService.java line 1014)."", 'commenter': 'vrozov'}, {'comment': 'On line 1014 the root cause of NPE was Container.getNodeId() returning null so I think it is the same issue. My comment was related to the timing - when the AllocatedContainer object is created it is passed a Container object and I was wondering if calling getNodeId() on that (at this point in time) could return null as well.', 'commenter': 'sanjaypujare'}, {'comment': 'Most likely, the root cause of NPE on line 1014 is that allocatedContainer is null, not that getNodeId() is null.', 'commenter': 'vrozov'}, {'comment': 'The PR was created on the basis that getNodeId() returned null and that is why nodeId was cached in the AllocatedContainer. If allocatedContainer is null on line 1014, what is the point of caching nodeId (and host)? Is the root cause not clear or are we adding a bunch of defensive fixes in this PR?', 'commenter': 'sanjaypujare'}, {'comment': '@sanjaypujare Please see JIRA. Currently, it has incorrect stack trace provided. The actual NPE that JIRA was filed for has stack trace similar to the issue that was fixed by f2bdef41fb35cc29e9abbf224f5ae49bf1b43b43.', 'commenter': 'vrozov'}, {'comment': ""@vrozov Yes, I did see the JIRA which doesn't have the stack information you asked for. My comments were based on the JIRA and PR content which don't show the stack trace you have mentioned. In any case, I still don't understand the reason for caching nodeId and host if the root cause was that allocatedContainer was null."", 'commenter': 'sanjaypujare'}, {'comment': '@vinaydt is running the tests on this PR to confirm that this PR is fixing the issue. So far the app is running for 19 hours without failing.', 'commenter': 'sandeshh'}, {'comment': '@sandeshh the JIRA description talked about ""..I was trying to delete different containers ( except the app master ) randomly....""  so I hope that was also tried to reproduce the bug. The stack trace currently pasted in the JIRA description points to allocatedContainer being null (after decoding the synthetic method code).\r\n\r\nAlso, note that AllocatedContainer can be made a static inner class - it doesn\'t use any outer class instance members. Also if the class is inner private, can we make all its members public so we won\'t see these synthetic methods?', 'commenter': 'sanjaypujare'}]"
566,engine/src/main/java/com/datatorrent/stram/cli/ApexCli.java,"@@ -3988,6 +3992,50 @@ public void execute(String[] args, ConsoleReader reader) throws Exception
     }
   }
 
+  private class SetLogLevelCommand implements Command
+  {
+
+    @Override
+    public void execute(String[] args, ConsoleReader reader) throws Exception
+    {
+      ApplicationReport appReport = currentApp;
+      String target = args[1];
+      String logLevel = args[2];
+
+      StramAgent.StramUriSpec uriSpec = new StramAgent.StramUriSpec();
+      uriSpec = uriSpec.path(StramWebServices.PATH_LOGGERS);
+      final JSONObject request = buildRequest(target, logLevel);
+
+      JSONObject response = getResource(uriSpec, appReport, new WebServicesClient.WebServicesHandler<JSONObject>()
+      {
+        @Override
+        public JSONObject process(WebResource.Builder webResource, Class<JSONObject> clazz)
+        {
+          return webResource.accept(MediaType.APPLICATION_JSON).post(JSONObject.class, request);
+        }
+
+      });
+
+      System.out.println(response);","[{'comment': 'Please see other commands that check `consolePresent` or use `printJson`.', 'commenter': 'tweise'}, {'comment': 'Using `printJson`', 'commenter': 'florianschmidt1994'}]"
567,engine/src/main/java/com/datatorrent/stram/engine/StreamingContainer.java,"@@ -1290,6 +1290,7 @@ private void deployInputStreams(List<OperatorDeployInfo> operatorList, HashMap<S
    */
   private void setupOiOGroups(Map<Integer, Integer> oioNodes)
   {
+    oioGroups.clear(); //Reset prior info if any before redeployment","[{'comment': 'should not clean up be handled in undeploy()? What if AM decides to deploy another operator/partition to a container that has few operators with thread local stream deployed? Will not that lead to the existing OiO groups being lost?', 'commenter': 'vrozov'}, {'comment': ""@vrozov I've done the changes. Please review."", 'commenter': 'francisf'}]"
567,engine/src/main/java/com/datatorrent/stram/engine/StreamingContainer.java,"@@ -143,6 +143,8 @@
    * value: list of nodes which are in oio with oio owning thread node
    */
   protected final Map<Integer, ArrayList<Integer>> oioGroups = new ConcurrentHashMap<>();
+  //a simple map which maps the oio node to it's the node which owns the thread.
+  protected final Map<Integer, Integer> oioNodes = new ConcurrentHashMap<>();","[{'comment': 'As you have made it global make sure that the entries are consistent with oioGroups. i.e the parent to children mapping and child to parent mapping is always consistent in both the structures.', 'commenter': 'tushargosavi'}]"
567,engine/src/main/java/com/datatorrent/stram/engine/StreamingContainer.java,"@@ -566,6 +568,11 @@ private synchronized void undeploy(List<Integer> nodeList)
 
     for (Integer operatorId : nodeList) {
       nodes.remove(operatorId);
+      Integer sourceNodeId = oioNodes.get(operatorId);
+      if (sourceNodeId != null) {
+        ArrayList<Integer> oioNodeIdList = oioGroups.get(sourceNodeId);","[{'comment': 'I feel the problem could be solve by changing list to set.', 'commenter': 'tushargosavi'}]"
568,engine/src/main/java/com/datatorrent/stram/StramLocalCluster.java,"@@ -76,7 +76,7 @@
   protected final StreamingContainerManager dnmgr;
   private final UmbilicalProtocolLocalImpl umbilical;
   private InetSocketAddress bufferServerAddress;
-  private boolean perContainerBufferServer;
+  private boolean perContainerBufferServer = true;","[{'comment': 'The default value for perContainerBufferServer should be false.', 'commenter': 'vrozov'}, {'comment': 'Will address that.', 'commenter': 'sandeshh'}]"
568,engine/src/main/java/com/datatorrent/stram/stream/QueueServerPublisher.java,"@@ -0,0 +1,204 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package com.datatorrent.stram.stream;
+
+import java.util.concurrent.atomic.AtomicLong;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import org.apache.apex.api.operator.ControlTuple;
+
+import com.datatorrent.api.StreamCodec;
+import com.datatorrent.bufferserver.packet.BeginWindowTuple;
+import com.datatorrent.bufferserver.packet.DataTuple;
+import com.datatorrent.bufferserver.packet.EndStreamTuple;
+import com.datatorrent.bufferserver.packet.EndWindowTuple;
+import com.datatorrent.bufferserver.packet.MessageType;
+import com.datatorrent.bufferserver.packet.PayloadTuple;
+import com.datatorrent.bufferserver.packet.ResetWindowTuple;
+import com.datatorrent.bufferserver.packet.WindowIdTuple;
+import com.datatorrent.bufferserver.server.Server;
+import com.datatorrent.netlet.util.Slice;
+import com.datatorrent.stram.codec.StatefulStreamCodec;
+import com.datatorrent.stram.engine.ByteCounterStream;
+import com.datatorrent.stram.engine.StreamContext;
+import com.datatorrent.stram.tuple.CustomControlTuple;
+import com.datatorrent.stram.tuple.Tuple;
+
+public class QueueServerPublisher implements ByteCounterStream
+{
+  private StreamCodec<Object> serde;
+  private final AtomicLong publishedByteCount;
+  private int count;
+  private StatefulStreamCodec<Object> statefulSerde;
+  private Server.QueuePublisher queuePublisher;
+  private Server server;
+  private String identifier;
+
+  public QueueServerPublisher(String sourceId, Server server)
+  {
+    this.server = server;
+    this.publishedByteCount = new AtomicLong(0);
+    this.identifier = sourceId;
+  }
+
+  @Override
+  public long getByteCount(boolean reset)
+  {
+    if (reset) {
+      return publishedByteCount.getAndSet(0);
+    }
+
+    return publishedByteCount.get();
+  }
+
+  @Override
+  public void teardown()
+  {
+
+  }
+
+  @Override
+  public void put(Object payload)","[{'comment': 'This seems to be a repeat of code in BufferServerPublisher. Why not reuse BufferServerPublisher and make the mechanism of sending data pluggable. You could recast the network mechanism based on netlet as one mechanism and the direct addition as a separate mechanism. Is there a need to replace the entire publisher? ', 'commenter': 'pramodin'}, {'comment': 'Netlet based mechanism doesn\'t offer any advantage, needs to be ""repealed and replaced"", but will keep it around until enough votes(aka releases) are done. But will still look into abstracting few pieces.', 'commenter': 'sandeshh'}, {'comment': '+1 for @PramodSSImmaneni request.', 'commenter': 'vrozov'}]"
568,bufferserver/src/main/java/com/datatorrent/bufferserver/internal/DataList.java,"@@ -106,12 +106,8 @@ public int getBlockSize()
     return blockSize;
   }
 
-  public void rewind(final int baseSeconds, final int windowId) throws IOException
+  public void rewind(final long longWindowId) throws IOException","[{'comment': 'long longWindowId is not a good style. It is already clear that windowId is long.', 'commenter': 'vrozov'}, {'comment': 'Fixed', 'commenter': 'sandeshh'}]"
568,bufferserver/src/main/java/com/datatorrent/bufferserver/internal/DataList.java,"@@ -158,6 +154,14 @@ public void rewind(final int baseSeconds, final int windowId) throws IOException
 
   }
 
+  public void rewind(final int baseSeconds, final int windowId) throws IOException","[{'comment': 'No reason to keep this method. Introduce a static method to Codec to get long windowId from baseSeconds and windowId.', 'commenter': 'vrozov'}, {'comment': 'Removing this method.', 'commenter': 'sandeshh'}]"
568,bufferserver/src/main/java/com/datatorrent/bufferserver/internal/DataList.java,"@@ -158,6 +154,14 @@ public void rewind(final int baseSeconds, final int windowId) throws IOException
 
   }
 
+  public void rewind(final int baseSeconds, final int windowId) throws IOException
+  {
+    final long longWindowId = (long)baseSeconds << 32 | windowId;
+    logger.debug(""Rewinding {} from window ID {} to window ID {}"", this, Codec.getStringWindowId(last.ending_window),","[{'comment': 'Move debug back.', 'commenter': 'vrozov'}, {'comment': 'Done.', 'commenter': 'sandeshh'}]"
568,bufferserver/src/main/java/com/datatorrent/bufferserver/internal/DataList.java,"@@ -234,64 +238,63 @@ public String getIdentifier()
 
   public void flush(final int writeOffset)
   {
-    //logger.debug(""size = {}, processingOffset = {}, nextOffset = {}, writeOffset = {}"", size, processingOffset,
-    //    nextOffset.integer, writeOffset);
-    flush:
-    do {
-      while (size == 0) {
-        size = VarInt.read(last.data, processingOffset, writeOffset, nextOffset);
-        if (nextOffset.integer > -5 && nextOffset.integer < 1) {
-          if (writeOffset == last.data.length) {
-            nextOffset.integer = 0;
-            processingOffset = 0;
-            size = 0;
-          }
-          break flush;
-        } else if (nextOffset.integer == -5) {
-          throw new RuntimeException(""problemo!"");
-        }
+    size = VarInt.read(last.data, processingOffset, writeOffset, nextOffset);","[{'comment': 'Why is the change necessary? When messages are received over a network socket, it is possible to get an incomplete message and the flush is necessary. There is no need to deal with such condition when messages are delivered over in memory queue.', 'commenter': 'vrozov'}, {'comment': 'Done.', 'commenter': 'sandeshh'}, {'comment': 'If I remove this block one unit test is failing. So keeping it as is, will analyze the reason for the unit test failure.', 'commenter': 'sandeshh'}, {'comment': 'Everything is working as expected, the issue was because of the bug while removing the code.', 'commenter': 'sandeshh'}]"
568,bufferserver/src/main/java/com/datatorrent/bufferserver/server/Server.java,"@@ -896,7 +896,102 @@ private void teardown()
         ln.boot();
       }
     }
+  }
+
+  public QueuePublisher handleQueuePublisher(long windowId, String identifier)","[{'comment': '-1: code duplication', 'commenter': 'vrozov'}]"
568,bufferserver/src/main/java/com/datatorrent/bufferserver/server/Server.java,"@@ -896,7 +896,102 @@ private void teardown()
         ln.boot();
       }
     }
+  }
+
+  public QueuePublisher handleQueuePublisher(long windowId, String identifier)
+  {
+    DataList dl = publisherBuffers.get(identifier);
+
+    if (dl != null) {
+      try {
+        dl.rewind(windowId);
+      } catch (IOException ie) {
+        throw new RuntimeException(ie);
+      }
+    } else {
+      dl = new DataList(identifier, blockSize, numberOfCacheBlocks, BACK_PRESSURE_ENABLED);
+      DataList odl = publisherBuffers.putIfAbsent(identifier, dl);
+      if (odl != null) {
+        dl = odl;
+      }
+    }
+
+    dl.setAutoFlushExecutor(serverHelperExecutor);
+    dl.setSecondaryStorage(storage, storageHelperExecutor);
+
+    QueuePublisher publisher = new QueuePublisher(dl, windowId);
+
+    return publisher;
+  }
+
+  public static class QueuePublisher
+  {
+    private final DataList dl;
+    private byte[] buffer;
+    private int offset = 0;
+    private byte[] scratch = new byte[5];","[{'comment': 'Is scratch necessary?', 'commenter': 'vrozov'}, {'comment': 'I used it for length, will calculate it using a different method.', 'commenter': 'sandeshh'}]"
568,engine/src/main/java/com/datatorrent/stram/StramLocalCluster.java,"@@ -311,6 +315,7 @@ public StramLocalCluster(LogicalPlan dag) throws IOException, ClassNotFoundExcep
     if (dag.getAttributes().get(OperatorContext.STORAGE_AGENT) == null) {
       dag.setAttribute(OperatorContext.STORAGE_AGENT, new AsyncFSStorageAgent(new Path(pathUri, LogicalPlan.SUBDIR_CHECKPOINTS).toString(), null));
     }
+    ifNeededStartBufferServer();","[{'comment': 'Why is it necessary to move this call here?', 'commenter': 'vrozov'}, {'comment': 'Not required, changing it back.', 'commenter': 'sandeshh'}]"
568,bufferserver/src/main/java/com/datatorrent/bufferserver/server/Server.java,"@@ -448,28 +448,32 @@ public String toString()
    */
   public DataList handlePublisherRequest(PublishRequestTuple request, AbstractLengthPrependerClient connection)
   {
-    String identifier = request.getIdentifier();
+    return publisherRequestHelper(request.getIdentifier(), (long)request.getBaseSeconds() << 32 | request.getWindowId(), connection);
+  }
 
+  private DataList publisherRequestHelper(String identifier, long windowId, AbstractLengthPrependerClient connection)
+  {
     DataList dl = publisherBuffers.get(identifier);
 
     if (dl != null) {
       /*
        * close previous connection with the same identifier which is guaranteed to be unique.
        */
-      AbstractLengthPrependerClient previous = publisherChannels.put(identifier, connection);
-      if (previous != null) {
-        eventloop.disconnect(previous);
+
+      if (connection != null) {
+        AbstractLengthPrependerClient previous = publisherChannels.put(identifier, connection);
+        if (previous != null) {
+          eventloop.disconnect(previous);
+        }
       }
 
       try {
-        dl.rewind(request.getBaseSeconds(), request.getWindowId());
+        dl.rewind(windowId);
       } catch (IOException ie) {
         throw new RuntimeException(ie);
       }
     } else {
-      dl = Tuple.FAST_VERSION.equals(request.getVersion()) ?
-          new FastDataList(identifier, blockSize, numberOfCacheBlocks, BACK_PRESSURE_ENABLED) :
-          new DataList(identifier, blockSize, numberOfCacheBlocks, BACK_PRESSURE_ENABLED);
+      dl = new DataList(identifier, blockSize, numberOfCacheBlocks, BACK_PRESSURE_ENABLED);","[{'comment': 'I would prefer to deprecate FastDataList as part of a separate JIRA and PR.', 'commenter': 'vrozov'}, {'comment': 'Done.', 'commenter': 'sandeshh'}]"
568,bufferserver/src/main/java/com/datatorrent/bufferserver/internal/DataList.java,"@@ -234,64 +233,43 @@ public String getIdentifier()
 
   public void flush(final int writeOffset)
   {
-    //logger.debug(""size = {}, processingOffset = {}, nextOffset = {}, writeOffset = {}"", size, processingOffset,
-    //    nextOffset.integer, writeOffset);
-    flush:
-    do {
-      while (size == 0) {
-        size = VarInt.read(last.data, processingOffset, writeOffset, nextOffset);
-        if (nextOffset.integer > -5 && nextOffset.integer < 1) {
-          if (writeOffset == last.data.length) {
-            nextOffset.integer = 0;
-            processingOffset = 0;
-            size = 0;
+    processingOffset = nextOffset.integer;","[{'comment': 'Need a different flush for BufferServerPublisher, will add that.', 'commenter': 'sandeshh'}]"
568,bufferserver/src/main/java/com/datatorrent/bufferserver/server/Server.java,"@@ -896,7 +909,89 @@ private void teardown()
         ln.boot();
       }
     }
+  }
+
+  public static class QueuePublisher
+  {
+    private final DataList dl;
+    private byte[] buffer;
+    private int offset = 0;
+
+    QueuePublisher(DataList dl, long windowId)
+    {
+      this.dl = dl;
+      buffer = dl.getBuffer(windowId);
+    }
+
+    public byte[] getBuffer()
+    {
+      return buffer;
+    }
+
+    public int getOffset()
+    {
+      return offset;
+    }
+
+    public void setOffset(int offset)
+    {
+      this.offset = offset;
+    }
 
+
+    private int len(int value)","[{'comment': 'VarInt.getSize()?', 'commenter': 'vrozov'}, {'comment': 'Done', 'commenter': 'sandeshh'}]"
569,engine/src/main/java/com/datatorrent/stram/client/StramClientUtils.java,"@@ -536,16 +536,18 @@ public static FileSystem newFileSystemInstance(Configuration conf) throws IOExce
    * @param dfsRootDir  value of dt.dfsRootDir or apex.app.dfsRootDir
    * @param userShortName  current user short name (either login user or current user depending on impersonation settings)
    * @param prependHomeDir  prepend user's home dir if dfsRootDir is relative path
-
+   * @param updateConf update the conf object with the evaluated value
    * @return
    */
   private static Path evalDFSRootDir(FileSystem fs, Configuration conf, String dfsRootDir, String userShortName,
-      boolean prependHomeDir)
+      boolean prependHomeDir, boolean updateConf)
   {
     try {
       if (userShortName != null && dfsRootDir.contains(DT_DFS_USER_NAME)) {
         dfsRootDir = dfsRootDir.replace(DT_DFS_USER_NAME, userShortName);
-        conf.set(DT_DFS_ROOT_DIR, dfsRootDir);
+        if (updateConf) {
+          conf.set(DT_DFS_ROOT_DIR, dfsRootDir);","[{'comment': 'Should a similar thing be done with APEX_DFS_ROOT_DIR, how is this property that is set on the conf used after the call, given that the method is also returning this path', 'commenter': 'pramodin'}, {'comment': '@PramodSSImmaneni  As mentioned elsewhere, we cannot do the same thing for APEX_DFS_ROOT_DIR since the value is per Hadoop login user so cannot cache it in the conf object', 'commenter': 'sanjaypujare'}, {'comment': 'Can the property be passed as a parameter and can this method capture more of the common processing between getApex and getDT methods such as reading the property value and getting the default root folder if the value is empty. The username and home folder can be passed from the respective get.. methods.', 'commenter': 'pramodin'}, {'comment': 'Done. Pls check', 'commenter': 'sanjaypujare'}]"
569,engine/src/main/java/com/datatorrent/stram/client/StramClientUtils.java,"@@ -584,38 +607,30 @@ private static String getDefaultRootFolder()
    */
   public static Path getApexDFSRootDir(FileSystem fs, Configuration conf)
   {
-    String apexDfsRootDir = conf.get(APEX_APP_DFS_ROOT_DIR);
     boolean useImpersonated = conf.getBoolean(StramUserLogin.DT_APP_PATH_IMPERSONATED, false);
     String userShortName = null;
     if (useImpersonated) {
       try {
         userShortName = UserGroupInformation.getCurrentUser().getShortUserName();
       } catch (IOException ex) {
-        LOG.warn(""Error getting current/login user name {}"", apexDfsRootDir, ex);
+        LOG.warn(""Error getting current/login user name in getApexDFSRootDir"", ex);","[{'comment': 'Remove the method name from the log message (already provided by logger).', 'commenter': 'tweise'}, {'comment': 'fixed ( in both cases)', 'commenter': 'sanjaypujare'}]"
569,engine/src/test/java/com/datatorrent/stram/client/StramClientUtilsTest.java,"@@ -175,6 +179,9 @@ public void getApexDFSRootDirLegacy() throws IOException
     FileSystem fs = FileSystem.newInstance(conf);
     Path path = StramClientUtils.getApexDFSRootDir(fs, conf);
     Assert.assertEquals(""file:/a/b/c"", path.toString());
+    // verify conf properties for DT_DFS_ROOT_DIR and APEX_APP_DFS_ROOT_DIR","[{'comment': 'Why is YarnConfiguration necessary? It is cast to Configuration anyway.', 'commenter': 'vrozov'}, {'comment': 'Good point. I copied the pattern from an existing test (most probably `testRMWebAddress()` where it is actually required). But I will change it to Configuration and verify.', 'commenter': 'sanjaypujare'}, {'comment': 'fixed', 'commenter': 'sanjaypujare'}]"
569,engine/src/test/java/com/datatorrent/stram/client/StramClientUtilsTest.java,"@@ -194,6 +201,9 @@ public void getApexDFSRootDirAbsPath() throws IOException
     UserGroupInformation.setLoginUser(testUser);","[{'comment': 'Is not this a global change? Any test must be written in such way that it does not affect other tests.', 'commenter': 'vrozov'}, {'comment': 'Good point. I will rewrite using `PowerMockito.mockStatic` (if possible) to prevent the global change', 'commenter': 'sanjaypujare'}, {'comment': 'fixed', 'commenter': 'sanjaypujare'}]"
569,engine/src/test/java/com/datatorrent/stram/client/StramClientUtilsTest.java,"@@ -162,75 +169,94 @@ public InetSocketAddress getSocketAddr(String name, String defaultAddress, int d
 
   /**
    * apex.dfsRootDirectory not set: legacy behavior of getDTDFSRootDir()
-   * @throws IOException
+   * @throws Exception
    *
    */
   @Test
-  public void getApexDFSRootDirLegacy() throws IOException
+  public void getApexDFSRootDirLegacy() throws Exception
   {
-    Configuration conf = new YarnConfiguration(new Configuration(false));
+    Configuration conf = new Configuration(false);
     conf.set(StramClientUtils.DT_DFS_ROOT_DIR, ""/a/b/c"");
     conf.setBoolean(StramUserLogin.DT_APP_PATH_IMPERSONATED, false);
 
+    UserGroupInformation testUser = UserGroupInformation.createUserForTesting(""testUser1"", new String[]{""""});
+    spy(UserGroupInformation.class);
+    doReturn(testUser).when(UserGroupInformation.class, ""getLoginUser"");
+
     FileSystem fs = FileSystem.newInstance(conf);
     Path path = StramClientUtils.getApexDFSRootDir(fs, conf);
     Assert.assertEquals(""file:/a/b/c"", path.toString());
+    // verify conf properties for DT_DFS_ROOT_DIR and APEX_APP_DFS_ROOT_DIR
+    Assert.assertEquals(""/a/b/c"", conf.get(StramClientUtils.DT_DFS_ROOT_DIR));
+    Assert.assertNull(conf.get(StramClientUtils.APEX_APP_DFS_ROOT_DIR));
   }
 
   /**
    * apex.dfsRootDirectory set: absolute path e.g. /x/y/z
-   * @throws IOException
+   * @throws Exception
    *
    */
   @Test
-  public void getApexDFSRootDirAbsPath() throws IOException
+  public void getApexDFSRootDirAbsPath() throws Exception
   {
-    Configuration conf = new YarnConfiguration(new Configuration(false));
+    Configuration conf = new Configuration(false);
     conf.set(StramClientUtils.APEX_APP_DFS_ROOT_DIR, ""/x/y/z"");
     conf.setBoolean(StramUserLogin.DT_APP_PATH_IMPERSONATED, false);
 
-    FileSystem fs = FileSystem.newInstance(conf);
     UserGroupInformation testUser = UserGroupInformation.createUserForTesting(""testUser1"", new String[]{""""});
-    UserGroupInformation.setLoginUser(testUser);
+    spy(UserGroupInformation.class);
+    doReturn(testUser).when(UserGroupInformation.class, ""getLoginUser"");
+
+    FileSystem fs = FileSystem.newInstance(conf);
     Path path = StramClientUtils.getApexDFSRootDir(fs, conf);
     Assert.assertEquals(fs.getHomeDirectory() + ""/datatorrent"", path.toString());
+    // verify conf properties for DT_DFS_ROOT_DIR and APEX_APP_DFS_ROOT_DIR
+    Assert.assertNull(conf.get(StramClientUtils.DT_DFS_ROOT_DIR));
+    Assert.assertEquals(""/x/y/z"", conf.get(StramClientUtils.APEX_APP_DFS_ROOT_DIR));
   }
 
   /**
    * apex.dfsRootDirectory set: absolute path with scheme e.g. file:/p/q/r
-   * @throws IOException
+   * @throws Exception
    *
    */
   @Test
-  public void getApexDFSRootDirScheme() throws IOException
+  public void getApexDFSRootDirScheme() throws Exception
   {
-    Configuration conf = new YarnConfiguration(new Configuration(false));
+    Configuration conf = new Configuration(false);
     conf.set(StramClientUtils.APEX_APP_DFS_ROOT_DIR, ""file:/p/q/r"");
     conf.setBoolean(StramUserLogin.DT_APP_PATH_IMPERSONATED, false);
 
-    FileSystem fs = FileSystem.newInstance(conf);
     UserGroupInformation testUser = UserGroupInformation.createUserForTesting(""testUser1"", new String[]{""""});
-    UserGroupInformation.setLoginUser(testUser);
+    spy(UserGroupInformation.class);","[{'comment': 'Can it be moved to `@Before` or `@BeforeClass`?', 'commenter': 'vrozov'}, {'comment': 'No. Not all tests need that. But I will refactor this into a function that will be called from all the affected functions.', 'commenter': 'sanjaypujare'}, {'comment': '@vrozov Let me know your comments on the latest changes and the current state of this PR. The PR shows ""vrozov requested changes"" and you had a -1 based on CI issues (not related to this PR).', 'commenter': 'sanjaypujare'}, {'comment': 'Even though not all test use UserGroupInformation, will it affect them? Note that you mock UserGroupInformation for the entire unit test, not only for the tests that need it.', 'commenter': 'vrozov'}, {'comment': ""Yes, I am using PrepareForTest and RunWith annotations at class level because they could not be made to work at method level. There is more than one way, but I didn't think it was worth the time for me to refactor to address the `@Before` suggestion esp. at this stage."", 'commenter': 'sanjaypujare'}, {'comment': ""The current way gives wrong impression that UserGroupInformation won't be modified unless `setupLoginTestUser()` is invoked. Also, should not it be static and `@BeforeClass`?"", 'commenter': 'vrozov'}]"
578,engine/src/main/java/com/datatorrent/stram/StreamingContainerManager.java,"@@ -934,7 +934,7 @@ private void aggregateMetrics(long windowId, Map<Integer, EndWindowStats> endWin
           metricPool.add(physicalMetrics);
         }
       }
-      if (metricPool.isEmpty()) {
+      if (metricPool.isEmpty() || physicalOperators.size() != metricPool.size()) {","[{'comment': 'The partial aggregates could turn out to be useful as they give real time progress of the aggregates. Instead of changing how these get computed, how about providing an option during query time to retrieve what is desired. For example, the current query to return the metrics aggregates could remain the same but you could add an option which when set would return only the last fully aggregated metric. ', 'commenter': 'pramodin'}, {'comment': 'Instead of this combined check, you can do a single check as follows:\r\n```\r\nif (metricPool.size() < physicalOperators.size()) \r\n```', 'commenter': 'sanjaypujare'}, {'comment': ""@PramodSSImmaneni & @chaithu14 I have another (better?) suggestion: we can instead use metrics from the previous window for a partition if they are not available for the current window for that partition. And that is relatively easy to implement in the code as follows:\r\n\r\nIn `calculateEndWindowStats()`\r\n```\r\n        Map<Integer, EndWindowStats> endWindowStatsMap = null;\r\n        Map<Integer, EndWindowStats> prevEndWindowStatsMap = null;\r\n      while (windowId != null) {\r\n        prevEndWindowStatsMap = endWindowStatsMap;\r\n        endWindowStatsMap = endWindowStatsOperatorMap.get(windowId);\r\n\r\n        aggregateMetrics(windowId, endWindowStatsMap, prevEndWindowStatsMap);\r\n```\r\n`aggregateMetrics` is modified to accept another param:\r\n```\r\naggregateMetrics(long windowId, Map<Integer, EndWindowStats> endWindowStatsMap, Map<Integer, EndWindowStats> prevEndWindowStatsMap)\r\n{\r\n...\r\n       EndWindowStats stats = endWindowStatsMap.get(operator.getId());\r\n       if ((stats == null || stats.metrics == null) && prevEndWindowStatsMap != null) {\r\n           stats = prevEndWindowStatsMap.get(operator.getId());\r\n        }\r\n        if (stats != null && stats.metrics != null) {\r\n          PhysicalMetricsContextImpl physicalMetrics = new PhysicalMetricsContextImpl(operator.getId(), stats.metrics);\r\n          metricPool.add(physicalMetrics);\r\n        }\r\n...\r\n}\r\n```\r\nEssentially what is happening is that if I don't find metrics for a physical operator for the current window then I copy them from the previous window for the same operator (which itself could have been copied from the previous window if the delay has been more than 2 windows for a physical operator).\r\n\r\nAnyway what is important is to design a test that will test this logic. Even otherwise I would like to see a robust test to test the existing logic implemented.\r\n"", 'commenter': 'sanjaypujare'}, {'comment': ""@PramodSSImmaneni Yes, it's a good option. I'll look into the code and will update.\r\n\r\n@sanjaypujare I think, this will more confuses to the user that the metrics are belongs to the current window or previous window. "", 'commenter': 'chaithu14'}]"
578,engine/src/test/java/com/datatorrent/stram/engine/AutoMetricTest.java,"@@ -266,7 +265,6 @@ public void testMetrics() throws Exception
   }
 
   @Test
-  @Ignore
   public void testMetricsAggregations() throws Exception","[{'comment': ""I don't understand how your modified test verifies the code changes you have made. Only the number of partitions have been changed from 2 to 4 but that doesn't verify the partial metrics change you have made."", 'commenter': 'sanjaypujare'}, {'comment': 'Previously, this test is intermediately failed due to the partial metrics. I think that the chance of failure this test would be more if we increased the number of partitions.', 'commenter': 'chaithu14'}]"
578,engine/src/main/java/com/datatorrent/stram/StreamingContainerManager.java,"@@ -2673,7 +2683,12 @@ private LogicalOperatorInfo fillLogicalOperatorInfo(OperatorMeta operator)
     if (physicalOperators.size() > 0 && checkpointTimeAggregate.getAvg() != null) {
       loi.checkpointTimeMA = checkpointTimeAggregate.getAvg().longValue();
       loi.counters = latestLogicalCounters.get(operator.getName());
-      loi.autoMetrics = latestLogicalMetrics.get(operator.getName());
+      if (isPartialAggregates) {
+        loi.autoMetrics = latestLogicalMetrics.get(operator.getName());","[{'comment': 'Why is return type different for complete and partial? window id should be available for both.', 'commenter': 'vrozov'}, {'comment': ""Latest metrics might be partial or complete. If the latest metrics is partial and user querying the complete metrics, then the latest window id and the window id which belongs to complete metrics is different. That's why adding the window id to the complete query. Due to that different window id, user will be aware of that the latest metrics is not complete."", 'commenter': 'chaithu14'}, {'comment': 'How as a user I know that the latest is not complete? What if the latest is complete? I strongly prefer to unify format for complete and incomplete metrics and avoid using different return types.', 'commenter': 'vrozov'}, {'comment': ""@chaithu14 Please reply or close the PR if you don't have time to look into it."", 'commenter': 'vrozov'}]"
598,docker/build.sh,"@@ -0,0 +1,7 @@
+#!/bin/bash
+
+# Build ubuntu 14.04 docker image","[{'comment': 'update comment', 'commenter': 'tweise'}, {'comment': 'Done', 'commenter': 'chinmaykolhatkar'}]"
598,docker/ubuntu/app/setup.sh,"@@ -0,0 +1,74 @@
+#!/bin/bash
+
+# Install softwares
+apt-get update -y
+apt-get -y -o APT::Immediate-Configure=false install wget
+wget -O- http://archive.apache.org/dist/bigtop/bigtop-1.1.0/repos/GPG-KEY-bigtop | sudo apt-key add -
+wget -O /etc/apt/sources.list.d/bigtop-1.1.0.list http://archive.apache.org/dist/bigtop/bigtop-1.1.0/repos/`lsb_release --codename --short`/bigtop.list
+
+# for openjdk-8-jre-headless
+apt-get -y install software-properties-common","[{'comment': 'is this still required with ubuntu-16.04?', 'commenter': 'tweise'}, {'comment': 'Nope... Removed.', 'commenter': 'chinmaykolhatkar'}]"
598,docker/ubuntu/app/setup.sh,"@@ -0,0 +1,74 @@
+#!/bin/bash
+
+# Install softwares
+apt-get update -y
+apt-get -y -o APT::Immediate-Configure=false install wget
+wget -O- http://archive.apache.org/dist/bigtop/bigtop-1.1.0/repos/GPG-KEY-bigtop | sudo apt-key add -","[{'comment': 'use latest bigtop?', 'commenter': 'tweise'}, {'comment': 'Updated to 1.2.1', 'commenter': 'chinmaykolhatkar'}]"
598,docker/ubuntu/app/setup.sh,"@@ -0,0 +1,74 @@
+#!/bin/bash
+
+# Install softwares
+apt-get update -y
+apt-get -y -o APT::Immediate-Configure=false install wget
+wget -O- http://archive.apache.org/dist/bigtop/bigtop-1.1.0/repos/GPG-KEY-bigtop | sudo apt-key add -
+wget -O /etc/apt/sources.list.d/bigtop-1.1.0.list http://archive.apache.org/dist/bigtop/bigtop-1.1.0/repos/`lsb_release --codename --short`/bigtop.list
+
+# for openjdk-8-jre-headless
+apt-get -y install software-properties-common
+add-apt-repository ppa:openjdk-r/ppa
+apt-get update -y
+
+apt-get install -y -q --no-install-recommends openjdk-8-jre-headless vim screen curl sudo unzip man openssh-server hadoop\*
+
+wget https://ci.bigtop.apache.org/job/Bigtop-trunk-packages/COMPONENTS=apex,OS=ubuntu-16.04/lastSuccessfulBuild/artifact/output/apex/apex_3.6.0-1_all.deb
+dpkg -i apex_3.6.0-1_all.deb
+rm apex_3.6.0-1_all.deb
+
+ln -s /usr/lib/jvm/java-1.8.0-openjdk-amd64 /usr/lib/jvm/java-openjdk","[{'comment': 'this is probably not needed with latest bigtop', 'commenter': 'tweise'}, {'comment': 'Yes... Not needed.', 'commenter': 'chinmaykolhatkar'}]"
598,docker/ubuntu/app/setup.sh,"@@ -0,0 +1,74 @@
+#!/bin/bash
+
+# Install softwares
+apt-get update -y
+apt-get -y -o APT::Immediate-Configure=false install wget
+wget -O- http://archive.apache.org/dist/bigtop/bigtop-1.1.0/repos/GPG-KEY-bigtop | sudo apt-key add -
+wget -O /etc/apt/sources.list.d/bigtop-1.1.0.list http://archive.apache.org/dist/bigtop/bigtop-1.1.0/repos/`lsb_release --codename --short`/bigtop.list
+
+# for openjdk-8-jre-headless
+apt-get -y install software-properties-common
+add-apt-repository ppa:openjdk-r/ppa
+apt-get update -y
+
+apt-get install -y -q --no-install-recommends openjdk-8-jre-headless vim screen curl sudo unzip man openssh-server hadoop\*
+
+wget https://ci.bigtop.apache.org/job/Bigtop-trunk-packages/COMPONENTS=apex,OS=ubuntu-16.04/lastSuccessfulBuild/artifact/output/apex/apex_3.6.0-1_all.deb","[{'comment': 'you can pull from https://github.com/atrato/apex-cli-package/releases/tag/v3.7.0 until we have the first binaries released in Apache', 'commenter': 'tweise'}, {'comment': ""Updated this PR with the atrato release. I can create a docker image for 3.7.0 using apex-core/master and only then I'll go about working on APEXCORE-813 (using binary from apex-core instead of atrato). Hope that is fine."", 'commenter': 'chinmaykolhatkar'}, {'comment': 'The `3.7.0` version should still be created from your repo, so that we have a release branch (vs. master). We can then switch to dockerhub to build from the Apache repo for `latest`.\r\n\r\nThe only thing that needs to be done prior to creating the `3.7.0` image is to change the apex package portion as done in this PR (rest was already tested).', 'commenter': 'tweise'}]"
608,bufferserver/src/main/java/com/datatorrent/bufferserver/policy/RandomOne.java,"@@ -53,7 +54,8 @@ private RandomOne()
   @Override
   public boolean distribute(Set<PhysicalNode> nodes, SerializedData data) throws InterruptedException
   {
-    int count = (int)(Math.random() * nodes.size());
+    Random rand = new Random();","[{'comment': '-1: there is no performance improvement as `Math.random()` delegates to `Random` `nextDouble()` method anyway. The change leads to unnecessary `Random()` instance creation.', 'commenter': 'vrozov'}, {'comment': 'The intent of the policy is to distribute tuples randomly each time. Using a single instance should work fine and Math.random() is doing that internally anyway.', 'commenter': 'pramodin'}]"
