Pull,Path,Diff_hunk,Comment
47,fe/src/com/baidu/palo/common/ErrorCode.java,"@@ -155,7 +155,7 @@
     ERR_CLUSTER_NAME_NULL(5041, new byte[] {'4', '2', '0', '0', '0'}, ""No cluster name""),
     ERR_CLUSTER_NO_PERMISSIONS(5042, new byte[] {'4', '2', '0', '0', '0'}, ""No permissions""),
     ERR_CLUSTER_CREATE_ISTANCE_NUM_ERROR(5043, new byte[] {'4', '2', '0', '0', '0'}, 
-            ""Instance num can't be less than 0""),
+            ""Instance num can't be less than or equal 0""),","[{'comment': ""can't be less than nor equal with 0"", 'commenter': 'imay'}, {'comment': '这个确定吗？我怎么查了这么写没错啊', 'commenter': 'chenhao7253886'}]"
47,fe/src/com/baidu/palo/analysis/ShowBackendsStmt.java,"@@ -0,0 +1,45 @@
+// Copyright 2017 The Apache Software Foundation
+// Modifications copyright (C) 2017, Baidu.com, Inc.
+
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package com.baidu.palo.analysis;
+
+import com.baidu.palo.catalog.Column;
+import com.baidu.palo.catalog.ColumnType;
+import com.baidu.palo.common.proc.BackendsProcDir;
+import com.baidu.palo.qe.ShowResultSetMetaData;
+
+public class ShowBackendsStmt extends ShowStmt {
+
+    public ShowBackendsStmt() {  
+    }
+    
+    @Override
+    public ShowResultSetMetaData getMetaData() {
+         ShowResultSetMetaData.Builder builder = ShowResultSetMetaData.builder();
+         for (String title : BackendsProcDir.TITLE_NAMES) {
+             if (title.equals(""HostName"") || title.equals(""HeartbeatPort"") 
+                     || title.equals(""BePort"") || title.equals(""HttpPort"")) {
+                 continue;
+             }
+            builder.addColumn(new Column(title, ColumnType.createVarchar(30)));
+        }
+        return builder.build();","[{'comment': '这个为什么不写成静态对象呢？每次生成的不是一样的么？', 'commenter': 'imay'}, {'comment': '这里用其他地方的静态对象生成的，避免重复吧。', 'commenter': 'chenhao7253886'}, {'comment': '这个没啥重复的吧？因为你每次build出来的内容都一样，那么写成一个静态的对象直接返回是没问题的', 'commenter': 'imay'}]"
47,fe/src/com/baidu/palo/cluster/ClusterNamespace.java,"@@ -85,7 +85,7 @@ public static String getDbNameFromFullName(String db) {
      * @return
      */
     public static String getUsrNameFromFullName(String usr) {
-        if (checkName(usr)) {
+        if (!checkName(usr)) {","[{'comment': '这里难道不应该是checkName不符合后，返回的是参数而不是null么？', 'commenter': 'imay'}, {'comment': '这个我改下，应该返回参数更合理些。', 'commenter': 'chenhao7253886'}]"
47,fe/src/com/baidu/palo/analysis/ShowBackendsStmt.java,"@@ -0,0 +1,45 @@
+// Copyright 2017 The Apache Software Foundation
+// Modifications copyright (C) 2017, Baidu.com, Inc.
+
+// Licensed to the Apache Software Foundation (ASF) under one","[{'comment': '这个Licence写的有问题，需要改下', 'commenter': 'imay'}]"
69,fe/src/com/baidu/palo/catalog/Type.java,"@@ -20,6 +20,20 @@
 
 package com.baidu.palo.catalog;
 
+// Copyright 2012 Cloudera Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the ""License"");","[{'comment': '这个license不需要了吧？直接去掉吧？', 'commenter': 'imay'}, {'comment': '把license去掉吧', 'commenter': 'imay'}]"
69,fe/src/com/baidu/palo/catalog/ScalarType.java,"@@ -20,6 +20,20 @@
 
 package com.baidu.palo.catalog;
 
+// Copyright 2012 Cloudera Inc.
+//","[{'comment': '这个同样', 'commenter': 'imay'}]"
69,fe/src/com/baidu/palo/system/SystemInfoService.java,"@@ -987,7 +987,7 @@ public void unregisterObserver(SystemInfoObserver observer) {
 
     public void replayAddBackend(Backend newBackend) {
         // update idToBackend
-        if (Catalog.getCurrentCatalogJournalVersion() < FeMetaVersion.VERSION_23) {
+        if (Catalog.getCurrentCatalogJournalVersion() < FeMetaVersion.VERSION_30) {","[{'comment': '这个为什么要改呢？', 'commenter': 'imay'}]"
89,fe/src/com/baidu/palo/http/BaseAction.java,"@@ -87,6 +86,7 @@ public void setQeService(QeService qeService) {
     @Override
     public void handleRequest(BaseRequest request) throws Exception {
         BaseResponse response = new BaseResponse();
+        LOG.debug(""receive http request. url="", request.getRequest().uri());","[{'comment': '少个 {}', 'commenter': 'morningman'}, {'comment': 'This log should remove', 'commenter': 'imay'}, {'comment': 'Done.', 'commenter': 'lingbin'}, {'comment': 'this log is to show reqeust url to help debug.', 'commenter': 'lingbin'}]"
91,be/src/common/config.h,"@@ -312,6 +312,9 @@ namespace config {
     // ""If true, Kudu features will be disabled.""
     CONF_Bool(disable_kudu, ""false"")
 
+    // to forward compatibility, will be removed later
+    CONF_Bool(disable_deprecated_download, ""false"");","[{'comment': '这个配置名字不好，应该直接说明你做的是什么。比如在1年后，download模型可能更新多次了，而这个配置选项看名字都不知道在干嘛了。\r\n另外，安全功能默认应该是开启的', 'commenter': 'imay'}, {'comment': '修改成enable_token_check了', 'commenter': 'lingbin'}]"
91,be/src/http/download_action.cpp,"@@ -215,5 +224,37 @@ std::string DownloadAction::get_content_type(const std::string& file_name) {
     return std::string();
 }
 
+Status DownloadAction::check_token(HttpRequest *req) {
+    const std::string& token_str = req->param(TOKEN_PARAMETER);
+    if (token_str.empty()) {
+        return Status(""token is not specified."");
+    }
+
+    StringParser::ParseResult parse_result = StringParser::PARSE_SUCCESS;
+    int32_t token = StringParser::string_to_int<int32_t>(
+            token_str.c_str(), token_str.size(), &parse_result);
+    if (parse_result != StringParser::PARSE_SUCCESS) {
+        return Status(""token format is wrong."");
+    }
+
+    int32_t local_token = static_cast<int32_t>(_exec_env->cluster_id());
+    if (token != local_token) {
+        return Status(""invalid token."");
+    }
+
+    return Status::OK;
+}
+
+Status DownloadAction::check_path(const std::string& file_path) {
+    for (auto& allow_path : _allow_paths) {
+        VLOG_ROW << ""allow path: "" << allow_path;","[{'comment': 'Remove this debug log', 'commenter': 'imay'}, {'comment': 'Done.', 'commenter': 'lingbin'}]"
91,be/src/util/filesystem_util.cc,"@@ -170,5 +170,38 @@ uint64_t FileSystemUtil::max_num_file_handles() {
     return 0ul;
 }
 
+// NOTE: the parent_path and sub_path can either dir or file.
+//   return true if patent_path == sub_path
+bool FileSystemUtil::contain_path(
+        const std::string& parent_path, const std::string& sub_path) {
+    boost::filesystem::path parent(parent_path);
+    boost::filesystem::path sub(sub_path);
+    parent = parent.lexically_normal();
+    sub = sub.lexically_normal();
+    VLOG_ROW << ""parent lexically_normal: "" << parent;","[{'comment': 'remove debug log', 'commenter': 'imay'}, {'comment': 'Done.', 'commenter': 'lingbin'}]"
91,be/src/util/filesystem_util.cc,"@@ -170,5 +170,38 @@ uint64_t FileSystemUtil::max_num_file_handles() {
     return 0ul;
 }
 
+// NOTE: the parent_path and sub_path can either dir or file.
+//   return true if patent_path == sub_path
+bool FileSystemUtil::contain_path(
+        const std::string& parent_path, const std::string& sub_path) {
+    boost::filesystem::path parent(parent_path);
+    boost::filesystem::path sub(sub_path);
+    parent = parent.lexically_normal();
+    sub = sub.lexically_normal();
+    VLOG_ROW << ""parent lexically_normal: "" << parent;
+    VLOG_ROW << ""sub lexically_normal: "" << sub;","[{'comment': 'remove debug log', 'commenter': 'imay'}, {'comment': 'Done.', 'commenter': 'lingbin'}]"
246,fe/src/com/baidu/palo/qe/SessionVariable.java,"@@ -403,6 +407,16 @@ public int getMtDop() {
     public void setMtDop(int mtDop) {
         this.mtDop = mtDop;
     }
+
+    public boolean isDisableColocateJoin()","[{'comment': '{ should in the same line\r\npublic boolean isDisableColocateJoin() {', 'commenter': 'imay'}]"
246,gensrc/thrift/PaloInternalService.thrift,"@@ -121,6 +121,9 @@ struct TQueryOptions {
 
   // multithreaded degree of intra-node parallelism 
   27: optional i32 mt_dop = 0;
+
+  //colocate join
+  28: optional bool disable_colocate_join = false;","[{'comment': 'This flag is useless to BE, no need to add this option', 'commenter': 'imay'}]"
246,fe/src/com/baidu/palo/planner/HashJoinNode.java,"@@ -114,6 +115,17 @@ public void setDistributionMode(DistributionMode distrMode) {
         this.distrMode = distrMode;
     }
 
+    public boolean isColocate()
+    {
+        return isColocate;
+    }
+
+    public void setColocate(boolean colocate)
+    {","[{'comment': 'code style ', 'commenter': 'imay'}]"
246,fe/src/com/baidu/palo/planner/DistributedPlanner.java,"@@ -393,6 +412,83 @@ private PlanFragment createHashJoinFragment(HashJoinNode node, PlanFragment righ
         }
     }
 
+    private boolean canColocateJoin(HashJoinNode node, PlanFragment leftChildFragment, PlanFragment rightChildFragment) {
+        if (ctx_.getQueryOptions().disable_colocate_join) {
+            return false;
+        }
+
+        PlanNode leftRoot = leftChildFragment.getPlanRoot();
+        PlanNode rightRoot = rightChildFragment.getPlanRoot();
+
+        //leftRoot could be ScanNode or HashJoinNode, rightRoot should be ScanNode
+        if (leftRoot instanceof OlapScanNode && rightRoot instanceof OlapScanNode) {
+            return canColocateJoin(node, leftRoot, rightRoot);
+        }
+
+        if (leftRoot instanceof HashJoinNode && rightRoot instanceof OlapScanNode) {
+            while (leftRoot instanceof HashJoinNode) {
+                if (((HashJoinNode)leftRoot).isColocate()) {
+                    leftRoot = leftRoot.getChild(0);
+                } else {
+                    return false;
+                }
+            }
+            return canColocateJoin(node, leftRoot, rightRoot);
+        }
+
+        return false;
+    }
+
+
+","[{'comment': 'code style: \r\nshould be one empty line', 'commenter': 'imay'}]"
246,fe/src/com/baidu/palo/alter/DecommissionBackendJob.java,"@@ -433,6 +438,12 @@ public synchronized int tryFinishJob() {
                             continue;
                         }
 
+                        if (olapTable.getColocateTable() != null) {
+                          LOG.info(""{} is colocate table, ColocateCloneChecker will handle the tablet clone"", olapTable.getName());","[{'comment': 'LOG.info will produce too many log? \r\n\r\nchange this to debug? or drop this log?', 'commenter': 'imay'}]"
246,fe/src/com/baidu/palo/catalog/Catalog.java,"@@ -3121,6 +3162,60 @@ private Table createOlapTable(Database db, CreateTableStmt stmt, boolean isResto
             throw new DdlException(e.getMessage());
         }
 
+        // colocateTable
+        try {
+            String colocateTable = PropertyAnalyzer.analyzeColocate(properties);
+            if (colocateTable != null) {
+                Table parentTable;
+                db.readLock();
+                try {
+                    parentTable = db.getTable(colocateTable);
+                } finally {
+                    db.readUnlock();
+                }
+
+                //for colocate child table
+                if (!colocateTable.equalsIgnoreCase(tableName)) {
+                    if (parentTable == null) {
+                      ErrorReport.reportDdlException(ErrorCode.ERR_COLOCATE_TABLE_NO_EXIT, colocateTable);
+                    }
+
+                    if (parentTable.type != (TableType.OLAP)) {
+                      ErrorReport.reportDdlException(ErrorCode.ERR_COLOCATE_TABLE_MUST_OLAP_TABLE);","[{'comment': 'code style: \r\n4 space indentation', 'commenter': 'imay'}]"
246,fe/src/com/baidu/palo/clone/BackendInfo.java,"@@ -0,0 +1,95 @@
+// Copyright (c) 2017, Baidu.com, Inc. All Rights Reserved
+
+// Licensed under the Apache License, Version 2.0 (the ""License"");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package com.baidu.palo.clone;
+
+public class BackendInfo","[{'comment': 'code style', 'commenter': 'imay'}]"
246,fe/src/com/baidu/palo/catalog/Catalog.java,"@@ -2569,6 +2601,13 @@ public void addPartition(Database db, String tableName, AddPartitionClause addPa
             // here we check partition's properties
             singlePartitionDesc.analyze(rangePartitionInfo.getPartitionColumns().size(), null);
 
+            if (olapTable.getColocateTable() != null) {
+                short tableReplicationNum = rangePartitionInfo.idToReplicationNum.entrySet().iterator().next().getValue();
+                if (tableReplicationNum != singlePartitionDesc.getReplicationNum()) {
+                    ErrorReport.reportDdlException(ErrorCode.ERR_COLOCATE_TABLE_MUST_SAME_REPLICAT_NUM, partitionName);","[{'comment': 'ERR_COLOCATE_TABLE_MUST_SAME_REPLICAT_NUM\r\nmisspelling REPLICAT', 'commenter': 'morningman'}]"
246,fe/src/main/java/org/apache/doris/analysis/ModifyTablePropertiesClause.java,"@@ -44,9 +45,15 @@ public void analyze(Analyzer analyzer) throws AnalysisException {
             throw new AnalysisException(""Properties is not set"");
         }
 
-        if (!Catalog.getCurrentCatalog().getAuth().checkGlobalPriv(ConnectContext.get(), PrivPredicate.ALTER)) {
+        if (properties.size() == 1 && properties.containsKey(KEY_COLOCATE_WITH)) {","[{'comment': ""This version has changed authorization, you need't add these code"", 'commenter': 'imay'}, {'comment': ""Currently, alter table colocate_with property need global ALTER Privilege.  so normal user  couldn't alter and we shouldn't grant  normal user  global ALTER Privilege."", 'commenter': 'kangkaisen'}, {'comment': ""Why? If one user have ALTER privilege for two tables, it's reasonable that he could make these two tables colocated."", 'commenter': 'imay'}]"
246,fe/src/main/java/org/apache/doris/catalog/Catalog.java,"@@ -2712,6 +2744,13 @@ public void addPartition(Database db, String tableName, AddPartitionClause addPa
             // here we check partition's properties
             singlePartitionDesc.analyze(rangePartitionInfo.getPartitionColumns().size(), null);
 
+            if (olapTable.getColocateTable() != null) {
+                short tableReplicationNum = rangePartitionInfo.idToReplicationNum.entrySet().iterator().next().getValue();
+                if (tableReplicationNum != singlePartitionDesc.getReplicationNum()) {
+                    ErrorReport.reportDdlException(ErrorCode.ERR_COLOCATE_TABLE_MUST_SAME_REPLICAT_NUM, partitionName);
+                }
+            }
+","[{'comment': 'I think there is need to check bucket number.\r\nsame to ` modifyPartition`', 'commenter': 'imay'}, {'comment': 'Yes. OK.', 'commenter': 'kangkaisen'}]"
246,fe/src/main/java/org/apache/doris/catalog/Catalog.java,"@@ -3261,6 +3302,59 @@ private Table createOlapTable(Database db, CreateTableStmt stmt, boolean isResto
             throw new DdlException(e.getMessage());
         }
 
+        // colocateTable
+        try {
+            String colocateTable = PropertyAnalyzer.analyzeColocate(properties);
+            if (colocateTable != null) {
+                Table parentTable;
+                db.readLock();
+                try {
+                    parentTable = db.getTable(colocateTable);
+                } finally {
+                    db.readUnlock();
+                }
+
+                //for colocate child table
+                if (!colocateTable.equalsIgnoreCase(tableName)) {
+                    if (parentTable == null) {
+                        ErrorReport.reportDdlException(ErrorCode.ERR_COLOCATE_TABLE_NO_EXIT, colocateTable);
+                    }
+
+                    if (parentTable.type != (TableType.OLAP)) {
+                        ErrorReport.reportDdlException(ErrorCode.ERR_COLOCATE_TABLE_MUST_OLAP_TABLE);
+                    }
+
+                    HashDistributionInfo parentDistribution = (HashDistributionInfo) ((OlapTable) parentTable).getDefaultDistributionInfo();
+                    if (parentDistribution.getBucketNum() != distributionInfo.getBucketNum()) {
+                        ErrorReport.reportDdlException(ErrorCode.ERR_COLOCATE_TABLE_MUST_SAME_BUCKNUM);","[{'comment': ""It's better to throw two `getBucketNum` when error happened, which is useful for user."", 'commenter': 'imay'}, {'comment': 'OK', 'commenter': 'kangkaisen'}]"
246,fe/src/main/java/org/apache/doris/catalog/Catalog.java,"@@ -3261,6 +3302,59 @@ private Table createOlapTable(Database db, CreateTableStmt stmt, boolean isResto
             throw new DdlException(e.getMessage());
         }
 
+        // colocateTable
+        try {
+            String colocateTable = PropertyAnalyzer.analyzeColocate(properties);
+            if (colocateTable != null) {
+                Table parentTable;
+                db.readLock();
+                try {
+                    parentTable = db.getTable(colocateTable);
+                } finally {
+                    db.readUnlock();
+                }
+
+                //for colocate child table
+                if (!colocateTable.equalsIgnoreCase(tableName)) {
+                    if (parentTable == null) {
+                        ErrorReport.reportDdlException(ErrorCode.ERR_COLOCATE_TABLE_NO_EXIT, colocateTable);
+                    }
+
+                    if (parentTable.type != (TableType.OLAP)) {
+                        ErrorReport.reportDdlException(ErrorCode.ERR_COLOCATE_TABLE_MUST_OLAP_TABLE);
+                    }
+
+                    HashDistributionInfo parentDistribution = (HashDistributionInfo) ((OlapTable) parentTable).getDefaultDistributionInfo();
+                    if (parentDistribution.getBucketNum() != distributionInfo.getBucketNum()) {
+                        ErrorReport.reportDdlException(ErrorCode.ERR_COLOCATE_TABLE_MUST_SAME_BUCKNUM);
+                    }
+
+                    List<Column> parentColumns = parentDistribution.getDistributionColumns();
+                    List<Column> columns = ((HashDistributionInfo) distributionInfo).getDistributionColumns();
+
+                    int columnSize = parentColumns.size();
+                    if (columnSize != columns.size()) {
+                        ErrorReport.reportDdlException(ErrorCode.ERR_COLOCATE_TABLE_MUST_SAME_DISTRIBUTED_COLUMNS);
+                    }
+
+                    for (int i = 0; i < columnSize; i++) {
+                        if (!parentColumns.get(i).getColumnType().equals(columns.get(i).getColumnType())) {
+                            ErrorReport.reportDdlException(ErrorCode.ERR_COLOCATE_TABLE_MUST_SAME_DISTRIBUTED_COLUMNS);","[{'comment': 'Better to tell user `column_name` and `column_type`', 'commenter': 'imay'}, {'comment': 'OK', 'commenter': 'kangkaisen'}]"
246,fe/src/main/java/org/apache/doris/catalog/Catalog.java,"@@ -3334,7 +3428,14 @@ private Table createOlapTable(Database db, CreateTableStmt stmt, boolean isResto
                     // just for remove entries in stmt.getProperties(),
                     // and then check if there still has unknown properties
                     PropertyAnalyzer.analyzeDataProperty(stmt.getProperties(), DataProperty.DEFAULT_HDD_DATA_PROPERTY);
-                    PropertyAnalyzer.analyzeReplicationNum(properties, FeConstants.default_replication_num);
+                    Short replicationNum = PropertyAnalyzer.analyzeReplicationNum(properties, FeConstants.default_replication_num);","[{'comment': ""replicationNum here is useless, replication value is got from `partitionInfo.getReplicationNum(),`\r\n\r\nAnd, you haven't check unpartitioned table, and number of bucket."", 'commenter': 'imay'}, {'comment': 'Yes. OK', 'commenter': 'kangkaisen'}]"
246,fe/src/main/java/org/apache/doris/catalog/Catalog.java,"@@ -3376,12 +3477,36 @@ private Table createOlapTable(Database db, CreateTableStmt stmt, boolean isResto
                 if (!db.createTableWithLock(olapTable, false, stmt.isSetIfNotExists())) {
                     ErrorReport.reportDdlException(ErrorCode.ERR_CANT_CREATE_TABLE, tableName, ""table already exists"");
                 }
+
+                //we have added these index to memory, only need to persist here
+                if (olapTable.getColocateTable() != null) {
+                    Long groupId = db.getTable(olapTable.getColocateTable()).getId();","[{'comment': 'db is not locked', 'commenter': 'imay'}, {'comment': 'OK', 'commenter': 'kangkaisen'}]"
246,fe/src/main/java/org/apache/doris/catalog/Catalog.java,"@@ -3376,12 +3477,36 @@ private Table createOlapTable(Database db, CreateTableStmt stmt, boolean isResto
                 if (!db.createTableWithLock(olapTable, false, stmt.isSetIfNotExists())) {
                     ErrorReport.reportDdlException(ErrorCode.ERR_CANT_CREATE_TABLE, tableName, ""table already exists"");
                 }
+
+                //we have added these index to memory, only need to persist here
+                if (olapTable.getColocateTable() != null) {
+                    Long groupId = db.getTable(olapTable.getColocateTable()).getId();
+                    ColocatePersistInfo info = ColocatePersistInfo.CreateForAddTable(tableId, groupId, db.getId());
+                    editLog.logColocateAddTable(info);
+
+                    if (getColocateTableIndex().isColocateParentTable(tableId)) {
+                        List<List<Long>> backendsPerBucketSeq = getColocateTableIndex().getBackendsPerBucketSeq(groupId);
+                        info = ColocatePersistInfo.CreateForBackendsPerBucketSeq(groupId, backendsPerBucketSeq);
+                        editLog.logColocateBackendsPerBucketSeq(info);
+                    }","[{'comment': ""It's bad to write two logs for one operation. \r\n\r\nIf second log failed, first log can't rollback, and this maybe lead to wrong system status"", 'commenter': 'imay'}, {'comment': 'Yes. I see.\r\n\r\nI could merge `logColocateAddTable` and `logColocateBackendsPerBucketSeq` to one ,but `createTableWithLock` still have on log. how do we handle this better ?', 'commenter': 'kangkaisen'}, {'comment': 'We must keep metadata keep consistent. If one operation failed, we should guarantee that thing can still work.\r\n\r\nFollowing is a suggest.\r\n\r\nIf we `createTableWithLock ` first, and log another operation `logChangeTableToColocate`.  And     if the second log failed, we also have complete meta, which have a table without colocate property. \r\n\r\nCan this method work and be implemented easily. ', 'commenter': 'imay'}]"
246,fe/src/main/java/org/apache/doris/catalog/Catalog.java,"@@ -4542,6 +4722,34 @@ public void replayRenameTable(TableInfo tableInfo) throws DdlException {
         }
     }
 
+    //the invoker should keep db write lock
+    public void modifyTableColocate(Database db, OlapTable table, String colocateTable) throws DdlException {","[{'comment': '1. db lock is not locked\r\n2. why colocate condition is not check.', 'commenter': 'imay'}, {'comment': ""1 schemaChangeHandler.process has kept  the db write lock, so we needn't add lock here.\r\n\r\n2 OK, I will add a check"", 'commenter': 'kangkaisen'}]"
246,fe/src/main/java/org/apache/doris/planner/DistributedPlanner.java,"@@ -395,6 +414,81 @@ private PlanFragment createHashJoinFragment(HashJoinNode node, PlanFragment righ
         }
     }
 
+    private boolean canColocateJoin(HashJoinNode node, PlanFragment leftChildFragment, PlanFragment rightChildFragment) {
+        if (ConnectContext.get().getSessionVariable().isDisableColocateJoin()) {
+            return false;
+        }
+
+        PlanNode leftRoot = leftChildFragment.getPlanRoot();
+        PlanNode rightRoot = rightChildFragment.getPlanRoot();
+
+        //leftRoot could be ScanNode or HashJoinNode, rightRoot should be ScanNode
+        if (leftRoot instanceof OlapScanNode && rightRoot instanceof OlapScanNode) {
+            return canColocateJoin(node, leftRoot, rightRoot);
+        }
+
+        if (leftRoot instanceof HashJoinNode && rightRoot instanceof OlapScanNode) {
+            while (leftRoot instanceof HashJoinNode) {
+                if (((HashJoinNode)leftRoot).isColocate()) {
+                    leftRoot = leftRoot.getChild(0);
+                } else {
+                    return false;
+                }
+            }","[{'comment': ""Other node can be `HashJoinNode`'s left node, so you should check `leftRoot`'s type.\r\n\r\nFor example: select * from (select id from t1 group id) a, (select id from t2 group by id) b where a.id = b.id;"", 'commenter': 'imay'}, {'comment': 'OK.', 'commenter': 'kangkaisen'}]"
246,fe/src/main/java/org/apache/doris/planner/DistributedPlanner.java,"@@ -395,6 +414,81 @@ private PlanFragment createHashJoinFragment(HashJoinNode node, PlanFragment righ
         }
     }
 
+    private boolean canColocateJoin(HashJoinNode node, PlanFragment leftChildFragment, PlanFragment rightChildFragment) {
+        if (ConnectContext.get().getSessionVariable().isDisableColocateJoin()) {
+            return false;
+        }
+
+        PlanNode leftRoot = leftChildFragment.getPlanRoot();
+        PlanNode rightRoot = rightChildFragment.getPlanRoot();
+
+        //leftRoot could be ScanNode or HashJoinNode, rightRoot should be ScanNode
+        if (leftRoot instanceof OlapScanNode && rightRoot instanceof OlapScanNode) {
+            return canColocateJoin(node, leftRoot, rightRoot);
+        }
+
+        if (leftRoot instanceof HashJoinNode && rightRoot instanceof OlapScanNode) {
+            while (leftRoot instanceof HashJoinNode) {
+                if (((HashJoinNode)leftRoot).isColocate()) {
+                    leftRoot = leftRoot.getChild(0);
+                } else {
+                    return false;
+                }
+            }
+            return canColocateJoin(node, leftRoot, rightRoot);
+        }
+
+        return false;
+    }
+
+    //the table must be colocate
+    //the colocate group must be stable
+    //the eqJoinConjuncts must contain the distributionColumns
+    private boolean canColocateJoin(HashJoinNode node, PlanNode leftRoot, PlanNode rightRoot) {
+        OlapTable leftTable = ((OlapScanNode) leftRoot).getOlapTable();
+        OlapTable rightTable = ((OlapScanNode) rightRoot).getOlapTable();
+
+        //1 the table must be colocate
+        if (leftTable.getColocateTable() != null &&
+                leftTable.getColocateTable().equalsIgnoreCase(rightTable.getColocateTable())) {","[{'comment': '```suggestion\r\n     if (leftTable.getColocateTable() == null ||\r\n                !leftTable.getColocateTable().equalsIgnoreCase(rightTable.getColocateTable())) {\r\n         return false;\r\n     }\r\n```\r\nI think this is better', 'commenter': 'imay'}, {'comment': 'OK', 'commenter': 'kangkaisen'}]"
247,README.md,"@@ -1,6 +1,6 @@
 # Introduction to Palo
 
-Palo is an MPP-based interactive SQL data warehousing for reporting and analysis. Palo mainly integrates the technology of Google Mesa and Apache Impala. Unlike other popular SQL-on-Hadoop systems, Palo is designed to be a simple and single tightly coupled system, not depending on other systems. Palo not only provides high concurrent low latency point query performance, but also provides high throughput queries of ad-hoc analysis. Palo not only provides batch data loading, but also provides near real-time mini-batch data loading. Palo also provides high availability, reliability, fault tolerance, and scalability. The simplicity (of developing, deploying and using) and meeting many data serving requirements in single system are the main features of Palo.
+Palo is a MPP-based interactive SQL data warehousing for reporting and analysis. Palo mainly integrates the technology of Google Mesa and Apache Impala. Unlike other popular SQL-on-Hadoop systems, Palo is designed to be a simple and single tightly coupled system, not depending on other systems. Palo not only provides high concurrent low latency point query performance, but also provides high throughput queries of ad-hoc analysis. Palo not only provides batch data loading, but also provides near real-time mini-batch data loading. Palo also provides high availability, reliability, fault tolerance, and scalability. The simplicity (of developing, deploying and using) and meeting many data serving requirements in single system are the main features of Palo.","[{'comment': 'an MPP, not a MPP', 'commenter': 'morningman'}]"
252,fe/src/main/cup/sql_parser.cup,"@@ -205,9 +205,9 @@ terminal String KW_ADD, KW_ADMIN, KW_AFTER, KW_AGGREGATE, KW_ALL, KW_ALTER, KW_A
     KW_INNER, KW_INSERT, KW_INT, KW_INTERVAL, KW_INTO, KW_IS, KW_ISNULL,  KW_ISOLATION,
     KW_JOIN,
     KW_KEY, KW_KILL,
-    KW_LABEL, KW_LARGEINT, KW_LEFT, KW_LESS, KW_LEVEL, KW_LIKE, KW_LIMIT, KW_LINK, KW_LOAD, KW_LOCAL, KW_LOCATION,
+    KW_LABEL, KW_LARGEINT, KW_LAST, KW_LEFT, KW_LESS, KW_LEVEL, KW_LIKE, KW_LIMIT, KW_LINK, KW_LOAD, KW_LOCAL, KW_LOCATION,
     KW_MAX, KW_MAX_VALUE, KW_MERGE, KW_MIN, KW_MIGRATE, KW_MIGRATIONS, KW_MODIFY,
-    KW_NAME, KW_NAMES, KW_NEGATIVE, KW_NO, KW_NOT, KW_NULL,
+    KW_NAME, KW_NAMES, KW_NEGATIVE, KW_NO, KW_NOT, KW_NULL, KW_NULLS,","[{'comment': ""can you add KW_LAST an KW_NULLS to keyword grammar? \r\n\r\nIf you don't add, old query may failed if there SQL contain these two words "", 'commenter': 'imay'}, {'comment': 'I think I had added  KW_LAST an KW_NULLS to keyword grammar. Did I miss something?', 'commenter': 'kangkaisen'}, {'comment': 'You should add these here. \r\n\r\n// Keyword that we allow for identifiers\r\nkeyword ::=\r\n    KW_AFTER:id\r\n    {: RESULT = id; :}\r\n    | KW_AGGREGATE:id\r\n    {: RESULT = id; :}\r\n    | KW_AUTHORS:id\r\n\r\n', 'commenter': 'imay'}, {'comment': 'I See, Thanks you!', 'commenter': 'kangkaisen'}]"
255,fe/src/main/java/org/apache/doris/rewrite/FoldConstantsRule.java,"@@ -0,0 +1,233 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.rewrite;
+
+
+import com.google.common.collect.ArrayListMultimap;
+import com.google.common.collect.Multimap;
+import org.apache.doris.analysis.Analyzer;
+import org.apache.doris.analysis.ArithmeticExpr;
+import org.apache.doris.analysis.CastExpr;
+import org.apache.doris.analysis.Expr;
+import org.apache.doris.analysis.FunctionCallExpr;
+import org.apache.doris.analysis.LiteralExpr;
+import org.apache.doris.analysis.NullLiteral;
+import org.apache.doris.catalog.Function;
+import org.apache.doris.catalog.ScalarType;
+import org.apache.doris.catalog.Type;
+import org.apache.doris.common.AnalysisException;
+
+import java.lang.reflect.InvocationTargetException;
+import java.lang.reflect.Method;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.List;
+import java.util.Objects;
+
+/**
+ * This rule replaces a constant Expr with its equivalent LiteralExpr by evaluating the
+ * Expr in the BE. Exprs that are already LiteralExprs are not changed.
+ *
+ * TODO: Expressions fed into this rule are currently not required to be analyzed
+ * in order to support constant folding in expressions that contain unresolved
+ * references to select-list aliases (such expressions cannot be analyzed).
+ * The cross-dependencies between rule transformations and analysis are vague at the
+ * moment and make rule application overly complicated.
+ *
+ * Examples:
+ * 1 + 1 + 1 --> 3
+ * toupper('abc') --> 'ABC'
+ * cast('2016-11-09' as timestamp) --> TIMESTAMP '2016-11-09 00:00:00'
+ */
+public class FoldConstantsRule implements ExprRewriteRule {
+    public static ExprRewriteRule INSTANCE = new FoldConstantsRule();
+
+    private Multimap<String, FEFunctionInvoker> functions = ArrayListMultimap.create();
+
+    @Override
+    public Expr apply(Expr expr, Analyzer analyzer) throws AnalysisException {
+        if (functions.isEmpty()) {
+            registerFunctions();
+        }
+
+        // Avoid calling Expr.isConstant() because that would lead to repeated traversals
+        // of the Expr tree. Assumes the bottom-up application of this rule. Constant
+        // children should have been folded at this point.
+        for (Expr child : expr.getChildren()) {
+            if (!child.isLiteral() && !(child instanceof CastExpr)) {
+                return expr;
+            }
+        }
+
+        if (expr.isLiteral() || !expr.isConstant()) {
+            return expr;
+        }
+
+        // Do not constant fold cast(null as dataType) because we cannot preserve the
+        // cast-to-types and that can lead to query failures, e.g., CTAS
+        if (expr instanceof CastExpr) {
+            CastExpr castExpr = (CastExpr) expr;
+            if (castExpr.getChild(0) instanceof NullLiteral) {
+                return expr;
+            }
+        }
+        // Analyze constant exprs, if necessary. Note that the 'expr' may become non-constant
+        // after analysis (e.g., aggregate functions).
+        if (!expr.isAnalyzed()) {
+            expr.analyze(analyzer);
+            if (!expr.isConstant()) {
+                return expr;
+            }
+        }
+        return simplify(expr);
+    }
+
+    private Expr simplify(Expr constExpr) throws AnalysisException {
+        if (constExpr instanceof ArithmeticExpr
+                || constExpr instanceof FunctionCallExpr
+                || constExpr instanceof CastExpr) {
+            Function fn = constExpr.getFn();
+            List<ScalarType> argTypes = new ArrayList<>();
+            for (Type type : fn.getArgs()) {
+                argTypes.add((ScalarType) type);
+            }
+            FEFunctionSignature signature = new FEFunctionSignature(fn.functionName(),
+                    argTypes.toArray(new ScalarType[argTypes.size()]), (ScalarType) fn.getReturnType());
+            FEFunctionInvoker invoker = getFunction(signature);
+            if (invoker != null) {
+                try {
+                    return invoker.invoke(constExpr.getChildrenWithoutCast());
+                } catch (AnalysisException e) {
+                    return constExpr;
+                }
+            }
+        }
+        return constExpr;
+    }
+
+    private FEFunctionInvoker getFunction(FEFunctionSignature signature) {
+        Collection<FEFunctionInvoker> functionInvokers = functions.get(signature.getName());
+
+        if (functionInvokers == null) {
+            return null;
+        }
+
+        for (FEFunctionInvoker invoker : functionInvokers) {
+            if (!invoker.getSignature().returnType.equals(signature.getReturnType())) {
+                continue;
+            }
+
+            ScalarType[] argTypes1 = invoker.getSignature().getArgTypes();
+            ScalarType[] argTypes2 = signature.getArgTypes();
+
+            if (!Arrays.equals(argTypes1, argTypes2)) {
+                continue;
+            }
+
+            return invoker;
+        }
+        return null;
+    }
+
+    private void registerFunctions() {","[{'comment': 'This should be synchronized? \r\nNow, there may concurrent calling', 'commenter': 'imay'}, {'comment': 'Yes.  Fixed.', 'commenter': 'kangkaisen'}]"
255,fe/src/main/java/org/apache/doris/rewrite/FoldConstantsRule.java,"@@ -0,0 +1,233 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.rewrite;
+
+
+import com.google.common.collect.ArrayListMultimap;
+import com.google.common.collect.Multimap;
+import org.apache.doris.analysis.Analyzer;
+import org.apache.doris.analysis.ArithmeticExpr;
+import org.apache.doris.analysis.CastExpr;
+import org.apache.doris.analysis.Expr;
+import org.apache.doris.analysis.FunctionCallExpr;
+import org.apache.doris.analysis.LiteralExpr;
+import org.apache.doris.analysis.NullLiteral;
+import org.apache.doris.catalog.Function;
+import org.apache.doris.catalog.ScalarType;
+import org.apache.doris.catalog.Type;
+import org.apache.doris.common.AnalysisException;
+
+import java.lang.reflect.InvocationTargetException;
+import java.lang.reflect.Method;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.List;
+import java.util.Objects;
+
+/**
+ * This rule replaces a constant Expr with its equivalent LiteralExpr by evaluating the
+ * Expr in the BE. Exprs that are already LiteralExprs are not changed.
+ *
+ * TODO: Expressions fed into this rule are currently not required to be analyzed
+ * in order to support constant folding in expressions that contain unresolved
+ * references to select-list aliases (such expressions cannot be analyzed).
+ * The cross-dependencies between rule transformations and analysis are vague at the
+ * moment and make rule application overly complicated.
+ *
+ * Examples:
+ * 1 + 1 + 1 --> 3
+ * toupper('abc') --> 'ABC'
+ * cast('2016-11-09' as timestamp) --> TIMESTAMP '2016-11-09 00:00:00'
+ */
+public class FoldConstantsRule implements ExprRewriteRule {
+    public static ExprRewriteRule INSTANCE = new FoldConstantsRule();
+
+    private Multimap<String, FEFunctionInvoker> functions = ArrayListMultimap.create();
+
+    @Override
+    public Expr apply(Expr expr, Analyzer analyzer) throws AnalysisException {
+        if (functions.isEmpty()) {
+            registerFunctions();
+        }
+
+        // Avoid calling Expr.isConstant() because that would lead to repeated traversals
+        // of the Expr tree. Assumes the bottom-up application of this rule. Constant
+        // children should have been folded at this point.
+        for (Expr child : expr.getChildren()) {
+            if (!child.isLiteral() && !(child instanceof CastExpr)) {
+                return expr;
+            }
+        }
+
+        if (expr.isLiteral() || !expr.isConstant()) {
+            return expr;
+        }
+
+        // Do not constant fold cast(null as dataType) because we cannot preserve the
+        // cast-to-types and that can lead to query failures, e.g., CTAS
+        if (expr instanceof CastExpr) {
+            CastExpr castExpr = (CastExpr) expr;
+            if (castExpr.getChild(0) instanceof NullLiteral) {
+                return expr;
+            }
+        }
+        // Analyze constant exprs, if necessary. Note that the 'expr' may become non-constant
+        // after analysis (e.g., aggregate functions).
+        if (!expr.isAnalyzed()) {
+            expr.analyze(analyzer);
+            if (!expr.isConstant()) {
+                return expr;
+            }
+        }
+        return simplify(expr);
+    }
+
+    private Expr simplify(Expr constExpr) throws AnalysisException {
+        if (constExpr instanceof ArithmeticExpr
+                || constExpr instanceof FunctionCallExpr
+                || constExpr instanceof CastExpr) {
+            Function fn = constExpr.getFn();
+            List<ScalarType> argTypes = new ArrayList<>();
+            for (Type type : fn.getArgs()) {
+                argTypes.add((ScalarType) type);
+            }
+            FEFunctionSignature signature = new FEFunctionSignature(fn.functionName(),
+                    argTypes.toArray(new ScalarType[argTypes.size()]), (ScalarType) fn.getReturnType());
+            FEFunctionInvoker invoker = getFunction(signature);
+            if (invoker != null) {
+                try {
+                    return invoker.invoke(constExpr.getChildrenWithoutCast());
+                } catch (AnalysisException e) {
+                    return constExpr;
+                }
+            }
+        }
+        return constExpr;
+    }
+
+    private FEFunctionInvoker getFunction(FEFunctionSignature signature) {
+        Collection<FEFunctionInvoker> functionInvokers = functions.get(signature.getName());
+
+        if (functionInvokers == null) {
+            return null;
+        }
+
+        for (FEFunctionInvoker invoker : functionInvokers) {
+            if (!invoker.getSignature().returnType.equals(signature.getReturnType())) {
+                continue;
+            }
+
+            ScalarType[] argTypes1 = invoker.getSignature().getArgTypes();
+            ScalarType[] argTypes2 = signature.getArgTypes();
+
+            if (!Arrays.equals(argTypes1, argTypes2)) {
+                continue;
+            }
+
+            return invoker;
+        }
+        return null;
+    }
+
+    private synchronized void registerFunctions() {","[{'comment': 'first you need to check if the map is empty() too. Other one function will be added multiple times', 'commenter': 'imay'}, {'comment': 'Yes. which like singleton-pattern. I have updated. \r\nBut in fact, even if one function added multiple times, the `getFunction` would still get a right Function.', 'commenter': 'kangkaisen'}]"
256,fe/src/main/java/org/apache/doris/planner/OlapScanNode.java,"@@ -447,6 +465,13 @@ private void addScanRangeLocations(Partition partition,
                 scanRangeLocations.addToLocations(scanRangeLocation);
                 paloRange.addToHosts(new TNetworkAddress(ip, port));
                 tabletIsNull = false;
+
+                //for CBO
+                if (replica.getRowCount() != -1) {
+                    cardinality += replica.getRowCount();
+                    totalBytes += replica.getDataSize();
+                }
+                scanBackendIds.add(backend.getId());","[{'comment': ""OlapScanNode will put one tablet's all replicas to paloRange, because Coordinator will balance these replicas and try to make that each backend will scan same rows.\r\n\r\nHowever, not all replicas will be read. If table has 3 replica, only one can be read. And this patch will add scan rows three times.\r\n\r\nSo, you can change cardinality and totalBytes outsize for each tablet other than replica."", 'commenter': 'imay'}, {'comment': 'Yes. I see.  I know only one replica will be read.\r\nthe `cardinality ` and `avgRowSize` is only for one replica.  the `cardinality ` and `avgRowSize` will divide replication_num in `computeStats`.', 'commenter': 'kangkaisen'}, {'comment': ""I think dividing `FeConstants.default_replication_num` is not a good way. \r\n\r\nBecause table's number of replica maybe not equal with it, and even they are equal, when balancing or some BE is down, `tablet.getQueryableReplicas` won't return replica which isn't available. And then, `computeStats` will get a wrong `cardinality`."", 'commenter': 'imay'}, {'comment': 'I agree with you. ', 'commenter': 'kangkaisen'}]"
264,be/src/aes/aes.cpp,"@@ -0,0 +1,171 @@
+// Copyright (c) 2018, Baidu.com, Inc. All Rights Reserved","[{'comment': ""please use ASF's header"", 'commenter': 'imay'}, {'comment': 'done', 'commenter': 'kangpinghuang'}]"
264,be/CMakeLists.txt,"@@ -585,6 +585,7 @@ if (${MAKE_TEST} STREQUAL ""ON"")
     add_subdirectory(${TEST_DIR}/exprs)
     add_subdirectory(${TEST_DIR}/runtime)
     add_subdirectory(${TEST_DIR}/http)
+    add_subdirectory(${TEST_DIR}/aes)","[{'comment': ""I think we don't need this 'aes' directory\r\n\r\nyou can put aes.h and aes.cpp to be/src/util, and rename these to aes_util.h??"", 'commenter': 'imay'}, {'comment': 'done', 'commenter': 'kangpinghuang'}]"
264,be/src/aes/aes.cpp,"@@ -0,0 +1,171 @@
+// Copyright (c) 2018, Baidu.com, Inc. All Rights Reserved
+
+// Licensed under the Apache License, Version 2.0 (the ""License"");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""aes.h""
+#include  <cstring>
+#include  <string>
+#include  <memory>
+#include  <iostream>
+
+#include <openssl/aes.h>
+#include <openssl/evp.h>
+#include <openssl/err.h>
+
+#include ""exprs/base64.h""
+
+namespace palo {
+
+static const int AES_MAX_KEY_LENGTH = 256;
+
+const EVP_CIPHER* get_evp_type(const AesMode mode) {
+    switch (mode) {
+        case AES_128_ECB:
+            return EVP_aes_128_ecb();
+        case AES_128_CBC:
+            return EVP_aes_128_cbc();
+        case AES_192_ECB:
+            return EVP_aes_192_ecb();
+        case AES_192_CBC:
+            return EVP_aes_192_cbc();
+        case AES_256_ECB:
+            return EVP_aes_256_ecb();
+        case AES_256_CBC:
+            return EVP_aes_256_cbc();
+        default:
+            return NULL;
+    }
+}
+
+static uint aes_mode_key_sizes[]= {
+    128 /* AES_128_ECB */, 
+    192 /* AES_192_ECB */, 
+    256 /* AES_256_ECB */, 
+    128 /* AES_128_CBC */, 
+    192 /* AES_192_CBC */, 
+    256 /* AES_256_CBC */
+};
+
+void aes_create_key(const unsigned char* origin_key, uint32_t key_length,
+        uint8_t* encrypt_key, AesMode mode) {
+    const uint key_size= aes_mode_key_sizes[mode] / 8;
+    uint8_t *origin_key_end= ((uint8_t*)origin_key) + key_length; /* origin key boundary*/
+    uint8_t *encrypt_key_end; /* encrypt key boundary */
+    encrypt_key_end= encrypt_key + key_size;
+
+    std::memset(encrypt_key, 0, key_size);          /* initialize key  */
+
+    uint8_t *ptr; /* Start of the encrypt key*/
+    uint8_t *origin_ptr; /* Start of the origin key */
+    for (ptr = encrypt_key, origin_ptr = (uint8_t*)origin_key;
+            origin_ptr < origin_key_end; ptr++, origin_ptr++) {
+        if (ptr == encrypt_key_end) {
+            /* loop over origin key until we used all key */
+            ptr = encrypt_key;
+        }
+        *ptr ^= *origin_ptr;
+    }
+}
+
+int do_encrypt(EVP_CIPHER_CTX* aes_ctx, const EVP_CIPHER* cipher,
+        const unsigned char* source, uint32_t source_length, const unsigned char* encrypt_key,
+        const unsigned char* iv, bool padding, unsigned char* encrypt, int* u_len, int* f_len) {
+    int ret = EVP_EncryptInit(aes_ctx, cipher, encrypt_key, iv);
+    if (!ret) {
+        return ret;
+    }
+    ret = EVP_CIPHER_CTX_set_padding(aes_ctx, padding);
+    if (!ret) {
+        return ret;
+    }
+    ret = EVP_EncryptUpdate(aes_ctx, encrypt, u_len, source, source_length);
+    if (!ret) {
+        return ret;
+    }
+    ret = EVP_EncryptFinal(aes_ctx, encrypt + *u_len, f_len);
+    return ret;
+}
+
+int AesTool::encrypt(AesMode mode, const unsigned char* source, uint32_t source_length,
+        const unsigned char* key, uint32_t key_length, const unsigned char* iv,
+        bool padding, unsigned char* encrypt) {
+    EVP_CIPHER_CTX aes_ctx;
+    const EVP_CIPHER* cipher= get_evp_type(mode);
+    int u_len, f_len;
+    /* The encrypt key to be used for encryption */
+    unsigned char encrypt_key[AES_MAX_KEY_LENGTH / 8];
+    aes_create_key(key, key_length, encrypt_key, mode);
+    
+    if (!cipher || (EVP_CIPHER_iv_length(cipher) > 0 && !iv)) {
+        return AES_BAD_DATA;
+    }
+    EVP_CIPHER_CTX_init(&aes_ctx);
+    int ret = do_encrypt(&aes_ctx, cipher, source,
+            source_length, encrypt_key, iv, padding, encrypt, &u_len, &f_len);
+    EVP_CIPHER_CTX_cleanup(&aes_ctx);
+    if (!ret) {
+        ERR_clear_error();
+        return AES_BAD_DATA;
+    } else {
+        return u_len + f_len;
+    }
+}
+
+int do_decrypt(EVP_CIPHER_CTX* aes_ctx, const EVP_CIPHER* cipher,","[{'comment': 'this function should be static ', 'commenter': 'imay'}, {'comment': 'done', 'commenter': 'kangpinghuang'}]"
264,be/src/util/aes_util.cpp,"@@ -0,0 +1,174 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""aes_util.h""
+#include  <cstring>
+#include  <string>
+#include  <memory>
+#include  <iostream>
+
+#include <openssl/aes.h>
+#include <openssl/evp.h>
+#include <openssl/err.h>
+
+#include ""exprs/base64.h""
+
+namespace palo {
+
+static const int AES_MAX_KEY_LENGTH = 256;
+
+const EVP_CIPHER* get_evp_type(const AesMode mode) {
+    switch (mode) {
+        case AES_128_ECB:
+            return EVP_aes_128_ecb();
+        case AES_128_CBC:
+            return EVP_aes_128_cbc();
+        case AES_192_ECB:
+            return EVP_aes_192_ecb();
+        case AES_192_CBC:
+            return EVP_aes_192_cbc();
+        case AES_256_ECB:
+            return EVP_aes_256_ecb();
+        case AES_256_CBC:
+            return EVP_aes_256_cbc();
+        default:
+            return NULL;
+    }
+}
+
+static uint aes_mode_key_sizes[]= {
+    128 /* AES_128_ECB */,
+    192 /* AES_192_ECB */,
+    256 /* AES_256_ECB */,
+    128 /* AES_128_CBC */,
+    192 /* AES_192_CBC */,
+    256 /* AES_256_CBC */
+};
+
+static void aes_create_key(const unsigned char* origin_key, uint32_t key_length,
+        uint8_t* encrypt_key, AesMode mode) {
+    const uint key_size= aes_mode_key_sizes[mode] / 8;
+    uint8_t *origin_key_end= ((uint8_t*)origin_key) + key_length; /* origin key boundary*/
+
+    uint8_t *encrypt_key_end; /* encrypt key boundary */
+    encrypt_key_end= encrypt_key + key_size;
+
+    std::memset(encrypt_key, 0, key_size);          /* initialize key  */
+
+    uint8_t *ptr; /* Start of the encrypt key*/
+    uint8_t *origin_ptr; /* Start of the origin key */
+    for (ptr = encrypt_key, origin_ptr = (uint8_t*)origin_key;
+            origin_ptr < origin_key_end; ptr++, origin_ptr++) {
+        if (ptr == encrypt_key_end) {
+            /* loop over origin key until we used all key */
+            ptr = encrypt_key;
+        }
+        *ptr ^= *origin_ptr;
+    }
+}
+
+static int do_encrypt(EVP_CIPHER_CTX* aes_ctx, const EVP_CIPHER* cipher,
+        const unsigned char* source, uint32_t source_length, const unsigned char* encrypt_key,
+        const unsigned char* iv, bool padding, unsigned char* encrypt, int* u_len, int* f_len) {
+    int ret = EVP_EncryptInit(aes_ctx, cipher, encrypt_key, iv);
+    if (!ret) {
+        return ret;
+    }
+    ret = EVP_CIPHER_CTX_set_padding(aes_ctx, padding);
+    if (!ret) {
+        return ret;
+    }
+    ret = EVP_EncryptUpdate(aes_ctx, encrypt, u_len, source, source_length);
+    if (!ret) {
+        return ret;
+    }
+    ret = EVP_EncryptFinal(aes_ctx, encrypt + *u_len, f_len);
+    return ret;
+}
+
+int AesUtil::encrypt(AesMode mode, const unsigned char* source, uint32_t source_length,
+        const unsigned char* key, uint32_t key_length, const unsigned char* iv,
+        bool padding, unsigned char* encrypt) {
+    EVP_CIPHER_CTX aes_ctx;
+    const EVP_CIPHER* cipher= get_evp_type(mode);
+    int u_len, f_len;
+    /* The encrypt key to be used for encryption */
+    unsigned char encrypt_key[AES_MAX_KEY_LENGTH / 8];
+    aes_create_key(key, key_length, encrypt_key, mode);
+    
+    if (!cipher || (EVP_CIPHER_iv_length(cipher) > 0 && !iv)) {","[{'comment': 'when compare pointer use ""cipher != nullptr"", compare integer use ""ret != 0"", check bool use ""!is_success""', 'commenter': 'imay'}, {'comment': 'done', 'commenter': 'kangpinghuang'}]"
264,be/src/util/aes_util.cpp,"@@ -0,0 +1,174 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""aes_util.h""
+#include  <cstring>
+#include  <string>
+#include  <memory>
+#include  <iostream>
+
+#include <openssl/aes.h>
+#include <openssl/evp.h>
+#include <openssl/err.h>
+
+#include ""exprs/base64.h""
+
+namespace palo {
+
+static const int AES_MAX_KEY_LENGTH = 256;
+
+const EVP_CIPHER* get_evp_type(const AesMode mode) {
+    switch (mode) {
+        case AES_128_ECB:
+            return EVP_aes_128_ecb();
+        case AES_128_CBC:
+            return EVP_aes_128_cbc();
+        case AES_192_ECB:
+            return EVP_aes_192_ecb();
+        case AES_192_CBC:
+            return EVP_aes_192_cbc();
+        case AES_256_ECB:
+            return EVP_aes_256_ecb();
+        case AES_256_CBC:
+            return EVP_aes_256_cbc();
+        default:
+            return NULL;
+    }
+}
+
+static uint aes_mode_key_sizes[]= {
+    128 /* AES_128_ECB */,
+    192 /* AES_192_ECB */,
+    256 /* AES_256_ECB */,
+    128 /* AES_128_CBC */,
+    192 /* AES_192_CBC */,
+    256 /* AES_256_CBC */
+};
+
+static void aes_create_key(const unsigned char* origin_key, uint32_t key_length,
+        uint8_t* encrypt_key, AesMode mode) {
+    const uint key_size= aes_mode_key_sizes[mode] / 8;
+    uint8_t *origin_key_end= ((uint8_t*)origin_key) + key_length; /* origin key boundary*/
+
+    uint8_t *encrypt_key_end; /* encrypt key boundary */
+    encrypt_key_end= encrypt_key + key_size;
+
+    std::memset(encrypt_key, 0, key_size);          /* initialize key  */
+
+    uint8_t *ptr; /* Start of the encrypt key*/
+    uint8_t *origin_ptr; /* Start of the origin key */
+    for (ptr = encrypt_key, origin_ptr = (uint8_t*)origin_key;
+            origin_ptr < origin_key_end; ptr++, origin_ptr++) {
+        if (ptr == encrypt_key_end) {
+            /* loop over origin key until we used all key */
+            ptr = encrypt_key;
+        }
+        *ptr ^= *origin_ptr;
+    }
+}
+
+static int do_encrypt(EVP_CIPHER_CTX* aes_ctx, const EVP_CIPHER* cipher,
+        const unsigned char* source, uint32_t source_length, const unsigned char* encrypt_key,
+        const unsigned char* iv, bool padding, unsigned char* encrypt, int* u_len, int* f_len) {
+    int ret = EVP_EncryptInit(aes_ctx, cipher, encrypt_key, iv);
+    if (!ret) {
+        return ret;
+    }
+    ret = EVP_CIPHER_CTX_set_padding(aes_ctx, padding);
+    if (!ret) {
+        return ret;
+    }
+    ret = EVP_EncryptUpdate(aes_ctx, encrypt, u_len, source, source_length);
+    if (!ret) {
+        return ret;
+    }
+    ret = EVP_EncryptFinal(aes_ctx, encrypt + *u_len, f_len);
+    return ret;
+}
+
+int AesUtil::encrypt(AesMode mode, const unsigned char* source, uint32_t source_length,
+        const unsigned char* key, uint32_t key_length, const unsigned char* iv,
+        bool padding, unsigned char* encrypt) {
+    EVP_CIPHER_CTX aes_ctx;
+    const EVP_CIPHER* cipher= get_evp_type(mode);
+    int u_len, f_len;
+    /* The encrypt key to be used for encryption */
+    unsigned char encrypt_key[AES_MAX_KEY_LENGTH / 8];
+    aes_create_key(key, key_length, encrypt_key, mode);
+    
+    if (!cipher || (EVP_CIPHER_iv_length(cipher) > 0 && !iv)) {
+        return AES_BAD_DATA;","[{'comment': ""when cipher is not null, cipher's memory leak"", 'commenter': 'imay'}, {'comment': 'cipher can not be delete by manually or it will core dump', 'commenter': 'kangpinghuang'}]"
264,be/src/util/aes_util.cpp,"@@ -0,0 +1,174 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""aes_util.h""
+#include  <cstring>
+#include  <string>
+#include  <memory>
+#include  <iostream>
+
+#include <openssl/aes.h>
+#include <openssl/evp.h>
+#include <openssl/err.h>
+
+#include ""exprs/base64.h""
+
+namespace palo {
+
+static const int AES_MAX_KEY_LENGTH = 256;
+
+const EVP_CIPHER* get_evp_type(const AesMode mode) {
+    switch (mode) {
+        case AES_128_ECB:","[{'comment': 'the indent is not right, \'case"" align with switch, no need to indent', 'commenter': 'imay'}, {'comment': 'done', 'commenter': 'kangpinghuang'}]"
265,fe/src/main/java/org/apache/doris/common/util/QueryableReentrantLock.java,"@@ -0,0 +1,24 @@
+package org.apache.doris.common.util;","[{'comment': 'No licence', 'commenter': 'imay'}]"
272,gensrc/script/gen_vector_functions.py,"@@ -410,7 +410,7 @@
 #ifndef BDG_PALO_OPCODE_VECTOR_FUNCTIONS_H\n\","[{'comment': 'BDG_PALO_OPCODE_VECTOR_FUNCTIONS_H -> DORIS_OPCODE_VECTOR_FUNCTIONS_H ', 'commenter': 'wuyunfeng'}]"
272,gensrc/script/gen_functions.py,"@@ -595,7 +595,7 @@
 #ifndef BDG_PALO_OPCODE_FUNCTIONS_H\n\","[{'comment': 'BDG_PALO_OPCODE_FUNCTIONS ->DORIS_OPCODE_FUNCTIONS', 'commenter': 'wuyunfeng'}]"
272,gensrc/script/gen_build_version.sh,"@@ -143,7 +143,7 @@ cat >""${GEN_CPP_DIR}/version.h"" <<EOF
 #ifndef PALO_GEN_CPP_VERSION_H
 #define PALO_GEN_CPP_VERSION_H
 
-namespace palo {
+namespace doris {
 
 #define PALO_BUILD_VERSION ""${build_version}""","[{'comment': 'PALO_BUILD_VERSION -> DORIS_BUILD_VERSION', 'commenter': 'wuyunfeng'}]"
275,README.md,"@@ -199,7 +199,7 @@ Currently we only support full data backup data rather than incremental backups
 In addition to improving data security, the backup function also provides a way to export the data. Data can be exported to other downstream systems for further processing.
 
 # Install
-Doris only supports Linux System. Oracle JDK 8.0+ and GCC 4.8.2+ are required. See the document of [INSTALL](https://github.com/apache/incubator-doris/wiki/Doris-Install) and [Deploy & Update](https://github.com/apache/incubator-doris/wiki/Doris-Deploy-%26-Upgrade)
+Doris only supports Linux System. Oracle JDK 8.0+ and GCC 4.8.2+ are required. See the document of [INSTALL](https://github.com/apache/incubator-doris/wiki/Palo-Install) and [Deploy & Update](https://github.com/apache/incubator-doris/wiki/Palo-Deploy-%26-Upgrade)","[{'comment': 'No need to modify', 'commenter': 'morningman'}]"
283,fe/src/main/java/org/apache/doris/analysis/TypeDef.java,"@@ -0,0 +1,128 @@
+// Modifications copyright (C) 2017, Baidu.com, Inc.
+// Copyright 2017 The Apache Software Foundation","[{'comment': 'your license is not right', 'commenter': 'imay'}, {'comment': 'Ok , i will fix it.', 'commenter': 'chenhao7253886'}]"
283,fe/src/main/java/org/apache/doris/analysis/CastExpr.java,"@@ -219,7 +195,9 @@ private void analyze() throws AnalysisException {
 
     @Override
     public void analyzeImpl(Analyzer analyzer) throws AnalysisException {
-        type = targetType;
+        Preconditions.checkState(!isImplicit);
+        targetTypeDef.analyze(analyzer);","[{'comment': 'targetTypeDef will null if this is implicit cast? And this will throw a NullPointerException', 'commenter': 'imay'}, {'comment': ""Implicit CastExpr can't call analyze() outside , so targetTypeDef will not throw NullPointerException."", 'commenter': 'chenhao7253886'}]"
285,be/src/runtime/lib_cache.h,"@@ -161,6 +161,36 @@ class LibCache {
         const std::string& hdfs_lib_file, LibType type,
         boost::unique_lock<boost::mutex>* entry_lock, LibCacheEntry** entry);
 
+    // map ""palo"" to ""doris"" in symbol, only for grayscale upgrading
+    std::string get_real_symbol(const std::string& symbol) {
+        const std::string& str1 = replace_all(symbol, ""8palo_udf"", ""9doris_udf"");
+        const std::string& str2 = replace_all(str1, ""4palo"", ""5doris""); 
+        return str2;
+    }
+
+    std::string replace_all(const std::string& str, const std::string& find, const std::string& replace) {","[{'comment': 'you can use c++11 std::regex_replace', 'commenter': 'imay'}, {'comment': 'ok, i will replace it.', 'commenter': 'chenhao7253886'}]"
285,be/src/runtime/lib_cache.h,"@@ -161,6 +161,36 @@ class LibCache {
         const std::string& hdfs_lib_file, LibType type,
         boost::unique_lock<boost::mutex>* entry_lock, LibCacheEntry** entry);
 
+    // map ""palo"" to ""doris"" in symbol, only for grayscale upgrading
+    std::string get_real_symbol(const std::string& symbol) {
+        const std::string& str1 = replace_all(symbol, ""8palo_udf"", ""9doris_udf"");
+        const std::string& str2 = replace_all(str1, ""4palo"", ""5doris""); ","[{'comment': ""you can't use reference here, just use std::string, complier will do RVO"", 'commenter': 'imay'}, {'comment': 'oh, variables in the stack will be released , when function finished. I forgot , i will correct it.', 'commenter': 'chenhao7253886'}]"
285,be/src/runtime/lib_cache.cpp,"@@ -126,10 +126,11 @@ LibCache::LibCacheEntry::~LibCacheEntry() {
 Status LibCache::get_so_function_ptr(
         const std::string& hdfs_lib_file, const std::string& symbol,
         void** fn_ptr, LibCacheEntry** ent, bool quiet) {
+    const std::string& real_symbol = get_real_symbol(symbol);","[{'comment': ""you can change argument symbol to origin_symbol, then you needn't change following symbol to real_symbol"", 'commenter': 'imay'}, {'comment': 'ok, it is more simple。i will change it.', 'commenter': 'chenhao7253886'}]"
285,be/src/runtime/lib_cache.h,"@@ -161,6 +162,15 @@ class LibCache {
         const std::string& hdfs_lib_file, LibType type,
         boost::unique_lock<boost::mutex>* entry_lock, LibCacheEntry** entry);
 
+    // map ""palo"" to ""doris"" in symbol, only for grayscale upgrading
+    std::string get_real_symbol(const std::string& symbol) {
+        std::regex rx1(""8palo_udf"");","[{'comment': 'I think std::regex constructor may be cost, so I suggest declare rx1 and rx2 variable static.', 'commenter': 'imay'}, {'comment': ""ok, it's right."", 'commenter': 'chenhao7253886'}]"
289,be/CMakeLists.txt,"@@ -359,11 +359,11 @@ endif()
 # modifying it at runtime, then re-compiling (and optimizing) the modified code. The final
 # optimizations will be less effective if the initial code is also optimized.
 set(CLANG_IR_CXX_FLAGS ""-gcc-toolchain"" ${GCC_HOME})
-set(CLANG_IR_CXX_FLAGS ${CLANG_IR_CXX_FLAGS} ""-std=gnu++11"" ""-c"" ""-emit-llvm"" ""-D__STDC_CONSTANT_MACROS"" ""-D__STDC_FORMAT_MACROS"" ""-D__STDC_LIMIT_MACROS"" ""-DIR_COMPILE"" ""-DNDEBUG"" ""-DHAVE_INTTYPES_H"" ""-DHAVE_NETINET_IN_H"" ""-DBOOST_DATE_TIME_POSIX_TIME_STD_CONFIG"" ""-D__GLIBCXX_BITSIZE_INT_N_0=128"" ""-D__GLIBCXX_TYPE_INT_N_0=__int128"" ""-U_GLIBCXX_USE_FLOAT128"" ""-DLLVM_ON_UNIX"")
-if (CMAKE_CXX_COMPILER_VERSION VERSION_GREATER 7.0)
+set(CLANG_IR_CXX_FLAGS ${CLANG_IR_CXX_FLAGS} ""-std=c++11"" ""-c"" ""-emit-llvm"" ""-D__STDC_CONSTANT_MACROS"" ""-D__STDC_FORMAT_MACROS"" ""-D__STDC_LIMIT_MACROS"" ""-DIR_COMPILE"" ""-DNDEBUG"" ""-DHAVE_INTTYPES_H"" ""-DHAVE_NETINET_IN_H"" ""-DBOOST_DATE_TIME_POSIX_TIME_STD_CONFIG"" ""-D__GLIBCXX_BITSIZE_INT_N_0=128"" ""-D__GLIBCXX_TYPE_INT_N_0=__int128"" ""-U_GLIBCXX_USE_FLOAT128"" ""-DLLVM_ON_UNIX"")
+# if (CMAKE_CXX_COMPILER_VERSION VERSION_GREATER 7.0)
     # for support float128
-    set(CLANG_IR_CXX_FLAGS ${CLANG_IR_CXX_FLAGS} ""-D__STRICT_ANSI__"")
-endif()
+    # set(CLANG_IR_CXX_FLAGS ${CLANG_IR_CXX_FLAGS} ""-D__STRICT_ANSI__"")
+# endif()","[{'comment': 'If this `if` block is useless, just remove it.', 'commenter': 'imay'}, {'comment': 'thanks, i have removed it.', 'commenter': 'uestctxq'}]"
299,fe/src/main/java/org/apache/doris/rewrite/FoldConstantsRule.java,"@@ -58,14 +64,14 @@
 public class FoldConstantsRule implements ExprRewriteRule {
     public static ExprRewriteRule INSTANCE = new FoldConstantsRule();
 
-    private Multimap<String, FEFunctionInvoker> functions = ArrayListMultimap.create();
+    private ImmutableMultimap<String, FEFunctionInvoker> functions;
+    private ImmutableSet<String> nonNullResultWithNullParamFunctions;","[{'comment': ""Need more comment here. I'm not very clear about this attribute's purpose"", 'commenter': 'imay'}, {'comment': 'ok, i will add some comments.', 'commenter': 'chenhao7253886'}]"
300,LICENSE.txt,"@@ -348,6 +348,29 @@ be/src/olap/lru_cache.cpp : BSD-style license
 
 --------------------------------------------------------------------------------
 
+be/src/olap/skiplist.h : BSD-style license
+
+  Copyright (c) 2011 The LevelDB Authors. All rights reserved.
+  Use of this source code is governed by a BSD-style license that can be
+  found in the LICENSE file. See the AUTHORS file for names of contributors.
+
+--------------------------------------------------------------------------------
+
+be/src/util/arena.cpp : BSD-style license
+
+  Copyright (c) 2011 The LevelDB Authors. All rights reserved.
+  Use of this source code is governed by a BSD-style license that can be
+  found in the LICENSE file. See the AUTHORS file for names of contributors.
+
+--------------------------------------------------------------------------------
+
+be/src/util/new_status.cpp : BSD-style license","[{'comment': 'also new_status.h and arena.h', 'commenter': 'imay'}, {'comment': 'OK, I will add this two files into LICENSE.txt', 'commenter': 'chaoyli'}]"
301,Docker_build.md,"@@ -0,0 +1,31 @@
+## Doris Develop Environment based on docker","[{'comment': 'you should add ASF license header, like this\r\n\r\n<!--\r\n\r\n    Licensed to the Apache Software Foundation (ASF) under one\r\n    or more contributor license agreements.  See the NOTICE file\r\n    distributed with this work for additional information\r\n    regarding copyright ownership.  The ASF licenses this file\r\n    to you under the Apache License, Version 2.0 (the\r\n    ""License""); you may not use this file except in compliance\r\n    with the License.  You may obtain a copy of the License at\r\n\r\n      http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n    Unless required by applicable law or agreed to in writing,\r\n    software distributed under the License is distributed on an\r\n    ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n    KIND, either express or implied.  See the License for the\r\n    specific language governing permissions and limitations\r\n    under the License.\r\n\r\n-->', 'commenter': 'imay'}, {'comment': 'OK', 'commenter': 'uestctxq'}]"
301,Docker_build.md,"@@ -0,0 +1,31 @@
+## Doris Develop Environment based on docker
+1、Clone Code: git clone https://github.com/apache/incubator-doris.git
+2、Create `/var/local/docker_build`，cp `**/incubator-doris/thirdparty` and `**/incubator-doris/Dockerfile ` to `/var/local/docker_build`
+
+3、Download thirdpary [palo-thirdparty-20181102.tar.gz](http://doris-opensource.bj.bcebos.com/doris-thirdparty-20181102.tar.gz?authorization=bce-auth-v1/069fc2786e464e63a5f1183824ddb522/2018-11-02T09:27:57Z/-1/host/b30621ca2be77596cec9477f6cfb3608b681206d73084338d1b2f1204a3e3848)
+
+4、tar `palo-thirdparty-{date}.tar.gz`，cp `palo-thirdparty-{date}/*` to `/var/local/docker_build/thirdparty/src`, please create `src` if `src` not exist
+
+5、Build docker image
+```aidl
+cd /var/local/docker_build 
+
+docker build -t palo:v1.0  .
+
+-- palo is docker image repository name and base is tag name , you can change them to what you like","[{'comment': 'you should better change palo to doris', 'commenter': 'kangpinghuang'}, {'comment': 'OK', 'commenter': 'uestctxq'}]"
301,Dockerfile,"@@ -0,0 +1,54 @@
+FROM centos:centos7.5.1804
+
+MAINTAINER tangxiaoqing214445
+
+ENV DEFAULT_DIR /var/local
+
+# add code repository
+ADD thirdparty /var/local/thirdparty
+","[{'comment': ""You should add palo source code in docker image too, otherwise you won't build palo."", 'commenter': 'EmmyMiao87'}, {'comment': 'OK', 'commenter': 'uestctxq'}]"
301,Dockerfile,"@@ -0,0 +1,54 @@
+FROM centos:centos7.5.1804
+
+MAINTAINER tangxiaoqing214445
+
+ENV DEFAULT_DIR /var/local
+
+# add code repository
+ADD thirdparty /var/local/thirdparty
+
+# change .bashrc
+RUN echo -e ""if [ ! -d ""/var/local/incubator-doris/thirdparty/installed"" ]; then\n\tmkdir /var/local/incubator-doris/thirdparty/installed\n\tcp -rf /var/local/thirdparty/installed/*  /var/local/incubator-doris/thirdparty/installed/\nfi"" >> /root/.bashrc
+
+ARG GCC_VERSION=7.3.0
+ARG GCC_URL=https://ftp.gnu.org/gnu/gcc/gcc-${GCC_VERSION}
+
+# install dependencies and build gcc 
+RUN yum install -y bzip2 wget gcc-c++ libstdc++-static cmake byacc flex automake libtool binutils-devel bison ncurses-devel make mlocate unzip patch which vim-common redhat-lsb-core zip \
+  && updatedb \
+  && mkdir -p  /var/local/gcc \
+  && curl -fsSL -o /tmp/gcc.tar.gz  ${GCC_URL}/gcc-${GCC_VERSION}.tar.gz \
+  && tar -xzf /tmp/gcc.tar.gz -C /var/local/gcc --strip-components=1 \
+  && cd /var/local/gcc \
+  && ./contrib/download_prerequisites \
+  && ./configure --disable-multilib --enable-languages=c,c++ --prefix=/usr \
+  && make -j 4 && make install \
+  && rm -rf /var/local/gcc \ 
+  && rm -f /tmp/gcc.tar.gz
+
+# install maven 3.6.0
+ARG MAVEN_VERSION=3.6.0
+ARG SHA=fae9c12b570c3ba18116a4e26ea524b29f7279c17cbaadc3326ca72927368924d9131d11b9e851b8dc9162228b6fdea955446be41207a5cfc61283dd8a561d2f
+ARG BASE_URL=https://apache.osuosl.org/maven/maven-3/${MAVEN_VERSION}/binaries
+
+RUN mkdir -p /usr/share/maven /usr/share/maven/ref \
+  && curl -fsSL -o /tmp/apache-maven.tar.gz ${BASE_URL}/apache-maven-${MAVEN_VERSION}-bin.tar.gz \
+  && echo ""${SHA}  /tmp/apache-maven.tar.gz"" | sha512sum -c - \
+  && tar -xzf /tmp/apache-maven.tar.gz -C /usr/share/maven --strip-components=1 \
+  && rm -f /tmp/apache-maven.tar.gz \
+  && ln -s /usr/share/maven/bin/mvn /usr/bin/mvn
+  
+ENV MAVEN_HOME /usr/share/maven
+  
+# build environment
+WORKDIR ${DEFAULT_DIR}
+
+# build third party 
+RUN /bin/bash thirdparty/build-thirdparty.sh \
+    && ln -s ${DEFAULT_DIR}/thirdparty/installed/bin/thrift /usr/bin/thrift  \
+    && ln -s ${DEFAULT_DIR}/thirdparty/installed/ant/bin/ant /usr/bin/ant  \","[{'comment': 'We are not using ant any more, using maven instead.', 'commenter': 'morningman'}, {'comment': 'I have create docker directory and delete ant and change as other suggest', 'commenter': 'uestctxq'}]"
305,env.sh,"@@ -53,12 +64,21 @@ if ! ${PYTHON} --version; then
     export PYTHON=python2.7
     if ! ${PYTHON} --version; then
         echo ""Error: python is not found""
-        exit
+        exit 1
     fi
 fi
 
-# set DORIS_THIRDPARTY
-if [ -z ${DORIS_THIRDPARTY} ]; then
-    export DORIS_THIRDPARTY=${DORIS_HOME}/thirdparty
+# set GCC HOME
+if [[ -z ${DORIS_GCC_HOME} ]]; then
+    # specify your GCC HOME if needed
+    export DORIS_GCC_HOME=$(dirname `which gcc`)/..
+fi
+
+gcc_ver=`${DORIS_GCC_HOME}/bin/gcc -dumpversion`
+required_ver=""5.3.1""
+if [[ ! ""$(printf '%s\n' ""$required_ver"" ""$gcc_ver"" | sort -V | head -n1)"" = ""$required_ver"" ]]; then 
+    echo ""Error: GCC version (${gcc_ver}) must be greater than or equal to ${required_ver}""
+    echo ""DORIS_GCC_HOME is need to be set in env.sh after you installed a new version of GCC.""","[{'comment': 'I think this prompt is not good, we should guide user to set environment variable DORIS_GCC_HOME to its available gcc', 'commenter': 'imay'}, {'comment': 'OK', 'commenter': 'lide-reed'}]"
309,fe/src/main/cup/sql_parser.cup,"@@ -597,6 +598,17 @@ alter_stmt ::=
     :}
     ;
 
+quota_expr ::=","[{'comment': 'quota_expr is not a good name, because everything whose unit is byte can use this like exec_mem_limit\r\n\r\ncan you change it?', 'commenter': 'imay'}]"
309,fe/src/main/java/org/apache/doris/analysis/AlterDatabaseQuotaStmt.java,"@@ -45,6 +65,37 @@ public long getQuota() {
         return quota;
     }
 
+    public String getUnit() {
+        return unit;
+    }
+
+    private void getQuotaFromExpression() throws UserException {
+        Pattern r = Pattern.compile(quotaPattern);
+        Matcher m = r.matcher(quotaExpression);
+        if (m.find( )) {","[{'comment': 'I think matches is what you want', 'commenter': 'imay'}]"
309,fe/src/main/java/org/apache/doris/analysis/AlterDatabaseQuotaStmt.java,"@@ -26,15 +26,35 @@
 import org.apache.doris.mysql.privilege.PrivPredicate;
 import org.apache.doris.qe.ConnectContext;
 
+import java.util.HashMap;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
 import com.google.common.base.Strings;
 
 public class AlterDatabaseQuotaStmt extends DdlStmt {
     private String dbName;
+    private String quotaExpression;
     private long quota;
+    private String unit = ""B"";
+    private HashMap<String, Long> validUnitMultiplier = new HashMap<String, Long>();
+    private String quotaPattern = ""(-?\\d+)(\\D*)"";","[{'comment': 'Do you need `-` here? ', 'commenter': 'imay'}]"
309,fe/src/main/java/org/apache/doris/analysis/AlterDatabaseQuotaStmt.java,"@@ -26,15 +26,35 @@
 import org.apache.doris.mysql.privilege.PrivPredicate;
 import org.apache.doris.qe.ConnectContext;
 
+import java.util.HashMap;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
 import com.google.common.base.Strings;
 
 public class AlterDatabaseQuotaStmt extends DdlStmt {
     private String dbName;
+    private String quotaExpression;
     private long quota;
+    private String unit = ""B"";
+    private HashMap<String, Long> validUnitMultiplier = new HashMap<String, Long>();","[{'comment': 'You can use a static immutable map to avoid new a map for every statement', 'commenter': 'imay'}]"
309,fe/src/main/java/org/apache/doris/analysis/AlterDatabaseQuotaStmt.java,"@@ -26,15 +26,35 @@
 import org.apache.doris.mysql.privilege.PrivPredicate;
 import org.apache.doris.qe.ConnectContext;
 
+import java.util.HashMap;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
 import com.google.common.base.Strings;
 
 public class AlterDatabaseQuotaStmt extends DdlStmt {
     private String dbName;
+    private String quotaExpression;
     private long quota;
+    private String unit = ""B"";","[{'comment': 'I think that unit should be a function variable', 'commenter': 'imay'}]"
309,fe/src/main/java/org/apache/doris/analysis/AlterDatabaseQuotaStmt.java,"@@ -8,7 +8,7 @@
 //
 //   http://www.apache.org/licenses/LICENSE-2.0
 //
-// Unless required by applicable law or agreed to in writing,
+// Unless required by applicable law or agreed to in writing)","[{'comment': 'typo error', 'commenter': 'imay'}]"
311,tools/row_to_column/get_ddl_stmt.py,"@@ -0,0 +1,51 @@
+#!/usr/bin/env python","[{'comment': 'lack license header', 'commenter': 'imay'}, {'comment': 'have add license to it', 'commenter': 'chaoyli'}]"
311,tools/row_to_column/pkg_resources.py,"@@ -0,0 +1,2677 @@
+""""""Package resource API","[{'comment': 'lack license header', 'commenter': 'imay'}, {'comment': 'have add license to it.', 'commenter': 'chaoyli'}]"
311,be/src/olap/olap_engine.cpp,"@@ -266,6 +266,117 @@ OLAPStatus OLAPEngine::load_one_tablet(
     return OLAP_SUCCESS;
 }
 
+void OLAPEngine::check_none_row_oriented_table(const std::vector<OlapStore*>& stores) {
+    std::vector<std::thread> threads;","[{'comment': '`threads` is not used', 'commenter': 'imay'}, {'comment': 'I have remove it.', 'commenter': 'chaoyli'}]"
311,be/src/olap/olap_header_manager.cpp,"@@ -126,19 +126,7 @@ OLAPStatus OlapHeaderManager::set_converted_flag(OlapStore* store) {
 }
 
 OLAPStatus OlapHeaderManager::traverse_headers(OlapMeta* meta,
-        std::function<bool(long, long, const std::string&)> const& func) {
-    auto traverse_header_func = [&func](const std::string& key, const std::string& value) -> bool {
-        std::vector<std::string> parts;
-        // key format: ""hdr_"" + tablet_id + ""_"" + schema_hash
-        split_string<char>(key, '_', &parts);
-        if (parts.size() != 3) {
-            LOG(WARNING) << ""invalid header key:"" << key << "", splitted size:"" << parts.size();
-            return true;
-        }
-        TTabletId tablet_id = std::stol(parts[1].c_str(), NULL, 10);
-        TSchemaHash schema_hash = std::stol(parts[2].c_str(), NULL, 10);
-        return func(tablet_id, schema_hash, value);
-    };","[{'comment': ""You can't change it, this encapsulate `tablet_id` and `schema_hash` encoding logic. If you put it logic outside, every one need to implement the same logic"", 'commenter': 'imay'}, {'comment': 'OK, I will reset it to last version.', 'commenter': 'chaoyli'}]"
311,be/src/olap/store.cpp,"@@ -543,17 +543,74 @@ OLAPStatus OlapStore::_load_table_from_header(OLAPEngine* engine, TTabletId tabl
 }
 
 OLAPStatus OlapStore::load_tables(OLAPEngine* engine) {
-    auto load_table_func = [this, engine](long tablet_id,
-            long schema_hash, const std::string& value) -> bool {
+    auto traverse_header_func = [this, engine](const std::string& key, const std::string& value) -> bool {
+        std::vector<std::string> parts;
+        // key format: ""hdr_"" + tablet_id + ""_"" + schema_hash
+        split_string<char>(key, '_', &parts);
+        if (parts.size() != 3) {
+            LOG(WARNING) << ""invalid header key:"" << key << "", splitted size:"" << parts.size();
+            return true;
+        }
+        TTabletId tablet_id = std::stol(parts[1].c_str(), NULL, 10);
+        TSchemaHash schema_hash = std::stol(parts[2].c_str(), NULL, 10);
         OLAPStatus status = _load_table_from_header(engine, tablet_id, schema_hash, value);
         if (status != OLAP_SUCCESS) {
-            LOG(WARNING) << ""load table from header failed.tablet_id:"" << tablet_id
-                    << "", schema_hash:"" << schema_hash << "", status:"" << status;
+            LOG(WARNING) << ""load table from header failed. status:"" << status
+                         << ""tablet="" << tablet_id << ""."" << schema_hash;
         };
         return true;
     };
-    OLAPStatus status = OlapHeaderManager::traverse_headers(_meta, load_table_func);
+    OLAPStatus status = OlapHeaderManager::traverse_headers(_meta, traverse_header_func);
     return status;
 }
 
+OLAPStatus OlapStore::check_none_row_oriented_table_in_store(OLAPEngine* engine) {
+    auto traverse_header_func = [this, engine](const std::string& key, const std::string& value) -> bool {
+        std::vector<std::string> parts;
+        // key format: ""hdr_"" + tablet_id + ""_"" + schema_hash
+        split_string<char>(key, '_', &parts);
+        if (parts.size() != 3) {
+            LOG(WARNING) << ""invalid header key:"" << key << "", splitted size:"" << parts.size();
+            return true;
+        }
+        TTabletId tablet_id = std::stol(parts[1].c_str(), NULL, 10);
+        TSchemaHash schema_hash = std::stol(parts[2].c_str(), NULL, 10);","[{'comment': 'Please use old interface', 'commenter': 'imay'}, {'comment': 'I will reset to last version.', 'commenter': 'chaoyli'}]"
313,fe/src/main/java/org/apache/doris/load/routineload/KafkaRoutineLoadProgress.java,"@@ -0,0 +1,24 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.load.routineload;
+
+public class KafkaRoutineLoadProgress {
+
+    private String partitionName;
+    private long offset;","[{'comment': 'fields would be not accessed ?', 'commenter': 'wuyunfeng'}, {'comment': 'I will add constructor in later commit.', 'commenter': 'EmmyMiao87'}]"
313,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoadJob.java,"@@ -0,0 +1,120 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.load.routineload;
+
+import org.apache.doris.common.io.Writable;
+import org.apache.doris.thrift.TResourceInfo;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+import java.util.List;
+
+public class RoutineLoadJob implements Writable {
+
+    public enum JobState {
+        NEED_SCHEDULER,","[{'comment': 'Would the verb INIT is better than NEED_SCHEDULER', 'commenter': 'wuyunfeng'}, {'comment': 'The init state of job is NEED_SCHEDULER. The routine load job will be added in idToNeedScheduler in the first step. So I think INIT state is unnecessary', 'commenter': 'EmmyMiao87'}]"
313,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoadJob.java,"@@ -0,0 +1,120 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.load.routineload;
+
+import org.apache.doris.common.io.Writable;
+import org.apache.doris.thrift.TResourceInfo;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+import java.util.List;
+
+public class RoutineLoadJob implements Writable {","[{'comment': 'Lack of the description the function of the class? I think one commit is one small feature or minimal function，WDYT？', 'commenter': 'wuyunfeng'}, {'comment': 'The commit is one of steps in routine load job which scheduler job into tasks.', 'commenter': 'EmmyMiao87'}, {'comment': 'I think so, AHA!', 'commenter': 'morningman'}, {'comment': 'Already fix', 'commenter': 'EmmyMiao87'}]"
313,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoadJob.java,"@@ -0,0 +1,120 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.load.routineload;
+
+import org.apache.doris.common.io.Writable;
+import org.apache.doris.thrift.TResourceInfo;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+import java.util.List;
+
+public class RoutineLoadJob implements Writable {
+
+    public enum JobState {
+        NEED_SCHEDULER,
+        RUNNING,
+        PAUSED,
+        STOPPED,
+        CANCELLED
+    }
+
+    public enum DataSourceType {
+        KAFKA
+    }
+
+    protected long id;
+    protected String name;
+    protected String userName;
+    protected long dbId;
+    protected long tableId;
+    protected String partitions;
+    protected String columns;
+    protected String where;
+    protected String columnSeparator;
+    protected int desireTaskConcurrentNum;
+    protected JobState state;
+    protected DataSourceType dataSourceType;
+    // max number of error data in ten thousand data
+    protected int maxErrorNum;
+    protected String progress;
+    // TODO(ml): error sample
+
+
+    public RoutineLoadJob() {
+    }
+
+    public RoutineLoadJob(long id, String name, String userName, long dbId, long tableId,
+                          String partitions, String columns, String where, String columnSeparator,
+                          int desireTaskConcurrentNum, JobState state, DataSourceType dataSourceType,
+                          int maxErrorNum, TResourceInfo resourceInfo) {
+        this.id = id;
+        this.name = name;
+        this.userName = userName;
+        this.dbId = dbId;
+        this.tableId = tableId;
+        this.partitions = partitions;
+        this.columns = columns;
+        this.where = where;
+        this.columnSeparator = columnSeparator;
+        this.desireTaskConcurrentNum = desireTaskConcurrentNum;
+        this.state = state;
+        this.dataSourceType = dataSourceType;
+        this.maxErrorNum = maxErrorNum;
+        this.resourceInfo = resourceInfo;
+        this.progress = """";
+    }
+
+    // thrift object
+    private TResourceInfo resourceInfo;
+
+    public long getId() {
+        return id;
+    }
+
+    public JobState getState() {
+        return state;
+    }
+
+    public void setState(JobState state) {
+        this.state = state;
+    }
+
+    public TResourceInfo getResourceInfo() {
+        return resourceInfo;
+    }
+
+    public List<RoutineLoadTask> divideRoutineLoadJob(int currentConcurrentTaskNum) {
+        return null;
+    }
+
+    public int calculateCurrentConcurrentTaskNum() {
+        return 0;
+    }
+
+    @Override
+    public void write(DataOutput out) throws IOException {
+        // TODO(ml)
+    }
+
+    @Override
+    public void readFields(DataInput in) throws IOException {
+        // TODO(ml)","[{'comment': 'why not commit this file after you finishing that?', 'commenter': 'wuyunfeng'}, {'comment': 'work in progress', 'commenter': 'EmmyMiao87'}, {'comment': 'Because she needs this class somewhere...', 'commenter': 'morningman'}]"
313,fe/src/main/java/org/apache/doris/persist/EditLog.java,"@@ -505,8 +506,8 @@ public static void loadJournal(Catalog catalog, JournalEntity journal) {
                     int version = Integer.parseInt(versionString);
                     if (catalog.getJournalVersion() > FeConstants.meta_version) {
                         LOG.error(""meta data version is out of date, image: {}. meta: {}.""
-                                + ""please update FeConstants.meta_version and restart."",
-                                  catalog.getJournalVersion(), FeConstants.meta_version);
+                                        + ""please update FeConstants.meta_version and restart."",
+                                catalog.getJournalVersion(), FeConstants.meta_version);","[{'comment': 'System.exit?', 'commenter': 'wuyunfeng'}, {'comment': '> System.exit?\r\nYes, this is a fatal error.', 'commenter': 'morningman'}]"
313,fe/src/main/java/org/apache/doris/persist/EditLog.java,"@@ -776,14 +777,36 @@ public void logLoadDone(LoadJob job) {
         logEdit(OperationType.OP_LOAD_DONE, job);
     }
 
+    public void logRoutineLoadByState(RoutineLoadJob job, RoutineLoadJob.JobState jobState) {
+        switch (jobState) {
+            case NEED_SCHEDULER:
+                logEdit(OperationType.OP_ROUTINE_LOAD_NEED_SCHEDULER, job);","[{'comment': 'I think you should put JobState inside the RoutineLoadJob, and only use ONE OperationType to log the job.\r\nlike: \r\n\r\nlogRoutineLoadJob(job) {\r\n    logEdit(OperationType.OP_ROUTINE_LOAD_JOB, job)\r\n}', 'commenter': 'morningman'}]"
313,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoad.java,"@@ -0,0 +1,200 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.load.routineload;
+
+import com.google.common.collect.Maps;
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.common.LoadException;
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.concurrent.locks.ReentrantReadWriteLock;
+import java.util.stream.Collectors;
+
+public class RoutineLoad {
+    private static final Logger LOG = LogManager.getLogger(RoutineLoad.class);
+    private static final int DEFAULT_BE_CONCURRENT_TASK_NUM = 100;
+
+    // TODO(ml): real-time calculate by be
+    private Map<Long, Integer> beIdTomaxConcurrentTasks;","[{'comment': 'beIdTomaxConcurrentTasks -> beIdToMaxConcurrentTasks', 'commenter': 'morningman'}]"
313,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoad.java,"@@ -0,0 +1,200 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.load.routineload;
+
+import com.google.common.collect.Maps;
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.common.LoadException;
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.concurrent.locks.ReentrantReadWriteLock;
+import java.util.stream.Collectors;
+
+public class RoutineLoad {
+    private static final Logger LOG = LogManager.getLogger(RoutineLoad.class);
+    private static final int DEFAULT_BE_CONCURRENT_TASK_NUM = 100;
+
+    // TODO(ml): real-time calculate by be
+    private Map<Long, Integer> beIdTomaxConcurrentTasks;
+
+    // stream load job meta
+    private Map<Long, RoutineLoadJob> idToRoutineLoadJob;
+    private Map<Long, RoutineLoadJob> idToNeedSchedulerRoutineLoadJob;
+    private Map<Long, RoutineLoadJob> idToRunningRoutineLoadJob;
+    private Map<Long, RoutineLoadJob> idToCancelledRoutineLoadJob;
+
+    // stream load tasks meta (not persistent)
+    private Map<Long, RoutineLoadTask> idToRoutineLoadTask;
+    private Map<Long, RoutineLoadTask> idToNeedSchedulerRoutineLoadTask;
+
+    private ReentrantReadWriteLock lock;
+
+    private void readLock() {
+        lock.readLock().lock();
+    }
+
+    private void readUnlock() {
+        lock.readLock().unlock();
+    }
+
+    private void writeLock() {
+        lock.writeLock().lock();
+    }
+
+    private void writeUnlock() {
+        lock.writeLock().unlock();
+    }
+
+    public RoutineLoad() {
+        idToRoutineLoadJob = Maps.newHashMap();
+        idToNeedSchedulerRoutineLoadJob = Maps.newHashMap();
+        idToRunningRoutineLoadJob = Maps.newHashMap();
+        idToCancelledRoutineLoadJob = Maps.newHashMap();
+        idToRoutineLoadTask = Maps.newHashMap();
+        idToNeedSchedulerRoutineLoadTask = Maps.newHashMap();
+        beIdTomaxConcurrentTasks = Catalog.getCurrentSystemInfo().getBackendIds(true)
+                .parallelStream().collect(Collectors.toMap(beId -> beId, beId -> DEFAULT_BE_CONCURRENT_TASK_NUM));
+        lock = new ReentrantReadWriteLock(true);
+    }
+
+    public int getTotalMaxConcurrentTaskNum() {
+        readLock();
+        try {
+            return beIdTomaxConcurrentTasks.values().stream().mapToInt(i -> i).sum();
+        } finally {
+            readUnlock();
+        }
+    }
+
+    public void addRoutineLoadJob(RoutineLoadJob routineLoadJob) {
+        writeLock();
+        try {
+            idToRoutineLoadJob.put(routineLoadJob.getId(), routineLoadJob);
+        } finally {
+            writeUnlock();
+        }
+    }
+
+    public void addRoutineLoadTasks(List<RoutineLoadTask> routineLoadTaskList) {
+        writeLock();
+        try {
+            idToRoutineLoadTask.putAll(routineLoadTaskList.parallelStream().collect(
+                    Collectors.toMap(task -> task.getSignature(), task -> task)));
+        } finally {
+            writeUnlock();
+        }
+    }
+
+    public Map<Long, RoutineLoadTask> getIdToRoutineLoadTask() {
+        return idToRoutineLoadTask;
+    }
+
+    public void addNeedSchedulerRoutineLoadTasks(List<RoutineLoadTask> routineLoadTaskList) {
+        writeLock();
+        try {
+            idToNeedSchedulerRoutineLoadTask.putAll(routineLoadTaskList.parallelStream().collect(
+                    Collectors.toMap(task -> task.getSignature(), task -> task)));
+        } finally {
+            writeUnlock();
+        }
+    }
+
+    public Map<Long, RoutineLoadTask> getIdToNeedSchedulerRoutineLoadTasks() {
+        readLock();
+        try {
+            return idToNeedSchedulerRoutineLoadTask;
+        } finally {
+            readUnlock();
+        }
+    }
+
+    public List<RoutineLoadJob> getRoutineLoadJobByState(RoutineLoadJob.JobState jobState) throws LoadException {
+        List<RoutineLoadJob> jobs = new ArrayList<>();
+        Collection<RoutineLoadJob> stateJobs = null;
+        readLock();
+        LOG.debug(""begin to get routine load job by state {}"", jobState.name());
+        try {
+            switch (jobState) {
+                case NEED_SCHEDULER:
+                    stateJobs = idToNeedSchedulerRoutineLoadJob.values();
+                    break;
+                case PAUSED:
+                    throw new LoadException(""not support getting paused routine load jobs"");
+                case RUNNING:
+                    stateJobs = idToRunningRoutineLoadJob.values();
+                    break;
+                case STOPPED:
+                    throw new LoadException(""not support getting stopped routine load jobs"");
+                default:
+                    break;
+            }
+            if (stateJobs != null) {
+                jobs.addAll(stateJobs);
+                LOG.info(""got {} routine load jobs by state {}"", jobs.size(), jobState.name());
+            }
+        } finally {
+            readUnlock();
+        }
+        return jobs;
+    }
+
+    public void updateRoutineLoadJobState(RoutineLoadJob routineLoadJob, RoutineLoadJob.JobState jobState) {
+        writeLock();
+        try {
+            RoutineLoadJob.JobState srcJobState = routineLoadJob.getState();
+            long jobId = routineLoadJob.getId();
+            LOG.debug(""begin to change job {} state from {} to {}"", jobId, srcJobState, jobState);","[{'comment': 'LOG.info maybe more suitable？', 'commenter': 'morningman'}]"
313,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoad.java,"@@ -0,0 +1,200 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.load.routineload;
+
+import com.google.common.collect.Maps;
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.common.LoadException;
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.concurrent.locks.ReentrantReadWriteLock;
+import java.util.stream.Collectors;
+
+public class RoutineLoad {
+    private static final Logger LOG = LogManager.getLogger(RoutineLoad.class);
+    private static final int DEFAULT_BE_CONCURRENT_TASK_NUM = 100;
+
+    // TODO(ml): real-time calculate by be
+    private Map<Long, Integer> beIdTomaxConcurrentTasks;
+
+    // stream load job meta
+    private Map<Long, RoutineLoadJob> idToRoutineLoadJob;
+    private Map<Long, RoutineLoadJob> idToNeedSchedulerRoutineLoadJob;
+    private Map<Long, RoutineLoadJob> idToRunningRoutineLoadJob;
+    private Map<Long, RoutineLoadJob> idToCancelledRoutineLoadJob;
+
+    // stream load tasks meta (not persistent)
+    private Map<Long, RoutineLoadTask> idToRoutineLoadTask;
+    private Map<Long, RoutineLoadTask> idToNeedSchedulerRoutineLoadTask;
+
+    private ReentrantReadWriteLock lock;
+
+    private void readLock() {
+        lock.readLock().lock();
+    }
+
+    private void readUnlock() {
+        lock.readLock().unlock();
+    }
+
+    private void writeLock() {
+        lock.writeLock().lock();
+    }
+
+    private void writeUnlock() {
+        lock.writeLock().unlock();
+    }
+
+    public RoutineLoad() {
+        idToRoutineLoadJob = Maps.newHashMap();
+        idToNeedSchedulerRoutineLoadJob = Maps.newHashMap();
+        idToRunningRoutineLoadJob = Maps.newHashMap();
+        idToCancelledRoutineLoadJob = Maps.newHashMap();
+        idToRoutineLoadTask = Maps.newHashMap();
+        idToNeedSchedulerRoutineLoadTask = Maps.newHashMap();
+        beIdTomaxConcurrentTasks = Catalog.getCurrentSystemInfo().getBackendIds(true)
+                .parallelStream().collect(Collectors.toMap(beId -> beId, beId -> DEFAULT_BE_CONCURRENT_TASK_NUM));
+        lock = new ReentrantReadWriteLock(true);
+    }
+
+    public int getTotalMaxConcurrentTaskNum() {
+        readLock();
+        try {
+            return beIdTomaxConcurrentTasks.values().stream().mapToInt(i -> i).sum();
+        } finally {
+            readUnlock();
+        }
+    }
+
+    public void addRoutineLoadJob(RoutineLoadJob routineLoadJob) {
+        writeLock();
+        try {
+            idToRoutineLoadJob.put(routineLoadJob.getId(), routineLoadJob);
+        } finally {
+            writeUnlock();
+        }
+    }
+
+    public void addRoutineLoadTasks(List<RoutineLoadTask> routineLoadTaskList) {
+        writeLock();
+        try {
+            idToRoutineLoadTask.putAll(routineLoadTaskList.parallelStream().collect(
+                    Collectors.toMap(task -> task.getSignature(), task -> task)));
+        } finally {
+            writeUnlock();
+        }
+    }
+
+    public Map<Long, RoutineLoadTask> getIdToRoutineLoadTask() {
+        return idToRoutineLoadTask;
+    }
+
+    public void addNeedSchedulerRoutineLoadTasks(List<RoutineLoadTask> routineLoadTaskList) {
+        writeLock();
+        try {
+            idToNeedSchedulerRoutineLoadTask.putAll(routineLoadTaskList.parallelStream().collect(
+                    Collectors.toMap(task -> task.getSignature(), task -> task)));
+        } finally {
+            writeUnlock();
+        }
+    }
+
+    public Map<Long, RoutineLoadTask> getIdToNeedSchedulerRoutineLoadTasks() {
+        readLock();
+        try {
+            return idToNeedSchedulerRoutineLoadTask;
+        } finally {
+            readUnlock();
+        }
+    }
+
+    public List<RoutineLoadJob> getRoutineLoadJobByState(RoutineLoadJob.JobState jobState) throws LoadException {
+        List<RoutineLoadJob> jobs = new ArrayList<>();
+        Collection<RoutineLoadJob> stateJobs = null;
+        readLock();
+        LOG.debug(""begin to get routine load job by state {}"", jobState.name());
+        try {
+            switch (jobState) {
+                case NEED_SCHEDULER:
+                    stateJobs = idToNeedSchedulerRoutineLoadJob.values();
+                    break;
+                case PAUSED:
+                    throw new LoadException(""not support getting paused routine load jobs"");
+                case RUNNING:
+                    stateJobs = idToRunningRoutineLoadJob.values();
+                    break;
+                case STOPPED:
+                    throw new LoadException(""not support getting stopped routine load jobs"");
+                default:
+                    break;
+            }
+            if (stateJobs != null) {
+                jobs.addAll(stateJobs);
+                LOG.info(""got {} routine load jobs by state {}"", jobs.size(), jobState.name());
+            }
+        } finally {
+            readUnlock();
+        }
+        return jobs;
+    }
+
+    public void updateRoutineLoadJobState(RoutineLoadJob routineLoadJob, RoutineLoadJob.JobState jobState) {
+        writeLock();
+        try {
+            RoutineLoadJob.JobState srcJobState = routineLoadJob.getState();","[{'comment': ""Don‘ t you need to check the validation of state transfer?\r\neg. It looks like it's invalid to transfer from NEDD_SCHEDULER to STOPPED, but how do you make sure this will not happen?"", 'commenter': 'morningman'}, {'comment': 'Maybe check validation of state transfer will be implement in another place such as mysql statement validation.', 'commenter': 'EmmyMiao87'}, {'comment': 'I think the best place to check this is here.\r\nBecause you check it somewhere else, e.g., analyze phase, you can not make it an atomic operation.\r\nThe check and transfer of state should be in one locking block.', 'commenter': 'morningman'}]"
313,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoadScheduler.java,"@@ -0,0 +1,79 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.load.routineload;
+
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.common.LoadException;
+import org.apache.doris.common.util.Daemon;
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.List;
+
+public class RoutineLoadScheduler extends Daemon {
+
+    private static final Logger LOG = LogManager.getLogger(RoutineLoadScheduler.class);
+
+    private RoutineLoad routineLoad = Catalog.getInstance().getRoutineLoadInstance();
+
+    @Override
+    protected void runOneCycle() {
+        // get need scheduler routine jobs
+        List<RoutineLoadJob> routineLoadJobList = null;
+        try {
+            routineLoadJobList = getNeedSchedulerRoutineJobs();
+        } catch (LoadException e) {
+            LOG.error(""failed to get need scheduler routine jobs"");
+        }
+
+        if (routineLoadJobList != null) {
+            for (RoutineLoadJob routineLoadJob : routineLoadJobList) {
+                // judge nums of tasks more then max concurrent tasks of cluster
+                int currentConcurrentTaskNum = routineLoadJob.calculateCurrentConcurrentTaskNum();
+                int totalTaskNum = currentConcurrentTaskNum + routineLoad.getIdToRoutineLoadTask().size();
+                if (totalTaskNum > routineLoad.getTotalMaxConcurrentTaskNum()) {
+                    LOG.info(""desried total task num = job {} concurrent task num {} + current total task num {}"",
+                            routineLoadJob.getId(), currentConcurrentTaskNum,
+                            routineLoad.getIdToRoutineLoadTask().size());
+                    LOG.info(""desired total task num {} more then total max task num {}, ""
+                                    + ""skip this turn of scheduler"",
+                            totalTaskNum, routineLoad.getTotalMaxConcurrentTaskNum());
+                    break;
+                }
+                // divide job into tasks
+                List<RoutineLoadTask> routineLoadTaskList =
+                        routineLoadJob.divideRoutineLoadJob(currentConcurrentTaskNum);
+
+                // update tasks meta
+                routineLoad.addRoutineLoadTasks(routineLoadTaskList);
+                routineLoad.addNeedSchedulerRoutineLoadTasks(routineLoadTaskList);
+
+                // change job state to running
+                routineLoad.updateRoutineLoadJobState(routineLoadJob, RoutineLoadJob.JobState.RUNNING);
+            }
+        }
+
+    }
+
+    private List<RoutineLoadJob> getNeedSchedulerRoutineJobs() throws LoadException {
+        RoutineLoad routineLoad = Catalog.getInstance().getRoutineLoadInstance();","[{'comment': 'You already had a private member of RoutineLoad in this class, no need to get it again', 'commenter': 'morningman'}, {'comment': 'Got it', 'commenter': 'EmmyMiao87'}]"
313,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoadScheduler.java,"@@ -0,0 +1,79 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.load.routineload;
+
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.common.LoadException;
+import org.apache.doris.common.util.Daemon;
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.List;
+
+public class RoutineLoadScheduler extends Daemon {
+
+    private static final Logger LOG = LogManager.getLogger(RoutineLoadScheduler.class);
+
+    private RoutineLoad routineLoad = Catalog.getInstance().getRoutineLoadInstance();
+
+    @Override
+    protected void runOneCycle() {
+        // get need scheduler routine jobs
+        List<RoutineLoadJob> routineLoadJobList = null;
+        try {
+            routineLoadJobList = getNeedSchedulerRoutineJobs();
+        } catch (LoadException e) {
+            LOG.error(""failed to get need scheduler routine jobs"");
+        }
+
+        if (routineLoadJobList != null) {","[{'comment': 'getNeedSchedulerRoutineJobs() never returns null, so...', 'commenter': 'morningman'}]"
313,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoadScheduler.java,"@@ -0,0 +1,79 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.load.routineload;
+
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.common.LoadException;
+import org.apache.doris.common.util.Daemon;
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.List;
+
+public class RoutineLoadScheduler extends Daemon {
+
+    private static final Logger LOG = LogManager.getLogger(RoutineLoadScheduler.class);
+
+    private RoutineLoad routineLoad = Catalog.getInstance().getRoutineLoadInstance();
+
+    @Override
+    protected void runOneCycle() {
+        // get need scheduler routine jobs
+        List<RoutineLoadJob> routineLoadJobList = null;
+        try {
+            routineLoadJobList = getNeedSchedulerRoutineJobs();
+        } catch (LoadException e) {
+            LOG.error(""failed to get need scheduler routine jobs"");
+        }
+
+        if (routineLoadJobList != null) {
+            for (RoutineLoadJob routineLoadJob : routineLoadJobList) {
+                // judge nums of tasks more then max concurrent tasks of cluster
+                int currentConcurrentTaskNum = routineLoadJob.calculateCurrentConcurrentTaskNum();
+                int totalTaskNum = currentConcurrentTaskNum + routineLoad.getIdToRoutineLoadTask().size();
+                if (totalTaskNum > routineLoad.getTotalMaxConcurrentTaskNum()) {
+                    LOG.info(""desried total task num = job {} concurrent task num {} + current total task num {}"",
+                            routineLoadJob.getId(), currentConcurrentTaskNum,
+                            routineLoad.getIdToRoutineLoadTask().size());
+                    LOG.info(""desired total task num {} more then total max task num {}, ""","[{'comment': 'merge 2 logs in one line should be better, or they may be far away from each other in fe.log', 'commenter': 'morningman'}]"
313,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoadScheduler.java,"@@ -0,0 +1,79 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.load.routineload;
+
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.common.LoadException;
+import org.apache.doris.common.util.Daemon;
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.List;
+
+public class RoutineLoadScheduler extends Daemon {
+
+    private static final Logger LOG = LogManager.getLogger(RoutineLoadScheduler.class);
+
+    private RoutineLoad routineLoad = Catalog.getInstance().getRoutineLoadInstance();
+
+    @Override
+    protected void runOneCycle() {
+        // get need scheduler routine jobs
+        List<RoutineLoadJob> routineLoadJobList = null;
+        try {
+            routineLoadJobList = getNeedSchedulerRoutineJobs();
+        } catch (LoadException e) {
+            LOG.error(""failed to get need scheduler routine jobs"");
+        }
+
+        if (routineLoadJobList != null) {
+            for (RoutineLoadJob routineLoadJob : routineLoadJobList) {
+                // judge nums of tasks more then max concurrent tasks of cluster
+                int currentConcurrentTaskNum = routineLoadJob.calculateCurrentConcurrentTaskNum();
+                int totalTaskNum = currentConcurrentTaskNum + routineLoad.getIdToRoutineLoadTask().size();
+                if (totalTaskNum > routineLoad.getTotalMaxConcurrentTaskNum()) {
+                    LOG.info(""desried total task num = job {} concurrent task num {} + current total task num {}"",
+                            routineLoadJob.getId(), currentConcurrentTaskNum,
+                            routineLoad.getIdToRoutineLoadTask().size());
+                    LOG.info(""desired total task num {} more then total max task num {}, ""
+                                    + ""skip this turn of scheduler"",
+                            totalTaskNum, routineLoad.getTotalMaxConcurrentTaskNum());
+                    break;
+                }
+                // divide job into tasks
+                List<RoutineLoadTask> routineLoadTaskList =
+                        routineLoadJob.divideRoutineLoadJob(currentConcurrentTaskNum);
+
+                // update tasks meta
+                routineLoad.addRoutineLoadTasks(routineLoadTaskList);
+                routineLoad.addNeedSchedulerRoutineLoadTasks(routineLoadTaskList);
+
+                // change job state to running
+                routineLoad.updateRoutineLoadJobState(routineLoadJob, RoutineLoadJob.JobState.RUNNING);","[{'comment': ""Your scheduler logic is:\r\n1. get NEED_SCHEDULER jobs from RoutineLoad\r\n2. do some scheduler\r\n3. change job's state to RUNNING(write edit log meanwhile)\r\n\r\nBut what if user PAUSE/STOP the job between step 1 and 3?\r\neg.\r\n    User STOP the job while you are doing step 2, and the job's state is STOPPED, and then you change it to RUNNING in step 3.\r\n\r\nI think you should make sure this will not happen."", 'commenter': 'morningman'}, {'comment': 'Maybe I need to add a write lock when operating routine load job', 'commenter': 'EmmyMiao87'}]"
313,fe/src/main/java/org/apache/doris/load/routineload/KafkaRoutineLoadJob.java,"@@ -0,0 +1,114 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.load.routineload;
+
+import com.google.common.base.Strings;
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.common.SystemIdGenerator;
+import org.apache.doris.system.SystemInfoService;
+import org.apache.doris.thrift.TResourceInfo;
+import org.apache.doris.thrift.TTaskType;
+import org.apache.kafka.clients.consumer.KafkaConsumer;
+import org.apache.kafka.common.PartitionInfo;
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.time.Duration;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Properties;
+
+public class KafkaRoutineLoadJob extends RoutineLoadJob {
+    private static final Logger LOG = LogManager.getLogger(KafkaRoutineLoadJob.class);
+
+    private static final String FE_GROUP_ID = ""fe_fetch_partitions"";
+    private static final int FETCH_PARTITIONS_TIMEOUT = 10;
+
+    private String serverAddress;
+    private String topic;
+    // optional
+    private List<Integer> kafkaPartitions;
+
+    public KafkaRoutineLoadJob() {
+    }
+
+    public KafkaRoutineLoadJob(long id, String name, String userName, long dbId, long tableId,
+                               String partitions, String columns, String where, String columnSeparator,
+                               int desireTaskConcurrentNum, JobState state, DataSourceType dataSourceType,
+                               int maxErrorNum, TResourceInfo resourceInfo, String serverAddress, String topic) {
+        super(id, name, userName, dbId, tableId, partitions, columns, where,
+                columnSeparator, desireTaskConcurrentNum, state, dataSourceType, maxErrorNum, resourceInfo);
+        this.serverAddress = serverAddress;
+        this.topic = topic;
+    }
+
+    @Override
+    public List<RoutineLoadTask> divideRoutineLoadJob(int currentConcurrentTaskNum) {
+        // divide kafkaPartitions into tasks
+        List<KafkaRoutineLoadTask> kafkaRoutineLoadTaskList = new ArrayList<>();
+        for (int i = 0; i < currentConcurrentTaskNum; i++) {
+            // TODO(ml): init load task
+            kafkaRoutineLoadTaskList.add(new KafkaRoutineLoadTask(getResourceInfo(), 0L, TTaskType.PUSH,
+                    dbId, tableId, 0L, 0L, 0L, SystemIdGenerator.getNextId()));
+        }
+        for (int i = 0; i < kafkaPartitions.size(); i++) {
+            kafkaRoutineLoadTaskList.get(i % currentConcurrentTaskNum).addKafkaPartition(kafkaPartitions.get(i));
+        }
+        List<RoutineLoadTask> result = new ArrayList<>();
+        result.addAll(kafkaRoutineLoadTaskList);
+        return result;
+    }
+
+    @Override
+    public int calculateCurrentConcurrentTaskNum() {
+        updatePartitions();
+        SystemInfoService clusterInfo = Catalog.getCurrentSystemInfo();
+        String clusterName = Catalog.getInstance().getDb(dbId).getClusterName();
+        if (Strings.isNullOrEmpty(clusterName)) {
+            LOG.debug(""database {} has no cluster name"", dbId);
+            clusterName = SystemInfoService.DEFAULT_CLUSTER;
+        }
+        int aliveBeNum = clusterInfo.getClusterBackendIds(clusterName).size();
+        int partitionNum = kafkaPartitions.size();
+
+        LOG.info(""current concurrent task number is min ""
+                        + ""(current size of partition {}, desire task concurrent num {}, alive be num {})"",
+                partitionNum, desireTaskConcurrentNum, aliveBeNum);
+        return partitionNum > desireTaskConcurrentNum ?
+                (desireTaskConcurrentNum > aliveBeNum ? aliveBeNum : desireTaskConcurrentNum) :
+                (partitionNum > aliveBeNum ? aliveBeNum : partitionNum);","[{'comment': 'WTF？\r\nWhy not using Math.min()?', 'commenter': 'morningman'}]"
313,fe/src/main/java/org/apache/doris/load/routineload/KafkaRoutineLoadJob.java,"@@ -0,0 +1,114 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.load.routineload;
+
+import com.google.common.base.Strings;
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.common.SystemIdGenerator;
+import org.apache.doris.system.SystemInfoService;
+import org.apache.doris.thrift.TResourceInfo;
+import org.apache.doris.thrift.TTaskType;
+import org.apache.kafka.clients.consumer.KafkaConsumer;
+import org.apache.kafka.common.PartitionInfo;
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.time.Duration;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Properties;
+
+public class KafkaRoutineLoadJob extends RoutineLoadJob {
+    private static final Logger LOG = LogManager.getLogger(KafkaRoutineLoadJob.class);
+
+    private static final String FE_GROUP_ID = ""fe_fetch_partitions"";
+    private static final int FETCH_PARTITIONS_TIMEOUT = 10;
+
+    private String serverAddress;
+    private String topic;
+    // optional
+    private List<Integer> kafkaPartitions;
+
+    public KafkaRoutineLoadJob() {
+    }
+
+    public KafkaRoutineLoadJob(long id, String name, String userName, long dbId, long tableId,
+                               String partitions, String columns, String where, String columnSeparator,
+                               int desireTaskConcurrentNum, JobState state, DataSourceType dataSourceType,
+                               int maxErrorNum, TResourceInfo resourceInfo, String serverAddress, String topic) {
+        super(id, name, userName, dbId, tableId, partitions, columns, where,
+                columnSeparator, desireTaskConcurrentNum, state, dataSourceType, maxErrorNum, resourceInfo);
+        this.serverAddress = serverAddress;
+        this.topic = topic;
+    }
+
+    @Override
+    public List<RoutineLoadTask> divideRoutineLoadJob(int currentConcurrentTaskNum) {
+        // divide kafkaPartitions into tasks
+        List<KafkaRoutineLoadTask> kafkaRoutineLoadTaskList = new ArrayList<>();
+        for (int i = 0; i < currentConcurrentTaskNum; i++) {
+            // TODO(ml): init load task
+            kafkaRoutineLoadTaskList.add(new KafkaRoutineLoadTask(getResourceInfo(), 0L, TTaskType.PUSH,
+                    dbId, tableId, 0L, 0L, 0L, SystemIdGenerator.getNextId()));
+        }
+        for (int i = 0; i < kafkaPartitions.size(); i++) {
+            kafkaRoutineLoadTaskList.get(i % currentConcurrentTaskNum).addKafkaPartition(kafkaPartitions.get(i));
+        }
+        List<RoutineLoadTask> result = new ArrayList<>();
+        result.addAll(kafkaRoutineLoadTaskList);
+        return result;
+    }
+
+    @Override
+    public int calculateCurrentConcurrentTaskNum() {
+        updatePartitions();
+        SystemInfoService clusterInfo = Catalog.getCurrentSystemInfo();","[{'comment': '`systemInfo`, not `clusterInfo`', 'commenter': 'morningman'}]"
313,fe/src/main/java/org/apache/doris/load/routineload/KafkaRoutineLoadJob.java,"@@ -0,0 +1,114 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.load.routineload;
+
+import com.google.common.base.Strings;
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.common.SystemIdGenerator;
+import org.apache.doris.system.SystemInfoService;
+import org.apache.doris.thrift.TResourceInfo;
+import org.apache.doris.thrift.TTaskType;
+import org.apache.kafka.clients.consumer.KafkaConsumer;
+import org.apache.kafka.common.PartitionInfo;
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.time.Duration;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Properties;
+
+public class KafkaRoutineLoadJob extends RoutineLoadJob {
+    private static final Logger LOG = LogManager.getLogger(KafkaRoutineLoadJob.class);
+
+    private static final String FE_GROUP_ID = ""fe_fetch_partitions"";
+    private static final int FETCH_PARTITIONS_TIMEOUT = 10;
+
+    private String serverAddress;
+    private String topic;
+    // optional
+    private List<Integer> kafkaPartitions;
+
+    public KafkaRoutineLoadJob() {
+    }
+
+    public KafkaRoutineLoadJob(long id, String name, String userName, long dbId, long tableId,
+                               String partitions, String columns, String where, String columnSeparator,
+                               int desireTaskConcurrentNum, JobState state, DataSourceType dataSourceType,
+                               int maxErrorNum, TResourceInfo resourceInfo, String serverAddress, String topic) {
+        super(id, name, userName, dbId, tableId, partitions, columns, where,
+                columnSeparator, desireTaskConcurrentNum, state, dataSourceType, maxErrorNum, resourceInfo);
+        this.serverAddress = serverAddress;
+        this.topic = topic;
+    }
+
+    @Override
+    public List<RoutineLoadTask> divideRoutineLoadJob(int currentConcurrentTaskNum) {
+        // divide kafkaPartitions into tasks
+        List<KafkaRoutineLoadTask> kafkaRoutineLoadTaskList = new ArrayList<>();
+        for (int i = 0; i < currentConcurrentTaskNum; i++) {
+            // TODO(ml): init load task
+            kafkaRoutineLoadTaskList.add(new KafkaRoutineLoadTask(getResourceInfo(), 0L, TTaskType.PUSH,
+                    dbId, tableId, 0L, 0L, 0L, SystemIdGenerator.getNextId()));
+        }
+        for (int i = 0; i < kafkaPartitions.size(); i++) {
+            kafkaRoutineLoadTaskList.get(i % currentConcurrentTaskNum).addKafkaPartition(kafkaPartitions.get(i));
+        }
+        List<RoutineLoadTask> result = new ArrayList<>();
+        result.addAll(kafkaRoutineLoadTaskList);
+        return result;
+    }
+
+    @Override
+    public int calculateCurrentConcurrentTaskNum() {
+        updatePartitions();
+        SystemInfoService clusterInfo = Catalog.getCurrentSystemInfo();
+        String clusterName = Catalog.getInstance().getDb(dbId).getClusterName();","[{'comment': '1. Use Catalog.getCurrentCatalog(), not getInstance();\r\n2. getDb(dbId) may return null if user drop the database concurrently, you should check it.', 'commenter': 'morningman'}]"
313,fe/src/main/java/org/apache/doris/load/routineload/KafkaRoutineLoadJob.java,"@@ -0,0 +1,112 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.load.routineload;
+
+import com.google.common.base.Strings;
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.common.SystemIdGenerator;
+import org.apache.doris.system.SystemInfoService;
+import org.apache.doris.thrift.TResourceInfo;
+import org.apache.doris.thrift.TTaskType;
+import org.apache.kafka.clients.consumer.KafkaConsumer;
+import org.apache.kafka.common.PartitionInfo;
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.time.Duration;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Properties;
+
+public class KafkaRoutineLoadJob extends RoutineLoadJob {
+    private static final Logger LOG = LogManager.getLogger(KafkaRoutineLoadJob.class);
+
+    private static final String FE_GROUP_ID = ""fe_fetch_partitions"";
+    private static final int FETCH_PARTITIONS_TIMEOUT = 10;
+
+    private String serverAddress;
+    private String topic;
+    // optional
+    private List<Integer> kafkaPartitions;
+
+    public KafkaRoutineLoadJob() {
+    }
+
+    public KafkaRoutineLoadJob(long id, String name, String userName, long dbId, long tableId,
+                               String partitions, String columns, String where, String columnSeparator,
+                               int desireTaskConcurrentNum, JobState state, DataSourceType dataSourceType,
+                               int maxErrorNum, TResourceInfo resourceInfo, String serverAddress, String topic) {
+        super(id, name, userName, dbId, tableId, partitions, columns, where,
+                columnSeparator, desireTaskConcurrentNum, state, dataSourceType, maxErrorNum, resourceInfo);
+        this.serverAddress = serverAddress;
+        this.topic = topic;
+    }
+
+    @Override
+    public List<RoutineLoadTask> divideRoutineLoadJob(int currentConcurrentTaskNum) {
+        // divide kafkaPartitions into tasks
+        List<KafkaRoutineLoadTask> kafkaRoutineLoadTaskList = new ArrayList<>();
+        for (int i = 0; i < currentConcurrentTaskNum; i++) {
+            // TODO(ml): init load task
+            kafkaRoutineLoadTaskList.add(new KafkaRoutineLoadTask(getResourceInfo(), 0L, TTaskType.PUSH,
+                    dbId, tableId, 0L, 0L, 0L, SystemIdGenerator.getNextId()));
+        }
+        for (int i = 0; i < kafkaPartitions.size(); i++) {
+            kafkaRoutineLoadTaskList.get(i % currentConcurrentTaskNum).addKafkaPartition(kafkaPartitions.get(i));
+        }
+        List<RoutineLoadTask> result = new ArrayList<>();
+        result.addAll(kafkaRoutineLoadTaskList);
+        return result;
+    }
+
+    @Override
+    public int calculateCurrentConcurrentTaskNum() {
+        updatePartitions();
+        SystemInfoService clusterInfo = Catalog.getCurrentSystemInfo();
+        String clusterName = Catalog.getInstance().getDb(dbId).getClusterName();
+        if (Strings.isNullOrEmpty(clusterName)) {
+            LOG.debug(""database {} has no cluster name"", dbId);
+            clusterName = SystemInfoService.DEFAULT_CLUSTER;
+        }
+        int aliveBeNum = clusterInfo.getClusterBackendIds(clusterName).size();","[{'comment': '` getClusterBackendIds(String clusterName)` does not return all ALIVE backends, it just return ALL backends.\r\nUsing `getClusterBackendIds(String clusterName, boolean needAlive)` instead.', 'commenter': 'morningman'}]"
313,fe/pom.xml,"@@ -517,6 +517,12 @@ under the License.
             <version>1.7.5</version>
         </dependency>
 
+        <dependency>
+            <groupId>org.apache.kafka</groupId>
+            <artifactId>kafka-clients</artifactId>
+            <version>2.0.0</version>","[{'comment': '2.0.0??', 'commenter': 'wuyunfeng'}]"
327,be/src/util/string_util.h,"@@ -57,6 +59,23 @@ struct StringCaseLess {
     }
 };
 
+struct PathHash {
+public:
+    std::size_t operator()(const std::string& identifier, const std::string& path) const {
+        std::size_t hash = std::hash<std::string>()(identifier);
+        std::vector<std::string> path_parts;
+        boost::split(path_parts, path, boost::is_any_of(""/""));
+        for (std::string part : path_parts) {
+            if (part.empty()) {
+                continue;
+            }
+
+            boost::hash_combine<std::string>(hash, part);","[{'comment': 'May be use the hash function in util/hash_util.hpp? it has been already optimized', 'commenter': 'chaoyli'}, {'comment': 'std::hash has already been used in other functions in string_util.h.\r\nSo just for unification. ', 'commenter': 'morningman'}]"
327,be/src/util/string_util.h,"@@ -57,6 +60,23 @@ struct StringCaseLess {
     }
 };
 
+struct PathHash {
+public:
+    std::size_t operator()(const std::string& identifier, const std::string& path) const {
+        std::size_t hash = std::hash<std::string>()(identifier);
+        std::vector<std::string> path_parts;
+        boost::split(path_parts, path, boost::is_any_of(""/""));
+        for (std::string part : path_parts) {","[{'comment': 'You should use reference for part, And you can use `auto`\r\n```suggestion\r\n        for (auto& part : path_parts) {\r\n```', 'commenter': 'imay'}, {'comment': 'fixed', 'commenter': 'morningman'}]"
327,be/src/olap/options.h,"@@ -29,6 +29,7 @@ struct StorePath {
     StorePath(const std::string& path_, int64_t capacity_bytes_)
         : path(path_), capacity_bytes(capacity_bytes_) { }
     std::string path;
+    int64_t path_hash;","[{'comment': ""Why add path_hash here.\r\nStorePath is what user can define, not store's status"", 'commenter': 'imay'}, {'comment': 'forgot to remove it', 'commenter': 'morningman'}]"
327,be/src/agent/heartbeat_server.cpp,"@@ -132,6 +141,12 @@ Status HeartbeatServer::_heartbeat(
         }
     }
 
+    if (need_report) {
+        LOG(INFO) << ""Master FE is changed or restarted. report tablet and disk info immediately"";
+        std::unique_lock<std::mutex> lk(OLAPEngine::get_instance()->report_mtx);
+        OLAPEngine::get_instance()->report_cv.notify_all();
+    }","[{'comment': ""1. `notify_all` needn't to hold mutex first.\r\n2. `report_cv.notify_all()` should encapsulate in a function\r\n3. you can use `_olap_engine` instead of `OLAPEngine::get_instance()`"", 'commenter': 'imay'}, {'comment': 'fixed', 'commenter': 'morningman'}]"
327,be/src/agent/heartbeat_server.cpp,"@@ -119,6 +121,13 @@ Status HeartbeatServer::_heartbeat(
                          << "" local epoch: "" << _epoch << "" received epoch: "" << master_info.epoch;
             return Status(""epoch is not greater than local. ignore heartbeat."");
         }
+    } else {
+        // when Master FE restarted, host and port remains the same, but epoch will be increased.
+        if (master_info.epoch > _epoch) {
+            _epoch = master_info.epoch;","[{'comment': 'this function may be called concurrently. and changing `_epoch` and testing `_epoch` without any protect would lead to strange result', 'commenter': 'imay'}, {'comment': 'Add a mutex to protect _heartbeat() function', 'commenter': 'morningman'}]"
327,be/src/olap/olap_engine.cpp,"@@ -561,7 +561,8 @@ void OLAPEngine::start_disk_stat_monitor() {
     // if drop tables
     // notify disk_state_worker_thread and olap_table_worker_thread until they received
     if (_is_drop_tables) {
-        disk_broken_cv.notify_all();
+        std::unique_lock<std::mutex> lk(report_mtx);
+        report_cv.notify_all();","[{'comment': ""`notify_all` doesn't need lock held"", 'commenter': 'imay'}, {'comment': 'fixed', 'commenter': 'morningman'}]"
327,be/src/util/string_util.h,"@@ -57,6 +60,23 @@ struct StringCaseLess {
     }
 };
 
+struct PathHash {","[{'comment': 'why do you define a struct? A function is better.', 'commenter': 'imay'}, {'comment': ""I don't know why other 'functions' in string_util are defined as struct. Just for unification. "", 'commenter': 'morningman'}, {'comment': ""em...\r\nOther struct can be used for `std::map` or `std::unordered_map`. However your `struct` can't be used as a comparator or a hasher for these container. \r\n\r\nSo, you'd better to define a function here."", 'commenter': 'imay'}, {'comment': 'I got it. Fixed.', 'commenter': 'morningman'}]"
333,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoad.java,"@@ -71,23 +78,69 @@ public RoutineLoad() {
         idToRunningRoutineLoadJob = Maps.newHashMap();
         idToCancelledRoutineLoadJob = Maps.newHashMap();
         idToRoutineLoadTask = Maps.newHashMap();
-        idToNeedSchedulerRoutineLoadTask = Maps.newHashMap();
+        needSchedulerRoutineLoadTask = Queues.newLinkedBlockingQueue();
+        beIdToConcurrentTasks = Maps.newHashMap();
+        taskIdToJobId = Maps.newHashMap();
         lock = new ReentrantReadWriteLock(true);
     }
 
+    public void initBeIdToMaxConcurrentTasks() {
+        if (beIdToMaxConcurrentTasks == null) {
+            beIdToMaxConcurrentTasks = Catalog.getCurrentSystemInfo().getBackendIds(true)
+                    .parallelStream().collect(Collectors.toMap(beId -> beId, beId -> DEFAULT_BE_CONCURRENT_TASK_NUM));
+        }
+    }
+
     public int getTotalMaxConcurrentTaskNum() {
         readLock();
         try {
-            if (beIdToMaxConcurrentTasks == null) {
-                beIdToMaxConcurrentTasks = Catalog.getCurrentSystemInfo().getBackendIds(true)
-                        .parallelStream().collect(Collectors.toMap(beId -> beId, beId -> DEFAULT_BE_CONCURRENT_TASK_NUM));
-            }
+            initBeIdToMaxConcurrentTasks();
             return beIdToMaxConcurrentTasks.values().stream().mapToInt(i -> i).sum();
         } finally {
             readUnlock();
         }
     }
 
+    public void updateBeIdTaskMaps() {
+        writeLock();
+        try {
+            initBeIdToMaxConcurrentTasks();
+            List<Long> beIds = Catalog.getCurrentSystemInfo().getBackendIds(true);
+
+            // diff beIds and beIdToMaxConcurrentTasks.keys()
+            List<Long> newBeIds = beIds.parallelStream().filter(entity -> beIdToMaxConcurrentTasks.get(entity) == null)
+                    .collect(Collectors.toList());
+            List<Long> decommissionBeIds = beIdToMaxConcurrentTasks.keySet().parallelStream()","[{'comment': ""Do not name as 'decommissionBeIds', cause 'decommission' means the backend is being decommissioned.\r\nSo just name it as 'unavailableBeIds'"", 'commenter': 'morningman'}]"
333,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoad.java,"@@ -71,23 +78,69 @@ public RoutineLoad() {
         idToRunningRoutineLoadJob = Maps.newHashMap();
         idToCancelledRoutineLoadJob = Maps.newHashMap();
         idToRoutineLoadTask = Maps.newHashMap();
-        idToNeedSchedulerRoutineLoadTask = Maps.newHashMap();
+        needSchedulerRoutineLoadTask = Queues.newLinkedBlockingQueue();
+        beIdToConcurrentTasks = Maps.newHashMap();
+        taskIdToJobId = Maps.newHashMap();
         lock = new ReentrantReadWriteLock(true);
     }
 
+    public void initBeIdToMaxConcurrentTasks() {
+        if (beIdToMaxConcurrentTasks == null) {
+            beIdToMaxConcurrentTasks = Catalog.getCurrentSystemInfo().getBackendIds(true)
+                    .parallelStream().collect(Collectors.toMap(beId -> beId, beId -> DEFAULT_BE_CONCURRENT_TASK_NUM));
+        }
+    }
+
     public int getTotalMaxConcurrentTaskNum() {
         readLock();
         try {
-            if (beIdToMaxConcurrentTasks == null) {
-                beIdToMaxConcurrentTasks = Catalog.getCurrentSystemInfo().getBackendIds(true)
-                        .parallelStream().collect(Collectors.toMap(beId -> beId, beId -> DEFAULT_BE_CONCURRENT_TASK_NUM));
-            }
+            initBeIdToMaxConcurrentTasks();
             return beIdToMaxConcurrentTasks.values().stream().mapToInt(i -> i).sum();
         } finally {
             readUnlock();
         }
     }
 
+    public void updateBeIdTaskMaps() {
+        writeLock();
+        try {
+            initBeIdToMaxConcurrentTasks();
+            List<Long> beIds = Catalog.getCurrentSystemInfo().getBackendIds(true);
+
+            // diff beIds and beIdToMaxConcurrentTasks.keys()
+            List<Long> newBeIds = beIds.parallelStream().filter(entity -> beIdToMaxConcurrentTasks.get(entity) == null)
+                    .collect(Collectors.toList());
+            List<Long> decommissionBeIds = beIdToMaxConcurrentTasks.keySet().parallelStream()
+                    .filter(entity -> !beIds.contains(entity))
+                    .collect(Collectors.toList());
+            newBeIds.parallelStream().forEach(entity -> beIdToMaxConcurrentTasks.put(entity, DEFAULT_BE_CONCURRENT_TASK_NUM));
+            for (long beId : decommissionBeIds) {
+                beIdToMaxConcurrentTasks.remove(beId);
+                beIdToConcurrentTasks.remove(beId);
+            }
+            LOG.info(""There are {} backends which participate in routine load scheduler. ""
+                            + ""There are {} new bes and {} decommission bes for routine load"",","[{'comment': ""change 'decommission'"", 'commenter': 'morningman'}]"
333,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoad.java,"@@ -44,8 +48,11 @@
     private Map<Long, RoutineLoadJob> idToCancelledRoutineLoadJob;
 
     // stream load tasks meta (not persistent)
-    private Map<Long, RoutineLoadTask> idToRoutineLoadTask;
-    private Map<Long, RoutineLoadTask> idToNeedSchedulerRoutineLoadTask;
+    private Map<Long, RoutineLoadTaskInfo> idToRoutineLoadTask;
+    // KafkaPartitions means that partitions belong to one task
+    // kafka partitions == routine load task (logical)
+    private Queue<RoutineLoadTaskInfo> needSchedulerRoutineLoadTask;","[{'comment': 'needSchedulerRoutineLoadTask -> needSchedulerRoutineLoadTasks', 'commenter': 'morningman'}]"
333,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoad.java,"@@ -251,6 +355,31 @@ public void updateRoutineLoadJobState(RoutineLoadJob routineLoadJob, RoutineLoad
         }
     }
 
+    public void processTimeOutTasks() {
+        List<RoutineLoadTaskInfo> runningTasks = new ArrayList<>(idToRoutineLoadTask.values());
+        runningTasks.removeAll(needSchedulerRoutineLoadTask);
+
+        for (RoutineLoadTaskInfo routineLoadTaskInfo : runningTasks) {
+            routineLoadTaskInfo.writeLock();","[{'comment': ""It's highly recommended NOT to expose lock outside a class.\r\nIt will cause a lot of troubles."", 'commenter': 'morningman'}, {'comment': ""And I can't see why you need a lock here?\r\nNothing need to be protected in RoutineLoadTaskInfo?"", 'commenter': 'morningman'}, {'comment': '‘idToRoutineLoadTask’  is the member of RoutineLoad, it should not be protected by lock in RoutineLoadTaskInfo.', 'commenter': 'morningman'}, {'comment': 'In logical, every task only can process one function, either `processTimeOutTask` or `commit task`. So it really need a segment lock for `idToRoutineLoadTask`. According to the lot of time of `commit task`, I will use one lock instead of per task per lock.', 'commenter': 'EmmyMiao87'}]"
333,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoadTaskScheduler.java,"@@ -0,0 +1,97 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.load.routineload;
+
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.common.util.Daemon;
+import org.apache.doris.task.AgentBatchTask;
+import org.apache.doris.task.AgentTaskExecutor;
+import org.apache.doris.task.AgentTaskQueue;
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.Queue;
+
+/**
+ * Routine load task scheduler is a function which allocate task to be.
+ * Step1: get total idle task num of backends.
+ * Step2: equally divide to be
+ * Step3: submit tasks to be
+ */
+public class RoutineLoadTaskScheduler extends Daemon {
+
+    private static final Logger LOG = LogManager.getLogger(RoutineLoadTaskScheduler.class);
+
+    private RoutineLoad routineLoad = Catalog.getInstance().getRoutineLoadInstance();
+
+    @Override
+    protected void runOneCycle() {
+        // update current beIdMaps for tasks
+        routineLoad.updateBeIdTaskMaps();
+
+        // check timeout tasks
+        routineLoad.processTimeOutTasks();
+
+        // get idle be task num
+        int totalIdleTaskNum = routineLoad.getTotalIdleTaskNum();
+        int allocatedTaskNum = 0;
+        Queue<RoutineLoadTaskInfo> routineLoadTaskList = routineLoad.getNeedSchedulerRoutineLoadTasks();
+        AgentBatchTask batchTask = new AgentBatchTask();
+
+        // allocate task to be
+        while (totalIdleTaskNum > 0) {
+            RoutineLoadTaskInfo routineLoadTaskInfo = routineLoadTaskList.poll();
+            // queue is not empty
+            if (routineLoadTaskInfo != null) {
+                // when routine load task is not abandoned
+                if (routineLoad.getIdToRoutineLoadTask().get(routineLoadTaskInfo.getSignature()) != null) {
+                    long beId = routineLoad.getMinTaskBeId();
+                    RoutineLoadJob routineLoadJob = routineLoad.getJobByTaskId(routineLoadTaskInfo.getSignature());
+                    RoutineLoadTask routineLoadTask = null;
+                    if (routineLoadTaskInfo instanceof KafkaTaskInfo) {
+                        routineLoadTask = new KafkaRoutineLoadTask(routineLoadJob.getResourceInfo(),","[{'comment': 'Is this better that adding a interface function to create `LoadTask`, like `RoutineLoadJob.createTask(TaksInfo)`', 'commenter': 'imay'}, {'comment': 'Is this better than? @imay ', 'commenter': 'wuyunfeng'}]"
333,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoadTaskScheduler.java,"@@ -0,0 +1,97 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.load.routineload;
+
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.common.util.Daemon;
+import org.apache.doris.task.AgentBatchTask;
+import org.apache.doris.task.AgentTaskExecutor;
+import org.apache.doris.task.AgentTaskQueue;
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.Queue;
+
+/**
+ * Routine load task scheduler is a function which allocate task to be.
+ * Step1: get total idle task num of backends.
+ * Step2: equally divide to be
+ * Step3: submit tasks to be
+ */
+public class RoutineLoadTaskScheduler extends Daemon {
+
+    private static final Logger LOG = LogManager.getLogger(RoutineLoadTaskScheduler.class);
+
+    private RoutineLoad routineLoad = Catalog.getInstance().getRoutineLoadInstance();
+
+    @Override
+    protected void runOneCycle() {
+        // update current beIdMaps for tasks
+        routineLoad.updateBeIdTaskMaps();
+
+        // check timeout tasks
+        routineLoad.processTimeOutTasks();
+
+        // get idle be task num
+        int totalIdleTaskNum = routineLoad.getTotalIdleTaskNum();","[{'comment': 'Idle task? task to be scheduled? Could you change a name?', 'commenter': 'imay'}]"
333,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoad.java,"@@ -251,6 +355,31 @@ public void updateRoutineLoadJobState(RoutineLoadJob routineLoadJob, RoutineLoad
         }
     }
 
+    public void processTimeOutTasks() {
+        List<RoutineLoadTaskInfo> runningTasks = new ArrayList<>(idToRoutineLoadTask.values());
+        runningTasks.removeAll(needSchedulerRoutineLoadTask);
+
+        for (RoutineLoadTaskInfo routineLoadTaskInfo : runningTasks) {
+            routineLoadTaskInfo.writeLock();
+            try {
+                if ((System.currentTimeMillis() - routineLoadTaskInfo.getLoadStartTimeMs())
+                        > DEFAULT_TASK_TIMEOUT_MINUTES * 60 * 1000) {
+                    if (routineLoadTaskInfo instanceof KafkaTaskInfo) {
+                        // remove old task
+                        idToRoutineLoadTask.remove(routineLoadTaskInfo.getSignature());
+                        // add new task
+                        KafkaTaskInfo kafkaTaskInfo = new KafkaTaskInfo((KafkaTaskInfo) routineLoadTaskInfo);
+                        idToRoutineLoadTask.put(kafkaTaskInfo.getSignature(), kafkaTaskInfo);
+                        needSchedulerRoutineLoadTask.add(kafkaTaskInfo);
+                    }
+                    LOG.debug(""Task {} was ran more then {} minutes. It was removed and rescheduled"");","[{'comment': 'log without parameter?', 'commenter': 'morningman'}]"
333,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoadTaskScheduler.java,"@@ -0,0 +1,97 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.load.routineload;
+
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.common.util.Daemon;
+import org.apache.doris.task.AgentBatchTask;
+import org.apache.doris.task.AgentTaskExecutor;
+import org.apache.doris.task.AgentTaskQueue;
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.Queue;
+
+/**
+ * Routine load task scheduler is a function which allocate task to be.
+ * Step1: get total idle task num of backends.
+ * Step2: equally divide to be
+ * Step3: submit tasks to be
+ */
+public class RoutineLoadTaskScheduler extends Daemon {
+
+    private static final Logger LOG = LogManager.getLogger(RoutineLoadTaskScheduler.class);
+
+    private RoutineLoad routineLoad = Catalog.getInstance().getRoutineLoadInstance();
+
+    @Override
+    protected void runOneCycle() {
+        // update current beIdMaps for tasks
+        routineLoad.updateBeIdTaskMaps();
+
+        // check timeout tasks
+        routineLoad.processTimeOutTasks();
+
+        // get idle be task num
+        int totalIdleTaskNum = routineLoad.getTotalIdleTaskNum();
+        int allocatedTaskNum = 0;","[{'comment': 'runingTask? I think that `allocate` is used for some resource. And task is to be scheduled, not allocated?', 'commenter': 'imay'}]"
333,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoadTaskScheduler.java,"@@ -0,0 +1,97 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.load.routineload;
+
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.common.util.Daemon;
+import org.apache.doris.task.AgentBatchTask;
+import org.apache.doris.task.AgentTaskExecutor;
+import org.apache.doris.task.AgentTaskQueue;
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.Queue;
+
+/**
+ * Routine load task scheduler is a function which allocate task to be.
+ * Step1: get total idle task num of backends.
+ * Step2: equally divide to be
+ * Step3: submit tasks to be
+ */
+public class RoutineLoadTaskScheduler extends Daemon {
+
+    private static final Logger LOG = LogManager.getLogger(RoutineLoadTaskScheduler.class);
+
+    private RoutineLoad routineLoad = Catalog.getInstance().getRoutineLoadInstance();
+
+    @Override
+    protected void runOneCycle() {","[{'comment': 'I suggest you put this function to another function like `process`, And this function call `process` and catch all `Throwable` to avoid this function throw a `RuntimeError` like `NullpointerException`', 'commenter': 'imay'}]"
333,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoadTaskInfo.java,"@@ -0,0 +1,72 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.load.routineload;
+
+import java.util.concurrent.locks.ReentrantReadWriteLock;
+
+/**
+ * Routine load task info is the task info include the only id (signature).
+ * For the kafka type of task info, it also include partitions which will be obtained data in this task.
+ * The routine load task info and routine load task are the same thing logically.
+ * Differently, routine load task is a agent task include backendId which will execute this task.
+ */
+public class RoutineLoadTaskInfo {
+
+    private long signature;
+
+    private long createTimeMs;
+    private long loadStartTimeMs;
+
+    private ReentrantReadWriteLock lock;
+
+    public RoutineLoadTaskInfo(long signature) {
+        this.signature = signature;
+        this.lock = new ReentrantReadWriteLock(true);","[{'comment': 'Does this simple class need a lock? ', 'commenter': 'imay'}, {'comment': 'After discuss with morningman I will use one lock between different `RoutineLoadTaskInfo`, because the time of locked can be ignore.', 'commenter': 'EmmyMiao87'}]"
333,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoadTaskScheduler.java,"@@ -0,0 +1,97 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.load.routineload;
+
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.common.util.Daemon;
+import org.apache.doris.task.AgentBatchTask;
+import org.apache.doris.task.AgentTaskExecutor;
+import org.apache.doris.task.AgentTaskQueue;
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.Queue;
+
+/**
+ * Routine load task scheduler is a function which allocate task to be.
+ * Step1: get total idle task num of backends.
+ * Step2: equally divide to be
+ * Step3: submit tasks to be
+ */
+public class RoutineLoadTaskScheduler extends Daemon {
+
+    private static final Logger LOG = LogManager.getLogger(RoutineLoadTaskScheduler.class);
+
+    private RoutineLoad routineLoad = Catalog.getInstance().getRoutineLoadInstance();
+
+    @Override
+    protected void runOneCycle() {
+        // update current beIdMaps for tasks
+        routineLoad.updateBeIdTaskMaps();
+
+        // check timeout tasks
+        routineLoad.processTimeOutTasks();
+
+        // get idle be task num
+        int totalIdleTaskNum = routineLoad.getTotalIdleTaskNum();
+        int allocatedTaskNum = 0;
+        Queue<RoutineLoadTaskInfo> routineLoadTaskList = routineLoad.getNeedSchedulerRoutineLoadTasks();
+        AgentBatchTask batchTask = new AgentBatchTask();
+
+        // allocate task to be
+        while (totalIdleTaskNum > 0) {
+            RoutineLoadTaskInfo routineLoadTaskInfo = routineLoadTaskList.poll();
+            // queue is not empty
+            if (routineLoadTaskInfo != null) {
+                // when routine load task is not abandoned
+                if (routineLoad.getIdToRoutineLoadTask().get(routineLoadTaskInfo.getSignature()) != null) {","[{'comment': 'routineLoad.getIdToRoutineLoadTask() is not protected by any locks?\r\nI think you should review your lock usage, not looking good...\r\n', 'commenter': 'morningman'}]"
333,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoad.java,"@@ -107,37 +160,88 @@ public void addRoutineLoadTasks(List<RoutineLoadTask> routineLoadTaskList) {
         }
     }
 
-    public Map<Long, RoutineLoadTask> getIdToRoutineLoadTask() {
+    public Map<Long, RoutineLoadTaskInfo> getIdToRoutineLoadTask() {
         return idToRoutineLoadTask;
     }
 
-    public void addNeedSchedulerRoutineLoadTasks(List<RoutineLoadTask> routineLoadTaskList) {
+    public void addNeedSchedulerRoutineLoadTasks(List<RoutineLoadTaskInfo> routineLoadTaskList, long routineLoadJobId) {
         writeLock();
         try {
-            idToNeedSchedulerRoutineLoadTask.putAll(routineLoadTaskList.parallelStream().collect(
-                    Collectors.toMap(task -> task.getSignature(), task -> task)));
+            routineLoadTaskList.parallelStream().forEach(entity -> needSchedulerRoutineLoadTask.add(entity));
+            routineLoadTaskList.parallelStream().forEach(entity ->
+                    taskIdToJobId.put(entity.getSignature(), routineLoadJobId));
         } finally {
             writeUnlock();
         }
     }
 
-    public void removeRoutineLoadTasks(List<RoutineLoadTask> routineLoadTasks) {
+    public void removeRoutineLoadTasks(List<RoutineLoadTaskInfo> routineLoadTasks) {
         if (routineLoadTasks != null) {
             writeLock();
             try {
                 routineLoadTasks.parallelStream().forEach(task -> idToRoutineLoadTask.remove(task.getSignature()));
                 routineLoadTasks.parallelStream().forEach(task ->
-                        idToNeedSchedulerRoutineLoadTask.remove(task.getSignature()));
+                        needSchedulerRoutineLoadTask.remove(task));
+                routineLoadTasks.parallelStream().forEach(task -> taskIdToJobId.remove(task.getSignature()));
             } finally {
                 writeUnlock();
             }
         }
     }
 
-    public Map<Long, RoutineLoadTask> getIdToNeedSchedulerRoutineLoadTasks() {
+    public int getTotalIdleTaskNum() {
+        readLock();
+        try {
+            int result = 0;
+            initBeIdToMaxConcurrentTasks();
+            for (Map.Entry<Long, Integer> entry : beIdToMaxConcurrentTasks.entrySet()) {
+                if (beIdToConcurrentTasks.get(entry.getKey()) == null) {
+                    result += entry.getValue();
+                } else {
+                    result += entry.getValue() - beIdToConcurrentTasks.get(entry.getKey());
+                }
+            }
+            return result;
+        } finally {
+            readUnlock();
+        }
+    }
+
+    public long getMinTaskBeId() {
+        readLock();
+        try {
+            long result = 0L;
+            int maxIdelTaskNum = 0;
+            initBeIdToMaxConcurrentTasks();
+            for (Map.Entry<Long, Integer> entry : beIdToMaxConcurrentTasks.entrySet()) {
+                if (beIdToConcurrentTasks.get(entry.getKey()) == null) {
+                    result = maxIdelTaskNum < entry.getValue() ? entry.getKey() : result;
+                    maxIdelTaskNum = Math.max(maxIdelTaskNum, entry.getValue());","[{'comment': 'maxIdelTaskNum: misspelling', 'commenter': 'morningman'}, {'comment': ""check all 'idel' misspelling, please..."", 'commenter': 'morningman'}, {'comment': 'I will pay attention next time =_=', 'commenter': 'EmmyMiao87'}]"
333,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoad.java,"@@ -27,15 +28,18 @@
 import java.util.Collection;
 import java.util.List;
 import java.util.Map;
+import java.util.Queue;
 import java.util.concurrent.locks.ReentrantReadWriteLock;
 import java.util.stream.Collectors;
 
 public class RoutineLoad {","[{'comment': 'I think `RoutineLoadManager` is better.', 'commenter': 'imay'}, {'comment': 'Why `Load` not use `LoadManager`?', 'commenter': 'EmmyMiao87'}, {'comment': 'I think RoutineLoadManager is better too.', 'commenter': 'EmmyMiao87'}]"
333,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoadTaskScheduler.java,"@@ -0,0 +1,97 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.load.routineload;
+
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.common.util.Daemon;
+import org.apache.doris.task.AgentBatchTask;
+import org.apache.doris.task.AgentTaskExecutor;
+import org.apache.doris.task.AgentTaskQueue;
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.Queue;
+
+/**
+ * Routine load task scheduler is a function which allocate task to be.
+ * Step1: get total idle task num of backends.
+ * Step2: equally divide to be
+ * Step3: submit tasks to be
+ */
+public class RoutineLoadTaskScheduler extends Daemon {
+
+    private static final Logger LOG = LogManager.getLogger(RoutineLoadTaskScheduler.class);
+
+    private RoutineLoad routineLoad = Catalog.getInstance().getRoutineLoadInstance();
+
+    @Override
+    protected void runOneCycle() {
+        // update current beIdMaps for tasks
+        routineLoad.updateBeIdTaskMaps();
+
+        // check timeout tasks
+        routineLoad.processTimeOutTasks();
+
+        // get idle be task num
+        int totalIdleTaskNum = routineLoad.getTotalIdleTaskNum();
+        int allocatedTaskNum = 0;
+        Queue<RoutineLoadTaskInfo> routineLoadTaskList = routineLoad.getNeedSchedulerRoutineLoadTasks();
+        AgentBatchTask batchTask = new AgentBatchTask();
+
+        // allocate task to be
+        while (totalIdleTaskNum > 0) {
+            RoutineLoadTaskInfo routineLoadTaskInfo = routineLoadTaskList.poll();
+            // queue is not empty
+            if (routineLoadTaskInfo != null) {
+                // when routine load task is not abandoned
+                if (routineLoad.getIdToRoutineLoadTask().get(routineLoadTaskInfo.getSignature()) != null) {
+                    long beId = routineLoad.getMinTaskBeId();","[{'comment': 'What if routineLoad.getMinTaskBeId() return 0, which means no backend has available slots to work?', 'commenter': 'morningman'}, {'comment': 'It will not return 0, because the `clusterIdleSlotNum` is more then 0.', 'commenter': 'EmmyMiao87'}]"
333,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoadTaskScheduler.java,"@@ -0,0 +1,97 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.load.routineload;
+
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.common.util.Daemon;
+import org.apache.doris.task.AgentBatchTask;
+import org.apache.doris.task.AgentTaskExecutor;
+import org.apache.doris.task.AgentTaskQueue;
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.Queue;
+
+/**
+ * Routine load task scheduler is a function which allocate task to be.
+ * Step1: get total idle task num of backends.
+ * Step2: equally divide to be
+ * Step3: submit tasks to be
+ */
+public class RoutineLoadTaskScheduler extends Daemon {
+
+    private static final Logger LOG = LogManager.getLogger(RoutineLoadTaskScheduler.class);
+
+    private RoutineLoad routineLoad = Catalog.getInstance().getRoutineLoadInstance();
+
+    @Override
+    protected void runOneCycle() {
+        // update current beIdMaps for tasks
+        routineLoad.updateBeIdTaskMaps();
+
+        // check timeout tasks
+        routineLoad.processTimeOutTasks();
+
+        // get idle be task num
+        int totalIdleTaskNum = routineLoad.getTotalIdleTaskNum();
+        int allocatedTaskNum = 0;
+        Queue<RoutineLoadTaskInfo> routineLoadTaskList = routineLoad.getNeedSchedulerRoutineLoadTasks();
+        AgentBatchTask batchTask = new AgentBatchTask();
+
+        // allocate task to be
+        while (totalIdleTaskNum > 0) {
+            RoutineLoadTaskInfo routineLoadTaskInfo = routineLoadTaskList.poll();
+            // queue is not empty
+            if (routineLoadTaskInfo != null) {
+                // when routine load task is not abandoned
+                if (routineLoad.getIdToRoutineLoadTask().get(routineLoadTaskInfo.getSignature()) != null) {
+                    long beId = routineLoad.getMinTaskBeId();
+                    RoutineLoadJob routineLoadJob = routineLoad.getJobByTaskId(routineLoadTaskInfo.getSignature());
+                    RoutineLoadTask routineLoadTask = null;
+                    if (routineLoadTaskInfo instanceof KafkaTaskInfo) {
+                        routineLoadTask = new KafkaRoutineLoadTask(routineLoadJob.getResourceInfo(),
+                                beId, routineLoadJob.getDbId(), routineLoadJob.getTableId(),
+                                0L, 0L, 0L, routineLoadJob.getColumns(),
+                                routineLoadJob.getWhere(), routineLoadJob.getColumnSeparator(),
+                                (KafkaTaskInfo) routineLoadTaskInfo,
+                                ((KafkaRoutineLoadJob) routineLoadJob).getProgress());
+                    }
+                    if (routineLoadTask != null) {
+                        routineLoadTaskInfo.setLoadStartTimeMs(System.currentTimeMillis());
+                        AgentTaskQueue.addTask(routineLoadTask);
+                        batchTask.addTask(routineLoadTask);
+                        totalIdleTaskNum--;
+                        allocatedTaskNum++;
+                        routineLoad.addNumOfConcurrentTasksByBeId(beId);
+                    }
+                } else {
+                    LOG.debug(""Task {} for job already has been discarded"", routineLoadTaskInfo.getSignature());","[{'comment': ""The correct grammar is: 'has already been' or 'has been already', not 'already has been'"", 'commenter': 'morningman'}]"
333,fe/src/main/java/org/apache/doris/load/routineload/KafkaRoutineLoadJob.java,"@@ -59,21 +59,23 @@ public KafkaRoutineLoadJob(long id, String name, String userName, long dbId, lon
         this.topic = topic;
     }
 
+    public KafkaProgress getProgress() {
+        Gson gson = new Gson();
+        return gson.fromJson(this.progress, KafkaProgress.class);
+    }
+
     @Override
-    public List<RoutineLoadTask> divideRoutineLoadJob(int currentConcurrentTaskNum) {
+    public List<RoutineLoadTaskInfo> divideRoutineLoadJob(int currentConcurrentTaskNum) {
         // divide kafkaPartitions into tasks
-        List<KafkaRoutineLoadTask> kafkaRoutineLoadTaskList = new ArrayList<>();
+        List<RoutineLoadTaskInfo> kafkaRoutineLoadTaskList = new ArrayList<>();
         for (int i = 0; i < currentConcurrentTaskNum; i++) {
-            // TODO(ml): init load task
-            kafkaRoutineLoadTaskList.add(new KafkaRoutineLoadTask(getResourceInfo(), 0L, TTaskType.PUSH,
-                    dbId, tableId, 0L, 0L, 0L, SystemIdGenerator.getNextId()));
+            kafkaRoutineLoadTaskList.add(new KafkaTaskInfo(SystemIdGenerator.getNextId()));","[{'comment': ""SystemIdGenerator will produce repetitive id if Frontend restart or Master FE changed.\r\nJust use a UUID or random Long is better, if you don't want to persist this info."", 'commenter': 'morningman'}, {'comment': 'I think maybe I can reuse `CatalogIdGenerator`, but `CatalogIdGenerator` need add a field named `name`.', 'commenter': 'EmmyMiao87'}]"
333,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoadScheduler.java,"@@ -45,7 +46,7 @@ protected void runOneCycle() {
         LOG.debug(""there are {} job need scheduler"", routineLoadJobList.size());
         for (RoutineLoadJob routineLoadJob : routineLoadJobList) {
             // judge nums of tasks more then max concurrent tasks of cluster
-            List<RoutineLoadTask> routineLoadTaskList = null;
+            List<RoutineLoadTaskInfo> routineLoadTaskList = null;
             try {
                 routineLoadJob.writeLock();","[{'comment': 'use routineLoadJob.writeLock() to protect is weird.\r\nAnd lock should be used as (lock() is outside the try{}):\r\nlock();\r\ntry {\r\n\r\n} finally {\r\n    unlock();\r\n}', 'commenter': 'morningman'}, {'comment': 'Using `routineLoadJob.writeLock()` means that every process of different `RoutineLoadJob` will not be blocked.', 'commenter': 'EmmyMiao87'}]"
333,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoadManager.java,"@@ -107,37 +160,93 @@ public void addRoutineLoadTasks(List<RoutineLoadTask> routineLoadTaskList) {
         }
     }
 
-    public Map<Long, RoutineLoadTask> getIdToRoutineLoadTask() {
-        return idToRoutineLoadTask;
+    public Map<Long, RoutineLoadTaskInfo> getIdToRoutineLoadTask() {
+        readLock();","[{'comment': ""This lock protects nothing...\r\nAfter the caller gets 'idToRoutineLoadTask', it can do anything without lock protection."", 'commenter': 'morningman'}]"
336,fe/src/main/java/org/apache/doris/clone/TabletScanner.java,"@@ -0,0 +1,181 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.clone;
+
+import com.google.common.base.Preconditions;
+import com.google.common.collect.HashBasedTable;
+import com.google.common.collect.Sets;
+
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.catalog.Database;
+import org.apache.doris.catalog.MaterializedIndex;
+import org.apache.doris.catalog.OlapTable;
+import org.apache.doris.catalog.Partition;
+import org.apache.doris.catalog.Table;
+import org.apache.doris.catalog.Tablet;
+import org.apache.doris.catalog.Tablet.TabletStatus;
+import org.apache.doris.common.Pair;
+import org.apache.doris.common.util.Daemon;
+import org.apache.doris.system.SystemInfoService;
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.List;
+import java.util.Set;
+
+/*
+ * This scanner is responsible for scanning all unhealthy tablets.
+ * It does not responsible for any scheduler of tablet repairing or balance
+ */
+public class TabletScanner extends Daemon {
+    private static final Logger LOG = LogManager.getLogger(TabletScanner.class);
+
+    private static long INTERVAL_MS = 60 * 1000L; // 1min
+
+    private Catalog catalog;
+    private SystemInfoService infoService;
+    private TabletFactory tabletFactory;
+
+    // db id -> (tbl id -> partition id)
+    // priority of replicas of partitions in this table will be set to VERY_HIGH if not healthy
+    private com.google.common.collect.Table<Long, Long, Set<Long>> priors = HashBasedTable.create();
+
+    public TabletScanner() {
+        super(""tablet scanner"", INTERVAL_MS);
+        catalog = Catalog.getCurrentCatalog();
+        infoService = Catalog.getCurrentSystemInfo();
+        tabletFactory = new TabletFactory();
+    }
+
+    public void addPriors(long dbId, long tblId, List<Long> partitionIds) {","[{'comment': '""Prior"" is adjective', 'commenter': 'chenhao7253886'}, {'comment': 'I just want use an abbreviation, which should be prio. I will fix it.', 'commenter': 'morningman'}]"
336,fe/src/main/java/org/apache/doris/clone/TabletScanner.java,"@@ -0,0 +1,181 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.clone;
+
+import com.google.common.base.Preconditions;
+import com.google.common.collect.HashBasedTable;
+import com.google.common.collect.Sets;
+
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.catalog.Database;
+import org.apache.doris.catalog.MaterializedIndex;
+import org.apache.doris.catalog.OlapTable;
+import org.apache.doris.catalog.Partition;
+import org.apache.doris.catalog.Table;
+import org.apache.doris.catalog.Tablet;
+import org.apache.doris.catalog.Tablet.TabletStatus;
+import org.apache.doris.common.Pair;
+import org.apache.doris.common.util.Daemon;
+import org.apache.doris.system.SystemInfoService;
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.List;
+import java.util.Set;
+
+/*
+ * This scanner is responsible for scanning all unhealthy tablets.
+ * It does not responsible for any scheduler of tablet repairing or balance
+ */
+public class TabletScanner extends Daemon {
+    private static final Logger LOG = LogManager.getLogger(TabletScanner.class);
+
+    private static long INTERVAL_MS = 60 * 1000L; // 1min
+
+    private Catalog catalog;
+    private SystemInfoService infoService;
+    private TabletFactory tabletFactory;
+
+    // db id -> (tbl id -> partition id)
+    // priority of replicas of partitions in this table will be set to VERY_HIGH if not healthy
+    private com.google.common.collect.Table<Long, Long, Set<Long>> priors = HashBasedTable.create();
+
+    public TabletScanner() {
+        super(""tablet scanner"", INTERVAL_MS);
+        catalog = Catalog.getCurrentCatalog();
+        infoService = Catalog.getCurrentSystemInfo();
+        tabletFactory = new TabletFactory();
+    }
+
+    public void addPriors(long dbId, long tblId, List<Long> partitionIds) {
+        Preconditions.checkArgument(!partitionIds.isEmpty());
+        synchronized (priors) {
+            Set<Long> parts = priors.get(dbId, tblId);
+            if (parts == null) {
+                parts = Sets.newHashSet();
+                priors.put(dbId, tblId, parts);
+            }
+            parts.addAll(partitionIds);
+        }
+    }
+
+    /*
+     * For each cycle, TabletScanner will scan all OlapTable's tablet.
+     * If a tablet is not healthy, a TabletInfo will be created and sent to TabletFactory for repairing.
+     */
+    @Override
+    protected void runOneCycle() {
+
+        if (!tabletFactory.isEmpty()) {
+            LOG.info(""tablet factory has unfinished tasks. skip this round of checking"");
+            return;
+        }
+
+        scanTablets();
+    }
+
+    private void scanTablets() {
+        List<Long> dbIds = catalog.getDbIds();
+        int unhealthyTabletNum = 0;
+        int totalTabletNum = 0;
+
+        for (Long dbId : dbIds) {
+            Database db = catalog.getDb(dbId);
+            if (db == null) {
+                continue;
+            }
+
+            if (db.isInfoSchemaDb()) {
+                continue;
+            }
+
+            db.readLock();
+            try {
+                for (Table table : db.getTables()) {
+                    if (!table.needScan()) {
+                        continue;
+                    }
+
+                    OlapTable olapTbl = (OlapTable) table;","[{'comment': 'MysqlTable?', 'commenter': 'chenhao7253886'}, {'comment': 'table.needScan() blocks any tables which type is not OLAP', 'commenter': 'morningman'}]"
336,fe/src/main/java/org/apache/doris/clone/TabletFactory.java,"@@ -0,0 +1,655 @@
+package org.apache.doris.clone;
+
+import com.google.common.base.Preconditions;
+import com.google.common.collect.ImmutableMap;
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+import com.google.common.collect.Sets;
+
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.catalog.Database;
+import org.apache.doris.catalog.MaterializedIndex;
+import org.apache.doris.catalog.OlapTable;
+import org.apache.doris.catalog.Partition;
+import org.apache.doris.catalog.Replica;
+import org.apache.doris.catalog.Replica.ReplicaState;
+import org.apache.doris.catalog.Tablet;
+import org.apache.doris.catalog.Tablet.TabletStatus;
+import org.apache.doris.catalog.TabletInvertedIndex;
+import org.apache.doris.common.Pair;
+import org.apache.doris.common.util.Daemon;
+import org.apache.doris.persist.ReplicaPersistInfo;
+import org.apache.doris.system.Backend;
+import org.apache.doris.system.SystemInfoService;
+import org.apache.doris.task.AgentTaskQueue;
+import org.apache.doris.task.CloneTask;
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.concurrent.PriorityBlockingQueue;
+import java.util.concurrent.TimeUnit;
+import java.util.stream.Collectors;
+
+/*
+ * TabletFactory saved the tablets produced by TabletScanner and try to repair them.
+ * It also try to balance the cluster load if there is no tablet need to be repaired.
+ * 
+ * We are expecting to an efficient way to recovery the entire cluster and make it balanced.
+ * Case 1:
+ *  A Backend is down. All tablets which has replica on this BE should be repaired as soon as possible.
+ *  
+ * Case 1.1:
+ *  As Backend is down, some tables should be repaired in high priority. So the repair task should be able
+ *  to preempted.
+ *  
+ * Case 2:
+ *  A new Backend is added to the cluster. Replicas should be transfer to that host to balance the cluster load.
+ * 1 sec
+ */
+public class TabletFactory extends Daemon {
+    private static final Logger LOG = LogManager.getLogger(TabletFactory.class);
+
+    private PriorityBlockingQueue<TabletInfo> q = new PriorityBlockingQueue<>();;
+    private Set<Long> runningTabletIds = Sets.newHashSet();;
+
+    /*
+     *  this is a init #working slot per backend.
+     *  We will increase or decrease the slot num dynamically based on the clone task statistic info
+     */
+    private static int BATCH_NUM = 10; // handle at most BATCH_NUM tablets in one loop
+
+    private static int SLOT_NUM_PER_PATH = 1;
+","[{'comment': 'Constant should be final.', 'commenter': 'chenhao7253886'}, {'comment': 'OK', 'commenter': 'morningman'}]"
336,fe/src/main/java/org/apache/doris/clone/TabletFactory.java,"@@ -0,0 +1,655 @@
+package org.apache.doris.clone;
+
+import com.google.common.base.Preconditions;
+import com.google.common.collect.ImmutableMap;
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+import com.google.common.collect.Sets;
+
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.catalog.Database;
+import org.apache.doris.catalog.MaterializedIndex;
+import org.apache.doris.catalog.OlapTable;
+import org.apache.doris.catalog.Partition;
+import org.apache.doris.catalog.Replica;
+import org.apache.doris.catalog.Replica.ReplicaState;
+import org.apache.doris.catalog.Tablet;
+import org.apache.doris.catalog.Tablet.TabletStatus;
+import org.apache.doris.catalog.TabletInvertedIndex;
+import org.apache.doris.common.Pair;
+import org.apache.doris.common.util.Daemon;
+import org.apache.doris.persist.ReplicaPersistInfo;
+import org.apache.doris.system.Backend;
+import org.apache.doris.system.SystemInfoService;
+import org.apache.doris.task.AgentTaskQueue;
+import org.apache.doris.task.CloneTask;
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.concurrent.PriorityBlockingQueue;
+import java.util.concurrent.TimeUnit;
+import java.util.stream.Collectors;
+
+/*
+ * TabletFactory saved the tablets produced by TabletScanner and try to repair them.
+ * It also try to balance the cluster load if there is no tablet need to be repaired.
+ * 
+ * We are expecting to an efficient way to recovery the entire cluster and make it balanced.
+ * Case 1:
+ *  A Backend is down. All tablets which has replica on this BE should be repaired as soon as possible.
+ *  
+ * Case 1.1:
+ *  As Backend is down, some tables should be repaired in high priority. So the repair task should be able
+ *  to preempted.
+ *  
+ * Case 2:
+ *  A new Backend is added to the cluster. Replicas should be transfer to that host to balance the cluster load.
+ * 1 sec
+ */
+public class TabletFactory extends Daemon {
+    private static final Logger LOG = LogManager.getLogger(TabletFactory.class);
+
+    private PriorityBlockingQueue<TabletInfo> q = new PriorityBlockingQueue<>();;
+    private Set<Long> runningTabletIds = Sets.newHashSet();;","[{'comment': ""Variable name should't be single letter in most conditions, to describle the function."", 'commenter': 'chenhao7253886'}, {'comment': 'OK', 'commenter': 'morningman'}]"
336,fe/src/main/java/org/apache/doris/clone/TabletFactory.java,"@@ -0,0 +1,655 @@
+package org.apache.doris.clone;
+
+import com.google.common.base.Preconditions;
+import com.google.common.collect.ImmutableMap;
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+import com.google.common.collect.Sets;
+
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.catalog.Database;
+import org.apache.doris.catalog.MaterializedIndex;
+import org.apache.doris.catalog.OlapTable;
+import org.apache.doris.catalog.Partition;
+import org.apache.doris.catalog.Replica;
+import org.apache.doris.catalog.Replica.ReplicaState;
+import org.apache.doris.catalog.Tablet;
+import org.apache.doris.catalog.Tablet.TabletStatus;
+import org.apache.doris.catalog.TabletInvertedIndex;
+import org.apache.doris.common.Pair;
+import org.apache.doris.common.util.Daemon;
+import org.apache.doris.persist.ReplicaPersistInfo;
+import org.apache.doris.system.Backend;
+import org.apache.doris.system.SystemInfoService;
+import org.apache.doris.task.AgentTaskQueue;
+import org.apache.doris.task.CloneTask;
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.concurrent.PriorityBlockingQueue;
+import java.util.concurrent.TimeUnit;
+import java.util.stream.Collectors;
+
+/*
+ * TabletFactory saved the tablets produced by TabletScanner and try to repair them.
+ * It also try to balance the cluster load if there is no tablet need to be repaired.
+ * 
+ * We are expecting to an efficient way to recovery the entire cluster and make it balanced.
+ * Case 1:
+ *  A Backend is down. All tablets which has replica on this BE should be repaired as soon as possible.
+ *  
+ * Case 1.1:
+ *  As Backend is down, some tables should be repaired in high priority. So the repair task should be able
+ *  to preempted.
+ *  
+ * Case 2:
+ *  A new Backend is added to the cluster. Replicas should be transfer to that host to balance the cluster load.
+ * 1 sec
+ */
+public class TabletFactory extends Daemon {
+    private static final Logger LOG = LogManager.getLogger(TabletFactory.class);
+
+    private PriorityBlockingQueue<TabletInfo> q = new PriorityBlockingQueue<>();;
+    private Set<Long> runningTabletIds = Sets.newHashSet();;
+
+    /*
+     *  this is a init #working slot per backend.
+     *  We will increase or decrease the slot num dynamically based on the clone task statistic info
+     */
+    private static int BATCH_NUM = 10; // handle at most BATCH_NUM tablets in one loop
+
+    private static int SLOT_NUM_PER_PATH = 1;
+
+    // be id -> #working slots
+    private Map<Long, Slot> backendsWorkingSlots = Maps.newConcurrentMap();;
+    
+    private Catalog catalog;
+    private SystemInfoService infoService;
+    private TabletInvertedIndex invertedIndex;
+
+    // cluster name -> load statistic
+    private Map<String, ClusterLoadStatistic> statisticMap = Maps.newConcurrentMap();
+    private long lastLoadStatisticUpdateTime = 0;
+    private static long UPDATE_INTERVAL_MS = 60 * 1000;
+
+    private boolean isInit = false;
+
+    public TabletFactory() {
+        super(""tablet factory"", 1000);
+        catalog = Catalog.getCurrentCatalog();
+        infoService = Catalog.getCurrentSystemInfo();
+        invertedIndex = Catalog.getCurrentInvertedIndex();
+    }
+
+    public boolean init() {
+        return initWorkingSlots();
+    }
+
+    private boolean initWorkingSlots() {
+        ImmutableMap<Long, Backend> backends = infoService.getBackendsInCluster(null);
+        for (Backend be : backends.values()) {
+            List<Long> pathHashes = be.getDisks().values().stream().map(v -> v.getPathHash()).collect(Collectors.toList());
+            Slot slot = new Slot(pathHashes, SLOT_NUM_PER_PATH);
+            backendsWorkingSlots.put(be.getId(), slot);
+            LOG.info(""init backend {} working slots: {}"", be.getId(), be.getDisks().size());
+        }
+
+        // TODO(cmy): the path hash in DiskInfo may not be available before the entire cluster being upgraded
+        // to the latest version.
+        // So we have to wait until we get all path hash info.
+        return false;
+    }
+
+    public void updateWorkingSlots() {
+        ImmutableMap<Long, Backend> backends = infoService.getBackendsInCluster(null);
+        Set<Long> deletedBeIds = Sets.newHashSet();
+
+        // update exist backends
+        for (Long beId : backendsWorkingSlots.keySet()) {
+            if (backends.containsKey(beId)) {
+                List<Long> pathHashes = backends.get(beId).getDisks().values().stream().map(v -> v.getPathHash()).collect(Collectors.toList());
+                backendsWorkingSlots.get(beId).updateSlots(pathHashes);
+            } else {
+                deletedBeIds.add(beId);
+            }
+        }
+
+        // delete non-exist backends
+        for (Long beId : deletedBeIds) {
+            backendsWorkingSlots.remove(beId);
+        }
+
+        // add new backends
+        for (Backend be : backends.values()) {
+            if (!backendsWorkingSlots.containsKey(be.getId())) {
+                List<Long> pathHashes = be.getDisks().values().stream().map(v -> v.getPathHash()).collect(Collectors.toList());
+                Slot slot = new Slot(pathHashes, SLOT_NUM_PER_PATH);
+                backendsWorkingSlots.put(be.getId(), slot);
+                LOG.info(""init backend {} working slots: {}"", be.getId(), be.getDisks().size());
+            }
+        }
+    }
+
+    public void addNewBackend(long newBackendId) {
+        // backendsWorkingSlots.putIfAbsent(newBackendId, INIT_BACKEND_WORKING_SLOT);
+    }
+
+    public void deleteBackend(long backendId) {
+        backendsWorkingSlots.remove(backendId);
+    }
+
+    public Map<Long, Slot> getBackendsWorkingSlots() {
+        return backendsWorkingSlots;
+    }
+
+    public synchronized boolean addTablet(TabletInfo tablet) {
+        if (runningTabletIds.contains(tablet.getTabletId())) {
+            return false;
+        }
+        runningTabletIds.add(tablet.getTabletId());
+        q.offer(tablet);
+        return true;
+    }
+
+    public boolean isEmpty() {
+        return q.isEmpty();
+    }
+
+    /*
+     * TabletFactory will run as a daemon thread at a very short interval(default 1 sec)
+     * It will try to repair the tablet in queue, and try to balance the cluster if possible.
+     */
+    @Override
+    protected void runOneCycle() {
+        if (!isInit && !init()) {
+            // not ready to start.
+            return;
+        }
+
+        updateClusterLoadStatisticsIfNecessary();
+
+        handleQ();
+","[{'comment': 'Q?', 'commenter': 'chenhao7253886'}, {'comment': 'tabletQ', 'commenter': 'morningman'}]"
336,fe/src/main/java/org/apache/doris/clone/TabletFactory.java,"@@ -0,0 +1,655 @@
+package org.apache.doris.clone;","[{'comment': 'no license header', 'commenter': 'imay'}, {'comment': 'Added', 'commenter': 'morningman'}]"
336,fe/src/main/java/org/apache/doris/clone/TabletChecker.java,"@@ -0,0 +1,185 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.clone;
+
+import com.google.common.base.Preconditions;
+import com.google.common.collect.HashBasedTable;
+import com.google.common.collect.Sets;
+
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.catalog.Database;
+import org.apache.doris.catalog.MaterializedIndex;
+import org.apache.doris.catalog.OlapTable;
+import org.apache.doris.catalog.Partition;
+import org.apache.doris.catalog.Table;
+import org.apache.doris.catalog.Tablet;
+import org.apache.doris.catalog.Tablet.TabletStatus;
+import org.apache.doris.common.Pair;
+import org.apache.doris.common.util.Daemon;
+import org.apache.doris.system.SystemInfoService;
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.List;
+import java.util.Set;
+
+/*
+ * This checker is responsible for checking all unhealthy tablets.
+ * It does not responsible for any scheduler of tablet repairing or balance
+ */
+public class TabletChecker extends Daemon {
+    private static final Logger LOG = LogManager.getLogger(TabletChecker.class);
+
+    private static final long INTERVAL_MS = 60 * 1000L; // 1min
+
+    private Catalog catalog;
+    private SystemInfoService infoService;
+    private TabletScheduler tabletScheduler;
+
+    // db id -> (tbl id -> partition id)
+    // priority of replicas of partitions in this table will be set to VERY_HIGH if not healthy
+    private com.google.common.collect.Table<Long, Long, Set<Long>> prios = HashBasedTable.create();","[{'comment': ""I think the abbreviation 'prios' is not a good name."", 'commenter': 'kangpinghuang'}, {'comment': 'I just want make the name shorter..', 'commenter': 'morningman'}]"
336,fe/src/main/java/org/apache/doris/clone/TabletScanner.java,"@@ -0,0 +1,181 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.clone;
+
+import com.google.common.base.Preconditions;
+import com.google.common.collect.HashBasedTable;
+import com.google.common.collect.Sets;
+
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.catalog.Database;
+import org.apache.doris.catalog.MaterializedIndex;
+import org.apache.doris.catalog.OlapTable;
+import org.apache.doris.catalog.Partition;
+import org.apache.doris.catalog.Table;
+import org.apache.doris.catalog.Tablet;
+import org.apache.doris.catalog.Tablet.TabletStatus;
+import org.apache.doris.common.Pair;
+import org.apache.doris.common.util.Daemon;
+import org.apache.doris.system.SystemInfoService;
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.List;
+import java.util.Set;
+
+/*
+ * This scanner is responsible for scanning all unhealthy tablets.
+ * It does not responsible for any scheduler of tablet repairing or balance
+ */
+public class TabletScanner extends Daemon {
+    private static final Logger LOG = LogManager.getLogger(TabletScanner.class);
+
+    private static long INTERVAL_MS = 60 * 1000L; // 1min
+
+    private Catalog catalog;
+    private SystemInfoService infoService;
+    private TabletFactory tabletFactory;
+
+    // db id -> (tbl id -> partition id)
+    // priority of replicas of partitions in this table will be set to VERY_HIGH if not healthy
+    private com.google.common.collect.Table<Long, Long, Set<Long>> priors = HashBasedTable.create();
+
+    public TabletScanner() {
+        super(""tablet scanner"", INTERVAL_MS);
+        catalog = Catalog.getCurrentCatalog();
+        infoService = Catalog.getCurrentSystemInfo();
+        tabletFactory = new TabletFactory();
+    }
+
+    public void addPriors(long dbId, long tblId, List<Long> partitionIds) {
+        Preconditions.checkArgument(!partitionIds.isEmpty());
+        synchronized (priors) {
+            Set<Long> parts = priors.get(dbId, tblId);
+            if (parts == null) {
+                parts = Sets.newHashSet();
+                priors.put(dbId, tblId, parts);
+            }
+            parts.addAll(partitionIds);
+        }
+    }
+
+    /*
+     * For each cycle, TabletScanner will scan all OlapTable's tablet.
+     * If a tablet is not healthy, a TabletInfo will be created and sent to TabletFactory for repairing.
+     */
+    @Override
+    protected void runOneCycle() {
+
+        if (!tabletFactory.isEmpty()) {
+            LOG.info(""tablet factory has unfinished tasks. skip this round of checking"");
+            return;
+        }
+
+        scanTablets();","[{'comment': 'The exception need catching in `runOneCycle`', 'commenter': 'EmmyMiao87'}, {'comment': 'Here we only catch expected exception, any unexpected exception is caught in `Deamon`', 'commenter': 'morningman'}]"
336,fe/src/main/java/org/apache/doris/clone/TabletChecker.java,"@@ -0,0 +1,213 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.clone;
+
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.catalog.Database;
+import org.apache.doris.catalog.MaterializedIndex;
+import org.apache.doris.catalog.OlapTable;
+import org.apache.doris.catalog.Partition;
+import org.apache.doris.catalog.Table;
+import org.apache.doris.catalog.Tablet;
+import org.apache.doris.catalog.Tablet.TabletStatus;
+import org.apache.doris.common.Pair;
+import org.apache.doris.common.util.Daemon;
+import org.apache.doris.system.SystemInfoService;
+
+import com.google.common.base.Preconditions;
+import com.google.common.collect.HashBasedTable;
+import com.google.common.collect.Sets;
+
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.List;
+import java.util.Set;
+
+/*
+ * This checker is responsible for checking all unhealthy tablets.
+ * It does not responsible for any scheduler of tablet repairing or balance
+ */
+public class TabletChecker extends Daemon {
+    private static final Logger LOG = LogManager.getLogger(TabletChecker.class);
+
+    private static final long INTERVAL_MS = 60 * 1000L; // 1min
+
+    private Catalog catalog;
+    private SystemInfoService infoService;
+    private TabletScheduler tabletScheduler;
+    private TabletSchedulerStat stat;
+
+    // db id -> (tbl id -> partition id)
+    // priority of replicas of partitions in this table will be set to VERY_HIGH if not healthy
+    private com.google.common.collect.Table<Long, Long, Set<Long>> prios = HashBasedTable.create();
+
+    public TabletChecker(Catalog catalog, SystemInfoService infoService, TabletScheduler tabletScheduler,
+            TabletSchedulerStat stat) {
+        super(""tablet checker"", INTERVAL_MS);
+        this.catalog = catalog;
+        this.infoService = infoService;
+        this.tabletScheduler = tabletScheduler;
+        this.stat = stat;
+    }
+
+    public void addPrios(long dbId, long tblId, List<Long> partitionIds) {
+        Preconditions.checkArgument(!partitionIds.isEmpty());
+        synchronized (prios) {
+            Set<Long> parts = prios.get(dbId, tblId);
+            if (parts == null) {
+                parts = Sets.newHashSet();
+                prios.put(dbId, tblId, parts);
+            }
+            parts.addAll(partitionIds);
+        }
+
+        // we also need to change the priority of tablets which are already in
+        tabletScheduler.changePriorityOfTablets(dbId, tblId, partitionIds);
+    }
+
+    /*
+     * For each cycle, TabletChecker will check all OlapTable's tablet.
+     * If a tablet is not healthy, a TabletInfo will be created and sent to TabletScheduler for repairing.
+     */
+    @Override
+    protected void runOneCycle() {
+        boolean schedulerHasTask = !tabletScheduler.isEmpty();
+
+        // if scheduler has tasks, we only check tablets in prios
+        checkTablets(schedulerHasTask);
+        stat.counterTabletCheckRound.incrementAndGet();
+
+        LOG.info(stat.incrementalBrief());","[{'comment': 'The exception of `checkTablets` need catching in `runOneCycle`', 'commenter': 'EmmyMiao87'}, {'comment': 'Here we only catch expected exception, any unexpected exception is caught in `Deamon`', 'commenter': 'morningman'}]"
336,be/src/olap/olap_engine.cpp,"@@ -2750,6 +2767,34 @@ OLAPStatus OLAPEngine::finish_clone(OLAPTablePtr tablet, const string& clone_dir
     return res;
 }
 
+OLAPStatus OLAPEngine::obtain_shard_path_by_hash(
+        int64_t path_hash, std::string* shard_path, OlapStore** store) {
+    OLAP_LOG_INFO(""begin to process obtain root path by hash: %ld"", path_hash);","[{'comment': 'OLAP_LOG_INFO, OLAP_LOG_WARNING has been deprecated. Use LOG(INFO), LOG(WARNING), LOG(FATAL) instead.', 'commenter': 'chaoyli'}]"
336,be/src/olap/olap_engine.cpp,"@@ -662,6 +665,8 @@ Status OLAPEngine::set_cluster_id(int32_t cluster_id) {
     return Status::OK;
 }
 
+
+","[{'comment': 'Empty line', 'commenter': 'chaoyli'}]"
336,fe/src/main/java/org/apache/doris/clone/TabletSchedulerStat.java,"@@ -0,0 +1,180 @@
+package org.apache.doris.clone;","[{'comment': 'license header', 'commenter': 'imay'}]"
336,fe/src/main/java/org/apache/doris/clone/TabletChecker.java,"@@ -0,0 +1,449 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.clone;
+
+import org.apache.doris.analysis.AdminCancelRepairTableStmt;
+import org.apache.doris.analysis.AdminRepairTableStmt;
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.catalog.Database;
+import org.apache.doris.catalog.MaterializedIndex;
+import org.apache.doris.catalog.OlapTable;
+import org.apache.doris.catalog.Partition;
+import org.apache.doris.catalog.Table;
+import org.apache.doris.catalog.Table.TableType;
+import org.apache.doris.catalog.Tablet;
+import org.apache.doris.catalog.Tablet.TabletStatus;
+import org.apache.doris.common.DdlException;
+import org.apache.doris.common.Pair;
+import org.apache.doris.common.util.Daemon;
+import org.apache.doris.system.SystemInfoService;
+
+import com.google.common.base.Preconditions;
+import com.google.common.collect.HashBasedTable;
+import com.google.common.collect.Lists;
+import com.google.common.collect.Sets;
+import com.google.common.collect.Table.Cell;
+
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.stream.Collectors;
+
+/*
+ * This checker is responsible for checking all unhealthy tablets.
+ * It does not responsible for any scheduler of tablet repairing or balance
+ */
+public class TabletChecker extends Daemon {
+    private static final Logger LOG = LogManager.getLogger(TabletChecker.class);
+
+    private static final long CHECK_INTERVAL_MS = 20 * 1000L; // 20 second
+
+    // if the number of scheduled tablets in TabletScheduler exceed this threshold
+    // skip checking.
+    private static final int MAX_SCHEDULING_TABLETS = 5000;
+
+    private Catalog catalog;
+    private SystemInfoService infoService;
+    private TabletScheduler tabletScheduler;
+    private TabletSchedulerStat stat;
+
+    // db id -> (tbl id -> PrioPart)
+    // priority of replicas of partitions in this table will be set to VERY_HIGH if not healthy
+    private com.google.common.collect.Table<Long, Long, Set<PrioPart>> prios = HashBasedTable.create();
+    
+    // represent a partition which need to be repaired preferentially
+    public static class PrioPart {
+        public long partId;
+        public long addTime;
+        public long timeoutMs;
+
+        public PrioPart(long partId, long addTime, long timeoutMs) {
+            this.partId = partId;
+            this.addTime = addTime;
+            this.timeoutMs = timeoutMs;
+        }
+
+        public boolean isTimeout() {
+            return System.currentTimeMillis() - addTime > timeoutMs;
+        }
+
+        @Override
+        public boolean equals(Object obj) {
+            if (!(obj instanceof PrioPart)) {
+                return false;
+            }
+            return partId == ((PrioPart) obj).partId;
+        }
+    }
+
+    public TabletChecker(Catalog catalog, SystemInfoService infoService, TabletScheduler tabletScheduler,
+            TabletSchedulerStat stat) {
+        super(""tablet checker"", CHECK_INTERVAL_MS);
+        this.catalog = catalog;
+        this.infoService = infoService;
+        this.tabletScheduler = tabletScheduler;
+        this.stat = stat;
+    }
+
+    public void addPrios(long dbId, long tblId, List<Long> partitionIds, long timeoutMs) {
+        Preconditions.checkArgument(!partitionIds.isEmpty());
+        long currentTime = System.currentTimeMillis();
+        synchronized (prios) {
+            Set<PrioPart> parts = prios.get(dbId, tblId);
+            if (parts == null) {
+                parts = Sets.newHashSet();
+                prios.put(dbId, tblId, parts);
+            }
+
+            for (long partId : partitionIds) {
+                PrioPart prioPart = new PrioPart(partId, currentTime, timeoutMs);
+                parts.add(prioPart);
+            }
+        }
+
+        // we also need to change the priority of tablets which are already in
+        tabletScheduler.changePriorityOfTablets(dbId, tblId, partitionIds);
+    }
+
+    private void removePrios(long dbId, long tblId, List<Long> partitionIds) {
+        Preconditions.checkArgument(!partitionIds.isEmpty());
+        synchronized (prios) {
+            Map<Long, Set<PrioPart>> tblMap = prios.row(dbId);
+            if (tblMap == null) {
+                return;
+            }
+            Set<PrioPart> parts = tblMap.get(tblId);
+            if (parts == null) {
+                return;
+            }
+            for (long partId : partitionIds) {
+                parts.remove(new PrioPart(partId, -1, -1));
+            }
+            if (parts.isEmpty()) {
+                tblMap.remove(tblId);
+            }
+        }
+
+    }
+
+    /*
+     * For each cycle, TabletChecker will check all OlapTable's tablet.
+     * If a tablet is not healthy, a TabletInfo will be created and sent to TabletScheduler for repairing.
+     */
+    @Override
+    protected void runOneCycle() {
+        if (tabletScheduler.getPendingNum() > MAX_SCHEDULING_TABLETS
+                || tabletScheduler.getRunningNum() > MAX_SCHEDULING_TABLETS) {
+            LOG.info(""too many tablets are being scheduled. pending: {}, running: {}, limit: {}. skip check"",
+                    tabletScheduler.getPendingNum(), tabletScheduler.getRunningNum(), MAX_SCHEDULING_TABLETS);
+            return;
+        }
+        
+        checkTablets();
+
+        removePriosIfNecessary();
+
+        stat.counterTabletCheckRound.incrementAndGet();
+        LOG.info(stat.incrementalBrief());
+    }
+
+    private void checkTablets() {
+        long start = System.currentTimeMillis();
+        long totalTabletNum = 0;
+        long unhealthyTabletNum = 0;
+        long addToSchedulerTabletNum = 0;
+
+        List<Long> dbIds = catalog.getDbIds();
+        for (Long dbId : dbIds) {
+            Database db = catalog.getDb(dbId);
+            if (db == null) {
+                continue;
+            }
+
+            if (db.isInfoSchemaDb()) {
+                continue;
+            }
+
+            db.readLock();
+            try {
+                for (Table table : db.getTables()) {
+                    if (!table.needSchedule()) {
+                        continue;
+                    }
+
+                    OlapTable olapTbl = (OlapTable) table;
+                    for (Partition partition : olapTbl.getPartitions()) {
+                        boolean isInPrios = isInPrios(dbId, table.getId(), partition.getId());
+                        boolean prioPartIsHealthy = true;
+                        for (MaterializedIndex idx : partition.getMaterializedIndices()) {
+                            for (Tablet tablet : idx.getTablets()) {
+                                totalTabletNum++;
+                                
+                                if (tabletScheduler.containsTablet(tablet.getId())) {","[{'comment': 'you can set a flag in tablet instead of searching tabletScheduler', 'commenter': 'imay'}]"
336,fe/src/main/java/org/apache/doris/clone/TabletScheduler.java,"@@ -0,0 +1,1186 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.clone;
+
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.catalog.Database;
+import org.apache.doris.catalog.MaterializedIndex;
+import org.apache.doris.catalog.OlapTable;
+import org.apache.doris.catalog.OlapTable.OlapTableState;
+import org.apache.doris.catalog.Partition;
+import org.apache.doris.catalog.Replica;
+import org.apache.doris.catalog.Replica.ReplicaState;
+import org.apache.doris.catalog.Tablet;
+import org.apache.doris.catalog.Tablet.TabletStatus;
+import org.apache.doris.catalog.TabletInvertedIndex;
+import org.apache.doris.clone.SchedException.Status;
+import org.apache.doris.clone.TabletInfo.Priority;
+import org.apache.doris.clone.TabletInfo.Type;
+import org.apache.doris.common.Config;
+import org.apache.doris.common.Pair;
+import org.apache.doris.common.util.Daemon;
+import org.apache.doris.persist.ReplicaPersistInfo;
+import org.apache.doris.system.Backend;
+import org.apache.doris.system.SystemInfoService;
+import org.apache.doris.task.AgentBatchTask;
+import org.apache.doris.task.AgentTask;
+import org.apache.doris.task.AgentTaskExecutor;
+import org.apache.doris.task.AgentTaskQueue;
+import org.apache.doris.task.CloneTask;
+import org.apache.doris.thrift.TFinishTaskRequest;
+
+import com.google.common.base.Preconditions;
+import com.google.common.collect.EvictingQueue;
+import com.google.common.collect.ImmutableMap;
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+import com.google.common.collect.Sets;
+
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.Collection;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.PriorityQueue;
+import java.util.Queue;
+import java.util.Set;
+import java.util.stream.Collectors;
+
+/*
+ * TabletScheduler saved the tablets produced by TabletChecker and try to schedule them.
+ * It also try to balance the cluster load.
+ * 
+ * We are expecting an efficient way to recovery the entire cluster and make it balanced.
+ * Case 1:
+ *  A Backend is down. All tablets which has replica on this BE should be repaired as soon as possible.
+ *  
+ * Case 1.1:
+ *  As Backend is down, some tables should be repaired in high priority. So the clone task should be able
+ *  to preempted.
+ *  
+ * Case 2:
+ *  A new Backend is added to the cluster. Replicas should be transfer to that host to balance the cluster load.
+ */
+public class TabletScheduler extends Daemon {
+    private static final Logger LOG = LogManager.getLogger(TabletScheduler.class);
+
+    // handle at most BATCH_NUM tablets in one loop
+    private static final int MIN_BATCH_NUM = 10;
+
+    // the minimum interval of updating cluster statistics and priority of tablet info
+    private static final long STAT_UPDATE_INTERVAL_MS = 60 * 1000; // 1min
+
+    private static final long SCHEDULE_INTERVAL_MS = 5000; // 5s
+
+    public static final int BALANCE_SLOT_NUM_FOR_PATH = 2;
+
+    /*
+     * Tablet is added to pendingTablets as well it's id in allTabletIds.
+     * TabletScheduler will take tablet from pendingTablets but will not remove it's id from allTabletIds when
+     * handling a tablet.
+     * Tablet' id can only be removed after the clone task is done(timeout, cancelled or finished).
+     * So if a tablet's id is still in allTabletIds, TabletChecker can not add tablet to TabletScheduler.
+     * 
+     * pendingTablets + runningTablets = allTabletIds
+     * 
+     * pendingTablets, allTabletIds, runningTablets and schedHistory are protected by 'synchronized' 
+     */
+    private PriorityQueue<TabletInfo> pendingTablets = new PriorityQueue<>();
+    private Set<Long> allTabletIds = Sets.newHashSet();
+    // contains all tabletInfos which state are RUNNING
+    private Map<Long, TabletInfo> runningTablets = Maps.newHashMap();
+    // save the latest 1000 scheduled tablet info
+    private Queue<TabletInfo> schedHistory = EvictingQueue.create(1000);
+
+    // be id -> #working slots
+    private Map<Long, PathSlot> backendsWorkingSlots = Maps.newConcurrentMap();
+    // cluster name -> load statistic
+    private Map<String, ClusterLoadStatistic> statisticMap = Maps.newConcurrentMap();
+    private long lastStatUpdateTime = 0;
+    
+    private long lastSlotAdjustTime = 0;
+
+    private Catalog catalog;
+    private SystemInfoService infoService;
+    private TabletInvertedIndex invertedIndex;
+    private TabletSchedulerStat stat;
+
+    public TabletScheduler(Catalog catalog, SystemInfoService infoService, TabletInvertedIndex invertedIndex,
+            TabletSchedulerStat stat) {
+        super(""tablet scheduler"", SCHEDULE_INTERVAL_MS);
+        this.catalog = catalog;
+        this.infoService = infoService;
+        this.invertedIndex = invertedIndex;
+        this.stat = stat;
+    }
+
+    public TabletSchedulerStat getStat() {
+        return stat;
+    }
+
+    /*
+     * update working slots at the beginning of each round
+     */
+    private boolean updateWorkingSlots() {
+        ImmutableMap<Long, Backend> backends = infoService.getBackendsInCluster(null);
+        for (Backend backend : backends.values()) {
+            if (!backend.hasPathHash() && backend.isAlive()) {
+                // when upgrading, backend may not get path info yet. so return false and wait for next round.
+                // and we should check if backend is alive. If backend is dead when upgrading, this backend
+                // will never report its path hash, and tablet scheduler is blocked.
+                LOG.info(""not all backends have path info"");
+                return false;
+            }
+        }
+
+        // update exist backends
+        Set<Long> deletedBeIds = Sets.newHashSet();
+        for (Long beId : backendsWorkingSlots.keySet()) {
+            if (backends.containsKey(beId)) {
+                List<Long> pathHashes = backends.get(beId).getDisks().values().stream().map(v -> v.getPathHash()).collect(Collectors.toList());
+                backendsWorkingSlots.get(beId).updatePaths(pathHashes);
+            } else {
+                deletedBeIds.add(beId);
+            }
+        }
+
+        // delete non-exist backends
+        for (Long beId : deletedBeIds) {
+            backendsWorkingSlots.remove(beId);
+            LOG.info(""delete non exist backend: {}"", beId);
+        }
+
+        // add new backends
+        for (Backend be : backends.values()) {
+            if (!backendsWorkingSlots.containsKey(be.getId())) {
+                List<Long> pathHashes = be.getDisks().values().stream().map(v -> v.getPathHash()).collect(Collectors.toList());
+                PathSlot slot = new PathSlot(pathHashes, Config.schedule_slot_num_per_path);
+                backendsWorkingSlots.put(be.getId(), slot);
+                LOG.info(""add new backend {} with slots num: {}"", be.getId(), be.getDisks().size());
+            }
+        }
+
+        return true;
+    }
+
+    public Map<Long, PathSlot> getBackendsWorkingSlots() {
+        return backendsWorkingSlots;
+    }
+
+    /*
+     * add a ready-to-be-scheduled tablet to pendingTablets, if it has not being added before.
+     * if force is true, do not check if tablet is already added before.
+     */
+    public synchronized boolean addTablet(TabletInfo tablet, boolean force) {
+        if (!force && containsTablet(tablet.getTabletId())) {
+            return false;
+        }
+        allTabletIds.add(tablet.getTabletId());
+        pendingTablets.offer(tablet);
+        return true;
+    }
+
+    public synchronized boolean containsTablet(long tabletId) {
+        return allTabletIds.contains(tabletId);
+    }
+
+    /*
+     * Iterate current tablets, change their priority if necessary.
+     */
+    public synchronized void changePriorityOfTablets(long dbId, long tblId, List<Long> partitionIds) {
+        PriorityQueue<TabletInfo> newPendingTablets = new PriorityQueue<>();
+        for (TabletInfo tabletInfo : pendingTablets) {
+            if (tabletInfo.getDbId() == dbId && tabletInfo.getTblId() == tblId
+                    && partitionIds.contains(tabletInfo.getPartitionId())) {
+                tabletInfo.setOrigPriority(Priority.VERY_HIGH);
+            }
+            newPendingTablets.add(tabletInfo);
+        }
+        pendingTablets = newPendingTablets;
+    }
+
+    /*
+     * TabletScheduler will run as a daemon thread at a very short interval(default 5 sec)
+     * Firstly, it will try to update cluster load statistic and check if priority need to be adjuested.
+     * Than, it will schedule the tablets in pendingTablets.
+     * Thirdly, it will check the current running tasks.
+     * Finally, it try to balance the cluster if possible.
+     * 
+     * Schedule rules:
+     * 1. tablet with higher priority will be scheduled first.
+     * 2. high priority should be downgraded if it fails to be schedule too many times.
+     * 3. priority may be upgraded if it is not being schedule for a long time.
+     * 4. every pending task should has a max scheduled time, if schedule fails too many times, if should be removed.
+     * 5. every running task should has a timeout, to avoid running forever.
+     * 6. every running task should also has a max failure time, if clone task fails too many times, if should be removed.
+     *
+     */
+    @Override
+    protected void runOneCycle() {
+        if (!updateWorkingSlots()) {
+            return;
+        }
+
+        updateClusterLoadStatisticsAndPriorityIfNecessary();
+
+        schedulePendingTablets();
+
+        handleRunningTablets();
+
+        selectTabletsForBalance();
+
+        stat.counterTabletScheduleRound.incrementAndGet();
+    }
+
+
+    private void updateClusterLoadStatisticsAndPriorityIfNecessary() {
+        if (System.currentTimeMillis() - lastStatUpdateTime < STAT_UPDATE_INTERVAL_MS) {
+            return;
+        }
+
+        updateClusterLoadStatistic();
+        adjustPriorities();
+
+        lastStatUpdateTime = System.currentTimeMillis();
+    }
+
+    /*
+     * Here is the only place we update the cluster load statistic info.
+     * We will not update this info dynamically along with the clone job's running.
+     * Although it will cause a little bit inaccurate, but is within a controllable range,
+     * because we already limit the total number of running clone jobs in cluster by 'backend slots'
+     */
+    private void updateClusterLoadStatistic() {
+        statisticMap.clear();
+        List<String> clusterNames = infoService.getClusterNames();
+        for (String clusterName : clusterNames) {
+            ClusterLoadStatistic clusterLoadStatistic = new ClusterLoadStatistic(clusterName, catalog,
+                    infoService, invertedIndex);
+            clusterLoadStatistic.init();
+            statisticMap.put(clusterName, clusterLoadStatistic);
+            LOG.info(""update cluster {} load statistic:\n {}"", clusterName, clusterLoadStatistic.getBrief());
+        }
+    }
+
+    public Map<String, ClusterLoadStatistic> getStatisticMap() {
+        return statisticMap;
+    }
+
+    /*
+     * adjust priorities of all tablet infos
+     */
+    private synchronized void adjustPriorities() {
+        int size = pendingTablets.size();
+        int changedNum = 0;
+        TabletInfo tabletInfo = null;
+        for (int i = 0; i < size; i++) {
+            tabletInfo = pendingTablets.poll();
+            if (tabletInfo == null) {
+                break;
+            }
+
+            if (tabletInfo.adjustPriority(stat)) {
+                changedNum++;
+            }
+            pendingTablets.add(tabletInfo);
+        }
+
+        LOG.info(""adjust priority for all tablets. changed: {}, total: {}"", changedNum, size);
+    }
+
+    /*
+     * get at most BATCH_NUM tablets from queue, and try to schedule them.
+     * After handle, the tablet info should be
+     * 1. in runningTablets with state RUNNING, if being scheduled success.
+     * 2. or in schedHistory with state CANCELLING, if some unrecoverable error happens.
+     * 3. or in pendingTablets with state PENDING, if failed to be scheduled.
+     * 
+     * if in schedHistory, it should be removed from allTabletIds.
+     */
+    private void schedulePendingTablets() {
+        long start = System.currentTimeMillis();
+        List<TabletInfo> currentBatch = getNextTabletInfoBatch();
+        LOG.debug(""get {} tablets to schedule"", currentBatch.size());
+
+        AgentBatchTask batchTask = new AgentBatchTask();
+        for (TabletInfo tabletInfo : currentBatch) {
+            try {
+                scheduleTablet(tabletInfo, batchTask);
+            } catch (SchedException e) {
+                tabletInfo.increaseFailedSchedCounter();
+                tabletInfo.setErrMsg(e.getMessage());
+
+                if (e.getStatus() == Status.SCHEDULE_FAILED) {
+                    // we must release resource it current hold, and be scheduled again
+                    tabletInfo.releaseResource(this);
+                    // adjust priority to avoid some higher priority always be the first in pendingTablets
+                    stat.counterTabletScheduledFailed.incrementAndGet();
+                    dynamicAdjustPrioAndAddBackToPendingTablets(tabletInfo, e.getMessage());
+                } else if (e.getStatus() == Status.FINISHED) {
+                    // schedule redundant tablet will throw this exception
+                    stat.counterTabletScheduledSucceeded.incrementAndGet();
+                    removeTabletInfo(tabletInfo, TabletInfo.State.FINISHED, e.getMessage());
+                } else {
+                    Preconditions.checkState(e.getStatus() == Status.UNRECOVERABLE, e.getStatus());
+                    // discard
+                    stat.counterTabletScheduledDiscard.incrementAndGet();
+                    removeTabletInfo(tabletInfo, TabletInfo.State.CANCELLED, e.getMessage());
+                }
+                continue;
+            }
+
+            Preconditions.checkState(tabletInfo.getState() == TabletInfo.State.RUNNING);
+            stat.counterTabletScheduledSucceeded.incrementAndGet();
+            addToRunningTablets(tabletInfo);
+        }
+
+        // must send task after adding tablet info to runningTablets.
+        for (AgentTask task : batchTask.getAllTasks()) {
+            if (AgentTaskQueue.addTask(task)) {
+                stat.counterCloneTask.incrementAndGet();
+            }
+            LOG.info(""add clone task to agent task queue: {}"", task);
+        }
+
+        // send task immediately
+        AgentTaskExecutor.submit(batchTask);
+
+        long cost = System.currentTimeMillis() - start;
+        stat.counterTabletScheduleCostMs.addAndGet(cost);
+    }
+
+    private synchronized void addToRunningTablets(TabletInfo tabletInfo) {
+        runningTablets.put(tabletInfo.getTabletId(), tabletInfo);
+    }
+
+    /*
+     * we take the tablet out of the runningTablets and than handle it,
+     * avoid other threads see it.
+     * Whoever takes this tablet, make sure to put it to the schedHistory or back to runningTablets.
+     */
+    private synchronized TabletInfo takeRunningTablets(long tabletId) {
+        return runningTablets.remove(tabletId);
+    }
+
+    /*
+     * Try to schedule a single tablet.
+     */
+    private void scheduleTablet(TabletInfo tabletInfo, AgentBatchTask batchTask) throws SchedException {
+        LOG.debug(""schedule tablet: {}"", tabletInfo.getTabletId());
+        long currentTime = System.currentTimeMillis();
+        tabletInfo.setLastSchedTime(currentTime);
+        tabletInfo.setLastVisitedTime(currentTime);
+        stat.counterTabletScheduled.incrementAndGet();
+
+        // check this tablet again
+        Database db = catalog.getDb(tabletInfo.getDbId());
+        if (db == null) {
+            throw new SchedException(Status.UNRECOVERABLE, ""db does not exist"");
+        }
+
+        Pair<TabletStatus, TabletInfo.Priority> statusPair = null;
+        db.writeLock();
+        try {
+            OlapTable tbl = (OlapTable) db.getTable(tabletInfo.getTblId());
+            if (tbl == null) {
+                throw new SchedException(Status.UNRECOVERABLE, ""tbl does not exist"");
+            }
+
+            // we may add a tablet of a NOT NORMAL table during balance, which should be blocked
+            if (tbl.getState() != OlapTableState.NORMAL) {
+                throw new SchedException(Status.UNRECOVERABLE, ""tbl's state is not normal: "" + tbl.getState());
+            }
+
+            Partition partition = tbl.getPartition(tabletInfo.getPartitionId());
+            if (partition == null) {
+                throw new SchedException(Status.UNRECOVERABLE, ""partition does not exist"");
+            }
+
+            MaterializedIndex idx = partition.getIndex(tabletInfo.getIndexId());
+            if (idx == null) {
+                throw new SchedException(Status.UNRECOVERABLE, ""index does not exist"");
+            }
+
+            Tablet tablet = idx.getTablet(tabletInfo.getTabletId());
+            Preconditions.checkNotNull(tablet);
+
+            statusPair = tablet.getHealthStatusWithPriority(
+                    infoService, tabletInfo.getCluster(),
+                    partition.getVisibleVersion(),
+                    partition.getVisibleVersionHash(),
+                    tbl.getPartitionInfo().getReplicationNum(partition.getId()));
+
+            tabletInfo.setTabletStatus(statusPair.first);
+            if (statusPair.first == TabletStatus.HEALTHY && tabletInfo.getType() == TabletInfo.Type.REPAIR) {
+                throw new SchedException(Status.UNRECOVERABLE, ""tablet is healthy"");
+            } else if (statusPair.first != TabletStatus.HEALTHY
+                    && tabletInfo.getType() == TabletInfo.Type.BALANCE) {
+                tabletInfo.releaseResource(this);
+                // we select an unhealthy tablet to do balance, which is not right.
+                // so here we change it to a REPAIR task, and also reset its priority
+                tabletInfo.setType(TabletInfo.Type.REPAIR);
+                tabletInfo.setOrigPriority(statusPair.second);
+            }
+
+            // we do not concern priority here.
+            // once we take the tablet out of priority queue, priority is meaningless.
+            tabletInfo.setTablet(tablet);
+            tabletInfo.setVersionInfo(partition.getVisibleVersion(), partition.getVisibleVersionHash(),
+                    partition.getCommittedVersion(), partition.getCommittedVersionHash());
+            tabletInfo.setSchemaHash(tbl.getSchemaHashByIndexId(idx.getId()));
+            tabletInfo.setStorageMedium(tbl.getPartitionInfo().getDataProperty(partition.getId()).getStorageMedium());
+
+            handleTabletByTypeAndStatus(statusPair.first, tabletInfo, batchTask);
+        } finally {
+            db.writeUnlock();
+        }
+    }
+
+    private void handleTabletByTypeAndStatus(TabletStatus status, TabletInfo tabletInfo, AgentBatchTask batchTask)
+            throws SchedException {
+        if (tabletInfo.getType() == Type.REPAIR) {
+            switch (status) {
+                case REPLICA_MISSING:
+                    handleReplicaMissing(tabletInfo, batchTask);
+                    break;
+                case VERSION_INCOMPLETE:
+                    handleReplicaVersionIncomplete(tabletInfo, batchTask);
+                    break;
+                case REDUNDANT:
+                    handleRedundantReplica(tabletInfo);
+                    break;
+                case REPLICA_MISSING_IN_CLUSTER:
+                    handleReplicaClusterMigration(tabletInfo, batchTask);
+                    break;
+                default:
+                    break;
+            }
+        } else {
+            // balance
+            doBalance(tabletInfo, batchTask);
+        }
+    }
+
+    /*
+     * Replica is missing, which means there is no enough alive replicas.
+     * So we need to find a destination backend to clone a new replica as possible as we can.
+     * 1. find an available path in a backend as destination:
+     *      1. backend need to be alive.
+     *      2. backend of existing replicas should be excluded.
+     *      3. backend has available slot for clone.
+     *      4. replica can fit in the path (consider the threshold of disk capacity and usage percent).
+     *      5. try to find a path with lowest load score.
+     * 2. find an appropriate source replica:
+     *      1. source replica should be healthy
+     *      2. backend of source replica has available slot for clone.
+     *      
+     * 3. send clone task to destination backend
+     */
+    private void handleReplicaMissing(TabletInfo tabletInfo, AgentBatchTask batchTask) throws SchedException {
+        stat.counterReplicaMissingErr.incrementAndGet();
+        // find an available dest backend and path
+        RootPathLoadStatistic destPath = chooseAvailableDestPath(tabletInfo);
+        Preconditions.checkNotNull(destPath);
+        tabletInfo.setDestination(destPath.getBeId(), destPath.getPathHash());
+
+        // choose a source replica for cloning from
+        tabletInfo.chooseSrcReplica(backendsWorkingSlots);
+
+        // create clone task
+        batchTask.addTask(tabletInfo.createCloneReplicaAndTask());
+    }
+
+    /*
+     * Replica version is incomplete, which means this replica is missing some version,
+     * and need to be cloned from a healthy replica, in-place.
+     * 
+     * 1. find the incomplete replica as destination replica
+     * 2. find a healthy replica as source replica
+     * 3. send clone task
+     */
+    private void handleReplicaVersionIncomplete(TabletInfo tabletInfo, AgentBatchTask batchTask)
+            throws SchedException {
+        stat.counterReplicaVersionMissingErr.incrementAndGet();
+        ClusterLoadStatistic statistic = statisticMap.get(tabletInfo.getCluster());
+        if (statistic == null) {
+            throw new SchedException(Status.UNRECOVERABLE, ""cluster does not exist"");
+        }
+
+        tabletInfo.chooseDestReplicaForVersionIncomplete(backendsWorkingSlots);
+        tabletInfo.chooseSrcReplicaForVersionIncomplete(backendsWorkingSlots);
+
+        // create clone task
+        batchTask.addTask(tabletInfo.createCloneReplicaAndTask());
+    }
+
+    /*
+     *  replica is redundant, which means there are more replicas than we expected, which need to be dropped.
+     *  we just drop one redundant replica at a time, for safety reason.
+     *  choosing a replica to drop base on following priority:
+     *  1. backend has been dropped
+     *  2. backend is not available
+     *  3. replica's state is CLONE
+     *  4. replica's last failed version > 0
+     *  5. replica with lower version
+     *  6. replica not in right cluster
+     *  7. replica in higher load backend
+     */
+    private void handleRedundantReplica(TabletInfo tabletInfo) throws SchedException {
+        stat.counterReplicaRedundantErr.incrementAndGet();
+        if (deleteBackendDropped(tabletInfo)
+                || deleteBackendUnavailable(tabletInfo)
+                || deleteCloneReplica(tabletInfo)
+                || deleteReplicaWithFailedVersion(tabletInfo)
+                || deleteReplicaWithLowerVersion(tabletInfo)
+                || deleteReplicaNotInCluster(tabletInfo)
+                || deleteReplicaOnHighLoadBackend(tabletInfo)) {
+            // if we delete at least one redundant replica, we still throw a SchedException with status FINISHED
+            // to remove this tablet from the pendingTablets(consider it as finished)
+            throw new SchedException(Status.FINISHED, ""redundant replica is deleted"");
+        }
+        throw new SchedException(Status.SCHEDULE_FAILED, ""unable to delete any redundant replicas"");
+    }
+
+    private boolean deleteBackendDropped(TabletInfo tabletInfo) {
+        for (Replica replica : tabletInfo.getReplicas()) {
+            long beId = replica.getBackendId();
+            if (infoService.getBackend(beId) == null) {
+                deleteReplicaInternal(tabletInfo, replica, ""backend dropped"");
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private boolean deleteBackendUnavailable(TabletInfo tabletInfo) {
+        for (Replica replica : tabletInfo.getReplicas()) {
+            Backend be = infoService.getBackend(replica.getBackendId());
+            if (be == null) {
+                // this case should be handled in deleteBackendDropped()
+                continue;
+            }
+            if (!be.isAvailable()) {
+                deleteReplicaInternal(tabletInfo, replica, ""backend unavailable"");
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private boolean deleteCloneReplica(TabletInfo tabletInfo) {
+        for (Replica replica : tabletInfo.getReplicas()) {
+            if (replica.getState() == ReplicaState.CLONE) {
+                deleteReplicaInternal(tabletInfo, replica, ""clone state"");
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private boolean deleteReplicaWithFailedVersion(TabletInfo tabletInfo) {
+        for (Replica replica : tabletInfo.getReplicas()) {
+            if (replica.getLastFailedVersion() > 0) {
+                deleteReplicaInternal(tabletInfo, replica, ""version incomplete"");
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private boolean deleteReplicaWithLowerVersion(TabletInfo tabletInfo) {
+        for (Replica replica : tabletInfo.getReplicas()) {
+            if (!replica.checkVersionCatchUp(tabletInfo.getCommittedVersion(), tabletInfo.getCommittedVersionHash())) {
+                deleteReplicaInternal(tabletInfo, replica, ""lower version"");
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private boolean deleteReplicaNotInCluster(TabletInfo tabletInfo) {
+        for (Replica replica : tabletInfo.getReplicas()) {
+            Backend be = infoService.getBackend(replica.getBackendId());
+            if (be == null) {
+                // this case should be handled in deleteBackendDropped()
+                continue;
+            }
+            if (!be.getOwnerClusterName().equals(tabletInfo.getCluster())) {
+                deleteReplicaInternal(tabletInfo, replica, ""not in cluster"");
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private boolean deleteReplicaOnHighLoadBackend(TabletInfo tabletInfo) {
+        ClusterLoadStatistic statistic = statisticMap.get(tabletInfo.getCluster());
+        if (statistic == null) {
+            return false;
+        }
+        
+        Replica chosenReplica = null;
+        double maxScore = 0;
+        for (Replica replica : tabletInfo.getReplicas()) {
+            BackendLoadStatistic beStatistic = statistic.getBackendLoadStatistic(replica.getBackendId());
+            if (beStatistic == null) {
+                continue;
+            }
+            if (beStatistic.getLoadScore() > maxScore) {
+                maxScore = beStatistic.getLoadScore();
+                chosenReplica = replica;
+            }
+        }
+
+        if (chosenReplica != null) {
+            deleteReplicaInternal(tabletInfo, chosenReplica, ""high load"");
+            return true;
+        }
+        return false;
+    }
+
+    private void deleteReplicaInternal(TabletInfo tabletInfo, Replica replica, String reason) {
+        // delete this replica from catalog.
+        // it will also delete replica from tablet inverted index.
+        tabletInfo.deleteReplica(replica);
+
+        // TODO(cmy): this should be removed after I finish modifying alter job logic
+        // Catalog.getInstance().handleJobsWhenDeleteReplica(tabletInfo.getTblId(), tabletInfo.getPartitionId(),
+        //                                                   tabletInfo.getIndexId(), tabletInfo.getTabletId(),
+        //                                                   replica.getId(), replica.getBackendId());
+
+        // write edit log
+        ReplicaPersistInfo info = ReplicaPersistInfo.createForDelete(tabletInfo.getDbId(),
+                                                                     tabletInfo.getTblId(),
+                                                                     tabletInfo.getPartitionId(),
+                                                                     tabletInfo.getIndexId(),
+                                                                     tabletInfo.getTabletId(),
+                                                                     replica.getBackendId());
+
+        Catalog.getInstance().getEditLog().logDeleteReplica(info);
+
+        LOG.info(""delete replica. tablet id: {}, backend id: {}. reason: {}"",
+                 tabletInfo.getTabletId(), replica.getBackendId(), reason);
+    }
+
+    /*
+     * Cluster migration, which means the tablet has enough healthy replicas,
+     * but some replicas are not in right cluster.
+     * It is just same as 'replica missing'.
+     * 
+     * after clone finished, the replica in wrong cluster will be treated as redundant, and will be deleted soon.
+     */
+    private void handleReplicaClusterMigration(TabletInfo tabletInfo, AgentBatchTask batchTask)
+            throws SchedException {
+        stat.counterReplicaMissingInClusterErr.incrementAndGet();
+        handleReplicaMissing(tabletInfo, batchTask);
+    }
+
+    /*
+     * Try to select some alternative tablets for balance. Add them to pendingTablets with priority LOW,
+     * and waiting to be scheduled.
+     */
+    private void selectTabletsForBalance() {
+        LoadBalancer loadBalancer = new LoadBalancer(statisticMap);
+        List<TabletInfo> alternativeTablets = loadBalancer.selectAlternativeTablets();
+        for (TabletInfo tabletInfo : alternativeTablets) {
+            addTablet(tabletInfo, false);
+        }
+    }
+
+    /*
+     * Try to create a balance task for a tablet.
+     */
+    private void doBalance(TabletInfo tabletInfo, AgentBatchTask batchTask) throws SchedException {
+        stat.counterBalanceSchedule.incrementAndGet();
+        LoadBalancer loadBalancer = new LoadBalancer(statisticMap);
+        loadBalancer.createBalanceTask(tabletInfo, backendsWorkingSlots, batchTask);
+    }
+
+    // choose a path on a backend which is fit for the tablet
+    private RootPathLoadStatistic chooseAvailableDestPath(TabletInfo tabletInfo) throws SchedException {
+        ClusterLoadStatistic statistic = statisticMap.get(tabletInfo.getCluster());
+        if (statistic == null) {
+            throw new SchedException(Status.UNRECOVERABLE, ""cluster does not exist"");
+        }
+        List<BackendLoadStatistic> beStatistics = statistic.getBeLoadStatistics();
+
+        // get all available paths which this tablet can fit in.
+        // beStatistics is sorted by load score in ascend order, so select from first to last.
+        List<RootPathLoadStatistic> allFitPaths = Lists.newArrayList();
+        for (int i = 0; i < beStatistics.size(); i++) {
+            BackendLoadStatistic bes = beStatistics.get(i);
+            // exclude BE which already has replica of this tablet
+            if (tabletInfo.containsBE(bes.getBeId())) {
+                continue;
+            }
+
+            List<RootPathLoadStatistic> resultPaths = Lists.newArrayList();
+            BalanceStatus st = bes.isFit(tabletInfo.getTabletSize(), resultPaths, true /* is supplement */);
+            if (!st.ok()) {
+                LOG.debug(""unable to find path for supplementing tablet: {}. {}"", tabletInfo, st);
+                continue;
+            }
+
+            Preconditions.checkState(resultPaths.size() == 1);
+            allFitPaths.add(resultPaths.get(0));
+        }
+
+        if (allFitPaths.isEmpty()) {
+            throw new SchedException(Status.SCHEDULE_FAILED, ""unable to find dest path for new replica"");
+        }
+
+        // all fit paths has already been sorted by load score in 'allFitPaths' in ascend order.
+        // just get first available path.
+        // we try to find a path with specified media type, if not find, arbitrarily use one.
+        for (RootPathLoadStatistic rootPathLoadStatistic : allFitPaths) {
+            if (rootPathLoadStatistic.getStorageMedium() != tabletInfo.getStorageMedium()) {
+                continue;
+            }
+
+            PathSlot slot = backendsWorkingSlots.get(rootPathLoadStatistic.getBeId());
+            if (slot == null) {
+                LOG.debug(""backend {} does not found when getting slots"", rootPathLoadStatistic.getBeId());
+                continue;
+            }
+
+            if (slot.takeSlot(rootPathLoadStatistic.getPathHash()) != -1) {
+                return rootPathLoadStatistic;
+            }
+        }
+
+        // no root path with specified media type is found, get arbitrary one.
+        for (RootPathLoadStatistic rootPathLoadStatistic : allFitPaths) {
+            PathSlot slot = backendsWorkingSlots.get(rootPathLoadStatistic.getBeId());
+            if (slot == null) {
+                LOG.debug(""backend {} does not found when getting slots"", rootPathLoadStatistic.getBeId());
+                continue;
+            }
+
+            if (slot.takeSlot(rootPathLoadStatistic.getPathHash()) != -1) {
+                return rootPathLoadStatistic;
+            }
+        }
+        
+        throw new SchedException(Status.SCHEDULE_FAILED, ""unable to find dest path which can be fit in"");
+    }
+
+    /*
+     * For some reason, a tablet info failed to be scheduled this time,
+     * So we dynamically change its priority and add back to queue, waiting for next round.
+     */
+    private void dynamicAdjustPrioAndAddBackToPendingTablets(TabletInfo tabletInfo, String message) {
+        Preconditions.checkState(tabletInfo.getState() == TabletInfo.State.PENDING);
+        tabletInfo.adjustPriority(stat);
+        addTablet(tabletInfo, true /* force */);
+    }
+
+    private synchronized void removeTabletInfo(TabletInfo tabletInfo, TabletInfo.State state, String reason) {
+        tabletInfo.setState(state);
+        tabletInfo.releaseResource(this);
+        tabletInfo.setFinishedTime(System.currentTimeMillis());
+        runningTablets.remove(tabletInfo.getTabletId());
+        allTabletIds.remove(tabletInfo.getTabletId());
+        schedHistory.add(tabletInfo);
+        LOG.info(""remove the tablet {}. because: {}"", tabletInfo.getTabletId(), reason);
+    }
+
+    // get next batch of tablets from queue.
+    private synchronized List<TabletInfo> getNextTabletInfoBatch() {
+        List<TabletInfo> list = Lists.newArrayList();
+        int count = Math.max(MIN_BATCH_NUM, getCurrentAvailableSlotNum());
+        while (count > 0) {
+            TabletInfo tablet = pendingTablets.poll();
+            if (tablet == null) {
+                // no more tablets
+                break;
+            }
+            list.add(tablet);
+            count--;
+        }
+        return list;
+    }
+
+    private int getCurrentAvailableSlotNum() {
+        int total = 0;
+        for (PathSlot pathSlot : backendsWorkingSlots.values()) {
+            total += pathSlot.getTotalAvailSlotNum();
+        }
+        return total;
+    }
+
+    /*
+     * return true if we want to remove the clone task from AgentTaskQueu
+     */
+    public boolean finishCloneTask(CloneTask cloneTask, TFinishTaskRequest request) {
+        long tabletId = cloneTask.getTabletId();
+        TabletInfo tabletInfo = takeRunningTablets(tabletId);
+        if (tabletInfo == null) {
+            LOG.warn(""tablet info does not exist: {}"", tabletId);
+            // tablet does not exist, no need to keep task.
+            return true;
+        }
+        Preconditions.checkState(tabletInfo.getState() == TabletInfo.State.RUNNING);
+        try {
+            tabletInfo.finishCloneTask(cloneTask, request);
+        } catch (SchedException e) {
+            tabletInfo.increaseFailedRunningCounter();
+            tabletInfo.setErrMsg(e.getMessage());
+            if (e.getStatus() == Status.RUNNING_FAILED) {
+                stat.counterCloneTaskFailed.incrementAndGet();
+                addToRunningTablets(tabletInfo);
+                return false;
+            } else {
+                Preconditions.checkState(e.getStatus() == Status.UNRECOVERABLE, e.getStatus());
+                // unrecoverable
+                stat.counterTabletScheduledDiscard.incrementAndGet();
+                removeTabletInfo(tabletInfo, TabletInfo.State.CANCELLED, e.getMessage());
+                return true;
+            }
+        }
+
+        Preconditions.checkState(tabletInfo.getState() == TabletInfo.State.FINISHED);
+        stat.counterCloneTaskSucceeded.incrementAndGet();
+        gatherStatistics(tabletInfo);
+        removeTabletInfo(tabletInfo, TabletInfo.State.FINISHED, ""finished"");
+        return true;
+    }
+
+    /*
+     * Gather the running statistic of the task.
+     * It will be evaluated for future strategy.  
+     * This should only be called when the tablet is down with state FINISHED.
+     */
+    private void gatherStatistics(TabletInfo tabletInfo) {
+        if (tabletInfo.getCopySize() > 0 && tabletInfo.getCopyTimeMs() > 0) {
+            if (tabletInfo.getSrcBackendId() != -1 && tabletInfo.getSrcPathHash() != -1) {
+                PathSlot pathSlot = backendsWorkingSlots.get(tabletInfo.getSrcBackendId());
+                if (pathSlot != null) {
+                    pathSlot.updateStatistic(tabletInfo.getSrcPathHash(), tabletInfo.getCopySize(),
+                            tabletInfo.getCopyTimeMs());
+                }
+            }
+
+            if (tabletInfo.getDestBackendId() != -1 && tabletInfo.getDestPathHash() != -1) {
+                PathSlot pathSlot = backendsWorkingSlots.get(tabletInfo.getDestBackendId());
+                if (pathSlot != null) {
+                    pathSlot.updateStatistic(tabletInfo.getDestPathHash(), tabletInfo.getCopySize(),
+                            tabletInfo.getCopyTimeMs());
+                }
+            }
+        }
+
+        if (System.currentTimeMillis() - lastSlotAdjustTime < STAT_UPDATE_INTERVAL_MS) {
+            return;
+        }
+
+        // TODO(cmy): update the slot num base on statistic.
+        // need to find a better way to determine the slot number.
+
+        lastSlotAdjustTime = System.currentTimeMillis();
+    }
+
+    /*
+     * handle tablets which are running.
+     * We should finished the task if
+     * 1. Tablet is already healthy
+     * 2. Task is timeout.
+     * 
+     * But here we just handle the timeout case here. Let the 'finishCloneTask()' check if tablet is healthy.
+     * We guarantee that if tablet is in runningTablets, the 'finishCloneTask()' will finally be called,
+     * so no need to worry that running tablets will never end.
+     * This is also avoid nesting 'synchronized' and database lock.
+     *
+     * If task is timeout, remove the tablet.
+     */
+    public synchronized void handleRunningTablets() {
+        List<TabletInfo> timeoutTablets = Lists.newArrayList();
+        runningTablets.values().stream().filter(t -> t.isTimeout()).forEach(t -> {
+            timeoutTablets.add(t);
+        });
+        
+        timeoutTablets.stream().forEach(t -> {
+            removeTabletInfo(t, TabletInfo.State.TIMEOUT, ""timeout"");
+            stat.counterCloneTaskTimeout.incrementAndGet();
+        });
+    }
+
+    public List<List<String>> getPendingTabletsInfo(int limit) {
+        List<TabletInfo> tabletInfos = getCopiedTablets(pendingTablets, limit);
+        return collectTabletInfo(tabletInfos);
+    }
+
+    public List<List<String>> getRunningTabletsInfo(int limit) {
+        List<TabletInfo> tabletInfos = getCopiedTablets(runningTablets.values(), limit);
+        return collectTabletInfo(tabletInfos);
+    }
+
+    public List<List<String>> getHistoryTabletsInfo(int limit) {
+        List<TabletInfo> tabletInfos = getCopiedTablets(schedHistory, limit);
+        return collectTabletInfo(tabletInfos);
+    }
+
+    private List<List<String>> collectTabletInfo(List<TabletInfo> tabletInfos) {
+        List<List<String>> result = Lists.newArrayList();
+        tabletInfos.stream().forEach(t -> {
+            result.add(t.getBrief());
+        });
+        return result;
+    }
+
+    private synchronized List<TabletInfo> getCopiedTablets(Collection<TabletInfo> source, int limit) {
+        List<TabletInfo> tabletInfos = Lists.newArrayList();
+        source.stream().limit(limit).forEach(t -> {
+            tabletInfos.add(t);
+        });
+        return tabletInfos;
+    }
+
+    public synchronized int getPendingNum() {
+        return pendingTablets.size();
+    }
+
+    public synchronized int getRunningNum() {
+        return runningTablets.size();
+    }
+
+    public synchronized int getHistoryNum() {
+        return schedHistory.size();
+    }
+
+    /*
+     * PathSlot keeps track of slot num per path of a Backend.
+     * Each path on a Backend has several slot.
+     * If a path's available slot num because 0, no task should be assigned to this path.
+     */
+    public class PathSlot {
+        // path hash -> slot num
+        private Map<Long, Slot> pathSlots = Maps.newConcurrentMap();
+
+        public PathSlot(List<Long> paths, int initSlotNum) {
+            for (Long pathHash : paths) {
+                pathSlots.put(pathHash, new Slot(initSlotNum));
+            }
+        }
+
+        // update the path
+        public synchronized void updatePaths(List<Long> paths) {
+            // delete non exist path
+            Iterator<Map.Entry<Long, Slot>> iter = pathSlots.entrySet().iterator();
+            while (iter.hasNext()) {
+                Map.Entry<Long, Slot> entry = iter.next();
+                if (!paths.contains(entry.getKey())) {
+                    iter.remove();
+                }
+            }
+
+            // add new path
+            for (Long pathHash : paths) {
+                if (!pathSlots.containsKey(pathHash)) {
+                    pathSlots.put(pathHash, new Slot(Config.schedule_slot_num_per_path));
+                }
+            }
+        }
+
+        // Update the total slots num of specified paths, increase or decrease
+        public synchronized void updateSlot(List<Long> pathHashs, boolean increase) {
+            for (Long pathHash : pathHashs) {
+                if (pathSlots.containsKey(pathHash)) {
+                    if (increase) {
+                        pathSlots.get(pathHash).total++;
+                    } else {
+                        pathSlots.get(pathHash).total--;
+                    }
+                    pathSlots.get(pathHash).rectify();
+                    LOG.debug(""decrease path {} slots num to {}"", pathHash, pathSlots.get(pathHash).total);
+                }
+            }
+        }
+
+        /*
+         * Update the statistic of specified path
+         */
+        public synchronized void updateStatistic(long pathHash, long copySize, long copyTimeMs) {
+            if (pathSlots.get(pathHash) == null) {
+                return;
+            }
+            pathSlots.get(pathHash).totalCopySize += copySize;
+            pathSlots.get(pathHash).totalCopyTimeMs += copyTimeMs;
+        }
+
+        /*
+         * If the specified 'pathHash' has available slot, decrease the slot number and return this path hash
+         */
+        public synchronized long takeSlot(long pathHash) {
+            Preconditions.checkArgument(pathHash != -1);
+            Slot slot = pathSlots.get(pathHash);
+            if (slot == null) {
+                return -1;
+            }
+            slot.rectify();
+            if (slot.available <= 0) {
+                return -1;
+            }
+            slot.available--;
+            return pathHash;
+        }
+
+        public synchronized void freeSlot(long pathHash) {
+            Slot slot = pathSlots.get(pathHash);
+            if (slot == null) {
+                return;
+            }
+            slot.available++;
+            slot.rectify();
+        }
+
+        public synchronized int peekSlot(long pathHash) {
+            Slot slot = pathSlots.get(pathHash);
+            if (slot == null) {
+                return -1;
+            }
+            slot.rectify();
+            return slot.available;
+        }
+
+        public synchronized int getTotalAvailSlotNum() {
+            int total = 0;
+            for (Slot slot : pathSlots.values()) {
+                total += slot.available;
+            }
+            return total;
+        }
+
+        /*
+         * get path whose balance slot num is larger than 0
+         */
+        public synchronized Set<Long> getAvailPathsForBalance() {
+            Set<Long> pathHashs = Sets.newHashSet();
+            for (Map.Entry<Long, Slot> entry : pathSlots.entrySet()) {
+                if (entry.getValue().balanceSlot > 0) {
+                    pathHashs.add(entry.getKey());
+                }
+            }
+            return pathHashs;
+        }
+
+        public synchronized int getAvailBalanceSlotNum() {
+            int num = 0;
+            for (Map.Entry<Long, Slot> entry : pathSlots.entrySet()) {
+                num += entry.getValue().balanceSlot;
+            }
+            return num;
+        }
+
+        public synchronized List<String> getSlotInfo(long beId) {
+            List<String> result = Lists.newArrayList();
+            pathSlots.entrySet().stream().forEach(t -> {
+                t.getValue().rectify();
+                result.add(String.valueOf(beId));
+                result.add(String.valueOf(t.getKey()));
+                result.add(String.valueOf(t.getValue().available));
+                result.add(String.valueOf(t.getValue().total));
+                result.add(String.valueOf(t.getValue().balanceSlot));
+                result.add(String.valueOf(t.getValue().getAvgRate()));
+            });
+            return result;
+        }
+
+        public synchronized long takeBalanceSlot(long pathHash) {
+            Slot slot = pathSlots.get(pathHash);
+            if (slot == null) {
+                return -1;
+            }
+            if (slot.balanceSlot > 0) {
+                slot.balanceSlot--;
+                return pathHash;
+            }
+            return -1;
+        }
+
+        public synchronized long takeAnAvailBalanceSlotFrom(Set<Long> pathHashs) {
+            for (Long pathHash : pathHashs) {
+                Slot slot = pathSlots.get(pathHash);
+                if (slot == null) {
+                    continue;
+                }
+                if (slot.balanceSlot > 0) {
+                    slot.balanceSlot--;
+                    return pathHash;
+                }
+            }
+            return -1;
+        }
+
+        public void freeBalanceSlot(long destPathHash) {
+            Slot slot = pathSlots.get(destPathHash);
+            if (slot == null) {
+                return;
+            }
+            slot.balanceSlot++;
+            slot.rectify();
+        }
+    }
+
+    public List<List<String>> getSlotsInfo() {
+        List<List<String>> result = Lists.newArrayList();
+        for (long beId : backendsWorkingSlots.keySet()) {
+            PathSlot slot = backendsWorkingSlots.get(beId);
+            result.add(slot.getSlotInfo(beId));
+        }
+        return result;
+    }
+
+    public class Slot {","[{'comment': 'public static class ?', 'commenter': 'imay'}]"
336,fe/src/main/java/org/apache/doris/clone/TabletScheduler.java,"@@ -0,0 +1,1186 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.clone;
+
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.catalog.Database;
+import org.apache.doris.catalog.MaterializedIndex;
+import org.apache.doris.catalog.OlapTable;
+import org.apache.doris.catalog.OlapTable.OlapTableState;
+import org.apache.doris.catalog.Partition;
+import org.apache.doris.catalog.Replica;
+import org.apache.doris.catalog.Replica.ReplicaState;
+import org.apache.doris.catalog.Tablet;
+import org.apache.doris.catalog.Tablet.TabletStatus;
+import org.apache.doris.catalog.TabletInvertedIndex;
+import org.apache.doris.clone.SchedException.Status;
+import org.apache.doris.clone.TabletInfo.Priority;
+import org.apache.doris.clone.TabletInfo.Type;
+import org.apache.doris.common.Config;
+import org.apache.doris.common.Pair;
+import org.apache.doris.common.util.Daemon;
+import org.apache.doris.persist.ReplicaPersistInfo;
+import org.apache.doris.system.Backend;
+import org.apache.doris.system.SystemInfoService;
+import org.apache.doris.task.AgentBatchTask;
+import org.apache.doris.task.AgentTask;
+import org.apache.doris.task.AgentTaskExecutor;
+import org.apache.doris.task.AgentTaskQueue;
+import org.apache.doris.task.CloneTask;
+import org.apache.doris.thrift.TFinishTaskRequest;
+
+import com.google.common.base.Preconditions;
+import com.google.common.collect.EvictingQueue;
+import com.google.common.collect.ImmutableMap;
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+import com.google.common.collect.Sets;
+
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.Collection;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.PriorityQueue;
+import java.util.Queue;
+import java.util.Set;
+import java.util.stream.Collectors;
+
+/*
+ * TabletScheduler saved the tablets produced by TabletChecker and try to schedule them.
+ * It also try to balance the cluster load.
+ * 
+ * We are expecting an efficient way to recovery the entire cluster and make it balanced.
+ * Case 1:
+ *  A Backend is down. All tablets which has replica on this BE should be repaired as soon as possible.
+ *  
+ * Case 1.1:
+ *  As Backend is down, some tables should be repaired in high priority. So the clone task should be able
+ *  to preempted.
+ *  
+ * Case 2:
+ *  A new Backend is added to the cluster. Replicas should be transfer to that host to balance the cluster load.
+ */
+public class TabletScheduler extends Daemon {
+    private static final Logger LOG = LogManager.getLogger(TabletScheduler.class);
+
+    // handle at most BATCH_NUM tablets in one loop
+    private static final int MIN_BATCH_NUM = 10;
+
+    // the minimum interval of updating cluster statistics and priority of tablet info
+    private static final long STAT_UPDATE_INTERVAL_MS = 60 * 1000; // 1min
+
+    private static final long SCHEDULE_INTERVAL_MS = 5000; // 5s
+
+    public static final int BALANCE_SLOT_NUM_FOR_PATH = 2;
+
+    /*
+     * Tablet is added to pendingTablets as well it's id in allTabletIds.
+     * TabletScheduler will take tablet from pendingTablets but will not remove it's id from allTabletIds when
+     * handling a tablet.
+     * Tablet' id can only be removed after the clone task is done(timeout, cancelled or finished).
+     * So if a tablet's id is still in allTabletIds, TabletChecker can not add tablet to TabletScheduler.
+     * 
+     * pendingTablets + runningTablets = allTabletIds
+     * 
+     * pendingTablets, allTabletIds, runningTablets and schedHistory are protected by 'synchronized' 
+     */
+    private PriorityQueue<TabletInfo> pendingTablets = new PriorityQueue<>();
+    private Set<Long> allTabletIds = Sets.newHashSet();
+    // contains all tabletInfos which state are RUNNING
+    private Map<Long, TabletInfo> runningTablets = Maps.newHashMap();
+    // save the latest 1000 scheduled tablet info
+    private Queue<TabletInfo> schedHistory = EvictingQueue.create(1000);
+
+    // be id -> #working slots
+    private Map<Long, PathSlot> backendsWorkingSlots = Maps.newConcurrentMap();
+    // cluster name -> load statistic
+    private Map<String, ClusterLoadStatistic> statisticMap = Maps.newConcurrentMap();
+    private long lastStatUpdateTime = 0;
+    
+    private long lastSlotAdjustTime = 0;
+
+    private Catalog catalog;
+    private SystemInfoService infoService;
+    private TabletInvertedIndex invertedIndex;
+    private TabletSchedulerStat stat;
+
+    public TabletScheduler(Catalog catalog, SystemInfoService infoService, TabletInvertedIndex invertedIndex,
+            TabletSchedulerStat stat) {
+        super(""tablet scheduler"", SCHEDULE_INTERVAL_MS);
+        this.catalog = catalog;
+        this.infoService = infoService;
+        this.invertedIndex = invertedIndex;
+        this.stat = stat;
+    }
+
+    public TabletSchedulerStat getStat() {
+        return stat;
+    }
+
+    /*
+     * update working slots at the beginning of each round
+     */
+    private boolean updateWorkingSlots() {
+        ImmutableMap<Long, Backend> backends = infoService.getBackendsInCluster(null);
+        for (Backend backend : backends.values()) {
+            if (!backend.hasPathHash() && backend.isAlive()) {
+                // when upgrading, backend may not get path info yet. so return false and wait for next round.
+                // and we should check if backend is alive. If backend is dead when upgrading, this backend
+                // will never report its path hash, and tablet scheduler is blocked.
+                LOG.info(""not all backends have path info"");
+                return false;
+            }
+        }
+
+        // update exist backends
+        Set<Long> deletedBeIds = Sets.newHashSet();
+        for (Long beId : backendsWorkingSlots.keySet()) {
+            if (backends.containsKey(beId)) {
+                List<Long> pathHashes = backends.get(beId).getDisks().values().stream().map(v -> v.getPathHash()).collect(Collectors.toList());
+                backendsWorkingSlots.get(beId).updatePaths(pathHashes);
+            } else {
+                deletedBeIds.add(beId);
+            }
+        }
+
+        // delete non-exist backends
+        for (Long beId : deletedBeIds) {
+            backendsWorkingSlots.remove(beId);
+            LOG.info(""delete non exist backend: {}"", beId);
+        }
+
+        // add new backends
+        for (Backend be : backends.values()) {
+            if (!backendsWorkingSlots.containsKey(be.getId())) {
+                List<Long> pathHashes = be.getDisks().values().stream().map(v -> v.getPathHash()).collect(Collectors.toList());
+                PathSlot slot = new PathSlot(pathHashes, Config.schedule_slot_num_per_path);
+                backendsWorkingSlots.put(be.getId(), slot);
+                LOG.info(""add new backend {} with slots num: {}"", be.getId(), be.getDisks().size());
+            }
+        }
+
+        return true;
+    }
+
+    public Map<Long, PathSlot> getBackendsWorkingSlots() {
+        return backendsWorkingSlots;
+    }
+
+    /*
+     * add a ready-to-be-scheduled tablet to pendingTablets, if it has not being added before.
+     * if force is true, do not check if tablet is already added before.
+     */
+    public synchronized boolean addTablet(TabletInfo tablet, boolean force) {
+        if (!force && containsTablet(tablet.getTabletId())) {
+            return false;
+        }
+        allTabletIds.add(tablet.getTabletId());
+        pendingTablets.offer(tablet);
+        return true;
+    }
+
+    public synchronized boolean containsTablet(long tabletId) {
+        return allTabletIds.contains(tabletId);
+    }
+
+    /*
+     * Iterate current tablets, change their priority if necessary.
+     */
+    public synchronized void changePriorityOfTablets(long dbId, long tblId, List<Long> partitionIds) {
+        PriorityQueue<TabletInfo> newPendingTablets = new PriorityQueue<>();
+        for (TabletInfo tabletInfo : pendingTablets) {
+            if (tabletInfo.getDbId() == dbId && tabletInfo.getTblId() == tblId
+                    && partitionIds.contains(tabletInfo.getPartitionId())) {
+                tabletInfo.setOrigPriority(Priority.VERY_HIGH);
+            }
+            newPendingTablets.add(tabletInfo);
+        }
+        pendingTablets = newPendingTablets;
+    }
+
+    /*
+     * TabletScheduler will run as a daemon thread at a very short interval(default 5 sec)
+     * Firstly, it will try to update cluster load statistic and check if priority need to be adjuested.
+     * Than, it will schedule the tablets in pendingTablets.
+     * Thirdly, it will check the current running tasks.
+     * Finally, it try to balance the cluster if possible.
+     * 
+     * Schedule rules:
+     * 1. tablet with higher priority will be scheduled first.
+     * 2. high priority should be downgraded if it fails to be schedule too many times.
+     * 3. priority may be upgraded if it is not being schedule for a long time.
+     * 4. every pending task should has a max scheduled time, if schedule fails too many times, if should be removed.
+     * 5. every running task should has a timeout, to avoid running forever.
+     * 6. every running task should also has a max failure time, if clone task fails too many times, if should be removed.
+     *
+     */
+    @Override
+    protected void runOneCycle() {
+        if (!updateWorkingSlots()) {
+            return;
+        }
+
+        updateClusterLoadStatisticsAndPriorityIfNecessary();
+
+        schedulePendingTablets();
+
+        handleRunningTablets();
+
+        selectTabletsForBalance();
+
+        stat.counterTabletScheduleRound.incrementAndGet();
+    }
+
+
+    private void updateClusterLoadStatisticsAndPriorityIfNecessary() {
+        if (System.currentTimeMillis() - lastStatUpdateTime < STAT_UPDATE_INTERVAL_MS) {
+            return;
+        }
+
+        updateClusterLoadStatistic();
+        adjustPriorities();
+
+        lastStatUpdateTime = System.currentTimeMillis();
+    }
+
+    /*
+     * Here is the only place we update the cluster load statistic info.
+     * We will not update this info dynamically along with the clone job's running.
+     * Although it will cause a little bit inaccurate, but is within a controllable range,
+     * because we already limit the total number of running clone jobs in cluster by 'backend slots'
+     */
+    private void updateClusterLoadStatistic() {
+        statisticMap.clear();
+        List<String> clusterNames = infoService.getClusterNames();
+        for (String clusterName : clusterNames) {
+            ClusterLoadStatistic clusterLoadStatistic = new ClusterLoadStatistic(clusterName, catalog,
+                    infoService, invertedIndex);
+            clusterLoadStatistic.init();
+            statisticMap.put(clusterName, clusterLoadStatistic);
+            LOG.info(""update cluster {} load statistic:\n {}"", clusterName, clusterLoadStatistic.getBrief());
+        }
+    }
+
+    public Map<String, ClusterLoadStatistic> getStatisticMap() {
+        return statisticMap;
+    }
+
+    /*
+     * adjust priorities of all tablet infos
+     */
+    private synchronized void adjustPriorities() {
+        int size = pendingTablets.size();
+        int changedNum = 0;
+        TabletInfo tabletInfo = null;
+        for (int i = 0; i < size; i++) {
+            tabletInfo = pendingTablets.poll();
+            if (tabletInfo == null) {
+                break;
+            }
+
+            if (tabletInfo.adjustPriority(stat)) {
+                changedNum++;
+            }
+            pendingTablets.add(tabletInfo);
+        }
+
+        LOG.info(""adjust priority for all tablets. changed: {}, total: {}"", changedNum, size);
+    }
+
+    /*
+     * get at most BATCH_NUM tablets from queue, and try to schedule them.
+     * After handle, the tablet info should be
+     * 1. in runningTablets with state RUNNING, if being scheduled success.
+     * 2. or in schedHistory with state CANCELLING, if some unrecoverable error happens.
+     * 3. or in pendingTablets with state PENDING, if failed to be scheduled.
+     * 
+     * if in schedHistory, it should be removed from allTabletIds.
+     */
+    private void schedulePendingTablets() {
+        long start = System.currentTimeMillis();
+        List<TabletInfo> currentBatch = getNextTabletInfoBatch();
+        LOG.debug(""get {} tablets to schedule"", currentBatch.size());
+
+        AgentBatchTask batchTask = new AgentBatchTask();
+        for (TabletInfo tabletInfo : currentBatch) {
+            try {
+                scheduleTablet(tabletInfo, batchTask);
+            } catch (SchedException e) {
+                tabletInfo.increaseFailedSchedCounter();
+                tabletInfo.setErrMsg(e.getMessage());
+
+                if (e.getStatus() == Status.SCHEDULE_FAILED) {
+                    // we must release resource it current hold, and be scheduled again
+                    tabletInfo.releaseResource(this);
+                    // adjust priority to avoid some higher priority always be the first in pendingTablets
+                    stat.counterTabletScheduledFailed.incrementAndGet();
+                    dynamicAdjustPrioAndAddBackToPendingTablets(tabletInfo, e.getMessage());
+                } else if (e.getStatus() == Status.FINISHED) {
+                    // schedule redundant tablet will throw this exception
+                    stat.counterTabletScheduledSucceeded.incrementAndGet();
+                    removeTabletInfo(tabletInfo, TabletInfo.State.FINISHED, e.getMessage());
+                } else {
+                    Preconditions.checkState(e.getStatus() == Status.UNRECOVERABLE, e.getStatus());
+                    // discard
+                    stat.counterTabletScheduledDiscard.incrementAndGet();
+                    removeTabletInfo(tabletInfo, TabletInfo.State.CANCELLED, e.getMessage());
+                }
+                continue;
+            }
+
+            Preconditions.checkState(tabletInfo.getState() == TabletInfo.State.RUNNING);
+            stat.counterTabletScheduledSucceeded.incrementAndGet();
+            addToRunningTablets(tabletInfo);
+        }
+
+        // must send task after adding tablet info to runningTablets.
+        for (AgentTask task : batchTask.getAllTasks()) {
+            if (AgentTaskQueue.addTask(task)) {
+                stat.counterCloneTask.incrementAndGet();
+            }
+            LOG.info(""add clone task to agent task queue: {}"", task);
+        }
+
+        // send task immediately
+        AgentTaskExecutor.submit(batchTask);
+
+        long cost = System.currentTimeMillis() - start;
+        stat.counterTabletScheduleCostMs.addAndGet(cost);
+    }
+
+    private synchronized void addToRunningTablets(TabletInfo tabletInfo) {
+        runningTablets.put(tabletInfo.getTabletId(), tabletInfo);
+    }
+
+    /*
+     * we take the tablet out of the runningTablets and than handle it,
+     * avoid other threads see it.
+     * Whoever takes this tablet, make sure to put it to the schedHistory or back to runningTablets.
+     */
+    private synchronized TabletInfo takeRunningTablets(long tabletId) {
+        return runningTablets.remove(tabletId);
+    }
+
+    /*
+     * Try to schedule a single tablet.
+     */
+    private void scheduleTablet(TabletInfo tabletInfo, AgentBatchTask batchTask) throws SchedException {
+        LOG.debug(""schedule tablet: {}"", tabletInfo.getTabletId());
+        long currentTime = System.currentTimeMillis();
+        tabletInfo.setLastSchedTime(currentTime);
+        tabletInfo.setLastVisitedTime(currentTime);
+        stat.counterTabletScheduled.incrementAndGet();
+
+        // check this tablet again
+        Database db = catalog.getDb(tabletInfo.getDbId());
+        if (db == null) {
+            throw new SchedException(Status.UNRECOVERABLE, ""db does not exist"");
+        }
+
+        Pair<TabletStatus, TabletInfo.Priority> statusPair = null;
+        db.writeLock();
+        try {
+            OlapTable tbl = (OlapTable) db.getTable(tabletInfo.getTblId());
+            if (tbl == null) {
+                throw new SchedException(Status.UNRECOVERABLE, ""tbl does not exist"");
+            }
+
+            // we may add a tablet of a NOT NORMAL table during balance, which should be blocked
+            if (tbl.getState() != OlapTableState.NORMAL) {
+                throw new SchedException(Status.UNRECOVERABLE, ""tbl's state is not normal: "" + tbl.getState());
+            }
+
+            Partition partition = tbl.getPartition(tabletInfo.getPartitionId());
+            if (partition == null) {
+                throw new SchedException(Status.UNRECOVERABLE, ""partition does not exist"");
+            }
+
+            MaterializedIndex idx = partition.getIndex(tabletInfo.getIndexId());
+            if (idx == null) {
+                throw new SchedException(Status.UNRECOVERABLE, ""index does not exist"");
+            }
+
+            Tablet tablet = idx.getTablet(tabletInfo.getTabletId());
+            Preconditions.checkNotNull(tablet);
+
+            statusPair = tablet.getHealthStatusWithPriority(
+                    infoService, tabletInfo.getCluster(),
+                    partition.getVisibleVersion(),
+                    partition.getVisibleVersionHash(),
+                    tbl.getPartitionInfo().getReplicationNum(partition.getId()));
+
+            tabletInfo.setTabletStatus(statusPair.first);
+            if (statusPair.first == TabletStatus.HEALTHY && tabletInfo.getType() == TabletInfo.Type.REPAIR) {
+                throw new SchedException(Status.UNRECOVERABLE, ""tablet is healthy"");
+            } else if (statusPair.first != TabletStatus.HEALTHY
+                    && tabletInfo.getType() == TabletInfo.Type.BALANCE) {
+                tabletInfo.releaseResource(this);
+                // we select an unhealthy tablet to do balance, which is not right.
+                // so here we change it to a REPAIR task, and also reset its priority
+                tabletInfo.setType(TabletInfo.Type.REPAIR);
+                tabletInfo.setOrigPriority(statusPair.second);
+            }
+
+            // we do not concern priority here.
+            // once we take the tablet out of priority queue, priority is meaningless.
+            tabletInfo.setTablet(tablet);
+            tabletInfo.setVersionInfo(partition.getVisibleVersion(), partition.getVisibleVersionHash(),
+                    partition.getCommittedVersion(), partition.getCommittedVersionHash());
+            tabletInfo.setSchemaHash(tbl.getSchemaHashByIndexId(idx.getId()));
+            tabletInfo.setStorageMedium(tbl.getPartitionInfo().getDataProperty(partition.getId()).getStorageMedium());
+
+            handleTabletByTypeAndStatus(statusPair.first, tabletInfo, batchTask);
+        } finally {
+            db.writeUnlock();
+        }
+    }
+
+    private void handleTabletByTypeAndStatus(TabletStatus status, TabletInfo tabletInfo, AgentBatchTask batchTask)
+            throws SchedException {
+        if (tabletInfo.getType() == Type.REPAIR) {
+            switch (status) {
+                case REPLICA_MISSING:
+                    handleReplicaMissing(tabletInfo, batchTask);
+                    break;
+                case VERSION_INCOMPLETE:
+                    handleReplicaVersionIncomplete(tabletInfo, batchTask);
+                    break;
+                case REDUNDANT:
+                    handleRedundantReplica(tabletInfo);
+                    break;
+                case REPLICA_MISSING_IN_CLUSTER:
+                    handleReplicaClusterMigration(tabletInfo, batchTask);
+                    break;
+                default:
+                    break;
+            }
+        } else {
+            // balance
+            doBalance(tabletInfo, batchTask);
+        }
+    }
+
+    /*
+     * Replica is missing, which means there is no enough alive replicas.
+     * So we need to find a destination backend to clone a new replica as possible as we can.
+     * 1. find an available path in a backend as destination:
+     *      1. backend need to be alive.
+     *      2. backend of existing replicas should be excluded.
+     *      3. backend has available slot for clone.
+     *      4. replica can fit in the path (consider the threshold of disk capacity and usage percent).
+     *      5. try to find a path with lowest load score.
+     * 2. find an appropriate source replica:
+     *      1. source replica should be healthy
+     *      2. backend of source replica has available slot for clone.
+     *      
+     * 3. send clone task to destination backend
+     */
+    private void handleReplicaMissing(TabletInfo tabletInfo, AgentBatchTask batchTask) throws SchedException {
+        stat.counterReplicaMissingErr.incrementAndGet();
+        // find an available dest backend and path
+        RootPathLoadStatistic destPath = chooseAvailableDestPath(tabletInfo);
+        Preconditions.checkNotNull(destPath);
+        tabletInfo.setDestination(destPath.getBeId(), destPath.getPathHash());
+
+        // choose a source replica for cloning from
+        tabletInfo.chooseSrcReplica(backendsWorkingSlots);
+
+        // create clone task
+        batchTask.addTask(tabletInfo.createCloneReplicaAndTask());
+    }
+
+    /*
+     * Replica version is incomplete, which means this replica is missing some version,
+     * and need to be cloned from a healthy replica, in-place.
+     * 
+     * 1. find the incomplete replica as destination replica
+     * 2. find a healthy replica as source replica
+     * 3. send clone task
+     */
+    private void handleReplicaVersionIncomplete(TabletInfo tabletInfo, AgentBatchTask batchTask)
+            throws SchedException {
+        stat.counterReplicaVersionMissingErr.incrementAndGet();
+        ClusterLoadStatistic statistic = statisticMap.get(tabletInfo.getCluster());
+        if (statistic == null) {
+            throw new SchedException(Status.UNRECOVERABLE, ""cluster does not exist"");
+        }
+
+        tabletInfo.chooseDestReplicaForVersionIncomplete(backendsWorkingSlots);
+        tabletInfo.chooseSrcReplicaForVersionIncomplete(backendsWorkingSlots);
+
+        // create clone task
+        batchTask.addTask(tabletInfo.createCloneReplicaAndTask());
+    }
+
+    /*
+     *  replica is redundant, which means there are more replicas than we expected, which need to be dropped.
+     *  we just drop one redundant replica at a time, for safety reason.
+     *  choosing a replica to drop base on following priority:
+     *  1. backend has been dropped
+     *  2. backend is not available
+     *  3. replica's state is CLONE
+     *  4. replica's last failed version > 0
+     *  5. replica with lower version
+     *  6. replica not in right cluster
+     *  7. replica in higher load backend
+     */
+    private void handleRedundantReplica(TabletInfo tabletInfo) throws SchedException {
+        stat.counterReplicaRedundantErr.incrementAndGet();
+        if (deleteBackendDropped(tabletInfo)
+                || deleteBackendUnavailable(tabletInfo)
+                || deleteCloneReplica(tabletInfo)
+                || deleteReplicaWithFailedVersion(tabletInfo)
+                || deleteReplicaWithLowerVersion(tabletInfo)
+                || deleteReplicaNotInCluster(tabletInfo)
+                || deleteReplicaOnHighLoadBackend(tabletInfo)) {
+            // if we delete at least one redundant replica, we still throw a SchedException with status FINISHED
+            // to remove this tablet from the pendingTablets(consider it as finished)
+            throw new SchedException(Status.FINISHED, ""redundant replica is deleted"");
+        }
+        throw new SchedException(Status.SCHEDULE_FAILED, ""unable to delete any redundant replicas"");
+    }
+
+    private boolean deleteBackendDropped(TabletInfo tabletInfo) {
+        for (Replica replica : tabletInfo.getReplicas()) {
+            long beId = replica.getBackendId();
+            if (infoService.getBackend(beId) == null) {
+                deleteReplicaInternal(tabletInfo, replica, ""backend dropped"");
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private boolean deleteBackendUnavailable(TabletInfo tabletInfo) {
+        for (Replica replica : tabletInfo.getReplicas()) {
+            Backend be = infoService.getBackend(replica.getBackendId());
+            if (be == null) {
+                // this case should be handled in deleteBackendDropped()
+                continue;
+            }
+            if (!be.isAvailable()) {
+                deleteReplicaInternal(tabletInfo, replica, ""backend unavailable"");
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private boolean deleteCloneReplica(TabletInfo tabletInfo) {
+        for (Replica replica : tabletInfo.getReplicas()) {
+            if (replica.getState() == ReplicaState.CLONE) {
+                deleteReplicaInternal(tabletInfo, replica, ""clone state"");
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private boolean deleteReplicaWithFailedVersion(TabletInfo tabletInfo) {
+        for (Replica replica : tabletInfo.getReplicas()) {
+            if (replica.getLastFailedVersion() > 0) {
+                deleteReplicaInternal(tabletInfo, replica, ""version incomplete"");
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private boolean deleteReplicaWithLowerVersion(TabletInfo tabletInfo) {
+        for (Replica replica : tabletInfo.getReplicas()) {
+            if (!replica.checkVersionCatchUp(tabletInfo.getCommittedVersion(), tabletInfo.getCommittedVersionHash())) {
+                deleteReplicaInternal(tabletInfo, replica, ""lower version"");
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private boolean deleteReplicaNotInCluster(TabletInfo tabletInfo) {
+        for (Replica replica : tabletInfo.getReplicas()) {
+            Backend be = infoService.getBackend(replica.getBackendId());
+            if (be == null) {
+                // this case should be handled in deleteBackendDropped()
+                continue;
+            }
+            if (!be.getOwnerClusterName().equals(tabletInfo.getCluster())) {
+                deleteReplicaInternal(tabletInfo, replica, ""not in cluster"");
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private boolean deleteReplicaOnHighLoadBackend(TabletInfo tabletInfo) {
+        ClusterLoadStatistic statistic = statisticMap.get(tabletInfo.getCluster());
+        if (statistic == null) {
+            return false;
+        }
+        
+        Replica chosenReplica = null;
+        double maxScore = 0;
+        for (Replica replica : tabletInfo.getReplicas()) {
+            BackendLoadStatistic beStatistic = statistic.getBackendLoadStatistic(replica.getBackendId());
+            if (beStatistic == null) {
+                continue;
+            }
+            if (beStatistic.getLoadScore() > maxScore) {
+                maxScore = beStatistic.getLoadScore();
+                chosenReplica = replica;
+            }
+        }
+
+        if (chosenReplica != null) {
+            deleteReplicaInternal(tabletInfo, chosenReplica, ""high load"");
+            return true;
+        }
+        return false;
+    }
+
+    private void deleteReplicaInternal(TabletInfo tabletInfo, Replica replica, String reason) {
+        // delete this replica from catalog.
+        // it will also delete replica from tablet inverted index.
+        tabletInfo.deleteReplica(replica);
+
+        // TODO(cmy): this should be removed after I finish modifying alter job logic
+        // Catalog.getInstance().handleJobsWhenDeleteReplica(tabletInfo.getTblId(), tabletInfo.getPartitionId(),
+        //                                                   tabletInfo.getIndexId(), tabletInfo.getTabletId(),
+        //                                                   replica.getId(), replica.getBackendId());
+
+        // write edit log
+        ReplicaPersistInfo info = ReplicaPersistInfo.createForDelete(tabletInfo.getDbId(),
+                                                                     tabletInfo.getTblId(),
+                                                                     tabletInfo.getPartitionId(),
+                                                                     tabletInfo.getIndexId(),
+                                                                     tabletInfo.getTabletId(),
+                                                                     replica.getBackendId());
+
+        Catalog.getInstance().getEditLog().logDeleteReplica(info);
+
+        LOG.info(""delete replica. tablet id: {}, backend id: {}. reason: {}"",
+                 tabletInfo.getTabletId(), replica.getBackendId(), reason);
+    }
+
+    /*
+     * Cluster migration, which means the tablet has enough healthy replicas,
+     * but some replicas are not in right cluster.
+     * It is just same as 'replica missing'.
+     * 
+     * after clone finished, the replica in wrong cluster will be treated as redundant, and will be deleted soon.
+     */
+    private void handleReplicaClusterMigration(TabletInfo tabletInfo, AgentBatchTask batchTask)
+            throws SchedException {
+        stat.counterReplicaMissingInClusterErr.incrementAndGet();
+        handleReplicaMissing(tabletInfo, batchTask);
+    }
+
+    /*
+     * Try to select some alternative tablets for balance. Add them to pendingTablets with priority LOW,
+     * and waiting to be scheduled.
+     */
+    private void selectTabletsForBalance() {
+        LoadBalancer loadBalancer = new LoadBalancer(statisticMap);
+        List<TabletInfo> alternativeTablets = loadBalancer.selectAlternativeTablets();
+        for (TabletInfo tabletInfo : alternativeTablets) {
+            addTablet(tabletInfo, false);
+        }
+    }
+
+    /*
+     * Try to create a balance task for a tablet.
+     */
+    private void doBalance(TabletInfo tabletInfo, AgentBatchTask batchTask) throws SchedException {
+        stat.counterBalanceSchedule.incrementAndGet();
+        LoadBalancer loadBalancer = new LoadBalancer(statisticMap);
+        loadBalancer.createBalanceTask(tabletInfo, backendsWorkingSlots, batchTask);
+    }
+
+    // choose a path on a backend which is fit for the tablet
+    private RootPathLoadStatistic chooseAvailableDestPath(TabletInfo tabletInfo) throws SchedException {
+        ClusterLoadStatistic statistic = statisticMap.get(tabletInfo.getCluster());
+        if (statistic == null) {
+            throw new SchedException(Status.UNRECOVERABLE, ""cluster does not exist"");
+        }
+        List<BackendLoadStatistic> beStatistics = statistic.getBeLoadStatistics();
+
+        // get all available paths which this tablet can fit in.
+        // beStatistics is sorted by load score in ascend order, so select from first to last.
+        List<RootPathLoadStatistic> allFitPaths = Lists.newArrayList();
+        for (int i = 0; i < beStatistics.size(); i++) {
+            BackendLoadStatistic bes = beStatistics.get(i);
+            // exclude BE which already has replica of this tablet
+            if (tabletInfo.containsBE(bes.getBeId())) {
+                continue;
+            }
+
+            List<RootPathLoadStatistic> resultPaths = Lists.newArrayList();
+            BalanceStatus st = bes.isFit(tabletInfo.getTabletSize(), resultPaths, true /* is supplement */);
+            if (!st.ok()) {
+                LOG.debug(""unable to find path for supplementing tablet: {}. {}"", tabletInfo, st);
+                continue;
+            }
+
+            Preconditions.checkState(resultPaths.size() == 1);
+            allFitPaths.add(resultPaths.get(0));
+        }
+
+        if (allFitPaths.isEmpty()) {
+            throw new SchedException(Status.SCHEDULE_FAILED, ""unable to find dest path for new replica"");
+        }
+
+        // all fit paths has already been sorted by load score in 'allFitPaths' in ascend order.
+        // just get first available path.
+        // we try to find a path with specified media type, if not find, arbitrarily use one.
+        for (RootPathLoadStatistic rootPathLoadStatistic : allFitPaths) {
+            if (rootPathLoadStatistic.getStorageMedium() != tabletInfo.getStorageMedium()) {
+                continue;
+            }
+
+            PathSlot slot = backendsWorkingSlots.get(rootPathLoadStatistic.getBeId());
+            if (slot == null) {
+                LOG.debug(""backend {} does not found when getting slots"", rootPathLoadStatistic.getBeId());
+                continue;
+            }
+
+            if (slot.takeSlot(rootPathLoadStatistic.getPathHash()) != -1) {
+                return rootPathLoadStatistic;
+            }
+        }
+
+        // no root path with specified media type is found, get arbitrary one.
+        for (RootPathLoadStatistic rootPathLoadStatistic : allFitPaths) {
+            PathSlot slot = backendsWorkingSlots.get(rootPathLoadStatistic.getBeId());
+            if (slot == null) {
+                LOG.debug(""backend {} does not found when getting slots"", rootPathLoadStatistic.getBeId());
+                continue;
+            }
+
+            if (slot.takeSlot(rootPathLoadStatistic.getPathHash()) != -1) {
+                return rootPathLoadStatistic;
+            }
+        }
+        
+        throw new SchedException(Status.SCHEDULE_FAILED, ""unable to find dest path which can be fit in"");
+    }
+
+    /*
+     * For some reason, a tablet info failed to be scheduled this time,
+     * So we dynamically change its priority and add back to queue, waiting for next round.
+     */
+    private void dynamicAdjustPrioAndAddBackToPendingTablets(TabletInfo tabletInfo, String message) {
+        Preconditions.checkState(tabletInfo.getState() == TabletInfo.State.PENDING);
+        tabletInfo.adjustPriority(stat);
+        addTablet(tabletInfo, true /* force */);
+    }
+
+    private synchronized void removeTabletInfo(TabletInfo tabletInfo, TabletInfo.State state, String reason) {
+        tabletInfo.setState(state);
+        tabletInfo.releaseResource(this);
+        tabletInfo.setFinishedTime(System.currentTimeMillis());
+        runningTablets.remove(tabletInfo.getTabletId());
+        allTabletIds.remove(tabletInfo.getTabletId());
+        schedHistory.add(tabletInfo);
+        LOG.info(""remove the tablet {}. because: {}"", tabletInfo.getTabletId(), reason);
+    }
+
+    // get next batch of tablets from queue.
+    private synchronized List<TabletInfo> getNextTabletInfoBatch() {
+        List<TabletInfo> list = Lists.newArrayList();
+        int count = Math.max(MIN_BATCH_NUM, getCurrentAvailableSlotNum());
+        while (count > 0) {
+            TabletInfo tablet = pendingTablets.poll();
+            if (tablet == null) {
+                // no more tablets
+                break;
+            }
+            list.add(tablet);
+            count--;
+        }
+        return list;
+    }
+
+    private int getCurrentAvailableSlotNum() {
+        int total = 0;
+        for (PathSlot pathSlot : backendsWorkingSlots.values()) {
+            total += pathSlot.getTotalAvailSlotNum();
+        }
+        return total;
+    }
+
+    /*
+     * return true if we want to remove the clone task from AgentTaskQueu
+     */
+    public boolean finishCloneTask(CloneTask cloneTask, TFinishTaskRequest request) {
+        long tabletId = cloneTask.getTabletId();
+        TabletInfo tabletInfo = takeRunningTablets(tabletId);
+        if (tabletInfo == null) {
+            LOG.warn(""tablet info does not exist: {}"", tabletId);
+            // tablet does not exist, no need to keep task.
+            return true;
+        }
+        Preconditions.checkState(tabletInfo.getState() == TabletInfo.State.RUNNING);
+        try {
+            tabletInfo.finishCloneTask(cloneTask, request);
+        } catch (SchedException e) {
+            tabletInfo.increaseFailedRunningCounter();
+            tabletInfo.setErrMsg(e.getMessage());
+            if (e.getStatus() == Status.RUNNING_FAILED) {
+                stat.counterCloneTaskFailed.incrementAndGet();
+                addToRunningTablets(tabletInfo);
+                return false;
+            } else {
+                Preconditions.checkState(e.getStatus() == Status.UNRECOVERABLE, e.getStatus());
+                // unrecoverable
+                stat.counterTabletScheduledDiscard.incrementAndGet();
+                removeTabletInfo(tabletInfo, TabletInfo.State.CANCELLED, e.getMessage());
+                return true;
+            }
+        }
+
+        Preconditions.checkState(tabletInfo.getState() == TabletInfo.State.FINISHED);
+        stat.counterCloneTaskSucceeded.incrementAndGet();
+        gatherStatistics(tabletInfo);
+        removeTabletInfo(tabletInfo, TabletInfo.State.FINISHED, ""finished"");
+        return true;
+    }
+
+    /*
+     * Gather the running statistic of the task.
+     * It will be evaluated for future strategy.  
+     * This should only be called when the tablet is down with state FINISHED.
+     */
+    private void gatherStatistics(TabletInfo tabletInfo) {
+        if (tabletInfo.getCopySize() > 0 && tabletInfo.getCopyTimeMs() > 0) {
+            if (tabletInfo.getSrcBackendId() != -1 && tabletInfo.getSrcPathHash() != -1) {
+                PathSlot pathSlot = backendsWorkingSlots.get(tabletInfo.getSrcBackendId());
+                if (pathSlot != null) {
+                    pathSlot.updateStatistic(tabletInfo.getSrcPathHash(), tabletInfo.getCopySize(),
+                            tabletInfo.getCopyTimeMs());
+                }
+            }
+
+            if (tabletInfo.getDestBackendId() != -1 && tabletInfo.getDestPathHash() != -1) {
+                PathSlot pathSlot = backendsWorkingSlots.get(tabletInfo.getDestBackendId());
+                if (pathSlot != null) {
+                    pathSlot.updateStatistic(tabletInfo.getDestPathHash(), tabletInfo.getCopySize(),
+                            tabletInfo.getCopyTimeMs());
+                }
+            }
+        }
+
+        if (System.currentTimeMillis() - lastSlotAdjustTime < STAT_UPDATE_INTERVAL_MS) {
+            return;
+        }
+
+        // TODO(cmy): update the slot num base on statistic.
+        // need to find a better way to determine the slot number.
+
+        lastSlotAdjustTime = System.currentTimeMillis();
+    }
+
+    /*
+     * handle tablets which are running.
+     * We should finished the task if
+     * 1. Tablet is already healthy
+     * 2. Task is timeout.
+     * 
+     * But here we just handle the timeout case here. Let the 'finishCloneTask()' check if tablet is healthy.
+     * We guarantee that if tablet is in runningTablets, the 'finishCloneTask()' will finally be called,
+     * so no need to worry that running tablets will never end.
+     * This is also avoid nesting 'synchronized' and database lock.
+     *
+     * If task is timeout, remove the tablet.
+     */
+    public synchronized void handleRunningTablets() {
+        List<TabletInfo> timeoutTablets = Lists.newArrayList();
+        runningTablets.values().stream().filter(t -> t.isTimeout()).forEach(t -> {
+            timeoutTablets.add(t);
+        });
+        
+        timeoutTablets.stream().forEach(t -> {
+            removeTabletInfo(t, TabletInfo.State.TIMEOUT, ""timeout"");
+            stat.counterCloneTaskTimeout.incrementAndGet();
+        });
+    }
+
+    public List<List<String>> getPendingTabletsInfo(int limit) {
+        List<TabletInfo> tabletInfos = getCopiedTablets(pendingTablets, limit);
+        return collectTabletInfo(tabletInfos);
+    }
+
+    public List<List<String>> getRunningTabletsInfo(int limit) {
+        List<TabletInfo> tabletInfos = getCopiedTablets(runningTablets.values(), limit);
+        return collectTabletInfo(tabletInfos);
+    }
+
+    public List<List<String>> getHistoryTabletsInfo(int limit) {
+        List<TabletInfo> tabletInfos = getCopiedTablets(schedHistory, limit);
+        return collectTabletInfo(tabletInfos);
+    }
+
+    private List<List<String>> collectTabletInfo(List<TabletInfo> tabletInfos) {
+        List<List<String>> result = Lists.newArrayList();
+        tabletInfos.stream().forEach(t -> {
+            result.add(t.getBrief());
+        });
+        return result;
+    }
+
+    private synchronized List<TabletInfo> getCopiedTablets(Collection<TabletInfo> source, int limit) {
+        List<TabletInfo> tabletInfos = Lists.newArrayList();
+        source.stream().limit(limit).forEach(t -> {
+            tabletInfos.add(t);
+        });
+        return tabletInfos;
+    }
+
+    public synchronized int getPendingNum() {
+        return pendingTablets.size();
+    }
+
+    public synchronized int getRunningNum() {
+        return runningTablets.size();
+    }
+
+    public synchronized int getHistoryNum() {
+        return schedHistory.size();
+    }
+
+    /*
+     * PathSlot keeps track of slot num per path of a Backend.
+     * Each path on a Backend has several slot.
+     * If a path's available slot num because 0, no task should be assigned to this path.","[{'comment': 'what does this mean?', 'commenter': 'imay'}]"
336,fe/src/main/java/org/apache/doris/clone/TabletScheduler.java,"@@ -0,0 +1,1186 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.clone;
+
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.catalog.Database;
+import org.apache.doris.catalog.MaterializedIndex;
+import org.apache.doris.catalog.OlapTable;
+import org.apache.doris.catalog.OlapTable.OlapTableState;
+import org.apache.doris.catalog.Partition;
+import org.apache.doris.catalog.Replica;
+import org.apache.doris.catalog.Replica.ReplicaState;
+import org.apache.doris.catalog.Tablet;
+import org.apache.doris.catalog.Tablet.TabletStatus;
+import org.apache.doris.catalog.TabletInvertedIndex;
+import org.apache.doris.clone.SchedException.Status;
+import org.apache.doris.clone.TabletInfo.Priority;
+import org.apache.doris.clone.TabletInfo.Type;
+import org.apache.doris.common.Config;
+import org.apache.doris.common.Pair;
+import org.apache.doris.common.util.Daemon;
+import org.apache.doris.persist.ReplicaPersistInfo;
+import org.apache.doris.system.Backend;
+import org.apache.doris.system.SystemInfoService;
+import org.apache.doris.task.AgentBatchTask;
+import org.apache.doris.task.AgentTask;
+import org.apache.doris.task.AgentTaskExecutor;
+import org.apache.doris.task.AgentTaskQueue;
+import org.apache.doris.task.CloneTask;
+import org.apache.doris.thrift.TFinishTaskRequest;
+
+import com.google.common.base.Preconditions;
+import com.google.common.collect.EvictingQueue;
+import com.google.common.collect.ImmutableMap;
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+import com.google.common.collect.Sets;
+
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.Collection;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.PriorityQueue;
+import java.util.Queue;
+import java.util.Set;
+import java.util.stream.Collectors;
+
+/*
+ * TabletScheduler saved the tablets produced by TabletChecker and try to schedule them.
+ * It also try to balance the cluster load.
+ * 
+ * We are expecting an efficient way to recovery the entire cluster and make it balanced.
+ * Case 1:
+ *  A Backend is down. All tablets which has replica on this BE should be repaired as soon as possible.
+ *  
+ * Case 1.1:
+ *  As Backend is down, some tables should be repaired in high priority. So the clone task should be able
+ *  to preempted.
+ *  
+ * Case 2:
+ *  A new Backend is added to the cluster. Replicas should be transfer to that host to balance the cluster load.
+ */
+public class TabletScheduler extends Daemon {
+    private static final Logger LOG = LogManager.getLogger(TabletScheduler.class);
+
+    // handle at most BATCH_NUM tablets in one loop
+    private static final int MIN_BATCH_NUM = 10;
+
+    // the minimum interval of updating cluster statistics and priority of tablet info
+    private static final long STAT_UPDATE_INTERVAL_MS = 60 * 1000; // 1min
+
+    private static final long SCHEDULE_INTERVAL_MS = 5000; // 5s
+
+    public static final int BALANCE_SLOT_NUM_FOR_PATH = 2;
+
+    /*
+     * Tablet is added to pendingTablets as well it's id in allTabletIds.
+     * TabletScheduler will take tablet from pendingTablets but will not remove it's id from allTabletIds when
+     * handling a tablet.
+     * Tablet' id can only be removed after the clone task is done(timeout, cancelled or finished).
+     * So if a tablet's id is still in allTabletIds, TabletChecker can not add tablet to TabletScheduler.
+     * 
+     * pendingTablets + runningTablets = allTabletIds
+     * 
+     * pendingTablets, allTabletIds, runningTablets and schedHistory are protected by 'synchronized' 
+     */
+    private PriorityQueue<TabletInfo> pendingTablets = new PriorityQueue<>();
+    private Set<Long> allTabletIds = Sets.newHashSet();
+    // contains all tabletInfos which state are RUNNING
+    private Map<Long, TabletInfo> runningTablets = Maps.newHashMap();
+    // save the latest 1000 scheduled tablet info
+    private Queue<TabletInfo> schedHistory = EvictingQueue.create(1000);
+
+    // be id -> #working slots
+    private Map<Long, PathSlot> backendsWorkingSlots = Maps.newConcurrentMap();
+    // cluster name -> load statistic
+    private Map<String, ClusterLoadStatistic> statisticMap = Maps.newConcurrentMap();
+    private long lastStatUpdateTime = 0;
+    
+    private long lastSlotAdjustTime = 0;
+
+    private Catalog catalog;
+    private SystemInfoService infoService;
+    private TabletInvertedIndex invertedIndex;
+    private TabletSchedulerStat stat;
+
+    public TabletScheduler(Catalog catalog, SystemInfoService infoService, TabletInvertedIndex invertedIndex,
+            TabletSchedulerStat stat) {
+        super(""tablet scheduler"", SCHEDULE_INTERVAL_MS);
+        this.catalog = catalog;
+        this.infoService = infoService;
+        this.invertedIndex = invertedIndex;
+        this.stat = stat;
+    }
+
+    public TabletSchedulerStat getStat() {
+        return stat;
+    }
+
+    /*
+     * update working slots at the beginning of each round
+     */
+    private boolean updateWorkingSlots() {
+        ImmutableMap<Long, Backend> backends = infoService.getBackendsInCluster(null);
+        for (Backend backend : backends.values()) {
+            if (!backend.hasPathHash() && backend.isAlive()) {
+                // when upgrading, backend may not get path info yet. so return false and wait for next round.
+                // and we should check if backend is alive. If backend is dead when upgrading, this backend
+                // will never report its path hash, and tablet scheduler is blocked.
+                LOG.info(""not all backends have path info"");
+                return false;
+            }
+        }
+
+        // update exist backends
+        Set<Long> deletedBeIds = Sets.newHashSet();
+        for (Long beId : backendsWorkingSlots.keySet()) {
+            if (backends.containsKey(beId)) {
+                List<Long> pathHashes = backends.get(beId).getDisks().values().stream().map(v -> v.getPathHash()).collect(Collectors.toList());
+                backendsWorkingSlots.get(beId).updatePaths(pathHashes);
+            } else {
+                deletedBeIds.add(beId);
+            }
+        }
+
+        // delete non-exist backends
+        for (Long beId : deletedBeIds) {
+            backendsWorkingSlots.remove(beId);
+            LOG.info(""delete non exist backend: {}"", beId);
+        }
+
+        // add new backends
+        for (Backend be : backends.values()) {
+            if (!backendsWorkingSlots.containsKey(be.getId())) {
+                List<Long> pathHashes = be.getDisks().values().stream().map(v -> v.getPathHash()).collect(Collectors.toList());
+                PathSlot slot = new PathSlot(pathHashes, Config.schedule_slot_num_per_path);
+                backendsWorkingSlots.put(be.getId(), slot);
+                LOG.info(""add new backend {} with slots num: {}"", be.getId(), be.getDisks().size());
+            }
+        }
+
+        return true;
+    }
+
+    public Map<Long, PathSlot> getBackendsWorkingSlots() {
+        return backendsWorkingSlots;
+    }
+
+    /*
+     * add a ready-to-be-scheduled tablet to pendingTablets, if it has not being added before.
+     * if force is true, do not check if tablet is already added before.
+     */
+    public synchronized boolean addTablet(TabletInfo tablet, boolean force) {
+        if (!force && containsTablet(tablet.getTabletId())) {
+            return false;
+        }
+        allTabletIds.add(tablet.getTabletId());
+        pendingTablets.offer(tablet);
+        return true;
+    }
+
+    public synchronized boolean containsTablet(long tabletId) {
+        return allTabletIds.contains(tabletId);
+    }
+
+    /*
+     * Iterate current tablets, change their priority if necessary.
+     */
+    public synchronized void changePriorityOfTablets(long dbId, long tblId, List<Long> partitionIds) {
+        PriorityQueue<TabletInfo> newPendingTablets = new PriorityQueue<>();
+        for (TabletInfo tabletInfo : pendingTablets) {
+            if (tabletInfo.getDbId() == dbId && tabletInfo.getTblId() == tblId
+                    && partitionIds.contains(tabletInfo.getPartitionId())) {
+                tabletInfo.setOrigPriority(Priority.VERY_HIGH);
+            }
+            newPendingTablets.add(tabletInfo);
+        }
+        pendingTablets = newPendingTablets;
+    }
+
+    /*
+     * TabletScheduler will run as a daemon thread at a very short interval(default 5 sec)
+     * Firstly, it will try to update cluster load statistic and check if priority need to be adjuested.
+     * Than, it will schedule the tablets in pendingTablets.
+     * Thirdly, it will check the current running tasks.
+     * Finally, it try to balance the cluster if possible.
+     * 
+     * Schedule rules:
+     * 1. tablet with higher priority will be scheduled first.
+     * 2. high priority should be downgraded if it fails to be schedule too many times.
+     * 3. priority may be upgraded if it is not being schedule for a long time.
+     * 4. every pending task should has a max scheduled time, if schedule fails too many times, if should be removed.
+     * 5. every running task should has a timeout, to avoid running forever.
+     * 6. every running task should also has a max failure time, if clone task fails too many times, if should be removed.
+     *
+     */
+    @Override
+    protected void runOneCycle() {
+        if (!updateWorkingSlots()) {
+            return;
+        }
+
+        updateClusterLoadStatisticsAndPriorityIfNecessary();
+
+        schedulePendingTablets();
+
+        handleRunningTablets();
+
+        selectTabletsForBalance();
+
+        stat.counterTabletScheduleRound.incrementAndGet();
+    }
+
+
+    private void updateClusterLoadStatisticsAndPriorityIfNecessary() {
+        if (System.currentTimeMillis() - lastStatUpdateTime < STAT_UPDATE_INTERVAL_MS) {
+            return;
+        }
+
+        updateClusterLoadStatistic();
+        adjustPriorities();
+
+        lastStatUpdateTime = System.currentTimeMillis();
+    }
+
+    /*
+     * Here is the only place we update the cluster load statistic info.
+     * We will not update this info dynamically along with the clone job's running.
+     * Although it will cause a little bit inaccurate, but is within a controllable range,
+     * because we already limit the total number of running clone jobs in cluster by 'backend slots'
+     */
+    private void updateClusterLoadStatistic() {
+        statisticMap.clear();
+        List<String> clusterNames = infoService.getClusterNames();
+        for (String clusterName : clusterNames) {
+            ClusterLoadStatistic clusterLoadStatistic = new ClusterLoadStatistic(clusterName, catalog,
+                    infoService, invertedIndex);
+            clusterLoadStatistic.init();
+            statisticMap.put(clusterName, clusterLoadStatistic);
+            LOG.info(""update cluster {} load statistic:\n {}"", clusterName, clusterLoadStatistic.getBrief());
+        }
+    }
+
+    public Map<String, ClusterLoadStatistic> getStatisticMap() {
+        return statisticMap;
+    }
+
+    /*
+     * adjust priorities of all tablet infos
+     */
+    private synchronized void adjustPriorities() {
+        int size = pendingTablets.size();
+        int changedNum = 0;
+        TabletInfo tabletInfo = null;
+        for (int i = 0; i < size; i++) {
+            tabletInfo = pendingTablets.poll();
+            if (tabletInfo == null) {
+                break;
+            }
+
+            if (tabletInfo.adjustPriority(stat)) {
+                changedNum++;
+            }
+            pendingTablets.add(tabletInfo);
+        }
+
+        LOG.info(""adjust priority for all tablets. changed: {}, total: {}"", changedNum, size);
+    }
+
+    /*
+     * get at most BATCH_NUM tablets from queue, and try to schedule them.
+     * After handle, the tablet info should be
+     * 1. in runningTablets with state RUNNING, if being scheduled success.
+     * 2. or in schedHistory with state CANCELLING, if some unrecoverable error happens.
+     * 3. or in pendingTablets with state PENDING, if failed to be scheduled.
+     * 
+     * if in schedHistory, it should be removed from allTabletIds.
+     */
+    private void schedulePendingTablets() {
+        long start = System.currentTimeMillis();
+        List<TabletInfo> currentBatch = getNextTabletInfoBatch();
+        LOG.debug(""get {} tablets to schedule"", currentBatch.size());
+
+        AgentBatchTask batchTask = new AgentBatchTask();
+        for (TabletInfo tabletInfo : currentBatch) {
+            try {
+                scheduleTablet(tabletInfo, batchTask);
+            } catch (SchedException e) {
+                tabletInfo.increaseFailedSchedCounter();
+                tabletInfo.setErrMsg(e.getMessage());
+
+                if (e.getStatus() == Status.SCHEDULE_FAILED) {
+                    // we must release resource it current hold, and be scheduled again
+                    tabletInfo.releaseResource(this);
+                    // adjust priority to avoid some higher priority always be the first in pendingTablets
+                    stat.counterTabletScheduledFailed.incrementAndGet();
+                    dynamicAdjustPrioAndAddBackToPendingTablets(tabletInfo, e.getMessage());
+                } else if (e.getStatus() == Status.FINISHED) {
+                    // schedule redundant tablet will throw this exception
+                    stat.counterTabletScheduledSucceeded.incrementAndGet();
+                    removeTabletInfo(tabletInfo, TabletInfo.State.FINISHED, e.getMessage());
+                } else {
+                    Preconditions.checkState(e.getStatus() == Status.UNRECOVERABLE, e.getStatus());
+                    // discard
+                    stat.counterTabletScheduledDiscard.incrementAndGet();
+                    removeTabletInfo(tabletInfo, TabletInfo.State.CANCELLED, e.getMessage());
+                }
+                continue;
+            }
+
+            Preconditions.checkState(tabletInfo.getState() == TabletInfo.State.RUNNING);
+            stat.counterTabletScheduledSucceeded.incrementAndGet();
+            addToRunningTablets(tabletInfo);
+        }
+
+        // must send task after adding tablet info to runningTablets.
+        for (AgentTask task : batchTask.getAllTasks()) {
+            if (AgentTaskQueue.addTask(task)) {
+                stat.counterCloneTask.incrementAndGet();
+            }
+            LOG.info(""add clone task to agent task queue: {}"", task);
+        }
+
+        // send task immediately
+        AgentTaskExecutor.submit(batchTask);
+
+        long cost = System.currentTimeMillis() - start;
+        stat.counterTabletScheduleCostMs.addAndGet(cost);
+    }
+
+    private synchronized void addToRunningTablets(TabletInfo tabletInfo) {
+        runningTablets.put(tabletInfo.getTabletId(), tabletInfo);
+    }
+
+    /*
+     * we take the tablet out of the runningTablets and than handle it,
+     * avoid other threads see it.
+     * Whoever takes this tablet, make sure to put it to the schedHistory or back to runningTablets.
+     */
+    private synchronized TabletInfo takeRunningTablets(long tabletId) {
+        return runningTablets.remove(tabletId);
+    }
+
+    /*
+     * Try to schedule a single tablet.
+     */
+    private void scheduleTablet(TabletInfo tabletInfo, AgentBatchTask batchTask) throws SchedException {
+        LOG.debug(""schedule tablet: {}"", tabletInfo.getTabletId());
+        long currentTime = System.currentTimeMillis();
+        tabletInfo.setLastSchedTime(currentTime);
+        tabletInfo.setLastVisitedTime(currentTime);
+        stat.counterTabletScheduled.incrementAndGet();
+
+        // check this tablet again
+        Database db = catalog.getDb(tabletInfo.getDbId());
+        if (db == null) {
+            throw new SchedException(Status.UNRECOVERABLE, ""db does not exist"");
+        }
+
+        Pair<TabletStatus, TabletInfo.Priority> statusPair = null;
+        db.writeLock();
+        try {
+            OlapTable tbl = (OlapTable) db.getTable(tabletInfo.getTblId());
+            if (tbl == null) {
+                throw new SchedException(Status.UNRECOVERABLE, ""tbl does not exist"");
+            }
+
+            // we may add a tablet of a NOT NORMAL table during balance, which should be blocked
+            if (tbl.getState() != OlapTableState.NORMAL) {
+                throw new SchedException(Status.UNRECOVERABLE, ""tbl's state is not normal: "" + tbl.getState());
+            }
+
+            Partition partition = tbl.getPartition(tabletInfo.getPartitionId());
+            if (partition == null) {
+                throw new SchedException(Status.UNRECOVERABLE, ""partition does not exist"");
+            }
+
+            MaterializedIndex idx = partition.getIndex(tabletInfo.getIndexId());
+            if (idx == null) {
+                throw new SchedException(Status.UNRECOVERABLE, ""index does not exist"");
+            }
+
+            Tablet tablet = idx.getTablet(tabletInfo.getTabletId());
+            Preconditions.checkNotNull(tablet);
+
+            statusPair = tablet.getHealthStatusWithPriority(
+                    infoService, tabletInfo.getCluster(),
+                    partition.getVisibleVersion(),
+                    partition.getVisibleVersionHash(),
+                    tbl.getPartitionInfo().getReplicationNum(partition.getId()));
+
+            tabletInfo.setTabletStatus(statusPair.first);
+            if (statusPair.first == TabletStatus.HEALTHY && tabletInfo.getType() == TabletInfo.Type.REPAIR) {
+                throw new SchedException(Status.UNRECOVERABLE, ""tablet is healthy"");
+            } else if (statusPair.first != TabletStatus.HEALTHY
+                    && tabletInfo.getType() == TabletInfo.Type.BALANCE) {
+                tabletInfo.releaseResource(this);
+                // we select an unhealthy tablet to do balance, which is not right.
+                // so here we change it to a REPAIR task, and also reset its priority
+                tabletInfo.setType(TabletInfo.Type.REPAIR);
+                tabletInfo.setOrigPriority(statusPair.second);
+            }
+
+            // we do not concern priority here.
+            // once we take the tablet out of priority queue, priority is meaningless.
+            tabletInfo.setTablet(tablet);
+            tabletInfo.setVersionInfo(partition.getVisibleVersion(), partition.getVisibleVersionHash(),
+                    partition.getCommittedVersion(), partition.getCommittedVersionHash());
+            tabletInfo.setSchemaHash(tbl.getSchemaHashByIndexId(idx.getId()));
+            tabletInfo.setStorageMedium(tbl.getPartitionInfo().getDataProperty(partition.getId()).getStorageMedium());
+
+            handleTabletByTypeAndStatus(statusPair.first, tabletInfo, batchTask);
+        } finally {
+            db.writeUnlock();
+        }
+    }
+
+    private void handleTabletByTypeAndStatus(TabletStatus status, TabletInfo tabletInfo, AgentBatchTask batchTask)
+            throws SchedException {
+        if (tabletInfo.getType() == Type.REPAIR) {
+            switch (status) {
+                case REPLICA_MISSING:
+                    handleReplicaMissing(tabletInfo, batchTask);
+                    break;
+                case VERSION_INCOMPLETE:
+                    handleReplicaVersionIncomplete(tabletInfo, batchTask);
+                    break;
+                case REDUNDANT:
+                    handleRedundantReplica(tabletInfo);
+                    break;
+                case REPLICA_MISSING_IN_CLUSTER:
+                    handleReplicaClusterMigration(tabletInfo, batchTask);
+                    break;
+                default:
+                    break;
+            }
+        } else {
+            // balance
+            doBalance(tabletInfo, batchTask);
+        }
+    }
+
+    /*
+     * Replica is missing, which means there is no enough alive replicas.
+     * So we need to find a destination backend to clone a new replica as possible as we can.
+     * 1. find an available path in a backend as destination:
+     *      1. backend need to be alive.
+     *      2. backend of existing replicas should be excluded.
+     *      3. backend has available slot for clone.
+     *      4. replica can fit in the path (consider the threshold of disk capacity and usage percent).
+     *      5. try to find a path with lowest load score.
+     * 2. find an appropriate source replica:
+     *      1. source replica should be healthy
+     *      2. backend of source replica has available slot for clone.
+     *      
+     * 3. send clone task to destination backend
+     */
+    private void handleReplicaMissing(TabletInfo tabletInfo, AgentBatchTask batchTask) throws SchedException {
+        stat.counterReplicaMissingErr.incrementAndGet();
+        // find an available dest backend and path
+        RootPathLoadStatistic destPath = chooseAvailableDestPath(tabletInfo);
+        Preconditions.checkNotNull(destPath);
+        tabletInfo.setDestination(destPath.getBeId(), destPath.getPathHash());
+
+        // choose a source replica for cloning from
+        tabletInfo.chooseSrcReplica(backendsWorkingSlots);
+
+        // create clone task
+        batchTask.addTask(tabletInfo.createCloneReplicaAndTask());
+    }
+
+    /*
+     * Replica version is incomplete, which means this replica is missing some version,
+     * and need to be cloned from a healthy replica, in-place.
+     * 
+     * 1. find the incomplete replica as destination replica
+     * 2. find a healthy replica as source replica
+     * 3. send clone task
+     */
+    private void handleReplicaVersionIncomplete(TabletInfo tabletInfo, AgentBatchTask batchTask)
+            throws SchedException {
+        stat.counterReplicaVersionMissingErr.incrementAndGet();
+        ClusterLoadStatistic statistic = statisticMap.get(tabletInfo.getCluster());
+        if (statistic == null) {
+            throw new SchedException(Status.UNRECOVERABLE, ""cluster does not exist"");
+        }
+
+        tabletInfo.chooseDestReplicaForVersionIncomplete(backendsWorkingSlots);
+        tabletInfo.chooseSrcReplicaForVersionIncomplete(backendsWorkingSlots);
+
+        // create clone task
+        batchTask.addTask(tabletInfo.createCloneReplicaAndTask());
+    }
+
+    /*
+     *  replica is redundant, which means there are more replicas than we expected, which need to be dropped.
+     *  we just drop one redundant replica at a time, for safety reason.
+     *  choosing a replica to drop base on following priority:
+     *  1. backend has been dropped
+     *  2. backend is not available
+     *  3. replica's state is CLONE
+     *  4. replica's last failed version > 0
+     *  5. replica with lower version
+     *  6. replica not in right cluster
+     *  7. replica in higher load backend
+     */
+    private void handleRedundantReplica(TabletInfo tabletInfo) throws SchedException {
+        stat.counterReplicaRedundantErr.incrementAndGet();
+        if (deleteBackendDropped(tabletInfo)
+                || deleteBackendUnavailable(tabletInfo)
+                || deleteCloneReplica(tabletInfo)
+                || deleteReplicaWithFailedVersion(tabletInfo)
+                || deleteReplicaWithLowerVersion(tabletInfo)
+                || deleteReplicaNotInCluster(tabletInfo)
+                || deleteReplicaOnHighLoadBackend(tabletInfo)) {
+            // if we delete at least one redundant replica, we still throw a SchedException with status FINISHED
+            // to remove this tablet from the pendingTablets(consider it as finished)
+            throw new SchedException(Status.FINISHED, ""redundant replica is deleted"");
+        }
+        throw new SchedException(Status.SCHEDULE_FAILED, ""unable to delete any redundant replicas"");
+    }
+
+    private boolean deleteBackendDropped(TabletInfo tabletInfo) {
+        for (Replica replica : tabletInfo.getReplicas()) {
+            long beId = replica.getBackendId();
+            if (infoService.getBackend(beId) == null) {
+                deleteReplicaInternal(tabletInfo, replica, ""backend dropped"");
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private boolean deleteBackendUnavailable(TabletInfo tabletInfo) {
+        for (Replica replica : tabletInfo.getReplicas()) {
+            Backend be = infoService.getBackend(replica.getBackendId());
+            if (be == null) {
+                // this case should be handled in deleteBackendDropped()
+                continue;
+            }
+            if (!be.isAvailable()) {
+                deleteReplicaInternal(tabletInfo, replica, ""backend unavailable"");
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private boolean deleteCloneReplica(TabletInfo tabletInfo) {
+        for (Replica replica : tabletInfo.getReplicas()) {
+            if (replica.getState() == ReplicaState.CLONE) {
+                deleteReplicaInternal(tabletInfo, replica, ""clone state"");
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private boolean deleteReplicaWithFailedVersion(TabletInfo tabletInfo) {
+        for (Replica replica : tabletInfo.getReplicas()) {
+            if (replica.getLastFailedVersion() > 0) {
+                deleteReplicaInternal(tabletInfo, replica, ""version incomplete"");
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private boolean deleteReplicaWithLowerVersion(TabletInfo tabletInfo) {
+        for (Replica replica : tabletInfo.getReplicas()) {
+            if (!replica.checkVersionCatchUp(tabletInfo.getCommittedVersion(), tabletInfo.getCommittedVersionHash())) {
+                deleteReplicaInternal(tabletInfo, replica, ""lower version"");
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private boolean deleteReplicaNotInCluster(TabletInfo tabletInfo) {
+        for (Replica replica : tabletInfo.getReplicas()) {
+            Backend be = infoService.getBackend(replica.getBackendId());
+            if (be == null) {
+                // this case should be handled in deleteBackendDropped()
+                continue;
+            }
+            if (!be.getOwnerClusterName().equals(tabletInfo.getCluster())) {
+                deleteReplicaInternal(tabletInfo, replica, ""not in cluster"");
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private boolean deleteReplicaOnHighLoadBackend(TabletInfo tabletInfo) {
+        ClusterLoadStatistic statistic = statisticMap.get(tabletInfo.getCluster());
+        if (statistic == null) {
+            return false;
+        }
+        
+        Replica chosenReplica = null;
+        double maxScore = 0;
+        for (Replica replica : tabletInfo.getReplicas()) {
+            BackendLoadStatistic beStatistic = statistic.getBackendLoadStatistic(replica.getBackendId());
+            if (beStatistic == null) {
+                continue;
+            }
+            if (beStatistic.getLoadScore() > maxScore) {
+                maxScore = beStatistic.getLoadScore();
+                chosenReplica = replica;
+            }
+        }
+
+        if (chosenReplica != null) {
+            deleteReplicaInternal(tabletInfo, chosenReplica, ""high load"");
+            return true;
+        }
+        return false;
+    }
+
+    private void deleteReplicaInternal(TabletInfo tabletInfo, Replica replica, String reason) {
+        // delete this replica from catalog.
+        // it will also delete replica from tablet inverted index.
+        tabletInfo.deleteReplica(replica);
+
+        // TODO(cmy): this should be removed after I finish modifying alter job logic
+        // Catalog.getInstance().handleJobsWhenDeleteReplica(tabletInfo.getTblId(), tabletInfo.getPartitionId(),
+        //                                                   tabletInfo.getIndexId(), tabletInfo.getTabletId(),
+        //                                                   replica.getId(), replica.getBackendId());
+
+        // write edit log
+        ReplicaPersistInfo info = ReplicaPersistInfo.createForDelete(tabletInfo.getDbId(),
+                                                                     tabletInfo.getTblId(),
+                                                                     tabletInfo.getPartitionId(),
+                                                                     tabletInfo.getIndexId(),
+                                                                     tabletInfo.getTabletId(),
+                                                                     replica.getBackendId());
+
+        Catalog.getInstance().getEditLog().logDeleteReplica(info);
+
+        LOG.info(""delete replica. tablet id: {}, backend id: {}. reason: {}"",
+                 tabletInfo.getTabletId(), replica.getBackendId(), reason);
+    }
+
+    /*
+     * Cluster migration, which means the tablet has enough healthy replicas,
+     * but some replicas are not in right cluster.
+     * It is just same as 'replica missing'.
+     * 
+     * after clone finished, the replica in wrong cluster will be treated as redundant, and will be deleted soon.
+     */
+    private void handleReplicaClusterMigration(TabletInfo tabletInfo, AgentBatchTask batchTask)
+            throws SchedException {
+        stat.counterReplicaMissingInClusterErr.incrementAndGet();
+        handleReplicaMissing(tabletInfo, batchTask);
+    }
+
+    /*
+     * Try to select some alternative tablets for balance. Add them to pendingTablets with priority LOW,
+     * and waiting to be scheduled.
+     */
+    private void selectTabletsForBalance() {
+        LoadBalancer loadBalancer = new LoadBalancer(statisticMap);
+        List<TabletInfo> alternativeTablets = loadBalancer.selectAlternativeTablets();
+        for (TabletInfo tabletInfo : alternativeTablets) {
+            addTablet(tabletInfo, false);
+        }
+    }
+
+    /*
+     * Try to create a balance task for a tablet.
+     */
+    private void doBalance(TabletInfo tabletInfo, AgentBatchTask batchTask) throws SchedException {
+        stat.counterBalanceSchedule.incrementAndGet();
+        LoadBalancer loadBalancer = new LoadBalancer(statisticMap);
+        loadBalancer.createBalanceTask(tabletInfo, backendsWorkingSlots, batchTask);
+    }
+
+    // choose a path on a backend which is fit for the tablet
+    private RootPathLoadStatistic chooseAvailableDestPath(TabletInfo tabletInfo) throws SchedException {
+        ClusterLoadStatistic statistic = statisticMap.get(tabletInfo.getCluster());
+        if (statistic == null) {
+            throw new SchedException(Status.UNRECOVERABLE, ""cluster does not exist"");
+        }
+        List<BackendLoadStatistic> beStatistics = statistic.getBeLoadStatistics();
+
+        // get all available paths which this tablet can fit in.
+        // beStatistics is sorted by load score in ascend order, so select from first to last.
+        List<RootPathLoadStatistic> allFitPaths = Lists.newArrayList();
+        for (int i = 0; i < beStatistics.size(); i++) {
+            BackendLoadStatistic bes = beStatistics.get(i);
+            // exclude BE which already has replica of this tablet
+            if (tabletInfo.containsBE(bes.getBeId())) {
+                continue;
+            }
+
+            List<RootPathLoadStatistic> resultPaths = Lists.newArrayList();
+            BalanceStatus st = bes.isFit(tabletInfo.getTabletSize(), resultPaths, true /* is supplement */);
+            if (!st.ok()) {
+                LOG.debug(""unable to find path for supplementing tablet: {}. {}"", tabletInfo, st);
+                continue;
+            }
+
+            Preconditions.checkState(resultPaths.size() == 1);
+            allFitPaths.add(resultPaths.get(0));
+        }
+
+        if (allFitPaths.isEmpty()) {
+            throw new SchedException(Status.SCHEDULE_FAILED, ""unable to find dest path for new replica"");
+        }
+
+        // all fit paths has already been sorted by load score in 'allFitPaths' in ascend order.
+        // just get first available path.
+        // we try to find a path with specified media type, if not find, arbitrarily use one.
+        for (RootPathLoadStatistic rootPathLoadStatistic : allFitPaths) {
+            if (rootPathLoadStatistic.getStorageMedium() != tabletInfo.getStorageMedium()) {
+                continue;
+            }
+
+            PathSlot slot = backendsWorkingSlots.get(rootPathLoadStatistic.getBeId());
+            if (slot == null) {
+                LOG.debug(""backend {} does not found when getting slots"", rootPathLoadStatistic.getBeId());
+                continue;
+            }
+
+            if (slot.takeSlot(rootPathLoadStatistic.getPathHash()) != -1) {
+                return rootPathLoadStatistic;
+            }
+        }
+
+        // no root path with specified media type is found, get arbitrary one.
+        for (RootPathLoadStatistic rootPathLoadStatistic : allFitPaths) {
+            PathSlot slot = backendsWorkingSlots.get(rootPathLoadStatistic.getBeId());
+            if (slot == null) {
+                LOG.debug(""backend {} does not found when getting slots"", rootPathLoadStatistic.getBeId());
+                continue;
+            }
+
+            if (slot.takeSlot(rootPathLoadStatistic.getPathHash()) != -1) {
+                return rootPathLoadStatistic;
+            }
+        }
+        
+        throw new SchedException(Status.SCHEDULE_FAILED, ""unable to find dest path which can be fit in"");
+    }
+
+    /*
+     * For some reason, a tablet info failed to be scheduled this time,
+     * So we dynamically change its priority and add back to queue, waiting for next round.
+     */
+    private void dynamicAdjustPrioAndAddBackToPendingTablets(TabletInfo tabletInfo, String message) {
+        Preconditions.checkState(tabletInfo.getState() == TabletInfo.State.PENDING);
+        tabletInfo.adjustPriority(stat);
+        addTablet(tabletInfo, true /* force */);
+    }
+
+    private synchronized void removeTabletInfo(TabletInfo tabletInfo, TabletInfo.State state, String reason) {
+        tabletInfo.setState(state);
+        tabletInfo.releaseResource(this);
+        tabletInfo.setFinishedTime(System.currentTimeMillis());
+        runningTablets.remove(tabletInfo.getTabletId());
+        allTabletIds.remove(tabletInfo.getTabletId());
+        schedHistory.add(tabletInfo);
+        LOG.info(""remove the tablet {}. because: {}"", tabletInfo.getTabletId(), reason);
+    }
+
+    // get next batch of tablets from queue.
+    private synchronized List<TabletInfo> getNextTabletInfoBatch() {
+        List<TabletInfo> list = Lists.newArrayList();
+        int count = Math.max(MIN_BATCH_NUM, getCurrentAvailableSlotNum());
+        while (count > 0) {
+            TabletInfo tablet = pendingTablets.poll();
+            if (tablet == null) {
+                // no more tablets
+                break;
+            }
+            list.add(tablet);
+            count--;
+        }
+        return list;
+    }
+
+    private int getCurrentAvailableSlotNum() {
+        int total = 0;
+        for (PathSlot pathSlot : backendsWorkingSlots.values()) {
+            total += pathSlot.getTotalAvailSlotNum();
+        }
+        return total;
+    }
+
+    /*
+     * return true if we want to remove the clone task from AgentTaskQueu
+     */
+    public boolean finishCloneTask(CloneTask cloneTask, TFinishTaskRequest request) {
+        long tabletId = cloneTask.getTabletId();
+        TabletInfo tabletInfo = takeRunningTablets(tabletId);
+        if (tabletInfo == null) {
+            LOG.warn(""tablet info does not exist: {}"", tabletId);
+            // tablet does not exist, no need to keep task.
+            return true;
+        }
+        Preconditions.checkState(tabletInfo.getState() == TabletInfo.State.RUNNING);
+        try {
+            tabletInfo.finishCloneTask(cloneTask, request);
+        } catch (SchedException e) {
+            tabletInfo.increaseFailedRunningCounter();
+            tabletInfo.setErrMsg(e.getMessage());
+            if (e.getStatus() == Status.RUNNING_FAILED) {
+                stat.counterCloneTaskFailed.incrementAndGet();
+                addToRunningTablets(tabletInfo);
+                return false;
+            } else {
+                Preconditions.checkState(e.getStatus() == Status.UNRECOVERABLE, e.getStatus());
+                // unrecoverable
+                stat.counterTabletScheduledDiscard.incrementAndGet();
+                removeTabletInfo(tabletInfo, TabletInfo.State.CANCELLED, e.getMessage());
+                return true;
+            }
+        }
+
+        Preconditions.checkState(tabletInfo.getState() == TabletInfo.State.FINISHED);
+        stat.counterCloneTaskSucceeded.incrementAndGet();
+        gatherStatistics(tabletInfo);
+        removeTabletInfo(tabletInfo, TabletInfo.State.FINISHED, ""finished"");
+        return true;
+    }
+
+    /*
+     * Gather the running statistic of the task.
+     * It will be evaluated for future strategy.  
+     * This should only be called when the tablet is down with state FINISHED.
+     */
+    private void gatherStatistics(TabletInfo tabletInfo) {
+        if (tabletInfo.getCopySize() > 0 && tabletInfo.getCopyTimeMs() > 0) {
+            if (tabletInfo.getSrcBackendId() != -1 && tabletInfo.getSrcPathHash() != -1) {
+                PathSlot pathSlot = backendsWorkingSlots.get(tabletInfo.getSrcBackendId());
+                if (pathSlot != null) {
+                    pathSlot.updateStatistic(tabletInfo.getSrcPathHash(), tabletInfo.getCopySize(),
+                            tabletInfo.getCopyTimeMs());
+                }
+            }
+
+            if (tabletInfo.getDestBackendId() != -1 && tabletInfo.getDestPathHash() != -1) {
+                PathSlot pathSlot = backendsWorkingSlots.get(tabletInfo.getDestBackendId());
+                if (pathSlot != null) {
+                    pathSlot.updateStatistic(tabletInfo.getDestPathHash(), tabletInfo.getCopySize(),
+                            tabletInfo.getCopyTimeMs());
+                }
+            }
+        }
+
+        if (System.currentTimeMillis() - lastSlotAdjustTime < STAT_UPDATE_INTERVAL_MS) {
+            return;
+        }
+
+        // TODO(cmy): update the slot num base on statistic.
+        // need to find a better way to determine the slot number.
+
+        lastSlotAdjustTime = System.currentTimeMillis();
+    }
+
+    /*
+     * handle tablets which are running.
+     * We should finished the task if
+     * 1. Tablet is already healthy
+     * 2. Task is timeout.
+     * 
+     * But here we just handle the timeout case here. Let the 'finishCloneTask()' check if tablet is healthy.
+     * We guarantee that if tablet is in runningTablets, the 'finishCloneTask()' will finally be called,
+     * so no need to worry that running tablets will never end.
+     * This is also avoid nesting 'synchronized' and database lock.
+     *
+     * If task is timeout, remove the tablet.
+     */
+    public synchronized void handleRunningTablets() {
+        List<TabletInfo> timeoutTablets = Lists.newArrayList();
+        runningTablets.values().stream().filter(t -> t.isTimeout()).forEach(t -> {
+            timeoutTablets.add(t);
+        });
+        
+        timeoutTablets.stream().forEach(t -> {
+            removeTabletInfo(t, TabletInfo.State.TIMEOUT, ""timeout"");
+            stat.counterCloneTaskTimeout.incrementAndGet();
+        });
+    }
+
+    public List<List<String>> getPendingTabletsInfo(int limit) {
+        List<TabletInfo> tabletInfos = getCopiedTablets(pendingTablets, limit);
+        return collectTabletInfo(tabletInfos);
+    }
+
+    public List<List<String>> getRunningTabletsInfo(int limit) {
+        List<TabletInfo> tabletInfos = getCopiedTablets(runningTablets.values(), limit);
+        return collectTabletInfo(tabletInfos);
+    }
+
+    public List<List<String>> getHistoryTabletsInfo(int limit) {
+        List<TabletInfo> tabletInfos = getCopiedTablets(schedHistory, limit);
+        return collectTabletInfo(tabletInfos);
+    }
+
+    private List<List<String>> collectTabletInfo(List<TabletInfo> tabletInfos) {
+        List<List<String>> result = Lists.newArrayList();
+        tabletInfos.stream().forEach(t -> {
+            result.add(t.getBrief());
+        });
+        return result;
+    }
+
+    private synchronized List<TabletInfo> getCopiedTablets(Collection<TabletInfo> source, int limit) {
+        List<TabletInfo> tabletInfos = Lists.newArrayList();
+        source.stream().limit(limit).forEach(t -> {
+            tabletInfos.add(t);
+        });
+        return tabletInfos;
+    }
+
+    public synchronized int getPendingNum() {
+        return pendingTablets.size();
+    }
+
+    public synchronized int getRunningNum() {
+        return runningTablets.size();
+    }
+
+    public synchronized int getHistoryNum() {
+        return schedHistory.size();
+    }
+
+    /*
+     * PathSlot keeps track of slot num per path of a Backend.
+     * Each path on a Backend has several slot.
+     * If a path's available slot num because 0, no task should be assigned to this path.
+     */
+    public class PathSlot {","[{'comment': 'public static class', 'commenter': 'imay'}]"
336,fe/src/main/java/org/apache/doris/clone/TabletScheduler.java,"@@ -0,0 +1,1186 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.clone;
+
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.catalog.Database;
+import org.apache.doris.catalog.MaterializedIndex;
+import org.apache.doris.catalog.OlapTable;
+import org.apache.doris.catalog.OlapTable.OlapTableState;
+import org.apache.doris.catalog.Partition;
+import org.apache.doris.catalog.Replica;
+import org.apache.doris.catalog.Replica.ReplicaState;
+import org.apache.doris.catalog.Tablet;
+import org.apache.doris.catalog.Tablet.TabletStatus;
+import org.apache.doris.catalog.TabletInvertedIndex;
+import org.apache.doris.clone.SchedException.Status;
+import org.apache.doris.clone.TabletInfo.Priority;
+import org.apache.doris.clone.TabletInfo.Type;
+import org.apache.doris.common.Config;
+import org.apache.doris.common.Pair;
+import org.apache.doris.common.util.Daemon;
+import org.apache.doris.persist.ReplicaPersistInfo;
+import org.apache.doris.system.Backend;
+import org.apache.doris.system.SystemInfoService;
+import org.apache.doris.task.AgentBatchTask;
+import org.apache.doris.task.AgentTask;
+import org.apache.doris.task.AgentTaskExecutor;
+import org.apache.doris.task.AgentTaskQueue;
+import org.apache.doris.task.CloneTask;
+import org.apache.doris.thrift.TFinishTaskRequest;
+
+import com.google.common.base.Preconditions;
+import com.google.common.collect.EvictingQueue;
+import com.google.common.collect.ImmutableMap;
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+import com.google.common.collect.Sets;
+
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.Collection;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.PriorityQueue;
+import java.util.Queue;
+import java.util.Set;
+import java.util.stream.Collectors;
+
+/*
+ * TabletScheduler saved the tablets produced by TabletChecker and try to schedule them.
+ * It also try to balance the cluster load.
+ * 
+ * We are expecting an efficient way to recovery the entire cluster and make it balanced.
+ * Case 1:
+ *  A Backend is down. All tablets which has replica on this BE should be repaired as soon as possible.
+ *  
+ * Case 1.1:
+ *  As Backend is down, some tables should be repaired in high priority. So the clone task should be able
+ *  to preempted.
+ *  
+ * Case 2:
+ *  A new Backend is added to the cluster. Replicas should be transfer to that host to balance the cluster load.
+ */
+public class TabletScheduler extends Daemon {
+    private static final Logger LOG = LogManager.getLogger(TabletScheduler.class);
+
+    // handle at most BATCH_NUM tablets in one loop
+    private static final int MIN_BATCH_NUM = 10;
+
+    // the minimum interval of updating cluster statistics and priority of tablet info
+    private static final long STAT_UPDATE_INTERVAL_MS = 60 * 1000; // 1min
+
+    private static final long SCHEDULE_INTERVAL_MS = 5000; // 5s
+
+    public static final int BALANCE_SLOT_NUM_FOR_PATH = 2;
+
+    /*
+     * Tablet is added to pendingTablets as well it's id in allTabletIds.
+     * TabletScheduler will take tablet from pendingTablets but will not remove it's id from allTabletIds when
+     * handling a tablet.
+     * Tablet' id can only be removed after the clone task is done(timeout, cancelled or finished).
+     * So if a tablet's id is still in allTabletIds, TabletChecker can not add tablet to TabletScheduler.
+     * 
+     * pendingTablets + runningTablets = allTabletIds
+     * 
+     * pendingTablets, allTabletIds, runningTablets and schedHistory are protected by 'synchronized' 
+     */
+    private PriorityQueue<TabletInfo> pendingTablets = new PriorityQueue<>();
+    private Set<Long> allTabletIds = Sets.newHashSet();
+    // contains all tabletInfos which state are RUNNING
+    private Map<Long, TabletInfo> runningTablets = Maps.newHashMap();
+    // save the latest 1000 scheduled tablet info
+    private Queue<TabletInfo> schedHistory = EvictingQueue.create(1000);
+
+    // be id -> #working slots
+    private Map<Long, PathSlot> backendsWorkingSlots = Maps.newConcurrentMap();
+    // cluster name -> load statistic
+    private Map<String, ClusterLoadStatistic> statisticMap = Maps.newConcurrentMap();
+    private long lastStatUpdateTime = 0;
+    
+    private long lastSlotAdjustTime = 0;
+
+    private Catalog catalog;
+    private SystemInfoService infoService;
+    private TabletInvertedIndex invertedIndex;
+    private TabletSchedulerStat stat;
+
+    public TabletScheduler(Catalog catalog, SystemInfoService infoService, TabletInvertedIndex invertedIndex,
+            TabletSchedulerStat stat) {
+        super(""tablet scheduler"", SCHEDULE_INTERVAL_MS);
+        this.catalog = catalog;
+        this.infoService = infoService;
+        this.invertedIndex = invertedIndex;
+        this.stat = stat;
+    }
+
+    public TabletSchedulerStat getStat() {
+        return stat;
+    }
+
+    /*
+     * update working slots at the beginning of each round
+     */
+    private boolean updateWorkingSlots() {
+        ImmutableMap<Long, Backend> backends = infoService.getBackendsInCluster(null);
+        for (Backend backend : backends.values()) {
+            if (!backend.hasPathHash() && backend.isAlive()) {
+                // when upgrading, backend may not get path info yet. so return false and wait for next round.
+                // and we should check if backend is alive. If backend is dead when upgrading, this backend
+                // will never report its path hash, and tablet scheduler is blocked.
+                LOG.info(""not all backends have path info"");
+                return false;
+            }
+        }
+
+        // update exist backends
+        Set<Long> deletedBeIds = Sets.newHashSet();
+        for (Long beId : backendsWorkingSlots.keySet()) {
+            if (backends.containsKey(beId)) {
+                List<Long> pathHashes = backends.get(beId).getDisks().values().stream().map(v -> v.getPathHash()).collect(Collectors.toList());
+                backendsWorkingSlots.get(beId).updatePaths(pathHashes);
+            } else {
+                deletedBeIds.add(beId);
+            }
+        }
+
+        // delete non-exist backends
+        for (Long beId : deletedBeIds) {
+            backendsWorkingSlots.remove(beId);
+            LOG.info(""delete non exist backend: {}"", beId);
+        }
+
+        // add new backends
+        for (Backend be : backends.values()) {
+            if (!backendsWorkingSlots.containsKey(be.getId())) {
+                List<Long> pathHashes = be.getDisks().values().stream().map(v -> v.getPathHash()).collect(Collectors.toList());
+                PathSlot slot = new PathSlot(pathHashes, Config.schedule_slot_num_per_path);
+                backendsWorkingSlots.put(be.getId(), slot);
+                LOG.info(""add new backend {} with slots num: {}"", be.getId(), be.getDisks().size());
+            }
+        }
+
+        return true;
+    }
+
+    public Map<Long, PathSlot> getBackendsWorkingSlots() {
+        return backendsWorkingSlots;
+    }
+
+    /*
+     * add a ready-to-be-scheduled tablet to pendingTablets, if it has not being added before.
+     * if force is true, do not check if tablet is already added before.
+     */
+    public synchronized boolean addTablet(TabletInfo tablet, boolean force) {
+        if (!force && containsTablet(tablet.getTabletId())) {
+            return false;
+        }
+        allTabletIds.add(tablet.getTabletId());
+        pendingTablets.offer(tablet);
+        return true;
+    }
+
+    public synchronized boolean containsTablet(long tabletId) {
+        return allTabletIds.contains(tabletId);
+    }
+
+    /*
+     * Iterate current tablets, change their priority if necessary.
+     */
+    public synchronized void changePriorityOfTablets(long dbId, long tblId, List<Long> partitionIds) {
+        PriorityQueue<TabletInfo> newPendingTablets = new PriorityQueue<>();
+        for (TabletInfo tabletInfo : pendingTablets) {
+            if (tabletInfo.getDbId() == dbId && tabletInfo.getTblId() == tblId
+                    && partitionIds.contains(tabletInfo.getPartitionId())) {
+                tabletInfo.setOrigPriority(Priority.VERY_HIGH);
+            }
+            newPendingTablets.add(tabletInfo);
+        }
+        pendingTablets = newPendingTablets;
+    }
+
+    /*
+     * TabletScheduler will run as a daemon thread at a very short interval(default 5 sec)
+     * Firstly, it will try to update cluster load statistic and check if priority need to be adjuested.
+     * Than, it will schedule the tablets in pendingTablets.
+     * Thirdly, it will check the current running tasks.
+     * Finally, it try to balance the cluster if possible.
+     * 
+     * Schedule rules:
+     * 1. tablet with higher priority will be scheduled first.
+     * 2. high priority should be downgraded if it fails to be schedule too many times.
+     * 3. priority may be upgraded if it is not being schedule for a long time.
+     * 4. every pending task should has a max scheduled time, if schedule fails too many times, if should be removed.
+     * 5. every running task should has a timeout, to avoid running forever.
+     * 6. every running task should also has a max failure time, if clone task fails too many times, if should be removed.
+     *
+     */
+    @Override
+    protected void runOneCycle() {
+        if (!updateWorkingSlots()) {
+            return;
+        }
+
+        updateClusterLoadStatisticsAndPriorityIfNecessary();
+
+        schedulePendingTablets();
+
+        handleRunningTablets();
+
+        selectTabletsForBalance();
+
+        stat.counterTabletScheduleRound.incrementAndGet();
+    }
+
+
+    private void updateClusterLoadStatisticsAndPriorityIfNecessary() {
+        if (System.currentTimeMillis() - lastStatUpdateTime < STAT_UPDATE_INTERVAL_MS) {
+            return;
+        }
+
+        updateClusterLoadStatistic();
+        adjustPriorities();
+
+        lastStatUpdateTime = System.currentTimeMillis();
+    }
+
+    /*
+     * Here is the only place we update the cluster load statistic info.
+     * We will not update this info dynamically along with the clone job's running.
+     * Although it will cause a little bit inaccurate, but is within a controllable range,
+     * because we already limit the total number of running clone jobs in cluster by 'backend slots'
+     */
+    private void updateClusterLoadStatistic() {
+        statisticMap.clear();
+        List<String> clusterNames = infoService.getClusterNames();
+        for (String clusterName : clusterNames) {
+            ClusterLoadStatistic clusterLoadStatistic = new ClusterLoadStatistic(clusterName, catalog,
+                    infoService, invertedIndex);
+            clusterLoadStatistic.init();
+            statisticMap.put(clusterName, clusterLoadStatistic);
+            LOG.info(""update cluster {} load statistic:\n {}"", clusterName, clusterLoadStatistic.getBrief());
+        }
+    }
+
+    public Map<String, ClusterLoadStatistic> getStatisticMap() {
+        return statisticMap;
+    }
+
+    /*
+     * adjust priorities of all tablet infos
+     */
+    private synchronized void adjustPriorities() {
+        int size = pendingTablets.size();
+        int changedNum = 0;
+        TabletInfo tabletInfo = null;
+        for (int i = 0; i < size; i++) {
+            tabletInfo = pendingTablets.poll();
+            if (tabletInfo == null) {
+                break;
+            }
+
+            if (tabletInfo.adjustPriority(stat)) {
+                changedNum++;
+            }
+            pendingTablets.add(tabletInfo);
+        }
+
+        LOG.info(""adjust priority for all tablets. changed: {}, total: {}"", changedNum, size);
+    }
+
+    /*
+     * get at most BATCH_NUM tablets from queue, and try to schedule them.
+     * After handle, the tablet info should be
+     * 1. in runningTablets with state RUNNING, if being scheduled success.
+     * 2. or in schedHistory with state CANCELLING, if some unrecoverable error happens.
+     * 3. or in pendingTablets with state PENDING, if failed to be scheduled.
+     * 
+     * if in schedHistory, it should be removed from allTabletIds.
+     */
+    private void schedulePendingTablets() {
+        long start = System.currentTimeMillis();
+        List<TabletInfo> currentBatch = getNextTabletInfoBatch();
+        LOG.debug(""get {} tablets to schedule"", currentBatch.size());
+
+        AgentBatchTask batchTask = new AgentBatchTask();
+        for (TabletInfo tabletInfo : currentBatch) {
+            try {
+                scheduleTablet(tabletInfo, batchTask);
+            } catch (SchedException e) {
+                tabletInfo.increaseFailedSchedCounter();
+                tabletInfo.setErrMsg(e.getMessage());
+
+                if (e.getStatus() == Status.SCHEDULE_FAILED) {
+                    // we must release resource it current hold, and be scheduled again
+                    tabletInfo.releaseResource(this);
+                    // adjust priority to avoid some higher priority always be the first in pendingTablets
+                    stat.counterTabletScheduledFailed.incrementAndGet();
+                    dynamicAdjustPrioAndAddBackToPendingTablets(tabletInfo, e.getMessage());
+                } else if (e.getStatus() == Status.FINISHED) {
+                    // schedule redundant tablet will throw this exception
+                    stat.counterTabletScheduledSucceeded.incrementAndGet();
+                    removeTabletInfo(tabletInfo, TabletInfo.State.FINISHED, e.getMessage());
+                } else {
+                    Preconditions.checkState(e.getStatus() == Status.UNRECOVERABLE, e.getStatus());
+                    // discard
+                    stat.counterTabletScheduledDiscard.incrementAndGet();
+                    removeTabletInfo(tabletInfo, TabletInfo.State.CANCELLED, e.getMessage());
+                }
+                continue;
+            }
+
+            Preconditions.checkState(tabletInfo.getState() == TabletInfo.State.RUNNING);
+            stat.counterTabletScheduledSucceeded.incrementAndGet();
+            addToRunningTablets(tabletInfo);
+        }
+
+        // must send task after adding tablet info to runningTablets.
+        for (AgentTask task : batchTask.getAllTasks()) {
+            if (AgentTaskQueue.addTask(task)) {
+                stat.counterCloneTask.incrementAndGet();
+            }
+            LOG.info(""add clone task to agent task queue: {}"", task);
+        }
+
+        // send task immediately
+        AgentTaskExecutor.submit(batchTask);
+
+        long cost = System.currentTimeMillis() - start;
+        stat.counterTabletScheduleCostMs.addAndGet(cost);
+    }
+
+    private synchronized void addToRunningTablets(TabletInfo tabletInfo) {
+        runningTablets.put(tabletInfo.getTabletId(), tabletInfo);
+    }
+
+    /*
+     * we take the tablet out of the runningTablets and than handle it,
+     * avoid other threads see it.
+     * Whoever takes this tablet, make sure to put it to the schedHistory or back to runningTablets.
+     */
+    private synchronized TabletInfo takeRunningTablets(long tabletId) {
+        return runningTablets.remove(tabletId);
+    }
+
+    /*
+     * Try to schedule a single tablet.
+     */
+    private void scheduleTablet(TabletInfo tabletInfo, AgentBatchTask batchTask) throws SchedException {
+        LOG.debug(""schedule tablet: {}"", tabletInfo.getTabletId());
+        long currentTime = System.currentTimeMillis();
+        tabletInfo.setLastSchedTime(currentTime);
+        tabletInfo.setLastVisitedTime(currentTime);
+        stat.counterTabletScheduled.incrementAndGet();
+
+        // check this tablet again
+        Database db = catalog.getDb(tabletInfo.getDbId());
+        if (db == null) {
+            throw new SchedException(Status.UNRECOVERABLE, ""db does not exist"");
+        }
+
+        Pair<TabletStatus, TabletInfo.Priority> statusPair = null;
+        db.writeLock();
+        try {
+            OlapTable tbl = (OlapTable) db.getTable(tabletInfo.getTblId());
+            if (tbl == null) {
+                throw new SchedException(Status.UNRECOVERABLE, ""tbl does not exist"");
+            }
+
+            // we may add a tablet of a NOT NORMAL table during balance, which should be blocked
+            if (tbl.getState() != OlapTableState.NORMAL) {
+                throw new SchedException(Status.UNRECOVERABLE, ""tbl's state is not normal: "" + tbl.getState());
+            }
+
+            Partition partition = tbl.getPartition(tabletInfo.getPartitionId());
+            if (partition == null) {
+                throw new SchedException(Status.UNRECOVERABLE, ""partition does not exist"");
+            }
+
+            MaterializedIndex idx = partition.getIndex(tabletInfo.getIndexId());
+            if (idx == null) {
+                throw new SchedException(Status.UNRECOVERABLE, ""index does not exist"");
+            }
+
+            Tablet tablet = idx.getTablet(tabletInfo.getTabletId());
+            Preconditions.checkNotNull(tablet);
+
+            statusPair = tablet.getHealthStatusWithPriority(
+                    infoService, tabletInfo.getCluster(),
+                    partition.getVisibleVersion(),
+                    partition.getVisibleVersionHash(),
+                    tbl.getPartitionInfo().getReplicationNum(partition.getId()));
+
+            tabletInfo.setTabletStatus(statusPair.first);
+            if (statusPair.first == TabletStatus.HEALTHY && tabletInfo.getType() == TabletInfo.Type.REPAIR) {
+                throw new SchedException(Status.UNRECOVERABLE, ""tablet is healthy"");
+            } else if (statusPair.first != TabletStatus.HEALTHY
+                    && tabletInfo.getType() == TabletInfo.Type.BALANCE) {
+                tabletInfo.releaseResource(this);
+                // we select an unhealthy tablet to do balance, which is not right.
+                // so here we change it to a REPAIR task, and also reset its priority
+                tabletInfo.setType(TabletInfo.Type.REPAIR);
+                tabletInfo.setOrigPriority(statusPair.second);
+            }
+
+            // we do not concern priority here.
+            // once we take the tablet out of priority queue, priority is meaningless.
+            tabletInfo.setTablet(tablet);
+            tabletInfo.setVersionInfo(partition.getVisibleVersion(), partition.getVisibleVersionHash(),
+                    partition.getCommittedVersion(), partition.getCommittedVersionHash());
+            tabletInfo.setSchemaHash(tbl.getSchemaHashByIndexId(idx.getId()));
+            tabletInfo.setStorageMedium(tbl.getPartitionInfo().getDataProperty(partition.getId()).getStorageMedium());
+
+            handleTabletByTypeAndStatus(statusPair.first, tabletInfo, batchTask);
+        } finally {
+            db.writeUnlock();
+        }
+    }
+
+    private void handleTabletByTypeAndStatus(TabletStatus status, TabletInfo tabletInfo, AgentBatchTask batchTask)
+            throws SchedException {
+        if (tabletInfo.getType() == Type.REPAIR) {
+            switch (status) {
+                case REPLICA_MISSING:
+                    handleReplicaMissing(tabletInfo, batchTask);
+                    break;
+                case VERSION_INCOMPLETE:
+                    handleReplicaVersionIncomplete(tabletInfo, batchTask);
+                    break;
+                case REDUNDANT:
+                    handleRedundantReplica(tabletInfo);
+                    break;
+                case REPLICA_MISSING_IN_CLUSTER:
+                    handleReplicaClusterMigration(tabletInfo, batchTask);
+                    break;
+                default:
+                    break;
+            }
+        } else {
+            // balance
+            doBalance(tabletInfo, batchTask);
+        }
+    }
+
+    /*
+     * Replica is missing, which means there is no enough alive replicas.
+     * So we need to find a destination backend to clone a new replica as possible as we can.
+     * 1. find an available path in a backend as destination:
+     *      1. backend need to be alive.
+     *      2. backend of existing replicas should be excluded.
+     *      3. backend has available slot for clone.
+     *      4. replica can fit in the path (consider the threshold of disk capacity and usage percent).
+     *      5. try to find a path with lowest load score.
+     * 2. find an appropriate source replica:
+     *      1. source replica should be healthy
+     *      2. backend of source replica has available slot for clone.
+     *      
+     * 3. send clone task to destination backend
+     */
+    private void handleReplicaMissing(TabletInfo tabletInfo, AgentBatchTask batchTask) throws SchedException {
+        stat.counterReplicaMissingErr.incrementAndGet();
+        // find an available dest backend and path
+        RootPathLoadStatistic destPath = chooseAvailableDestPath(tabletInfo);
+        Preconditions.checkNotNull(destPath);
+        tabletInfo.setDestination(destPath.getBeId(), destPath.getPathHash());
+
+        // choose a source replica for cloning from
+        tabletInfo.chooseSrcReplica(backendsWorkingSlots);
+
+        // create clone task
+        batchTask.addTask(tabletInfo.createCloneReplicaAndTask());
+    }
+
+    /*
+     * Replica version is incomplete, which means this replica is missing some version,
+     * and need to be cloned from a healthy replica, in-place.
+     * 
+     * 1. find the incomplete replica as destination replica
+     * 2. find a healthy replica as source replica
+     * 3. send clone task
+     */
+    private void handleReplicaVersionIncomplete(TabletInfo tabletInfo, AgentBatchTask batchTask)
+            throws SchedException {
+        stat.counterReplicaVersionMissingErr.incrementAndGet();
+        ClusterLoadStatistic statistic = statisticMap.get(tabletInfo.getCluster());
+        if (statistic == null) {
+            throw new SchedException(Status.UNRECOVERABLE, ""cluster does not exist"");
+        }
+
+        tabletInfo.chooseDestReplicaForVersionIncomplete(backendsWorkingSlots);
+        tabletInfo.chooseSrcReplicaForVersionIncomplete(backendsWorkingSlots);
+
+        // create clone task
+        batchTask.addTask(tabletInfo.createCloneReplicaAndTask());
+    }
+
+    /*
+     *  replica is redundant, which means there are more replicas than we expected, which need to be dropped.
+     *  we just drop one redundant replica at a time, for safety reason.
+     *  choosing a replica to drop base on following priority:
+     *  1. backend has been dropped
+     *  2. backend is not available
+     *  3. replica's state is CLONE
+     *  4. replica's last failed version > 0
+     *  5. replica with lower version
+     *  6. replica not in right cluster
+     *  7. replica in higher load backend
+     */
+    private void handleRedundantReplica(TabletInfo tabletInfo) throws SchedException {
+        stat.counterReplicaRedundantErr.incrementAndGet();
+        if (deleteBackendDropped(tabletInfo)
+                || deleteBackendUnavailable(tabletInfo)
+                || deleteCloneReplica(tabletInfo)
+                || deleteReplicaWithFailedVersion(tabletInfo)
+                || deleteReplicaWithLowerVersion(tabletInfo)
+                || deleteReplicaNotInCluster(tabletInfo)
+                || deleteReplicaOnHighLoadBackend(tabletInfo)) {
+            // if we delete at least one redundant replica, we still throw a SchedException with status FINISHED
+            // to remove this tablet from the pendingTablets(consider it as finished)
+            throw new SchedException(Status.FINISHED, ""redundant replica is deleted"");
+        }
+        throw new SchedException(Status.SCHEDULE_FAILED, ""unable to delete any redundant replicas"");
+    }
+
+    private boolean deleteBackendDropped(TabletInfo tabletInfo) {
+        for (Replica replica : tabletInfo.getReplicas()) {
+            long beId = replica.getBackendId();
+            if (infoService.getBackend(beId) == null) {
+                deleteReplicaInternal(tabletInfo, replica, ""backend dropped"");
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private boolean deleteBackendUnavailable(TabletInfo tabletInfo) {
+        for (Replica replica : tabletInfo.getReplicas()) {
+            Backend be = infoService.getBackend(replica.getBackendId());
+            if (be == null) {
+                // this case should be handled in deleteBackendDropped()
+                continue;
+            }
+            if (!be.isAvailable()) {
+                deleteReplicaInternal(tabletInfo, replica, ""backend unavailable"");
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private boolean deleteCloneReplica(TabletInfo tabletInfo) {
+        for (Replica replica : tabletInfo.getReplicas()) {
+            if (replica.getState() == ReplicaState.CLONE) {
+                deleteReplicaInternal(tabletInfo, replica, ""clone state"");
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private boolean deleteReplicaWithFailedVersion(TabletInfo tabletInfo) {
+        for (Replica replica : tabletInfo.getReplicas()) {
+            if (replica.getLastFailedVersion() > 0) {
+                deleteReplicaInternal(tabletInfo, replica, ""version incomplete"");
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private boolean deleteReplicaWithLowerVersion(TabletInfo tabletInfo) {
+        for (Replica replica : tabletInfo.getReplicas()) {
+            if (!replica.checkVersionCatchUp(tabletInfo.getCommittedVersion(), tabletInfo.getCommittedVersionHash())) {
+                deleteReplicaInternal(tabletInfo, replica, ""lower version"");
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private boolean deleteReplicaNotInCluster(TabletInfo tabletInfo) {
+        for (Replica replica : tabletInfo.getReplicas()) {
+            Backend be = infoService.getBackend(replica.getBackendId());
+            if (be == null) {
+                // this case should be handled in deleteBackendDropped()
+                continue;
+            }
+            if (!be.getOwnerClusterName().equals(tabletInfo.getCluster())) {
+                deleteReplicaInternal(tabletInfo, replica, ""not in cluster"");
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private boolean deleteReplicaOnHighLoadBackend(TabletInfo tabletInfo) {
+        ClusterLoadStatistic statistic = statisticMap.get(tabletInfo.getCluster());
+        if (statistic == null) {
+            return false;
+        }
+        
+        Replica chosenReplica = null;
+        double maxScore = 0;
+        for (Replica replica : tabletInfo.getReplicas()) {
+            BackendLoadStatistic beStatistic = statistic.getBackendLoadStatistic(replica.getBackendId());
+            if (beStatistic == null) {
+                continue;
+            }
+            if (beStatistic.getLoadScore() > maxScore) {
+                maxScore = beStatistic.getLoadScore();
+                chosenReplica = replica;
+            }
+        }
+
+        if (chosenReplica != null) {
+            deleteReplicaInternal(tabletInfo, chosenReplica, ""high load"");
+            return true;
+        }
+        return false;
+    }
+
+    private void deleteReplicaInternal(TabletInfo tabletInfo, Replica replica, String reason) {
+        // delete this replica from catalog.
+        // it will also delete replica from tablet inverted index.
+        tabletInfo.deleteReplica(replica);
+
+        // TODO(cmy): this should be removed after I finish modifying alter job logic
+        // Catalog.getInstance().handleJobsWhenDeleteReplica(tabletInfo.getTblId(), tabletInfo.getPartitionId(),
+        //                                                   tabletInfo.getIndexId(), tabletInfo.getTabletId(),
+        //                                                   replica.getId(), replica.getBackendId());
+
+        // write edit log
+        ReplicaPersistInfo info = ReplicaPersistInfo.createForDelete(tabletInfo.getDbId(),
+                                                                     tabletInfo.getTblId(),
+                                                                     tabletInfo.getPartitionId(),
+                                                                     tabletInfo.getIndexId(),
+                                                                     tabletInfo.getTabletId(),
+                                                                     replica.getBackendId());
+
+        Catalog.getInstance().getEditLog().logDeleteReplica(info);
+
+        LOG.info(""delete replica. tablet id: {}, backend id: {}. reason: {}"",
+                 tabletInfo.getTabletId(), replica.getBackendId(), reason);
+    }
+
+    /*
+     * Cluster migration, which means the tablet has enough healthy replicas,
+     * but some replicas are not in right cluster.
+     * It is just same as 'replica missing'.
+     * 
+     * after clone finished, the replica in wrong cluster will be treated as redundant, and will be deleted soon.
+     */
+    private void handleReplicaClusterMigration(TabletInfo tabletInfo, AgentBatchTask batchTask)
+            throws SchedException {
+        stat.counterReplicaMissingInClusterErr.incrementAndGet();
+        handleReplicaMissing(tabletInfo, batchTask);
+    }
+
+    /*
+     * Try to select some alternative tablets for balance. Add them to pendingTablets with priority LOW,
+     * and waiting to be scheduled.
+     */
+    private void selectTabletsForBalance() {
+        LoadBalancer loadBalancer = new LoadBalancer(statisticMap);
+        List<TabletInfo> alternativeTablets = loadBalancer.selectAlternativeTablets();
+        for (TabletInfo tabletInfo : alternativeTablets) {
+            addTablet(tabletInfo, false);
+        }
+    }
+
+    /*
+     * Try to create a balance task for a tablet.
+     */
+    private void doBalance(TabletInfo tabletInfo, AgentBatchTask batchTask) throws SchedException {
+        stat.counterBalanceSchedule.incrementAndGet();
+        LoadBalancer loadBalancer = new LoadBalancer(statisticMap);
+        loadBalancer.createBalanceTask(tabletInfo, backendsWorkingSlots, batchTask);
+    }
+
+    // choose a path on a backend which is fit for the tablet
+    private RootPathLoadStatistic chooseAvailableDestPath(TabletInfo tabletInfo) throws SchedException {
+        ClusterLoadStatistic statistic = statisticMap.get(tabletInfo.getCluster());
+        if (statistic == null) {
+            throw new SchedException(Status.UNRECOVERABLE, ""cluster does not exist"");
+        }
+        List<BackendLoadStatistic> beStatistics = statistic.getBeLoadStatistics();
+
+        // get all available paths which this tablet can fit in.
+        // beStatistics is sorted by load score in ascend order, so select from first to last.
+        List<RootPathLoadStatistic> allFitPaths = Lists.newArrayList();
+        for (int i = 0; i < beStatistics.size(); i++) {
+            BackendLoadStatistic bes = beStatistics.get(i);
+            // exclude BE which already has replica of this tablet
+            if (tabletInfo.containsBE(bes.getBeId())) {
+                continue;
+            }
+
+            List<RootPathLoadStatistic> resultPaths = Lists.newArrayList();
+            BalanceStatus st = bes.isFit(tabletInfo.getTabletSize(), resultPaths, true /* is supplement */);
+            if (!st.ok()) {
+                LOG.debug(""unable to find path for supplementing tablet: {}. {}"", tabletInfo, st);
+                continue;
+            }
+
+            Preconditions.checkState(resultPaths.size() == 1);
+            allFitPaths.add(resultPaths.get(0));
+        }
+
+        if (allFitPaths.isEmpty()) {
+            throw new SchedException(Status.SCHEDULE_FAILED, ""unable to find dest path for new replica"");
+        }
+
+        // all fit paths has already been sorted by load score in 'allFitPaths' in ascend order.
+        // just get first available path.
+        // we try to find a path with specified media type, if not find, arbitrarily use one.
+        for (RootPathLoadStatistic rootPathLoadStatistic : allFitPaths) {
+            if (rootPathLoadStatistic.getStorageMedium() != tabletInfo.getStorageMedium()) {
+                continue;
+            }
+
+            PathSlot slot = backendsWorkingSlots.get(rootPathLoadStatistic.getBeId());
+            if (slot == null) {
+                LOG.debug(""backend {} does not found when getting slots"", rootPathLoadStatistic.getBeId());
+                continue;
+            }
+
+            if (slot.takeSlot(rootPathLoadStatistic.getPathHash()) != -1) {
+                return rootPathLoadStatistic;
+            }
+        }
+
+        // no root path with specified media type is found, get arbitrary one.
+        for (RootPathLoadStatistic rootPathLoadStatistic : allFitPaths) {
+            PathSlot slot = backendsWorkingSlots.get(rootPathLoadStatistic.getBeId());
+            if (slot == null) {
+                LOG.debug(""backend {} does not found when getting slots"", rootPathLoadStatistic.getBeId());
+                continue;
+            }
+
+            if (slot.takeSlot(rootPathLoadStatistic.getPathHash()) != -1) {
+                return rootPathLoadStatistic;
+            }
+        }
+        
+        throw new SchedException(Status.SCHEDULE_FAILED, ""unable to find dest path which can be fit in"");
+    }
+
+    /*
+     * For some reason, a tablet info failed to be scheduled this time,
+     * So we dynamically change its priority and add back to queue, waiting for next round.
+     */
+    private void dynamicAdjustPrioAndAddBackToPendingTablets(TabletInfo tabletInfo, String message) {
+        Preconditions.checkState(tabletInfo.getState() == TabletInfo.State.PENDING);
+        tabletInfo.adjustPriority(stat);
+        addTablet(tabletInfo, true /* force */);
+    }
+
+    private synchronized void removeTabletInfo(TabletInfo tabletInfo, TabletInfo.State state, String reason) {
+        tabletInfo.setState(state);
+        tabletInfo.releaseResource(this);
+        tabletInfo.setFinishedTime(System.currentTimeMillis());
+        runningTablets.remove(tabletInfo.getTabletId());
+        allTabletIds.remove(tabletInfo.getTabletId());
+        schedHistory.add(tabletInfo);
+        LOG.info(""remove the tablet {}. because: {}"", tabletInfo.getTabletId(), reason);
+    }
+
+    // get next batch of tablets from queue.
+    private synchronized List<TabletInfo> getNextTabletInfoBatch() {
+        List<TabletInfo> list = Lists.newArrayList();
+        int count = Math.max(MIN_BATCH_NUM, getCurrentAvailableSlotNum());
+        while (count > 0) {
+            TabletInfo tablet = pendingTablets.poll();
+            if (tablet == null) {
+                // no more tablets
+                break;
+            }
+            list.add(tablet);
+            count--;
+        }
+        return list;
+    }
+
+    private int getCurrentAvailableSlotNum() {
+        int total = 0;
+        for (PathSlot pathSlot : backendsWorkingSlots.values()) {
+            total += pathSlot.getTotalAvailSlotNum();
+        }
+        return total;
+    }
+
+    /*
+     * return true if we want to remove the clone task from AgentTaskQueu
+     */
+    public boolean finishCloneTask(CloneTask cloneTask, TFinishTaskRequest request) {
+        long tabletId = cloneTask.getTabletId();
+        TabletInfo tabletInfo = takeRunningTablets(tabletId);
+        if (tabletInfo == null) {
+            LOG.warn(""tablet info does not exist: {}"", tabletId);
+            // tablet does not exist, no need to keep task.
+            return true;
+        }
+        Preconditions.checkState(tabletInfo.getState() == TabletInfo.State.RUNNING);
+        try {
+            tabletInfo.finishCloneTask(cloneTask, request);
+        } catch (SchedException e) {
+            tabletInfo.increaseFailedRunningCounter();
+            tabletInfo.setErrMsg(e.getMessage());
+            if (e.getStatus() == Status.RUNNING_FAILED) {
+                stat.counterCloneTaskFailed.incrementAndGet();
+                addToRunningTablets(tabletInfo);
+                return false;
+            } else {
+                Preconditions.checkState(e.getStatus() == Status.UNRECOVERABLE, e.getStatus());
+                // unrecoverable
+                stat.counterTabletScheduledDiscard.incrementAndGet();
+                removeTabletInfo(tabletInfo, TabletInfo.State.CANCELLED, e.getMessage());
+                return true;
+            }
+        }
+
+        Preconditions.checkState(tabletInfo.getState() == TabletInfo.State.FINISHED);
+        stat.counterCloneTaskSucceeded.incrementAndGet();
+        gatherStatistics(tabletInfo);
+        removeTabletInfo(tabletInfo, TabletInfo.State.FINISHED, ""finished"");
+        return true;
+    }
+
+    /*
+     * Gather the running statistic of the task.
+     * It will be evaluated for future strategy.  
+     * This should only be called when the tablet is down with state FINISHED.
+     */
+    private void gatherStatistics(TabletInfo tabletInfo) {
+        if (tabletInfo.getCopySize() > 0 && tabletInfo.getCopyTimeMs() > 0) {
+            if (tabletInfo.getSrcBackendId() != -1 && tabletInfo.getSrcPathHash() != -1) {
+                PathSlot pathSlot = backendsWorkingSlots.get(tabletInfo.getSrcBackendId());
+                if (pathSlot != null) {
+                    pathSlot.updateStatistic(tabletInfo.getSrcPathHash(), tabletInfo.getCopySize(),
+                            tabletInfo.getCopyTimeMs());
+                }
+            }
+
+            if (tabletInfo.getDestBackendId() != -1 && tabletInfo.getDestPathHash() != -1) {
+                PathSlot pathSlot = backendsWorkingSlots.get(tabletInfo.getDestBackendId());
+                if (pathSlot != null) {
+                    pathSlot.updateStatistic(tabletInfo.getDestPathHash(), tabletInfo.getCopySize(),
+                            tabletInfo.getCopyTimeMs());
+                }
+            }
+        }
+
+        if (System.currentTimeMillis() - lastSlotAdjustTime < STAT_UPDATE_INTERVAL_MS) {
+            return;
+        }
+
+        // TODO(cmy): update the slot num base on statistic.
+        // need to find a better way to determine the slot number.
+
+        lastSlotAdjustTime = System.currentTimeMillis();
+    }
+
+    /*
+     * handle tablets which are running.
+     * We should finished the task if
+     * 1. Tablet is already healthy
+     * 2. Task is timeout.
+     * 
+     * But here we just handle the timeout case here. Let the 'finishCloneTask()' check if tablet is healthy.
+     * We guarantee that if tablet is in runningTablets, the 'finishCloneTask()' will finally be called,
+     * so no need to worry that running tablets will never end.
+     * This is also avoid nesting 'synchronized' and database lock.
+     *
+     * If task is timeout, remove the tablet.
+     */
+    public synchronized void handleRunningTablets() {
+        List<TabletInfo> timeoutTablets = Lists.newArrayList();
+        runningTablets.values().stream().filter(t -> t.isTimeout()).forEach(t -> {
+            timeoutTablets.add(t);
+        });
+        
+        timeoutTablets.stream().forEach(t -> {
+            removeTabletInfo(t, TabletInfo.State.TIMEOUT, ""timeout"");
+            stat.counterCloneTaskTimeout.incrementAndGet();
+        });
+    }
+
+    public List<List<String>> getPendingTabletsInfo(int limit) {
+        List<TabletInfo> tabletInfos = getCopiedTablets(pendingTablets, limit);
+        return collectTabletInfo(tabletInfos);
+    }
+
+    public List<List<String>> getRunningTabletsInfo(int limit) {
+        List<TabletInfo> tabletInfos = getCopiedTablets(runningTablets.values(), limit);
+        return collectTabletInfo(tabletInfos);
+    }
+
+    public List<List<String>> getHistoryTabletsInfo(int limit) {
+        List<TabletInfo> tabletInfos = getCopiedTablets(schedHistory, limit);
+        return collectTabletInfo(tabletInfos);
+    }
+
+    private List<List<String>> collectTabletInfo(List<TabletInfo> tabletInfos) {
+        List<List<String>> result = Lists.newArrayList();
+        tabletInfos.stream().forEach(t -> {
+            result.add(t.getBrief());
+        });
+        return result;
+    }
+
+    private synchronized List<TabletInfo> getCopiedTablets(Collection<TabletInfo> source, int limit) {
+        List<TabletInfo> tabletInfos = Lists.newArrayList();
+        source.stream().limit(limit).forEach(t -> {
+            tabletInfos.add(t);
+        });
+        return tabletInfos;
+    }
+
+    public synchronized int getPendingNum() {
+        return pendingTablets.size();
+    }
+
+    public synchronized int getRunningNum() {
+        return runningTablets.size();
+    }
+
+    public synchronized int getHistoryNum() {
+        return schedHistory.size();
+    }
+
+    /*
+     * PathSlot keeps track of slot num per path of a Backend.
+     * Each path on a Backend has several slot.
+     * If a path's available slot num because 0, no task should be assigned to this path.
+     */
+    public class PathSlot {
+        // path hash -> slot num
+        private Map<Long, Slot> pathSlots = Maps.newConcurrentMap();
+
+        public PathSlot(List<Long> paths, int initSlotNum) {
+            for (Long pathHash : paths) {
+                pathSlots.put(pathHash, new Slot(initSlotNum));
+            }
+        }
+
+        // update the path
+        public synchronized void updatePaths(List<Long> paths) {","[{'comment': 'allocate a new map to substitute old one? ', 'commenter': 'imay'}]"
336,fe/src/main/java/org/apache/doris/clone/TabletScheduler.java,"@@ -0,0 +1,1186 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.clone;
+
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.catalog.Database;
+import org.apache.doris.catalog.MaterializedIndex;
+import org.apache.doris.catalog.OlapTable;
+import org.apache.doris.catalog.OlapTable.OlapTableState;
+import org.apache.doris.catalog.Partition;
+import org.apache.doris.catalog.Replica;
+import org.apache.doris.catalog.Replica.ReplicaState;
+import org.apache.doris.catalog.Tablet;
+import org.apache.doris.catalog.Tablet.TabletStatus;
+import org.apache.doris.catalog.TabletInvertedIndex;
+import org.apache.doris.clone.SchedException.Status;
+import org.apache.doris.clone.TabletInfo.Priority;
+import org.apache.doris.clone.TabletInfo.Type;
+import org.apache.doris.common.Config;
+import org.apache.doris.common.Pair;
+import org.apache.doris.common.util.Daemon;
+import org.apache.doris.persist.ReplicaPersistInfo;
+import org.apache.doris.system.Backend;
+import org.apache.doris.system.SystemInfoService;
+import org.apache.doris.task.AgentBatchTask;
+import org.apache.doris.task.AgentTask;
+import org.apache.doris.task.AgentTaskExecutor;
+import org.apache.doris.task.AgentTaskQueue;
+import org.apache.doris.task.CloneTask;
+import org.apache.doris.thrift.TFinishTaskRequest;
+
+import com.google.common.base.Preconditions;
+import com.google.common.collect.EvictingQueue;
+import com.google.common.collect.ImmutableMap;
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+import com.google.common.collect.Sets;
+
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.Collection;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.PriorityQueue;
+import java.util.Queue;
+import java.util.Set;
+import java.util.stream.Collectors;
+
+/*
+ * TabletScheduler saved the tablets produced by TabletChecker and try to schedule them.
+ * It also try to balance the cluster load.
+ * 
+ * We are expecting an efficient way to recovery the entire cluster and make it balanced.
+ * Case 1:
+ *  A Backend is down. All tablets which has replica on this BE should be repaired as soon as possible.
+ *  
+ * Case 1.1:
+ *  As Backend is down, some tables should be repaired in high priority. So the clone task should be able
+ *  to preempted.
+ *  
+ * Case 2:
+ *  A new Backend is added to the cluster. Replicas should be transfer to that host to balance the cluster load.
+ */
+public class TabletScheduler extends Daemon {
+    private static final Logger LOG = LogManager.getLogger(TabletScheduler.class);
+
+    // handle at most BATCH_NUM tablets in one loop
+    private static final int MIN_BATCH_NUM = 10;
+
+    // the minimum interval of updating cluster statistics and priority of tablet info
+    private static final long STAT_UPDATE_INTERVAL_MS = 60 * 1000; // 1min
+
+    private static final long SCHEDULE_INTERVAL_MS = 5000; // 5s
+
+    public static final int BALANCE_SLOT_NUM_FOR_PATH = 2;
+
+    /*
+     * Tablet is added to pendingTablets as well it's id in allTabletIds.
+     * TabletScheduler will take tablet from pendingTablets but will not remove it's id from allTabletIds when
+     * handling a tablet.
+     * Tablet' id can only be removed after the clone task is done(timeout, cancelled or finished).
+     * So if a tablet's id is still in allTabletIds, TabletChecker can not add tablet to TabletScheduler.
+     * 
+     * pendingTablets + runningTablets = allTabletIds
+     * 
+     * pendingTablets, allTabletIds, runningTablets and schedHistory are protected by 'synchronized' 
+     */
+    private PriorityQueue<TabletInfo> pendingTablets = new PriorityQueue<>();
+    private Set<Long> allTabletIds = Sets.newHashSet();
+    // contains all tabletInfos which state are RUNNING
+    private Map<Long, TabletInfo> runningTablets = Maps.newHashMap();
+    // save the latest 1000 scheduled tablet info
+    private Queue<TabletInfo> schedHistory = EvictingQueue.create(1000);
+
+    // be id -> #working slots
+    private Map<Long, PathSlot> backendsWorkingSlots = Maps.newConcurrentMap();
+    // cluster name -> load statistic
+    private Map<String, ClusterLoadStatistic> statisticMap = Maps.newConcurrentMap();
+    private long lastStatUpdateTime = 0;
+    
+    private long lastSlotAdjustTime = 0;
+
+    private Catalog catalog;
+    private SystemInfoService infoService;
+    private TabletInvertedIndex invertedIndex;
+    private TabletSchedulerStat stat;
+
+    public TabletScheduler(Catalog catalog, SystemInfoService infoService, TabletInvertedIndex invertedIndex,
+            TabletSchedulerStat stat) {
+        super(""tablet scheduler"", SCHEDULE_INTERVAL_MS);
+        this.catalog = catalog;
+        this.infoService = infoService;
+        this.invertedIndex = invertedIndex;
+        this.stat = stat;
+    }
+
+    public TabletSchedulerStat getStat() {
+        return stat;
+    }
+
+    /*
+     * update working slots at the beginning of each round
+     */
+    private boolean updateWorkingSlots() {
+        ImmutableMap<Long, Backend> backends = infoService.getBackendsInCluster(null);
+        for (Backend backend : backends.values()) {
+            if (!backend.hasPathHash() && backend.isAlive()) {
+                // when upgrading, backend may not get path info yet. so return false and wait for next round.
+                // and we should check if backend is alive. If backend is dead when upgrading, this backend
+                // will never report its path hash, and tablet scheduler is blocked.
+                LOG.info(""not all backends have path info"");
+                return false;
+            }
+        }
+
+        // update exist backends
+        Set<Long> deletedBeIds = Sets.newHashSet();
+        for (Long beId : backendsWorkingSlots.keySet()) {
+            if (backends.containsKey(beId)) {
+                List<Long> pathHashes = backends.get(beId).getDisks().values().stream().map(v -> v.getPathHash()).collect(Collectors.toList());
+                backendsWorkingSlots.get(beId).updatePaths(pathHashes);
+            } else {
+                deletedBeIds.add(beId);
+            }
+        }
+
+        // delete non-exist backends
+        for (Long beId : deletedBeIds) {
+            backendsWorkingSlots.remove(beId);
+            LOG.info(""delete non exist backend: {}"", beId);
+        }
+
+        // add new backends
+        for (Backend be : backends.values()) {
+            if (!backendsWorkingSlots.containsKey(be.getId())) {
+                List<Long> pathHashes = be.getDisks().values().stream().map(v -> v.getPathHash()).collect(Collectors.toList());
+                PathSlot slot = new PathSlot(pathHashes, Config.schedule_slot_num_per_path);
+                backendsWorkingSlots.put(be.getId(), slot);
+                LOG.info(""add new backend {} with slots num: {}"", be.getId(), be.getDisks().size());
+            }
+        }
+
+        return true;
+    }
+
+    public Map<Long, PathSlot> getBackendsWorkingSlots() {
+        return backendsWorkingSlots;
+    }
+
+    /*
+     * add a ready-to-be-scheduled tablet to pendingTablets, if it has not being added before.
+     * if force is true, do not check if tablet is already added before.
+     */
+    public synchronized boolean addTablet(TabletInfo tablet, boolean force) {
+        if (!force && containsTablet(tablet.getTabletId())) {
+            return false;
+        }
+        allTabletIds.add(tablet.getTabletId());
+        pendingTablets.offer(tablet);
+        return true;
+    }
+
+    public synchronized boolean containsTablet(long tabletId) {
+        return allTabletIds.contains(tabletId);
+    }
+
+    /*
+     * Iterate current tablets, change their priority if necessary.
+     */
+    public synchronized void changePriorityOfTablets(long dbId, long tblId, List<Long> partitionIds) {
+        PriorityQueue<TabletInfo> newPendingTablets = new PriorityQueue<>();
+        for (TabletInfo tabletInfo : pendingTablets) {
+            if (tabletInfo.getDbId() == dbId && tabletInfo.getTblId() == tblId
+                    && partitionIds.contains(tabletInfo.getPartitionId())) {
+                tabletInfo.setOrigPriority(Priority.VERY_HIGH);
+            }
+            newPendingTablets.add(tabletInfo);
+        }
+        pendingTablets = newPendingTablets;
+    }
+
+    /*
+     * TabletScheduler will run as a daemon thread at a very short interval(default 5 sec)
+     * Firstly, it will try to update cluster load statistic and check if priority need to be adjuested.
+     * Than, it will schedule the tablets in pendingTablets.
+     * Thirdly, it will check the current running tasks.
+     * Finally, it try to balance the cluster if possible.
+     * 
+     * Schedule rules:
+     * 1. tablet with higher priority will be scheduled first.
+     * 2. high priority should be downgraded if it fails to be schedule too many times.
+     * 3. priority may be upgraded if it is not being schedule for a long time.
+     * 4. every pending task should has a max scheduled time, if schedule fails too many times, if should be removed.
+     * 5. every running task should has a timeout, to avoid running forever.
+     * 6. every running task should also has a max failure time, if clone task fails too many times, if should be removed.
+     *
+     */
+    @Override
+    protected void runOneCycle() {
+        if (!updateWorkingSlots()) {
+            return;
+        }
+
+        updateClusterLoadStatisticsAndPriorityIfNecessary();
+
+        schedulePendingTablets();
+
+        handleRunningTablets();
+
+        selectTabletsForBalance();
+
+        stat.counterTabletScheduleRound.incrementAndGet();
+    }
+
+
+    private void updateClusterLoadStatisticsAndPriorityIfNecessary() {
+        if (System.currentTimeMillis() - lastStatUpdateTime < STAT_UPDATE_INTERVAL_MS) {
+            return;
+        }
+
+        updateClusterLoadStatistic();
+        adjustPriorities();
+
+        lastStatUpdateTime = System.currentTimeMillis();
+    }
+
+    /*
+     * Here is the only place we update the cluster load statistic info.
+     * We will not update this info dynamically along with the clone job's running.
+     * Although it will cause a little bit inaccurate, but is within a controllable range,
+     * because we already limit the total number of running clone jobs in cluster by 'backend slots'
+     */
+    private void updateClusterLoadStatistic() {
+        statisticMap.clear();
+        List<String> clusterNames = infoService.getClusterNames();
+        for (String clusterName : clusterNames) {
+            ClusterLoadStatistic clusterLoadStatistic = new ClusterLoadStatistic(clusterName, catalog,
+                    infoService, invertedIndex);
+            clusterLoadStatistic.init();
+            statisticMap.put(clusterName, clusterLoadStatistic);
+            LOG.info(""update cluster {} load statistic:\n {}"", clusterName, clusterLoadStatistic.getBrief());
+        }
+    }
+
+    public Map<String, ClusterLoadStatistic> getStatisticMap() {
+        return statisticMap;
+    }
+
+    /*
+     * adjust priorities of all tablet infos
+     */
+    private synchronized void adjustPriorities() {
+        int size = pendingTablets.size();
+        int changedNum = 0;
+        TabletInfo tabletInfo = null;
+        for (int i = 0; i < size; i++) {
+            tabletInfo = pendingTablets.poll();
+            if (tabletInfo == null) {
+                break;
+            }
+
+            if (tabletInfo.adjustPriority(stat)) {
+                changedNum++;
+            }
+            pendingTablets.add(tabletInfo);
+        }
+
+        LOG.info(""adjust priority for all tablets. changed: {}, total: {}"", changedNum, size);
+    }
+
+    /*
+     * get at most BATCH_NUM tablets from queue, and try to schedule them.
+     * After handle, the tablet info should be
+     * 1. in runningTablets with state RUNNING, if being scheduled success.
+     * 2. or in schedHistory with state CANCELLING, if some unrecoverable error happens.
+     * 3. or in pendingTablets with state PENDING, if failed to be scheduled.
+     * 
+     * if in schedHistory, it should be removed from allTabletIds.
+     */
+    private void schedulePendingTablets() {
+        long start = System.currentTimeMillis();
+        List<TabletInfo> currentBatch = getNextTabletInfoBatch();
+        LOG.debug(""get {} tablets to schedule"", currentBatch.size());
+
+        AgentBatchTask batchTask = new AgentBatchTask();
+        for (TabletInfo tabletInfo : currentBatch) {
+            try {
+                scheduleTablet(tabletInfo, batchTask);
+            } catch (SchedException e) {
+                tabletInfo.increaseFailedSchedCounter();
+                tabletInfo.setErrMsg(e.getMessage());
+
+                if (e.getStatus() == Status.SCHEDULE_FAILED) {
+                    // we must release resource it current hold, and be scheduled again
+                    tabletInfo.releaseResource(this);
+                    // adjust priority to avoid some higher priority always be the first in pendingTablets
+                    stat.counterTabletScheduledFailed.incrementAndGet();
+                    dynamicAdjustPrioAndAddBackToPendingTablets(tabletInfo, e.getMessage());
+                } else if (e.getStatus() == Status.FINISHED) {
+                    // schedule redundant tablet will throw this exception
+                    stat.counterTabletScheduledSucceeded.incrementAndGet();
+                    removeTabletInfo(tabletInfo, TabletInfo.State.FINISHED, e.getMessage());
+                } else {
+                    Preconditions.checkState(e.getStatus() == Status.UNRECOVERABLE, e.getStatus());
+                    // discard
+                    stat.counterTabletScheduledDiscard.incrementAndGet();
+                    removeTabletInfo(tabletInfo, TabletInfo.State.CANCELLED, e.getMessage());
+                }
+                continue;
+            }
+
+            Preconditions.checkState(tabletInfo.getState() == TabletInfo.State.RUNNING);
+            stat.counterTabletScheduledSucceeded.incrementAndGet();
+            addToRunningTablets(tabletInfo);
+        }
+
+        // must send task after adding tablet info to runningTablets.
+        for (AgentTask task : batchTask.getAllTasks()) {
+            if (AgentTaskQueue.addTask(task)) {
+                stat.counterCloneTask.incrementAndGet();
+            }
+            LOG.info(""add clone task to agent task queue: {}"", task);
+        }
+
+        // send task immediately
+        AgentTaskExecutor.submit(batchTask);
+
+        long cost = System.currentTimeMillis() - start;
+        stat.counterTabletScheduleCostMs.addAndGet(cost);
+    }
+
+    private synchronized void addToRunningTablets(TabletInfo tabletInfo) {
+        runningTablets.put(tabletInfo.getTabletId(), tabletInfo);
+    }
+
+    /*
+     * we take the tablet out of the runningTablets and than handle it,
+     * avoid other threads see it.
+     * Whoever takes this tablet, make sure to put it to the schedHistory or back to runningTablets.
+     */
+    private synchronized TabletInfo takeRunningTablets(long tabletId) {
+        return runningTablets.remove(tabletId);
+    }
+
+    /*
+     * Try to schedule a single tablet.
+     */
+    private void scheduleTablet(TabletInfo tabletInfo, AgentBatchTask batchTask) throws SchedException {
+        LOG.debug(""schedule tablet: {}"", tabletInfo.getTabletId());
+        long currentTime = System.currentTimeMillis();
+        tabletInfo.setLastSchedTime(currentTime);
+        tabletInfo.setLastVisitedTime(currentTime);
+        stat.counterTabletScheduled.incrementAndGet();
+
+        // check this tablet again
+        Database db = catalog.getDb(tabletInfo.getDbId());
+        if (db == null) {
+            throw new SchedException(Status.UNRECOVERABLE, ""db does not exist"");
+        }
+
+        Pair<TabletStatus, TabletInfo.Priority> statusPair = null;
+        db.writeLock();
+        try {
+            OlapTable tbl = (OlapTable) db.getTable(tabletInfo.getTblId());
+            if (tbl == null) {
+                throw new SchedException(Status.UNRECOVERABLE, ""tbl does not exist"");
+            }
+
+            // we may add a tablet of a NOT NORMAL table during balance, which should be blocked
+            if (tbl.getState() != OlapTableState.NORMAL) {
+                throw new SchedException(Status.UNRECOVERABLE, ""tbl's state is not normal: "" + tbl.getState());
+            }
+
+            Partition partition = tbl.getPartition(tabletInfo.getPartitionId());
+            if (partition == null) {
+                throw new SchedException(Status.UNRECOVERABLE, ""partition does not exist"");
+            }
+
+            MaterializedIndex idx = partition.getIndex(tabletInfo.getIndexId());
+            if (idx == null) {
+                throw new SchedException(Status.UNRECOVERABLE, ""index does not exist"");
+            }
+
+            Tablet tablet = idx.getTablet(tabletInfo.getTabletId());
+            Preconditions.checkNotNull(tablet);
+
+            statusPair = tablet.getHealthStatusWithPriority(
+                    infoService, tabletInfo.getCluster(),
+                    partition.getVisibleVersion(),
+                    partition.getVisibleVersionHash(),
+                    tbl.getPartitionInfo().getReplicationNum(partition.getId()));
+
+            tabletInfo.setTabletStatus(statusPair.first);
+            if (statusPair.first == TabletStatus.HEALTHY && tabletInfo.getType() == TabletInfo.Type.REPAIR) {
+                throw new SchedException(Status.UNRECOVERABLE, ""tablet is healthy"");
+            } else if (statusPair.first != TabletStatus.HEALTHY
+                    && tabletInfo.getType() == TabletInfo.Type.BALANCE) {
+                tabletInfo.releaseResource(this);
+                // we select an unhealthy tablet to do balance, which is not right.
+                // so here we change it to a REPAIR task, and also reset its priority
+                tabletInfo.setType(TabletInfo.Type.REPAIR);
+                tabletInfo.setOrigPriority(statusPair.second);
+            }
+
+            // we do not concern priority here.
+            // once we take the tablet out of priority queue, priority is meaningless.
+            tabletInfo.setTablet(tablet);
+            tabletInfo.setVersionInfo(partition.getVisibleVersion(), partition.getVisibleVersionHash(),
+                    partition.getCommittedVersion(), partition.getCommittedVersionHash());
+            tabletInfo.setSchemaHash(tbl.getSchemaHashByIndexId(idx.getId()));
+            tabletInfo.setStorageMedium(tbl.getPartitionInfo().getDataProperty(partition.getId()).getStorageMedium());
+
+            handleTabletByTypeAndStatus(statusPair.first, tabletInfo, batchTask);
+        } finally {
+            db.writeUnlock();
+        }
+    }
+
+    private void handleTabletByTypeAndStatus(TabletStatus status, TabletInfo tabletInfo, AgentBatchTask batchTask)
+            throws SchedException {
+        if (tabletInfo.getType() == Type.REPAIR) {
+            switch (status) {
+                case REPLICA_MISSING:
+                    handleReplicaMissing(tabletInfo, batchTask);
+                    break;
+                case VERSION_INCOMPLETE:
+                    handleReplicaVersionIncomplete(tabletInfo, batchTask);
+                    break;
+                case REDUNDANT:
+                    handleRedundantReplica(tabletInfo);
+                    break;
+                case REPLICA_MISSING_IN_CLUSTER:
+                    handleReplicaClusterMigration(tabletInfo, batchTask);
+                    break;
+                default:
+                    break;
+            }
+        } else {
+            // balance
+            doBalance(tabletInfo, batchTask);
+        }
+    }
+
+    /*
+     * Replica is missing, which means there is no enough alive replicas.
+     * So we need to find a destination backend to clone a new replica as possible as we can.
+     * 1. find an available path in a backend as destination:
+     *      1. backend need to be alive.
+     *      2. backend of existing replicas should be excluded.
+     *      3. backend has available slot for clone.
+     *      4. replica can fit in the path (consider the threshold of disk capacity and usage percent).
+     *      5. try to find a path with lowest load score.
+     * 2. find an appropriate source replica:
+     *      1. source replica should be healthy
+     *      2. backend of source replica has available slot for clone.
+     *      
+     * 3. send clone task to destination backend
+     */
+    private void handleReplicaMissing(TabletInfo tabletInfo, AgentBatchTask batchTask) throws SchedException {
+        stat.counterReplicaMissingErr.incrementAndGet();
+        // find an available dest backend and path
+        RootPathLoadStatistic destPath = chooseAvailableDestPath(tabletInfo);
+        Preconditions.checkNotNull(destPath);
+        tabletInfo.setDestination(destPath.getBeId(), destPath.getPathHash());
+
+        // choose a source replica for cloning from
+        tabletInfo.chooseSrcReplica(backendsWorkingSlots);
+
+        // create clone task
+        batchTask.addTask(tabletInfo.createCloneReplicaAndTask());
+    }
+
+    /*
+     * Replica version is incomplete, which means this replica is missing some version,
+     * and need to be cloned from a healthy replica, in-place.
+     * 
+     * 1. find the incomplete replica as destination replica
+     * 2. find a healthy replica as source replica
+     * 3. send clone task
+     */
+    private void handleReplicaVersionIncomplete(TabletInfo tabletInfo, AgentBatchTask batchTask)
+            throws SchedException {
+        stat.counterReplicaVersionMissingErr.incrementAndGet();
+        ClusterLoadStatistic statistic = statisticMap.get(tabletInfo.getCluster());
+        if (statistic == null) {
+            throw new SchedException(Status.UNRECOVERABLE, ""cluster does not exist"");
+        }
+
+        tabletInfo.chooseDestReplicaForVersionIncomplete(backendsWorkingSlots);
+        tabletInfo.chooseSrcReplicaForVersionIncomplete(backendsWorkingSlots);
+
+        // create clone task
+        batchTask.addTask(tabletInfo.createCloneReplicaAndTask());
+    }
+
+    /*
+     *  replica is redundant, which means there are more replicas than we expected, which need to be dropped.
+     *  we just drop one redundant replica at a time, for safety reason.
+     *  choosing a replica to drop base on following priority:
+     *  1. backend has been dropped
+     *  2. backend is not available
+     *  3. replica's state is CLONE
+     *  4. replica's last failed version > 0
+     *  5. replica with lower version
+     *  6. replica not in right cluster
+     *  7. replica in higher load backend
+     */
+    private void handleRedundantReplica(TabletInfo tabletInfo) throws SchedException {
+        stat.counterReplicaRedundantErr.incrementAndGet();
+        if (deleteBackendDropped(tabletInfo)
+                || deleteBackendUnavailable(tabletInfo)
+                || deleteCloneReplica(tabletInfo)
+                || deleteReplicaWithFailedVersion(tabletInfo)
+                || deleteReplicaWithLowerVersion(tabletInfo)
+                || deleteReplicaNotInCluster(tabletInfo)
+                || deleteReplicaOnHighLoadBackend(tabletInfo)) {
+            // if we delete at least one redundant replica, we still throw a SchedException with status FINISHED
+            // to remove this tablet from the pendingTablets(consider it as finished)
+            throw new SchedException(Status.FINISHED, ""redundant replica is deleted"");
+        }
+        throw new SchedException(Status.SCHEDULE_FAILED, ""unable to delete any redundant replicas"");
+    }
+
+    private boolean deleteBackendDropped(TabletInfo tabletInfo) {
+        for (Replica replica : tabletInfo.getReplicas()) {
+            long beId = replica.getBackendId();
+            if (infoService.getBackend(beId) == null) {
+                deleteReplicaInternal(tabletInfo, replica, ""backend dropped"");
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private boolean deleteBackendUnavailable(TabletInfo tabletInfo) {
+        for (Replica replica : tabletInfo.getReplicas()) {
+            Backend be = infoService.getBackend(replica.getBackendId());
+            if (be == null) {
+                // this case should be handled in deleteBackendDropped()
+                continue;
+            }
+            if (!be.isAvailable()) {
+                deleteReplicaInternal(tabletInfo, replica, ""backend unavailable"");
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private boolean deleteCloneReplica(TabletInfo tabletInfo) {
+        for (Replica replica : tabletInfo.getReplicas()) {
+            if (replica.getState() == ReplicaState.CLONE) {
+                deleteReplicaInternal(tabletInfo, replica, ""clone state"");
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private boolean deleteReplicaWithFailedVersion(TabletInfo tabletInfo) {
+        for (Replica replica : tabletInfo.getReplicas()) {
+            if (replica.getLastFailedVersion() > 0) {
+                deleteReplicaInternal(tabletInfo, replica, ""version incomplete"");
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private boolean deleteReplicaWithLowerVersion(TabletInfo tabletInfo) {
+        for (Replica replica : tabletInfo.getReplicas()) {
+            if (!replica.checkVersionCatchUp(tabletInfo.getCommittedVersion(), tabletInfo.getCommittedVersionHash())) {
+                deleteReplicaInternal(tabletInfo, replica, ""lower version"");
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private boolean deleteReplicaNotInCluster(TabletInfo tabletInfo) {
+        for (Replica replica : tabletInfo.getReplicas()) {
+            Backend be = infoService.getBackend(replica.getBackendId());
+            if (be == null) {
+                // this case should be handled in deleteBackendDropped()
+                continue;
+            }
+            if (!be.getOwnerClusterName().equals(tabletInfo.getCluster())) {
+                deleteReplicaInternal(tabletInfo, replica, ""not in cluster"");
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private boolean deleteReplicaOnHighLoadBackend(TabletInfo tabletInfo) {
+        ClusterLoadStatistic statistic = statisticMap.get(tabletInfo.getCluster());
+        if (statistic == null) {
+            return false;
+        }
+        
+        Replica chosenReplica = null;
+        double maxScore = 0;
+        for (Replica replica : tabletInfo.getReplicas()) {
+            BackendLoadStatistic beStatistic = statistic.getBackendLoadStatistic(replica.getBackendId());
+            if (beStatistic == null) {
+                continue;
+            }
+            if (beStatistic.getLoadScore() > maxScore) {
+                maxScore = beStatistic.getLoadScore();
+                chosenReplica = replica;
+            }
+        }
+
+        if (chosenReplica != null) {
+            deleteReplicaInternal(tabletInfo, chosenReplica, ""high load"");
+            return true;
+        }
+        return false;
+    }
+
+    private void deleteReplicaInternal(TabletInfo tabletInfo, Replica replica, String reason) {
+        // delete this replica from catalog.
+        // it will also delete replica from tablet inverted index.
+        tabletInfo.deleteReplica(replica);
+
+        // TODO(cmy): this should be removed after I finish modifying alter job logic
+        // Catalog.getInstance().handleJobsWhenDeleteReplica(tabletInfo.getTblId(), tabletInfo.getPartitionId(),
+        //                                                   tabletInfo.getIndexId(), tabletInfo.getTabletId(),
+        //                                                   replica.getId(), replica.getBackendId());
+
+        // write edit log
+        ReplicaPersistInfo info = ReplicaPersistInfo.createForDelete(tabletInfo.getDbId(),
+                                                                     tabletInfo.getTblId(),
+                                                                     tabletInfo.getPartitionId(),
+                                                                     tabletInfo.getIndexId(),
+                                                                     tabletInfo.getTabletId(),
+                                                                     replica.getBackendId());
+
+        Catalog.getInstance().getEditLog().logDeleteReplica(info);
+
+        LOG.info(""delete replica. tablet id: {}, backend id: {}. reason: {}"",
+                 tabletInfo.getTabletId(), replica.getBackendId(), reason);
+    }
+
+    /*
+     * Cluster migration, which means the tablet has enough healthy replicas,
+     * but some replicas are not in right cluster.
+     * It is just same as 'replica missing'.
+     * 
+     * after clone finished, the replica in wrong cluster will be treated as redundant, and will be deleted soon.
+     */
+    private void handleReplicaClusterMigration(TabletInfo tabletInfo, AgentBatchTask batchTask)
+            throws SchedException {
+        stat.counterReplicaMissingInClusterErr.incrementAndGet();
+        handleReplicaMissing(tabletInfo, batchTask);
+    }
+
+    /*
+     * Try to select some alternative tablets for balance. Add them to pendingTablets with priority LOW,
+     * and waiting to be scheduled.
+     */
+    private void selectTabletsForBalance() {
+        LoadBalancer loadBalancer = new LoadBalancer(statisticMap);
+        List<TabletInfo> alternativeTablets = loadBalancer.selectAlternativeTablets();
+        for (TabletInfo tabletInfo : alternativeTablets) {
+            addTablet(tabletInfo, false);
+        }
+    }
+
+    /*
+     * Try to create a balance task for a tablet.
+     */
+    private void doBalance(TabletInfo tabletInfo, AgentBatchTask batchTask) throws SchedException {
+        stat.counterBalanceSchedule.incrementAndGet();
+        LoadBalancer loadBalancer = new LoadBalancer(statisticMap);
+        loadBalancer.createBalanceTask(tabletInfo, backendsWorkingSlots, batchTask);
+    }
+
+    // choose a path on a backend which is fit for the tablet
+    private RootPathLoadStatistic chooseAvailableDestPath(TabletInfo tabletInfo) throws SchedException {
+        ClusterLoadStatistic statistic = statisticMap.get(tabletInfo.getCluster());
+        if (statistic == null) {
+            throw new SchedException(Status.UNRECOVERABLE, ""cluster does not exist"");
+        }
+        List<BackendLoadStatistic> beStatistics = statistic.getBeLoadStatistics();
+
+        // get all available paths which this tablet can fit in.
+        // beStatistics is sorted by load score in ascend order, so select from first to last.
+        List<RootPathLoadStatistic> allFitPaths = Lists.newArrayList();
+        for (int i = 0; i < beStatistics.size(); i++) {
+            BackendLoadStatistic bes = beStatistics.get(i);
+            // exclude BE which already has replica of this tablet
+            if (tabletInfo.containsBE(bes.getBeId())) {
+                continue;
+            }
+
+            List<RootPathLoadStatistic> resultPaths = Lists.newArrayList();
+            BalanceStatus st = bes.isFit(tabletInfo.getTabletSize(), resultPaths, true /* is supplement */);
+            if (!st.ok()) {
+                LOG.debug(""unable to find path for supplementing tablet: {}. {}"", tabletInfo, st);
+                continue;
+            }
+
+            Preconditions.checkState(resultPaths.size() == 1);
+            allFitPaths.add(resultPaths.get(0));
+        }
+
+        if (allFitPaths.isEmpty()) {
+            throw new SchedException(Status.SCHEDULE_FAILED, ""unable to find dest path for new replica"");
+        }
+
+        // all fit paths has already been sorted by load score in 'allFitPaths' in ascend order.
+        // just get first available path.
+        // we try to find a path with specified media type, if not find, arbitrarily use one.
+        for (RootPathLoadStatistic rootPathLoadStatistic : allFitPaths) {
+            if (rootPathLoadStatistic.getStorageMedium() != tabletInfo.getStorageMedium()) {
+                continue;
+            }
+
+            PathSlot slot = backendsWorkingSlots.get(rootPathLoadStatistic.getBeId());
+            if (slot == null) {
+                LOG.debug(""backend {} does not found when getting slots"", rootPathLoadStatistic.getBeId());
+                continue;
+            }
+
+            if (slot.takeSlot(rootPathLoadStatistic.getPathHash()) != -1) {
+                return rootPathLoadStatistic;
+            }
+        }
+
+        // no root path with specified media type is found, get arbitrary one.
+        for (RootPathLoadStatistic rootPathLoadStatistic : allFitPaths) {
+            PathSlot slot = backendsWorkingSlots.get(rootPathLoadStatistic.getBeId());
+            if (slot == null) {
+                LOG.debug(""backend {} does not found when getting slots"", rootPathLoadStatistic.getBeId());
+                continue;
+            }
+
+            if (slot.takeSlot(rootPathLoadStatistic.getPathHash()) != -1) {
+                return rootPathLoadStatistic;
+            }
+        }
+        
+        throw new SchedException(Status.SCHEDULE_FAILED, ""unable to find dest path which can be fit in"");
+    }
+
+    /*
+     * For some reason, a tablet info failed to be scheduled this time,
+     * So we dynamically change its priority and add back to queue, waiting for next round.
+     */
+    private void dynamicAdjustPrioAndAddBackToPendingTablets(TabletInfo tabletInfo, String message) {
+        Preconditions.checkState(tabletInfo.getState() == TabletInfo.State.PENDING);
+        tabletInfo.adjustPriority(stat);
+        addTablet(tabletInfo, true /* force */);
+    }
+
+    private synchronized void removeTabletInfo(TabletInfo tabletInfo, TabletInfo.State state, String reason) {
+        tabletInfo.setState(state);
+        tabletInfo.releaseResource(this);
+        tabletInfo.setFinishedTime(System.currentTimeMillis());
+        runningTablets.remove(tabletInfo.getTabletId());
+        allTabletIds.remove(tabletInfo.getTabletId());
+        schedHistory.add(tabletInfo);
+        LOG.info(""remove the tablet {}. because: {}"", tabletInfo.getTabletId(), reason);
+    }
+
+    // get next batch of tablets from queue.
+    private synchronized List<TabletInfo> getNextTabletInfoBatch() {
+        List<TabletInfo> list = Lists.newArrayList();
+        int count = Math.max(MIN_BATCH_NUM, getCurrentAvailableSlotNum());
+        while (count > 0) {
+            TabletInfo tablet = pendingTablets.poll();
+            if (tablet == null) {
+                // no more tablets
+                break;
+            }
+            list.add(tablet);
+            count--;
+        }
+        return list;
+    }
+
+    private int getCurrentAvailableSlotNum() {
+        int total = 0;
+        for (PathSlot pathSlot : backendsWorkingSlots.values()) {
+            total += pathSlot.getTotalAvailSlotNum();
+        }
+        return total;
+    }
+
+    /*
+     * return true if we want to remove the clone task from AgentTaskQueu
+     */
+    public boolean finishCloneTask(CloneTask cloneTask, TFinishTaskRequest request) {
+        long tabletId = cloneTask.getTabletId();
+        TabletInfo tabletInfo = takeRunningTablets(tabletId);
+        if (tabletInfo == null) {
+            LOG.warn(""tablet info does not exist: {}"", tabletId);
+            // tablet does not exist, no need to keep task.
+            return true;
+        }
+        Preconditions.checkState(tabletInfo.getState() == TabletInfo.State.RUNNING);
+        try {
+            tabletInfo.finishCloneTask(cloneTask, request);
+        } catch (SchedException e) {
+            tabletInfo.increaseFailedRunningCounter();
+            tabletInfo.setErrMsg(e.getMessage());
+            if (e.getStatus() == Status.RUNNING_FAILED) {
+                stat.counterCloneTaskFailed.incrementAndGet();
+                addToRunningTablets(tabletInfo);
+                return false;
+            } else {
+                Preconditions.checkState(e.getStatus() == Status.UNRECOVERABLE, e.getStatus());
+                // unrecoverable
+                stat.counterTabletScheduledDiscard.incrementAndGet();
+                removeTabletInfo(tabletInfo, TabletInfo.State.CANCELLED, e.getMessage());
+                return true;
+            }
+        }
+
+        Preconditions.checkState(tabletInfo.getState() == TabletInfo.State.FINISHED);
+        stat.counterCloneTaskSucceeded.incrementAndGet();
+        gatherStatistics(tabletInfo);
+        removeTabletInfo(tabletInfo, TabletInfo.State.FINISHED, ""finished"");
+        return true;
+    }
+
+    /*
+     * Gather the running statistic of the task.
+     * It will be evaluated for future strategy.  
+     * This should only be called when the tablet is down with state FINISHED.
+     */
+    private void gatherStatistics(TabletInfo tabletInfo) {
+        if (tabletInfo.getCopySize() > 0 && tabletInfo.getCopyTimeMs() > 0) {
+            if (tabletInfo.getSrcBackendId() != -1 && tabletInfo.getSrcPathHash() != -1) {
+                PathSlot pathSlot = backendsWorkingSlots.get(tabletInfo.getSrcBackendId());
+                if (pathSlot != null) {
+                    pathSlot.updateStatistic(tabletInfo.getSrcPathHash(), tabletInfo.getCopySize(),
+                            tabletInfo.getCopyTimeMs());
+                }
+            }
+
+            if (tabletInfo.getDestBackendId() != -1 && tabletInfo.getDestPathHash() != -1) {
+                PathSlot pathSlot = backendsWorkingSlots.get(tabletInfo.getDestBackendId());
+                if (pathSlot != null) {
+                    pathSlot.updateStatistic(tabletInfo.getDestPathHash(), tabletInfo.getCopySize(),
+                            tabletInfo.getCopyTimeMs());
+                }
+            }
+        }
+
+        if (System.currentTimeMillis() - lastSlotAdjustTime < STAT_UPDATE_INTERVAL_MS) {
+            return;
+        }
+
+        // TODO(cmy): update the slot num base on statistic.
+        // need to find a better way to determine the slot number.
+
+        lastSlotAdjustTime = System.currentTimeMillis();
+    }
+
+    /*
+     * handle tablets which are running.
+     * We should finished the task if
+     * 1. Tablet is already healthy
+     * 2. Task is timeout.
+     * 
+     * But here we just handle the timeout case here. Let the 'finishCloneTask()' check if tablet is healthy.
+     * We guarantee that if tablet is in runningTablets, the 'finishCloneTask()' will finally be called,
+     * so no need to worry that running tablets will never end.
+     * This is also avoid nesting 'synchronized' and database lock.
+     *
+     * If task is timeout, remove the tablet.
+     */
+    public synchronized void handleRunningTablets() {
+        List<TabletInfo> timeoutTablets = Lists.newArrayList();
+        runningTablets.values().stream().filter(t -> t.isTimeout()).forEach(t -> {
+            timeoutTablets.add(t);
+        });
+        
+        timeoutTablets.stream().forEach(t -> {
+            removeTabletInfo(t, TabletInfo.State.TIMEOUT, ""timeout"");
+            stat.counterCloneTaskTimeout.incrementAndGet();
+        });
+    }
+
+    public List<List<String>> getPendingTabletsInfo(int limit) {
+        List<TabletInfo> tabletInfos = getCopiedTablets(pendingTablets, limit);
+        return collectTabletInfo(tabletInfos);
+    }
+
+    public List<List<String>> getRunningTabletsInfo(int limit) {
+        List<TabletInfo> tabletInfos = getCopiedTablets(runningTablets.values(), limit);
+        return collectTabletInfo(tabletInfos);
+    }
+
+    public List<List<String>> getHistoryTabletsInfo(int limit) {
+        List<TabletInfo> tabletInfos = getCopiedTablets(schedHistory, limit);
+        return collectTabletInfo(tabletInfos);
+    }
+
+    private List<List<String>> collectTabletInfo(List<TabletInfo> tabletInfos) {
+        List<List<String>> result = Lists.newArrayList();
+        tabletInfos.stream().forEach(t -> {
+            result.add(t.getBrief());
+        });
+        return result;
+    }
+
+    private synchronized List<TabletInfo> getCopiedTablets(Collection<TabletInfo> source, int limit) {
+        List<TabletInfo> tabletInfos = Lists.newArrayList();
+        source.stream().limit(limit).forEach(t -> {
+            tabletInfos.add(t);
+        });
+        return tabletInfos;
+    }
+
+    public synchronized int getPendingNum() {
+        return pendingTablets.size();
+    }
+
+    public synchronized int getRunningNum() {
+        return runningTablets.size();
+    }
+
+    public synchronized int getHistoryNum() {
+        return schedHistory.size();
+    }
+
+    /*
+     * PathSlot keeps track of slot num per path of a Backend.
+     * Each path on a Backend has several slot.
+     * If a path's available slot num because 0, no task should be assigned to this path.
+     */
+    public class PathSlot {
+        // path hash -> slot num
+        private Map<Long, Slot> pathSlots = Maps.newConcurrentMap();
+
+        public PathSlot(List<Long> paths, int initSlotNum) {
+            for (Long pathHash : paths) {
+                pathSlots.put(pathHash, new Slot(initSlotNum));
+            }
+        }
+
+        // update the path
+        public synchronized void updatePaths(List<Long> paths) {
+            // delete non exist path
+            Iterator<Map.Entry<Long, Slot>> iter = pathSlots.entrySet().iterator();
+            while (iter.hasNext()) {
+                Map.Entry<Long, Slot> entry = iter.next();
+                if (!paths.contains(entry.getKey())) {
+                    iter.remove();
+                }
+            }
+
+            // add new path
+            for (Long pathHash : paths) {
+                if (!pathSlots.containsKey(pathHash)) {
+                    pathSlots.put(pathHash, new Slot(Config.schedule_slot_num_per_path));
+                }
+            }
+        }
+
+        // Update the total slots num of specified paths, increase or decrease
+        public synchronized void updateSlot(List<Long> pathHashs, boolean increase) {
+            for (Long pathHash : pathHashs) {
+                if (pathSlots.containsKey(pathHash)) {","[{'comment': '```suggestion\r\n                slot = pathSlots.get(pathHash);\r\n```', 'commenter': 'imay'}]"
336,fe/src/main/java/org/apache/doris/clone/TabletScheduler.java,"@@ -0,0 +1,1186 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.clone;
+
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.catalog.Database;
+import org.apache.doris.catalog.MaterializedIndex;
+import org.apache.doris.catalog.OlapTable;
+import org.apache.doris.catalog.OlapTable.OlapTableState;
+import org.apache.doris.catalog.Partition;
+import org.apache.doris.catalog.Replica;
+import org.apache.doris.catalog.Replica.ReplicaState;
+import org.apache.doris.catalog.Tablet;
+import org.apache.doris.catalog.Tablet.TabletStatus;
+import org.apache.doris.catalog.TabletInvertedIndex;
+import org.apache.doris.clone.SchedException.Status;
+import org.apache.doris.clone.TabletInfo.Priority;
+import org.apache.doris.clone.TabletInfo.Type;
+import org.apache.doris.common.Config;
+import org.apache.doris.common.Pair;
+import org.apache.doris.common.util.Daemon;
+import org.apache.doris.persist.ReplicaPersistInfo;
+import org.apache.doris.system.Backend;
+import org.apache.doris.system.SystemInfoService;
+import org.apache.doris.task.AgentBatchTask;
+import org.apache.doris.task.AgentTask;
+import org.apache.doris.task.AgentTaskExecutor;
+import org.apache.doris.task.AgentTaskQueue;
+import org.apache.doris.task.CloneTask;
+import org.apache.doris.thrift.TFinishTaskRequest;
+
+import com.google.common.base.Preconditions;
+import com.google.common.collect.EvictingQueue;
+import com.google.common.collect.ImmutableMap;
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+import com.google.common.collect.Sets;
+
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.Collection;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.PriorityQueue;
+import java.util.Queue;
+import java.util.Set;
+import java.util.stream.Collectors;
+
+/*
+ * TabletScheduler saved the tablets produced by TabletChecker and try to schedule them.
+ * It also try to balance the cluster load.
+ * 
+ * We are expecting an efficient way to recovery the entire cluster and make it balanced.
+ * Case 1:
+ *  A Backend is down. All tablets which has replica on this BE should be repaired as soon as possible.
+ *  
+ * Case 1.1:
+ *  As Backend is down, some tables should be repaired in high priority. So the clone task should be able
+ *  to preempted.
+ *  
+ * Case 2:
+ *  A new Backend is added to the cluster. Replicas should be transfer to that host to balance the cluster load.
+ */
+public class TabletScheduler extends Daemon {
+    private static final Logger LOG = LogManager.getLogger(TabletScheduler.class);
+
+    // handle at most BATCH_NUM tablets in one loop
+    private static final int MIN_BATCH_NUM = 10;
+
+    // the minimum interval of updating cluster statistics and priority of tablet info
+    private static final long STAT_UPDATE_INTERVAL_MS = 60 * 1000; // 1min
+
+    private static final long SCHEDULE_INTERVAL_MS = 5000; // 5s
+
+    public static final int BALANCE_SLOT_NUM_FOR_PATH = 2;
+
+    /*
+     * Tablet is added to pendingTablets as well it's id in allTabletIds.
+     * TabletScheduler will take tablet from pendingTablets but will not remove it's id from allTabletIds when
+     * handling a tablet.
+     * Tablet' id can only be removed after the clone task is done(timeout, cancelled or finished).
+     * So if a tablet's id is still in allTabletIds, TabletChecker can not add tablet to TabletScheduler.
+     * 
+     * pendingTablets + runningTablets = allTabletIds
+     * 
+     * pendingTablets, allTabletIds, runningTablets and schedHistory are protected by 'synchronized' 
+     */
+    private PriorityQueue<TabletInfo> pendingTablets = new PriorityQueue<>();
+    private Set<Long> allTabletIds = Sets.newHashSet();
+    // contains all tabletInfos which state are RUNNING
+    private Map<Long, TabletInfo> runningTablets = Maps.newHashMap();
+    // save the latest 1000 scheduled tablet info
+    private Queue<TabletInfo> schedHistory = EvictingQueue.create(1000);
+
+    // be id -> #working slots
+    private Map<Long, PathSlot> backendsWorkingSlots = Maps.newConcurrentMap();
+    // cluster name -> load statistic
+    private Map<String, ClusterLoadStatistic> statisticMap = Maps.newConcurrentMap();
+    private long lastStatUpdateTime = 0;
+    
+    private long lastSlotAdjustTime = 0;
+
+    private Catalog catalog;
+    private SystemInfoService infoService;
+    private TabletInvertedIndex invertedIndex;
+    private TabletSchedulerStat stat;
+
+    public TabletScheduler(Catalog catalog, SystemInfoService infoService, TabletInvertedIndex invertedIndex,
+            TabletSchedulerStat stat) {
+        super(""tablet scheduler"", SCHEDULE_INTERVAL_MS);
+        this.catalog = catalog;
+        this.infoService = infoService;
+        this.invertedIndex = invertedIndex;
+        this.stat = stat;
+    }
+
+    public TabletSchedulerStat getStat() {
+        return stat;
+    }
+
+    /*
+     * update working slots at the beginning of each round
+     */
+    private boolean updateWorkingSlots() {
+        ImmutableMap<Long, Backend> backends = infoService.getBackendsInCluster(null);
+        for (Backend backend : backends.values()) {
+            if (!backend.hasPathHash() && backend.isAlive()) {
+                // when upgrading, backend may not get path info yet. so return false and wait for next round.
+                // and we should check if backend is alive. If backend is dead when upgrading, this backend
+                // will never report its path hash, and tablet scheduler is blocked.
+                LOG.info(""not all backends have path info"");
+                return false;
+            }
+        }
+
+        // update exist backends
+        Set<Long> deletedBeIds = Sets.newHashSet();
+        for (Long beId : backendsWorkingSlots.keySet()) {
+            if (backends.containsKey(beId)) {
+                List<Long> pathHashes = backends.get(beId).getDisks().values().stream().map(v -> v.getPathHash()).collect(Collectors.toList());
+                backendsWorkingSlots.get(beId).updatePaths(pathHashes);
+            } else {
+                deletedBeIds.add(beId);
+            }
+        }
+
+        // delete non-exist backends
+        for (Long beId : deletedBeIds) {
+            backendsWorkingSlots.remove(beId);
+            LOG.info(""delete non exist backend: {}"", beId);
+        }
+
+        // add new backends
+        for (Backend be : backends.values()) {
+            if (!backendsWorkingSlots.containsKey(be.getId())) {
+                List<Long> pathHashes = be.getDisks().values().stream().map(v -> v.getPathHash()).collect(Collectors.toList());
+                PathSlot slot = new PathSlot(pathHashes, Config.schedule_slot_num_per_path);
+                backendsWorkingSlots.put(be.getId(), slot);
+                LOG.info(""add new backend {} with slots num: {}"", be.getId(), be.getDisks().size());
+            }
+        }
+
+        return true;
+    }
+
+    public Map<Long, PathSlot> getBackendsWorkingSlots() {
+        return backendsWorkingSlots;
+    }
+
+    /*
+     * add a ready-to-be-scheduled tablet to pendingTablets, if it has not being added before.
+     * if force is true, do not check if tablet is already added before.
+     */
+    public synchronized boolean addTablet(TabletInfo tablet, boolean force) {
+        if (!force && containsTablet(tablet.getTabletId())) {
+            return false;
+        }
+        allTabletIds.add(tablet.getTabletId());
+        pendingTablets.offer(tablet);
+        return true;
+    }
+
+    public synchronized boolean containsTablet(long tabletId) {
+        return allTabletIds.contains(tabletId);
+    }
+
+    /*
+     * Iterate current tablets, change their priority if necessary.
+     */
+    public synchronized void changePriorityOfTablets(long dbId, long tblId, List<Long> partitionIds) {
+        PriorityQueue<TabletInfo> newPendingTablets = new PriorityQueue<>();
+        for (TabletInfo tabletInfo : pendingTablets) {
+            if (tabletInfo.getDbId() == dbId && tabletInfo.getTblId() == tblId
+                    && partitionIds.contains(tabletInfo.getPartitionId())) {
+                tabletInfo.setOrigPriority(Priority.VERY_HIGH);
+            }
+            newPendingTablets.add(tabletInfo);
+        }
+        pendingTablets = newPendingTablets;
+    }
+
+    /*
+     * TabletScheduler will run as a daemon thread at a very short interval(default 5 sec)
+     * Firstly, it will try to update cluster load statistic and check if priority need to be adjuested.
+     * Than, it will schedule the tablets in pendingTablets.
+     * Thirdly, it will check the current running tasks.
+     * Finally, it try to balance the cluster if possible.
+     * 
+     * Schedule rules:
+     * 1. tablet with higher priority will be scheduled first.
+     * 2. high priority should be downgraded if it fails to be schedule too many times.
+     * 3. priority may be upgraded if it is not being schedule for a long time.
+     * 4. every pending task should has a max scheduled time, if schedule fails too many times, if should be removed.
+     * 5. every running task should has a timeout, to avoid running forever.
+     * 6. every running task should also has a max failure time, if clone task fails too many times, if should be removed.
+     *
+     */
+    @Override
+    protected void runOneCycle() {
+        if (!updateWorkingSlots()) {
+            return;
+        }
+
+        updateClusterLoadStatisticsAndPriorityIfNecessary();
+
+        schedulePendingTablets();
+
+        handleRunningTablets();
+
+        selectTabletsForBalance();
+
+        stat.counterTabletScheduleRound.incrementAndGet();
+    }
+
+
+    private void updateClusterLoadStatisticsAndPriorityIfNecessary() {
+        if (System.currentTimeMillis() - lastStatUpdateTime < STAT_UPDATE_INTERVAL_MS) {
+            return;
+        }
+
+        updateClusterLoadStatistic();
+        adjustPriorities();
+
+        lastStatUpdateTime = System.currentTimeMillis();
+    }
+
+    /*
+     * Here is the only place we update the cluster load statistic info.
+     * We will not update this info dynamically along with the clone job's running.
+     * Although it will cause a little bit inaccurate, but is within a controllable range,
+     * because we already limit the total number of running clone jobs in cluster by 'backend slots'
+     */
+    private void updateClusterLoadStatistic() {
+        statisticMap.clear();
+        List<String> clusterNames = infoService.getClusterNames();
+        for (String clusterName : clusterNames) {
+            ClusterLoadStatistic clusterLoadStatistic = new ClusterLoadStatistic(clusterName, catalog,
+                    infoService, invertedIndex);
+            clusterLoadStatistic.init();
+            statisticMap.put(clusterName, clusterLoadStatistic);
+            LOG.info(""update cluster {} load statistic:\n {}"", clusterName, clusterLoadStatistic.getBrief());
+        }
+    }
+
+    public Map<String, ClusterLoadStatistic> getStatisticMap() {
+        return statisticMap;
+    }
+
+    /*
+     * adjust priorities of all tablet infos
+     */
+    private synchronized void adjustPriorities() {
+        int size = pendingTablets.size();
+        int changedNum = 0;
+        TabletInfo tabletInfo = null;
+        for (int i = 0; i < size; i++) {
+            tabletInfo = pendingTablets.poll();
+            if (tabletInfo == null) {
+                break;
+            }
+
+            if (tabletInfo.adjustPriority(stat)) {
+                changedNum++;
+            }
+            pendingTablets.add(tabletInfo);
+        }
+
+        LOG.info(""adjust priority for all tablets. changed: {}, total: {}"", changedNum, size);
+    }
+
+    /*
+     * get at most BATCH_NUM tablets from queue, and try to schedule them.
+     * After handle, the tablet info should be
+     * 1. in runningTablets with state RUNNING, if being scheduled success.
+     * 2. or in schedHistory with state CANCELLING, if some unrecoverable error happens.
+     * 3. or in pendingTablets with state PENDING, if failed to be scheduled.
+     * 
+     * if in schedHistory, it should be removed from allTabletIds.
+     */
+    private void schedulePendingTablets() {
+        long start = System.currentTimeMillis();
+        List<TabletInfo> currentBatch = getNextTabletInfoBatch();
+        LOG.debug(""get {} tablets to schedule"", currentBatch.size());
+
+        AgentBatchTask batchTask = new AgentBatchTask();
+        for (TabletInfo tabletInfo : currentBatch) {
+            try {
+                scheduleTablet(tabletInfo, batchTask);
+            } catch (SchedException e) {
+                tabletInfo.increaseFailedSchedCounter();
+                tabletInfo.setErrMsg(e.getMessage());
+
+                if (e.getStatus() == Status.SCHEDULE_FAILED) {
+                    // we must release resource it current hold, and be scheduled again
+                    tabletInfo.releaseResource(this);
+                    // adjust priority to avoid some higher priority always be the first in pendingTablets
+                    stat.counterTabletScheduledFailed.incrementAndGet();
+                    dynamicAdjustPrioAndAddBackToPendingTablets(tabletInfo, e.getMessage());
+                } else if (e.getStatus() == Status.FINISHED) {
+                    // schedule redundant tablet will throw this exception
+                    stat.counterTabletScheduledSucceeded.incrementAndGet();
+                    removeTabletInfo(tabletInfo, TabletInfo.State.FINISHED, e.getMessage());
+                } else {
+                    Preconditions.checkState(e.getStatus() == Status.UNRECOVERABLE, e.getStatus());
+                    // discard
+                    stat.counterTabletScheduledDiscard.incrementAndGet();
+                    removeTabletInfo(tabletInfo, TabletInfo.State.CANCELLED, e.getMessage());
+                }
+                continue;
+            }
+
+            Preconditions.checkState(tabletInfo.getState() == TabletInfo.State.RUNNING);
+            stat.counterTabletScheduledSucceeded.incrementAndGet();
+            addToRunningTablets(tabletInfo);
+        }
+
+        // must send task after adding tablet info to runningTablets.
+        for (AgentTask task : batchTask.getAllTasks()) {
+            if (AgentTaskQueue.addTask(task)) {
+                stat.counterCloneTask.incrementAndGet();
+            }
+            LOG.info(""add clone task to agent task queue: {}"", task);
+        }
+
+        // send task immediately
+        AgentTaskExecutor.submit(batchTask);
+
+        long cost = System.currentTimeMillis() - start;
+        stat.counterTabletScheduleCostMs.addAndGet(cost);
+    }
+
+    private synchronized void addToRunningTablets(TabletInfo tabletInfo) {
+        runningTablets.put(tabletInfo.getTabletId(), tabletInfo);
+    }
+
+    /*
+     * we take the tablet out of the runningTablets and than handle it,
+     * avoid other threads see it.
+     * Whoever takes this tablet, make sure to put it to the schedHistory or back to runningTablets.
+     */
+    private synchronized TabletInfo takeRunningTablets(long tabletId) {
+        return runningTablets.remove(tabletId);
+    }
+
+    /*
+     * Try to schedule a single tablet.
+     */
+    private void scheduleTablet(TabletInfo tabletInfo, AgentBatchTask batchTask) throws SchedException {
+        LOG.debug(""schedule tablet: {}"", tabletInfo.getTabletId());
+        long currentTime = System.currentTimeMillis();
+        tabletInfo.setLastSchedTime(currentTime);
+        tabletInfo.setLastVisitedTime(currentTime);
+        stat.counterTabletScheduled.incrementAndGet();
+
+        // check this tablet again
+        Database db = catalog.getDb(tabletInfo.getDbId());
+        if (db == null) {
+            throw new SchedException(Status.UNRECOVERABLE, ""db does not exist"");
+        }
+
+        Pair<TabletStatus, TabletInfo.Priority> statusPair = null;
+        db.writeLock();
+        try {
+            OlapTable tbl = (OlapTable) db.getTable(tabletInfo.getTblId());
+            if (tbl == null) {
+                throw new SchedException(Status.UNRECOVERABLE, ""tbl does not exist"");
+            }
+
+            // we may add a tablet of a NOT NORMAL table during balance, which should be blocked
+            if (tbl.getState() != OlapTableState.NORMAL) {
+                throw new SchedException(Status.UNRECOVERABLE, ""tbl's state is not normal: "" + tbl.getState());
+            }
+
+            Partition partition = tbl.getPartition(tabletInfo.getPartitionId());
+            if (partition == null) {
+                throw new SchedException(Status.UNRECOVERABLE, ""partition does not exist"");
+            }
+
+            MaterializedIndex idx = partition.getIndex(tabletInfo.getIndexId());
+            if (idx == null) {
+                throw new SchedException(Status.UNRECOVERABLE, ""index does not exist"");
+            }
+
+            Tablet tablet = idx.getTablet(tabletInfo.getTabletId());
+            Preconditions.checkNotNull(tablet);
+
+            statusPair = tablet.getHealthStatusWithPriority(
+                    infoService, tabletInfo.getCluster(),
+                    partition.getVisibleVersion(),
+                    partition.getVisibleVersionHash(),
+                    tbl.getPartitionInfo().getReplicationNum(partition.getId()));
+
+            tabletInfo.setTabletStatus(statusPair.first);
+            if (statusPair.first == TabletStatus.HEALTHY && tabletInfo.getType() == TabletInfo.Type.REPAIR) {
+                throw new SchedException(Status.UNRECOVERABLE, ""tablet is healthy"");
+            } else if (statusPair.first != TabletStatus.HEALTHY
+                    && tabletInfo.getType() == TabletInfo.Type.BALANCE) {
+                tabletInfo.releaseResource(this);
+                // we select an unhealthy tablet to do balance, which is not right.
+                // so here we change it to a REPAIR task, and also reset its priority
+                tabletInfo.setType(TabletInfo.Type.REPAIR);
+                tabletInfo.setOrigPriority(statusPair.second);
+            }
+
+            // we do not concern priority here.
+            // once we take the tablet out of priority queue, priority is meaningless.
+            tabletInfo.setTablet(tablet);
+            tabletInfo.setVersionInfo(partition.getVisibleVersion(), partition.getVisibleVersionHash(),
+                    partition.getCommittedVersion(), partition.getCommittedVersionHash());
+            tabletInfo.setSchemaHash(tbl.getSchemaHashByIndexId(idx.getId()));
+            tabletInfo.setStorageMedium(tbl.getPartitionInfo().getDataProperty(partition.getId()).getStorageMedium());
+
+            handleTabletByTypeAndStatus(statusPair.first, tabletInfo, batchTask);
+        } finally {
+            db.writeUnlock();
+        }
+    }
+
+    private void handleTabletByTypeAndStatus(TabletStatus status, TabletInfo tabletInfo, AgentBatchTask batchTask)
+            throws SchedException {
+        if (tabletInfo.getType() == Type.REPAIR) {
+            switch (status) {
+                case REPLICA_MISSING:
+                    handleReplicaMissing(tabletInfo, batchTask);
+                    break;
+                case VERSION_INCOMPLETE:
+                    handleReplicaVersionIncomplete(tabletInfo, batchTask);
+                    break;
+                case REDUNDANT:
+                    handleRedundantReplica(tabletInfo);
+                    break;
+                case REPLICA_MISSING_IN_CLUSTER:
+                    handleReplicaClusterMigration(tabletInfo, batchTask);
+                    break;
+                default:
+                    break;
+            }
+        } else {
+            // balance
+            doBalance(tabletInfo, batchTask);
+        }
+    }
+
+    /*
+     * Replica is missing, which means there is no enough alive replicas.
+     * So we need to find a destination backend to clone a new replica as possible as we can.
+     * 1. find an available path in a backend as destination:
+     *      1. backend need to be alive.
+     *      2. backend of existing replicas should be excluded.
+     *      3. backend has available slot for clone.
+     *      4. replica can fit in the path (consider the threshold of disk capacity and usage percent).
+     *      5. try to find a path with lowest load score.
+     * 2. find an appropriate source replica:
+     *      1. source replica should be healthy
+     *      2. backend of source replica has available slot for clone.
+     *      
+     * 3. send clone task to destination backend
+     */
+    private void handleReplicaMissing(TabletInfo tabletInfo, AgentBatchTask batchTask) throws SchedException {
+        stat.counterReplicaMissingErr.incrementAndGet();
+        // find an available dest backend and path
+        RootPathLoadStatistic destPath = chooseAvailableDestPath(tabletInfo);
+        Preconditions.checkNotNull(destPath);
+        tabletInfo.setDestination(destPath.getBeId(), destPath.getPathHash());
+
+        // choose a source replica for cloning from
+        tabletInfo.chooseSrcReplica(backendsWorkingSlots);
+
+        // create clone task
+        batchTask.addTask(tabletInfo.createCloneReplicaAndTask());
+    }
+
+    /*
+     * Replica version is incomplete, which means this replica is missing some version,
+     * and need to be cloned from a healthy replica, in-place.
+     * 
+     * 1. find the incomplete replica as destination replica
+     * 2. find a healthy replica as source replica
+     * 3. send clone task
+     */
+    private void handleReplicaVersionIncomplete(TabletInfo tabletInfo, AgentBatchTask batchTask)
+            throws SchedException {
+        stat.counterReplicaVersionMissingErr.incrementAndGet();
+        ClusterLoadStatistic statistic = statisticMap.get(tabletInfo.getCluster());
+        if (statistic == null) {
+            throw new SchedException(Status.UNRECOVERABLE, ""cluster does not exist"");
+        }
+
+        tabletInfo.chooseDestReplicaForVersionIncomplete(backendsWorkingSlots);
+        tabletInfo.chooseSrcReplicaForVersionIncomplete(backendsWorkingSlots);
+
+        // create clone task
+        batchTask.addTask(tabletInfo.createCloneReplicaAndTask());
+    }
+
+    /*
+     *  replica is redundant, which means there are more replicas than we expected, which need to be dropped.
+     *  we just drop one redundant replica at a time, for safety reason.
+     *  choosing a replica to drop base on following priority:
+     *  1. backend has been dropped
+     *  2. backend is not available
+     *  3. replica's state is CLONE
+     *  4. replica's last failed version > 0
+     *  5. replica with lower version
+     *  6. replica not in right cluster
+     *  7. replica in higher load backend
+     */
+    private void handleRedundantReplica(TabletInfo tabletInfo) throws SchedException {
+        stat.counterReplicaRedundantErr.incrementAndGet();
+        if (deleteBackendDropped(tabletInfo)
+                || deleteBackendUnavailable(tabletInfo)
+                || deleteCloneReplica(tabletInfo)
+                || deleteReplicaWithFailedVersion(tabletInfo)
+                || deleteReplicaWithLowerVersion(tabletInfo)
+                || deleteReplicaNotInCluster(tabletInfo)
+                || deleteReplicaOnHighLoadBackend(tabletInfo)) {
+            // if we delete at least one redundant replica, we still throw a SchedException with status FINISHED
+            // to remove this tablet from the pendingTablets(consider it as finished)
+            throw new SchedException(Status.FINISHED, ""redundant replica is deleted"");
+        }
+        throw new SchedException(Status.SCHEDULE_FAILED, ""unable to delete any redundant replicas"");
+    }
+
+    private boolean deleteBackendDropped(TabletInfo tabletInfo) {
+        for (Replica replica : tabletInfo.getReplicas()) {
+            long beId = replica.getBackendId();
+            if (infoService.getBackend(beId) == null) {
+                deleteReplicaInternal(tabletInfo, replica, ""backend dropped"");
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private boolean deleteBackendUnavailable(TabletInfo tabletInfo) {
+        for (Replica replica : tabletInfo.getReplicas()) {
+            Backend be = infoService.getBackend(replica.getBackendId());
+            if (be == null) {
+                // this case should be handled in deleteBackendDropped()
+                continue;
+            }
+            if (!be.isAvailable()) {
+                deleteReplicaInternal(tabletInfo, replica, ""backend unavailable"");
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private boolean deleteCloneReplica(TabletInfo tabletInfo) {
+        for (Replica replica : tabletInfo.getReplicas()) {
+            if (replica.getState() == ReplicaState.CLONE) {
+                deleteReplicaInternal(tabletInfo, replica, ""clone state"");
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private boolean deleteReplicaWithFailedVersion(TabletInfo tabletInfo) {
+        for (Replica replica : tabletInfo.getReplicas()) {
+            if (replica.getLastFailedVersion() > 0) {
+                deleteReplicaInternal(tabletInfo, replica, ""version incomplete"");
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private boolean deleteReplicaWithLowerVersion(TabletInfo tabletInfo) {
+        for (Replica replica : tabletInfo.getReplicas()) {
+            if (!replica.checkVersionCatchUp(tabletInfo.getCommittedVersion(), tabletInfo.getCommittedVersionHash())) {
+                deleteReplicaInternal(tabletInfo, replica, ""lower version"");
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private boolean deleteReplicaNotInCluster(TabletInfo tabletInfo) {
+        for (Replica replica : tabletInfo.getReplicas()) {
+            Backend be = infoService.getBackend(replica.getBackendId());
+            if (be == null) {
+                // this case should be handled in deleteBackendDropped()
+                continue;
+            }
+            if (!be.getOwnerClusterName().equals(tabletInfo.getCluster())) {
+                deleteReplicaInternal(tabletInfo, replica, ""not in cluster"");
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private boolean deleteReplicaOnHighLoadBackend(TabletInfo tabletInfo) {
+        ClusterLoadStatistic statistic = statisticMap.get(tabletInfo.getCluster());
+        if (statistic == null) {
+            return false;
+        }
+        
+        Replica chosenReplica = null;
+        double maxScore = 0;
+        for (Replica replica : tabletInfo.getReplicas()) {
+            BackendLoadStatistic beStatistic = statistic.getBackendLoadStatistic(replica.getBackendId());
+            if (beStatistic == null) {
+                continue;
+            }
+            if (beStatistic.getLoadScore() > maxScore) {
+                maxScore = beStatistic.getLoadScore();
+                chosenReplica = replica;
+            }
+        }
+
+        if (chosenReplica != null) {
+            deleteReplicaInternal(tabletInfo, chosenReplica, ""high load"");
+            return true;
+        }
+        return false;
+    }
+
+    private void deleteReplicaInternal(TabletInfo tabletInfo, Replica replica, String reason) {
+        // delete this replica from catalog.
+        // it will also delete replica from tablet inverted index.
+        tabletInfo.deleteReplica(replica);
+
+        // TODO(cmy): this should be removed after I finish modifying alter job logic
+        // Catalog.getInstance().handleJobsWhenDeleteReplica(tabletInfo.getTblId(), tabletInfo.getPartitionId(),
+        //                                                   tabletInfo.getIndexId(), tabletInfo.getTabletId(),
+        //                                                   replica.getId(), replica.getBackendId());
+
+        // write edit log
+        ReplicaPersistInfo info = ReplicaPersistInfo.createForDelete(tabletInfo.getDbId(),
+                                                                     tabletInfo.getTblId(),
+                                                                     tabletInfo.getPartitionId(),
+                                                                     tabletInfo.getIndexId(),
+                                                                     tabletInfo.getTabletId(),
+                                                                     replica.getBackendId());
+
+        Catalog.getInstance().getEditLog().logDeleteReplica(info);
+
+        LOG.info(""delete replica. tablet id: {}, backend id: {}. reason: {}"",
+                 tabletInfo.getTabletId(), replica.getBackendId(), reason);
+    }
+
+    /*
+     * Cluster migration, which means the tablet has enough healthy replicas,
+     * but some replicas are not in right cluster.
+     * It is just same as 'replica missing'.
+     * 
+     * after clone finished, the replica in wrong cluster will be treated as redundant, and will be deleted soon.
+     */
+    private void handleReplicaClusterMigration(TabletInfo tabletInfo, AgentBatchTask batchTask)
+            throws SchedException {
+        stat.counterReplicaMissingInClusterErr.incrementAndGet();
+        handleReplicaMissing(tabletInfo, batchTask);
+    }
+
+    /*
+     * Try to select some alternative tablets for balance. Add them to pendingTablets with priority LOW,
+     * and waiting to be scheduled.
+     */
+    private void selectTabletsForBalance() {
+        LoadBalancer loadBalancer = new LoadBalancer(statisticMap);
+        List<TabletInfo> alternativeTablets = loadBalancer.selectAlternativeTablets();
+        for (TabletInfo tabletInfo : alternativeTablets) {
+            addTablet(tabletInfo, false);
+        }
+    }
+
+    /*
+     * Try to create a balance task for a tablet.
+     */
+    private void doBalance(TabletInfo tabletInfo, AgentBatchTask batchTask) throws SchedException {
+        stat.counterBalanceSchedule.incrementAndGet();
+        LoadBalancer loadBalancer = new LoadBalancer(statisticMap);
+        loadBalancer.createBalanceTask(tabletInfo, backendsWorkingSlots, batchTask);
+    }
+
+    // choose a path on a backend which is fit for the tablet
+    private RootPathLoadStatistic chooseAvailableDestPath(TabletInfo tabletInfo) throws SchedException {
+        ClusterLoadStatistic statistic = statisticMap.get(tabletInfo.getCluster());
+        if (statistic == null) {
+            throw new SchedException(Status.UNRECOVERABLE, ""cluster does not exist"");
+        }
+        List<BackendLoadStatistic> beStatistics = statistic.getBeLoadStatistics();
+
+        // get all available paths which this tablet can fit in.
+        // beStatistics is sorted by load score in ascend order, so select from first to last.
+        List<RootPathLoadStatistic> allFitPaths = Lists.newArrayList();
+        for (int i = 0; i < beStatistics.size(); i++) {
+            BackendLoadStatistic bes = beStatistics.get(i);
+            // exclude BE which already has replica of this tablet
+            if (tabletInfo.containsBE(bes.getBeId())) {
+                continue;
+            }
+
+            List<RootPathLoadStatistic> resultPaths = Lists.newArrayList();
+            BalanceStatus st = bes.isFit(tabletInfo.getTabletSize(), resultPaths, true /* is supplement */);
+            if (!st.ok()) {
+                LOG.debug(""unable to find path for supplementing tablet: {}. {}"", tabletInfo, st);
+                continue;
+            }
+
+            Preconditions.checkState(resultPaths.size() == 1);
+            allFitPaths.add(resultPaths.get(0));
+        }
+
+        if (allFitPaths.isEmpty()) {
+            throw new SchedException(Status.SCHEDULE_FAILED, ""unable to find dest path for new replica"");
+        }
+
+        // all fit paths has already been sorted by load score in 'allFitPaths' in ascend order.
+        // just get first available path.
+        // we try to find a path with specified media type, if not find, arbitrarily use one.
+        for (RootPathLoadStatistic rootPathLoadStatistic : allFitPaths) {
+            if (rootPathLoadStatistic.getStorageMedium() != tabletInfo.getStorageMedium()) {
+                continue;
+            }
+
+            PathSlot slot = backendsWorkingSlots.get(rootPathLoadStatistic.getBeId());
+            if (slot == null) {
+                LOG.debug(""backend {} does not found when getting slots"", rootPathLoadStatistic.getBeId());
+                continue;
+            }
+
+            if (slot.takeSlot(rootPathLoadStatistic.getPathHash()) != -1) {
+                return rootPathLoadStatistic;
+            }
+        }
+
+        // no root path with specified media type is found, get arbitrary one.
+        for (RootPathLoadStatistic rootPathLoadStatistic : allFitPaths) {
+            PathSlot slot = backendsWorkingSlots.get(rootPathLoadStatistic.getBeId());
+            if (slot == null) {
+                LOG.debug(""backend {} does not found when getting slots"", rootPathLoadStatistic.getBeId());
+                continue;
+            }
+
+            if (slot.takeSlot(rootPathLoadStatistic.getPathHash()) != -1) {
+                return rootPathLoadStatistic;
+            }
+        }
+        
+        throw new SchedException(Status.SCHEDULE_FAILED, ""unable to find dest path which can be fit in"");
+    }
+
+    /*
+     * For some reason, a tablet info failed to be scheduled this time,
+     * So we dynamically change its priority and add back to queue, waiting for next round.
+     */
+    private void dynamicAdjustPrioAndAddBackToPendingTablets(TabletInfo tabletInfo, String message) {
+        Preconditions.checkState(tabletInfo.getState() == TabletInfo.State.PENDING);
+        tabletInfo.adjustPriority(stat);
+        addTablet(tabletInfo, true /* force */);
+    }
+
+    private synchronized void removeTabletInfo(TabletInfo tabletInfo, TabletInfo.State state, String reason) {
+        tabletInfo.setState(state);
+        tabletInfo.releaseResource(this);
+        tabletInfo.setFinishedTime(System.currentTimeMillis());
+        runningTablets.remove(tabletInfo.getTabletId());
+        allTabletIds.remove(tabletInfo.getTabletId());
+        schedHistory.add(tabletInfo);
+        LOG.info(""remove the tablet {}. because: {}"", tabletInfo.getTabletId(), reason);
+    }
+
+    // get next batch of tablets from queue.
+    private synchronized List<TabletInfo> getNextTabletInfoBatch() {
+        List<TabletInfo> list = Lists.newArrayList();
+        int count = Math.max(MIN_BATCH_NUM, getCurrentAvailableSlotNum());
+        while (count > 0) {
+            TabletInfo tablet = pendingTablets.poll();
+            if (tablet == null) {
+                // no more tablets
+                break;
+            }
+            list.add(tablet);
+            count--;
+        }
+        return list;
+    }
+
+    private int getCurrentAvailableSlotNum() {
+        int total = 0;
+        for (PathSlot pathSlot : backendsWorkingSlots.values()) {
+            total += pathSlot.getTotalAvailSlotNum();
+        }
+        return total;
+    }
+
+    /*
+     * return true if we want to remove the clone task from AgentTaskQueu
+     */
+    public boolean finishCloneTask(CloneTask cloneTask, TFinishTaskRequest request) {
+        long tabletId = cloneTask.getTabletId();
+        TabletInfo tabletInfo = takeRunningTablets(tabletId);
+        if (tabletInfo == null) {
+            LOG.warn(""tablet info does not exist: {}"", tabletId);
+            // tablet does not exist, no need to keep task.
+            return true;
+        }
+        Preconditions.checkState(tabletInfo.getState() == TabletInfo.State.RUNNING);
+        try {
+            tabletInfo.finishCloneTask(cloneTask, request);
+        } catch (SchedException e) {
+            tabletInfo.increaseFailedRunningCounter();
+            tabletInfo.setErrMsg(e.getMessage());
+            if (e.getStatus() == Status.RUNNING_FAILED) {
+                stat.counterCloneTaskFailed.incrementAndGet();
+                addToRunningTablets(tabletInfo);
+                return false;
+            } else {
+                Preconditions.checkState(e.getStatus() == Status.UNRECOVERABLE, e.getStatus());
+                // unrecoverable
+                stat.counterTabletScheduledDiscard.incrementAndGet();
+                removeTabletInfo(tabletInfo, TabletInfo.State.CANCELLED, e.getMessage());
+                return true;
+            }
+        }
+
+        Preconditions.checkState(tabletInfo.getState() == TabletInfo.State.FINISHED);
+        stat.counterCloneTaskSucceeded.incrementAndGet();
+        gatherStatistics(tabletInfo);
+        removeTabletInfo(tabletInfo, TabletInfo.State.FINISHED, ""finished"");
+        return true;
+    }
+
+    /*
+     * Gather the running statistic of the task.
+     * It will be evaluated for future strategy.  
+     * This should only be called when the tablet is down with state FINISHED.
+     */
+    private void gatherStatistics(TabletInfo tabletInfo) {
+        if (tabletInfo.getCopySize() > 0 && tabletInfo.getCopyTimeMs() > 0) {
+            if (tabletInfo.getSrcBackendId() != -1 && tabletInfo.getSrcPathHash() != -1) {
+                PathSlot pathSlot = backendsWorkingSlots.get(tabletInfo.getSrcBackendId());
+                if (pathSlot != null) {
+                    pathSlot.updateStatistic(tabletInfo.getSrcPathHash(), tabletInfo.getCopySize(),
+                            tabletInfo.getCopyTimeMs());
+                }
+            }
+
+            if (tabletInfo.getDestBackendId() != -1 && tabletInfo.getDestPathHash() != -1) {
+                PathSlot pathSlot = backendsWorkingSlots.get(tabletInfo.getDestBackendId());
+                if (pathSlot != null) {
+                    pathSlot.updateStatistic(tabletInfo.getDestPathHash(), tabletInfo.getCopySize(),
+                            tabletInfo.getCopyTimeMs());
+                }
+            }
+        }
+
+        if (System.currentTimeMillis() - lastSlotAdjustTime < STAT_UPDATE_INTERVAL_MS) {
+            return;
+        }
+
+        // TODO(cmy): update the slot num base on statistic.
+        // need to find a better way to determine the slot number.
+
+        lastSlotAdjustTime = System.currentTimeMillis();
+    }
+
+    /*
+     * handle tablets which are running.
+     * We should finished the task if
+     * 1. Tablet is already healthy
+     * 2. Task is timeout.
+     * 
+     * But here we just handle the timeout case here. Let the 'finishCloneTask()' check if tablet is healthy.
+     * We guarantee that if tablet is in runningTablets, the 'finishCloneTask()' will finally be called,
+     * so no need to worry that running tablets will never end.
+     * This is also avoid nesting 'synchronized' and database lock.
+     *
+     * If task is timeout, remove the tablet.
+     */
+    public synchronized void handleRunningTablets() {
+        List<TabletInfo> timeoutTablets = Lists.newArrayList();
+        runningTablets.values().stream().filter(t -> t.isTimeout()).forEach(t -> {
+            timeoutTablets.add(t);
+        });
+        
+        timeoutTablets.stream().forEach(t -> {
+            removeTabletInfo(t, TabletInfo.State.TIMEOUT, ""timeout"");
+            stat.counterCloneTaskTimeout.incrementAndGet();
+        });
+    }
+
+    public List<List<String>> getPendingTabletsInfo(int limit) {
+        List<TabletInfo> tabletInfos = getCopiedTablets(pendingTablets, limit);
+        return collectTabletInfo(tabletInfos);
+    }
+
+    public List<List<String>> getRunningTabletsInfo(int limit) {
+        List<TabletInfo> tabletInfos = getCopiedTablets(runningTablets.values(), limit);
+        return collectTabletInfo(tabletInfos);
+    }
+
+    public List<List<String>> getHistoryTabletsInfo(int limit) {
+        List<TabletInfo> tabletInfos = getCopiedTablets(schedHistory, limit);
+        return collectTabletInfo(tabletInfos);
+    }
+
+    private List<List<String>> collectTabletInfo(List<TabletInfo> tabletInfos) {
+        List<List<String>> result = Lists.newArrayList();
+        tabletInfos.stream().forEach(t -> {
+            result.add(t.getBrief());
+        });
+        return result;
+    }
+
+    private synchronized List<TabletInfo> getCopiedTablets(Collection<TabletInfo> source, int limit) {
+        List<TabletInfo> tabletInfos = Lists.newArrayList();
+        source.stream().limit(limit).forEach(t -> {
+            tabletInfos.add(t);
+        });
+        return tabletInfos;
+    }
+
+    public synchronized int getPendingNum() {
+        return pendingTablets.size();
+    }
+
+    public synchronized int getRunningNum() {
+        return runningTablets.size();
+    }
+
+    public synchronized int getHistoryNum() {
+        return schedHistory.size();
+    }
+
+    /*
+     * PathSlot keeps track of slot num per path of a Backend.
+     * Each path on a Backend has several slot.
+     * If a path's available slot num because 0, no task should be assigned to this path.
+     */
+    public class PathSlot {
+        // path hash -> slot num
+        private Map<Long, Slot> pathSlots = Maps.newConcurrentMap();
+
+        public PathSlot(List<Long> paths, int initSlotNum) {
+            for (Long pathHash : paths) {
+                pathSlots.put(pathHash, new Slot(initSlotNum));
+            }
+        }
+
+        // update the path
+        public synchronized void updatePaths(List<Long> paths) {
+            // delete non exist path
+            Iterator<Map.Entry<Long, Slot>> iter = pathSlots.entrySet().iterator();
+            while (iter.hasNext()) {
+                Map.Entry<Long, Slot> entry = iter.next();
+                if (!paths.contains(entry.getKey())) {
+                    iter.remove();
+                }
+            }
+
+            // add new path
+            for (Long pathHash : paths) {
+                if (!pathSlots.containsKey(pathHash)) {
+                    pathSlots.put(pathHash, new Slot(Config.schedule_slot_num_per_path));
+                }
+            }
+        }
+
+        // Update the total slots num of specified paths, increase or decrease
+        public synchronized void updateSlot(List<Long> pathHashs, boolean increase) {","[{'comment': '```suggestion\r\n        public synchronized void updateSlot(List<Long> pathHashs, int delta) {\r\n```', 'commenter': 'imay'}]"
336,fe/src/main/java/org/apache/doris/clone/TabletScheduler.java,"@@ -0,0 +1,1186 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.clone;
+
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.catalog.Database;
+import org.apache.doris.catalog.MaterializedIndex;
+import org.apache.doris.catalog.OlapTable;
+import org.apache.doris.catalog.OlapTable.OlapTableState;
+import org.apache.doris.catalog.Partition;
+import org.apache.doris.catalog.Replica;
+import org.apache.doris.catalog.Replica.ReplicaState;
+import org.apache.doris.catalog.Tablet;
+import org.apache.doris.catalog.Tablet.TabletStatus;
+import org.apache.doris.catalog.TabletInvertedIndex;
+import org.apache.doris.clone.SchedException.Status;
+import org.apache.doris.clone.TabletInfo.Priority;
+import org.apache.doris.clone.TabletInfo.Type;
+import org.apache.doris.common.Config;
+import org.apache.doris.common.Pair;
+import org.apache.doris.common.util.Daemon;
+import org.apache.doris.persist.ReplicaPersistInfo;
+import org.apache.doris.system.Backend;
+import org.apache.doris.system.SystemInfoService;
+import org.apache.doris.task.AgentBatchTask;
+import org.apache.doris.task.AgentTask;
+import org.apache.doris.task.AgentTaskExecutor;
+import org.apache.doris.task.AgentTaskQueue;
+import org.apache.doris.task.CloneTask;
+import org.apache.doris.thrift.TFinishTaskRequest;
+
+import com.google.common.base.Preconditions;
+import com.google.common.collect.EvictingQueue;
+import com.google.common.collect.ImmutableMap;
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+import com.google.common.collect.Sets;
+
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.Collection;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.PriorityQueue;
+import java.util.Queue;
+import java.util.Set;
+import java.util.stream.Collectors;
+
+/*
+ * TabletScheduler saved the tablets produced by TabletChecker and try to schedule them.
+ * It also try to balance the cluster load.
+ * 
+ * We are expecting an efficient way to recovery the entire cluster and make it balanced.
+ * Case 1:
+ *  A Backend is down. All tablets which has replica on this BE should be repaired as soon as possible.
+ *  
+ * Case 1.1:
+ *  As Backend is down, some tables should be repaired in high priority. So the clone task should be able
+ *  to preempted.
+ *  
+ * Case 2:
+ *  A new Backend is added to the cluster. Replicas should be transfer to that host to balance the cluster load.
+ */
+public class TabletScheduler extends Daemon {
+    private static final Logger LOG = LogManager.getLogger(TabletScheduler.class);
+
+    // handle at most BATCH_NUM tablets in one loop
+    private static final int MIN_BATCH_NUM = 10;
+
+    // the minimum interval of updating cluster statistics and priority of tablet info
+    private static final long STAT_UPDATE_INTERVAL_MS = 60 * 1000; // 1min
+
+    private static final long SCHEDULE_INTERVAL_MS = 5000; // 5s
+
+    public static final int BALANCE_SLOT_NUM_FOR_PATH = 2;
+
+    /*
+     * Tablet is added to pendingTablets as well it's id in allTabletIds.
+     * TabletScheduler will take tablet from pendingTablets but will not remove it's id from allTabletIds when
+     * handling a tablet.
+     * Tablet' id can only be removed after the clone task is done(timeout, cancelled or finished).
+     * So if a tablet's id is still in allTabletIds, TabletChecker can not add tablet to TabletScheduler.
+     * 
+     * pendingTablets + runningTablets = allTabletIds
+     * 
+     * pendingTablets, allTabletIds, runningTablets and schedHistory are protected by 'synchronized' 
+     */
+    private PriorityQueue<TabletInfo> pendingTablets = new PriorityQueue<>();
+    private Set<Long> allTabletIds = Sets.newHashSet();
+    // contains all tabletInfos which state are RUNNING
+    private Map<Long, TabletInfo> runningTablets = Maps.newHashMap();
+    // save the latest 1000 scheduled tablet info
+    private Queue<TabletInfo> schedHistory = EvictingQueue.create(1000);
+
+    // be id -> #working slots
+    private Map<Long, PathSlot> backendsWorkingSlots = Maps.newConcurrentMap();
+    // cluster name -> load statistic
+    private Map<String, ClusterLoadStatistic> statisticMap = Maps.newConcurrentMap();
+    private long lastStatUpdateTime = 0;
+    
+    private long lastSlotAdjustTime = 0;
+
+    private Catalog catalog;
+    private SystemInfoService infoService;
+    private TabletInvertedIndex invertedIndex;
+    private TabletSchedulerStat stat;
+
+    public TabletScheduler(Catalog catalog, SystemInfoService infoService, TabletInvertedIndex invertedIndex,
+            TabletSchedulerStat stat) {
+        super(""tablet scheduler"", SCHEDULE_INTERVAL_MS);
+        this.catalog = catalog;
+        this.infoService = infoService;
+        this.invertedIndex = invertedIndex;
+        this.stat = stat;
+    }
+
+    public TabletSchedulerStat getStat() {
+        return stat;
+    }
+
+    /*
+     * update working slots at the beginning of each round
+     */
+    private boolean updateWorkingSlots() {
+        ImmutableMap<Long, Backend> backends = infoService.getBackendsInCluster(null);
+        for (Backend backend : backends.values()) {
+            if (!backend.hasPathHash() && backend.isAlive()) {
+                // when upgrading, backend may not get path info yet. so return false and wait for next round.
+                // and we should check if backend is alive. If backend is dead when upgrading, this backend
+                // will never report its path hash, and tablet scheduler is blocked.
+                LOG.info(""not all backends have path info"");
+                return false;
+            }
+        }
+
+        // update exist backends
+        Set<Long> deletedBeIds = Sets.newHashSet();
+        for (Long beId : backendsWorkingSlots.keySet()) {
+            if (backends.containsKey(beId)) {
+                List<Long> pathHashes = backends.get(beId).getDisks().values().stream().map(v -> v.getPathHash()).collect(Collectors.toList());
+                backendsWorkingSlots.get(beId).updatePaths(pathHashes);
+            } else {
+                deletedBeIds.add(beId);
+            }
+        }
+
+        // delete non-exist backends
+        for (Long beId : deletedBeIds) {
+            backendsWorkingSlots.remove(beId);
+            LOG.info(""delete non exist backend: {}"", beId);
+        }
+
+        // add new backends
+        for (Backend be : backends.values()) {
+            if (!backendsWorkingSlots.containsKey(be.getId())) {
+                List<Long> pathHashes = be.getDisks().values().stream().map(v -> v.getPathHash()).collect(Collectors.toList());
+                PathSlot slot = new PathSlot(pathHashes, Config.schedule_slot_num_per_path);
+                backendsWorkingSlots.put(be.getId(), slot);
+                LOG.info(""add new backend {} with slots num: {}"", be.getId(), be.getDisks().size());
+            }
+        }
+
+        return true;
+    }
+
+    public Map<Long, PathSlot> getBackendsWorkingSlots() {
+        return backendsWorkingSlots;
+    }
+
+    /*
+     * add a ready-to-be-scheduled tablet to pendingTablets, if it has not being added before.
+     * if force is true, do not check if tablet is already added before.
+     */
+    public synchronized boolean addTablet(TabletInfo tablet, boolean force) {
+        if (!force && containsTablet(tablet.getTabletId())) {
+            return false;
+        }
+        allTabletIds.add(tablet.getTabletId());
+        pendingTablets.offer(tablet);
+        return true;
+    }
+
+    public synchronized boolean containsTablet(long tabletId) {
+        return allTabletIds.contains(tabletId);
+    }
+
+    /*
+     * Iterate current tablets, change their priority if necessary.
+     */
+    public synchronized void changePriorityOfTablets(long dbId, long tblId, List<Long> partitionIds) {
+        PriorityQueue<TabletInfo> newPendingTablets = new PriorityQueue<>();
+        for (TabletInfo tabletInfo : pendingTablets) {
+            if (tabletInfo.getDbId() == dbId && tabletInfo.getTblId() == tblId
+                    && partitionIds.contains(tabletInfo.getPartitionId())) {
+                tabletInfo.setOrigPriority(Priority.VERY_HIGH);
+            }
+            newPendingTablets.add(tabletInfo);
+        }
+        pendingTablets = newPendingTablets;
+    }
+
+    /*
+     * TabletScheduler will run as a daemon thread at a very short interval(default 5 sec)
+     * Firstly, it will try to update cluster load statistic and check if priority need to be adjuested.
+     * Than, it will schedule the tablets in pendingTablets.
+     * Thirdly, it will check the current running tasks.
+     * Finally, it try to balance the cluster if possible.
+     * 
+     * Schedule rules:
+     * 1. tablet with higher priority will be scheduled first.
+     * 2. high priority should be downgraded if it fails to be schedule too many times.
+     * 3. priority may be upgraded if it is not being schedule for a long time.
+     * 4. every pending task should has a max scheduled time, if schedule fails too many times, if should be removed.
+     * 5. every running task should has a timeout, to avoid running forever.
+     * 6. every running task should also has a max failure time, if clone task fails too many times, if should be removed.
+     *
+     */
+    @Override
+    protected void runOneCycle() {
+        if (!updateWorkingSlots()) {
+            return;
+        }
+
+        updateClusterLoadStatisticsAndPriorityIfNecessary();
+
+        schedulePendingTablets();
+
+        handleRunningTablets();
+
+        selectTabletsForBalance();
+
+        stat.counterTabletScheduleRound.incrementAndGet();
+    }
+
+
+    private void updateClusterLoadStatisticsAndPriorityIfNecessary() {
+        if (System.currentTimeMillis() - lastStatUpdateTime < STAT_UPDATE_INTERVAL_MS) {
+            return;
+        }
+
+        updateClusterLoadStatistic();
+        adjustPriorities();
+
+        lastStatUpdateTime = System.currentTimeMillis();
+    }
+
+    /*
+     * Here is the only place we update the cluster load statistic info.
+     * We will not update this info dynamically along with the clone job's running.
+     * Although it will cause a little bit inaccurate, but is within a controllable range,
+     * because we already limit the total number of running clone jobs in cluster by 'backend slots'
+     */
+    private void updateClusterLoadStatistic() {
+        statisticMap.clear();
+        List<String> clusterNames = infoService.getClusterNames();
+        for (String clusterName : clusterNames) {
+            ClusterLoadStatistic clusterLoadStatistic = new ClusterLoadStatistic(clusterName, catalog,
+                    infoService, invertedIndex);
+            clusterLoadStatistic.init();
+            statisticMap.put(clusterName, clusterLoadStatistic);
+            LOG.info(""update cluster {} load statistic:\n {}"", clusterName, clusterLoadStatistic.getBrief());
+        }
+    }
+
+    public Map<String, ClusterLoadStatistic> getStatisticMap() {
+        return statisticMap;
+    }
+
+    /*
+     * adjust priorities of all tablet infos
+     */
+    private synchronized void adjustPriorities() {
+        int size = pendingTablets.size();
+        int changedNum = 0;
+        TabletInfo tabletInfo = null;
+        for (int i = 0; i < size; i++) {
+            tabletInfo = pendingTablets.poll();
+            if (tabletInfo == null) {
+                break;
+            }
+
+            if (tabletInfo.adjustPriority(stat)) {
+                changedNum++;
+            }
+            pendingTablets.add(tabletInfo);
+        }
+
+        LOG.info(""adjust priority for all tablets. changed: {}, total: {}"", changedNum, size);
+    }
+
+    /*
+     * get at most BATCH_NUM tablets from queue, and try to schedule them.
+     * After handle, the tablet info should be
+     * 1. in runningTablets with state RUNNING, if being scheduled success.
+     * 2. or in schedHistory with state CANCELLING, if some unrecoverable error happens.
+     * 3. or in pendingTablets with state PENDING, if failed to be scheduled.
+     * 
+     * if in schedHistory, it should be removed from allTabletIds.
+     */
+    private void schedulePendingTablets() {
+        long start = System.currentTimeMillis();
+        List<TabletInfo> currentBatch = getNextTabletInfoBatch();
+        LOG.debug(""get {} tablets to schedule"", currentBatch.size());
+
+        AgentBatchTask batchTask = new AgentBatchTask();
+        for (TabletInfo tabletInfo : currentBatch) {
+            try {
+                scheduleTablet(tabletInfo, batchTask);
+            } catch (SchedException e) {
+                tabletInfo.increaseFailedSchedCounter();
+                tabletInfo.setErrMsg(e.getMessage());
+
+                if (e.getStatus() == Status.SCHEDULE_FAILED) {
+                    // we must release resource it current hold, and be scheduled again
+                    tabletInfo.releaseResource(this);
+                    // adjust priority to avoid some higher priority always be the first in pendingTablets
+                    stat.counterTabletScheduledFailed.incrementAndGet();
+                    dynamicAdjustPrioAndAddBackToPendingTablets(tabletInfo, e.getMessage());
+                } else if (e.getStatus() == Status.FINISHED) {
+                    // schedule redundant tablet will throw this exception
+                    stat.counterTabletScheduledSucceeded.incrementAndGet();
+                    removeTabletInfo(tabletInfo, TabletInfo.State.FINISHED, e.getMessage());
+                } else {
+                    Preconditions.checkState(e.getStatus() == Status.UNRECOVERABLE, e.getStatus());
+                    // discard
+                    stat.counterTabletScheduledDiscard.incrementAndGet();
+                    removeTabletInfo(tabletInfo, TabletInfo.State.CANCELLED, e.getMessage());
+                }
+                continue;
+            }
+
+            Preconditions.checkState(tabletInfo.getState() == TabletInfo.State.RUNNING);
+            stat.counterTabletScheduledSucceeded.incrementAndGet();
+            addToRunningTablets(tabletInfo);
+        }
+
+        // must send task after adding tablet info to runningTablets.
+        for (AgentTask task : batchTask.getAllTasks()) {
+            if (AgentTaskQueue.addTask(task)) {
+                stat.counterCloneTask.incrementAndGet();
+            }
+            LOG.info(""add clone task to agent task queue: {}"", task);
+        }
+
+        // send task immediately
+        AgentTaskExecutor.submit(batchTask);
+
+        long cost = System.currentTimeMillis() - start;
+        stat.counterTabletScheduleCostMs.addAndGet(cost);
+    }
+
+    private synchronized void addToRunningTablets(TabletInfo tabletInfo) {
+        runningTablets.put(tabletInfo.getTabletId(), tabletInfo);
+    }
+
+    /*
+     * we take the tablet out of the runningTablets and than handle it,
+     * avoid other threads see it.
+     * Whoever takes this tablet, make sure to put it to the schedHistory or back to runningTablets.
+     */
+    private synchronized TabletInfo takeRunningTablets(long tabletId) {
+        return runningTablets.remove(tabletId);
+    }
+
+    /*
+     * Try to schedule a single tablet.
+     */
+    private void scheduleTablet(TabletInfo tabletInfo, AgentBatchTask batchTask) throws SchedException {
+        LOG.debug(""schedule tablet: {}"", tabletInfo.getTabletId());
+        long currentTime = System.currentTimeMillis();
+        tabletInfo.setLastSchedTime(currentTime);
+        tabletInfo.setLastVisitedTime(currentTime);
+        stat.counterTabletScheduled.incrementAndGet();
+
+        // check this tablet again
+        Database db = catalog.getDb(tabletInfo.getDbId());
+        if (db == null) {
+            throw new SchedException(Status.UNRECOVERABLE, ""db does not exist"");
+        }
+
+        Pair<TabletStatus, TabletInfo.Priority> statusPair = null;
+        db.writeLock();
+        try {
+            OlapTable tbl = (OlapTable) db.getTable(tabletInfo.getTblId());
+            if (tbl == null) {
+                throw new SchedException(Status.UNRECOVERABLE, ""tbl does not exist"");
+            }
+
+            // we may add a tablet of a NOT NORMAL table during balance, which should be blocked
+            if (tbl.getState() != OlapTableState.NORMAL) {
+                throw new SchedException(Status.UNRECOVERABLE, ""tbl's state is not normal: "" + tbl.getState());
+            }
+
+            Partition partition = tbl.getPartition(tabletInfo.getPartitionId());
+            if (partition == null) {
+                throw new SchedException(Status.UNRECOVERABLE, ""partition does not exist"");
+            }
+
+            MaterializedIndex idx = partition.getIndex(tabletInfo.getIndexId());
+            if (idx == null) {
+                throw new SchedException(Status.UNRECOVERABLE, ""index does not exist"");
+            }
+
+            Tablet tablet = idx.getTablet(tabletInfo.getTabletId());
+            Preconditions.checkNotNull(tablet);
+
+            statusPair = tablet.getHealthStatusWithPriority(
+                    infoService, tabletInfo.getCluster(),
+                    partition.getVisibleVersion(),
+                    partition.getVisibleVersionHash(),
+                    tbl.getPartitionInfo().getReplicationNum(partition.getId()));
+
+            tabletInfo.setTabletStatus(statusPair.first);
+            if (statusPair.first == TabletStatus.HEALTHY && tabletInfo.getType() == TabletInfo.Type.REPAIR) {
+                throw new SchedException(Status.UNRECOVERABLE, ""tablet is healthy"");
+            } else if (statusPair.first != TabletStatus.HEALTHY
+                    && tabletInfo.getType() == TabletInfo.Type.BALANCE) {
+                tabletInfo.releaseResource(this);
+                // we select an unhealthy tablet to do balance, which is not right.
+                // so here we change it to a REPAIR task, and also reset its priority
+                tabletInfo.setType(TabletInfo.Type.REPAIR);
+                tabletInfo.setOrigPriority(statusPair.second);
+            }
+
+            // we do not concern priority here.
+            // once we take the tablet out of priority queue, priority is meaningless.
+            tabletInfo.setTablet(tablet);
+            tabletInfo.setVersionInfo(partition.getVisibleVersion(), partition.getVisibleVersionHash(),
+                    partition.getCommittedVersion(), partition.getCommittedVersionHash());
+            tabletInfo.setSchemaHash(tbl.getSchemaHashByIndexId(idx.getId()));
+            tabletInfo.setStorageMedium(tbl.getPartitionInfo().getDataProperty(partition.getId()).getStorageMedium());
+
+            handleTabletByTypeAndStatus(statusPair.first, tabletInfo, batchTask);
+        } finally {
+            db.writeUnlock();
+        }
+    }
+
+    private void handleTabletByTypeAndStatus(TabletStatus status, TabletInfo tabletInfo, AgentBatchTask batchTask)
+            throws SchedException {
+        if (tabletInfo.getType() == Type.REPAIR) {
+            switch (status) {
+                case REPLICA_MISSING:
+                    handleReplicaMissing(tabletInfo, batchTask);
+                    break;
+                case VERSION_INCOMPLETE:
+                    handleReplicaVersionIncomplete(tabletInfo, batchTask);
+                    break;
+                case REDUNDANT:
+                    handleRedundantReplica(tabletInfo);
+                    break;
+                case REPLICA_MISSING_IN_CLUSTER:
+                    handleReplicaClusterMigration(tabletInfo, batchTask);
+                    break;
+                default:
+                    break;
+            }
+        } else {
+            // balance
+            doBalance(tabletInfo, batchTask);
+        }
+    }
+
+    /*
+     * Replica is missing, which means there is no enough alive replicas.
+     * So we need to find a destination backend to clone a new replica as possible as we can.
+     * 1. find an available path in a backend as destination:
+     *      1. backend need to be alive.
+     *      2. backend of existing replicas should be excluded.
+     *      3. backend has available slot for clone.
+     *      4. replica can fit in the path (consider the threshold of disk capacity and usage percent).
+     *      5. try to find a path with lowest load score.
+     * 2. find an appropriate source replica:
+     *      1. source replica should be healthy
+     *      2. backend of source replica has available slot for clone.
+     *      
+     * 3. send clone task to destination backend
+     */
+    private void handleReplicaMissing(TabletInfo tabletInfo, AgentBatchTask batchTask) throws SchedException {
+        stat.counterReplicaMissingErr.incrementAndGet();
+        // find an available dest backend and path
+        RootPathLoadStatistic destPath = chooseAvailableDestPath(tabletInfo);
+        Preconditions.checkNotNull(destPath);
+        tabletInfo.setDestination(destPath.getBeId(), destPath.getPathHash());
+
+        // choose a source replica for cloning from
+        tabletInfo.chooseSrcReplica(backendsWorkingSlots);
+
+        // create clone task
+        batchTask.addTask(tabletInfo.createCloneReplicaAndTask());
+    }
+
+    /*
+     * Replica version is incomplete, which means this replica is missing some version,
+     * and need to be cloned from a healthy replica, in-place.
+     * 
+     * 1. find the incomplete replica as destination replica
+     * 2. find a healthy replica as source replica
+     * 3. send clone task
+     */
+    private void handleReplicaVersionIncomplete(TabletInfo tabletInfo, AgentBatchTask batchTask)
+            throws SchedException {
+        stat.counterReplicaVersionMissingErr.incrementAndGet();
+        ClusterLoadStatistic statistic = statisticMap.get(tabletInfo.getCluster());
+        if (statistic == null) {
+            throw new SchedException(Status.UNRECOVERABLE, ""cluster does not exist"");
+        }
+
+        tabletInfo.chooseDestReplicaForVersionIncomplete(backendsWorkingSlots);
+        tabletInfo.chooseSrcReplicaForVersionIncomplete(backendsWorkingSlots);
+
+        // create clone task
+        batchTask.addTask(tabletInfo.createCloneReplicaAndTask());
+    }
+
+    /*
+     *  replica is redundant, which means there are more replicas than we expected, which need to be dropped.
+     *  we just drop one redundant replica at a time, for safety reason.
+     *  choosing a replica to drop base on following priority:
+     *  1. backend has been dropped
+     *  2. backend is not available
+     *  3. replica's state is CLONE
+     *  4. replica's last failed version > 0
+     *  5. replica with lower version
+     *  6. replica not in right cluster
+     *  7. replica in higher load backend
+     */
+    private void handleRedundantReplica(TabletInfo tabletInfo) throws SchedException {
+        stat.counterReplicaRedundantErr.incrementAndGet();
+        if (deleteBackendDropped(tabletInfo)
+                || deleteBackendUnavailable(tabletInfo)
+                || deleteCloneReplica(tabletInfo)
+                || deleteReplicaWithFailedVersion(tabletInfo)
+                || deleteReplicaWithLowerVersion(tabletInfo)
+                || deleteReplicaNotInCluster(tabletInfo)
+                || deleteReplicaOnHighLoadBackend(tabletInfo)) {
+            // if we delete at least one redundant replica, we still throw a SchedException with status FINISHED
+            // to remove this tablet from the pendingTablets(consider it as finished)
+            throw new SchedException(Status.FINISHED, ""redundant replica is deleted"");
+        }
+        throw new SchedException(Status.SCHEDULE_FAILED, ""unable to delete any redundant replicas"");
+    }
+
+    private boolean deleteBackendDropped(TabletInfo tabletInfo) {
+        for (Replica replica : tabletInfo.getReplicas()) {
+            long beId = replica.getBackendId();
+            if (infoService.getBackend(beId) == null) {
+                deleteReplicaInternal(tabletInfo, replica, ""backend dropped"");
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private boolean deleteBackendUnavailable(TabletInfo tabletInfo) {
+        for (Replica replica : tabletInfo.getReplicas()) {
+            Backend be = infoService.getBackend(replica.getBackendId());
+            if (be == null) {
+                // this case should be handled in deleteBackendDropped()
+                continue;
+            }
+            if (!be.isAvailable()) {
+                deleteReplicaInternal(tabletInfo, replica, ""backend unavailable"");
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private boolean deleteCloneReplica(TabletInfo tabletInfo) {
+        for (Replica replica : tabletInfo.getReplicas()) {
+            if (replica.getState() == ReplicaState.CLONE) {
+                deleteReplicaInternal(tabletInfo, replica, ""clone state"");
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private boolean deleteReplicaWithFailedVersion(TabletInfo tabletInfo) {
+        for (Replica replica : tabletInfo.getReplicas()) {
+            if (replica.getLastFailedVersion() > 0) {
+                deleteReplicaInternal(tabletInfo, replica, ""version incomplete"");
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private boolean deleteReplicaWithLowerVersion(TabletInfo tabletInfo) {
+        for (Replica replica : tabletInfo.getReplicas()) {
+            if (!replica.checkVersionCatchUp(tabletInfo.getCommittedVersion(), tabletInfo.getCommittedVersionHash())) {
+                deleteReplicaInternal(tabletInfo, replica, ""lower version"");
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private boolean deleteReplicaNotInCluster(TabletInfo tabletInfo) {
+        for (Replica replica : tabletInfo.getReplicas()) {
+            Backend be = infoService.getBackend(replica.getBackendId());
+            if (be == null) {
+                // this case should be handled in deleteBackendDropped()
+                continue;
+            }
+            if (!be.getOwnerClusterName().equals(tabletInfo.getCluster())) {
+                deleteReplicaInternal(tabletInfo, replica, ""not in cluster"");
+                return true;
+            }
+        }
+        return false;
+    }
+
+    private boolean deleteReplicaOnHighLoadBackend(TabletInfo tabletInfo) {
+        ClusterLoadStatistic statistic = statisticMap.get(tabletInfo.getCluster());
+        if (statistic == null) {
+            return false;
+        }
+        
+        Replica chosenReplica = null;
+        double maxScore = 0;
+        for (Replica replica : tabletInfo.getReplicas()) {
+            BackendLoadStatistic beStatistic = statistic.getBackendLoadStatistic(replica.getBackendId());
+            if (beStatistic == null) {
+                continue;
+            }
+            if (beStatistic.getLoadScore() > maxScore) {
+                maxScore = beStatistic.getLoadScore();
+                chosenReplica = replica;
+            }
+        }
+
+        if (chosenReplica != null) {
+            deleteReplicaInternal(tabletInfo, chosenReplica, ""high load"");
+            return true;
+        }
+        return false;
+    }
+
+    private void deleteReplicaInternal(TabletInfo tabletInfo, Replica replica, String reason) {
+        // delete this replica from catalog.
+        // it will also delete replica from tablet inverted index.
+        tabletInfo.deleteReplica(replica);
+
+        // TODO(cmy): this should be removed after I finish modifying alter job logic
+        // Catalog.getInstance().handleJobsWhenDeleteReplica(tabletInfo.getTblId(), tabletInfo.getPartitionId(),
+        //                                                   tabletInfo.getIndexId(), tabletInfo.getTabletId(),
+        //                                                   replica.getId(), replica.getBackendId());
+
+        // write edit log
+        ReplicaPersistInfo info = ReplicaPersistInfo.createForDelete(tabletInfo.getDbId(),
+                                                                     tabletInfo.getTblId(),
+                                                                     tabletInfo.getPartitionId(),
+                                                                     tabletInfo.getIndexId(),
+                                                                     tabletInfo.getTabletId(),
+                                                                     replica.getBackendId());
+
+        Catalog.getInstance().getEditLog().logDeleteReplica(info);
+
+        LOG.info(""delete replica. tablet id: {}, backend id: {}. reason: {}"",
+                 tabletInfo.getTabletId(), replica.getBackendId(), reason);
+    }
+
+    /*
+     * Cluster migration, which means the tablet has enough healthy replicas,
+     * but some replicas are not in right cluster.
+     * It is just same as 'replica missing'.
+     * 
+     * after clone finished, the replica in wrong cluster will be treated as redundant, and will be deleted soon.
+     */
+    private void handleReplicaClusterMigration(TabletInfo tabletInfo, AgentBatchTask batchTask)
+            throws SchedException {
+        stat.counterReplicaMissingInClusterErr.incrementAndGet();
+        handleReplicaMissing(tabletInfo, batchTask);
+    }
+
+    /*
+     * Try to select some alternative tablets for balance. Add them to pendingTablets with priority LOW,
+     * and waiting to be scheduled.
+     */
+    private void selectTabletsForBalance() {
+        LoadBalancer loadBalancer = new LoadBalancer(statisticMap);
+        List<TabletInfo> alternativeTablets = loadBalancer.selectAlternativeTablets();
+        for (TabletInfo tabletInfo : alternativeTablets) {
+            addTablet(tabletInfo, false);
+        }
+    }
+
+    /*
+     * Try to create a balance task for a tablet.
+     */
+    private void doBalance(TabletInfo tabletInfo, AgentBatchTask batchTask) throws SchedException {
+        stat.counterBalanceSchedule.incrementAndGet();
+        LoadBalancer loadBalancer = new LoadBalancer(statisticMap);
+        loadBalancer.createBalanceTask(tabletInfo, backendsWorkingSlots, batchTask);
+    }
+
+    // choose a path on a backend which is fit for the tablet
+    private RootPathLoadStatistic chooseAvailableDestPath(TabletInfo tabletInfo) throws SchedException {
+        ClusterLoadStatistic statistic = statisticMap.get(tabletInfo.getCluster());
+        if (statistic == null) {
+            throw new SchedException(Status.UNRECOVERABLE, ""cluster does not exist"");
+        }
+        List<BackendLoadStatistic> beStatistics = statistic.getBeLoadStatistics();
+
+        // get all available paths which this tablet can fit in.
+        // beStatistics is sorted by load score in ascend order, so select from first to last.
+        List<RootPathLoadStatistic> allFitPaths = Lists.newArrayList();
+        for (int i = 0; i < beStatistics.size(); i++) {
+            BackendLoadStatistic bes = beStatistics.get(i);
+            // exclude BE which already has replica of this tablet
+            if (tabletInfo.containsBE(bes.getBeId())) {
+                continue;
+            }
+
+            List<RootPathLoadStatistic> resultPaths = Lists.newArrayList();
+            BalanceStatus st = bes.isFit(tabletInfo.getTabletSize(), resultPaths, true /* is supplement */);
+            if (!st.ok()) {
+                LOG.debug(""unable to find path for supplementing tablet: {}. {}"", tabletInfo, st);
+                continue;
+            }
+
+            Preconditions.checkState(resultPaths.size() == 1);
+            allFitPaths.add(resultPaths.get(0));
+        }
+
+        if (allFitPaths.isEmpty()) {
+            throw new SchedException(Status.SCHEDULE_FAILED, ""unable to find dest path for new replica"");
+        }
+
+        // all fit paths has already been sorted by load score in 'allFitPaths' in ascend order.
+        // just get first available path.
+        // we try to find a path with specified media type, if not find, arbitrarily use one.
+        for (RootPathLoadStatistic rootPathLoadStatistic : allFitPaths) {
+            if (rootPathLoadStatistic.getStorageMedium() != tabletInfo.getStorageMedium()) {
+                continue;
+            }
+
+            PathSlot slot = backendsWorkingSlots.get(rootPathLoadStatistic.getBeId());
+            if (slot == null) {
+                LOG.debug(""backend {} does not found when getting slots"", rootPathLoadStatistic.getBeId());
+                continue;
+            }
+
+            if (slot.takeSlot(rootPathLoadStatistic.getPathHash()) != -1) {
+                return rootPathLoadStatistic;
+            }
+        }
+
+        // no root path with specified media type is found, get arbitrary one.
+        for (RootPathLoadStatistic rootPathLoadStatistic : allFitPaths) {
+            PathSlot slot = backendsWorkingSlots.get(rootPathLoadStatistic.getBeId());
+            if (slot == null) {
+                LOG.debug(""backend {} does not found when getting slots"", rootPathLoadStatistic.getBeId());
+                continue;
+            }
+
+            if (slot.takeSlot(rootPathLoadStatistic.getPathHash()) != -1) {
+                return rootPathLoadStatistic;
+            }
+        }
+        
+        throw new SchedException(Status.SCHEDULE_FAILED, ""unable to find dest path which can be fit in"");
+    }
+
+    /*
+     * For some reason, a tablet info failed to be scheduled this time,
+     * So we dynamically change its priority and add back to queue, waiting for next round.
+     */
+    private void dynamicAdjustPrioAndAddBackToPendingTablets(TabletInfo tabletInfo, String message) {
+        Preconditions.checkState(tabletInfo.getState() == TabletInfo.State.PENDING);
+        tabletInfo.adjustPriority(stat);
+        addTablet(tabletInfo, true /* force */);
+    }
+
+    private synchronized void removeTabletInfo(TabletInfo tabletInfo, TabletInfo.State state, String reason) {
+        tabletInfo.setState(state);
+        tabletInfo.releaseResource(this);
+        tabletInfo.setFinishedTime(System.currentTimeMillis());
+        runningTablets.remove(tabletInfo.getTabletId());
+        allTabletIds.remove(tabletInfo.getTabletId());
+        schedHistory.add(tabletInfo);
+        LOG.info(""remove the tablet {}. because: {}"", tabletInfo.getTabletId(), reason);
+    }
+
+    // get next batch of tablets from queue.
+    private synchronized List<TabletInfo> getNextTabletInfoBatch() {
+        List<TabletInfo> list = Lists.newArrayList();
+        int count = Math.max(MIN_BATCH_NUM, getCurrentAvailableSlotNum());
+        while (count > 0) {
+            TabletInfo tablet = pendingTablets.poll();
+            if (tablet == null) {
+                // no more tablets
+                break;
+            }
+            list.add(tablet);
+            count--;
+        }
+        return list;
+    }
+
+    private int getCurrentAvailableSlotNum() {
+        int total = 0;
+        for (PathSlot pathSlot : backendsWorkingSlots.values()) {
+            total += pathSlot.getTotalAvailSlotNum();
+        }
+        return total;
+    }
+
+    /*
+     * return true if we want to remove the clone task from AgentTaskQueu
+     */
+    public boolean finishCloneTask(CloneTask cloneTask, TFinishTaskRequest request) {
+        long tabletId = cloneTask.getTabletId();
+        TabletInfo tabletInfo = takeRunningTablets(tabletId);
+        if (tabletInfo == null) {
+            LOG.warn(""tablet info does not exist: {}"", tabletId);
+            // tablet does not exist, no need to keep task.
+            return true;
+        }
+        Preconditions.checkState(tabletInfo.getState() == TabletInfo.State.RUNNING);
+        try {
+            tabletInfo.finishCloneTask(cloneTask, request);
+        } catch (SchedException e) {
+            tabletInfo.increaseFailedRunningCounter();
+            tabletInfo.setErrMsg(e.getMessage());
+            if (e.getStatus() == Status.RUNNING_FAILED) {
+                stat.counterCloneTaskFailed.incrementAndGet();
+                addToRunningTablets(tabletInfo);
+                return false;
+            } else {
+                Preconditions.checkState(e.getStatus() == Status.UNRECOVERABLE, e.getStatus());
+                // unrecoverable
+                stat.counterTabletScheduledDiscard.incrementAndGet();
+                removeTabletInfo(tabletInfo, TabletInfo.State.CANCELLED, e.getMessage());
+                return true;
+            }
+        }
+
+        Preconditions.checkState(tabletInfo.getState() == TabletInfo.State.FINISHED);
+        stat.counterCloneTaskSucceeded.incrementAndGet();
+        gatherStatistics(tabletInfo);
+        removeTabletInfo(tabletInfo, TabletInfo.State.FINISHED, ""finished"");
+        return true;
+    }
+
+    /*
+     * Gather the running statistic of the task.
+     * It will be evaluated for future strategy.  
+     * This should only be called when the tablet is down with state FINISHED.
+     */
+    private void gatherStatistics(TabletInfo tabletInfo) {
+        if (tabletInfo.getCopySize() > 0 && tabletInfo.getCopyTimeMs() > 0) {
+            if (tabletInfo.getSrcBackendId() != -1 && tabletInfo.getSrcPathHash() != -1) {
+                PathSlot pathSlot = backendsWorkingSlots.get(tabletInfo.getSrcBackendId());
+                if (pathSlot != null) {
+                    pathSlot.updateStatistic(tabletInfo.getSrcPathHash(), tabletInfo.getCopySize(),
+                            tabletInfo.getCopyTimeMs());
+                }
+            }
+
+            if (tabletInfo.getDestBackendId() != -1 && tabletInfo.getDestPathHash() != -1) {
+                PathSlot pathSlot = backendsWorkingSlots.get(tabletInfo.getDestBackendId());
+                if (pathSlot != null) {
+                    pathSlot.updateStatistic(tabletInfo.getDestPathHash(), tabletInfo.getCopySize(),
+                            tabletInfo.getCopyTimeMs());
+                }
+            }
+        }
+
+        if (System.currentTimeMillis() - lastSlotAdjustTime < STAT_UPDATE_INTERVAL_MS) {
+            return;
+        }
+
+        // TODO(cmy): update the slot num base on statistic.
+        // need to find a better way to determine the slot number.
+
+        lastSlotAdjustTime = System.currentTimeMillis();
+    }
+
+    /*
+     * handle tablets which are running.
+     * We should finished the task if
+     * 1. Tablet is already healthy
+     * 2. Task is timeout.
+     * 
+     * But here we just handle the timeout case here. Let the 'finishCloneTask()' check if tablet is healthy.
+     * We guarantee that if tablet is in runningTablets, the 'finishCloneTask()' will finally be called,
+     * so no need to worry that running tablets will never end.
+     * This is also avoid nesting 'synchronized' and database lock.
+     *
+     * If task is timeout, remove the tablet.
+     */
+    public synchronized void handleRunningTablets() {
+        List<TabletInfo> timeoutTablets = Lists.newArrayList();
+        runningTablets.values().stream().filter(t -> t.isTimeout()).forEach(t -> {
+            timeoutTablets.add(t);
+        });
+        
+        timeoutTablets.stream().forEach(t -> {
+            removeTabletInfo(t, TabletInfo.State.TIMEOUT, ""timeout"");
+            stat.counterCloneTaskTimeout.incrementAndGet();
+        });
+    }
+
+    public List<List<String>> getPendingTabletsInfo(int limit) {
+        List<TabletInfo> tabletInfos = getCopiedTablets(pendingTablets, limit);
+        return collectTabletInfo(tabletInfos);
+    }
+
+    public List<List<String>> getRunningTabletsInfo(int limit) {
+        List<TabletInfo> tabletInfos = getCopiedTablets(runningTablets.values(), limit);
+        return collectTabletInfo(tabletInfos);
+    }
+
+    public List<List<String>> getHistoryTabletsInfo(int limit) {
+        List<TabletInfo> tabletInfos = getCopiedTablets(schedHistory, limit);
+        return collectTabletInfo(tabletInfos);
+    }
+
+    private List<List<String>> collectTabletInfo(List<TabletInfo> tabletInfos) {
+        List<List<String>> result = Lists.newArrayList();
+        tabletInfos.stream().forEach(t -> {
+            result.add(t.getBrief());
+        });
+        return result;
+    }
+
+    private synchronized List<TabletInfo> getCopiedTablets(Collection<TabletInfo> source, int limit) {
+        List<TabletInfo> tabletInfos = Lists.newArrayList();
+        source.stream().limit(limit).forEach(t -> {
+            tabletInfos.add(t);
+        });
+        return tabletInfos;
+    }
+
+    public synchronized int getPendingNum() {
+        return pendingTablets.size();
+    }
+
+    public synchronized int getRunningNum() {
+        return runningTablets.size();
+    }
+
+    public synchronized int getHistoryNum() {
+        return schedHistory.size();
+    }
+
+    /*
+     * PathSlot keeps track of slot num per path of a Backend.
+     * Each path on a Backend has several slot.
+     * If a path's available slot num because 0, no task should be assigned to this path.
+     */
+    public class PathSlot {
+        // path hash -> slot num
+        private Map<Long, Slot> pathSlots = Maps.newConcurrentMap();
+
+        public PathSlot(List<Long> paths, int initSlotNum) {
+            for (Long pathHash : paths) {
+                pathSlots.put(pathHash, new Slot(initSlotNum));
+            }
+        }
+
+        // update the path
+        public synchronized void updatePaths(List<Long> paths) {
+            // delete non exist path
+            Iterator<Map.Entry<Long, Slot>> iter = pathSlots.entrySet().iterator();
+            while (iter.hasNext()) {
+                Map.Entry<Long, Slot> entry = iter.next();
+                if (!paths.contains(entry.getKey())) {
+                    iter.remove();
+                }
+            }
+
+            // add new path
+            for (Long pathHash : paths) {
+                if (!pathSlots.containsKey(pathHash)) {
+                    pathSlots.put(pathHash, new Slot(Config.schedule_slot_num_per_path));
+                }
+            }
+        }
+
+        // Update the total slots num of specified paths, increase or decrease
+        public synchronized void updateSlot(List<Long> pathHashs, boolean increase) {
+            for (Long pathHash : pathHashs) {
+                if (pathSlots.containsKey(pathHash)) {
+                    if (increase) {
+                        pathSlots.get(pathHash).total++;
+                    } else {
+                        pathSlots.get(pathHash).total--;
+                    }
+                    pathSlots.get(pathHash).rectify();
+                    LOG.debug(""decrease path {} slots num to {}"", pathHash, pathSlots.get(pathHash).total);
+                }
+            }
+        }
+
+        /*
+         * Update the statistic of specified path
+         */
+        public synchronized void updateStatistic(long pathHash, long copySize, long copyTimeMs) {
+            if (pathSlots.get(pathHash) == null) {
+                return;
+            }
+            pathSlots.get(pathHash).totalCopySize += copySize;
+            pathSlots.get(pathHash).totalCopyTimeMs += copyTimeMs;","[{'comment': 'introduce a local variable instead of call `get()` twice', 'commenter': 'imay'}]"
336,fe/src/main/java/org/apache/doris/clone/TabletInfo.java,"@@ -0,0 +1,922 @@
+
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.clone;
+
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.catalog.Database;
+import org.apache.doris.catalog.MaterializedIndex;
+import org.apache.doris.catalog.OlapTable;
+import org.apache.doris.catalog.Partition;
+import org.apache.doris.catalog.Replica;
+import org.apache.doris.catalog.Replica.ReplicaState;
+import org.apache.doris.catalog.Tablet;
+import org.apache.doris.catalog.Tablet.TabletStatus;
+import org.apache.doris.clone.SchedException.Status;
+import org.apache.doris.clone.TabletScheduler.PathSlot;
+import org.apache.doris.common.util.TimeUtils;
+import org.apache.doris.persist.ReplicaPersistInfo;
+import org.apache.doris.system.Backend;
+import org.apache.doris.system.SystemInfoService;
+import org.apache.doris.task.AgentTaskQueue;
+import org.apache.doris.task.CloneTask;
+import org.apache.doris.thrift.TBackend;
+import org.apache.doris.thrift.TFinishTaskRequest;
+import org.apache.doris.thrift.TStatusCode;
+import org.apache.doris.thrift.TStorageMedium;
+import org.apache.doris.thrift.TTabletInfo;
+import org.apache.doris.thrift.TTaskType;
+
+import com.google.common.base.Preconditions;
+import com.google.common.base.Strings;
+import com.google.common.collect.Lists;
+
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.List;
+import java.util.Map;
+
+/*
+ * TabletInfo contains all information which is created during tablet scheduler processing.
+ */
+public class TabletInfo implements Comparable<TabletInfo> {","[{'comment': 'TabletScheduleContext? TabletInfo name is too common', 'commenter': 'imay'}]"
338,thirdparty/build-thirdparty.sh,"@@ -57,17 +57,18 @@ ${TP_DIR}/download-thirdparty.sh
 
 export LD_LIBRARY_PATH=$TP_DIR/installed/lib:$LD_LIBRARY_PATH
 
-if [ -f ${DORIS_TOOLCHAIN}/gcc730/bin/gcc ]; then
-    GCC_HOME=${DORIS_TOOLCHAIN}/gcc730
-    export CC=${GCC_HOME}/bin/gcc
-    export CPP=${GCC_HOME}/bin/cpp
-    export CXX=${GCC_HOME}/bin/g++
+# set COMPILER
+if [[ -z ${DORIS_GCC_HOME} ]]; then","[{'comment': 'you can remove this `if`', 'commenter': 'imay'}]"
338,build.sh,"@@ -35,6 +35,18 @@ export DORIS_HOME=${ROOT}
 
 . ${DORIS_HOME}/env.sh
 
+# build thirdparty libraries if necessary
+if [[ ! -d ${DORIS_THIRDPARTY}/installed ]]; then","[{'comment': 'should check a lib that generated by build-thirdparty.sh', 'commenter': 'imay'}]"
340,be/src/http/download_action.cpp,"@@ -254,7 +258,8 @@ Status DownloadAction::check_path_is_allowed(const std::string& file_path) {
 
 Status DownloadAction::check_log_path_is_allowed(const std::string& file_path) {
     DCHECK_EQ(_download_type, ERROR_LOG);
-    if (FileSystemUtil::contain_path(_error_log_root_dir, file_path)) {
+    std::string canonical_file_path = canonical(file_path).string();
+    if (FileSystemUtil::contain_path(canonical(_error_log_root_dir).string(), canonical_file_path)) {","[{'comment': '`_error_log_root_dir` should be set after canonical when initializing.', 'commenter': 'imay'}, {'comment': 'why not just call canonical in DownloadAction construct method?', 'commenter': 'morningman'}]"
340,be/src/http/download_action.cpp,"@@ -243,8 +246,9 @@ Status DownloadAction::check_token(HttpRequest *req) {
 
 Status DownloadAction::check_path_is_allowed(const std::string& file_path) {
     DCHECK_EQ(_download_type, NORMAL);
+    std::string canonical_file_path = canonical(file_path).string();
     for (auto& allow_path : _allow_paths) {
-        if (FileSystemUtil::contain_path(allow_path, file_path)) {
+        if (FileSystemUtil::contain_path(canonical(allow_path).string(), canonical_file_path)) {","[{'comment': 'same with `_error_log_root_dir ` to `_allow_paths`', 'commenter': 'imay'}]"
340,be/src/http/download_action.cpp,"@@ -254,7 +258,8 @@ Status DownloadAction::check_path_is_allowed(const std::string& file_path) {
 
 Status DownloadAction::check_log_path_is_allowed(const std::string& file_path) {
     DCHECK_EQ(_download_type, ERROR_LOG);
-    if (FileSystemUtil::contain_path(_error_log_root_dir, file_path)) {","[{'comment': 'Is it better to get canonical path in a unified place?\r\nOr you may miss somewhere.', 'commenter': 'morningman'}]"
340,be/src/http/download_action.cpp,"@@ -47,16 +50,16 @@ const std::string TOKEN_PARAMETER = ""token"";
 
 DownloadAction::DownloadAction(ExecEnv* exec_env, const std::vector<std::string>& allow_dirs) :
     _exec_env(exec_env),
-    _download_type(NORMAL),
-    _allow_paths(allow_dirs) {
-
+    _download_type(NORMAL) {
+    for (auto& dir : allow_dirs) {
+        _allow_paths.emplace_back(std::move(canonical(dir).string()));","[{'comment': ""`canonical(dir).string()` is already a rvalue, you needn't to call `std::move`"", 'commenter': 'imay'}]"
342,README.md,"@@ -1,215 +1,55 @@
-# Introduction to Apache Doris (incubating)
+# Apache Doris (incubating) Project
 
-Apache Doris is an MPP-based interactive SQL data warehousing for reporting and analysis. Doris mainly integrates the technology of Google Mesa and Apache Impala. Unlike other popular SQL-on-Hadoop systems, Doris is designed to be a simple and single tightly coupled system, not depending on other systems. Doris not only provides high concurrent low latency point query performance, but also provides high throughput queries of ad-hoc analysis. Doris not only provides batch data loading, but also provides near real-time mini-batch data loading. Doris also provides high availability, reliability, fault tolerance, and scalability. The simplicity (of developing, deploying and using) and meeting many data serving requirements in single system are the main features of Doris.
+Apache Doris is an MPP-based interactive SQL data warehousing for reporting and analysis. It open-sourced by Baidu. Please visit [Doris official site ](http://doris.incubator.apache.org) for more detail information, as well as [Wiki](https://github.com/apache/incubator-doris/wiki) for documents of install, deploy, best pracitices and FAQs.
 
-## 1. Background
+## Compile and install","[{'comment': ""I think you'd add a 'Getting started' link and link it to the Install WIKI, or no one will read the wiki...\r\nAnd now we prefer to compile the Doris source in the docker-dev environment.\r\nAnd we may have to translate some wiki into English."", 'commenter': 'morningman'}]"
342,README.md,"@@ -1,215 +1,55 @@
-# Introduction to Apache Doris (incubating)
+# Apache Doris (incubating) Project
 
-Apache Doris is an MPP-based interactive SQL data warehousing for reporting and analysis. Doris mainly integrates the technology of Google Mesa and Apache Impala. Unlike other popular SQL-on-Hadoop systems, Doris is designed to be a simple and single tightly coupled system, not depending on other systems. Doris not only provides high concurrent low latency point query performance, but also provides high throughput queries of ad-hoc analysis. Doris not only provides batch data loading, but also provides near real-time mini-batch data loading. Doris also provides high availability, reliability, fault tolerance, and scalability. The simplicity (of developing, deploying and using) and meeting many data serving requirements in single system are the main features of Doris.
+Apache Doris is an MPP-based interactive SQL data warehousing for reporting and analysis. It open-sourced by Baidu. Please visit [Doris official site ](http://doris.incubator.apache.org) for more detail information, as well as [Wiki](https://github.com/apache/incubator-doris/wiki) for documents of install, deploy, best pracitices and FAQs.","[{'comment': 'What about adding some ‘agitative’ description, or a brief of best Doris use case? like [kafka](https://kafka.apache.org/) or [druid](http://druid.io/)', 'commenter': 'morningman'}, {'comment': 'Our official website is not ready for users, there is nothing on it now. So here is our ground now.', 'commenter': 'morningman'}]"
344,be/src/exec/olap_common.h,"@@ -662,6 +662,8 @@ Status OlapScanKeys::extend_scan_key(ColumnValueRange<T>& range) {
         return Status::OK;
     }
 
+    bool _has_converted = false;","[{'comment': ""Only member variable's name starts with '_', so change it to `has_converted`"", 'commenter': 'imay'}, {'comment': 'OK', 'commenter': 'kangkaisen'}]"
345,README.md,"@@ -1,215 +1,127 @@
-# Introduction to Apache Doris (incubating)
+# Apache Doris (incubating)
 
-Apache Doris is an MPP-based interactive SQL data warehousing for reporting and analysis. Doris mainly integrates the technology of Google Mesa and Apache Impala. Unlike other popular SQL-on-Hadoop systems, Doris is designed to be a simple and single tightly coupled system, not depending on other systems. Doris not only provides high concurrent low latency point query performance, but also provides high throughput queries of ad-hoc analysis. Doris not only provides batch data loading, but also provides near real-time mini-batch data loading. Doris also provides high availability, reliability, fault tolerance, and scalability. The simplicity (of developing, deploying and using) and meeting many data serving requirements in single system are the main features of Doris.
+Doris is an MPP-based interactive SQL data warehousing for reporting and analysis. It open-sourced by Baidu. 
 
-## 1. Background
+## 1. License
 
-In Baidu, the largest Chinese search engine, we run a two-tiered data warehousing system for data processing, reporting and analysis. Similar to lambda architecture, the whole data warehouse comprises data processing and data serving. Data processing does the heavy lifting of big data: cleaning data, merging and transforming it, analyzing it and preparing it for use by end user queries; data serving is designed to serve queries against that data for different use cases. Currently data processing includes batch data processing and stream data processing technology, like Hadoop, Spark and Storm; Doris is a SQL data warehouse for serving online and interactive data reporting and analysis querying.
+[Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0)
 
-Prior to Doris, different tools were deployed to solve diverse requirements in many ways. For example, the advertising platform needs to provide some detailed statistics associated with each served ad for every advertiser. The platform must support continuous updates, both new rows and incremental updates to existing rows within minutes. It must support latency-sensitive users serving live customer reports with very low latency requirements and batch ad-hoc multiple dimensions data analysis requiring very high throughput. In the past,this platform was built on top of sharded MySQL. But with the growth of data, MySQL cannot meet the requirements. Then, based on our existing KV system, we developed our own proprietary distributed statistical database. But, the simple KV storage was not efficient on scan performance. Because the system depends on many other systems, it is very complex to operate and maintain. Using RPC API, more complex querying usually required code programming, but users wants an MPP SQL engine. In addition to advertising system, a large number of internal BI Reporting / Analysis, also used a variety of tools. Some used the combination of SparkSQL / Impala + HDFS / HBASE. Some used MySQL to store the results that were prepared by distributed MapReduce computing. Some also bought commercial databases to use.
+## 2. Technology
+Doris mainly integrates the technology of Google Mesa and Apache Impala, and it based on a column-oriented storage engine and can communicate by MySQL client.
 
-However, when a use case requires the simultaneous availability of capabilities that cannot all be provided by a single tool, users were forced to build hybrid architectures that stitch multiple tools together. Users often choose to ingest and update data in one storage system, but later reorganize this data to optimize for an analytical reporting use-case served from another. Our users had been successfully deploying and maintaining these hybrid architectures, but we believe that they shouldn't need to accept their inherent complexity. A storage system built to provide great performance across a broad range of workloads provides a more elegant solution to the problems that hybrid architectures aim to solve. Doris is the solution. Doris is designed to be a simple and single tightly coupled system, not depending on other systems. Doris provides high concurrent low latency point query performance, but also provides high throughput queries of ad-hoc analysis. Doris provides bulk-batch data loading, but also provides near real-time mini-batch data loading. Doris also provides high availability, reliability, fault tolerance, and scalability.
+## 3. User cases
+Doris not only provides high concurrent low latency point query performance, but also provides high throughput queries of ad-hoc analysis. 
 
-Generally speaking, Doris is the technology combination of Google Mesa and Apache Impala. Mesa is a highly scalable analytic data storage system that stores critical measurement data related to Google's Internet advertising business. Mesa is designed to satisfy complex and challenging set of users' and systems' requirements, including near real-time data ingestion and query ability, as well as high availability, reliability, fault tolerance, and scalability for large data and query volumes. Impala is a modern, open-source MPP SQL engine architected from the ground up for the Hadoop data processing environment. At present, by virtue of its superior performance and rich functionality, Impala has been comparable to many commercial MPP database query engine. Mesa can satisfy the needs of many of our storage requirements, however Mesa itself does not provide a SQL query engine; Impala is a very good MPP SQL query engine, but the lack of a perfect distributed storage engine. So in the end we chose the combination of these two technologies.
+Doris not only provides batch data loading, but also provides near real-time mini-batch data loading. 
 
-Learning from Mesa's data model, we developed a distributed storage engine. Unlike Mesa, this storage engine does not rely on any distributed file system. Then we deeply integrate this storage engine with Impala query engine. Query compiling, query execution coordination and catalog management of storage engine are integrated to be frontend daemon; query execution and data storage are integrated to be backend daemon. With this integration, we implemented a single, full-featured, high performance state the art of MPP database, as well as maintaining the simplicity.
+Doris also provides high availability, reliability, fault tolerance, and scalability. 
 
-## 2. System Overview
+The simplicity (of developing, deploying and using) and meeting many data serving requirements in single system are the main features of Doris.
 
-Doris' implementation consists of two daemons: frontend (FE) and backend (BE). The following figures gives the overview of architecture and usage.
+## 4. Compile and install
 
-![](./docs/resources/palo_architecture.jpg)
+Currently support Docker environment and Linux OS: 
+Docker（Linux/Windows/Mac), Ubuntu 16.04+ and CentOS 7.5+","[{'comment': 'Symbol ‘+’ should be removed, we may not support higher version.', 'commenter': 'morningman'}, {'comment': 'We need to resolve llvm compile issue.', 'commenter': 'lide-reed'}]"
345,README.md,"@@ -1,215 +1,127 @@
-# Introduction to Apache Doris (incubating)
+# Apache Doris (incubating)
 
-Apache Doris is an MPP-based interactive SQL data warehousing for reporting and analysis. Doris mainly integrates the technology of Google Mesa and Apache Impala. Unlike other popular SQL-on-Hadoop systems, Doris is designed to be a simple and single tightly coupled system, not depending on other systems. Doris not only provides high concurrent low latency point query performance, but also provides high throughput queries of ad-hoc analysis. Doris not only provides batch data loading, but also provides near real-time mini-batch data loading. Doris also provides high availability, reliability, fault tolerance, and scalability. The simplicity (of developing, deploying and using) and meeting many data serving requirements in single system are the main features of Doris.
+Doris is an MPP-based interactive SQL data warehousing for reporting and analysis. It open-sourced by Baidu. 
 
-## 1. Background
+## 1. License
 
-In Baidu, the largest Chinese search engine, we run a two-tiered data warehousing system for data processing, reporting and analysis. Similar to lambda architecture, the whole data warehouse comprises data processing and data serving. Data processing does the heavy lifting of big data: cleaning data, merging and transforming it, analyzing it and preparing it for use by end user queries; data serving is designed to serve queries against that data for different use cases. Currently data processing includes batch data processing and stream data processing technology, like Hadoop, Spark and Storm; Doris is a SQL data warehouse for serving online and interactive data reporting and analysis querying.
+[Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0)
 
-Prior to Doris, different tools were deployed to solve diverse requirements in many ways. For example, the advertising platform needs to provide some detailed statistics associated with each served ad for every advertiser. The platform must support continuous updates, both new rows and incremental updates to existing rows within minutes. It must support latency-sensitive users serving live customer reports with very low latency requirements and batch ad-hoc multiple dimensions data analysis requiring very high throughput. In the past,this platform was built on top of sharded MySQL. But with the growth of data, MySQL cannot meet the requirements. Then, based on our existing KV system, we developed our own proprietary distributed statistical database. But, the simple KV storage was not efficient on scan performance. Because the system depends on many other systems, it is very complex to operate and maintain. Using RPC API, more complex querying usually required code programming, but users wants an MPP SQL engine. In addition to advertising system, a large number of internal BI Reporting / Analysis, also used a variety of tools. Some used the combination of SparkSQL / Impala + HDFS / HBASE. Some used MySQL to store the results that were prepared by distributed MapReduce computing. Some also bought commercial databases to use.
+## 2. Technology
+Doris mainly integrates the technology of Google Mesa and Apache Impala, and it based on a column-oriented storage engine and can communicate by MySQL client.
 
-However, when a use case requires the simultaneous availability of capabilities that cannot all be provided by a single tool, users were forced to build hybrid architectures that stitch multiple tools together. Users often choose to ingest and update data in one storage system, but later reorganize this data to optimize for an analytical reporting use-case served from another. Our users had been successfully deploying and maintaining these hybrid architectures, but we believe that they shouldn't need to accept their inherent complexity. A storage system built to provide great performance across a broad range of workloads provides a more elegant solution to the problems that hybrid architectures aim to solve. Doris is the solution. Doris is designed to be a simple and single tightly coupled system, not depending on other systems. Doris provides high concurrent low latency point query performance, but also provides high throughput queries of ad-hoc analysis. Doris provides bulk-batch data loading, but also provides near real-time mini-batch data loading. Doris also provides high availability, reliability, fault tolerance, and scalability.
+## 3. User cases
+Doris not only provides high concurrent low latency point query performance, but also provides high throughput queries of ad-hoc analysis. 
 
-Generally speaking, Doris is the technology combination of Google Mesa and Apache Impala. Mesa is a highly scalable analytic data storage system that stores critical measurement data related to Google's Internet advertising business. Mesa is designed to satisfy complex and challenging set of users' and systems' requirements, including near real-time data ingestion and query ability, as well as high availability, reliability, fault tolerance, and scalability for large data and query volumes. Impala is a modern, open-source MPP SQL engine architected from the ground up for the Hadoop data processing environment. At present, by virtue of its superior performance and rich functionality, Impala has been comparable to many commercial MPP database query engine. Mesa can satisfy the needs of many of our storage requirements, however Mesa itself does not provide a SQL query engine; Impala is a very good MPP SQL query engine, but the lack of a perfect distributed storage engine. So in the end we chose the combination of these two technologies.
+Doris not only provides batch data loading, but also provides near real-time mini-batch data loading. 
 
-Learning from Mesa's data model, we developed a distributed storage engine. Unlike Mesa, this storage engine does not rely on any distributed file system. Then we deeply integrate this storage engine with Impala query engine. Query compiling, query execution coordination and catalog management of storage engine are integrated to be frontend daemon; query execution and data storage are integrated to be backend daemon. With this integration, we implemented a single, full-featured, high performance state the art of MPP database, as well as maintaining the simplicity.
+Doris also provides high availability, reliability, fault tolerance, and scalability. 
 
-## 2. System Overview
+The simplicity (of developing, deploying and using) and meeting many data serving requirements in single system are the main features of Doris.
 
-Doris' implementation consists of two daemons: frontend (FE) and backend (BE). The following figures gives the overview of architecture and usage.
+## 4. Compile and install
 
-![](./docs/resources/palo_architecture.jpg)
+Currently support Docker environment and Linux OS: 
+Docker（Linux/Windows/Mac), Ubuntu 16.04+ and CentOS 7.5+
 
-Frontend daemon consists of query coordinator and catalog manager. Query coordinator is responsible for receiving user's sql queries, compiling queries and managing queries execution. Catalog manager is responsible for managing metadata such as databases, tables, partitions, replicas and etc. Several frontend daemons could be deployed to guarantee fault-tolerance, and load balancing.
+### 4.1 For Docker
 
-Backend daemon stores the data and executes the query fragments. Many backend daemons could also be deployed to provide scalability and fault-tolerance.
+#### Step 1: Install Docker
 
-A typical Doris cluster generally composes of several frontend daemons and dozens to hundreds of backend daemons.
+Take CentOS as an example:
 
-Clients can use MySQL-related tools to connect any frontend daemon to submit SQL query. The frontend receives the query and compiles it into query plans executable by the backends. Then frontend sends the query plan fragments to backend. Backends will build a query execution DAG. Data is fetched and pipelined into the DAG. The final result response is sent to client via frontend. The distribution of query fragment execution takes minimizing data movement and maximizing scan locality as the main goal. Because Doris is designed to provide interactive analysis, so the average execution time of queries is short. Considering this, we adopt query re-execution to meet the fault tolerance of query execution.
-
-A table is splitted into many tablets. Tablets are managed by backends. The backend daemon could be configured to use multiple directories. Any directory's IO failure doesn't influence the normal running of backend daemon. Doris will recover and rebalance the whole cluster automatically when necessary.
-
-## 3. Frontend
-
-In-memory catalog, multiple frontends, MySQL networking protocol, consistency guarantee, and two-level table partitioning are the main features of Doris' frontend design.
-
-#### 3.1 In-Memory Catalog
-
-Traditional data warehouse always uses a RDBMS database to store their catalog metadata. In order to produce query execution plan, frontend needs to look up the catalog metadata. This kind of catalog storage may be enough for low concurrent ad-hoc analysis queries. But for online high concurrent queries, its performance is very bad,resulting in increased response latency. For example, Hive metadata query latency is sometimes up to tens of seconds or even minutes. In order to speedup the metadata access, we adopt the in-memory catalog storage.
-
-![](./docs/resources/log_replication.jpg)
-
-In-memory catalog storage has three functional modules: real-time memory data structures, memory checkpoints on local disk and an operation relay log. When modifying catalog, the mutation operation is written into the log file firstly. Then, the mutation operation is applied into the memory data structures. Periodically, a thread does the checkpoint that dumps memory data structure image into local disk. Checkpoint mechanism enables the fast startup of frontend and reduces the disk storage occupancy. Actually, in-memory catalog also simplifies the implementation of multiple frontends.
-
-#### 3.2 Multiple Frontends
-
-Many data warehouses only support single frontend-like node. There are some systems supporting master and slave deploying. But for online data serving, high availability is an essential feature. Further, the number of queries per seconds may be very large, so high scalability is also needed. In Doris, we provide the feature of multiple frontends using replicated-state-machine technology.
-
-Frontends can be configured to three kinds of roles: leader, follower and observer. Through a voting protocol, follower frontends firstly elect a leader frontend. All the write requests of metadata are forwarded to the leader, then the leader writes the operation into the replicated log file. If the new log entry will be replicated to at least quorum followers successfully, the leader commits the operation into memory, and responses the write request. Followers always replay the replicated logs to apply them into their memory metadata. If the leader crashes, a new leader will be elected from the leftover followers. Leader and follower mainly solve the problem of write availability and partly solve the problem of read scalability.
-
-Usually one leader frontend and several follower frontends can meet most applications' write availability and read scalability requirements. For very high concurrent reading, continuing to increase the number of followers is not a good practice. Leader replicates log stream to followers synchronously, so adding more followers will increases write latency. Like Zookeeper,we have introduced a new type of frontend node called observer that helps addressing this problem and further improving metadata read scalability. Leader replicates log stream to observers asynchronously. Observers don't involve leader election.
-
-The replicated-state-machine is implemented based on BerkeleyDB java version (BDB-JE). BDB-JE has achieved high availability by implementing a Paxos-like consensus algorithm.  We use BDB-JE to implement Doris' log replication and leader election.
-
-#### 3.3    Consistency Guarantee
-
-If a client process connects to the leader, it will see up-to-date metadata, so that strong consistency semantics is guaranteed. If the client connects to followers or observers, it will see metadata lagging a little behind of the leader, but the monotonic consistency is guaranteed. In most Doris' use cases, monotonic consistency is accepted.
-
-If the client always connects to the same frontend, monotonic consistency semantics is obviously guaranteed; however if the client connects to other frontends due to failover, the semantics may be violated. Doris provides a SYNC command to guarantee metadata monotonic consistency semantics during failover. When failover happens, the client can send a SYNC command to the new connected frontend, who will get the latest operation log number from the leader. The SYNC command will not return to client as long as local applied log number is still less than fetched operation log number. This mechanism can guarantee the metadata on the connected frontend is newer than the client have seen during its last connection.
-
-#### 3.4 MySQL Networking Protocol
-
-MySQL compatible networking protocol is implemented in Doris' frontend. Firstly, SQL interface is preferred for engineers; Secondly, compatibility with MySQL protocol makes the integrating with current existing BI software, such as Tableau, easier; Lastly, rich MySQL client libraries and tools reduce our development costs, but also reduces the user's using cost.
-
-Through the SQL interface, administrator can adjust system configuration, add and remove frontend nodes or backend nodes, and create new database for user; user can create tables, load data, and submit SQL query.
-
-Online help document and Linux Proc-like mechanism are also supported in SQL. Users can submit queries to get the help of related SQL statements or show Doris' internal running state.
-
-In frontend, a small response buffer is allocated to every MySQL connection. The maximum size of this buffer is limited to 1MB. The buffer is responsible for buffering the query response data. Only if the response is finished or the buffer size reaches the 1MB,the response data will begin to be sent to client. Through this small trick, frontend can re-execution most of queries if errors occurred during query execution.
-
-#### 3.5 Two-Level Partitioning
-
-Like most of the distributed database system, data in Doris is horizontally partitioned. However, a single-level partitioning rule (hash partitioning or range partitioning) may not be a good solution to all scenarios. For example, there have a user-based fact table that stores rows of the form (date, userid, metric). Choosing only hash partitioning by column userid may lead to uneven distribution of data, when one user's data is very large. If choosing range partitioning according to column date, it will also lead to uneven distribution of data due to the likely data explosion in a certain period of time.
-
-Therefore we support the two-level partitioning rule. The first level is range partitioning. User can specify a column (usually the time series column) range of values for the data partition. In one partition, the user can also specify one or more columns and a number of buckets to do the hash partitioning. User can combine with different partitioning rules to better divide the data. Figure 4 gives an example of two-level partitioning.
-
-Three benefits are gained by using the two-level partitioning mechanism. Firstly, old and new data could be separated, and stored on different storage mediums; Secondly, storage engine of backend can reduce the consumption of IO and CPU for unnecessary data merging, because the data in some partitions is no longer be updated; Lastly,every partition's buckets number can be different and adjusted according to the change of data size.
-
-```SQL
--- Create partitions using CREATE TABLE --
-CREATE TABLE example_tbl (
-    `date`      DATE,
-    userid      BIGINT,
-    metric      BIGINT SUM
-) PARTITION BY RANGE (`date`) (
-    PARTITION p201601 VALUES LESS THAN (""2016-02-01""),
-    PARTITION p201602 VALUES LESS THAN (""2016-03-01""),
-    PARTITION p201603 VALUES LESS THAN (""2016-04-01""),
-    PARTITION p201604 VALUES LESS THAN (""2016-05-01"")
-) DISTRIBUTED BY HASH(userid) BUCKETS 32;
-
--- Add partition using ALTER TABLE --
-ALTER TABLE example_tbl ADD PARTITION p201605 VALUES LESS THAN (""2016-06-01"");
+```
+yum -y install docker-io
+service docker start
 ```
 
-## 4. Backend
-
-#### 4.1 Data Storage Model
-
-Doris combines Google Mesa's data model and ORCFile / Parquet storage technology.
-
-Data in Mesa is inherently multi-dimensional fact table. These facts in table typically consist of two types of attributes: dimensional attributes (which we call keys) and measure attributes (which we call values). The table schema also specifies the aggregation function F: V ×V → V which is used to aggregate the values corresponding to the same key. To achieve high update throughput, Mesa loads data in batch. Each batch of data will be converted to a delta file. Mesa uses MVCC approach to manage these delta files, and so to enforce update atomicity. Mesa also supports creating materialized rollups, which contain a column subset of schema to gain better aggregation effect.
-
-Mesa's data model performs well in many interactive data service, but it also has some drawbacks:
-1. Users have difficulty in understanding key and value space, as well as aggregation function, especially when they rarely have such aggregation demand in analysis query scenarios.
-
-2. In order to ensure the aggregation semantic, count operation on a single column must read all columns in key space, resulting in a large number of additional read overheads. There is also unable to push down the predicates on the value column to storage engine, which also leads to additional read overheads.
-
-3.  Essentially, it is still a key-value model. In order to aggregate the values corresponding to the same key, all key columns must store in order. When a table contains hundreds of columns, sorting cost becomes the bottleneck of ETL process.
-
-To solve these problems, we introduce ORCFile / Parquet technology widely used in the open source community, such as MapReduce + ORCFile, SparkSQL + Parquet, mainly used for ad-hoc analysis of large amounts of data with low concurrency. These data does not distinguish between key and value. In addition, compared with the row-oriented database, column-oriented organization is more efficient when an aggregate needs to be computed over many rows but only for a small subset of all columns of data, because reading that smaller subset of data can be faster than reading all data. And columnar storage is also space-friendly due to the high compression ratio of each column. Further, column support block-level storage technology such as min/max index and bloom filter index. Query executor can filter out a lot of blocks that do not meet the predicate, to further improve the query performance. However, due to the underlying storage does not require data order, query time complexity is linear corresponding to the data volume.
-
-Like traditional databases, Doris stores structured data represented as tables. Each table has a well-defined schema consisting of a finite number of columns. We combine Mesa data model and ORCFile/Parquet technology to develop a distributed analytical database. User can create two types of table to meet different needs in interactive query scenarios.
+#### Step 2: Create Docker image
 
-In non-aggregation type of table, columns are not distinguished between dimensions and metrics, but should specify the sort columns in order to sort all rows. Doris will sort the table data according to the sort columns without any aggregation. The following figure gives an example of creating non-aggregation table.
+Given your work space is /my/workspace, and you can download Doris docker file as following:
 
-```SQL
--- Create non-aggregation table --
-CREATE TABLE example_tbl (
-    `date`      DATE,
-    id          BIGINT,
-    country     VARCHAR(32),
-    click       BIGINT,
-    cost        BIGINT
-) DUPLICATE KEY(`date`, id, country)
-DISTRIBUTED BY HASH(id) BUCKETS 32;
 ```
-
-In aggregation data analysis case, we reference Mesa's data model, and distinguish columns between key and value, and specify the value columns with aggregation method, such as SUM, REPLACE, etc. In the following figure, we create an aggregation table like the non-aggregation table, including two SUM aggregation columns (clicks, cost). Different from the non-aggregation table, data in the table needs to be sorted on all key columns for delta compaction and value aggregation.
-
-```SQL
--- Create aggregation table --
-CREATE TABLE example_tbl (
-    `date`      DATE,
-    id          BIGINT,
-    country     VARCHAR(32),
-    click       BIGINT          SUM,
-    cost        BIGINT          SUM
-) DISTRIBUTED BY HASH(id) BUCKETS 32;
+wget https://github.com/apache/incubator-doris/blob/master/docker/Dockerfile -O /my/workspace/Dockerfile
 ```
 
-Rollup is a materialized view that contains a column subset of schema in Doris. A table may contain multiple rollups with columns in different order. According to sort key index and column covering of the rollups, Doris can select the best rollup for different query. Because most rollups only contain a few columns, the size of aggregated data is typically much smaller and query performance can greatly be improved. All the rollups in the same table are updated atomically. Because rollups are materialized, users should make a trade-off between query latency and storage space when using them.
-
-To achieve high update throughput, Doris only applies updates in batches at the smallest frequency of every minute. Each update batch specifies an increased version number and generates a delta data file, commits the version when updates of quorum replicas are complete. You can query all committed data using the committed version, and the uncommitted version would not be used in query. All update versions are strictly be in increasing order. If an update contains more than one table, the versions of these tables are committed atomically. The MVCC mechanism allows Doris to guarantee multiple table atomic updates and query consistency. In addition, Doris uses compaction policies to merge delta files to reduce delta number, also reduce the cost of delta merging during query for higher performance.
-
-Doris' data file is stored by column. The rows are stored in sorted order by the sort columns in delta data files, and are organized into row blocks, each block is compressed by type-specific columnar encodings, such as run-length encoding for integer columns, then stored into separate streams. In order to improve the performance of queries that have a specific key, we also store a sparse sort key index file corresponding to each delta data file. An index entry contains the short key for the row block, which is a fixed size prefix of the first sort columns for the row block, and the block id in the data file. Index files are usually directly loaded into memory, as they are very small. The algorithm for querying a specific key includes two steps. First, use a binary search on the sort key index to find blocks that may contain the specific key, and then perform a binary search on the compressed blocks in the data files to find the desired key. We also store block-level min/max index into separate index streams, and queries can use this to filter undesired blocks. In addition to those basic columnar features, we also offers an optional block-level bloom filter index for queries with IN or EQUAL conditions to further filter undesired blocks. Bloom filter index is stored in a separate stream, and is loaded on demand.
+Now build your image: (It will take more time and use nohup if needed)","[{'comment': 'It will take more time and use nohup if needed\r\n->\r\nIt may take a long time(40min to 1 hour)', 'commenter': 'morningman'}, {'comment': ""no need to mention 'nohup' here."", 'commenter': 'morningman'}]"
345,README.md,"@@ -1,215 +1,127 @@
-# Introduction to Apache Doris (incubating)
+# Apache Doris (incubating)
 
-Apache Doris is an MPP-based interactive SQL data warehousing for reporting and analysis. Doris mainly integrates the technology of Google Mesa and Apache Impala. Unlike other popular SQL-on-Hadoop systems, Doris is designed to be a simple and single tightly coupled system, not depending on other systems. Doris not only provides high concurrent low latency point query performance, but also provides high throughput queries of ad-hoc analysis. Doris not only provides batch data loading, but also provides near real-time mini-batch data loading. Doris also provides high availability, reliability, fault tolerance, and scalability. The simplicity (of developing, deploying and using) and meeting many data serving requirements in single system are the main features of Doris.
+Doris is an MPP-based interactive SQL data warehousing for reporting and analysis. It open-sourced by Baidu. 
 
-## 1. Background
+## 1. License
 
-In Baidu, the largest Chinese search engine, we run a two-tiered data warehousing system for data processing, reporting and analysis. Similar to lambda architecture, the whole data warehouse comprises data processing and data serving. Data processing does the heavy lifting of big data: cleaning data, merging and transforming it, analyzing it and preparing it for use by end user queries; data serving is designed to serve queries against that data for different use cases. Currently data processing includes batch data processing and stream data processing technology, like Hadoop, Spark and Storm; Doris is a SQL data warehouse for serving online and interactive data reporting and analysis querying.
+[Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0)
 
-Prior to Doris, different tools were deployed to solve diverse requirements in many ways. For example, the advertising platform needs to provide some detailed statistics associated with each served ad for every advertiser. The platform must support continuous updates, both new rows and incremental updates to existing rows within minutes. It must support latency-sensitive users serving live customer reports with very low latency requirements and batch ad-hoc multiple dimensions data analysis requiring very high throughput. In the past,this platform was built on top of sharded MySQL. But with the growth of data, MySQL cannot meet the requirements. Then, based on our existing KV system, we developed our own proprietary distributed statistical database. But, the simple KV storage was not efficient on scan performance. Because the system depends on many other systems, it is very complex to operate and maintain. Using RPC API, more complex querying usually required code programming, but users wants an MPP SQL engine. In addition to advertising system, a large number of internal BI Reporting / Analysis, also used a variety of tools. Some used the combination of SparkSQL / Impala + HDFS / HBASE. Some used MySQL to store the results that were prepared by distributed MapReduce computing. Some also bought commercial databases to use.
+## 2. Technology
+Doris mainly integrates the technology of Google Mesa and Apache Impala, and it based on a column-oriented storage engine and can communicate by MySQL client.
 
-However, when a use case requires the simultaneous availability of capabilities that cannot all be provided by a single tool, users were forced to build hybrid architectures that stitch multiple tools together. Users often choose to ingest and update data in one storage system, but later reorganize this data to optimize for an analytical reporting use-case served from another. Our users had been successfully deploying and maintaining these hybrid architectures, but we believe that they shouldn't need to accept their inherent complexity. A storage system built to provide great performance across a broad range of workloads provides a more elegant solution to the problems that hybrid architectures aim to solve. Doris is the solution. Doris is designed to be a simple and single tightly coupled system, not depending on other systems. Doris provides high concurrent low latency point query performance, but also provides high throughput queries of ad-hoc analysis. Doris provides bulk-batch data loading, but also provides near real-time mini-batch data loading. Doris also provides high availability, reliability, fault tolerance, and scalability.
+## 3. User cases
+Doris not only provides high concurrent low latency point query performance, but also provides high throughput queries of ad-hoc analysis. 
 
-Generally speaking, Doris is the technology combination of Google Mesa and Apache Impala. Mesa is a highly scalable analytic data storage system that stores critical measurement data related to Google's Internet advertising business. Mesa is designed to satisfy complex and challenging set of users' and systems' requirements, including near real-time data ingestion and query ability, as well as high availability, reliability, fault tolerance, and scalability for large data and query volumes. Impala is a modern, open-source MPP SQL engine architected from the ground up for the Hadoop data processing environment. At present, by virtue of its superior performance and rich functionality, Impala has been comparable to many commercial MPP database query engine. Mesa can satisfy the needs of many of our storage requirements, however Mesa itself does not provide a SQL query engine; Impala is a very good MPP SQL query engine, but the lack of a perfect distributed storage engine. So in the end we chose the combination of these two technologies.
+Doris not only provides batch data loading, but also provides near real-time mini-batch data loading. 
 
-Learning from Mesa's data model, we developed a distributed storage engine. Unlike Mesa, this storage engine does not rely on any distributed file system. Then we deeply integrate this storage engine with Impala query engine. Query compiling, query execution coordination and catalog management of storage engine are integrated to be frontend daemon; query execution and data storage are integrated to be backend daemon. With this integration, we implemented a single, full-featured, high performance state the art of MPP database, as well as maintaining the simplicity.
+Doris also provides high availability, reliability, fault tolerance, and scalability. 
 
-## 2. System Overview
+The simplicity (of developing, deploying and using) and meeting many data serving requirements in single system are the main features of Doris.
 
-Doris' implementation consists of two daemons: frontend (FE) and backend (BE). The following figures gives the overview of architecture and usage.
+## 4. Compile and install
 
-![](./docs/resources/palo_architecture.jpg)
+Currently support Docker environment and Linux OS: 
+Docker（Linux/Windows/Mac), Ubuntu 16.04+ and CentOS 7.5+
 
-Frontend daemon consists of query coordinator and catalog manager. Query coordinator is responsible for receiving user's sql queries, compiling queries and managing queries execution. Catalog manager is responsible for managing metadata such as databases, tables, partitions, replicas and etc. Several frontend daemons could be deployed to guarantee fault-tolerance, and load balancing.
+### 4.1 For Docker
 
-Backend daemon stores the data and executes the query fragments. Many backend daemons could also be deployed to provide scalability and fault-tolerance.
+#### Step 1: Install Docker
 
-A typical Doris cluster generally composes of several frontend daemons and dozens to hundreds of backend daemons.
+Take CentOS as an example:
 
-Clients can use MySQL-related tools to connect any frontend daemon to submit SQL query. The frontend receives the query and compiles it into query plans executable by the backends. Then frontend sends the query plan fragments to backend. Backends will build a query execution DAG. Data is fetched and pipelined into the DAG. The final result response is sent to client via frontend. The distribution of query fragment execution takes minimizing data movement and maximizing scan locality as the main goal. Because Doris is designed to provide interactive analysis, so the average execution time of queries is short. Considering this, we adopt query re-execution to meet the fault tolerance of query execution.
-
-A table is splitted into many tablets. Tablets are managed by backends. The backend daemon could be configured to use multiple directories. Any directory's IO failure doesn't influence the normal running of backend daemon. Doris will recover and rebalance the whole cluster automatically when necessary.
-
-## 3. Frontend
-
-In-memory catalog, multiple frontends, MySQL networking protocol, consistency guarantee, and two-level table partitioning are the main features of Doris' frontend design.
-
-#### 3.1 In-Memory Catalog
-
-Traditional data warehouse always uses a RDBMS database to store their catalog metadata. In order to produce query execution plan, frontend needs to look up the catalog metadata. This kind of catalog storage may be enough for low concurrent ad-hoc analysis queries. But for online high concurrent queries, its performance is very bad,resulting in increased response latency. For example, Hive metadata query latency is sometimes up to tens of seconds or even minutes. In order to speedup the metadata access, we adopt the in-memory catalog storage.
-
-![](./docs/resources/log_replication.jpg)
-
-In-memory catalog storage has three functional modules: real-time memory data structures, memory checkpoints on local disk and an operation relay log. When modifying catalog, the mutation operation is written into the log file firstly. Then, the mutation operation is applied into the memory data structures. Periodically, a thread does the checkpoint that dumps memory data structure image into local disk. Checkpoint mechanism enables the fast startup of frontend and reduces the disk storage occupancy. Actually, in-memory catalog also simplifies the implementation of multiple frontends.
-
-#### 3.2 Multiple Frontends
-
-Many data warehouses only support single frontend-like node. There are some systems supporting master and slave deploying. But for online data serving, high availability is an essential feature. Further, the number of queries per seconds may be very large, so high scalability is also needed. In Doris, we provide the feature of multiple frontends using replicated-state-machine technology.
-
-Frontends can be configured to three kinds of roles: leader, follower and observer. Through a voting protocol, follower frontends firstly elect a leader frontend. All the write requests of metadata are forwarded to the leader, then the leader writes the operation into the replicated log file. If the new log entry will be replicated to at least quorum followers successfully, the leader commits the operation into memory, and responses the write request. Followers always replay the replicated logs to apply them into their memory metadata. If the leader crashes, a new leader will be elected from the leftover followers. Leader and follower mainly solve the problem of write availability and partly solve the problem of read scalability.
-
-Usually one leader frontend and several follower frontends can meet most applications' write availability and read scalability requirements. For very high concurrent reading, continuing to increase the number of followers is not a good practice. Leader replicates log stream to followers synchronously, so adding more followers will increases write latency. Like Zookeeper,we have introduced a new type of frontend node called observer that helps addressing this problem and further improving metadata read scalability. Leader replicates log stream to observers asynchronously. Observers don't involve leader election.
-
-The replicated-state-machine is implemented based on BerkeleyDB java version (BDB-JE). BDB-JE has achieved high availability by implementing a Paxos-like consensus algorithm.  We use BDB-JE to implement Doris' log replication and leader election.
-
-#### 3.3    Consistency Guarantee
-
-If a client process connects to the leader, it will see up-to-date metadata, so that strong consistency semantics is guaranteed. If the client connects to followers or observers, it will see metadata lagging a little behind of the leader, but the monotonic consistency is guaranteed. In most Doris' use cases, monotonic consistency is accepted.
-
-If the client always connects to the same frontend, monotonic consistency semantics is obviously guaranteed; however if the client connects to other frontends due to failover, the semantics may be violated. Doris provides a SYNC command to guarantee metadata monotonic consistency semantics during failover. When failover happens, the client can send a SYNC command to the new connected frontend, who will get the latest operation log number from the leader. The SYNC command will not return to client as long as local applied log number is still less than fetched operation log number. This mechanism can guarantee the metadata on the connected frontend is newer than the client have seen during its last connection.
-
-#### 3.4 MySQL Networking Protocol
-
-MySQL compatible networking protocol is implemented in Doris' frontend. Firstly, SQL interface is preferred for engineers; Secondly, compatibility with MySQL protocol makes the integrating with current existing BI software, such as Tableau, easier; Lastly, rich MySQL client libraries and tools reduce our development costs, but also reduces the user's using cost.
-
-Through the SQL interface, administrator can adjust system configuration, add and remove frontend nodes or backend nodes, and create new database for user; user can create tables, load data, and submit SQL query.
-
-Online help document and Linux Proc-like mechanism are also supported in SQL. Users can submit queries to get the help of related SQL statements or show Doris' internal running state.
-
-In frontend, a small response buffer is allocated to every MySQL connection. The maximum size of this buffer is limited to 1MB. The buffer is responsible for buffering the query response data. Only if the response is finished or the buffer size reaches the 1MB,the response data will begin to be sent to client. Through this small trick, frontend can re-execution most of queries if errors occurred during query execution.
-
-#### 3.5 Two-Level Partitioning
-
-Like most of the distributed database system, data in Doris is horizontally partitioned. However, a single-level partitioning rule (hash partitioning or range partitioning) may not be a good solution to all scenarios. For example, there have a user-based fact table that stores rows of the form (date, userid, metric). Choosing only hash partitioning by column userid may lead to uneven distribution of data, when one user's data is very large. If choosing range partitioning according to column date, it will also lead to uneven distribution of data due to the likely data explosion in a certain period of time.
-
-Therefore we support the two-level partitioning rule. The first level is range partitioning. User can specify a column (usually the time series column) range of values for the data partition. In one partition, the user can also specify one or more columns and a number of buckets to do the hash partitioning. User can combine with different partitioning rules to better divide the data. Figure 4 gives an example of two-level partitioning.
-
-Three benefits are gained by using the two-level partitioning mechanism. Firstly, old and new data could be separated, and stored on different storage mediums; Secondly, storage engine of backend can reduce the consumption of IO and CPU for unnecessary data merging, because the data in some partitions is no longer be updated; Lastly,every partition's buckets number can be different and adjusted according to the change of data size.
-
-```SQL
--- Create partitions using CREATE TABLE --
-CREATE TABLE example_tbl (
-    `date`      DATE,
-    userid      BIGINT,
-    metric      BIGINT SUM
-) PARTITION BY RANGE (`date`) (
-    PARTITION p201601 VALUES LESS THAN (""2016-02-01""),
-    PARTITION p201602 VALUES LESS THAN (""2016-03-01""),
-    PARTITION p201603 VALUES LESS THAN (""2016-04-01""),
-    PARTITION p201604 VALUES LESS THAN (""2016-05-01"")
-) DISTRIBUTED BY HASH(userid) BUCKETS 32;
-
--- Add partition using ALTER TABLE --
-ALTER TABLE example_tbl ADD PARTITION p201605 VALUES LESS THAN (""2016-06-01"");
+```
+yum -y install docker-io
+service docker start
 ```
 
-## 4. Backend
-
-#### 4.1 Data Storage Model
-
-Doris combines Google Mesa's data model and ORCFile / Parquet storage technology.
-
-Data in Mesa is inherently multi-dimensional fact table. These facts in table typically consist of two types of attributes: dimensional attributes (which we call keys) and measure attributes (which we call values). The table schema also specifies the aggregation function F: V ×V → V which is used to aggregate the values corresponding to the same key. To achieve high update throughput, Mesa loads data in batch. Each batch of data will be converted to a delta file. Mesa uses MVCC approach to manage these delta files, and so to enforce update atomicity. Mesa also supports creating materialized rollups, which contain a column subset of schema to gain better aggregation effect.
-
-Mesa's data model performs well in many interactive data service, but it also has some drawbacks:
-1. Users have difficulty in understanding key and value space, as well as aggregation function, especially when they rarely have such aggregation demand in analysis query scenarios.
-
-2. In order to ensure the aggregation semantic, count operation on a single column must read all columns in key space, resulting in a large number of additional read overheads. There is also unable to push down the predicates on the value column to storage engine, which also leads to additional read overheads.
-
-3.  Essentially, it is still a key-value model. In order to aggregate the values corresponding to the same key, all key columns must store in order. When a table contains hundreds of columns, sorting cost becomes the bottleneck of ETL process.
-
-To solve these problems, we introduce ORCFile / Parquet technology widely used in the open source community, such as MapReduce + ORCFile, SparkSQL + Parquet, mainly used for ad-hoc analysis of large amounts of data with low concurrency. These data does not distinguish between key and value. In addition, compared with the row-oriented database, column-oriented organization is more efficient when an aggregate needs to be computed over many rows but only for a small subset of all columns of data, because reading that smaller subset of data can be faster than reading all data. And columnar storage is also space-friendly due to the high compression ratio of each column. Further, column support block-level storage technology such as min/max index and bloom filter index. Query executor can filter out a lot of blocks that do not meet the predicate, to further improve the query performance. However, due to the underlying storage does not require data order, query time complexity is linear corresponding to the data volume.
-
-Like traditional databases, Doris stores structured data represented as tables. Each table has a well-defined schema consisting of a finite number of columns. We combine Mesa data model and ORCFile/Parquet technology to develop a distributed analytical database. User can create two types of table to meet different needs in interactive query scenarios.
+#### Step 2: Create Docker image","[{'comment': 'Create Docker image\r\n->\r\nCreate Docker image for compilation', 'commenter': 'morningman'}]"
345,README.md,"@@ -1,215 +1,127 @@
-# Introduction to Apache Doris (incubating)
+# Apache Doris (incubating)
 
-Apache Doris is an MPP-based interactive SQL data warehousing for reporting and analysis. Doris mainly integrates the technology of Google Mesa and Apache Impala. Unlike other popular SQL-on-Hadoop systems, Doris is designed to be a simple and single tightly coupled system, not depending on other systems. Doris not only provides high concurrent low latency point query performance, but also provides high throughput queries of ad-hoc analysis. Doris not only provides batch data loading, but also provides near real-time mini-batch data loading. Doris also provides high availability, reliability, fault tolerance, and scalability. The simplicity (of developing, deploying and using) and meeting many data serving requirements in single system are the main features of Doris.
+Doris is an MPP-based interactive SQL data warehousing for reporting and analysis. It open-sourced by Baidu. 
 
-## 1. Background
+## 1. License
 
-In Baidu, the largest Chinese search engine, we run a two-tiered data warehousing system for data processing, reporting and analysis. Similar to lambda architecture, the whole data warehouse comprises data processing and data serving. Data processing does the heavy lifting of big data: cleaning data, merging and transforming it, analyzing it and preparing it for use by end user queries; data serving is designed to serve queries against that data for different use cases. Currently data processing includes batch data processing and stream data processing technology, like Hadoop, Spark and Storm; Doris is a SQL data warehouse for serving online and interactive data reporting and analysis querying.
+[Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0)
 
-Prior to Doris, different tools were deployed to solve diverse requirements in many ways. For example, the advertising platform needs to provide some detailed statistics associated with each served ad for every advertiser. The platform must support continuous updates, both new rows and incremental updates to existing rows within minutes. It must support latency-sensitive users serving live customer reports with very low latency requirements and batch ad-hoc multiple dimensions data analysis requiring very high throughput. In the past,this platform was built on top of sharded MySQL. But with the growth of data, MySQL cannot meet the requirements. Then, based on our existing KV system, we developed our own proprietary distributed statistical database. But, the simple KV storage was not efficient on scan performance. Because the system depends on many other systems, it is very complex to operate and maintain. Using RPC API, more complex querying usually required code programming, but users wants an MPP SQL engine. In addition to advertising system, a large number of internal BI Reporting / Analysis, also used a variety of tools. Some used the combination of SparkSQL / Impala + HDFS / HBASE. Some used MySQL to store the results that were prepared by distributed MapReduce computing. Some also bought commercial databases to use.
+## 2. Technology
+Doris mainly integrates the technology of Google Mesa and Apache Impala, and it based on a column-oriented storage engine and can communicate by MySQL client.
 
-However, when a use case requires the simultaneous availability of capabilities that cannot all be provided by a single tool, users were forced to build hybrid architectures that stitch multiple tools together. Users often choose to ingest and update data in one storage system, but later reorganize this data to optimize for an analytical reporting use-case served from another. Our users had been successfully deploying and maintaining these hybrid architectures, but we believe that they shouldn't need to accept their inherent complexity. A storage system built to provide great performance across a broad range of workloads provides a more elegant solution to the problems that hybrid architectures aim to solve. Doris is the solution. Doris is designed to be a simple and single tightly coupled system, not depending on other systems. Doris provides high concurrent low latency point query performance, but also provides high throughput queries of ad-hoc analysis. Doris provides bulk-batch data loading, but also provides near real-time mini-batch data loading. Doris also provides high availability, reliability, fault tolerance, and scalability.
+## 3. User cases
+Doris not only provides high concurrent low latency point query performance, but also provides high throughput queries of ad-hoc analysis. 
 
-Generally speaking, Doris is the technology combination of Google Mesa and Apache Impala. Mesa is a highly scalable analytic data storage system that stores critical measurement data related to Google's Internet advertising business. Mesa is designed to satisfy complex and challenging set of users' and systems' requirements, including near real-time data ingestion and query ability, as well as high availability, reliability, fault tolerance, and scalability for large data and query volumes. Impala is a modern, open-source MPP SQL engine architected from the ground up for the Hadoop data processing environment. At present, by virtue of its superior performance and rich functionality, Impala has been comparable to many commercial MPP database query engine. Mesa can satisfy the needs of many of our storage requirements, however Mesa itself does not provide a SQL query engine; Impala is a very good MPP SQL query engine, but the lack of a perfect distributed storage engine. So in the end we chose the combination of these two technologies.
+Doris not only provides batch data loading, but also provides near real-time mini-batch data loading. 
 
-Learning from Mesa's data model, we developed a distributed storage engine. Unlike Mesa, this storage engine does not rely on any distributed file system. Then we deeply integrate this storage engine with Impala query engine. Query compiling, query execution coordination and catalog management of storage engine are integrated to be frontend daemon; query execution and data storage are integrated to be backend daemon. With this integration, we implemented a single, full-featured, high performance state the art of MPP database, as well as maintaining the simplicity.
+Doris also provides high availability, reliability, fault tolerance, and scalability. 
 
-## 2. System Overview
+The simplicity (of developing, deploying and using) and meeting many data serving requirements in single system are the main features of Doris.
 
-Doris' implementation consists of two daemons: frontend (FE) and backend (BE). The following figures gives the overview of architecture and usage.
+## 4. Compile and install
 
-![](./docs/resources/palo_architecture.jpg)
+Currently support Docker environment and Linux OS: 
+Docker（Linux/Windows/Mac), Ubuntu 16.04+ and CentOS 7.5+
 
-Frontend daemon consists of query coordinator and catalog manager. Query coordinator is responsible for receiving user's sql queries, compiling queries and managing queries execution. Catalog manager is responsible for managing metadata such as databases, tables, partitions, replicas and etc. Several frontend daemons could be deployed to guarantee fault-tolerance, and load balancing.
+### 4.1 For Docker
 
-Backend daemon stores the data and executes the query fragments. Many backend daemons could also be deployed to provide scalability and fault-tolerance.
+#### Step 1: Install Docker
 
-A typical Doris cluster generally composes of several frontend daemons and dozens to hundreds of backend daemons.
+Take CentOS as an example:
 
-Clients can use MySQL-related tools to connect any frontend daemon to submit SQL query. The frontend receives the query and compiles it into query plans executable by the backends. Then frontend sends the query plan fragments to backend. Backends will build a query execution DAG. Data is fetched and pipelined into the DAG. The final result response is sent to client via frontend. The distribution of query fragment execution takes minimizing data movement and maximizing scan locality as the main goal. Because Doris is designed to provide interactive analysis, so the average execution time of queries is short. Considering this, we adopt query re-execution to meet the fault tolerance of query execution.
-
-A table is splitted into many tablets. Tablets are managed by backends. The backend daemon could be configured to use multiple directories. Any directory's IO failure doesn't influence the normal running of backend daemon. Doris will recover and rebalance the whole cluster automatically when necessary.
-
-## 3. Frontend
-
-In-memory catalog, multiple frontends, MySQL networking protocol, consistency guarantee, and two-level table partitioning are the main features of Doris' frontend design.
-
-#### 3.1 In-Memory Catalog
-
-Traditional data warehouse always uses a RDBMS database to store their catalog metadata. In order to produce query execution plan, frontend needs to look up the catalog metadata. This kind of catalog storage may be enough for low concurrent ad-hoc analysis queries. But for online high concurrent queries, its performance is very bad,resulting in increased response latency. For example, Hive metadata query latency is sometimes up to tens of seconds or even minutes. In order to speedup the metadata access, we adopt the in-memory catalog storage.
-
-![](./docs/resources/log_replication.jpg)
-
-In-memory catalog storage has three functional modules: real-time memory data structures, memory checkpoints on local disk and an operation relay log. When modifying catalog, the mutation operation is written into the log file firstly. Then, the mutation operation is applied into the memory data structures. Periodically, a thread does the checkpoint that dumps memory data structure image into local disk. Checkpoint mechanism enables the fast startup of frontend and reduces the disk storage occupancy. Actually, in-memory catalog also simplifies the implementation of multiple frontends.
-
-#### 3.2 Multiple Frontends
-
-Many data warehouses only support single frontend-like node. There are some systems supporting master and slave deploying. But for online data serving, high availability is an essential feature. Further, the number of queries per seconds may be very large, so high scalability is also needed. In Doris, we provide the feature of multiple frontends using replicated-state-machine technology.
-
-Frontends can be configured to three kinds of roles: leader, follower and observer. Through a voting protocol, follower frontends firstly elect a leader frontend. All the write requests of metadata are forwarded to the leader, then the leader writes the operation into the replicated log file. If the new log entry will be replicated to at least quorum followers successfully, the leader commits the operation into memory, and responses the write request. Followers always replay the replicated logs to apply them into their memory metadata. If the leader crashes, a new leader will be elected from the leftover followers. Leader and follower mainly solve the problem of write availability and partly solve the problem of read scalability.
-
-Usually one leader frontend and several follower frontends can meet most applications' write availability and read scalability requirements. For very high concurrent reading, continuing to increase the number of followers is not a good practice. Leader replicates log stream to followers synchronously, so adding more followers will increases write latency. Like Zookeeper,we have introduced a new type of frontend node called observer that helps addressing this problem and further improving metadata read scalability. Leader replicates log stream to observers asynchronously. Observers don't involve leader election.
-
-The replicated-state-machine is implemented based on BerkeleyDB java version (BDB-JE). BDB-JE has achieved high availability by implementing a Paxos-like consensus algorithm.  We use BDB-JE to implement Doris' log replication and leader election.
-
-#### 3.3    Consistency Guarantee
-
-If a client process connects to the leader, it will see up-to-date metadata, so that strong consistency semantics is guaranteed. If the client connects to followers or observers, it will see metadata lagging a little behind of the leader, but the monotonic consistency is guaranteed. In most Doris' use cases, monotonic consistency is accepted.
-
-If the client always connects to the same frontend, monotonic consistency semantics is obviously guaranteed; however if the client connects to other frontends due to failover, the semantics may be violated. Doris provides a SYNC command to guarantee metadata monotonic consistency semantics during failover. When failover happens, the client can send a SYNC command to the new connected frontend, who will get the latest operation log number from the leader. The SYNC command will not return to client as long as local applied log number is still less than fetched operation log number. This mechanism can guarantee the metadata on the connected frontend is newer than the client have seen during its last connection.
-
-#### 3.4 MySQL Networking Protocol
-
-MySQL compatible networking protocol is implemented in Doris' frontend. Firstly, SQL interface is preferred for engineers; Secondly, compatibility with MySQL protocol makes the integrating with current existing BI software, such as Tableau, easier; Lastly, rich MySQL client libraries and tools reduce our development costs, but also reduces the user's using cost.
-
-Through the SQL interface, administrator can adjust system configuration, add and remove frontend nodes or backend nodes, and create new database for user; user can create tables, load data, and submit SQL query.
-
-Online help document and Linux Proc-like mechanism are also supported in SQL. Users can submit queries to get the help of related SQL statements or show Doris' internal running state.
-
-In frontend, a small response buffer is allocated to every MySQL connection. The maximum size of this buffer is limited to 1MB. The buffer is responsible for buffering the query response data. Only if the response is finished or the buffer size reaches the 1MB,the response data will begin to be sent to client. Through this small trick, frontend can re-execution most of queries if errors occurred during query execution.
-
-#### 3.5 Two-Level Partitioning
-
-Like most of the distributed database system, data in Doris is horizontally partitioned. However, a single-level partitioning rule (hash partitioning or range partitioning) may not be a good solution to all scenarios. For example, there have a user-based fact table that stores rows of the form (date, userid, metric). Choosing only hash partitioning by column userid may lead to uneven distribution of data, when one user's data is very large. If choosing range partitioning according to column date, it will also lead to uneven distribution of data due to the likely data explosion in a certain period of time.
-
-Therefore we support the two-level partitioning rule. The first level is range partitioning. User can specify a column (usually the time series column) range of values for the data partition. In one partition, the user can also specify one or more columns and a number of buckets to do the hash partitioning. User can combine with different partitioning rules to better divide the data. Figure 4 gives an example of two-level partitioning.
-
-Three benefits are gained by using the two-level partitioning mechanism. Firstly, old and new data could be separated, and stored on different storage mediums; Secondly, storage engine of backend can reduce the consumption of IO and CPU for unnecessary data merging, because the data in some partitions is no longer be updated; Lastly,every partition's buckets number can be different and adjusted according to the change of data size.
-
-```SQL
--- Create partitions using CREATE TABLE --
-CREATE TABLE example_tbl (
-    `date`      DATE,
-    userid      BIGINT,
-    metric      BIGINT SUM
-) PARTITION BY RANGE (`date`) (
-    PARTITION p201601 VALUES LESS THAN (""2016-02-01""),
-    PARTITION p201602 VALUES LESS THAN (""2016-03-01""),
-    PARTITION p201603 VALUES LESS THAN (""2016-04-01""),
-    PARTITION p201604 VALUES LESS THAN (""2016-05-01"")
-) DISTRIBUTED BY HASH(userid) BUCKETS 32;
-
--- Add partition using ALTER TABLE --
-ALTER TABLE example_tbl ADD PARTITION p201605 VALUES LESS THAN (""2016-06-01"");
+```
+yum -y install docker-io
+service docker start
 ```
 
-## 4. Backend
-
-#### 4.1 Data Storage Model
-
-Doris combines Google Mesa's data model and ORCFile / Parquet storage technology.
-
-Data in Mesa is inherently multi-dimensional fact table. These facts in table typically consist of two types of attributes: dimensional attributes (which we call keys) and measure attributes (which we call values). The table schema also specifies the aggregation function F: V ×V → V which is used to aggregate the values corresponding to the same key. To achieve high update throughput, Mesa loads data in batch. Each batch of data will be converted to a delta file. Mesa uses MVCC approach to manage these delta files, and so to enforce update atomicity. Mesa also supports creating materialized rollups, which contain a column subset of schema to gain better aggregation effect.
-
-Mesa's data model performs well in many interactive data service, but it also has some drawbacks:
-1. Users have difficulty in understanding key and value space, as well as aggregation function, especially when they rarely have such aggregation demand in analysis query scenarios.
-
-2. In order to ensure the aggregation semantic, count operation on a single column must read all columns in key space, resulting in a large number of additional read overheads. There is also unable to push down the predicates on the value column to storage engine, which also leads to additional read overheads.
-
-3.  Essentially, it is still a key-value model. In order to aggregate the values corresponding to the same key, all key columns must store in order. When a table contains hundreds of columns, sorting cost becomes the bottleneck of ETL process.
-
-To solve these problems, we introduce ORCFile / Parquet technology widely used in the open source community, such as MapReduce + ORCFile, SparkSQL + Parquet, mainly used for ad-hoc analysis of large amounts of data with low concurrency. These data does not distinguish between key and value. In addition, compared with the row-oriented database, column-oriented organization is more efficient when an aggregate needs to be computed over many rows but only for a small subset of all columns of data, because reading that smaller subset of data can be faster than reading all data. And columnar storage is also space-friendly due to the high compression ratio of each column. Further, column support block-level storage technology such as min/max index and bloom filter index. Query executor can filter out a lot of blocks that do not meet the predicate, to further improve the query performance. However, due to the underlying storage does not require data order, query time complexity is linear corresponding to the data volume.
-
-Like traditional databases, Doris stores structured data represented as tables. Each table has a well-defined schema consisting of a finite number of columns. We combine Mesa data model and ORCFile/Parquet technology to develop a distributed analytical database. User can create two types of table to meet different needs in interactive query scenarios.
+#### Step 2: Create Docker image
 
-In non-aggregation type of table, columns are not distinguished between dimensions and metrics, but should specify the sort columns in order to sort all rows. Doris will sort the table data according to the sort columns without any aggregation. The following figure gives an example of creating non-aggregation table.
+Given your work space is /my/workspace, and you can download Doris docker file as following:
 
-```SQL
--- Create non-aggregation table --
-CREATE TABLE example_tbl (
-    `date`      DATE,
-    id          BIGINT,
-    country     VARCHAR(32),
-    click       BIGINT,
-    cost        BIGINT
-) DUPLICATE KEY(`date`, id, country)
-DISTRIBUTED BY HASH(id) BUCKETS 32;
 ```
-
-In aggregation data analysis case, we reference Mesa's data model, and distinguish columns between key and value, and specify the value columns with aggregation method, such as SUM, REPLACE, etc. In the following figure, we create an aggregation table like the non-aggregation table, including two SUM aggregation columns (clicks, cost). Different from the non-aggregation table, data in the table needs to be sorted on all key columns for delta compaction and value aggregation.
-
-```SQL
--- Create aggregation table --
-CREATE TABLE example_tbl (
-    `date`      DATE,
-    id          BIGINT,
-    country     VARCHAR(32),
-    click       BIGINT          SUM,
-    cost        BIGINT          SUM
-) DISTRIBUTED BY HASH(id) BUCKETS 32;
+wget https://github.com/apache/incubator-doris/blob/master/docker/Dockerfile -O /my/workspace/Dockerfile
 ```
 
-Rollup is a materialized view that contains a column subset of schema in Doris. A table may contain multiple rollups with columns in different order. According to sort key index and column covering of the rollups, Doris can select the best rollup for different query. Because most rollups only contain a few columns, the size of aggregated data is typically much smaller and query performance can greatly be improved. All the rollups in the same table are updated atomically. Because rollups are materialized, users should make a trade-off between query latency and storage space when using them.
-
-To achieve high update throughput, Doris only applies updates in batches at the smallest frequency of every minute. Each update batch specifies an increased version number and generates a delta data file, commits the version when updates of quorum replicas are complete. You can query all committed data using the committed version, and the uncommitted version would not be used in query. All update versions are strictly be in increasing order. If an update contains more than one table, the versions of these tables are committed atomically. The MVCC mechanism allows Doris to guarantee multiple table atomic updates and query consistency. In addition, Doris uses compaction policies to merge delta files to reduce delta number, also reduce the cost of delta merging during query for higher performance.
-
-Doris' data file is stored by column. The rows are stored in sorted order by the sort columns in delta data files, and are organized into row blocks, each block is compressed by type-specific columnar encodings, such as run-length encoding for integer columns, then stored into separate streams. In order to improve the performance of queries that have a specific key, we also store a sparse sort key index file corresponding to each delta data file. An index entry contains the short key for the row block, which is a fixed size prefix of the first sort columns for the row block, and the block id in the data file. Index files are usually directly loaded into memory, as they are very small. The algorithm for querying a specific key includes two steps. First, use a binary search on the sort key index to find blocks that may contain the specific key, and then perform a binary search on the compressed blocks in the data files to find the desired key. We also store block-level min/max index into separate index streams, and queries can use this to filter undesired blocks. In addition to those basic columnar features, we also offers an optional block-level bloom filter index for queries with IN or EQUAL conditions to further filter undesired blocks. Bloom filter index is stored in a separate stream, and is loaded on demand.
+Now build your image: (It will take more time and use nohup if needed)
 
-#### 4.2 Data Loading
+```
+cd /my/workspace && docker build -t doris-dev:v1.0 .
+```
 
-Doris applies updates in batches. Three types of data loading are supported: Hadoop-batch loading, loading ,mini-batch loading.
+#### Step 3: Compile and install Doris
 
-1. Hadoop-batch loading. When a large amount of data volume needs to be loaded into Doris, the hadoop-batch loading is recommended to achieve high loading throughput. The data batches themselves are produced by an external Hadoop system, typically at a frequency of every few minutes. Unlike traditional data warehouses that use their own computing resource to do the heavy data preparation, Doris could use Hadoop to prepare the data (shuffle, sort and aggregate, etc.). By using this approach, the most time-consuming computations are handed over to Hadoop to complete. This will not only improve computational efficiency, but also reduce the performance pressure of Doris cluster and ensure the stability of the query service. The stability of the online data services is the most important point.
+Clone Doris source:
 
-2. Loading. After deploying the fs-brokers, you can use Doris' query engine to import data. This type of loading is recommended for incremental data loading.
+```
+git clone https://github.com/apache/incubator-doris.git /path/to/incubator-doris/
+```
 
-3. Mini-batch loading. When a small amount of data needs to be loaded into Doris, the mini-batch loading is recommended to achieve low loading latency.  By using http interface, raw data is pushed into a backend. Then the backend does the data preparing computing and completes the final loading. Http tools could connect frontend or backend. If frontend is connected, it will redirect the request randomly to a backend.
+Start a docker named doris-dev-test, and map /path/to/incubator-doris/ to /var/local/incubator-doris/ which in docker.","[{'comment': 'Start a docker \r\n->\r\nStart a container', 'commenter': 'morningman'}, {'comment': 'which in docker\r\n-> in container', 'commenter': 'morningman'}]"
345,README.md,"@@ -1,215 +1,127 @@
-# Introduction to Apache Doris (incubating)
+# Apache Doris (incubating)
 
-Apache Doris is an MPP-based interactive SQL data warehousing for reporting and analysis. Doris mainly integrates the technology of Google Mesa and Apache Impala. Unlike other popular SQL-on-Hadoop systems, Doris is designed to be a simple and single tightly coupled system, not depending on other systems. Doris not only provides high concurrent low latency point query performance, but also provides high throughput queries of ad-hoc analysis. Doris not only provides batch data loading, but also provides near real-time mini-batch data loading. Doris also provides high availability, reliability, fault tolerance, and scalability. The simplicity (of developing, deploying and using) and meeting many data serving requirements in single system are the main features of Doris.
+Doris is an MPP-based interactive SQL data warehousing for reporting and analysis. It open-sourced by Baidu. 
 
-## 1. Background
+## 1. License
 
-In Baidu, the largest Chinese search engine, we run a two-tiered data warehousing system for data processing, reporting and analysis. Similar to lambda architecture, the whole data warehouse comprises data processing and data serving. Data processing does the heavy lifting of big data: cleaning data, merging and transforming it, analyzing it and preparing it for use by end user queries; data serving is designed to serve queries against that data for different use cases. Currently data processing includes batch data processing and stream data processing technology, like Hadoop, Spark and Storm; Doris is a SQL data warehouse for serving online and interactive data reporting and analysis querying.
+[Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0)
 
-Prior to Doris, different tools were deployed to solve diverse requirements in many ways. For example, the advertising platform needs to provide some detailed statistics associated with each served ad for every advertiser. The platform must support continuous updates, both new rows and incremental updates to existing rows within minutes. It must support latency-sensitive users serving live customer reports with very low latency requirements and batch ad-hoc multiple dimensions data analysis requiring very high throughput. In the past,this platform was built on top of sharded MySQL. But with the growth of data, MySQL cannot meet the requirements. Then, based on our existing KV system, we developed our own proprietary distributed statistical database. But, the simple KV storage was not efficient on scan performance. Because the system depends on many other systems, it is very complex to operate and maintain. Using RPC API, more complex querying usually required code programming, but users wants an MPP SQL engine. In addition to advertising system, a large number of internal BI Reporting / Analysis, also used a variety of tools. Some used the combination of SparkSQL / Impala + HDFS / HBASE. Some used MySQL to store the results that were prepared by distributed MapReduce computing. Some also bought commercial databases to use.
+## 2. Technology
+Doris mainly integrates the technology of Google Mesa and Apache Impala, and it based on a column-oriented storage engine and can communicate by MySQL client.
 
-However, when a use case requires the simultaneous availability of capabilities that cannot all be provided by a single tool, users were forced to build hybrid architectures that stitch multiple tools together. Users often choose to ingest and update data in one storage system, but later reorganize this data to optimize for an analytical reporting use-case served from another. Our users had been successfully deploying and maintaining these hybrid architectures, but we believe that they shouldn't need to accept their inherent complexity. A storage system built to provide great performance across a broad range of workloads provides a more elegant solution to the problems that hybrid architectures aim to solve. Doris is the solution. Doris is designed to be a simple and single tightly coupled system, not depending on other systems. Doris provides high concurrent low latency point query performance, but also provides high throughput queries of ad-hoc analysis. Doris provides bulk-batch data loading, but also provides near real-time mini-batch data loading. Doris also provides high availability, reliability, fault tolerance, and scalability.
+## 3. User cases
+Doris not only provides high concurrent low latency point query performance, but also provides high throughput queries of ad-hoc analysis. 
 
-Generally speaking, Doris is the technology combination of Google Mesa and Apache Impala. Mesa is a highly scalable analytic data storage system that stores critical measurement data related to Google's Internet advertising business. Mesa is designed to satisfy complex and challenging set of users' and systems' requirements, including near real-time data ingestion and query ability, as well as high availability, reliability, fault tolerance, and scalability for large data and query volumes. Impala is a modern, open-source MPP SQL engine architected from the ground up for the Hadoop data processing environment. At present, by virtue of its superior performance and rich functionality, Impala has been comparable to many commercial MPP database query engine. Mesa can satisfy the needs of many of our storage requirements, however Mesa itself does not provide a SQL query engine; Impala is a very good MPP SQL query engine, but the lack of a perfect distributed storage engine. So in the end we chose the combination of these two technologies.
+Doris not only provides batch data loading, but also provides near real-time mini-batch data loading. 
 
-Learning from Mesa's data model, we developed a distributed storage engine. Unlike Mesa, this storage engine does not rely on any distributed file system. Then we deeply integrate this storage engine with Impala query engine. Query compiling, query execution coordination and catalog management of storage engine are integrated to be frontend daemon; query execution and data storage are integrated to be backend daemon. With this integration, we implemented a single, full-featured, high performance state the art of MPP database, as well as maintaining the simplicity.
+Doris also provides high availability, reliability, fault tolerance, and scalability. 
 
-## 2. System Overview
+The simplicity (of developing, deploying and using) and meeting many data serving requirements in single system are the main features of Doris.
 
-Doris' implementation consists of two daemons: frontend (FE) and backend (BE). The following figures gives the overview of architecture and usage.
+## 4. Compile and install
 
-![](./docs/resources/palo_architecture.jpg)
+Currently support Docker environment and Linux OS: 
+Docker（Linux/Windows/Mac), Ubuntu 16.04+ and CentOS 7.5+
 
-Frontend daemon consists of query coordinator and catalog manager. Query coordinator is responsible for receiving user's sql queries, compiling queries and managing queries execution. Catalog manager is responsible for managing metadata such as databases, tables, partitions, replicas and etc. Several frontend daemons could be deployed to guarantee fault-tolerance, and load balancing.
+### 4.1 For Docker
 
-Backend daemon stores the data and executes the query fragments. Many backend daemons could also be deployed to provide scalability and fault-tolerance.
+#### Step 1: Install Docker
 
-A typical Doris cluster generally composes of several frontend daemons and dozens to hundreds of backend daemons.
+Take CentOS as an example:
 
-Clients can use MySQL-related tools to connect any frontend daemon to submit SQL query. The frontend receives the query and compiles it into query plans executable by the backends. Then frontend sends the query plan fragments to backend. Backends will build a query execution DAG. Data is fetched and pipelined into the DAG. The final result response is sent to client via frontend. The distribution of query fragment execution takes minimizing data movement and maximizing scan locality as the main goal. Because Doris is designed to provide interactive analysis, so the average execution time of queries is short. Considering this, we adopt query re-execution to meet the fault tolerance of query execution.
-
-A table is splitted into many tablets. Tablets are managed by backends. The backend daemon could be configured to use multiple directories. Any directory's IO failure doesn't influence the normal running of backend daemon. Doris will recover and rebalance the whole cluster automatically when necessary.
-
-## 3. Frontend
-
-In-memory catalog, multiple frontends, MySQL networking protocol, consistency guarantee, and two-level table partitioning are the main features of Doris' frontend design.
-
-#### 3.1 In-Memory Catalog
-
-Traditional data warehouse always uses a RDBMS database to store their catalog metadata. In order to produce query execution plan, frontend needs to look up the catalog metadata. This kind of catalog storage may be enough for low concurrent ad-hoc analysis queries. But for online high concurrent queries, its performance is very bad,resulting in increased response latency. For example, Hive metadata query latency is sometimes up to tens of seconds or even minutes. In order to speedup the metadata access, we adopt the in-memory catalog storage.
-
-![](./docs/resources/log_replication.jpg)
-
-In-memory catalog storage has three functional modules: real-time memory data structures, memory checkpoints on local disk and an operation relay log. When modifying catalog, the mutation operation is written into the log file firstly. Then, the mutation operation is applied into the memory data structures. Periodically, a thread does the checkpoint that dumps memory data structure image into local disk. Checkpoint mechanism enables the fast startup of frontend and reduces the disk storage occupancy. Actually, in-memory catalog also simplifies the implementation of multiple frontends.
-
-#### 3.2 Multiple Frontends
-
-Many data warehouses only support single frontend-like node. There are some systems supporting master and slave deploying. But for online data serving, high availability is an essential feature. Further, the number of queries per seconds may be very large, so high scalability is also needed. In Doris, we provide the feature of multiple frontends using replicated-state-machine technology.
-
-Frontends can be configured to three kinds of roles: leader, follower and observer. Through a voting protocol, follower frontends firstly elect a leader frontend. All the write requests of metadata are forwarded to the leader, then the leader writes the operation into the replicated log file. If the new log entry will be replicated to at least quorum followers successfully, the leader commits the operation into memory, and responses the write request. Followers always replay the replicated logs to apply them into their memory metadata. If the leader crashes, a new leader will be elected from the leftover followers. Leader and follower mainly solve the problem of write availability and partly solve the problem of read scalability.
-
-Usually one leader frontend and several follower frontends can meet most applications' write availability and read scalability requirements. For very high concurrent reading, continuing to increase the number of followers is not a good practice. Leader replicates log stream to followers synchronously, so adding more followers will increases write latency. Like Zookeeper,we have introduced a new type of frontend node called observer that helps addressing this problem and further improving metadata read scalability. Leader replicates log stream to observers asynchronously. Observers don't involve leader election.
-
-The replicated-state-machine is implemented based on BerkeleyDB java version (BDB-JE). BDB-JE has achieved high availability by implementing a Paxos-like consensus algorithm.  We use BDB-JE to implement Doris' log replication and leader election.
-
-#### 3.3    Consistency Guarantee
-
-If a client process connects to the leader, it will see up-to-date metadata, so that strong consistency semantics is guaranteed. If the client connects to followers or observers, it will see metadata lagging a little behind of the leader, but the monotonic consistency is guaranteed. In most Doris' use cases, monotonic consistency is accepted.
-
-If the client always connects to the same frontend, monotonic consistency semantics is obviously guaranteed; however if the client connects to other frontends due to failover, the semantics may be violated. Doris provides a SYNC command to guarantee metadata monotonic consistency semantics during failover. When failover happens, the client can send a SYNC command to the new connected frontend, who will get the latest operation log number from the leader. The SYNC command will not return to client as long as local applied log number is still less than fetched operation log number. This mechanism can guarantee the metadata on the connected frontend is newer than the client have seen during its last connection.
-
-#### 3.4 MySQL Networking Protocol
-
-MySQL compatible networking protocol is implemented in Doris' frontend. Firstly, SQL interface is preferred for engineers; Secondly, compatibility with MySQL protocol makes the integrating with current existing BI software, such as Tableau, easier; Lastly, rich MySQL client libraries and tools reduce our development costs, but also reduces the user's using cost.
-
-Through the SQL interface, administrator can adjust system configuration, add and remove frontend nodes or backend nodes, and create new database for user; user can create tables, load data, and submit SQL query.
-
-Online help document and Linux Proc-like mechanism are also supported in SQL. Users can submit queries to get the help of related SQL statements or show Doris' internal running state.
-
-In frontend, a small response buffer is allocated to every MySQL connection. The maximum size of this buffer is limited to 1MB. The buffer is responsible for buffering the query response data. Only if the response is finished or the buffer size reaches the 1MB,the response data will begin to be sent to client. Through this small trick, frontend can re-execution most of queries if errors occurred during query execution.
-
-#### 3.5 Two-Level Partitioning
-
-Like most of the distributed database system, data in Doris is horizontally partitioned. However, a single-level partitioning rule (hash partitioning or range partitioning) may not be a good solution to all scenarios. For example, there have a user-based fact table that stores rows of the form (date, userid, metric). Choosing only hash partitioning by column userid may lead to uneven distribution of data, when one user's data is very large. If choosing range partitioning according to column date, it will also lead to uneven distribution of data due to the likely data explosion in a certain period of time.
-
-Therefore we support the two-level partitioning rule. The first level is range partitioning. User can specify a column (usually the time series column) range of values for the data partition. In one partition, the user can also specify one or more columns and a number of buckets to do the hash partitioning. User can combine with different partitioning rules to better divide the data. Figure 4 gives an example of two-level partitioning.
-
-Three benefits are gained by using the two-level partitioning mechanism. Firstly, old and new data could be separated, and stored on different storage mediums; Secondly, storage engine of backend can reduce the consumption of IO and CPU for unnecessary data merging, because the data in some partitions is no longer be updated; Lastly,every partition's buckets number can be different and adjusted according to the change of data size.
-
-```SQL
--- Create partitions using CREATE TABLE --
-CREATE TABLE example_tbl (
-    `date`      DATE,
-    userid      BIGINT,
-    metric      BIGINT SUM
-) PARTITION BY RANGE (`date`) (
-    PARTITION p201601 VALUES LESS THAN (""2016-02-01""),
-    PARTITION p201602 VALUES LESS THAN (""2016-03-01""),
-    PARTITION p201603 VALUES LESS THAN (""2016-04-01""),
-    PARTITION p201604 VALUES LESS THAN (""2016-05-01"")
-) DISTRIBUTED BY HASH(userid) BUCKETS 32;
-
--- Add partition using ALTER TABLE --
-ALTER TABLE example_tbl ADD PARTITION p201605 VALUES LESS THAN (""2016-06-01"");
+```
+yum -y install docker-io
+service docker start
 ```
 
-## 4. Backend
-
-#### 4.1 Data Storage Model
-
-Doris combines Google Mesa's data model and ORCFile / Parquet storage technology.
-
-Data in Mesa is inherently multi-dimensional fact table. These facts in table typically consist of two types of attributes: dimensional attributes (which we call keys) and measure attributes (which we call values). The table schema also specifies the aggregation function F: V ×V → V which is used to aggregate the values corresponding to the same key. To achieve high update throughput, Mesa loads data in batch. Each batch of data will be converted to a delta file. Mesa uses MVCC approach to manage these delta files, and so to enforce update atomicity. Mesa also supports creating materialized rollups, which contain a column subset of schema to gain better aggregation effect.
-
-Mesa's data model performs well in many interactive data service, but it also has some drawbacks:
-1. Users have difficulty in understanding key and value space, as well as aggregation function, especially when they rarely have such aggregation demand in analysis query scenarios.
-
-2. In order to ensure the aggregation semantic, count operation on a single column must read all columns in key space, resulting in a large number of additional read overheads. There is also unable to push down the predicates on the value column to storage engine, which also leads to additional read overheads.
-
-3.  Essentially, it is still a key-value model. In order to aggregate the values corresponding to the same key, all key columns must store in order. When a table contains hundreds of columns, sorting cost becomes the bottleneck of ETL process.
-
-To solve these problems, we introduce ORCFile / Parquet technology widely used in the open source community, such as MapReduce + ORCFile, SparkSQL + Parquet, mainly used for ad-hoc analysis of large amounts of data with low concurrency. These data does not distinguish between key and value. In addition, compared with the row-oriented database, column-oriented organization is more efficient when an aggregate needs to be computed over many rows but only for a small subset of all columns of data, because reading that smaller subset of data can be faster than reading all data. And columnar storage is also space-friendly due to the high compression ratio of each column. Further, column support block-level storage technology such as min/max index and bloom filter index. Query executor can filter out a lot of blocks that do not meet the predicate, to further improve the query performance. However, due to the underlying storage does not require data order, query time complexity is linear corresponding to the data volume.
-
-Like traditional databases, Doris stores structured data represented as tables. Each table has a well-defined schema consisting of a finite number of columns. We combine Mesa data model and ORCFile/Parquet technology to develop a distributed analytical database. User can create two types of table to meet different needs in interactive query scenarios.
+#### Step 2: Create Docker image
 
-In non-aggregation type of table, columns are not distinguished between dimensions and metrics, but should specify the sort columns in order to sort all rows. Doris will sort the table data according to the sort columns without any aggregation. The following figure gives an example of creating non-aggregation table.
+Given your work space is /my/workspace, and you can download Doris docker file as following:
 
-```SQL
--- Create non-aggregation table --
-CREATE TABLE example_tbl (
-    `date`      DATE,
-    id          BIGINT,
-    country     VARCHAR(32),
-    click       BIGINT,
-    cost        BIGINT
-) DUPLICATE KEY(`date`, id, country)
-DISTRIBUTED BY HASH(id) BUCKETS 32;
 ```
-
-In aggregation data analysis case, we reference Mesa's data model, and distinguish columns between key and value, and specify the value columns with aggregation method, such as SUM, REPLACE, etc. In the following figure, we create an aggregation table like the non-aggregation table, including two SUM aggregation columns (clicks, cost). Different from the non-aggregation table, data in the table needs to be sorted on all key columns for delta compaction and value aggregation.
-
-```SQL
--- Create aggregation table --
-CREATE TABLE example_tbl (
-    `date`      DATE,
-    id          BIGINT,
-    country     VARCHAR(32),
-    click       BIGINT          SUM,
-    cost        BIGINT          SUM
-) DISTRIBUTED BY HASH(id) BUCKETS 32;
+wget https://github.com/apache/incubator-doris/blob/master/docker/Dockerfile -O /my/workspace/Dockerfile
 ```
 
-Rollup is a materialized view that contains a column subset of schema in Doris. A table may contain multiple rollups with columns in different order. According to sort key index and column covering of the rollups, Doris can select the best rollup for different query. Because most rollups only contain a few columns, the size of aggregated data is typically much smaller and query performance can greatly be improved. All the rollups in the same table are updated atomically. Because rollups are materialized, users should make a trade-off between query latency and storage space when using them.
-
-To achieve high update throughput, Doris only applies updates in batches at the smallest frequency of every minute. Each update batch specifies an increased version number and generates a delta data file, commits the version when updates of quorum replicas are complete. You can query all committed data using the committed version, and the uncommitted version would not be used in query. All update versions are strictly be in increasing order. If an update contains more than one table, the versions of these tables are committed atomically. The MVCC mechanism allows Doris to guarantee multiple table atomic updates and query consistency. In addition, Doris uses compaction policies to merge delta files to reduce delta number, also reduce the cost of delta merging during query for higher performance.
-
-Doris' data file is stored by column. The rows are stored in sorted order by the sort columns in delta data files, and are organized into row blocks, each block is compressed by type-specific columnar encodings, such as run-length encoding for integer columns, then stored into separate streams. In order to improve the performance of queries that have a specific key, we also store a sparse sort key index file corresponding to each delta data file. An index entry contains the short key for the row block, which is a fixed size prefix of the first sort columns for the row block, and the block id in the data file. Index files are usually directly loaded into memory, as they are very small. The algorithm for querying a specific key includes two steps. First, use a binary search on the sort key index to find blocks that may contain the specific key, and then perform a binary search on the compressed blocks in the data files to find the desired key. We also store block-level min/max index into separate index streams, and queries can use this to filter undesired blocks. In addition to those basic columnar features, we also offers an optional block-level bloom filter index for queries with IN or EQUAL conditions to further filter undesired blocks. Bloom filter index is stored in a separate stream, and is loaded on demand.
+Now build your image: (It will take more time and use nohup if needed)
 
-#### 4.2 Data Loading
+```
+cd /my/workspace && docker build -t doris-dev:v1.0 .
+```
 
-Doris applies updates in batches. Three types of data loading are supported: Hadoop-batch loading, loading ,mini-batch loading.
+#### Step 3: Compile and install Doris
 
-1. Hadoop-batch loading. When a large amount of data volume needs to be loaded into Doris, the hadoop-batch loading is recommended to achieve high loading throughput. The data batches themselves are produced by an external Hadoop system, typically at a frequency of every few minutes. Unlike traditional data warehouses that use their own computing resource to do the heavy data preparation, Doris could use Hadoop to prepare the data (shuffle, sort and aggregate, etc.). By using this approach, the most time-consuming computations are handed over to Hadoop to complete. This will not only improve computational efficiency, but also reduce the performance pressure of Doris cluster and ensure the stability of the query service. The stability of the online data services is the most important point.
+Clone Doris source:
 
-2. Loading. After deploying the fs-brokers, you can use Doris' query engine to import data. This type of loading is recommended for incremental data loading.
+```
+git clone https://github.com/apache/incubator-doris.git /path/to/incubator-doris/
+```
 
-3. Mini-batch loading. When a small amount of data needs to be loaded into Doris, the mini-batch loading is recommended to achieve low loading latency.  By using http interface, raw data is pushed into a backend. Then the backend does the data preparing computing and completes the final loading. Http tools could connect frontend or backend. If frontend is connected, it will redirect the request randomly to a backend.
+Start a docker named doris-dev-test, and map /path/to/incubator-doris/ to /var/local/incubator-doris/ which in docker.
 
-All the loading work is handled asynchronously. When load request is submitted, a label needs to be provided. By using the load label, users can submit show load request to get the loading status or submit cancel load request to cancel the loading. If the status of loading task is successful or in progress, its load label is not allowed to reuse again. The label of failed task is allowed to be reused.
+```
+docker run -it --name doris-dev-test -v /path/to/incubator-doris/:/var/local/incubator-doris/ doris-dev:v1.0
+```
 
-#### 4.3 Resource Isolation
+Compile Doris source:
 
-1. Multi-tenancy Isolation：Multiple virtual cluster can be created in one pysical Doris cluster. Every backend node can deploy multiple backend processes. Every backend process only belongs to one virtual cluster. Virtual cluster is one tenancy.
+```
+sh build.sh 
+```
 
-2. User Isolation: There are many users in one virtual cluster. You can allocate the resource among different users and ensure that all users' tasks are executed under limited resource quota.
+After successful build, it will install binary files to the path of output/.","[{'comment': 'After successful build\r\n->\r\nAfter successfully building', 'commenter': 'morningman'}, {'comment': 'to the path of output/.\r\n->\r\nin the directory `output/`', 'commenter': 'morningman'}]"
345,README.md,"@@ -1,215 +1,127 @@
-# Introduction to Apache Doris (incubating)
+# Apache Doris (incubating)
 
-Apache Doris is an MPP-based interactive SQL data warehousing for reporting and analysis. Doris mainly integrates the technology of Google Mesa and Apache Impala. Unlike other popular SQL-on-Hadoop systems, Doris is designed to be a simple and single tightly coupled system, not depending on other systems. Doris not only provides high concurrent low latency point query performance, but also provides high throughput queries of ad-hoc analysis. Doris not only provides batch data loading, but also provides near real-time mini-batch data loading. Doris also provides high availability, reliability, fault tolerance, and scalability. The simplicity (of developing, deploying and using) and meeting many data serving requirements in single system are the main features of Doris.
+Doris is an MPP-based interactive SQL data warehousing for reporting and analysis. It open-sourced by Baidu. 
 
-## 1. Background
+## 1. License
 
-In Baidu, the largest Chinese search engine, we run a two-tiered data warehousing system for data processing, reporting and analysis. Similar to lambda architecture, the whole data warehouse comprises data processing and data serving. Data processing does the heavy lifting of big data: cleaning data, merging and transforming it, analyzing it and preparing it for use by end user queries; data serving is designed to serve queries against that data for different use cases. Currently data processing includes batch data processing and stream data processing technology, like Hadoop, Spark and Storm; Doris is a SQL data warehouse for serving online and interactive data reporting and analysis querying.
+[Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0)
 
-Prior to Doris, different tools were deployed to solve diverse requirements in many ways. For example, the advertising platform needs to provide some detailed statistics associated with each served ad for every advertiser. The platform must support continuous updates, both new rows and incremental updates to existing rows within minutes. It must support latency-sensitive users serving live customer reports with very low latency requirements and batch ad-hoc multiple dimensions data analysis requiring very high throughput. In the past,this platform was built on top of sharded MySQL. But with the growth of data, MySQL cannot meet the requirements. Then, based on our existing KV system, we developed our own proprietary distributed statistical database. But, the simple KV storage was not efficient on scan performance. Because the system depends on many other systems, it is very complex to operate and maintain. Using RPC API, more complex querying usually required code programming, but users wants an MPP SQL engine. In addition to advertising system, a large number of internal BI Reporting / Analysis, also used a variety of tools. Some used the combination of SparkSQL / Impala + HDFS / HBASE. Some used MySQL to store the results that were prepared by distributed MapReduce computing. Some also bought commercial databases to use.
+## 2. Technology
+Doris mainly integrates the technology of Google Mesa and Apache Impala, and it based on a column-oriented storage engine and can communicate by MySQL client.
 
-However, when a use case requires the simultaneous availability of capabilities that cannot all be provided by a single tool, users were forced to build hybrid architectures that stitch multiple tools together. Users often choose to ingest and update data in one storage system, but later reorganize this data to optimize for an analytical reporting use-case served from another. Our users had been successfully deploying and maintaining these hybrid architectures, but we believe that they shouldn't need to accept their inherent complexity. A storage system built to provide great performance across a broad range of workloads provides a more elegant solution to the problems that hybrid architectures aim to solve. Doris is the solution. Doris is designed to be a simple and single tightly coupled system, not depending on other systems. Doris provides high concurrent low latency point query performance, but also provides high throughput queries of ad-hoc analysis. Doris provides bulk-batch data loading, but also provides near real-time mini-batch data loading. Doris also provides high availability, reliability, fault tolerance, and scalability.
+## 3. User cases
+Doris not only provides high concurrent low latency point query performance, but also provides high throughput queries of ad-hoc analysis. 
 
-Generally speaking, Doris is the technology combination of Google Mesa and Apache Impala. Mesa is a highly scalable analytic data storage system that stores critical measurement data related to Google's Internet advertising business. Mesa is designed to satisfy complex and challenging set of users' and systems' requirements, including near real-time data ingestion and query ability, as well as high availability, reliability, fault tolerance, and scalability for large data and query volumes. Impala is a modern, open-source MPP SQL engine architected from the ground up for the Hadoop data processing environment. At present, by virtue of its superior performance and rich functionality, Impala has been comparable to many commercial MPP database query engine. Mesa can satisfy the needs of many of our storage requirements, however Mesa itself does not provide a SQL query engine; Impala is a very good MPP SQL query engine, but the lack of a perfect distributed storage engine. So in the end we chose the combination of these two technologies.
+Doris not only provides batch data loading, but also provides near real-time mini-batch data loading. 
 
-Learning from Mesa's data model, we developed a distributed storage engine. Unlike Mesa, this storage engine does not rely on any distributed file system. Then we deeply integrate this storage engine with Impala query engine. Query compiling, query execution coordination and catalog management of storage engine are integrated to be frontend daemon; query execution and data storage are integrated to be backend daemon. With this integration, we implemented a single, full-featured, high performance state the art of MPP database, as well as maintaining the simplicity.
+Doris also provides high availability, reliability, fault tolerance, and scalability. 
 
-## 2. System Overview
+The simplicity (of developing, deploying and using) and meeting many data serving requirements in single system are the main features of Doris.
 
-Doris' implementation consists of two daemons: frontend (FE) and backend (BE). The following figures gives the overview of architecture and usage.
+## 4. Compile and install
 
-![](./docs/resources/palo_architecture.jpg)
+Currently support Docker environment and Linux OS: 
+Docker（Linux/Windows/Mac), Ubuntu 16.04+ and CentOS 7.5+
 
-Frontend daemon consists of query coordinator and catalog manager. Query coordinator is responsible for receiving user's sql queries, compiling queries and managing queries execution. Catalog manager is responsible for managing metadata such as databases, tables, partitions, replicas and etc. Several frontend daemons could be deployed to guarantee fault-tolerance, and load balancing.
+### 4.1 For Docker
 
-Backend daemon stores the data and executes the query fragments. Many backend daemons could also be deployed to provide scalability and fault-tolerance.
+#### Step 1: Install Docker
 
-A typical Doris cluster generally composes of several frontend daemons and dozens to hundreds of backend daemons.
+Take CentOS as an example:
 
-Clients can use MySQL-related tools to connect any frontend daemon to submit SQL query. The frontend receives the query and compiles it into query plans executable by the backends. Then frontend sends the query plan fragments to backend. Backends will build a query execution DAG. Data is fetched and pipelined into the DAG. The final result response is sent to client via frontend. The distribution of query fragment execution takes minimizing data movement and maximizing scan locality as the main goal. Because Doris is designed to provide interactive analysis, so the average execution time of queries is short. Considering this, we adopt query re-execution to meet the fault tolerance of query execution.
-
-A table is splitted into many tablets. Tablets are managed by backends. The backend daemon could be configured to use multiple directories. Any directory's IO failure doesn't influence the normal running of backend daemon. Doris will recover and rebalance the whole cluster automatically when necessary.
-
-## 3. Frontend
-
-In-memory catalog, multiple frontends, MySQL networking protocol, consistency guarantee, and two-level table partitioning are the main features of Doris' frontend design.
-
-#### 3.1 In-Memory Catalog
-
-Traditional data warehouse always uses a RDBMS database to store their catalog metadata. In order to produce query execution plan, frontend needs to look up the catalog metadata. This kind of catalog storage may be enough for low concurrent ad-hoc analysis queries. But for online high concurrent queries, its performance is very bad,resulting in increased response latency. For example, Hive metadata query latency is sometimes up to tens of seconds or even minutes. In order to speedup the metadata access, we adopt the in-memory catalog storage.
-
-![](./docs/resources/log_replication.jpg)
-
-In-memory catalog storage has three functional modules: real-time memory data structures, memory checkpoints on local disk and an operation relay log. When modifying catalog, the mutation operation is written into the log file firstly. Then, the mutation operation is applied into the memory data structures. Periodically, a thread does the checkpoint that dumps memory data structure image into local disk. Checkpoint mechanism enables the fast startup of frontend and reduces the disk storage occupancy. Actually, in-memory catalog also simplifies the implementation of multiple frontends.
-
-#### 3.2 Multiple Frontends
-
-Many data warehouses only support single frontend-like node. There are some systems supporting master and slave deploying. But for online data serving, high availability is an essential feature. Further, the number of queries per seconds may be very large, so high scalability is also needed. In Doris, we provide the feature of multiple frontends using replicated-state-machine technology.
-
-Frontends can be configured to three kinds of roles: leader, follower and observer. Through a voting protocol, follower frontends firstly elect a leader frontend. All the write requests of metadata are forwarded to the leader, then the leader writes the operation into the replicated log file. If the new log entry will be replicated to at least quorum followers successfully, the leader commits the operation into memory, and responses the write request. Followers always replay the replicated logs to apply them into their memory metadata. If the leader crashes, a new leader will be elected from the leftover followers. Leader and follower mainly solve the problem of write availability and partly solve the problem of read scalability.
-
-Usually one leader frontend and several follower frontends can meet most applications' write availability and read scalability requirements. For very high concurrent reading, continuing to increase the number of followers is not a good practice. Leader replicates log stream to followers synchronously, so adding more followers will increases write latency. Like Zookeeper,we have introduced a new type of frontend node called observer that helps addressing this problem and further improving metadata read scalability. Leader replicates log stream to observers asynchronously. Observers don't involve leader election.
-
-The replicated-state-machine is implemented based on BerkeleyDB java version (BDB-JE). BDB-JE has achieved high availability by implementing a Paxos-like consensus algorithm.  We use BDB-JE to implement Doris' log replication and leader election.
-
-#### 3.3    Consistency Guarantee
-
-If a client process connects to the leader, it will see up-to-date metadata, so that strong consistency semantics is guaranteed. If the client connects to followers or observers, it will see metadata lagging a little behind of the leader, but the monotonic consistency is guaranteed. In most Doris' use cases, monotonic consistency is accepted.
-
-If the client always connects to the same frontend, monotonic consistency semantics is obviously guaranteed; however if the client connects to other frontends due to failover, the semantics may be violated. Doris provides a SYNC command to guarantee metadata monotonic consistency semantics during failover. When failover happens, the client can send a SYNC command to the new connected frontend, who will get the latest operation log number from the leader. The SYNC command will not return to client as long as local applied log number is still less than fetched operation log number. This mechanism can guarantee the metadata on the connected frontend is newer than the client have seen during its last connection.
-
-#### 3.4 MySQL Networking Protocol
-
-MySQL compatible networking protocol is implemented in Doris' frontend. Firstly, SQL interface is preferred for engineers; Secondly, compatibility with MySQL protocol makes the integrating with current existing BI software, such as Tableau, easier; Lastly, rich MySQL client libraries and tools reduce our development costs, but also reduces the user's using cost.
-
-Through the SQL interface, administrator can adjust system configuration, add and remove frontend nodes or backend nodes, and create new database for user; user can create tables, load data, and submit SQL query.
-
-Online help document and Linux Proc-like mechanism are also supported in SQL. Users can submit queries to get the help of related SQL statements or show Doris' internal running state.
-
-In frontend, a small response buffer is allocated to every MySQL connection. The maximum size of this buffer is limited to 1MB. The buffer is responsible for buffering the query response data. Only if the response is finished or the buffer size reaches the 1MB,the response data will begin to be sent to client. Through this small trick, frontend can re-execution most of queries if errors occurred during query execution.
-
-#### 3.5 Two-Level Partitioning
-
-Like most of the distributed database system, data in Doris is horizontally partitioned. However, a single-level partitioning rule (hash partitioning or range partitioning) may not be a good solution to all scenarios. For example, there have a user-based fact table that stores rows of the form (date, userid, metric). Choosing only hash partitioning by column userid may lead to uneven distribution of data, when one user's data is very large. If choosing range partitioning according to column date, it will also lead to uneven distribution of data due to the likely data explosion in a certain period of time.
-
-Therefore we support the two-level partitioning rule. The first level is range partitioning. User can specify a column (usually the time series column) range of values for the data partition. In one partition, the user can also specify one or more columns and a number of buckets to do the hash partitioning. User can combine with different partitioning rules to better divide the data. Figure 4 gives an example of two-level partitioning.
-
-Three benefits are gained by using the two-level partitioning mechanism. Firstly, old and new data could be separated, and stored on different storage mediums; Secondly, storage engine of backend can reduce the consumption of IO and CPU for unnecessary data merging, because the data in some partitions is no longer be updated; Lastly,every partition's buckets number can be different and adjusted according to the change of data size.
-
-```SQL
--- Create partitions using CREATE TABLE --
-CREATE TABLE example_tbl (
-    `date`      DATE,
-    userid      BIGINT,
-    metric      BIGINT SUM
-) PARTITION BY RANGE (`date`) (
-    PARTITION p201601 VALUES LESS THAN (""2016-02-01""),
-    PARTITION p201602 VALUES LESS THAN (""2016-03-01""),
-    PARTITION p201603 VALUES LESS THAN (""2016-04-01""),
-    PARTITION p201604 VALUES LESS THAN (""2016-05-01"")
-) DISTRIBUTED BY HASH(userid) BUCKETS 32;
-
--- Add partition using ALTER TABLE --
-ALTER TABLE example_tbl ADD PARTITION p201605 VALUES LESS THAN (""2016-06-01"");
+```
+yum -y install docker-io
+service docker start
 ```
 
-## 4. Backend
-
-#### 4.1 Data Storage Model
-
-Doris combines Google Mesa's data model and ORCFile / Parquet storage technology.
-
-Data in Mesa is inherently multi-dimensional fact table. These facts in table typically consist of two types of attributes: dimensional attributes (which we call keys) and measure attributes (which we call values). The table schema also specifies the aggregation function F: V ×V → V which is used to aggregate the values corresponding to the same key. To achieve high update throughput, Mesa loads data in batch. Each batch of data will be converted to a delta file. Mesa uses MVCC approach to manage these delta files, and so to enforce update atomicity. Mesa also supports creating materialized rollups, which contain a column subset of schema to gain better aggregation effect.
-
-Mesa's data model performs well in many interactive data service, but it also has some drawbacks:
-1. Users have difficulty in understanding key and value space, as well as aggregation function, especially when they rarely have such aggregation demand in analysis query scenarios.
-
-2. In order to ensure the aggregation semantic, count operation on a single column must read all columns in key space, resulting in a large number of additional read overheads. There is also unable to push down the predicates on the value column to storage engine, which also leads to additional read overheads.
-
-3.  Essentially, it is still a key-value model. In order to aggregate the values corresponding to the same key, all key columns must store in order. When a table contains hundreds of columns, sorting cost becomes the bottleneck of ETL process.
-
-To solve these problems, we introduce ORCFile / Parquet technology widely used in the open source community, such as MapReduce + ORCFile, SparkSQL + Parquet, mainly used for ad-hoc analysis of large amounts of data with low concurrency. These data does not distinguish between key and value. In addition, compared with the row-oriented database, column-oriented organization is more efficient when an aggregate needs to be computed over many rows but only for a small subset of all columns of data, because reading that smaller subset of data can be faster than reading all data. And columnar storage is also space-friendly due to the high compression ratio of each column. Further, column support block-level storage technology such as min/max index and bloom filter index. Query executor can filter out a lot of blocks that do not meet the predicate, to further improve the query performance. However, due to the underlying storage does not require data order, query time complexity is linear corresponding to the data volume.
-
-Like traditional databases, Doris stores structured data represented as tables. Each table has a well-defined schema consisting of a finite number of columns. We combine Mesa data model and ORCFile/Parquet technology to develop a distributed analytical database. User can create two types of table to meet different needs in interactive query scenarios.
+#### Step 2: Create Docker image
 
-In non-aggregation type of table, columns are not distinguished between dimensions and metrics, but should specify the sort columns in order to sort all rows. Doris will sort the table data according to the sort columns without any aggregation. The following figure gives an example of creating non-aggregation table.
+Given your work space is /my/workspace, and you can download Doris docker file as following:
 
-```SQL
--- Create non-aggregation table --
-CREATE TABLE example_tbl (
-    `date`      DATE,
-    id          BIGINT,
-    country     VARCHAR(32),
-    click       BIGINT,
-    cost        BIGINT
-) DUPLICATE KEY(`date`, id, country)
-DISTRIBUTED BY HASH(id) BUCKETS 32;
 ```
-
-In aggregation data analysis case, we reference Mesa's data model, and distinguish columns between key and value, and specify the value columns with aggregation method, such as SUM, REPLACE, etc. In the following figure, we create an aggregation table like the non-aggregation table, including two SUM aggregation columns (clicks, cost). Different from the non-aggregation table, data in the table needs to be sorted on all key columns for delta compaction and value aggregation.
-
-```SQL
--- Create aggregation table --
-CREATE TABLE example_tbl (
-    `date`      DATE,
-    id          BIGINT,
-    country     VARCHAR(32),
-    click       BIGINT          SUM,
-    cost        BIGINT          SUM
-) DISTRIBUTED BY HASH(id) BUCKETS 32;
+wget https://github.com/apache/incubator-doris/blob/master/docker/Dockerfile -O /my/workspace/Dockerfile
 ```
 
-Rollup is a materialized view that contains a column subset of schema in Doris. A table may contain multiple rollups with columns in different order. According to sort key index and column covering of the rollups, Doris can select the best rollup for different query. Because most rollups only contain a few columns, the size of aggregated data is typically much smaller and query performance can greatly be improved. All the rollups in the same table are updated atomically. Because rollups are materialized, users should make a trade-off between query latency and storage space when using them.
-
-To achieve high update throughput, Doris only applies updates in batches at the smallest frequency of every minute. Each update batch specifies an increased version number and generates a delta data file, commits the version when updates of quorum replicas are complete. You can query all committed data using the committed version, and the uncommitted version would not be used in query. All update versions are strictly be in increasing order. If an update contains more than one table, the versions of these tables are committed atomically. The MVCC mechanism allows Doris to guarantee multiple table atomic updates and query consistency. In addition, Doris uses compaction policies to merge delta files to reduce delta number, also reduce the cost of delta merging during query for higher performance.
-
-Doris' data file is stored by column. The rows are stored in sorted order by the sort columns in delta data files, and are organized into row blocks, each block is compressed by type-specific columnar encodings, such as run-length encoding for integer columns, then stored into separate streams. In order to improve the performance of queries that have a specific key, we also store a sparse sort key index file corresponding to each delta data file. An index entry contains the short key for the row block, which is a fixed size prefix of the first sort columns for the row block, and the block id in the data file. Index files are usually directly loaded into memory, as they are very small. The algorithm for querying a specific key includes two steps. First, use a binary search on the sort key index to find blocks that may contain the specific key, and then perform a binary search on the compressed blocks in the data files to find the desired key. We also store block-level min/max index into separate index streams, and queries can use this to filter undesired blocks. In addition to those basic columnar features, we also offers an optional block-level bloom filter index for queries with IN or EQUAL conditions to further filter undesired blocks. Bloom filter index is stored in a separate stream, and is loaded on demand.
+Now build your image: (It will take more time and use nohup if needed)
 
-#### 4.2 Data Loading
+```
+cd /my/workspace && docker build -t doris-dev:v1.0 .
+```
 
-Doris applies updates in batches. Three types of data loading are supported: Hadoop-batch loading, loading ,mini-batch loading.
+#### Step 3: Compile and install Doris
 
-1. Hadoop-batch loading. When a large amount of data volume needs to be loaded into Doris, the hadoop-batch loading is recommended to achieve high loading throughput. The data batches themselves are produced by an external Hadoop system, typically at a frequency of every few minutes. Unlike traditional data warehouses that use their own computing resource to do the heavy data preparation, Doris could use Hadoop to prepare the data (shuffle, sort and aggregate, etc.). By using this approach, the most time-consuming computations are handed over to Hadoop to complete. This will not only improve computational efficiency, but also reduce the performance pressure of Doris cluster and ensure the stability of the query service. The stability of the online data services is the most important point.
+Clone Doris source:
 
-2. Loading. After deploying the fs-brokers, you can use Doris' query engine to import data. This type of loading is recommended for incremental data loading.
+```
+git clone https://github.com/apache/incubator-doris.git /path/to/incubator-doris/
+```
 
-3. Mini-batch loading. When a small amount of data needs to be loaded into Doris, the mini-batch loading is recommended to achieve low loading latency.  By using http interface, raw data is pushed into a backend. Then the backend does the data preparing computing and completes the final loading. Http tools could connect frontend or backend. If frontend is connected, it will redirect the request randomly to a backend.
+Start a docker named doris-dev-test, and map /path/to/incubator-doris/ to /var/local/incubator-doris/ which in docker.
 
-All the loading work is handled asynchronously. When load request is submitted, a label needs to be provided. By using the load label, users can submit show load request to get the loading status or submit cancel load request to cancel the loading. If the status of loading task is successful or in progress, its load label is not allowed to reuse again. The label of failed task is allowed to be reused.
+```
+docker run -it --name doris-dev-test -v /path/to/incubator-doris/:/var/local/incubator-doris/ doris-dev:v1.0
+```
 
-#### 4.3 Resource Isolation
+Compile Doris source:
 
-1. Multi-tenancy Isolation：Multiple virtual cluster can be created in one pysical Doris cluster. Every backend node can deploy multiple backend processes. Every backend process only belongs to one virtual cluster. Virtual cluster is one tenancy.
+```
+sh build.sh 
+```
 
-2. User Isolation: There are many users in one virtual cluster. You can allocate the resource among different users and ensure that all users' tasks are executed under limited resource quota.
+After successful build, it will install binary files to the path of output/.
 
-3. Priority Isolation: There are three priorities isolation group for one user. User could control resource allocated to different tasks submitted by themselves, for example user's query task and loading tasks require different resource quota.
+### 4.2 For Linux
 
-#### 4.4 Multi-Medium Storage
+#### Prerequisites
 
-Most machines in modern datacenter are equipped with both SSDs and HDDs. SSD has good random read capability that is the ideal medium for query that needs a large number of random read operations. However, SSD's capacity is small and is very expensive, we could not deploy it at a large scale. HDD is cheap and has huge capacity that is suitable to store large scale data but with high read latency. In OLAP scenario, we find user usually submit a lot of queries to query the latest data (hot data) and expect low latency. User occasionally executes query on historical data (cold data). This kind of query usually needs to scan large scale of data and is high latency. Multi-Medium Storage allows users to manage the storage medium of the data to meet different query scenarios and reduce the latency. For example, user could put latest data on SSD and historical data which is not used frequently on HDD, user will get low latency when querying latest data while get high latency when query historical data which is normal because it needs scan large scale data.
+GCC 5.3.1+，Oracle JDK 1.8+，Python 2.7+, Apache Maven 3.5.4+
 
-In the following figure, user alters partition 'p201601' storage_medium to SSD and storage_cooldown_time to '2016-07-01 00:00:00'. The setting means data in this partition will be put on SSD and it will start to migrate to HDD after the time of storage_cooldown_time.
+* For Ubuntu: 
 
-```SQL
-ALTER TABLE example_tbl MODIFY PARTITION p201601
-SET (""storage_medium"" = ""SSD"", ""storage_cooldown_time"" = ""2016-07-01 00:00:00"");
+```
+sudo apt-get install g++ ant cmake zip byacc flex automake libtool binutils-dev libiberty-dev bison python2.7 libncurses5-dev
+sudo updatedb
 ```
 
-#### 4.5 Vectorized Query Execution
-
-Runtime code generation using LLVM is one of the techniques employed extensively by Impala to improve query execution times. Performance could gains of 5X or more are typical for representative workloads.
-
-But, runtime code generation is not suitable for low latency query, because the generation overhead costs about 100ms. Runtime code generation is more suitable for large-scale ad-hoc query. To accelerate the small queries (of course, big queries will also obtain benefits), we introduced vectorized query execution into Doris.
+* For CentOS:
 
-Vectorized query execution is a feature that greatly reduces the CPU usage for typical query operations like scans, filters, aggregates, and joins. A standard query execution system processes one row at a time. This involves long code paths and significant metadata interpretation in the inner loop of execution. Vectorized query execution streamlines operations by processing a block of many rows at a time. Within the block, each column is stored as a vector (an array of a primitive data type). Simple operations like arithmetic and comparisons are done by quickly iterating through the vectors in a tight loop, with no or very few function calls or conditional branches inside the loop. These loops compile in a streamlined way that uses relatively few instructions and finishes each instruction in fewer clock cycles, on average, by effectively using the processor pipeline and cache memory.
+```
+sudo yum install gcc-c++ libstdc++-static ant cmake byacc flex automake libtool binutils-devel bison ncurses-devel
+sudo updatedb
+```
 
-The result of benchmark shows 2x~4x speedup in our typical queries.
+If your GCC version less than 5.3.1, you can run:","[{'comment': 'GCC version less than\r\n->\r\nGCC version is less than', 'commenter': 'morningman'}]"
345,README.md,"@@ -1,215 +1,127 @@
-# Introduction to Apache Doris (incubating)
+# Apache Doris (incubating)
 
-Apache Doris is an MPP-based interactive SQL data warehousing for reporting and analysis. Doris mainly integrates the technology of Google Mesa and Apache Impala. Unlike other popular SQL-on-Hadoop systems, Doris is designed to be a simple and single tightly coupled system, not depending on other systems. Doris not only provides high concurrent low latency point query performance, but also provides high throughput queries of ad-hoc analysis. Doris not only provides batch data loading, but also provides near real-time mini-batch data loading. Doris also provides high availability, reliability, fault tolerance, and scalability. The simplicity (of developing, deploying and using) and meeting many data serving requirements in single system are the main features of Doris.
+Doris is an MPP-based interactive SQL data warehousing for reporting and analysis. It open-sourced by Baidu. 
 
-## 1. Background
+## 1. License
 
-In Baidu, the largest Chinese search engine, we run a two-tiered data warehousing system for data processing, reporting and analysis. Similar to lambda architecture, the whole data warehouse comprises data processing and data serving. Data processing does the heavy lifting of big data: cleaning data, merging and transforming it, analyzing it and preparing it for use by end user queries; data serving is designed to serve queries against that data for different use cases. Currently data processing includes batch data processing and stream data processing technology, like Hadoop, Spark and Storm; Doris is a SQL data warehouse for serving online and interactive data reporting and analysis querying.
+[Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0)
 
-Prior to Doris, different tools were deployed to solve diverse requirements in many ways. For example, the advertising platform needs to provide some detailed statistics associated with each served ad for every advertiser. The platform must support continuous updates, both new rows and incremental updates to existing rows within minutes. It must support latency-sensitive users serving live customer reports with very low latency requirements and batch ad-hoc multiple dimensions data analysis requiring very high throughput. In the past,this platform was built on top of sharded MySQL. But with the growth of data, MySQL cannot meet the requirements. Then, based on our existing KV system, we developed our own proprietary distributed statistical database. But, the simple KV storage was not efficient on scan performance. Because the system depends on many other systems, it is very complex to operate and maintain. Using RPC API, more complex querying usually required code programming, but users wants an MPP SQL engine. In addition to advertising system, a large number of internal BI Reporting / Analysis, also used a variety of tools. Some used the combination of SparkSQL / Impala + HDFS / HBASE. Some used MySQL to store the results that were prepared by distributed MapReduce computing. Some also bought commercial databases to use.
+## 2. Technology
+Doris mainly integrates the technology of Google Mesa and Apache Impala, and it based on a column-oriented storage engine and can communicate by MySQL client.
 
-However, when a use case requires the simultaneous availability of capabilities that cannot all be provided by a single tool, users were forced to build hybrid architectures that stitch multiple tools together. Users often choose to ingest and update data in one storage system, but later reorganize this data to optimize for an analytical reporting use-case served from another. Our users had been successfully deploying and maintaining these hybrid architectures, but we believe that they shouldn't need to accept their inherent complexity. A storage system built to provide great performance across a broad range of workloads provides a more elegant solution to the problems that hybrid architectures aim to solve. Doris is the solution. Doris is designed to be a simple and single tightly coupled system, not depending on other systems. Doris provides high concurrent low latency point query performance, but also provides high throughput queries of ad-hoc analysis. Doris provides bulk-batch data loading, but also provides near real-time mini-batch data loading. Doris also provides high availability, reliability, fault tolerance, and scalability.
+## 3. User cases
+Doris not only provides high concurrent low latency point query performance, but also provides high throughput queries of ad-hoc analysis. 
 
-Generally speaking, Doris is the technology combination of Google Mesa and Apache Impala. Mesa is a highly scalable analytic data storage system that stores critical measurement data related to Google's Internet advertising business. Mesa is designed to satisfy complex and challenging set of users' and systems' requirements, including near real-time data ingestion and query ability, as well as high availability, reliability, fault tolerance, and scalability for large data and query volumes. Impala is a modern, open-source MPP SQL engine architected from the ground up for the Hadoop data processing environment. At present, by virtue of its superior performance and rich functionality, Impala has been comparable to many commercial MPP database query engine. Mesa can satisfy the needs of many of our storage requirements, however Mesa itself does not provide a SQL query engine; Impala is a very good MPP SQL query engine, but the lack of a perfect distributed storage engine. So in the end we chose the combination of these two technologies.
+Doris not only provides batch data loading, but also provides near real-time mini-batch data loading. 
 
-Learning from Mesa's data model, we developed a distributed storage engine. Unlike Mesa, this storage engine does not rely on any distributed file system. Then we deeply integrate this storage engine with Impala query engine. Query compiling, query execution coordination and catalog management of storage engine are integrated to be frontend daemon; query execution and data storage are integrated to be backend daemon. With this integration, we implemented a single, full-featured, high performance state the art of MPP database, as well as maintaining the simplicity.
+Doris also provides high availability, reliability, fault tolerance, and scalability. 
 
-## 2. System Overview
+The simplicity (of developing, deploying and using) and meeting many data serving requirements in single system are the main features of Doris.
 
-Doris' implementation consists of two daemons: frontend (FE) and backend (BE). The following figures gives the overview of architecture and usage.
+## 4. Compile and install
 
-![](./docs/resources/palo_architecture.jpg)
+Currently support Docker environment and Linux OS: 
+Docker（Linux/Windows/Mac), Ubuntu 16.04+ and CentOS 7.5+
 
-Frontend daemon consists of query coordinator and catalog manager. Query coordinator is responsible for receiving user's sql queries, compiling queries and managing queries execution. Catalog manager is responsible for managing metadata such as databases, tables, partitions, replicas and etc. Several frontend daemons could be deployed to guarantee fault-tolerance, and load balancing.
+### 4.1 For Docker
 
-Backend daemon stores the data and executes the query fragments. Many backend daemons could also be deployed to provide scalability and fault-tolerance.
+#### Step 1: Install Docker
 
-A typical Doris cluster generally composes of several frontend daemons and dozens to hundreds of backend daemons.
+Take CentOS as an example:
 
-Clients can use MySQL-related tools to connect any frontend daemon to submit SQL query. The frontend receives the query and compiles it into query plans executable by the backends. Then frontend sends the query plan fragments to backend. Backends will build a query execution DAG. Data is fetched and pipelined into the DAG. The final result response is sent to client via frontend. The distribution of query fragment execution takes minimizing data movement and maximizing scan locality as the main goal. Because Doris is designed to provide interactive analysis, so the average execution time of queries is short. Considering this, we adopt query re-execution to meet the fault tolerance of query execution.
-
-A table is splitted into many tablets. Tablets are managed by backends. The backend daemon could be configured to use multiple directories. Any directory's IO failure doesn't influence the normal running of backend daemon. Doris will recover and rebalance the whole cluster automatically when necessary.
-
-## 3. Frontend
-
-In-memory catalog, multiple frontends, MySQL networking protocol, consistency guarantee, and two-level table partitioning are the main features of Doris' frontend design.
-
-#### 3.1 In-Memory Catalog
-
-Traditional data warehouse always uses a RDBMS database to store their catalog metadata. In order to produce query execution plan, frontend needs to look up the catalog metadata. This kind of catalog storage may be enough for low concurrent ad-hoc analysis queries. But for online high concurrent queries, its performance is very bad,resulting in increased response latency. For example, Hive metadata query latency is sometimes up to tens of seconds or even minutes. In order to speedup the metadata access, we adopt the in-memory catalog storage.
-
-![](./docs/resources/log_replication.jpg)
-
-In-memory catalog storage has three functional modules: real-time memory data structures, memory checkpoints on local disk and an operation relay log. When modifying catalog, the mutation operation is written into the log file firstly. Then, the mutation operation is applied into the memory data structures. Periodically, a thread does the checkpoint that dumps memory data structure image into local disk. Checkpoint mechanism enables the fast startup of frontend and reduces the disk storage occupancy. Actually, in-memory catalog also simplifies the implementation of multiple frontends.
-
-#### 3.2 Multiple Frontends
-
-Many data warehouses only support single frontend-like node. There are some systems supporting master and slave deploying. But for online data serving, high availability is an essential feature. Further, the number of queries per seconds may be very large, so high scalability is also needed. In Doris, we provide the feature of multiple frontends using replicated-state-machine technology.
-
-Frontends can be configured to three kinds of roles: leader, follower and observer. Through a voting protocol, follower frontends firstly elect a leader frontend. All the write requests of metadata are forwarded to the leader, then the leader writes the operation into the replicated log file. If the new log entry will be replicated to at least quorum followers successfully, the leader commits the operation into memory, and responses the write request. Followers always replay the replicated logs to apply them into their memory metadata. If the leader crashes, a new leader will be elected from the leftover followers. Leader and follower mainly solve the problem of write availability and partly solve the problem of read scalability.
-
-Usually one leader frontend and several follower frontends can meet most applications' write availability and read scalability requirements. For very high concurrent reading, continuing to increase the number of followers is not a good practice. Leader replicates log stream to followers synchronously, so adding more followers will increases write latency. Like Zookeeper,we have introduced a new type of frontend node called observer that helps addressing this problem and further improving metadata read scalability. Leader replicates log stream to observers asynchronously. Observers don't involve leader election.
-
-The replicated-state-machine is implemented based on BerkeleyDB java version (BDB-JE). BDB-JE has achieved high availability by implementing a Paxos-like consensus algorithm.  We use BDB-JE to implement Doris' log replication and leader election.
-
-#### 3.3    Consistency Guarantee
-
-If a client process connects to the leader, it will see up-to-date metadata, so that strong consistency semantics is guaranteed. If the client connects to followers or observers, it will see metadata lagging a little behind of the leader, but the monotonic consistency is guaranteed. In most Doris' use cases, monotonic consistency is accepted.
-
-If the client always connects to the same frontend, monotonic consistency semantics is obviously guaranteed; however if the client connects to other frontends due to failover, the semantics may be violated. Doris provides a SYNC command to guarantee metadata monotonic consistency semantics during failover. When failover happens, the client can send a SYNC command to the new connected frontend, who will get the latest operation log number from the leader. The SYNC command will not return to client as long as local applied log number is still less than fetched operation log number. This mechanism can guarantee the metadata on the connected frontend is newer than the client have seen during its last connection.
-
-#### 3.4 MySQL Networking Protocol
-
-MySQL compatible networking protocol is implemented in Doris' frontend. Firstly, SQL interface is preferred for engineers; Secondly, compatibility with MySQL protocol makes the integrating with current existing BI software, such as Tableau, easier; Lastly, rich MySQL client libraries and tools reduce our development costs, but also reduces the user's using cost.
-
-Through the SQL interface, administrator can adjust system configuration, add and remove frontend nodes or backend nodes, and create new database for user; user can create tables, load data, and submit SQL query.
-
-Online help document and Linux Proc-like mechanism are also supported in SQL. Users can submit queries to get the help of related SQL statements or show Doris' internal running state.
-
-In frontend, a small response buffer is allocated to every MySQL connection. The maximum size of this buffer is limited to 1MB. The buffer is responsible for buffering the query response data. Only if the response is finished or the buffer size reaches the 1MB,the response data will begin to be sent to client. Through this small trick, frontend can re-execution most of queries if errors occurred during query execution.
-
-#### 3.5 Two-Level Partitioning
-
-Like most of the distributed database system, data in Doris is horizontally partitioned. However, a single-level partitioning rule (hash partitioning or range partitioning) may not be a good solution to all scenarios. For example, there have a user-based fact table that stores rows of the form (date, userid, metric). Choosing only hash partitioning by column userid may lead to uneven distribution of data, when one user's data is very large. If choosing range partitioning according to column date, it will also lead to uneven distribution of data due to the likely data explosion in a certain period of time.
-
-Therefore we support the two-level partitioning rule. The first level is range partitioning. User can specify a column (usually the time series column) range of values for the data partition. In one partition, the user can also specify one or more columns and a number of buckets to do the hash partitioning. User can combine with different partitioning rules to better divide the data. Figure 4 gives an example of two-level partitioning.
-
-Three benefits are gained by using the two-level partitioning mechanism. Firstly, old and new data could be separated, and stored on different storage mediums; Secondly, storage engine of backend can reduce the consumption of IO and CPU for unnecessary data merging, because the data in some partitions is no longer be updated; Lastly,every partition's buckets number can be different and adjusted according to the change of data size.
-
-```SQL
--- Create partitions using CREATE TABLE --
-CREATE TABLE example_tbl (
-    `date`      DATE,
-    userid      BIGINT,
-    metric      BIGINT SUM
-) PARTITION BY RANGE (`date`) (
-    PARTITION p201601 VALUES LESS THAN (""2016-02-01""),
-    PARTITION p201602 VALUES LESS THAN (""2016-03-01""),
-    PARTITION p201603 VALUES LESS THAN (""2016-04-01""),
-    PARTITION p201604 VALUES LESS THAN (""2016-05-01"")
-) DISTRIBUTED BY HASH(userid) BUCKETS 32;
-
--- Add partition using ALTER TABLE --
-ALTER TABLE example_tbl ADD PARTITION p201605 VALUES LESS THAN (""2016-06-01"");
+```
+yum -y install docker-io
+service docker start
 ```
 
-## 4. Backend
-
-#### 4.1 Data Storage Model
-
-Doris combines Google Mesa's data model and ORCFile / Parquet storage technology.
-
-Data in Mesa is inherently multi-dimensional fact table. These facts in table typically consist of two types of attributes: dimensional attributes (which we call keys) and measure attributes (which we call values). The table schema also specifies the aggregation function F: V ×V → V which is used to aggregate the values corresponding to the same key. To achieve high update throughput, Mesa loads data in batch. Each batch of data will be converted to a delta file. Mesa uses MVCC approach to manage these delta files, and so to enforce update atomicity. Mesa also supports creating materialized rollups, which contain a column subset of schema to gain better aggregation effect.
-
-Mesa's data model performs well in many interactive data service, but it also has some drawbacks:
-1. Users have difficulty in understanding key and value space, as well as aggregation function, especially when they rarely have such aggregation demand in analysis query scenarios.
-
-2. In order to ensure the aggregation semantic, count operation on a single column must read all columns in key space, resulting in a large number of additional read overheads. There is also unable to push down the predicates on the value column to storage engine, which also leads to additional read overheads.
-
-3.  Essentially, it is still a key-value model. In order to aggregate the values corresponding to the same key, all key columns must store in order. When a table contains hundreds of columns, sorting cost becomes the bottleneck of ETL process.
-
-To solve these problems, we introduce ORCFile / Parquet technology widely used in the open source community, such as MapReduce + ORCFile, SparkSQL + Parquet, mainly used for ad-hoc analysis of large amounts of data with low concurrency. These data does not distinguish between key and value. In addition, compared with the row-oriented database, column-oriented organization is more efficient when an aggregate needs to be computed over many rows but only for a small subset of all columns of data, because reading that smaller subset of data can be faster than reading all data. And columnar storage is also space-friendly due to the high compression ratio of each column. Further, column support block-level storage technology such as min/max index and bloom filter index. Query executor can filter out a lot of blocks that do not meet the predicate, to further improve the query performance. However, due to the underlying storage does not require data order, query time complexity is linear corresponding to the data volume.
-
-Like traditional databases, Doris stores structured data represented as tables. Each table has a well-defined schema consisting of a finite number of columns. We combine Mesa data model and ORCFile/Parquet technology to develop a distributed analytical database. User can create two types of table to meet different needs in interactive query scenarios.
+#### Step 2: Create Docker image
 
-In non-aggregation type of table, columns are not distinguished between dimensions and metrics, but should specify the sort columns in order to sort all rows. Doris will sort the table data according to the sort columns without any aggregation. The following figure gives an example of creating non-aggregation table.
+Given your work space is /my/workspace, and you can download Doris docker file as following:
 
-```SQL
--- Create non-aggregation table --
-CREATE TABLE example_tbl (
-    `date`      DATE,
-    id          BIGINT,
-    country     VARCHAR(32),
-    click       BIGINT,
-    cost        BIGINT
-) DUPLICATE KEY(`date`, id, country)
-DISTRIBUTED BY HASH(id) BUCKETS 32;
 ```
-
-In aggregation data analysis case, we reference Mesa's data model, and distinguish columns between key and value, and specify the value columns with aggregation method, such as SUM, REPLACE, etc. In the following figure, we create an aggregation table like the non-aggregation table, including two SUM aggregation columns (clicks, cost). Different from the non-aggregation table, data in the table needs to be sorted on all key columns for delta compaction and value aggregation.
-
-```SQL
--- Create aggregation table --
-CREATE TABLE example_tbl (
-    `date`      DATE,
-    id          BIGINT,
-    country     VARCHAR(32),
-    click       BIGINT          SUM,
-    cost        BIGINT          SUM
-) DISTRIBUTED BY HASH(id) BUCKETS 32;
+wget https://github.com/apache/incubator-doris/blob/master/docker/Dockerfile -O /my/workspace/Dockerfile
 ```
 
-Rollup is a materialized view that contains a column subset of schema in Doris. A table may contain multiple rollups with columns in different order. According to sort key index and column covering of the rollups, Doris can select the best rollup for different query. Because most rollups only contain a few columns, the size of aggregated data is typically much smaller and query performance can greatly be improved. All the rollups in the same table are updated atomically. Because rollups are materialized, users should make a trade-off between query latency and storage space when using them.
-
-To achieve high update throughput, Doris only applies updates in batches at the smallest frequency of every minute. Each update batch specifies an increased version number and generates a delta data file, commits the version when updates of quorum replicas are complete. You can query all committed data using the committed version, and the uncommitted version would not be used in query. All update versions are strictly be in increasing order. If an update contains more than one table, the versions of these tables are committed atomically. The MVCC mechanism allows Doris to guarantee multiple table atomic updates and query consistency. In addition, Doris uses compaction policies to merge delta files to reduce delta number, also reduce the cost of delta merging during query for higher performance.
-
-Doris' data file is stored by column. The rows are stored in sorted order by the sort columns in delta data files, and are organized into row blocks, each block is compressed by type-specific columnar encodings, such as run-length encoding for integer columns, then stored into separate streams. In order to improve the performance of queries that have a specific key, we also store a sparse sort key index file corresponding to each delta data file. An index entry contains the short key for the row block, which is a fixed size prefix of the first sort columns for the row block, and the block id in the data file. Index files are usually directly loaded into memory, as they are very small. The algorithm for querying a specific key includes two steps. First, use a binary search on the sort key index to find blocks that may contain the specific key, and then perform a binary search on the compressed blocks in the data files to find the desired key. We also store block-level min/max index into separate index streams, and queries can use this to filter undesired blocks. In addition to those basic columnar features, we also offers an optional block-level bloom filter index for queries with IN or EQUAL conditions to further filter undesired blocks. Bloom filter index is stored in a separate stream, and is loaded on demand.
+Now build your image: (It will take more time and use nohup if needed)
 
-#### 4.2 Data Loading
+```
+cd /my/workspace && docker build -t doris-dev:v1.0 .
+```
 
-Doris applies updates in batches. Three types of data loading are supported: Hadoop-batch loading, loading ,mini-batch loading.
+#### Step 3: Compile and install Doris
 
-1. Hadoop-batch loading. When a large amount of data volume needs to be loaded into Doris, the hadoop-batch loading is recommended to achieve high loading throughput. The data batches themselves are produced by an external Hadoop system, typically at a frequency of every few minutes. Unlike traditional data warehouses that use their own computing resource to do the heavy data preparation, Doris could use Hadoop to prepare the data (shuffle, sort and aggregate, etc.). By using this approach, the most time-consuming computations are handed over to Hadoop to complete. This will not only improve computational efficiency, but also reduce the performance pressure of Doris cluster and ensure the stability of the query service. The stability of the online data services is the most important point.
+Clone Doris source:
 
-2. Loading. After deploying the fs-brokers, you can use Doris' query engine to import data. This type of loading is recommended for incremental data loading.
+```
+git clone https://github.com/apache/incubator-doris.git /path/to/incubator-doris/
+```
 
-3. Mini-batch loading. When a small amount of data needs to be loaded into Doris, the mini-batch loading is recommended to achieve low loading latency.  By using http interface, raw data is pushed into a backend. Then the backend does the data preparing computing and completes the final loading. Http tools could connect frontend or backend. If frontend is connected, it will redirect the request randomly to a backend.
+Start a docker named doris-dev-test, and map /path/to/incubator-doris/ to /var/local/incubator-doris/ which in docker.
 
-All the loading work is handled asynchronously. When load request is submitted, a label needs to be provided. By using the load label, users can submit show load request to get the loading status or submit cancel load request to cancel the loading. If the status of loading task is successful or in progress, its load label is not allowed to reuse again. The label of failed task is allowed to be reused.
+```
+docker run -it --name doris-dev-test -v /path/to/incubator-doris/:/var/local/incubator-doris/ doris-dev:v1.0
+```
 
-#### 4.3 Resource Isolation
+Compile Doris source:
 
-1. Multi-tenancy Isolation：Multiple virtual cluster can be created in one pysical Doris cluster. Every backend node can deploy multiple backend processes. Every backend process only belongs to one virtual cluster. Virtual cluster is one tenancy.
+```
+sh build.sh 
+```
 
-2. User Isolation: There are many users in one virtual cluster. You can allocate the resource among different users and ensure that all users' tasks are executed under limited resource quota.
+After successful build, it will install binary files to the path of output/.
 
-3. Priority Isolation: There are three priorities isolation group for one user. User could control resource allocated to different tasks submitted by themselves, for example user's query task and loading tasks require different resource quota.
+### 4.2 For Linux
 
-#### 4.4 Multi-Medium Storage
+#### Prerequisites
 
-Most machines in modern datacenter are equipped with both SSDs and HDDs. SSD has good random read capability that is the ideal medium for query that needs a large number of random read operations. However, SSD's capacity is small and is very expensive, we could not deploy it at a large scale. HDD is cheap and has huge capacity that is suitable to store large scale data but with high read latency. In OLAP scenario, we find user usually submit a lot of queries to query the latest data (hot data) and expect low latency. User occasionally executes query on historical data (cold data). This kind of query usually needs to scan large scale of data and is high latency. Multi-Medium Storage allows users to manage the storage medium of the data to meet different query scenarios and reduce the latency. For example, user could put latest data on SSD and historical data which is not used frequently on HDD, user will get low latency when querying latest data while get high latency when query historical data which is normal because it needs scan large scale data.
+GCC 5.3.1+，Oracle JDK 1.8+，Python 2.7+, Apache Maven 3.5.4+
 
-In the following figure, user alters partition 'p201601' storage_medium to SSD and storage_cooldown_time to '2016-07-01 00:00:00'. The setting means data in this partition will be put on SSD and it will start to migrate to HDD after the time of storage_cooldown_time.
+* For Ubuntu: 
 
-```SQL
-ALTER TABLE example_tbl MODIFY PARTITION p201601
-SET (""storage_medium"" = ""SSD"", ""storage_cooldown_time"" = ""2016-07-01 00:00:00"");
+```
+sudo apt-get install g++ ant cmake zip byacc flex automake libtool binutils-dev libiberty-dev bison python2.7 libncurses5-dev
+sudo updatedb
 ```
 
-#### 4.5 Vectorized Query Execution
-
-Runtime code generation using LLVM is one of the techniques employed extensively by Impala to improve query execution times. Performance could gains of 5X or more are typical for representative workloads.
-
-But, runtime code generation is not suitable for low latency query, because the generation overhead costs about 100ms. Runtime code generation is more suitable for large-scale ad-hoc query. To accelerate the small queries (of course, big queries will also obtain benefits), we introduced vectorized query execution into Doris.
+* For CentOS:
 
-Vectorized query execution is a feature that greatly reduces the CPU usage for typical query operations like scans, filters, aggregates, and joins. A standard query execution system processes one row at a time. This involves long code paths and significant metadata interpretation in the inner loop of execution. Vectorized query execution streamlines operations by processing a block of many rows at a time. Within the block, each column is stored as a vector (an array of a primitive data type). Simple operations like arithmetic and comparisons are done by quickly iterating through the vectors in a tight loop, with no or very few function calls or conditional branches inside the loop. These loops compile in a streamlined way that uses relatively few instructions and finishes each instruction in fewer clock cycles, on average, by effectively using the processor pipeline and cache memory.
+```
+sudo yum install gcc-c++ libstdc++-static ant cmake byacc flex automake libtool binutils-devel bison ncurses-devel
+sudo updatedb
+```
 
-The result of benchmark shows 2x~4x speedup in our typical queries.
+If your GCC version less than 5.3.1, you can run:
 
-## 5.   Backup and Recovery
+```
+sudo yum install devtoolset-4-toolchain -y
+```
 
-Data backup function is provided to enhance data security. The minimum granularity of backup and recovery is partition.  Users can develop plugins to backup data to any specified remote storage. The backup data can always be recovered to Doris at all time, to achieve the data rollback purpose.
+and then, set the path of gcc (e.g /opt/rh/devtoolset-4/root/usr/bin) to the environment variabl PATH.
 
-Currently we only support full data backup data rather than incremental backups for the following reasons:
 
-1. Remote storage system is beyond the control of the Doris system. We cannot guarantee whether the data has been changed between two backup operations. And data verification operations always come at a high price.
+#### Compile and install
 
-2. We support data backup on partition granularity. And majority of applications are time series applications. By dividing data using time column, it has been able to meet the needs of the vast majority of incremental backup in chronological order.
+Run following script, it will comiple thirdparty libraries and build whole Doris.","[{'comment': 'comiple\r\n->\r\ncompile', 'commenter': 'morningman'}]"
345,README.md,"@@ -1,215 +1,127 @@
-# Introduction to Apache Doris (incubating)
+# Apache Doris (incubating)
 
-Apache Doris is an MPP-based interactive SQL data warehousing for reporting and analysis. Doris mainly integrates the technology of Google Mesa and Apache Impala. Unlike other popular SQL-on-Hadoop systems, Doris is designed to be a simple and single tightly coupled system, not depending on other systems. Doris not only provides high concurrent low latency point query performance, but also provides high throughput queries of ad-hoc analysis. Doris not only provides batch data loading, but also provides near real-time mini-batch data loading. Doris also provides high availability, reliability, fault tolerance, and scalability. The simplicity (of developing, deploying and using) and meeting many data serving requirements in single system are the main features of Doris.
+Doris is an MPP-based interactive SQL data warehousing for reporting and analysis. It open-sourced by Baidu. 
 
-## 1. Background
+## 1. License
 
-In Baidu, the largest Chinese search engine, we run a two-tiered data warehousing system for data processing, reporting and analysis. Similar to lambda architecture, the whole data warehouse comprises data processing and data serving. Data processing does the heavy lifting of big data: cleaning data, merging and transforming it, analyzing it and preparing it for use by end user queries; data serving is designed to serve queries against that data for different use cases. Currently data processing includes batch data processing and stream data processing technology, like Hadoop, Spark and Storm; Doris is a SQL data warehouse for serving online and interactive data reporting and analysis querying.
+[Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0)
 
-Prior to Doris, different tools were deployed to solve diverse requirements in many ways. For example, the advertising platform needs to provide some detailed statistics associated with each served ad for every advertiser. The platform must support continuous updates, both new rows and incremental updates to existing rows within minutes. It must support latency-sensitive users serving live customer reports with very low latency requirements and batch ad-hoc multiple dimensions data analysis requiring very high throughput. In the past,this platform was built on top of sharded MySQL. But with the growth of data, MySQL cannot meet the requirements. Then, based on our existing KV system, we developed our own proprietary distributed statistical database. But, the simple KV storage was not efficient on scan performance. Because the system depends on many other systems, it is very complex to operate and maintain. Using RPC API, more complex querying usually required code programming, but users wants an MPP SQL engine. In addition to advertising system, a large number of internal BI Reporting / Analysis, also used a variety of tools. Some used the combination of SparkSQL / Impala + HDFS / HBASE. Some used MySQL to store the results that were prepared by distributed MapReduce computing. Some also bought commercial databases to use.
+## 2. Technology
+Doris mainly integrates the technology of Google Mesa and Apache Impala, and it based on a column-oriented storage engine and can communicate by MySQL client.
 
-However, when a use case requires the simultaneous availability of capabilities that cannot all be provided by a single tool, users were forced to build hybrid architectures that stitch multiple tools together. Users often choose to ingest and update data in one storage system, but later reorganize this data to optimize for an analytical reporting use-case served from another. Our users had been successfully deploying and maintaining these hybrid architectures, but we believe that they shouldn't need to accept their inherent complexity. A storage system built to provide great performance across a broad range of workloads provides a more elegant solution to the problems that hybrid architectures aim to solve. Doris is the solution. Doris is designed to be a simple and single tightly coupled system, not depending on other systems. Doris provides high concurrent low latency point query performance, but also provides high throughput queries of ad-hoc analysis. Doris provides bulk-batch data loading, but also provides near real-time mini-batch data loading. Doris also provides high availability, reliability, fault tolerance, and scalability.
+## 3. User cases
+Doris not only provides high concurrent low latency point query performance, but also provides high throughput queries of ad-hoc analysis. 
 
-Generally speaking, Doris is the technology combination of Google Mesa and Apache Impala. Mesa is a highly scalable analytic data storage system that stores critical measurement data related to Google's Internet advertising business. Mesa is designed to satisfy complex and challenging set of users' and systems' requirements, including near real-time data ingestion and query ability, as well as high availability, reliability, fault tolerance, and scalability for large data and query volumes. Impala is a modern, open-source MPP SQL engine architected from the ground up for the Hadoop data processing environment. At present, by virtue of its superior performance and rich functionality, Impala has been comparable to many commercial MPP database query engine. Mesa can satisfy the needs of many of our storage requirements, however Mesa itself does not provide a SQL query engine; Impala is a very good MPP SQL query engine, but the lack of a perfect distributed storage engine. So in the end we chose the combination of these two technologies.
+Doris not only provides batch data loading, but also provides near real-time mini-batch data loading. 
 
-Learning from Mesa's data model, we developed a distributed storage engine. Unlike Mesa, this storage engine does not rely on any distributed file system. Then we deeply integrate this storage engine with Impala query engine. Query compiling, query execution coordination and catalog management of storage engine are integrated to be frontend daemon; query execution and data storage are integrated to be backend daemon. With this integration, we implemented a single, full-featured, high performance state the art of MPP database, as well as maintaining the simplicity.
+Doris also provides high availability, reliability, fault tolerance, and scalability. 
 
-## 2. System Overview
+The simplicity (of developing, deploying and using) and meeting many data serving requirements in single system are the main features of Doris.
 
-Doris' implementation consists of two daemons: frontend (FE) and backend (BE). The following figures gives the overview of architecture and usage.
+## 4. Compile and install
 
-![](./docs/resources/palo_architecture.jpg)
+Currently support Docker environment and Linux OS: 
+Docker（Linux/Windows/Mac), Ubuntu 16.04+ and CentOS 7.5+
 
-Frontend daemon consists of query coordinator and catalog manager. Query coordinator is responsible for receiving user's sql queries, compiling queries and managing queries execution. Catalog manager is responsible for managing metadata such as databases, tables, partitions, replicas and etc. Several frontend daemons could be deployed to guarantee fault-tolerance, and load balancing.
+### 4.1 For Docker
 
-Backend daemon stores the data and executes the query fragments. Many backend daemons could also be deployed to provide scalability and fault-tolerance.
+#### Step 1: Install Docker
 
-A typical Doris cluster generally composes of several frontend daemons and dozens to hundreds of backend daemons.
+Take CentOS as an example:
 
-Clients can use MySQL-related tools to connect any frontend daemon to submit SQL query. The frontend receives the query and compiles it into query plans executable by the backends. Then frontend sends the query plan fragments to backend. Backends will build a query execution DAG. Data is fetched and pipelined into the DAG. The final result response is sent to client via frontend. The distribution of query fragment execution takes minimizing data movement and maximizing scan locality as the main goal. Because Doris is designed to provide interactive analysis, so the average execution time of queries is short. Considering this, we adopt query re-execution to meet the fault tolerance of query execution.
-
-A table is splitted into many tablets. Tablets are managed by backends. The backend daemon could be configured to use multiple directories. Any directory's IO failure doesn't influence the normal running of backend daemon. Doris will recover and rebalance the whole cluster automatically when necessary.
-
-## 3. Frontend
-
-In-memory catalog, multiple frontends, MySQL networking protocol, consistency guarantee, and two-level table partitioning are the main features of Doris' frontend design.
-
-#### 3.1 In-Memory Catalog
-
-Traditional data warehouse always uses a RDBMS database to store their catalog metadata. In order to produce query execution plan, frontend needs to look up the catalog metadata. This kind of catalog storage may be enough for low concurrent ad-hoc analysis queries. But for online high concurrent queries, its performance is very bad,resulting in increased response latency. For example, Hive metadata query latency is sometimes up to tens of seconds or even minutes. In order to speedup the metadata access, we adopt the in-memory catalog storage.
-
-![](./docs/resources/log_replication.jpg)
-
-In-memory catalog storage has three functional modules: real-time memory data structures, memory checkpoints on local disk and an operation relay log. When modifying catalog, the mutation operation is written into the log file firstly. Then, the mutation operation is applied into the memory data structures. Periodically, a thread does the checkpoint that dumps memory data structure image into local disk. Checkpoint mechanism enables the fast startup of frontend and reduces the disk storage occupancy. Actually, in-memory catalog also simplifies the implementation of multiple frontends.
-
-#### 3.2 Multiple Frontends
-
-Many data warehouses only support single frontend-like node. There are some systems supporting master and slave deploying. But for online data serving, high availability is an essential feature. Further, the number of queries per seconds may be very large, so high scalability is also needed. In Doris, we provide the feature of multiple frontends using replicated-state-machine technology.
-
-Frontends can be configured to three kinds of roles: leader, follower and observer. Through a voting protocol, follower frontends firstly elect a leader frontend. All the write requests of metadata are forwarded to the leader, then the leader writes the operation into the replicated log file. If the new log entry will be replicated to at least quorum followers successfully, the leader commits the operation into memory, and responses the write request. Followers always replay the replicated logs to apply them into their memory metadata. If the leader crashes, a new leader will be elected from the leftover followers. Leader and follower mainly solve the problem of write availability and partly solve the problem of read scalability.
-
-Usually one leader frontend and several follower frontends can meet most applications' write availability and read scalability requirements. For very high concurrent reading, continuing to increase the number of followers is not a good practice. Leader replicates log stream to followers synchronously, so adding more followers will increases write latency. Like Zookeeper,we have introduced a new type of frontend node called observer that helps addressing this problem and further improving metadata read scalability. Leader replicates log stream to observers asynchronously. Observers don't involve leader election.
-
-The replicated-state-machine is implemented based on BerkeleyDB java version (BDB-JE). BDB-JE has achieved high availability by implementing a Paxos-like consensus algorithm.  We use BDB-JE to implement Doris' log replication and leader election.
-
-#### 3.3    Consistency Guarantee
-
-If a client process connects to the leader, it will see up-to-date metadata, so that strong consistency semantics is guaranteed. If the client connects to followers or observers, it will see metadata lagging a little behind of the leader, but the monotonic consistency is guaranteed. In most Doris' use cases, monotonic consistency is accepted.
-
-If the client always connects to the same frontend, monotonic consistency semantics is obviously guaranteed; however if the client connects to other frontends due to failover, the semantics may be violated. Doris provides a SYNC command to guarantee metadata monotonic consistency semantics during failover. When failover happens, the client can send a SYNC command to the new connected frontend, who will get the latest operation log number from the leader. The SYNC command will not return to client as long as local applied log number is still less than fetched operation log number. This mechanism can guarantee the metadata on the connected frontend is newer than the client have seen during its last connection.
-
-#### 3.4 MySQL Networking Protocol
-
-MySQL compatible networking protocol is implemented in Doris' frontend. Firstly, SQL interface is preferred for engineers; Secondly, compatibility with MySQL protocol makes the integrating with current existing BI software, such as Tableau, easier; Lastly, rich MySQL client libraries and tools reduce our development costs, but also reduces the user's using cost.
-
-Through the SQL interface, administrator can adjust system configuration, add and remove frontend nodes or backend nodes, and create new database for user; user can create tables, load data, and submit SQL query.
-
-Online help document and Linux Proc-like mechanism are also supported in SQL. Users can submit queries to get the help of related SQL statements or show Doris' internal running state.
-
-In frontend, a small response buffer is allocated to every MySQL connection. The maximum size of this buffer is limited to 1MB. The buffer is responsible for buffering the query response data. Only if the response is finished or the buffer size reaches the 1MB,the response data will begin to be sent to client. Through this small trick, frontend can re-execution most of queries if errors occurred during query execution.
-
-#### 3.5 Two-Level Partitioning
-
-Like most of the distributed database system, data in Doris is horizontally partitioned. However, a single-level partitioning rule (hash partitioning or range partitioning) may not be a good solution to all scenarios. For example, there have a user-based fact table that stores rows of the form (date, userid, metric). Choosing only hash partitioning by column userid may lead to uneven distribution of data, when one user's data is very large. If choosing range partitioning according to column date, it will also lead to uneven distribution of data due to the likely data explosion in a certain period of time.
-
-Therefore we support the two-level partitioning rule. The first level is range partitioning. User can specify a column (usually the time series column) range of values for the data partition. In one partition, the user can also specify one or more columns and a number of buckets to do the hash partitioning. User can combine with different partitioning rules to better divide the data. Figure 4 gives an example of two-level partitioning.
-
-Three benefits are gained by using the two-level partitioning mechanism. Firstly, old and new data could be separated, and stored on different storage mediums; Secondly, storage engine of backend can reduce the consumption of IO and CPU for unnecessary data merging, because the data in some partitions is no longer be updated; Lastly,every partition's buckets number can be different and adjusted according to the change of data size.
-
-```SQL
--- Create partitions using CREATE TABLE --
-CREATE TABLE example_tbl (
-    `date`      DATE,
-    userid      BIGINT,
-    metric      BIGINT SUM
-) PARTITION BY RANGE (`date`) (
-    PARTITION p201601 VALUES LESS THAN (""2016-02-01""),
-    PARTITION p201602 VALUES LESS THAN (""2016-03-01""),
-    PARTITION p201603 VALUES LESS THAN (""2016-04-01""),
-    PARTITION p201604 VALUES LESS THAN (""2016-05-01"")
-) DISTRIBUTED BY HASH(userid) BUCKETS 32;
-
--- Add partition using ALTER TABLE --
-ALTER TABLE example_tbl ADD PARTITION p201605 VALUES LESS THAN (""2016-06-01"");
+```
+yum -y install docker-io
+service docker start
 ```
 
-## 4. Backend
-
-#### 4.1 Data Storage Model
-
-Doris combines Google Mesa's data model and ORCFile / Parquet storage technology.
-
-Data in Mesa is inherently multi-dimensional fact table. These facts in table typically consist of two types of attributes: dimensional attributes (which we call keys) and measure attributes (which we call values). The table schema also specifies the aggregation function F: V ×V → V which is used to aggregate the values corresponding to the same key. To achieve high update throughput, Mesa loads data in batch. Each batch of data will be converted to a delta file. Mesa uses MVCC approach to manage these delta files, and so to enforce update atomicity. Mesa also supports creating materialized rollups, which contain a column subset of schema to gain better aggregation effect.
-
-Mesa's data model performs well in many interactive data service, but it also has some drawbacks:
-1. Users have difficulty in understanding key and value space, as well as aggregation function, especially when they rarely have such aggregation demand in analysis query scenarios.
-
-2. In order to ensure the aggregation semantic, count operation on a single column must read all columns in key space, resulting in a large number of additional read overheads. There is also unable to push down the predicates on the value column to storage engine, which also leads to additional read overheads.
-
-3.  Essentially, it is still a key-value model. In order to aggregate the values corresponding to the same key, all key columns must store in order. When a table contains hundreds of columns, sorting cost becomes the bottleneck of ETL process.
-
-To solve these problems, we introduce ORCFile / Parquet technology widely used in the open source community, such as MapReduce + ORCFile, SparkSQL + Parquet, mainly used for ad-hoc analysis of large amounts of data with low concurrency. These data does not distinguish between key and value. In addition, compared with the row-oriented database, column-oriented organization is more efficient when an aggregate needs to be computed over many rows but only for a small subset of all columns of data, because reading that smaller subset of data can be faster than reading all data. And columnar storage is also space-friendly due to the high compression ratio of each column. Further, column support block-level storage technology such as min/max index and bloom filter index. Query executor can filter out a lot of blocks that do not meet the predicate, to further improve the query performance. However, due to the underlying storage does not require data order, query time complexity is linear corresponding to the data volume.
-
-Like traditional databases, Doris stores structured data represented as tables. Each table has a well-defined schema consisting of a finite number of columns. We combine Mesa data model and ORCFile/Parquet technology to develop a distributed analytical database. User can create two types of table to meet different needs in interactive query scenarios.
+#### Step 2: Create Docker image
 
-In non-aggregation type of table, columns are not distinguished between dimensions and metrics, but should specify the sort columns in order to sort all rows. Doris will sort the table data according to the sort columns without any aggregation. The following figure gives an example of creating non-aggregation table.
+Given your work space is /my/workspace, and you can download Doris docker file as following:
 
-```SQL
--- Create non-aggregation table --
-CREATE TABLE example_tbl (
-    `date`      DATE,
-    id          BIGINT,
-    country     VARCHAR(32),
-    click       BIGINT,
-    cost        BIGINT
-) DUPLICATE KEY(`date`, id, country)
-DISTRIBUTED BY HASH(id) BUCKETS 32;
 ```
-
-In aggregation data analysis case, we reference Mesa's data model, and distinguish columns between key and value, and specify the value columns with aggregation method, such as SUM, REPLACE, etc. In the following figure, we create an aggregation table like the non-aggregation table, including two SUM aggregation columns (clicks, cost). Different from the non-aggregation table, data in the table needs to be sorted on all key columns for delta compaction and value aggregation.
-
-```SQL
--- Create aggregation table --
-CREATE TABLE example_tbl (
-    `date`      DATE,
-    id          BIGINT,
-    country     VARCHAR(32),
-    click       BIGINT          SUM,
-    cost        BIGINT          SUM
-) DISTRIBUTED BY HASH(id) BUCKETS 32;
+wget https://github.com/apache/incubator-doris/blob/master/docker/Dockerfile -O /my/workspace/Dockerfile
 ```
 
-Rollup is a materialized view that contains a column subset of schema in Doris. A table may contain multiple rollups with columns in different order. According to sort key index and column covering of the rollups, Doris can select the best rollup for different query. Because most rollups only contain a few columns, the size of aggregated data is typically much smaller and query performance can greatly be improved. All the rollups in the same table are updated atomically. Because rollups are materialized, users should make a trade-off between query latency and storage space when using them.
-
-To achieve high update throughput, Doris only applies updates in batches at the smallest frequency of every minute. Each update batch specifies an increased version number and generates a delta data file, commits the version when updates of quorum replicas are complete. You can query all committed data using the committed version, and the uncommitted version would not be used in query. All update versions are strictly be in increasing order. If an update contains more than one table, the versions of these tables are committed atomically. The MVCC mechanism allows Doris to guarantee multiple table atomic updates and query consistency. In addition, Doris uses compaction policies to merge delta files to reduce delta number, also reduce the cost of delta merging during query for higher performance.
-
-Doris' data file is stored by column. The rows are stored in sorted order by the sort columns in delta data files, and are organized into row blocks, each block is compressed by type-specific columnar encodings, such as run-length encoding for integer columns, then stored into separate streams. In order to improve the performance of queries that have a specific key, we also store a sparse sort key index file corresponding to each delta data file. An index entry contains the short key for the row block, which is a fixed size prefix of the first sort columns for the row block, and the block id in the data file. Index files are usually directly loaded into memory, as they are very small. The algorithm for querying a specific key includes two steps. First, use a binary search on the sort key index to find blocks that may contain the specific key, and then perform a binary search on the compressed blocks in the data files to find the desired key. We also store block-level min/max index into separate index streams, and queries can use this to filter undesired blocks. In addition to those basic columnar features, we also offers an optional block-level bloom filter index for queries with IN or EQUAL conditions to further filter undesired blocks. Bloom filter index is stored in a separate stream, and is loaded on demand.
+Now build your image: (It will take more time and use nohup if needed)
 
-#### 4.2 Data Loading
+```
+cd /my/workspace && docker build -t doris-dev:v1.0 .
+```
 
-Doris applies updates in batches. Three types of data loading are supported: Hadoop-batch loading, loading ,mini-batch loading.
+#### Step 3: Compile and install Doris
 
-1. Hadoop-batch loading. When a large amount of data volume needs to be loaded into Doris, the hadoop-batch loading is recommended to achieve high loading throughput. The data batches themselves are produced by an external Hadoop system, typically at a frequency of every few minutes. Unlike traditional data warehouses that use their own computing resource to do the heavy data preparation, Doris could use Hadoop to prepare the data (shuffle, sort and aggregate, etc.). By using this approach, the most time-consuming computations are handed over to Hadoop to complete. This will not only improve computational efficiency, but also reduce the performance pressure of Doris cluster and ensure the stability of the query service. The stability of the online data services is the most important point.
+Clone Doris source:
 
-2. Loading. After deploying the fs-brokers, you can use Doris' query engine to import data. This type of loading is recommended for incremental data loading.
+```
+git clone https://github.com/apache/incubator-doris.git /path/to/incubator-doris/
+```
 
-3. Mini-batch loading. When a small amount of data needs to be loaded into Doris, the mini-batch loading is recommended to achieve low loading latency.  By using http interface, raw data is pushed into a backend. Then the backend does the data preparing computing and completes the final loading. Http tools could connect frontend or backend. If frontend is connected, it will redirect the request randomly to a backend.
+Start a docker named doris-dev-test, and map /path/to/incubator-doris/ to /var/local/incubator-doris/ which in docker.
 
-All the loading work is handled asynchronously. When load request is submitted, a label needs to be provided. By using the load label, users can submit show load request to get the loading status or submit cancel load request to cancel the loading. If the status of loading task is successful or in progress, its load label is not allowed to reuse again. The label of failed task is allowed to be reused.
+```
+docker run -it --name doris-dev-test -v /path/to/incubator-doris/:/var/local/incubator-doris/ doris-dev:v1.0
+```
 
-#### 4.3 Resource Isolation
+Compile Doris source:
 
-1. Multi-tenancy Isolation：Multiple virtual cluster can be created in one pysical Doris cluster. Every backend node can deploy multiple backend processes. Every backend process only belongs to one virtual cluster. Virtual cluster is one tenancy.
+```
+sh build.sh 
+```
 
-2. User Isolation: There are many users in one virtual cluster. You can allocate the resource among different users and ensure that all users' tasks are executed under limited resource quota.
+After successful build, it will install binary files to the path of output/.
 
-3. Priority Isolation: There are three priorities isolation group for one user. User could control resource allocated to different tasks submitted by themselves, for example user's query task and loading tasks require different resource quota.
+### 4.2 For Linux
 
-#### 4.4 Multi-Medium Storage
+#### Prerequisites
 
-Most machines in modern datacenter are equipped with both SSDs and HDDs. SSD has good random read capability that is the ideal medium for query that needs a large number of random read operations. However, SSD's capacity is small and is very expensive, we could not deploy it at a large scale. HDD is cheap and has huge capacity that is suitable to store large scale data but with high read latency. In OLAP scenario, we find user usually submit a lot of queries to query the latest data (hot data) and expect low latency. User occasionally executes query on historical data (cold data). This kind of query usually needs to scan large scale of data and is high latency. Multi-Medium Storage allows users to manage the storage medium of the data to meet different query scenarios and reduce the latency. For example, user could put latest data on SSD and historical data which is not used frequently on HDD, user will get low latency when querying latest data while get high latency when query historical data which is normal because it needs scan large scale data.
+GCC 5.3.1+，Oracle JDK 1.8+，Python 2.7+, Apache Maven 3.5.4+
 
-In the following figure, user alters partition 'p201601' storage_medium to SSD and storage_cooldown_time to '2016-07-01 00:00:00'. The setting means data in this partition will be put on SSD and it will start to migrate to HDD after the time of storage_cooldown_time.
+* For Ubuntu: 
 
-```SQL
-ALTER TABLE example_tbl MODIFY PARTITION p201601
-SET (""storage_medium"" = ""SSD"", ""storage_cooldown_time"" = ""2016-07-01 00:00:00"");
+```
+sudo apt-get install g++ ant cmake zip byacc flex automake libtool binutils-dev libiberty-dev bison python2.7 libncurses5-dev
+sudo updatedb
 ```
 
-#### 4.5 Vectorized Query Execution
-
-Runtime code generation using LLVM is one of the techniques employed extensively by Impala to improve query execution times. Performance could gains of 5X or more are typical for representative workloads.
-
-But, runtime code generation is not suitable for low latency query, because the generation overhead costs about 100ms. Runtime code generation is more suitable for large-scale ad-hoc query. To accelerate the small queries (of course, big queries will also obtain benefits), we introduced vectorized query execution into Doris.
+* For CentOS:
 
-Vectorized query execution is a feature that greatly reduces the CPU usage for typical query operations like scans, filters, aggregates, and joins. A standard query execution system processes one row at a time. This involves long code paths and significant metadata interpretation in the inner loop of execution. Vectorized query execution streamlines operations by processing a block of many rows at a time. Within the block, each column is stored as a vector (an array of a primitive data type). Simple operations like arithmetic and comparisons are done by quickly iterating through the vectors in a tight loop, with no or very few function calls or conditional branches inside the loop. These loops compile in a streamlined way that uses relatively few instructions and finishes each instruction in fewer clock cycles, on average, by effectively using the processor pipeline and cache memory.
+```
+sudo yum install gcc-c++ libstdc++-static ant cmake byacc flex automake libtool binutils-devel bison ncurses-devel
+sudo updatedb
+```
 
-The result of benchmark shows 2x~4x speedup in our typical queries.
+If your GCC version less than 5.3.1, you can run:
 
-## 5.   Backup and Recovery
+```
+sudo yum install devtoolset-4-toolchain -y
+```
 
-Data backup function is provided to enhance data security. The minimum granularity of backup and recovery is partition.  Users can develop plugins to backup data to any specified remote storage. The backup data can always be recovered to Doris at all time, to achieve the data rollback purpose.
+and then, set the path of gcc (e.g /opt/rh/devtoolset-4/root/usr/bin) to the environment variabl PATH.
 
-Currently we only support full data backup data rather than incremental backups for the following reasons:
 
-1. Remote storage system is beyond the control of the Doris system. We cannot guarantee whether the data has been changed between two backup operations. And data verification operations always come at a high price.
+#### Compile and install
 
-2. We support data backup on partition granularity. And majority of applications are time series applications. By dividing data using time column, it has been able to meet the needs of the vast majority of incremental backup in chronological order.
+Run following script, it will comiple thirdparty libraries and build whole Doris.
 
-In addition to improving data security, the backup function also provides a way to export the data. Data can be exported to other downstream systems for further processing.
+```
+sh build.sh
+```
 
-# Install
-Doris only supports Linux System. Oracle JDK 8.0+ and GCC 4.8.2+ are required. See the document of [INSTALL](https://github.com/apache/incubator-doris/wiki/Doris-Install) and [Deploy & Update](https://github.com/apache/incubator-doris/wiki/Doris-Deploy-%26-Upgrade)
+After successful build, it will install binary files to the path of output/.","[{'comment': 'After successful build\r\n->\r\nAfter successfully building', 'commenter': 'morningman'}, {'comment': 'to the path of output/.\r\n->\r\nin the directory `output/`', 'commenter': 'morningman'}]"
345,README.md,"@@ -1,215 +1,127 @@
-# Introduction to Apache Doris (incubating)
+# Apache Doris (incubating)
 
-Apache Doris is an MPP-based interactive SQL data warehousing for reporting and analysis. Doris mainly integrates the technology of Google Mesa and Apache Impala. Unlike other popular SQL-on-Hadoop systems, Doris is designed to be a simple and single tightly coupled system, not depending on other systems. Doris not only provides high concurrent low latency point query performance, but also provides high throughput queries of ad-hoc analysis. Doris not only provides batch data loading, but also provides near real-time mini-batch data loading. Doris also provides high availability, reliability, fault tolerance, and scalability. The simplicity (of developing, deploying and using) and meeting many data serving requirements in single system are the main features of Doris.
+Doris is an MPP-based interactive SQL data warehousing for reporting and analysis. It open-sourced by Baidu. 
 
-## 1. Background
+## 1. License
 
-In Baidu, the largest Chinese search engine, we run a two-tiered data warehousing system for data processing, reporting and analysis. Similar to lambda architecture, the whole data warehouse comprises data processing and data serving. Data processing does the heavy lifting of big data: cleaning data, merging and transforming it, analyzing it and preparing it for use by end user queries; data serving is designed to serve queries against that data for different use cases. Currently data processing includes batch data processing and stream data processing technology, like Hadoop, Spark and Storm; Doris is a SQL data warehouse for serving online and interactive data reporting and analysis querying.
+[Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0)
 
-Prior to Doris, different tools were deployed to solve diverse requirements in many ways. For example, the advertising platform needs to provide some detailed statistics associated with each served ad for every advertiser. The platform must support continuous updates, both new rows and incremental updates to existing rows within minutes. It must support latency-sensitive users serving live customer reports with very low latency requirements and batch ad-hoc multiple dimensions data analysis requiring very high throughput. In the past,this platform was built on top of sharded MySQL. But with the growth of data, MySQL cannot meet the requirements. Then, based on our existing KV system, we developed our own proprietary distributed statistical database. But, the simple KV storage was not efficient on scan performance. Because the system depends on many other systems, it is very complex to operate and maintain. Using RPC API, more complex querying usually required code programming, but users wants an MPP SQL engine. In addition to advertising system, a large number of internal BI Reporting / Analysis, also used a variety of tools. Some used the combination of SparkSQL / Impala + HDFS / HBASE. Some used MySQL to store the results that were prepared by distributed MapReduce computing. Some also bought commercial databases to use.
+## 2. Technology
+Doris mainly integrates the technology of Google Mesa and Apache Impala, and it based on a column-oriented storage engine and can communicate by MySQL client.
 
-However, when a use case requires the simultaneous availability of capabilities that cannot all be provided by a single tool, users were forced to build hybrid architectures that stitch multiple tools together. Users often choose to ingest and update data in one storage system, but later reorganize this data to optimize for an analytical reporting use-case served from another. Our users had been successfully deploying and maintaining these hybrid architectures, but we believe that they shouldn't need to accept their inherent complexity. A storage system built to provide great performance across a broad range of workloads provides a more elegant solution to the problems that hybrid architectures aim to solve. Doris is the solution. Doris is designed to be a simple and single tightly coupled system, not depending on other systems. Doris provides high concurrent low latency point query performance, but also provides high throughput queries of ad-hoc analysis. Doris provides bulk-batch data loading, but also provides near real-time mini-batch data loading. Doris also provides high availability, reliability, fault tolerance, and scalability.
+## 3. User cases
+Doris not only provides high concurrent low latency point query performance, but also provides high throughput queries of ad-hoc analysis. 
 
-Generally speaking, Doris is the technology combination of Google Mesa and Apache Impala. Mesa is a highly scalable analytic data storage system that stores critical measurement data related to Google's Internet advertising business. Mesa is designed to satisfy complex and challenging set of users' and systems' requirements, including near real-time data ingestion and query ability, as well as high availability, reliability, fault tolerance, and scalability for large data and query volumes. Impala is a modern, open-source MPP SQL engine architected from the ground up for the Hadoop data processing environment. At present, by virtue of its superior performance and rich functionality, Impala has been comparable to many commercial MPP database query engine. Mesa can satisfy the needs of many of our storage requirements, however Mesa itself does not provide a SQL query engine; Impala is a very good MPP SQL query engine, but the lack of a perfect distributed storage engine. So in the end we chose the combination of these two technologies.
+Doris not only provides batch data loading, but also provides near real-time mini-batch data loading. 
 
-Learning from Mesa's data model, we developed a distributed storage engine. Unlike Mesa, this storage engine does not rely on any distributed file system. Then we deeply integrate this storage engine with Impala query engine. Query compiling, query execution coordination and catalog management of storage engine are integrated to be frontend daemon; query execution and data storage are integrated to be backend daemon. With this integration, we implemented a single, full-featured, high performance state the art of MPP database, as well as maintaining the simplicity.
+Doris also provides high availability, reliability, fault tolerance, and scalability. 
 
-## 2. System Overview
+The simplicity (of developing, deploying and using) and meeting many data serving requirements in single system are the main features of Doris.","[{'comment': 'How about adding the Overview.md link here?', 'commenter': 'morningman'}]"
345,README.md,"@@ -1,215 +1,127 @@
-# Introduction to Apache Doris (incubating)
+# Apache Doris (incubating)
 
-Apache Doris is an MPP-based interactive SQL data warehousing for reporting and analysis. Doris mainly integrates the technology of Google Mesa and Apache Impala. Unlike other popular SQL-on-Hadoop systems, Doris is designed to be a simple and single tightly coupled system, not depending on other systems. Doris not only provides high concurrent low latency point query performance, but also provides high throughput queries of ad-hoc analysis. Doris not only provides batch data loading, but also provides near real-time mini-batch data loading. Doris also provides high availability, reliability, fault tolerance, and scalability. The simplicity (of developing, deploying and using) and meeting many data serving requirements in single system are the main features of Doris.
+Doris is an MPP-based interactive SQL data warehousing for reporting and analysis. It open-sourced by Baidu. 
 
-## 1. Background
+## 1. License
 
-In Baidu, the largest Chinese search engine, we run a two-tiered data warehousing system for data processing, reporting and analysis. Similar to lambda architecture, the whole data warehouse comprises data processing and data serving. Data processing does the heavy lifting of big data: cleaning data, merging and transforming it, analyzing it and preparing it for use by end user queries; data serving is designed to serve queries against that data for different use cases. Currently data processing includes batch data processing and stream data processing technology, like Hadoop, Spark and Storm; Doris is a SQL data warehouse for serving online and interactive data reporting and analysis querying.
+[Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0)
 
-Prior to Doris, different tools were deployed to solve diverse requirements in many ways. For example, the advertising platform needs to provide some detailed statistics associated with each served ad for every advertiser. The platform must support continuous updates, both new rows and incremental updates to existing rows within minutes. It must support latency-sensitive users serving live customer reports with very low latency requirements and batch ad-hoc multiple dimensions data analysis requiring very high throughput. In the past,this platform was built on top of sharded MySQL. But with the growth of data, MySQL cannot meet the requirements. Then, based on our existing KV system, we developed our own proprietary distributed statistical database. But, the simple KV storage was not efficient on scan performance. Because the system depends on many other systems, it is very complex to operate and maintain. Using RPC API, more complex querying usually required code programming, but users wants an MPP SQL engine. In addition to advertising system, a large number of internal BI Reporting / Analysis, also used a variety of tools. Some used the combination of SparkSQL / Impala + HDFS / HBASE. Some used MySQL to store the results that were prepared by distributed MapReduce computing. Some also bought commercial databases to use.
+## 2. Technology
+Doris mainly integrates the technology of Google Mesa and Apache Impala, and it based on a column-oriented storage engine and can communicate by MySQL client.
 
-However, when a use case requires the simultaneous availability of capabilities that cannot all be provided by a single tool, users were forced to build hybrid architectures that stitch multiple tools together. Users often choose to ingest and update data in one storage system, but later reorganize this data to optimize for an analytical reporting use-case served from another. Our users had been successfully deploying and maintaining these hybrid architectures, but we believe that they shouldn't need to accept their inherent complexity. A storage system built to provide great performance across a broad range of workloads provides a more elegant solution to the problems that hybrid architectures aim to solve. Doris is the solution. Doris is designed to be a simple and single tightly coupled system, not depending on other systems. Doris provides high concurrent low latency point query performance, but also provides high throughput queries of ad-hoc analysis. Doris provides bulk-batch data loading, but also provides near real-time mini-batch data loading. Doris also provides high availability, reliability, fault tolerance, and scalability.
+## 3. User cases
+Doris not only provides high concurrent low latency point query performance, but also provides high throughput queries of ad-hoc analysis. 
 
-Generally speaking, Doris is the technology combination of Google Mesa and Apache Impala. Mesa is a highly scalable analytic data storage system that stores critical measurement data related to Google's Internet advertising business. Mesa is designed to satisfy complex and challenging set of users' and systems' requirements, including near real-time data ingestion and query ability, as well as high availability, reliability, fault tolerance, and scalability for large data and query volumes. Impala is a modern, open-source MPP SQL engine architected from the ground up for the Hadoop data processing environment. At present, by virtue of its superior performance and rich functionality, Impala has been comparable to many commercial MPP database query engine. Mesa can satisfy the needs of many of our storage requirements, however Mesa itself does not provide a SQL query engine; Impala is a very good MPP SQL query engine, but the lack of a perfect distributed storage engine. So in the end we chose the combination of these two technologies.
+Doris not only provides batch data loading, but also provides near real-time mini-batch data loading. 
 
-Learning from Mesa's data model, we developed a distributed storage engine. Unlike Mesa, this storage engine does not rely on any distributed file system. Then we deeply integrate this storage engine with Impala query engine. Query compiling, query execution coordination and catalog management of storage engine are integrated to be frontend daemon; query execution and data storage are integrated to be backend daemon. With this integration, we implemented a single, full-featured, high performance state the art of MPP database, as well as maintaining the simplicity.
+Doris also provides high availability, reliability, fault tolerance, and scalability. 
 
-## 2. System Overview
+The simplicity (of developing, deploying and using) and meeting many data serving requirements in single system are the main features of Doris.
 
-Doris' implementation consists of two daemons: frontend (FE) and backend (BE). The following figures gives the overview of architecture and usage.
+## 4. Compile and install
 
-![](./docs/resources/palo_architecture.jpg)
+Currently support Docker environment and Linux OS: 
+Docker（Linux/Windows/Mac), Ubuntu 16.04+ and CentOS 7.5+
 
-Frontend daemon consists of query coordinator and catalog manager. Query coordinator is responsible for receiving user's sql queries, compiling queries and managing queries execution. Catalog manager is responsible for managing metadata such as databases, tables, partitions, replicas and etc. Several frontend daemons could be deployed to guarantee fault-tolerance, and load balancing.
+### 4.1 For Docker
 
-Backend daemon stores the data and executes the query fragments. Many backend daemons could also be deployed to provide scalability and fault-tolerance.
+#### Step 1: Install Docker
 
-A typical Doris cluster generally composes of several frontend daemons and dozens to hundreds of backend daemons.
+Take CentOS as an example:
 
-Clients can use MySQL-related tools to connect any frontend daemon to submit SQL query. The frontend receives the query and compiles it into query plans executable by the backends. Then frontend sends the query plan fragments to backend. Backends will build a query execution DAG. Data is fetched and pipelined into the DAG. The final result response is sent to client via frontend. The distribution of query fragment execution takes minimizing data movement and maximizing scan locality as the main goal. Because Doris is designed to provide interactive analysis, so the average execution time of queries is short. Considering this, we adopt query re-execution to meet the fault tolerance of query execution.
-
-A table is splitted into many tablets. Tablets are managed by backends. The backend daemon could be configured to use multiple directories. Any directory's IO failure doesn't influence the normal running of backend daemon. Doris will recover and rebalance the whole cluster automatically when necessary.
-
-## 3. Frontend
-
-In-memory catalog, multiple frontends, MySQL networking protocol, consistency guarantee, and two-level table partitioning are the main features of Doris' frontend design.
-
-#### 3.1 In-Memory Catalog
-
-Traditional data warehouse always uses a RDBMS database to store their catalog metadata. In order to produce query execution plan, frontend needs to look up the catalog metadata. This kind of catalog storage may be enough for low concurrent ad-hoc analysis queries. But for online high concurrent queries, its performance is very bad,resulting in increased response latency. For example, Hive metadata query latency is sometimes up to tens of seconds or even minutes. In order to speedup the metadata access, we adopt the in-memory catalog storage.
-
-![](./docs/resources/log_replication.jpg)
-
-In-memory catalog storage has three functional modules: real-time memory data structures, memory checkpoints on local disk and an operation relay log. When modifying catalog, the mutation operation is written into the log file firstly. Then, the mutation operation is applied into the memory data structures. Periodically, a thread does the checkpoint that dumps memory data structure image into local disk. Checkpoint mechanism enables the fast startup of frontend and reduces the disk storage occupancy. Actually, in-memory catalog also simplifies the implementation of multiple frontends.
-
-#### 3.2 Multiple Frontends
-
-Many data warehouses only support single frontend-like node. There are some systems supporting master and slave deploying. But for online data serving, high availability is an essential feature. Further, the number of queries per seconds may be very large, so high scalability is also needed. In Doris, we provide the feature of multiple frontends using replicated-state-machine technology.
-
-Frontends can be configured to three kinds of roles: leader, follower and observer. Through a voting protocol, follower frontends firstly elect a leader frontend. All the write requests of metadata are forwarded to the leader, then the leader writes the operation into the replicated log file. If the new log entry will be replicated to at least quorum followers successfully, the leader commits the operation into memory, and responses the write request. Followers always replay the replicated logs to apply them into their memory metadata. If the leader crashes, a new leader will be elected from the leftover followers. Leader and follower mainly solve the problem of write availability and partly solve the problem of read scalability.
-
-Usually one leader frontend and several follower frontends can meet most applications' write availability and read scalability requirements. For very high concurrent reading, continuing to increase the number of followers is not a good practice. Leader replicates log stream to followers synchronously, so adding more followers will increases write latency. Like Zookeeper,we have introduced a new type of frontend node called observer that helps addressing this problem and further improving metadata read scalability. Leader replicates log stream to observers asynchronously. Observers don't involve leader election.
-
-The replicated-state-machine is implemented based on BerkeleyDB java version (BDB-JE). BDB-JE has achieved high availability by implementing a Paxos-like consensus algorithm.  We use BDB-JE to implement Doris' log replication and leader election.
-
-#### 3.3    Consistency Guarantee
-
-If a client process connects to the leader, it will see up-to-date metadata, so that strong consistency semantics is guaranteed. If the client connects to followers or observers, it will see metadata lagging a little behind of the leader, but the monotonic consistency is guaranteed. In most Doris' use cases, monotonic consistency is accepted.
-
-If the client always connects to the same frontend, monotonic consistency semantics is obviously guaranteed; however if the client connects to other frontends due to failover, the semantics may be violated. Doris provides a SYNC command to guarantee metadata monotonic consistency semantics during failover. When failover happens, the client can send a SYNC command to the new connected frontend, who will get the latest operation log number from the leader. The SYNC command will not return to client as long as local applied log number is still less than fetched operation log number. This mechanism can guarantee the metadata on the connected frontend is newer than the client have seen during its last connection.
-
-#### 3.4 MySQL Networking Protocol
-
-MySQL compatible networking protocol is implemented in Doris' frontend. Firstly, SQL interface is preferred for engineers; Secondly, compatibility with MySQL protocol makes the integrating with current existing BI software, such as Tableau, easier; Lastly, rich MySQL client libraries and tools reduce our development costs, but also reduces the user's using cost.
-
-Through the SQL interface, administrator can adjust system configuration, add and remove frontend nodes or backend nodes, and create new database for user; user can create tables, load data, and submit SQL query.
-
-Online help document and Linux Proc-like mechanism are also supported in SQL. Users can submit queries to get the help of related SQL statements or show Doris' internal running state.
-
-In frontend, a small response buffer is allocated to every MySQL connection. The maximum size of this buffer is limited to 1MB. The buffer is responsible for buffering the query response data. Only if the response is finished or the buffer size reaches the 1MB,the response data will begin to be sent to client. Through this small trick, frontend can re-execution most of queries if errors occurred during query execution.
-
-#### 3.5 Two-Level Partitioning
-
-Like most of the distributed database system, data in Doris is horizontally partitioned. However, a single-level partitioning rule (hash partitioning or range partitioning) may not be a good solution to all scenarios. For example, there have a user-based fact table that stores rows of the form (date, userid, metric). Choosing only hash partitioning by column userid may lead to uneven distribution of data, when one user's data is very large. If choosing range partitioning according to column date, it will also lead to uneven distribution of data due to the likely data explosion in a certain period of time.
-
-Therefore we support the two-level partitioning rule. The first level is range partitioning. User can specify a column (usually the time series column) range of values for the data partition. In one partition, the user can also specify one or more columns and a number of buckets to do the hash partitioning. User can combine with different partitioning rules to better divide the data. Figure 4 gives an example of two-level partitioning.
-
-Three benefits are gained by using the two-level partitioning mechanism. Firstly, old and new data could be separated, and stored on different storage mediums; Secondly, storage engine of backend can reduce the consumption of IO and CPU for unnecessary data merging, because the data in some partitions is no longer be updated; Lastly,every partition's buckets number can be different and adjusted according to the change of data size.
-
-```SQL
--- Create partitions using CREATE TABLE --
-CREATE TABLE example_tbl (
-    `date`      DATE,
-    userid      BIGINT,
-    metric      BIGINT SUM
-) PARTITION BY RANGE (`date`) (
-    PARTITION p201601 VALUES LESS THAN (""2016-02-01""),
-    PARTITION p201602 VALUES LESS THAN (""2016-03-01""),
-    PARTITION p201603 VALUES LESS THAN (""2016-04-01""),
-    PARTITION p201604 VALUES LESS THAN (""2016-05-01"")
-) DISTRIBUTED BY HASH(userid) BUCKETS 32;
-
--- Add partition using ALTER TABLE --
-ALTER TABLE example_tbl ADD PARTITION p201605 VALUES LESS THAN (""2016-06-01"");
+```
+yum -y install docker-io
+service docker start
 ```
 
-## 4. Backend
-
-#### 4.1 Data Storage Model
-
-Doris combines Google Mesa's data model and ORCFile / Parquet storage technology.
-
-Data in Mesa is inherently multi-dimensional fact table. These facts in table typically consist of two types of attributes: dimensional attributes (which we call keys) and measure attributes (which we call values). The table schema also specifies the aggregation function F: V ×V → V which is used to aggregate the values corresponding to the same key. To achieve high update throughput, Mesa loads data in batch. Each batch of data will be converted to a delta file. Mesa uses MVCC approach to manage these delta files, and so to enforce update atomicity. Mesa also supports creating materialized rollups, which contain a column subset of schema to gain better aggregation effect.
-
-Mesa's data model performs well in many interactive data service, but it also has some drawbacks:
-1. Users have difficulty in understanding key and value space, as well as aggregation function, especially when they rarely have such aggregation demand in analysis query scenarios.
-
-2. In order to ensure the aggregation semantic, count operation on a single column must read all columns in key space, resulting in a large number of additional read overheads. There is also unable to push down the predicates on the value column to storage engine, which also leads to additional read overheads.
-
-3.  Essentially, it is still a key-value model. In order to aggregate the values corresponding to the same key, all key columns must store in order. When a table contains hundreds of columns, sorting cost becomes the bottleneck of ETL process.
-
-To solve these problems, we introduce ORCFile / Parquet technology widely used in the open source community, such as MapReduce + ORCFile, SparkSQL + Parquet, mainly used for ad-hoc analysis of large amounts of data with low concurrency. These data does not distinguish between key and value. In addition, compared with the row-oriented database, column-oriented organization is more efficient when an aggregate needs to be computed over many rows but only for a small subset of all columns of data, because reading that smaller subset of data can be faster than reading all data. And columnar storage is also space-friendly due to the high compression ratio of each column. Further, column support block-level storage technology such as min/max index and bloom filter index. Query executor can filter out a lot of blocks that do not meet the predicate, to further improve the query performance. However, due to the underlying storage does not require data order, query time complexity is linear corresponding to the data volume.
-
-Like traditional databases, Doris stores structured data represented as tables. Each table has a well-defined schema consisting of a finite number of columns. We combine Mesa data model and ORCFile/Parquet technology to develop a distributed analytical database. User can create two types of table to meet different needs in interactive query scenarios.
+#### Step 2: Create Docker image
 
-In non-aggregation type of table, columns are not distinguished between dimensions and metrics, but should specify the sort columns in order to sort all rows. Doris will sort the table data according to the sort columns without any aggregation. The following figure gives an example of creating non-aggregation table.
+Given your work space is /my/workspace, and you can download Doris docker file as following:
 
-```SQL
--- Create non-aggregation table --
-CREATE TABLE example_tbl (
-    `date`      DATE,
-    id          BIGINT,
-    country     VARCHAR(32),
-    click       BIGINT,
-    cost        BIGINT
-) DUPLICATE KEY(`date`, id, country)
-DISTRIBUTED BY HASH(id) BUCKETS 32;
 ```
-
-In aggregation data analysis case, we reference Mesa's data model, and distinguish columns between key and value, and specify the value columns with aggregation method, such as SUM, REPLACE, etc. In the following figure, we create an aggregation table like the non-aggregation table, including two SUM aggregation columns (clicks, cost). Different from the non-aggregation table, data in the table needs to be sorted on all key columns for delta compaction and value aggregation.
-
-```SQL
--- Create aggregation table --
-CREATE TABLE example_tbl (
-    `date`      DATE,
-    id          BIGINT,
-    country     VARCHAR(32),
-    click       BIGINT          SUM,
-    cost        BIGINT          SUM
-) DISTRIBUTED BY HASH(id) BUCKETS 32;
+wget https://github.com/apache/incubator-doris/blob/master/docker/Dockerfile -O /my/workspace/Dockerfile
 ```
 
-Rollup is a materialized view that contains a column subset of schema in Doris. A table may contain multiple rollups with columns in different order. According to sort key index and column covering of the rollups, Doris can select the best rollup for different query. Because most rollups only contain a few columns, the size of aggregated data is typically much smaller and query performance can greatly be improved. All the rollups in the same table are updated atomically. Because rollups are materialized, users should make a trade-off between query latency and storage space when using them.
-
-To achieve high update throughput, Doris only applies updates in batches at the smallest frequency of every minute. Each update batch specifies an increased version number and generates a delta data file, commits the version when updates of quorum replicas are complete. You can query all committed data using the committed version, and the uncommitted version would not be used in query. All update versions are strictly be in increasing order. If an update contains more than one table, the versions of these tables are committed atomically. The MVCC mechanism allows Doris to guarantee multiple table atomic updates and query consistency. In addition, Doris uses compaction policies to merge delta files to reduce delta number, also reduce the cost of delta merging during query for higher performance.
-
-Doris' data file is stored by column. The rows are stored in sorted order by the sort columns in delta data files, and are organized into row blocks, each block is compressed by type-specific columnar encodings, such as run-length encoding for integer columns, then stored into separate streams. In order to improve the performance of queries that have a specific key, we also store a sparse sort key index file corresponding to each delta data file. An index entry contains the short key for the row block, which is a fixed size prefix of the first sort columns for the row block, and the block id in the data file. Index files are usually directly loaded into memory, as they are very small. The algorithm for querying a specific key includes two steps. First, use a binary search on the sort key index to find blocks that may contain the specific key, and then perform a binary search on the compressed blocks in the data files to find the desired key. We also store block-level min/max index into separate index streams, and queries can use this to filter undesired blocks. In addition to those basic columnar features, we also offers an optional block-level bloom filter index for queries with IN or EQUAL conditions to further filter undesired blocks. Bloom filter index is stored in a separate stream, and is loaded on demand.
+Now build your image: (It will take more time and use nohup if needed)
 
-#### 4.2 Data Loading
+```
+cd /my/workspace && docker build -t doris-dev:v1.0 .
+```
 
-Doris applies updates in batches. Three types of data loading are supported: Hadoop-batch loading, loading ,mini-batch loading.
+#### Step 3: Compile and install Doris
 
-1. Hadoop-batch loading. When a large amount of data volume needs to be loaded into Doris, the hadoop-batch loading is recommended to achieve high loading throughput. The data batches themselves are produced by an external Hadoop system, typically at a frequency of every few minutes. Unlike traditional data warehouses that use their own computing resource to do the heavy data preparation, Doris could use Hadoop to prepare the data (shuffle, sort and aggregate, etc.). By using this approach, the most time-consuming computations are handed over to Hadoop to complete. This will not only improve computational efficiency, but also reduce the performance pressure of Doris cluster and ensure the stability of the query service. The stability of the online data services is the most important point.
+Clone Doris source:
 
-2. Loading. After deploying the fs-brokers, you can use Doris' query engine to import data. This type of loading is recommended for incremental data loading.
+```
+git clone https://github.com/apache/incubator-doris.git /path/to/incubator-doris/
+```
 
-3. Mini-batch loading. When a small amount of data needs to be loaded into Doris, the mini-batch loading is recommended to achieve low loading latency.  By using http interface, raw data is pushed into a backend. Then the backend does the data preparing computing and completes the final loading. Http tools could connect frontend or backend. If frontend is connected, it will redirect the request randomly to a backend.
+Start a docker named doris-dev-test, and map /path/to/incubator-doris/ to /var/local/incubator-doris/ which in docker.
 
-All the loading work is handled asynchronously. When load request is submitted, a label needs to be provided. By using the load label, users can submit show load request to get the loading status or submit cancel load request to cancel the loading. If the status of loading task is successful or in progress, its load label is not allowed to reuse again. The label of failed task is allowed to be reused.
+```
+docker run -it --name doris-dev-test -v /path/to/incubator-doris/:/var/local/incubator-doris/ doris-dev:v1.0
+```
 
-#### 4.3 Resource Isolation
+Compile Doris source:
 
-1. Multi-tenancy Isolation：Multiple virtual cluster can be created in one pysical Doris cluster. Every backend node can deploy multiple backend processes. Every backend process only belongs to one virtual cluster. Virtual cluster is one tenancy.
+```
+sh build.sh 
+```
 
-2. User Isolation: There are many users in one virtual cluster. You can allocate the resource among different users and ensure that all users' tasks are executed under limited resource quota.
+After successful build, it will install binary files to the path of output/.
 
-3. Priority Isolation: There are three priorities isolation group for one user. User could control resource allocated to different tasks submitted by themselves, for example user's query task and loading tasks require different resource quota.
+### 4.2 For Linux
 
-#### 4.4 Multi-Medium Storage
+#### Prerequisites
 
-Most machines in modern datacenter are equipped with both SSDs and HDDs. SSD has good random read capability that is the ideal medium for query that needs a large number of random read operations. However, SSD's capacity is small and is very expensive, we could not deploy it at a large scale. HDD is cheap and has huge capacity that is suitable to store large scale data but with high read latency. In OLAP scenario, we find user usually submit a lot of queries to query the latest data (hot data) and expect low latency. User occasionally executes query on historical data (cold data). This kind of query usually needs to scan large scale of data and is high latency. Multi-Medium Storage allows users to manage the storage medium of the data to meet different query scenarios and reduce the latency. For example, user could put latest data on SSD and historical data which is not used frequently on HDD, user will get low latency when querying latest data while get high latency when query historical data which is normal because it needs scan large scale data.
+GCC 5.3.1+，Oracle JDK 1.8+，Python 2.7+, Apache Maven 3.5.4+
 
-In the following figure, user alters partition 'p201601' storage_medium to SSD and storage_cooldown_time to '2016-07-01 00:00:00'. The setting means data in this partition will be put on SSD and it will start to migrate to HDD after the time of storage_cooldown_time.
+* For Ubuntu: 
 
-```SQL
-ALTER TABLE example_tbl MODIFY PARTITION p201601
-SET (""storage_medium"" = ""SSD"", ""storage_cooldown_time"" = ""2016-07-01 00:00:00"");
+```
+sudo apt-get install g++ ant cmake zip byacc flex automake libtool binutils-dev libiberty-dev bison python2.7 libncurses5-dev
+sudo updatedb
 ```
 
-#### 4.5 Vectorized Query Execution
-
-Runtime code generation using LLVM is one of the techniques employed extensively by Impala to improve query execution times. Performance could gains of 5X or more are typical for representative workloads.
-
-But, runtime code generation is not suitable for low latency query, because the generation overhead costs about 100ms. Runtime code generation is more suitable for large-scale ad-hoc query. To accelerate the small queries (of course, big queries will also obtain benefits), we introduced vectorized query execution into Doris.
+* For CentOS:
 
-Vectorized query execution is a feature that greatly reduces the CPU usage for typical query operations like scans, filters, aggregates, and joins. A standard query execution system processes one row at a time. This involves long code paths and significant metadata interpretation in the inner loop of execution. Vectorized query execution streamlines operations by processing a block of many rows at a time. Within the block, each column is stored as a vector (an array of a primitive data type). Simple operations like arithmetic and comparisons are done by quickly iterating through the vectors in a tight loop, with no or very few function calls or conditional branches inside the loop. These loops compile in a streamlined way that uses relatively few instructions and finishes each instruction in fewer clock cycles, on average, by effectively using the processor pipeline and cache memory.
+```
+sudo yum install gcc-c++ libstdc++-static ant cmake byacc flex automake libtool binutils-devel bison ncurses-devel
+sudo updatedb
+```
 
-The result of benchmark shows 2x~4x speedup in our typical queries.
+If your GCC version less than 5.3.1, you can run:
 
-## 5.   Backup and Recovery
+```
+sudo yum install devtoolset-4-toolchain -y
+```
 
-Data backup function is provided to enhance data security. The minimum granularity of backup and recovery is partition.  Users can develop plugins to backup data to any specified remote storage. The backup data can always be recovered to Doris at all time, to achieve the data rollback purpose.
+and then, set the path of gcc (e.g /opt/rh/devtoolset-4/root/usr/bin) to the environment variabl PATH.
 
-Currently we only support full data backup data rather than incremental backups for the following reasons:
 
-1. Remote storage system is beyond the control of the Doris system. We cannot guarantee whether the data has been changed between two backup operations. And data verification operations always come at a high price.
+#### Compile and install
 
-2. We support data backup on partition granularity. And majority of applications are time series applications. By dividing data using time column, it has been able to meet the needs of the vast majority of incremental backup in chronological order.
+Run following script, it will comiple thirdparty libraries and build whole Doris.
 
-In addition to improving data security, the backup function also provides a way to export the data. Data can be exported to other downstream systems for further processing.
+```
+sh build.sh
+```
 
-# Install
-Doris only supports Linux System. Oracle JDK 8.0+ and GCC 4.8.2+ are required. See the document of [INSTALL](https://github.com/apache/incubator-doris/wiki/Doris-Install) and [Deploy & Update](https://github.com/apache/incubator-doris/wiki/Doris-Deploy-%26-Upgrade)
+After successful build, it will install binary files to the path of output/.
 
-# User Guide
-See the [Doris Wiki](https://github.com/apache/incubator-doris/wiki) for more information.
+## 5. Reporting Issues
 
-# Contact us
-<dev@doris.apache.org>
+If you find any bugs, please file a [GitHub issue](https://github.com/apache/incubator-doris/issues).
 
-需要加入Doris微信技术讨论群的,请加微信号：morningman-cmy, 然后备注一下：加入Doris技术讨论群
-（Doris开源一群已满）
+## 6. Links
 
-# Blogs
-1. [浅谈从Google Mesa到Apache Doris](http://neoremind.com/2017/09/%E6%B5%85%E8%B0%88%E4%BB%8Egoogle-mesa%E5%88%B0%E7%99%BE%E5%BA%A6palo/comment-page-1/)
-2. [Apache Kylin VS Apache Doris](https://blog.bcmeng.com/post/apache-kylin-vs-baidu-palo.html)
+* Doris official site - <http://doris.incubator.apache.org>
+* User Manual (GitHub Wiki) - <https://github.com/apache/incubator-doris/wiki>
+* Developer Mailing list - Subscribe to <dev@doris.incubator.apache.org> to discuss with us.
+* Gitter channel - <https://gitter.im/apache-doris/Lobby> - Online chat room with Doris developers.
+* Overview - <https://github.com/apache/incubator-doris/wiki/Doris-Overview>","[{'comment': 'Overview - <https://github.com/apache/incubator-doris/wiki/Doris-Overview>\r\nadd it to the frontend of README.md?', 'commenter': 'morningman'}]"
359,fe/src/main/java/org/apache/doris/planner/StreamLoadScanNode.java,"@@ -329,24 +329,13 @@ private void finalizeParams() throws UserException {
     }
 
     private Expr castToSlot(SlotDescriptor slotDesc, Expr expr) throws UserException {
-        if (slotDesc.getType().isNull()) {
-            return expr;
-        }
         PrimitiveType dstType = slotDesc.getType().getPrimitiveType();
         PrimitiveType srcType = expr.getType().getPrimitiveType();
-        if (dstType.isStringType()) {
-            if (srcType.isStringType()) {
-                return expr;
-            } else {
-                CastExpr castExpr = (CastExpr)expr.castTo(Type.VARCHAR);
-                return castExpr;
-            }
-        } else if (dstType != srcType) {
-            CastExpr castExpr = (CastExpr)expr.castTo(slotDesc.getType());
-            return castExpr;
+        if (dstType != srcType) {","[{'comment': 'why you delete origin `stringType` check\r\n\r\n`stringType` check is used to make work with `char` and `varchar`, do you have test your path with `char` and `varchar` type with convert function which return is not these type?', 'commenter': 'imay'}, {'comment': ""It won't produce CastExpr, when source and target are char or varchar, they are all treated as String in CastFunction. Expr has handled it in member function 'castTo'.  And i have tested cases that table's attribute types contain char or varchar."", 'commenter': 'chenhao7253886'}]"
368,gensrc/thrift/FrontendService.thrift,"@@ -531,6 +531,22 @@ struct TLoadTxnRollbackResult {
     1: required Status.TStatus status
 }
 
+struct TRLTaskCommitRequest {
+    1: required string taskId
+    2: required i64 signature
+    3: required i64 backendId
+    4: required TLoadTxnCommitRequest loadTxnCommitRequest
+    // kafka progress such as {""partitionIdToOffset"":{""1"":100,""2"",200}}
+    5: required string progress","[{'comment': 'you should define a struct for Kafka load progress', 'commenter': 'imay'}, {'comment': 'Yes, there is a kafka progress, but not here. ', 'commenter': 'EmmyMiao87'}]"
368,gensrc/thrift/FrontendService.thrift,"@@ -531,6 +531,22 @@ struct TLoadTxnRollbackResult {
     1: required Status.TStatus status
 }
 
+struct TRLTaskCommitRequest {
+    1: required string taskId
+    2: required i64 signature
+    3: required i64 backendId","[{'comment': 'why backendId is **required**?', 'commenter': 'imay'}, {'comment': 'I need to remove task for `AgentTaskQueue`. And the params of removeTask need backendId', 'commenter': 'EmmyMiao87'}]"
368,gensrc/thrift/FrontendService.thrift,"@@ -531,6 +531,22 @@ struct TLoadTxnRollbackResult {
     1: required Status.TStatus status
 }
 
+struct TRLTaskCommitRequest {","[{'comment': ""I think you should put this request to `TLoadTxnCommitRequest` as an optional field. Then you needn't define a new RPC interface. In addition, if you add new type of RLTask, you just add new option field."", 'commenter': 'imay'}, {'comment': 'For me, all of type of RLTask will commit task by `This method`.  The new option field is in `TRLTaskCommitRequest` not in `TLoadTxnCommitRequest`. Txn just is  a property in RLTask instead of RLTask is a property in Txn.  ', 'commenter': 'EmmyMiao87'}, {'comment': 'We only commit transaction, task can be finished through callback after transaction commit process.', 'commenter': 'imay'}]"
368,fe/src/main/java/org/apache/doris/load/routineload/KafkaRoutineLoadJob.java,"@@ -36,6 +37,9 @@
 import java.util.Properties;
 import java.util.UUID;
 
+/**
+ * The progress of KafkaRoutineLoadJob such as ""{""partition1"": offset1, ""partition2"": offset2}""","[{'comment': 'Is this the description of class `KafkaRoutineLoadJob`??', 'commenter': 'morningman'}]"
368,fe/src/main/java/org/apache/doris/transaction/TransactionState.java,"@@ -59,11 +60,13 @@ public static LoadJobSourceType valueOf(int flag) {
                     return BACKEND_STREAMING;
                 case 3:
                     return INSERT_STREAMING;
+                case 4:
+                    return ROUTINE_LOAD_TASK;
                 default:
                     return null;
             }
         }
-        
+","[{'comment': 'You miss the  toString() method', 'commenter': 'morningman'}]"
368,fe/src/main/java/org/apache/doris/transaction/TransactionState.java,"@@ -212,6 +219,7 @@ public void readFields(DataInput in) throws IOException {
         for (int i = 0; i < errorReplicaNum; ++i) {
             errorReplicas.add(in.readLong());
         }
+        extra = Text.readString(in);","[{'comment': 'If you are adding a new member in edit log, you have to lift the meta version.\r\nSee Catalog.getCurrentCatalogJournalVersion() usage.', 'commenter': 'morningman'}]"
368,fe/src/main/java/org/apache/doris/transaction/TransactionState.java,"@@ -92,13 +95,16 @@ public String toString() {
     private String reason;
     private Set<Long> errorReplicas;
     private CountDownLatch latch;
-    
+
     // this state need not to be serialized
     private Map<Long, PublishVersionTask> publishVersionTasks;
     private boolean hasSendTask;
     private long publishVersionTime;
     private TransactionStatus preStatus = null;
-    
+
+    // optional
+    private String extra;","[{'comment': ""You'd better describe what exactly this member means."", 'commenter': 'morningman'}]"
368,fe/src/main/java/org/apache/doris/transaction/GlobalTransactionMgr.java,"@@ -70,7 +70,7 @@
  * 1. begin
  * 2. commit
  * 3. abort
- * 
+ * <p>","[{'comment': ""<p> what's this?"", 'commenter': 'morningman'}]"
368,fe/src/main/java/org/apache/doris/transaction/GlobalTransactionMgr.java,"@@ -356,10 +357,23 @@ public void commitTransaction(long dbId, long transactionId, List<TabletCommitIn
         // 6. update nextVersion because of the failure of persistent transaction resulting in error version
         updateCatalogAfterCommitted(transactionState, db);
         LOG.info(""transaction:[{}] successfully committed"", transactionState);
+
+        // TODO: call updateCatalogAfterCommitted by transactionCallback without if
+        if (transactionState.getSourceType() == LoadJobSourceType.ROUTINE_LOAD_TASK) {
+            try {
+                Catalog.getCurrentCatalog().getRoutineLoadInstance().getJobByTaskId(transactionState.getLabel())
+                        .updateCatalogAfterCommitted(transactionState);
+            } catch (MetaNotFoundException e) {
+                LOG.error(""failed to update routine load task {} after committed txn {}"",","[{'comment': 'use warning log level.\r\nerror level means the process should exit.', 'commenter': 'morningman'}, {'comment': 'Yes, it is. If update routine load task failed after committed txn, it means that offset in routine load is in back of data in palo', 'commenter': 'EmmyMiao87'}]"
368,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoadJob.java,"@@ -241,4 +303,99 @@ private void checkStateTransform(RoutineLoadJob.JobState currentState, RoutineLo
         }
     }
 
+    private void loadTxnCommit(TLoadTxnCommitRequest request) throws TException {
+        FrontendServiceImpl frontendService = new FrontendServiceImpl(ExecuteEnv.getInstance());
+        frontendService.loadTxnCommit(request);
+    }
+
+    private void updateNumOfData(int numOfErrorData, int numOfTotalData) {
+        currentErrorNum += numOfErrorData;
+        currentTotalNum += numOfTotalData;
+        if (currentTotalNum > 10000) {
+            if (currentErrorNum > maxErrorNum) {
+                LOG.info(""current error num {} of job {} is more then max error num {}. begin to pause job"",
+                        currentErrorNum, id, maxErrorNum);
+                // remove all of task in jobs and change job state to paused
+                // TODO(ml): edit log
+                state = JobState.PAUSED;
+                routineLoadTaskInfoList.clear();
+                needSchedulerTaskInfoList.clear();
+
+            }
+
+            // reset currentTotalNum and currentErrorNum
+            currentErrorNum = 0;
+            currentTotalNum = 0;
+        } else if (currentErrorNum > maxErrorNum) {
+            LOG.info(""current error num {} of job {} is more then max error num {}. begin to pause job"",
+                    currentErrorNum, id, maxErrorNum);
+            // remove all of task in jobs and change job state to paused
+            // TODO(ml): edit log
+            state = JobState.PAUSED;
+            routineLoadTaskInfoList.clear();
+            needSchedulerTaskInfoList.clear();
+            // reset currentTotalNum and currentErrorNum
+            currentErrorNum = 0;
+            currentTotalNum = 0;
+        }
+    }
+
+    abstract RoutineLoadTaskInfo reNewTask(RoutineLoadTaskInfo routineLoadTaskInfo) throws AnalysisException,
+            LabelAlreadyExistsException, BeginTransactionException;
+
+    @Override
+    public boolean checkTxnHasRelatedJob(TransactionState txnState) {","[{'comment': ""should this method name be called 'checkTxnHasRelatedTask()'?"", 'commenter': 'morningman'}, {'comment': 'This check works for broker load job also. If I change to task , I think it is confused for broker load job', 'commenter': 'EmmyMiao87'}]"
368,fe/src/main/java/org/apache/doris/transaction/GlobalTransactionMgr.java,"@@ -766,6 +781,16 @@ private boolean checkTxnHasRelatedJob(TransactionState txnState, Map<Long, Set<L
             return true;
         }
 
+        if (txnState.getSourceType() == LoadJobSourceType.ROUTINE_LOAD_TASK) {
+            try {
+                return Catalog.getCurrentCatalog().getRoutineLoadInstance().getJobByTaskId(txnState.getLabel())
+                        .checkTxnHasRelatedJob(txnState);
+            } catch (MetaNotFoundException e) {
+                // could not find job by task id while task has been deleted","[{'comment': 'If failed to failed job by task id, this should return false, i think?', 'commenter': 'morningman'}]"
368,fe/src/main/java/org/apache/doris/service/FrontendServiceImpl.java,"@@ -666,6 +672,12 @@ private boolean loadTxnCommitImpl(TLoadTxnCommitRequest request) throws UserExce
             throw new UserException(""unknown database, database="" + dbName);
         }
 
+        // update extra in transaction state
+        if (!Strings.isNullOrEmpty(request.getExtra())) {","[{'comment': 'use request.isSetExtra() to check if the member is set.', 'commenter': 'morningman'}]"
368,gensrc/thrift/FrontendService.thrift,"@@ -510,6 +510,7 @@ struct TLoadTxnCommitRequest {
     7: required i64 txnId
     8: required bool sync
     9: optional list<Types.TTabletCommitInfo> commitInfos
+    10: optional string extra","[{'comment': ""It's a bad idea to use string,  use another thrift struct to define your need"", 'commenter': 'imay'}, {'comment': 'But if I use a thrift struct which routine load task need, this is not useful for other transaction', 'commenter': 'EmmyMiao87'}]"
368,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoadJob.java,"@@ -195,25 +221,35 @@ public void processTimeoutTasks() {
                 if ((System.currentTimeMillis() - routineLoadTaskInfo.getLoadStartTimeMs())
                         > DEFAULT_TASK_TIMEOUT_SECONDS * 1000) {
                     String oldSignature = routineLoadTaskInfo.getId();
-                    if (routineLoadTaskInfo instanceof KafkaTaskInfo) {
-                        // remove old task
-                        routineLoadTaskInfoList.remove(routineLoadTaskInfo);
-                        // add new task
-                        KafkaTaskInfo kafkaTaskInfo = new KafkaTaskInfo((KafkaTaskInfo) routineLoadTaskInfo);
-                        routineLoadTaskInfoList.add(kafkaTaskInfo);
-                        needSchedulerTaskInfoList.add(kafkaTaskInfo);
+                    // abort txn if not committed
+                    try {
+                        Catalog.getCurrentGlobalTransactionMgr()
+                                .abortTransaction(routineLoadTaskInfo.getTxnId(), ""routine load task of txn was timeout"");
+                    } catch (UserException e) {
+                        if (e.getMessage().contains(""committed"")) {
+                            LOG.debug(""txn of task {} has been committed, timeout task has been ignored"", oldSignature);
+                            continue;
+                        }
+                    }
+
+                    try {
+                        result.add(reNewTask(routineLoadTaskInfo));
+                        LOG.debug(""Task {} was ran more then {} minutes. It was removed and rescheduled"",
+                                oldSignature, DEFAULT_TASK_TIMEOUT_SECONDS);
+                    } catch (UserException e) {
+                        state = JobState.CANCELLED;
+                        // TODO(ml): edit log","[{'comment': 'do you need to make some clean tasks when job is being cancelled?', 'commenter': 'morningman'}]"
368,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoadJob.java,"@@ -229,8 +265,34 @@ public void readFields(DataInput in) throws IOException {
         // TODO(ml)
     }
 
-    abstract RoutineLoadTask createTask(RoutineLoadTaskInfo routineLoadTaskInfo, long beId);
 
+    public void removeNeedSchedulerTask(RoutineLoadTaskInfo routineLoadTaskInfo) {
+        writeLock();
+        try {
+            needSchedulerTaskInfoList.remove(routineLoadTaskInfo);","[{'comment': ""I don't find that you implement the equals() method in RoutineLoadTaskInfo class. So is that correct to call List.remove()?"", 'commenter': 'morningman'}]"
368,fe/src/main/java/org/apache/doris/load/TransactionCallBack.java,"@@ -0,0 +1,39 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.load;
+
+import org.apache.doris.transaction.TransactionState;
+
+public interface TransactionCallBack {
+
+    /**
+     * check txn has related job
+     * return true if txn has related job, return false if job of txn has been deleted
+     *
+     * @param txnState
+     * @return
+     */
+    boolean checkTxnHasRelatedJob(TransactionState txnState);
+
+    /**
+     * update catalog of job which has related txn after transaction has been committed
+     *
+     * @param txnState
+     */
+    void updateCatalogAfterCommitted(TransactionState txnState);","[{'comment': ""Usually, callback function's name is like `onCommit`, `onAbort`"", 'commenter': 'imay'}]"
368,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoadJob.java,"@@ -229,8 +265,34 @@ public void readFields(DataInput in) throws IOException {
         // TODO(ml)
     }
 
-    abstract RoutineLoadTask createTask(RoutineLoadTaskInfo routineLoadTaskInfo, long beId);
 
+    public void removeNeedSchedulerTask(RoutineLoadTaskInfo routineLoadTaskInfo) {
+        writeLock();
+        try {
+            needSchedulerTaskInfoList.remove(routineLoadTaskInfo);
+        } finally {
+            writeUnlock();
+        }
+    }
+
+    abstract void updateProgress(String progress);
+
+    @Nullable
+    public RoutineLoadJob getJobByTaskId(String taskId) {","[{'comment': 'I think this method should be called: containsTask() and return boolean?\r\nIt is so weird to call a RoutineLoadJob. getJobByTaskId() and return null?', 'commenter': 'morningman'}]"
368,fe/src/main/java/org/apache/doris/transaction/GlobalTransactionMgr.java,"@@ -766,6 +781,16 @@ private boolean checkTxnHasRelatedJob(TransactionState txnState, Map<Long, Set<L
             return true;
         }
 
+        if (txnState.getSourceType() == LoadJobSourceType.ROUTINE_LOAD_TASK) {","[{'comment': ""It's not good to judge type here. You can put a callback when you begin a transaction, and then when transaction state is changed, your callback function will be called"", 'commenter': 'imay'}]"
368,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoadJob.java,"@@ -241,4 +303,99 @@ private void checkStateTransform(RoutineLoadJob.JobState currentState, RoutineLo
         }
     }
 
+    private void loadTxnCommit(TLoadTxnCommitRequest request) throws TException {
+        FrontendServiceImpl frontendService = new FrontendServiceImpl(ExecuteEnv.getInstance());
+        frontendService.loadTxnCommit(request);","[{'comment': 'Are you sure to call the method of FrontendServiceImpl in RountineLoadJob?\r\nI think you misunderstanding the hierarchical design.', 'commenter': 'morningman'}]"
368,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoadJob.java,"@@ -241,4 +303,99 @@ private void checkStateTransform(RoutineLoadJob.JobState currentState, RoutineLo
         }
     }
 
+    private void loadTxnCommit(TLoadTxnCommitRequest request) throws TException {
+        FrontendServiceImpl frontendService = new FrontendServiceImpl(ExecuteEnv.getInstance());
+        frontendService.loadTxnCommit(request);
+    }
+
+    private void updateNumOfData(int numOfErrorData, int numOfTotalData) {
+        currentErrorNum += numOfErrorData;
+        currentTotalNum += numOfTotalData;
+        if (currentTotalNum > 10000) {","[{'comment': 'change the 10000 to a Configuration or a static final variable, or you will forget to change it later.', 'commenter': 'morningman'}]"
368,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoadJob.java,"@@ -241,4 +303,99 @@ private void checkStateTransform(RoutineLoadJob.JobState currentState, RoutineLo
         }
     }
 
+    private void loadTxnCommit(TLoadTxnCommitRequest request) throws TException {
+        FrontendServiceImpl frontendService = new FrontendServiceImpl(ExecuteEnv.getInstance());
+        frontendService.loadTxnCommit(request);
+    }
+
+    private void updateNumOfData(int numOfErrorData, int numOfTotalData) {
+        currentErrorNum += numOfErrorData;
+        currentTotalNum += numOfTotalData;
+        if (currentTotalNum > 10000) {
+            if (currentErrorNum > maxErrorNum) {
+                LOG.info(""current error num {} of job {} is more then max error num {}. begin to pause job"",
+                        currentErrorNum, id, maxErrorNum);
+                // remove all of task in jobs and change job state to paused
+                // TODO(ml): edit log
+                state = JobState.PAUSED;
+                routineLoadTaskInfoList.clear();
+                needSchedulerTaskInfoList.clear();
+
+            }
+
+            // reset currentTotalNum and currentErrorNum
+            currentErrorNum = 0;
+            currentTotalNum = 0;
+        } else if (currentErrorNum > maxErrorNum) {
+            LOG.info(""current error num {} of job {} is more then max error num {}. begin to pause job"",
+                    currentErrorNum, id, maxErrorNum);
+            // remove all of task in jobs and change job state to paused
+            // TODO(ml): edit log
+            state = JobState.PAUSED;
+            routineLoadTaskInfoList.clear();
+            needSchedulerTaskInfoList.clear();
+            // reset currentTotalNum and currentErrorNum
+            currentErrorNum = 0;
+            currentTotalNum = 0;
+        }
+    }
+
+    abstract RoutineLoadTaskInfo reNewTask(RoutineLoadTaskInfo routineLoadTaskInfo) throws AnalysisException,
+            LabelAlreadyExistsException, BeginTransactionException;
+
+    @Override
+    public boolean checkTxnHasRelatedJob(TransactionState txnState) {
+        readLock();
+        try {
+            String taskId = txnState.getLabel();
+            if (routineLoadTaskInfoList.parallelStream().filter(entity -> entity.getId().equals(taskId)).count() < 1) {","[{'comment': 'You can use anyMatch()', 'commenter': 'morningman'}]"
368,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoadTask.java,"@@ -27,6 +27,7 @@
 public class RoutineLoadTask extends AgentTask {","[{'comment': 'This class should be placed in package org.apache.doris.task', 'commenter': 'morningman'}]"
368,fe/src/main/java/org/apache/doris/load/routineload/KafkaRoutineLoadTask.java,"@@ -31,9 +31,9 @@
     public KafkaRoutineLoadTask(TResourceInfo resourceInfo, long backendId,
                                 long dbId, long tableId, long partitionId, long indexId, long tabletId,
                                 String columns, String where, String columnSeparator,
-                                KafkaTaskInfo kafkaTaskInfo, KafkaProgress kafkaProgress) {
+                                KafkaTaskInfo kafkaTaskInfo, KafkaProgress kafkaProgress, long txnId) {","[{'comment': 'This class should be placed in package org.apache.doris.task', 'commenter': 'morningman'}]"
370,be/src/agent/pusher.cpp,"@@ -146,11 +146,11 @@ AgentStatus Pusher::_download_file() {
         rate = (double) _push_req.http_file_size / cost / 1024;
     }
     if (status == DORIS_SUCCESS) {
-        OLAP_LOG_INFO(""down load file success. local_file=%s, remote_file=%s, ""
-                      ""tablet=%d, cost=%ld, file size: %ld B, download rate: %f KB/s"",
-                _downloader_param.local_file_path.c_str(),
-                _downloader_param.remote_file_path.c_str(),
-                _push_req.tablet_id, cost, _push_req.http_file_size, rate);
+        LOG(INFO) << ""down load file success. local_file="" << _downloader_param.local_file_path << "", ""
+                  << ""remote_file="" << _downloader_param.remote_file_path << "", ""
+                  << ""tablet_id"" << _push_req.tablet_id << "", ""","[{'comment': '```suggestion\r\n                  << "", tablet_id"" << _push_req.tablet_id \r\n```', 'commenter': 'imay'}]"
370,be/src/olap/base_compaction.cpp,"@@ -83,10 +83,9 @@ OLAPStatus BaseCompaction::init(OLAPTablePtr table, bool is_manual_trigger) {
 }
 
 OLAPStatus BaseCompaction::run() {
-    OLAP_LOG_INFO(""start base compaction. [table=%s; old_base_version=%d; new_base_version=%d]"",
-                  _table->full_name().c_str(),
-                  _old_base_version.second,
-                  _new_base_version.second);
+    LOG(INFO) << ""start base compaction. tablet="" << _table->full_name() << "", ""
+              << ""old_base_version="" << _old_base_version.second << "", ""","[{'comment': '```suggestion\r\n              << "", old_base_version="" << _old_base_version.second \r\n```', 'commenter': 'imay'}]"
373,fs_brokers/apache_hdfs_broker/src/main/java/org/apache/doris/broker/hdfs/FileSystemManager.java,"@@ -91,6 +82,25 @@ public FileSystemManager() {
         writeBufferSize = BrokerConfig.hdfs_write_buffer_size_kb << 10;
         handleManagementPool.schedule(new FileSystemExpirationChecker(), 0, TimeUnit.SECONDS);
     }
+
+    private static String preparePrincipal(String originalPrincipal) throws UnknownHostException {
+        String finalPrincipal = originalPrincipal;
+        String[] components = originalPrincipal.split(""[/@]"");
+        if (components != null && components.length == 3) {
+            if (components[1].equals(""_HOST"")) {
+                // Convert hostname(fqdn) to lower case according to SecurityUtil.getServerPrincipal","[{'comment': 'delete unuseful comment?\r\n\r\n', 'commenter': 'wuyunfeng'}, {'comment': ""I don't  have an environmental to test the case with the principal of the hostname(fqdn), so I think it's better to keep the comment for reminders. "", 'commenter': 'zhaidongbo'}]"
387,be/src/agent/task_worker_pool.cpp,"@@ -1042,7 +1042,7 @@ void* TaskWorkerPool::_clone_worker_thread_callback(void* arg_this) {
         string src_file_path;
         TBackend src_host;
         // Check local tablet exist or not
-        OLAPTablePtr tablet =
+        TabletPtr tablet =","[{'comment': 'TabletSharedPtr', 'commenter': 'chaoyli'}, {'comment': 'done', 'commenter': 'yiguolei'}]"
390,env.sh,"@@ -31,23 +31,6 @@ if [[ -z ${DORIS_THIRDPARTY} ]]; then
     export DORIS_THIRDPARTY=${DORIS_HOME}/thirdparty
 fi
 
-# check java version","[{'comment': 'we need to check Java and maven version here', 'commenter': 'imay'}, {'comment': 'I have seen the build script, the check java also in build.sh.  Why you build jdk after check java in the build-thirdparty.sh? And I have test the build after I delete this. You can test it in your environment.', 'commenter': 'uestctxq'}, {'comment': ""JAVA_HOME can be set by user in his environment, and not everyone use thirdparty's JDK as his JAVA_HOME.\r\nSo, we need to check java's version to make sure it can work with our project."", 'commenter': 'imay'}]"
399,be/src/rowset/rowset.h,"@@ -0,0 +1,47 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#ifndef DORIS_BE_SRC_ROWSET_ROWSET_H
+#define DORIS_BE_SRC_ROWSET_ROWSET_H
+
+#include ""olap/olap_define.h""
+#include ""rowset/rowset_reader.h""
+#include ""rowset/rowset_writer.h""
+
+#include <memory>
+
+namespace doris {
+
+class Rowset {
+public:
+    // 初始化Rowset
+    OLAPStatus init(RowsetMeta rowset_meta, const DataDir* dir) = 0;","[{'comment': 'OLAPStatus is deprecated. use NewStatus Instead.', 'commenter': 'chaoyli'}]"
399,be/src/rowset/rowset_meta.h,"@@ -0,0 +1,161 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#ifndef DORIS_BE_SRC_ROWSET_ROWSET_META_H
+#define DORIS_BE_SRC_ROWSET_ROWSET_META_H
+
+#include ""olap/olap_define.h""
+#include ""rowset/rowset_reader.h""
+#include ""rowset/rowset_writer.h""
+#include ""gen_cpp/olap_file.pb.h""
+
+#include <memory>
+#include <vector>
+
+namespace doris {
+
+class Rowset {
+public:
+    virtual void init(const RowsetMetaPb& rowset_meta) {
+        _rowset_meta = rowset_meta;
+    }
+
+    virtual bool deserialize_extra_properties()  = 0;
+
+    virtual void get_rowset_meta_pb(RowsetMetaPb* rowset_meta) {
+        rowset_meta = &_rowset_meta;
+    }
+    virtual int64_t get_rowset_id() {
+        return _rowset_meta.rowset_id();
+    }
+
+    virtual void set_rowset_id(int64_t rowset_id) {
+        _rowset_meta.set_rowset_id(rowset_id);
+    }
+
+    virtual int64_t get_version() {
+        return _rowset_meta->version();
+    }
+
+    virtual void set_version(int64_t version) {
+        _rowset_meta->set_version(version);
+    }
+
+    virtual int64_t get_tablet_id() {
+        return _rowset_meta->tablet_id();
+    }
+
+    virtual void set_tablet_id(int64_t tablet_id) {
+        _rowset_meta->set_tablet_id(tablet_id);
+    }
+
+    virtual int32_t get_tablet_schema_hash() {
+        return _rowset_meta->tablet_schema_hash();
+    }
+
+    virtual void set_tablet_schema_hash(int64_t tablet_schema_hash) {
+        _rowset_meta->set_tablet_schema_hash(tablet_schema_hash);
+    }
+
+    virtual RowsetType get_rowset_type() {
+        return _rowset_meta->rowset_type();
+    }
+
+    virtual void set_rowset_type(RowsetType rowset_type) {
+        _rowset_meta->set_rowset_type(rowset_type);
+    }
+
+    virtual RowsetState get_rowset_state() {
+        return _rowset_meta->rowset_state();
+    }
+
+    virtual void set_rowset_state(RowsetState rowset_state) {
+        _rowset_meta->set_rowset_state(rowset_state);
+    }
+
+    virtual int get_start_version() {
+        return _rowset_meta->start_version();
+    }
+
+    virtual void set_start_version(int start_version) {
+        _rowset_meta->set_start_version(start_version);
+    }
+    
+    virtual int get_end_version() {
+        return _rowset_meta->end_version();
+    }
+
+    virtual void set_end_version(int end_version) {
+        _rowset_meta->set_end_version(end_version);
+    }
+    
+    virtual int get_row_number() {
+        return _rowset_meta->row_number();
+    }
+
+    virtual void set_row_number(int row_number) {
+        _rowset_meta->set_row_number(row_number);
+    }
+
+    virtual int get_total_disk_size() {
+        return _rowset_meta->total_disk_size();
+    }
+
+    virtual void set_total_disk_size(int total_disk_size) {
+        _rowset_meta->set_total_disk_size(total_disk_size);
+    }
+
+    virtual int get_data_disk_size() {
+        return _rowset_meta->data_disk_size();
+    }
+
+    virtual void set_data_disk_size(int data_disk_size) {
+        _rowset_meta->set_data_disk_size(data_disk_size);
+    }
+
+    virtual int get_index_disk_size() {
+        return _rowset_meta->index_disk_size();
+    }
+
+    virtual void set_index_disk_size(int index_disk_size) {
+        _rowset_meta->set_index_disk_size(index_disk_size);
+    }
+
+    virtual void get_column_statistics(std::vector<ColumnPruning>* column_statistics) {
+        *column_statistics = _rowset_meta->column_statistics();
+    }
+
+    virtual void set_column_statistics(std::vector<ColumnPruning> column_statistics) {
+        std::vector<ColumnPruning>* new_column_statistics = _rowset_meta.mutable_column_pruning();
+        *new_column_statistics = column_statistics;
+    }
+
+    virtual DeleteConditionMessage get_delete_condition() {
+        return _rowset_meta->delete_condition();
+    }
+
+    virtual void set_delete_condition(DeleteConditionMessage delete_condition) {
+        _rowset_meta->set_delete_condition(delete_condition);
+    }
+
+private:
+    RowsetMetaPb _rowset_meta;","[{'comment': 'We shoulde have a RowsetMeta class, it encapsulate RowsetMetaPB. In future,  we may not use protobuf as meta storage type, so RowsetMeta may be a better abstract.', 'commenter': 'chaoyli'}, {'comment': 'the class is RowsetMeta, it is a misspell', 'commenter': 'kangpinghuang'}]"
399,gensrc/proto/olap_file.proto,"@@ -58,6 +58,39 @@ message PDelta {
     optional DeleteConditionMessage delete_condition = 6;
 }
 
+enum RowsetType {
+    ALPHA_ROWSET = 0; // doris原有的列存格式
+    BETA_ROWSET  = 1; // 新列存
+}
+
+enum RowsetState {
+    PREPARING = 0; // 表示正在写入Rowset
+    COMMITTED = 1; // 表示rowset 写入完成，但是用户还不可见；这个状态下的rowset，BE不能自行判断是否删除，必须由FE的指令
+    VISIBLE = 2; // 表示rowset 已经对用户可见
+}
+
+message RowsetMetaPb {
+    required int64 rowset_id = 1;
+    // RowsetMeta update version
+    required int64 version = 2;","[{'comment': '1. why update version is need? \r\n2. RowsetMeta also has txn_id.', 'commenter': 'chaoyli'}, {'comment': 'update version is for indicating which RowsetMetaPb is new.\r\ntxn_id is for transaction.', 'commenter': 'kangpinghuang'}]"
399,be/src/rowset/rowset_writer.h,"@@ -0,0 +1,49 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#ifndef DORIS_BE_SRC_ROWSET_ROWSET_WRITER_H
+#define DORIS_BE_SRC_ROWSET_ROWSET_WRITER_H
+
+#include ""rowset/rowset.h""
+#include ""olap/olap_define.h""
+#include ""olap/schema.h""
+#include ""olap/row_block.h""
+
+namespace doris {
+
+class RowsetBuilder {","[{'comment': 'In rowset_writer.h, RowsetWriter may be a better name than RowsetBuilder？', 'commenter': 'chaoyli'}, {'comment': 'I thin RowsetBuilder is better， because it not only write rowset data, but also returns rowset object', 'commenter': 'kangpinghuang'}]"
399,gensrc/proto/olap_file.proto,"@@ -58,6 +58,39 @@ message PDelta {
     optional DeleteConditionMessage delete_condition = 6;","[{'comment': 'I think delete_predicate may be a better name.', 'commenter': 'chaoyli'}]"
399,be/src/rowset/rowset_reader.h,"@@ -0,0 +1,59 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#ifndef DORIS_BE_SRC_ROWSET_ROWSET_READER_H
+#define DORIS_BE_SRC_ROWSET_ROWSET_READER_H
+
+#include ""olap/olap_define.h""
+#include ""olap/schema.h""
+#include ""olap/column_predicate.h""
+#include ""olap/row_cursor.h""
+#include ""olap/row_block.h""
+
+#include <memory>
+
+namespace doris {
+
+struct ReadContext {
+	const Schema* projection; // 投影列信息
+    std::unordered_map<std::string, ColumnPredicate> predicates; //过滤条件
+    const RowCursor* lower_bound_key; // key下界
+    const RowCursor* exclusive_upper_bound_key; // key上界
+};
+
+class RowsetReader {
+public:
+    // reader初始化函数
+    // init逻辑中需要判断rowset是否可以直接被统计信息过滤删除、是否被删除条件删除
+    // 如果被删除，则has_next直接返回false
+    OLAPStatus init(ReadContext* read_context) = 0;
+
+    // 判断是否还有数据
+    bool has_next() = 0;
+
+	// 读取下一个Block的数据
+    OLAPStatus next_block(RowBlock* row_block) = 0;
+
+   // 关闭reader
+   // 会触发下层数据读取逻辑的close操作，进行类似关闭文件，
+   // 更新统计信息等操作
+	void close() = 0;","[{'comment': 'Annotation may be not correct. FileDescriptor is managed by file_descriptor_cache.', 'commenter': 'chaoyli'}]"
399,be/src/rowset/rowset_reader.h,"@@ -0,0 +1,59 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#ifndef DORIS_BE_SRC_ROWSET_ROWSET_READER_H
+#define DORIS_BE_SRC_ROWSET_ROWSET_READER_H
+
+#include ""olap/olap_define.h""
+#include ""olap/schema.h""
+#include ""olap/column_predicate.h""
+#include ""olap/row_cursor.h""
+#include ""olap/row_block.h""
+
+#include <memory>
+
+namespace doris {
+
+struct ReadContext {
+	const Schema* projection; // 投影列信息
+    std::unordered_map<std::string, ColumnPredicate> predicates; //过滤条件
+    const RowCursor* lower_bound_key; // key下界
+    const RowCursor* exclusive_upper_bound_key; // key上界
+};
+
+class RowsetReader {
+public:
+    // reader初始化函数","[{'comment': 'It may be better to write annotation in English.', 'commenter': 'chaoyli'}]"
399,gensrc/proto/olap_file.proto,"@@ -58,6 +58,39 @@ message PDelta {
     optional DeleteConditionMessage delete_condition = 6;
 }
 
+enum RowsetType {
+    ALPHA_ROWSET = 0; // doris原有的列存格式
+    BETA_ROWSET  = 1; // 新列存
+}
+
+enum RowsetState {
+    PREPARING = 0; // 表示正在写入Rowset
+    COMMITTED = 1; // 表示rowset 写入完成，但是用户还不可见；这个状态下的rowset，BE不能自行判断是否删除，必须由FE的指令
+    VISIBLE = 2; // 表示rowset 已经对用户可见
+}
+
+message RowsetMetaPb {
+    required int64 rowset_id = 1;
+    // RowsetMeta update version
+    required int64 version = 2;
+    optional int64 tablet_id = 3;
+    optional int32 tablet_schema_hash = 4;
+    optional RowsetType rowset_type = 5;
+    optional RowsetState rowset_state = 6;
+    // Rowset data version range
+    optional int32 start_version = 7;
+    optional int32 end_version = 8;
+    optional int32 row_number = 9;
+    optional int64 total_disk_size = 10;
+    optional int64 data_disk_size = 11;
+    optional int64 index_disk_size = 12;
+    // column min/max/null flag statistic info
+    repeated ColumnPruning column_statistics = 13;
+    optional DeleteConditionMessage delete_condition = 14;
+    // spare field id 15-49 for future use
+    optional bytes extra_properties = 50;","[{'comment': 'If necessary, adding new optional field sounds OK. Why setting extra_properties here? ', 'commenter': 'chaoyli'}, {'comment': 'we do not want to modify the RowsetMetaPb message when add a new Rowset Type', 'commenter': 'kangpinghuang'}]"
399,be/src/olap/rowset_reader.h,"@@ -0,0 +1,57 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#ifndef DORIS_BE_SRC_ROWSET_ROWSET_READER_H
+#define DORIS_BE_SRC_ROWSET_ROWSET_READER_H","[{'comment': 'DORIS_BE_SRC_ROWSET_ROWSET_READER_H to  DORIS_BE_SRC_OLAP_ROWSET_READER_H', 'commenter': 'yiguolei'}, {'comment': 'done', 'commenter': 'kangpinghuang'}]"
403,fe/src/main/java/org/apache/doris/catalog/FsBroker.java,"@@ -0,0 +1,129 @@
+package org.apache.doris.catalog;
+
+import org.apache.doris.common.io.Text;
+import org.apache.doris.common.io.Writable;
+import org.apache.doris.system.BrokerHbResponse;
+import org.apache.doris.system.HeartbeatResponse.HbStatus;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+//Licensed to the Apache Software Foundation (ASF) under one","[{'comment': 'license header should be before package and import', 'commenter': 'imay'}, {'comment': 'Done', 'commenter': 'morningman'}]"
403,fe/src/main/java/org/apache/doris/system/Backend.java,"@@ -549,5 +526,48 @@ public DecommissionType getDecommissionType() {
         return DecommissionType.SystemDecommission;
     }
 
+    /*
+     * handle Backend's heartbeat response.
+     * return true if any port changed, or alive state is changed.
+     */
+    public boolean heartbeat(BackendHbResponse hbResponse) {","[{'comment': '```suggestion\r\n    public boolean handleHbResponse(BackendHbResponse hbResponse) {\r\n```', 'commenter': 'imay'}, {'comment': 'Done', 'commenter': 'morningman'}]"
403,fe/src/main/java/org/apache/doris/persist/HbPackage.java,"@@ -0,0 +1,68 @@
+package org.apache.doris.persist;
+
+import org.apache.doris.common.io.Writable;
+import org.apache.doris.system.HeartbeatResponse;
+
+import com.google.common.collect.Lists;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+import java.util.List;
+//Licensed to the Apache Software Foundation (ASF) under one","[{'comment': 'license header position', 'commenter': 'imay'}, {'comment': 'Done', 'commenter': 'morningman'}]"
416,be/src/olap/storage_engine.cpp,"@@ -918,95 +918,36 @@ OLAPStatus StorageEngine::add_tablet(TTabletId tablet_id, SchemaHash schema_hash
 OLAPStatus StorageEngine::add_transaction(","[{'comment': 'transaction and txn is the same. Using one variable name may be better. ', 'commenter': 'chaoyli'}, {'comment': 'transaction related method in storage engine will be removed in the future. They will be replaced with create_xxx_task. Txn related method will not be called outside.', 'commenter': 'yiguolei'}]"
429,be/src/olap/rowset/rowset.h,"@@ -1,45 +1,45 @@
-// Licensed to the Apache Software Foundation (ASF) under one
-// or more contributor license agreements.  See the NOTICE file
-// distributed with this work for additional information
-// regarding copyright ownership.  The ASF licenses this file
-// to you under the Apache License, Version 2.0 (the
-// ""License""); you may not use this file except in compliance
-// with the License.  You may obtain a copy of the License at
-//
-//   http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing,
-// software distributed under the License is distributed on an
-// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-// KIND, either express or implied.  See the License for the
-// specific language governing permissions and limitations
-// under the License.
-
-#ifndef DORIS_BE_SRC_OLAP_ROWSET_H
-#define DORIS_BE_SRC_OLAP_ROWSET_H
-
-#include ""olap/new_status.h""
-#include ""olap/rowset_reader.h""
-#include ""olap/rowset_builder.h""
-
-#include <memory>
-
-namespace doris {
-
-class Rowset {
-public:
-    NewStatus init(const RowsetMeta& rowset_meta, const DataDir* dir) = 0;
-
-    std::unique_ptr<RowsetReader> create_reader() = 0;
-
-    NewStatus copy(RowsetBuilder* dest_rowset_builder) = 0;
-
-    NewStatus delete() = 0;
-
-private:
-    RowsetMeta _rowset_meta;
-};
-
-}
-
-#endif // DORIS_BE_SRC_OLAP_ROWSET_H
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#ifndef DORIS_BE_SRC_OLAP_ROWSET_ROWSET_H
+#define DORIS_BE_SRC_OLAP_ROWSET_ROWSET_H
+
+#include ""olap/new_status.h""
+#include ""olap/rowset/rowset_reader.h""
+#include ""olap/rowset/rowset_builder.h""
+
+#include <memory>
+
+namespace doris {
+
+class Rowset {
+public:
+    NewStatus init(const RowsetMeta& rowset_meta) = 0;
+
+    std::unique_ptr<RowsetReader> create_reader() = 0;
+
+    NewStatus copy(RowsetBuilder* dest_rowset_builder) = 0;","[{'comment': 'no virtual is OK?', 'commenter': 'chaoyli'}, {'comment': 'Fixed', 'commenter': 'kangpinghuang'}]"
429,be/src/olap/rowset/rowset_meta.h,"@@ -0,0 +1,173 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#ifndef DORIS_BE_SRC_OLAP_ROWSET_ROWSET_META_H
+#define DORIS_BE_SRC_OLAP_ROWSET_ROWSET_META_H
+
+#include ""gen_cpp/olap_file.pb.h""
+
+#include <memory>
+#include <vector>
+
+#include ""olap/new_status.h""
+
+namespace doris {
+
+class RowsetMeta {
+public:
+    virtual void init(const RowsetMetaPb& rowset_meta_pb) {
+        _rowset_meta_pb = rowset_meta_pb;
+    }
+
+    virtual bool deserialize_extra_properties() {
+        return true;
+    }
+
+    virtual void get_rowset_meta_pb(RowsetMetaPb* rowset_meta_pb) {
+        *rowset_meta_pb = _rowset_meta_pb;
+    }
+
+    virtual int64_t get_rowset_id() {
+        return _rowset_meta_pb.rowset_id();
+    }
+
+    virtual void set_rowset_id(int64_t rowset_id) {
+        _rowset_meta_pb.set_rowset_id(rowset_id);
+    }
+
+    virtual int64_t get_tablet_id() {
+        return _rowset_meta_pb.tablet_id();
+    }
+
+    virtual void set_tablet_id(int64_t tablet_id) {
+        _rowset_meta_pb.set_tablet_id(tablet_id);
+    }
+
+    virtual int32_t get_tablet_schema_hash() {
+        return _rowset_meta_pb.tablet_schema_hash();
+    }
+
+    virtual void set_tablet_schema_hash(int64_t tablet_schema_hash) {
+        _rowset_meta_pb.set_tablet_schema_hash(tablet_schema_hash);
+    }
+
+    virtual RowsetType get_rowset_type() {
+        return _rowset_meta_pb.rowset_type();
+    }
+
+    virtual void set_rowset_type(RowsetType rowset_type) {
+        _rowset_meta_pb.set_rowset_type(rowset_type);
+    }
+
+    virtual RowsetState get_rowset_state() {
+        return _rowset_meta_pb.rowset_state();
+    }
+
+    virtual void set_rowset_state(RowsetState rowset_state) {
+        _rowset_meta_pb.set_rowset_state(rowset_state);
+    }
+
+    virtual int get_start_version() {
+        return _rowset_meta_pb.start_version();
+    }
+
+    virtual void set_start_version(int start_version) {
+        _rowset_meta_pb.set_start_version(start_version);
+    }
+    
+    virtual int get_end_version() {
+        return _rowset_meta_pb.end_version();
+    }
+
+    virtual void set_end_version(int end_version) {
+        _rowset_meta_pb.set_end_version(end_version);
+    }
+    
+    virtual int get_row_number() {
+        return _rowset_meta_pb.row_number();
+    }
+
+    virtual void set_row_number(int row_number) {
+        _rowset_meta_pb.set_row_number(row_number);
+    }
+
+    virtual int get_total_disk_size() {
+        return _rowset_meta_pb.total_disk_size();
+    }
+
+    virtual void set_total_disk_size(int total_disk_size) {
+        _rowset_meta_pb.set_total_disk_size(total_disk_size);
+    }
+
+    virtual int get_data_disk_size() {
+        return _rowset_meta_pb.data_disk_size();
+    }
+
+    virtual void set_data_disk_size(int data_disk_size) {
+        _rowset_meta_pb.set_data_disk_size(data_disk_size);
+    }
+
+    virtual int get_index_disk_size() {
+        return _rowset_meta_pb.index_disk_size();
+    }
+
+    virtual void set_index_disk_size(int index_disk_size) {
+        _rowset_meta_pb.set_index_disk_size(index_disk_size);
+    }
+
+    virtual void get_column_statistics(std::vector<ColumnPruning>* column_statistics) {","[{'comment': 'ColumnPruning may be better to replaced by ZoneMap?', 'commenter': 'chaoyli'}, {'comment': ""On the one side, I don't think ZoneMap is a better name; One the other side, Rename could be done in another cr, because there is many other places using ColumnPruning."", 'commenter': 'kangpinghuang'}, {'comment': 'ZoneMap is the standard to describe min/max/avg/count statistics.', 'commenter': 'chaoyli'}]"
429,be/src/olap/rowset/rowset_meta.h,"@@ -0,0 +1,173 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#ifndef DORIS_BE_SRC_OLAP_ROWSET_ROWSET_META_H
+#define DORIS_BE_SRC_OLAP_ROWSET_ROWSET_META_H
+
+#include ""gen_cpp/olap_file.pb.h""
+
+#include <memory>
+#include <vector>
+
+#include ""olap/new_status.h""
+
+namespace doris {
+
+class RowsetMeta {
+public:
+    virtual void init(const RowsetMetaPb& rowset_meta_pb) {
+        _rowset_meta_pb = rowset_meta_pb;
+    }
+
+    virtual bool deserialize_extra_properties() {
+        return true;
+    }
+
+    virtual void get_rowset_meta_pb(RowsetMetaPb* rowset_meta_pb) {
+        *rowset_meta_pb = _rowset_meta_pb;
+    }
+
+    virtual int64_t get_rowset_id() {
+        return _rowset_meta_pb.rowset_id();
+    }
+
+    virtual void set_rowset_id(int64_t rowset_id) {
+        _rowset_meta_pb.set_rowset_id(rowset_id);
+    }
+
+    virtual int64_t get_tablet_id() {
+        return _rowset_meta_pb.tablet_id();
+    }
+
+    virtual void set_tablet_id(int64_t tablet_id) {
+        _rowset_meta_pb.set_tablet_id(tablet_id);
+    }
+
+    virtual int32_t get_tablet_schema_hash() {
+        return _rowset_meta_pb.tablet_schema_hash();
+    }
+
+    virtual void set_tablet_schema_hash(int64_t tablet_schema_hash) {
+        _rowset_meta_pb.set_tablet_schema_hash(tablet_schema_hash);
+    }
+
+    virtual RowsetType get_rowset_type() {
+        return _rowset_meta_pb.rowset_type();
+    }
+
+    virtual void set_rowset_type(RowsetType rowset_type) {
+        _rowset_meta_pb.set_rowset_type(rowset_type);
+    }
+
+    virtual RowsetState get_rowset_state() {
+        return _rowset_meta_pb.rowset_state();
+    }
+
+    virtual void set_rowset_state(RowsetState rowset_state) {
+        _rowset_meta_pb.set_rowset_state(rowset_state);
+    }
+
+    virtual int get_start_version() {
+        return _rowset_meta_pb.start_version();
+    }
+
+    virtual void set_start_version(int start_version) {
+        _rowset_meta_pb.set_start_version(start_version);
+    }
+    
+    virtual int get_end_version() {
+        return _rowset_meta_pb.end_version();
+    }
+
+    virtual void set_end_version(int end_version) {
+        _rowset_meta_pb.set_end_version(end_version);
+    }
+    
+    virtual int get_row_number() {
+        return _rowset_meta_pb.row_number();
+    }
+
+    virtual void set_row_number(int row_number) {
+        _rowset_meta_pb.set_row_number(row_number);
+    }
+
+    virtual int get_total_disk_size() {
+        return _rowset_meta_pb.total_disk_size();
+    }
+
+    virtual void set_total_disk_size(int total_disk_size) {
+        _rowset_meta_pb.set_total_disk_size(total_disk_size);
+    }
+
+    virtual int get_data_disk_size() {
+        return _rowset_meta_pb.data_disk_size();
+    }
+
+    virtual void set_data_disk_size(int data_disk_size) {
+        _rowset_meta_pb.set_data_disk_size(data_disk_size);
+    }
+
+    virtual int get_index_disk_size() {
+        return _rowset_meta_pb.index_disk_size();
+    }
+
+    virtual void set_index_disk_size(int index_disk_size) {
+        _rowset_meta_pb.set_index_disk_size(index_disk_size);
+    }
+
+    virtual void get_column_statistics(std::vector<ColumnPruning>* column_statistics) {
+        for (const ColumnPruning& column_statistic : _rowset_meta_pb.column_statistics()) {
+            column_statistics->push_back(column_statistic);
+        }
+    }
+
+    virtual void set_column_statistics(const std::vector<ColumnPruning>& column_statistics) {
+        for (const ColumnPruning& column_statistic : column_statistics) {
+            ColumnPruning* new_column_statistic = _rowset_meta_pb.add_column_statistics();
+            *new_column_statistic = column_statistic;
+        }
+    }
+
+    virtual void add_column_statistic(const ColumnPruning& column_statistic) {
+        ColumnPruning* new_column_statistic = _rowset_meta_pb.add_column_statistics();
+        *new_column_statistic = column_statistic;
+    }
+
+    virtual const DeleteConditionMessage& get_delete_predicate() {
+        return _rowset_meta_pb.delete_predicate();
+    }
+
+    virtual void set_delete_predicate(DeleteConditionMessage& delete_predicate) {
+        DeleteConditionMessage* new_delete_condition = _rowset_meta_pb.mutable_delete_predicate();
+        *new_delete_condition = delete_predicate;
+    }
+
+    virtual int64_t get_transaction_id() {","[{'comment': 'transaction_id has unified to txn_id.', 'commenter': 'chaoyli'}, {'comment': 'ok.', 'commenter': 'kangpinghuang'}]"
429,be/src/olap/rowset/rowset_meta_manager.cpp,"@@ -0,0 +1,147 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""olap/rowset/rowset_meta_manager.h""
+
+#include <vector>
+#include <sstream>
+#include <string>
+#include <fstream>
+#include <boost/algorithm/string/trim.hpp>
+
+#include ""olap/olap_define.h""
+#include ""olap/storage_engine.h""
+#include ""common/logging.h""
+#include ""json2pb/json_to_pb.h""
+#include ""json2pb/pb_to_json.h""
+
+namespace doris {
+
+const std::string ROWSET_PREFIX = ""rst_"";
+
+NewStatus convert_meta_status(OLAPStatus status) {
+    if (status == OLAP_SUCCESS) {
+        return NewStatus::OK();
+    } else {
+        std::string error_msg = ""meta operation failed"";
+        LOG(WARNING) << error_msg;
+        return NewStatus::IOError(error_msg);
+    }
+}
+
+NewStatus RowsetMetaManager::get_rowset_meta(OlapMeta* meta, int64_t rowset_id, RowsetMeta* rowset_meta) {
+    std::string key = ROWSET_PREFIX + std::to_string(rowset_id);
+    std::string value;
+    OLAPStatus s = meta->get(META_COLUMN_FAMILY_INDEX, key, value);
+    if (s == OLAP_ERR_META_KEY_NOT_FOUND) {
+        std::string error_msg = ""rowset id:"" + std::to_string(rowset_id) + "" not found."";
+        LOG(WARNING) << error_msg;
+        return NewStatus::NotFound(error_msg);
+    } else if (s != OLAP_SUCCESS) {
+        std::string error_msg = ""load rowset id:"" + std::to_string(rowset_id) + "" failed."";
+        LOG(WARNING) << error_msg;
+        return NewStatus::IOError(error_msg);
+    }
+    RowsetMetaPb rowset_meta_pb;
+    bool parsed = rowset_meta_pb.ParseFromString(value);
+    if (!parsed) {
+        std::string error_msg = ""parser rowset pb failed. rowset id:"" + std::to_string(rowset_id);
+        LOG(WARNING) << error_msg;
+        return NewStatus::Corruption(error_msg);
+    }
+    rowset_meta->init(rowset_meta_pb);
+    return NewStatus::OK();
+}
+
+NewStatus RowsetMetaManager::get_json_rowset_meta(OlapMeta* meta, int64_t rowset_id, std::string* json_rowset_meta) {
+    RowsetMeta rowset_meta;
+    NewStatus s = get_rowset_meta(meta, rowset_id, &rowset_meta);
+    if (!s.ok()) {
+        return s;
+    }
+    json2pb::Pb2JsonOptions json_options;
+    json_options.pretty_json = true;
+    RowsetMetaPb rowset_meta_pb;
+    rowset_meta.get_rowset_meta_pb(&rowset_meta_pb);
+    json2pb::ProtoMessageToJson(rowset_meta_pb, json_rowset_meta, json_options);
+    return NewStatus::OK();
+}
+
+NewStatus RowsetMetaManager::save(OlapMeta* meta, int64_t rowset_id, RowsetMeta* rowset_meta) {
+    std::string key = ROWSET_PREFIX + std::to_string(rowset_id);
+    RowsetMetaPb rowset_meta_pb;
+    rowset_meta->get_rowset_meta_pb(&rowset_meta_pb);","[{'comment': 'RowsetMetaManager should not know RowsetMetaPB. RowsetMeta is stored as pb or another format is transparent \r\n to RowsetMetaManager. Instead, Rowset provide a serialize interface to convert pb to bytes.', 'commenter': 'chaoyli'}]"
429,be/src/olap/rowset/rowset_meta.h,"@@ -0,0 +1,173 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#ifndef DORIS_BE_SRC_OLAP_ROWSET_ROWSET_META_H
+#define DORIS_BE_SRC_OLAP_ROWSET_ROWSET_META_H
+
+#include ""gen_cpp/olap_file.pb.h""
+
+#include <memory>
+#include <vector>
+
+#include ""olap/new_status.h""
+
+namespace doris {
+
+class RowsetMeta {
+public:
+    virtual void init(const RowsetMetaPb& rowset_meta_pb) {","[{'comment': 'RowsetMeta use deserialize interface to convert bytes to RowsetMetaPB. \r\nOnly RowsetMeta can know it is stored as pb or another format.', 'commenter': 'chaoyli'}, {'comment': 'ok', 'commenter': 'kangpinghuang'}]"
429,be/src/olap/rowset/rowset_meta.h,"@@ -0,0 +1,173 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#ifndef DORIS_BE_SRC_OLAP_ROWSET_ROWSET_META_H
+#define DORIS_BE_SRC_OLAP_ROWSET_ROWSET_META_H
+
+#include ""gen_cpp/olap_file.pb.h""
+
+#include <memory>
+#include <vector>
+
+#include ""olap/new_status.h""
+
+namespace doris {
+
+class RowsetMeta {
+public:","[{'comment': 'Base Class need a virtual destructor.', 'commenter': 'chaoyli'}, {'comment': 'ok', 'commenter': 'kangpinghuang'}]"
429,be/src/olap/rowset/rowset_meta.h,"@@ -0,0 +1,173 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#ifndef DORIS_BE_SRC_OLAP_ROWSET_ROWSET_META_H
+#define DORIS_BE_SRC_OLAP_ROWSET_ROWSET_META_H
+
+#include ""gen_cpp/olap_file.pb.h""
+
+#include <memory>
+#include <vector>
+
+#include ""olap/new_status.h""
+
+namespace doris {
+
+class RowsetMeta {
+public:
+    virtual void init(const RowsetMetaPb& rowset_meta_pb) {
+        _rowset_meta_pb = rowset_meta_pb;
+    }
+
+    virtual bool deserialize_extra_properties() {
+        return true;
+    }
+
+    virtual void get_rowset_meta_pb(RowsetMetaPb* rowset_meta_pb) {
+        *rowset_meta_pb = _rowset_meta_pb;
+    }
+
+    virtual int64_t get_rowset_id() {
+        return _rowset_meta_pb.rowset_id();
+    }
+
+    virtual void set_rowset_id(int64_t rowset_id) {
+        _rowset_meta_pb.set_rowset_id(rowset_id);
+    }
+
+    virtual int64_t get_tablet_id() {
+        return _rowset_meta_pb.tablet_id();
+    }
+
+    virtual void set_tablet_id(int64_t tablet_id) {
+        _rowset_meta_pb.set_tablet_id(tablet_id);
+    }
+
+    virtual int32_t get_tablet_schema_hash() {
+        return _rowset_meta_pb.tablet_schema_hash();
+    }
+
+    virtual void set_tablet_schema_hash(int64_t tablet_schema_hash) {
+        _rowset_meta_pb.set_tablet_schema_hash(tablet_schema_hash);
+    }
+
+    virtual RowsetType get_rowset_type() {
+        return _rowset_meta_pb.rowset_type();
+    }
+
+    virtual void set_rowset_type(RowsetType rowset_type) {
+        _rowset_meta_pb.set_rowset_type(rowset_type);
+    }
+
+    virtual RowsetState get_rowset_state() {
+        return _rowset_meta_pb.rowset_state();
+    }
+
+    virtual void set_rowset_state(RowsetState rowset_state) {
+        _rowset_meta_pb.set_rowset_state(rowset_state);
+    }
+
+    virtual int get_start_version() {
+        return _rowset_meta_pb.start_version();
+    }
+
+    virtual void set_start_version(int start_version) {
+        _rowset_meta_pb.set_start_version(start_version);","[{'comment': 'set_start_version and set_end_version can be merged into one set_verson function.\r\nSimilarly, get_start_version and get_end_version can be merged into on get_verson.', 'commenter': 'chaoyli'}, {'comment': 'ok', 'commenter': 'kangpinghuang'}]"
432,be/src/olap/segment_group.cpp,"@@ -198,24 +199,43 @@ void SegmentGroup::delete_all_files() {
     }
 }
 
+
 OLAPStatus SegmentGroup::add_column_statistics_for_linked_schema_change(
-        const std::vector<std::pair<WrapperField*, WrapperField*>>& column_statistic_fields) {
+        const std::vector<std::pair<WrapperField*, WrapperField*>>& column_statistic_fields,
+        const SchemaMapping& schema_mapping) {
     //When add rollup table, the base table index maybe empty
     if (column_statistic_fields.size() == 0) {
         return OLAP_SUCCESS;
     }
 
-    //Should use _table->num_key_fields(), not column_statistic_fields.size()
-    //as rollup table num_key_fields will less than base table column_statistic_fields.size().
-    //For LinkedSchemaChange, the rollup table keys order is the same as base table
+    //1 for LinkedSchemaChange, the rollup table keys order is the same as base table
+    //2 when user add a new key column to base table, _table->num_key_fields() size will
+    // greater than _column_statistics size
+    int num_new_keys = 0;
     for (size_t i = 0; i < _table->num_key_fields(); ++i) {
-        WrapperField* first = WrapperField::create(_table->tablet_schema()[i]);
+        const FieldInfo& column_schema = _table->tablet_schema()[i];
+
+        WrapperField* first = WrapperField::create(column_schema);
         DCHECK(first != NULL) << ""failed to allocate memory for field: "" << i;
-        first->copy(column_statistic_fields[i].first);
 
-        WrapperField* second = WrapperField::create(_table->tablet_schema()[i]);
+        WrapperField* second = WrapperField::create(column_schema);
         DCHECK(second != NULL) << ""failed to allocate memory for field: "" << i;
-        second->copy(column_statistic_fields[i].second);
+
+        //for new key column, use default value to fill into column_statistics
+        if (schema_mapping[i].ref_column == -1) {
+            num_new_keys++;
+
+            if (true == column_schema.is_allow_null && column_schema.default_value.length() == 0) {","[{'comment': 'first->copy(schema_mapping[i].default_value);\r\nsecond->copy(schema_mapping[i].default_value);\r\nwill be ok.', 'commenter': 'chaoyli'}, {'comment': 'OK.', 'commenter': 'kangkaisen'}]"
432,be/src/olap/segment_group.cpp,"@@ -198,24 +199,43 @@ void SegmentGroup::delete_all_files() {
     }
 }
 
+","[{'comment': 'empty line', 'commenter': 'chaoyli'}, {'comment': 'empty line', 'commenter': 'chaoyli'}]"
433,be/src/olap/utils.cpp,"@@ -1104,11 +1104,11 @@ OLAPStatus Mutex::lock() {
 }
 
 OLAPStatus Mutex::trylock() {
-    if (0 != pthread_mutex_trylock(&_lock)) {
-        VLOG(3) << ""failed to got the mutex lock. err="" << strerror(errno);
+    int rv = pthread_mutex_trylock(&_lock);
+    if (rv != 0) {
+        VLOG(3) << ""failed to got the mutex lock. error="" << strerror(rv);","[{'comment': 'use strerror_r to make it thread safe', 'commenter': 'imay'}, {'comment': 'This place is thread-safe. Because the error returned here is all defined by pthread_mutex_trylock.', 'commenter': 'chaoyli'}]"
437,be/src/olap/rowset_meta_manager.h,"@@ -0,0 +1,56 @@
+// Licensed to the Apache Software Foundation (ASF) under one","[{'comment': 'This file should be renamed to tablet_meta_manager.h?', 'commenter': 'kangpinghuang'}, {'comment': 'it is rowset meta manager, but I have not used it yet. I will remove it.', 'commenter': 'yiguolei'}]"
437,be/src/olap/rowset_meta_manager.h,"@@ -0,0 +1,56 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#ifndef DORIS_BE_SRC_OLAP_OLAP_HEADER_MANAGER_H
+#define DORIS_BE_SRC_OLAP_OLAP_HEADER_MANAGER_H","[{'comment': 'the header macro should be modified?', 'commenter': 'kangpinghuang'}]"
437,be/src/olap/rowset_meta_manager.h,"@@ -0,0 +1,56 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#ifndef DORIS_BE_SRC_OLAP_OLAP_HEADER_MANAGER_H
+#define DORIS_BE_SRC_OLAP_OLAP_HEADER_MANAGER_H
+
+#include <string>
+
+#include ""olap/rowset_meta.h""
+#include ""olap/olap_define.h""
+#include ""olap/store.h""
+
+namespace doris {
+
+// Helper Class for managing tablet headers of one root path.
+class RowsetMetaManager {","[{'comment': 'RowsetMetaManager -> TabletMetaManager?', 'commenter': 'kangpinghuang'}]"
437,be/src/olap/storage_engine.h,"@@ -489,37 +449,18 @@ class StorageEngine {
         bool is_used;
     };
 
-    typedef std::map<int64_t, TableInstances> tablet_map_t;
-    typedef std::map<std::string, uint32_t> file_system_task_count_t;
-
-    TabletSharedPtr _get_tablet_with_no_lock(TTabletId tablet_id, SchemaHash schema_hash);
 
-    // 遍历root所指定目录, 通过dirs返回此目录下所有有文件夹的名字, files返回所有文件的名字
-    OLAPStatus _dir_walk(const std::string& root,
-                     std::set<std::string>* dirs,
-                     std::set<std::string>* files);
+    typedef std::map<std::string, uint32_t> file_system_task_count_t;
 
     // 扫描目录, 加载表
     OLAPStatus _load_store(OlapStore* store);
 
-    OLAPStatus _create_new_tablet_header(const TCreateTabletReq& request,
-                                             OlapStore* store,
-                                             const bool is_schema_change_tablet,
-                                             const TabletSharedPtr ref_tablet,
-                                             TabletMeta* header);
-
-    OLAPStatus _check_existed_or_else_create_dir(const std::string& path);
-
     TabletSharedPtr _find_best_tablet_to_compaction(CompactionType compaction_type);
-    bool _can_do_compaction(TabletSharedPtr tablet);
-
-    void _cancel_unfinished_schema_change();
 
     OLAPStatus _do_sweep(
             const std::string& scan_root, const time_t& local_tm_now, const uint32_t expire);
 
-    void _build_tablet_info(TabletSharedPtr tablet, TTabletInfo* tablet_info);
-    void _build_tablet_stat();
+    ","[{'comment': 'additional empty line?', 'commenter': 'kangpinghuang'}]"
437,be/src/olap/store.cpp,"@@ -437,18 +437,19 @@ OLAPStatus OlapStore::deregister_tablet(Tablet* tablet) {
     return OLAP_SUCCESS;
 }
 
-std::string OlapStore::get_shard_path_from_header(const std::string& shard_string) {
+std::string OlapStore::get_absolute_shard_path(const std::string& shard_string) {
     return _path + DATA_PREFIX + ""/"" + shard_string;
 }
 
-std::string OlapStore::get_tablet_schema_hash_path_from_header(TabletMeta* header) {
-    return _path + DATA_PREFIX + ""/"" + std::to_string(header->shard())
+std::string OlapStore::get_tablet_path_from_header(TabletMeta* header, bool with_schema_hash) {","[{'comment': 'I think the function name  ""get_tablet_path""  is better， header will be renamed to tablet meta. and ""from_header"" is additional, because we do not have ""get_tablet_path_from_other""\r\n', 'commenter': 'kangpinghuang'}, {'comment': 'I rename it to get_absolute_tablet_path', 'commenter': 'yiguolei'}]"
437,be/src/olap/tablet_manager.h,"@@ -0,0 +1,199 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#ifndef DORIS_BE_SRC_OLAP_TABLET_MANAGER_H
+#define DORIS_BE_SRC_OLAP_TABLET_MANAGER_H
+
+#include <ctime>
+#include <list>
+#include <map>
+#include <mutex>
+#include <condition_variable>
+#include <set>
+#include <string>
+#include <vector>
+#include <thread>
+
+#include <rapidjson/document.h>
+#include <pthread.h>
+
+#include ""agent/status.h""
+#include ""common/status.h""
+#include ""gen_cpp/AgentService_types.h""
+#include ""gen_cpp/BackendService_types.h""
+#include ""gen_cpp/MasterService_types.h""
+#include ""olap/atomic.h""
+#include ""olap/lru_cache.h""
+#include ""olap/olap_common.h""
+#include ""olap/olap_define.h""
+#include ""olap/tablet.h""
+#include ""olap/olap_meta.h""
+#include ""olap/options.h""
+
+namespace doris {
+
+class Tablet;
+class OlapStore;
+
+// TabletManager provides get,add, delete tablet method for storage engine
+class TabletManager {
+public:
+    TabletManager();
+    ~TabletManager() {
+        _tablet_map.clear();
+        _global_tablet_id = 0;
+    }
+
+    // Add a tablet pointer to StorageEngine
+    // If force, drop the existing tablet add this new one
+    //
+    // Return OLAP_SUCCESS, if run ok
+    //        OLAP_ERR_TABLE_INSERT_DUPLICATION_ERROR, if find duplication
+    //        OLAP_ERR_NOT_INITED, if not inited
+    OLAPStatus add_tablet(TTabletId tablet_id, SchemaHash schema_hash,
+                         const TabletSharedPtr& tablet, bool force = false);
+
+    void cancel_unfinished_schema_change();
+
+    bool check_tablet_id_exist(TTabletId tablet_id);
+
+    void clear();
+    
+    // Add empty data for Tablet
+    //
+    // Return OLAP_SUCCESS, if run ok
+    OLAPStatus create_init_version(
+            TTabletId tablet_id, SchemaHash schema_hash,
+            Version version, VersionHash version_hash);
+
+    OLAPStatus create_tablet(const TCreateTabletReq& request, 
+                             std::vector<OlapStore*> stores);
+
+    // Create new tablet for StorageEngine
+    //
+    // Return Tablet *  succeeded; Otherwise, return NULL if failed
+    TabletSharedPtr create_tablet(const TCreateTabletReq& request,","[{'comment': 'OLAPStatus is used to indicate status like disk error, and so on. So return a TabletSharedPtr may can not include this error.', 'commenter': 'chaoyli'}, {'comment': 'StorageEngine has two create method， TabletManager has two related method. And I will remove the duplcate method when refactoring upper services.', 'commenter': 'yiguolei'}]"
450,be/src/exec/es_scan_node.h,"@@ -0,0 +1,78 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#ifndef  BDG_PALO_BE_SRC_QUERY_EXEC_ES_SCAN_NODE_H
+#define  BDG_PALO_BE_SRC_QUERY_EXEC_ES_SCAN_NODE_H
+
+#include <memory>
+#include <vector>
+
+#include ""runtime/descriptors.h""
+#include ""runtime/tuple.h""
+#include ""exec/scan_node.h""
+#include ""runtime/exec_env.h""
+#include ""gen_cpp/TExtDataSourceService.h""
+#include ""gen_cpp/PaloExternalDataSourceService_types.h""
+
+namespace doris {
+
+class TupleDescriptor;
+class RuntimeState;
+class Status;
+
+class EsScanNode : public ScanNode {
+public:
+    EsScanNode(ObjectPool* pool, const TPlanNode& tnode, const DescriptorTbl& descs);
+    ~EsScanNode();
+
+    virtual Status prepare(RuntimeState* state) override;
+    virtual Status open(RuntimeState* state) override;
+    virtual Status get_next(RuntimeState* state, RowBatch* row_batch, bool* eos) override;
+    virtual Status close(RuntimeState* state) override;
+    virtual Status set_scan_ranges(const std::vector<TScanRangeParams>& scan_ranges) override;
+
+protected:
+    // Write debug string of this into out.
+    virtual void debug_string(int indentation_level, std::stringstream* out) const;
+
+private:
+    TupleId _tuple_id;
+    std::map<std::string, std::string> _properties;
+    const TupleDescriptor* _tuple_desc;
+    ExecEnv* _env;
+    std::vector<TEsScanRange> _scan_ranges;
+
+    // scan range's iterator, used in get_next()
+    int _scan_range_idx;
+
+    // store every scan range's netaddress/handle/offset
+    std::vector<TNetworkAddress> _addresses;
+    std::vector<std::string> _scan_handles;
+    std::vector<int> _offsets;
+
+    Status open_es(TNetworkAddress& address, TExtOpenResult& result, TExtOpenParams& params);","[{'comment': 'put these functions before member variables', 'commenter': 'imay'}]"
450,be/src/exec/es_scan_node.cpp,"@@ -0,0 +1,660 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""es_scan_node.h""
+
+#include <string>
+#include <boost/algorithm/string.hpp>
+#include <gutil/strings/substitute.h>
+
+#include ""gen_cpp/PlanNodes_types.h""
+#include ""gen_cpp/Exprs_types.h""
+#include ""runtime/runtime_state.h""
+#include ""runtime/row_batch.h""
+#include ""runtime/string_value.h""
+#include ""runtime/tuple_row.h""
+#include ""runtime/client_cache.h""
+#include ""util/runtime_profile.h""
+#include ""util/debug_util.h""
+#include ""service/backend_options.h""
+#include ""olap/olap_common.h""
+#include ""olap/utils.h""
+#include ""exprs/expr_context.h""
+#include ""exprs/expr.h""
+#include ""exprs/slot_ref.h""
+
+namespace doris {
+
+// $0 = column type (e.g. INT)
+const string ERROR_INVALID_COL_DATA = ""Data source returned inconsistent column data. ""
+    ""Expected value of type $0 based on column metadata. This likely indicates a ""
+    ""problem with the data source library."";
+const string ERROR_MEM_LIMIT_EXCEEDED = ""DataSourceScanNode::$0() failed to allocate ""
+    ""$1 bytes for $2."";
+
+EsScanNode::EsScanNode(
+        ObjectPool* pool,
+        const TPlanNode& tnode,
+        const DescriptorTbl& descs) :
+            ScanNode(pool, tnode, descs),
+            _tuple_id(tnode.es_scan_node.tuple_id),
+            _scan_range_idx(0) {
+    if (tnode.es_scan_node.__isset.properties) {
+        _properties = tnode.es_scan_node.properties;
+    }
+}
+
+EsScanNode::~EsScanNode() {
+}
+
+Status EsScanNode::prepare(RuntimeState* state) {
+    LOG(INFO) << ""EsScanNode::Prepare"";","[{'comment': 'This log should not be a INFO level', 'commenter': 'imay'}]"
450,be/src/exec/es_scan_node.cpp,"@@ -0,0 +1,660 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""es_scan_node.h""
+
+#include <string>
+#include <boost/algorithm/string.hpp>
+#include <gutil/strings/substitute.h>
+
+#include ""gen_cpp/PlanNodes_types.h""
+#include ""gen_cpp/Exprs_types.h""
+#include ""runtime/runtime_state.h""
+#include ""runtime/row_batch.h""
+#include ""runtime/string_value.h""
+#include ""runtime/tuple_row.h""
+#include ""runtime/client_cache.h""
+#include ""util/runtime_profile.h""
+#include ""util/debug_util.h""
+#include ""service/backend_options.h""
+#include ""olap/olap_common.h""
+#include ""olap/utils.h""
+#include ""exprs/expr_context.h""
+#include ""exprs/expr.h""
+#include ""exprs/slot_ref.h""
+
+namespace doris {
+
+// $0 = column type (e.g. INT)
+const string ERROR_INVALID_COL_DATA = ""Data source returned inconsistent column data. ""
+    ""Expected value of type $0 based on column metadata. This likely indicates a ""
+    ""problem with the data source library."";
+const string ERROR_MEM_LIMIT_EXCEEDED = ""DataSourceScanNode::$0() failed to allocate ""
+    ""$1 bytes for $2."";
+
+EsScanNode::EsScanNode(
+        ObjectPool* pool,
+        const TPlanNode& tnode,
+        const DescriptorTbl& descs) :
+            ScanNode(pool, tnode, descs),
+            _tuple_id(tnode.es_scan_node.tuple_id),
+            _scan_range_idx(0) {
+    if (tnode.es_scan_node.__isset.properties) {
+        _properties = tnode.es_scan_node.properties;
+    }
+}
+
+EsScanNode::~EsScanNode() {
+}
+
+Status EsScanNode::prepare(RuntimeState* state) {
+    LOG(INFO) << ""EsScanNode::Prepare"";
+
+    RETURN_IF_ERROR(ScanNode::prepare(state));
+    _tuple_desc = state->desc_tbl().get_tuple_descriptor(_tuple_id);
+    if (nullptr == _tuple_desc) {","[{'comment': '```suggestion\r\n     if (_tuple_desc == nullptr) {\r\n```\r\n\r\ncompiler can help us ', 'commenter': 'imay'}]"
450,be/src/exec/es_scan_node.cpp,"@@ -0,0 +1,660 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""es_scan_node.h""
+
+#include <string>
+#include <boost/algorithm/string.hpp>
+#include <gutil/strings/substitute.h>
+
+#include ""gen_cpp/PlanNodes_types.h""
+#include ""gen_cpp/Exprs_types.h""
+#include ""runtime/runtime_state.h""
+#include ""runtime/row_batch.h""
+#include ""runtime/string_value.h""
+#include ""runtime/tuple_row.h""
+#include ""runtime/client_cache.h""
+#include ""util/runtime_profile.h""
+#include ""util/debug_util.h""
+#include ""service/backend_options.h""
+#include ""olap/olap_common.h""
+#include ""olap/utils.h""
+#include ""exprs/expr_context.h""
+#include ""exprs/expr.h""
+#include ""exprs/slot_ref.h""
+
+namespace doris {
+
+// $0 = column type (e.g. INT)
+const string ERROR_INVALID_COL_DATA = ""Data source returned inconsistent column data. ""
+    ""Expected value of type $0 based on column metadata. This likely indicates a ""
+    ""problem with the data source library."";
+const string ERROR_MEM_LIMIT_EXCEEDED = ""DataSourceScanNode::$0() failed to allocate ""
+    ""$1 bytes for $2."";
+
+EsScanNode::EsScanNode(
+        ObjectPool* pool,
+        const TPlanNode& tnode,
+        const DescriptorTbl& descs) :
+            ScanNode(pool, tnode, descs),
+            _tuple_id(tnode.es_scan_node.tuple_id),
+            _scan_range_idx(0) {
+    if (tnode.es_scan_node.__isset.properties) {
+        _properties = tnode.es_scan_node.properties;
+    }
+}
+
+EsScanNode::~EsScanNode() {
+}
+
+Status EsScanNode::prepare(RuntimeState* state) {
+    LOG(INFO) << ""EsScanNode::Prepare"";
+
+    RETURN_IF_ERROR(ScanNode::prepare(state));
+    _tuple_desc = state->desc_tbl().get_tuple_descriptor(_tuple_id);
+    if (nullptr == _tuple_desc) {
+        std::stringstream ss;
+        ss << ""es tuple descriptor is null, _tuple_id="" << _tuple_id;
+        LOG(WARNING) << ss.str();
+        return Status(ss.str());
+    }
+    _env = state->exec_env();
+
+    return Status::OK;
+}
+
+Status EsScanNode::open(RuntimeState* state) {
+    LOG(INFO) << ""EsScanNode::Open"";","[{'comment': 'LOG level', 'commenter': 'imay'}]"
450,be/src/exec/es_scan_node.cpp,"@@ -0,0 +1,660 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""es_scan_node.h""
+
+#include <string>
+#include <boost/algorithm/string.hpp>
+#include <gutil/strings/substitute.h>
+
+#include ""gen_cpp/PlanNodes_types.h""
+#include ""gen_cpp/Exprs_types.h""
+#include ""runtime/runtime_state.h""
+#include ""runtime/row_batch.h""
+#include ""runtime/string_value.h""
+#include ""runtime/tuple_row.h""
+#include ""runtime/client_cache.h""
+#include ""util/runtime_profile.h""
+#include ""util/debug_util.h""
+#include ""service/backend_options.h""
+#include ""olap/olap_common.h""
+#include ""olap/utils.h""
+#include ""exprs/expr_context.h""
+#include ""exprs/expr.h""
+#include ""exprs/slot_ref.h""
+
+namespace doris {
+
+// $0 = column type (e.g. INT)
+const string ERROR_INVALID_COL_DATA = ""Data source returned inconsistent column data. ""
+    ""Expected value of type $0 based on column metadata. This likely indicates a ""
+    ""problem with the data source library."";
+const string ERROR_MEM_LIMIT_EXCEEDED = ""DataSourceScanNode::$0() failed to allocate ""
+    ""$1 bytes for $2."";
+
+EsScanNode::EsScanNode(
+        ObjectPool* pool,
+        const TPlanNode& tnode,
+        const DescriptorTbl& descs) :
+            ScanNode(pool, tnode, descs),
+            _tuple_id(tnode.es_scan_node.tuple_id),
+            _scan_range_idx(0) {
+    if (tnode.es_scan_node.__isset.properties) {
+        _properties = tnode.es_scan_node.properties;
+    }
+}
+
+EsScanNode::~EsScanNode() {
+}
+
+Status EsScanNode::prepare(RuntimeState* state) {
+    LOG(INFO) << ""EsScanNode::Prepare"";
+
+    RETURN_IF_ERROR(ScanNode::prepare(state));
+    _tuple_desc = state->desc_tbl().get_tuple_descriptor(_tuple_id);
+    if (nullptr == _tuple_desc) {
+        std::stringstream ss;
+        ss << ""es tuple descriptor is null, _tuple_id="" << _tuple_id;
+        LOG(WARNING) << ss.str();
+        return Status(ss.str());
+    }
+    _env = state->exec_env();
+
+    return Status::OK;
+}
+
+Status EsScanNode::open(RuntimeState* state) {
+    LOG(INFO) << ""EsScanNode::Open"";
+
+    RETURN_IF_ERROR(exec_debug_action(TExecNodePhase::OPEN));
+    RETURN_IF_CANCELLED(state);
+    SCOPED_TIMER(_runtime_profile->total_time_counter());
+    RETURN_IF_ERROR(ExecNode::open(state));
+
+    // TExtOpenParams.row_schema
+    vector<TExtColumnDesc> cols;
+    for (const SlotDescriptor* slot : _tuple_desc->slots()) {
+        TExtColumnDesc col;
+        col.__set_name(slot->col_name());
+        col.__set_type(slot->type().to_thrift());
+        cols.push_back(std::move(col));","[{'comment': '```suggestion\r\n         cols.emplace_back(std::move(col));\r\n```', 'commenter': 'imay'}]"
450,be/src/exec/es_scan_node.cpp,"@@ -0,0 +1,660 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""es_scan_node.h""
+
+#include <string>
+#include <boost/algorithm/string.hpp>
+#include <gutil/strings/substitute.h>
+
+#include ""gen_cpp/PlanNodes_types.h""
+#include ""gen_cpp/Exprs_types.h""
+#include ""runtime/runtime_state.h""
+#include ""runtime/row_batch.h""
+#include ""runtime/string_value.h""
+#include ""runtime/tuple_row.h""
+#include ""runtime/client_cache.h""
+#include ""util/runtime_profile.h""
+#include ""util/debug_util.h""
+#include ""service/backend_options.h""
+#include ""olap/olap_common.h""
+#include ""olap/utils.h""
+#include ""exprs/expr_context.h""
+#include ""exprs/expr.h""
+#include ""exprs/slot_ref.h""
+
+namespace doris {
+
+// $0 = column type (e.g. INT)
+const string ERROR_INVALID_COL_DATA = ""Data source returned inconsistent column data. ""
+    ""Expected value of type $0 based on column metadata. This likely indicates a ""
+    ""problem with the data source library."";
+const string ERROR_MEM_LIMIT_EXCEEDED = ""DataSourceScanNode::$0() failed to allocate ""
+    ""$1 bytes for $2."";
+
+EsScanNode::EsScanNode(
+        ObjectPool* pool,
+        const TPlanNode& tnode,
+        const DescriptorTbl& descs) :
+            ScanNode(pool, tnode, descs),
+            _tuple_id(tnode.es_scan_node.tuple_id),
+            _scan_range_idx(0) {
+    if (tnode.es_scan_node.__isset.properties) {
+        _properties = tnode.es_scan_node.properties;
+    }
+}
+
+EsScanNode::~EsScanNode() {
+}
+
+Status EsScanNode::prepare(RuntimeState* state) {
+    LOG(INFO) << ""EsScanNode::Prepare"";
+
+    RETURN_IF_ERROR(ScanNode::prepare(state));
+    _tuple_desc = state->desc_tbl().get_tuple_descriptor(_tuple_id);
+    if (nullptr == _tuple_desc) {
+        std::stringstream ss;
+        ss << ""es tuple descriptor is null, _tuple_id="" << _tuple_id;
+        LOG(WARNING) << ss.str();
+        return Status(ss.str());
+    }
+    _env = state->exec_env();
+
+    return Status::OK;
+}
+
+Status EsScanNode::open(RuntimeState* state) {
+    LOG(INFO) << ""EsScanNode::Open"";
+
+    RETURN_IF_ERROR(exec_debug_action(TExecNodePhase::OPEN));
+    RETURN_IF_CANCELLED(state);
+    SCOPED_TIMER(_runtime_profile->total_time_counter());
+    RETURN_IF_ERROR(ExecNode::open(state));
+
+    // TExtOpenParams.row_schema
+    vector<TExtColumnDesc> cols;
+    for (const SlotDescriptor* slot : _tuple_desc->slots()) {
+        TExtColumnDesc col;
+        col.__set_name(slot->col_name());
+        col.__set_type(slot->type().to_thrift());
+        cols.push_back(std::move(col));
+    }
+    TExtTableSchema row_schema;
+    row_schema.cols = std::move(cols);
+    row_schema.__isset.cols = true;
+
+    // TExtOpenParams.predicates
+    vector<vector<TExtPredicate> > predicates;
+    vector<int> conjunct_idxes;
+    for (int i = 0; i < _conjunct_ctxs.size(); ++i) {
+        VLOG(1) << ""conjunct: "" << _conjunct_ctxs[i]->root()->debug_string();
+        vector<TExtPredicate> disjuncts;
+        if (get_disjuncts(_conjunct_ctxs[i], _conjunct_ctxs[i]->root(), disjuncts)) {
+            predicates.push_back(std::move(disjuncts));
+            conjunct_idxes.push_back(i);
+        }
+    }
+
+    // open every scan range
+    int conjunct_accepted_times[_conjunct_ctxs.size()]; 
+    for (int i = 0; i < _scan_ranges.size(); ++i) {
+        TEsScanRange es_scan_range = _scan_ranges[i];
+
+        // TExtOpenParams
+        TExtOpenParams params;
+        params.__set_query_id(state->query_id());
+        _properties[""index""] = es_scan_range.index;
+        if (es_scan_range.__isset.type) {
+            _properties[""type""] = es_scan_range.type;
+        }
+        _properties[""shard_id""] = std::to_string(es_scan_range.shard_id);
+        params.__set_properties(_properties);
+        params.__set_row_schema(row_schema);
+        params.__set_batch_size(state->batch_size());
+        params.__set_predicates(predicates);
+        TExtOpenResult result;
+
+        // check es host
+        if (es_scan_range.es_hosts.empty()) {
+            std::stringstream ss;
+            ss << ""es fail to open: hosts empty"";
+            LOG(ERROR) << ss.str();
+            return Status(ss.str());
+        }
+
+        // choose an es node, local is better
+        TNetworkAddress es_host_selected = es_scan_range.es_hosts[0];
+        int selected_idx = 0;
+        for (int j = 0; j < es_scan_range.es_hosts.size(); j++) {
+            TNetworkAddress& es_host = es_scan_range.es_hosts[j];
+            if (es_host.hostname == BackendOptions::get_localhost()) {
+                es_host_selected = es_host;
+                selected_idx = j;
+                break;
+            }
+        }
+
+        // if shard not found, try other nodes
+        Status status = open_es(es_host_selected, result, params);
+        if (status.code() == TStatusCode::ES_SHARD_NOT_FOUND) {
+            for (int j = 0; j < es_scan_range.es_hosts.size(); j++) {
+                if (j == selected_idx) continue;
+                es_host_selected = es_scan_range.es_hosts[j];
+                status = open_es(es_host_selected, result, params);
+                if (status.code() == TStatusCode::ES_SHARD_NOT_FOUND) {
+                    continue;
+                } else {
+                    break;
+                }
+            }
+        }
+
+        if (!status.ok()) {
+            LOG(WARNING) << ""es open error: scan_range_idx="" << i
+                         << "", address="" << es_host_selected
+                         << "", msg="" << status.get_error_msg();
+            return status;
+        }
+
+        // get accepted_conjuncts
+        if (result.__isset.accepted_conjuncts) {
+            for (int conjunct_index : result.accepted_conjuncts) {
+                conjunct_accepted_times[conjunct_index]++;
+            }
+        }
+
+        _addresses.push_back(es_host_selected);
+        _scan_handles.push_back(result.scan_handle);
+        VLOG(1) << ""es open success: scan_range_idx="" << i
+                << "", params="" << apache::thrift::ThriftDebugString(params)
+                << "", result="" << apache::thrift::ThriftDebugString(result);
+        }
+
+    // remove those conjuncts that conjunct_accepted_times[i] == _scan_ranges.size()
+    for (int i = conjunct_idxes.size() - 1; i >= 0; --i) {
+        if (conjunct_accepted_times[i] == _scan_ranges.size()) {
+            _conjunct_ctxs.erase(_conjunct_ctxs.begin() + i);
+        }
+    }
+
+    return Status::OK;
+}
+
+Status EsScanNode::get_next(RuntimeState* state, RowBatch* row_batch, bool* eos) {
+    LOG(INFO) << ""EsScanNode::GetNext"";
+
+    RETURN_IF_ERROR(exec_debug_action(TExecNodePhase::GETNEXT));
+    RETURN_IF_CANCELLED(state);
+    SCOPED_TIMER(_runtime_profile->total_time_counter());
+    SCOPED_TIMER(materialize_tuple_timer());
+
+    // create tuple
+    MemPool* tuple_pool = row_batch->tuple_data_pool();
+    int64_t tuple_buffer_size;
+    uint8_t* tuple_buffer = nullptr;
+    RETURN_IF_ERROR(row_batch->resize_and_allocate_tuple_buffer(state, &tuple_buffer_size, &tuple_buffer));
+    Tuple* tuple = reinterpret_cast<Tuple*>(tuple_buffer);
+    
+    // get batch
+    TExtGetNextResult result;
+    RETURN_IF_ERROR(get_next_from_es(result));
+    VLOG(1) << ""es get next success: result="" << apache::thrift::ThriftDebugString(result);
+    _offsets[_scan_range_idx] += result.rows.num_rows;
+
+    // convert
+    VLOG(1) << ""begin to convert: scan_range_idx="" << _scan_range_idx
+            << "", num_rows="" << result.rows.num_rows;
+    vector<TExtColumnData>& cols = result.rows.cols;
+    // indexes of the next non-null value in the row batch, per column. 
+    vector<int> cols_next_val_idx(_tuple_desc->slots().size(), 0);
+    for (int row_idx = 0; row_idx < result.rows.num_rows; row_idx++) {
+        if (reached_limit()) {
+            *eos = true;
+            break;
+        }
+        RETURN_IF_ERROR(materialize_row(tuple_pool, tuple, cols, row_idx, cols_next_val_idx));
+        TupleRow* tuple_row = row_batch->get_row(row_batch->add_row());
+        tuple_row->set_tuple(0, tuple);
+        if (ExecNode::eval_conjuncts(_conjunct_ctxs.data(), _conjunct_ctxs.size(), tuple_row)) {
+            row_batch->commit_last_row();
+            tuple = reinterpret_cast<Tuple*>(
+                reinterpret_cast<uint8_t*>(tuple) + _tuple_desc->byte_size());
+            ++_num_rows_returned;
+        }
+    }
+
+    VLOG(1) << ""finish one batch: num_rows="" << row_batch->num_rows();
+    COUNTER_SET(_rows_returned_counter, _num_rows_returned);
+    if (result.__isset.eos && result.eos) {
+        VLOG(1) << ""es finish one scan_range: scan_range_idx="" << _scan_range_idx;
+        ++_scan_range_idx;
+    }
+    if (_scan_range_idx == _scan_ranges.size()) {
+        *eos = true;
+    }
+
+    return Status::OK;
+}
+
+Status EsScanNode::close(RuntimeState* state) {
+    if (is_closed()) return Status::OK;
+    LOG(INFO) << ""EsScanNode::Close"";
+    RETURN_IF_ERROR(exec_debug_action(TExecNodePhase::CLOSE));
+    SCOPED_TIMER(_runtime_profile->total_time_counter());
+
+    for (int i = 0; i < _addresses.size(); ++i) {
+        TExtCloseParams params;
+        params.__set_scan_handle(_scan_handles[i]);
+        TExtCloseResult result;
+
+#ifndef BE_TEST
+        const TNetworkAddress& address = _addresses[i];
+        try {
+            Status status;
+            ExtDataSourceServiceClientCache* client_cache = _env->extdatasource_client_cache();
+            ExtDataSourceServiceConnection client(client_cache, address, 10000, &status);
+            if (!status.ok()) {
+                LOG(WARNING) << ""es create client error: scan_range_idx="" << i
+                             << "", address="" << address
+                             << "", msg="" << status.get_error_msg();
+                return status;
+            }
+
+            try {
+                client->close(result, params);
+            } catch (apache::thrift::transport::TTransportException& e) {
+                RETURN_IF_ERROR(client.reopen());
+                client->close(result, params);
+            }
+        } catch (apache::thrift::TException &e) {
+            std::stringstream ss;
+            ss << ""es close error: scan_range_idx="" << i
+               << "", msg="" << e.what();
+            LOG(WARNING) << ss.str();
+            return Status(TStatusCode::THRIFT_RPC_ERROR, ss.str(), false);
+        }
+
+        Status status(result.status);
+        if (!status.ok()) {
+            LOG(WARNING) << ""es close error: : scan_range_idx="" << i
+                         << "", msg="" << status.get_error_msg();
+            return status;
+        }
+#else
+        TStatus status;
+        result.__set_status(status);
+#endif
+    }
+
+    RETURN_IF_ERROR(ExecNode::close(state));
+    return Status::OK;
+}
+
+void EsScanNode::debug_string(int indentation_level, stringstream* out) const {
+    *out << string(indentation_level * 2, ' ');
+    *out << ""EsScanNode(tupleid="" << _tuple_id;
+    *out << "")"" << std::endl;
+
+    for (int i = 0; i < _children.size(); ++i) {
+        _children[i]->debug_string(indentation_level + 1, out);
+    }
+}
+
+Status EsScanNode::set_scan_ranges(const vector<TScanRangeParams>& scan_ranges) {
+    for (int i = 0; i < scan_ranges.size(); ++i) {
+        TScanRangeParams scan_range = scan_ranges[i];
+        DCHECK(scan_range.scan_range.__isset.es_scan_range);
+        TEsScanRange es_scan_range = scan_range.scan_range.es_scan_range;
+        _scan_ranges.push_back(es_scan_range);
+    }
+
+    _offsets.resize(scan_ranges.size(), 0);
+    return Status::OK;
+}
+
+Status EsScanNode::open_es(TNetworkAddress& address, TExtOpenResult& result, TExtOpenParams& params) {
+#ifndef BE_TEST
+    try {
+        ExtDataSourceServiceClientCache* client_cache = _env->extdatasource_client_cache();
+        Status status;
+        ExtDataSourceServiceConnection client(client_cache, address, 10000, &status);
+        if (!status.ok()) {
+            std::stringstream ss;
+            ss << ""es create client error: address="" << address
+               << "", msg="" << status.get_error_msg();
+            return Status(ss.str());
+        }
+
+        try {
+            client->open(result, params);
+        } catch (apache::thrift::transport::TTransportException& e) {
+            RETURN_IF_ERROR(client.reopen());
+            client->open(result, params);
+        }
+        return Status(result.status);
+    } catch (apache::thrift::TException &e) {
+        std::stringstream ss;
+        ss << ""es open error: address="" << address << "", msg="" << e.what();
+        return Status(ss.str());
+    }
+#else
+    TStatus status;
+    result.__set_status(status);
+    result.__set_scan_handle(""0"");
+    return Status(status);
+#endif
+}
+
+bool EsScanNode::get_disjuncts(ExprContext* context, Expr* conjunct,
+                               vector<TExtPredicate>& disjuncts) {
+    if (TExprNodeType::BINARY_PRED == conjunct->node_type()) {
+        if (conjunct->children().size() != 2) {
+            VLOG(1) << ""get disjuncts fail: number of childs is not 2"";
+            return false;
+        }
+        SlotRef* slotRef;
+        TExprOpcode::type op;
+        Expr* expr;
+        if (TExprNodeType::SLOT_REF == conjunct->get_child(0)->node_type()) {
+            expr = conjunct->get_child(1);
+            slotRef = (SlotRef*)(conjunct->get_child(0));
+            op = conjunct->op();
+        } else if (TExprNodeType::SLOT_REF == conjunct->get_child(1)->node_type()) {
+            expr = conjunct->get_child(0);
+            slotRef = (SlotRef*)(conjunct->get_child(1));
+            op = conjunct->op();
+        } else {
+            VLOG(1) << ""get disjuncts fail: no SLOT_REF child"";
+            return false;
+        }
+
+        std::vector<SlotId> slot_ids;
+        slotRef->get_slot_ids(&slot_ids);
+        SlotDescriptor* slot_desc = nullptr;
+        for (SlotDescriptor* slot : _tuple_desc->slots()) {
+            if (slot->id() == slot_ids[0]) {
+                slot_desc = slot;
+                break;
+            }
+        }
+        if (nullptr == slot_desc) {
+            VLOG(1) << ""get disjuncts fail: slot_desc is null"";
+            return false;
+        }
+
+        TExtColumnDesc columnDesc;
+        columnDesc.__set_name(slot_desc->col_name());
+        columnDesc.__set_type(slot_desc->type().to_thrift());
+        TExtBinaryPredicate binaryPredicate;
+        binaryPredicate.__set_col(columnDesc);
+        binaryPredicate.__set_op(op);
+        binaryPredicate.__set_value(get_literal(context, expr));
+        TExtPredicate predicate;
+        predicate.__set_node_type(TExprNodeType::BINARY_PRED);
+        predicate.__set_binary_predicate(binaryPredicate);
+        disjuncts.push_back(std::move(predicate));
+        return true;
+    } else if (TExprNodeType::COMPOUND_PRED == conjunct->node_type()) {
+        if (TExprOpcode::COMPOUND_OR != conjunct->op()) {
+            VLOG(1) << ""get disjuncts fail: op is not COMPOUND_OR"";
+            return false;
+        }
+        if (!get_disjuncts(context, conjunct->get_child(0), disjuncts)) {
+            return false;
+        }
+        if (!get_disjuncts(context, conjunct->get_child(1), disjuncts)) {
+            return false;
+        }
+        return true;
+    } else {
+        VLOG(1) << ""get disjuncts fail: node type is "" << conjunct->node_type()
+                << "", should be BINARY_PRED or COMPOUND_PRED"";
+        return false;
+    }
+}
+
+TExtLiteral EsScanNode::get_literal(ExprContext* context, Expr* expr) {","[{'comment': '```suggestion\r\nTExtLiteral EsScanNode::to_exe_literal(ExprContext* context, Expr* expr) {\r\n```', 'commenter': 'imay'}]"
450,be/src/exec/es_scan_node.cpp,"@@ -0,0 +1,660 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""es_scan_node.h""
+
+#include <string>
+#include <boost/algorithm/string.hpp>
+#include <gutil/strings/substitute.h>
+
+#include ""gen_cpp/PlanNodes_types.h""
+#include ""gen_cpp/Exprs_types.h""
+#include ""runtime/runtime_state.h""
+#include ""runtime/row_batch.h""
+#include ""runtime/string_value.h""
+#include ""runtime/tuple_row.h""
+#include ""runtime/client_cache.h""
+#include ""util/runtime_profile.h""
+#include ""util/debug_util.h""
+#include ""service/backend_options.h""
+#include ""olap/olap_common.h""
+#include ""olap/utils.h""
+#include ""exprs/expr_context.h""
+#include ""exprs/expr.h""
+#include ""exprs/slot_ref.h""
+
+namespace doris {
+
+// $0 = column type (e.g. INT)
+const string ERROR_INVALID_COL_DATA = ""Data source returned inconsistent column data. ""
+    ""Expected value of type $0 based on column metadata. This likely indicates a ""
+    ""problem with the data source library."";
+const string ERROR_MEM_LIMIT_EXCEEDED = ""DataSourceScanNode::$0() failed to allocate ""
+    ""$1 bytes for $2."";
+
+EsScanNode::EsScanNode(
+        ObjectPool* pool,
+        const TPlanNode& tnode,
+        const DescriptorTbl& descs) :
+            ScanNode(pool, tnode, descs),
+            _tuple_id(tnode.es_scan_node.tuple_id),
+            _scan_range_idx(0) {
+    if (tnode.es_scan_node.__isset.properties) {
+        _properties = tnode.es_scan_node.properties;
+    }
+}
+
+EsScanNode::~EsScanNode() {
+}
+
+Status EsScanNode::prepare(RuntimeState* state) {
+    LOG(INFO) << ""EsScanNode::Prepare"";
+
+    RETURN_IF_ERROR(ScanNode::prepare(state));
+    _tuple_desc = state->desc_tbl().get_tuple_descriptor(_tuple_id);
+    if (nullptr == _tuple_desc) {
+        std::stringstream ss;
+        ss << ""es tuple descriptor is null, _tuple_id="" << _tuple_id;
+        LOG(WARNING) << ss.str();
+        return Status(ss.str());
+    }
+    _env = state->exec_env();
+
+    return Status::OK;
+}
+
+Status EsScanNode::open(RuntimeState* state) {
+    LOG(INFO) << ""EsScanNode::Open"";
+
+    RETURN_IF_ERROR(exec_debug_action(TExecNodePhase::OPEN));
+    RETURN_IF_CANCELLED(state);
+    SCOPED_TIMER(_runtime_profile->total_time_counter());
+    RETURN_IF_ERROR(ExecNode::open(state));
+
+    // TExtOpenParams.row_schema
+    vector<TExtColumnDesc> cols;
+    for (const SlotDescriptor* slot : _tuple_desc->slots()) {
+        TExtColumnDesc col;
+        col.__set_name(slot->col_name());
+        col.__set_type(slot->type().to_thrift());
+        cols.push_back(std::move(col));
+    }
+    TExtTableSchema row_schema;
+    row_schema.cols = std::move(cols);
+    row_schema.__isset.cols = true;
+
+    // TExtOpenParams.predicates
+    vector<vector<TExtPredicate> > predicates;
+    vector<int> conjunct_idxes;
+    for (int i = 0; i < _conjunct_ctxs.size(); ++i) {
+        VLOG(1) << ""conjunct: "" << _conjunct_ctxs[i]->root()->debug_string();
+        vector<TExtPredicate> disjuncts;
+        if (get_disjuncts(_conjunct_ctxs[i], _conjunct_ctxs[i]->root(), disjuncts)) {
+            predicates.push_back(std::move(disjuncts));
+            conjunct_idxes.push_back(i);
+        }
+    }
+
+    // open every scan range
+    int conjunct_accepted_times[_conjunct_ctxs.size()]; 
+    for (int i = 0; i < _scan_ranges.size(); ++i) {
+        TEsScanRange es_scan_range = _scan_ranges[i];
+
+        // TExtOpenParams
+        TExtOpenParams params;
+        params.__set_query_id(state->query_id());
+        _properties[""index""] = es_scan_range.index;
+        if (es_scan_range.__isset.type) {
+            _properties[""type""] = es_scan_range.type;
+        }
+        _properties[""shard_id""] = std::to_string(es_scan_range.shard_id);
+        params.__set_properties(_properties);
+        params.__set_row_schema(row_schema);
+        params.__set_batch_size(state->batch_size());
+        params.__set_predicates(predicates);
+        TExtOpenResult result;
+
+        // check es host
+        if (es_scan_range.es_hosts.empty()) {
+            std::stringstream ss;
+            ss << ""es fail to open: hosts empty"";
+            LOG(ERROR) << ss.str();
+            return Status(ss.str());
+        }
+
+        // choose an es node, local is better
+        TNetworkAddress es_host_selected = es_scan_range.es_hosts[0];
+        int selected_idx = 0;
+        for (int j = 0; j < es_scan_range.es_hosts.size(); j++) {
+            TNetworkAddress& es_host = es_scan_range.es_hosts[j];
+            if (es_host.hostname == BackendOptions::get_localhost()) {
+                es_host_selected = es_host;
+                selected_idx = j;
+                break;
+            }
+        }
+
+        // if shard not found, try other nodes
+        Status status = open_es(es_host_selected, result, params);
+        if (status.code() == TStatusCode::ES_SHARD_NOT_FOUND) {
+            for (int j = 0; j < es_scan_range.es_hosts.size(); j++) {
+                if (j == selected_idx) continue;
+                es_host_selected = es_scan_range.es_hosts[j];
+                status = open_es(es_host_selected, result, params);
+                if (status.code() == TStatusCode::ES_SHARD_NOT_FOUND) {
+                    continue;
+                } else {
+                    break;
+                }
+            }
+        }
+
+        if (!status.ok()) {
+            LOG(WARNING) << ""es open error: scan_range_idx="" << i
+                         << "", address="" << es_host_selected
+                         << "", msg="" << status.get_error_msg();
+            return status;
+        }
+
+        // get accepted_conjuncts
+        if (result.__isset.accepted_conjuncts) {
+            for (int conjunct_index : result.accepted_conjuncts) {
+                conjunct_accepted_times[conjunct_index]++;
+            }
+        }
+
+        _addresses.push_back(es_host_selected);
+        _scan_handles.push_back(result.scan_handle);
+        VLOG(1) << ""es open success: scan_range_idx="" << i
+                << "", params="" << apache::thrift::ThriftDebugString(params)
+                << "", result="" << apache::thrift::ThriftDebugString(result);
+        }
+
+    // remove those conjuncts that conjunct_accepted_times[i] == _scan_ranges.size()
+    for (int i = conjunct_idxes.size() - 1; i >= 0; --i) {
+        if (conjunct_accepted_times[i] == _scan_ranges.size()) {
+            _conjunct_ctxs.erase(_conjunct_ctxs.begin() + i);
+        }
+    }
+
+    return Status::OK;
+}
+
+Status EsScanNode::get_next(RuntimeState* state, RowBatch* row_batch, bool* eos) {
+    LOG(INFO) << ""EsScanNode::GetNext"";
+
+    RETURN_IF_ERROR(exec_debug_action(TExecNodePhase::GETNEXT));
+    RETURN_IF_CANCELLED(state);
+    SCOPED_TIMER(_runtime_profile->total_time_counter());
+    SCOPED_TIMER(materialize_tuple_timer());
+
+    // create tuple
+    MemPool* tuple_pool = row_batch->tuple_data_pool();
+    int64_t tuple_buffer_size;
+    uint8_t* tuple_buffer = nullptr;
+    RETURN_IF_ERROR(row_batch->resize_and_allocate_tuple_buffer(state, &tuple_buffer_size, &tuple_buffer));
+    Tuple* tuple = reinterpret_cast<Tuple*>(tuple_buffer);
+    
+    // get batch
+    TExtGetNextResult result;
+    RETURN_IF_ERROR(get_next_from_es(result));
+    VLOG(1) << ""es get next success: result="" << apache::thrift::ThriftDebugString(result);
+    _offsets[_scan_range_idx] += result.rows.num_rows;
+
+    // convert
+    VLOG(1) << ""begin to convert: scan_range_idx="" << _scan_range_idx
+            << "", num_rows="" << result.rows.num_rows;
+    vector<TExtColumnData>& cols = result.rows.cols;
+    // indexes of the next non-null value in the row batch, per column. 
+    vector<int> cols_next_val_idx(_tuple_desc->slots().size(), 0);
+    for (int row_idx = 0; row_idx < result.rows.num_rows; row_idx++) {
+        if (reached_limit()) {
+            *eos = true;
+            break;
+        }
+        RETURN_IF_ERROR(materialize_row(tuple_pool, tuple, cols, row_idx, cols_next_val_idx));
+        TupleRow* tuple_row = row_batch->get_row(row_batch->add_row());
+        tuple_row->set_tuple(0, tuple);
+        if (ExecNode::eval_conjuncts(_conjunct_ctxs.data(), _conjunct_ctxs.size(), tuple_row)) {
+            row_batch->commit_last_row();
+            tuple = reinterpret_cast<Tuple*>(
+                reinterpret_cast<uint8_t*>(tuple) + _tuple_desc->byte_size());
+            ++_num_rows_returned;
+        }
+    }
+
+    VLOG(1) << ""finish one batch: num_rows="" << row_batch->num_rows();
+    COUNTER_SET(_rows_returned_counter, _num_rows_returned);
+    if (result.__isset.eos && result.eos) {
+        VLOG(1) << ""es finish one scan_range: scan_range_idx="" << _scan_range_idx;
+        ++_scan_range_idx;
+    }
+    if (_scan_range_idx == _scan_ranges.size()) {
+        *eos = true;
+    }
+
+    return Status::OK;
+}
+
+Status EsScanNode::close(RuntimeState* state) {
+    if (is_closed()) return Status::OK;
+    LOG(INFO) << ""EsScanNode::Close"";
+    RETURN_IF_ERROR(exec_debug_action(TExecNodePhase::CLOSE));
+    SCOPED_TIMER(_runtime_profile->total_time_counter());
+
+    for (int i = 0; i < _addresses.size(); ++i) {
+        TExtCloseParams params;
+        params.__set_scan_handle(_scan_handles[i]);
+        TExtCloseResult result;
+
+#ifndef BE_TEST
+        const TNetworkAddress& address = _addresses[i];
+        try {
+            Status status;
+            ExtDataSourceServiceClientCache* client_cache = _env->extdatasource_client_cache();
+            ExtDataSourceServiceConnection client(client_cache, address, 10000, &status);
+            if (!status.ok()) {
+                LOG(WARNING) << ""es create client error: scan_range_idx="" << i
+                             << "", address="" << address
+                             << "", msg="" << status.get_error_msg();
+                return status;
+            }
+
+            try {
+                client->close(result, params);
+            } catch (apache::thrift::transport::TTransportException& e) {
+                RETURN_IF_ERROR(client.reopen());
+                client->close(result, params);
+            }
+        } catch (apache::thrift::TException &e) {
+            std::stringstream ss;
+            ss << ""es close error: scan_range_idx="" << i
+               << "", msg="" << e.what();
+            LOG(WARNING) << ss.str();
+            return Status(TStatusCode::THRIFT_RPC_ERROR, ss.str(), false);
+        }
+
+        Status status(result.status);
+        if (!status.ok()) {
+            LOG(WARNING) << ""es close error: : scan_range_idx="" << i
+                         << "", msg="" << status.get_error_msg();
+            return status;
+        }
+#else
+        TStatus status;
+        result.__set_status(status);
+#endif
+    }
+
+    RETURN_IF_ERROR(ExecNode::close(state));
+    return Status::OK;
+}
+
+void EsScanNode::debug_string(int indentation_level, stringstream* out) const {
+    *out << string(indentation_level * 2, ' ');
+    *out << ""EsScanNode(tupleid="" << _tuple_id;
+    *out << "")"" << std::endl;
+
+    for (int i = 0; i < _children.size(); ++i) {
+        _children[i]->debug_string(indentation_level + 1, out);
+    }
+}
+
+Status EsScanNode::set_scan_ranges(const vector<TScanRangeParams>& scan_ranges) {
+    for (int i = 0; i < scan_ranges.size(); ++i) {
+        TScanRangeParams scan_range = scan_ranges[i];
+        DCHECK(scan_range.scan_range.__isset.es_scan_range);
+        TEsScanRange es_scan_range = scan_range.scan_range.es_scan_range;
+        _scan_ranges.push_back(es_scan_range);
+    }
+
+    _offsets.resize(scan_ranges.size(), 0);
+    return Status::OK;
+}
+
+Status EsScanNode::open_es(TNetworkAddress& address, TExtOpenResult& result, TExtOpenParams& params) {
+#ifndef BE_TEST
+    try {
+        ExtDataSourceServiceClientCache* client_cache = _env->extdatasource_client_cache();
+        Status status;
+        ExtDataSourceServiceConnection client(client_cache, address, 10000, &status);
+        if (!status.ok()) {
+            std::stringstream ss;
+            ss << ""es create client error: address="" << address
+               << "", msg="" << status.get_error_msg();
+            return Status(ss.str());
+        }
+
+        try {
+            client->open(result, params);
+        } catch (apache::thrift::transport::TTransportException& e) {
+            RETURN_IF_ERROR(client.reopen());
+            client->open(result, params);
+        }
+        return Status(result.status);
+    } catch (apache::thrift::TException &e) {
+        std::stringstream ss;
+        ss << ""es open error: address="" << address << "", msg="" << e.what();
+        return Status(ss.str());
+    }
+#else
+    TStatus status;
+    result.__set_status(status);
+    result.__set_scan_handle(""0"");
+    return Status(status);
+#endif
+}
+
+bool EsScanNode::get_disjuncts(ExprContext* context, Expr* conjunct,
+                               vector<TExtPredicate>& disjuncts) {
+    if (TExprNodeType::BINARY_PRED == conjunct->node_type()) {
+        if (conjunct->children().size() != 2) {
+            VLOG(1) << ""get disjuncts fail: number of childs is not 2"";
+            return false;
+        }
+        SlotRef* slotRef;
+        TExprOpcode::type op;
+        Expr* expr;
+        if (TExprNodeType::SLOT_REF == conjunct->get_child(0)->node_type()) {
+            expr = conjunct->get_child(1);
+            slotRef = (SlotRef*)(conjunct->get_child(0));
+            op = conjunct->op();
+        } else if (TExprNodeType::SLOT_REF == conjunct->get_child(1)->node_type()) {
+            expr = conjunct->get_child(0);
+            slotRef = (SlotRef*)(conjunct->get_child(1));
+            op = conjunct->op();
+        } else {
+            VLOG(1) << ""get disjuncts fail: no SLOT_REF child"";
+            return false;
+        }
+
+        std::vector<SlotId> slot_ids;
+        slotRef->get_slot_ids(&slot_ids);
+        SlotDescriptor* slot_desc = nullptr;
+        for (SlotDescriptor* slot : _tuple_desc->slots()) {
+            if (slot->id() == slot_ids[0]) {
+                slot_desc = slot;
+                break;
+            }
+        }
+        if (nullptr == slot_desc) {
+            VLOG(1) << ""get disjuncts fail: slot_desc is null"";
+            return false;
+        }
+
+        TExtColumnDesc columnDesc;
+        columnDesc.__set_name(slot_desc->col_name());
+        columnDesc.__set_type(slot_desc->type().to_thrift());
+        TExtBinaryPredicate binaryPredicate;
+        binaryPredicate.__set_col(columnDesc);
+        binaryPredicate.__set_op(op);
+        binaryPredicate.__set_value(get_literal(context, expr));
+        TExtPredicate predicate;
+        predicate.__set_node_type(TExprNodeType::BINARY_PRED);
+        predicate.__set_binary_predicate(binaryPredicate);
+        disjuncts.push_back(std::move(predicate));
+        return true;
+    } else if (TExprNodeType::COMPOUND_PRED == conjunct->node_type()) {
+        if (TExprOpcode::COMPOUND_OR != conjunct->op()) {
+            VLOG(1) << ""get disjuncts fail: op is not COMPOUND_OR"";
+            return false;
+        }
+        if (!get_disjuncts(context, conjunct->get_child(0), disjuncts)) {
+            return false;
+        }
+        if (!get_disjuncts(context, conjunct->get_child(1), disjuncts)) {
+            return false;
+        }
+        return true;
+    } else {
+        VLOG(1) << ""get disjuncts fail: node type is "" << conjunct->node_type()
+                << "", should be BINARY_PRED or COMPOUND_PRED"";
+        return false;
+    }
+}
+
+TExtLiteral EsScanNode::get_literal(ExprContext* context, Expr* expr) {
+    void* value = context->get_value(expr, NULL);
+    TExtLiteral literal;
+    literal.__set_node_type(expr->node_type());
+    switch (expr->node_type()) {
+    case TExprNodeType::BOOL_LITERAL: {
+        TBoolLiteral bool_literal;
+        bool_literal.__set_value(*reinterpret_cast<bool*>(value));
+        literal.__set_bool_literal(bool_literal);
+        break;
+    }
+    case TExprNodeType::DATE_LITERAL: {
+        DateTimeValue date_value = *reinterpret_cast<DateTimeValue*>(value);
+        char str[MAX_DTVALUE_STR_LEN];
+        date_value.to_string(str);
+        TDateLiteral date_literal;
+        date_literal.__set_value(str);
+        literal.__set_date_literal(date_literal);
+        break;
+    }
+    case TExprNodeType::FLOAT_LITERAL: {
+        TFloatLiteral float_literal;
+        float_literal.__set_value(*reinterpret_cast<float*>(value));
+        literal.__set_float_literal(float_literal);
+        break;
+    }
+    case TExprNodeType::INT_LITERAL: {
+        TIntLiteral int_literal;
+        int_literal.__set_value(*reinterpret_cast<int32_t*>(value));
+        literal.__set_int_literal(int_literal);
+        break;
+    }
+    case TExprNodeType::STRING_LITERAL: {
+        TStringLiteral string_literal;
+        string_literal.__set_value(*reinterpret_cast<string*>(value));
+        literal.__set_string_literal(string_literal);
+        break;
+    }
+    case TExprNodeType::DECIMAL_LITERAL: {
+        TDecimalLiteral decimal_literal;
+        decimal_literal.__set_value(reinterpret_cast<DecimalValue*>(value)->to_string());
+        literal.__set_decimal_literal(decimal_literal);
+        break;
+    }
+    case TExprNodeType::LARGE_INT_LITERAL: {
+        char buf[48];
+        int len = 48;
+        char* v = LargeIntValue::to_string(*reinterpret_cast<__int128*>(value), buf, &len);
+        TLargeIntLiteral large_int_literal;
+        large_int_literal.__set_value(v);
+        literal.__set_large_int_literal(large_int_literal);
+        break;
+    }
+    default:
+        break;
+    }
+    return literal;
+}
+
+Status EsScanNode::get_next_from_es(TExtGetNextResult& result) {
+    TExtGetNextParams params;
+    params.__set_scan_handle(_scan_handles[_scan_range_idx]);
+    params.__set_offset(_offsets[_scan_range_idx]);
+
+    // getNext
+    const TNetworkAddress &address = _addresses[_scan_range_idx];
+#ifndef BE_TEST
+    try {
+        Status create_client_status;
+        ExtDataSourceServiceClientCache *client_cache = _env->extdatasource_client_cache();
+        ExtDataSourceServiceConnection client(client_cache, address, 10000, &create_client_status);
+        if (!create_client_status.ok()) {
+            LOG(WARNING) << ""es create client error: scan_range_idx="" << _scan_range_idx
+                         << "", address="" << address
+                         << "", msg="" << create_client_status.get_error_msg();
+            return create_client_status;
+        }
+
+        try {
+            client->getNext(result, params);
+        } catch (apache::thrift::transport::TTransportException& e) {
+            RETURN_IF_ERROR(client.reopen());
+            client->getNext(result, params);
+        }
+    } catch (apache::thrift::TException &e) {
+        std::stringstream ss;
+        ss << ""es get_next error: scan_range_idx="" << _scan_range_idx
+           << "", msg="" << e.what();
+        LOG(WARNING) << ss.str();
+        return Status(TStatusCode::THRIFT_RPC_ERROR, ss.str(), false);
+    }
+#else
+    TStatus status;
+    result.__set_status(status);
+    result.__set_eos(true);
+    TExtColumnData col_data;
+    std::vector<bool> is_null;
+    is_null.push_back(false);
+    col_data.__set_is_null(is_null);
+    std::vector<int32_t> int_vals;
+    int_vals.push_back(1);
+    int_vals.push_back(2);
+    col_data.__set_int_vals(int_vals);
+    std::vector<TExtColumnData> cols;
+    cols.push_back(col_data);
+    TExtRowBatch rows;
+    rows.__set_cols(cols);
+    rows.__set_num_rows(2);
+    result.__set_rows(rows);
+    return Status(status);
+#endif
+
+    // check result
+    Status get_next_status(result.status);
+    if (!get_next_status.ok()) {
+        LOG(WARNING) << ""es get_next error: scan_range_idx="" << _scan_range_idx
+                     << "", address="" << address
+                     << "", msg="" << get_next_status.get_error_msg();
+        return get_next_status;
+    }
+    if (!result.__isset.rows || !result.rows.__isset.num_rows) {
+        std::stringstream ss;
+        ss << ""es get_next error: scan_range_idx="" << _scan_range_idx
+           << "", msg=rows or num_rows not in result"";
+        LOG(WARNING) << ss.str();
+        return Status(ss.str());
+    }
+
+    return Status::OK;
+}
+
+Status EsScanNode::materialize_row(MemPool* tuple_pool, Tuple* tuple,
+                                   const vector<TExtColumnData>& cols, int row_idx,
+                                   vector<int>& cols_next_val_idx) {
+  tuple->init(_tuple_desc->byte_size());
+
+  for (int i = 0; i < _tuple_desc->slots().size(); ++i) {
+    const SlotDescriptor* slot_desc = _tuple_desc->slots()[i];
+    void* slot = tuple->get_slot(slot_desc->tuple_offset());
+    const TExtColumnData& col = cols[i];","[{'comment': 'If slot_desc->is_materialized() is false, you should skip this slot', 'commenter': 'imay'}]"
450,be/src/exec/es_scan_node.cpp,"@@ -0,0 +1,660 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""es_scan_node.h""
+
+#include <string>
+#include <boost/algorithm/string.hpp>
+#include <gutil/strings/substitute.h>
+
+#include ""gen_cpp/PlanNodes_types.h""
+#include ""gen_cpp/Exprs_types.h""
+#include ""runtime/runtime_state.h""
+#include ""runtime/row_batch.h""
+#include ""runtime/string_value.h""
+#include ""runtime/tuple_row.h""
+#include ""runtime/client_cache.h""
+#include ""util/runtime_profile.h""
+#include ""util/debug_util.h""
+#include ""service/backend_options.h""
+#include ""olap/olap_common.h""
+#include ""olap/utils.h""
+#include ""exprs/expr_context.h""
+#include ""exprs/expr.h""
+#include ""exprs/slot_ref.h""
+
+namespace doris {
+
+// $0 = column type (e.g. INT)
+const string ERROR_INVALID_COL_DATA = ""Data source returned inconsistent column data. ""
+    ""Expected value of type $0 based on column metadata. This likely indicates a ""
+    ""problem with the data source library."";
+const string ERROR_MEM_LIMIT_EXCEEDED = ""DataSourceScanNode::$0() failed to allocate ""
+    ""$1 bytes for $2."";
+
+EsScanNode::EsScanNode(
+        ObjectPool* pool,
+        const TPlanNode& tnode,
+        const DescriptorTbl& descs) :
+            ScanNode(pool, tnode, descs),
+            _tuple_id(tnode.es_scan_node.tuple_id),
+            _scan_range_idx(0) {
+    if (tnode.es_scan_node.__isset.properties) {
+        _properties = tnode.es_scan_node.properties;
+    }
+}
+
+EsScanNode::~EsScanNode() {
+}
+
+Status EsScanNode::prepare(RuntimeState* state) {
+    LOG(INFO) << ""EsScanNode::Prepare"";
+
+    RETURN_IF_ERROR(ScanNode::prepare(state));
+    _tuple_desc = state->desc_tbl().get_tuple_descriptor(_tuple_id);
+    if (nullptr == _tuple_desc) {
+        std::stringstream ss;
+        ss << ""es tuple descriptor is null, _tuple_id="" << _tuple_id;
+        LOG(WARNING) << ss.str();
+        return Status(ss.str());
+    }
+    _env = state->exec_env();
+
+    return Status::OK;
+}
+
+Status EsScanNode::open(RuntimeState* state) {
+    LOG(INFO) << ""EsScanNode::Open"";
+
+    RETURN_IF_ERROR(exec_debug_action(TExecNodePhase::OPEN));
+    RETURN_IF_CANCELLED(state);
+    SCOPED_TIMER(_runtime_profile->total_time_counter());
+    RETURN_IF_ERROR(ExecNode::open(state));
+
+    // TExtOpenParams.row_schema
+    vector<TExtColumnDesc> cols;
+    for (const SlotDescriptor* slot : _tuple_desc->slots()) {
+        TExtColumnDesc col;
+        col.__set_name(slot->col_name());
+        col.__set_type(slot->type().to_thrift());
+        cols.push_back(std::move(col));
+    }
+    TExtTableSchema row_schema;
+    row_schema.cols = std::move(cols);
+    row_schema.__isset.cols = true;
+
+    // TExtOpenParams.predicates
+    vector<vector<TExtPredicate> > predicates;
+    vector<int> conjunct_idxes;
+    for (int i = 0; i < _conjunct_ctxs.size(); ++i) {
+        VLOG(1) << ""conjunct: "" << _conjunct_ctxs[i]->root()->debug_string();
+        vector<TExtPredicate> disjuncts;
+        if (get_disjuncts(_conjunct_ctxs[i], _conjunct_ctxs[i]->root(), disjuncts)) {
+            predicates.push_back(std::move(disjuncts));
+            conjunct_idxes.push_back(i);
+        }
+    }
+
+    // open every scan range
+    int conjunct_accepted_times[_conjunct_ctxs.size()]; 
+    for (int i = 0; i < _scan_ranges.size(); ++i) {
+        TEsScanRange es_scan_range = _scan_ranges[i];
+
+        // TExtOpenParams
+        TExtOpenParams params;
+        params.__set_query_id(state->query_id());
+        _properties[""index""] = es_scan_range.index;
+        if (es_scan_range.__isset.type) {
+            _properties[""type""] = es_scan_range.type;
+        }
+        _properties[""shard_id""] = std::to_string(es_scan_range.shard_id);
+        params.__set_properties(_properties);
+        params.__set_row_schema(row_schema);
+        params.__set_batch_size(state->batch_size());
+        params.__set_predicates(predicates);
+        TExtOpenResult result;
+
+        // check es host
+        if (es_scan_range.es_hosts.empty()) {
+            std::stringstream ss;
+            ss << ""es fail to open: hosts empty"";
+            LOG(ERROR) << ss.str();
+            return Status(ss.str());
+        }
+
+        // choose an es node, local is better
+        TNetworkAddress es_host_selected = es_scan_range.es_hosts[0];
+        int selected_idx = 0;
+        for (int j = 0; j < es_scan_range.es_hosts.size(); j++) {
+            TNetworkAddress& es_host = es_scan_range.es_hosts[j];
+            if (es_host.hostname == BackendOptions::get_localhost()) {
+                es_host_selected = es_host;
+                selected_idx = j;
+                break;
+            }
+        }
+
+        // if shard not found, try other nodes
+        Status status = open_es(es_host_selected, result, params);
+        if (status.code() == TStatusCode::ES_SHARD_NOT_FOUND) {
+            for (int j = 0; j < es_scan_range.es_hosts.size(); j++) {
+                if (j == selected_idx) continue;
+                es_host_selected = es_scan_range.es_hosts[j];
+                status = open_es(es_host_selected, result, params);
+                if (status.code() == TStatusCode::ES_SHARD_NOT_FOUND) {
+                    continue;
+                } else {
+                    break;
+                }
+            }
+        }
+
+        if (!status.ok()) {
+            LOG(WARNING) << ""es open error: scan_range_idx="" << i
+                         << "", address="" << es_host_selected
+                         << "", msg="" << status.get_error_msg();
+            return status;
+        }
+
+        // get accepted_conjuncts
+        if (result.__isset.accepted_conjuncts) {
+            for (int conjunct_index : result.accepted_conjuncts) {
+                conjunct_accepted_times[conjunct_index]++;
+            }
+        }
+
+        _addresses.push_back(es_host_selected);
+        _scan_handles.push_back(result.scan_handle);
+        VLOG(1) << ""es open success: scan_range_idx="" << i
+                << "", params="" << apache::thrift::ThriftDebugString(params)
+                << "", result="" << apache::thrift::ThriftDebugString(result);
+        }
+
+    // remove those conjuncts that conjunct_accepted_times[i] == _scan_ranges.size()
+    for (int i = conjunct_idxes.size() - 1; i >= 0; --i) {
+        if (conjunct_accepted_times[i] == _scan_ranges.size()) {
+            _conjunct_ctxs.erase(_conjunct_ctxs.begin() + i);
+        }
+    }
+
+    return Status::OK;
+}
+
+Status EsScanNode::get_next(RuntimeState* state, RowBatch* row_batch, bool* eos) {
+    LOG(INFO) << ""EsScanNode::GetNext"";
+
+    RETURN_IF_ERROR(exec_debug_action(TExecNodePhase::GETNEXT));
+    RETURN_IF_CANCELLED(state);
+    SCOPED_TIMER(_runtime_profile->total_time_counter());
+    SCOPED_TIMER(materialize_tuple_timer());
+
+    // create tuple
+    MemPool* tuple_pool = row_batch->tuple_data_pool();
+    int64_t tuple_buffer_size;
+    uint8_t* tuple_buffer = nullptr;
+    RETURN_IF_ERROR(row_batch->resize_and_allocate_tuple_buffer(state, &tuple_buffer_size, &tuple_buffer));
+    Tuple* tuple = reinterpret_cast<Tuple*>(tuple_buffer);
+    
+    // get batch
+    TExtGetNextResult result;
+    RETURN_IF_ERROR(get_next_from_es(result));
+    VLOG(1) << ""es get next success: result="" << apache::thrift::ThriftDebugString(result);
+    _offsets[_scan_range_idx] += result.rows.num_rows;
+
+    // convert
+    VLOG(1) << ""begin to convert: scan_range_idx="" << _scan_range_idx
+            << "", num_rows="" << result.rows.num_rows;
+    vector<TExtColumnData>& cols = result.rows.cols;
+    // indexes of the next non-null value in the row batch, per column. 
+    vector<int> cols_next_val_idx(_tuple_desc->slots().size(), 0);
+    for (int row_idx = 0; row_idx < result.rows.num_rows; row_idx++) {
+        if (reached_limit()) {
+            *eos = true;
+            break;
+        }
+        RETURN_IF_ERROR(materialize_row(tuple_pool, tuple, cols, row_idx, cols_next_val_idx));
+        TupleRow* tuple_row = row_batch->get_row(row_batch->add_row());
+        tuple_row->set_tuple(0, tuple);
+        if (ExecNode::eval_conjuncts(_conjunct_ctxs.data(), _conjunct_ctxs.size(), tuple_row)) {
+            row_batch->commit_last_row();
+            tuple = reinterpret_cast<Tuple*>(
+                reinterpret_cast<uint8_t*>(tuple) + _tuple_desc->byte_size());
+            ++_num_rows_returned;
+        }
+    }
+
+    VLOG(1) << ""finish one batch: num_rows="" << row_batch->num_rows();
+    COUNTER_SET(_rows_returned_counter, _num_rows_returned);
+    if (result.__isset.eos && result.eos) {
+        VLOG(1) << ""es finish one scan_range: scan_range_idx="" << _scan_range_idx;
+        ++_scan_range_idx;
+    }
+    if (_scan_range_idx == _scan_ranges.size()) {
+        *eos = true;
+    }
+
+    return Status::OK;
+}
+
+Status EsScanNode::close(RuntimeState* state) {
+    if (is_closed()) return Status::OK;
+    LOG(INFO) << ""EsScanNode::Close"";
+    RETURN_IF_ERROR(exec_debug_action(TExecNodePhase::CLOSE));
+    SCOPED_TIMER(_runtime_profile->total_time_counter());
+
+    for (int i = 0; i < _addresses.size(); ++i) {
+        TExtCloseParams params;
+        params.__set_scan_handle(_scan_handles[i]);
+        TExtCloseResult result;
+
+#ifndef BE_TEST
+        const TNetworkAddress& address = _addresses[i];
+        try {
+            Status status;
+            ExtDataSourceServiceClientCache* client_cache = _env->extdatasource_client_cache();
+            ExtDataSourceServiceConnection client(client_cache, address, 10000, &status);
+            if (!status.ok()) {
+                LOG(WARNING) << ""es create client error: scan_range_idx="" << i
+                             << "", address="" << address
+                             << "", msg="" << status.get_error_msg();
+                return status;
+            }
+
+            try {
+                client->close(result, params);
+            } catch (apache::thrift::transport::TTransportException& e) {
+                RETURN_IF_ERROR(client.reopen());
+                client->close(result, params);
+            }
+        } catch (apache::thrift::TException &e) {
+            std::stringstream ss;
+            ss << ""es close error: scan_range_idx="" << i
+               << "", msg="" << e.what();
+            LOG(WARNING) << ss.str();
+            return Status(TStatusCode::THRIFT_RPC_ERROR, ss.str(), false);
+        }
+
+        Status status(result.status);
+        if (!status.ok()) {
+            LOG(WARNING) << ""es close error: : scan_range_idx="" << i
+                         << "", msg="" << status.get_error_msg();
+            return status;
+        }
+#else
+        TStatus status;
+        result.__set_status(status);
+#endif
+    }
+
+    RETURN_IF_ERROR(ExecNode::close(state));
+    return Status::OK;
+}
+
+void EsScanNode::debug_string(int indentation_level, stringstream* out) const {
+    *out << string(indentation_level * 2, ' ');
+    *out << ""EsScanNode(tupleid="" << _tuple_id;
+    *out << "")"" << std::endl;
+
+    for (int i = 0; i < _children.size(); ++i) {
+        _children[i]->debug_string(indentation_level + 1, out);
+    }
+}
+
+Status EsScanNode::set_scan_ranges(const vector<TScanRangeParams>& scan_ranges) {
+    for (int i = 0; i < scan_ranges.size(); ++i) {
+        TScanRangeParams scan_range = scan_ranges[i];
+        DCHECK(scan_range.scan_range.__isset.es_scan_range);
+        TEsScanRange es_scan_range = scan_range.scan_range.es_scan_range;
+        _scan_ranges.push_back(es_scan_range);
+    }
+
+    _offsets.resize(scan_ranges.size(), 0);
+    return Status::OK;
+}
+
+Status EsScanNode::open_es(TNetworkAddress& address, TExtOpenResult& result, TExtOpenParams& params) {
+#ifndef BE_TEST
+    try {
+        ExtDataSourceServiceClientCache* client_cache = _env->extdatasource_client_cache();
+        Status status;
+        ExtDataSourceServiceConnection client(client_cache, address, 10000, &status);
+        if (!status.ok()) {
+            std::stringstream ss;
+            ss << ""es create client error: address="" << address
+               << "", msg="" << status.get_error_msg();
+            return Status(ss.str());
+        }
+
+        try {
+            client->open(result, params);
+        } catch (apache::thrift::transport::TTransportException& e) {
+            RETURN_IF_ERROR(client.reopen());
+            client->open(result, params);
+        }
+        return Status(result.status);
+    } catch (apache::thrift::TException &e) {
+        std::stringstream ss;
+        ss << ""es open error: address="" << address << "", msg="" << e.what();
+        return Status(ss.str());
+    }
+#else
+    TStatus status;
+    result.__set_status(status);
+    result.__set_scan_handle(""0"");
+    return Status(status);
+#endif
+}
+
+bool EsScanNode::get_disjuncts(ExprContext* context, Expr* conjunct,
+                               vector<TExtPredicate>& disjuncts) {
+    if (TExprNodeType::BINARY_PRED == conjunct->node_type()) {
+        if (conjunct->children().size() != 2) {
+            VLOG(1) << ""get disjuncts fail: number of childs is not 2"";
+            return false;
+        }
+        SlotRef* slotRef;
+        TExprOpcode::type op;
+        Expr* expr;
+        if (TExprNodeType::SLOT_REF == conjunct->get_child(0)->node_type()) {
+            expr = conjunct->get_child(1);
+            slotRef = (SlotRef*)(conjunct->get_child(0));
+            op = conjunct->op();
+        } else if (TExprNodeType::SLOT_REF == conjunct->get_child(1)->node_type()) {
+            expr = conjunct->get_child(0);
+            slotRef = (SlotRef*)(conjunct->get_child(1));
+            op = conjunct->op();
+        } else {
+            VLOG(1) << ""get disjuncts fail: no SLOT_REF child"";
+            return false;
+        }
+
+        std::vector<SlotId> slot_ids;
+        slotRef->get_slot_ids(&slot_ids);
+        SlotDescriptor* slot_desc = nullptr;
+        for (SlotDescriptor* slot : _tuple_desc->slots()) {
+            if (slot->id() == slot_ids[0]) {
+                slot_desc = slot;
+                break;
+            }
+        }
+        if (nullptr == slot_desc) {
+            VLOG(1) << ""get disjuncts fail: slot_desc is null"";
+            return false;
+        }
+
+        TExtColumnDesc columnDesc;
+        columnDesc.__set_name(slot_desc->col_name());
+        columnDesc.__set_type(slot_desc->type().to_thrift());
+        TExtBinaryPredicate binaryPredicate;
+        binaryPredicate.__set_col(columnDesc);
+        binaryPredicate.__set_op(op);
+        binaryPredicate.__set_value(get_literal(context, expr));
+        TExtPredicate predicate;
+        predicate.__set_node_type(TExprNodeType::BINARY_PRED);
+        predicate.__set_binary_predicate(binaryPredicate);
+        disjuncts.push_back(std::move(predicate));
+        return true;
+    } else if (TExprNodeType::COMPOUND_PRED == conjunct->node_type()) {
+        if (TExprOpcode::COMPOUND_OR != conjunct->op()) {
+            VLOG(1) << ""get disjuncts fail: op is not COMPOUND_OR"";
+            return false;
+        }
+        if (!get_disjuncts(context, conjunct->get_child(0), disjuncts)) {
+            return false;
+        }
+        if (!get_disjuncts(context, conjunct->get_child(1), disjuncts)) {
+            return false;
+        }
+        return true;
+    } else {
+        VLOG(1) << ""get disjuncts fail: node type is "" << conjunct->node_type()
+                << "", should be BINARY_PRED or COMPOUND_PRED"";
+        return false;
+    }
+}
+
+TExtLiteral EsScanNode::get_literal(ExprContext* context, Expr* expr) {
+    void* value = context->get_value(expr, NULL);
+    TExtLiteral literal;
+    literal.__set_node_type(expr->node_type());
+    switch (expr->node_type()) {
+    case TExprNodeType::BOOL_LITERAL: {
+        TBoolLiteral bool_literal;
+        bool_literal.__set_value(*reinterpret_cast<bool*>(value));
+        literal.__set_bool_literal(bool_literal);
+        break;
+    }
+    case TExprNodeType::DATE_LITERAL: {
+        DateTimeValue date_value = *reinterpret_cast<DateTimeValue*>(value);
+        char str[MAX_DTVALUE_STR_LEN];
+        date_value.to_string(str);
+        TDateLiteral date_literal;
+        date_literal.__set_value(str);
+        literal.__set_date_literal(date_literal);
+        break;
+    }
+    case TExprNodeType::FLOAT_LITERAL: {
+        TFloatLiteral float_literal;
+        float_literal.__set_value(*reinterpret_cast<float*>(value));
+        literal.__set_float_literal(float_literal);
+        break;
+    }
+    case TExprNodeType::INT_LITERAL: {
+        TIntLiteral int_literal;
+        int_literal.__set_value(*reinterpret_cast<int32_t*>(value));
+        literal.__set_int_literal(int_literal);
+        break;
+    }
+    case TExprNodeType::STRING_LITERAL: {
+        TStringLiteral string_literal;
+        string_literal.__set_value(*reinterpret_cast<string*>(value));
+        literal.__set_string_literal(string_literal);
+        break;
+    }
+    case TExprNodeType::DECIMAL_LITERAL: {
+        TDecimalLiteral decimal_literal;
+        decimal_literal.__set_value(reinterpret_cast<DecimalValue*>(value)->to_string());
+        literal.__set_decimal_literal(decimal_literal);
+        break;
+    }
+    case TExprNodeType::LARGE_INT_LITERAL: {
+        char buf[48];
+        int len = 48;
+        char* v = LargeIntValue::to_string(*reinterpret_cast<__int128*>(value), buf, &len);
+        TLargeIntLiteral large_int_literal;
+        large_int_literal.__set_value(v);
+        literal.__set_large_int_literal(large_int_literal);
+        break;
+    }
+    default:
+        break;
+    }
+    return literal;
+}
+
+Status EsScanNode::get_next_from_es(TExtGetNextResult& result) {
+    TExtGetNextParams params;
+    params.__set_scan_handle(_scan_handles[_scan_range_idx]);
+    params.__set_offset(_offsets[_scan_range_idx]);
+
+    // getNext
+    const TNetworkAddress &address = _addresses[_scan_range_idx];
+#ifndef BE_TEST
+    try {
+        Status create_client_status;
+        ExtDataSourceServiceClientCache *client_cache = _env->extdatasource_client_cache();
+        ExtDataSourceServiceConnection client(client_cache, address, 10000, &create_client_status);
+        if (!create_client_status.ok()) {
+            LOG(WARNING) << ""es create client error: scan_range_idx="" << _scan_range_idx
+                         << "", address="" << address
+                         << "", msg="" << create_client_status.get_error_msg();
+            return create_client_status;
+        }
+
+        try {
+            client->getNext(result, params);
+        } catch (apache::thrift::transport::TTransportException& e) {
+            RETURN_IF_ERROR(client.reopen());
+            client->getNext(result, params);
+        }
+    } catch (apache::thrift::TException &e) {
+        std::stringstream ss;
+        ss << ""es get_next error: scan_range_idx="" << _scan_range_idx
+           << "", msg="" << e.what();
+        LOG(WARNING) << ss.str();
+        return Status(TStatusCode::THRIFT_RPC_ERROR, ss.str(), false);
+    }
+#else
+    TStatus status;
+    result.__set_status(status);
+    result.__set_eos(true);
+    TExtColumnData col_data;
+    std::vector<bool> is_null;
+    is_null.push_back(false);
+    col_data.__set_is_null(is_null);
+    std::vector<int32_t> int_vals;
+    int_vals.push_back(1);
+    int_vals.push_back(2);
+    col_data.__set_int_vals(int_vals);
+    std::vector<TExtColumnData> cols;
+    cols.push_back(col_data);
+    TExtRowBatch rows;
+    rows.__set_cols(cols);
+    rows.__set_num_rows(2);
+    result.__set_rows(rows);
+    return Status(status);
+#endif
+
+    // check result
+    Status get_next_status(result.status);
+    if (!get_next_status.ok()) {
+        LOG(WARNING) << ""es get_next error: scan_range_idx="" << _scan_range_idx
+                     << "", address="" << address
+                     << "", msg="" << get_next_status.get_error_msg();
+        return get_next_status;
+    }
+    if (!result.__isset.rows || !result.rows.__isset.num_rows) {
+        std::stringstream ss;
+        ss << ""es get_next error: scan_range_idx="" << _scan_range_idx
+           << "", msg=rows or num_rows not in result"";
+        LOG(WARNING) << ss.str();
+        return Status(ss.str());
+    }
+
+    return Status::OK;
+}
+
+Status EsScanNode::materialize_row(MemPool* tuple_pool, Tuple* tuple,
+                                   const vector<TExtColumnData>& cols, int row_idx,
+                                   vector<int>& cols_next_val_idx) {
+  tuple->init(_tuple_desc->byte_size());
+
+  for (int i = 0; i < _tuple_desc->slots().size(); ++i) {
+    const SlotDescriptor* slot_desc = _tuple_desc->slots()[i];
+    void* slot = tuple->get_slot(slot_desc->tuple_offset());
+    const TExtColumnData& col = cols[i];
+
+    if (col.is_null[row_idx]) {
+      tuple->set_null(slot_desc->null_indicator_offset());
+      continue;
+    }
+
+    int val_idx = cols_next_val_idx[i]++;
+    switch (slot_desc->type().type) {
+      case TYPE_VARCHAR: {
+          if (val_idx >= col.string_vals.size()) {
+            return Status(strings::Substitute(ERROR_INVALID_COL_DATA, ""STRING""));
+          }
+          const string& val = col.string_vals[val_idx];
+          size_t val_size = val.size();
+          char* buffer = reinterpret_cast<char*>(tuple_pool->try_allocate_unaligned(val_size));
+          if (UNLIKELY(buffer == NULL)) {
+            string details = strings::Substitute(ERROR_MEM_LIMIT_EXCEEDED, ""MaterializeNextRow"",
+                val_size, ""string slot"");
+            return tuple_pool->mem_tracker()->MemLimitExceeded(NULL, details, val_size);
+          }
+          memcpy(buffer, val.data(), val_size);
+          reinterpret_cast<StringValue*>(slot)->ptr = buffer;
+          reinterpret_cast<StringValue*>(slot)->len = val_size;
+          break;
+        }
+      case TYPE_TINYINT:
+        if (val_idx >= col.byte_vals.size()) {","[{'comment': ""why isn't size() col's function?\r\n"", 'commenter': 'imay'}, {'comment': 'col:\r\n  1: required list<bool> is_null;\r\n  2: optional list<bool> bool_vals;\r\n  3: optional list<byte> byte_vals;\r\n  4: optional list<i16> short_vals;\r\n  5: optional list<i32> int_vals;\r\n  6: optional list<i64> long_vals;\r\n  7: optional list<double> double_vals;\r\n  8: optional list<string> string_vals;\r\n  9: optional list<binary> binary_vals;', 'commenter': 'Salieri1969'}]"
450,be/src/exec/es_scan_node.cpp,"@@ -0,0 +1,660 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""es_scan_node.h""
+
+#include <string>
+#include <boost/algorithm/string.hpp>
+#include <gutil/strings/substitute.h>
+
+#include ""gen_cpp/PlanNodes_types.h""
+#include ""gen_cpp/Exprs_types.h""
+#include ""runtime/runtime_state.h""
+#include ""runtime/row_batch.h""
+#include ""runtime/string_value.h""
+#include ""runtime/tuple_row.h""
+#include ""runtime/client_cache.h""
+#include ""util/runtime_profile.h""
+#include ""util/debug_util.h""
+#include ""service/backend_options.h""
+#include ""olap/olap_common.h""
+#include ""olap/utils.h""
+#include ""exprs/expr_context.h""
+#include ""exprs/expr.h""
+#include ""exprs/slot_ref.h""
+
+namespace doris {
+
+// $0 = column type (e.g. INT)
+const string ERROR_INVALID_COL_DATA = ""Data source returned inconsistent column data. ""
+    ""Expected value of type $0 based on column metadata. This likely indicates a ""
+    ""problem with the data source library."";
+const string ERROR_MEM_LIMIT_EXCEEDED = ""DataSourceScanNode::$0() failed to allocate ""
+    ""$1 bytes for $2."";
+
+EsScanNode::EsScanNode(
+        ObjectPool* pool,
+        const TPlanNode& tnode,
+        const DescriptorTbl& descs) :
+            ScanNode(pool, tnode, descs),
+            _tuple_id(tnode.es_scan_node.tuple_id),
+            _scan_range_idx(0) {
+    if (tnode.es_scan_node.__isset.properties) {
+        _properties = tnode.es_scan_node.properties;
+    }
+}
+
+EsScanNode::~EsScanNode() {
+}
+
+Status EsScanNode::prepare(RuntimeState* state) {
+    LOG(INFO) << ""EsScanNode::Prepare"";
+
+    RETURN_IF_ERROR(ScanNode::prepare(state));
+    _tuple_desc = state->desc_tbl().get_tuple_descriptor(_tuple_id);
+    if (nullptr == _tuple_desc) {
+        std::stringstream ss;
+        ss << ""es tuple descriptor is null, _tuple_id="" << _tuple_id;
+        LOG(WARNING) << ss.str();
+        return Status(ss.str());
+    }
+    _env = state->exec_env();
+
+    return Status::OK;
+}
+
+Status EsScanNode::open(RuntimeState* state) {
+    LOG(INFO) << ""EsScanNode::Open"";
+
+    RETURN_IF_ERROR(exec_debug_action(TExecNodePhase::OPEN));
+    RETURN_IF_CANCELLED(state);
+    SCOPED_TIMER(_runtime_profile->total_time_counter());
+    RETURN_IF_ERROR(ExecNode::open(state));
+
+    // TExtOpenParams.row_schema
+    vector<TExtColumnDesc> cols;
+    for (const SlotDescriptor* slot : _tuple_desc->slots()) {
+        TExtColumnDesc col;
+        col.__set_name(slot->col_name());
+        col.__set_type(slot->type().to_thrift());
+        cols.push_back(std::move(col));
+    }
+    TExtTableSchema row_schema;
+    row_schema.cols = std::move(cols);
+    row_schema.__isset.cols = true;
+
+    // TExtOpenParams.predicates
+    vector<vector<TExtPredicate> > predicates;
+    vector<int> conjunct_idxes;
+    for (int i = 0; i < _conjunct_ctxs.size(); ++i) {
+        VLOG(1) << ""conjunct: "" << _conjunct_ctxs[i]->root()->debug_string();
+        vector<TExtPredicate> disjuncts;
+        if (get_disjuncts(_conjunct_ctxs[i], _conjunct_ctxs[i]->root(), disjuncts)) {
+            predicates.push_back(std::move(disjuncts));
+            conjunct_idxes.push_back(i);
+        }
+    }
+
+    // open every scan range
+    int conjunct_accepted_times[_conjunct_ctxs.size()]; 
+    for (int i = 0; i < _scan_ranges.size(); ++i) {
+        TEsScanRange es_scan_range = _scan_ranges[i];
+
+        // TExtOpenParams
+        TExtOpenParams params;
+        params.__set_query_id(state->query_id());
+        _properties[""index""] = es_scan_range.index;
+        if (es_scan_range.__isset.type) {
+            _properties[""type""] = es_scan_range.type;
+        }
+        _properties[""shard_id""] = std::to_string(es_scan_range.shard_id);
+        params.__set_properties(_properties);
+        params.__set_row_schema(row_schema);
+        params.__set_batch_size(state->batch_size());
+        params.__set_predicates(predicates);
+        TExtOpenResult result;
+
+        // check es host
+        if (es_scan_range.es_hosts.empty()) {
+            std::stringstream ss;
+            ss << ""es fail to open: hosts empty"";
+            LOG(ERROR) << ss.str();
+            return Status(ss.str());
+        }
+
+        // choose an es node, local is better
+        TNetworkAddress es_host_selected = es_scan_range.es_hosts[0];
+        int selected_idx = 0;
+        for (int j = 0; j < es_scan_range.es_hosts.size(); j++) {
+            TNetworkAddress& es_host = es_scan_range.es_hosts[j];
+            if (es_host.hostname == BackendOptions::get_localhost()) {
+                es_host_selected = es_host;
+                selected_idx = j;
+                break;
+            }
+        }
+
+        // if shard not found, try other nodes
+        Status status = open_es(es_host_selected, result, params);
+        if (status.code() == TStatusCode::ES_SHARD_NOT_FOUND) {
+            for (int j = 0; j < es_scan_range.es_hosts.size(); j++) {
+                if (j == selected_idx) continue;
+                es_host_selected = es_scan_range.es_hosts[j];
+                status = open_es(es_host_selected, result, params);
+                if (status.code() == TStatusCode::ES_SHARD_NOT_FOUND) {
+                    continue;
+                } else {
+                    break;
+                }
+            }
+        }
+
+        if (!status.ok()) {
+            LOG(WARNING) << ""es open error: scan_range_idx="" << i
+                         << "", address="" << es_host_selected
+                         << "", msg="" << status.get_error_msg();
+            return status;
+        }
+
+        // get accepted_conjuncts
+        if (result.__isset.accepted_conjuncts) {
+            for (int conjunct_index : result.accepted_conjuncts) {
+                conjunct_accepted_times[conjunct_index]++;
+            }
+        }
+
+        _addresses.push_back(es_host_selected);
+        _scan_handles.push_back(result.scan_handle);
+        VLOG(1) << ""es open success: scan_range_idx="" << i
+                << "", params="" << apache::thrift::ThriftDebugString(params)
+                << "", result="" << apache::thrift::ThriftDebugString(result);
+        }
+
+    // remove those conjuncts that conjunct_accepted_times[i] == _scan_ranges.size()
+    for (int i = conjunct_idxes.size() - 1; i >= 0; --i) {
+        if (conjunct_accepted_times[i] == _scan_ranges.size()) {
+            _conjunct_ctxs.erase(_conjunct_ctxs.begin() + i);
+        }
+    }
+
+    return Status::OK;
+}
+
+Status EsScanNode::get_next(RuntimeState* state, RowBatch* row_batch, bool* eos) {
+    LOG(INFO) << ""EsScanNode::GetNext"";
+
+    RETURN_IF_ERROR(exec_debug_action(TExecNodePhase::GETNEXT));
+    RETURN_IF_CANCELLED(state);
+    SCOPED_TIMER(_runtime_profile->total_time_counter());
+    SCOPED_TIMER(materialize_tuple_timer());
+
+    // create tuple
+    MemPool* tuple_pool = row_batch->tuple_data_pool();
+    int64_t tuple_buffer_size;
+    uint8_t* tuple_buffer = nullptr;
+    RETURN_IF_ERROR(row_batch->resize_and_allocate_tuple_buffer(state, &tuple_buffer_size, &tuple_buffer));
+    Tuple* tuple = reinterpret_cast<Tuple*>(tuple_buffer);
+    
+    // get batch
+    TExtGetNextResult result;
+    RETURN_IF_ERROR(get_next_from_es(result));
+    VLOG(1) << ""es get next success: result="" << apache::thrift::ThriftDebugString(result);
+    _offsets[_scan_range_idx] += result.rows.num_rows;
+
+    // convert
+    VLOG(1) << ""begin to convert: scan_range_idx="" << _scan_range_idx
+            << "", num_rows="" << result.rows.num_rows;
+    vector<TExtColumnData>& cols = result.rows.cols;
+    // indexes of the next non-null value in the row batch, per column. 
+    vector<int> cols_next_val_idx(_tuple_desc->slots().size(), 0);
+    for (int row_idx = 0; row_idx < result.rows.num_rows; row_idx++) {
+        if (reached_limit()) {
+            *eos = true;
+            break;
+        }
+        RETURN_IF_ERROR(materialize_row(tuple_pool, tuple, cols, row_idx, cols_next_val_idx));
+        TupleRow* tuple_row = row_batch->get_row(row_batch->add_row());
+        tuple_row->set_tuple(0, tuple);
+        if (ExecNode::eval_conjuncts(_conjunct_ctxs.data(), _conjunct_ctxs.size(), tuple_row)) {
+            row_batch->commit_last_row();
+            tuple = reinterpret_cast<Tuple*>(
+                reinterpret_cast<uint8_t*>(tuple) + _tuple_desc->byte_size());
+            ++_num_rows_returned;
+        }
+    }
+
+    VLOG(1) << ""finish one batch: num_rows="" << row_batch->num_rows();
+    COUNTER_SET(_rows_returned_counter, _num_rows_returned);
+    if (result.__isset.eos && result.eos) {
+        VLOG(1) << ""es finish one scan_range: scan_range_idx="" << _scan_range_idx;
+        ++_scan_range_idx;
+    }
+    if (_scan_range_idx == _scan_ranges.size()) {
+        *eos = true;
+    }
+
+    return Status::OK;
+}
+
+Status EsScanNode::close(RuntimeState* state) {
+    if (is_closed()) return Status::OK;
+    LOG(INFO) << ""EsScanNode::Close"";
+    RETURN_IF_ERROR(exec_debug_action(TExecNodePhase::CLOSE));
+    SCOPED_TIMER(_runtime_profile->total_time_counter());
+
+    for (int i = 0; i < _addresses.size(); ++i) {
+        TExtCloseParams params;
+        params.__set_scan_handle(_scan_handles[i]);
+        TExtCloseResult result;
+
+#ifndef BE_TEST
+        const TNetworkAddress& address = _addresses[i];
+        try {
+            Status status;
+            ExtDataSourceServiceClientCache* client_cache = _env->extdatasource_client_cache();
+            ExtDataSourceServiceConnection client(client_cache, address, 10000, &status);
+            if (!status.ok()) {
+                LOG(WARNING) << ""es create client error: scan_range_idx="" << i
+                             << "", address="" << address
+                             << "", msg="" << status.get_error_msg();
+                return status;
+            }
+
+            try {
+                client->close(result, params);
+            } catch (apache::thrift::transport::TTransportException& e) {
+                RETURN_IF_ERROR(client.reopen());
+                client->close(result, params);
+            }
+        } catch (apache::thrift::TException &e) {
+            std::stringstream ss;
+            ss << ""es close error: scan_range_idx="" << i
+               << "", msg="" << e.what();
+            LOG(WARNING) << ss.str();
+            return Status(TStatusCode::THRIFT_RPC_ERROR, ss.str(), false);
+        }
+
+        Status status(result.status);
+        if (!status.ok()) {
+            LOG(WARNING) << ""es close error: : scan_range_idx="" << i
+                         << "", msg="" << status.get_error_msg();
+            return status;
+        }
+#else
+        TStatus status;
+        result.__set_status(status);
+#endif
+    }
+
+    RETURN_IF_ERROR(ExecNode::close(state));
+    return Status::OK;
+}
+
+void EsScanNode::debug_string(int indentation_level, stringstream* out) const {
+    *out << string(indentation_level * 2, ' ');
+    *out << ""EsScanNode(tupleid="" << _tuple_id;
+    *out << "")"" << std::endl;
+
+    for (int i = 0; i < _children.size(); ++i) {
+        _children[i]->debug_string(indentation_level + 1, out);
+    }
+}
+
+Status EsScanNode::set_scan_ranges(const vector<TScanRangeParams>& scan_ranges) {
+    for (int i = 0; i < scan_ranges.size(); ++i) {
+        TScanRangeParams scan_range = scan_ranges[i];
+        DCHECK(scan_range.scan_range.__isset.es_scan_range);
+        TEsScanRange es_scan_range = scan_range.scan_range.es_scan_range;
+        _scan_ranges.push_back(es_scan_range);
+    }
+
+    _offsets.resize(scan_ranges.size(), 0);
+    return Status::OK;
+}
+
+Status EsScanNode::open_es(TNetworkAddress& address, TExtOpenResult& result, TExtOpenParams& params) {
+#ifndef BE_TEST
+    try {
+        ExtDataSourceServiceClientCache* client_cache = _env->extdatasource_client_cache();
+        Status status;
+        ExtDataSourceServiceConnection client(client_cache, address, 10000, &status);
+        if (!status.ok()) {
+            std::stringstream ss;
+            ss << ""es create client error: address="" << address
+               << "", msg="" << status.get_error_msg();
+            return Status(ss.str());
+        }
+
+        try {
+            client->open(result, params);
+        } catch (apache::thrift::transport::TTransportException& e) {
+            RETURN_IF_ERROR(client.reopen());
+            client->open(result, params);
+        }
+        return Status(result.status);
+    } catch (apache::thrift::TException &e) {
+        std::stringstream ss;
+        ss << ""es open error: address="" << address << "", msg="" << e.what();
+        return Status(ss.str());
+    }
+#else
+    TStatus status;
+    result.__set_status(status);
+    result.__set_scan_handle(""0"");
+    return Status(status);
+#endif
+}
+
+bool EsScanNode::get_disjuncts(ExprContext* context, Expr* conjunct,
+                               vector<TExtPredicate>& disjuncts) {
+    if (TExprNodeType::BINARY_PRED == conjunct->node_type()) {
+        if (conjunct->children().size() != 2) {
+            VLOG(1) << ""get disjuncts fail: number of childs is not 2"";
+            return false;
+        }
+        SlotRef* slotRef;
+        TExprOpcode::type op;
+        Expr* expr;
+        if (TExprNodeType::SLOT_REF == conjunct->get_child(0)->node_type()) {
+            expr = conjunct->get_child(1);
+            slotRef = (SlotRef*)(conjunct->get_child(0));
+            op = conjunct->op();
+        } else if (TExprNodeType::SLOT_REF == conjunct->get_child(1)->node_type()) {
+            expr = conjunct->get_child(0);
+            slotRef = (SlotRef*)(conjunct->get_child(1));
+            op = conjunct->op();
+        } else {
+            VLOG(1) << ""get disjuncts fail: no SLOT_REF child"";
+            return false;
+        }
+
+        std::vector<SlotId> slot_ids;
+        slotRef->get_slot_ids(&slot_ids);
+        SlotDescriptor* slot_desc = nullptr;
+        for (SlotDescriptor* slot : _tuple_desc->slots()) {
+            if (slot->id() == slot_ids[0]) {
+                slot_desc = slot;
+                break;
+            }
+        }
+        if (nullptr == slot_desc) {
+            VLOG(1) << ""get disjuncts fail: slot_desc is null"";
+            return false;
+        }
+
+        TExtColumnDesc columnDesc;
+        columnDesc.__set_name(slot_desc->col_name());
+        columnDesc.__set_type(slot_desc->type().to_thrift());
+        TExtBinaryPredicate binaryPredicate;
+        binaryPredicate.__set_col(columnDesc);
+        binaryPredicate.__set_op(op);
+        binaryPredicate.__set_value(get_literal(context, expr));
+        TExtPredicate predicate;
+        predicate.__set_node_type(TExprNodeType::BINARY_PRED);
+        predicate.__set_binary_predicate(binaryPredicate);
+        disjuncts.push_back(std::move(predicate));
+        return true;
+    } else if (TExprNodeType::COMPOUND_PRED == conjunct->node_type()) {
+        if (TExprOpcode::COMPOUND_OR != conjunct->op()) {
+            VLOG(1) << ""get disjuncts fail: op is not COMPOUND_OR"";
+            return false;
+        }
+        if (!get_disjuncts(context, conjunct->get_child(0), disjuncts)) {
+            return false;
+        }
+        if (!get_disjuncts(context, conjunct->get_child(1), disjuncts)) {
+            return false;
+        }
+        return true;
+    } else {
+        VLOG(1) << ""get disjuncts fail: node type is "" << conjunct->node_type()
+                << "", should be BINARY_PRED or COMPOUND_PRED"";
+        return false;
+    }
+}
+
+TExtLiteral EsScanNode::get_literal(ExprContext* context, Expr* expr) {
+    void* value = context->get_value(expr, NULL);
+    TExtLiteral literal;
+    literal.__set_node_type(expr->node_type());
+    switch (expr->node_type()) {
+    case TExprNodeType::BOOL_LITERAL: {
+        TBoolLiteral bool_literal;
+        bool_literal.__set_value(*reinterpret_cast<bool*>(value));
+        literal.__set_bool_literal(bool_literal);
+        break;
+    }
+    case TExprNodeType::DATE_LITERAL: {
+        DateTimeValue date_value = *reinterpret_cast<DateTimeValue*>(value);
+        char str[MAX_DTVALUE_STR_LEN];
+        date_value.to_string(str);
+        TDateLiteral date_literal;
+        date_literal.__set_value(str);
+        literal.__set_date_literal(date_literal);
+        break;
+    }
+    case TExprNodeType::FLOAT_LITERAL: {
+        TFloatLiteral float_literal;
+        float_literal.__set_value(*reinterpret_cast<float*>(value));
+        literal.__set_float_literal(float_literal);
+        break;
+    }
+    case TExprNodeType::INT_LITERAL: {
+        TIntLiteral int_literal;
+        int_literal.__set_value(*reinterpret_cast<int32_t*>(value));
+        literal.__set_int_literal(int_literal);
+        break;
+    }
+    case TExprNodeType::STRING_LITERAL: {
+        TStringLiteral string_literal;
+        string_literal.__set_value(*reinterpret_cast<string*>(value));
+        literal.__set_string_literal(string_literal);
+        break;
+    }
+    case TExprNodeType::DECIMAL_LITERAL: {
+        TDecimalLiteral decimal_literal;
+        decimal_literal.__set_value(reinterpret_cast<DecimalValue*>(value)->to_string());
+        literal.__set_decimal_literal(decimal_literal);
+        break;
+    }
+    case TExprNodeType::LARGE_INT_LITERAL: {
+        char buf[48];
+        int len = 48;
+        char* v = LargeIntValue::to_string(*reinterpret_cast<__int128*>(value), buf, &len);
+        TLargeIntLiteral large_int_literal;
+        large_int_literal.__set_value(v);
+        literal.__set_large_int_literal(large_int_literal);
+        break;
+    }
+    default:
+        break;
+    }
+    return literal;
+}
+
+Status EsScanNode::get_next_from_es(TExtGetNextResult& result) {
+    TExtGetNextParams params;
+    params.__set_scan_handle(_scan_handles[_scan_range_idx]);
+    params.__set_offset(_offsets[_scan_range_idx]);
+
+    // getNext
+    const TNetworkAddress &address = _addresses[_scan_range_idx];
+#ifndef BE_TEST
+    try {
+        Status create_client_status;
+        ExtDataSourceServiceClientCache *client_cache = _env->extdatasource_client_cache();
+        ExtDataSourceServiceConnection client(client_cache, address, 10000, &create_client_status);
+        if (!create_client_status.ok()) {
+            LOG(WARNING) << ""es create client error: scan_range_idx="" << _scan_range_idx
+                         << "", address="" << address
+                         << "", msg="" << create_client_status.get_error_msg();
+            return create_client_status;
+        }
+
+        try {
+            client->getNext(result, params);
+        } catch (apache::thrift::transport::TTransportException& e) {
+            RETURN_IF_ERROR(client.reopen());
+            client->getNext(result, params);
+        }
+    } catch (apache::thrift::TException &e) {
+        std::stringstream ss;
+        ss << ""es get_next error: scan_range_idx="" << _scan_range_idx
+           << "", msg="" << e.what();
+        LOG(WARNING) << ss.str();
+        return Status(TStatusCode::THRIFT_RPC_ERROR, ss.str(), false);
+    }
+#else
+    TStatus status;
+    result.__set_status(status);
+    result.__set_eos(true);
+    TExtColumnData col_data;
+    std::vector<bool> is_null;
+    is_null.push_back(false);
+    col_data.__set_is_null(is_null);
+    std::vector<int32_t> int_vals;
+    int_vals.push_back(1);
+    int_vals.push_back(2);
+    col_data.__set_int_vals(int_vals);
+    std::vector<TExtColumnData> cols;
+    cols.push_back(col_data);
+    TExtRowBatch rows;
+    rows.__set_cols(cols);
+    rows.__set_num_rows(2);
+    result.__set_rows(rows);
+    return Status(status);
+#endif
+
+    // check result
+    Status get_next_status(result.status);
+    if (!get_next_status.ok()) {
+        LOG(WARNING) << ""es get_next error: scan_range_idx="" << _scan_range_idx
+                     << "", address="" << address
+                     << "", msg="" << get_next_status.get_error_msg();
+        return get_next_status;
+    }
+    if (!result.__isset.rows || !result.rows.__isset.num_rows) {
+        std::stringstream ss;
+        ss << ""es get_next error: scan_range_idx="" << _scan_range_idx
+           << "", msg=rows or num_rows not in result"";
+        LOG(WARNING) << ss.str();
+        return Status(ss.str());
+    }
+
+    return Status::OK;
+}
+
+Status EsScanNode::materialize_row(MemPool* tuple_pool, Tuple* tuple,
+                                   const vector<TExtColumnData>& cols, int row_idx,
+                                   vector<int>& cols_next_val_idx) {
+  tuple->init(_tuple_desc->byte_size());
+
+  for (int i = 0; i < _tuple_desc->slots().size(); ++i) {
+    const SlotDescriptor* slot_desc = _tuple_desc->slots()[i];
+    void* slot = tuple->get_slot(slot_desc->tuple_offset());
+    const TExtColumnData& col = cols[i];
+
+    if (col.is_null[row_idx]) {
+      tuple->set_null(slot_desc->null_indicator_offset());
+      continue;
+    }","[{'comment': 'else {\r\ntuple->set_not_null()\r\n}', 'commenter': 'imay'}]"
450,be/src/exec/es_scan_node.cpp,"@@ -0,0 +1,667 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""es_scan_node.h""
+
+#include <string>
+#include <boost/algorithm/string.hpp>
+#include <gutil/strings/substitute.h>
+
+#include ""gen_cpp/PlanNodes_types.h""
+#include ""gen_cpp/Exprs_types.h""
+#include ""runtime/runtime_state.h""
+#include ""runtime/row_batch.h""
+#include ""runtime/string_value.h""
+#include ""runtime/tuple_row.h""
+#include ""runtime/client_cache.h""
+#include ""util/runtime_profile.h""
+#include ""util/debug_util.h""
+#include ""service/backend_options.h""
+#include ""olap/olap_common.h""
+#include ""olap/utils.h""
+#include ""exprs/expr_context.h""
+#include ""exprs/expr.h""
+#include ""exprs/slot_ref.h""
+
+namespace doris {
+
+// $0 = column type (e.g. INT)
+const string ERROR_INVALID_COL_DATA = ""Data source returned inconsistent column data. ""
+    ""Expected value of type $0 based on column metadata. This likely indicates a ""
+    ""problem with the data source library."";
+const string ERROR_MEM_LIMIT_EXCEEDED = ""DataSourceScanNode::$0() failed to allocate ""
+    ""$1 bytes for $2."";
+
+EsScanNode::EsScanNode(
+        ObjectPool* pool,
+        const TPlanNode& tnode,
+        const DescriptorTbl& descs) :
+            ScanNode(pool, tnode, descs),
+            _tuple_id(tnode.es_scan_node.tuple_id),
+            _scan_range_idx(0) {
+    if (tnode.es_scan_node.__isset.properties) {
+        _properties = tnode.es_scan_node.properties;
+    }
+}
+
+EsScanNode::~EsScanNode() {
+}
+
+Status EsScanNode::prepare(RuntimeState* state) {
+    VLOG(1) << ""EsScanNode::Prepare"";
+
+    RETURN_IF_ERROR(ScanNode::prepare(state));
+    _tuple_desc = state->desc_tbl().get_tuple_descriptor(_tuple_id);
+    if (_tuple_desc == nullptr) {
+        std::stringstream ss;
+        ss << ""es tuple descriptor is null, _tuple_id="" << _tuple_id;
+        LOG(WARNING) << ss.str();
+        return Status(ss.str());
+    }
+    _env = state->exec_env();
+
+    return Status::OK;
+}
+
+Status EsScanNode::open(RuntimeState* state) {
+    VLOG(1) << ""EsScanNode::Open"";
+
+    RETURN_IF_ERROR(exec_debug_action(TExecNodePhase::OPEN));
+    RETURN_IF_CANCELLED(state);
+    SCOPED_TIMER(_runtime_profile->total_time_counter());
+    RETURN_IF_ERROR(ExecNode::open(state));
+
+    // TExtOpenParams.row_schema
+    vector<TExtColumnDesc> cols;
+    for (const SlotDescriptor* slot : _tuple_desc->slots()) {
+        TExtColumnDesc col;
+        col.__set_name(slot->col_name());
+        col.__set_type(slot->type().to_thrift());
+        cols.emplace_back(std::move(col));
+    }
+    TExtTableSchema row_schema;
+    row_schema.cols = std::move(cols);
+    row_schema.__isset.cols = true;
+
+    // TExtOpenParams.predicates
+    vector<vector<TExtPredicate> > predicates;
+    vector<int> conjunct_idxes;
+    for (int i = 0; i < _conjunct_ctxs.size(); ++i) {
+        VLOG(1) << ""conjunct: "" << _conjunct_ctxs[i]->root()->debug_string();
+        vector<TExtPredicate> disjuncts;
+        if (get_disjuncts(_conjunct_ctxs[i], _conjunct_ctxs[i]->root(), disjuncts)) {
+            predicates.push_back(std::move(disjuncts));
+            conjunct_idxes.push_back(i);
+        }
+    }
+
+    // open every scan range
+    int conjunct_accepted_times[_conjunct_ctxs.size()]; 
+    for (int i = 0; i < _scan_ranges.size(); ++i) {
+        TEsScanRange es_scan_range = _scan_ranges[i];","[{'comment': 'reference', 'commenter': 'imay'}]"
450,be/src/exec/es_scan_node.cpp,"@@ -0,0 +1,667 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""es_scan_node.h""
+
+#include <string>
+#include <boost/algorithm/string.hpp>
+#include <gutil/strings/substitute.h>
+
+#include ""gen_cpp/PlanNodes_types.h""
+#include ""gen_cpp/Exprs_types.h""
+#include ""runtime/runtime_state.h""
+#include ""runtime/row_batch.h""
+#include ""runtime/string_value.h""
+#include ""runtime/tuple_row.h""
+#include ""runtime/client_cache.h""
+#include ""util/runtime_profile.h""
+#include ""util/debug_util.h""
+#include ""service/backend_options.h""
+#include ""olap/olap_common.h""
+#include ""olap/utils.h""
+#include ""exprs/expr_context.h""
+#include ""exprs/expr.h""
+#include ""exprs/slot_ref.h""
+
+namespace doris {
+
+// $0 = column type (e.g. INT)
+const string ERROR_INVALID_COL_DATA = ""Data source returned inconsistent column data. ""
+    ""Expected value of type $0 based on column metadata. This likely indicates a ""
+    ""problem with the data source library."";
+const string ERROR_MEM_LIMIT_EXCEEDED = ""DataSourceScanNode::$0() failed to allocate ""
+    ""$1 bytes for $2."";
+
+EsScanNode::EsScanNode(
+        ObjectPool* pool,
+        const TPlanNode& tnode,
+        const DescriptorTbl& descs) :
+            ScanNode(pool, tnode, descs),
+            _tuple_id(tnode.es_scan_node.tuple_id),
+            _scan_range_idx(0) {
+    if (tnode.es_scan_node.__isset.properties) {
+        _properties = tnode.es_scan_node.properties;
+    }
+}
+
+EsScanNode::~EsScanNode() {
+}
+
+Status EsScanNode::prepare(RuntimeState* state) {
+    VLOG(1) << ""EsScanNode::Prepare"";
+
+    RETURN_IF_ERROR(ScanNode::prepare(state));
+    _tuple_desc = state->desc_tbl().get_tuple_descriptor(_tuple_id);
+    if (_tuple_desc == nullptr) {
+        std::stringstream ss;
+        ss << ""es tuple descriptor is null, _tuple_id="" << _tuple_id;
+        LOG(WARNING) << ss.str();
+        return Status(ss.str());
+    }
+    _env = state->exec_env();
+
+    return Status::OK;
+}
+
+Status EsScanNode::open(RuntimeState* state) {
+    VLOG(1) << ""EsScanNode::Open"";
+
+    RETURN_IF_ERROR(exec_debug_action(TExecNodePhase::OPEN));
+    RETURN_IF_CANCELLED(state);
+    SCOPED_TIMER(_runtime_profile->total_time_counter());
+    RETURN_IF_ERROR(ExecNode::open(state));
+
+    // TExtOpenParams.row_schema
+    vector<TExtColumnDesc> cols;
+    for (const SlotDescriptor* slot : _tuple_desc->slots()) {
+        TExtColumnDesc col;
+        col.__set_name(slot->col_name());
+        col.__set_type(slot->type().to_thrift());
+        cols.emplace_back(std::move(col));
+    }
+    TExtTableSchema row_schema;
+    row_schema.cols = std::move(cols);
+    row_schema.__isset.cols = true;
+
+    // TExtOpenParams.predicates
+    vector<vector<TExtPredicate> > predicates;
+    vector<int> conjunct_idxes;
+    for (int i = 0; i < _conjunct_ctxs.size(); ++i) {
+        VLOG(1) << ""conjunct: "" << _conjunct_ctxs[i]->root()->debug_string();
+        vector<TExtPredicate> disjuncts;
+        if (get_disjuncts(_conjunct_ctxs[i], _conjunct_ctxs[i]->root(), disjuncts)) {
+            predicates.push_back(std::move(disjuncts));
+            conjunct_idxes.push_back(i);
+        }
+    }
+
+    // open every scan range
+    int conjunct_accepted_times[_conjunct_ctxs.size()]; 
+    for (int i = 0; i < _scan_ranges.size(); ++i) {
+        TEsScanRange es_scan_range = _scan_ranges[i];
+
+        // TExtOpenParams
+        TExtOpenParams params;
+        params.__set_query_id(state->query_id());
+        _properties[""index""] = es_scan_range.index;
+        if (es_scan_range.__isset.type) {
+            _properties[""type""] = es_scan_range.type;
+        }
+        _properties[""shard_id""] = std::to_string(es_scan_range.shard_id);
+        params.__set_properties(_properties);
+        params.__set_row_schema(row_schema);
+        params.__set_batch_size(state->batch_size());
+        params.__set_predicates(predicates);
+        TExtOpenResult result;
+
+        // check es host
+        if (es_scan_range.es_hosts.empty()) {
+            std::stringstream ss;
+            ss << ""es fail to open: hosts empty"";
+            LOG(ERROR) << ss.str();","[{'comment': 'use WARNING instead, ERROR means serious things happened', 'commenter': 'imay'}]"
450,be/src/exec/es_scan_node.cpp,"@@ -0,0 +1,667 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""es_scan_node.h""
+
+#include <string>
+#include <boost/algorithm/string.hpp>
+#include <gutil/strings/substitute.h>
+
+#include ""gen_cpp/PlanNodes_types.h""
+#include ""gen_cpp/Exprs_types.h""
+#include ""runtime/runtime_state.h""
+#include ""runtime/row_batch.h""
+#include ""runtime/string_value.h""
+#include ""runtime/tuple_row.h""
+#include ""runtime/client_cache.h""
+#include ""util/runtime_profile.h""
+#include ""util/debug_util.h""
+#include ""service/backend_options.h""
+#include ""olap/olap_common.h""
+#include ""olap/utils.h""
+#include ""exprs/expr_context.h""
+#include ""exprs/expr.h""
+#include ""exprs/slot_ref.h""
+
+namespace doris {
+
+// $0 = column type (e.g. INT)
+const string ERROR_INVALID_COL_DATA = ""Data source returned inconsistent column data. ""
+    ""Expected value of type $0 based on column metadata. This likely indicates a ""
+    ""problem with the data source library."";
+const string ERROR_MEM_LIMIT_EXCEEDED = ""DataSourceScanNode::$0() failed to allocate ""
+    ""$1 bytes for $2."";
+
+EsScanNode::EsScanNode(
+        ObjectPool* pool,
+        const TPlanNode& tnode,
+        const DescriptorTbl& descs) :
+            ScanNode(pool, tnode, descs),
+            _tuple_id(tnode.es_scan_node.tuple_id),
+            _scan_range_idx(0) {
+    if (tnode.es_scan_node.__isset.properties) {
+        _properties = tnode.es_scan_node.properties;
+    }
+}
+
+EsScanNode::~EsScanNode() {
+}
+
+Status EsScanNode::prepare(RuntimeState* state) {
+    VLOG(1) << ""EsScanNode::Prepare"";
+
+    RETURN_IF_ERROR(ScanNode::prepare(state));
+    _tuple_desc = state->desc_tbl().get_tuple_descriptor(_tuple_id);
+    if (_tuple_desc == nullptr) {
+        std::stringstream ss;
+        ss << ""es tuple descriptor is null, _tuple_id="" << _tuple_id;
+        LOG(WARNING) << ss.str();
+        return Status(ss.str());
+    }
+    _env = state->exec_env();
+
+    return Status::OK;
+}
+
+Status EsScanNode::open(RuntimeState* state) {
+    VLOG(1) << ""EsScanNode::Open"";
+
+    RETURN_IF_ERROR(exec_debug_action(TExecNodePhase::OPEN));
+    RETURN_IF_CANCELLED(state);
+    SCOPED_TIMER(_runtime_profile->total_time_counter());
+    RETURN_IF_ERROR(ExecNode::open(state));
+
+    // TExtOpenParams.row_schema
+    vector<TExtColumnDesc> cols;
+    for (const SlotDescriptor* slot : _tuple_desc->slots()) {
+        TExtColumnDesc col;
+        col.__set_name(slot->col_name());
+        col.__set_type(slot->type().to_thrift());
+        cols.emplace_back(std::move(col));
+    }
+    TExtTableSchema row_schema;
+    row_schema.cols = std::move(cols);
+    row_schema.__isset.cols = true;
+
+    // TExtOpenParams.predicates
+    vector<vector<TExtPredicate> > predicates;
+    vector<int> conjunct_idxes;
+    for (int i = 0; i < _conjunct_ctxs.size(); ++i) {
+        VLOG(1) << ""conjunct: "" << _conjunct_ctxs[i]->root()->debug_string();
+        vector<TExtPredicate> disjuncts;
+        if (get_disjuncts(_conjunct_ctxs[i], _conjunct_ctxs[i]->root(), disjuncts)) {
+            predicates.push_back(std::move(disjuncts));
+            conjunct_idxes.push_back(i);
+        }
+    }
+
+    // open every scan range
+    int conjunct_accepted_times[_conjunct_ctxs.size()]; 
+    for (int i = 0; i < _scan_ranges.size(); ++i) {
+        TEsScanRange es_scan_range = _scan_ranges[i];
+
+        // TExtOpenParams
+        TExtOpenParams params;
+        params.__set_query_id(state->query_id());
+        _properties[""index""] = es_scan_range.index;
+        if (es_scan_range.__isset.type) {
+            _properties[""type""] = es_scan_range.type;
+        }
+        _properties[""shard_id""] = std::to_string(es_scan_range.shard_id);
+        params.__set_properties(_properties);
+        params.__set_row_schema(row_schema);
+        params.__set_batch_size(state->batch_size());
+        params.__set_predicates(predicates);
+        TExtOpenResult result;
+
+        // check es host
+        if (es_scan_range.es_hosts.empty()) {
+            std::stringstream ss;
+            ss << ""es fail to open: hosts empty"";
+            LOG(ERROR) << ss.str();
+            return Status(ss.str());
+        }
+
+        // choose an es node, local is better
+        TNetworkAddress es_host_selected = es_scan_range.es_hosts[0];
+        int selected_idx = 0;
+        for (int j = 0; j < es_scan_range.es_hosts.size(); j++) {
+            TNetworkAddress& es_host = es_scan_range.es_hosts[j];
+            if (es_host.hostname == BackendOptions::get_localhost()) {
+                es_host_selected = es_host;
+                selected_idx = j;
+                break;
+            }
+        }
+
+        // if shard not found, try other nodes
+        Status status = open_es(es_host_selected, result, params);
+        if (status.code() == TStatusCode::ES_SHARD_NOT_FOUND) {
+            for (int j = 0; j < es_scan_range.es_hosts.size(); j++) {
+                if (j == selected_idx) continue;
+                es_host_selected = es_scan_range.es_hosts[j];
+                status = open_es(es_host_selected, result, params);
+                if (status.code() == TStatusCode::ES_SHARD_NOT_FOUND) {
+                    continue;
+                } else {
+                    break;
+                }
+            }
+        }","[{'comment': 'you can use \r\n\r\n```\r\nfor (int i = 0; i < 2; ++i) {\r\n   for (auto& host : es_scan_range.es_host) {\r\n        if (i == 0 && host != localhost) {\r\n         continue;\r\n        } else (i == 1 &&  j == selected_idx) {\r\n         continue;\r\n        }\r\n    }\r\n}\r\n```', 'commenter': 'imay'}]"
450,be/src/exec/es_scan_node.cpp,"@@ -0,0 +1,667 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""es_scan_node.h""
+
+#include <string>
+#include <boost/algorithm/string.hpp>
+#include <gutil/strings/substitute.h>
+
+#include ""gen_cpp/PlanNodes_types.h""
+#include ""gen_cpp/Exprs_types.h""
+#include ""runtime/runtime_state.h""
+#include ""runtime/row_batch.h""
+#include ""runtime/string_value.h""
+#include ""runtime/tuple_row.h""
+#include ""runtime/client_cache.h""
+#include ""util/runtime_profile.h""
+#include ""util/debug_util.h""
+#include ""service/backend_options.h""
+#include ""olap/olap_common.h""
+#include ""olap/utils.h""
+#include ""exprs/expr_context.h""
+#include ""exprs/expr.h""
+#include ""exprs/slot_ref.h""
+
+namespace doris {
+
+// $0 = column type (e.g. INT)
+const string ERROR_INVALID_COL_DATA = ""Data source returned inconsistent column data. ""
+    ""Expected value of type $0 based on column metadata. This likely indicates a ""
+    ""problem with the data source library."";
+const string ERROR_MEM_LIMIT_EXCEEDED = ""DataSourceScanNode::$0() failed to allocate ""
+    ""$1 bytes for $2."";
+
+EsScanNode::EsScanNode(
+        ObjectPool* pool,
+        const TPlanNode& tnode,
+        const DescriptorTbl& descs) :
+            ScanNode(pool, tnode, descs),
+            _tuple_id(tnode.es_scan_node.tuple_id),
+            _scan_range_idx(0) {
+    if (tnode.es_scan_node.__isset.properties) {
+        _properties = tnode.es_scan_node.properties;
+    }
+}
+
+EsScanNode::~EsScanNode() {
+}
+
+Status EsScanNode::prepare(RuntimeState* state) {
+    VLOG(1) << ""EsScanNode::Prepare"";
+
+    RETURN_IF_ERROR(ScanNode::prepare(state));
+    _tuple_desc = state->desc_tbl().get_tuple_descriptor(_tuple_id);
+    if (_tuple_desc == nullptr) {
+        std::stringstream ss;
+        ss << ""es tuple descriptor is null, _tuple_id="" << _tuple_id;
+        LOG(WARNING) << ss.str();
+        return Status(ss.str());
+    }
+    _env = state->exec_env();
+
+    return Status::OK;
+}
+
+Status EsScanNode::open(RuntimeState* state) {
+    VLOG(1) << ""EsScanNode::Open"";
+
+    RETURN_IF_ERROR(exec_debug_action(TExecNodePhase::OPEN));
+    RETURN_IF_CANCELLED(state);
+    SCOPED_TIMER(_runtime_profile->total_time_counter());
+    RETURN_IF_ERROR(ExecNode::open(state));
+
+    // TExtOpenParams.row_schema
+    vector<TExtColumnDesc> cols;
+    for (const SlotDescriptor* slot : _tuple_desc->slots()) {
+        TExtColumnDesc col;
+        col.__set_name(slot->col_name());
+        col.__set_type(slot->type().to_thrift());
+        cols.emplace_back(std::move(col));
+    }
+    TExtTableSchema row_schema;
+    row_schema.cols = std::move(cols);
+    row_schema.__isset.cols = true;
+
+    // TExtOpenParams.predicates
+    vector<vector<TExtPredicate> > predicates;
+    vector<int> conjunct_idxes;
+    for (int i = 0; i < _conjunct_ctxs.size(); ++i) {
+        VLOG(1) << ""conjunct: "" << _conjunct_ctxs[i]->root()->debug_string();
+        vector<TExtPredicate> disjuncts;
+        if (get_disjuncts(_conjunct_ctxs[i], _conjunct_ctxs[i]->root(), disjuncts)) {
+            predicates.push_back(std::move(disjuncts));
+            conjunct_idxes.push_back(i);
+        }
+    }
+
+    // open every scan range
+    int conjunct_accepted_times[_conjunct_ctxs.size()]; ","[{'comment': 'without initialize\r\n\r\nuse std::vector<int> instead', 'commenter': 'imay'}]"
450,be/src/exec/es_scan_node.cpp,"@@ -128,66 +136,59 @@ Status EsScanNode::open(RuntimeState* state) {
         params.__set_predicates(predicates);
         TExtOpenResult result;
 
-        // check es host
-        if (es_scan_range.es_hosts.empty()) {
-            std::stringstream ss;
-            ss << ""es fail to open: hosts empty"";
-            LOG(ERROR) << ss.str();
-            return Status(ss.str());
-        }
-
-        // choose an es node, local is better
-        TNetworkAddress es_host_selected = es_scan_range.es_hosts[0];
-        int selected_idx = 0;
-        for (int j = 0; j < es_scan_range.es_hosts.size(); j++) {
-            TNetworkAddress& es_host = es_scan_range.es_hosts[j];
-            if (es_host.hostname == BackendOptions::get_localhost()) {
-                es_host_selected = es_host;
-                selected_idx = j;
-                break;
-            }
-        }
-
-        // if shard not found, try other nodes
-        Status status = open_es(es_host_selected, result, params);
-        if (status.code() == TStatusCode::ES_SHARD_NOT_FOUND) {
-            for (int j = 0; j < es_scan_range.es_hosts.size(); j++) {
-                if (j == selected_idx) continue;
-                es_host_selected = es_scan_range.es_hosts[j];
-                status = open_es(es_host_selected, result, params);
-                if (status.code() == TStatusCode::ES_SHARD_NOT_FOUND) {
+        // choose an es node, local is the first choice
+        std::string localhost = BackendOptions::get_localhost();
+        bool is_success = false;
+        for (int j = 0; j < 2; ++j) {
+            for (auto& es_host : es_scan_range.es_hosts) {
+                if ((j == 0 && es_host.hostname != localhost)
+                    || (j == 1 && es_host.hostname == localhost)) {
                     continue;
                 } else {","[{'comment': 'there is no need to have a `else`', 'commenter': 'imay'}]"
450,be/src/exec/es_scan_node.cpp,"@@ -0,0 +1,668 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""es_scan_node.h""
+
+#include <string>
+#include <boost/algorithm/string.hpp>
+#include <gutil/strings/substitute.h>
+
+#include ""gen_cpp/PlanNodes_types.h""
+#include ""gen_cpp/Exprs_types.h""
+#include ""runtime/runtime_state.h""
+#include ""runtime/row_batch.h""
+#include ""runtime/string_value.h""
+#include ""runtime/tuple_row.h""
+#include ""runtime/client_cache.h""
+#include ""util/runtime_profile.h""
+#include ""util/debug_util.h""
+#include ""service/backend_options.h""
+#include ""olap/olap_common.h""
+#include ""olap/utils.h""
+#include ""exprs/expr_context.h""
+#include ""exprs/expr.h""
+#include ""exprs/slot_ref.h""
+
+namespace doris {
+
+// $0 = column type (e.g. INT)
+const string ERROR_INVALID_COL_DATA = ""Data source returned inconsistent column data. ""
+    ""Expected value of type $0 based on column metadata. This likely indicates a ""
+    ""problem with the data source library."";
+const string ERROR_MEM_LIMIT_EXCEEDED = ""DataSourceScanNode::$0() failed to allocate ""
+    ""$1 bytes for $2."";
+
+EsScanNode::EsScanNode(
+        ObjectPool* pool,
+        const TPlanNode& tnode,
+        const DescriptorTbl& descs) :
+            ScanNode(pool, tnode, descs),
+            _tuple_id(tnode.es_scan_node.tuple_id),
+            _scan_range_idx(0) {
+    if (tnode.es_scan_node.__isset.properties) {
+        _properties = tnode.es_scan_node.properties;
+    }
+}
+
+EsScanNode::~EsScanNode() {
+}
+
+Status EsScanNode::prepare(RuntimeState* state) {
+    VLOG(1) << ""EsScanNode::Prepare"";
+
+    RETURN_IF_ERROR(ScanNode::prepare(state));
+    _tuple_desc = state->desc_tbl().get_tuple_descriptor(_tuple_id);
+    if (_tuple_desc == nullptr) {
+        std::stringstream ss;
+        ss << ""es tuple descriptor is null, _tuple_id="" << _tuple_id;
+        LOG(WARNING) << ss.str();
+        return Status(ss.str());
+    }
+    _env = state->exec_env();
+
+    return Status::OK;
+}
+
+Status EsScanNode::open(RuntimeState* state) {
+    VLOG(1) << ""EsScanNode::Open"";
+
+    RETURN_IF_ERROR(exec_debug_action(TExecNodePhase::OPEN));
+    RETURN_IF_CANCELLED(state);
+    SCOPED_TIMER(_runtime_profile->total_time_counter());
+    RETURN_IF_ERROR(ExecNode::open(state));
+
+    // TExtOpenParams.row_schema
+    vector<TExtColumnDesc> cols;
+    for (const SlotDescriptor* slot : _tuple_desc->slots()) {
+        TExtColumnDesc col;
+        col.__set_name(slot->col_name());
+        col.__set_type(slot->type().to_thrift());
+        cols.emplace_back(std::move(col));
+    }
+    TExtTableSchema row_schema;
+    row_schema.cols = std::move(cols);
+    row_schema.__isset.cols = true;
+
+    // TExtOpenParams.predicates
+    vector<vector<TExtPredicate> > predicates;
+    vector<int> predicate_to_conjunct;
+    for (int i = 0; i < _conjunct_ctxs.size(); ++i) {
+        VLOG(1) << ""conjunct: "" << _conjunct_ctxs[i]->root()->debug_string();
+        vector<TExtPredicate> disjuncts;
+        if (get_disjuncts(_conjunct_ctxs[i], _conjunct_ctxs[i]->root(), disjuncts)) {
+            predicates.emplace_back(std::move(disjuncts));
+            predicate_to_conjunct.push_back(i);
+        }
+    }
+
+    // open every scan range
+    vector<int> conjunct_accepted_times(_conjunct_ctxs.size(), 0); 
+    for (int i = 0; i < _scan_ranges.size(); ++i) {
+        TEsScanRange& es_scan_range = _scan_ranges[i];
+
+        if (es_scan_range.es_hosts.empty()) {
+            std::stringstream ss;
+            ss << ""es fail to open: hosts empty"";
+            LOG(WARNING) << ss.str();
+            return Status(ss.str());
+        }
+
+
+        // TExtOpenParams
+        TExtOpenParams params;
+        params.__set_query_id(state->query_id());
+        _properties[""index""] = es_scan_range.index;
+        if (es_scan_range.__isset.type) {
+            _properties[""type""] = es_scan_range.type;
+        }
+        _properties[""shard_id""] = std::to_string(es_scan_range.shard_id);
+        params.__set_properties(_properties);
+        params.__set_row_schema(row_schema);
+        params.__set_batch_size(state->batch_size());
+        params.__set_predicates(predicates);
+        TExtOpenResult result;
+
+        // choose an es node, local is the first choice
+        std::string localhost = BackendOptions::get_localhost();
+        bool is_success = false;
+        for (int j = 0; j < 2; ++j) {
+            for (auto& es_host : es_scan_range.es_hosts) {
+                if ((j == 0 && es_host.hostname != localhost)
+                    || (j == 1 && es_host.hostname == localhost)) {
+                    continue;
+                }
+                Status status = open_es(es_host, result, params);
+                if (status.ok()) {
+                   is_success = true;
+                   _addresses.push_back(es_host);
+                   _scan_handles.push_back(result.scan_handle);
+                   if (result.__isset.accepted_conjuncts) {
+                       for (int index : result.accepted_conjuncts) {
+                           conjunct_accepted_times[predicate_to_conjunct[index]]++;
+                       }
+                   }
+                   VLOG(1) << ""es open success: scan_range_idx="" << i
+                           << "", params="" << apache::thrift::ThriftDebugString(params)
+                           << "", result="" << apache::thrift::ThriftDebugString(result);
+                   break;
+                } else if (status.code() == TStatusCode::ES_SHARD_NOT_FOUND) {
+                    // if shard not found, try other nodes
+                    LOG(WARNING) << ""shard not found on es node: ""
+                                 << "", address="" << es_host
+                                 << "", scan_range_idx="" << i << "", try other nodes"";
+                } else {
+                    LOG(WARNING) << ""es open error: scan_range_idx="" << i
+                                 << "", address="" << es_host
+                                 << "", msg="" << status.get_error_msg();
+                    return status;
+                } 
+            }
+            if (is_success) {
+                break;
+            }
+        }
+
+        if (!is_success) {
+            std::stringstream ss;
+            ss << ""es open error: scan_range_idx="" << i
+               << "", can't find shard on any node"";
+            return Status(ss.str());
+        }
+    }
+
+    // remove those conjuncts that accepted by all scan ranges
+    for (int i = predicate_to_conjunct.size() - 1; i >= 0; i--) {
+        int conjunct_index = predicate_to_conjunct[i];
+        if (conjunct_accepted_times[conjunct_index] == _scan_ranges.size()) {
+            _conjunct_ctxs.erase(_conjunct_ctxs.begin() + conjunct_index);
+        }
+    }
+
+    return Status::OK;
+}
+
+Status EsScanNode::get_next(RuntimeState* state, RowBatch* row_batch, bool* eos) {
+    VLOG(1) << ""EsScanNode::GetNext"";
+
+    RETURN_IF_ERROR(exec_debug_action(TExecNodePhase::GETNEXT));
+    RETURN_IF_CANCELLED(state);
+    SCOPED_TIMER(_runtime_profile->total_time_counter());
+    SCOPED_TIMER(materialize_tuple_timer());
+
+    // create tuple
+    MemPool* tuple_pool = row_batch->tuple_data_pool();
+    int64_t tuple_buffer_size;
+    uint8_t* tuple_buffer = nullptr;
+    RETURN_IF_ERROR(row_batch->resize_and_allocate_tuple_buffer(state, &tuple_buffer_size, &tuple_buffer));
+    Tuple* tuple = reinterpret_cast<Tuple*>(tuple_buffer);
+    
+    // get batch
+    TExtGetNextResult result;
+    RETURN_IF_ERROR(get_next_from_es(result));
+    VLOG(1) << ""es get next success: result="" << apache::thrift::ThriftDebugString(result);
+    _offsets[_scan_range_idx] += result.rows.num_rows;
+
+    // convert
+    VLOG(1) << ""begin to convert: scan_range_idx="" << _scan_range_idx
+            << "", num_rows="" << result.rows.num_rows;
+    vector<TExtColumnData>& cols = result.rows.cols;
+    // indexes of the next non-null value in the row batch, per column. 
+    vector<int> cols_next_val_idx(_tuple_desc->slots().size(), 0);
+    for (int row_idx = 0; row_idx < result.rows.num_rows; row_idx++) {
+        if (reached_limit()) {
+            *eos = true;
+            break;
+        }
+        RETURN_IF_ERROR(materialize_row(tuple_pool, tuple, cols, row_idx, cols_next_val_idx));
+        TupleRow* tuple_row = row_batch->get_row(row_batch->add_row());
+        tuple_row->set_tuple(0, tuple);
+        if (ExecNode::eval_conjuncts(_conjunct_ctxs.data(), _conjunct_ctxs.size(), tuple_row)) {
+            row_batch->commit_last_row();
+            tuple = reinterpret_cast<Tuple*>(
+                reinterpret_cast<uint8_t*>(tuple) + _tuple_desc->byte_size());
+            ++_num_rows_returned;
+        }
+    }
+
+    VLOG(1) << ""finish one batch: num_rows="" << row_batch->num_rows();
+    COUNTER_SET(_rows_returned_counter, _num_rows_returned);
+    if (result.__isset.eos && result.eos) {
+        VLOG(1) << ""es finish one scan_range: scan_range_idx="" << _scan_range_idx;
+        ++_scan_range_idx;
+    }
+    if (_scan_range_idx == _scan_ranges.size()) {
+        *eos = true;
+    }
+
+    return Status::OK;
+}
+
+Status EsScanNode::close(RuntimeState* state) {
+    if (is_closed()) return Status::OK;
+    VLOG(1) << ""EsScanNode::Close"";
+    RETURN_IF_ERROR(exec_debug_action(TExecNodePhase::CLOSE));
+    SCOPED_TIMER(_runtime_profile->total_time_counter());
+
+    for (int i = 0; i < _addresses.size(); ++i) {
+        TExtCloseParams params;
+        params.__set_scan_handle(_scan_handles[i]);
+        TExtCloseResult result;
+
+#ifndef BE_TEST
+        const TNetworkAddress& address = _addresses[i];
+        try {
+            Status status;
+            ExtDataSourceServiceClientCache* client_cache = _env->extdatasource_client_cache();
+            ExtDataSourceServiceConnection client(client_cache, address, 10000, &status);
+            if (!status.ok()) {
+                LOG(WARNING) << ""es create client error: scan_range_idx="" << i
+                             << "", address="" << address
+                             << "", msg="" << status.get_error_msg();
+                return status;
+            }
+
+            try {
+                client->close(result, params);
+            } catch (apache::thrift::transport::TTransportException& e) {
+                RETURN_IF_ERROR(client.reopen());
+                client->close(result, params);
+            }
+        } catch (apache::thrift::TException &e) {
+            std::stringstream ss;
+            ss << ""es close error: scan_range_idx="" << i
+               << "", msg="" << e.what();
+            LOG(WARNING) << ss.str();
+            return Status(TStatusCode::THRIFT_RPC_ERROR, ss.str(), false);
+        }
+
+        Status status(result.status);
+        if (!status.ok()) {
+            LOG(WARNING) << ""es close error: : scan_range_idx="" << i
+                         << "", msg="" << status.get_error_msg();
+            return status;
+        }
+#else
+        TStatus status;
+        result.__set_status(status);
+#endif
+    }
+
+    RETURN_IF_ERROR(ExecNode::close(state));
+    return Status::OK;
+}
+
+void EsScanNode::debug_string(int indentation_level, stringstream* out) const {
+    *out << string(indentation_level * 2, ' ');
+    *out << ""EsScanNode(tupleid="" << _tuple_id;
+    *out << "")"" << std::endl;
+
+    for (int i = 0; i < _children.size(); ++i) {
+        _children[i]->debug_string(indentation_level + 1, out);
+    }
+}
+
+Status EsScanNode::set_scan_ranges(const vector<TScanRangeParams>& scan_ranges) {
+    for (int i = 0; i < scan_ranges.size(); ++i) {
+        TScanRangeParams scan_range = scan_ranges[i];
+        DCHECK(scan_range.scan_range.__isset.es_scan_range);
+        TEsScanRange es_scan_range = scan_range.scan_range.es_scan_range;
+        _scan_ranges.push_back(es_scan_range);
+    }
+
+    _offsets.resize(scan_ranges.size(), 0);
+    return Status::OK;
+}
+
+Status EsScanNode::open_es(TNetworkAddress& address, TExtOpenResult& result, TExtOpenParams& params) {","[{'comment': 'input argument use `const reference` and output argument use `pointer`', 'commenter': 'imay'}]"
450,be/src/exec/es_scan_node.cpp,"@@ -0,0 +1,668 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""es_scan_node.h""
+
+#include <string>
+#include <boost/algorithm/string.hpp>
+#include <gutil/strings/substitute.h>
+
+#include ""gen_cpp/PlanNodes_types.h""
+#include ""gen_cpp/Exprs_types.h""
+#include ""runtime/runtime_state.h""
+#include ""runtime/row_batch.h""
+#include ""runtime/string_value.h""
+#include ""runtime/tuple_row.h""
+#include ""runtime/client_cache.h""
+#include ""util/runtime_profile.h""
+#include ""util/debug_util.h""
+#include ""service/backend_options.h""
+#include ""olap/olap_common.h""
+#include ""olap/utils.h""
+#include ""exprs/expr_context.h""
+#include ""exprs/expr.h""
+#include ""exprs/slot_ref.h""
+
+namespace doris {
+
+// $0 = column type (e.g. INT)
+const string ERROR_INVALID_COL_DATA = ""Data source returned inconsistent column data. ""
+    ""Expected value of type $0 based on column metadata. This likely indicates a ""
+    ""problem with the data source library."";
+const string ERROR_MEM_LIMIT_EXCEEDED = ""DataSourceScanNode::$0() failed to allocate ""
+    ""$1 bytes for $2."";
+
+EsScanNode::EsScanNode(
+        ObjectPool* pool,
+        const TPlanNode& tnode,
+        const DescriptorTbl& descs) :
+            ScanNode(pool, tnode, descs),
+            _tuple_id(tnode.es_scan_node.tuple_id),
+            _scan_range_idx(0) {
+    if (tnode.es_scan_node.__isset.properties) {
+        _properties = tnode.es_scan_node.properties;
+    }
+}
+
+EsScanNode::~EsScanNode() {
+}
+
+Status EsScanNode::prepare(RuntimeState* state) {
+    VLOG(1) << ""EsScanNode::Prepare"";
+
+    RETURN_IF_ERROR(ScanNode::prepare(state));
+    _tuple_desc = state->desc_tbl().get_tuple_descriptor(_tuple_id);
+    if (_tuple_desc == nullptr) {
+        std::stringstream ss;
+        ss << ""es tuple descriptor is null, _tuple_id="" << _tuple_id;
+        LOG(WARNING) << ss.str();
+        return Status(ss.str());
+    }
+    _env = state->exec_env();
+
+    return Status::OK;
+}
+
+Status EsScanNode::open(RuntimeState* state) {
+    VLOG(1) << ""EsScanNode::Open"";
+
+    RETURN_IF_ERROR(exec_debug_action(TExecNodePhase::OPEN));
+    RETURN_IF_CANCELLED(state);
+    SCOPED_TIMER(_runtime_profile->total_time_counter());
+    RETURN_IF_ERROR(ExecNode::open(state));
+
+    // TExtOpenParams.row_schema
+    vector<TExtColumnDesc> cols;
+    for (const SlotDescriptor* slot : _tuple_desc->slots()) {
+        TExtColumnDesc col;
+        col.__set_name(slot->col_name());
+        col.__set_type(slot->type().to_thrift());
+        cols.emplace_back(std::move(col));
+    }
+    TExtTableSchema row_schema;
+    row_schema.cols = std::move(cols);
+    row_schema.__isset.cols = true;
+
+    // TExtOpenParams.predicates
+    vector<vector<TExtPredicate> > predicates;
+    vector<int> predicate_to_conjunct;
+    for (int i = 0; i < _conjunct_ctxs.size(); ++i) {
+        VLOG(1) << ""conjunct: "" << _conjunct_ctxs[i]->root()->debug_string();
+        vector<TExtPredicate> disjuncts;
+        if (get_disjuncts(_conjunct_ctxs[i], _conjunct_ctxs[i]->root(), disjuncts)) {
+            predicates.emplace_back(std::move(disjuncts));
+            predicate_to_conjunct.push_back(i);
+        }
+    }
+
+    // open every scan range
+    vector<int> conjunct_accepted_times(_conjunct_ctxs.size(), 0); 
+    for (int i = 0; i < _scan_ranges.size(); ++i) {
+        TEsScanRange& es_scan_range = _scan_ranges[i];
+
+        if (es_scan_range.es_hosts.empty()) {
+            std::stringstream ss;
+            ss << ""es fail to open: hosts empty"";
+            LOG(WARNING) << ss.str();
+            return Status(ss.str());
+        }
+
+
+        // TExtOpenParams
+        TExtOpenParams params;
+        params.__set_query_id(state->query_id());
+        _properties[""index""] = es_scan_range.index;
+        if (es_scan_range.__isset.type) {
+            _properties[""type""] = es_scan_range.type;
+        }
+        _properties[""shard_id""] = std::to_string(es_scan_range.shard_id);
+        params.__set_properties(_properties);
+        params.__set_row_schema(row_schema);
+        params.__set_batch_size(state->batch_size());
+        params.__set_predicates(predicates);
+        TExtOpenResult result;
+
+        // choose an es node, local is the first choice
+        std::string localhost = BackendOptions::get_localhost();
+        bool is_success = false;
+        for (int j = 0; j < 2; ++j) {
+            for (auto& es_host : es_scan_range.es_hosts) {
+                if ((j == 0 && es_host.hostname != localhost)
+                    || (j == 1 && es_host.hostname == localhost)) {
+                    continue;
+                }
+                Status status = open_es(es_host, result, params);
+                if (status.ok()) {
+                   is_success = true;
+                   _addresses.push_back(es_host);
+                   _scan_handles.push_back(result.scan_handle);
+                   if (result.__isset.accepted_conjuncts) {
+                       for (int index : result.accepted_conjuncts) {
+                           conjunct_accepted_times[predicate_to_conjunct[index]]++;
+                       }
+                   }
+                   VLOG(1) << ""es open success: scan_range_idx="" << i
+                           << "", params="" << apache::thrift::ThriftDebugString(params)
+                           << "", result="" << apache::thrift::ThriftDebugString(result);
+                   break;
+                } else if (status.code() == TStatusCode::ES_SHARD_NOT_FOUND) {
+                    // if shard not found, try other nodes
+                    LOG(WARNING) << ""shard not found on es node: ""
+                                 << "", address="" << es_host
+                                 << "", scan_range_idx="" << i << "", try other nodes"";
+                } else {
+                    LOG(WARNING) << ""es open error: scan_range_idx="" << i
+                                 << "", address="" << es_host
+                                 << "", msg="" << status.get_error_msg();
+                    return status;
+                } 
+            }
+            if (is_success) {
+                break;
+            }
+        }
+
+        if (!is_success) {
+            std::stringstream ss;
+            ss << ""es open error: scan_range_idx="" << i
+               << "", can't find shard on any node"";
+            return Status(ss.str());
+        }
+    }
+
+    // remove those conjuncts that accepted by all scan ranges
+    for (int i = predicate_to_conjunct.size() - 1; i >= 0; i--) {
+        int conjunct_index = predicate_to_conjunct[i];
+        if (conjunct_accepted_times[conjunct_index] == _scan_ranges.size()) {
+            _conjunct_ctxs.erase(_conjunct_ctxs.begin() + conjunct_index);
+        }
+    }
+
+    return Status::OK;
+}
+
+Status EsScanNode::get_next(RuntimeState* state, RowBatch* row_batch, bool* eos) {
+    VLOG(1) << ""EsScanNode::GetNext"";
+
+    RETURN_IF_ERROR(exec_debug_action(TExecNodePhase::GETNEXT));
+    RETURN_IF_CANCELLED(state);
+    SCOPED_TIMER(_runtime_profile->total_time_counter());
+    SCOPED_TIMER(materialize_tuple_timer());
+
+    // create tuple
+    MemPool* tuple_pool = row_batch->tuple_data_pool();
+    int64_t tuple_buffer_size;
+    uint8_t* tuple_buffer = nullptr;
+    RETURN_IF_ERROR(row_batch->resize_and_allocate_tuple_buffer(state, &tuple_buffer_size, &tuple_buffer));
+    Tuple* tuple = reinterpret_cast<Tuple*>(tuple_buffer);
+    
+    // get batch
+    TExtGetNextResult result;
+    RETURN_IF_ERROR(get_next_from_es(result));
+    VLOG(1) << ""es get next success: result="" << apache::thrift::ThriftDebugString(result);
+    _offsets[_scan_range_idx] += result.rows.num_rows;
+
+    // convert
+    VLOG(1) << ""begin to convert: scan_range_idx="" << _scan_range_idx
+            << "", num_rows="" << result.rows.num_rows;
+    vector<TExtColumnData>& cols = result.rows.cols;
+    // indexes of the next non-null value in the row batch, per column. 
+    vector<int> cols_next_val_idx(_tuple_desc->slots().size(), 0);
+    for (int row_idx = 0; row_idx < result.rows.num_rows; row_idx++) {
+        if (reached_limit()) {
+            *eos = true;
+            break;
+        }
+        RETURN_IF_ERROR(materialize_row(tuple_pool, tuple, cols, row_idx, cols_next_val_idx));
+        TupleRow* tuple_row = row_batch->get_row(row_batch->add_row());
+        tuple_row->set_tuple(0, tuple);
+        if (ExecNode::eval_conjuncts(_conjunct_ctxs.data(), _conjunct_ctxs.size(), tuple_row)) {
+            row_batch->commit_last_row();
+            tuple = reinterpret_cast<Tuple*>(
+                reinterpret_cast<uint8_t*>(tuple) + _tuple_desc->byte_size());
+            ++_num_rows_returned;
+        }
+    }
+
+    VLOG(1) << ""finish one batch: num_rows="" << row_batch->num_rows();
+    COUNTER_SET(_rows_returned_counter, _num_rows_returned);
+    if (result.__isset.eos && result.eos) {
+        VLOG(1) << ""es finish one scan_range: scan_range_idx="" << _scan_range_idx;
+        ++_scan_range_idx;
+    }
+    if (_scan_range_idx == _scan_ranges.size()) {
+        *eos = true;
+    }
+
+    return Status::OK;
+}
+
+Status EsScanNode::close(RuntimeState* state) {
+    if (is_closed()) return Status::OK;
+    VLOG(1) << ""EsScanNode::Close"";
+    RETURN_IF_ERROR(exec_debug_action(TExecNodePhase::CLOSE));
+    SCOPED_TIMER(_runtime_profile->total_time_counter());
+
+    for (int i = 0; i < _addresses.size(); ++i) {
+        TExtCloseParams params;
+        params.__set_scan_handle(_scan_handles[i]);
+        TExtCloseResult result;
+
+#ifndef BE_TEST
+        const TNetworkAddress& address = _addresses[i];
+        try {
+            Status status;
+            ExtDataSourceServiceClientCache* client_cache = _env->extdatasource_client_cache();
+            ExtDataSourceServiceConnection client(client_cache, address, 10000, &status);
+            if (!status.ok()) {
+                LOG(WARNING) << ""es create client error: scan_range_idx="" << i
+                             << "", address="" << address
+                             << "", msg="" << status.get_error_msg();
+                return status;
+            }
+
+            try {
+                client->close(result, params);
+            } catch (apache::thrift::transport::TTransportException& e) {
+                RETURN_IF_ERROR(client.reopen());
+                client->close(result, params);
+            }
+        } catch (apache::thrift::TException &e) {
+            std::stringstream ss;
+            ss << ""es close error: scan_range_idx="" << i
+               << "", msg="" << e.what();
+            LOG(WARNING) << ss.str();
+            return Status(TStatusCode::THRIFT_RPC_ERROR, ss.str(), false);
+        }
+
+        Status status(result.status);
+        if (!status.ok()) {
+            LOG(WARNING) << ""es close error: : scan_range_idx="" << i
+                         << "", msg="" << status.get_error_msg();
+            return status;
+        }
+#else
+        TStatus status;
+        result.__set_status(status);
+#endif
+    }
+
+    RETURN_IF_ERROR(ExecNode::close(state));
+    return Status::OK;
+}
+
+void EsScanNode::debug_string(int indentation_level, stringstream* out) const {
+    *out << string(indentation_level * 2, ' ');
+    *out << ""EsScanNode(tupleid="" << _tuple_id;
+    *out << "")"" << std::endl;
+
+    for (int i = 0; i < _children.size(); ++i) {
+        _children[i]->debug_string(indentation_level + 1, out);
+    }
+}
+
+Status EsScanNode::set_scan_ranges(const vector<TScanRangeParams>& scan_ranges) {
+    for (int i = 0; i < scan_ranges.size(); ++i) {
+        TScanRangeParams scan_range = scan_ranges[i];
+        DCHECK(scan_range.scan_range.__isset.es_scan_range);
+        TEsScanRange es_scan_range = scan_range.scan_range.es_scan_range;
+        _scan_ranges.push_back(es_scan_range);
+    }
+
+    _offsets.resize(scan_ranges.size(), 0);
+    return Status::OK;
+}
+
+Status EsScanNode::open_es(TNetworkAddress& address, TExtOpenResult& result, TExtOpenParams& params) {
+#ifndef BE_TEST
+    try {
+        ExtDataSourceServiceClientCache* client_cache = _env->extdatasource_client_cache();
+        Status status;
+        ExtDataSourceServiceConnection client(client_cache, address, 10000, &status);
+        if (!status.ok()) {
+            std::stringstream ss;
+            ss << ""es create client error: address="" << address
+               << "", msg="" << status.get_error_msg();
+            return Status(ss.str());
+        }
+
+        try {
+            client->open(result, params);
+        } catch (apache::thrift::transport::TTransportException& e) {
+            RETURN_IF_ERROR(client.reopen());
+            client->open(result, params);
+        }
+        return Status(result.status);
+    } catch (apache::thrift::TException &e) {
+        std::stringstream ss;
+        ss << ""es open error: address="" << address << "", msg="" << e.what();
+        return Status(ss.str());
+    }
+#else
+    TStatus status;
+    result.__set_status(status);
+    result.__set_scan_handle(""0"");
+    return Status(status);
+#endif
+}
+
+bool EsScanNode::get_disjuncts(ExprContext* context, Expr* conjunct,
+                               vector<TExtPredicate>& disjuncts) {","[{'comment': 'output use pointer, and input use const reference', 'commenter': 'imay'}]"
450,be/src/exec/es_scan_node.cpp,"@@ -0,0 +1,668 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""es_scan_node.h""
+
+#include <string>
+#include <boost/algorithm/string.hpp>
+#include <gutil/strings/substitute.h>
+
+#include ""gen_cpp/PlanNodes_types.h""
+#include ""gen_cpp/Exprs_types.h""
+#include ""runtime/runtime_state.h""
+#include ""runtime/row_batch.h""
+#include ""runtime/string_value.h""
+#include ""runtime/tuple_row.h""
+#include ""runtime/client_cache.h""
+#include ""util/runtime_profile.h""
+#include ""util/debug_util.h""
+#include ""service/backend_options.h""
+#include ""olap/olap_common.h""
+#include ""olap/utils.h""
+#include ""exprs/expr_context.h""
+#include ""exprs/expr.h""
+#include ""exprs/slot_ref.h""
+
+namespace doris {
+
+// $0 = column type (e.g. INT)
+const string ERROR_INVALID_COL_DATA = ""Data source returned inconsistent column data. ""
+    ""Expected value of type $0 based on column metadata. This likely indicates a ""
+    ""problem with the data source library."";
+const string ERROR_MEM_LIMIT_EXCEEDED = ""DataSourceScanNode::$0() failed to allocate ""
+    ""$1 bytes for $2."";
+
+EsScanNode::EsScanNode(
+        ObjectPool* pool,
+        const TPlanNode& tnode,
+        const DescriptorTbl& descs) :
+            ScanNode(pool, tnode, descs),
+            _tuple_id(tnode.es_scan_node.tuple_id),
+            _scan_range_idx(0) {
+    if (tnode.es_scan_node.__isset.properties) {
+        _properties = tnode.es_scan_node.properties;
+    }
+}
+
+EsScanNode::~EsScanNode() {
+}
+
+Status EsScanNode::prepare(RuntimeState* state) {
+    VLOG(1) << ""EsScanNode::Prepare"";
+
+    RETURN_IF_ERROR(ScanNode::prepare(state));
+    _tuple_desc = state->desc_tbl().get_tuple_descriptor(_tuple_id);
+    if (_tuple_desc == nullptr) {
+        std::stringstream ss;
+        ss << ""es tuple descriptor is null, _tuple_id="" << _tuple_id;
+        LOG(WARNING) << ss.str();
+        return Status(ss.str());
+    }
+    _env = state->exec_env();
+
+    return Status::OK;
+}
+
+Status EsScanNode::open(RuntimeState* state) {
+    VLOG(1) << ""EsScanNode::Open"";
+
+    RETURN_IF_ERROR(exec_debug_action(TExecNodePhase::OPEN));
+    RETURN_IF_CANCELLED(state);
+    SCOPED_TIMER(_runtime_profile->total_time_counter());
+    RETURN_IF_ERROR(ExecNode::open(state));
+
+    // TExtOpenParams.row_schema
+    vector<TExtColumnDesc> cols;
+    for (const SlotDescriptor* slot : _tuple_desc->slots()) {
+        TExtColumnDesc col;
+        col.__set_name(slot->col_name());
+        col.__set_type(slot->type().to_thrift());
+        cols.emplace_back(std::move(col));
+    }
+    TExtTableSchema row_schema;
+    row_schema.cols = std::move(cols);
+    row_schema.__isset.cols = true;
+
+    // TExtOpenParams.predicates
+    vector<vector<TExtPredicate> > predicates;
+    vector<int> predicate_to_conjunct;
+    for (int i = 0; i < _conjunct_ctxs.size(); ++i) {
+        VLOG(1) << ""conjunct: "" << _conjunct_ctxs[i]->root()->debug_string();
+        vector<TExtPredicate> disjuncts;
+        if (get_disjuncts(_conjunct_ctxs[i], _conjunct_ctxs[i]->root(), disjuncts)) {
+            predicates.emplace_back(std::move(disjuncts));
+            predicate_to_conjunct.push_back(i);
+        }
+    }
+
+    // open every scan range
+    vector<int> conjunct_accepted_times(_conjunct_ctxs.size(), 0); 
+    for (int i = 0; i < _scan_ranges.size(); ++i) {
+        TEsScanRange& es_scan_range = _scan_ranges[i];
+
+        if (es_scan_range.es_hosts.empty()) {
+            std::stringstream ss;
+            ss << ""es fail to open: hosts empty"";
+            LOG(WARNING) << ss.str();
+            return Status(ss.str());
+        }
+
+
+        // TExtOpenParams
+        TExtOpenParams params;
+        params.__set_query_id(state->query_id());
+        _properties[""index""] = es_scan_range.index;
+        if (es_scan_range.__isset.type) {
+            _properties[""type""] = es_scan_range.type;
+        }
+        _properties[""shard_id""] = std::to_string(es_scan_range.shard_id);
+        params.__set_properties(_properties);
+        params.__set_row_schema(row_schema);
+        params.__set_batch_size(state->batch_size());
+        params.__set_predicates(predicates);
+        TExtOpenResult result;
+
+        // choose an es node, local is the first choice
+        std::string localhost = BackendOptions::get_localhost();
+        bool is_success = false;
+        for (int j = 0; j < 2; ++j) {
+            for (auto& es_host : es_scan_range.es_hosts) {
+                if ((j == 0 && es_host.hostname != localhost)
+                    || (j == 1 && es_host.hostname == localhost)) {
+                    continue;
+                }
+                Status status = open_es(es_host, result, params);
+                if (status.ok()) {
+                   is_success = true;
+                   _addresses.push_back(es_host);
+                   _scan_handles.push_back(result.scan_handle);
+                   if (result.__isset.accepted_conjuncts) {
+                       for (int index : result.accepted_conjuncts) {
+                           conjunct_accepted_times[predicate_to_conjunct[index]]++;
+                       }
+                   }
+                   VLOG(1) << ""es open success: scan_range_idx="" << i
+                           << "", params="" << apache::thrift::ThriftDebugString(params)
+                           << "", result="" << apache::thrift::ThriftDebugString(result);
+                   break;
+                } else if (status.code() == TStatusCode::ES_SHARD_NOT_FOUND) {
+                    // if shard not found, try other nodes
+                    LOG(WARNING) << ""shard not found on es node: ""
+                                 << "", address="" << es_host
+                                 << "", scan_range_idx="" << i << "", try other nodes"";
+                } else {
+                    LOG(WARNING) << ""es open error: scan_range_idx="" << i
+                                 << "", address="" << es_host
+                                 << "", msg="" << status.get_error_msg();
+                    return status;
+                } 
+            }
+            if (is_success) {
+                break;
+            }
+        }
+
+        if (!is_success) {
+            std::stringstream ss;
+            ss << ""es open error: scan_range_idx="" << i
+               << "", can't find shard on any node"";
+            return Status(ss.str());
+        }
+    }
+
+    // remove those conjuncts that accepted by all scan ranges
+    for (int i = predicate_to_conjunct.size() - 1; i >= 0; i--) {
+        int conjunct_index = predicate_to_conjunct[i];
+        if (conjunct_accepted_times[conjunct_index] == _scan_ranges.size()) {
+            _conjunct_ctxs.erase(_conjunct_ctxs.begin() + conjunct_index);
+        }
+    }
+
+    return Status::OK;
+}
+
+Status EsScanNode::get_next(RuntimeState* state, RowBatch* row_batch, bool* eos) {
+    VLOG(1) << ""EsScanNode::GetNext"";
+
+    RETURN_IF_ERROR(exec_debug_action(TExecNodePhase::GETNEXT));
+    RETURN_IF_CANCELLED(state);
+    SCOPED_TIMER(_runtime_profile->total_time_counter());
+    SCOPED_TIMER(materialize_tuple_timer());
+
+    // create tuple
+    MemPool* tuple_pool = row_batch->tuple_data_pool();
+    int64_t tuple_buffer_size;
+    uint8_t* tuple_buffer = nullptr;
+    RETURN_IF_ERROR(row_batch->resize_and_allocate_tuple_buffer(state, &tuple_buffer_size, &tuple_buffer));
+    Tuple* tuple = reinterpret_cast<Tuple*>(tuple_buffer);
+    
+    // get batch
+    TExtGetNextResult result;
+    RETURN_IF_ERROR(get_next_from_es(result));
+    VLOG(1) << ""es get next success: result="" << apache::thrift::ThriftDebugString(result);
+    _offsets[_scan_range_idx] += result.rows.num_rows;
+
+    // convert
+    VLOG(1) << ""begin to convert: scan_range_idx="" << _scan_range_idx
+            << "", num_rows="" << result.rows.num_rows;
+    vector<TExtColumnData>& cols = result.rows.cols;
+    // indexes of the next non-null value in the row batch, per column. 
+    vector<int> cols_next_val_idx(_tuple_desc->slots().size(), 0);
+    for (int row_idx = 0; row_idx < result.rows.num_rows; row_idx++) {
+        if (reached_limit()) {
+            *eos = true;
+            break;
+        }
+        RETURN_IF_ERROR(materialize_row(tuple_pool, tuple, cols, row_idx, cols_next_val_idx));
+        TupleRow* tuple_row = row_batch->get_row(row_batch->add_row());
+        tuple_row->set_tuple(0, tuple);
+        if (ExecNode::eval_conjuncts(_conjunct_ctxs.data(), _conjunct_ctxs.size(), tuple_row)) {
+            row_batch->commit_last_row();
+            tuple = reinterpret_cast<Tuple*>(
+                reinterpret_cast<uint8_t*>(tuple) + _tuple_desc->byte_size());
+            ++_num_rows_returned;
+        }
+    }
+
+    VLOG(1) << ""finish one batch: num_rows="" << row_batch->num_rows();
+    COUNTER_SET(_rows_returned_counter, _num_rows_returned);
+    if (result.__isset.eos && result.eos) {
+        VLOG(1) << ""es finish one scan_range: scan_range_idx="" << _scan_range_idx;
+        ++_scan_range_idx;
+    }
+    if (_scan_range_idx == _scan_ranges.size()) {
+        *eos = true;
+    }
+
+    return Status::OK;
+}
+
+Status EsScanNode::close(RuntimeState* state) {
+    if (is_closed()) return Status::OK;
+    VLOG(1) << ""EsScanNode::Close"";
+    RETURN_IF_ERROR(exec_debug_action(TExecNodePhase::CLOSE));
+    SCOPED_TIMER(_runtime_profile->total_time_counter());
+
+    for (int i = 0; i < _addresses.size(); ++i) {
+        TExtCloseParams params;
+        params.__set_scan_handle(_scan_handles[i]);
+        TExtCloseResult result;
+
+#ifndef BE_TEST
+        const TNetworkAddress& address = _addresses[i];
+        try {
+            Status status;
+            ExtDataSourceServiceClientCache* client_cache = _env->extdatasource_client_cache();
+            ExtDataSourceServiceConnection client(client_cache, address, 10000, &status);
+            if (!status.ok()) {
+                LOG(WARNING) << ""es create client error: scan_range_idx="" << i
+                             << "", address="" << address
+                             << "", msg="" << status.get_error_msg();
+                return status;
+            }
+
+            try {
+                client->close(result, params);
+            } catch (apache::thrift::transport::TTransportException& e) {
+                RETURN_IF_ERROR(client.reopen());
+                client->close(result, params);
+            }
+        } catch (apache::thrift::TException &e) {
+            std::stringstream ss;
+            ss << ""es close error: scan_range_idx="" << i
+               << "", msg="" << e.what();
+            LOG(WARNING) << ss.str();
+            return Status(TStatusCode::THRIFT_RPC_ERROR, ss.str(), false);
+        }
+
+        Status status(result.status);
+        if (!status.ok()) {
+            LOG(WARNING) << ""es close error: : scan_range_idx="" << i
+                         << "", msg="" << status.get_error_msg();
+            return status;
+        }
+#else
+        TStatus status;
+        result.__set_status(status);
+#endif
+    }
+
+    RETURN_IF_ERROR(ExecNode::close(state));
+    return Status::OK;
+}
+
+void EsScanNode::debug_string(int indentation_level, stringstream* out) const {
+    *out << string(indentation_level * 2, ' ');
+    *out << ""EsScanNode(tupleid="" << _tuple_id;
+    *out << "")"" << std::endl;
+
+    for (int i = 0; i < _children.size(); ++i) {
+        _children[i]->debug_string(indentation_level + 1, out);
+    }
+}
+
+Status EsScanNode::set_scan_ranges(const vector<TScanRangeParams>& scan_ranges) {
+    for (int i = 0; i < scan_ranges.size(); ++i) {
+        TScanRangeParams scan_range = scan_ranges[i];
+        DCHECK(scan_range.scan_range.__isset.es_scan_range);
+        TEsScanRange es_scan_range = scan_range.scan_range.es_scan_range;
+        _scan_ranges.push_back(es_scan_range);
+    }
+
+    _offsets.resize(scan_ranges.size(), 0);
+    return Status::OK;
+}
+
+Status EsScanNode::open_es(TNetworkAddress& address, TExtOpenResult& result, TExtOpenParams& params) {
+#ifndef BE_TEST
+    try {
+        ExtDataSourceServiceClientCache* client_cache = _env->extdatasource_client_cache();
+        Status status;
+        ExtDataSourceServiceConnection client(client_cache, address, 10000, &status);
+        if (!status.ok()) {
+            std::stringstream ss;
+            ss << ""es create client error: address="" << address
+               << "", msg="" << status.get_error_msg();
+            return Status(ss.str());
+        }
+
+        try {
+            client->open(result, params);
+        } catch (apache::thrift::transport::TTransportException& e) {
+            RETURN_IF_ERROR(client.reopen());
+            client->open(result, params);
+        }
+        return Status(result.status);
+    } catch (apache::thrift::TException &e) {
+        std::stringstream ss;
+        ss << ""es open error: address="" << address << "", msg="" << e.what();
+        return Status(ss.str());
+    }
+#else
+    TStatus status;
+    result.__set_status(status);
+    result.__set_scan_handle(""0"");
+    return Status(status);
+#endif
+}
+
+bool EsScanNode::get_disjuncts(ExprContext* context, Expr* conjunct,
+                               vector<TExtPredicate>& disjuncts) {
+    if (TExprNodeType::BINARY_PRED == conjunct->node_type()) {
+        if (conjunct->children().size() != 2) {
+            VLOG(1) << ""get disjuncts fail: number of childs is not 2"";
+            return false;
+        }
+        SlotRef* slotRef;
+        TExprOpcode::type op;
+        Expr* expr;
+        if (TExprNodeType::SLOT_REF == conjunct->get_child(0)->node_type()) {
+            expr = conjunct->get_child(1);
+            slotRef = (SlotRef*)(conjunct->get_child(0));
+            op = conjunct->op();
+        } else if (TExprNodeType::SLOT_REF == conjunct->get_child(1)->node_type()) {
+            expr = conjunct->get_child(0);
+            slotRef = (SlotRef*)(conjunct->get_child(1));
+            op = conjunct->op();
+        } else {
+            VLOG(1) << ""get disjuncts fail: no SLOT_REF child"";
+            return false;
+        }
+
+        std::vector<SlotId> slot_ids;
+        slotRef->get_slot_ids(&slot_ids);
+        SlotDescriptor* slot_desc = nullptr;
+        for (SlotDescriptor* slot : _tuple_desc->slots()) {
+            if (slot->id() == slot_ids[0]) {
+                slot_desc = slot;
+                break;
+            }
+        }
+        if (nullptr == slot_desc) {
+            VLOG(1) << ""get disjuncts fail: slot_desc is null"";
+            return false;
+        }
+
+        TExtColumnDesc columnDesc;
+        columnDesc.__set_name(slot_desc->col_name());
+        columnDesc.__set_type(slot_desc->type().to_thrift());
+        TExtBinaryPredicate binaryPredicate;
+        binaryPredicate.__set_col(columnDesc);
+        binaryPredicate.__set_op(op);
+        binaryPredicate.__set_value(to_exe_literal(context, expr));
+        TExtPredicate predicate;
+        predicate.__set_node_type(TExprNodeType::BINARY_PRED);
+        predicate.__set_binary_predicate(binaryPredicate);
+        disjuncts.push_back(std::move(predicate));
+        return true;
+    } else if (TExprNodeType::COMPOUND_PRED == conjunct->node_type()) {
+        if (TExprOpcode::COMPOUND_OR != conjunct->op()) {
+            VLOG(1) << ""get disjuncts fail: op is not COMPOUND_OR"";
+            return false;
+        }
+        if (!get_disjuncts(context, conjunct->get_child(0), disjuncts)) {
+            return false;
+        }
+        if (!get_disjuncts(context, conjunct->get_child(1), disjuncts)) {
+            return false;
+        }
+        return true;
+    } else {
+        VLOG(1) << ""get disjuncts fail: node type is "" << conjunct->node_type()
+                << "", should be BINARY_PRED or COMPOUND_PRED"";
+        return false;
+    }
+}
+
+TExtLiteral EsScanNode::to_exe_literal(ExprContext* context, Expr* expr) {
+    void* value = context->get_value(expr, NULL);
+    TExtLiteral literal;
+    literal.__set_node_type(expr->node_type());","[{'comment': 'ExprContext ctx\r\nexpr.get_xxx_value(&ctx, nullptr)', 'commenter': 'imay'}]"
450,be/src/exec/es_scan_node.cpp,"@@ -0,0 +1,668 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""es_scan_node.h""
+
+#include <string>
+#include <boost/algorithm/string.hpp>
+#include <gutil/strings/substitute.h>
+
+#include ""gen_cpp/PlanNodes_types.h""
+#include ""gen_cpp/Exprs_types.h""
+#include ""runtime/runtime_state.h""
+#include ""runtime/row_batch.h""
+#include ""runtime/string_value.h""
+#include ""runtime/tuple_row.h""
+#include ""runtime/client_cache.h""
+#include ""util/runtime_profile.h""
+#include ""util/debug_util.h""
+#include ""service/backend_options.h""
+#include ""olap/olap_common.h""
+#include ""olap/utils.h""
+#include ""exprs/expr_context.h""
+#include ""exprs/expr.h""
+#include ""exprs/slot_ref.h""
+
+namespace doris {
+
+// $0 = column type (e.g. INT)
+const string ERROR_INVALID_COL_DATA = ""Data source returned inconsistent column data. ""
+    ""Expected value of type $0 based on column metadata. This likely indicates a ""
+    ""problem with the data source library."";
+const string ERROR_MEM_LIMIT_EXCEEDED = ""DataSourceScanNode::$0() failed to allocate ""
+    ""$1 bytes for $2."";
+
+EsScanNode::EsScanNode(
+        ObjectPool* pool,
+        const TPlanNode& tnode,
+        const DescriptorTbl& descs) :
+            ScanNode(pool, tnode, descs),
+            _tuple_id(tnode.es_scan_node.tuple_id),
+            _scan_range_idx(0) {
+    if (tnode.es_scan_node.__isset.properties) {
+        _properties = tnode.es_scan_node.properties;
+    }
+}
+
+EsScanNode::~EsScanNode() {
+}
+
+Status EsScanNode::prepare(RuntimeState* state) {
+    VLOG(1) << ""EsScanNode::Prepare"";
+
+    RETURN_IF_ERROR(ScanNode::prepare(state));
+    _tuple_desc = state->desc_tbl().get_tuple_descriptor(_tuple_id);
+    if (_tuple_desc == nullptr) {
+        std::stringstream ss;
+        ss << ""es tuple descriptor is null, _tuple_id="" << _tuple_id;
+        LOG(WARNING) << ss.str();
+        return Status(ss.str());
+    }
+    _env = state->exec_env();
+
+    return Status::OK;
+}
+
+Status EsScanNode::open(RuntimeState* state) {
+    VLOG(1) << ""EsScanNode::Open"";
+
+    RETURN_IF_ERROR(exec_debug_action(TExecNodePhase::OPEN));
+    RETURN_IF_CANCELLED(state);
+    SCOPED_TIMER(_runtime_profile->total_time_counter());
+    RETURN_IF_ERROR(ExecNode::open(state));
+
+    // TExtOpenParams.row_schema
+    vector<TExtColumnDesc> cols;
+    for (const SlotDescriptor* slot : _tuple_desc->slots()) {
+        TExtColumnDesc col;
+        col.__set_name(slot->col_name());
+        col.__set_type(slot->type().to_thrift());
+        cols.emplace_back(std::move(col));
+    }
+    TExtTableSchema row_schema;
+    row_schema.cols = std::move(cols);
+    row_schema.__isset.cols = true;
+
+    // TExtOpenParams.predicates
+    vector<vector<TExtPredicate> > predicates;
+    vector<int> predicate_to_conjunct;
+    for (int i = 0; i < _conjunct_ctxs.size(); ++i) {
+        VLOG(1) << ""conjunct: "" << _conjunct_ctxs[i]->root()->debug_string();
+        vector<TExtPredicate> disjuncts;
+        if (get_disjuncts(_conjunct_ctxs[i], _conjunct_ctxs[i]->root(), disjuncts)) {
+            predicates.emplace_back(std::move(disjuncts));
+            predicate_to_conjunct.push_back(i);
+        }
+    }
+
+    // open every scan range
+    vector<int> conjunct_accepted_times(_conjunct_ctxs.size(), 0); 
+    for (int i = 0; i < _scan_ranges.size(); ++i) {
+        TEsScanRange& es_scan_range = _scan_ranges[i];
+
+        if (es_scan_range.es_hosts.empty()) {
+            std::stringstream ss;
+            ss << ""es fail to open: hosts empty"";
+            LOG(WARNING) << ss.str();
+            return Status(ss.str());
+        }
+
+
+        // TExtOpenParams
+        TExtOpenParams params;
+        params.__set_query_id(state->query_id());
+        _properties[""index""] = es_scan_range.index;
+        if (es_scan_range.__isset.type) {
+            _properties[""type""] = es_scan_range.type;
+        }
+        _properties[""shard_id""] = std::to_string(es_scan_range.shard_id);
+        params.__set_properties(_properties);
+        params.__set_row_schema(row_schema);
+        params.__set_batch_size(state->batch_size());
+        params.__set_predicates(predicates);
+        TExtOpenResult result;
+
+        // choose an es node, local is the first choice
+        std::string localhost = BackendOptions::get_localhost();
+        bool is_success = false;
+        for (int j = 0; j < 2; ++j) {
+            for (auto& es_host : es_scan_range.es_hosts) {
+                if ((j == 0 && es_host.hostname != localhost)
+                    || (j == 1 && es_host.hostname == localhost)) {
+                    continue;
+                }
+                Status status = open_es(es_host, result, params);
+                if (status.ok()) {
+                   is_success = true;
+                   _addresses.push_back(es_host);
+                   _scan_handles.push_back(result.scan_handle);
+                   if (result.__isset.accepted_conjuncts) {
+                       for (int index : result.accepted_conjuncts) {
+                           conjunct_accepted_times[predicate_to_conjunct[index]]++;
+                       }
+                   }
+                   VLOG(1) << ""es open success: scan_range_idx="" << i
+                           << "", params="" << apache::thrift::ThriftDebugString(params)
+                           << "", result="" << apache::thrift::ThriftDebugString(result);
+                   break;
+                } else if (status.code() == TStatusCode::ES_SHARD_NOT_FOUND) {
+                    // if shard not found, try other nodes
+                    LOG(WARNING) << ""shard not found on es node: ""
+                                 << "", address="" << es_host
+                                 << "", scan_range_idx="" << i << "", try other nodes"";
+                } else {
+                    LOG(WARNING) << ""es open error: scan_range_idx="" << i
+                                 << "", address="" << es_host
+                                 << "", msg="" << status.get_error_msg();
+                    return status;
+                } 
+            }
+            if (is_success) {
+                break;
+            }
+        }
+
+        if (!is_success) {
+            std::stringstream ss;
+            ss << ""es open error: scan_range_idx="" << i
+               << "", can't find shard on any node"";
+            return Status(ss.str());
+        }
+    }
+
+    // remove those conjuncts that accepted by all scan ranges
+    for (int i = predicate_to_conjunct.size() - 1; i >= 0; i--) {
+        int conjunct_index = predicate_to_conjunct[i];
+        if (conjunct_accepted_times[conjunct_index] == _scan_ranges.size()) {
+            _conjunct_ctxs.erase(_conjunct_ctxs.begin() + conjunct_index);
+        }
+    }
+
+    return Status::OK;
+}
+
+Status EsScanNode::get_next(RuntimeState* state, RowBatch* row_batch, bool* eos) {
+    VLOG(1) << ""EsScanNode::GetNext"";
+
+    RETURN_IF_ERROR(exec_debug_action(TExecNodePhase::GETNEXT));
+    RETURN_IF_CANCELLED(state);
+    SCOPED_TIMER(_runtime_profile->total_time_counter());
+    SCOPED_TIMER(materialize_tuple_timer());
+
+    // create tuple
+    MemPool* tuple_pool = row_batch->tuple_data_pool();
+    int64_t tuple_buffer_size;
+    uint8_t* tuple_buffer = nullptr;
+    RETURN_IF_ERROR(row_batch->resize_and_allocate_tuple_buffer(state, &tuple_buffer_size, &tuple_buffer));
+    Tuple* tuple = reinterpret_cast<Tuple*>(tuple_buffer);
+    
+    // get batch
+    TExtGetNextResult result;
+    RETURN_IF_ERROR(get_next_from_es(result));
+    VLOG(1) << ""es get next success: result="" << apache::thrift::ThriftDebugString(result);
+    _offsets[_scan_range_idx] += result.rows.num_rows;
+
+    // convert
+    VLOG(1) << ""begin to convert: scan_range_idx="" << _scan_range_idx
+            << "", num_rows="" << result.rows.num_rows;
+    vector<TExtColumnData>& cols = result.rows.cols;
+    // indexes of the next non-null value in the row batch, per column. 
+    vector<int> cols_next_val_idx(_tuple_desc->slots().size(), 0);
+    for (int row_idx = 0; row_idx < result.rows.num_rows; row_idx++) {
+        if (reached_limit()) {
+            *eos = true;
+            break;
+        }
+        RETURN_IF_ERROR(materialize_row(tuple_pool, tuple, cols, row_idx, cols_next_val_idx));
+        TupleRow* tuple_row = row_batch->get_row(row_batch->add_row());
+        tuple_row->set_tuple(0, tuple);
+        if (ExecNode::eval_conjuncts(_conjunct_ctxs.data(), _conjunct_ctxs.size(), tuple_row)) {
+            row_batch->commit_last_row();
+            tuple = reinterpret_cast<Tuple*>(
+                reinterpret_cast<uint8_t*>(tuple) + _tuple_desc->byte_size());
+            ++_num_rows_returned;
+        }
+    }
+
+    VLOG(1) << ""finish one batch: num_rows="" << row_batch->num_rows();
+    COUNTER_SET(_rows_returned_counter, _num_rows_returned);
+    if (result.__isset.eos && result.eos) {
+        VLOG(1) << ""es finish one scan_range: scan_range_idx="" << _scan_range_idx;
+        ++_scan_range_idx;
+    }
+    if (_scan_range_idx == _scan_ranges.size()) {
+        *eos = true;
+    }
+
+    return Status::OK;
+}
+
+Status EsScanNode::close(RuntimeState* state) {
+    if (is_closed()) return Status::OK;
+    VLOG(1) << ""EsScanNode::Close"";
+    RETURN_IF_ERROR(exec_debug_action(TExecNodePhase::CLOSE));
+    SCOPED_TIMER(_runtime_profile->total_time_counter());
+
+    for (int i = 0; i < _addresses.size(); ++i) {
+        TExtCloseParams params;
+        params.__set_scan_handle(_scan_handles[i]);
+        TExtCloseResult result;
+
+#ifndef BE_TEST
+        const TNetworkAddress& address = _addresses[i];
+        try {
+            Status status;
+            ExtDataSourceServiceClientCache* client_cache = _env->extdatasource_client_cache();
+            ExtDataSourceServiceConnection client(client_cache, address, 10000, &status);
+            if (!status.ok()) {
+                LOG(WARNING) << ""es create client error: scan_range_idx="" << i
+                             << "", address="" << address
+                             << "", msg="" << status.get_error_msg();
+                return status;
+            }
+
+            try {
+                client->close(result, params);
+            } catch (apache::thrift::transport::TTransportException& e) {
+                RETURN_IF_ERROR(client.reopen());
+                client->close(result, params);
+            }
+        } catch (apache::thrift::TException &e) {
+            std::stringstream ss;
+            ss << ""es close error: scan_range_idx="" << i
+               << "", msg="" << e.what();
+            LOG(WARNING) << ss.str();
+            return Status(TStatusCode::THRIFT_RPC_ERROR, ss.str(), false);
+        }
+
+        Status status(result.status);
+        if (!status.ok()) {
+            LOG(WARNING) << ""es close error: : scan_range_idx="" << i
+                         << "", msg="" << status.get_error_msg();
+            return status;
+        }
+#else
+        TStatus status;
+        result.__set_status(status);
+#endif
+    }
+
+    RETURN_IF_ERROR(ExecNode::close(state));
+    return Status::OK;
+}
+
+void EsScanNode::debug_string(int indentation_level, stringstream* out) const {
+    *out << string(indentation_level * 2, ' ');
+    *out << ""EsScanNode(tupleid="" << _tuple_id;
+    *out << "")"" << std::endl;
+
+    for (int i = 0; i < _children.size(); ++i) {
+        _children[i]->debug_string(indentation_level + 1, out);
+    }
+}
+
+Status EsScanNode::set_scan_ranges(const vector<TScanRangeParams>& scan_ranges) {
+    for (int i = 0; i < scan_ranges.size(); ++i) {
+        TScanRangeParams scan_range = scan_ranges[i];
+        DCHECK(scan_range.scan_range.__isset.es_scan_range);
+        TEsScanRange es_scan_range = scan_range.scan_range.es_scan_range;
+        _scan_ranges.push_back(es_scan_range);
+    }
+
+    _offsets.resize(scan_ranges.size(), 0);
+    return Status::OK;
+}
+
+Status EsScanNode::open_es(TNetworkAddress& address, TExtOpenResult& result, TExtOpenParams& params) {
+#ifndef BE_TEST
+    try {
+        ExtDataSourceServiceClientCache* client_cache = _env->extdatasource_client_cache();
+        Status status;
+        ExtDataSourceServiceConnection client(client_cache, address, 10000, &status);
+        if (!status.ok()) {
+            std::stringstream ss;
+            ss << ""es create client error: address="" << address
+               << "", msg="" << status.get_error_msg();
+            return Status(ss.str());
+        }
+
+        try {
+            client->open(result, params);
+        } catch (apache::thrift::transport::TTransportException& e) {
+            RETURN_IF_ERROR(client.reopen());
+            client->open(result, params);
+        }
+        return Status(result.status);
+    } catch (apache::thrift::TException &e) {
+        std::stringstream ss;
+        ss << ""es open error: address="" << address << "", msg="" << e.what();
+        return Status(ss.str());
+    }
+#else
+    TStatus status;
+    result.__set_status(status);
+    result.__set_scan_handle(""0"");
+    return Status(status);
+#endif
+}
+
+bool EsScanNode::get_disjuncts(ExprContext* context, Expr* conjunct,
+                               vector<TExtPredicate>& disjuncts) {
+    if (TExprNodeType::BINARY_PRED == conjunct->node_type()) {
+        if (conjunct->children().size() != 2) {
+            VLOG(1) << ""get disjuncts fail: number of childs is not 2"";
+            return false;
+        }
+        SlotRef* slotRef;
+        TExprOpcode::type op;
+        Expr* expr;
+        if (TExprNodeType::SLOT_REF == conjunct->get_child(0)->node_type()) {
+            expr = conjunct->get_child(1);
+            slotRef = (SlotRef*)(conjunct->get_child(0));
+            op = conjunct->op();
+        } else if (TExprNodeType::SLOT_REF == conjunct->get_child(1)->node_type()) {
+            expr = conjunct->get_child(0);
+            slotRef = (SlotRef*)(conjunct->get_child(1));
+            op = conjunct->op();
+        } else {
+            VLOG(1) << ""get disjuncts fail: no SLOT_REF child"";
+            return false;
+        }
+","[{'comment': 'you need to check if `expr` is a literal', 'commenter': 'imay'}]"
452,be/src/olap/rowset/alpha_rowset_reader.h,"@@ -0,0 +1,78 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#ifndef DORIS_BE_SRC_OLAP_ROWSET_ALPHA_ROWSET_READER_H
+#define DORIS_BE_SRC_OLAP_ROWSET_ALPHA_ROWSET_READER_H
+P
+#include ""olap/rowset/rowset_reader.h""
+#include ""olap/segment_group.h""
+#include ""olap/column_data.h""","[{'comment': 'if segment group only belongs to alpha rowset reader, why not move it to rowset folder?', 'commenter': 'yiguolei'}, {'comment': 'Ok, I will move everything related to rowset folder', 'commenter': 'kangpinghuang'}]"
452,be/src/olap/rowset/alpha_rowset.cpp,"@@ -0,0 +1,139 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""olap/rowset/alpha_rowset.h""
+
+namespace doris {
+
+AlphaRowset::AlphaRowset(const RowFields& tablet_schema,
+        int num_key_fields, int num_short_key_fields,
+        int num_rows_per_row_block, const std::string rowset_path,
+        std::shared_ptr<RowsetMeta> rowset_meta) : _tablet_schema(tablet_schema),","[{'comment': 'typedef RowsetMetaSharedPtr?', 'commenter': 'yiguolei'}, {'comment': 'ok, I will add it.', 'commenter': 'kangpinghuang'}]"
452,be/src/olap/rowset/alpha_rowset.h,"@@ -0,0 +1,60 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#ifndef DORIS_BE_SRC_OLAP_ROWSET_ALPHA_ROWSET_H
+#define DORIS_BE_SRC_OLAP_ROWSET_ALPHA_ROWSET_H
+
+#include ""olap/rowset/rowset.h""
+#include ""olap/segment_group.h""
+#include ""olap/alpha_rowset_reader.h""
+#include ""olap/alpha_rowset_builder.h""
+
+#include <vector>
+#include <memory>
+
+namespace doris {
+
+class AlphaRowset {
+public:
+    AlphaRowset(const RowFields& tablet_schema,
+        int num_key_fields, int num_short_key_fields,
+        int num_rows_per_row_block, const std::string rowset_path,
+        std::shared_ptr<RowsetMeta> rowset_meta);
+
+    virtual NewStatus init();
+
+    virtual std::unique_ptr<RowsetReader> create_reader();
+
+    virtual NewStatus copy(RowsetBuilder* dest_rowset_builder)；
+
+    virtual NewStatus delete();
+
+    virtual NewStatus save_meta();","[{'comment': 'Rowset should provide set Version  and get_meta API?', 'commenter': 'yiguolei'}, {'comment': 'OK, I will add them', 'commenter': 'kangpinghuang'}]"
452,be/src/olap/segment_group.cpp,"@@ -115,15 +125,14 @@ SegmentGroup::SegmentGroup(Tablet* tablet, bool delete_flag,
     _new_segment_created = false;
     _empty = false;
 
-    const RowFields& tablet_schema = _tablet->tablet_schema();
-    for (size_t i = 0; i < _tablet->num_short_key_fields(); ++i) {
-        _short_key_info_list.push_back(tablet_schema[i]);
-        _short_key_length += tablet_schema[i].index_length + 1;// 1 for null byte
-        if (tablet_schema[i].type == OLAP_FIELD_TYPE_CHAR ||
-            tablet_schema[i].type == OLAP_FIELD_TYPE_VARCHAR) {
+    for (size_t i = 0; i < _num_key_fields; ++i) {
+        _short_key_info_list.push_back(_tablet_schema[i]);
+        _short_key_length += _tablet_schema[i].index_length + 1;// 1 for null byte
+        if (_tablet_schema[i].type == OLAP_FIELD_TYPE_CHAR ||
+            _tablet_schema[i].type == OLAP_FIELD_TYPE_VARCHAR) {
             _new_short_key_length += sizeof(Slice) + 1;
         } else {
-            _new_short_key_length += tablet_schema[i].index_length + 1;","[{'comment': 'pls make sure it is _tablet_meta not tablet_meta!!!', 'commenter': 'yiguolei'}, {'comment': 'ok', 'commenter': 'kangpinghuang'}]"
452,be/src/olap/segment_group.cpp,"@@ -139,19 +148,58 @@ SegmentGroup::~SegmentGroup() {
     _seg_pb_map.clear();
 }
 
+std::string SegmentGroup::_construct_pending_file_path(int32_t segment, const std::string& suffix) {","[{'comment': 'using segment_no ? ', 'commenter': 'yiguolei'}]"
452,be/src/olap/segment_group.cpp,"@@ -276,28 +324,26 @@ OLAPStatus SegmentGroup::load() {
     }
 
     if (_index.init(_short_key_length, _new_short_key_length,
-                    _tablet->num_short_key_fields(), &_short_key_info_list) != OLAP_SUCCESS) {
+                    _num_short_key_fields, &_short_key_info_list) != OLAP_SUCCESS) {
         OLAP_LOG_WARNING(""fail to create MemIndex. [num_segment=%d]"", _num_segments);
         return res;
     }
 
     // for each segment
     for (uint32_t seg_id = 0; seg_id < _num_segments; ++seg_id) {
-        if (COLUMN_ORIENTED_FILE == _tablet->data_file_type()) {
-            string seg_path = construct_data_file_path(_segment_group_id, seg_id);
-            if (OLAP_SUCCESS != (res = load_pb(seg_path.c_str(), seg_id))) {
-                LOG(WARNING) << ""failed to load pb structures. [seg_path='"" << seg_path << ""']"";
-                _check_io_error(res);
-                return res;
-            }
+        string seg_path = construct_data_file_path(_segment_group_id, seg_id);","[{'comment': 'why not use _construct file path?', 'commenter': 'yiguolei'}, {'comment': 'Because there is difference between pending_segment_group and segment_group', 'commenter': 'kangpinghuang'}]"
452,be/src/olap/segment_group.cpp,"@@ -276,28 +324,26 @@ OLAPStatus SegmentGroup::load() {
     }
 
     if (_index.init(_short_key_length, _new_short_key_length,
-                    _tablet->num_short_key_fields(), &_short_key_info_list) != OLAP_SUCCESS) {
+                    _num_short_key_fields, &_short_key_info_list) != OLAP_SUCCESS) {
         OLAP_LOG_WARNING(""fail to create MemIndex. [num_segment=%d]"", _num_segments);
         return res;
     }
 
     // for each segment
     for (uint32_t seg_id = 0; seg_id < _num_segments; ++seg_id) {
-        if (COLUMN_ORIENTED_FILE == _tablet->data_file_type()) {
-            string seg_path = construct_data_file_path(_segment_group_id, seg_id);
-            if (OLAP_SUCCESS != (res = load_pb(seg_path.c_str(), seg_id))) {
-                LOG(WARNING) << ""failed to load pb structures. [seg_path='"" << seg_path << ""']"";
-                _check_io_error(res);
-                return res;
-            }
+        string seg_path = construct_data_file_path(_segment_group_id, seg_id);
+        if (OLAP_SUCCESS != (res = load_pb(seg_path.c_str(), seg_id))) {
+            LOG(WARNING) << ""failed to load pb structures. [seg_path='"" << seg_path << ""']"";
+            
+            return res;
         }
-
+        
         // get full path for one segment
         string path = construct_index_file_path(_segment_group_id, seg_id);","[{'comment': 'why not use _construct_file_path?', 'commenter': 'yiguolei'}, {'comment': 'Because there is difference between pending_segment_group and segment_group', 'commenter': 'kangpinghuang'}]"
452,be/src/olap/segment_group.cpp,"@@ -551,23 +595,20 @@ OLAPStatus SegmentGroup::add_short_key(const RowCursor& short_key, const uint32_
             char errmsg[64];
             LOG(WARNING) << ""can not create file. [file_path='"" << file_path
                 << ""' err='"" << strerror_r(errno, errmsg, 64) << ""']"";
-            _check_io_error(res);
             return res;
         }
         _new_segment_created = true;
 
         // 准备FileHeader
         if ((res = _file_header.prepare(&_current_file_handler)) != OLAP_SUCCESS) {
             OLAP_LOG_WARNING(""write file header error. [err=%m]"");
-            _check_io_error(res);
             return res;
         }
 
         // 跳过FileHeader
         if (_current_file_handler.seek(_file_header.size(), SEEK_SET) == -1) {
             OLAP_LOG_WARNING(""lseek header file error. [err=%m]"");
             res = OLAP_ERR_IO_ERROR;
-            _check_io_error(res);","[{'comment': '@chaoyli  pay attention to this!!!', 'commenter': 'yiguolei'}]"
452,be/src/olap/rowset/alpha_rowset_meta.cpp,"@@ -0,0 +1,48 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""olap/rowset/alpha_rowset_meta.h""
+
+namespace doris {
+
+bool AlphaRowsetMeta::deserialize_extra_properties() {
+    std::string extra_properties = _rowset_meta->extra_properties();
+    bool parsed = _extra_meta_pb->ParseFromString(extra_properties);
+    if (!parsed) {
+        LOG(WARNING) << ""alpha rowset parse extra propertis failed."";
+    }
+    return parsed;
+}
+
+void AlphaRowsetMeta::get_segment_groups(std::vector<PSegmentGroup>* segment_groups) {
+    for (auto& segment_group : _extra_meta_pb.segment_groups()) {
+        segment_groups->push_back(segment_group);
+    }
+}
+
+void AlphaRowsetMeta::add_segment_group(PSegmentGroup& segment_group) {","[{'comment': 'I found that RowsetMetaPB has set pb is suffix. It may be better to change PSegmentGroup to SegmentGroupPB?\r\nAnd also const SegmentGroupPB& may be better?', 'commenter': 'chaoyli'}, {'comment': 'I will use const PSegmentGroup&, and I will add another patch to fix PSegmentGroup to SegmentGroupPB', 'commenter': 'kangpinghuang'}]"
452,be/src/olap/rowset/alpha_rowset_builder.h,"@@ -0,0 +1,43 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#ifndef DORIS_BE_SRC_OLAP_ROWSET_ALPHA_ROWSET_BUILDER_H
+#define DORIS_BE_SRC_OLAP_ROWSET_ALPHA_ROWSET_BUILDER_H
+
+#include ""olap/rowset/rowset_builder.h""
+
+namespace doris {
+
+class AlphaRowsetBuilder {
+public:
+    virtual NewStatus init(std::string rowset_id, const std::string& rowset_path_prefix, Schema* schema);","[{'comment': 'std::string -> int64_t', 'commenter': 'chaoyli'}, {'comment': 'ok, I will fix it', 'commenter': 'kangpinghuang'}]"
452,be/src/olap/rowset/alpha_rowset_builder.cpp,"@@ -0,0 +1,38 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""olap/rowset/alpha_rowset_builder.h""
+
+namespace doris {
+
+NewStatus AlphaRowsetBuilder::init(std::string rowset_id, const std::string& rowset_path_prefix, Schema* schema) {
+    return NewStatus.OK();
+}
+
+NewStatus AlphaRowsetBuilder::add_row_block(RowBlock* row_block) {","[{'comment': 'row_block will not be changed. May be use const RowBlock&', 'commenter': 'chaoyli'}, {'comment': 'ok', 'commenter': 'kangpinghuang'}]"
452,be/src/olap/rowset/alpha_rowset.cpp,"@@ -0,0 +1,147 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""olap/rowset/alpha_rowset.h""
+
+namespace doris {
+
+AlphaRowset::AlphaRowset(const RowFields& tablet_schema,
+        int num_key_fields, int num_short_key_fields,
+        int num_rows_per_row_block, const std::string rowset_path,
+        RowsetMetaSharedPtr rowset_meta) : _tablet_schema(tablet_schema),
+        _num_key_fields(num_key_fields),
+        _num_short_key_fields(num_short_key_fields), 
+        _num_rows_per_row_block(num_rows_per_row_block),
+        _rowset_path(rowset_path),
+        _rowset_meta(rowset_meta),
+        _segment_group_size(0),
+        _is_cumulative_rowset(false) {
+    Version version = _rowset_meta->get_version();
+    if (veresion.first == version.second) {
+        _is_cumulative_rowset = false;
+    } else {
+        _is_cumulative_rowset = true;
+    }
+}
+
+NewStatus AlphaRowset::init() {
+    _init_segment_groups();
+    return NewStatus.OK();
+}
+
+std::unique_ptr<RowsetReader> AlphaRowset::create_reader() {
+    std::vector<SegmentGroup*> segment_groups;
+    for (auto& segment_group : _segment_groups) {
+        segment_groups.push_back(segment_group.get());
+    }
+    AlphaRowsetReader* rowset_reader = new AlphaRowsetReader(_tablet_schema,
+            _num_key_fields, _num_short_key_fields, _num_rows_per_row_block,
+            _rowset_path, _rowset_meta.get(), _segment_groups);
+    return std::unique_ptr<RowsetReader>(rowset_reader);","[{'comment': 'unique_ptr can be assigned?', 'commenter': 'chaoyli'}, {'comment': 'unique_ptr can be used as return value', 'commenter': 'kangpinghuang'}]"
456,fe/src/main/cup/sql_parser.cup,"@@ -1103,6 +1122,63 @@ opt_cluster ::=
     :}
     ;
 
+// Routine load statement
+create_routine_load_stmt ::=
+    KW_CREATE KW_ROUTINE_LOAD ident:jobName KW_ON ident:db DOT ident:table","[{'comment': '1. split KW_ROUTINE_LOAD into KW_ROUTINE and KW_LOAD\r\n2. db.table have table_name which you can reuse', 'commenter': 'imay'}]"
456,fe/src/main/cup/sql_parser.cup,"@@ -205,7 +206,9 @@ terminal String KW_ADD, KW_ADMIN, KW_AFTER, KW_AGGREGATE, KW_ALL, KW_ALTER, KW_A
     KW_INNER, KW_INSERT, KW_INT, KW_INTERVAL, KW_INTO, KW_IS, KW_ISNULL,  KW_ISOLATION,
     KW_JOIN,
     KW_KEY, KW_KILL,
-    KW_LABEL, KW_LARGEINT, KW_LAST, KW_LEFT, KW_LESS, KW_LEVEL, KW_LIKE, KW_LIMIT, KW_LINK, KW_LOAD, KW_LOCAL, KW_LOCATION,
+    KW_LABEL, KW_LARGEINT, KW_LAST, KW_LEFT, KW_LESS, KW_LEVEL, KW_LIKE, KW_LIMIT, KW_LINK, KW_LOAD,
+    KW_ROUTINE_LOAD, KW_KAFKA, KW_PAUSE, KW_RESUME, KW_STOP,","[{'comment': 'put KW_ROUTINE KW_KAFKA to keyword', 'commenter': 'imay'}, {'comment': 'KW_KAFKA already in keyword', 'commenter': 'EmmyMiao87'}]"
456,fe/src/main/cup/sql_parser.cup,"@@ -1103,6 +1122,63 @@ opt_cluster ::=
     :}
     ;
 
+// Routine load statement
+create_routine_load_stmt ::=
+    KW_CREATE KW_ROUTINE_LOAD ident:jobName KW_ON ident:db DOT ident:table
+    LPAREN routine_load_desc:routineLoadDesc RPAREN
+    opt_properties:routineLoadProperties
+    KW_FROM stream_data_source_type:type LPAREN key_value_map:typeProperties RPAREN
+    {:
+        RESULT = new CreateRoutineLoadStmt(jobName, db, table, routineLoadDesc, routineLoadProperties, type, typeProperties);
+    :}
+    ;
+
+routine_load_desc ::=","[{'comment': 'I think you can put all of these to properties', 'commenter': 'imay'}]"
456,fe/src/main/java/org/apache/doris/analysis/CreateRoutineLoadStmt.java,"@@ -0,0 +1,280 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.analysis;
+
+import com.google.common.base.Strings;
+import com.google.common.collect.ImmutableSet;
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.catalog.Database;
+import org.apache.doris.catalog.OlapTable;
+import org.apache.doris.catalog.StreamDataSourceType;
+import org.apache.doris.catalog.Table;
+import org.apache.doris.common.AnalysisException;
+import org.apache.doris.common.ErrorCode;
+import org.apache.doris.common.ErrorReport;
+import org.apache.doris.common.FeNameFormat;
+import org.apache.doris.common.UserException;
+import org.apache.doris.mysql.privilege.PrivPredicate;
+import org.apache.doris.qe.ConnectContext;
+
+import java.util.Map;
+import java.util.Optional;
+import java.util.regex.Pattern;
+
+/*
+ Create routine Load statement,  continually load data from a streaming app
+
+ syntax:
+      CREATE ROUTINE LOAD name ON database.table
+      (properties of routine load)
+      [PROPERTIES (key1=value1, )]
+      FROM [KAFKA](type of routine load)
+      (properties of this type)
+
+      properties of routine load:
+          [COLUMNS TERMINATED BY separator ]
+          [(col1, ...)]
+          [SET (k1=f1(xx), k2=f2(xx))]
+          WHERE
+          [PARTITION (p1, p2)]
+
+      type of routine load:
+          KAFKA
+
+      different type has different properties
+      properties of this type:
+          k1 = v1
+          k2 = v2
+
+*/
+public class CreateRoutineLoadStmt extends DdlStmt {
+    // routine load properties
+    public static final String DESIRED_CONCURRENT_NUMBER_PROPERTY = ""desired_concurrent_number"";
+    // max error number in ten thousand records
+    public static final String MAX_ERROR_NUMBER_PROPERTY = ""max_error_number"";
+
+    // kafka type properties
+    public static final String KAFKA_ENDPOINT_PROPERTY = ""kafka_endpoint"";
+    public static final String KAFKA_TOPIC_PROPERTY = ""kafka_topic"";
+    // optional
+    public static final String KAFKA_PARTITIONS_PROPERTY = ""kafka_partitions"";
+
+    private static final String NAME_TYPE = ""ROUTINE LOAD NAME"";
+    private static final String ENDPOINT_REGEX = ""([a-z]+\\.*)+:[0-9]+"";
+    private static final String EMPTY_STRING = """";
+
+    private static final ImmutableSet<String> PROPERTIES_SET = new ImmutableSet.Builder<String>()
+            .add(DESIRED_CONCURRENT_NUMBER_PROPERTY)
+            .build();
+
+    private static final ImmutableSet<String> KAFKA_PROPERTIES_SET = new ImmutableSet.Builder<String>()
+            .add(KAFKA_ENDPOINT_PROPERTY)
+            .add(KAFKA_TOPIC_PROPERTY)
+            .add(KAFKA_PARTITIONS_PROPERTY)
+            .build();
+
+    private final String name;
+    private final String dbName;
+    private final String tableName;
+    private final RoutineLoadDesc routineLoadDesc;
+    private final Map<String, String> properties;
+    private final StreamDataSourceType type;
+    private final Map<String, String> typeProperties;
+    private String userName;
+
+    public CreateRoutineLoadStmt(String name, String dbName, String tableName,
+                                 RoutineLoadDesc routineLoadDesc, Map<String, String> properties,
+                                 StreamDataSourceType type, Map<String, String> typeProperties) {
+        this.name = name;
+        this.dbName = dbName;
+        this.tableName = tableName;
+        this.routineLoadDesc = routineLoadDesc;
+        this.properties = properties;
+        this.type = type;
+        this.typeProperties = typeProperties;
+    }
+
+    public String getName() {
+        return name;
+    }
+
+    public String getDbName() {
+        return dbName;
+    }
+
+    public String getTableName() {
+        return tableName;
+    }
+
+    public RoutineLoadDesc getRoutineLoadDesc() {
+        return routineLoadDesc;
+    }
+
+    public Map<String, String> getProperties() {
+        return properties;
+    }
+
+    public StreamDataSourceType getType() {
+        return type;
+    }
+
+    public Map<String, String> getTypeProperties() {
+        return typeProperties;
+    }
+
+    public String getUserName() {
+        return userName;
+    }
+
+    @Override
+    public void analyze(Analyzer analyzer) throws AnalysisException, UserException {
+        super.analyze(analyzer);
+        // check name
+        FeNameFormat.checkCommonName(NAME_TYPE, name);
+        // check dbName and tableName
+        if (Strings.isNullOrEmpty(dbName) || Strings.isNullOrEmpty(tableName)) {
+            throw new AnalysisException(""empty db name or table name in create routine load statement"");
+        }
+        // check routineLoadDesc include column separator etc.
+        routineLoadDesc.analyze();
+        // check routineLoad properties
+        checkProperties();
+        // check type properties
+        checkTypeProperties();
+        // check auth
+        userName = ConnectContext.get().getQualifiedUser();
+        if (!Catalog.getCurrentCatalog().getAuth().checkTblPriv(ConnectContext.get(), dbName, tableName,
+                                                                PrivPredicate.LOAD)) {
+            ErrorReport.reportAnalysisException(ErrorCode.ERR_TABLEACCESS_DENIED_ERROR, ""LOAD"",
+                                                userName,
+                                                ConnectContext.get().getRemoteIP(), tableName);
+        }
+        // check table belong to db, partitions belong to table
+        checkDBSemantics();
+    }
+
+    private void checkProperties() throws AnalysisException {
+        Optional<String> optional = properties.keySet().parallelStream()
+                .filter(entity -> !PROPERTIES_SET.contains(entity)).findFirst();
+        if (optional != null) {
+            throw new AnalysisException(optional.get() + "" is invalid property"");
+        }
+
+        // check desired concurrent number
+        final String desiredConcurrentNumberString = properties.get(DESIRED_CONCURRENT_NUMBER_PROPERTY);
+        if (desiredConcurrentNumberString != null) {
+            int desiredConcurrentNumber = getIntegerValueFromString(desiredConcurrentNumberString,
+                                                                    DESIRED_CONCURRENT_NUMBER_PROPERTY);
+            if (desiredConcurrentNumber <= 0) {
+                throw new AnalysisException(DESIRED_CONCURRENT_NUMBER_PROPERTY + "" must be greater then 0"");
+            }
+        }
+
+        // check max error number
+        final String maxErrorNumberString = properties.get(MAX_ERROR_NUMBER_PROPERTY);
+        if (maxErrorNumberString != null) {
+            int maxErrorNumber = getIntegerValueFromString(maxErrorNumberString, MAX_ERROR_NUMBER_PROPERTY);
+            if (maxErrorNumber < 0) {
+                throw new AnalysisException(MAX_ERROR_NUMBER_PROPERTY + "" must be greater then or equal to 0"");
+            }
+
+        }
+    }
+
+    private void checkTypeProperties() throws AnalysisException {
+        switch (type) {
+            case KAFKA:
+                Optional<String> optional = typeProperties.keySet().parallelStream()
+                        .filter(entity -> !KAFKA_PROPERTIES_SET.contains(entity)).findFirst();
+                if (optional != null) {
+                    throw new AnalysisException(optional.get() + "" is invalid "" + type.name() + "" property"");
+                }
+                // check endpoint
+                final String kafkaEndpointString = typeProperties.get(KAFKA_ENDPOINT_PROPERTY);
+                if (Strings.isNullOrEmpty(kafkaEndpointString)) {
+                    throw new AnalysisException(KAFKA_ENDPOINT_PROPERTY + "" is required property"");
+                } else {
+                    if (!Pattern.matches(ENDPOINT_REGEX, kafkaEndpointString)) {
+                        throw new AnalysisException(KAFKA_ENDPOINT_PROPERTY + "" not match pattern "" + ENDPOINT_REGEX);
+                    }
+                }
+                // check topic
+                if (Strings.isNullOrEmpty(typeProperties.get(KAFKA_TOPIC_PROPERTY))) {
+                    throw new AnalysisException(KAFKA_TOPIC_PROPERTY + "" is required property"");
+                }
+                // check partitions
+                final String kafkaPartitionsString = typeProperties.get(KAFKA_PARTITIONS_PROPERTY);
+                if (kafkaPartitionsString != null) {
+                    if (kafkaEndpointString.equals(EMPTY_STRING)) {
+                        throw new AnalysisException(KAFKA_PARTITIONS_PROPERTY + "" could not be a empty string"");
+                    }
+                    String[] kafkaPartionsStringList = kafkaPartitionsString.split("","");
+                    for (String s : kafkaPartionsStringList) {
+                        try {
+                            getIntegerValueFromString(s, KAFKA_PARTITIONS_PROPERTY);
+                        } catch (AnalysisException e) {
+                            throw new AnalysisException(KAFKA_PARTITIONS_PROPERTY
+                                                                + "" must be a number string with comma-separated"");
+                        }
+                    }
+                }
+                break;
+            default:
+                break;
+        }
+    }
+
+    private void checkDBSemantics() throws AnalysisException {
+        // check database","[{'comment': 'Do not check these semantics in analysis phase.', 'commenter': 'morningman'}]"
456,fe/src/main/java/org/apache/doris/load/routineload/KafkaRoutineLoadJob.java,"@@ -149,6 +165,35 @@ protected RoutineLoadTaskInfo reNewTask(RoutineLoadTaskInfo routineLoadTaskInfo)
         return kafkaTaskInfo;
     }
 
+    public static KafkaRoutineLoadJob fromCreateStmt(CreateRoutineLoadStmt stmt) {
+        // find dbId","[{'comment': 'check db semantics here, and you should get database lock before getting table from database', 'commenter': 'morningman'}, {'comment': ""If lock of database should be get from outside of `Database`? Although it has public method 'readlock'"", 'commenter': 'EmmyMiao87'}]"
456,fe/src/main/cup/sql_parser.cup,"@@ -1103,6 +1146,87 @@ opt_cluster ::=
     :}
     ;
 
+// Routine load statement
+create_routine_load_stmt ::=
+    KW_CREATE KW_ROUTINE KW_LOAD ident:jobName KW_ON table_name:dbTableName
+    opt_load_property_list:loadPropertyList
+    opt_properties:properties
+    KW_FROM ident:type LPAREN key_value_map:typeProperties RPAREN
+    {:
+        RESULT = new CreateRoutineLoadStmt(jobName, dbTableName, loadPropertyList, properties, type, typeProperties);
+    :}
+    ;
+
+opt_load_property_list ::=","[{'comment': 'You will add `null` to the list. \r\nI think you should return a empty list not a list containing `null`', 'commenter': 'imay'}]"
456,fe/src/main/java/org/apache/doris/analysis/LoadProperty.java,"@@ -0,0 +1,25 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.analysis;
+
+import org.apache.doris.common.AnalysisException;
+
+public interface LoadProperty {
+
+    void analyze() throws AnalysisException;","[{'comment': 'You should not add `analyze` method.\r\nBecause there is already a `analyze` method in `Expr`.\r\nI think empty interface is enough', 'commenter': 'imay'}, {'comment': 'The `analyze` interface has been added in `LoadProperty`, because all of `LoadProperty` need to be analyze. But there is already a `analyze` method in `Expr` and it could not be abstract the same method. So I have to remove it.', 'commenter': 'EmmyMiao87'}]"
456,fe/src/main/java/org/apache/doris/analysis/CreateRoutineLoadStmt.java,"@@ -0,0 +1,337 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.analysis;
+
+import com.google.common.base.Strings;
+import com.google.common.collect.ImmutableSet;
+import com.sun.istack.internal.Nullable;
+import org.apache.doris.catalog.StreamDataSourceType;
+import org.apache.doris.common.AnalysisException;
+import org.apache.doris.common.FeNameFormat;
+import org.apache.doris.common.UserException;
+
+import java.util.List;
+import java.util.Map;
+import java.util.Optional;
+import java.util.regex.Pattern;
+
+/*
+ Create routine Load statement,  continually load data from a streaming app
+
+ syntax:
+      CREATE ROUTINE LOAD name ON database.table
+      [load properties]
+      [PROPERTIES
+      (
+          desired_concurrent_number = xxx,
+          max_error_number = xxx,
+          k1 = v1,
+          ...
+          kn = vn
+      )]
+      FROM type of routine load
+      [(
+          k1 = v1,
+          ...
+          kn = vn
+      )]
+
+      load properties:
+          load property [[,] load property] ...
+
+      load property:
+          column separator | columns | partitions | where
+
+      column separator:
+          COLUMNS TERMINATED BY xxx
+      columns:
+          COLUMNS (c1, c2, c3) set (c1, c2, c3=c1+c2)
+      partitions:
+          PARTITIONS (p1, p2, p3)
+      where:
+          WHERE xxx
+
+      type of routine load:
+          KAFKA
+*/
+public class CreateRoutineLoadStmt extends DdlStmt {
+    // routine load properties
+    public static final String DESIRED_CONCURRENT_NUMBER_PROPERTY = ""desired_concurrent_number"";
+    // max error number in ten thousand records
+    public static final String MAX_ERROR_NUMBER_PROPERTY = ""max_error_number"";
+
+    // kafka type properties
+    public static final String KAFKA_ENDPOINT_PROPERTY = ""kafka_endpoint"";
+    public static final String KAFKA_TOPIC_PROPERTY = ""kafka_topic"";
+    // optional
+    public static final String KAFKA_PARTITIONS_PROPERTY = ""kafka_partitions"";
+
+    private static final String NAME_TYPE = ""ROUTINE LOAD NAME"";
+    private static final String ENDPOINT_REGEX = ""([a-z]+\\.*)+:[0-9]+"";
+    private static final String EMPTY_STRING = """";
+
+    private static final ImmutableSet<String> PROPERTIES_SET = new ImmutableSet.Builder<String>()
+            .add(DESIRED_CONCURRENT_NUMBER_PROPERTY)
+            .add(MAX_ERROR_NUMBER_PROPERTY)
+            .build();
+
+    private static final ImmutableSet<String> KAFKA_PROPERTIES_SET = new ImmutableSet.Builder<String>()
+            .add(KAFKA_ENDPOINT_PROPERTY)
+            .add(KAFKA_TOPIC_PROPERTY)
+            .add(KAFKA_PARTITIONS_PROPERTY)
+            .build();
+
+    private final String name;
+    private final TableName dbTableName;
+    private final List<ParseNode> loadPropertyList;
+    private final Map<String, String> properties;
+    private final String typeName;
+    private final Map<String, String> typeProperties;
+
+
+    // those load properties will be initialized after analyze
+    private ColumnSeparator columnSeparator;
+    private LoadColumnsInfo columnsInfo;
+    private Expr wherePredicate;
+    private PartitionNames partitionNames;
+
+
+    public CreateRoutineLoadStmt(String name, TableName dbTableName, List<ParseNode> loadPropertyList,
+                                 Map<String, String> properties,
+                                 String typeName, Map<String, String> typeProperties) {
+        this.name = name;
+        this.dbTableName = dbTableName;
+        this.loadPropertyList = loadPropertyList;
+        this.properties = properties;
+        this.typeName = typeName;
+        this.typeProperties = typeProperties;
+    }
+
+    public String getName() {
+        return name;
+    }
+
+    public TableName getDBTableName() {
+        return dbTableName;
+    }
+
+    public Map<String, String> getProperties() {
+        return properties;
+    }
+
+    public String getTypeName() {
+        return typeName;
+    }
+
+    public Map<String, String> getTypeProperties() {
+        return typeProperties;
+    }
+
+    @Nullable
+    public ColumnSeparator getColumnSeparator() {
+        return columnSeparator;
+    }
+
+    @Nullable
+    public LoadColumnsInfo getColumnsInfo() {
+        return columnsInfo;
+    }
+
+    @Nullable
+    public Expr getWherePredicate() {
+        return wherePredicate;
+    }
+
+    @Nullable
+    public PartitionNames getPartitionNames() {
+        return partitionNames;
+    }
+
+    @Override
+    public void analyze(Analyzer analyzer) throws AnalysisException, UserException {
+        super.analyze(analyzer);
+        // check name
+        FeNameFormat.checkCommonName(NAME_TYPE, name);
+        // check dbName and tableName
+        if (Strings.isNullOrEmpty(dbTableName.getDb()) || Strings.isNullOrEmpty(dbTableName.getTbl())) {
+            throw new AnalysisException(""empty db name or table name in create routine load statement"");
+        }
+        // check load properties include column separator etc.
+        checkLoadProperties(analyzer);
+        // check routine load properties include desired concurrent number etc.
+        checkRoutineLoadProperties();
+        // check type
+        try {
+            StreamDataSourceType.valueOf(typeName);
+        } catch (IllegalArgumentException e) {
+            throw new AnalysisException(""routine load job does not support this type "" + typeName);
+        }
+        // check type properties
+        checkTypeProperties();
+    }
+
+    private void checkLoadProperties(Analyzer analyzer) throws AnalysisException {
+        for (ParseNode parseNode : loadPropertyList) {
+            if (parseNode instanceof ColumnSeparator) {
+                // check column separator
+                if (columnSeparator != null) {
+                    throw new AnalysisException(""repeat setting of column separator"");
+                }
+                columnSeparator = (ColumnSeparator) parseNode;
+                columnSeparator.analyze(analyzer);
+            } else if (parseNode instanceof LoadColumnsInfo) {
+                // check columns info
+                if (columnsInfo != null) {
+                    throw new AnalysisException(""repeat setting of columns info"");
+                }
+                columnsInfo = (LoadColumnsInfo) parseNode;
+                columnsInfo.analyze(analyzer);
+            } else if (parseNode instanceof Expr) {
+                // check where expr
+                if (wherePredicate != null) {
+                    throw new AnalysisException(""repeat setting of where predicate"");
+                }
+                wherePredicate = (Expr) parseNode;
+                analyzePredicate(wherePredicate);
+            } else if (parseNode instanceof PartitionNames) {
+                // check partition names
+                if (partitionNames != null) {
+                    throw new AnalysisException(""repeat setting of partition names"");
+                }
+                partitionNames = (PartitionNames) parseNode;
+                partitionNames.analyze(analyzer);
+            }
+        }
+
+    }
+
+    private void checkRoutineLoadProperties() throws AnalysisException {
+        Optional<String> optional = properties.keySet().parallelStream()
+                .filter(entity -> !PROPERTIES_SET.contains(entity)).findFirst();
+        if (optional != null) {
+            throw new AnalysisException(optional.get() + "" is invalid property"");
+        }
+
+        // check desired concurrent number
+        final String desiredConcurrentNumberString = properties.get(DESIRED_CONCURRENT_NUMBER_PROPERTY);
+        if (desiredConcurrentNumberString != null) {
+            int desiredConcurrentNumber = getIntegerValueFromString(desiredConcurrentNumberString,
+                                                                    DESIRED_CONCURRENT_NUMBER_PROPERTY);
+            if (desiredConcurrentNumber <= 0) {
+                throw new AnalysisException(DESIRED_CONCURRENT_NUMBER_PROPERTY + "" must be greater then 0"");
+            }
+        }
+
+        // check max error number
+        final String maxErrorNumberString = properties.get(MAX_ERROR_NUMBER_PROPERTY);
+        if (maxErrorNumberString != null) {
+            int maxErrorNumber = getIntegerValueFromString(maxErrorNumberString, MAX_ERROR_NUMBER_PROPERTY);
+            if (maxErrorNumber < 0) {
+                throw new AnalysisException(MAX_ERROR_NUMBER_PROPERTY + "" must be greater then or equal to 0"");
+            }
+
+        }
+    }
+
+    private void checkTypeProperties() throws AnalysisException {
+        StreamDataSourceType type = StreamDataSourceType.valueOf(typeName);
+        switch (type) {
+            case KAFKA:
+                Optional<String> optional = typeProperties.keySet().parallelStream()
+                        .filter(entity -> !KAFKA_PROPERTIES_SET.contains(entity)).findFirst();
+                if (optional != null) {","[{'comment': '```suggestion\r\n                 if (!optional.isPresent())\r\n--\r\n\r\n\r\n) {\r\n```optio', 'commenter': 'imay'}]"
456,fe/src/main/java/org/apache/doris/analysis/CreateRoutineLoadStmt.java,"@@ -0,0 +1,337 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.analysis;
+
+import com.google.common.base.Strings;
+import com.google.common.collect.ImmutableSet;
+import com.sun.istack.internal.Nullable;
+import org.apache.doris.catalog.StreamDataSourceType;
+import org.apache.doris.common.AnalysisException;
+import org.apache.doris.common.FeNameFormat;
+import org.apache.doris.common.UserException;
+
+import java.util.List;
+import java.util.Map;
+import java.util.Optional;
+import java.util.regex.Pattern;
+
+/*
+ Create routine Load statement,  continually load data from a streaming app
+
+ syntax:
+      CREATE ROUTINE LOAD name ON database.table
+      [load properties]
+      [PROPERTIES
+      (
+          desired_concurrent_number = xxx,
+          max_error_number = xxx,
+          k1 = v1,
+          ...
+          kn = vn
+      )]
+      FROM type of routine load
+      [(
+          k1 = v1,
+          ...
+          kn = vn
+      )]
+
+      load properties:
+          load property [[,] load property] ...
+
+      load property:
+          column separator | columns | partitions | where
+
+      column separator:
+          COLUMNS TERMINATED BY xxx
+      columns:
+          COLUMNS (c1, c2, c3) set (c1, c2, c3=c1+c2)
+      partitions:
+          PARTITIONS (p1, p2, p3)
+      where:
+          WHERE xxx
+
+      type of routine load:
+          KAFKA
+*/
+public class CreateRoutineLoadStmt extends DdlStmt {
+    // routine load properties
+    public static final String DESIRED_CONCURRENT_NUMBER_PROPERTY = ""desired_concurrent_number"";
+    // max error number in ten thousand records
+    public static final String MAX_ERROR_NUMBER_PROPERTY = ""max_error_number"";
+
+    // kafka type properties
+    public static final String KAFKA_ENDPOINT_PROPERTY = ""kafka_endpoint"";
+    public static final String KAFKA_TOPIC_PROPERTY = ""kafka_topic"";
+    // optional
+    public static final String KAFKA_PARTITIONS_PROPERTY = ""kafka_partitions"";
+
+    private static final String NAME_TYPE = ""ROUTINE LOAD NAME"";
+    private static final String ENDPOINT_REGEX = ""([a-z]+\\.*)+:[0-9]+"";
+    private static final String EMPTY_STRING = """";
+
+    private static final ImmutableSet<String> PROPERTIES_SET = new ImmutableSet.Builder<String>()
+            .add(DESIRED_CONCURRENT_NUMBER_PROPERTY)
+            .add(MAX_ERROR_NUMBER_PROPERTY)
+            .build();
+
+    private static final ImmutableSet<String> KAFKA_PROPERTIES_SET = new ImmutableSet.Builder<String>()
+            .add(KAFKA_ENDPOINT_PROPERTY)
+            .add(KAFKA_TOPIC_PROPERTY)
+            .add(KAFKA_PARTITIONS_PROPERTY)
+            .build();
+
+    private final String name;
+    private final TableName dbTableName;
+    private final List<ParseNode> loadPropertyList;
+    private final Map<String, String> properties;
+    private final String typeName;
+    private final Map<String, String> typeProperties;
+
+
+    // those load properties will be initialized after analyze
+    private ColumnSeparator columnSeparator;
+    private LoadColumnsInfo columnsInfo;
+    private Expr wherePredicate;
+    private PartitionNames partitionNames;
+
+
+    public CreateRoutineLoadStmt(String name, TableName dbTableName, List<ParseNode> loadPropertyList,
+                                 Map<String, String> properties,
+                                 String typeName, Map<String, String> typeProperties) {
+        this.name = name;
+        this.dbTableName = dbTableName;
+        this.loadPropertyList = loadPropertyList;
+        this.properties = properties;
+        this.typeName = typeName;
+        this.typeProperties = typeProperties;
+    }
+
+    public String getName() {
+        return name;
+    }
+
+    public TableName getDBTableName() {
+        return dbTableName;
+    }
+
+    public Map<String, String> getProperties() {
+        return properties;
+    }
+
+    public String getTypeName() {
+        return typeName;
+    }
+
+    public Map<String, String> getTypeProperties() {
+        return typeProperties;
+    }
+
+    @Nullable
+    public ColumnSeparator getColumnSeparator() {
+        return columnSeparator;
+    }
+
+    @Nullable
+    public LoadColumnsInfo getColumnsInfo() {
+        return columnsInfo;
+    }
+
+    @Nullable
+    public Expr getWherePredicate() {
+        return wherePredicate;
+    }
+
+    @Nullable
+    public PartitionNames getPartitionNames() {
+        return partitionNames;
+    }
+
+    @Override
+    public void analyze(Analyzer analyzer) throws AnalysisException, UserException {
+        super.analyze(analyzer);
+        // check name
+        FeNameFormat.checkCommonName(NAME_TYPE, name);
+        // check dbName and tableName
+        if (Strings.isNullOrEmpty(dbTableName.getDb()) || Strings.isNullOrEmpty(dbTableName.getTbl())) {
+            throw new AnalysisException(""empty db name or table name in create routine load statement"");
+        }
+        // check load properties include column separator etc.
+        checkLoadProperties(analyzer);
+        // check routine load properties include desired concurrent number etc.
+        checkRoutineLoadProperties();
+        // check type
+        try {
+            StreamDataSourceType.valueOf(typeName);
+        } catch (IllegalArgumentException e) {
+            throw new AnalysisException(""routine load job does not support this type "" + typeName);
+        }
+        // check type properties
+        checkTypeProperties();
+    }
+
+    private void checkLoadProperties(Analyzer analyzer) throws AnalysisException {
+        for (ParseNode parseNode : loadPropertyList) {
+            if (parseNode instanceof ColumnSeparator) {
+                // check column separator
+                if (columnSeparator != null) {
+                    throw new AnalysisException(""repeat setting of column separator"");
+                }
+                columnSeparator = (ColumnSeparator) parseNode;
+                columnSeparator.analyze(analyzer);
+            } else if (parseNode instanceof LoadColumnsInfo) {
+                // check columns info
+                if (columnsInfo != null) {
+                    throw new AnalysisException(""repeat setting of columns info"");
+                }
+                columnsInfo = (LoadColumnsInfo) parseNode;
+                columnsInfo.analyze(analyzer);
+            } else if (parseNode instanceof Expr) {
+                // check where expr
+                if (wherePredicate != null) {
+                    throw new AnalysisException(""repeat setting of where predicate"");
+                }
+                wherePredicate = (Expr) parseNode;
+                analyzePredicate(wherePredicate);
+            } else if (parseNode instanceof PartitionNames) {
+                // check partition names
+                if (partitionNames != null) {
+                    throw new AnalysisException(""repeat setting of partition names"");
+                }
+                partitionNames = (PartitionNames) parseNode;
+                partitionNames.analyze(analyzer);
+            }
+        }
+
+    }
+
+    private void checkRoutineLoadProperties() throws AnalysisException {
+        Optional<String> optional = properties.keySet().parallelStream()
+                .filter(entity -> !PROPERTIES_SET.contains(entity)).findFirst();
+        if (optional != null) {
+            throw new AnalysisException(optional.get() + "" is invalid property"");
+        }
+
+        // check desired concurrent number
+        final String desiredConcurrentNumberString = properties.get(DESIRED_CONCURRENT_NUMBER_PROPERTY);
+        if (desiredConcurrentNumberString != null) {
+            int desiredConcurrentNumber = getIntegerValueFromString(desiredConcurrentNumberString,
+                                                                    DESIRED_CONCURRENT_NUMBER_PROPERTY);
+            if (desiredConcurrentNumber <= 0) {
+                throw new AnalysisException(DESIRED_CONCURRENT_NUMBER_PROPERTY + "" must be greater then 0"");
+            }
+        }
+
+        // check max error number
+        final String maxErrorNumberString = properties.get(MAX_ERROR_NUMBER_PROPERTY);
+        if (maxErrorNumberString != null) {
+            int maxErrorNumber = getIntegerValueFromString(maxErrorNumberString, MAX_ERROR_NUMBER_PROPERTY);
+            if (maxErrorNumber < 0) {
+                throw new AnalysisException(MAX_ERROR_NUMBER_PROPERTY + "" must be greater then or equal to 0"");
+            }
+
+        }
+    }
+
+    private void checkTypeProperties() throws AnalysisException {
+        StreamDataSourceType type = StreamDataSourceType.valueOf(typeName);
+        switch (type) {
+            case KAFKA:
+                Optional<String> optional = typeProperties.keySet().parallelStream()
+                        .filter(entity -> !KAFKA_PROPERTIES_SET.contains(entity)).findFirst();
+                if (optional != null) {
+                    throw new AnalysisException(optional.get() + "" is invalid "" + type.name() + "" property"");
+                }
+                // check endpoint
+                final String kafkaEndpointString = typeProperties.get(KAFKA_ENDPOINT_PROPERTY);
+                if (Strings.isNullOrEmpty(kafkaEndpointString)) {
+                    throw new AnalysisException(KAFKA_ENDPOINT_PROPERTY + "" is required property"");
+                } else {","[{'comment': 'Do you need `else`?', 'commenter': 'imay'}, {'comment': 'Oh, I saw.', 'commenter': 'EmmyMiao87'}]"
456,fe/src/main/java/org/apache/doris/load/routineload/KafkaRoutineLoadJob.java,"@@ -55,16 +58,26 @@
     // optional, user want to load partitions.
     private List<Integer> kafkaPartitions;
 
-    public KafkaRoutineLoadJob() {
+    public KafkaRoutineLoadJob(String name, String userName, long dbId, long tableId,","[{'comment': 'I think there is too many arguments.\r\ndataSourceType only can be Kafka, why you need user to pass it, and also state.\r\nyou should reduce its arguments number', 'commenter': 'imay'}]"
456,fe/src/main/java/org/apache/doris/analysis/CreateRoutineLoadStmt.java,"@@ -0,0 +1,336 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.analysis;
+
+import com.google.common.base.Strings;
+import com.google.common.collect.ImmutableSet;
+import com.sun.istack.internal.Nullable;
+import org.apache.doris.catalog.StreamDataSourceType;
+import org.apache.doris.common.AnalysisException;
+import org.apache.doris.common.FeNameFormat;
+import org.apache.doris.common.UserException;
+
+import java.util.List;
+import java.util.Map;
+import java.util.Optional;
+import java.util.regex.Pattern;
+
+/*
+ Create routine Load statement,  continually load data from a streaming app
+
+ syntax:
+      CREATE ROUTINE LOAD name ON database.table
+      [load properties]
+      [PROPERTIES
+      (
+          desired_concurrent_number = xxx,
+          max_error_number = xxx,
+          k1 = v1,
+          ...
+          kn = vn
+      )]
+      FROM type of routine load
+      [(
+          k1 = v1,
+          ...
+          kn = vn
+      )]
+
+      load properties:
+          load property [[,] load property] ...
+
+      load property:
+          column separator | columns | partitions | where
+
+      column separator:
+          COLUMNS TERMINATED BY xxx
+      columns:
+          COLUMNS (c1, c2, c3) set (c1, c2, c3=c1+c2)
+      partitions:
+          PARTITIONS (p1, p2, p3)
+      where:
+          WHERE xxx
+
+      type of routine load:
+          KAFKA
+*/
+public class CreateRoutineLoadStmt extends DdlStmt {
+    // routine load properties
+    public static final String DESIRED_CONCURRENT_NUMBER_PROPERTY = ""desired_concurrent_number"";
+    // max error number in ten thousand records
+    public static final String MAX_ERROR_NUMBER_PROPERTY = ""max_error_number"";
+
+    // kafka type properties
+    public static final String KAFKA_ENDPOINT_PROPERTY = ""kafka_endpoint"";
+    public static final String KAFKA_TOPIC_PROPERTY = ""kafka_topic"";
+    // optional
+    public static final String KAFKA_PARTITIONS_PROPERTY = ""kafka_partitions"";
+
+    private static final String NAME_TYPE = ""ROUTINE LOAD NAME"";
+    private static final String ENDPOINT_REGEX = ""([a-z]+\\.*)+:[0-9]+"";
+    private static final String EMPTY_STRING = """";
+
+    private static final ImmutableSet<String> PROPERTIES_SET = new ImmutableSet.Builder<String>()
+            .add(DESIRED_CONCURRENT_NUMBER_PROPERTY)
+            .add(MAX_ERROR_NUMBER_PROPERTY)
+            .build();
+
+    private static final ImmutableSet<String> KAFKA_PROPERTIES_SET = new ImmutableSet.Builder<String>()
+            .add(KAFKA_ENDPOINT_PROPERTY)
+            .add(KAFKA_TOPIC_PROPERTY)
+            .add(KAFKA_PARTITIONS_PROPERTY)
+            .build();
+
+    private final String name;
+    private final TableName dbTableName;
+    private final List<ParseNode> loadPropertyList;
+    private final Map<String, String> properties;
+    private final String typeName;
+    private final Map<String, String> typeProperties;
+
+
+    // those load properties will be initialized after analyze
+    private ColumnSeparator columnSeparator;
+    private LoadColumnsInfo columnsInfo;
+    private Expr wherePredicate;
+    private PartitionNames partitionNames;
+
+
+    public CreateRoutineLoadStmt(String name, TableName dbTableName, List<ParseNode> loadPropertyList,
+                                 Map<String, String> properties,
+                                 String typeName, Map<String, String> typeProperties) {
+        this.name = name;
+        this.dbTableName = dbTableName;
+        this.loadPropertyList = loadPropertyList;
+        this.properties = properties;
+        this.typeName = typeName;
+        this.typeProperties = typeProperties;
+    }
+
+    public String getName() {
+        return name;
+    }
+
+    public TableName getDBTableName() {
+        return dbTableName;
+    }
+
+    public Map<String, String> getProperties() {
+        return properties;
+    }
+
+    public String getTypeName() {
+        return typeName;
+    }
+
+    public Map<String, String> getTypeProperties() {
+        return typeProperties;
+    }
+
+    @Nullable
+    public ColumnSeparator getColumnSeparator() {
+        return columnSeparator;
+    }
+
+    @Nullable
+    public LoadColumnsInfo getColumnsInfo() {
+        return columnsInfo;
+    }
+
+    @Nullable
+    public Expr getWherePredicate() {
+        return wherePredicate;
+    }
+
+    @Nullable
+    public PartitionNames getPartitionNames() {
+        return partitionNames;
+    }
+
+    @Override
+    public void analyze(Analyzer analyzer) throws AnalysisException, UserException {
+        super.analyze(analyzer);
+        // check name
+        FeNameFormat.checkCommonName(NAME_TYPE, name);
+        // check dbName and tableName
+        if (Strings.isNullOrEmpty(dbTableName.getDb()) || Strings.isNullOrEmpty(dbTableName.getTbl())) {
+            throw new AnalysisException(""empty db name or table name in create routine load statement"");
+        }
+        // check load properties include column separator etc.
+        checkLoadProperties(analyzer);
+        // check routine load properties include desired concurrent number etc.
+        checkRoutineLoadProperties();
+        // check type
+        try {
+            StreamDataSourceType.valueOf(typeName);
+        } catch (IllegalArgumentException e) {
+            throw new AnalysisException(""routine load job does not support this type "" + typeName);
+        }
+        // check type properties
+        checkTypeProperties();
+    }
+
+    private void checkLoadProperties(Analyzer analyzer) throws AnalysisException {
+        for (ParseNode parseNode : loadPropertyList) {
+            if (parseNode instanceof ColumnSeparator) {
+                // check column separator
+                if (columnSeparator != null) {
+                    throw new AnalysisException(""repeat setting of column separator"");
+                }
+                columnSeparator = (ColumnSeparator) parseNode;
+                columnSeparator.analyze(analyzer);
+            } else if (parseNode instanceof LoadColumnsInfo) {
+                // check columns info
+                if (columnsInfo != null) {
+                    throw new AnalysisException(""repeat setting of columns info"");
+                }
+                columnsInfo = (LoadColumnsInfo) parseNode;
+                columnsInfo.analyze(analyzer);
+            } else if (parseNode instanceof Expr) {
+                // check where expr
+                if (wherePredicate != null) {
+                    throw new AnalysisException(""repeat setting of where predicate"");
+                }
+                wherePredicate = (Expr) parseNode;
+                analyzePredicate(wherePredicate);
+            } else if (parseNode instanceof PartitionNames) {
+                // check partition names
+                if (partitionNames != null) {
+                    throw new AnalysisException(""repeat setting of partition names"");
+                }
+                partitionNames = (PartitionNames) parseNode;
+                partitionNames.analyze(analyzer);
+            }
+        }
+
+    }
+
+    private void checkRoutineLoadProperties() throws AnalysisException {
+        Optional<String> optional = properties.keySet().parallelStream()
+                .filter(entity -> !PROPERTIES_SET.contains(entity)).findFirst();
+        if (!optional.isPresent()) {
+            throw new AnalysisException(optional.get() + "" is invalid property"");
+        }
+
+        // check desired concurrent number
+        final String desiredConcurrentNumberString = properties.get(DESIRED_CONCURRENT_NUMBER_PROPERTY);
+        if (desiredConcurrentNumberString != null) {
+            int desiredConcurrentNumber = getIntegerValueFromString(desiredConcurrentNumberString,
+                                                                    DESIRED_CONCURRENT_NUMBER_PROPERTY);
+            if (desiredConcurrentNumber <= 0) {
+                throw new AnalysisException(DESIRED_CONCURRENT_NUMBER_PROPERTY + "" must be greater then 0"");
+            }
+        }
+
+        // check max error number
+        final String maxErrorNumberString = properties.get(MAX_ERROR_NUMBER_PROPERTY);
+        if (maxErrorNumberString != null) {
+            int maxErrorNumber = getIntegerValueFromString(maxErrorNumberString, MAX_ERROR_NUMBER_PROPERTY);
+            if (maxErrorNumber < 0) {
+                throw new AnalysisException(MAX_ERROR_NUMBER_PROPERTY + "" must be greater then or equal to 0"");
+            }
+
+        }
+    }
+
+    private void checkTypeProperties() throws AnalysisException {
+        StreamDataSourceType type = StreamDataSourceType.valueOf(typeName);
+        switch (type) {
+            case KAFKA:
+                Optional<String> optional = typeProperties.keySet().parallelStream()
+                        .filter(entity -> !KAFKA_PROPERTIES_SET.contains(entity)).findFirst();
+                if (optional != null) {
+                    throw new AnalysisException(optional.get() + "" is invalid "" + type.name() + "" property"");
+                }
+                // check endpoint
+                final String kafkaEndpointString = typeProperties.get(KAFKA_ENDPOINT_PROPERTY);
+                if (Strings.isNullOrEmpty(kafkaEndpointString)) {
+                    throw new AnalysisException(KAFKA_ENDPOINT_PROPERTY + "" is required property"");
+                }
+                if (!Pattern.matches(ENDPOINT_REGEX, kafkaEndpointString)) {
+                    throw new AnalysisException(KAFKA_ENDPOINT_PROPERTY + "" not match pattern "" + ENDPOINT_REGEX);
+                }
+                // check topic
+                if (Strings.isNullOrEmpty(typeProperties.get(KAFKA_TOPIC_PROPERTY))) {
+                    throw new AnalysisException(KAFKA_TOPIC_PROPERTY + "" is required property"");
+                }
+                // check partitions
+                final String kafkaPartitionsString = typeProperties.get(KAFKA_PARTITIONS_PROPERTY);
+                if (kafkaPartitionsString != null) {
+                    if (kafkaEndpointString.equals(EMPTY_STRING)) {
+                        throw new AnalysisException(KAFKA_PARTITIONS_PROPERTY + "" could not be a empty string"");
+                    }
+                    String[] kafkaPartionsStringList = kafkaPartitionsString.split("","");
+                    for (String s : kafkaPartionsStringList) {
+                        try {
+                            getIntegerValueFromString(s, KAFKA_PARTITIONS_PROPERTY);
+                        } catch (AnalysisException e) {
+                            throw new AnalysisException(KAFKA_PARTITIONS_PROPERTY
+                                                                + "" must be a number string with comma-separated"");
+                        }
+                    }
+                }
+                break;
+            default:
+                break;
+        }
+    }
+
+    private void analyzePredicate(Expr predicate) throws AnalysisException {","[{'comment': ""You should use Expr's analyze(). If predicate return `BOOL` type, this is a valid predicate"", 'commenter': 'imay'}, {'comment': ""This is a Outdated code. I already changed to Expr's analyze in latest code."", 'commenter': 'EmmyMiao87'}]"
456,fe/src/main/cup/sql_parser.cup,"@@ -1103,6 +1146,87 @@ opt_cluster ::=
     :}
     ;
 
+// Routine load statement
+create_routine_load_stmt ::=
+    KW_CREATE KW_ROUTINE KW_LOAD ident:jobName KW_ON table_name:dbTableName
+    opt_load_property_list:loadPropertyList
+    opt_properties:properties
+    KW_FROM ident:type LPAREN key_value_map:typeProperties RPAREN
+    {:
+        RESULT = new CreateRoutineLoadStmt(jobName, dbTableName, loadPropertyList, properties, type, typeProperties);
+    :}
+    ;
+
+opt_load_property_list ::=
+    {:
+        RESULT = null;
+    :}
+    | opt_load_property:loadProperty
+    {:
+        RESULT = Lists.newArrayList(loadProperty);
+    :}
+    | opt_load_property_list:list COMMA opt_load_property:loadProperty
+    {:
+        list.add(loadProperty);
+        RESULT = list;
+    :}
+    ;
+
+opt_load_property ::=","[{'comment': '```suggestion\r\n load_property ::=\r\n```', 'commenter': 'imay'}]"
456,fe/src/main/java/org/apache/doris/analysis/CreateRoutineLoadStmt.java,"@@ -0,0 +1,323 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.analysis;
+
+import com.google.common.base.Strings;
+import com.google.common.collect.ImmutableSet;
+import com.sun.istack.internal.Nullable;
+import org.apache.doris.catalog.StreamDataSourceType;
+import org.apache.doris.common.AnalysisException;
+import org.apache.doris.common.FeNameFormat;
+import org.apache.doris.common.UserException;
+import org.apache.doris.load.RoutineLoadDesc;
+
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+import java.util.Optional;
+import java.util.regex.Pattern;
+
+/*
+ Create routine Load statement,  continually load data from a streaming app
+
+ syntax:
+      CREATE ROUTINE LOAD name ON database.table
+      [load properties]
+      [PROPERTIES
+      (
+          desired_concurrent_number = xxx,
+          max_error_number = xxx,
+          k1 = v1,
+          ...
+          kn = vn
+      )]
+      FROM type of routine load
+      [(
+          k1 = v1,
+          ...
+          kn = vn
+      )]
+
+      load properties:
+          load property [[,] load property] ...
+
+      load property:
+          column separator | columns | partitions | where
+
+      column separator:
+          COLUMNS TERMINATED BY xxx
+      columns:
+          COLUMNS (c1, c2, c3) set (c1, c2, c3=c1+c2)
+      partitions:
+          PARTITIONS (p1, p2, p3)
+      where:
+          WHERE xxx
+
+      type of routine load:
+          KAFKA
+*/
+public class CreateRoutineLoadStmt extends DdlStmt {
+    // routine load properties
+    public static final String DESIRED_CONCURRENT_NUMBER_PROPERTY = ""desired_concurrent_number"";
+    // max error number in ten thousand records
+    public static final String MAX_ERROR_NUMBER_PROPERTY = ""max_error_number"";
+
+    // kafka type properties
+    public static final String KAFKA_ENDPOINT_PROPERTY = ""kafka_endpoint"";
+    public static final String KAFKA_TOPIC_PROPERTY = ""kafka_topic"";
+    // optional
+    public static final String KAFKA_PARTITIONS_PROPERTY = ""kafka_partitions"";
+
+    private static final String NAME_TYPE = ""ROUTINE LOAD NAME"";
+    private static final String ENDPOINT_REGEX = ""([a-z]+\\.*)+:[0-9]+"";
+    private static final String EMPTY_STRING = """";
+
+    private static final ImmutableSet<String> PROPERTIES_SET = new ImmutableSet.Builder<String>()
+            .add(DESIRED_CONCURRENT_NUMBER_PROPERTY)
+            .add(MAX_ERROR_NUMBER_PROPERTY)
+            .build();
+
+    private static final ImmutableSet<String> KAFKA_PROPERTIES_SET = new ImmutableSet.Builder<String>()
+            .add(KAFKA_ENDPOINT_PROPERTY)
+            .add(KAFKA_TOPIC_PROPERTY)
+            .add(KAFKA_PARTITIONS_PROPERTY)
+            .build();
+
+    private final String name;
+    private final TableName dbTableName;
+    private final List<ParseNode> loadPropertyList;
+    private final Map<String, String> properties;
+    private final String typeName;
+    private final Map<String, String> typeProperties;
+
+
+    // those load properties will be initialized after analyze
+    private RoutineLoadDesc routineLoadDesc;
+    private int desiredConcurrentNum;
+    private int maxErrorNum;
+    private String kafkaEndpoint;
+    private String kafkaTopic;
+    private List<Integer> kafkaPartitions;
+
+    public CreateRoutineLoadStmt(String name, TableName dbTableName, List<ParseNode> loadPropertyList,
+                                 Map<String, String> properties,
+                                 String typeName, Map<String, String> typeProperties) {
+        this.name = name;
+        this.dbTableName = dbTableName;
+        this.loadPropertyList = loadPropertyList;
+        this.properties = properties;
+        this.typeName = typeName;
+        this.typeProperties = typeProperties;
+    }
+
+    public String getName() {
+        return name;
+    }
+
+    public TableName getDBTableName() {
+        return dbTableName;
+    }
+
+    public Map<String, String> getProperties() {
+        return properties;
+    }
+
+    public String getTypeName() {
+        return typeName;
+    }
+
+    public Map<String, String> getTypeProperties() {
+        return typeProperties;
+    }
+
+    @Nullable
+    public RoutineLoadDesc getRoutineLoadDesc() {
+        return routineLoadDesc;
+    }
+
+    public int getDesiredConcurrentNum() {
+        return desiredConcurrentNum;
+    }
+
+    public int getMaxErrorNum() {
+        return maxErrorNum;
+    }
+
+    public String getKafkaEndpoint() {
+        return kafkaEndpoint;
+    }
+
+    public String getKafkaTopic() {
+        return kafkaTopic;
+    }
+
+    public List<Integer> getKafkaPartitions() {
+        return kafkaPartitions;
+    }
+
+    @Override
+    public void analyze(Analyzer analyzer) throws AnalysisException, UserException {
+        super.analyze(analyzer);
+        // check name
+        FeNameFormat.checkCommonName(NAME_TYPE, name);
+        // check dbName and tableName
+        if (Strings.isNullOrEmpty(dbTableName.getDb()) || Strings.isNullOrEmpty(dbTableName.getTbl())) {
+            throw new AnalysisException(""empty db name or table name in create routine load statement"");","[{'comment': 'db name can be empty in dbTableName, it will be set by analyzing dbTableName.', 'commenter': 'morningman'}]"
456,fe/src/main/java/org/apache/doris/analysis/CreateRoutineLoadStmt.java,"@@ -0,0 +1,323 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.analysis;
+
+import com.google.common.base.Strings;
+import com.google.common.collect.ImmutableSet;
+import com.sun.istack.internal.Nullable;
+import org.apache.doris.catalog.StreamDataSourceType;
+import org.apache.doris.common.AnalysisException;
+import org.apache.doris.common.FeNameFormat;
+import org.apache.doris.common.UserException;
+import org.apache.doris.load.RoutineLoadDesc;
+
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+import java.util.Optional;
+import java.util.regex.Pattern;
+
+/*
+ Create routine Load statement,  continually load data from a streaming app
+
+ syntax:
+      CREATE ROUTINE LOAD name ON database.table
+      [load properties]
+      [PROPERTIES
+      (
+          desired_concurrent_number = xxx,
+          max_error_number = xxx,
+          k1 = v1,
+          ...
+          kn = vn
+      )]
+      FROM type of routine load
+      [(
+          k1 = v1,
+          ...
+          kn = vn
+      )]
+
+      load properties:
+          load property [[,] load property] ...
+
+      load property:
+          column separator | columns | partitions | where
+
+      column separator:
+          COLUMNS TERMINATED BY xxx
+      columns:
+          COLUMNS (c1, c2, c3) set (c1, c2, c3=c1+c2)
+      partitions:
+          PARTITIONS (p1, p2, p3)
+      where:
+          WHERE xxx
+
+      type of routine load:
+          KAFKA
+*/
+public class CreateRoutineLoadStmt extends DdlStmt {
+    // routine load properties
+    public static final String DESIRED_CONCURRENT_NUMBER_PROPERTY = ""desired_concurrent_number"";
+    // max error number in ten thousand records
+    public static final String MAX_ERROR_NUMBER_PROPERTY = ""max_error_number"";
+
+    // kafka type properties
+    public static final String KAFKA_ENDPOINT_PROPERTY = ""kafka_endpoint"";
+    public static final String KAFKA_TOPIC_PROPERTY = ""kafka_topic"";
+    // optional
+    public static final String KAFKA_PARTITIONS_PROPERTY = ""kafka_partitions"";
+
+    private static final String NAME_TYPE = ""ROUTINE LOAD NAME"";
+    private static final String ENDPOINT_REGEX = ""([a-z]+\\.*)+:[0-9]+"";
+    private static final String EMPTY_STRING = """";
+
+    private static final ImmutableSet<String> PROPERTIES_SET = new ImmutableSet.Builder<String>()
+            .add(DESIRED_CONCURRENT_NUMBER_PROPERTY)
+            .add(MAX_ERROR_NUMBER_PROPERTY)
+            .build();
+
+    private static final ImmutableSet<String> KAFKA_PROPERTIES_SET = new ImmutableSet.Builder<String>()
+            .add(KAFKA_ENDPOINT_PROPERTY)
+            .add(KAFKA_TOPIC_PROPERTY)
+            .add(KAFKA_PARTITIONS_PROPERTY)
+            .build();
+
+    private final String name;
+    private final TableName dbTableName;
+    private final List<ParseNode> loadPropertyList;
+    private final Map<String, String> properties;
+    private final String typeName;
+    private final Map<String, String> typeProperties;
+
+
+    // those load properties will be initialized after analyze
+    private RoutineLoadDesc routineLoadDesc;
+    private int desiredConcurrentNum;
+    private int maxErrorNum;
+    private String kafkaEndpoint;
+    private String kafkaTopic;
+    private List<Integer> kafkaPartitions;
+
+    public CreateRoutineLoadStmt(String name, TableName dbTableName, List<ParseNode> loadPropertyList,
+                                 Map<String, String> properties,
+                                 String typeName, Map<String, String> typeProperties) {
+        this.name = name;
+        this.dbTableName = dbTableName;
+        this.loadPropertyList = loadPropertyList;
+        this.properties = properties;
+        this.typeName = typeName;
+        this.typeProperties = typeProperties;
+    }
+
+    public String getName() {
+        return name;
+    }
+
+    public TableName getDBTableName() {
+        return dbTableName;
+    }
+
+    public Map<String, String> getProperties() {
+        return properties;
+    }
+
+    public String getTypeName() {
+        return typeName;
+    }
+
+    public Map<String, String> getTypeProperties() {
+        return typeProperties;
+    }
+
+    @Nullable
+    public RoutineLoadDesc getRoutineLoadDesc() {
+        return routineLoadDesc;
+    }
+
+    public int getDesiredConcurrentNum() {
+        return desiredConcurrentNum;
+    }
+
+    public int getMaxErrorNum() {
+        return maxErrorNum;
+    }
+
+    public String getKafkaEndpoint() {
+        return kafkaEndpoint;
+    }
+
+    public String getKafkaTopic() {
+        return kafkaTopic;
+    }
+
+    public List<Integer> getKafkaPartitions() {
+        return kafkaPartitions;
+    }
+
+    @Override
+    public void analyze(Analyzer analyzer) throws AnalysisException, UserException {
+        super.analyze(analyzer);
+        // check name
+        FeNameFormat.checkCommonName(NAME_TYPE, name);
+        // check dbName and tableName
+        if (Strings.isNullOrEmpty(dbTableName.getDb()) || Strings.isNullOrEmpty(dbTableName.getTbl())) {
+            throw new AnalysisException(""empty db name or table name in create routine load statement"");
+        }
+        // check load properties include column separator etc.
+        checkLoadProperties(analyzer);
+        // check routine load properties include desired concurrent number etc.
+        checkRoutineLoadProperties();
+        // check type
+        try {
+            StreamDataSourceType.valueOf(typeName);","[{'comment': 'this statement will confuse others, just do it in checkTypeProperties()', 'commenter': 'imay'}]"
456,fe/src/main/java/org/apache/doris/analysis/CreateRoutineLoadStmt.java,"@@ -0,0 +1,323 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.analysis;
+
+import com.google.common.base.Strings;
+import com.google.common.collect.ImmutableSet;
+import com.sun.istack.internal.Nullable;
+import org.apache.doris.catalog.StreamDataSourceType;
+import org.apache.doris.common.AnalysisException;
+import org.apache.doris.common.FeNameFormat;
+import org.apache.doris.common.UserException;
+import org.apache.doris.load.RoutineLoadDesc;
+
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+import java.util.Optional;
+import java.util.regex.Pattern;
+
+/*
+ Create routine Load statement,  continually load data from a streaming app
+
+ syntax:
+      CREATE ROUTINE LOAD name ON database.table
+      [load properties]
+      [PROPERTIES
+      (
+          desired_concurrent_number = xxx,
+          max_error_number = xxx,
+          k1 = v1,
+          ...
+          kn = vn
+      )]
+      FROM type of routine load
+      [(
+          k1 = v1,
+          ...
+          kn = vn
+      )]
+
+      load properties:
+          load property [[,] load property] ...
+
+      load property:
+          column separator | columns | partitions | where
+
+      column separator:
+          COLUMNS TERMINATED BY xxx
+      columns:
+          COLUMNS (c1, c2, c3) set (c1, c2, c3=c1+c2)
+      partitions:
+          PARTITIONS (p1, p2, p3)
+      where:
+          WHERE xxx
+
+      type of routine load:
+          KAFKA
+*/
+public class CreateRoutineLoadStmt extends DdlStmt {
+    // routine load properties
+    public static final String DESIRED_CONCURRENT_NUMBER_PROPERTY = ""desired_concurrent_number"";
+    // max error number in ten thousand records
+    public static final String MAX_ERROR_NUMBER_PROPERTY = ""max_error_number"";
+
+    // kafka type properties
+    public static final String KAFKA_ENDPOINT_PROPERTY = ""kafka_endpoint"";
+    public static final String KAFKA_TOPIC_PROPERTY = ""kafka_topic"";
+    // optional
+    public static final String KAFKA_PARTITIONS_PROPERTY = ""kafka_partitions"";
+
+    private static final String NAME_TYPE = ""ROUTINE LOAD NAME"";
+    private static final String ENDPOINT_REGEX = ""([a-z]+\\.*)+:[0-9]+"";
+    private static final String EMPTY_STRING = """";
+
+    private static final ImmutableSet<String> PROPERTIES_SET = new ImmutableSet.Builder<String>()
+            .add(DESIRED_CONCURRENT_NUMBER_PROPERTY)
+            .add(MAX_ERROR_NUMBER_PROPERTY)
+            .build();
+
+    private static final ImmutableSet<String> KAFKA_PROPERTIES_SET = new ImmutableSet.Builder<String>()
+            .add(KAFKA_ENDPOINT_PROPERTY)
+            .add(KAFKA_TOPIC_PROPERTY)
+            .add(KAFKA_PARTITIONS_PROPERTY)
+            .build();
+
+    private final String name;
+    private final TableName dbTableName;
+    private final List<ParseNode> loadPropertyList;
+    private final Map<String, String> properties;
+    private final String typeName;
+    private final Map<String, String> typeProperties;
+
+
+    // those load properties will be initialized after analyze
+    private RoutineLoadDesc routineLoadDesc;
+    private int desiredConcurrentNum;
+    private int maxErrorNum;
+    private String kafkaEndpoint;
+    private String kafkaTopic;
+    private List<Integer> kafkaPartitions;
+
+    public CreateRoutineLoadStmt(String name, TableName dbTableName, List<ParseNode> loadPropertyList,
+                                 Map<String, String> properties,
+                                 String typeName, Map<String, String> typeProperties) {
+        this.name = name;
+        this.dbTableName = dbTableName;
+        this.loadPropertyList = loadPropertyList;
+        this.properties = properties;
+        this.typeName = typeName;
+        this.typeProperties = typeProperties;
+    }
+
+    public String getName() {
+        return name;
+    }
+
+    public TableName getDBTableName() {
+        return dbTableName;
+    }
+
+    public Map<String, String> getProperties() {
+        return properties;
+    }
+
+    public String getTypeName() {
+        return typeName;
+    }
+
+    public Map<String, String> getTypeProperties() {
+        return typeProperties;
+    }
+
+    @Nullable
+    public RoutineLoadDesc getRoutineLoadDesc() {
+        return routineLoadDesc;
+    }
+
+    public int getDesiredConcurrentNum() {
+        return desiredConcurrentNum;
+    }
+
+    public int getMaxErrorNum() {
+        return maxErrorNum;
+    }
+
+    public String getKafkaEndpoint() {
+        return kafkaEndpoint;
+    }
+
+    public String getKafkaTopic() {
+        return kafkaTopic;
+    }
+
+    public List<Integer> getKafkaPartitions() {
+        return kafkaPartitions;
+    }
+
+    @Override
+    public void analyze(Analyzer analyzer) throws AnalysisException, UserException {
+        super.analyze(analyzer);
+        // check name
+        FeNameFormat.checkCommonName(NAME_TYPE, name);
+        // check dbName and tableName
+        if (Strings.isNullOrEmpty(dbTableName.getDb()) || Strings.isNullOrEmpty(dbTableName.getTbl())) {
+            throw new AnalysisException(""empty db name or table name in create routine load statement"");
+        }
+        // check load properties include column separator etc.
+        checkLoadProperties(analyzer);
+        // check routine load properties include desired concurrent number etc.
+        checkRoutineLoadProperties();
+        // check type
+        try {
+            StreamDataSourceType.valueOf(typeName);
+        } catch (IllegalArgumentException e) {
+            throw new AnalysisException(""routine load job does not support this type "" + typeName);
+        }
+        // check type properties
+        checkTypeProperties();
+    }
+
+    private void checkLoadProperties(Analyzer analyzer) throws AnalysisException {
+        if (loadPropertyList != null) {","[{'comment': '```suggestion\r\n         if (loadPropertyList == null) {\r\n             return;\r\n         }\r\n```', 'commenter': 'imay'}]"
456,fe/src/main/java/org/apache/doris/analysis/CreateRoutineLoadStmt.java,"@@ -0,0 +1,323 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.analysis;
+
+import com.google.common.base.Strings;
+import com.google.common.collect.ImmutableSet;
+import com.sun.istack.internal.Nullable;
+import org.apache.doris.catalog.StreamDataSourceType;
+import org.apache.doris.common.AnalysisException;
+import org.apache.doris.common.FeNameFormat;
+import org.apache.doris.common.UserException;
+import org.apache.doris.load.RoutineLoadDesc;
+
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+import java.util.Optional;
+import java.util.regex.Pattern;
+
+/*
+ Create routine Load statement,  continually load data from a streaming app
+
+ syntax:
+      CREATE ROUTINE LOAD name ON database.table
+      [load properties]
+      [PROPERTIES
+      (
+          desired_concurrent_number = xxx,
+          max_error_number = xxx,
+          k1 = v1,
+          ...
+          kn = vn
+      )]
+      FROM type of routine load
+      [(
+          k1 = v1,
+          ...
+          kn = vn
+      )]
+
+      load properties:
+          load property [[,] load property] ...
+
+      load property:
+          column separator | columns | partitions | where
+
+      column separator:
+          COLUMNS TERMINATED BY xxx
+      columns:
+          COLUMNS (c1, c2, c3) set (c1, c2, c3=c1+c2)
+      partitions:
+          PARTITIONS (p1, p2, p3)
+      where:
+          WHERE xxx
+
+      type of routine load:
+          KAFKA
+*/
+public class CreateRoutineLoadStmt extends DdlStmt {
+    // routine load properties
+    public static final String DESIRED_CONCURRENT_NUMBER_PROPERTY = ""desired_concurrent_number"";
+    // max error number in ten thousand records
+    public static final String MAX_ERROR_NUMBER_PROPERTY = ""max_error_number"";
+
+    // kafka type properties
+    public static final String KAFKA_ENDPOINT_PROPERTY = ""kafka_endpoint"";
+    public static final String KAFKA_TOPIC_PROPERTY = ""kafka_topic"";
+    // optional
+    public static final String KAFKA_PARTITIONS_PROPERTY = ""kafka_partitions"";
+
+    private static final String NAME_TYPE = ""ROUTINE LOAD NAME"";
+    private static final String ENDPOINT_REGEX = ""([a-z]+\\.*)+:[0-9]+"";
+    private static final String EMPTY_STRING = """";
+
+    private static final ImmutableSet<String> PROPERTIES_SET = new ImmutableSet.Builder<String>()
+            .add(DESIRED_CONCURRENT_NUMBER_PROPERTY)
+            .add(MAX_ERROR_NUMBER_PROPERTY)
+            .build();
+
+    private static final ImmutableSet<String> KAFKA_PROPERTIES_SET = new ImmutableSet.Builder<String>()
+            .add(KAFKA_ENDPOINT_PROPERTY)
+            .add(KAFKA_TOPIC_PROPERTY)
+            .add(KAFKA_PARTITIONS_PROPERTY)
+            .build();
+
+    private final String name;
+    private final TableName dbTableName;
+    private final List<ParseNode> loadPropertyList;
+    private final Map<String, String> properties;
+    private final String typeName;
+    private final Map<String, String> typeProperties;
+
+
+    // those load properties will be initialized after analyze
+    private RoutineLoadDesc routineLoadDesc;
+    private int desiredConcurrentNum;
+    private int maxErrorNum;
+    private String kafkaEndpoint;
+    private String kafkaTopic;
+    private List<Integer> kafkaPartitions;
+
+    public CreateRoutineLoadStmt(String name, TableName dbTableName, List<ParseNode> loadPropertyList,
+                                 Map<String, String> properties,
+                                 String typeName, Map<String, String> typeProperties) {
+        this.name = name;
+        this.dbTableName = dbTableName;
+        this.loadPropertyList = loadPropertyList;
+        this.properties = properties;
+        this.typeName = typeName;
+        this.typeProperties = typeProperties;
+    }
+
+    public String getName() {
+        return name;
+    }
+
+    public TableName getDBTableName() {
+        return dbTableName;
+    }
+
+    public Map<String, String> getProperties() {
+        return properties;
+    }
+
+    public String getTypeName() {
+        return typeName;
+    }
+
+    public Map<String, String> getTypeProperties() {
+        return typeProperties;
+    }
+
+    @Nullable
+    public RoutineLoadDesc getRoutineLoadDesc() {
+        return routineLoadDesc;
+    }
+
+    public int getDesiredConcurrentNum() {
+        return desiredConcurrentNum;
+    }
+
+    public int getMaxErrorNum() {
+        return maxErrorNum;
+    }
+
+    public String getKafkaEndpoint() {
+        return kafkaEndpoint;
+    }
+
+    public String getKafkaTopic() {
+        return kafkaTopic;
+    }
+
+    public List<Integer> getKafkaPartitions() {
+        return kafkaPartitions;
+    }
+
+    @Override
+    public void analyze(Analyzer analyzer) throws AnalysisException, UserException {
+        super.analyze(analyzer);
+        // check name
+        FeNameFormat.checkCommonName(NAME_TYPE, name);
+        // check dbName and tableName
+        if (Strings.isNullOrEmpty(dbTableName.getDb()) || Strings.isNullOrEmpty(dbTableName.getTbl())) {
+            throw new AnalysisException(""empty db name or table name in create routine load statement"");
+        }
+        // check load properties include column separator etc.
+        checkLoadProperties(analyzer);
+        // check routine load properties include desired concurrent number etc.
+        checkRoutineLoadProperties();
+        // check type
+        try {
+            StreamDataSourceType.valueOf(typeName);
+        } catch (IllegalArgumentException e) {
+            throw new AnalysisException(""routine load job does not support this type "" + typeName);
+        }
+        // check type properties
+        checkTypeProperties();
+    }
+
+    private void checkLoadProperties(Analyzer analyzer) throws AnalysisException {
+        if (loadPropertyList != null) {
+            ColumnSeparator columnSeparator = null;
+            LoadColumnsInfo columnsInfo = null;
+            Expr wherePredicate = null;
+            PartitionNames partitionNames = null;
+            for (ParseNode parseNode : loadPropertyList) {
+                if (parseNode instanceof ColumnSeparator) {
+                    // check column separator
+                    if (columnSeparator != null) {
+                        throw new AnalysisException(""repeat setting of column separator"");
+                    }
+                    columnSeparator = (ColumnSeparator) parseNode;
+                    columnSeparator.analyze(analyzer);
+                } else if (parseNode instanceof LoadColumnsInfo) {
+                    // check columns info
+                    if (columnsInfo != null) {
+                        throw new AnalysisException(""repeat setting of columns info"");
+                    }
+                    columnsInfo = (LoadColumnsInfo) parseNode;
+                    columnsInfo.analyze(analyzer);
+                } else if (parseNode instanceof Expr) {
+                    // check where expr
+                    if (wherePredicate != null) {
+                        throw new AnalysisException(""repeat setting of where predicate"");
+                    }
+                    wherePredicate = (Expr) parseNode;
+                    wherePredicate.analyze(analyzer);
+                } else if (parseNode instanceof PartitionNames) {
+                    // check partition names
+                    if (partitionNames != null) {
+                        throw new AnalysisException(""repeat setting of partition names"");
+                    }
+                    partitionNames = (PartitionNames) parseNode;
+                    partitionNames.analyze(analyzer);
+                }
+            }
+            routineLoadDesc = new RoutineLoadDesc(columnSeparator, columnsInfo, wherePredicate,
+                                                  partitionNames.getPartitionNames());
+        }
+    }
+
+    private void checkRoutineLoadProperties() throws AnalysisException {
+        Optional<String> optional = properties.keySet().parallelStream()
+                .filter(entity -> !PROPERTIES_SET.contains(entity)).findFirst();
+        if (!optional.isPresent()) {","[{'comment': 'should it be optional.isPresent(), not !optional.isPresent()?', 'commenter': 'morningman'}]"
456,fe/src/main/java/org/apache/doris/analysis/CreateRoutineLoadStmt.java,"@@ -0,0 +1,323 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.analysis;
+
+import com.google.common.base.Strings;
+import com.google.common.collect.ImmutableSet;
+import com.sun.istack.internal.Nullable;
+import org.apache.doris.catalog.StreamDataSourceType;
+import org.apache.doris.common.AnalysisException;
+import org.apache.doris.common.FeNameFormat;
+import org.apache.doris.common.UserException;
+import org.apache.doris.load.RoutineLoadDesc;
+
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+import java.util.Optional;
+import java.util.regex.Pattern;
+
+/*
+ Create routine Load statement,  continually load data from a streaming app
+
+ syntax:
+      CREATE ROUTINE LOAD name ON database.table
+      [load properties]
+      [PROPERTIES
+      (
+          desired_concurrent_number = xxx,
+          max_error_number = xxx,
+          k1 = v1,
+          ...
+          kn = vn
+      )]
+      FROM type of routine load
+      [(
+          k1 = v1,
+          ...
+          kn = vn
+      )]
+
+      load properties:
+          load property [[,] load property] ...
+
+      load property:
+          column separator | columns | partitions | where
+
+      column separator:
+          COLUMNS TERMINATED BY xxx
+      columns:
+          COLUMNS (c1, c2, c3) set (c1, c2, c3=c1+c2)
+      partitions:
+          PARTITIONS (p1, p2, p3)
+      where:
+          WHERE xxx
+
+      type of routine load:
+          KAFKA
+*/
+public class CreateRoutineLoadStmt extends DdlStmt {
+    // routine load properties
+    public static final String DESIRED_CONCURRENT_NUMBER_PROPERTY = ""desired_concurrent_number"";
+    // max error number in ten thousand records
+    public static final String MAX_ERROR_NUMBER_PROPERTY = ""max_error_number"";
+
+    // kafka type properties
+    public static final String KAFKA_ENDPOINT_PROPERTY = ""kafka_endpoint"";
+    public static final String KAFKA_TOPIC_PROPERTY = ""kafka_topic"";
+    // optional
+    public static final String KAFKA_PARTITIONS_PROPERTY = ""kafka_partitions"";
+
+    private static final String NAME_TYPE = ""ROUTINE LOAD NAME"";
+    private static final String ENDPOINT_REGEX = ""([a-z]+\\.*)+:[0-9]+"";
+    private static final String EMPTY_STRING = """";
+
+    private static final ImmutableSet<String> PROPERTIES_SET = new ImmutableSet.Builder<String>()
+            .add(DESIRED_CONCURRENT_NUMBER_PROPERTY)
+            .add(MAX_ERROR_NUMBER_PROPERTY)
+            .build();
+
+    private static final ImmutableSet<String> KAFKA_PROPERTIES_SET = new ImmutableSet.Builder<String>()
+            .add(KAFKA_ENDPOINT_PROPERTY)
+            .add(KAFKA_TOPIC_PROPERTY)
+            .add(KAFKA_PARTITIONS_PROPERTY)
+            .build();
+
+    private final String name;
+    private final TableName dbTableName;
+    private final List<ParseNode> loadPropertyList;
+    private final Map<String, String> properties;
+    private final String typeName;
+    private final Map<String, String> typeProperties;","[{'comment': '```suggestion\r\n     private final Map<String, String> customProperties;\r\n```', 'commenter': 'imay'}]"
456,fe/src/main/java/org/apache/doris/analysis/CreateRoutineLoadStmt.java,"@@ -0,0 +1,323 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.analysis;
+
+import com.google.common.base.Strings;
+import com.google.common.collect.ImmutableSet;
+import com.sun.istack.internal.Nullable;
+import org.apache.doris.catalog.StreamDataSourceType;
+import org.apache.doris.common.AnalysisException;
+import org.apache.doris.common.FeNameFormat;
+import org.apache.doris.common.UserException;
+import org.apache.doris.load.RoutineLoadDesc;
+
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+import java.util.Optional;
+import java.util.regex.Pattern;
+
+/*
+ Create routine Load statement,  continually load data from a streaming app
+
+ syntax:
+      CREATE ROUTINE LOAD name ON database.table
+      [load properties]
+      [PROPERTIES
+      (
+          desired_concurrent_number = xxx,
+          max_error_number = xxx,
+          k1 = v1,
+          ...
+          kn = vn
+      )]
+      FROM type of routine load
+      [(
+          k1 = v1,
+          ...
+          kn = vn
+      )]
+
+      load properties:
+          load property [[,] load property] ...
+
+      load property:
+          column separator | columns | partitions | where
+
+      column separator:
+          COLUMNS TERMINATED BY xxx
+      columns:
+          COLUMNS (c1, c2, c3) set (c1, c2, c3=c1+c2)
+      partitions:
+          PARTITIONS (p1, p2, p3)
+      where:
+          WHERE xxx
+
+      type of routine load:
+          KAFKA
+*/
+public class CreateRoutineLoadStmt extends DdlStmt {
+    // routine load properties
+    public static final String DESIRED_CONCURRENT_NUMBER_PROPERTY = ""desired_concurrent_number"";
+    // max error number in ten thousand records
+    public static final String MAX_ERROR_NUMBER_PROPERTY = ""max_error_number"";
+
+    // kafka type properties
+    public static final String KAFKA_ENDPOINT_PROPERTY = ""kafka_endpoint"";
+    public static final String KAFKA_TOPIC_PROPERTY = ""kafka_topic"";
+    // optional
+    public static final String KAFKA_PARTITIONS_PROPERTY = ""kafka_partitions"";
+
+    private static final String NAME_TYPE = ""ROUTINE LOAD NAME"";
+    private static final String ENDPOINT_REGEX = ""([a-z]+\\.*)+:[0-9]+"";
+    private static final String EMPTY_STRING = """";
+
+    private static final ImmutableSet<String> PROPERTIES_SET = new ImmutableSet.Builder<String>()
+            .add(DESIRED_CONCURRENT_NUMBER_PROPERTY)
+            .add(MAX_ERROR_NUMBER_PROPERTY)
+            .build();
+
+    private static final ImmutableSet<String> KAFKA_PROPERTIES_SET = new ImmutableSet.Builder<String>()
+            .add(KAFKA_ENDPOINT_PROPERTY)
+            .add(KAFKA_TOPIC_PROPERTY)
+            .add(KAFKA_PARTITIONS_PROPERTY)
+            .build();
+
+    private final String name;
+    private final TableName dbTableName;
+    private final List<ParseNode> loadPropertyList;
+    private final Map<String, String> properties;
+    private final String typeName;
+    private final Map<String, String> typeProperties;
+
+
+    // those load properties will be initialized after analyze
+    private RoutineLoadDesc routineLoadDesc;
+    private int desiredConcurrentNum;
+    private int maxErrorNum;
+    private String kafkaEndpoint;
+    private String kafkaTopic;
+    private List<Integer> kafkaPartitions;
+
+    public CreateRoutineLoadStmt(String name, TableName dbTableName, List<ParseNode> loadPropertyList,
+                                 Map<String, String> properties,
+                                 String typeName, Map<String, String> typeProperties) {
+        this.name = name;
+        this.dbTableName = dbTableName;
+        this.loadPropertyList = loadPropertyList;
+        this.properties = properties;
+        this.typeName = typeName;
+        this.typeProperties = typeProperties;
+    }
+
+    public String getName() {
+        return name;
+    }
+
+    public TableName getDBTableName() {
+        return dbTableName;
+    }
+
+    public Map<String, String> getProperties() {
+        return properties;
+    }
+
+    public String getTypeName() {
+        return typeName;
+    }
+
+    public Map<String, String> getTypeProperties() {
+        return typeProperties;
+    }
+
+    @Nullable
+    public RoutineLoadDesc getRoutineLoadDesc() {
+        return routineLoadDesc;
+    }
+
+    public int getDesiredConcurrentNum() {
+        return desiredConcurrentNum;
+    }
+
+    public int getMaxErrorNum() {
+        return maxErrorNum;
+    }
+
+    public String getKafkaEndpoint() {
+        return kafkaEndpoint;
+    }
+
+    public String getKafkaTopic() {
+        return kafkaTopic;
+    }
+
+    public List<Integer> getKafkaPartitions() {
+        return kafkaPartitions;
+    }
+
+    @Override
+    public void analyze(Analyzer analyzer) throws AnalysisException, UserException {
+        super.analyze(analyzer);
+        // check name
+        FeNameFormat.checkCommonName(NAME_TYPE, name);
+        // check dbName and tableName
+        if (Strings.isNullOrEmpty(dbTableName.getDb()) || Strings.isNullOrEmpty(dbTableName.getTbl())) {
+            throw new AnalysisException(""empty db name or table name in create routine load statement"");
+        }
+        // check load properties include column separator etc.
+        checkLoadProperties(analyzer);
+        // check routine load properties include desired concurrent number etc.
+        checkRoutineLoadProperties();
+        // check type
+        try {
+            StreamDataSourceType.valueOf(typeName);
+        } catch (IllegalArgumentException e) {
+            throw new AnalysisException(""routine load job does not support this type "" + typeName);
+        }
+        // check type properties
+        checkTypeProperties();
+    }
+
+    private void checkLoadProperties(Analyzer analyzer) throws AnalysisException {
+        if (loadPropertyList != null) {
+            ColumnSeparator columnSeparator = null;
+            LoadColumnsInfo columnsInfo = null;
+            Expr wherePredicate = null;
+            PartitionNames partitionNames = null;
+            for (ParseNode parseNode : loadPropertyList) {
+                if (parseNode instanceof ColumnSeparator) {
+                    // check column separator
+                    if (columnSeparator != null) {
+                        throw new AnalysisException(""repeat setting of column separator"");
+                    }
+                    columnSeparator = (ColumnSeparator) parseNode;
+                    columnSeparator.analyze(analyzer);
+                } else if (parseNode instanceof LoadColumnsInfo) {
+                    // check columns info
+                    if (columnsInfo != null) {
+                        throw new AnalysisException(""repeat setting of columns info"");
+                    }
+                    columnsInfo = (LoadColumnsInfo) parseNode;
+                    columnsInfo.analyze(analyzer);
+                } else if (parseNode instanceof Expr) {
+                    // check where expr
+                    if (wherePredicate != null) {
+                        throw new AnalysisException(""repeat setting of where predicate"");
+                    }
+                    wherePredicate = (Expr) parseNode;
+                    wherePredicate.analyze(analyzer);
+                } else if (parseNode instanceof PartitionNames) {
+                    // check partition names
+                    if (partitionNames != null) {
+                        throw new AnalysisException(""repeat setting of partition names"");
+                    }
+                    partitionNames = (PartitionNames) parseNode;
+                    partitionNames.analyze(analyzer);
+                }
+            }
+            routineLoadDesc = new RoutineLoadDesc(columnSeparator, columnsInfo, wherePredicate,
+                                                  partitionNames.getPartitionNames());
+        }
+    }
+
+    private void checkRoutineLoadProperties() throws AnalysisException {
+        Optional<String> optional = properties.keySet().parallelStream()
+                .filter(entity -> !PROPERTIES_SET.contains(entity)).findFirst();
+        if (!optional.isPresent()) {
+            throw new AnalysisException(optional.get() + "" is invalid property"");
+        }
+
+        // check desired concurrent number
+        final String desiredConcurrentNumberString = properties.get(DESIRED_CONCURRENT_NUMBER_PROPERTY);
+        if (desiredConcurrentNumberString != null) {
+            desiredConcurrentNum = getIntegerValueFromString(desiredConcurrentNumberString,
+                                                                    DESIRED_CONCURRENT_NUMBER_PROPERTY);
+            if (desiredConcurrentNum <= 0) {
+                throw new AnalysisException(DESIRED_CONCURRENT_NUMBER_PROPERTY + "" must be greater then 0"");
+            }
+        }
+
+        // check max error number
+        final String maxErrorNumberString = properties.get(MAX_ERROR_NUMBER_PROPERTY);
+        if (maxErrorNumberString != null) {
+            maxErrorNum = getIntegerValueFromString(maxErrorNumberString, MAX_ERROR_NUMBER_PROPERTY);
+            if (maxErrorNum < 0) {
+                throw new AnalysisException(MAX_ERROR_NUMBER_PROPERTY + "" must be greater then or equal to 0"");
+            }
+
+        }
+    }
+
+    private void checkTypeProperties() throws AnalysisException {
+        StreamDataSourceType type = StreamDataSourceType.valueOf(typeName);
+        switch (type) {
+            case KAFKA:
+                Optional<String> optional = typeProperties.keySet().parallelStream()
+                        .filter(entity -> !KAFKA_PROPERTIES_SET.contains(entity)).findFirst();
+                if (!optional.isPresent()) {","[{'comment': 'remove the !', 'commenter': 'morningman'}]"
456,fe/src/main/java/org/apache/doris/analysis/CreateRoutineLoadStmt.java,"@@ -0,0 +1,323 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.analysis;
+
+import com.google.common.base.Strings;
+import com.google.common.collect.ImmutableSet;
+import com.sun.istack.internal.Nullable;
+import org.apache.doris.catalog.StreamDataSourceType;
+import org.apache.doris.common.AnalysisException;
+import org.apache.doris.common.FeNameFormat;
+import org.apache.doris.common.UserException;
+import org.apache.doris.load.RoutineLoadDesc;
+
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+import java.util.Optional;
+import java.util.regex.Pattern;
+
+/*
+ Create routine Load statement,  continually load data from a streaming app
+
+ syntax:
+      CREATE ROUTINE LOAD name ON database.table
+      [load properties]
+      [PROPERTIES
+      (
+          desired_concurrent_number = xxx,
+          max_error_number = xxx,
+          k1 = v1,
+          ...
+          kn = vn
+      )]
+      FROM type of routine load
+      [(
+          k1 = v1,
+          ...
+          kn = vn
+      )]
+
+      load properties:
+          load property [[,] load property] ...
+
+      load property:
+          column separator | columns | partitions | where
+
+      column separator:
+          COLUMNS TERMINATED BY xxx
+      columns:
+          COLUMNS (c1, c2, c3) set (c1, c2, c3=c1+c2)
+      partitions:
+          PARTITIONS (p1, p2, p3)
+      where:
+          WHERE xxx
+
+      type of routine load:
+          KAFKA
+*/
+public class CreateRoutineLoadStmt extends DdlStmt {
+    // routine load properties
+    public static final String DESIRED_CONCURRENT_NUMBER_PROPERTY = ""desired_concurrent_number"";
+    // max error number in ten thousand records
+    public static final String MAX_ERROR_NUMBER_PROPERTY = ""max_error_number"";
+
+    // kafka type properties
+    public static final String KAFKA_ENDPOINT_PROPERTY = ""kafka_endpoint"";
+    public static final String KAFKA_TOPIC_PROPERTY = ""kafka_topic"";
+    // optional
+    public static final String KAFKA_PARTITIONS_PROPERTY = ""kafka_partitions"";
+
+    private static final String NAME_TYPE = ""ROUTINE LOAD NAME"";
+    private static final String ENDPOINT_REGEX = ""([a-z]+\\.*)+:[0-9]+"";
+    private static final String EMPTY_STRING = """";
+
+    private static final ImmutableSet<String> PROPERTIES_SET = new ImmutableSet.Builder<String>()
+            .add(DESIRED_CONCURRENT_NUMBER_PROPERTY)
+            .add(MAX_ERROR_NUMBER_PROPERTY)
+            .build();
+
+    private static final ImmutableSet<String> KAFKA_PROPERTIES_SET = new ImmutableSet.Builder<String>()
+            .add(KAFKA_ENDPOINT_PROPERTY)
+            .add(KAFKA_TOPIC_PROPERTY)
+            .add(KAFKA_PARTITIONS_PROPERTY)
+            .build();
+
+    private final String name;
+    private final TableName dbTableName;
+    private final List<ParseNode> loadPropertyList;
+    private final Map<String, String> properties;
+    private final String typeName;
+    private final Map<String, String> typeProperties;
+
+
+    // those load properties will be initialized after analyze
+    private RoutineLoadDesc routineLoadDesc;
+    private int desiredConcurrentNum;
+    private int maxErrorNum;
+    private String kafkaEndpoint;
+    private String kafkaTopic;
+    private List<Integer> kafkaPartitions;
+
+    public CreateRoutineLoadStmt(String name, TableName dbTableName, List<ParseNode> loadPropertyList,
+                                 Map<String, String> properties,
+                                 String typeName, Map<String, String> typeProperties) {
+        this.name = name;
+        this.dbTableName = dbTableName;
+        this.loadPropertyList = loadPropertyList;
+        this.properties = properties;
+        this.typeName = typeName;
+        this.typeProperties = typeProperties;
+    }
+
+    public String getName() {
+        return name;
+    }
+
+    public TableName getDBTableName() {
+        return dbTableName;
+    }
+
+    public Map<String, String> getProperties() {
+        return properties;
+    }
+
+    public String getTypeName() {
+        return typeName;
+    }
+
+    public Map<String, String> getTypeProperties() {
+        return typeProperties;
+    }
+
+    @Nullable
+    public RoutineLoadDesc getRoutineLoadDesc() {
+        return routineLoadDesc;
+    }
+
+    public int getDesiredConcurrentNum() {
+        return desiredConcurrentNum;
+    }
+
+    public int getMaxErrorNum() {
+        return maxErrorNum;
+    }
+
+    public String getKafkaEndpoint() {
+        return kafkaEndpoint;
+    }
+
+    public String getKafkaTopic() {
+        return kafkaTopic;
+    }
+
+    public List<Integer> getKafkaPartitions() {
+        return kafkaPartitions;
+    }
+
+    @Override
+    public void analyze(Analyzer analyzer) throws AnalysisException, UserException {
+        super.analyze(analyzer);
+        // check name
+        FeNameFormat.checkCommonName(NAME_TYPE, name);
+        // check dbName and tableName
+        if (Strings.isNullOrEmpty(dbTableName.getDb()) || Strings.isNullOrEmpty(dbTableName.getTbl())) {
+            throw new AnalysisException(""empty db name or table name in create routine load statement"");
+        }
+        // check load properties include column separator etc.
+        checkLoadProperties(analyzer);
+        // check routine load properties include desired concurrent number etc.
+        checkRoutineLoadProperties();
+        // check type
+        try {
+            StreamDataSourceType.valueOf(typeName);
+        } catch (IllegalArgumentException e) {
+            throw new AnalysisException(""routine load job does not support this type "" + typeName);
+        }
+        // check type properties
+        checkTypeProperties();
+    }
+
+    private void checkLoadProperties(Analyzer analyzer) throws AnalysisException {
+        if (loadPropertyList != null) {
+            ColumnSeparator columnSeparator = null;
+            LoadColumnsInfo columnsInfo = null;
+            Expr wherePredicate = null;
+            PartitionNames partitionNames = null;
+            for (ParseNode parseNode : loadPropertyList) {
+                if (parseNode instanceof ColumnSeparator) {
+                    // check column separator
+                    if (columnSeparator != null) {
+                        throw new AnalysisException(""repeat setting of column separator"");
+                    }
+                    columnSeparator = (ColumnSeparator) parseNode;
+                    columnSeparator.analyze(analyzer);
+                } else if (parseNode instanceof LoadColumnsInfo) {
+                    // check columns info
+                    if (columnsInfo != null) {
+                        throw new AnalysisException(""repeat setting of columns info"");
+                    }
+                    columnsInfo = (LoadColumnsInfo) parseNode;
+                    columnsInfo.analyze(analyzer);
+                } else if (parseNode instanceof Expr) {
+                    // check where expr
+                    if (wherePredicate != null) {
+                        throw new AnalysisException(""repeat setting of where predicate"");
+                    }
+                    wherePredicate = (Expr) parseNode;
+                    wherePredicate.analyze(analyzer);
+                } else if (parseNode instanceof PartitionNames) {
+                    // check partition names
+                    if (partitionNames != null) {
+                        throw new AnalysisException(""repeat setting of partition names"");
+                    }
+                    partitionNames = (PartitionNames) parseNode;
+                    partitionNames.analyze(analyzer);
+                }
+            }
+            routineLoadDesc = new RoutineLoadDesc(columnSeparator, columnsInfo, wherePredicate,
+                                                  partitionNames.getPartitionNames());
+        }
+    }
+
+    private void checkRoutineLoadProperties() throws AnalysisException {
+        Optional<String> optional = properties.keySet().parallelStream()
+                .filter(entity -> !PROPERTIES_SET.contains(entity)).findFirst();
+        if (!optional.isPresent()) {
+            throw new AnalysisException(optional.get() + "" is invalid property"");
+        }
+
+        // check desired concurrent number
+        final String desiredConcurrentNumberString = properties.get(DESIRED_CONCURRENT_NUMBER_PROPERTY);
+        if (desiredConcurrentNumberString != null) {
+            desiredConcurrentNum = getIntegerValueFromString(desiredConcurrentNumberString,
+                                                                    DESIRED_CONCURRENT_NUMBER_PROPERTY);
+            if (desiredConcurrentNum <= 0) {
+                throw new AnalysisException(DESIRED_CONCURRENT_NUMBER_PROPERTY + "" must be greater then 0"");
+            }
+        }
+
+        // check max error number
+        final String maxErrorNumberString = properties.get(MAX_ERROR_NUMBER_PROPERTY);
+        if (maxErrorNumberString != null) {
+            maxErrorNum = getIntegerValueFromString(maxErrorNumberString, MAX_ERROR_NUMBER_PROPERTY);
+            if (maxErrorNum < 0) {
+                throw new AnalysisException(MAX_ERROR_NUMBER_PROPERTY + "" must be greater then or equal to 0"");
+            }
+
+        }
+    }
+
+    private void checkTypeProperties() throws AnalysisException {
+        StreamDataSourceType type = StreamDataSourceType.valueOf(typeName);
+        switch (type) {
+            case KAFKA:","[{'comment': 'you can write a function named `checkKafkaCustomProperties()`', 'commenter': 'imay'}]"
456,fe/src/main/java/org/apache/doris/analysis/PartitionNames.java,"@@ -0,0 +1,58 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.analysis;
+
+import com.google.common.base.Strings;
+import org.apache.doris.common.AnalysisException;
+
+import java.util.List;
+
+public class PartitionNames implements ParseNode {
+
+    private final List<String> partitionNames;
+
+    public PartitionNames(List<String> partitionNames) {
+        this.partitionNames = partitionNames;
+    }
+
+    public List<String> getPartitionNames() {
+        return partitionNames;
+    }
+
+    @Override
+    public void analyze(Analyzer analyzer) throws AnalysisException {
+        // check if partiton name is not empty string
+        if (partitionNames.parallelStream().anyMatch(entity -> Strings.isNullOrEmpty(entity))) {
+            throw new AnalysisException(""there are empty partition name"");
+        }
+    }
+
+    @Override
+    public String toSql() {
+        StringBuilder sb = new StringBuilder();
+        sb.append(""PARTITIONS ("");","[{'comment': 'There is a simple way: Joiner.on("","").join(partitionNames); LiaoJieYiXia', 'commenter': 'morningman'}]"
456,fe/src/main/java/org/apache/doris/catalog/StreamDataSourceType.java,"@@ -0,0 +1,23 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+
+package org.apache.doris.catalog;
+
+public enum StreamDataSourceType {","[{'comment': 'move this class to load/rountineload？', 'commenter': 'morningman'}]"
456,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoadJob.java,"@@ -55,155 +69,224 @@
  * The routine load job support different streaming medium such as KAFKA
  */
 public abstract class RoutineLoadJob implements Writable, TxnStateChangeListener {
-    
+
     private static final Logger LOG = LogManager.getLogger(RoutineLoadJob.class);
-    
+
     private static final int DEFAULT_TASK_TIMEOUT_SECONDS = 10;
     private static final int BASE_OF_ERROR_RATE = 10000;
-    
+    private static final String STAR_STRING = ""*"";
+
     public enum JobState {","[{'comment': ""you'd better to add state transition diagram in the comments"", 'commenter': 'imay'}]"
456,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoadManager.java,"@@ -132,8 +149,140 @@ public void addNumOfConcurrentTasksByBeId(long beId) {
         }
     }
 
-    public void addRoutineLoadJob(RoutineLoadJob routineLoadJob) {
-        idToRoutineLoadJob.put(routineLoadJob.getId(), routineLoadJob);
+    public void addRoutineLoadJob(CreateRoutineLoadStmt createRoutineLoadStmt)
+            throws AnalysisException, DdlException, LoadException {
+        // check load auth","[{'comment': ""I think it's better to move auth check to statement analysis. Then this class needs not to know about auth things"", 'commenter': 'imay'}, {'comment': 'Analysis is used to check `Semantics`， but auth is not belong to `Semantics`. So maybe put auth check in here is better?', 'commenter': 'EmmyMiao87'}]"
456,fe/src/main/java/org/apache/doris/analysis/PauseRoutineLoadStmt.java,"@@ -0,0 +1,50 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.analysis;
+
+import com.google.common.base.Strings;
+import org.apache.doris.common.AnalysisException;
+import org.apache.doris.common.UserException;
+
+/*
+  Pause routine load by name
+
+  syntax:
+      PAUSE ROUTINE LOAD name
+ */
+public class PauseRoutineLoadStmt extends DdlStmt {","[{'comment': 'I think you can merget the PAUSE/RESUME/STOP Routine load stmt together, to make less code', 'commenter': 'morningman'}]"
467,be/src/olap/tablet.cpp,"@@ -76,7 +76,7 @@ OLAPStatus Tablet::init_once() {
     for (int ser = 0; ser < _tablet_meta.rowset_size(); ++ser) {
         const RowsetMeta* rs_meta = _tablet_meta.get_rs_meta(ser);
         Version version = rs_meta->version();
-        Rowset* rowset = new Rowset(rs_meta);
+        std::shared_ptr rowset(new Rowset(rs_meta));
         _version_rowset_map[version] = rowset;","[{'comment': 'std::shared_ptr<Rowset> ?\r\n\r\nIt is better to define RowsetSharedPtr instead of using std::shared_ptr<Rowset>', 'commenter': 'yiguolei'}, {'comment': 'std::shared_ptr< Rowset > ?', 'commenter': 'kangpinghuang'}, {'comment': 'OK', 'commenter': 'chaoyli'}]"
467,be/src/olap/tablet.cpp,"@@ -228,12 +228,13 @@ void Tablet::calc_missed_versions(int64_t spec_version,
     }
 }
 
-NewStatus Tablet::modify_rowsets(vector<Rowset*>& to_add, vector<Rowset*>& to_delete) {
+NewStatus Tablet::modify_rowsets(vector<std::shared_ptr<Rowset>>& to_add,
+                                 vector<std::shared_ptr<RowsetL>& to_delete) {","[{'comment': 'vector<std::shared_ptr<RowsetL>& to_delete -> vector<std::shared_ptr<Rowset>>& to_delete', 'commenter': 'kangpinghuang'}, {'comment': 'OK', 'commenter': 'chaoyli'}]"
467,be/src/olap/tablet_meta.cpp,"@@ -47,12 +47,12 @@ NewStatus TabletMeta::deserialize_unlock(const string& meta_binary) {
     _shard_id = _tablet_meta_pb.shard_id();
     RETURN_NOT_OK(_schema.deserialize_from_pb(_tablet_meta_pb.schema()));
     for (auto& it : _tablet_meta_pb.rs_metas()) {
-        RowsetMeta rs_meta;
-        rs_meta.deserialize_from_pb(it);
+        std::shared_ptr<RowsetMeta> rs_meta(new RowsetMeta());
+        rs_meta.deserialize_from_pb(rs_meta);","[{'comment': 'use rs_meta.init()', 'commenter': 'kangpinghuang'}, {'comment': 'I will change it to init_from_pb', 'commenter': 'chaoyli'}]"
467,be/src/olap/tablet_meta.cpp,"@@ -172,8 +172,8 @@ const RowsetMeta* TabletMeta::get_inc_rowset(const Version& version) const;
     return rs_meta;
 }
 
-NewStatus TabletMeta::modify_rowsets(const vector<RowsetMeta>& to_add,
-                                     const vector<RowsetMeta>& to_delete) {
+NewStatus TabletMeta::modify_rowsets(const vector<std::shared_ptr<RowsetMeta>>& to_add,
+                                     const vector<std::shared_ptr<RowsetMeta>>& to_delete) {
     std::lock_guard<std::mutex> lock(_mutex);
     for (auto& to_delete_rs : to_delete) {
         if (ContainsKey(to_delete, _rs_metas)) {","[{'comment': 'to_delete -> to_delete_rs?', 'commenter': 'kangpinghuang'}, {'comment': 'ContainsKey(to_delete, _rs_metas) -> ContainsKey(to_delete_rs, _rs_metas)?\r\nto_delete_rs vs to_delete, I think these two are not good name.', 'commenter': 'kangpinghuang'}]"
467,be/src/olap/tablet_meta.cpp,"@@ -172,8 +172,8 @@ const RowsetMeta* TabletMeta::get_inc_rowset(const Version& version) const;
     return rs_meta;
 }
 
-NewStatus TabletMeta::modify_rowsets(const vector<RowsetMeta>& to_add,
-                                     const vector<RowsetMeta>& to_delete) {
+NewStatus TabletMeta::modify_rowsets(const vector<std::shared_ptr<RowsetMeta>>& to_add,
+                                     const vector<std::shared_ptr<RowsetMeta>>& to_delete) {","[{'comment': 'I think rowsets_to_add and rowsets_to_delete  is better name\r\n', 'commenter': 'kangpinghuang'}, {'comment': 'Name is long may be not better, because line may be long. ', 'commenter': 'chaoyli'}]"
467,gensrc/proto/olap_file.proto,"@@ -183,6 +183,47 @@ message OLAPHeaderMessage {
     repeated RowsetMetaPB incremental_rowsets = 25;
 }
 
+enum AlterTabletStatePB {
+    ALTERING = 0;
+    FINISHED = 1;
+    FAILED = 2;
+}
+
+enum AlterTabletTypePB {
+    SCHEMA_CHANGE = 0;
+    ROLLUP = 1;
+}
+
+message AlterTabletPB {
+    required int64 related_tablet_id = 1;
+    optional int64 related_schema_hash = 2;
+    repeated RowsetMetaPB rowsets_to_alter = 3;
+    optional AlterTabletStatePB alter_state = 4;
+    optional AlterTabletTypePB alter_type = 5; 
+}
+
+enum TabletStatePB {
+    NOTREADY = 0; // under alter table, rollup, clone
+    RUNNING = 1;
+    TOMBSTONED = 2;
+    STOPPED = 3;
+    SHUTDOWN = 4;
+}
+
+message TabletMetaPB {
+    optional int64 table_id = 1;
+    optional int64 partition_id = 2;
+    optional int64 tablet_id = 3;
+    optional int64 schema_hash = 4;
+    optional int32 shard_id = 5;
+
+    optional SchemaPB schema = 6;
+    repeated RowsetMetaPB rs_meta = 7;","[{'comment': 'rs_meta -> rowset_metas', 'commenter': 'kangpinghuang'}, {'comment': 'Sometime I may have function named as delete_inc_rs_meta_by_version this function, so abbreviation is better to read.\r\nOf course, I should use the plural form.', 'commenter': 'chaoyli'}, {'comment': 'plural form?', 'commenter': 'kangpinghuang'}]"
467,gensrc/proto/olap_file.proto,"@@ -183,6 +183,47 @@ message OLAPHeaderMessage {
     repeated RowsetMetaPB incremental_rowsets = 25;
 }
 
+enum AlterTabletStatePB {
+    ALTERING = 0;
+    FINISHED = 1;
+    FAILED = 2;
+}
+
+enum AlterTabletTypePB {
+    SCHEMA_CHANGE = 0;
+    ROLLUP = 1;
+}
+
+message AlterTabletPB {
+    required int64 related_tablet_id = 1;
+    optional int64 related_schema_hash = 2;
+    repeated RowsetMetaPB rowsets_to_alter = 3;
+    optional AlterTabletStatePB alter_state = 4;
+    optional AlterTabletTypePB alter_type = 5; 
+}
+
+enum TabletStatePB {
+    NOTREADY = 0; // under alter table, rollup, clone
+    RUNNING = 1;
+    TOMBSTONED = 2;
+    STOPPED = 3;
+    SHUTDOWN = 4;
+}
+
+message TabletMetaPB {
+    optional int64 table_id = 1;
+    optional int64 partition_id = 2;
+    optional int64 tablet_id = 3;
+    optional int64 schema_hash = 4;
+    optional int32 shard_id = 5;
+
+    optional SchemaPB schema = 6;
+    repeated RowsetMetaPB rs_meta = 7;
+    repeated RowsetMetaPB inc_rs_meta = 8;","[{'comment': 'inc_rs_meta -> inc_rowset_metas', 'commenter': 'kangpinghuang'}, {'comment': 'Sometime I may have function named as delete_inc_rs_meta_by_version this function, so abbreviation is better to read.\r\nOf course, I should use the plural form.', 'commenter': 'chaoyli'}, {'comment': 'plural form?', 'commenter': 'kangpinghuang'}]"
467,be/src/olap/tablet_meta.h,"@@ -90,12 +90,13 @@ class TabletMeta {
 
     Newstatus add_inc_rs_meta(const RowsetMeta& rs_meta);
     NewStatus delete_inc_rs_meta_by_version(const Version& version);
-    const RowsetMeta* get_inc_rs_meta(const Version& version) const;
+    const std::shared_ptr<RowsetMeta> get_inc_rs_meta(const Version& version) const;
 
     const std::vector<RowsetMeta>& all_inc_rs_metas() const;
     const std::vector<RowsetMeta>& all_rs_metas() const;
 
-    NewStatus modify_rowsets(const vector<RowsetMeta>& to_add, const vector<RowsetMeta>& to_delete);
+    NewStatus modify_rowsets(const vector<std::shared_ptr<RowsetMeta>>& to_add,
+                             const vector<std::shared_ptr<RowsetMeta>>& to_delete);","[{'comment': 'pls modify to_add and to_delete', 'commenter': 'kangpinghuang'}, {'comment': 'It may be better to use less word.', 'commenter': 'chaoyli'}]"
468,fe/src/main/java/org/apache/doris/analysis/CreateFunctionStmt.java,"@@ -17,38 +17,129 @@
 
 package org.apache.doris.analysis;
 
+import com.google.common.collect.ImmutableSortedMap;
+import org.apache.commons.codec.binary.Hex;
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.catalog.Function;
+import org.apache.doris.catalog.ScalarFunction;
 import org.apache.doris.common.AnalysisException;
+import org.apache.doris.common.ErrorCode;
+import org.apache.doris.common.ErrorReport;
 import org.apache.doris.common.UserException;
+import org.apache.doris.mysql.privilege.PrivPredicate;
+import org.apache.doris.qe.ConnectContext;
 
-import java.util.List;
+import java.io.IOException;
+import java.io.InputStream;
+import java.net.URL;
+import java.net.URLConnection;
+import java.security.MessageDigest;
+import java.security.NoSuchAlgorithmException;
 import java.util.Map;
 
-/**
- * Created by zhaochun on 14-7-30.
- */
-public class CreateFunctionStmt extends StatementBase {
-    private final FunctionName                            functionName;
-    private final List<org.apache.doris.catalog.ColumnType> argumentType;
-    private final org.apache.doris.catalog.ColumnType       returnType;
-    private final String                                  soFilePath;
-    private final Map<String, String>                     properties;
-    private final boolean                                 isAggregate;
-
-    public CreateFunctionStmt(FunctionName functionName,
-      List<org.apache.doris.catalog.ColumnType> argumentType,
-      org.apache.doris.catalog.ColumnType returnType, String soFilePath,
-      Map<String, String> properties, boolean isAggregate) {
+// create a user define function
+public class CreateFunctionStmt extends DdlStmt {
+
+    private final FunctionName functionName;
+    private final boolean isAggregate;
+    private final FunctionArgsDef argsDef;
+    private final TypeDef returnType;
+    private final Map<String, String> properties;
+
+    // needed item set after analyzed
+    private String objectFile;
+    private Function function;
+    private String checksum;
+
+    public CreateFunctionStmt(boolean isAggregate, FunctionName functionName, FunctionArgsDef argsDef,
+                              TypeDef returnType, Map<String, String> properties) {
         this.functionName = functionName;
-        this.argumentType = argumentType;
-        this.returnType = returnType;
-        this.soFilePath = soFilePath;
-        this.properties = properties;
         this.isAggregate = isAggregate;
+        this.argsDef = argsDef;
+        this.returnType = returnType;
+        if (properties == null) {
+            this.properties = ImmutableSortedMap.of();
+        } else {
+            this.properties = ImmutableSortedMap.copyOf(properties, String.CASE_INSENSITIVE_ORDER);
+        }
     }
 
+    public FunctionName getFunctionName() { return functionName; }
+    public Function getFunction() { return function; }
+
     @Override
-    public void analyze(Analyzer analyzer) throws AnalysisException, UserException {
+    public void analyze(Analyzer analyzer) throws UserException {
         super.analyze(analyzer);
+
+        analyzeCommon(analyzer);
+        // check
+        if (isAggregate) {
+            analyzeUda();
+        } else {
+            analyzeUdf();
+        }
+    }
+
+    private void analyzeCommon(Analyzer analyzer) throws AnalysisException {
+        // check function name
+        functionName.analyze(analyzer);
+
+        // check operation privilege
+        if (!Catalog.getCurrentCatalog().getAuth().checkDbPriv(
+                ConnectContext.get(), functionName.getDb(), PrivPredicate.ADMIN)) {","[{'comment': 'ADMIN_PRIV is not for Database level, is for global.', 'commenter': 'morningman'}, {'comment': 'Use checkGlobalPriv()', 'commenter': 'morningman'}, {'comment': '> Use checkGlobalPriv()\r\n\r\nOK', 'commenter': 'imay'}]"
468,fe/src/main/java/org/apache/doris/analysis/CreateFunctionStmt.java,"@@ -17,38 +17,129 @@
 
 package org.apache.doris.analysis;
 
+import com.google.common.collect.ImmutableSortedMap;
+import org.apache.commons.codec.binary.Hex;
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.catalog.Function;
+import org.apache.doris.catalog.ScalarFunction;
 import org.apache.doris.common.AnalysisException;
+import org.apache.doris.common.ErrorCode;
+import org.apache.doris.common.ErrorReport;
 import org.apache.doris.common.UserException;
+import org.apache.doris.mysql.privilege.PrivPredicate;
+import org.apache.doris.qe.ConnectContext;
 
-import java.util.List;
+import java.io.IOException;
+import java.io.InputStream;
+import java.net.URL;
+import java.net.URLConnection;
+import java.security.MessageDigest;
+import java.security.NoSuchAlgorithmException;
 import java.util.Map;
 
-/**
- * Created by zhaochun on 14-7-30.
- */
-public class CreateFunctionStmt extends StatementBase {
-    private final FunctionName                            functionName;
-    private final List<org.apache.doris.catalog.ColumnType> argumentType;
-    private final org.apache.doris.catalog.ColumnType       returnType;
-    private final String                                  soFilePath;
-    private final Map<String, String>                     properties;
-    private final boolean                                 isAggregate;
-
-    public CreateFunctionStmt(FunctionName functionName,
-      List<org.apache.doris.catalog.ColumnType> argumentType,
-      org.apache.doris.catalog.ColumnType returnType, String soFilePath,
-      Map<String, String> properties, boolean isAggregate) {
+// create a user define function
+public class CreateFunctionStmt extends DdlStmt {
+
+    private final FunctionName functionName;
+    private final boolean isAggregate;
+    private final FunctionArgsDef argsDef;
+    private final TypeDef returnType;
+    private final Map<String, String> properties;
+
+    // needed item set after analyzed
+    private String objectFile;
+    private Function function;
+    private String checksum;
+
+    public CreateFunctionStmt(boolean isAggregate, FunctionName functionName, FunctionArgsDef argsDef,
+                              TypeDef returnType, Map<String, String> properties) {
         this.functionName = functionName;
-        this.argumentType = argumentType;
-        this.returnType = returnType;
-        this.soFilePath = soFilePath;
-        this.properties = properties;
         this.isAggregate = isAggregate;
+        this.argsDef = argsDef;
+        this.returnType = returnType;
+        if (properties == null) {
+            this.properties = ImmutableSortedMap.of();
+        } else {
+            this.properties = ImmutableSortedMap.copyOf(properties, String.CASE_INSENSITIVE_ORDER);
+        }
     }
 
+    public FunctionName getFunctionName() { return functionName; }
+    public Function getFunction() { return function; }
+
     @Override
-    public void analyze(Analyzer analyzer) throws AnalysisException, UserException {
+    public void analyze(Analyzer analyzer) throws UserException {
         super.analyze(analyzer);
+
+        analyzeCommon(analyzer);
+        // check
+        if (isAggregate) {
+            analyzeUda();
+        } else {
+            analyzeUdf();
+        }
+    }
+
+    private void analyzeCommon(Analyzer analyzer) throws AnalysisException {
+        // check function name
+        functionName.analyze(analyzer);
+
+        // check operation privilege
+        if (!Catalog.getCurrentCatalog().getAuth().checkDbPriv(
+                ConnectContext.get(), functionName.getDb(), PrivPredicate.ADMIN)) {
+            ErrorReport.reportAnalysisException(ErrorCode.ERR_SPECIFIC_ACCESS_DENIED_ERROR, ""ADMIN"");
+        }
+        // check argument
+        argsDef.analyze(analyzer);
+
+        returnType.analyze(analyzer);
+
+        String OBJECT_FILE_KEY = ""object_file"";
+        objectFile = properties.get(OBJECT_FILE_KEY);
+        if (objectFile == null) {
+            throw new AnalysisException(""No 'object_file' in properties"");
+        }
+        try {
+            computeObjectChecksum();
+        } catch (IOException | NoSuchAlgorithmException e) {
+            throw new AnalysisException(""cannot to compute object's checksum"");
+        }
+    }
+
+    private void computeObjectChecksum() throws IOException, NoSuchAlgorithmException {
+        URL url = new URL(objectFile);
+        URLConnection urlConnection = url.openConnection();
+        InputStream inputStream = urlConnection.getInputStream();
+
+        MessageDigest digest = MessageDigest.getInstance(""MD5"");
+        byte[] buf = new byte[4096];
+        int bytesRead = 0;
+        do {
+            bytesRead = inputStream.read(buf);
+            if (bytesRead < 0) {
+                break;
+            }
+            digest.update(buf, 0, bytesRead);
+        } while (true);
+
+        checksum = Hex.encodeHexString(digest.digest());
+    }
+
+    private void analyzeUda() throws AnalysisException {
+        throw new AnalysisException(""Not support aggregate function now."");
+    }
+
+    private void analyzeUdf() throws AnalysisException {
+        final String SYMBOL_KEY = ""symbol"";
+        String symbol = properties.get(SYMBOL_KEY);
+        if (symbol == null) {","[{'comment': 'null or empty?', 'commenter': 'morningman'}, {'comment': ""OK，I'll change it"", 'commenter': 'imay'}]"
468,fe/src/main/java/org/apache/doris/analysis/CreateFunctionStmt.java,"@@ -17,38 +17,129 @@
 
 package org.apache.doris.analysis;
 
+import com.google.common.collect.ImmutableSortedMap;
+import org.apache.commons.codec.binary.Hex;
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.catalog.Function;
+import org.apache.doris.catalog.ScalarFunction;
 import org.apache.doris.common.AnalysisException;
+import org.apache.doris.common.ErrorCode;
+import org.apache.doris.common.ErrorReport;
 import org.apache.doris.common.UserException;
+import org.apache.doris.mysql.privilege.PrivPredicate;
+import org.apache.doris.qe.ConnectContext;
 
-import java.util.List;
+import java.io.IOException;
+import java.io.InputStream;
+import java.net.URL;
+import java.net.URLConnection;
+import java.security.MessageDigest;
+import java.security.NoSuchAlgorithmException;
 import java.util.Map;
 
-/**
- * Created by zhaochun on 14-7-30.
- */
-public class CreateFunctionStmt extends StatementBase {
-    private final FunctionName                            functionName;
-    private final List<org.apache.doris.catalog.ColumnType> argumentType;
-    private final org.apache.doris.catalog.ColumnType       returnType;
-    private final String                                  soFilePath;
-    private final Map<String, String>                     properties;
-    private final boolean                                 isAggregate;
-
-    public CreateFunctionStmt(FunctionName functionName,
-      List<org.apache.doris.catalog.ColumnType> argumentType,
-      org.apache.doris.catalog.ColumnType returnType, String soFilePath,
-      Map<String, String> properties, boolean isAggregate) {
+// create a user define function
+public class CreateFunctionStmt extends DdlStmt {
+
+    private final FunctionName functionName;
+    private final boolean isAggregate;
+    private final FunctionArgsDef argsDef;
+    private final TypeDef returnType;
+    private final Map<String, String> properties;
+
+    // needed item set after analyzed
+    private String objectFile;
+    private Function function;
+    private String checksum;
+
+    public CreateFunctionStmt(boolean isAggregate, FunctionName functionName, FunctionArgsDef argsDef,","[{'comment': 'What if user create a function same as a builtin function? Do we allow it?', 'commenter': 'morningman'}, {'comment': 'And is function name case insensitive?', 'commenter': 'morningman'}, {'comment': '> What if user create a function same as a builtin function? Do we allow it?\r\n\r\nNow, I allow user create a same name UDF, but can not be used. I will add check later.\r\n\r\n\r\n> And is function name case insensitive?\r\n\r\nYes, case insensitive', 'commenter': 'imay'}]"
468,fe/src/main/java/org/apache/doris/catalog/Function.java,"@@ -439,4 +500,160 @@ public static String getUdfType(PrimitiveType t) {
                 return """";
         }
     }
+
+    public static Function getFunction(List<Function> fns, Function desc, CompareMode mode) {
+        if (fns == null) {
+            return null;
+        }
+        // First check for identical
+        for (Function f : fns) {
+            if (f.compare(desc, Function.CompareMode.IS_IDENTICAL)) {
+                return f;
+            }
+        }
+        if (mode == Function.CompareMode.IS_IDENTICAL) {
+            return null;
+        }
+
+        // Next check for indistinguishable
+        for (Function f : fns) {
+            if (f.compare(desc, Function.CompareMode.IS_INDISTINGUISHABLE)) {
+                return f;
+            }
+        }
+        if (mode == Function.CompareMode.IS_INDISTINGUISHABLE) {
+            return null;
+        }
+
+        // Next check for strict supertypes
+        for (Function f : fns) {
+            if (f.compare(desc, Function.CompareMode.IS_SUPERTYPE_OF)) {
+                return f;
+            }
+        }
+        if (mode == Function.CompareMode.IS_SUPERTYPE_OF) {
+            return null;
+        }
+        // Finally check for non-strict supertypes
+        for (Function f : fns) {
+            if (f.compare(desc, Function.CompareMode.IS_NONSTRICT_SUPERTYPE_OF)) {
+                return f;
+            }
+        }
+        return null;
+    }
+
+    enum FunctionType {
+        ORIGIN(0),
+        SCALAR(1),
+        AGGREGATE(2);
+
+        private int code;
+
+        FunctionType(int code) {
+            this.code = code;
+        }
+        public int getCode() {
+            return code;
+        }
+
+        public static FunctionType fromCode(int code) {
+            switch (code) {
+                case 0:
+                    return ORIGIN;
+                case 1:
+                    return SCALAR;
+                case 2:
+                    return AGGREGATE;
+            }
+            return null;
+        }
+
+        public void write(DataOutput output) throws IOException {
+            output.writeInt(code);
+        }
+        public static FunctionType read(DataInput input) throws IOException {
+            return fromCode(input.readInt());
+        }
+    };
+
+    protected void writeOptionString(DataOutput output, String value) throws IOException {","[{'comment': 'move writeOptionString() and readOptionStringOrNull() to common.util?', 'commenter': 'morningman'}, {'comment': 'OK', 'commenter': 'imay'}]"
477,be/src/olap/olap_engine.cpp,"@@ -1725,14 +1725,18 @@ void OLAPEngine::get_tablet_stat(TTabletStatResult& result) {
     int64_t current_time = UnixMillis();
     
     _tablet_map_lock.rdlock();","[{'comment': '_tablet_map_lock should not be locked here.\r\nIt should be locked inside _build_tablet_stat()', 'commenter': 'morningman'}, {'comment': 'OK.', 'commenter': 'chaoyli'}]"
490,fe/src/main/java/org/apache/doris/load/BrokerLoadErrorHub.java,"@@ -0,0 +1,111 @@
+package org.apache.doris.load;","[{'comment': 'license header', 'commenter': 'imay'}, {'comment': 'done', 'commenter': 'morningman'}]"
490,fs_brokers/apache_hdfs_broker/src/main/java/org/apache/doris/common/WildcardURI.java,"@@ -0,0 +1,63 @@
+package org.apache.doris.common;","[{'comment': 'license header', 'commenter': 'imay'}, {'comment': 'done', 'commenter': 'morningman'}]"
490,gensrc/thrift/FrontendService.thrift,"@@ -492,6 +492,7 @@ struct TStreamLoadPutRequest {
     14: optional string columnSeparator
 
     15: optional string partitions
+    16: optional string enable_hub","[{'comment': ""You should use `boolean` instead of `string` to a switch filed\r\n\r\nHowever, I think you'd better not to add this filed."", 'commenter': 'imay'}, {'comment': 'removed', 'commenter': 'morningman'}]"
490,be/src/util/broker_load_error_hub.h,"@@ -0,0 +1,88 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#ifndef DORIS_BE_SRC_UTIL_BROKER_LOAD_ERROR_HUB_H
+#define DORIS_BE_SRC_UTIL_BROKER_LOAD_ERROR_HUB_H
+
+#include <sstream>
+#include <string>
+#include <mutex>
+#include <queue>
+
+
+#include ""util/load_error_hub.h""
+#include ""gen_cpp/PaloInternalService_types.h""
+
+namespace doris {
+
+class BrokerWriter;
+class ExecEnv;
+
+// Broker load error hub will write load error info to the sepcified
+// remote storage via broker.
+// We should only open this error hub if there are error line.
+// Because open the writer via broker may cost several seconds.
+class BrokerLoadErrorHub : public LoadErrorHub {
+public:
+    struct BrokerInfo {
+        std::vector<TNetworkAddress> addrs;
+        // path should be like:
+        // xxx://yyy/file_name
+        std::string path;
+        std::map<std::string, std::string> props;
+
+        BrokerInfo(const TBrokerErrorHubInfo& t_info,
+                   const std::string& error_log_file_name) :
+                props(t_info.prop) {
+            path = t_info.path + ""/"" + error_log_file_name;
+            addrs.push_back(t_info.broker_addr);
+        }
+    };
+
+    BrokerLoadErrorHub(ExecEnv* env, const TBrokerErrorHubInfo& info,
+            const std::string& error_log_file_name);
+
+    virtual ~BrokerLoadErrorHub();
+
+    virtual Status prepare();
+
+    virtual Status export_error(const ErrorMsg& error_msg);
+
+    virtual Status close();
+
+    virtual std::string debug_string() const;
+
+private:
+    Status write_to_broker();
+
+    ExecEnv* _env;
+    BrokerInfo _info;
+
+    // the number in a write batch.
+    static const int32_t EXPORTER_THRESHOLD = 20;
+
+    BrokerWriter* _broker_writer;","[{'comment': '```suggestion\r\n     BrokerWriter* _broker_writer = nullptr;\r\n```', 'commenter': 'imay'}, {'comment': 'done', 'commenter': 'morningman'}]"
490,be/src/util/broker_load_error_hub.cpp,"@@ -0,0 +1,102 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""util/broker_load_error_hub.h""
+
+#include ""exec/broker_writer.h""
+#include ""util/defer_op.h""
+
+namespace doris {
+
+BrokerLoadErrorHub::BrokerLoadErrorHub(
+        ExecEnv* env,
+        const TBrokerErrorHubInfo& info,
+        const std::string& error_log_file_name) :
+        _env(env),
+        _info(info, error_log_file_name) {
+}
+
+BrokerLoadErrorHub::~BrokerLoadErrorHub() {
+}
+
+Status BrokerLoadErrorHub::prepare() {
+    _broker_writer = new BrokerWriter(_env, _info.addrs,
+            _info.props, _info.path, 0); 
+
+    RETURN_IF_ERROR(_broker_writer->open());
+
+    _is_valid = true;
+    return Status::OK;
+}
+
+Status BrokerLoadErrorHub::export_error(const ErrorMsg& error_msg) {
+    std::lock_guard<std::mutex> lock(_mtx);
+    ++_total_error_num;
+
+    if (!_is_valid) {
+        return Status::OK;
+    }
+
+    _error_msgs.push(error_msg);
+    if (_error_msgs.size() >= EXPORTER_THRESHOLD) {
+        RETURN_IF_ERROR(write_to_broker());
+    }
+
+    return Status::OK;
+}
+
+Status BrokerLoadErrorHub::close() {
+    std::lock_guard<std::mutex> lock(_mtx);
+
+    if (!_is_valid) {
+        return Status::OK;
+    }
+
+    if (!_error_msgs.empty()) {
+        RETURN_IF_ERROR(write_to_broker());
+    }
+
+    _broker_writer->close();
+    delete _broker_writer;","[{'comment': 'put this to destructor ', 'commenter': 'imay'}, {'comment': 'moved', 'commenter': 'morningman'}]"
490,be/src/http/download_action.cpp,"@@ -95,13 +95,6 @@ void DownloadAction::handle_error_log(
         const std::string& file_param) {
     const std::string absolute_path = _error_log_root_dir + ""/"" + file_param;
 
-    Status status = check_log_path_is_allowed(absolute_path);","[{'comment': 'why you delete this', 'commenter': 'imay'}, {'comment': 'add it back', 'commenter': 'morningman'}]"
492,be/src/olap/rowset/rowset_meta.h,"@@ -42,6 +42,11 @@ class RowsetMeta {
         return _deserialize_from_pb(pb_rowset_meta);
     }
 
+    virtual bool init_from_pb(const RowsetMetaPB rowset_meta_pb) {","[{'comment': 'const RowsetMetaPB&', 'commenter': 'chaoyli'}, {'comment': 'ok, I will fix it.', 'commenter': 'kangpinghuang'}]"
492,be/src/olap/rowset/segment_reader.cpp,"@@ -39,7 +39,6 @@ SegmentReader::SegmentReader(
         const std::vector<uint32_t>& used_columns,
         const std::set<uint32_t>& load_bf_columns,
         const Conditions* conditions,
-        const std::vector<ColumnPredicate*>* col_predicates,","[{'comment': 'You should make you initialization order in accordance with variable declaration order in header file.', 'commenter': 'chaoyli'}, {'comment': 'yes, this modification is for this purpose to fix compile warning problems', 'commenter': 'kangpinghuang'}]"
497,fe/src/main/java/org/apache/doris/rpc/PTiggerReportProfileRequest.java,"@@ -0,0 +1,23 @@
+package org.apache.doris.rpc;
+
+import com.baidu.bjf.remoting.protobuf.FieldType;
+import com.baidu.bjf.remoting.protobuf.annotation.Protobuf;
+import com.baidu.bjf.remoting.protobuf.annotation.ProtobufClass;
+import com.google.common.collect.Lists;
+
+import java.util.List;
+
+@ProtobufClass
+public class PTiggerReportProfileRequest extends AttachmentRequest {","[{'comment': 'Trigger, not Tigger', 'commenter': 'morningman'}, {'comment': 'ok, i will fix it.', 'commenter': 'chenhao7253886'}]"
497,fe/src/main/java/org/apache/doris/rpc/PUniqueId.java,"@@ -33,4 +34,31 @@ public PUniqueId(TUniqueId tid) {
     public long hi;
     @Protobuf(order = 2, required = true)
     public long lo;
+
+    @Override
+    public int hashCode() {
+        return ((int)hi + (int)(hi >> 32) + (int)lo + (int)(lo >> 32));
+    }
+
+    @Override
+    public String toString() {
+        return new StringBuilder().append(hi).append("":"").append(lo).toString();","[{'comment': 'using DebugUtil.printId().', 'commenter': 'morningman'}, {'comment': 'ok', 'commenter': 'chenhao7253886'}]"
497,fe/src/main/java/org/apache/doris/rpc/PTiggerReportProfileRequest.java,"@@ -0,0 +1,23 @@
+package org.apache.doris.rpc;","[{'comment': 'No licence', 'commenter': 'morningman'}, {'comment': 'ok, i will add.', 'commenter': 'chenhao7253886'}]"
497,fe/src/main/java/org/apache/doris/rpc/PTiggerReportProfileResult.java,"@@ -0,0 +1,13 @@
+package org.apache.doris.rpc;","[{'comment': 'No licence', 'commenter': 'morningman'}, {'comment': 'ok, i will add.', 'commenter': 'chenhao7253886'}]"
497,fe/src/main/java/org/apache/doris/common/util/RuntimeProfile.java,"@@ -39,26 +39,32 @@
 import java.util.Map;
 import java.util.Set;
 
+/**
+ * It is accessed by two kinds of threads, one is thread which create
+ * RuntimeProfile, namely query thread, the other is thread which call
+ * {@link org.apache.doris.common.proc.CurrentQueryInfoProvider}.","[{'comment': ""It is accessed by two kinds of thread. One is to create\r\nthis RuntimeProfile, named 'query thread', the other is to call this profile."", 'commenter': 'morningman'}, {'comment': ""What I want to emphasize is that the thread which call CurrentQueryInfoProvider in 'show proc' . The last words seemed redundant. i will fix the grammar problem."", 'commenter': 'chenhao7253886'}]"
497,fe/src/main/java/org/apache/doris/common/util/RuntimeProfile.java,"@@ -67,19 +73,23 @@ public RuntimeProfile() {
         this.counterMap.put(""TotalTime"", counterTotalTime);
     }
     
-    public Counter getCounterTotalTime() {
+    public synchronized Counter getCounterTotalTime() {
         return counterTotalTime;
     }
 
-    public Map<String, Counter> getCounterMap() {
+    public synchronized Map<String, Counter> getCounterMap() {","[{'comment': ""this 'synchronized' is protect nothing. counterMap is returned, and the caller will get it and do anything without lock protection."", 'commenter': 'morningman'}, {'comment': ""counterMap is ConcurrentMap， and it's value Counter, which members are all atomic. So it does't cause problems."", 'commenter': 'chenhao7253886'}]"
497,fe/src/main/java/org/apache/doris/common/proc/CurrentQueryInfoProvider.java,"@@ -0,0 +1,518 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.common.proc;
+
+import com.google.common.base.Preconditions;
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.common.AnalysisException;
+import org.apache.doris.common.Pair;
+import org.apache.doris.common.util.Counter;
+import org.apache.doris.common.util.DebugUtil;
+import org.apache.doris.common.util.RuntimeProfile;
+import org.apache.doris.qe.QueryStatisticsItem;
+import org.apache.doris.rpc.*;
+import org.apache.doris.system.Backend;
+import org.apache.doris.thrift.TNetworkAddress;
+import org.apache.doris.thrift.TStatusCode;
+import org.apache.doris.thrift.TUniqueId;
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.Collection;
+import java.util.List;
+import java.util.Map;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.Future;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
+
+/**
+ * Provide running query's PlanNode informations, includeing execution State
+ * , IO consumption and CPU consumption.
+ */
+public class CurrentQueryInfoProvider {
+    private static final Logger LOG = LogManager.getLogger(CurrentQueryInfoProvider.class);
+
+    public CurrentQueryInfoProvider() {
+    }
+
+    /**
+     * Firstly send request to trigger report profile for specified query and wait a while,
+     * Secondly get Counters from Coordinator's RuntimeProfile and return query's consumption.
+     *
+     * @param item
+     * @return
+     * @throws AnalysisException
+     */
+    public Consumption getQueryConsumption(QueryStatisticsItem item) throws AnalysisException {
+        triggerReportAndWait(item, getWaitTime(1), false);
+        return new Consumption(item.getQueryProfile());
+    }
+
+    /**
+     * Same as getQueryConsumption, but this will cause BE to report all queries profile.
+     *
+     * @param items
+     * @return
+     * @throws AnalysisException
+     */
+    public Map<String, Consumption> getQueriesConsumptions(Collection<QueryStatisticsItem> items)
+            throws AnalysisException {
+        triggerReportAndWait(items, getWaitTime(items.size()), true);
+        final Map<String, Consumption> queryConsumpations = Maps.newHashMap();
+        for (QueryStatisticsItem item : items) {
+            queryConsumpations.put(item.getQueryId(), new Consumption(item.getQueryProfile()));
+        }
+        return queryConsumpations;
+    }
+
+    /**
+     * Return query's instances consumption.
+     *
+     * @param item
+     * @return
+     * @throws AnalysisException
+     */
+    public Collection<InstanceConsumption> getQueryInstancesConsumptions(QueryStatisticsItem item) throws AnalysisException {
+        triggerReportAndWait(item, getWaitTime(1), false);
+        final Map<String, RuntimeProfile> instanceProfiles = collectInstanceProfile(item.getQueryProfile());
+        final List<InstanceConsumption> instanceConsumptions = Lists.newArrayList();
+        for (QueryStatisticsItem.FragmentInstanceInfo instanceInfo : item.getFragmentInstanceInfos()) {
+            final RuntimeProfile instanceProfile = instanceProfiles.get(DebugUtil.printId(instanceInfo.getInstanceId()));
+            Preconditions.checkNotNull(instanceProfile);
+            final InstanceConsumption consumption =
+                    new InstanceConsumption(
+                            instanceInfo.getFragmentId(),
+                            instanceInfo.getInstanceId(),
+                            instanceInfo.getAddress(),
+                            instanceProfile);
+            instanceConsumptions.add(consumption);
+        }
+        return instanceConsumptions;
+    }
+
+    /**
+     * Profile trees is query profile -> fragment profile -> instance profile ....
+     * @param queryProfile
+     * @return instanceProfiles
+     */
+    private Map<String, RuntimeProfile> collectInstanceProfile(RuntimeProfile queryProfile) {
+        final Map<String, RuntimeProfile> instanceProfiles = Maps.newHashMap();
+        for (RuntimeProfile fragmentProfile : queryProfile.getChildMap().values()) {
+            for (Map.Entry<String, RuntimeProfile> entry: fragmentProfile.getChildMap().entrySet()) {
+                Preconditions.checkState(instanceProfiles.put(parseInstanceId(entry.getKey()), entry.getValue()) == null);
+            }
+        }
+        return instanceProfiles;
+    }
+
+    /**
+     * Instance profile key is ""Instance ${instance_id} (host=$host $port)""
+     * @param str
+     * @return
+     */
+    private String parseInstanceId(String str) {
+        final String[] elements = str.split("" "");
+        if (elements.length == 4) {
+            return  elements[1];
+        } else {
+            Preconditions.checkState(false);
+            return """";
+        }
+    }
+
+    private long getWaitTime(int numOfQuery) {
+        final int oneQueryWaitTime = 200;","[{'comment': ""You'd better add a suffix like 'ms' or 's' to let others know about the time unit."", 'commenter': 'morningman'}, {'comment': 'ok', 'commenter': 'chenhao7253886'}]"
497,fe/src/main/java/org/apache/doris/common/proc/CurrentQueryInfoProvider.java,"@@ -0,0 +1,518 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.common.proc;
+
+import com.google.common.base.Preconditions;
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.common.AnalysisException;
+import org.apache.doris.common.Pair;
+import org.apache.doris.common.util.Counter;
+import org.apache.doris.common.util.DebugUtil;
+import org.apache.doris.common.util.RuntimeProfile;
+import org.apache.doris.qe.QueryStatisticsItem;
+import org.apache.doris.rpc.*;
+import org.apache.doris.system.Backend;
+import org.apache.doris.thrift.TNetworkAddress;
+import org.apache.doris.thrift.TStatusCode;
+import org.apache.doris.thrift.TUniqueId;
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.Collection;
+import java.util.List;
+import java.util.Map;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.Future;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
+
+/**
+ * Provide running query's PlanNode informations, includeing execution State
+ * , IO consumption and CPU consumption.
+ */
+public class CurrentQueryInfoProvider {
+    private static final Logger LOG = LogManager.getLogger(CurrentQueryInfoProvider.class);
+
+    public CurrentQueryInfoProvider() {
+    }
+
+    /**
+     * Firstly send request to trigger report profile for specified query and wait a while,
+     * Secondly get Counters from Coordinator's RuntimeProfile and return query's consumption.
+     *
+     * @param item
+     * @return
+     * @throws AnalysisException
+     */
+    public Consumption getQueryConsumption(QueryStatisticsItem item) throws AnalysisException {
+        triggerReportAndWait(item, getWaitTime(1), false);
+        return new Consumption(item.getQueryProfile());
+    }
+
+    /**
+     * Same as getQueryConsumption, but this will cause BE to report all queries profile.
+     *
+     * @param items
+     * @return
+     * @throws AnalysisException
+     */
+    public Map<String, Consumption> getQueriesConsumptions(Collection<QueryStatisticsItem> items)
+            throws AnalysisException {
+        triggerReportAndWait(items, getWaitTime(items.size()), true);
+        final Map<String, Consumption> queryConsumpations = Maps.newHashMap();
+        for (QueryStatisticsItem item : items) {
+            queryConsumpations.put(item.getQueryId(), new Consumption(item.getQueryProfile()));
+        }
+        return queryConsumpations;
+    }
+
+    /**
+     * Return query's instances consumption.
+     *
+     * @param item
+     * @return
+     * @throws AnalysisException
+     */
+    public Collection<InstanceConsumption> getQueryInstancesConsumptions(QueryStatisticsItem item) throws AnalysisException {
+        triggerReportAndWait(item, getWaitTime(1), false);
+        final Map<String, RuntimeProfile> instanceProfiles = collectInstanceProfile(item.getQueryProfile());
+        final List<InstanceConsumption> instanceConsumptions = Lists.newArrayList();
+        for (QueryStatisticsItem.FragmentInstanceInfo instanceInfo : item.getFragmentInstanceInfos()) {
+            final RuntimeProfile instanceProfile = instanceProfiles.get(DebugUtil.printId(instanceInfo.getInstanceId()));
+            Preconditions.checkNotNull(instanceProfile);
+            final InstanceConsumption consumption =
+                    new InstanceConsumption(
+                            instanceInfo.getFragmentId(),
+                            instanceInfo.getInstanceId(),
+                            instanceInfo.getAddress(),
+                            instanceProfile);
+            instanceConsumptions.add(consumption);
+        }
+        return instanceConsumptions;
+    }
+
+    /**
+     * Profile trees is query profile -> fragment profile -> instance profile ....
+     * @param queryProfile
+     * @return instanceProfiles
+     */
+    private Map<String, RuntimeProfile> collectInstanceProfile(RuntimeProfile queryProfile) {
+        final Map<String, RuntimeProfile> instanceProfiles = Maps.newHashMap();
+        for (RuntimeProfile fragmentProfile : queryProfile.getChildMap().values()) {
+            for (Map.Entry<String, RuntimeProfile> entry: fragmentProfile.getChildMap().entrySet()) {
+                Preconditions.checkState(instanceProfiles.put(parseInstanceId(entry.getKey()), entry.getValue()) == null);
+            }
+        }
+        return instanceProfiles;
+    }
+
+    /**
+     * Instance profile key is ""Instance ${instance_id} (host=$host $port)""
+     * @param str
+     * @return
+     */
+    private String parseInstanceId(String str) {
+        final String[] elements = str.split("" "");
+        if (elements.length == 4) {
+            return  elements[1];","[{'comment': 'return  elements[1];  double space', 'commenter': 'morningman'}, {'comment': 'ok, i will fix it.', 'commenter': 'chenhao7253886'}]"
497,fe/src/main/java/org/apache/doris/common/proc/CurrentQueryInfoProvider.java,"@@ -0,0 +1,518 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.common.proc;
+
+import com.google.common.base.Preconditions;
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.common.AnalysisException;
+import org.apache.doris.common.Pair;
+import org.apache.doris.common.util.Counter;
+import org.apache.doris.common.util.DebugUtil;
+import org.apache.doris.common.util.RuntimeProfile;
+import org.apache.doris.qe.QueryStatisticsItem;
+import org.apache.doris.rpc.*;
+import org.apache.doris.system.Backend;
+import org.apache.doris.thrift.TNetworkAddress;
+import org.apache.doris.thrift.TStatusCode;
+import org.apache.doris.thrift.TUniqueId;
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.Collection;
+import java.util.List;
+import java.util.Map;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.Future;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
+
+/**
+ * Provide running query's PlanNode informations, includeing execution State
+ * , IO consumption and CPU consumption.
+ */
+public class CurrentQueryInfoProvider {
+    private static final Logger LOG = LogManager.getLogger(CurrentQueryInfoProvider.class);
+
+    public CurrentQueryInfoProvider() {
+    }
+
+    /**
+     * Firstly send request to trigger report profile for specified query and wait a while,
+     * Secondly get Counters from Coordinator's RuntimeProfile and return query's consumption.
+     *
+     * @param item
+     * @return
+     * @throws AnalysisException
+     */
+    public Consumption getQueryConsumption(QueryStatisticsItem item) throws AnalysisException {
+        triggerReportAndWait(item, getWaitTime(1), false);
+        return new Consumption(item.getQueryProfile());
+    }
+
+    /**
+     * Same as getQueryConsumption, but this will cause BE to report all queries profile.
+     *
+     * @param items
+     * @return
+     * @throws AnalysisException
+     */
+    public Map<String, Consumption> getQueriesConsumptions(Collection<QueryStatisticsItem> items)
+            throws AnalysisException {
+        triggerReportAndWait(items, getWaitTime(items.size()), true);
+        final Map<String, Consumption> queryConsumpations = Maps.newHashMap();
+        for (QueryStatisticsItem item : items) {
+            queryConsumpations.put(item.getQueryId(), new Consumption(item.getQueryProfile()));
+        }
+        return queryConsumpations;
+    }
+
+    /**
+     * Return query's instances consumption.
+     *
+     * @param item
+     * @return
+     * @throws AnalysisException
+     */
+    public Collection<InstanceConsumption> getQueryInstancesConsumptions(QueryStatisticsItem item) throws AnalysisException {
+        triggerReportAndWait(item, getWaitTime(1), false);
+        final Map<String, RuntimeProfile> instanceProfiles = collectInstanceProfile(item.getQueryProfile());
+        final List<InstanceConsumption> instanceConsumptions = Lists.newArrayList();
+        for (QueryStatisticsItem.FragmentInstanceInfo instanceInfo : item.getFragmentInstanceInfos()) {
+            final RuntimeProfile instanceProfile = instanceProfiles.get(DebugUtil.printId(instanceInfo.getInstanceId()));
+            Preconditions.checkNotNull(instanceProfile);
+            final InstanceConsumption consumption =
+                    new InstanceConsumption(
+                            instanceInfo.getFragmentId(),
+                            instanceInfo.getInstanceId(),
+                            instanceInfo.getAddress(),
+                            instanceProfile);
+            instanceConsumptions.add(consumption);
+        }
+        return instanceConsumptions;
+    }
+
+    /**
+     * Profile trees is query profile -> fragment profile -> instance profile ....
+     * @param queryProfile
+     * @return instanceProfiles
+     */
+    private Map<String, RuntimeProfile> collectInstanceProfile(RuntimeProfile queryProfile) {
+        final Map<String, RuntimeProfile> instanceProfiles = Maps.newHashMap();
+        for (RuntimeProfile fragmentProfile : queryProfile.getChildMap().values()) {
+            for (Map.Entry<String, RuntimeProfile> entry: fragmentProfile.getChildMap().entrySet()) {
+                Preconditions.checkState(instanceProfiles.put(parseInstanceId(entry.getKey()), entry.getValue()) == null);
+            }
+        }
+        return instanceProfiles;
+    }
+
+    /**
+     * Instance profile key is ""Instance ${instance_id} (host=$host $port)""
+     * @param str
+     * @return
+     */
+    private String parseInstanceId(String str) {
+        final String[] elements = str.split("" "");
+        if (elements.length == 4) {
+            return  elements[1];
+        } else {
+            Preconditions.checkState(false);
+            return """";
+        }
+    }
+
+    private long getWaitTime(int numOfQuery) {
+        final int oneQueryWaitTime = 200;
+        final int allQueryMaxWaitTime = 2000;
+        final int waitTime = numOfQuery * oneQueryWaitTime;
+        return waitTime > allQueryMaxWaitTime ? allQueryMaxWaitTime : waitTime;
+    }
+
+    private void triggerReportAndWait(QueryStatisticsItem item, long waitTime, boolean allQuery)
+            throws AnalysisException {
+        final List<QueryStatisticsItem> items = Lists.newArrayList(item);
+        triggerReportAndWait(items, waitTime, allQuery);
+    }
+
+    private void triggerReportAndWait(Collection<QueryStatisticsItem> items, long waitTime, boolean allQuery)
+            throws AnalysisException {
+        triggerReportProfile(items, allQuery);
+        try {
+            Thread.currentThread().sleep(waitTime);
+        } catch (InterruptedException e) {
+        }
+    }
+
+    /**
+     * send report profile request.
+     * @param items
+     * @param allQuery true:all queries profile will be reported, false:specified queries profile will be reported.
+     * @throws AnalysisException
+     */
+    private void triggerReportProfile(Collection<QueryStatisticsItem> items, boolean allQuery) throws AnalysisException {
+        final Map<TNetworkAddress, Request> requestMap = Maps.newHashMap();
+        final Map<TNetworkAddress, TNetworkAddress> brpcAddressMap = Maps.newHashMap();
+        for (QueryStatisticsItem item : items) {
+            for (QueryStatisticsItem.FragmentInstanceInfo instanceInfo : item.getFragmentInstanceInfos()) {
+                // use brpc address
+                TNetworkAddress brpcNetAddress = brpcAddressMap.get(instanceInfo.getAddress());
+                if (brpcNetAddress == null) {
+                    try {
+                        brpcNetAddress = toBrpcHost(instanceInfo.getAddress());
+                        brpcAddressMap.put(instanceInfo.getAddress(), brpcNetAddress);
+                    } catch (Exception e) {
+                        LOG.warn(e.getMessage());
+                        throw new AnalysisException(e.getMessage());
+                    }
+                }
+                // merge different requests
+                Request request = requestMap.get(brpcNetAddress);
+                if (request == null) {
+                    request = new Request(brpcNetAddress);
+                    requestMap.put(brpcNetAddress, request);
+                }
+                // specified query instance which will report.
+                if (!allQuery) {
+                    final PUniqueId pUId = new PUniqueId(instanceInfo.getInstanceId());
+                    request.addInstanceId(pUId);
+                }
+            }
+        }
+        recvResponse(sendRequest(requestMap));
+    }
+
+    private List<Pair<Request, Future<PTiggerReportProfileResult>>> sendRequest(
+            Map<TNetworkAddress, Request> requestMap) throws AnalysisException {
+        final List<Pair<Request, Future<PTiggerReportProfileResult>>> futures = Lists.newArrayList();
+        for (TNetworkAddress address : requestMap.keySet()) {
+            final Request request = requestMap.get(address);
+            final PTiggerReportProfileRequest pbRequest =
+                    new PTiggerReportProfileRequest(request.getInstanceIds());
+            try {
+                futures.add(Pair.create(request, BackendServiceProxy.getInstance().
+                        triggerReportProfileAsync(address, pbRequest)));
+            } catch (RpcException e) {
+                throw new AnalysisException(""Sending request fails for query's execution informations."");
+            }
+        }
+        return futures;
+    }
+
+    private void recvResponse(List<Pair<Request, Future<PTiggerReportProfileResult>>> futures)
+            throws AnalysisException {
+        final String reasonPrefix = ""Fail to receive result."";
+        for (Pair<Request, Future<PTiggerReportProfileResult>> pair : futures) {
+            try {
+                final PTiggerReportProfileResult result
+                        = pair.second.get(10, TimeUnit.SECONDS);","[{'comment': ""You wait here for at most 10 seconds. But getWaitTime() is at most 2 seconds? What's that mean?"", 'commenter': 'morningman'}, {'comment': 'I will fix it.', 'commenter': 'chenhao7253886'}]"
497,be/src/runtime/fragment_mgr.cpp,"@@ -501,41 +500,32 @@ void FragmentMgr::cancel_worker() {
     LOG(INFO) << ""FragmentMgr cancel worker is going to exit."";
 }
 
-
-Status FragmentMgr::fetch_fragment_exec_infos(PFetchFragmentExecInfosResult* result,
-                                              const PFetchFragmentExecInfoRequest* request) {
-    int fragment_id_list_size = request->finst_id_size();
-    for (int i = 0; i < fragment_id_list_size; i++) {
-        const PUniqueId& p_fragment_id = request->finst_id(i);
-        TUniqueId id;
-        id.__set_hi(p_fragment_id.hi());
-        id.__set_lo(p_fragment_id.lo()); 
-        PFragmentExecInfo* info = result->add_fragment_exec_info();
-        PUniqueId* finst_id = info->mutable_finst_id();
-        finst_id->set_hi(p_fragment_id.hi());
-        finst_id->set_lo(p_fragment_id.lo()); 
-
-        bool is_running = false; 
-        std::lock_guard<std::mutex> lock(_lock);
-        {
-            auto iter = _fragment_map.find(id);
-            if (iter == _fragment_map.end()) {
-                info->set_exec_status(PFragmentExecStatus::FINISHED);
-                continue;
+Status FragmentMgr::trigger_profile_report(const PTriggerProfileReportRequest* request) {
+    if (request->instance_ids_size() > 0) {
+        for (int i = 0; i < request->instance_ids_size(); i++) {
+            const PUniqueId& p_fragment_id = request->instance_ids(i);
+            TUniqueId id;
+            id.__set_hi(p_fragment_id.hi());
+            id.__set_lo(p_fragment_id.lo());
+            {
+                std::lock_guard<std::mutex> lock(_lock);
+                auto iter = _fragment_map.find(id);","[{'comment': 'auto iter -> auto& iter', 'commenter': 'kangpinghuang'}, {'comment': 'ok, i will fix it.', 'commenter': 'chenhao7253886'}, {'comment': ""the for_each of map return Pair class value as function result, so it can't be refered as left-value reference."", 'commenter': 'chenhao7253886'}]"
497,be/src/runtime/fragment_mgr.cpp,"@@ -501,41 +500,32 @@ void FragmentMgr::cancel_worker() {
     LOG(INFO) << ""FragmentMgr cancel worker is going to exit."";
 }
 
-
-Status FragmentMgr::fetch_fragment_exec_infos(PFetchFragmentExecInfosResult* result,
-                                              const PFetchFragmentExecInfoRequest* request) {
-    int fragment_id_list_size = request->finst_id_size();
-    for (int i = 0; i < fragment_id_list_size; i++) {
-        const PUniqueId& p_fragment_id = request->finst_id(i);
-        TUniqueId id;
-        id.__set_hi(p_fragment_id.hi());
-        id.__set_lo(p_fragment_id.lo()); 
-        PFragmentExecInfo* info = result->add_fragment_exec_info();
-        PUniqueId* finst_id = info->mutable_finst_id();
-        finst_id->set_hi(p_fragment_id.hi());
-        finst_id->set_lo(p_fragment_id.lo()); 
-
-        bool is_running = false; 
-        std::lock_guard<std::mutex> lock(_lock);
-        {
-            auto iter = _fragment_map.find(id);
-            if (iter == _fragment_map.end()) {
-                info->set_exec_status(PFragmentExecStatus::FINISHED);
-                continue;
+Status FragmentMgr::trigger_profile_report(const PTriggerProfileReportRequest* request) {
+    if (request->instance_ids_size() > 0) {
+        for (int i = 0; i < request->instance_ids_size(); i++) {
+            const PUniqueId& p_fragment_id = request->instance_ids(i);
+            TUniqueId id;
+            id.__set_hi(p_fragment_id.hi());
+            id.__set_lo(p_fragment_id.lo());
+            {
+                std::lock_guard<std::mutex> lock(_lock);
+                auto iter = _fragment_map.find(id);
+                if (iter != _fragment_map.end()) {
+                    iter->second->executor()->report_profile_once();
+                }
             }
-            is_running = iter->second->executor()->runtime_state()->is_running();
         }
-
-        if (is_running) {
-            info->set_exec_status(PFragmentExecStatus::RUNNING);
-        } else {
-            info->set_exec_status(PFragmentExecStatus::WAIT);
+    } else {
+        std::lock_guard<std::mutex> lock(_lock);
+        auto iter = _fragment_map.begin();","[{'comment': 'auto iter -> auto& iter', 'commenter': 'kangpinghuang'}, {'comment': 'ok, i will fix it.', 'commenter': 'chenhao7253886'}]"
506,be/src/olap/rowset/segment_group.cpp,"@@ -688,4 +688,93 @@ int64_t SegmentGroup::get_tablet_id() {
     return _tablet_id;
 }
 
+bool SegmentGroup::create_hard_links() {","[{'comment': 'No gc when failed.', 'commenter': 'chaoyli'}]"
508,be/src/olap/task/engine_cancel_delete_task.cpp,"@@ -53,7 +53,8 @@ OLAPStatus EngineCancelDeleteTask::_cancel_delete() {
     DeleteConditionHandler cond_handler;
     for (TabletSharedPtr temp_tablet : table_list) {
         temp_tablet->obtain_header_wrlock();
-        res = cond_handler.delete_cond(temp_tablet, _request.version, false);
+        del_cond_array* delete_conditions = temp_tablet->mutable_delete_predicate();","[{'comment': 'here should use mutable_delete_data_conditions()', 'commenter': 'kangpinghuang'}]"
510,be/src/olap/rowset/alpha_rowset.cpp,"@@ -52,11 +52,7 @@ OLAPStatus AlphaRowset::init() {
 }
 
 std::unique_ptr<RowsetReader> AlphaRowset::create_reader() {
-    std::vector<SegmentGroup*> segment_groups;
-    for (auto& segment_group : _segment_groups) {
-        segment_groups.push_back(segment_group.get());
-    }
-    return std::unique_ptr<RowsetReader>(new AlphaRowsetReader(_tablet_schema,
+    return std::unique_ptr<RowsetReader>(new AlphaRowsetReader(","[{'comment': 'unique_ptr cannot be assigned', 'commenter': 'chaoyli'}, {'comment': 'unique_ptr can be used as returned value', 'commenter': 'kangpinghuang'}]"
510,be/src/olap/rowset/alpha_rowset.h,"@@ -44,14 +44,25 @@ class AlphaRowset : public Rowset {
 
     virtual OLAPStatus remove();
 
-    virtual RowsetMetaSharedPtr get_meta();
+    virtual RowsetMetaSharedPtr get_rs_meta() const;
 
     virtual void set_version(Version version);
 
+    virtual int get_data_disk_size() const;
+
+    virtual int get_index_disk_size() const;
+
+    virtual bool empty() const;
+
+    virtual bool zero_num_rows() const;
+
+    virtual size_t get_num_rows() const;
+
     bool create_hard_links(std::vector<std::string>* success_links);
 
     bool remove_old_files(std::vector<std::string>* removed_links);
 
+","[{'comment': 'empty line', 'commenter': 'chaoyli'}]"
510,be/src/olap/rowset/alpha_rowset.cpp,"@@ -72,7 +68,7 @@ OLAPStatus AlphaRowset::remove() {
     return OLAP_SUCCESS;
 }
 
-RowsetMetaSharedPtr AlphaRowset::get_meta() {
+RowsetMetaSharedPtr AlphaRowset::get_rs_meta() const {","[{'comment': 'May it is better to named as rowset_meta()?', 'commenter': 'chaoyli'}]"
510,be/src/olap/rowset/alpha_rowset.cpp,"@@ -105,14 +101,34 @@ bool AlphaRowset::remove_old_files(std::vector<std::string>* removed_links) {
     return true;
 }
 
+int AlphaRowset::get_data_disk_size() const {
+    return _rowset_meta->total_disk_size();
+}
+
+int AlphaRowset::get_index_disk_size() const {
+    return _rowset_meta->index_disk_size();
+}
+
+bool AlphaRowset::empty() const {
+    return _rowset_meta->empty();
+}
+
+bool AlphaRowset::zero_num_rows() const {
+    return _rowset_meta->row_number() == 0;
+}
+
+size_t AlphaRowset::get_num_rows() const {
+    return _rowset_meta->row_number();","[{'comment': 'The same question like above.', 'commenter': 'chaoyli'}]"
510,be/src/olap/rowset/alpha_rowset_meta.cpp,"@@ -22,8 +22,8 @@
 namespace doris {
 
 bool AlphaRowsetMeta::deserialize_extra_properties() {
-    std::string extra_properties = get_extra_properties();
-    bool parsed = _extra_meta_pb.ParseFromString(extra_properties);
+    std::string properties = extra_properties();
+    bool parsed = _extra_meta_pb.ParseFromString(properties);","[{'comment': 'May be better to like this?\r\n```\r\nreturn _extra_meta_pb.ParseFromString(properties); \r\n```', 'commenter': 'chaoyli'}]"
510,be/src/olap/rowset/alpha_rowset.cpp,"@@ -105,14 +101,34 @@ bool AlphaRowset::remove_old_files(std::vector<std::string>* removed_links) {
     return true;
 }
 
+int AlphaRowset::get_data_disk_size() const {
+    return _rowset_meta->total_disk_size();
+}
+
+int AlphaRowset::get_index_disk_size() const {","[{'comment': 'May be better to make function name unified?', 'commenter': 'chaoyli'}]"
514,be/src/olap/rowset/rowset_reader_context_builder.h,"@@ -0,0 +1,154 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#ifndef DORIS_BE_SRC_OLAP_ROWSET_ROWSET_READER_CONTEXT_BUILDER_H
+#define DORIS_BE_SRC_OLAP_ROWSET_ROWSET_READER_CONTEXT_BUILDER_H
+
+#include ""olap/schema.h""
+#include ""olap/column_predicate.h""
+#include ""olap/row_cursor.h""
+#include ""olap/row_block.h""
+#include ""olap/lru_cache.h""
+#include ""olap/olap_cond.h""
+#include ""olap/delete_handler.h""
+#include ""runtime/runtime_state.h""
+
+namespace doris {
+
+struct ReaderContext {
+	RowFields* tablet_schema;","[{'comment': 'not aligned.', 'commenter': 'chaoyli'}]"
516,be/src/olap/base_compaction.cpp,"@@ -142,12 +139,20 @@ OLAPStatus BaseCompaction::run() {
         return res;
     }
 
+    //  validate that delete action is right
+    //  if error happened, sleep 1 hour. Report a fatal log every 1 minute
+    if (_validate_delete_file_action() != OLAP_SUCCESS) {
+        LOG(WARNING) << ""failed to do base compaction. delete action has error."";
+        _garbage_collection();
+        return OLAP_ERR_BE_ERROR_DELETE_ACTION;
+    }
+
     VLOG(3) << ""elapsed time of doing base compaction:"" << stage_watch.get_elapse_time_us();
 
     // 4. make new versions visable.
     //    If success, remove files belong to old versions;
     //    If fail, gc files belong to new versions.
-    vector<SegmentGroup*> unused_olap_indices;
+    vector<RowsetSharedPtr> unused_olap_indices;","[{'comment': 'change the unused_olap_indices to unused_rowsets?', 'commenter': 'kangpinghuang'}, {'comment': 'OK. I will change it to unused_rowsets.', 'commenter': 'chaoyli'}]"
516,be/src/olap/base_compaction.cpp,"@@ -319,22 +306,30 @@ bool BaseCompaction::_check_whether_satisfy_policy(bool is_manual_trigger,
 }
 
 OLAPStatus BaseCompaction::_do_base_compaction(VersionHash new_base_version_hash,
-                                               vector<ColumnData*>* base_data_sources,
+                                               const vector<RowsetSharedPtr>& rowsets,
                                                uint64_t* row_count) {
     // 1. 生成新base文件对应的olap index
-    /*
-    SegmentGroup* new_base = new (std::nothrow) SegmentGroup(_tablet.get(),
-                                                       _new_base_version,
-                                                       new_base_version_hash,
-                                                       false, 0, 0);
-    */
-
-    SegmentGroup* new_base = nullptr;
-    if (new_base == NULL) {
-        OLAP_LOG_WARNING(""fail to new SegmentGroup."");
+    RowsetId rowset_id = 0;
+    RowsetIdGenerator::instance()->get_next_id(_tablet->data_dir(), &rowset_id);
+    RowsetBuilderContext context = {_tablet->partition_id(), _tablet->tablet_id(),
+                                    _tablet->schema_hash(), rowset_id, 
+                                    RowsetTypePB::ALPHA_ROWSET, _tablet->rowset_path_prefix(),
+                                    _tablet->tablet_schema(), _tablet->num_key_fields(),
+                                    _tablet->num_short_key_fields(), _tablet->num_rows_per_row_block(),
+                                    _tablet->compress_kind(), _tablet->bloom_filter_fpp()};
+    RowsetBuilder* builder = new AlphaRowsetBuilder(); 
+    builder->init(context);
+    if (builder == nullptr) {","[{'comment': 'move the judgement up before builder->init', 'commenter': 'kangpinghuang'}, {'comment': 'OK', 'commenter': 'chaoyli'}]"
516,be/src/olap/base_compaction.cpp,"@@ -319,22 +306,30 @@ bool BaseCompaction::_check_whether_satisfy_policy(bool is_manual_trigger,
 }
 
 OLAPStatus BaseCompaction::_do_base_compaction(VersionHash new_base_version_hash,
-                                               vector<ColumnData*>* base_data_sources,
+                                               const vector<RowsetSharedPtr>& rowsets,
                                                uint64_t* row_count) {
     // 1. 生成新base文件对应的olap index
-    /*
-    SegmentGroup* new_base = new (std::nothrow) SegmentGroup(_tablet.get(),
-                                                       _new_base_version,
-                                                       new_base_version_hash,
-                                                       false, 0, 0);
-    */
-
-    SegmentGroup* new_base = nullptr;
-    if (new_base == NULL) {
-        OLAP_LOG_WARNING(""fail to new SegmentGroup."");
+    RowsetId rowset_id = 0;
+    RowsetIdGenerator::instance()->get_next_id(_tablet->data_dir(), &rowset_id);
+    RowsetBuilderContext context = {_tablet->partition_id(), _tablet->tablet_id(),
+                                    _tablet->schema_hash(), rowset_id, 
+                                    RowsetTypePB::ALPHA_ROWSET, _tablet->rowset_path_prefix(),
+                                    _tablet->tablet_schema(), _tablet->num_key_fields(),
+                                    _tablet->num_short_key_fields(), _tablet->num_rows_per_row_block(),
+                                    _tablet->compress_kind(), _tablet->bloom_filter_fpp()};
+    RowsetBuilder* builder = new AlphaRowsetBuilder(); 
+    builder->init(context);
+    if (builder == nullptr) {
+        OLAP_LOG_WARNING(""fail to new rowset."");","[{'comment': 'use LOG(WARNING)', 'commenter': 'kangpinghuang'}, {'comment': 'OK', 'commenter': 'chaoyli'}]"
516,be/src/olap/cumulative_compaction.cpp,"@@ -462,12 +460,12 @@ OLAPStatus CumulativeCompaction::_do_cumulative_compaction() {
     return res;
 }
 
-OLAPStatus CumulativeCompaction::_update_header(vector<SegmentGroup*>* unused_indices) {
-    vector<SegmentGroup*> new_indices;
-    new_indices.push_back(_new_segment_group);
+OLAPStatus CumulativeCompaction::_update_header(vector<RowsetSharedPtr>* unused_indices) {
+    vector<RowsetSharedPtr> new_indices;
+    new_indices.push_back(_rowset);
 
     OLAPStatus res = OLAP_SUCCESS;
-    res = _tablet->replace_data_sources(&_need_merged_versions, &new_indices, unused_indices);
+    res = _tablet->modify_rowsets(&_need_merged_versions, &new_indices, unused_indices);","[{'comment': 'I think new_indices and unused_indices are not good names. pls change them', 'commenter': 'kangpinghuang'}, {'comment': 'OK', 'commenter': 'chaoyli'}]"
516,be/src/olap/cumulative_compaction.h,"@@ -110,13 +112,13 @@ class CumulativeCompaction {
     // 返回值：
     // - 如果成功，返回OLAP_SUCCESS
     // - 如果不成功，返回相应错误码
-    OLAPStatus _update_header(std::vector<SegmentGroup*>* unused_indices);
+    OLAPStatus _update_header(std::vector<RowsetSharedPtr>* unused_indices);","[{'comment': 'change the unused_indices name', 'commenter': 'kangpinghuang'}]"
516,be/src/olap/merger.cpp,"@@ -35,24 +35,24 @@ using std::vector;
 
 namespace doris {
 
-Merger::Merger(TabletSharedPtr tablet, SegmentGroup* segment_group, ReaderType type) : 
+Merger::Merger(TabletSharedPtr tablet, RowsetBuilder* builder, ReaderType type) : 
         _tablet(tablet),
-        _segment_group(segment_group),
+        _builder(builder),
         _reader_type(type),
         _row_count(0) {}
 
-OLAPStatus Merger::merge(const vector<ColumnData*>& olap_data_arr,
-                         uint64_t* merged_rows, uint64_t* filted_rows) {
+OLAPStatus Merger::merge(const vector<RowsetReaderSharedPtr>& rs_readers,
+                         const Version& version, uint64_t* merged_rows, uint64_t* filted_rows) {
     // Create and initiate reader for scanning and multi-merging specified
     // OLAPDatas.
     Reader reader;
     ReaderParams reader_params;
     reader_params.tablet = _tablet;
     reader_params.reader_type = _reader_type;
-    reader_params.olap_data_arr = olap_data_arr;
+    reader_params.olap_data_arr = rs_readers;","[{'comment': 'change the olap_data_arr name', 'commenter': 'kangpinghuang'}, {'comment': 'I will change it to rs_readers name.', 'commenter': 'chaoyli'}]"
516,be/src/olap/rowset/rowset.h,"@@ -58,6 +58,12 @@ class Rowset {
     virtual bool zero_num_rows() const = 0;
 
     virtual size_t num_rows() const = 0;
+
+    virtual Version version() const;","[{'comment': 'should use virtual Version version const = 0;\r\nchange the following.', 'commenter': 'kangpinghuang'}]"
516,be/src/olap/tablet.cpp,"@@ -105,56 +105,101 @@ bool Tablet::can_do_compaction() {
     return true;
 }
 
-OLAPStatus Tablet::capture_consistent_versions(
-                        const Version& version, vector<Version>* span_versions) const {
-    OLAPStatus status = _rs_graph->capture_consistent_versions(version, span_versions);
-    if (status != OLAP_SUCCESS) {
-        LOG(WARNING) << ""fail to generate shortest version path. tablet="" << full_name()
-                     << "", version='"" << version.first << ""-"" << version.second;
+OLAPStatus Tablet::compute_all_versions_hash(const vector<Version>& versions,
+                                             VersionHash* version_hash) const {
+    DCHECK(version_hash != nullptr) << ""invalid parameter, version_hash is nullptr"";
+
+    int64_t v_hash  = 0L;
+    for (auto version : versions) {
+        auto it = _rs_version_map.find(version);
+        if (it == _rs_version_map.end()) {
+            LOG(WARNING) << ""fail to find Rowset. ""
+                << ""version="" << version.first << ""-"" << version.second;
+            return OLAP_ERR_TABLE_VERSION_INDEX_MISMATCH_ERROR;
+        }
+        v_hash ^= it->second->version_hash();
     }
-    return status;
+    *version_hash = v_hash;
+    return OLAP_SUCCESS;
 }
 
 OLAPStatus Tablet::capture_consistent_rowsets(const Version& spec_version,
-                                             vector<std::shared_ptr<RowsetReader>>* rs_readers) {
+                                              vector<RowsetSharedPtr>* rowsets) const {
     vector<Version> version_path;
     _rs_graph->capture_consistent_versions(spec_version, &version_path);
 
-    acquire_rs_reader_by_version(version_path, rs_readers);
+    capture_consistent_rowsets(version_path, rowsets);
     return OLAP_SUCCESS;
 }
 
-void Tablet::acquire_rs_reader_by_version(const vector<Version>& version_vec,
-                                          vector<std::shared_ptr<RowsetReader>>* rs_readers) const {
-    DCHECK(rs_readers != NULL && rs_readers->empty());
-    for (auto version : version_vec) {
-        auto it2 = _rs_version_map.find(version);
-        if (it2 == _rs_version_map.end()) {
+OLAPStatus Tablet::capture_consistent_rowsets(const vector<Version>& version_path,
+                                        vector<RowsetSharedPtr>* rowsets) const {
+    DCHECK(rowsets != nullptr && rowsets->empty());
+    for (auto version : version_path) {
+        auto it = _rs_version_map.find(version);","[{'comment': 'auto -> auto&?', 'commenter': 'kangpinghuang'}]"
516,be/src/olap/base_compaction.h,"@@ -82,35 +84,35 @@ class BaseCompaction {
     // 
     // 输入参数：
     // - new_base_version_hash: 新Base的VersionHash
-    // - base_data_sources: 生成新Base需要的ColumnData*
+    // - rs_readers : 生成新Base需要的RowsetReaders*
     // - row_count: 生成Base过程中产生的row_count
     //
     // 返回值：
     // - 如果执行成功，则返回OLAP_SUCCESS；
     // - 其它情况下，返回相应的错误码
     OLAPStatus _do_base_compaction(VersionHash new_base_version_hash,
-                                  std::vector<ColumnData*>* base_data_sources,
-                                  uint64_t* row_count);
+                                   const std::vector<RowsetSharedPtr>& rowsets,
+                                   uint64_t* row_count);","[{'comment': 'wrong indent?', 'commenter': 'kangpinghuang'}, {'comment': ""It's OK"", 'commenter': 'chaoyli'}]"
516,be/src/olap/base_compaction.h,"@@ -82,35 +84,35 @@ class BaseCompaction {
     // 
     // 输入参数：
     // - new_base_version_hash: 新Base的VersionHash
-    // - base_data_sources: 生成新Base需要的ColumnData*
+    // - rs_readers : 生成新Base需要的RowsetReaders*
     // - row_count: 生成Base过程中产生的row_count
     //
     // 返回值：
     // - 如果执行成功，则返回OLAP_SUCCESS；
     // - 其它情况下，返回相应的错误码
     OLAPStatus _do_base_compaction(VersionHash new_base_version_hash,
-                                  std::vector<ColumnData*>* base_data_sources,
-                                  uint64_t* row_count);
+                                   const std::vector<RowsetSharedPtr>& rowsets,
+                                   uint64_t* row_count);
    
     // 更新Header使得修改对外可见
     // 输出参数：
-    // - unused_olap_indices: 需要被物理删除的SegmentGroup*
+    // - unused_rowsets: 需要被物理删除的Rowset*
     //
     // 返回值：
     // - 如果执行成功，则返回OLAP_SUCCESS；
     // - 其它情况下，返回相应的错误码
     OLAPStatus _update_header(uint64_t row_count,
-                              std::vector<SegmentGroup*>* unused_olap_indices);
+                              std::vector<RowsetSharedPtr>* unused_rowsets);","[{'comment': 'wrong indent? two tab before next line?', 'commenter': 'kangpinghuang'}, {'comment': ""It's OK"", 'commenter': 'chaoyli'}]"
516,be/src/olap/cumulative_compaction.h,"@@ -27,10 +27,12 @@
 #include ""olap/rowset/column_data.h""","[{'comment': 'remove this header include', 'commenter': 'kangpinghuang'}, {'comment': 'OK', 'commenter': 'chaoyli'}]"
516,be/src/olap/merger.cpp,"@@ -35,24 +35,24 @@ using std::vector;
 ","[{'comment': 'remove column_data.h and segment_group  column_data_writer.h header include', 'commenter': 'kangpinghuang'}]"
516,be/src/olap/merger.h,"@@ -29,23 +29,25 @@ class ColumnData;
 class Merger {
 public:
     // parameter index is created by caller, and it is empty.
-    Merger(TabletSharedPtr tablet, SegmentGroup* index, ReaderType type);
+    //Merger(TabletSharedPtr tablet, RowsetSharedPtr rowset, ReaderType type);","[{'comment': 'remove this line', 'commenter': 'kangpinghuang'}, {'comment': 'OK', 'commenter': 'chaoyli'}]"
516,be/src/olap/merger.h,"@@ -29,23 +29,25 @@ class ColumnData;
 class Merger {
 public:
     // parameter index is created by caller, and it is empty.
-    Merger(TabletSharedPtr tablet, SegmentGroup* index, ReaderType type);
+    //Merger(TabletSharedPtr tablet, RowsetSharedPtr rowset, ReaderType type);
+    Merger(TabletSharedPtr tablet, RowsetBuilder* builder, ReaderType type);
 
     virtual ~Merger() {};
 
     // @brief read from multiple OLAPData and SegmentGroup, then write into single OLAPData and SegmentGroup
     // @return  OLAPStatus: OLAP_SUCCESS or FAIL
     // @note it will take long time to finish.
-    OLAPStatus merge(const std::vector<ColumnData*>& olap_data_arr, 
-                     uint64_t* merged_rows, uint64_t* filted_rows);
+    OLAPStatus merge(const std::vector<RowsetReaderSharedPtr>& rs_readers, 
+                     const Version& version, uint64_t* merged_rows, uint64_t* filted_rows);
 
     // 获取在做merge过程中累积的行数
     uint64_t row_count() {
         return _row_count;
     }
 private:
     TabletSharedPtr _tablet;
-    SegmentGroup* _segment_group;
+    //RowsetSharedPtr _rowset;","[{'comment': 'remove this line', 'commenter': 'kangpinghuang'}, {'comment': 'OK', 'commenter': 'chaoyli'}]"
516,be/src/olap/reader.cpp,"@@ -308,13 +308,13 @@ OLAPStatus Reader::init(const ReaderParams& read_params) {
         return res;
     }
 
-    res = _acquire_data_sources(read_params);
+    res = _capture_rs_readers(read_params);
     if (res != OLAP_SUCCESS) {
         OLAP_LOG_WARNING(""fail to init reader when acquire data sources.[res=%d]"", res);      
         return res;
     }
 
-    for (auto i_data: _data_sources) {
+    for (auto i_data : _rs_readers) {","[{'comment': 'change i_data to rs_reader?\r\nauto -> auto&?', 'commenter': 'kangpinghuang'}, {'comment': 'This place will be changed in reader refactor.', 'commenter': 'chaoyli'}]"
516,be/src/olap/schema_change.cpp,"@@ -1523,22 +1523,21 @@ OLAPStatus SchemaChangeHandler::_do_alter_tablet(
         }
 
         // acquire data sources correspond to history versions
-        ref_tablet->acquire_data_sources_by_versions(
-                versions_to_be_changed, &olap_data_arr);
-        if (olap_data_arr.size() < 1) {
+        ref_tablet->capture_rs_readers(versions_to_be_changed, &rs_readers);
+        if (rs_readers.size() < 1) {
             OLAP_LOG_WARNING(""fail to acquire all data sources.""
                              ""[version_num=%d data_source_num=%d]"",
                              versions_to_be_changed.size(),
-                             olap_data_arr.size());
+                             rs_readers.size());","[{'comment': 'change OLAP_LOG_WARNING -> LOG(WARNING)', 'commenter': 'kangpinghuang'}, {'comment': 'OK', 'commenter': 'chaoyli'}]"
529,be/src/olap/delta_writer.cpp,"@@ -143,20 +148,7 @@ OLAPStatus DeltaWriter::write(Tuple* tuple) {
 
     _mem_table->insert(tuple);
     if (_mem_table->memory_usage() >= config::write_buffer_size) {
-        RETURN_NOT_OK(_mem_table->flush(_writer));
-
-        ++_segment_group_id;
-        //_cur_segment_group = new SegmentGroup(_tablet.get(), false, _segment_group_id, 0, true,
-        //                           _req.partition_id, _req.transaction_id);
-        _cur_segment_group = nullptr; 
-        DCHECK(_cur_segment_group != nullptr) << ""failed to malloc SegmentGroup"";
-        _cur_segment_group->acquire();
-        _cur_segment_group->set_load_id(_req.load_id);
-        _segment_group_vec.push_back(_cur_segment_group);
-
-        SAFE_DELETE(_writer);
-        _writer = ColumnDataWriter::create(_cur_segment_group, true, _tablet->compress_kind(), _tablet->bloom_filter_fpp());
-        DCHECK(_writer != nullptr) << ""memory error occur when creating writer"";
+        RETURN_NOT_OK(_mem_table->flush(_rowset_builder.get()));","[{'comment': 'May use shared_ptr will OK?', 'commenter': 'chaoyli'}, {'comment': 'OK, I will modify it', 'commenter': 'kangpinghuang'}]"
529,be/src/olap/delta_writer.cpp,"@@ -172,15 +164,18 @@ OLAPStatus DeltaWriter::close(google::protobuf::RepeatedPtrField<PTabletInfo>* t
             return st;
         }
     }
-    RETURN_NOT_OK(_mem_table->close(_writer));
+    RETURN_NOT_OK(_mem_table->close(_rowset_builder.get()));
 
     OLAPStatus res = OLAP_SUCCESS;
     //add pending data to tablet
-    RETURN_NOT_OK(_tablet->add_pending_version(_req.partition_id, _req.transaction_id, nullptr));
-    for (SegmentGroup* segment_group : _segment_group_vec) {
-        RETURN_NOT_OK(_tablet->add_pending_segment_group(segment_group));
-        RETURN_NOT_OK(segment_group->load());
-    }
+    RETURN_NOT_OK(_tablet->add_pending_version(_req.partition_id, _req.txn_id, nullptr));","[{'comment': 'tablet has no add_pending_version interface, you should add it into RowsetMetaManger.', 'commenter': 'chaoyli'}, {'comment': 'OK, I think this should be removed', 'commenter': 'kangpinghuang'}]"
529,be/src/olap/delta_writer.cpp,"@@ -172,15 +164,18 @@ OLAPStatus DeltaWriter::close(google::protobuf::RepeatedPtrField<PTabletInfo>* t
             return st;
         }
     }
-    RETURN_NOT_OK(_mem_table->close(_writer));
+    RETURN_NOT_OK(_mem_table->close(_rowset_builder.get()));
 
     OLAPStatus res = OLAP_SUCCESS;
     //add pending data to tablet
-    RETURN_NOT_OK(_tablet->add_pending_version(_req.partition_id, _req.transaction_id, nullptr));
-    for (SegmentGroup* segment_group : _segment_group_vec) {
-        RETURN_NOT_OK(_tablet->add_pending_segment_group(segment_group));
-        RETURN_NOT_OK(segment_group->load());
-    }
+    RETURN_NOT_OK(_tablet->add_pending_version(_req.partition_id, _req.txn_id, nullptr));
+    // use rowset meta manager to save meta
+    _cur_rowset = _rowset_builder->build();
+    RowsetMetaManager::save(
+            _tablet->data_dir()->get_meta(),
+            _cur_rowset->rowset_id(),
+            _cur_rowset->rowset_meta().get());","[{'comment': 'Also, use std::shared_ptr<RowsetMeta> instead', 'commenter': 'chaoyli'}, {'comment': 'OK, I will modify it', 'commenter': 'kangpinghuang'}]"
529,be/src/olap/delta_writer.cpp,"@@ -29,34 +31,27 @@ OLAPStatus DeltaWriter::open(WriteRequest* req, DeltaWriter** writer) {
 
 DeltaWriter::DeltaWriter(WriteRequest* req)
     : _req(*req), _tablet(nullptr),
-      _cur_segment_group(nullptr), _new_tablet(nullptr),
-      _writer(nullptr), _mem_table(nullptr),
+      _cur_rowset(nullptr), _related_rowset(nullptr), _new_tablet(nullptr),","[{'comment': 'The two variable names  _related_rowset and _new_tablet should be consistent.', 'commenter': 'chaoyli'}]"
529,be/src/olap/rowset/rowset_builder_context.h,"@@ -0,0 +1,148 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#ifndef DORIS_BE_SRC_OLAP_ROWSET_ROWSET_BUILDER_CONTEXT_H
+#define DORIS_BE_SRC_OLAP_ROWSET_ROWSET_BUILDER_CONTEXT_H
+
+#include ""gen_cpp/olap_file.pb.h""
+
+namespace doris {
+
+struct RowsetBuilderContext {
+    int64_t rowset_id;
+    int64_t tablet_id;
+    int64_t tablet_schema_hash;
+    int64_t partition_id;
+    RowsetTypePB rowset_type;
+    std::string rowset_path_prefix;
+    RowFields tablet_schema;
+    size_t num_key_fields;
+    size_t num_short_key_fields;
+    size_t num_rows_per_row_block;
+    CompressKind compress_kind;
+    double bloom_filter_fpp;
+    // PREPARING/COMMITTED for pending rowset
+    // VISIBLE for non-pending rowset
+    RowsetStatePB rowset_state;
+    // properties for non-pending rowset
+    Version version;
+    VersionHash version_hash;
+
+    // properties for pending rowset
+    int64_t txn_id;
+    PUniqueId load_id;
+};
+
+class RowsetBuilderContextBuilder {","[{'comment': 'This name may be thought deeply. It may confused the others.', 'commenter': 'chaoyli'}]"
529,be/src/olap/memtable.h,"@@ -23,6 +23,7 @@
 #include ""olap/schema.h""
 #include ""olap/skiplist.h""
 #include ""runtime/tuple.h""
+#include ""olap/rowset/rowset_builder.h""
 
 namespace doris {
 ","[{'comment': 'ColumnDataWriter declaration should be removed.', 'commenter': 'chaoyli'}, {'comment': 'OK, I will fix it.', 'commenter': 'kangpinghuang'}]"
529,be/src/olap/rowset/rowset_builder_context.h,"@@ -0,0 +1,148 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#ifndef DORIS_BE_SRC_OLAP_ROWSET_ROWSET_BUILDER_CONTEXT_H
+#define DORIS_BE_SRC_OLAP_ROWSET_ROWSET_BUILDER_CONTEXT_H
+
+#include ""gen_cpp/olap_file.pb.h""
+
+namespace doris {
+
+struct RowsetBuilderContext {
+    int64_t rowset_id;
+    int64_t tablet_id;
+    int64_t tablet_schema_hash;
+    int64_t partition_id;
+    RowsetTypePB rowset_type;
+    std::string rowset_path_prefix;
+    RowFields tablet_schema;
+    size_t num_key_fields;
+    size_t num_short_key_fields;
+    size_t num_rows_per_row_block;
+    CompressKind compress_kind;
+    double bloom_filter_fpp;
+    // PREPARING/COMMITTED for pending rowset
+    // VISIBLE for non-pending rowset
+    RowsetStatePB rowset_state;
+    // properties for non-pending rowset
+    Version version;
+    VersionHash version_hash;
+
+    // properties for pending rowset
+    int64_t txn_id;
+    PUniqueId load_id;
+};
+
+class RowsetBuilderContextBuilder {
+public:
+    RowsetBuilderContextBuilder& set_rowset_id(int64_t rowset_id) {
+        _rowset_builder_context.rowset_id = rowset_id;
+        return *this;
+    }
+
+    RowsetBuilderContextBuilder& set_tablet_id(int64_t tablet_id) {
+        _rowset_builder_context.tablet_id = tablet_id;
+        return *this;
+    }
+
+    RowsetBuilderContextBuilder& set_tablet_schema_hash(int64_t tablet_schema_hash) {
+        _rowset_builder_context.tablet_schema_hash = tablet_schema_hash;
+        return *this;
+    }
+    
+    RowsetBuilderContextBuilder& set_partition_id(int64_t partition_id) {
+        _rowset_builder_context.partition_id = partition_id;
+        return *this;
+    }
+
+    RowsetBuilderContextBuilder& set_rowset_type(RowsetTypePB rowset_type) {
+        _rowset_builder_context.rowset_type = rowset_type;
+        return *this;
+    }
+
+    RowsetBuilderContextBuilder& set_rowset_path_prefix(std::string rowset_path_prefix) {","[{'comment': 'const string&', 'commenter': 'chaoyli'}, {'comment': 'OK, I will fix it', 'commenter': 'kangpinghuang'}]"
529,be/src/olap/rowset/rowset_builder_context.h,"@@ -0,0 +1,148 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#ifndef DORIS_BE_SRC_OLAP_ROWSET_ROWSET_BUILDER_CONTEXT_H
+#define DORIS_BE_SRC_OLAP_ROWSET_ROWSET_BUILDER_CONTEXT_H
+
+#include ""gen_cpp/olap_file.pb.h""
+
+namespace doris {
+
+struct RowsetBuilderContext {
+    int64_t rowset_id;
+    int64_t tablet_id;
+    int64_t tablet_schema_hash;
+    int64_t partition_id;
+    RowsetTypePB rowset_type;
+    std::string rowset_path_prefix;
+    RowFields tablet_schema;
+    size_t num_key_fields;
+    size_t num_short_key_fields;
+    size_t num_rows_per_row_block;
+    CompressKind compress_kind;
+    double bloom_filter_fpp;
+    // PREPARING/COMMITTED for pending rowset
+    // VISIBLE for non-pending rowset
+    RowsetStatePB rowset_state;
+    // properties for non-pending rowset
+    Version version;
+    VersionHash version_hash;
+
+    // properties for pending rowset
+    int64_t txn_id;
+    PUniqueId load_id;
+};
+
+class RowsetBuilderContextBuilder {
+public:
+    RowsetBuilderContextBuilder& set_rowset_id(int64_t rowset_id) {
+        _rowset_builder_context.rowset_id = rowset_id;
+        return *this;
+    }
+
+    RowsetBuilderContextBuilder& set_tablet_id(int64_t tablet_id) {
+        _rowset_builder_context.tablet_id = tablet_id;
+        return *this;
+    }
+
+    RowsetBuilderContextBuilder& set_tablet_schema_hash(int64_t tablet_schema_hash) {
+        _rowset_builder_context.tablet_schema_hash = tablet_schema_hash;
+        return *this;
+    }
+    
+    RowsetBuilderContextBuilder& set_partition_id(int64_t partition_id) {
+        _rowset_builder_context.partition_id = partition_id;
+        return *this;
+    }
+
+    RowsetBuilderContextBuilder& set_rowset_type(RowsetTypePB rowset_type) {","[{'comment': 'const RowsetTypePB& ', 'commenter': 'chaoyli'}, {'comment': 'This is a enum, not a object\r\n\r\n', 'commenter': 'kangpinghuang'}]"
529,be/src/olap/rowset/rowset_builder_context.h,"@@ -0,0 +1,148 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#ifndef DORIS_BE_SRC_OLAP_ROWSET_ROWSET_BUILDER_CONTEXT_H
+#define DORIS_BE_SRC_OLAP_ROWSET_ROWSET_BUILDER_CONTEXT_H
+
+#include ""gen_cpp/olap_file.pb.h""
+
+namespace doris {
+
+struct RowsetBuilderContext {
+    int64_t rowset_id;
+    int64_t tablet_id;
+    int64_t tablet_schema_hash;
+    int64_t partition_id;
+    RowsetTypePB rowset_type;
+    std::string rowset_path_prefix;
+    RowFields tablet_schema;
+    size_t num_key_fields;
+    size_t num_short_key_fields;
+    size_t num_rows_per_row_block;
+    CompressKind compress_kind;
+    double bloom_filter_fpp;
+    // PREPARING/COMMITTED for pending rowset
+    // VISIBLE for non-pending rowset
+    RowsetStatePB rowset_state;
+    // properties for non-pending rowset
+    Version version;
+    VersionHash version_hash;
+
+    // properties for pending rowset
+    int64_t txn_id;
+    PUniqueId load_id;
+};
+
+class RowsetBuilderContextBuilder {
+public:
+    RowsetBuilderContextBuilder& set_rowset_id(int64_t rowset_id) {
+        _rowset_builder_context.rowset_id = rowset_id;
+        return *this;
+    }
+
+    RowsetBuilderContextBuilder& set_tablet_id(int64_t tablet_id) {
+        _rowset_builder_context.tablet_id = tablet_id;
+        return *this;
+    }
+
+    RowsetBuilderContextBuilder& set_tablet_schema_hash(int64_t tablet_schema_hash) {
+        _rowset_builder_context.tablet_schema_hash = tablet_schema_hash;
+        return *this;
+    }
+    
+    RowsetBuilderContextBuilder& set_partition_id(int64_t partition_id) {
+        _rowset_builder_context.partition_id = partition_id;
+        return *this;
+    }
+
+    RowsetBuilderContextBuilder& set_rowset_type(RowsetTypePB rowset_type) {
+        _rowset_builder_context.rowset_type = rowset_type;
+        return *this;
+    }
+
+    RowsetBuilderContextBuilder& set_rowset_path_prefix(std::string rowset_path_prefix) {
+        _rowset_builder_context.rowset_path_prefix = rowset_path_prefix;
+        return *this;
+    }
+
+    RowsetBuilderContextBuilder& set_tablet_schema(const RowFields& tablet_schema) {
+        _rowset_builder_context.tablet_schema = tablet_schema;
+        return *this;
+    }
+
+    RowsetBuilderContextBuilder& set_num_key_fields(size_t num_key_fields) {
+        _rowset_builder_context.num_key_fields = num_key_fields;
+        return *this;
+    }
+
+    RowsetBuilderContextBuilder& set_num_short_key_fields(size_t num_short_key_fields) {
+        _rowset_builder_context.num_short_key_fields = num_short_key_fields;
+        return *this;
+    }
+
+    RowsetBuilderContextBuilder& set_num_rows_per_row_block(size_t num_rows_per_row_block) {
+        _rowset_builder_context.num_rows_per_row_block = num_rows_per_row_block;
+        return *this;
+    }
+
+    RowsetBuilderContextBuilder& set_compress_kind(CompressKind compress_kind) {
+        _rowset_builder_context.compress_kind = compress_kind;
+        return *this;
+    }
+
+    RowsetBuilderContextBuilder& set_bloom_filter_fpp(double bloom_filter_fpp) {
+        _rowset_builder_context.bloom_filter_fpp = bloom_filter_fpp;
+        return *this;
+    }
+
+    RowsetBuilderContextBuilder& set_rowset_state(RowsetStatePB rowset_state) {","[{'comment': 'const RowsetStatePB& ', 'commenter': 'chaoyli'}, {'comment': 'This is a enum too.', 'commenter': 'kangpinghuang'}]"
529,be/src/olap/rowset/rowset_builder_context.h,"@@ -0,0 +1,148 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#ifndef DORIS_BE_SRC_OLAP_ROWSET_ROWSET_BUILDER_CONTEXT_H
+#define DORIS_BE_SRC_OLAP_ROWSET_ROWSET_BUILDER_CONTEXT_H
+
+#include ""gen_cpp/olap_file.pb.h""
+
+namespace doris {
+
+struct RowsetBuilderContext {
+    int64_t rowset_id;
+    int64_t tablet_id;
+    int64_t tablet_schema_hash;
+    int64_t partition_id;
+    RowsetTypePB rowset_type;
+    std::string rowset_path_prefix;
+    RowFields tablet_schema;
+    size_t num_key_fields;
+    size_t num_short_key_fields;
+    size_t num_rows_per_row_block;
+    CompressKind compress_kind;
+    double bloom_filter_fpp;
+    // PREPARING/COMMITTED for pending rowset
+    // VISIBLE for non-pending rowset
+    RowsetStatePB rowset_state;
+    // properties for non-pending rowset
+    Version version;
+    VersionHash version_hash;
+
+    // properties for pending rowset
+    int64_t txn_id;
+    PUniqueId load_id;
+};
+
+class RowsetBuilderContextBuilder {
+public:
+    RowsetBuilderContextBuilder& set_rowset_id(int64_t rowset_id) {
+        _rowset_builder_context.rowset_id = rowset_id;
+        return *this;
+    }
+
+    RowsetBuilderContextBuilder& set_tablet_id(int64_t tablet_id) {
+        _rowset_builder_context.tablet_id = tablet_id;
+        return *this;
+    }
+
+    RowsetBuilderContextBuilder& set_tablet_schema_hash(int64_t tablet_schema_hash) {
+        _rowset_builder_context.tablet_schema_hash = tablet_schema_hash;
+        return *this;
+    }
+    
+    RowsetBuilderContextBuilder& set_partition_id(int64_t partition_id) {
+        _rowset_builder_context.partition_id = partition_id;
+        return *this;
+    }
+
+    RowsetBuilderContextBuilder& set_rowset_type(RowsetTypePB rowset_type) {
+        _rowset_builder_context.rowset_type = rowset_type;
+        return *this;
+    }
+
+    RowsetBuilderContextBuilder& set_rowset_path_prefix(std::string rowset_path_prefix) {
+        _rowset_builder_context.rowset_path_prefix = rowset_path_prefix;
+        return *this;
+    }
+
+    RowsetBuilderContextBuilder& set_tablet_schema(const RowFields& tablet_schema) {
+        _rowset_builder_context.tablet_schema = tablet_schema;
+        return *this;
+    }
+
+    RowsetBuilderContextBuilder& set_num_key_fields(size_t num_key_fields) {
+        _rowset_builder_context.num_key_fields = num_key_fields;
+        return *this;
+    }
+
+    RowsetBuilderContextBuilder& set_num_short_key_fields(size_t num_short_key_fields) {
+        _rowset_builder_context.num_short_key_fields = num_short_key_fields;
+        return *this;
+    }
+
+    RowsetBuilderContextBuilder& set_num_rows_per_row_block(size_t num_rows_per_row_block) {
+        _rowset_builder_context.num_rows_per_row_block = num_rows_per_row_block;
+        return *this;
+    }
+
+    RowsetBuilderContextBuilder& set_compress_kind(CompressKind compress_kind) {","[{'comment': 'const CompressKind&', 'commenter': 'chaoyli'}, {'comment': 'this is a enum', 'commenter': 'kangpinghuang'}]"
529,be/src/olap/rowset/alpha_rowset_builder.cpp,"@@ -72,27 +83,47 @@ OLAPStatus AlphaRowsetBuilder::flush() {
     return OLAP_SUCCESS;
 }
 
-std::shared_ptr<Rowset> AlphaRowsetBuilder::build() {
-    // TODO: set total_disk_size/data_disk_size/index_disk_size
+RowsetSharedPtr AlphaRowsetBuilder::build() {
     for (auto& segment_group : _segment_groups) {
-        PendingSegmentGroupPB pending_segment_group_pb;
-        pending_segment_group_pb.set_pending_segment_group_id(segment_group->segment_group_id());
-        pending_segment_group_pb.set_num_segments(segment_group->num_segments());
-        //PUniqueId* unique_id = pending_segment_group_pb.mutable_load_id();
-        //unique_id->set_hi(_rowset_builder_context.load_id.hi());
-        //unique_id->set_lo(_rowset_builder_context.load_id.lo());
-        pending_segment_group_pb.set_empty(segment_group->empty());
-        const std::vector<KeyRange>* column_statistics = &(segment_group->get_column_statistics());
-        if (column_statistics != nullptr) {
-            for (size_t i = 0; i < column_statistics->size(); ++i) {
-                ColumnPruning* column_pruning = pending_segment_group_pb.add_column_pruning();
-                column_pruning->set_min(column_statistics->at(i).first->to_string());
-                column_pruning->set_max(column_statistics->at(i).second->to_string());
-                column_pruning->set_null_flag(column_statistics->at(i).first->is_null());
+        if (is_pending_rowset) {
+            PendingSegmentGroupPB pending_segment_group_pb;
+            pending_segment_group_pb.set_pending_segment_group_id(segment_group->segment_group_id());
+            pending_segment_group_pb.set_num_segments(segment_group->num_segments());
+            PUniqueId* unique_id = pending_segment_group_pb.mutable_load_id();
+            unique_id->set_hi(_rowset_builder_context.load_id.hi());
+            unique_id->set_lo(_rowset_builder_context.load_id.lo());
+            pending_segment_group_pb.set_empty(segment_group->empty());
+            const std::vector<KeyRange>* column_statistics = &(segment_group->get_column_statistics());
+            if (column_statistics != nullptr) {
+                for (size_t i = 0; i < column_statistics->size(); ++i) {
+                    ColumnPruning* column_pruning = pending_segment_group_pb.add_column_pruning();
+                    column_pruning->set_min(column_statistics->at(i).first->to_string());
+                    column_pruning->set_max(column_statistics->at(i).second->to_string());
+                    column_pruning->set_null_flag(column_statistics->at(i).first->is_null());
+                }
+            }
+            AlphaRowsetMeta* alpha_rowset_meta = (AlphaRowsetMeta*)_current_rowset_meta.get();
+            alpha_rowset_meta->add_pending_segment_group(pending_segment_group_pb);
+        } else {
+            SegmentGroupPB segment_group_pb;
+            segment_group_pb.set_segment_group_id(segment_group->segment_group_id());
+            segment_group_pb.set_num_segments(segment_group->num_segments());
+            segment_group_pb.set_index_size(segment_group->index_size());
+            segment_group_pb.set_data_size(segment_group->data_size());
+            segment_group_pb.set_num_rows(segment_group->num_rows());
+            const std::vector<KeyRange>* column_statistics = &(segment_group->get_column_statistics());
+            if (column_statistics != nullptr) {
+                for (size_t i = 0; i < column_statistics->size(); ++i) {
+                    ColumnPruning* column_pruning = segment_group_pb.add_column_pruning();
+                    column_pruning->set_min(column_statistics->at(i).first->to_string());
+                    column_pruning->set_max(column_statistics->at(i).second->to_string());
+                    column_pruning->set_null_flag(column_statistics->at(i).first->is_null());
+                }
             }
+            segment_group_pb.set_empty(segment_group->empty());","[{'comment': 'std::reinterpret_cast<> is better to convert from base class to inherited class.', 'commenter': 'chaoyli'}, {'comment': 'OK， I will fix it', 'commenter': 'kangpinghuang'}]"
531,gensrc/proto/data.proto,"@@ -19,6 +19,11 @@ syntax=""proto2"";
 
 package doris;
 
+message PQueryStatistic {
+    optional int64 cpu_by_row = 1;
+    optional int64 io_by_byte = 2;","[{'comment': '```suggestion\r\n    optional int64 scan_rows = 2;\r\n```', 'commenter': 'imay'}]"
531,gensrc/proto/data.proto,"@@ -19,6 +19,11 @@ syntax=""proto2"";
 
 package doris;
 
+message PQueryStatistic {
+    optional int64 cpu_by_row = 1;","[{'comment': 'process_rows', 'commenter': 'imay'}]"
531,fe/src/main/java/org/apache/doris/rpc/PQueryStatistic.java,"@@ -0,0 +1,29 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.rpc;
+
+import com.baidu.bjf.remoting.protobuf.annotation.Protobuf;
+import com.baidu.bjf.remoting.protobuf.annotation.ProtobufClass;
+
+@ProtobufClass
+public class PQueryStatistic {
+    @Protobuf(order = 1, required = false)
+    public long cpu;
+    @Protobuf(order = 2, required = false)
+    public long io;","[{'comment': '```suggestion\r\n    public long scanRows;\r\n```', 'commenter': 'imay'}]"
531,fe/src/main/java/org/apache/doris/rpc/PQueryStatistic.java,"@@ -0,0 +1,29 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.rpc;
+
+import com.baidu.bjf.remoting.protobuf.annotation.Protobuf;
+import com.baidu.bjf.remoting.protobuf.annotation.ProtobufClass;
+
+@ProtobufClass
+public class PQueryStatistic {
+    @Protobuf(order = 1, required = false)
+    public long cpu;","[{'comment': 'processRows', 'commenter': 'imay'}, {'comment': 'ok, i will fix it.', 'commenter': 'chenhao7253886'}]"
531,fe/src/main/java/org/apache/doris/planner/Planner.java,"@@ -230,4 +234,38 @@ private PlanNode addUnassignedConjuncts(Analyzer analyzer, PlanNode root)
         Preconditions.checkState(selectNode.hasValidStats());
         return selectNode;
     }
+
+    private static class QueryStatisticTransferOptimizer {
+        private final PlanFragment root;
+        
+        public QueryStatisticTransferOptimizer(PlanFragment root) {
+            Preconditions.checkNotNull(root);
+            this.root = root;
+        }
+
+        public void optimizeTransferQueryStatistic() {
+            optimizeTransferQueryStatistic(root, null);
+        }
+
+        private void optimizeTransferQueryStatistic(PlanFragment fragment, PlanFragment parent) {
+            if (parent != null && hasLimit(parent.getPlanRoot())) {
+                fragment.setTransferQueryStatisticWithEveryBatch(true);
+            }
+            for (PlanFragment child : fragment.getChildren()) {
+                optimizeTransferQueryStatistic(child, fragment);
+            }
+        }
+
+        private boolean hasLimit(PlanNode planNode) {","[{'comment': ""it should judge limit by it's immediate  parent.  i will fix it."", 'commenter': 'chenhao7253886'}]"
531,be/src/runtime/data_stream_sender.cpp,"@@ -305,11 +318,17 @@ DataStreamSender::DataStreamSender(
             || sink.output_partition.type == TPartitionType::RANGE_PARTITIONED);
     // TODO: use something like google3's linked_ptr here (scoped_ptr isn't copyable)
     for (int i = 0; i < destinations.size(); ++i) {
+        bool is_transfer_chain = false;","[{'comment': '```suggestion\r\n        bool is_transfer_stats = (i == 0);\r\n```', 'commenter': 'imay'}, {'comment': 'ok, i will modify it.', 'commenter': 'chenhao7253886'}]"
531,be/src/service/internal_service.cpp,"@@ -54,6 +54,18 @@ void PInternalServiceImpl<T>::transmit_data(google::protobuf::RpcController* cnt
             request->be_number(), request->packet_seq(),
             eos ? nullptr : &done);
     }
+
+    if (request->has_query_statistic()) {","[{'comment': 'you should update query statistics in `add_data` function', 'commenter': 'imay'}, {'comment': ""Now add_data is used for add batch,  when eos is true and request does't contain batch,  add_data will not be called, but request may contains query statistics, so it is more simple to handle query statistics update separately."", 'commenter': 'chenhao7253886'}]"
531,fe/src/main/java/org/apache/doris/qe/ConnectProcessor.java,"@@ -92,12 +93,16 @@ private void handlePing() {
         ctx.getState().setOk();
     }
 
-    private void auditAfterExec(String origStmt, StatementBase parsedStmt) {
+    private void auditAfterExec(String origStmt, StatementBase parsedStmt,
+                StmtExecutor.QueryStatistic queryStatistic) {
         // slow query
         long elapseMs = System.currentTimeMillis() - ctx.getStartTime();
         // query state log
         ctx.getAuditBuilder().put(""state"", ctx.getState());
         ctx.getAuditBuilder().put(""time"", elapseMs);
+        Preconditions.checkNotNull(queryStatistic); 
+        ctx.getAuditBuilder().put(""cpu"", queryStatistic.getFormattingCpu());
+        ctx.getAuditBuilder().put(""io"", queryStatistic.getFormattingIo());","[{'comment': ""use scanRows, processRows instead of cpu and io. because a man can't know what's io/cpu stand for "", 'commenter': 'imay'}, {'comment': 'ok, i will fix it.', 'commenter': 'chenhao7253886'}]"
531,be/src/exec/analytic_eval_node.h,"@@ -68,6 +68,7 @@ class AnalyticEvalNode : public ExecNode {
     virtual Status prepare(RuntimeState* state);
     virtual Status open(RuntimeState* state);
     virtual Status get_next(RuntimeState* state, RowBatch* row_batch, bool* eos);
+    virtual Status collect_query_statistics(QueryStatistics* statistics);","[{'comment': '```suggestion\r\n    Status collect_query_statistics(QueryStatistics* statistics) override;\r\n```', 'commenter': 'imay'}]"
531,be/src/runtime/query_statistics.h,"@@ -0,0 +1,220 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#ifndef DORIS_BE_EXEC_QUERY_STATISTICS_H
+#define DORIS_BE_EXEC_QUERY_STATISTICS_H
+
+#include ""common/atomic.h""
+#include ""gen_cpp/data.pb.h""
+#include ""util/spinlock.h""
+
+namespace doris {
+
+// This is responsible for collecting query statistics, usually it consist of 
+// two parts, one is current fragment or plan's statistics, the other is sub fragment
+// or plan's statistics.
+class QueryStatistics {
+public:
+
+    class Statistics {
+    public:
+
+        Statistics() : process_rows(0), scan_bytes(0) {
+        }
+
+        void add(const Statistics& other) {
+            process_rows += other.process_rows;
+            scan_bytes += other.scan_bytes;","[{'comment': 'scan rows？', 'commenter': 'imay'}]"
531,be/src/runtime/query_statistics.h,"@@ -0,0 +1,220 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#ifndef DORIS_BE_EXEC_QUERY_STATISTICS_H
+#define DORIS_BE_EXEC_QUERY_STATISTICS_H
+
+#include ""common/atomic.h""
+#include ""gen_cpp/data.pb.h""
+#include ""util/spinlock.h""
+
+namespace doris {
+
+// This is responsible for collecting query statistics, usually it consist of 
+// two parts, one is current fragment or plan's statistics, the other is sub fragment
+// or plan's statistics.
+class QueryStatistics {
+public:
+
+    class Statistics {
+    public:
+
+        Statistics() : process_rows(0), scan_bytes(0) {
+        }
+
+        void add(const Statistics& other) {
+            process_rows += other.process_rows;
+            scan_bytes += other.scan_bytes;
+        }
+    
+        void add(const PQueryStatistics& other) {
+            process_rows += other.process_rows();
+            scan_bytes += other.scan_bytes();
+        }    
+
+        void reset() {
+            process_rows = 0;
+            scan_bytes = 0;
+        }
+
+        void set_scan_bytes(int64_t scan_bytes) {
+            this->scan_bytes = scan_bytes;
+        }
+
+        void add_scan_bytes(int64_t scan_bytes) {
+            this->scan_bytes += scan_bytes;
+        }
+
+        long get_scan_bytes() {
+            return scan_bytes;        
+        }
+
+        void set_process_rows(int64_t process_rows) {
+            this->process_rows = process_rows;
+        }
+
+        void add_process_rows(int64_t process_rows) {
+            this->process_rows += process_rows;
+        }
+
+        long get_process_rows() {
+            return process_rows;
+        }
+
+        void serialize(PQueryStatistics* statistics) {
+            DCHECK(statistics != nullptr);
+            statistics->set_process_rows(process_rows);
+            statistics->set_scan_bytes(scan_bytes);
+        }
+
+        void deserialize(const PQueryStatistics& statistics) {
+            process_rows = statistics.process_rows();
+            scan_bytes = statistics.scan_bytes();
+         }
+
+    private:
+
+        long process_rows;
+        long scan_bytes;
+    };
+
+    QueryStatistics() {
+    }
+
+    // It can't be called by this and other at the same time in different threads.
+    // Otherwise it will cause dead lock.
+    void add(QueryStatistics* other) {
+        boost::lock_guard<SpinLock> l(_lock);
+        boost::lock_guard<SpinLock> other_l(other->_lock);
+        auto other_iter = other->_statistics_map.begin();
+        while (other_iter != other->_statistics_map.end()) {
+            auto iter = _statistics_map.find(other_iter->first);
+            Statistics* statistics = nullptr;          
+            if (iter == _statistics_map.end()) {
+                statistics = new Statistics();
+                _statistics_map[other_iter->first] = statistics;
+            } else {
+                statistics = iter->second;
+            }
+            Statistics* other_statistics = other_iter->second;
+            statistics->add(*other_statistics); 
+            other_iter++;
+        }
+    }
+
+    void add_process_rows(long process_rows) {
+       boost::lock_guard<SpinLock> l(_lock);
+       auto statistics = find(DEFAULT_SENDER_ID);
+       statistics->add_process_rows(process_rows);
+    }   
+
+    void add_scan_bytes(long scan_bytes) {
+       boost::lock_guard<SpinLock> l(_lock);
+       auto statistics = find(DEFAULT_SENDER_ID);
+       statistics->add_scan_bytes(scan_bytes);
+    }
+
+    void deserialize(const PQueryStatistics& other, int sender_id) {
+        boost::lock_guard<SpinLock> l(_lock);
+        auto statistics = find(sender_id);
+        statistics->deserialize(other);
+    }
+
+    void serialize(PQueryStatistics* statistics) { 
+        DCHECK(statistics != nullptr);
+        boost::lock_guard<SpinLock> l(_lock);","[{'comment': 'boost::lock_guard -> std::lock_guard', 'commenter': 'imay'}]"
531,be/src/exec/exchange_node.h,"@@ -109,6 +111,9 @@ class ExchangeNode : public ExecNode {
     int64_t _num_rows_skipped;
 
     RuntimeProfile::Counter* _merge_rows_counter;
+
+    // Query statistics from sub plan.
+    boost::scoped_ptr<QueryStatistics> _sub_plan_statistics;","[{'comment': 'use std first ', 'commenter': 'imay'}, {'comment': ""std does't include scoped_ptr."", 'commenter': 'chenhao7253886'}]"
531,be/src/runtime/query_statistics.h,"@@ -0,0 +1,220 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#ifndef DORIS_BE_EXEC_QUERY_STATISTICS_H
+#define DORIS_BE_EXEC_QUERY_STATISTICS_H
+
+#include ""common/atomic.h""
+#include ""gen_cpp/data.pb.h""
+#include ""util/spinlock.h""
+
+namespace doris {
+
+// This is responsible for collecting query statistics, usually it consist of 
+// two parts, one is current fragment or plan's statistics, the other is sub fragment
+// or plan's statistics.
+class QueryStatistics {
+public:
+
+    class Statistics {
+    public:
+
+        Statistics() : process_rows(0), scan_bytes(0) {
+        }
+
+        void add(const Statistics& other) {
+            process_rows += other.process_rows;
+            scan_bytes += other.scan_bytes;
+        }
+    
+        void add(const PQueryStatistics& other) {
+            process_rows += other.process_rows();
+            scan_bytes += other.scan_bytes();
+        }    
+
+        void reset() {
+            process_rows = 0;
+            scan_bytes = 0;
+        }
+
+        void set_scan_bytes(int64_t scan_bytes) {
+            this->scan_bytes = scan_bytes;
+        }
+
+        void add_scan_bytes(int64_t scan_bytes) {
+            this->scan_bytes += scan_bytes;
+        }
+
+        long get_scan_bytes() {
+            return scan_bytes;        
+        }
+
+        void set_process_rows(int64_t process_rows) {
+            this->process_rows = process_rows;
+        }
+
+        void add_process_rows(int64_t process_rows) {
+            this->process_rows += process_rows;
+        }
+
+        long get_process_rows() {
+            return process_rows;
+        }
+
+        void serialize(PQueryStatistics* statistics) {
+            DCHECK(statistics != nullptr);
+            statistics->set_process_rows(process_rows);
+            statistics->set_scan_bytes(scan_bytes);
+        }
+
+        void deserialize(const PQueryStatistics& statistics) {
+            process_rows = statistics.process_rows();
+            scan_bytes = statistics.scan_bytes();
+         }
+
+    private:
+
+        long process_rows;
+        long scan_bytes;
+    };
+
+    QueryStatistics() {
+    }
+
+    // It can't be called by this and other at the same time in different threads.
+    // Otherwise it will cause dead lock.
+    void add(QueryStatistics* other) {
+        boost::lock_guard<SpinLock> l(_lock);
+        boost::lock_guard<SpinLock> other_l(other->_lock);
+        auto other_iter = other->_statistics_map.begin();
+        while (other_iter != other->_statistics_map.end()) {
+            auto iter = _statistics_map.find(other_iter->first);
+            Statistics* statistics = nullptr;          
+            if (iter == _statistics_map.end()) {
+                statistics = new Statistics();
+                _statistics_map[other_iter->first] = statistics;
+            } else {
+                statistics = iter->second;
+            }
+            Statistics* other_statistics = other_iter->second;
+            statistics->add(*other_statistics); 
+            other_iter++;
+        }
+    }
+
+    void add_process_rows(long process_rows) {
+       boost::lock_guard<SpinLock> l(_lock);
+       auto statistics = find(DEFAULT_SENDER_ID);
+       statistics->add_process_rows(process_rows);
+    }   
+
+    void add_scan_bytes(long scan_bytes) {
+       boost::lock_guard<SpinLock> l(_lock);
+       auto statistics = find(DEFAULT_SENDER_ID);
+       statistics->add_scan_bytes(scan_bytes);
+    }
+
+    void deserialize(const PQueryStatistics& other, int sender_id) {
+        boost::lock_guard<SpinLock> l(_lock);
+        auto statistics = find(sender_id);
+        statistics->deserialize(other);
+    }
+
+    void serialize(PQueryStatistics* statistics) { 
+        DCHECK(statistics != nullptr);
+        boost::lock_guard<SpinLock> l(_lock);
+        Statistics total_statistics = get_total_statistics();
+        total_statistics.serialize(statistics);
+    }
+
+    long get_process_rows() {
+        boost::lock_guard<SpinLock> l(_lock);","[{'comment': 'do you need a lock here?', 'commenter': 'imay'}, {'comment': 'i will fix it.', 'commenter': 'chenhao7253886'}]"
531,fe/src/main/java/org/apache/doris/qe/StmtExecutor.java,"@@ -774,4 +787,40 @@ private void handleExportStmt() throws Exception {
         ExportStmt exportStmt = (ExportStmt) parsedStmt;
         context.getCatalog().getExportMgr().addExportJob(exportStmt);
     }
+
+    public QueryStatistics getQueryStatisticsForAuditLog() {
+        if (statisticsForAuditLog == null) {
+            statisticsForAuditLog = new QueryStatistics();
+        }
+        return statisticsForAuditLog;
+    }
+
+    public static class QueryStatistics {","[{'comment': 'Can  PQueryStatistics replace this class?', 'commenter': 'imay'}, {'comment': 'yes, i will replace this with PQueryStatistics.', 'commenter': 'chenhao7253886'}]"
531,fe/src/main/java/org/apache/doris/qe/RowBatch.java,"@@ -0,0 +1,55 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.qe;
+
+import org.apache.doris.rpc.PQueryStatistics;
+import org.apache.doris.thrift.TResultBatch;
+
+public final class RowBatch {
+    private TResultBatch batch;
+    private PQueryStatistics statistics;
+    private boolean eos;","[{'comment': 'why not use batch.eos?', 'commenter': 'imay'}, {'comment': ""TResultBatch does't include EOS member, but it's wapper PFetchDataResult."", 'commenter': 'chenhao7253886'}]"
531,fe/src/main/java/org/apache/doris/qe/StmtExecutor.java,"@@ -537,26 +542,34 @@ private void handleQueryStmt() throws Exception {
         // so We need to send fields after first batch arrived
 
         // send result
-        TResultBatch batch;
+        RowBatch batch;
         MysqlChannel channel = context.getMysqlChannel();
         boolean isSendFields = false;
-        while ((batch = coord.getNext()) != null) {
+      
+        while ((batch = coord.getNext()) != null && !batch.isEos()) {
             if (!isSendFields) {
                 sendFields(queryStmt.getColLabels(), queryStmt.getResultExprs());
             }
             isSendFields = true;
-
-            for (ByteBuffer row : batch.getRows()) {
+            for (ByteBuffer row : batch.getBatch().getRows()) {
                 channel.sendOnePacket(row);
             }
-            context.updateReturnRows(batch.getRows().size());
+            context.updateReturnRows(batch.getBatch().getRows().size());
         }
+        setQueryStatisticsForAuditLog(batch);
         if (!isSendFields) {
             sendFields(queryStmt.getColLabels(), queryStmt.getResultExprs());
         }
         context.getState().setEof();
     }
 
+    private void setQueryStatisticsForAuditLog(RowBatch batch) {
+        if (batch != null) {
+            final PQueryStatistics statistics = batch.getQueryStatistics();","[{'comment': '`batch.getQueryStatistics()` may return nullptr?', 'commenter': 'imay'}]"
531,fe/src/main/java/org/apache/doris/qe/ConnectProcessor.java,"@@ -92,12 +93,16 @@ private void handlePing() {
         ctx.getState().setOk();
     }
 
-    private void auditAfterExec(String origStmt, StatementBase parsedStmt) {
+    private void auditAfterExec(String origStmt, StatementBase parsedStmt,
+                StmtExecutor.QueryStatistics statistics) {
         // slow query
         long elapseMs = System.currentTimeMillis() - ctx.getStartTime();
         // query state log
         ctx.getAuditBuilder().put(""state"", ctx.getState());
         ctx.getAuditBuilder().put(""time"", elapseMs);
+        Preconditions.checkNotNull(statistics); 
+        ctx.getAuditBuilder().put(""ScanRows"", statistics.getFormattingScanRows());
+        ctx.getAuditBuilder().put(""ScanRawData"", statistics.getFormattingScanBytes());","[{'comment': ""I think it's good to format printing information in `statistics.getFormattingScanRows`.\r\nIt should be that caller formats returned value as its wanted format."", 'commenter': 'imay'}]"
531,be/src/runtime/query_statistics.h,"@@ -0,0 +1,117 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#ifndef DORIS_BE_EXEC_QUERY_STATISTICS_H
+#define DORIS_BE_EXEC_QUERY_STATISTICS_H
+
+#include <mutex>
+
+#include ""gen_cpp/data.pb.h""
+#include ""util/spinlock.h""
+
+namespace doris {
+
+// This is responsible for collecting query statistics, usually it consists of 
+// two parts, one is current fragment or plan's statistics, the other is sub fragment
+// or plan's statistics and QueryStatisticsRecvr is responsible for collecting it.
+class QueryStatistics {
+public:
+
+    QueryStatistics() : scan_rows(0), scan_bytes(0) {
+    }
+
+    void add(const QueryStatistics& other) {
+        scan_rows += other.scan_rows;
+        scan_bytes += other.scan_bytes;
+    }
+
+    void add_scan_rows(long scan_rows) {
+        this->scan_rows += scan_rows;
+    }
+
+    void add_scan_bytes(long scan_bytes) {
+        this->scan_bytes += scan_bytes;
+    }
+
+    void clear() {
+        scan_rows = 0;
+        scan_bytes = 0;
+    }
+
+    void serialize(PQueryStatistics* statistics) {
+        DCHECK(statistics != nullptr);
+        statistics->set_scan_rows(scan_rows);
+        statistics->set_scan_bytes(scan_bytes);
+    }
+
+    void deserialize(const PQueryStatistics& statistics) {
+        scan_rows = statistics.scan_rows();
+        scan_bytes = statistics.scan_bytes();
+    }
+
+private:
+
+    long scan_rows;
+    long scan_bytes;","[{'comment': 'int64_t', 'commenter': 'imay'}]"
531,be/src/runtime/query_statistics.h,"@@ -0,0 +1,117 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#ifndef DORIS_BE_EXEC_QUERY_STATISTICS_H
+#define DORIS_BE_EXEC_QUERY_STATISTICS_H
+
+#include <mutex>
+
+#include ""gen_cpp/data.pb.h""
+#include ""util/spinlock.h""
+
+namespace doris {
+
+// This is responsible for collecting query statistics, usually it consists of 
+// two parts, one is current fragment or plan's statistics, the other is sub fragment
+// or plan's statistics and QueryStatisticsRecvr is responsible for collecting it.
+class QueryStatistics {","[{'comment': 'change it to struct', 'commenter': 'imay'}]"
531,be/src/runtime/query_statistics.h,"@@ -0,0 +1,117 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#ifndef DORIS_BE_EXEC_QUERY_STATISTICS_H
+#define DORIS_BE_EXEC_QUERY_STATISTICS_H
+
+#include <mutex>
+
+#include ""gen_cpp/data.pb.h""
+#include ""util/spinlock.h""
+
+namespace doris {
+
+// This is responsible for collecting query statistics, usually it consists of 
+// two parts, one is current fragment or plan's statistics, the other is sub fragment
+// or plan's statistics and QueryStatisticsRecvr is responsible for collecting it.
+class QueryStatistics {
+public:
+
+    QueryStatistics() : scan_rows(0), scan_bytes(0) {
+    }
+
+    void add(const QueryStatistics& other) {
+        scan_rows += other.scan_rows;
+        scan_bytes += other.scan_bytes;
+    }
+
+    void add_scan_rows(long scan_rows) {
+        this->scan_rows += scan_rows;
+    }
+
+    void add_scan_bytes(long scan_bytes) {
+        this->scan_bytes += scan_bytes;
+    }
+
+    void clear() {
+        scan_rows = 0;
+        scan_bytes = 0;
+    }
+
+    void serialize(PQueryStatistics* statistics) {","[{'comment': 'to_pb', 'commenter': 'imay'}]"
531,be/src/runtime/query_statistics.h,"@@ -0,0 +1,117 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#ifndef DORIS_BE_EXEC_QUERY_STATISTICS_H
+#define DORIS_BE_EXEC_QUERY_STATISTICS_H
+
+#include <mutex>
+
+#include ""gen_cpp/data.pb.h""
+#include ""util/spinlock.h""
+
+namespace doris {
+
+// This is responsible for collecting query statistics, usually it consists of 
+// two parts, one is current fragment or plan's statistics, the other is sub fragment
+// or plan's statistics and QueryStatisticsRecvr is responsible for collecting it.
+class QueryStatistics {
+public:
+
+    QueryStatistics() : scan_rows(0), scan_bytes(0) {
+    }
+
+    void add(const QueryStatistics& other) {
+        scan_rows += other.scan_rows;
+        scan_bytes += other.scan_bytes;
+    }
+
+    void add_scan_rows(long scan_rows) {
+        this->scan_rows += scan_rows;
+    }
+
+    void add_scan_bytes(long scan_bytes) {
+        this->scan_bytes += scan_bytes;
+    }
+
+    void clear() {
+        scan_rows = 0;
+        scan_bytes = 0;
+    }
+
+    void serialize(PQueryStatistics* statistics) {
+        DCHECK(statistics != nullptr);
+        statistics->set_scan_rows(scan_rows);
+        statistics->set_scan_bytes(scan_bytes);
+    }
+
+    void deserialize(const PQueryStatistics& statistics) {","[{'comment': 'from_pb', 'commenter': 'imay'}]"
531,be/src/exec/exchange_node.cpp,"@@ -95,6 +96,12 @@ Status ExchangeNode::open(RuntimeState* state) {
     return Status::OK;
 }
 
+Status ExchangeNode::collect_query_statistics(QueryStatistics* statistics) {
+    RETURN_IF_ERROR(ExecNode::collect_query_statistics(statistics));
+    _sub_plan_query_statistics_recvr->add_to(statistics);","[{'comment': 'statistics.merge()', 'commenter': 'imay'}]"
532,be/src/olap/reader.cpp,"@@ -503,43 +485,93 @@ OLAPStatus Reader::_capture_rs_readers(const ReaderParams& read_params) {
         is_using_cache = false;
     }
 
-    for (auto i_data: *rs_readers) {
-        // skip empty version
-        if (i_data->empty() || i_data->zero_num_rows()) {
-            continue;
-        }
-        i_data->set_delete_handler(_delete_handler);
-        i_data->set_read_params(_return_columns,
-                                _load_bf_columns,
-                                _conditions,
-                                _col_predicates,
-                                _keys_param.start_keys,
-                                _keys_param.end_keys,
-                                is_using_cache,
-                                read_params.runtime_state);
-        if (i_data->delta_pruning_filter()) {
-            VLOG(3) << ""filter delta in query in condition:""
-                    << i_data->version().first << "", "" << i_data->version().second;
-            _stats.rows_stats_filtered += i_data->num_rows();
-            continue;
+    bool eof = false;
+    RowCursor* start_key = nullptr;
+    RowCursor* end_key = nullptr;
+    bool is_lower_key_included = false;
+    bool is_upper_key_included = false;
+    for (int i = 0; i < _keys_param.start_keys.size(); ++i) {
+        start_key = _keys_param.start_keys[i];
+        end_key = _keys_param.end_keys[i];
+        if (_keys_param.end_range.compare(""lt"") == 0) {
+            is_upper_key_included = false;
+        } else if (_keys_param.end_range.compare(""le"")) {
+            is_upper_key_included = true;
+        } else {
+            LOG(WARNING) << ""reader params end_range is error. ""
+                         << ""range="" << _keys_param.to_string();
+            return OLAP_ERR_READER_GET_ITERATOR_ERROR;
         }
-        int ret = i_data->delete_pruning_filter();
-        if (ret == DEL_SATISFIED) {
-            VLOG(3) << ""filter delta in delete predicate:""
-                    << i_data->version().first << "", "" << i_data->version().second;
-            _stats.rows_del_filtered += i_data->num_rows();
-            continue;
-        } else if (ret == DEL_PARTIAL_SATISFIED) {
-            VLOG(3) << ""filter delta partially in delete predicate:""
-                    << i_data->version().first << "", "" << i_data->version().second;
-            i_data->set_delete_status(DEL_PARTIAL_SATISFIED);
+
+        if (_keys_param.range.compare(""gt"") == 0) {
+            if (end_key != nullptr && start_key->cmp(*end_key) >= 0) {
+                VLOG(3) << ""return EOF when range="" << _keys_param.range
+                        << "", start_key="" << start_key->to_string()
+                        << "", end_key="" << end_key->to_string();
+                eof = true;
+                break;
+            }
+            is_lower_key_included = true;
+        } else if (_keys_param.range.compare(""ge"") == 0) {
+            if (end_key != nullptr && start_key->cmp(*end_key) > 0) {
+                VLOG(3) << ""return EOF when range="" << _keys_param.range
+                        << "", start_key="" << start_key->to_string()
+                        << "", end_key="" << end_key->to_string();
+                eof = true;
+                break;
+            }
+            is_lower_key_included = false;
+        } else if (0 == _keys_param.range.compare(""eq"")) {
+            is_lower_key_included = false;
+            is_upper_key_included = true;
         } else {
-            VLOG(3) << ""not filter delta in delete predicate:""
-                    << i_data->version().first << "", "" << i_data->version().second;
-            i_data->set_delete_status(DEL_NOT_SATISFIED);
+            LOG(WARNING) << ""reader params range is error. ""
+                         << ""range="" << _keys_param.to_string();
+            return OLAP_ERR_READER_GET_ITERATOR_ERROR;
+        }
+        _is_lower_keys_included.push_back(is_lower_key_included);
+        _is_upper_keys_included.push_back(is_upper_key_included);
+    }
+
+    if (eof) { return OLAP_SUCCESS; }
+
+    RowsetReaderContextBuilder context_builder;
+    context_builder.set_tablet_schema(&_tablet->tablet_schema())
+                   .set_return_columns(&_return_columns)
+                   .set_load_bf_columns(&_load_bf_columns)
+                   .set_conditions(&_conditions)
+                   .set_predicates(&_col_predicates)
+                   .set_lower_bound_keys(&_keys_param.start_keys)
+                   .set_is_lower_keys_included(&_is_lower_keys_included)
+                   .set_upper_bound_keys(&_keys_param.end_keys)
+                   .set_is_upper_keys_included(&_is_upper_keys_included)
+                   .set_delete_handler(&_delete_handler)
+                   .set_stats(&_stats)
+                   .set_is_using_cache(is_using_cache)
+                   .set_runtime_state(read_params.runtime_state);
+    ReaderContext context = context_builder.build();
+    for (auto& rs_reader : *rs_readers) {
+        rs_reader->init(&context);
+        _rs_readers.push_back(rs_reader);
+    }
+
+    for (auto& rs_reader : _rs_readers) {
+        RowBlock* block = nullptr;
+        auto res = rs_reader->next_block(&block);
+        if (res == OLAP_SUCCESS) {
+            res = _collect_iter->add_child(rs_reader, block);
+            if (res != OLAP_SUCCESS && res != OLAP_ERR_DATA_EOF) {
+                LOG(WARNING) << ""failed to add child to iterator"";
+                return res;
+            } else if (res == OLAP_ERR_DATA_EOF) {
+                continue;
+            } else {
+                LOG(WARNING) << ""prepare block failed, res="" << res;","[{'comment': ""I think there is a bug here. 'else' here stands for OLAP_SUCCESS"", 'commenter': 'kangpinghuang'}, {'comment': 'OK, I will fixed it.', 'commenter': 'chaoyli'}]"
536,be/src/olap/push_handler.cpp,"@@ -277,22 +272,35 @@ void PushHandler::_get_tablet_infos(
 OLAPStatus PushHandler::_convert(
         TabletSharedPtr curr_tablet,
         TabletSharedPtr new_tablet,
-        Indices* curr_olap_indices,
-        Indices* new_olap_indices,
+        std::vector<RowsetSharedPtr>* cur_rowsets,
+        std::vector<RowsetSharedPtr>* related_rowsets,
         AlterTabletType alter_tablet_type) {
     OLAPStatus res = OLAP_SUCCESS;
     RowCursor row;
     BinaryFile raw_file;
     IBinaryReader* reader = NULL;
     ColumnDataWriter* writer = NULL;","[{'comment': 'ColumnDataWriter should not be left exposed.', 'commenter': 'chaoyli'}]"
536,be/src/olap/push_handler.cpp,"@@ -73,13 +125,13 @@ OLAPStatus PushHandler::process_realtime_push(
 
         // if push finished, report success to fe
         if (tablet->has_pending_data(request.transaction_id)) {","[{'comment': 'Tablet will has no pending message. You should get it from transaction manager.', 'commenter': 'chaoyli'}]"
536,be/src/olap/push_handler.cpp,"@@ -54,10 +57,59 @@ OLAPStatus PushHandler::process_realtime_push(
 
     OLAPStatus res = OLAP_SUCCESS;
     _request = request;
-    vector<TabletVars> tablet_infos(1);
-    tablet_infos[0].tablet = tablet;
-    AlterTabletType alter_tablet_type;
+    vector<TabletVars> tablet_vars(1);
+    tablet_vars[0].tablet = tablet;
+    res = _do_realtime_push(tablet, request, push_type, &tablet_vars, tablet_info_vec);
+    // if transaction existed in engine but push not finished, not report to fe
+    if (res == OLAP_ERR_PUSH_TRANSACTION_ALREADY_EXIST) {
+        OLAP_LOG_WARNING(""find transaction existed when realtime push, not report. "",","[{'comment': 'LOG(WARNING)', 'commenter': 'chaoyli'}, {'comment': 'OK, I will fix it', 'commenter': 'kangpinghuang'}]"
536,be/src/olap/push_handler.cpp,"@@ -401,46 +394,42 @@ OLAPStatus PushHandler::_convert(
             // Convert from raw to delta
             VLOG(3) << ""start to convert row file to delta."";
             while (!reader->eof()) {
-                if (OLAP_SUCCESS != (res = writer->attached_by(&row))) {
-                    OLAP_LOG_WARNING(
-                            ""fail to attach row to writer. [res=%d tablet='%s' read_rows=%u]"",
-                            res, curr_tablet->full_name().c_str(), num_rows);
-                    break;
-                }
-
-                res = reader->next(&row, writer->mem_pool());
+                res = reader->next(&row, rowset_builder->mem_pool());
                 if (OLAP_SUCCESS != res) {
-                    OLAP_LOG_WARNING(""read next row failed. [res=%d read_rows=%u]"",
-                                     res, num_rows);
+                    LOG(WARNING) << ""read next row failed.""
+                                 << "" res="" << res
+                                 << "" read_rows="" << num_rows;
                     break;
                 } else {
-                    writer->next(row);
+                    if (OLAP_SUCCESS != (res = rowset_builder->add_row(&row))) {","[{'comment': 'AlphaRowsetBuilder add_row encapsulates attached_by function.\r\nattached by function should be called previous by reader->next().', 'commenter': 'chaoyli'}, {'comment': 'OK, I will fix it', 'commenter': 'kangpinghuang'}]"
536,be/src/olap/push_handler.cpp,"@@ -454,13 +443,13 @@ OLAPStatus PushHandler::_convert(
                     break;
                 }
             }
-
-            SchemaChangeHandler schema_change;
-            res = schema_change.schema_version_convert(
-                    curr_tablet,
-                    new_tablet,
-                    curr_olap_indices,
-                    new_olap_indices);
+            // TODO(hkp): modify schema change
+            //SchemaChangeHandler schema_change;
+            //res = schema_change.schema_version_convert(
+            //        curr_tablet,
+            //        new_tablet,
+            //        curr_olap_indices,
+            //        new_olap_indices);
             if (res != OLAP_SUCCESS) {
                 OLAP_LOG_WARNING(""failed to change schema version for delta.""","[{'comment': 'LOG(WARNING)', 'commenter': 'chaoyli'}]"
536,be/src/olap/push_handler.h,"@@ -81,6 +76,15 @@ class PushHandler {
             const std::vector<TabletVars>& tablet_infos,
             std::vector<TTabletInfo>* tablet_info_vec);
 
+    OLAPStatus _do_realtime_push(","[{'comment': 'Using streaming_ingestion instead of realtime_push.', 'commenter': 'chaoyli'}]"
536,be/src/olap/delta_writer.cpp,"@@ -115,9 +121,9 @@ OLAPStatus DeltaWriter::init() {
             .set_load_id(_req.load_id);
     RowsetWriterContext writer_context = context_builder.build();
 
-    // TODO: new RowsetWriter according to tablet storage type
-    _rowset_writer.reset(new AlphaRowsetWriter());
-    OLAPStatus status = _rowset_writer->init(writer_context);
+    // TODO: new RowsetBuilder according to tablet storage type
+    _rowset_builder.reset(new AlphaRowsetBuilder());
+    status = _rowset_builder->init(builder_context);","[{'comment': '_rowset_writer', 'commenter': 'chaoyli'}]"
536,be/src/olap/push_handler.cpp,"@@ -330,69 +339,55 @@ OLAPStatus PushHandler::_convert(
             }
         }
 
-        // 2. New SegmentGroup of curr_tablet for current push
-        VLOG(3) << ""init SegmentGroup."";
+        // 2. init RowsetBuilder of cur_tablet for current push
+        VLOG(3) << ""init RowsetBuilder."";
 
         if (_request.__isset.transaction_id) {
-            // create pending data dir
-            string dir_path = curr_tablet->construct_pending_data_dir_path();
-            if (!check_dir_existed(dir_path) && (res = create_dirs(dir_path)) != OLAP_SUCCESS) {
-                if (!check_dir_existed(dir_path)) {
-                    OLAP_LOG_WARNING(""fail to create pending dir. [res=%d tablet=%s]"",
-                                     res, curr_tablet->full_name().c_str());
-                    break;
-                }
-            }
-
-            delta_segment_group = new(std::nothrow) SegmentGroup(
-                curr_tablet->tablet_id(),
-                0,
-                curr_tablet->tablet_schema(),
-                curr_tablet->num_key_fields(),
-                curr_tablet->num_short_key_fields(),
-                curr_tablet->num_rows_per_row_block(),
-                curr_tablet->rowset_path_prefix(),
-                (_request.push_type == TPushType::LOAD_DELETE),
-                0, 0, true, _request.partition_id, _request.transaction_id);
+            RowsetBuilderContextBuilder context_builder;","[{'comment': 'RowsetWriterContextBuilder', 'commenter': 'chaoyli'}]"
536,be/src/olap/push_handler.cpp,"@@ -54,10 +57,59 @@ OLAPStatus PushHandler::process_realtime_push(
 
     OLAPStatus res = OLAP_SUCCESS;
     _request = request;
-    vector<TabletVars> tablet_infos(1);
-    tablet_infos[0].tablet = tablet;
-    AlterTabletType alter_tablet_type;
+    vector<TabletVars> tablet_vars(1);
+    tablet_vars[0].tablet = tablet;
+    res = _do_streaming_ingestion(tablet, request, push_type, &tablet_vars, tablet_info_vec);
+    // if transaction existed in engine but push not finished, not report to fe
+    if (res == OLAP_ERR_PUSH_TRANSACTION_ALREADY_EXIST) {
+        LOG(WARNING) << ""find transaction existed when realtime push, not report. ""
+                     << ""[tablet="" << tablet->full_name()
+                     << "" partition_id="" << request.partition_id
+                     << ""transaction_id="" << request.transaction_id << ""]"";
+        return res;
+    }
+
+    if (res == OLAP_SUCCESS) {
+        if (tablet_info_vec != NULL) {
+            _get_tablet_infos(tablet_vars, tablet_info_vec);
+        }
+        LOG(INFO) << ""process realtime push successfully. ""
+                  << ""tablet="" << tablet->full_name()
+                  << "", partition_id="" << request.partition_id
+                  << "", transaction_id="" << request.transaction_id;
+    } else {
 
+        // error happens, clear
+        LOG(WARNING) << ""failed to process realtime push.""
+                     << "" table="" << tablet->full_name()
+                     << ""transaction_id="" << request.transaction_id;
+        for (TabletVars& tablet_var : tablet_vars) {
+            if (tablet_var.tablet.get() == NULL) {
+                continue;
+            }
+
+            StorageEngine::get_instance()->delete_transaction(","[{'comment': 'instance(), delete_txn()', 'commenter': 'chaoyli'}]"
536,be/src/olap/delta_writer.cpp,"@@ -96,8 +97,13 @@ OLAPStatus DeltaWriter::init() {
         }
     }
 
-    int32_t rowset_id = 0; // get rowset_id from id generator
-    RowsetWriterContextBuilder context_builder;
+    RowsetId rowset_id = 0; // get rowset_id from id generator
+    OLAPStatus status = RowsetIdGenerator::instance()->get_next_id(_tablet->data_dir(), &rowset_id);
+    if (status != OLAP_SUCCESS) {
+        LOG(WARNING) << ""generate rowset id failed, status:"" << status;
+        return OLAP_ERR_ROWSET_GENERATE_ID_FAILED;
+    }
+    RowsetBuilderContextBuilder context_builder;","[{'comment': 'Some interface has changed.', 'commenter': 'chaoyli'}]"
548,fe/src/main/java/org/apache/doris/clone/ColocateTableBalancer.java,"@@ -196,6 +196,11 @@ private synchronized void tryBalanceWhenBackendChange() {
 
         Set<Long> allGroupIds = colocateIndex.getAllGroupIds();
         for (Long groupId : allGroupIds) {
+            if (colocateIndex.isGroupBalancing(groupId)) {
+                LOG.warn(""colocate group {} is balancing"", groupId);","[{'comment': ""Change the log level from 'warn' to 'info'"", 'commenter': 'morningman'}, {'comment': 'OK', 'commenter': 'kangkaisen'}]"
548,fe/src/main/java/org/apache/doris/clone/ColocateTableBalancer.java,"@@ -587,6 +592,8 @@ private void balanceForBackendAdded(Long groupId, Database db, List<Long> addedB
                                 backends.add(cloneReplicaBackendId);
                             }
 
+                            Preconditions.checkState(replicateNum == backends.size());","[{'comment': '```suggestion\r\n                            Preconditions.checkState(replicateNum == backends.size(), replicateNum + "" vs. "" + backends.size());\r\n```\r\nBetter for debug', 'commenter': 'morningman'}, {'comment': 'OK', 'commenter': 'kangkaisen'}]"
548,fe/src/main/java/org/apache/doris/catalog/Catalog.java,"@@ -4033,6 +4033,8 @@ private void createTablets(String clusterName, MaterializedIndex index, ReplicaS
                     Replica replica = new Replica(replicaId, backendId, replicaState, version, versionHash);
                     tablet.addReplica(replica);
                 }
+
+                Preconditions.checkState(chosenBackendIds.size() == replicationNum);","[{'comment': '```suggestion\r\n                Preconditions.checkState(chosenBackendIds.size() == replicationNum, chosenBackendIds.size() + "" vs. ""+ replicationNum);\r\n```', 'commenter': 'morningman'}]"
562,fe/src/main/java/org/apache/doris/http/meta/ColocateMetaService.java,"@@ -0,0 +1,250 @@
+package org.apache.doris.http.meta;","[{'comment': 'license header', 'commenter': 'imay'}, {'comment': 'OK', 'commenter': 'kangkaisen'}]"
562,fe/src/main/java/org/apache/doris/http/common/DorisHttpObjectAggregator.java,"@@ -0,0 +1,62 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.http.common;
+
+import io.netty.channel.ChannelPipeline;
+import io.netty.handler.codec.http.HttpContent;
+import io.netty.handler.codec.http.HttpMessage;
+import io.netty.handler.codec.http.HttpObject;
+import io.netty.handler.codec.http.HttpObjectAggregator;
+import io.netty.handler.codec.http.HttpRequest;
+import io.netty.handler.codec.http.HttpUtil;
+
+/*
+ * don't handle 100-continue and chunked transfer-encoding http header
+ */
+public class DorisHttpObjectAggregator extends HttpObjectAggregator {
+    //the flag for aggregator whether has started
+    //in order not to handle chunked transfer-encoding header in {@link isContentMessage} method","[{'comment': '// the flag', 'commenter': 'imay'}, {'comment': '?', 'commenter': 'kangkaisen'}, {'comment': 'add a space after ""//"" ', 'commenter': 'imay'}, {'comment': 'OK', 'commenter': 'kangkaisen'}]"
562,fe/src/main/java/org/apache/doris/http/common/DorisHttpObjectAggregator.java,"@@ -0,0 +1,62 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.http.common;
+
+import io.netty.channel.ChannelPipeline;
+import io.netty.handler.codec.http.HttpContent;
+import io.netty.handler.codec.http.HttpMessage;
+import io.netty.handler.codec.http.HttpObject;
+import io.netty.handler.codec.http.HttpObjectAggregator;
+import io.netty.handler.codec.http.HttpRequest;
+import io.netty.handler.codec.http.HttpUtil;
+
+/*
+ * don't handle 100-continue and chunked transfer-encoding http header
+ */
+public class DorisHttpObjectAggregator extends HttpObjectAggregator {
+    // the flag for aggregator whether has started
+    // in order not to handle chunked transfer-encoding header in {@link isContentMessage} method","[{'comment': 'I\'m not sure why we can\'t handle chunked encoding. ""100-continue"" has problem with `LoadAction`, what about chunked', 'commenter': 'imay'}, {'comment': '`HttpObjectAggregator.beginAggregation` will remove chunked transfer-encoding header.\r\n\r\n`HttpObjectAggregator.finishAggregation`  will set content-length header.', 'commenter': 'kangkaisen'}, {'comment': 'what problem will it cause that aggregator replaces chucked with content-length? \r\nI think request body is aggregated and pass to handler, and it is OK.', 'commenter': 'imay'}, {'comment': ""For stream load, there should be chunked transfer-encoding header, and the header should deliver to BE, we shouldn't remove the chunked transfer-encoding header."", 'commenter': 'kangkaisen'}, {'comment': 'I think if there is 100-continue header, FE returns redirect to client, then client will send request to BE, and the headers which be received is sent from client not from FE.\r\n\r\nAnd if there is no 100-continue header, FE will reject this request. \r\n\r\nIf all loading data is routed by FE, it is a huge load for it. So we need client must add ""100-continue"" header to request\'s header, and then FE can redirect the client to some BE to handle this load request', 'commenter': 'imay'}, {'comment': ""Yes. If we want to get request body, we need to aggregate it. but for the `LoadAction`, we shouldn't let FE handle the Load data. which is conflicting. So, I think we could make `DorisHttpObjectAggregator` only handle post request. what do you think of it ?"", 'commenter': 'kangkaisen'}, {'comment': ""The followings are what I want to figure out for current `DorisHttpObjectAggregator`\r\n\r\n1. If request has 100-continue header,  does it receive request body before call action's handler? If this is true, I think in normal case for `LoadAction`, FE won't receive load data.\r\n\r\n2. If request has no 100-continue header and request body is exceed max body size, will it return an error response to client ? If this is true, I think it's OK to receive relatively small data, however it is difficult to let client known that request misses 100-continue header, because aggregator is a common path for all input HTTP request.\r\n\r\nMaking `DorisHttpObjectAggregator` only apply POST request seems OK."", 'commenter': 'imay'}, {'comment': ""1 Yes.  `DorisHttpObjectAggregator`  receive all request body before call  `HttpServerHandler` action's handler. \r\n\r\n2 Yes.  `DorisHttpObjectAggregator`  will response `413 Request Entity Too Large`.  "", 'commenter': 'kangkaisen'}, {'comment': 'OK, I see\r\n\r\nI think enabling chunked encoding in this aggregator is acceptable.\r\n\r\n1. If you apply this aggregator to all HTTP method, when client send `Load` request with ""Content-Length"" header and without ""100-continue"", this aggregator would also receive data. There is no difference between chunked or non chunked.\r\n\r\n2. If you only apply this aggregator to POST method. it\'s OK to receive chunked encoding body to POST action.  This aggregator won\'t receive body for `Load` action.\r\n\r\nSo, I think you can let the aggregator to receive chunked body\r\n\r\n', 'commenter': 'imay'}, {'comment': 'OK. I see.  I agree with you. ', 'commenter': 'kangkaisen'}]"
562,fe/src/main/java/org/apache/doris/http/meta/ColocateMetaService.java,"@@ -0,0 +1,281 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.http.meta;
+
+import com.google.common.base.Preconditions;
+import com.google.common.base.Strings;
+import com.google.gson.Gson;
+import com.google.gson.reflect.TypeToken;
+import io.netty.handler.codec.http.HttpMethod;
+import io.netty.handler.codec.http.HttpResponseStatus;
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.catalog.ColocateTableIndex;
+import org.apache.doris.catalog.Database;
+import org.apache.doris.common.DdlException;
+import org.apache.doris.http.ActionController;
+import org.apache.doris.http.BaseRequest;
+import org.apache.doris.http.BaseResponse;
+import org.apache.doris.http.IllegalArgException;
+import org.apache.doris.http.rest.RestBaseAction;
+import org.apache.doris.http.rest.RestBaseResult;
+import org.apache.doris.http.rest.RestResult;
+import org.apache.doris.mysql.privilege.PrivPredicate;
+import org.apache.doris.persist.ColocatePersistInfo;
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.ArrayList;
+import java.util.List;
+import java.lang.reflect.Type;
+
+/*
+ * the colocate meta define in {@link ColocateTableIndex}
+ */
+public class ColocateMetaService {
+    private static final Logger LOG = LogManager.getLogger(ColocateMetaService.class);
+    private static final String GROUP_ID = ""group_id"";
+    private static final String TABLE_ID = ""table_id"";
+    private static final String DB_ID = ""db_id"";
+
+    private static ColocateTableIndex colocateIndex = Catalog.getCurrentColocateIndex();
+
+    private static long checkAndGetGroupId(BaseRequest request) throws DdlException {
+        long groupId = Long.valueOf(request.getSingleParameter(GROUP_ID).trim());
+        LOG.info(""groupId is {}"", groupId);","[{'comment': 'remove this debug log?', 'commenter': 'imay'}, {'comment': 'OK', 'commenter': 'kangkaisen'}]"
562,fe/src/main/java/org/apache/doris/http/meta/ColocateMetaService.java,"@@ -0,0 +1,281 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.http.meta;
+
+import com.google.common.base.Preconditions;
+import com.google.common.base.Strings;
+import com.google.gson.Gson;
+import com.google.gson.reflect.TypeToken;
+import io.netty.handler.codec.http.HttpMethod;
+import io.netty.handler.codec.http.HttpResponseStatus;
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.catalog.ColocateTableIndex;
+import org.apache.doris.catalog.Database;
+import org.apache.doris.common.DdlException;
+import org.apache.doris.http.ActionController;
+import org.apache.doris.http.BaseRequest;
+import org.apache.doris.http.BaseResponse;
+import org.apache.doris.http.IllegalArgException;
+import org.apache.doris.http.rest.RestBaseAction;
+import org.apache.doris.http.rest.RestBaseResult;
+import org.apache.doris.http.rest.RestResult;
+import org.apache.doris.mysql.privilege.PrivPredicate;
+import org.apache.doris.persist.ColocatePersistInfo;
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.ArrayList;
+import java.util.List;
+import java.lang.reflect.Type;
+
+/*
+ * the colocate meta define in {@link ColocateTableIndex}
+ */
+public class ColocateMetaService {
+    private static final Logger LOG = LogManager.getLogger(ColocateMetaService.class);
+    private static final String GROUP_ID = ""group_id"";
+    private static final String TABLE_ID = ""table_id"";
+    private static final String DB_ID = ""db_id"";
+
+    private static ColocateTableIndex colocateIndex = Catalog.getCurrentColocateIndex();
+
+    private static long checkAndGetGroupId(BaseRequest request) throws DdlException {
+        long groupId = Long.valueOf(request.getSingleParameter(GROUP_ID).trim());
+        LOG.info(""groupId is {}"", groupId);
+        if (!colocateIndex.isGroupExist(groupId)) {
+            throw new DdlException(""the group "" + groupId + ""isn't  exist"");
+        }
+        return groupId;
+    }
+
+    private static long getTableId(BaseRequest request) throws DdlException {
+        long tableId = Long.valueOf(request.getSingleParameter(TABLE_ID).trim());
+        LOG.info(""tableId is {}"", tableId);","[{'comment': 'remove this log?', 'commenter': 'imay'}, {'comment': 'OK', 'commenter': 'kangkaisen'}]"
563,be/src/olap/schema_change.cpp,"@@ -1756,97 +1638,99 @@ OLAPStatus SchemaChangeHandler::schema_version_convert(
     }
 
     // c. 转换数据
-    ColumnData* olap_data = NULL;
-    for (vector<SegmentGroup*>::iterator it = ref_segment_groups->begin();
-            it != ref_segment_groups->end(); ++it) {
-        ColumnData* olap_data = ColumnData::create(*it);
-        if (NULL == olap_data) {
-            OLAP_LOG_WARNING(""fail to create ColumnData."");
-            res = OLAP_ERR_MALLOC_ERROR;
-            goto SCHEMA_VERSION_CONVERT_ERR;
-        }
-
-        olap_data->init();
-
-        SegmentGroup* new_segment_group = nullptr;
-        if ((*it)->transaction_id() == 0) {
-            new_segment_group = new SegmentGroup(dest_tablet->tablet_id(),
-                                                 0,
-                                                 dest_tablet->tablet_schema(),
-                                                 dest_tablet->num_key_fields(),
-                                                 dest_tablet->num_short_key_fields(),
-                                                 dest_tablet->num_rows_per_row_block(),
-                                                 dest_tablet->rowset_path_prefix(),
-                                                 olap_data->version(),
-                                                 olap_data->version_hash(),
-                                                 olap_data->delete_flag(),
-                                                 (*it)->segment_group_id(), 0);
+    for (vector<RowsetSharedPtr>::iterator it = ref_rowsets->begin();
+            it != ref_rowsets->end(); ++it) {
+        RowsetReaderSharedPtr rowset_reader = (*it)->create_reader();
+        rowset_reader->init(nullptr);
+
+        RowsetBuilderContextBuilder context_builder;
+        RowsetId rowset_id = 0;
+        RowsetIdGenerator::instance()->get_next_id(dest_tablet->data_dir(), &rowset_id);
+        if ((*it)->is_pending()) {
+            PUniqueId load_id;
+            load_id.set_hi(0);
+            load_id.set_lo(0);
+            context_builder.set_rowset_id(rowset_id)
+                           .set_tablet_id(dest_tablet->tablet_id())
+                           .set_partition_id(dest_tablet->partition_id())
+                           .set_tablet_schema_hash(dest_tablet->schema_hash())
+                           .set_rowset_type(ALPHA_ROWSET)
+                           .set_rowset_path_prefix(dest_tablet->tablet_path())
+                           .set_tablet_schema(dest_tablet->tablet_schema())
+                           .set_num_key_fields(dest_tablet->num_key_fields())
+                           .set_num_short_key_fields(dest_tablet->num_short_key_fields())
+                           .set_num_rows_per_row_block(dest_tablet->num_rows_per_row_block())
+                           .set_compress_kind(dest_tablet->compress_kind())
+                           .set_bloom_filter_fpp(dest_tablet->bloom_filter_fpp())
+                           .set_rowset_state(PREPARING)
+                           .set_txn_id((*it)->txn_id())
+                           .set_load_id(load_id);
         } else {
-            new_segment_group = new SegmentGroup(dest_tablet->tablet_id(),
-                                                 0,
-                                                 dest_tablet->tablet_schema(),
-                                                 dest_tablet->num_key_fields(),
-                                                 dest_tablet->num_short_key_fields(),
-                                                 dest_tablet->num_rows_per_row_block(),
-                                                 dest_tablet->rowset_path_prefix(),
-                                                 olap_data->delete_flag(),
-                                                 (*it)->segment_group_id(), 0,
-                                                 (*it)->is_pending(),
-                                                 (*it)->partition_id(),
-                                                 (*it)->transaction_id());
-        }
-
-        if (NULL == new_segment_group) {
-            LOG(FATAL) << ""failed to malloc SegmentGroup. size="" << sizeof(SegmentGroup);
-            res = OLAP_ERR_MALLOC_ERROR;
-            goto SCHEMA_VERSION_CONVERT_ERR;
-        }
-
-        new_segment_groups->push_back(new_segment_group);
-
-        if (!sc_procedure->process(olap_data, new_segment_group, dest_tablet)) {
+            context_builder.set_rowset_id(rowset_id)
+                           .set_tablet_id(dest_tablet->tablet_id())
+                           .set_partition_id(dest_tablet->partition_id())
+                           .set_tablet_schema_hash(dest_tablet->schema_hash())
+                           .set_rowset_type(ALPHA_ROWSET)
+                           .set_rowset_path_prefix(dest_tablet->tablet_path())
+                           .set_tablet_schema(dest_tablet->tablet_schema())
+                           .set_num_key_fields(dest_tablet->num_key_fields())
+                           .set_num_short_key_fields(dest_tablet->num_short_key_fields())
+                           .set_num_rows_per_row_block(dest_tablet->num_rows_per_row_block())
+                           .set_compress_kind(dest_tablet->compress_kind())
+                           .set_bloom_filter_fpp(dest_tablet->bloom_filter_fpp())
+                           .set_rowset_state(VISIBLE)
+                           .set_version((*it)->version())
+                           .set_version_hash((*it)->version_hash());
+        }
+        RowsetBuilderContext context = context_builder.build();
+        RowsetBuilderSharedPtr rowset_builder(new AlphaRowsetBuilder());
+        rowset_builder->init(context);
+
+        if (!sc_procedure->process(rowset_reader, rowset_builder, dest_tablet)) {
             if ((*it)->is_pending()) {
-                OLAP_LOG_WARNING(""failed to process the transaction when schema change. ""
-                                 ""[tablet='%s' transaction=%ld]"",
-                                 dest_tablet->full_name().c_str(),
-                                 (*it)->transaction_id());
+                LOG(WARNING) << ""failed to process the transaction when schema change. ""
+                             << ""[tablet='"" << dest_tablet->full_name() << ""'""
+                             << "" transaction=""<< (*it)->txn_id() << ""]"";
             } else {
-                OLAP_LOG_WARNING(""failed to process the version. [version='%d-%d']"",
-                                 (*it)->version().first,
-                                 (*it)->version().second);
+                LOG(WARNING) << ""failed to process the version. ""
+                             << ""[version='"" << (*it)->version().first
+                             << ""-"" << (*it)->version().second << ""']"";
             }
+            rowset_builder->release();
             res = OLAP_ERR_INPUT_PARAMETER_ERROR;
             goto SCHEMA_VERSION_CONVERT_ERR;
         }
-
-        SAFE_DELETE(olap_data);
+        RowsetSharedPtr new_rowset = rowset_builder->build();
+        if (new_rowset == nullptr) {
+            LOG(WARNING) << ""build rowset failed."";
+            res = OLAP_ERR_MALLOC_ERROR;
+            goto SCHEMA_VERSION_CONVERT_ERR;
+        }
+        new_rowsets->push_back(new_rowset);
     }
 
     SAFE_DELETE(sc_procedure);
-    SAFE_DELETE(olap_data);
 
     return res;
 
 SCHEMA_VERSION_CONVERT_ERR:
-    while (!new_segment_groups->empty()) {
-        SegmentGroup* segment_group = new_segment_groups->back();
-        segment_group->delete_all_files();
-        SAFE_DELETE(segment_group);
-        new_segment_groups->pop_back();
+    while (!new_rowsets->empty()) {
+        RowsetSharedPtr new_rowset = new_rowsets->back();
+        new_rowset->remove();
+        new_rowsets->pop_back();
     }
 
     SAFE_DELETE(sc_procedure);
-    SAFE_DELETE(olap_data);
     return res;
 }
 
 OLAPStatus SchemaChangeHandler::_get_versions_to_be_changed(
         TabletSharedPtr ref_tablet,
         vector<Version>& versions_to_be_changed) {
     int32_t request_version = 0;
-    const PDelta* lastest_version = ref_tablet->lastest_version();
-    if (lastest_version != NULL) {
-        request_version = lastest_version->end_version() - 1;
+    RowsetSharedPtr rowset = ref_tablet->rowset_with_max_version();
+    if (rowset != NULL) {
+        request_version = rowset->version().second - 1;
     } else {
         OLAP_LOG_WARNING(""Table has no version. [path='%s']"",","[{'comment': 'LOG(WARNING)', 'commenter': 'chaoyli'}]"
563,be/src/olap/rowset/alpha_rowset.cpp,"@@ -181,17 +189,65 @@ OLAPStatus AlphaRowset::_init_segment_groups() {
                         << ""version="" << version.first << ""-""
                         << version.second << "", ""
                         << ""version_hash="" << version_hash;
-                return res; 
+                return res;
             }
         }
     }
     _segment_group_size = _segment_groups.size();
     if (_is_cumulative_rowset && _segment_group_size > 1) {
         LOG(WARNING) << ""invalid segment group meta for cumulative rowset. segment group size:""
                 << _segment_group_size;
-        return OLAP_ERR_ENGINE_LOAD_INDEX_TABLE_ERROR; 
+        return OLAP_ERR_ENGINE_LOAD_INDEX_TABLE_ERROR;
+    }
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus AlphaRowset::_init_pending_segment_groups() {
+    std::vector<PendingSegmentGroupPB> pending_segment_group_metas;
+    AlphaRowsetMeta* _alpha_rowset_meta = (AlphaRowsetMeta*)_rowset_meta.get();
+    _alpha_rowset_meta->get_pending_segment_groups(&pending_segment_group_metas);
+    for (auto& pending_segment_group_meta : pending_segment_group_metas) {
+        Version version = _rowset_meta->version();
+        int64_t version_hash = _rowset_meta->version_hash();
+        int64_t txn_id = _rowset_meta->txn_id();
+        int64_t partition_id = _rowset_meta->partition_id();
+        std::shared_ptr<SegmentGroup> segment_group(new SegmentGroup(_rowset_meta->tablet_id(),
+                _rowset_meta->rowset_id(), _tablet_schema, _num_key_fields, _num_short_key_fields,
+                _num_rows_per_row_block, _rowset_path, false, pending_segment_group_meta.pending_segment_group_id(),
+                pending_segment_group_meta.num_segments(), true, partition_id, txn_id));
+        if (segment_group.get() == nullptr) {","[{'comment': 'segment_group == nullptr is OK.', 'commenter': 'chaoyli'}]"
563,be/src/olap/rowset/segment_group.cpp,"@@ -732,6 +735,35 @@ bool SegmentGroup::remove_old_files(std::vector<std::string>* removed_links) {
     return true;
 }
 
+bool SegmentGroup::copy_segments_to_path(const std::string& dest_path) {
+    if (dest_path == """" || dest_path == _rowset_path_prefix) {","[{'comment': 'dest_path.empty()', 'commenter': 'chaoyli'}]"
563,be/src/olap/schema_change.cpp,"@@ -1115,121 +1057,109 @@ bool SchemaChangeWithSorting::process(ColumnData* olap_data, SegmentGroup* new_s
 }
 
 bool SchemaChangeWithSorting::_internal_sorting(const vector<RowBlock*>& row_block_arr,
-                                                const Version& temp_delta_versions,
-                                                SegmentGroup** temp_segment_group) {
-    ColumnDataWriter* writer = NULL;
+                                                const Version& version,
+                                                const VersionHash version_hash,
+                                                RowsetSharedPtr* rowset) {
     uint64_t merged_rows = 0;
     RowBlockMerger merger(_tablet);
 
-    (*temp_segment_group) =
-        new(nothrow) SegmentGroup(_tablet->tablet_id(),
-                                  0,
-                                  _tablet->tablet_schema(),
-                                  _tablet->num_key_fields(),
-                                  _tablet->num_short_key_fields(),
-                                  _tablet->num_rows_per_row_block(),
-                                  _tablet->rowset_path_prefix(),
-                                  temp_delta_versions,
-                                  rand(),
-                                  false,
-                                  0, 0);
-    if (NULL == (*temp_segment_group)) {
-        OLAP_LOG_WARNING(""failed to malloc SegmentGroup. [size=%ld]"", sizeof(SegmentGroup));
-        goto INTERNAL_SORTING_ERR;
+    RowsetBuilderContextBuilderSharedPtr context_builder(new RowsetBuilderContextBuilder());","[{'comment': 'RowsetBuilderContextBuilderSharedPtr -> RowsetWriterContextBuilderSharedPtr', 'commenter': 'chaoyli'}]"
563,be/src/olap/schema_change.cpp,"@@ -1115,121 +1057,109 @@ bool SchemaChangeWithSorting::process(ColumnData* olap_data, SegmentGroup* new_s
 }
 
 bool SchemaChangeWithSorting::_internal_sorting(const vector<RowBlock*>& row_block_arr,
-                                                const Version& temp_delta_versions,
-                                                SegmentGroup** temp_segment_group) {
-    ColumnDataWriter* writer = NULL;
+                                                const Version& version,
+                                                const VersionHash version_hash,
+                                                RowsetSharedPtr* rowset) {
     uint64_t merged_rows = 0;
     RowBlockMerger merger(_tablet);
 
-    (*temp_segment_group) =
-        new(nothrow) SegmentGroup(_tablet->tablet_id(),
-                                  0,
-                                  _tablet->tablet_schema(),
-                                  _tablet->num_key_fields(),
-                                  _tablet->num_short_key_fields(),
-                                  _tablet->num_rows_per_row_block(),
-                                  _tablet->rowset_path_prefix(),
-                                  temp_delta_versions,
-                                  rand(),
-                                  false,
-                                  0, 0);
-    if (NULL == (*temp_segment_group)) {
-        OLAP_LOG_WARNING(""failed to malloc SegmentGroup. [size=%ld]"", sizeof(SegmentGroup));
-        goto INTERNAL_SORTING_ERR;
+    RowsetBuilderContextBuilderSharedPtr context_builder(new RowsetBuilderContextBuilder());
+    RowsetId rowset_id = 0;
+    OLAPStatus status = RowsetIdGenerator::instance()->get_next_id(_tablet->data_dir(), &rowset_id);
+    if (status == OLAP_SUCCESS) {
+        LOG(WARNING) << ""get next rowset id failed"";
+        return false;
     }
-
-    VLOG(3) << ""init writer. tablet="" << _tablet->full_name()
-            << "", block_row_size="" << _tablet->num_rows_per_row_block();
-    writer = ColumnDataWriter::create(*temp_segment_group, false,
-                                      _tablet->compress_kind(), _tablet->bloom_filter_fpp());
-    if (NULL == writer) {
-        OLAP_LOG_WARNING(""failed to create writer."");
-        goto INTERNAL_SORTING_ERR;
+    context_builder->set_rowset_id(rowset_id)
+                   .set_tablet_id(_tablet->tablet_id())
+                   .set_partition_id(_tablet->partition_id())
+                   .set_tablet_schema_hash(_tablet->schema_hash())
+                   .set_rowset_type(ALPHA_ROWSET)
+                   .set_rowset_path_prefix(_tablet->tablet_path())
+                   .set_tablet_schema(_tablet->tablet_schema())
+                   .set_num_key_fields(_tablet->num_key_fields())
+                   .set_num_short_key_fields(_tablet->num_short_key_fields())
+                   .set_num_rows_per_row_block(_tablet->num_rows_per_row_block())
+                   .set_compress_kind(_tablet->compress_kind())
+                   .set_bloom_filter_fpp(_tablet->bloom_filter_fpp())
+                   .set_rowset_state(VISIBLE)
+                   .set_version(version)
+                   .set_version_hash(version_hash);
+    
+    RowsetBuilderContext context = context_builder->build();
+    RowsetBuilderSharedPtr rowset_builder(new AlphaRowsetBuilder());
+    if (rowset_builder == nullptr) {
+        LOG(WARNING) << ""new rowset builder failed"";
+        return false;
     }
+    VLOG(3) << ""init rowset builder. tablet="" << _tablet->full_name()
+            << "", block_row_size="" << _tablet->num_rows_per_row_block();
+    rowset_builder->init(context);
 
-    if (!merger.merge(row_block_arr, writer, &merged_rows)) {
-        OLAP_LOG_WARNING(""failed to merge row blocks."");
+    if (!merger.merge(row_block_arr, rowset_builder, &merged_rows)) {
+        LOG(WARNING) << ""failed to merge row blocks."";
         goto INTERNAL_SORTING_ERR;
     }
     add_merged_rows(merged_rows);
 
-    if (OLAP_SUCCESS != (*temp_segment_group)->load()) {
-        OLAP_LOG_WARNING(""failed to reload olap index."");
+    *rowset = rowset_builder->build();
+
+    if (OLAP_SUCCESS != (*rowset)->init()) {
+        LOG(WARNING) << ""failed to reload olap index."";
         goto INTERNAL_SORTING_ERR;
     }
 
-    SAFE_DELETE(writer);
     return true;
 
 INTERNAL_SORTING_ERR:
-    SAFE_DELETE(writer);
-
-    (*temp_segment_group)->delete_all_files();
-    SAFE_DELETE(*temp_segment_group);
+    rowset_builder->release();
     return false;
 }
 
 bool SchemaChangeWithSorting::_external_sorting(
-        vector<SegmentGroup*>& src_segment_groups,
-        SegmentGroup* dest_segment_group,
+        vector<RowsetSharedPtr>& src_rowsets,
+        RowsetBuilderSharedPtr rowset_builder,
         TabletSharedPtr tablet) {
-    Merger merger(_tablet, dest_segment_group, READER_ALTER_TABLE);
+    Merger merger(_tablet, rowset_builder, READER_ALTER_TABLE);
 
     uint64_t merged_rows = 0;
     uint64_t filted_rows = 0;
     vector<RowsetReaderSharedPtr> rs_readers;
 
-    for (vector<SegmentGroup*>::iterator it = src_segment_groups.begin();
-            it != src_segment_groups.end(); ++it) {
-        ColumnData* olap_data = ColumnData::create(*it);
-        if (NULL == olap_data) {
-            OLAP_LOG_WARNING(""fail to create ColumnData."");
+    for (vector<RowsetSharedPtr>::iterator it = src_rowsets.begin();
+            it != src_rowsets.end(); ++it) {
+        RowsetReaderSharedPtr rowset_reader = (*it)->create_reader();
+        if (rowset_reader == nullptr) {
+            LOG(WARNING) << ""fail to create rowset reader."";
             goto EXTERNAL_SORTING_ERR;
         }
-
-        rs_readers.push_back(olap_data);
-
-        if (OLAP_SUCCESS != olap_data->init()) {
-            OLAP_LOG_WARNING(""fail to initial olap data. [version='%d-%d' tablet='%s']"",
-                             (*it)->version().first,
-                             (*it)->version().second,
-                             tablet->full_name().c_str());
+        OLAPStatus status = rowset_reader->init(nullptr);","[{'comment': 'init(nullptr) is OK?', 'commenter': 'chaoyli'}]"
563,be/src/olap/schema_change.h,"@@ -273,8 +279,8 @@ class SchemaChangeHandler {
 
     OLAPStatus schema_version_convert(TabletSharedPtr ref_tablet,
                                       TabletSharedPtr new_tablet,
-                                      std::vector<SegmentGroup*>* ref_segment_groups,
-                                      std::vector<SegmentGroup*>* new_segment_groups);
+                                      std::vector<RowsetSharedPtr>* ref_rowsets,","[{'comment': 'new_rowsets may be corresponds to old_rowsets. So old_rowsets may be a better name.', 'commenter': 'chaoyli'}]"
563,be/src/olap/schema_change.cpp,"@@ -2347,56 +2213,26 @@ OLAPStatus SchemaChange::create_initial_rowset(
         SchemaHash schema_hash,
         Version version,
         VersionHash version_hash,
-        SegmentGroup* segment_group) {
+        RowsetBuilderSharedPtr rowset_builder) {
     VLOG(3) << ""begin to create init version. ""
             << ""begin="" << version.first << "", end="" << version.second;
 
-    TabletSharedPtr tablet;
-    ColumnDataWriter* writer = NULL;
     OLAPStatus res = OLAP_SUCCESS;
 
     do {
         if (version.first > version.second) {
-            OLAP_LOG_WARNING(""begin should not larger than end. [begin=%d end=%d]"",
-                             version.first, version.second);
+            LOG(WARNING) << ""begin should not larger than end. ""
+                         << "" [begin="" << version.first","[{'comment': 'bracket is not necessary.', 'commenter': 'chaoyli'}]"
563,be/src/olap/schema_change.cpp,"@@ -2176,7 +2040,9 @@ OLAPStatus SchemaChangeHandler::_alter_tablet(SchemaChangeParams* sc_params) {
                 << ""status="" << sc_params->ref_tablet->schema_change_status().status;
     }
 
-    sc_params->ref_tablet->release_rs_readers(&(sc_params->ref_olap_data_arr));
+    for (auto rs_reader : sc_params->ref_rowset_readers) {","[{'comment': 'auto& rs_reader', 'commenter': 'chaoyli'}]"
563,be/src/olap/rowset/alpha_rowset_writer.cpp,"@@ -70,22 +71,54 @@ OLAPStatus AlphaRowsetWriter::add_row(const char* row, Schema* schema) {
     if (status != OLAP_SUCCESS) {
         std::string error_msg = ""add row failed"";
         LOG(WARNING) << error_msg;
-        return status; 
+        return status;
     }
     _column_data_writer->next(row, schema);
+    _is_pending_rowset++;
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus AlphaRowsetWriter::add_row_block(RowBlock* row_block) {
+    size_t pos = 0;
+    row_block->set_pos(pos);
+    RowCursor row_cursor;
+    row_cursor.init(_rowset_builder_context.tablet_schema);
+    while (pos < row_block->limit()) {
+        row_block->get_row(pos, &row_cursor);
+        add_row(&row_cursor);
+        row_block->pos_inc();
+        pos = row_block->pos();
+        _is_pending_rowset++;
+    }
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus AlphaRowsetWriter::add_rowset(RowsetSharedPtr rowset) {","[{'comment': 'This name is confused. add_rowset_for_linked_schema_change', 'commenter': 'chaoyli'}]"
563,be/src/olap/schema_change.cpp,"@@ -663,66 +675,18 @@ SchemaChangeDirectly::~SchemaChangeDirectly() {
     SAFE_DELETE(_dst_cursor);
 }
 
-bool SchemaChangeDirectly::_write_row_block(ColumnDataWriter* writer, RowBlock* row_block) {
+bool SchemaChangeDirectly::_write_row_block(RowsetBuilderSharedPtr rowset_builder, RowBlock* row_block) {
     for (uint32_t i = 0; i < row_block->row_block_info().row_num; i++) {
-        if (OLAP_SUCCESS != writer->attached_by(_dst_cursor)) {
-            OLAP_LOG_WARNING(""fail to attach writer"");
+        if (OLAP_SUCCESS != rowset_builder->add_row(_dst_cursor)) {","[{'comment': 'where content of _dst_cursor comes from? ', 'commenter': 'chaoyli'}]"
563,be/src/olap/schema_change.cpp,"@@ -764,10 +728,10 @@ bool SchemaChangeDirectly::process(ColumnData* olap_data, SegmentGroup* new_segm
     RowBlock* ref_row_block = NULL;
     bool need_create_empty_version = false;
     OLAPStatus res = OLAP_SUCCESS;
-    if (!olap_data->empty()) {
-        res = olap_data->get_first_row_block(&ref_row_block);
+    if (!rowset_reader->has_next()) {","[{'comment': 'has_next() is the same as empty() ?', 'commenter': 'chaoyli'}]"
563,be/src/olap/schema_change.cpp,"@@ -843,52 +795,42 @@ bool SchemaChangeDirectly::process(ColumnData* olap_data, SegmentGroup* new_segm
         }
         add_filted_rows(filted_rows);
 
-        if (!_write_row_block(writer, new_row_block)) {
+        if (!_write_row_block(rowset_builder, new_row_block)) {
             OLAP_LOG_WARNING(""failed to write row block."");
             result = false;
             goto DIRECTLY_PROCESS_ERR;
         }
 
-        olap_data->get_next_row_block(&ref_row_block);
-
-    }
+        rowset_reader->next_block(&ref_row_block);
 
-    if (OLAP_SUCCESS != writer->finalize()) {
-        result = false;
-        goto DIRECTLY_PROCESS_ERR;
     }
 
-    if (OLAP_SUCCESS != new_segment_group->load()) {","[{'comment': 'where to load rowset?', 'commenter': 'chaoyli'}]"
563,be/src/olap/schema_change.cpp,"@@ -930,29 +874,22 @@ bool SchemaChangeWithSorting::process(ColumnData* olap_data, SegmentGroup* new_s
     RowBlock* ref_row_block = NULL;
     bool need_create_empty_version = false;
     OLAPStatus res = OLAP_SUCCESS;
-    if (!olap_data->empty()) {
-        res = olap_data->get_first_row_block(&ref_row_block);
+    if (rowset_reader->has_next()) {","[{'comment': 'has_next() is the same as empty() ?', 'commenter': 'chaoyli'}]"
563,be/src/olap/schema_change.cpp,"@@ -1700,10 +1582,10 @@ OLAPStatus SchemaChangeHandler::_create_new_tablet(
 OLAPStatus SchemaChangeHandler::schema_version_convert(","[{'comment': 'If you have duplicated rowset to handle ingestion when schema change, this function is redundant.', 'commenter': 'chaoyli'}]"
564,be/src/olap/base_compaction.cpp,"@@ -309,13 +309,13 @@ OLAPStatus BaseCompaction::_do_base_compaction(VersionHash new_base_version_hash
     // 1. 生成新base文件对应的olap index
     RowsetId rowset_id = 0;
     RowsetIdGenerator::instance()->get_next_id(_tablet->data_dir(), &rowset_id);
-    RowsetBuilderContext context = {_tablet->partition_id(), _tablet->tablet_id(),
-                                    _tablet->schema_hash(), rowset_id, 
-                                    RowsetTypePB::ALPHA_ROWSET, _tablet->rowset_path_prefix(),
-                                    _tablet->tablet_schema(), _tablet->num_key_fields(),
-                                    _tablet->num_short_key_fields(), _tablet->num_rows_per_row_block(),
-                                    _tablet->compress_kind(), _tablet->bloom_filter_fpp()};
-    RowsetBuilder* builder = new AlphaRowsetBuilder(); 
+    RowsetWriterContext context = {_tablet->partition_id(), _tablet->tablet_id(),
+                                   _tablet->schema_hash(), rowset_id, 
+                                   RowsetTypePB::ALPHA_ROWSET, _tablet->rowset_path_prefix(),
+                                   _tablet->tablet_schema(), _tablet->num_key_fields(),
+                                   _tablet->num_short_key_fields(), _tablet->num_rows_per_row_block(),
+                                   _tablet->compress_kind(), _tablet->bloom_filter_fpp()};
+    RowsetWriter* builder = new AlphaRowsetWriter(); ","[{'comment': 'Maybe should create a rowsetwriterfactory to create different rowsetwrite according to the rowset type.', 'commenter': 'yiguolei'}, {'comment': 'use shared_ptr or you will delete the pointer in any return place', 'commenter': 'kangpinghuang'}, {'comment': 'This reply on interface of RowsetWriter, which has no factory function now.', 'commenter': 'chaoyli'}, {'comment': 'OK, I will fixed it.', 'commenter': 'chaoyli'}]"
564,be/src/olap/delta_writer.cpp,"@@ -113,13 +113,13 @@ OLAPStatus DeltaWriter::init() {
             .set_rowset_state(PREPARING)
             .set_txn_id(_req.txn_id)
             .set_load_id(_req.load_id);
-    RowsetBuilderContext builder_context = context_builder.build();
+    RowsetWriterContext builder_context = context_builder.build();","[{'comment': 'if the class name is RowsetWriter then the param name should be writer_context?', 'commenter': 'yiguolei'}, {'comment': 'OK', 'commenter': 'chaoyli'}]"
564,be/src/olap/merger.cpp,"@@ -33,7 +33,7 @@ using std::vector;
 
 namespace doris {
 
-Merger::Merger(TabletSharedPtr tablet, RowsetBuilder* builder, ReaderType type) : 
+Merger::Merger(TabletSharedPtr tablet, RowsetWriter* builder, ReaderType type) : ","[{'comment': 'writer?', 'commenter': 'yiguolei'}, {'comment': 'OK', 'commenter': 'chaoyli'}]"
564,be/src/olap/merger.h,"@@ -30,7 +30,7 @@ class ColumnData;
 class Merger {
 public:
     // parameter index is created by caller, and it is empty.
-    Merger(TabletSharedPtr tablet, RowsetBuilder* builder, ReaderType type);
+    Merger(TabletSharedPtr tablet, RowsetWriter* builder, ReaderType type);","[{'comment': 'writer?', 'commenter': 'yiguolei'}, {'comment': 'I will fixed it.', 'commenter': 'chaoyli'}]"
564,be/src/olap/merger.h,"@@ -46,7 +46,7 @@ class Merger {
     }
 private:
     TabletSharedPtr _tablet;
-    RowsetBuilder* _builder;
+    RowsetWriter* _builder;","[{'comment': 'writer?', 'commenter': 'yiguolei'}, {'comment': 'OK', 'commenter': 'chaoyli'}]"
564,be/src/olap/base_compaction.cpp,"@@ -309,13 +309,13 @@ OLAPStatus BaseCompaction::_do_base_compaction(VersionHash new_base_version_hash
     // 1. 生成新base文件对应的olap index
     RowsetId rowset_id = 0;
     RowsetIdGenerator::instance()->get_next_id(_tablet->data_dir(), &rowset_id);
-    RowsetBuilderContext context = {_tablet->partition_id(), _tablet->tablet_id(),
-                                    _tablet->schema_hash(), rowset_id, 
-                                    RowsetTypePB::ALPHA_ROWSET, _tablet->rowset_path_prefix(),
-                                    _tablet->tablet_schema(), _tablet->num_key_fields(),
-                                    _tablet->num_short_key_fields(), _tablet->num_rows_per_row_block(),
-                                    _tablet->compress_kind(), _tablet->bloom_filter_fpp()};
-    RowsetBuilder* builder = new AlphaRowsetBuilder(); 
+    RowsetWriterContext context = {_tablet->partition_id(), _tablet->tablet_id(),
+                                   _tablet->schema_hash(), rowset_id, 
+                                   RowsetTypePB::ALPHA_ROWSET, _tablet->rowset_path_prefix(),
+                                   _tablet->tablet_schema(), _tablet->num_key_fields(),
+                                   _tablet->num_short_key_fields(), _tablet->num_rows_per_row_block(),
+                                   _tablet->compress_kind(), _tablet->bloom_filter_fpp()};","[{'comment': 'I have written a RowserWriterContextBuilder, you can use it', 'commenter': 'kangpinghuang'}, {'comment': 'This place I will changed after schema is refactored.', 'commenter': 'chaoyli'}]"
564,be/src/olap/cumulative_compaction.h,"@@ -172,7 +172,7 @@ class CumulativeCompaction {
     VersionHash _cumulative_version_hash;
     // 新cumulative文件对应的olap index
     RowsetSharedPtr _rowset;
-    RowsetBuilder* _builder;
+    RowsetWriter* _builder;","[{'comment': 'use std::shared_ptr', 'commenter': 'kangpinghuang'}, {'comment': 'OK', 'commenter': 'chaoyli'}]"
567,be/src/olap/delta_writer.cpp,"@@ -45,12 +45,10 @@ DeltaWriter::~DeltaWriter() {
 }
 
 void DeltaWriter::_garbage_collection() {
-    StorageEngine::instance()->delete_transaction(_req.partition_id, _req.txn_id,
-                                                  _req.tablet_id, _req.schema_hash);
+    TxnManager::instance()->delete_txn(_req.partition_id, _req.txn_id,_req.tablet_id, _req.schema_hash);
     StorageEngine::instance()->add_unused_rowset(_cur_rowset);
     if (_related_tablet != nullptr) {
-        StorageEngine::instance()->delete_transaction(_req.partition_id, _req.txn_id,
-                                                      _related_tablet->tablet_id(), _related_tablet->schema_hash());
+        TxnManager::instance()->delete_txn(_req.partition_id, _req.txn_id, _related_tablet->tablet_id(), _related_tablet->schema_hash());","[{'comment': 'This line is too long.', 'commenter': 'kangpinghuang'}]"
567,be/src/olap/storage_engine.cpp,"@@ -115,96 +115,138 @@ StorageEngine::~StorageEngine() {
     clear();
 }
 
+// TODO(ygl): deal with rowsets and tablets when load failed
 OLAPStatus StorageEngine::_load_data_dir(DataDir* data_dir) {
     std::string data_dir_path = data_dir->path();
     LOG(INFO) <<""start to load tablets from data_dir_path:"" << data_dir_path;
-
     bool is_header_converted = false;
     OLAPStatus res = TabletMetaManager::get_header_converted(data_dir, is_header_converted);
     if (res != OLAP_SUCCESS) {
         LOG(WARNING) << ""get convert flag from meta failed"";
         return res;
     }
+    if (!is_header_converted) {
+        // ygl: could not be compatable with old doris data. User has to use previous version to parse
+        // header file into meta env first.
+        LOG(WARNING) << ""header is not converted to tablet meta yet, could not use this Doris version. "" 
+                    << ""[dir path ="" << data_dir_path << ""]"";
+        return OLAP_ERR_INIT_FAILED;
+    }
 
-    // load rowset meta from metaenv and create rowset
+    // load rowset meta from meta env and create rowset
     // COMMITTED: add to txn manager
     // VISIBLE: add to tablet
     // if one rowset load failed, then the total data dir will not be loaded
-    std::vector<std::shared_ptr<RowsetMeta>> dir_rowset_metas;
-    if (is_header_converted) {
-        LOG(INFO) << ""begin loading rowset from meta"";
-        bool has_error = false;
-        auto load_rowset_func = [this, data_dir, &has_error, &dir_rowset_metas](RowsetId rowset_id, const std::string& meta_str) -> bool {
-            std::shared_ptr<RowsetMeta> rowset_meta(new RowsetMeta());
-            bool parsed = rowset_meta->init(meta_str);
-            if (!parsed) {
-                LOG(WARNING) << ""parse rowset meta string failed for rowset_id:"" << rowset_id;
-                has_error = true;
-                // return false will break meta iterator
-                return false;
-            }
-            dir_rowset_metas.push_back(rowset_meta);
+    std::vector<RowsetMetaSharedPtr> dir_rowset_metas;
+    LOG(INFO) << ""begin loading rowset from meta"";
+    auto load_rowset_func = [this, data_dir, &dir_rowset_metas](RowsetId rowset_id, const std::string& meta_str) -> bool {
+        std::shared_ptr<RowsetMeta> rowset_meta(new RowsetMeta());","[{'comment': 'use RowsetMetaSharedPtr;', 'commenter': 'kangpinghuang'}]"
567,be/src/olap/storage_engine.cpp,"@@ -115,96 +115,138 @@ StorageEngine::~StorageEngine() {
     clear();
 }
 
+// TODO(ygl): deal with rowsets and tablets when load failed
 OLAPStatus StorageEngine::_load_data_dir(DataDir* data_dir) {
     std::string data_dir_path = data_dir->path();
     LOG(INFO) <<""start to load tablets from data_dir_path:"" << data_dir_path;
-
     bool is_header_converted = false;
     OLAPStatus res = TabletMetaManager::get_header_converted(data_dir, is_header_converted);
     if (res != OLAP_SUCCESS) {
         LOG(WARNING) << ""get convert flag from meta failed"";
         return res;
     }
+    if (!is_header_converted) {
+        // ygl: could not be compatable with old doris data. User has to use previous version to parse
+        // header file into meta env first.
+        LOG(WARNING) << ""header is not converted to tablet meta yet, could not use this Doris version. "" 
+                    << ""[dir path ="" << data_dir_path << ""]"";
+        return OLAP_ERR_INIT_FAILED;
+    }
 
-    // load rowset meta from metaenv and create rowset
+    // load rowset meta from meta env and create rowset
     // COMMITTED: add to txn manager
     // VISIBLE: add to tablet
     // if one rowset load failed, then the total data dir will not be loaded
-    std::vector<std::shared_ptr<RowsetMeta>> dir_rowset_metas;
-    if (is_header_converted) {
-        LOG(INFO) << ""begin loading rowset from meta"";
-        bool has_error = false;
-        auto load_rowset_func = [this, data_dir, &has_error, &dir_rowset_metas](RowsetId rowset_id, const std::string& meta_str) -> bool {
-            std::shared_ptr<RowsetMeta> rowset_meta(new RowsetMeta());
-            bool parsed = rowset_meta->init(meta_str);
-            if (!parsed) {
-                LOG(WARNING) << ""parse rowset meta string failed for rowset_id:"" << rowset_id;
-                has_error = true;
-                // return false will break meta iterator
-                return false;
-            }
-            dir_rowset_metas.push_back(rowset_meta);
+    std::vector<RowsetMetaSharedPtr> dir_rowset_metas;
+    LOG(INFO) << ""begin loading rowset from meta"";
+    auto load_rowset_func = [this, data_dir, &dir_rowset_metas](RowsetId rowset_id, const std::string& meta_str) -> bool {
+        std::shared_ptr<RowsetMeta> rowset_meta(new RowsetMeta());
+        bool parsed = rowset_meta->init(meta_str);
+        if (!parsed) {
+            LOG(WARNING) << ""parse rowset meta string failed for rowset_id:"" << rowset_id;
+            // return false will break meta iterator
             return true;
-        };
-        OLAPStatus s = RowsetMetaManager::traverse_rowset_metas(data_dir->get_meta(), load_rowset_func);
-        if (has_error) {
-            LOG(WARNING) << ""errors when load rowset meta from meta env, skip this data dir:"" << data_dir_path;
-            return OLAP_ERR_META_ITERATOR;
-        }
-        LOG(INFO) << ""load header from meta finished"";
-        if (s != OLAP_SUCCESS) {
-            LOG(WARNING) << ""errors when load rowset meta from meta env, skip this data dir:"" << data_dir_path;
-            return s;
-        } else {
-            return OLAP_SUCCESS;
         }
+        dir_rowset_metas.push_back(rowset_meta);
+        return true;
+    };
+    OLAPStatus load_rowset_status = RowsetMetaManager::traverse_rowset_metas(data_dir->get_meta(), load_rowset_func);
+
+    if (load_rowset_status != OLAP_SUCCESS) {
+        LOG(WARNING) << ""errors when load rowset meta from meta env, skip this data dir:"" << data_dir_path;
+    } else {
+        LOG(INFO) << ""load rowset from meta finished, data dir: "" << data_dir_path;
     }
+
     // load tablet
     // create tablet from tablet meta and add it to tablet mgr
-    if (is_header_converted) {
-        LOG(INFO) << ""load header from meta"";
-        auto load_tablet_func = [this, data_dir](long tablet_id,
-            long schema_hash, const std::string& value) -> bool {
-            
-            OLAPStatus status = TabletManager::instance()->load_tablet_from_header(data_dir, tablet_id, schema_hash, value);
-            if (status != OLAP_SUCCESS) {
-                LOG(WARNING) << ""load tablet from header failed. status:"" << status
-                    << ""tablet="" << tablet_id << ""."" << schema_hash;
-            };
-            return true;
+    LOG(INFO) << ""begin loading tablet from meta"";
+    auto load_tablet_func = [this, data_dir](long tablet_id,
+        long schema_hash, const std::string& value) -> bool {
+        OLAPStatus status = TabletManager::instance()->load_tablet_from_header(data_dir, tablet_id, schema_hash, value);
+        if (status != OLAP_SUCCESS) {
+            LOG(WARNING) << ""load tablet from header failed. status:"" << status
+                << ""tablet="" << tablet_id << ""."" << schema_hash;
         };
-        OLAPStatus s = TabletMetaManager::traverse_headers(data_dir->get_meta(), load_tablet_func);
-        LOG(INFO) << ""load header from meta finished"";
-        if (s != OLAP_SUCCESS) {
-            LOG(WARNING) << ""there is failure when loading tablet headers, path:"" << data_dir_path;
-            return s;
-        } else {
-            return OLAP_SUCCESS;
-        }
+        return true;
+    };
+    OLAPStatus load_tablet_status = TabletMetaManager::traverse_headers(data_dir->get_meta(), load_tablet_func);
+    if (load_tablet_status != OLAP_SUCCESS) {
+        LOG(WARNING) << ""there is failure when loading tablet headers, path:"" << data_dir_path;
     } else {
-        // ygl: could not be compatable with old doris data. User has to use previous version to parse
-        // header file into meta env first.
-        LOG(WARNING) << ""header is not converted to tablet meta yet, could not use this Doris version. "" 
-                    << ""[dir path ="" << data_dir_path << ""]"";
-        return OLAP_ERR_INIT_FAILED;
+        LOG(INFO) << ""load rowset from meta finished, data dir: "" << data_dir_path;
     }
 
+    // tranverse rowset 
+    // 1. add committed rowset to txn map
+    // 2. add visible rowset to tablet
+    // ignore any errors when load tablet or rowset, because fe will repair them after report
     for (auto rowset_meta : dir_rowset_metas) {
-        // TODO(ygl)
-        // 1. build rowset from meta
-        // 2. add committed rowset to txn
-        // 3. add visible rowset to tablet
+        TabletSharedPtr tablet = TabletManager::instance()->get_tablet(rowset_meta->tablet_id(), rowset_meta->tablet_schema_hash());
+        // tablet maybe dropped, but not drop related rowset meta
+        if (tablet.get() == NULL) {
+            LOG(WARNING) << ""could not find tablet id: "" << rowset_meta->tablet_id()
+                         << "" schema hash: "" << rowset_meta->tablet_schema_hash()
+                         << "" for rowset: "" << rowset_meta->rowset_id()
+                         << "" skip this rowset"";
+            continue;
+        }
+        // TODO(ygl): build rowset from rowset meta using tablet info
+        RowsetSharedPtr rowset;
+        if (rowset_meta->rowset_state() == RowsetStatePB::COMMITTED) {
+            // build rowset from meta and create 
+            PUniqueId load_id;
+            load_id.set_hi(0);
+            load_id.set_lo(0);
+            // TODO(ygl): create rowset from rowset meta
+            OLAPStatus add_txn_status = TxnManager::instance()->add_txn(rowset_meta->partition_id(), rowset_meta->txn_id(), ","[{'comment': 'too long', 'commenter': 'kangpinghuang'}]"
567,be/src/olap/storage_engine.cpp,"@@ -115,96 +115,138 @@ StorageEngine::~StorageEngine() {
     clear();
 }
 
+// TODO(ygl): deal with rowsets and tablets when load failed
 OLAPStatus StorageEngine::_load_data_dir(DataDir* data_dir) {
     std::string data_dir_path = data_dir->path();
     LOG(INFO) <<""start to load tablets from data_dir_path:"" << data_dir_path;
-
     bool is_header_converted = false;
     OLAPStatus res = TabletMetaManager::get_header_converted(data_dir, is_header_converted);
     if (res != OLAP_SUCCESS) {
         LOG(WARNING) << ""get convert flag from meta failed"";
         return res;
     }
+    if (!is_header_converted) {
+        // ygl: could not be compatable with old doris data. User has to use previous version to parse
+        // header file into meta env first.
+        LOG(WARNING) << ""header is not converted to tablet meta yet, could not use this Doris version. "" 
+                    << ""[dir path ="" << data_dir_path << ""]"";
+        return OLAP_ERR_INIT_FAILED;
+    }
 
-    // load rowset meta from metaenv and create rowset
+    // load rowset meta from meta env and create rowset
     // COMMITTED: add to txn manager
     // VISIBLE: add to tablet
     // if one rowset load failed, then the total data dir will not be loaded
-    std::vector<std::shared_ptr<RowsetMeta>> dir_rowset_metas;
-    if (is_header_converted) {
-        LOG(INFO) << ""begin loading rowset from meta"";
-        bool has_error = false;
-        auto load_rowset_func = [this, data_dir, &has_error, &dir_rowset_metas](RowsetId rowset_id, const std::string& meta_str) -> bool {
-            std::shared_ptr<RowsetMeta> rowset_meta(new RowsetMeta());
-            bool parsed = rowset_meta->init(meta_str);
-            if (!parsed) {
-                LOG(WARNING) << ""parse rowset meta string failed for rowset_id:"" << rowset_id;
-                has_error = true;
-                // return false will break meta iterator
-                return false;
-            }
-            dir_rowset_metas.push_back(rowset_meta);
+    std::vector<RowsetMetaSharedPtr> dir_rowset_metas;
+    LOG(INFO) << ""begin loading rowset from meta"";
+    auto load_rowset_func = [this, data_dir, &dir_rowset_metas](RowsetId rowset_id, const std::string& meta_str) -> bool {
+        std::shared_ptr<RowsetMeta> rowset_meta(new RowsetMeta());
+        bool parsed = rowset_meta->init(meta_str);
+        if (!parsed) {
+            LOG(WARNING) << ""parse rowset meta string failed for rowset_id:"" << rowset_id;
+            // return false will break meta iterator
             return true;
-        };
-        OLAPStatus s = RowsetMetaManager::traverse_rowset_metas(data_dir->get_meta(), load_rowset_func);
-        if (has_error) {
-            LOG(WARNING) << ""errors when load rowset meta from meta env, skip this data dir:"" << data_dir_path;
-            return OLAP_ERR_META_ITERATOR;
-        }
-        LOG(INFO) << ""load header from meta finished"";
-        if (s != OLAP_SUCCESS) {
-            LOG(WARNING) << ""errors when load rowset meta from meta env, skip this data dir:"" << data_dir_path;
-            return s;
-        } else {
-            return OLAP_SUCCESS;
         }
+        dir_rowset_metas.push_back(rowset_meta);
+        return true;
+    };
+    OLAPStatus load_rowset_status = RowsetMetaManager::traverse_rowset_metas(data_dir->get_meta(), load_rowset_func);
+
+    if (load_rowset_status != OLAP_SUCCESS) {
+        LOG(WARNING) << ""errors when load rowset meta from meta env, skip this data dir:"" << data_dir_path;
+    } else {
+        LOG(INFO) << ""load rowset from meta finished, data dir: "" << data_dir_path;
     }
+
     // load tablet
     // create tablet from tablet meta and add it to tablet mgr
-    if (is_header_converted) {
-        LOG(INFO) << ""load header from meta"";
-        auto load_tablet_func = [this, data_dir](long tablet_id,
-            long schema_hash, const std::string& value) -> bool {
-            
-            OLAPStatus status = TabletManager::instance()->load_tablet_from_header(data_dir, tablet_id, schema_hash, value);
-            if (status != OLAP_SUCCESS) {
-                LOG(WARNING) << ""load tablet from header failed. status:"" << status
-                    << ""tablet="" << tablet_id << ""."" << schema_hash;
-            };
-            return true;
+    LOG(INFO) << ""begin loading tablet from meta"";
+    auto load_tablet_func = [this, data_dir](long tablet_id,
+        long schema_hash, const std::string& value) -> bool {
+        OLAPStatus status = TabletManager::instance()->load_tablet_from_header(data_dir, tablet_id, schema_hash, value);
+        if (status != OLAP_SUCCESS) {
+            LOG(WARNING) << ""load tablet from header failed. status:"" << status
+                << ""tablet="" << tablet_id << ""."" << schema_hash;
         };
-        OLAPStatus s = TabletMetaManager::traverse_headers(data_dir->get_meta(), load_tablet_func);
-        LOG(INFO) << ""load header from meta finished"";
-        if (s != OLAP_SUCCESS) {
-            LOG(WARNING) << ""there is failure when loading tablet headers, path:"" << data_dir_path;
-            return s;
-        } else {
-            return OLAP_SUCCESS;
-        }
+        return true;
+    };
+    OLAPStatus load_tablet_status = TabletMetaManager::traverse_headers(data_dir->get_meta(), load_tablet_func);
+    if (load_tablet_status != OLAP_SUCCESS) {
+        LOG(WARNING) << ""there is failure when loading tablet headers, path:"" << data_dir_path;
     } else {
-        // ygl: could not be compatable with old doris data. User has to use previous version to parse
-        // header file into meta env first.
-        LOG(WARNING) << ""header is not converted to tablet meta yet, could not use this Doris version. "" 
-                    << ""[dir path ="" << data_dir_path << ""]"";
-        return OLAP_ERR_INIT_FAILED;
+        LOG(INFO) << ""load rowset from meta finished, data dir: "" << data_dir_path;
     }
 
+    // tranverse rowset 
+    // 1. add committed rowset to txn map
+    // 2. add visible rowset to tablet
+    // ignore any errors when load tablet or rowset, because fe will repair them after report
     for (auto rowset_meta : dir_rowset_metas) {
-        // TODO(ygl)
-        // 1. build rowset from meta
-        // 2. add committed rowset to txn
-        // 3. add visible rowset to tablet
+        TabletSharedPtr tablet = TabletManager::instance()->get_tablet(rowset_meta->tablet_id(), rowset_meta->tablet_schema_hash());
+        // tablet maybe dropped, but not drop related rowset meta
+        if (tablet.get() == NULL) {
+            LOG(WARNING) << ""could not find tablet id: "" << rowset_meta->tablet_id()
+                         << "" schema hash: "" << rowset_meta->tablet_schema_hash()
+                         << "" for rowset: "" << rowset_meta->rowset_id()
+                         << "" skip this rowset"";
+            continue;
+        }
+        // TODO(ygl): build rowset from rowset meta using tablet info
+        RowsetSharedPtr rowset;
+        if (rowset_meta->rowset_state() == RowsetStatePB::COMMITTED) {
+            // build rowset from meta and create 
+            PUniqueId load_id;
+            load_id.set_hi(0);
+            load_id.set_lo(0);
+            // TODO(ygl): create rowset from rowset meta
+            OLAPStatus add_txn_status = TxnManager::instance()->add_txn(rowset_meta->partition_id(), rowset_meta->txn_id(), 
+                                            rowset_meta->tablet_id(), rowset_meta->tablet_schema_hash(), 
+                                            load_id, NULL);
+            if (add_txn_status != OLAP_SUCCESS && add_txn_status != OLAP_ERR_PUSH_TRANSACTION_ALREADY_EXIST) {
+                LOG(WARNING) << ""failed to add committed rowset: "" << rowset_meta->rowset_id()
+                             << "" to tablet: "" << rowset_meta->tablet_id() 
+                             << "" for txn: "" << rowset_meta->txn_id();
+            } else {
+                LOG(INFO) << ""successfully to add committed rowset: "" << rowset_meta->rowset_id()
+                             << "" to tablet: "" << rowset_meta->tablet_id() 
+                             << "" schema hash: "" << rowset_meta->tablet_schema_hash()
+                             << "" for txn: "" << rowset_meta->txn_id();
+            }
+        } else if (rowset_meta->rowset_state() == RowsetStatePB::VISIBLE) {
+            // add visible rowset to tablet, it maybe use in the future
+            // there should be only preparing rowset in meta env because visible rowset is persist with tablet meta currently","[{'comment': 'too long', 'commenter': 'kangpinghuang'}]"
567,be/src/olap/storage_engine.cpp,"@@ -1002,4 +954,56 @@ OLAPStatus StorageEngine::load_header(
     return res;
 }
 
+
+OLAPStatus StorageEngine::execute_task(EngineTask* task) {
+    // 1. add wlock to related tablets
+    // 2. do prepare work
+    // 3. release wlock
+    {
+        vector<TabletInfo> tablet_infos;
+        task->get_related_tablets(&tablet_infos);
+        sort(tablet_infos.begin(), tablet_infos.end());
+        for (TabletInfo& tablet_info : tablet_infos) {
+            TabletSharedPtr tablet = TabletManager::instance()->get_tablet(tablet_info.tablet_id, tablet_info.schema_hash, false);
+            tablet->obtain_header_wrlock();
+        }
+        // add write lock to all related tablets
+        OLAPStatus prepare_status = task->prepare();
+        for (TabletInfo& tablet_info : tablet_infos) {
+            TabletSharedPtr tablet = TabletManager::instance()->get_tablet(tablet_info.tablet_id, tablet_info.schema_hash, false);","[{'comment': 'too long', 'commenter': 'kangpinghuang'}]"
567,be/src/olap/storage_engine.cpp,"@@ -1002,4 +954,56 @@ OLAPStatus StorageEngine::load_header(
     return res;
 }
 
+
+OLAPStatus StorageEngine::execute_task(EngineTask* task) {
+    // 1. add wlock to related tablets
+    // 2. do prepare work
+    // 3. release wlock
+    {
+        vector<TabletInfo> tablet_infos;
+        task->get_related_tablets(&tablet_infos);
+        sort(tablet_infos.begin(), tablet_infos.end());
+        for (TabletInfo& tablet_info : tablet_infos) {
+            TabletSharedPtr tablet = TabletManager::instance()->get_tablet(tablet_info.tablet_id, tablet_info.schema_hash, false);","[{'comment': 'too long', 'commenter': 'kangpinghuang'}]"
567,be/src/olap/storage_engine.cpp,"@@ -1002,4 +954,56 @@ OLAPStatus StorageEngine::load_header(
     return res;
 }
 
+
+OLAPStatus StorageEngine::execute_task(EngineTask* task) {
+    // 1. add wlock to related tablets
+    // 2. do prepare work
+    // 3. release wlock
+    {
+        vector<TabletInfo> tablet_infos;
+        task->get_related_tablets(&tablet_infos);
+        sort(tablet_infos.begin(), tablet_infos.end());
+        for (TabletInfo& tablet_info : tablet_infos) {
+            TabletSharedPtr tablet = TabletManager::instance()->get_tablet(tablet_info.tablet_id, tablet_info.schema_hash, false);
+            tablet->obtain_header_wrlock();
+        }
+        // add write lock to all related tablets
+        OLAPStatus prepare_status = task->prepare();
+        for (TabletInfo& tablet_info : tablet_infos) {
+            TabletSharedPtr tablet = TabletManager::instance()->get_tablet(tablet_info.tablet_id, tablet_info.schema_hash, false);
+            tablet->release_header_lock();
+        }
+        if (prepare_status != OLAP_SUCCESS) {
+            return prepare_status;
+        }
+    }
+
+    // do execute work without lock
+    OLAPStatus exec_status = task->execute();
+    if (exec_status != OLAP_SUCCESS) {
+        return exec_status;
+    }
+    
+    // 1. add wlock to related tablets
+    // 2. do finish work
+    // 3. release wlock
+    {
+        vector<TabletInfo> tablet_infos;
+        // related tablets may be changed after execute task, so that get them here again
+        task->get_related_tablets(&tablet_infos);
+        sort(tablet_infos.begin(), tablet_infos.end());
+        for (TabletInfo& tablet_info : tablet_infos) {
+            TabletSharedPtr tablet = TabletManager::instance()->get_tablet(tablet_info.tablet_id, tablet_info.schema_hash, false);","[{'comment': 'too long', 'commenter': 'kangpinghuang'}]"
567,be/src/olap/storage_engine.cpp,"@@ -1002,4 +954,56 @@ OLAPStatus StorageEngine::load_header(
     return res;
 }
 
+
+OLAPStatus StorageEngine::execute_task(EngineTask* task) {
+    // 1. add wlock to related tablets
+    // 2. do prepare work
+    // 3. release wlock
+    {
+        vector<TabletInfo> tablet_infos;
+        task->get_related_tablets(&tablet_infos);
+        sort(tablet_infos.begin(), tablet_infos.end());
+        for (TabletInfo& tablet_info : tablet_infos) {
+            TabletSharedPtr tablet = TabletManager::instance()->get_tablet(tablet_info.tablet_id, tablet_info.schema_hash, false);
+            tablet->obtain_header_wrlock();
+        }
+        // add write lock to all related tablets
+        OLAPStatus prepare_status = task->prepare();
+        for (TabletInfo& tablet_info : tablet_infos) {
+            TabletSharedPtr tablet = TabletManager::instance()->get_tablet(tablet_info.tablet_id, tablet_info.schema_hash, false);
+            tablet->release_header_lock();
+        }
+        if (prepare_status != OLAP_SUCCESS) {
+            return prepare_status;
+        }
+    }
+
+    // do execute work without lock
+    OLAPStatus exec_status = task->execute();
+    if (exec_status != OLAP_SUCCESS) {
+        return exec_status;
+    }
+    
+    // 1. add wlock to related tablets
+    // 2. do finish work
+    // 3. release wlock
+    {
+        vector<TabletInfo> tablet_infos;
+        // related tablets may be changed after execute task, so that get them here again
+        task->get_related_tablets(&tablet_infos);
+        sort(tablet_infos.begin(), tablet_infos.end());
+        for (TabletInfo& tablet_info : tablet_infos) {
+            TabletSharedPtr tablet = TabletManager::instance()->get_tablet(tablet_info.tablet_id, tablet_info.schema_hash, false);
+            tablet->obtain_header_wrlock();
+        }
+        // add write lock to all related tablets
+        OLAPStatus fin_status = task->finish();
+        for (TabletInfo& tablet_info : tablet_infos) {
+            TabletSharedPtr tablet = TabletManager::instance()->get_tablet(tablet_info.tablet_id, tablet_info.schema_hash, false);","[{'comment': 'too long', 'commenter': 'kangpinghuang'}, {'comment': 'the code seem to be the same with line 962-979, Is it possible the put them in a function?', 'commenter': 'kangpinghuang'}]"
567,be/src/olap/task/engine_publish_version_task.cpp,"@@ -0,0 +1,129 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""olap/task/engine_publish_version_task.h""
+#include ""olap/data_dir.h""
+#include ""olap/rowset/rowset_meta_manager.h""
+#include ""olap/tablet_manager.h""
+#include <map>
+
+namespace doris {
+
+using std::map;
+
+EnginePublishVersionTask::EnginePublishVersionTask(TPublishVersionRequest& publish_version_req, vector<TTabletId>* error_tablet_ids)","[{'comment': 'too long', 'commenter': 'kangpinghuang'}]"
567,be/src/olap/task/engine_publish_version_task.cpp,"@@ -0,0 +1,129 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""olap/task/engine_publish_version_task.h""
+#include ""olap/data_dir.h""
+#include ""olap/rowset/rowset_meta_manager.h""
+#include ""olap/tablet_manager.h""
+#include <map>
+
+namespace doris {
+
+using std::map;
+
+EnginePublishVersionTask::EnginePublishVersionTask(TPublishVersionRequest& publish_version_req, vector<TTabletId>* error_tablet_ids)
+    : _publish_version_req(publish_version_req), 
+      _error_tablet_ids(error_tablet_ids) {}
+
+OLAPStatus EnginePublishVersionTask::finish() {
+    LOG(INFO) << ""begin to process publish version. transaction_id=""
+        << _publish_version_req.transaction_id;
+
+    int64_t transaction_id = _publish_version_req.transaction_id;
+    OLAPStatus res = OLAP_SUCCESS;
+
+    // each partition
+    for (const TPartitionVersionInfo& partitionVersionInfo
+         : _publish_version_req.partition_version_infos) {
+
+        int64_t partition_id = partitionVersionInfo.partition_id;
+        map<TabletInfo, RowsetSharedPtr> tablet_related_rs;
+        TxnManager::instance()->get_txn_related_tablets(transaction_id, partition_id, &tablet_related_rs);
+
+        Version version(partitionVersionInfo.version, partitionVersionInfo.version);
+        VersionHash version_hash = partitionVersionInfo.version_hash;
+
+        // each tablet
+        for (auto& tablet_rs : tablet_related_rs) {
+            OLAPStatus publish_status = OLAP_SUCCESS;
+            TabletInfo tablet_info = tablet_rs.first;
+            RowsetSharedPtr rowset = tablet_rs.second;
+            LOG(INFO) << ""begin to publish version on tablet. ""
+                    << ""tablet_id="" << tablet_info.tablet_id
+                    << "", schema_hash="" << tablet_info.schema_hash
+                    << "", version="" << version.first
+                    << "", version_hash="" << version_hash
+                    << "", transaction_id="" << transaction_id;
+            TabletSharedPtr tablet = TabletManager::instance()->get_tablet(tablet_info.tablet_id, tablet_info.schema_hash);
+
+            if (tablet.get() == NULL) {
+                OLAP_LOG_WARNING(""can't get tablet when publish version. [tablet_id=%ld schema_hash=%d]"",","[{'comment': 'LOG(WARNING)', 'commenter': 'kangpinghuang'}]"
567,be/src/olap/task/engine_publish_version_task.cpp,"@@ -0,0 +1,129 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""olap/task/engine_publish_version_task.h""
+#include ""olap/data_dir.h""
+#include ""olap/rowset/rowset_meta_manager.h""
+#include ""olap/tablet_manager.h""
+#include <map>
+
+namespace doris {
+
+using std::map;
+
+EnginePublishVersionTask::EnginePublishVersionTask(TPublishVersionRequest& publish_version_req, vector<TTabletId>* error_tablet_ids)
+    : _publish_version_req(publish_version_req), 
+      _error_tablet_ids(error_tablet_ids) {}
+
+OLAPStatus EnginePublishVersionTask::finish() {
+    LOG(INFO) << ""begin to process publish version. transaction_id=""
+        << _publish_version_req.transaction_id;
+
+    int64_t transaction_id = _publish_version_req.transaction_id;
+    OLAPStatus res = OLAP_SUCCESS;
+
+    // each partition
+    for (const TPartitionVersionInfo& partitionVersionInfo
+         : _publish_version_req.partition_version_infos) {
+
+        int64_t partition_id = partitionVersionInfo.partition_id;
+        map<TabletInfo, RowsetSharedPtr> tablet_related_rs;
+        TxnManager::instance()->get_txn_related_tablets(transaction_id, partition_id, &tablet_related_rs);
+
+        Version version(partitionVersionInfo.version, partitionVersionInfo.version);
+        VersionHash version_hash = partitionVersionInfo.version_hash;
+
+        // each tablet
+        for (auto& tablet_rs : tablet_related_rs) {
+            OLAPStatus publish_status = OLAP_SUCCESS;
+            TabletInfo tablet_info = tablet_rs.first;
+            RowsetSharedPtr rowset = tablet_rs.second;
+            LOG(INFO) << ""begin to publish version on tablet. ""
+                    << ""tablet_id="" << tablet_info.tablet_id
+                    << "", schema_hash="" << tablet_info.schema_hash
+                    << "", version="" << version.first
+                    << "", version_hash="" << version_hash
+                    << "", transaction_id="" << transaction_id;
+            TabletSharedPtr tablet = TabletManager::instance()->get_tablet(tablet_info.tablet_id, tablet_info.schema_hash);
+
+            if (tablet.get() == NULL) {
+                OLAP_LOG_WARNING(""can't get tablet when publish version. [tablet_id=%ld schema_hash=%d]"",
+                                 tablet_info.tablet_id, tablet_info.schema_hash);
+                _error_tablet_ids->push_back(tablet_info.tablet_id);
+                res = OLAP_ERR_PUSH_TABLE_NOT_EXIST;
+                continue;
+            }
+
+            // publish version, one tablet only has one rowset for a single txn
+            // if one tablet has more than one rowset for a single txn, should consider part of rowset meta persist 
+            // failed, it is very difficault, so that not deal with this case
+            // get rowsets from txn manager according to tablet and txn id
+            // set rowset version 
+            // persist rowset meta
+            rowset->set_version(version);
+            // TODO(ygl): currently, tablet meta will be persist when add a new rowset, so that not persist rowset meta here","[{'comment': 'more than 120, too long', 'commenter': 'kangpinghuang'}]"
567,be/src/olap/task/engine_publish_version_task.cpp,"@@ -0,0 +1,129 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""olap/task/engine_publish_version_task.h""
+#include ""olap/data_dir.h""
+#include ""olap/rowset/rowset_meta_manager.h""
+#include ""olap/tablet_manager.h""
+#include <map>
+
+namespace doris {
+
+using std::map;
+
+EnginePublishVersionTask::EnginePublishVersionTask(TPublishVersionRequest& publish_version_req, vector<TTabletId>* error_tablet_ids)
+    : _publish_version_req(publish_version_req), 
+      _error_tablet_ids(error_tablet_ids) {}
+
+OLAPStatus EnginePublishVersionTask::finish() {
+    LOG(INFO) << ""begin to process publish version. transaction_id=""
+        << _publish_version_req.transaction_id;
+
+    int64_t transaction_id = _publish_version_req.transaction_id;
+    OLAPStatus res = OLAP_SUCCESS;
+
+    // each partition
+    for (const TPartitionVersionInfo& partitionVersionInfo
+         : _publish_version_req.partition_version_infos) {
+
+        int64_t partition_id = partitionVersionInfo.partition_id;
+        map<TabletInfo, RowsetSharedPtr> tablet_related_rs;
+        TxnManager::instance()->get_txn_related_tablets(transaction_id, partition_id, &tablet_related_rs);
+
+        Version version(partitionVersionInfo.version, partitionVersionInfo.version);
+        VersionHash version_hash = partitionVersionInfo.version_hash;
+
+        // each tablet
+        for (auto& tablet_rs : tablet_related_rs) {
+            OLAPStatus publish_status = OLAP_SUCCESS;
+            TabletInfo tablet_info = tablet_rs.first;
+            RowsetSharedPtr rowset = tablet_rs.second;
+            LOG(INFO) << ""begin to publish version on tablet. ""
+                    << ""tablet_id="" << tablet_info.tablet_id
+                    << "", schema_hash="" << tablet_info.schema_hash
+                    << "", version="" << version.first
+                    << "", version_hash="" << version_hash
+                    << "", transaction_id="" << transaction_id;
+            TabletSharedPtr tablet = TabletManager::instance()->get_tablet(tablet_info.tablet_id, tablet_info.schema_hash);
+
+            if (tablet.get() == NULL) {
+                OLAP_LOG_WARNING(""can't get tablet when publish version. [tablet_id=%ld schema_hash=%d]"",
+                                 tablet_info.tablet_id, tablet_info.schema_hash);
+                _error_tablet_ids->push_back(tablet_info.tablet_id);
+                res = OLAP_ERR_PUSH_TABLE_NOT_EXIST;
+                continue;
+            }
+
+            // publish version, one tablet only has one rowset for a single txn
+            // if one tablet has more than one rowset for a single txn, should consider part of rowset meta persist 
+            // failed, it is very difficault, so that not deal with this case
+            // get rowsets from txn manager according to tablet and txn id
+            // set rowset version 
+            // persist rowset meta
+            rowset->set_version(version);
+            // TODO(ygl): currently, tablet meta will be persist when add a new rowset, so that not persist rowset meta here
+            /**
+            publish_status = RowsetMetaManager::save(
+                tablet->data_dir()->get_meta(),
+                rowset->rowset_id(),
+                rowset->rowset_meta());
+            if (publish_status != OLAP_SUCCESS) {
+                LOG(WARNING) << ""save pending rowset failed. rowset_id:"" << rowset->rowset_id() 
+                             << ""tablet id: "" << tablet_info.tablet_id
+                             << ""txn id:"" << transaction_id;
+                error_tablet_ids->push_back(tablet_info.tablet_id);
+                res = OLAP_ERR_ROWSET_SAVE_FAILED;
+                continue;
+            }
+            */
+            // add visible rowset to tablet
+            publish_status = tablet->add_inc_rowset(*rowset);
+            if (publish_status != OLAP_SUCCESS) {
+                LOG(WARNING) << ""add visilbe rowset to tablet failed rowset_id:"" << rowset->rowset_id()
+                             << ""tablet id: "" << tablet_info.tablet_id
+                             << ""txn id:"" << transaction_id;
+                _error_tablet_ids->push_back(tablet_info.tablet_id);
+                res = OLAP_ERR_ROWSET_SAVE_FAILED;
+                continue;
+            }
+
+            if (publish_status == OLAP_SUCCESS) {
+                LOG(INFO) << ""publish version successfully on tablet. tablet="" << tablet->full_name()
+                          << "", transaction_id="" << transaction_id << "", version="" << version.first;
+                // delete rowset from meta env
+                RowsetMetaManager::remove(tablet->data_dir()->get_meta(), rowset->rowset_id());
+                // delete txn info
+                TxnManager::instance()->delete_txn(partition_id, transaction_id, 
+                    tablet_info.tablet_id, tablet_info.schema_hash);
+            } else {
+                OLAP_LOG_WARNING(""fail to publish version on tablet. ""","[{'comment': 'LOG(WARNING)', 'commenter': 'kangpinghuang'}]"
567,be/src/olap/txn_manager.cpp,"@@ -151,7 +151,7 @@ OLAPStatus TxnManager::delete_txn(
 }
 
 void TxnManager::get_tablet_related_txns(TabletSharedPtr tablet, int64_t* partition_id,
-                                            std::set<int64_t>* transaction_ids) {
+                 std::set<int64_t>* transaction_ids) {","[{'comment': 'invalid identity?', 'commenter': 'kangpinghuang'}]"
567,be/src/olap/txn_manager.cpp,"@@ -151,7 +151,7 @@ OLAPStatus TxnManager::delete_txn(
 }
 
 void TxnManager::get_tablet_related_txns(TabletSharedPtr tablet, int64_t* partition_id,
-                                            std::set<int64_t>* transaction_ids) {
+                 std::set<int64_t>* transaction_ids) {
     if (tablet.get() == nullptr || partition_id == nullptr || transaction_ids == nullptr) {
         OLAP_LOG_WARNING(""parameter is null when get transactions by tablet"");","[{'comment': 'LOG(WARNING)', 'commenter': 'kangpinghuang'}]"
567,be/src/olap/txn_manager.cpp,"@@ -172,9 +172,8 @@ void TxnManager::get_tablet_related_txns(TabletSharedPtr tablet, int64_t* partit
 }
 
 void TxnManager::get_txn_related_tablets(const TTransactionId transaction_id,
-                                        TPartitionId partition_id,
-                                        std::vector<TabletInfo>* tablet_infos,
-                                        std::vector<RowsetSharedPtr>* rowset_infos) {
+                 TPartitionId partition_id,
+                 std::map<TabletInfo, RowsetSharedPtr>* tablet_infos) {","[{'comment': 'invalid indentity', 'commenter': 'kangpinghuang'}]"
567,be/src/olap/txn_manager.cpp,"@@ -190,11 +189,7 @@ void TxnManager::get_txn_related_tablets(const TTransactionId transaction_id,
     // each tablet
     for (auto& load_info : load_info_map) {
         const TabletInfo& tablet_info = load_info.first;
-        tablet_infos->push_back(tablet_info);
-        if (rowset_infos != NULL) {
-            // TODO(ygl) check rowsetptr is null?
-            rowset_infos->push_back(load_info.second.second);
-        }
+	    tablet_infos->insert(std::make_pair(tablet_info,load_info.second.second));","[{'comment': 'invalid indent', 'commenter': 'kangpinghuang'}, {'comment': 'emplace_back(tablet_info, load_info.second.second) will be better than tablet_infos->insert(std::make_pair(tablet_info,load_info.second.second));', 'commenter': 'chaoyli'}]"
567,be/src/olap/txn_manager.cpp,"@@ -126,9 +126,8 @@ OLAPStatus TxnManager::add_txn(
     return OLAP_SUCCESS;
 }
 
-OLAPStatus TxnManager::delete_txn(
-    TPartitionId partition_id, TTransactionId transaction_id,
-    TTabletId tablet_id, SchemaHash schema_hash) {
+OLAPStatus TxnManager::delete_txn(TPartitionId partition_id, TTransactionId transaction_id,
+                                  TTabletId tablet_id, SchemaHash schema_hash) {
 
     pair<int64_t, int64_t> key(partition_id, transaction_id);
     TabletInfo tablet_info(tablet_id, schema_hash);","[{'comment': 'When to GC pending data?', 'commenter': 'chaoyli'}, {'comment': 'delete txn should not do clear pending data task, because a txn is published and will call delete_txn, if clear pending rowset, then the visible data is deleted.\r\nthe caller method should call rowset. remove to delete the data. or add the delete rowset to a queue and write a single gc thread to gc it.', 'commenter': 'yiguolei'}]"
567,be/src/olap/task/engine_publish_version_task.h,"@@ -0,0 +1,43 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#ifndef DORIS_BE_SRC_OLAP_TASK_ENGINE_PUBLISH_VERSION_TASK_H
+#define DORIS_BE_SRC_OLAP_TASK_ENGINE_PUBLISH_VERSION_TASK_H
+
+#include ""gen_cpp/AgentService_types.h""
+#include ""olap/olap_define.h""
+#include ""olap/task/engine_task.h""
+
+namespace doris {
+
+// base class for storage engine
+// add ""Engine"" as task prefix to prevent duplicate name with agent task
+class EnginePublishVersionTask : public EngineTask {
+
+public:
+    EnginePublishVersionTask(TPublishVersionRequest& publish_version_req, vector<TTabletId>* error_tablet_ids);
+    ~EnginePublishVersionTask() {}","[{'comment': 'No prepare() function', 'commenter': 'chaoyli'}, {'comment': 'prepare function is empty by default.', 'commenter': 'yiguolei'}]"
567,be/src/olap/task/engine_publish_version_task.cpp,"@@ -0,0 +1,132 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""olap/task/engine_publish_version_task.h""
+#include ""olap/data_dir.h""
+#include ""olap/rowset/rowset_meta_manager.h""
+#include ""olap/tablet_manager.h""
+#include <map>
+
+namespace doris {
+
+using std::map;
+
+EnginePublishVersionTask::EnginePublishVersionTask(TPublishVersionRequest& publish_version_req, 
+                                                   vector<TTabletId>* error_tablet_ids)
+    : _publish_version_req(publish_version_req), 
+      _error_tablet_ids(error_tablet_ids) {}
+
+OLAPStatus EnginePublishVersionTask::finish() {
+    LOG(INFO) << ""begin to process publish version. transaction_id=""
+        << _publish_version_req.transaction_id;
+
+    int64_t transaction_id = _publish_version_req.transaction_id;
+    OLAPStatus res = OLAP_SUCCESS;
+
+    // each partition
+    for (const TPartitionVersionInfo& partitionVersionInfo","[{'comment': 'for (auto& partitionVersionInfo : _publish_version_req.partition_version_infos)', 'commenter': 'chaoyli'}]"
567,be/src/olap/task/engine_publish_version_task.cpp,"@@ -0,0 +1,132 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""olap/task/engine_publish_version_task.h""
+#include ""olap/data_dir.h""
+#include ""olap/rowset/rowset_meta_manager.h""
+#include ""olap/tablet_manager.h""
+#include <map>
+
+namespace doris {
+
+using std::map;
+
+EnginePublishVersionTask::EnginePublishVersionTask(TPublishVersionRequest& publish_version_req, 
+                                                   vector<TTabletId>* error_tablet_ids)
+    : _publish_version_req(publish_version_req), 
+      _error_tablet_ids(error_tablet_ids) {}
+
+OLAPStatus EnginePublishVersionTask::finish() {
+    LOG(INFO) << ""begin to process publish version. transaction_id=""
+        << _publish_version_req.transaction_id;
+
+    int64_t transaction_id = _publish_version_req.transaction_id;
+    OLAPStatus res = OLAP_SUCCESS;
+
+    // each partition
+    for (const TPartitionVersionInfo& partitionVersionInfo
+         : _publish_version_req.partition_version_infos) {
+
+        int64_t partition_id = partitionVersionInfo.partition_id;
+        map<TabletInfo, RowsetSharedPtr> tablet_related_rs;
+        TxnManager::instance()->get_txn_related_tablets(transaction_id, partition_id, &tablet_related_rs);
+
+        Version version(partitionVersionInfo.version, partitionVersionInfo.version);
+        VersionHash version_hash = partitionVersionInfo.version_hash;
+
+        // each tablet
+        for (auto& tablet_rs : tablet_related_rs) {
+            OLAPStatus publish_status = OLAP_SUCCESS;
+            TabletInfo tablet_info = tablet_rs.first;
+            RowsetSharedPtr rowset = tablet_rs.second;
+            LOG(INFO) << ""begin to publish version on tablet. ""
+                    << ""tablet_id="" << tablet_info.tablet_id
+                    << "", schema_hash="" << tablet_info.schema_hash
+                    << "", version="" << version.first
+                    << "", version_hash="" << version_hash
+                    << "", transaction_id="" << transaction_id;
+            TabletSharedPtr tablet = TabletManager::instance()->get_tablet(tablet_info.tablet_id, tablet_info.schema_hash);
+
+            if (tablet.get() == NULL) {
+                LOG(WARNING) << ""can't get tablet when publish version. tablet_id="" << tablet_info.tablet_id
+                             << ""schema_hash="" << tablet_info.schema_hash;
+                _error_tablet_ids->push_back(tablet_info.tablet_id);
+                res = OLAP_ERR_PUSH_TABLE_NOT_EXIST;
+                continue;
+            }
+
+            // publish version, one tablet only has one rowset for a single txn
+            // if one tablet has more than one rowset for a single txn, 
+            // should consider part of rowset meta persist 
+            // failed, it is very difficault, so that not deal with this case
+            // get rowsets from txn manager according to tablet and txn id
+            // set rowset version 
+            // persist rowset meta
+            rowset->set_version(version);","[{'comment': 'rowset->set_version_hash()?', 'commenter': 'chaoyli'}]"
567,be/src/olap/task/engine_publish_version_task.cpp,"@@ -0,0 +1,132 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""olap/task/engine_publish_version_task.h""
+#include ""olap/data_dir.h""
+#include ""olap/rowset/rowset_meta_manager.h""
+#include ""olap/tablet_manager.h""
+#include <map>
+
+namespace doris {
+
+using std::map;
+
+EnginePublishVersionTask::EnginePublishVersionTask(TPublishVersionRequest& publish_version_req, 
+                                                   vector<TTabletId>* error_tablet_ids)
+    : _publish_version_req(publish_version_req), 
+      _error_tablet_ids(error_tablet_ids) {}
+
+OLAPStatus EnginePublishVersionTask::finish() {
+    LOG(INFO) << ""begin to process publish version. transaction_id=""
+        << _publish_version_req.transaction_id;
+
+    int64_t transaction_id = _publish_version_req.transaction_id;
+    OLAPStatus res = OLAP_SUCCESS;
+
+    // each partition
+    for (const TPartitionVersionInfo& partitionVersionInfo
+         : _publish_version_req.partition_version_infos) {
+
+        int64_t partition_id = partitionVersionInfo.partition_id;
+        map<TabletInfo, RowsetSharedPtr> tablet_related_rs;
+        TxnManager::instance()->get_txn_related_tablets(transaction_id, partition_id, &tablet_related_rs);
+
+        Version version(partitionVersionInfo.version, partitionVersionInfo.version);
+        VersionHash version_hash = partitionVersionInfo.version_hash;
+
+        // each tablet
+        for (auto& tablet_rs : tablet_related_rs) {
+            OLAPStatus publish_status = OLAP_SUCCESS;
+            TabletInfo tablet_info = tablet_rs.first;
+            RowsetSharedPtr rowset = tablet_rs.second;
+            LOG(INFO) << ""begin to publish version on tablet. ""
+                    << ""tablet_id="" << tablet_info.tablet_id
+                    << "", schema_hash="" << tablet_info.schema_hash
+                    << "", version="" << version.first
+                    << "", version_hash="" << version_hash
+                    << "", transaction_id="" << transaction_id;
+            TabletSharedPtr tablet = TabletManager::instance()->get_tablet(tablet_info.tablet_id, tablet_info.schema_hash);
+
+            if (tablet.get() == NULL) {
+                LOG(WARNING) << ""can't get tablet when publish version. tablet_id="" << tablet_info.tablet_id
+                             << ""schema_hash="" << tablet_info.schema_hash;
+                _error_tablet_ids->push_back(tablet_info.tablet_id);
+                res = OLAP_ERR_PUSH_TABLE_NOT_EXIST;
+                continue;
+            }
+
+            // publish version, one tablet only has one rowset for a single txn
+            // if one tablet has more than one rowset for a single txn, 
+            // should consider part of rowset meta persist 
+            // failed, it is very difficault, so that not deal with this case
+            // get rowsets from txn manager according to tablet and txn id
+            // set rowset version 
+            // persist rowset meta
+            rowset->set_version(version);
+            // TODO(ygl): currently, tablet meta will be persist when add a new rowset,
+            // so that not persist rowset meta here
+            /**
+            publish_status = RowsetMetaManager::save(
+                tablet->data_dir()->get_meta(),
+                rowset->rowset_id(),
+                rowset->rowset_meta());
+            if (publish_status != OLAP_SUCCESS) {
+                LOG(WARNING) << ""save pending rowset failed. rowset_id:"" << rowset->rowset_id() 
+                             << ""tablet id: "" << tablet_info.tablet_id
+                             << ""txn id:"" << transaction_id;
+                error_tablet_ids->push_back(tablet_info.tablet_id);
+                res = OLAP_ERR_ROWSET_SAVE_FAILED;
+                continue;
+            }
+            */
+            // add visible rowset to tablet","[{'comment': 'Where to add rowset into tablet?', 'commenter': 'chaoyli'}, {'comment': 'add_inc_rowset', 'commenter': 'yiguolei'}]"
567,be/src/olap/storage_engine.cpp,"@@ -115,96 +115,140 @@ StorageEngine::~StorageEngine() {
     clear();
 }
 
+// TODO(ygl): deal with rowsets and tablets when load failed
 OLAPStatus StorageEngine::_load_data_dir(DataDir* data_dir) {
     std::string data_dir_path = data_dir->path();
     LOG(INFO) <<""start to load tablets from data_dir_path:"" << data_dir_path;
-
     bool is_header_converted = false;
     OLAPStatus res = TabletMetaManager::get_header_converted(data_dir, is_header_converted);","[{'comment': '&is_header_converted', 'commenter': 'chaoyli'}]"
567,be/src/olap/storage_engine.cpp,"@@ -115,96 +115,140 @@ StorageEngine::~StorageEngine() {
     clear();
 }
 
+// TODO(ygl): deal with rowsets and tablets when load failed
 OLAPStatus StorageEngine::_load_data_dir(DataDir* data_dir) {
     std::string data_dir_path = data_dir->path();
     LOG(INFO) <<""start to load tablets from data_dir_path:"" << data_dir_path;
-
     bool is_header_converted = false;
     OLAPStatus res = TabletMetaManager::get_header_converted(data_dir, is_header_converted);
     if (res != OLAP_SUCCESS) {
         LOG(WARNING) << ""get convert flag from meta failed"";
         return res;
     }
+    if (!is_header_converted) {
+        // ygl: could not be compatable with old doris data. User has to use previous version to parse
+        // header file into meta env first.
+        LOG(WARNING) << ""header is not converted to tablet meta yet, could not use this Doris version. "" 
+                    << ""[dir path ="" << data_dir_path << ""]"";
+        return OLAP_ERR_INIT_FAILED;
+    }
 
-    // load rowset meta from metaenv and create rowset
+    // load rowset meta from meta env and create rowset
     // COMMITTED: add to txn manager
     // VISIBLE: add to tablet
     // if one rowset load failed, then the total data dir will not be loaded
-    std::vector<std::shared_ptr<RowsetMeta>> dir_rowset_metas;
-    if (is_header_converted) {
-        LOG(INFO) << ""begin loading rowset from meta"";
-        bool has_error = false;
-        auto load_rowset_func = [this, data_dir, &has_error, &dir_rowset_metas](RowsetId rowset_id, const std::string& meta_str) -> bool {
-            std::shared_ptr<RowsetMeta> rowset_meta(new RowsetMeta());
-            bool parsed = rowset_meta->init(meta_str);
-            if (!parsed) {
-                LOG(WARNING) << ""parse rowset meta string failed for rowset_id:"" << rowset_id;
-                has_error = true;
-                // return false will break meta iterator
-                return false;
-            }
-            dir_rowset_metas.push_back(rowset_meta);
+    std::vector<RowsetMetaSharedPtr> dir_rowset_metas;
+    LOG(INFO) << ""begin loading rowset from meta"";
+    auto load_rowset_func = [this, data_dir, &dir_rowset_metas](RowsetId rowset_id, const std::string& meta_str) -> bool {
+        RowsetMetaSharedPtr rowset_meta(new RowsetMeta());
+        bool parsed = rowset_meta->init(meta_str);
+        if (!parsed) {
+            LOG(WARNING) << ""parse rowset meta string failed for rowset_id:"" << rowset_id;
+            // return false will break meta iterator
             return true;
-        };
-        OLAPStatus s = RowsetMetaManager::traverse_rowset_metas(data_dir->get_meta(), load_rowset_func);
-        if (has_error) {
-            LOG(WARNING) << ""errors when load rowset meta from meta env, skip this data dir:"" << data_dir_path;
-            return OLAP_ERR_META_ITERATOR;
-        }
-        LOG(INFO) << ""load header from meta finished"";
-        if (s != OLAP_SUCCESS) {
-            LOG(WARNING) << ""errors when load rowset meta from meta env, skip this data dir:"" << data_dir_path;
-            return s;
-        } else {
-            return OLAP_SUCCESS;
         }
+        dir_rowset_metas.push_back(rowset_meta);
+        return true;
+    };
+    OLAPStatus load_rowset_status = RowsetMetaManager::traverse_rowset_metas(data_dir->get_meta(), load_rowset_func);
+
+    if (load_rowset_status != OLAP_SUCCESS) {
+        LOG(WARNING) << ""errors when load rowset meta from meta env, skip this data dir:"" << data_dir_path;
+    } else {
+        LOG(INFO) << ""load rowset from meta finished, data dir: "" << data_dir_path;
     }
+
     // load tablet
     // create tablet from tablet meta and add it to tablet mgr
-    if (is_header_converted) {
-        LOG(INFO) << ""load header from meta"";
-        auto load_tablet_func = [this, data_dir](long tablet_id,
-            long schema_hash, const std::string& value) -> bool {
-            
-            OLAPStatus status = TabletManager::instance()->load_tablet_from_header(data_dir, tablet_id, schema_hash, value);
-            if (status != OLAP_SUCCESS) {
-                LOG(WARNING) << ""load tablet from header failed. status:"" << status
-                    << ""tablet="" << tablet_id << ""."" << schema_hash;
-            };
-            return true;
+    LOG(INFO) << ""begin loading tablet from meta"";
+    auto load_tablet_func = [this, data_dir](long tablet_id,
+        long schema_hash, const std::string& value) -> bool {
+        OLAPStatus status = TabletManager::instance()->load_tablet_from_header(data_dir, tablet_id, schema_hash, value);","[{'comment': 'load_tablet_from_meta?', 'commenter': 'chaoyli'}]"
567,be/src/olap/storage_engine.cpp,"@@ -1002,4 +956,60 @@ OLAPStatus StorageEngine::load_header(
     return res;
 }
 
+
+OLAPStatus StorageEngine::execute_task(EngineTask* task) {
+    // 1. add wlock to related tablets
+    // 2. do prepare work
+    // 3. release wlock
+    {
+        vector<TabletInfo> tablet_infos;
+        task->get_related_tablets(&tablet_infos);
+        sort(tablet_infos.begin(), tablet_infos.end());
+        for (TabletInfo& tablet_info : tablet_infos) {
+            TabletSharedPtr tablet = TabletManager::instance()->get_tablet(
+                tablet_info.tablet_id, tablet_info.schema_hash, false);
+            tablet->obtain_header_wrlock();
+        }
+        // add write lock to all related tablets
+        OLAPStatus prepare_status = task->prepare();
+        for (TabletInfo& tablet_info : tablet_infos) {
+            TabletSharedPtr tablet = TabletManager::instance()->get_tablet(
+                tablet_info.tablet_id, tablet_info.schema_hash, false);
+            tablet->release_header_lock();
+        }
+        if (prepare_status != OLAP_SUCCESS) {
+            return prepare_status;
+        }
+    }
+
+    // do execute work without lock
+    OLAPStatus exec_status = task->execute();
+    if (exec_status != OLAP_SUCCESS) {
+        return exec_status;
+    }
+    
+    // 1. add wlock to related tablets
+    // 2. do finish work
+    // 3. release wlock
+    {
+        vector<TabletInfo> tablet_infos;","[{'comment': 'why not use vector<TabletSharedPtr> tablets instead?', 'commenter': 'chaoyli'}]"
567,be/src/olap/task/engine_publish_version_task.h,"@@ -0,0 +1,43 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#ifndef DORIS_BE_SRC_OLAP_TASK_ENGINE_PUBLISH_VERSION_TASK_H
+#define DORIS_BE_SRC_OLAP_TASK_ENGINE_PUBLISH_VERSION_TASK_H
+
+#include ""gen_cpp/AgentService_types.h""
+#include ""olap/olap_define.h""
+#include ""olap/task/engine_task.h""
+
+namespace doris {
+
+// base class for storage engine
+// add ""Engine"" as task prefix to prevent duplicate name with agent task
+class EnginePublishVersionTask : public EngineTask {
+
+public:
+    EnginePublishVersionTask(TPublishVersionRequest& publish_version_req, vector<TTabletId>* error_tablet_ids);","[{'comment': 'what the use of error_tablet_ids?', 'commenter': 'chaoyli'}]"
575,be/src/olap/rowset/alpha_rowset.cpp,"@@ -117,9 +136,13 @@ int64_t AlphaRowset::start_version() const {
     return _rowset_meta->version().first;
 }
 
-bool AlphaRowset::create_hard_links(std::vector<std::string>* success_links) {
+bool AlphaRowset::create_files_with_new_name(std::vector<std::string>* success_links) {","[{'comment': 'make_snapshot will be better?', 'commenter': 'chaoyli'}]"
575,be/src/olap/rowset/alpha_rowset.cpp,"@@ -75,6 +90,10 @@ RowsetMetaSharedPtr AlphaRowset::rowset_meta() const {
 }
 
 void AlphaRowset::set_version(Version version) {
+    if (_removed) {","[{'comment': 'I think Rowset is already removed in memory structure synchronously if _removed is true.\r\nSo this judgement will be redundant.', 'commenter': 'chaoyli'}]"
575,be/src/olap/rowset/alpha_rowset.cpp,"@@ -60,9 +68,16 @@ OLAPStatus AlphaRowset::copy(RowsetWriter* dest_rowset_writer) {
 }
 
 OLAPStatus AlphaRowset::remove() {
-    // TODO(hkp) : add delete code
-    // delete rowset from meta
-    // delete segment groups 
+    OlapMeta* meta = _data_dir->get_meta();
+    OLAPStatus status = RowsetMetaManager::remove(meta, rowset_id());","[{'comment': 'If process core dump after remove rowset_meta, how to remove_files in segment_group?', 'commenter': 'chaoyli'}, {'comment': 'file gc', 'commenter': 'kangpinghuang'}]"
588,be/src/olap/tablet.h,"@@ -75,10 +75,9 @@ class Tablet : public std::enable_shared_from_this<Tablet> {
     OLAPStatus save_tablet_meta();
 
     void delete_expire_incremental_data();
-    OLAPStatus publish_version(int64_t transaction_id, Version version, VersionHash version_hash);
     const PDelta* get_incremental_delta(Version version) const;
-    OLAPStatus clone_data(const TabletMeta& clone_header,
-                          const std::vector<const PDelta*>& clone_deltas,
+    OLAPStatus clone_data(const TabletMeta& tablet_meta,
+                          const std::vector<RowsetMetaSharedPtr>& clone_deltas,","[{'comment': 'clone_deltas -> clone_rowsets?', 'commenter': 'kangpinghuang'}, {'comment': 'I will fix it', 'commenter': 'chaoyli'}]"
588,be/src/olap/task/engine_clone_task.cpp,"@@ -586,11 +586,11 @@ OLAPStatus EngineCloneTask::_finish_clone(TabletSharedPtr tablet, const string&
         }
 
         // load src header
-        string clone_header_file = clone_dir + ""/"" + std::to_string(tablet->tablet_id()) + "".hdr"";
-        TabletMeta clone_header(clone_header_file);
-        if ((res = clone_header.load_and_init()) != OLAP_SUCCESS) {
-            OLAP_LOG_WARNING(""fail to load src header when clone. [clone_header_file=%s]"",
-                             clone_header_file.c_str());
+        string cloned_tablet_meta_file = clone_dir + ""/"" + std::to_string(tablet->tablet_id()) + "".hdr"";
+        TabletMeta cloned_tablet_meta(cloned_tablet_meta_file);
+        if ((res = cloned_tablet_meta.load_and_init()) != OLAP_SUCCESS) {
+            OLAP_LOG_WARNING(""fail to load src header when clone. [cloned_tablet_meta_file=%s]"",","[{'comment': 'LOG(WARNING)', 'commenter': 'kangpinghuang'}, {'comment': 'OK', 'commenter': 'chaoyli'}]"
588,be/src/olap/task/engine_clone_task.cpp,"@@ -666,7 +666,7 @@ OLAPStatus EngineCloneTask::_finish_clone(TabletSharedPtr tablet, const string&
 }
 
 
-OLAPStatus EngineCloneTask::_clone_incremental_data(TabletSharedPtr tablet, TabletMeta& clone_header,
+OLAPStatus EngineCloneTask::_clone_incremental_data(TabletSharedPtr tablet, TabletMeta& cloned_tablet_meta,","[{'comment': 'TabletMeta& cloned_tablet_meta -> TabletMeta* cloned_tablet_met', 'commenter': 'kangpinghuang'}, {'comment': 'OK', 'commenter': 'chaoyli'}]"
596,be/src/olap/olap_common.h,"@@ -169,6 +170,7 @@ enum AlterTableStatus {
     ALTER_TABLE_FINISHED = 2,
     ALTER_TABLE_FAILED = 3,
 };
+*/","[{'comment': 'delete these codes?', 'commenter': 'kangpinghuang'}, {'comment': 'OK', 'commenter': 'chaoyli'}]"
596,be/src/olap/rowset/alpha_rowset_writer.cpp,"@@ -160,9 +161,9 @@ RowsetSharedPtr AlphaRowsetWriter::build() {
             alpha_rowset_meta->add_segment_group(segment_group_pb);
         }
     }
-    Rowset* rowset = new AlphaRowset(_rowset_writer_context.tablet_schema,
-                                     _rowset_writer_context.rowset_path_prefix,
-                                     _rowset_writer_context.data_dir, _current_rowset_meta);
+    RowsetSharedPtr rowset(new AlphaRowset(_rowset_writer_context.tablet_schema,
+                                    _rowset_writer_context.rowset_path_prefix,
+                                    _rowset_writer_context.data_dir, _current_rowset_meta));
     rowset->init();
     return std::shared_ptr<Rowset>(rowset);","[{'comment': 'return rowset directly', 'commenter': 'kangpinghuang'}, {'comment': 'OK', 'commenter': 'chaoyli'}]"
596,be/src/olap/snapshot_manager.cpp,"@@ -190,41 +192,11 @@ OLAPStatus SnapshotManager::_link_index_and_data_files(
         const vector<VersionEntity>& version_entity_vec) {
     OLAPStatus res = OLAP_SUCCESS;
 
-    std::stringstream prefix_stream;
-    prefix_stream << schema_hash_path << ""/"" << ref_tablet->tablet_id();
-    std::string tablet_path_prefix = prefix_stream.str();
-    for (const VersionEntity& entity : version_entity_vec) {
+    for (auto& entity : version_entity_vec) {
         Version version = entity.version;
-        VersionHash v_hash = entity.version_hash;
-        for (SegmentGroupEntity segment_group_entity : entity.segment_group_vec) {
-            int32_t segment_group_id = segment_group_entity.segment_group_id;
-            for (int seg_id = 0; seg_id < segment_group_entity.num_segments; ++seg_id) {
-                std::string index_path =
-                    construct_index_file_path(tablet_path_prefix, version, v_hash, segment_group_id, seg_id);
-                std::string ref_tablet_index_path =
-                    ref_tablet->construct_index_file_path(version, v_hash, segment_group_id, seg_id);
-                res = _create_hard_link(ref_tablet_index_path, index_path);
-                if (res != OLAP_SUCCESS) {
-                    LOG(WARNING) << ""fail to create hard link. ""
-                        << "" schema_hash_path="" << schema_hash_path
-                        << "" from_path="" << ref_tablet_index_path
-                        << "" to_path="" << index_path;
-                    return res;
-                }
-
-                std:: string data_path =
-                    construct_data_file_path(tablet_path_prefix, version, v_hash, segment_group_id, seg_id);
-                std::string ref_tablet_data_path =
-                    ref_tablet->construct_data_file_path(version, v_hash, segment_group_id, seg_id);
-                res = _create_hard_link(ref_tablet_data_path, data_path);
-                if (res != OLAP_SUCCESS) {
-                    LOG(WARNING) << ""fail to create hard link.""
-                        << ""tablet_path_prefix="" << tablet_path_prefix
-                        << "", from_path="" << ref_tablet_data_path << "", to_path="" << data_path;
-                    return res;
-                }
-            }
-        }
+        const RowsetSharedPtr rowset = ref_tablet->get_rowset_by_version(version);
+        std::vector<std::string> success_files;
+        RETURN_NOT_OK(rowset->make_snapshot(&success_files));","[{'comment': 'this make_snapshot can not do snapshot as above. Add new api to realize it.', 'commenter': 'kangpinghuang'}]"
596,be/src/olap/snapshot_manager.cpp,"@@ -453,46 +425,25 @@ OLAPStatus SnapshotManager::_create_incremental_snapshot_files(
             break;
         }
 
-        for (int64_t missing_version : request.missing_version) {
-
-            // find missing version
-            const PDelta* incremental_delta =
-                ref_tablet->get_incremental_delta(Version(missing_version, missing_version));
-            if (incremental_delta != nullptr) {
-                VLOG(3) << ""success to find missing version when snapshot, ""
+        for (int64_t missed_version : request.missing_version) {
+            Version version = { missed_version, missed_version };
+            const RowsetSharedPtr rowset = ref_tablet->get_rowset_by_version(version);
+            if (rowset != nullptr) {
+                VLOG(3) << ""success to find miss version when snapshot, ""
                         << ""begin to link files. tablet_id="" << request.tablet_id
                         << "", schema_hash="" << request.schema_hash
-                        << "", version="" << missing_version;
-                // link files
-                for (uint32_t i = 0; i < incremental_delta->segment_group(0).num_segments(); i++) {
-                    int32_t segment_group_id = incremental_delta->segment_group(0).segment_group_id();
-                    string from = ref_tablet->construct_incremental_index_file_path(
-                                Version(missing_version, missing_version),
-                                incremental_delta->version_hash(), segment_group_id, i);
-                    string to = schema_full_path + '/' + basename(from.c_str());
-                    if ((res = _create_hard_link(from, to)) != OLAP_SUCCESS) {
-                        break;
-                    }
-
-                    from = ref_tablet->construct_incremental_data_file_path(
-                                Version(missing_version, missing_version),
-                                incremental_delta->version_hash(), segment_group_id, i);
-                    to = schema_full_path + '/' + basename(from.c_str());
-                    if ((res = _create_hard_link(from, to)) != OLAP_SUCCESS) {
-                        break;
-                    }
-                }
-
-                if (res != OLAP_SUCCESS) {
-                    break;
-                }
-
+                        << "", version="" << version.first << ""-"" << version.second;
+                std::vector<std::string> success_files;
+                res = rowset->make_snapshot(&success_files);","[{'comment': 'this should be modified', 'commenter': 'kangpinghuang'}]"
596,be/src/olap/tablet.cpp,"@@ -54,8 +56,62 @@ using std::vector;
 
 namespace doris {
 
-Tablet::Tablet(TabletMeta* tablet_meta, DataDir* data_dir) {
-    _data_dir = data_dir;
+TabletSharedPtr Tablet::create_from_tablet_meta_file(
+            const string& file_path, DataDir* data_dir) {
+    TabletMeta* tablet_meta = NULL;
+    tablet_meta = new(nothrow) TabletMeta();
+    if (tablet_meta == nullptr) {
+        LOG(WARNING) << ""fail to malloc TabletMeta."";
+        return NULL;
+    }
+
+    if (tablet_meta->create_from_file(file_path) != OLAP_SUCCESS) {
+        LOG(WARNING) << ""fail to load tablet_meta. file_path="" << file_path;
+        delete tablet_meta;
+        return nullptr;
+    }
+
+    // add new fields
+    boost::filesystem::path file_path_path(file_path);
+    std::string shard_path = file_path_path.parent_path().parent_path().parent_path().string();
+    std::string shard_str = shard_path.substr(shard_path.find_last_of('/') + 1);
+    uint64_t shard = stol(shard_str);
+    tablet_meta->set_shard_id(shard);
+
+    // save tablet_meta info to kv db
+    // tablet_meta key format: tablet_id + ""_"" + schema_hash
+    OLAPStatus res = TabletMetaManager::save(data_dir, tablet_meta->tablet_id(),
+                                             tablet_meta->schema_hash(), tablet_meta);
+    if (res != OLAP_SUCCESS) {
+        OLAP_LOG_WARNING(""fail to save tablet_meta to db. [file_path=%s]"", file_path.c_str());","[{'comment': 'LOG(WARNING)', 'commenter': 'kangpinghuang'}, {'comment': 'OK', 'commenter': 'chaoyli'}]"
596,be/src/olap/tablet.cpp,"@@ -65,9 +121,54 @@ Tablet::~Tablet() {
     _rs_version_map.clear();
 }
 
+OLAPStatus Tablet::load() {
+    OLAPStatus res = OLAP_SUCCESS;
+    MutexLock l(&_load_lock);
+
+    if (_is_loaded) {
+        return OLAP_SUCCESS;
+    }
+
+    res = load_indices();","[{'comment': 'I think load_indices should be renamed to load_rowsets?', 'commenter': 'kangpinghuang'}, {'comment': 'OK', 'commenter': 'chaoyli'}]"
596,be/src/olap/tablet.cpp,"@@ -503,6 +633,11 @@ void Tablet::list_versions(vector<Version>* versions) const {
     }
 }
 
+void Tablet::list_version_entities(vector<VersionEntity>* version_entities) const {
+    DCHECK(version_entities != nullptr && version_entities->empty());
+    return;
+}
+","[{'comment': 'what is the difference between list_version_entities and list_entities?\r\nand it seems that list_version_entities do not has realization?', 'commenter': 'kangpinghuang'}, {'comment': 'I will remove list_version_entities function.', 'commenter': 'chaoyli'}]"
596,be/src/olap/tablet.cpp,"@@ -401,7 +530,8 @@ OLAPStatus Tablet::split_range(
     }
 
     ReadLock rdlock(get_header_lock_ptr());
-    SegmentGroup* base_index = get_largest_index();
+    //SegmentGroup* base_index = get_largest_index();
+    SegmentGroup* base_index = nullptr;","[{'comment': 'SegmentGroup can not be used here', 'commenter': 'kangpinghuang'}]"
596,be/src/olap/tablet.cpp,"@@ -638,45 +744,213 @@ OLAPStatus Tablet::register_tablet_into_dir() {
     return _data_dir->register_tablet(this);
 }
 
-OLAPStatus Tablet::clear_schema_change_info(
-        AlterTabletType* type,
-        bool only_one,
-        bool check_only) {
-    if (!check_only) {
-        WriteLock w(&_meta_lock);
-        _unprotect_clear_schema_change_info(type, only_one, check_only);
-    } else {
-        ReadLock r(&_meta_lock);
-        _unprotect_clear_schema_change_info(type, only_one, check_only);
+bool Tablet::version_for_delete_predicate(const Version& version) {
+    return _tablet_meta->version_for_delete_predicate(version);
+}
+
+bool Tablet::version_for_load_deletion(const Version& version) {
+    RowsetSharedPtr rowset = _rs_version_map.at(version);
+    return rowset->delete_flag();
+}
+
+const RowsetSharedPtr Tablet::get_rowset_by_version(const Version& version) const {
+    RowsetSharedPtr rowset = _rs_version_map.at(version);
+    return rowset;
+}
+
+const RowsetSharedPtr Tablet::rowset_with_max_version() const {
+    Version max_version = _tablet_meta->max_version();
+    RowsetSharedPtr rowset = _rs_version_map.at(max_version);
+    return rowset;
+}
+
+std::string Tablet::storage_root_path_name() const {
+    return _data_dir->path();
+}
+
+std::string Tablet::tablet_path() const {
+    return _tablet_path;
+}
+
+const uint32_t Tablet::calc_cumulative_compaction_score() const {
+    uint32_t score = 0;
+    bool base_rowset_exist = false;
+    const int32_t point = cumulative_layer_point();
+    for (auto& rs_meta : _tablet_meta->all_rs_metas()) {
+        if (rs_meta->start_version() >= point) {
+            score++;
+        }
+        if (rs_meta->start_version() == 0) {
+            base_rowset_exist = true;
+        }
     }
-    return OLAP_SUCCESS;
+
+    // base不存在可能是tablet正在做alter table，先不选它，设score=0
+    return base_rowset_exist ? score : 0;
 }
 
-OLAPStatus Tablet::_unprotect_clear_schema_change_info(
-        AlterTabletType* type,
-        bool only_one,
-        bool check_only) {
-    OLAPStatus res = OLAP_SUCCESS;
+const uint32_t Tablet::calc_base_compaction_score() const {
+    uint32_t score = 0;
+    const int32_t point = cumulative_layer_point();
+    bool base_rowset_exist = false;
+    for (auto& rs_meta : _tablet_meta->all_rs_metas()) {
+        if (rs_meta->start_version() < point) {
+            score++;
+        }
+        if (rs_meta->start_version() == 0) {
+            base_rowset_exist = true;
+        }
+    }
+    score = score < config::base_compaction_num_cumulative_deltas ? 0 : score;
 
-    vector<Version> versions_to_be_changed;
-    if (this->get_schema_change_request(NULL,
-                                              NULL,
-                                              &versions_to_be_changed,
-                                              NULL)) {
-        if (versions_to_be_changed.size() != 0) {
-            OLAP_LOG_WARNING(""schema change is not allowed now, ""
-                             ""until previous schema change is done. [tablet='%s']"",
-                             full_name().c_str());
-            return OLAP_ERR_PREVIOUS_SCHEMA_CHANGE_NOT_FINISHED;
+    // base不存在可能是tablet正在做alter table，先不选它，设score=0
+    return base_rowset_exist ? score : 0;
+}
+
+bool Tablet::has_expired_incremental_rowset() {
+    bool exist = false;
+    time_t now = time(NULL);
+    ReadLock rdlock(&_meta_lock);
+    for (auto& rs_meta : _tablet_meta->all_inc_rs_metas()) {
+        double diff = difftime(now, rs_meta->creation_time());","[{'comment': 'I think here can be optimized if inc_rs_metas are ordered by creation_time asc. You can only check the first one to decide  whether there is expired rowset.', 'commenter': 'kangpinghuang'}, {'comment': 'Order by creation_time will lose the order property of version ', 'commenter': 'chaoyli'}]"
596,be/src/olap/tablet.cpp,"@@ -638,45 +744,213 @@ OLAPStatus Tablet::register_tablet_into_dir() {
     return _data_dir->register_tablet(this);
 }
 
-OLAPStatus Tablet::clear_schema_change_info(
-        AlterTabletType* type,
-        bool only_one,
-        bool check_only) {
-    if (!check_only) {
-        WriteLock w(&_meta_lock);
-        _unprotect_clear_schema_change_info(type, only_one, check_only);
-    } else {
-        ReadLock r(&_meta_lock);
-        _unprotect_clear_schema_change_info(type, only_one, check_only);
+bool Tablet::version_for_delete_predicate(const Version& version) {
+    return _tablet_meta->version_for_delete_predicate(version);
+}
+
+bool Tablet::version_for_load_deletion(const Version& version) {
+    RowsetSharedPtr rowset = _rs_version_map.at(version);
+    return rowset->delete_flag();
+}
+
+const RowsetSharedPtr Tablet::get_rowset_by_version(const Version& version) const {
+    RowsetSharedPtr rowset = _rs_version_map.at(version);
+    return rowset;
+}
+
+const RowsetSharedPtr Tablet::rowset_with_max_version() const {
+    Version max_version = _tablet_meta->max_version();
+    RowsetSharedPtr rowset = _rs_version_map.at(max_version);
+    return rowset;
+}
+
+std::string Tablet::storage_root_path_name() const {
+    return _data_dir->path();
+}
+
+std::string Tablet::tablet_path() const {
+    return _tablet_path;
+}
+
+const uint32_t Tablet::calc_cumulative_compaction_score() const {
+    uint32_t score = 0;
+    bool base_rowset_exist = false;
+    const int32_t point = cumulative_layer_point();
+    for (auto& rs_meta : _tablet_meta->all_rs_metas()) {
+        if (rs_meta->start_version() >= point) {
+            score++;
+        }
+        if (rs_meta->start_version() == 0) {
+            base_rowset_exist = true;
+        }
     }
-    return OLAP_SUCCESS;
+
+    // base不存在可能是tablet正在做alter table，先不选它，设score=0
+    return base_rowset_exist ? score : 0;
 }
 
-OLAPStatus Tablet::_unprotect_clear_schema_change_info(
-        AlterTabletType* type,
-        bool only_one,
-        bool check_only) {
-    OLAPStatus res = OLAP_SUCCESS;
+const uint32_t Tablet::calc_base_compaction_score() const {
+    uint32_t score = 0;
+    const int32_t point = cumulative_layer_point();
+    bool base_rowset_exist = false;
+    for (auto& rs_meta : _tablet_meta->all_rs_metas()) {
+        if (rs_meta->start_version() < point) {
+            score++;
+        }
+        if (rs_meta->start_version() == 0) {
+            base_rowset_exist = true;
+        }
+    }
+    score = score < config::base_compaction_num_cumulative_deltas ? 0 : score;
 
-    vector<Version> versions_to_be_changed;
-    if (this->get_schema_change_request(NULL,
-                                              NULL,
-                                              &versions_to_be_changed,
-                                              NULL)) {
-        if (versions_to_be_changed.size() != 0) {
-            OLAP_LOG_WARNING(""schema change is not allowed now, ""
-                             ""until previous schema change is done. [tablet='%s']"",
-                             full_name().c_str());
-            return OLAP_ERR_PREVIOUS_SCHEMA_CHANGE_NOT_FINISHED;
+    // base不存在可能是tablet正在做alter table，先不选它，设score=0
+    return base_rowset_exist ? score : 0;
+}
+
+bool Tablet::has_expired_incremental_rowset() {
+    bool exist = false;
+    time_t now = time(NULL);
+    ReadLock rdlock(&_meta_lock);
+    for (auto& rs_meta : _tablet_meta->all_inc_rs_metas()) {
+        double diff = difftime(now, rs_meta->creation_time());
+        if (diff >= config::inc_rowset_expired_sec) {
+            exist = true;
+            break;
         }
     }
+    return exist;
+}
 
-    if (!check_only) {
-        VLOG(3) << ""broke old schema change chain"";
-        clear_schema_change_request();
+void Tablet::delete_expired_incremental_rowset() {
+    time_t now = time(NULL);
+    std::vector<std::pair<Version, VersionHash>> expired_versions;
+    std::vector<string> files_to_remove;
+    WriteLock wrlock(&_meta_lock);
+    for (auto& rs_meta : _tablet_meta->all_inc_rs_metas()) {
+        double diff = difftime(now, rs_meta->creation_time());
+        if (diff >= config::inc_rowset_expired_sec) {
+            Version version(rs_meta->start_version(), rs_meta->end_version());
+            expired_versions.push_back(std::make_pair(version, rs_meta->version_hash()));
+            VLOG(3) << ""find expire incremental rowset. tablet="" << full_name() << "", ""
+                    << ""version="" << rs_meta->start_version() << ""-"" << rs_meta->end_version() << "", ""
+                    << ""exist_sec="" << diff;
+        }
     }
 
+    if (expired_versions.empty()) { return; }
+
+    for (auto& pair: expired_versions) {
+        _delete_incremental_rowset(pair.first, pair.second, &files_to_remove);
+        VLOG(3) << ""delete expire incremental data. table="" << full_name() << "", ""
+                << ""version="" << pair.first.first << ""-"" << pair.first.second;
+    }
+
+    if (save_tablet_meta() != OLAP_SUCCESS) {
+        LOG(FATAL) << ""fail to save tablet_meta when delete expire incremental data.""
+                   << ""tablet="" << full_name();
+    }
+    remove_files(files_to_remove);
+}
+
+OLAPStatus Tablet::clone_data(const TabletMeta& tablet_meta,
+                              const std::vector<RowsetMetaSharedPtr>& rowsets_to_clone,
+                              const std::vector<Version>& versions_to_delete) {
+    LOG(INFO) << ""begin to clone data to tablet. tablet="" << full_name()
+              << "", rowsets_to_clone="" << rowsets_to_clone.size()
+              << "", versions_to_delete_size="" << versions_to_delete.size();
+    OLAPStatus res = OLAP_SUCCESS;
+    do {
+        // load new local tablet_meta to operate on
+        TabletMeta new_tablet_meta;
+        TabletMetaManager::get_header(_data_dir, tablet_id(), schema_hash(), &new_tablet_meta);
+
+        // delete versions from new local tablet_meta
+        for (const Version& version : versions_to_delete) {
+            res = new_tablet_meta.delete_rowset_by_version(version);
+            if (res != OLAP_SUCCESS) {
+                LOG(WARNING) << ""failed to delete version from new local tablet meta. tablet="" << full_name()
+                             << "", version="" << version.first << ""-"" << version.second;
+                break;
+            }
+            if (new_tablet_meta.version_for_delete_predicate(version)) {
+                new_tablet_meta.remove_delete_predicate_by_version(version);
+            }
+            LOG(INFO) << ""delete version from new local tablet_meta when clone. [table='"" << full_name()
+                      << ""', version="" << version.first << ""-"" << version.second << ""]"";
+        }
+
+        if (res != OLAP_SUCCESS) {
+            break;
+        }
+
+        for (auto& rs_meta : rowsets_to_clone) {
+            Version version(rs_meta->start_version(), rs_meta->end_version());
+            new_tablet_meta.add_rs_meta(rs_meta);
+
+            // add delete conditions to new local tablet_meta, if it exists in tablet_meta
+            if (version.first == version.second) {
+                for (auto it = tablet_meta.delete_predicates().begin();
+                     it != tablet_meta.delete_predicates().end(); ++it) {
+                    if (it->version() == version.first) {
+                        // add it
+                        new_tablet_meta.add_delete_predicate(*it, version.first);
+                        LOG(INFO) << ""add delete condition when clone. [table="" << full_name()
+                                  << "" version="" << it->version() << ""]"";
+                        break;
+                    }
+                }
+            }
+        }
+
+        if (res != OLAP_SUCCESS) {
+            break;
+        }
+
+       VLOG(3) << ""load indices successfully when clone. tablet="" << full_name()","[{'comment': 'indices -> rowsets?', 'commenter': 'kangpinghuang'}, {'comment': 'OK', 'commenter': 'chaoyli'}]"
596,be/src/olap/tablet.cpp,"@@ -638,45 +744,213 @@ OLAPStatus Tablet::register_tablet_into_dir() {
     return _data_dir->register_tablet(this);
 }
 
-OLAPStatus Tablet::clear_schema_change_info(
-        AlterTabletType* type,
-        bool only_one,
-        bool check_only) {
-    if (!check_only) {
-        WriteLock w(&_meta_lock);
-        _unprotect_clear_schema_change_info(type, only_one, check_only);
-    } else {
-        ReadLock r(&_meta_lock);
-        _unprotect_clear_schema_change_info(type, only_one, check_only);
+bool Tablet::version_for_delete_predicate(const Version& version) {
+    return _tablet_meta->version_for_delete_predicate(version);
+}
+
+bool Tablet::version_for_load_deletion(const Version& version) {
+    RowsetSharedPtr rowset = _rs_version_map.at(version);
+    return rowset->delete_flag();
+}
+
+const RowsetSharedPtr Tablet::get_rowset_by_version(const Version& version) const {
+    RowsetSharedPtr rowset = _rs_version_map.at(version);
+    return rowset;
+}
+
+const RowsetSharedPtr Tablet::rowset_with_max_version() const {
+    Version max_version = _tablet_meta->max_version();
+    RowsetSharedPtr rowset = _rs_version_map.at(max_version);
+    return rowset;
+}
+
+std::string Tablet::storage_root_path_name() const {
+    return _data_dir->path();
+}
+
+std::string Tablet::tablet_path() const {
+    return _tablet_path;
+}
+
+const uint32_t Tablet::calc_cumulative_compaction_score() const {
+    uint32_t score = 0;
+    bool base_rowset_exist = false;
+    const int32_t point = cumulative_layer_point();
+    for (auto& rs_meta : _tablet_meta->all_rs_metas()) {
+        if (rs_meta->start_version() >= point) {
+            score++;
+        }
+        if (rs_meta->start_version() == 0) {
+            base_rowset_exist = true;
+        }
     }
-    return OLAP_SUCCESS;
+
+    // base不存在可能是tablet正在做alter table，先不选它，设score=0
+    return base_rowset_exist ? score : 0;
 }
 
-OLAPStatus Tablet::_unprotect_clear_schema_change_info(
-        AlterTabletType* type,
-        bool only_one,
-        bool check_only) {
-    OLAPStatus res = OLAP_SUCCESS;
+const uint32_t Tablet::calc_base_compaction_score() const {
+    uint32_t score = 0;
+    const int32_t point = cumulative_layer_point();
+    bool base_rowset_exist = false;
+    for (auto& rs_meta : _tablet_meta->all_rs_metas()) {
+        if (rs_meta->start_version() < point) {
+            score++;
+        }
+        if (rs_meta->start_version() == 0) {
+            base_rowset_exist = true;
+        }
+    }
+    score = score < config::base_compaction_num_cumulative_deltas ? 0 : score;
 
-    vector<Version> versions_to_be_changed;
-    if (this->get_schema_change_request(NULL,
-                                              NULL,
-                                              &versions_to_be_changed,
-                                              NULL)) {
-        if (versions_to_be_changed.size() != 0) {
-            OLAP_LOG_WARNING(""schema change is not allowed now, ""
-                             ""until previous schema change is done. [tablet='%s']"",
-                             full_name().c_str());
-            return OLAP_ERR_PREVIOUS_SCHEMA_CHANGE_NOT_FINISHED;
+    // base不存在可能是tablet正在做alter table，先不选它，设score=0
+    return base_rowset_exist ? score : 0;
+}
+
+bool Tablet::has_expired_incremental_rowset() {
+    bool exist = false;
+    time_t now = time(NULL);
+    ReadLock rdlock(&_meta_lock);
+    for (auto& rs_meta : _tablet_meta->all_inc_rs_metas()) {
+        double diff = difftime(now, rs_meta->creation_time());
+        if (diff >= config::inc_rowset_expired_sec) {
+            exist = true;
+            break;
         }
     }
+    return exist;
+}
 
-    if (!check_only) {
-        VLOG(3) << ""broke old schema change chain"";
-        clear_schema_change_request();
+void Tablet::delete_expired_incremental_rowset() {
+    time_t now = time(NULL);
+    std::vector<std::pair<Version, VersionHash>> expired_versions;
+    std::vector<string> files_to_remove;
+    WriteLock wrlock(&_meta_lock);
+    for (auto& rs_meta : _tablet_meta->all_inc_rs_metas()) {
+        double diff = difftime(now, rs_meta->creation_time());
+        if (diff >= config::inc_rowset_expired_sec) {
+            Version version(rs_meta->start_version(), rs_meta->end_version());
+            expired_versions.push_back(std::make_pair(version, rs_meta->version_hash()));
+            VLOG(3) << ""find expire incremental rowset. tablet="" << full_name() << "", ""
+                    << ""version="" << rs_meta->start_version() << ""-"" << rs_meta->end_version() << "", ""
+                    << ""exist_sec="" << diff;
+        }
     }
 
+    if (expired_versions.empty()) { return; }
+
+    for (auto& pair: expired_versions) {
+        _delete_incremental_rowset(pair.first, pair.second, &files_to_remove);
+        VLOG(3) << ""delete expire incremental data. table="" << full_name() << "", ""
+                << ""version="" << pair.first.first << ""-"" << pair.first.second;
+    }
+
+    if (save_tablet_meta() != OLAP_SUCCESS) {
+        LOG(FATAL) << ""fail to save tablet_meta when delete expire incremental data.""
+                   << ""tablet="" << full_name();
+    }
+    remove_files(files_to_remove);
+}
+
+OLAPStatus Tablet::clone_data(const TabletMeta& tablet_meta,
+                              const std::vector<RowsetMetaSharedPtr>& rowsets_to_clone,
+                              const std::vector<Version>& versions_to_delete) {
+    LOG(INFO) << ""begin to clone data to tablet. tablet="" << full_name()","[{'comment': 'This function only process meta, not real data. I think clone_data is not a good function name', 'commenter': 'kangpinghuang'}, {'comment': 'I will replace it using revise_tablet_meta.', 'commenter': 'chaoyli'}]"
596,be/src/olap/tablet_meta.cpp,"@@ -225,6 +327,27 @@ OLAPStatus TabletMeta::to_tablet_pb_unlock(TabletMetaPB* tablet_meta_pb) {
     return OLAP_SUCCESS;
 }
 
+OLAPStatus TabletMeta::add_rs_meta(const RowsetMetaSharedPtr& rs_meta) {
+    std::lock_guard<std::mutex> lock(_mutex);
+
+    // check RowsetMeta is valid
+    for (auto rs : _rs_metas) {","[{'comment': 'auto& rs', 'commenter': 'kangpinghuang'}, {'comment': 'OK', 'commenter': 'chaoyli'}]"
596,be/src/olap/task/engine_storage_migration_task.cpp,"@@ -217,42 +213,13 @@ OLAPStatus EngineStorageMigrationTask::_copy_index_and_data_files(
         const string& schema_hash_path,
         const TabletSharedPtr& ref_tablet,
         vector<VersionEntity>& version_entity_vec) {
-    std::stringstream prefix_stream;
-    prefix_stream << schema_hash_path << ""/"" << ref_tablet->tablet_id();
-    std::string tablet_path_prefix = prefix_stream.str();
-    for (VersionEntity& entity : version_entity_vec) {
+    // TODO(lcy). copy function should be implemented
+    for (auto& entity : version_entity_vec) {
         Version version = entity.version;
-        VersionHash v_hash = entity.version_hash;
-        for (SegmentGroupEntity segment_group_entity : entity.segment_group_vec) {
-            int32_t segment_group_id = segment_group_entity.segment_group_id;
-            for (int seg_id = 0; seg_id < segment_group_entity.num_segments; ++seg_id) {
-                string index_path =
-                    SnapshotManager::instance()->construct_index_file_path(tablet_path_prefix, version, v_hash, segment_group_id, seg_id);
-                string ref_tablet_index_path = ref_tablet->construct_index_file_path(
-                        version, v_hash, segment_group_id, seg_id);
-                Status res = FileUtils::copy_file(ref_tablet_index_path, index_path);
-                if (!res.ok()) {
-                    LOG(WARNING) << ""fail to copy index file.""
-                                 << ""dest="" << index_path
-                                 << "", src="" << ref_tablet_index_path;
-                    return OLAP_ERR_COPY_FILE_ERROR;
-                }
-
-                string data_path =
-                    SnapshotManager::instance()->construct_data_file_path(tablet_path_prefix, version, v_hash, segment_group_id, seg_id);
-                string ref_tablet_data_path = ref_tablet->construct_data_file_path(
-                    version, v_hash, segment_group_id, seg_id);
-                res = FileUtils::copy_file(ref_tablet_data_path, data_path);
-                if (!res.ok()) {
-                    LOG(WARNING) << ""fail to copy data file.""
-                                 << ""dest="" << index_path
-                                 << "", src="" << ref_tablet_index_path;
-                    return OLAP_ERR_COPY_FILE_ERROR;
-                }
-            }
-        }
+        const RowsetSharedPtr rowset = ref_tablet->get_rowset_by_version(version);
+        std::vector<std::string> success_files;
+        RETURN_NOT_OK(rowset->make_snapshot(&success_files));","[{'comment': 'this should be modified', 'commenter': 'kangpinghuang'}]"
607,fe/src/main/java/org/apache/doris/common/proc/FrontendsProcNode.java,"@@ -80,6 +89,17 @@ public static void getFrontendsInfo(Catalog catalog, List<List<String>> infos) {
             List<String> info = new ArrayList<String>();
             info.add(fe.getNodeName());
             info.add(fe.getHost());
+
+            String hostName = ""N/A"";
+            try {
+                InetAddress address = InetAddress.getByName(fe.getHost());
+                hostName = address.getHostName();","[{'comment': 'We has unified address entrance.', 'commenter': 'chaoyli'}]"
628,be/src/olap/task/engine_clear_alter_task.cpp,"@@ -46,26 +46,21 @@ OLAPStatus EngineClearAlterTask::_clear_alter_task(const TTabletId tablet_id,
     const AlterTabletTask& alter_task = tablet->alter_task();
     AlterTabletState alter_state = alter_task.alter_state();
     TTabletId related_tablet_id = alter_task.related_tablet_id();
-    TSchemaHash related_schema_hash = alter_task.related_schema_hash();;
+    TSchemaHash related_schema_hash = alter_task.related_schema_hash();
     tablet->release_header_lock();
     if (alter_state == AlterTabletState::ALTER_NONE) {
         return OLAP_SUCCESS;
-    } else {
+    } 
+    
+    if (alter_state == AlterTabletState::ALTER_ALTERING) { 
         LOG(WARNING) << ""find alter task unfinished when process clear alter task. ""
                      << ""tablet="" << tablet->full_name();
         return OLAP_ERR_PREVIOUS_SCHEMA_CHANGE_NOT_FINISHED;
     }
 
     // clear schema change info
-    tablet->obtain_header_wrlock();
-    tablet->delete_alter_task();
-    OLAPStatus res = tablet->save_tablet_meta();
-    if (res != OLAP_SUCCESS) {
-        LOG(FATAL) << ""fail to save header. [res="" << res << "" tablet='"" << tablet->full_name() << ""']"";
-    } else {
-        LOG(INFO) << ""clear alter task on tablet. [tablet='"" << tablet->full_name() << ""']"";
-    }
-    tablet->release_header_lock();
+    OLAPStatus res = OLAP_SUCCESS;
+    res = tablet->protected_delete_alter_task();","[{'comment': 'OLAPStatus res = protected_delete_alter_task();', 'commenter': 'chaoyli'}, {'comment': 'done', 'commenter': 'yiguolei'}]"
628,be/src/olap/tablet.cpp,"@@ -635,6 +637,24 @@ OLAPStatus Tablet::delete_alter_task() {
     return _tablet_meta->delete_alter_task();
 }
 
+OLAPStatus Tablet::protected_delete_alter_task() {","[{'comment': 'If you have changed all code calling delete_alter_task, it is possible to merge delete_alter_task() and protected_delete_alter_task()?', 'commenter': 'chaoyli'}, {'comment': 'No. sometimes delete alter task is called under a write lock, and call other methods like _drop_tablet_directly_unlocked, so could not merge them.', 'commenter': 'yiguolei'}]"
628,be/src/olap/txn_manager.cpp,"@@ -292,10 +299,12 @@ OLAPStatus TxnManager::rollback_txn(TPartitionId partition_id, TTransactionId tr
         }
         return OLAP_SUCCESS;
     } else {","[{'comment': 'else is redundant.', 'commenter': 'chaoyli'}]"
628,be/src/olap/txn_manager.cpp,"@@ -263,6 +266,10 @@ OLAPStatus TxnManager::publish_txn(OlapMeta* meta, TPartitionId partition_id, TT
     }
 }
 
+// txn could be rollbacked if it does not have related rowset
+// if the txn has related rowset then could not rollback it, because it 
+// may be committed in another thread and our current thread meets erros when writing to data file
+// be has to wait for fe call clear txn api","[{'comment': 'In what scenario, one thread want to rollback and another thread commit it?', 'commenter': 'chaoyli'}, {'comment': 'In stream load scenario, if coordinator be send two ingest commands, there maybe two running thread to load the rowset.', 'commenter': 'yiguolei'}]"
628,be/src/olap/delta_writer.cpp,"@@ -46,12 +46,20 @@ DeltaWriter::~DeltaWriter() {
 }
 
 void DeltaWriter::_garbage_collection() {
-    TxnManager::instance()->rollback_txn(_req.partition_id, _req.txn_id,_req.tablet_id, _req.schema_hash);
-    StorageEngine::instance()->add_unused_rowset(_cur_rowset);
+    OLAPStatus rollback_status = TxnManager::instance()->rollback_txn(_req.partition_id, ","[{'comment': 'In this place, if data has generated but rowset is not been created, how to GC data?', 'commenter': 'chaoyli'}]"
634,be/src/olap/push_handler.cpp,"@@ -395,17 +395,6 @@ OLAPStatus PushHandler::_convert(
         // 7. Convert data for schema change tables
         VLOG(10) << ""load to related tables of schema_change if possible."";
         if (NULL != new_tablet.get()) {","[{'comment': 'new_tablet != nullptr', 'commenter': 'kangpinghuang'}, {'comment': 'I will fix it.', 'commenter': 'chaoyli'}]"
635,be/src/olap/rowset/alpha_rowset_reader.cpp,"@@ -150,10 +149,7 @@ OLAPStatus AlphaRowsetReader::_get_next_not_filtered_row(size_t pos, RowCursor**
             }
         }
         if (!current_row_block->has_remaining()) {
-            OLAPStatus status = _get_next_block(pos, &current_row_block);
-            if (status != OLAP_SUCCESS) {
-                return status;
-            }
+            _get_next_block(pos, &current_row_block);","[{'comment': 'You should return OLAPStatus when error happens', 'commenter': 'chaoyli'}]"
635,be/src/olap/rowset/alpha_rowset_reader.cpp,"@@ -185,10 +181,7 @@ OLAPStatus AlphaRowsetReader::_get_next_row_for_singleton_rowset(RowCursor** row
     RowBlock* row_block = _row_blocks[min_index];
     row_block->pos_inc();
     if (!row_block->has_remaining()) {
-        OLAPStatus status = _get_next_block(min_index, &row_block);
-        if (status != OLAP_SUCCESS) {
-            return status;
-        }
+        _get_next_block(min_index, &row_block);","[{'comment': 'There is no need to call _get_next_block', 'commenter': 'chaoyli'}]"
643,be/src/olap/tablet_meta.h,"@@ -91,29 +112,51 @@ class TabletMeta {
                uint64_t shard_id, const TTabletSchema& tablet_schema,
                uint32_t next_unique_id,
                const std::unordered_map<uint32_t, uint32_t>& col_ordinal_to_unique_id);
-    OLAPStatus save(const std::string& file_path);
+
+    // Function create_from_file is used to be compatible with previous tablet_meta.
+    // Previous tablet_meta is a physical file in tablet dir, which is not stored in rocksdb.
     OLAPStatus create_from_file(const std::string& file_path);
+    OLAPStatus save(const std::string& file_path);
+    OLAPStatus save_meta();
 
     OLAPStatus serialize(string* meta_binary) const;
-    OLAPStatus serialize_unlock(string* meta_binary) const;
-
-    OLAPStatus init_from_pb(const TabletMetaPB& tablet_meta_pb);
     OLAPStatus deserialize(const string& meta_binary);
+    OLAPStatus init_from_pb(const TabletMetaPB& tablet_meta_pb);
 
-    OLAPStatus save_meta();
-    OLAPStatus save_meta_unlock();
+    OLAPStatus to_meta_pb(TabletMetaPB* tablet_meta_pb);
+    OLAPStatus to_json(std::string* json_string, json2pb::Pb2JsonOptions& options);
 
-    OLAPStatus to_tablet_pb(TabletMetaPB* tablet_meta_pb);
-    OLAPStatus to_tablet_pb_unlock(TabletMetaPB* tablet_meta_pb);
+    inline void set_data_dir(DataDir* data_dir);","[{'comment': 'why add this api? why not put it in Constructor? Is there any chance to change the data dir of a tablet?', 'commenter': 'kangpinghuang'}, {'comment': 'When cloning tablet_meta from remote host, data_dir is empty.', 'commenter': 'chaoyli'}]"
643,be/src/olap/tablet_meta.h,"@@ -123,80 +166,55 @@ class TabletMeta {
     DelPredicateArray delete_predicates() const;
     bool version_for_delete_predicate(const Version& version);
 
-    inline const int64_t table_id() const;
-    inline const int64_t partition_id() const;
-    inline const int64_t tablet_id() const;
-    inline const int64_t schema_hash() const;
-    inline const int16_t shard_id() const;
-    void set_shard_id(int32_t shard_id);
-    inline int64_t creation_time() const;
-    void set_creation_time(int64_t creation_time);
-
-    inline const size_t num_rows() const;
-    inline const size_t data_size() const;
-    inline const size_t version_count() const;
-    Version max_version() const;
-
-    inline const TabletState& tablet_state() const;
     inline const AlterTabletTask& alter_task() const;
     inline AlterTabletTask* mutable_alter_task();
     OLAPStatus add_alter_task(const AlterTabletTask& alter_task);
     OLAPStatus delete_alter_task();
-    inline int32_t cumulative_layer_point() const;
-    void set_cumulative_layer_point(int32_t new_point);
-    inline const TabletSchema& tablet_schema() const;
 
-    inline void set_data_dir(DataDir* data_dir);
 private:
-    int64_t _table_id;
-    int64_t _partition_id;
-    int64_t _tablet_id;
-    int64_t _schema_hash;
-    int16_t _shard_id;
-
-    int64_t _creation_time;
-    int32_t _cumulative_layer_point;
-
+    TabletState _tablet_state;
     TabletSchema _schema;
+
     vector<RowsetMetaSharedPtr> _rs_metas;
     vector<RowsetMetaSharedPtr> _inc_rs_metas;
     DelPredicateArray _del_pred_array;
 
-    TabletState _tablet_state;
     AlterTabletTask _alter_task;
 
-    TabletMetaPB _tablet_meta_pb;
     DataDir* _data_dir;
-
-    mutable std::mutex _mutex;
+    TabletMetaPB _tablet_meta_pb;","[{'comment': 'where to init this PB?', 'commenter': 'kangpinghuang'}, {'comment': 'In construtor', 'commenter': 'chaoyli'}]"
643,be/src/olap/tablet_manager.cpp,"@@ -555,8 +552,8 @@ TabletSharedPtr TabletManager::get_tablet(TTabletId tablet_id, SchemaHash schema
         if (!tablet->is_used()) {
             OLAP_LOG_WARNING(""tablet cannot be used. [tablet=%ld]"", tablet_id);
             tablet.reset();
-        } else if (load_tablet && !tablet->is_loaded()) {
-            if (tablet->load() != OLAP_SUCCESS) {
+        } else if (load_tablet && !tablet->init_success()) {
+            if (tablet->init() != OLAP_SUCCESS) {
                 OLAP_LOG_WARNING(""fail to load tablet. [tablet=%ld]"", tablet_id);","[{'comment': 'LOG(INFO)', 'commenter': 'kangpinghuang'}, {'comment': 'OK', 'commenter': 'chaoyli'}]"
643,be/src/olap/tablet_manager.cpp,"@@ -555,8 +552,8 @@ TabletSharedPtr TabletManager::get_tablet(TTabletId tablet_id, SchemaHash schema
         if (!tablet->is_used()) {
             OLAP_LOG_WARNING(""tablet cannot be used. [tablet=%ld]"", tablet_id);
             tablet.reset();
-        } else if (load_tablet && !tablet->is_loaded()) {
-            if (tablet->load() != OLAP_SUCCESS) {
+        } else if (load_tablet && !tablet->init_success()) {","[{'comment': 'I think is_inited is a better name?', 'commenter': 'kangpinghuang'}, {'comment': 'I think init_succeeded may be better to describe it.\r\nis_inited can not differentiate success or fail literally', 'commenter': 'chaoyli'}]"
643,be/src/olap/tablet.h,"@@ -27,221 +27,211 @@
 
 #include ""gen_cpp/AgentService_types.h""
 #include ""gen_cpp/olap_file.pb.h""
-#include ""olap/field.h""
 #include ""olap/olap_define.h""
-#include ""olap/tablet_meta.h""
 #include ""olap/tuple.h""
 #include ""olap/row_cursor.h""
 #include ""olap/rowset_graph.h""
-#include ""olap/utils.h""
+#include ""olap/rowset/rowset.h""
 #include ""olap/rowset/rowset_reader.h""
+#include ""olap/tablet_meta.h""
+#include ""olap/utils.h""
+#include ""util/once.h""
+
+using std::pair;
+using std::nothrow;
+using std::sort;
+using std::string;
+using std::vector;
 
 namespace doris {
-class TabletMeta;
-class Rowset;
-class Tablet;
-class RowBlockPosition;
+
 class DataDir;
-class RowsetReader;
-class ColumnData;
-class SegmentGroup;
+class Tablet;
+class TabletMeta;
 
 using TabletSharedPtr = std::shared_ptr<Tablet>;
 
 class Tablet : public std::enable_shared_from_this<Tablet> {
 public:
-    static TabletSharedPtr create_from_tablet_meta_file(
+    static TabletSharedPtr create_tablet_from_meta_file(
             const std::string& header_file,
             DataDir* data_dir = nullptr);
-    static TabletSharedPtr create_from_tablet_meta(
+    static TabletSharedPtr create_tablet_from_meta(
             TabletMeta* meta,
             DataDir* data_dir  = nullptr);
 
     Tablet(TabletMeta* tablet_meta, DataDir* data_dir);
     ~Tablet();
 
-    OLAPStatus load();
-    OLAPStatus load_rowsets();
-    inline bool is_loaded();
-    OLAPStatus save_tablet_meta();
+    OLAPStatus init_once();
+    OLAPStatus init();
+    inline bool init_success();
 
-    bool has_expired_incremental_rowset();
-    void delete_expired_incremental_rowset();
-    OLAPStatus revise_tablet_meta(const TabletMeta& tablet_meta,
-                                  const std::vector<RowsetMetaSharedPtr>& rowsets_to_clone,
-                                  const std::vector<Version>& versions_to_delete);
-    OLAPStatus compute_all_versions_hash(const std::vector<Version>& versions,
-                                         VersionHash* version_hash) const;
-    OLAPStatus merge_tablet_meta(const TabletMeta& hdr, int to_version);
-    bool has_version(const Version& version) const;
-    void list_versions(std::vector<Version>* versions) const;
-    void mark_dropped() { _is_dropped = true; }
-    bool is_dropped() { return _is_dropped; }
-    void delete_all_files();
-    void obtain_header_rdlock() { _meta_lock.rdlock(); }
-    void obtain_header_wrlock() { _meta_lock.wrlock(); }
-    void release_header_lock() { _meta_lock.unlock(); }
-    RWMutex* get_header_lock_ptr() { return &_meta_lock; }
-    void obtain_push_lock() { _ingest_lock.lock(); }
-    void release_push_lock() { _ingest_lock.unlock(); }
-    Mutex* get_push_lock() { return &_ingest_lock; }
-    bool try_base_compaction_lock() { return _base_lock.trylock() == OLAP_SUCCESS; }
-    void obtain_base_compaction_lock() { _base_lock.lock(); }
-    void release_base_compaction_lock() { _base_lock.unlock(); }
-    bool try_cumulative_lock() { return (OLAP_SUCCESS == _cumulative_lock.trylock()); }
-    void obtain_cumulative_lock() { _cumulative_lock.lock(); }
-    void release_cumulative_lock() { _cumulative_lock.unlock(); }
-    std::string construct_dir_path() const;
-    int version_count() const;
-    const uint32_t calc_cumulative_compaction_score() const;
-    const uint32_t calc_base_compaction_score() const;
-    inline KeysType keys_type() const;
-    bool version_for_delete_predicate(const Version& version);
-    bool version_for_load_deletion(const Version& version);
-    inline const int64_t creation_time() const;
-    void set_creation_time(int64_t creation_time);
-    inline const int32_t cumulative_layer_point() const;
-    inline void set_cumulative_layer_point(const int32_t new_point);
-    AlterTabletState alter_state();
-    OLAPStatus set_alter_state(AlterTabletState state);
-    bool is_schema_changing();
-    OLAPStatus delete_alter_task();
-    OLAPStatus protected_delete_alter_task();
-    void add_alter_task(int64_t tablet_id, int64_t schema_hash,
-                        const vector<Version>& versions_to_alter,
-                        const AlterTabletType alter_type);
-    const AlterTabletTask& alter_task();
     bool is_used();
-    std::string storage_root_path_name() const;
-    std::string tablet_path() const;
-    OLAPStatus test_version(const Version& version);
-    size_t get_version_data_size(const Version& version);
-    OLAPStatus recover_tablet_until_specfic_version(const int64_t& spec_version,
-                                                    const int64_t& version_hash);
-    const std::string& rowset_path_prefix();
-    const size_t id() { return _id; }
-    void set_id(size_t id) { _id = id; }
+    inline DataDir* data_dir() const;
     OLAPStatus register_tablet_into_dir();
+    std::string dir_path() const;","[{'comment': 'I think dir_path is not a good name? data_dir_path?\r\n', 'commenter': 'kangpinghuang'}, {'comment': 'OK, I remove dir_path() from tablet.h', 'commenter': 'chaoyli'}]"
643,be/src/olap/tablet.cpp,"@@ -73,8 +56,8 @@ TabletSharedPtr Tablet::create_from_tablet_meta_file(
 
     // add new fields
     boost::filesystem::path file_path_path(file_path);
-    std::string shard_path = file_path_path.parent_path().parent_path().parent_path().string();
-    std::string shard_str = shard_path.substr(shard_path.find_last_of('/') + 1);
+    string shard_path = file_path_path.parent_path().parent_path().parent_path().string();
+    string shard_str = shard_path.substr(shard_path.find_last_of('/') + 1);","[{'comment': 'you delete using std::string, can this be compiled?', 'commenter': 'kangpinghuang'}, {'comment': 'using std::string is in tablet.h', 'commenter': 'chaoyli'}]"
643,be/src/olap/base_compaction.cpp,"@@ -200,7 +200,7 @@ bool BaseCompaction::_check_whether_satisfy_policy(bool is_manual_trigger,
         // base文件
         if (temp.first == 0) {
             _old_base_version = temp;
-            base_size = _tablet->get_version_data_size(temp);
+            base_size = _tablet->get_rowset_size_by_version(temp);","[{'comment': 'temp is not a good name?', 'commenter': 'kangpinghuang'}]"
643,be/src/olap/base_compaction.cpp,"@@ -43,7 +43,7 @@ namespace doris {
 OLAPStatus BaseCompaction::init(TabletSharedPtr tablet, bool is_manual_trigger) {
     // 表在首次查询或PUSH等操作时，会被加载到内存
     // 如果表没有被加载，表明该表上目前没有任何操作，所以不进行BE操作
-    if (!tablet->is_loaded()) {
+    if (!tablet->init_success()) {","[{'comment': 'init_success -> is_inited?', 'commenter': 'kangpinghuang'}]"
647,be/src/olap/rowset/alpha_rowset.cpp,"@@ -129,13 +129,13 @@ void AlphaRowset::set_version_and_version_hash(Version version,  VersionHash ver
         segment_group_pb.set_index_size(segment_group->index_size());
         segment_group_pb.set_data_size(segment_group->data_size());
         segment_group_pb.set_num_rows(segment_group->num_rows());
-        const std::vector<KeyRange>* column_statistics = &(segment_group->get_column_statistics());
-        if (column_statistics != nullptr) {
-            for (size_t i = 0; i < column_statistics->size(); ++i) {
+        const std::vector<KeyRange> column_statistics = segment_group->get_column_statistics();","[{'comment': 'You should use reference const std::vector<KeyRange>&', 'commenter': 'chaoyli'}]"
647,be/src/olap/rowset/alpha_rowset_writer.cpp,"@@ -127,14 +127,23 @@ OLAPStatus AlphaRowsetWriter::add_rowset(RowsetSharedPtr rowset) {
 }
 
 OLAPStatus AlphaRowsetWriter::flush() {
-    OLAPStatus status = _column_data_writer->finalize();
-    SAFE_DELETE(_column_data_writer);
-    _cur_segment_group->load();
-    _is_inited = false;
-    return status;
+    if (_is_inited) {","[{'comment': 'There is not suitable for using _is_inited variable here.', 'commenter': 'chaoyli'}]"
647,be/src/olap/rowset/alpha_rowset_reader.cpp,"@@ -77,7 +77,13 @@ OLAPStatus AlphaRowsetReader::next_block(RowBlock** block) {
         RowCursor* row_cursor = nullptr;
         OLAPStatus status = next(&row_cursor);
         if (status == OLAP_ERR_DATA_EOF) {
-            return OLAP_ERR_DATA_EOF;
+            if ((*block)->has_remaining()) {
+                (*block)->set_pos(0);
+                (*block)->set_limit((*block)->pos());","[{'comment': '(*block)->set_limit(num_rows_in_block)', 'commenter': 'chaoyli'}]"
653,be/src/olap/rowset/alpha_rowset_reader.cpp,"@@ -194,11 +192,21 @@ OLAPStatus AlphaRowsetReader::_get_next_row_for_singleton_rowset(RowCursor** row
         }
     }
     if (min_row == nullptr || min_index == -1) {
-        return OLAP_ERR_READER_READING_ERROR;
+        return OLAP_ERR_DATA_EOF;
     }
     *row = min_row;
     RowBlock* row_block = _row_blocks[min_index];","[{'comment': '_row_blocks[min_index]->pos_inc() may be better', 'commenter': 'chaoyli'}, {'comment': 'ok', 'commenter': 'kangpinghuang'}]"
653,be/src/olap/rowset/alpha_rowset_reader.cpp,"@@ -207,6 +215,16 @@ OLAPStatus AlphaRowsetReader::_get_next_row_for_cumulative_rowset(RowCursor** ro
     OLAPStatus status = _get_next_not_filtered_row(pos, row);
     RowBlock* row_block = _row_blocks[pos];
     row_block->pos_inc();","[{'comment': '_row_blocks[pos]->pos_inc() may be better', 'commenter': 'chaoyli'}, {'comment': 'ok', 'commenter': 'kangpinghuang'}]"
653,be/src/olap/rowset/alpha_rowset_reader.cpp,"@@ -154,19 +154,18 @@ OLAPStatus AlphaRowsetReader::_get_next_not_filtered_row(size_t pos, RowCursor**
             if (row_filtered) {
                 _current_read_context->stats->rows_del_filtered++;
                 current_row_block->pos_inc();
+                if (!current_row_block->has_remaining()) {","[{'comment': '    const DeleteHandler* delete_handler = _current_read_context->delete_handler;\r\n    while (!found_row) {\r\n        if (!current_row_block->has_remaining()) {\r\n            OLAPStatus status = _get_next_block(pos, &current_row_block);\r\n            if (status != OLAP_SUCCESS) {\r\n                LOG(WARNING) << ""_get_next_block failed, status:"" << status;\r\n                return status;\r\n            }\r\n        }\r\n\r\n        size_t pos = current_row_block->pos();\r\n        current_row_block->get_row(pos, *row);\r\n        found_row = delete_handler->is_filter_data(_alpha_rowset_meta->version().second, *(*row));\r\n    }', 'commenter': 'chaoyli'}, {'comment': 'ok', 'commenter': 'kangpinghuang'}]"
654,be/src/olap/delete_handler.cpp,"@@ -45,6 +45,34 @@ using google::protobuf::RepeatedPtrField;
 
 namespace doris {
 
+OLAPStatus DeleteConditionHandler::generate_delete_predicate(
+        const TabletSchema& schema,
+        const std::vector<TCondition>& conditions,
+        DeletePredicatePB* del_pred) {
+    if (conditions.size() == 0) {
+        LOG(WARNING) << ""invalid parameters for store_cond.""
+                     << "" condition_size="" << conditions.size();
+        return OLAP_ERR_DELETE_INVALID_PARAMETERS;
+    }
+
+    // 检查删除条件是否符合要求
+    for (const TCondition& condition : conditions) {
+        if (check_condition_valid(schema, condition) != OLAP_SUCCESS) {
+            LOG(WARNING) << ""invalid condition. condition="" << ThriftDebugString(condition);
+            return OLAP_ERR_DELETE_INVALID_CONDITION;
+        }
+    }
+
+    // 存储删除条件
+    for (const TCondition& condition : conditions) {
+        string condition_str = construct_sub_predicates(condition);
+        del_pred->add_sub_predicates(condition_str);
+        LOG(INFO) << ""store one sub-delete condition. condition="" << condition_str;","[{'comment': 'is this log necessary? if true, use vlog', 'commenter': 'kangpinghuang'}, {'comment': 'This is used to know how many delete operations.\r\nTake account of deleting operation is not so much, so LOG(INFO) is OK.', 'commenter': 'chaoyli'}]"
654,be/src/olap/push_handler.cpp,"@@ -191,7 +182,7 @@ OLAPStatus PushHandler::_do_streaming_ingestion(
                      << "", table="" << tablet->full_name()
                      << "", transaction_id="" << request.transaction_id;
         for (TabletVars& tablet_var : *tablet_vars) {
-            if (tablet_var.tablet.get() == NULL) {
+            if (tablet_var.tablet.get() == nullptr) {","[{'comment': 'tablet_var.tablet == nullptr', 'commenter': 'kangpinghuang'}, {'comment': 'OK', 'commenter': 'chaoyli'}]"
654,be/src/olap/tablet_meta.cpp,"@@ -371,12 +374,18 @@ OLAPStatus TabletMeta::delete_rs_meta_by_version(const Version& version) {
 
 OLAPStatus TabletMeta::modify_rs_metas(const vector<RowsetMetaSharedPtr>& to_add,
                                        const vector<RowsetMetaSharedPtr>& to_delete) {
-    for (auto del_rs : to_delete) {
+    for (auto rs_to_del : to_delete) {
         auto it = _rs_metas.begin();
         while (it != _rs_metas.end()) {
-            if (del_rs->version().first == (*it)->version().first
-                  && del_rs->version().second == (*it)->version().second) {
+            LOG(INFO) << ""it version: "" << (*it)->version().first << ""-"" << (*it)->version().second;","[{'comment': 'remove this log', 'commenter': 'kangpinghuang'}, {'comment': 'OK', 'commenter': 'chaoyli'}]"
656,be/src/olap/cumulative_compaction.cpp,"@@ -452,9 +453,6 @@ OLAPStatus CumulativeCompaction::_do_cumulative_compaction() {
     // 6. delete delta files which have been merged into new cumulative file
     _delete_unused_delta_files(&unused_rowsets);","[{'comment': 'rename this function name to _delete_unused_rowset?', 'commenter': 'kangpinghuang'}, {'comment': 'OK', 'commenter': 'chaoyli'}]"
656,be/src/olap/base_compaction.cpp,"@@ -174,7 +172,10 @@ OLAPStatus BaseCompaction::run() {
     _release_base_compaction_lock();
 
     LOG(INFO) << ""succeed to do base compaction. tablet="" << _tablet->full_name()
-              << "", base_version="" << _new_base_version.first << ""-"" << _new_base_version.second;
+              << "", base_version="" << _new_base_version.first << ""-"" << _new_base_version.second
+              << "". elapsed time of doing base compaction""
+              << "", time="" << stage_watch.get_elapse_time_us() / (100000.0) << ""s"";","[{'comment': 'I think there is need to  add a get_elapse_time_s() function?', 'commenter': 'kangpinghuang'}, {'comment': 'OK', 'commenter': 'chaoyli'}]"
656,be/src/olap/olap_cond.cpp,"@@ -601,25 +601,24 @@ bool Conditions::delete_conditions_eval(const RowCursor& row) const {
 }
 
 bool Conditions::delta_pruning_filter(","[{'comment': 'rowset_pruning_filter?', 'commenter': 'kangpinghuang'}, {'comment': 'OK', 'commenter': 'chaoyli'}]"
656,be/src/olap/olap_cond.cpp,"@@ -601,25 +601,24 @@ bool Conditions::delete_conditions_eval(const RowCursor& row) const {
 }
 
 bool Conditions::delta_pruning_filter(
-        const std::vector<std::pair<WrapperField*, WrapperField*>>& column_statistics) const {
+        const std::vector<std::pair<WrapperField*, WrapperField*>>& zone_maps) const {","[{'comment': 'use KeyRange to confirm to the statement of this function', 'commenter': 'kangpinghuang'}, {'comment': 'OK', 'commenter': 'chaoyli'}]"
656,be/src/olap/olap_cond.cpp,"@@ -601,25 +601,24 @@ bool Conditions::delete_conditions_eval(const RowCursor& row) const {
 }
 
 bool Conditions::delta_pruning_filter(
-        const std::vector<std::pair<WrapperField*, WrapperField*>>& column_statistics) const {
+        const std::vector<std::pair<WrapperField*, WrapperField*>>& zone_maps) const {
     //通过所有列上的删除条件对version进行过滤
     for (auto& cond_it : _columns) {
-        if (cond_it.second->is_key() && cond_it.first > column_statistics.size()) {
-            OLAP_LOG_WARNING(""where condition not equal column statistics size.""
-                    ""[cond_id=%d, column_statistics_size=%lu]"", 
-                    cond_it.first,
-                    column_statistics.size());
+        if (cond_it.second->is_key() && cond_it.first > zone_maps.size()) {
+            LOG(WARNING) << ""where condition not equal zone maps size. ""
+                         << ""cond_id="" << cond_it.first
+                         << "", zone_map_size="" << zone_maps.size();
             return false;
         }
-        if (cond_it.second->is_key() && !cond_it.second->eval(column_statistics[cond_it.first])) {
+        if (cond_it.second->is_key() && !cond_it.second->eval(zone_maps[cond_it.first])) {
             return true;
         }
     }
     return false;
 }
 
 int Conditions::delete_pruning_filter(
-        const std::vector<std::pair<WrapperField*, WrapperField*>>& col_stat) const {
+        const std::vector<std::pair<WrapperField*, WrapperField*>>& zone_maps) const {","[{'comment': 'Use KeyRange', 'commenter': 'kangpinghuang'}, {'comment': 'OK', 'commenter': 'chaoyli'}]"
656,be/src/olap/olap_cond.cpp,"@@ -656,7 +654,7 @@ int Conditions::delete_pruning_filter(
         }
     }
 
-    if (true == del_not_satisfied || 0 == _columns.size()) {
+    if (del_not_satisfied || _columns.empty()) {","[{'comment': 'put _columns.empty() in the first line of this function', 'commenter': 'kangpinghuang'}, {'comment': 'OK', 'commenter': 'chaoyli'}]"
656,be/src/olap/rowset/column_data_writer.cpp,"@@ -197,7 +196,7 @@ OLAPStatus ColumnDataWriter::finalize() {
         return res;
     }
 
-    res = _segment_group->add_column_statistics(_column_statistics);
+    res = _segment_group->add_zone_maps(_zone_maps);
     if (res != OLAP_SUCCESS) {
         OLAP_LOG_WARNING(""Fail to set delta pruning![res=%d]"", res);","[{'comment': 'LOG(WARNING)', 'commenter': 'kangpinghuang'}, {'comment': 'OK', 'commenter': 'chaoyli'}]"
656,be/src/olap/schema_change.h,"@@ -268,7 +268,7 @@ class SchemaChangeHandler {
     OLAPStatus process_alter_tablet(AlterTabletType alter_tablet_type,
                                    const TAlterTabletReq& request);
 
-    OLAPStatus schema_version_convert(TabletSharedPtr ref_tablet,
+    OLAPStatus schema_version_convert(TabletSharedPtr base_tablet,","[{'comment': 'call this function in delta_writer', 'commenter': 'kangpinghuang'}, {'comment': 'I will change this name in a following commit. So call it in the next commit.', 'commenter': 'chaoyli'}]"
656,be/src/olap/schema_change.cpp,"@@ -1172,100 +1172,65 @@ OLAPStatus SchemaChangeHandler::_check_and_clear_schema_change_info(
 
 OLAPStatus SchemaChangeHandler::process_alter_tablet(AlterTabletType type,
                                                      const TAlterTabletReq& request) {
-    OLAPStatus res = OLAP_SUCCESS;
-    LOG(INFO) << ""begin to validate alter tablet request."";
+    LOG(INFO) << ""begin to validate alter tablet request. base_tablet_id="" << request.base_tablet_id
+              << "", new_tablet_id="" << request.new_tablet_req.tablet_id;
 
     // 1. Lock schema_change_lock util schema change info is stored in tablet header
     if (!TabletManager::instance()->try_schema_change_lock(request.base_tablet_id)) {
-        OLAP_LOG_WARNING(""failed to obtain schema change lock. [res=%d tablet=%ld]"",
-                         res, request.base_tablet_id);
+        LOG(WARNING) << ""failed to obtain schema change lock. ""
+                     << ""base_tablet="" << request.base_tablet_id;
         return OLAP_ERR_TRY_LOCK_FAILED;
     }
 
     // 2. Get base tablet
-    TabletSharedPtr ref_tablet = TabletManager::instance()->get_tablet(
+    TabletSharedPtr base_tablet = TabletManager::instance()->get_tablet(
             request.base_tablet_id, request.base_schema_hash);
-    if (ref_tablet.get() == NULL) {
-        OLAP_LOG_WARNING(""fail to find base tablet. [base_tablet=%ld base_schema_hash=%d]"",
-                         request.base_tablet_id, request.base_schema_hash);
+    if (base_tablet == nullptr) {
+        LOG(WARNING) << ""fail to find base tablet. base_tablet="" << request.base_tablet_id
+                     << "", base_schema_hash="" << request.base_schema_hash;
         TabletManager::instance()->release_schema_change_lock(request.base_tablet_id);
         return OLAP_ERR_TABLE_NOT_FOUND;
     }
 
     // 3. Check if history schema change information exist,
     //    if exist, it will be cleaned only when all delta versions converted
-    res = _check_and_clear_schema_change_info(ref_tablet, request);
+    OLAPStatus res = _check_and_clear_schema_change_info(base_tablet, request);
     if (res != OLAP_SUCCESS) {
-        OLAP_LOG_WARNING(""failed to check and clear schema change info. [tablet='%s']"",
-                         ref_tablet->full_name().c_str());
+        LOG(WARNING) << ""failed to check and clear schema change info.""
+                     << "" base_tablet="" << base_tablet->full_name();
         TabletManager::instance()->release_schema_change_lock(request.base_tablet_id);
         return res;
     }
 
     // 4. return failed if new tablet already exist in StorageEngine.
     TabletSharedPtr new_tablet = TabletManager::instance()->get_tablet(
             request.new_tablet_req.tablet_id, request.new_tablet_req.tablet_schema.schema_hash);
-    if (new_tablet.get() != NULL) {
-        res = OLAP_SUCCESS;
-    } else {
-        res = _do_alter_tablet(type, ref_tablet, request);
+    if (new_tablet != nullptr) {
+        TabletManager::instance()->release_schema_change_lock(request.base_tablet_id);","[{'comment': 'where to take the new tablet schema change lock before?\r\nthis place should not unlock', 'commenter': 'kangpinghuang'}, {'comment': 'Only call try_schema_change_lock() of base_tablet.', 'commenter': 'chaoyli'}]"
656,be/src/olap/schema_change.cpp,"@@ -1279,56 +1244,48 @@ OLAPStatus SchemaChangeHandler::_do_alter_tablet(
         for (int64_t transaction_id : transaction_ids) {
             if (!TxnManager::instance()->has_txn(
                 partition_id, transaction_id,
-                ref_tablet->tablet_id(), ref_tablet->schema_hash())) {
+                base_tablet->tablet_id(), base_tablet->schema_hash())) {
                 finished_transactions.push_back(transaction_id);
             }
         }
         for (int64_t transaction_id : finished_transactions) {
             transaction_ids.erase(transaction_id);
             VLOG(3) << ""transaction finished when schema change is waiting. ""
-                    << ""tablet="" << ref_tablet->full_name()
+                    << ""tablet="" << base_tablet->full_name()
                     << "", transaction_id="" << transaction_id
                     << "", transaction_size="" << transaction_ids.size();
         }
     }
 
     // 2. Get version_to_be_changed and store into tablet header
-    ref_tablet->obtain_push_lock();
-    ref_tablet->obtain_header_wrlock();
+    base_tablet->obtain_push_lock();
+    base_tablet->obtain_header_wrlock();","[{'comment': 'this name should be rename to obtain_meta_wrlock?\r\nheader should not be used', 'commenter': 'kangpinghuang'}, {'comment': 'OK, this modification may refer to many codes, I will push it on a lonely commit.', 'commenter': 'chaoyli'}]"
656,be/src/olap/schema_change.cpp,"@@ -46,12 +46,12 @@ using std::vector;
 namespace doris {","[{'comment': 'This file should be rename to alter_tablet_handler.cpp?', 'commenter': 'kangpinghuang'}, {'comment': 'OK, this modification may refer to many codes, I will push it on a lonely commit.', 'commenter': 'chaoyli'}]"
656,be/src/olap/schema_change.cpp,"@@ -1488,8 +1419,8 @@ OLAPStatus SchemaChangeHandler::_create_new_tablet(
                     << "", tablet="" << request.tablet_id
                     << "":"" << request.tablet_schema.schema_hash;
             }
-        } else if (NULL != new_tablet) {
-            new_tablet->delete_all_files();
+        } else if (NULL != tablet_to_create) {","[{'comment': 'tablet_to_create != nullptr', 'commenter': 'kangpinghuang'}, {'comment': 'OK', 'commenter': 'chaoyli'}]"
656,be/src/olap/schema_change.cpp,"@@ -1632,166 +1563,156 @@ OLAPStatus SchemaChangeHandler::schema_version_convert(
 }
 
 OLAPStatus SchemaChangeHandler::_get_versions_to_be_changed(
-        TabletSharedPtr ref_tablet,
+        TabletSharedPtr base_tablet,
         vector<Version>& versions_to_be_changed) {
-    int32_t request_version = 0;
-    RowsetSharedPtr rowset = ref_tablet->rowset_with_max_version();
-    if (rowset != NULL) {
-        request_version = rowset->version().second - 1;
-    } else {
-        LOG(WARNING) << ""Table has no version. path="" << ref_tablet->full_name();
+    Version request_version = { -1, 0 };
+    RowsetSharedPtr rowset = base_tablet->rowset_with_max_version();
+    if (rowset == nullptr) {
+        LOG(WARNING) << ""Tablet has no version. base_tablet="" << base_tablet->full_name();
         return OLAP_ERR_ALTER_DELTA_DOES_NOT_EXISTS;
+    } else {
+        request_version = rowset->version();
     }
 
-    // 最新版本的delta可以被重导覆盖，因此计算获取的路径中，
-    // 必须包含最新版本的delta
-    if (request_version >= 0) {
-        vector<Version> span_versions;
-        ref_tablet->capture_consistent_versions(Version(0, request_version), &span_versions);
-
-        // get all version list
-        vector<Version> all_versions;
-        ref_tablet->list_versions(&all_versions);
-        if (0 == all_versions.size()) {
-            LOG(WARNING) << ""there'is no any version in the tablet. tablet="" << ref_tablet->full_name();
-            return OLAP_ERR_VERSION_NOT_EXIST;
-        }
-
-        for (uint32_t i = 0; i < span_versions.size(); i++) {
-            versions_to_be_changed.push_back(span_versions[i]);
-        }
+    vector<Version> span_versions;
+    base_tablet->capture_consistent_versions(request_version, &span_versions);
+    for (uint32_t i = 0; i < span_versions.size(); i++) {
+        versions_to_be_changed.push_back(span_versions[i]);
     }
-    versions_to_be_changed.push_back(
-            Version(rowset->version().first, rowset->version().second));
 
     return OLAP_SUCCESS;
 }
 
 // 增加A->(B|C|...) 的schema_change信息
-OLAPStatus SchemaChangeHandler::_save_schema_change_info(
+OLAPStatus SchemaChangeHandler::_add_alter_tablet_task(
         AlterTabletType alter_tablet_type,
-        TabletSharedPtr ref_tablet,
+        TabletSharedPtr base_tablet,
         TabletSharedPtr new_tablet,
         const vector<Version>& versions_to_be_changed) {
 
     // check new tablet exists,
     // prevent to set base's status after new's dropping (clear base's status)
     if (TabletManager::instance()->get_tablet(
             new_tablet->tablet_id(), new_tablet->schema_hash()).get() == NULL) {
-        OLAP_LOG_WARNING(""fail to find tablet before saving status. [tablet='%s']"",
-                         new_tablet->full_name().c_str());
+        LOG(WARNING) << ""new_tablet does not exist. tablet="" << new_tablet->full_name();
         return OLAP_ERR_TABLE_NOT_FOUND;
     }
 
-    OLAPStatus res = OLAP_SUCCESS;
-
     // 1. 在新表和旧表中添加schema change标志
-    ref_tablet->delete_alter_task();
-    ref_tablet->add_alter_task(new_tablet->tablet_id(),
-                               new_tablet->schema_hash(),
-                               versions_to_be_changed,
-                               alter_tablet_type);
+    base_tablet->delete_alter_task();
+    base_tablet->add_alter_task(new_tablet->tablet_id(),","[{'comment': 'there is bug in add_alter_task function, pls check it.', 'commenter': 'kangpinghuang'}, {'comment': 'OK', 'commenter': 'chaoyli'}]"
656,be/src/olap/schema_change.cpp,"@@ -1632,166 +1563,156 @@ OLAPStatus SchemaChangeHandler::schema_version_convert(
 }
 
 OLAPStatus SchemaChangeHandler::_get_versions_to_be_changed(
-        TabletSharedPtr ref_tablet,
+        TabletSharedPtr base_tablet,
         vector<Version>& versions_to_be_changed) {
-    int32_t request_version = 0;
-    RowsetSharedPtr rowset = ref_tablet->rowset_with_max_version();
-    if (rowset != NULL) {
-        request_version = rowset->version().second - 1;
-    } else {
-        LOG(WARNING) << ""Table has no version. path="" << ref_tablet->full_name();
+    Version request_version = { -1, 0 };
+    RowsetSharedPtr rowset = base_tablet->rowset_with_max_version();
+    if (rowset == nullptr) {
+        LOG(WARNING) << ""Tablet has no version. base_tablet="" << base_tablet->full_name();
         return OLAP_ERR_ALTER_DELTA_DOES_NOT_EXISTS;
+    } else {
+        request_version = rowset->version();
     }
 
-    // 最新版本的delta可以被重导覆盖，因此计算获取的路径中，
-    // 必须包含最新版本的delta
-    if (request_version >= 0) {
-        vector<Version> span_versions;
-        ref_tablet->capture_consistent_versions(Version(0, request_version), &span_versions);
-
-        // get all version list
-        vector<Version> all_versions;
-        ref_tablet->list_versions(&all_versions);
-        if (0 == all_versions.size()) {
-            LOG(WARNING) << ""there'is no any version in the tablet. tablet="" << ref_tablet->full_name();
-            return OLAP_ERR_VERSION_NOT_EXIST;
-        }
-
-        for (uint32_t i = 0; i < span_versions.size(); i++) {
-            versions_to_be_changed.push_back(span_versions[i]);
-        }
+    vector<Version> span_versions;
+    base_tablet->capture_consistent_versions(request_version, &span_versions);
+    for (uint32_t i = 0; i < span_versions.size(); i++) {
+        versions_to_be_changed.push_back(span_versions[i]);
     }
-    versions_to_be_changed.push_back(
-            Version(rowset->version().first, rowset->version().second));
 
     return OLAP_SUCCESS;
 }
 
 // 增加A->(B|C|...) 的schema_change信息
-OLAPStatus SchemaChangeHandler::_save_schema_change_info(
+OLAPStatus SchemaChangeHandler::_add_alter_tablet_task(
         AlterTabletType alter_tablet_type,
-        TabletSharedPtr ref_tablet,
+        TabletSharedPtr base_tablet,
         TabletSharedPtr new_tablet,
         const vector<Version>& versions_to_be_changed) {
 
     // check new tablet exists,
     // prevent to set base's status after new's dropping (clear base's status)
     if (TabletManager::instance()->get_tablet(
             new_tablet->tablet_id(), new_tablet->schema_hash()).get() == NULL) {
-        OLAP_LOG_WARNING(""fail to find tablet before saving status. [tablet='%s']"",
-                         new_tablet->full_name().c_str());
+        LOG(WARNING) << ""new_tablet does not exist. tablet="" << new_tablet->full_name();
         return OLAP_ERR_TABLE_NOT_FOUND;
     }
 
-    OLAPStatus res = OLAP_SUCCESS;
-
     // 1. 在新表和旧表中添加schema change标志
-    ref_tablet->delete_alter_task();
-    ref_tablet->add_alter_task(new_tablet->tablet_id(),
-                               new_tablet->schema_hash(),
-                               versions_to_be_changed,
-                               alter_tablet_type);
+    base_tablet->delete_alter_task();
+    base_tablet->add_alter_task(new_tablet->tablet_id(),
+                                new_tablet->schema_hash(),
+                                versions_to_be_changed,
+                                alter_tablet_type);
+    base_tablet->set_alter_state(AlterTabletState::ALTER_ALTERING);
+    OLAPStatus res = base_tablet->save_meta();
+    if (res != OLAP_SUCCESS) {
+        LOG(FATAL) << ""fail to save ref tablet header. res="" << res
+                   << "", tablet="" << base_tablet->full_name();
+        return res;
+    }
 
-    new_tablet->add_alter_task(ref_tablet->tablet_id(),
-                               ref_tablet->schema_hash(),
+    new_tablet->add_alter_task(base_tablet->tablet_id(),
+                               base_tablet->schema_hash(),
                                vector<Version>(),  // empty versions
                                alter_tablet_type);
-
-    // save new tablet header :只有一个父ref tablet
+    new_tablet->set_alter_state(AlterTabletState::ALTER_ALTERING);
     res = new_tablet->save_meta();
     if (res != OLAP_SUCCESS) {
         LOG(FATAL) << ""fail to save new tablet header. res="" << res
                    << "", tablet="" << new_tablet->full_name();
         return res;
     }
 
-    res = ref_tablet->save_meta();
+    return res;
+}
+
+OLAPStatus SchemaChangeHandler::_save_alter_tablet_state(
+        AlterTabletState state,
+        TabletSharedPtr base_tablet,
+        TabletSharedPtr new_tablet) {
+    base_tablet->set_alter_state(state);
+    OLAPStatus res = base_tablet->save_meta();
+    if (res != OLAP_SUCCESS) {
+        LOG(FATAL) << ""fail to save ref tablet header. res="" << res
+                   << "", tablet="" << base_tablet->full_name();
+        return res;
+    }
+
+    new_tablet->set_alter_state(state);
+    res = new_tablet->save_meta();","[{'comment': 'redundant codes!!!', 'commenter': 'kangpinghuang'}, {'comment': 'OK', 'commenter': 'chaoyli'}]"
665,fe/src/main/java/org/apache/doris/load/routineload/KafkaRoutineLoadJob.java,"@@ -57,13 +59,20 @@
     private String serverAddress;
     private String topic;
     // optional, user want to load partitions.
-    private List<Integer> kafkaPartitions;
+    private List<Integer> customKafkaPartitions;
+    // current kafka partitions is the actually partition which will be fetched
+    private List<Integer> currentKafkaPartitions;
+
+    private KafkaConsumer<String, String> consumer;","[{'comment': 'Better to comment the meaning of <String, String>', 'commenter': 'morningman'}]"
665,fe/src/main/java/org/apache/doris/load/routineload/KafkaRoutineLoadJob.java,"@@ -173,6 +180,38 @@ protected RoutineLoadTaskInfo reNewTask(RoutineLoadTaskInfo routineLoadTaskInfo)
         return kafkaTaskInfo;
     }
 
+    // if customKafkaPartition is not null, then return false immediately
+    // else if kafka partitions of topic has been changed, return true.
+    // else return false
+    @Override
+    protected boolean needRescheduler() {","[{'comment': 'needRescheduler -> needReschedule', 'commenter': 'morningman'}]"
665,fe/src/main/java/org/apache/doris/load/routineload/KafkaRoutineLoadJob.java,"@@ -206,24 +235,33 @@ public static KafkaRoutineLoadJob fromCreateStmt(CreateRoutineLoadStmt stmt) thr
         return kafkaRoutineLoadJob;
     }
 
-    private void updatePartitions() {
-        // fetch all of kafkaPartitions in topic
-        if (kafkaPartitions == null || kafkaPartitions.size() == 0) {
-            kafkaPartitions = new ArrayList<>();
-            Properties props = new Properties();
-            props.put(""bootstrap.servers"", this.serverAddress);
-            props.put(""group.id"", FE_GROUP_ID);
-            props.put(""key.deserializer"", ""org.apache.kafka.common.serialization.StringDeserializer"");
-            props.put(""value.deserializer"", ""org.apache.kafka.common.serialization.StringDeserializer"");
-            KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
-            List<PartitionInfo> partitionList = consumer.partitionsFor(
-                    topic, Duration.ofSeconds(FETCH_PARTITIONS_TIMEOUT));
-            for (PartitionInfo partitionInfo : partitionList) {
-                kafkaPartitions.add(partitionInfo.partition());
+    // current kafka partitions = customKafkaPartitions == 0 ? all of partition of kafka topic : customKafkaPartitions
+    private void updateCurrentKafkaPartitions() {
+        if (customKafkaPartitions == null || customKafkaPartitions.size() == 0) {
+            LOG.debug(""All of partitions which belong to topic will be load for {} routine load job"", name);","[{'comment': 'be load -> be loaded', 'commenter': 'morningman'}]"
665,fe/src/main/java/org/apache/doris/load/routineload/KafkaRoutineLoadJob.java,"@@ -206,24 +235,33 @@ public static KafkaRoutineLoadJob fromCreateStmt(CreateRoutineLoadStmt stmt) thr
         return kafkaRoutineLoadJob;
     }
 
-    private void updatePartitions() {
-        // fetch all of kafkaPartitions in topic
-        if (kafkaPartitions == null || kafkaPartitions.size() == 0) {
-            kafkaPartitions = new ArrayList<>();
-            Properties props = new Properties();
-            props.put(""bootstrap.servers"", this.serverAddress);
-            props.put(""group.id"", FE_GROUP_ID);
-            props.put(""key.deserializer"", ""org.apache.kafka.common.serialization.StringDeserializer"");
-            props.put(""value.deserializer"", ""org.apache.kafka.common.serialization.StringDeserializer"");
-            KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
-            List<PartitionInfo> partitionList = consumer.partitionsFor(
-                    topic, Duration.ofSeconds(FETCH_PARTITIONS_TIMEOUT));
-            for (PartitionInfo partitionInfo : partitionList) {
-                kafkaPartitions.add(partitionInfo.partition());
+    // current kafka partitions = customKafkaPartitions == 0 ? all of partition of kafka topic : customKafkaPartitions
+    private void updateCurrentKafkaPartitions() {
+        if (customKafkaPartitions == null || customKafkaPartitions.size() == 0) {
+            LOG.debug(""All of partitions which belong to topic will be load for {} routine load job"", name);
+            // fetch all of kafkaPartitions in topic
+            currentKafkaPartitions.addAll(getAllKafkaPartitions());
+        } else {
+            currentKafkaPartitions = customKafkaPartitions;
+        }
+        for (Integer kafkaPartition : currentKafkaPartitions) {
+            try {
+                ((KafkaProgress) progress).getPartitionIdToOffset().get(kafkaPartition);","[{'comment': ""What's this for?"", 'commenter': 'morningman'}, {'comment': 'Update the progress of new partitions', 'commenter': 'EmmyMiao87'}]"
665,fe/src/main/java/org/apache/doris/load/routineload/KafkaRoutineLoadJob.java,"@@ -206,24 +235,33 @@ public static KafkaRoutineLoadJob fromCreateStmt(CreateRoutineLoadStmt stmt) thr
         return kafkaRoutineLoadJob;
     }
 
-    private void updatePartitions() {
-        // fetch all of kafkaPartitions in topic
-        if (kafkaPartitions == null || kafkaPartitions.size() == 0) {
-            kafkaPartitions = new ArrayList<>();
-            Properties props = new Properties();
-            props.put(""bootstrap.servers"", this.serverAddress);
-            props.put(""group.id"", FE_GROUP_ID);
-            props.put(""key.deserializer"", ""org.apache.kafka.common.serialization.StringDeserializer"");
-            props.put(""value.deserializer"", ""org.apache.kafka.common.serialization.StringDeserializer"");
-            KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
-            List<PartitionInfo> partitionList = consumer.partitionsFor(
-                    topic, Duration.ofSeconds(FETCH_PARTITIONS_TIMEOUT));
-            for (PartitionInfo partitionInfo : partitionList) {
-                kafkaPartitions.add(partitionInfo.partition());
+    // current kafka partitions = customKafkaPartitions == 0 ? all of partition of kafka topic : customKafkaPartitions
+    private void updateCurrentKafkaPartitions() {
+        if (customKafkaPartitions == null || customKafkaPartitions.size() == 0) {
+            LOG.debug(""All of partitions which belong to topic will be load for {} routine load job"", name);
+            // fetch all of kafkaPartitions in topic
+            currentKafkaPartitions.addAll(getAllKafkaPartitions());
+        } else {
+            currentKafkaPartitions = customKafkaPartitions;","[{'comment': 'Do you check whether the user specified partitions are all valid partitions? ', 'commenter': 'morningman'}, {'comment': 'I will check it in setOptional()', 'commenter': 'EmmyMiao87'}]"
665,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoadJob.java,"@@ -500,10 +500,14 @@ public void onCommitted(TransactionState txnState) {
                 Catalog.getCurrentCatalog().getRoutineLoadManager()
                         .getNeedSchedulerTasksQueue().addAll(Lists.newArrayList(newRoutineLoadTaskInfo));","[{'comment': 'Why using addAll() to add a single elements?', 'commenter': 'morningman'}]"
665,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoadJob.java,"@@ -606,4 +610,23 @@ public void stop() {
             writeUnlock();
         }
     }
+
+    public void rescheduler() {","[{'comment': 'rescheduler -> reschedule', 'commenter': 'morningman'}]"
665,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoadJob.java,"@@ -606,4 +610,23 @@ public void stop() {
             writeUnlock();
         }
     }
+
+    public void rescheduler() {
+        if (needRescheduler()) {
+            writeLock();
+            try {
+                if (state == JobState.RUNNING) {
+                    state = JobState.NEED_SCHEDULER;","[{'comment': 'NEED_SCHEDULER -> NEED_SCHEDULE', 'commenter': 'morningman'}]"
665,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoadJob.java,"@@ -606,4 +610,23 @@ public void stop() {
             writeUnlock();
         }
     }
+
+    public void rescheduler() {
+        if (needRescheduler()) {
+            writeLock();
+            try {
+                if (state == JobState.RUNNING) {
+                    state = JobState.NEED_SCHEDULER;
+                    routineLoadTaskInfoList.clear();
+                    needSchedulerTaskInfoList.clear();
+                }
+            } finally {
+                writeUnlock();
+            }
+        }
+    }
+
+    protected boolean needRescheduler() {","[{'comment': 'needRescheduler -> needReschedule', 'commenter': 'morningman'}]"
665,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoadManager.java,"@@ -402,4 +396,10 @@ public void removeOldRoutineLoadJobs() {
         // TODO(ml): remove old routine load job
     }
 
+    public void reSchedulerRoutineLoadJob() {","[{'comment': 'reSchedulerRoutineLoadJob  -> rescheduleRoutineLoadJob', 'commenter': 'morningman'}]"
665,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoadTaskScheduler.java,"@@ -28,52 +28,74 @@
 import org.apache.logging.log4j.LogManager;
 import org.apache.logging.log4j.Logger;
 
+import java.sql.Date;
 import java.util.Queue;
+import java.util.concurrent.LinkedBlockingQueue;
+import java.util.concurrent.atomic.AtomicBoolean;
 
 /**
  * Routine load task scheduler is a function which allocate task to be.
  * Step1: get total idle task num of backends.
+ *   Step1.1: if total idle task num == 0, exit this round and switch to the next round immediately
  * Step2: equally divide to be
+ *   Step2.1: if there is no task in queue, waiting task until an element becomes available.
+ *   Step2.2: divide task to be
  * Step3: submit tasks to be
  */
-// TODO(ml): change interval ms in constructor
 public class RoutineLoadTaskScheduler extends Daemon {
 
     private static final Logger LOG = LogManager.getLogger(RoutineLoadTaskScheduler.class);
 
-    private RoutineLoadManager routineLoadManager = Catalog.getInstance().getRoutineLoadManager();
+    private RoutineLoadManager routineLoadManager;
+    private LinkedBlockingQueue<RoutineLoadTaskInfo> needSchedulerTasksQueue;
+
+    public RoutineLoadTaskScheduler() {
+        super(""routine load task"", 0);
+        routineLoadManager = Catalog.getInstance().getRoutineLoadManager();
+        needSchedulerTasksQueue = (LinkedBlockingQueue) routineLoadManager.getNeedSchedulerTasksQueue();
+    }
 
     @Override
     protected void runOneCycle() {
         try {
             process();
         } catch (Throwable e) {
             LOG.warn(""Failed to process one round of RoutineLoadTaskScheduler with error message {}"",
-                    e.getMessage(), e);
+                     e.getMessage(), e);
         }
     }
 
     private void process() throws LoadException {
         // update current beIdMaps for tasks
         routineLoadManager.updateBeIdTaskMaps();
 
-        // get idle be task num
+        LOG.info(""There are {} need scheduler task in queue when {}"",
+                 needSchedulerTasksQueue.size(), System.currentTimeMillis());
+        AgentBatchTask batchTask = new AgentBatchTask();
+        int sizeOfTasksQueue = needSchedulerTasksQueue.size();
         int clusterIdleSlotNum = routineLoadManager.getClusterIdleSlotNum();
+        int needScheduledTaskNum = sizeOfTasksQueue < clusterIdleSlotNum ? sizeOfTasksQueue : clusterIdleSlotNum;
         int scheduledTaskNum = 0;
-        Queue<RoutineLoadTaskInfo> needSchedulerTasksQueue = routineLoadManager.getNeedSchedulerTasksQueue();
-        AgentBatchTask batchTask = new AgentBatchTask();
-
+        // get idle be task num
         // allocate task to be
-        while (clusterIdleSlotNum > 0) {
-            if (needSchedulerTasksQueue.peek() != null) {
-                RoutineLoadTaskInfo routineLoadTaskInfo = needSchedulerTasksQueue.poll();
+        while (needScheduledTaskNum > 0) {
+            RoutineLoadTaskInfo routineLoadTaskInfo = null;
+            try {
+                routineLoadTaskInfo = needSchedulerTasksQueue.take();
+            } catch (InterruptedException e) {
+                LOG.warn(""Taking routine load task from queue has been interrupted with error msg {}"",
+                         e.getMessage());
+                return;
+            }
+
+            if (clusterIdleSlotNum > 0) {","[{'comment': ""You don't need to check if clusterIdleSlotNum > 0 here. Because if clusterIdleSlotNum <= 0, you will not enter this while loop at all. And you already make sure that clusterIdleSlotNum is always larger than needScheduledTaskNum."", 'commenter': 'morningman'}]"
665,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoadTaskScheduler.java,"@@ -28,52 +28,74 @@
 import org.apache.logging.log4j.LogManager;
 import org.apache.logging.log4j.Logger;
 
+import java.sql.Date;
 import java.util.Queue;
+import java.util.concurrent.LinkedBlockingQueue;
+import java.util.concurrent.atomic.AtomicBoolean;
 
 /**
  * Routine load task scheduler is a function which allocate task to be.
  * Step1: get total idle task num of backends.
+ *   Step1.1: if total idle task num == 0, exit this round and switch to the next round immediately
  * Step2: equally divide to be
+ *   Step2.1: if there is no task in queue, waiting task until an element becomes available.
+ *   Step2.2: divide task to be
  * Step3: submit tasks to be
  */
-// TODO(ml): change interval ms in constructor
 public class RoutineLoadTaskScheduler extends Daemon {
 
     private static final Logger LOG = LogManager.getLogger(RoutineLoadTaskScheduler.class);
 
-    private RoutineLoadManager routineLoadManager = Catalog.getInstance().getRoutineLoadManager();
+    private RoutineLoadManager routineLoadManager;
+    private LinkedBlockingQueue<RoutineLoadTaskInfo> needSchedulerTasksQueue;
+
+    public RoutineLoadTaskScheduler() {
+        super(""routine load task"", 0);
+        routineLoadManager = Catalog.getInstance().getRoutineLoadManager();
+        needSchedulerTasksQueue = (LinkedBlockingQueue) routineLoadManager.getNeedSchedulerTasksQueue();
+    }
 
     @Override
     protected void runOneCycle() {
         try {
             process();
         } catch (Throwable e) {
             LOG.warn(""Failed to process one round of RoutineLoadTaskScheduler with error message {}"",
-                    e.getMessage(), e);
+                     e.getMessage(), e);
         }
     }
 
     private void process() throws LoadException {
         // update current beIdMaps for tasks
         routineLoadManager.updateBeIdTaskMaps();
 
-        // get idle be task num
+        LOG.info(""There are {} need scheduler task in queue when {}"",
+                 needSchedulerTasksQueue.size(), System.currentTimeMillis());
+        AgentBatchTask batchTask = new AgentBatchTask();
+        int sizeOfTasksQueue = needSchedulerTasksQueue.size();
         int clusterIdleSlotNum = routineLoadManager.getClusterIdleSlotNum();
+        int needScheduledTaskNum = sizeOfTasksQueue < clusterIdleSlotNum ? sizeOfTasksQueue : clusterIdleSlotNum;
         int scheduledTaskNum = 0;
-        Queue<RoutineLoadTaskInfo> needSchedulerTasksQueue = routineLoadManager.getNeedSchedulerTasksQueue();
-        AgentBatchTask batchTask = new AgentBatchTask();
-
+        // get idle be task num
         // allocate task to be
-        while (clusterIdleSlotNum > 0) {
-            if (needSchedulerTasksQueue.peek() != null) {
-                RoutineLoadTaskInfo routineLoadTaskInfo = needSchedulerTasksQueue.poll();
+        while (needScheduledTaskNum > 0) {
+            RoutineLoadTaskInfo routineLoadTaskInfo = null;
+            try {
+                routineLoadTaskInfo = needSchedulerTasksQueue.take();
+            } catch (InterruptedException e) {
+                LOG.warn(""Taking routine load task from queue has been interrupted with error msg {}"",
+                         e.getMessage());
+                return;
+            }
+
+            if (clusterIdleSlotNum > 0) {
                 long beId = routineLoadManager.getMinTaskBeId();
                 RoutineLoadJob routineLoadJob = null;
                 try {
                     routineLoadJob = routineLoadManager.getJobByTaskId(routineLoadTaskInfo.getId());
                 } catch (MetaNotFoundException e) {
                     LOG.warn(""task {} has been abandoned"", routineLoadTaskInfo.getId());
-                    continue;
+                    return;
                 }
                 RoutineLoadTask routineLoadTask = routineLoadTaskInfo.createStreamLoadTask(beId);
                 if (routineLoadTask != null) {","[{'comment': 'createStreamLoadTask seems never return null?', 'commenter': 'morningman'}]"
669,be/src/olap/olap_meta.cpp,"@@ -155,4 +155,25 @@ std::string OlapMeta::get_root_path() {
     return _root_path;
 }
 
+OLAPStatus OlapMeta::get_tablet_convert_finished(bool& flag) {
+    // get is_header_converted flag
+    std::string value;
+    std::string key = TABLET_CONVERT_FINISHED;
+    OLAPStatus s = get(DEFAULT_COLUMN_FAMILY_INDEX, key, value);","[{'comment': 'Should use pointer for output parameter', 'commenter': 'imay'}]"
669,be/src/olap/olap_snapshot_converter.cpp,"@@ -0,0 +1,367 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""olap/olap_snapshot_converter.h""
+
+#include ""olap/rowset/alpha_rowset.h""
+#include ""olap/rowset/rowset_id_generator.h""
+
+namespace doris {
+
+OLAPStatus OlapSnapshotConverter::to_olap_header(const TabletMetaPB& tablet_meta_pb, OLAPHeaderMessage* olap_header) {
+    olap_header->set_num_rows_per_data_block(tablet_meta_pb.schema().num_rows_per_row_block());
+    olap_header->set_cumulative_layer_point(tablet_meta_pb.cumulative_layer_point());
+    olap_header->set_num_short_key_fields(tablet_meta_pb.schema().num_short_key_columns());
+
+    for (auto& column : tablet_meta_pb.schema().column()) {
+        ColumnMessage* column_msg = olap_header->add_column();
+        to_column_msg(column, column_msg);
+    }
+
+    olap_header->set_creation_time(tablet_meta_pb.creation_time());
+    olap_header->set_data_file_type(DataFileType::COLUMN_ORIENTED_FILE);
+    olap_header->set_next_column_unique_id(tablet_meta_pb.schema().next_column_unique_id());
+    olap_header->set_compress_kind(tablet_meta_pb.schema().compress_kind());
+   
+    olap_header->set_bf_fpp(tablet_meta_pb.schema().bf_fpp());
+    olap_header->set_keys_type(tablet_meta_pb.schema().keys_type());
+
+    for (auto& rs_meta : tablet_meta_pb.rs_metas()) {
+        PDelta* pdelta = olap_header->add_delta();
+        convert_to_pdelta(rs_meta, pdelta);
+    }
+    // not add pending delta, it is usedless in clone or backup restore
+    for (auto& inc_rs_meta : tablet_meta_pb.inc_rs_metas()) {
+        PDelta* pdelta = olap_header->add_incremental_delta();
+        convert_to_pdelta(inc_rs_meta, pdelta);
+    }
+
+    olap_header->set_in_restore_mode(tablet_meta_pb.in_restore_mode());
+    olap_header->set_tablet_id(tablet_meta_pb.tablet_id());
+    olap_header->set_schema_hash(tablet_meta_pb.schema_hash());
+    olap_header->set_shard(tablet_meta_pb.shard_id());
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus OlapSnapshotConverter::to_tablet_meta_pb(const OLAPHeaderMessage& olap_header, TabletMetaPB* tablet_meta_pb, 
+                                                  vector<RowsetMetaPB>* pending_rowsets, DataDir* data_dir) {
+    tablet_meta_pb->set_tablet_id(olap_header.tablet_id());
+    tablet_meta_pb->set_schema_hash(olap_header.schema_hash());
+    tablet_meta_pb->set_shard_id(olap_header.shard());
+    tablet_meta_pb->set_creation_time(olap_header.creation_time());
+    tablet_meta_pb->set_cumulative_layer_point(olap_header.cumulative_layer_point());
+
+    TabletSchemaPB* schema = tablet_meta_pb->mutable_schema();
+    for (auto& column_msg : olap_header.column()) {
+        ColumnPB* column_pb = schema->add_column();
+        to_column_pb(column_msg, column_pb);
+    }
+    schema->set_keys_type(olap_header.keys_type());
+    schema->set_num_short_key_columns(olap_header.num_short_key_fields());
+    schema->set_num_rows_per_row_block(olap_header.num_rows_per_data_block());
+    schema->set_compress_kind(olap_header.compress_kind());
+    schema->set_bf_fpp(olap_header.bf_fpp());
+    schema->set_next_column_unique_id(olap_header.next_column_unique_id());
+
+    for (auto& delta : olap_header.delta()) {
+        RowsetMetaPB* rowset_meta = tablet_meta_pb->add_rs_metas();
+        RowsetId next_id;
+        RETURN_NOT_OK(RowsetIdGenerator::instance()->get_next_id(data_dir, &next_id));
+        convert_to_rowset_meta(delta, rowset_meta, next_id, olap_header.tablet_id(), olap_header.schema_hash());
+    }
+
+    for (auto& inc_delta : olap_header.incremental_delta()) {
+        RowsetMetaPB* rowset_meta = tablet_meta_pb->add_inc_rs_metas();
+        RowsetId next_id;
+        RETURN_NOT_OK(RowsetIdGenerator::instance()->get_next_id(data_dir, &next_id));
+        convert_to_rowset_meta(inc_delta, rowset_meta, next_id, olap_header.tablet_id(), olap_header.schema_hash());
+    }
+
+    for (auto& pending_delta : olap_header.pending_delta()) {
+        RowsetMetaPB rowset_meta;
+        RowsetId next_id;
+        RETURN_NOT_OK(RowsetIdGenerator::instance()->get_next_id(data_dir, &next_id));
+        convert_to_rowset_meta(pending_delta, &rowset_meta, next_id, olap_header.tablet_id(), olap_header.schema_hash());
+        pending_rowsets->push_back(std::move(rowset_meta));","[{'comment': '```suggestion\r\n        pending_rowsets->emplace_back(std::move(rowset_meta));\r\n```', 'commenter': 'imay'}]"
669,be/src/olap/olap_snapshot_converter.cpp,"@@ -0,0 +1,367 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""olap/olap_snapshot_converter.h""
+
+#include ""olap/rowset/alpha_rowset.h""
+#include ""olap/rowset/rowset_id_generator.h""
+
+namespace doris {
+
+OLAPStatus OlapSnapshotConverter::to_olap_header(const TabletMetaPB& tablet_meta_pb, OLAPHeaderMessage* olap_header) {
+    olap_header->set_num_rows_per_data_block(tablet_meta_pb.schema().num_rows_per_row_block());
+    olap_header->set_cumulative_layer_point(tablet_meta_pb.cumulative_layer_point());
+    olap_header->set_num_short_key_fields(tablet_meta_pb.schema().num_short_key_columns());
+
+    for (auto& column : tablet_meta_pb.schema().column()) {
+        ColumnMessage* column_msg = olap_header->add_column();
+        to_column_msg(column, column_msg);
+    }
+
+    olap_header->set_creation_time(tablet_meta_pb.creation_time());
+    olap_header->set_data_file_type(DataFileType::COLUMN_ORIENTED_FILE);
+    olap_header->set_next_column_unique_id(tablet_meta_pb.schema().next_column_unique_id());
+    olap_header->set_compress_kind(tablet_meta_pb.schema().compress_kind());
+   
+    olap_header->set_bf_fpp(tablet_meta_pb.schema().bf_fpp());
+    olap_header->set_keys_type(tablet_meta_pb.schema().keys_type());
+
+    for (auto& rs_meta : tablet_meta_pb.rs_metas()) {
+        PDelta* pdelta = olap_header->add_delta();
+        convert_to_pdelta(rs_meta, pdelta);
+    }
+    // not add pending delta, it is usedless in clone or backup restore
+    for (auto& inc_rs_meta : tablet_meta_pb.inc_rs_metas()) {
+        PDelta* pdelta = olap_header->add_incremental_delta();
+        convert_to_pdelta(inc_rs_meta, pdelta);
+    }
+
+    olap_header->set_in_restore_mode(tablet_meta_pb.in_restore_mode());
+    olap_header->set_tablet_id(tablet_meta_pb.tablet_id());
+    olap_header->set_schema_hash(tablet_meta_pb.schema_hash());
+    olap_header->set_shard(tablet_meta_pb.shard_id());
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus OlapSnapshotConverter::to_tablet_meta_pb(const OLAPHeaderMessage& olap_header, TabletMetaPB* tablet_meta_pb, 
+                                                  vector<RowsetMetaPB>* pending_rowsets, DataDir* data_dir) {
+    tablet_meta_pb->set_tablet_id(olap_header.tablet_id());
+    tablet_meta_pb->set_schema_hash(olap_header.schema_hash());
+    tablet_meta_pb->set_shard_id(olap_header.shard());
+    tablet_meta_pb->set_creation_time(olap_header.creation_time());
+    tablet_meta_pb->set_cumulative_layer_point(olap_header.cumulative_layer_point());
+
+    TabletSchemaPB* schema = tablet_meta_pb->mutable_schema();
+    for (auto& column_msg : olap_header.column()) {
+        ColumnPB* column_pb = schema->add_column();
+        to_column_pb(column_msg, column_pb);
+    }
+    schema->set_keys_type(olap_header.keys_type());
+    schema->set_num_short_key_columns(olap_header.num_short_key_fields());
+    schema->set_num_rows_per_row_block(olap_header.num_rows_per_data_block());
+    schema->set_compress_kind(olap_header.compress_kind());
+    schema->set_bf_fpp(olap_header.bf_fpp());
+    schema->set_next_column_unique_id(olap_header.next_column_unique_id());
+
+    for (auto& delta : olap_header.delta()) {
+        RowsetMetaPB* rowset_meta = tablet_meta_pb->add_rs_metas();
+        RowsetId next_id;
+        RETURN_NOT_OK(RowsetIdGenerator::instance()->get_next_id(data_dir, &next_id));
+        convert_to_rowset_meta(delta, rowset_meta, next_id, olap_header.tablet_id(), olap_header.schema_hash());
+    }
+
+    for (auto& inc_delta : olap_header.incremental_delta()) {
+        RowsetMetaPB* rowset_meta = tablet_meta_pb->add_inc_rs_metas();
+        RowsetId next_id;
+        RETURN_NOT_OK(RowsetIdGenerator::instance()->get_next_id(data_dir, &next_id));
+        convert_to_rowset_meta(inc_delta, rowset_meta, next_id, olap_header.tablet_id(), olap_header.schema_hash());
+    }
+
+    for (auto& pending_delta : olap_header.pending_delta()) {
+        RowsetMetaPB rowset_meta;
+        RowsetId next_id;
+        RETURN_NOT_OK(RowsetIdGenerator::instance()->get_next_id(data_dir, &next_id));
+        convert_to_rowset_meta(pending_delta, &rowset_meta, next_id, olap_header.tablet_id(), olap_header.schema_hash());
+        pending_rowsets->push_back(std::move(rowset_meta));
+    }
+    AlterTabletPB* alter_tablet_pb = tablet_meta_pb->mutable_alter_tablet_task();
+    to_alter_tablet_pb(olap_header.schema_change_status(), alter_tablet_pb);
+    tablet_meta_pb->set_in_restore_mode(olap_header.in_restore_mode());
+    tablet_meta_pb->set_tablet_state(TabletStatePB::PB_RUNNING);
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus OlapSnapshotConverter::convert_to_pdelta(const RowsetMetaPB& rowset_meta_pb, PDelta* delta) {
+    delta->set_start_version(rowset_meta_pb.start_version());
+    delta->set_end_version(rowset_meta_pb.end_version());
+    delta->set_version_hash(rowset_meta_pb.version_hash());
+    delta->set_creation_time(rowset_meta_pb.creation_time());
+    AlphaRowsetExtraMetaPB extra_meta_pb;
+    extra_meta_pb.ParseFromString(rowset_meta_pb.extra_properties());
+    
+    for (auto& segment_group : extra_meta_pb.segment_groups()) {
+        SegmentGroupPB* new_segment_group = delta->add_segment_group();
+        *new_segment_group = segment_group;
+    }
+    DeletePredicatePB* delete_condition = delta->mutable_delete_condition();
+    *delete_condition = rowset_meta_pb.delete_predicate();
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus OlapSnapshotConverter::convert_to_ppending_delta(const RowsetMetaPB& rowset_meta_pb, PPendingDelta* pending_delta) {
+    pending_delta->set_partition_id(rowset_meta_pb.partition_id());
+    pending_delta->set_transaction_id(rowset_meta_pb.txn_id());
+    pending_delta->set_creation_time(rowset_meta_pb.creation_time());
+
+    AlphaRowsetExtraMetaPB extra_meta_pb;
+    extra_meta_pb.ParseFromString(rowset_meta_pb.extra_properties());
+    for (auto& pending_segment_group : extra_meta_pb.pending_segment_groups()) {
+        PendingSegmentGroupPB* new_pending_segment_group = pending_delta->add_pending_segment_group();
+        *new_pending_segment_group = pending_segment_group;
+    }
+    DeletePredicatePB* delete_condition = pending_delta->mutable_delete_condition();
+    *delete_condition = rowset_meta_pb.delete_predicate();
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus OlapSnapshotConverter::convert_to_rowset_meta(const PDelta& delta, RowsetMetaPB* rowset_meta_pb, 
+        int64_t rowset_id, int64_t tablet_id, int32_t schema_hash) {
+    rowset_meta_pb->set_rowset_id(rowset_id);
+    rowset_meta_pb->set_tablet_id(tablet_id);
+    rowset_meta_pb->set_tablet_schema_hash(schema_hash);
+    rowset_meta_pb->set_rowset_type(RowsetTypePB::ALPHA_ROWSET);
+    rowset_meta_pb->set_rowset_state(RowsetStatePB::VISIBLE);
+    rowset_meta_pb->set_start_version(delta.start_version());
+    rowset_meta_pb->set_end_version(delta.end_version());
+    rowset_meta_pb->set_version_hash(delta.version_hash());
+    
+    bool empty = true;
+    int64_t num_rows = 0;
+    int64_t index_size = 0;
+    int64_t data_size = 0;
+    AlphaRowsetExtraMetaPB extra_meta_pb;
+    for (auto& segment_group : delta.segment_group()) {
+        SegmentGroupPB* new_segment_group = extra_meta_pb.add_segment_groups();
+        *new_segment_group = segment_group;
+        if (!segment_group.empty()) {
+            empty = false;
+        }
+        num_rows += segment_group.num_rows();
+        index_size += segment_group.index_size();
+        data_size += segment_group.data_size();
+    }
+    std::string extra_properties;
+    extra_meta_pb.SerializeToString(&extra_properties);
+    rowset_meta_pb->set_extra_properties(extra_properties);
+
+    rowset_meta_pb->set_empty(empty);
+    rowset_meta_pb->set_num_rows(num_rows);
+    rowset_meta_pb->set_data_disk_size(data_size);
+    rowset_meta_pb->set_index_disk_size(index_size);
+    rowset_meta_pb->set_total_disk_size(data_size + index_size);
+    DeletePredicatePB* delete_condition = rowset_meta_pb->mutable_delete_predicate();
+    *delete_condition = delta.delete_condition();
+    rowset_meta_pb->set_creation_time(delta.creation_time());
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus OlapSnapshotConverter::convert_to_rowset_meta(const PPendingDelta& pending_delta, RowsetMetaPB* rowset_meta_pb, 
+        int64_t rowset_id, int64_t tablet_id, int32_t schema_hash) {","[{'comment': 'move the rowset_meta_pb argument to the last', 'commenter': 'kangpinghuang'}]"
669,be/src/olap/olap_snapshot_converter.cpp,"@@ -0,0 +1,367 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""olap/olap_snapshot_converter.h""
+
+#include ""olap/rowset/alpha_rowset.h""
+#include ""olap/rowset/rowset_id_generator.h""
+
+namespace doris {
+
+OLAPStatus OlapSnapshotConverter::to_olap_header(const TabletMetaPB& tablet_meta_pb, OLAPHeaderMessage* olap_header) {
+    olap_header->set_num_rows_per_data_block(tablet_meta_pb.schema().num_rows_per_row_block());
+    olap_header->set_cumulative_layer_point(tablet_meta_pb.cumulative_layer_point());
+    olap_header->set_num_short_key_fields(tablet_meta_pb.schema().num_short_key_columns());
+
+    for (auto& column : tablet_meta_pb.schema().column()) {
+        ColumnMessage* column_msg = olap_header->add_column();
+        to_column_msg(column, column_msg);
+    }
+
+    olap_header->set_creation_time(tablet_meta_pb.creation_time());
+    olap_header->set_data_file_type(DataFileType::COLUMN_ORIENTED_FILE);
+    olap_header->set_next_column_unique_id(tablet_meta_pb.schema().next_column_unique_id());
+    olap_header->set_compress_kind(tablet_meta_pb.schema().compress_kind());
+   
+    olap_header->set_bf_fpp(tablet_meta_pb.schema().bf_fpp());
+    olap_header->set_keys_type(tablet_meta_pb.schema().keys_type());
+
+    for (auto& rs_meta : tablet_meta_pb.rs_metas()) {
+        PDelta* pdelta = olap_header->add_delta();
+        convert_to_pdelta(rs_meta, pdelta);
+    }
+    // not add pending delta, it is usedless in clone or backup restore
+    for (auto& inc_rs_meta : tablet_meta_pb.inc_rs_metas()) {
+        PDelta* pdelta = olap_header->add_incremental_delta();
+        convert_to_pdelta(inc_rs_meta, pdelta);
+    }
+
+    olap_header->set_in_restore_mode(tablet_meta_pb.in_restore_mode());
+    olap_header->set_tablet_id(tablet_meta_pb.tablet_id());
+    olap_header->set_schema_hash(tablet_meta_pb.schema_hash());
+    olap_header->set_shard(tablet_meta_pb.shard_id());
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus OlapSnapshotConverter::to_tablet_meta_pb(const OLAPHeaderMessage& olap_header, TabletMetaPB* tablet_meta_pb, 
+                                                  vector<RowsetMetaPB>* pending_rowsets, DataDir* data_dir) {
+    tablet_meta_pb->set_tablet_id(olap_header.tablet_id());
+    tablet_meta_pb->set_schema_hash(olap_header.schema_hash());
+    tablet_meta_pb->set_shard_id(olap_header.shard());
+    tablet_meta_pb->set_creation_time(olap_header.creation_time());
+    tablet_meta_pb->set_cumulative_layer_point(olap_header.cumulative_layer_point());
+
+    TabletSchemaPB* schema = tablet_meta_pb->mutable_schema();
+    for (auto& column_msg : olap_header.column()) {
+        ColumnPB* column_pb = schema->add_column();
+        to_column_pb(column_msg, column_pb);
+    }
+    schema->set_keys_type(olap_header.keys_type());
+    schema->set_num_short_key_columns(olap_header.num_short_key_fields());
+    schema->set_num_rows_per_row_block(olap_header.num_rows_per_data_block());
+    schema->set_compress_kind(olap_header.compress_kind());
+    schema->set_bf_fpp(olap_header.bf_fpp());
+    schema->set_next_column_unique_id(olap_header.next_column_unique_id());
+
+    for (auto& delta : olap_header.delta()) {
+        RowsetMetaPB* rowset_meta = tablet_meta_pb->add_rs_metas();
+        RowsetId next_id;
+        RETURN_NOT_OK(RowsetIdGenerator::instance()->get_next_id(data_dir, &next_id));
+        convert_to_rowset_meta(delta, rowset_meta, next_id, olap_header.tablet_id(), olap_header.schema_hash());
+    }
+
+    for (auto& inc_delta : olap_header.incremental_delta()) {
+        RowsetMetaPB* rowset_meta = tablet_meta_pb->add_inc_rs_metas();
+        RowsetId next_id;
+        RETURN_NOT_OK(RowsetIdGenerator::instance()->get_next_id(data_dir, &next_id));
+        convert_to_rowset_meta(inc_delta, rowset_meta, next_id, olap_header.tablet_id(), olap_header.schema_hash());
+    }
+
+    for (auto& pending_delta : olap_header.pending_delta()) {
+        RowsetMetaPB rowset_meta;
+        RowsetId next_id;
+        RETURN_NOT_OK(RowsetIdGenerator::instance()->get_next_id(data_dir, &next_id));
+        convert_to_rowset_meta(pending_delta, &rowset_meta, next_id, olap_header.tablet_id(), olap_header.schema_hash());
+        pending_rowsets->push_back(std::move(rowset_meta));
+    }
+    AlterTabletPB* alter_tablet_pb = tablet_meta_pb->mutable_alter_tablet_task();
+    to_alter_tablet_pb(olap_header.schema_change_status(), alter_tablet_pb);
+    tablet_meta_pb->set_in_restore_mode(olap_header.in_restore_mode());
+    tablet_meta_pb->set_tablet_state(TabletStatePB::PB_RUNNING);
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus OlapSnapshotConverter::convert_to_pdelta(const RowsetMetaPB& rowset_meta_pb, PDelta* delta) {
+    delta->set_start_version(rowset_meta_pb.start_version());
+    delta->set_end_version(rowset_meta_pb.end_version());
+    delta->set_version_hash(rowset_meta_pb.version_hash());
+    delta->set_creation_time(rowset_meta_pb.creation_time());
+    AlphaRowsetExtraMetaPB extra_meta_pb;
+    extra_meta_pb.ParseFromString(rowset_meta_pb.extra_properties());
+    
+    for (auto& segment_group : extra_meta_pb.segment_groups()) {
+        SegmentGroupPB* new_segment_group = delta->add_segment_group();
+        *new_segment_group = segment_group;
+    }
+    DeletePredicatePB* delete_condition = delta->mutable_delete_condition();
+    *delete_condition = rowset_meta_pb.delete_predicate();
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus OlapSnapshotConverter::convert_to_ppending_delta(const RowsetMetaPB& rowset_meta_pb, PPendingDelta* pending_delta) {
+    pending_delta->set_partition_id(rowset_meta_pb.partition_id());
+    pending_delta->set_transaction_id(rowset_meta_pb.txn_id());
+    pending_delta->set_creation_time(rowset_meta_pb.creation_time());
+
+    AlphaRowsetExtraMetaPB extra_meta_pb;
+    extra_meta_pb.ParseFromString(rowset_meta_pb.extra_properties());
+    for (auto& pending_segment_group : extra_meta_pb.pending_segment_groups()) {
+        PendingSegmentGroupPB* new_pending_segment_group = pending_delta->add_pending_segment_group();
+        *new_pending_segment_group = pending_segment_group;
+    }
+    DeletePredicatePB* delete_condition = pending_delta->mutable_delete_condition();
+    *delete_condition = rowset_meta_pb.delete_predicate();
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus OlapSnapshotConverter::convert_to_rowset_meta(const PDelta& delta, RowsetMetaPB* rowset_meta_pb, 
+        int64_t rowset_id, int64_t tablet_id, int32_t schema_hash) {
+    rowset_meta_pb->set_rowset_id(rowset_id);
+    rowset_meta_pb->set_tablet_id(tablet_id);
+    rowset_meta_pb->set_tablet_schema_hash(schema_hash);
+    rowset_meta_pb->set_rowset_type(RowsetTypePB::ALPHA_ROWSET);
+    rowset_meta_pb->set_rowset_state(RowsetStatePB::VISIBLE);
+    rowset_meta_pb->set_start_version(delta.start_version());
+    rowset_meta_pb->set_end_version(delta.end_version());
+    rowset_meta_pb->set_version_hash(delta.version_hash());
+    
+    bool empty = true;
+    int64_t num_rows = 0;
+    int64_t index_size = 0;
+    int64_t data_size = 0;
+    AlphaRowsetExtraMetaPB extra_meta_pb;
+    for (auto& segment_group : delta.segment_group()) {
+        SegmentGroupPB* new_segment_group = extra_meta_pb.add_segment_groups();
+        *new_segment_group = segment_group;
+        if (!segment_group.empty()) {
+            empty = false;
+        }
+        num_rows += segment_group.num_rows();
+        index_size += segment_group.index_size();
+        data_size += segment_group.data_size();
+    }
+    std::string extra_properties;
+    extra_meta_pb.SerializeToString(&extra_properties);
+    rowset_meta_pb->set_extra_properties(extra_properties);
+
+    rowset_meta_pb->set_empty(empty);
+    rowset_meta_pb->set_num_rows(num_rows);
+    rowset_meta_pb->set_data_disk_size(data_size);
+    rowset_meta_pb->set_index_disk_size(index_size);
+    rowset_meta_pb->set_total_disk_size(data_size + index_size);
+    DeletePredicatePB* delete_condition = rowset_meta_pb->mutable_delete_predicate();
+    *delete_condition = delta.delete_condition();
+    rowset_meta_pb->set_creation_time(delta.creation_time());
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus OlapSnapshotConverter::convert_to_rowset_meta(const PPendingDelta& pending_delta, RowsetMetaPB* rowset_meta_pb, 
+        int64_t rowset_id, int64_t tablet_id, int32_t schema_hash) {
+    rowset_meta_pb->set_rowset_id(rowset_id);
+    rowset_meta_pb->set_tablet_id(tablet_id);
+    rowset_meta_pb->set_tablet_schema_hash(schema_hash);
+    rowset_meta_pb->set_rowset_type(RowsetTypePB::ALPHA_ROWSET);
+    rowset_meta_pb->set_rowset_state(RowsetStatePB::COMMITTED);
+    rowset_meta_pb->set_partition_id(pending_delta.partition_id());
+    rowset_meta_pb->set_txn_id(pending_delta.transaction_id());
+    rowset_meta_pb->set_creation_time(pending_delta.creation_time());
+    
+    bool empty = true;
+    int64_t num_rows = 0;
+    int64_t index_size = 0;
+    int64_t data_size = 0;
+    AlphaRowsetExtraMetaPB extra_meta_pb;
+    for (auto& pending_segment_group : pending_delta.pending_segment_group()) {
+        PendingSegmentGroupPB* new_pending_segment_group = extra_meta_pb.add_pending_segment_groups();
+        *new_pending_segment_group = pending_segment_group;
+        if (!pending_segment_group.empty()) {
+            empty = false;
+        }
+        // num_rows += pending_segment_group.num_rows();
+        // index_size += pending_segment_group.index_size();
+        // data_size += pending_segment_group.data_size();","[{'comment': 'should add these lines of code', 'commenter': 'kangpinghuang'}]"
669,be/src/olap/olap_snapshot_converter.cpp,"@@ -0,0 +1,367 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""olap/olap_snapshot_converter.h""
+
+#include ""olap/rowset/alpha_rowset.h""
+#include ""olap/rowset/rowset_id_generator.h""
+
+namespace doris {
+
+OLAPStatus OlapSnapshotConverter::to_olap_header(const TabletMetaPB& tablet_meta_pb, OLAPHeaderMessage* olap_header) {
+    olap_header->set_num_rows_per_data_block(tablet_meta_pb.schema().num_rows_per_row_block());
+    olap_header->set_cumulative_layer_point(tablet_meta_pb.cumulative_layer_point());
+    olap_header->set_num_short_key_fields(tablet_meta_pb.schema().num_short_key_columns());
+
+    for (auto& column : tablet_meta_pb.schema().column()) {
+        ColumnMessage* column_msg = olap_header->add_column();
+        to_column_msg(column, column_msg);
+    }
+
+    olap_header->set_creation_time(tablet_meta_pb.creation_time());
+    olap_header->set_data_file_type(DataFileType::COLUMN_ORIENTED_FILE);
+    olap_header->set_next_column_unique_id(tablet_meta_pb.schema().next_column_unique_id());
+    olap_header->set_compress_kind(tablet_meta_pb.schema().compress_kind());
+   
+    olap_header->set_bf_fpp(tablet_meta_pb.schema().bf_fpp());
+    olap_header->set_keys_type(tablet_meta_pb.schema().keys_type());
+
+    for (auto& rs_meta : tablet_meta_pb.rs_metas()) {
+        PDelta* pdelta = olap_header->add_delta();
+        convert_to_pdelta(rs_meta, pdelta);
+    }
+    // not add pending delta, it is usedless in clone or backup restore
+    for (auto& inc_rs_meta : tablet_meta_pb.inc_rs_metas()) {
+        PDelta* pdelta = olap_header->add_incremental_delta();
+        convert_to_pdelta(inc_rs_meta, pdelta);
+    }
+
+    olap_header->set_in_restore_mode(tablet_meta_pb.in_restore_mode());
+    olap_header->set_tablet_id(tablet_meta_pb.tablet_id());
+    olap_header->set_schema_hash(tablet_meta_pb.schema_hash());
+    olap_header->set_shard(tablet_meta_pb.shard_id());
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus OlapSnapshotConverter::to_tablet_meta_pb(const OLAPHeaderMessage& olap_header, TabletMetaPB* tablet_meta_pb, 
+                                                  vector<RowsetMetaPB>* pending_rowsets, DataDir* data_dir) {
+    tablet_meta_pb->set_tablet_id(olap_header.tablet_id());
+    tablet_meta_pb->set_schema_hash(olap_header.schema_hash());
+    tablet_meta_pb->set_shard_id(olap_header.shard());
+    tablet_meta_pb->set_creation_time(olap_header.creation_time());
+    tablet_meta_pb->set_cumulative_layer_point(olap_header.cumulative_layer_point());
+
+    TabletSchemaPB* schema = tablet_meta_pb->mutable_schema();
+    for (auto& column_msg : olap_header.column()) {
+        ColumnPB* column_pb = schema->add_column();
+        to_column_pb(column_msg, column_pb);
+    }
+    schema->set_keys_type(olap_header.keys_type());
+    schema->set_num_short_key_columns(olap_header.num_short_key_fields());
+    schema->set_num_rows_per_row_block(olap_header.num_rows_per_data_block());
+    schema->set_compress_kind(olap_header.compress_kind());
+    schema->set_bf_fpp(olap_header.bf_fpp());
+    schema->set_next_column_unique_id(olap_header.next_column_unique_id());
+
+    for (auto& delta : olap_header.delta()) {
+        RowsetMetaPB* rowset_meta = tablet_meta_pb->add_rs_metas();
+        RowsetId next_id;
+        RETURN_NOT_OK(RowsetIdGenerator::instance()->get_next_id(data_dir, &next_id));
+        convert_to_rowset_meta(delta, rowset_meta, next_id, olap_header.tablet_id(), olap_header.schema_hash());
+    }
+
+    for (auto& inc_delta : olap_header.incremental_delta()) {
+        RowsetMetaPB* rowset_meta = tablet_meta_pb->add_inc_rs_metas();
+        RowsetId next_id;
+        RETURN_NOT_OK(RowsetIdGenerator::instance()->get_next_id(data_dir, &next_id));
+        convert_to_rowset_meta(inc_delta, rowset_meta, next_id, olap_header.tablet_id(), olap_header.schema_hash());
+    }
+
+    for (auto& pending_delta : olap_header.pending_delta()) {
+        RowsetMetaPB rowset_meta;
+        RowsetId next_id;
+        RETURN_NOT_OK(RowsetIdGenerator::instance()->get_next_id(data_dir, &next_id));
+        convert_to_rowset_meta(pending_delta, &rowset_meta, next_id, olap_header.tablet_id(), olap_header.schema_hash());
+        pending_rowsets->push_back(std::move(rowset_meta));
+    }
+    AlterTabletPB* alter_tablet_pb = tablet_meta_pb->mutable_alter_tablet_task();
+    to_alter_tablet_pb(olap_header.schema_change_status(), alter_tablet_pb);
+    tablet_meta_pb->set_in_restore_mode(olap_header.in_restore_mode());
+    tablet_meta_pb->set_tablet_state(TabletStatePB::PB_RUNNING);
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus OlapSnapshotConverter::convert_to_pdelta(const RowsetMetaPB& rowset_meta_pb, PDelta* delta) {
+    delta->set_start_version(rowset_meta_pb.start_version());
+    delta->set_end_version(rowset_meta_pb.end_version());
+    delta->set_version_hash(rowset_meta_pb.version_hash());
+    delta->set_creation_time(rowset_meta_pb.creation_time());
+    AlphaRowsetExtraMetaPB extra_meta_pb;
+    extra_meta_pb.ParseFromString(rowset_meta_pb.extra_properties());
+    
+    for (auto& segment_group : extra_meta_pb.segment_groups()) {
+        SegmentGroupPB* new_segment_group = delta->add_segment_group();
+        *new_segment_group = segment_group;
+    }
+    DeletePredicatePB* delete_condition = delta->mutable_delete_condition();
+    *delete_condition = rowset_meta_pb.delete_predicate();
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus OlapSnapshotConverter::convert_to_ppending_delta(const RowsetMetaPB& rowset_meta_pb, PPendingDelta* pending_delta) {
+    pending_delta->set_partition_id(rowset_meta_pb.partition_id());
+    pending_delta->set_transaction_id(rowset_meta_pb.txn_id());
+    pending_delta->set_creation_time(rowset_meta_pb.creation_time());
+
+    AlphaRowsetExtraMetaPB extra_meta_pb;
+    extra_meta_pb.ParseFromString(rowset_meta_pb.extra_properties());
+    for (auto& pending_segment_group : extra_meta_pb.pending_segment_groups()) {
+        PendingSegmentGroupPB* new_pending_segment_group = pending_delta->add_pending_segment_group();
+        *new_pending_segment_group = pending_segment_group;
+    }
+    DeletePredicatePB* delete_condition = pending_delta->mutable_delete_condition();
+    *delete_condition = rowset_meta_pb.delete_predicate();
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus OlapSnapshotConverter::convert_to_rowset_meta(const PDelta& delta, RowsetMetaPB* rowset_meta_pb, 
+        int64_t rowset_id, int64_t tablet_id, int32_t schema_hash) {
+    rowset_meta_pb->set_rowset_id(rowset_id);
+    rowset_meta_pb->set_tablet_id(tablet_id);
+    rowset_meta_pb->set_tablet_schema_hash(schema_hash);
+    rowset_meta_pb->set_rowset_type(RowsetTypePB::ALPHA_ROWSET);
+    rowset_meta_pb->set_rowset_state(RowsetStatePB::VISIBLE);
+    rowset_meta_pb->set_start_version(delta.start_version());
+    rowset_meta_pb->set_end_version(delta.end_version());
+    rowset_meta_pb->set_version_hash(delta.version_hash());
+    
+    bool empty = true;
+    int64_t num_rows = 0;
+    int64_t index_size = 0;
+    int64_t data_size = 0;
+    AlphaRowsetExtraMetaPB extra_meta_pb;
+    for (auto& segment_group : delta.segment_group()) {
+        SegmentGroupPB* new_segment_group = extra_meta_pb.add_segment_groups();
+        *new_segment_group = segment_group;
+        if (!segment_group.empty()) {
+            empty = false;
+        }
+        num_rows += segment_group.num_rows();
+        index_size += segment_group.index_size();
+        data_size += segment_group.data_size();
+    }
+    std::string extra_properties;
+    extra_meta_pb.SerializeToString(&extra_properties);
+    rowset_meta_pb->set_extra_properties(extra_properties);
+
+    rowset_meta_pb->set_empty(empty);
+    rowset_meta_pb->set_num_rows(num_rows);
+    rowset_meta_pb->set_data_disk_size(data_size);
+    rowset_meta_pb->set_index_disk_size(index_size);
+    rowset_meta_pb->set_total_disk_size(data_size + index_size);
+    DeletePredicatePB* delete_condition = rowset_meta_pb->mutable_delete_predicate();
+    *delete_condition = delta.delete_condition();
+    rowset_meta_pb->set_creation_time(delta.creation_time());
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus OlapSnapshotConverter::convert_to_rowset_meta(const PPendingDelta& pending_delta, RowsetMetaPB* rowset_meta_pb, 
+        int64_t rowset_id, int64_t tablet_id, int32_t schema_hash) {
+    rowset_meta_pb->set_rowset_id(rowset_id);
+    rowset_meta_pb->set_tablet_id(tablet_id);
+    rowset_meta_pb->set_tablet_schema_hash(schema_hash);
+    rowset_meta_pb->set_rowset_type(RowsetTypePB::ALPHA_ROWSET);
+    rowset_meta_pb->set_rowset_state(RowsetStatePB::COMMITTED);
+    rowset_meta_pb->set_partition_id(pending_delta.partition_id());
+    rowset_meta_pb->set_txn_id(pending_delta.transaction_id());
+    rowset_meta_pb->set_creation_time(pending_delta.creation_time());
+    
+    bool empty = true;
+    int64_t num_rows = 0;
+    int64_t index_size = 0;
+    int64_t data_size = 0;
+    AlphaRowsetExtraMetaPB extra_meta_pb;
+    for (auto& pending_segment_group : pending_delta.pending_segment_group()) {
+        PendingSegmentGroupPB* new_pending_segment_group = extra_meta_pb.add_pending_segment_groups();
+        *new_pending_segment_group = pending_segment_group;
+        if (!pending_segment_group.empty()) {
+            empty = false;
+        }
+        // num_rows += pending_segment_group.num_rows();
+        // index_size += pending_segment_group.index_size();
+        // data_size += pending_segment_group.data_size();
+    }
+    std::string extra_properties;
+    extra_meta_pb.SerializeToString(&extra_properties);
+    rowset_meta_pb->set_extra_properties(extra_properties);
+
+    rowset_meta_pb->set_empty(empty);
+    rowset_meta_pb->set_num_rows(num_rows);
+    rowset_meta_pb->set_data_disk_size(data_size);
+    rowset_meta_pb->set_index_disk_size(index_size);
+    rowset_meta_pb->set_total_disk_size(data_size + index_size);
+
+    DeletePredicatePB* delete_condition = rowset_meta_pb->mutable_delete_predicate();
+    *delete_condition = pending_delta.delete_condition();
+    rowset_meta_pb->set_creation_time(pending_delta.creation_time());
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus OlapSnapshotConverter::to_column_pb(const ColumnMessage& column_msg, ColumnPB* column_pb) {
+    column_pb->set_unique_id(column_msg.unique_id());
+    column_pb->set_name(column_msg.name());
+    column_pb->set_type(column_msg.type());
+    column_pb->set_is_key(column_msg.is_key());
+    column_pb->set_aggregation(column_msg.aggregation());
+    column_pb->set_is_nullable(column_msg.is_allow_null());
+    column_pb->set_default_value(column_msg.default_value());
+    column_pb->set_precision(column_msg.precision());
+    column_pb->set_frac(column_msg.frac());
+    column_pb->set_length(column_msg.length());
+    column_pb->set_index_length(column_msg.index_length());
+    column_pb->set_is_bf_column(column_msg.is_bf_column());
+    // TODO(ygl) calculate column id from column list
+    // column_pb->set_referenced_column_id(column_msg.());
+    column_pb->set_referenced_column(column_msg.referenced_column());
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus OlapSnapshotConverter::to_column_msg(const ColumnPB& column_pb, ColumnMessage* column_msg) {
+    column_msg->set_name(column_pb.name());
+    column_msg->set_type(column_pb.type());
+    column_msg->set_aggregation(column_pb.aggregation());
+    column_msg->set_length(column_pb.length());
+    column_msg->set_is_key(column_pb.is_key());
+    column_msg->set_default_value(column_pb.default_value());
+    column_msg->set_referenced_column(column_pb.referenced_column());
+    column_msg->set_index_length(column_pb.index_length());
+    column_msg->set_precision(column_pb.precision());
+    column_msg->set_frac(column_pb.frac());
+    column_msg->set_is_allow_null(column_pb.is_nullable());
+    column_msg->set_unique_id(column_pb.unique_id());
+    column_msg->set_is_bf_column(column_pb.is_bf_column());
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus OlapSnapshotConverter::to_alter_tablet_pb(const SchemaChangeStatusMessage& schema_change_msg, ","[{'comment': ""I think all these converter functions's return value could be void"", 'commenter': 'kangpinghuang'}]"
669,be/src/olap/olap_snapshot_converter.cpp,"@@ -0,0 +1,367 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""olap/olap_snapshot_converter.h""
+
+#include ""olap/rowset/alpha_rowset.h""
+#include ""olap/rowset/rowset_id_generator.h""
+
+namespace doris {
+
+OLAPStatus OlapSnapshotConverter::to_olap_header(const TabletMetaPB& tablet_meta_pb, OLAPHeaderMessage* olap_header) {
+    olap_header->set_num_rows_per_data_block(tablet_meta_pb.schema().num_rows_per_row_block());
+    olap_header->set_cumulative_layer_point(tablet_meta_pb.cumulative_layer_point());
+    olap_header->set_num_short_key_fields(tablet_meta_pb.schema().num_short_key_columns());
+
+    for (auto& column : tablet_meta_pb.schema().column()) {
+        ColumnMessage* column_msg = olap_header->add_column();
+        to_column_msg(column, column_msg);
+    }
+
+    olap_header->set_creation_time(tablet_meta_pb.creation_time());
+    olap_header->set_data_file_type(DataFileType::COLUMN_ORIENTED_FILE);
+    olap_header->set_next_column_unique_id(tablet_meta_pb.schema().next_column_unique_id());
+    olap_header->set_compress_kind(tablet_meta_pb.schema().compress_kind());
+   
+    olap_header->set_bf_fpp(tablet_meta_pb.schema().bf_fpp());
+    olap_header->set_keys_type(tablet_meta_pb.schema().keys_type());
+
+    for (auto& rs_meta : tablet_meta_pb.rs_metas()) {
+        PDelta* pdelta = olap_header->add_delta();
+        convert_to_pdelta(rs_meta, pdelta);
+    }
+    // not add pending delta, it is usedless in clone or backup restore
+    for (auto& inc_rs_meta : tablet_meta_pb.inc_rs_metas()) {
+        PDelta* pdelta = olap_header->add_incremental_delta();
+        convert_to_pdelta(inc_rs_meta, pdelta);
+    }
+
+    olap_header->set_in_restore_mode(tablet_meta_pb.in_restore_mode());
+    olap_header->set_tablet_id(tablet_meta_pb.tablet_id());
+    olap_header->set_schema_hash(tablet_meta_pb.schema_hash());
+    olap_header->set_shard(tablet_meta_pb.shard_id());
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus OlapSnapshotConverter::to_tablet_meta_pb(const OLAPHeaderMessage& olap_header, TabletMetaPB* tablet_meta_pb, 
+                                                  vector<RowsetMetaPB>* pending_rowsets, DataDir* data_dir) {
+    tablet_meta_pb->set_tablet_id(olap_header.tablet_id());
+    tablet_meta_pb->set_schema_hash(olap_header.schema_hash());
+    tablet_meta_pb->set_shard_id(olap_header.shard());
+    tablet_meta_pb->set_creation_time(olap_header.creation_time());
+    tablet_meta_pb->set_cumulative_layer_point(olap_header.cumulative_layer_point());
+
+    TabletSchemaPB* schema = tablet_meta_pb->mutable_schema();
+    for (auto& column_msg : olap_header.column()) {
+        ColumnPB* column_pb = schema->add_column();
+        to_column_pb(column_msg, column_pb);
+    }
+    schema->set_keys_type(olap_header.keys_type());
+    schema->set_num_short_key_columns(olap_header.num_short_key_fields());
+    schema->set_num_rows_per_row_block(olap_header.num_rows_per_data_block());
+    schema->set_compress_kind(olap_header.compress_kind());
+    schema->set_bf_fpp(olap_header.bf_fpp());
+    schema->set_next_column_unique_id(olap_header.next_column_unique_id());
+
+    for (auto& delta : olap_header.delta()) {
+        RowsetMetaPB* rowset_meta = tablet_meta_pb->add_rs_metas();
+        RowsetId next_id;
+        RETURN_NOT_OK(RowsetIdGenerator::instance()->get_next_id(data_dir, &next_id));
+        convert_to_rowset_meta(delta, rowset_meta, next_id, olap_header.tablet_id(), olap_header.schema_hash());
+    }
+
+    for (auto& inc_delta : olap_header.incremental_delta()) {
+        RowsetMetaPB* rowset_meta = tablet_meta_pb->add_inc_rs_metas();
+        RowsetId next_id;
+        RETURN_NOT_OK(RowsetIdGenerator::instance()->get_next_id(data_dir, &next_id));
+        convert_to_rowset_meta(inc_delta, rowset_meta, next_id, olap_header.tablet_id(), olap_header.schema_hash());
+    }
+
+    for (auto& pending_delta : olap_header.pending_delta()) {
+        RowsetMetaPB rowset_meta;
+        RowsetId next_id;
+        RETURN_NOT_OK(RowsetIdGenerator::instance()->get_next_id(data_dir, &next_id));
+        convert_to_rowset_meta(pending_delta, &rowset_meta, next_id, olap_header.tablet_id(), olap_header.schema_hash());
+        pending_rowsets->push_back(std::move(rowset_meta));
+    }
+    AlterTabletPB* alter_tablet_pb = tablet_meta_pb->mutable_alter_tablet_task();
+    to_alter_tablet_pb(olap_header.schema_change_status(), alter_tablet_pb);
+    tablet_meta_pb->set_in_restore_mode(olap_header.in_restore_mode());
+    tablet_meta_pb->set_tablet_state(TabletStatePB::PB_RUNNING);
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus OlapSnapshotConverter::convert_to_pdelta(const RowsetMetaPB& rowset_meta_pb, PDelta* delta) {
+    delta->set_start_version(rowset_meta_pb.start_version());
+    delta->set_end_version(rowset_meta_pb.end_version());
+    delta->set_version_hash(rowset_meta_pb.version_hash());
+    delta->set_creation_time(rowset_meta_pb.creation_time());
+    AlphaRowsetExtraMetaPB extra_meta_pb;
+    extra_meta_pb.ParseFromString(rowset_meta_pb.extra_properties());
+    
+    for (auto& segment_group : extra_meta_pb.segment_groups()) {
+        SegmentGroupPB* new_segment_group = delta->add_segment_group();
+        *new_segment_group = segment_group;
+    }
+    DeletePredicatePB* delete_condition = delta->mutable_delete_condition();
+    *delete_condition = rowset_meta_pb.delete_predicate();
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus OlapSnapshotConverter::convert_to_ppending_delta(const RowsetMetaPB& rowset_meta_pb, PPendingDelta* pending_delta) {
+    pending_delta->set_partition_id(rowset_meta_pb.partition_id());
+    pending_delta->set_transaction_id(rowset_meta_pb.txn_id());
+    pending_delta->set_creation_time(rowset_meta_pb.creation_time());
+
+    AlphaRowsetExtraMetaPB extra_meta_pb;
+    extra_meta_pb.ParseFromString(rowset_meta_pb.extra_properties());
+    for (auto& pending_segment_group : extra_meta_pb.pending_segment_groups()) {
+        PendingSegmentGroupPB* new_pending_segment_group = pending_delta->add_pending_segment_group();
+        *new_pending_segment_group = pending_segment_group;
+    }
+    DeletePredicatePB* delete_condition = pending_delta->mutable_delete_condition();
+    *delete_condition = rowset_meta_pb.delete_predicate();
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus OlapSnapshotConverter::convert_to_rowset_meta(const PDelta& delta, RowsetMetaPB* rowset_meta_pb, 
+        int64_t rowset_id, int64_t tablet_id, int32_t schema_hash) {
+    rowset_meta_pb->set_rowset_id(rowset_id);
+    rowset_meta_pb->set_tablet_id(tablet_id);
+    rowset_meta_pb->set_tablet_schema_hash(schema_hash);
+    rowset_meta_pb->set_rowset_type(RowsetTypePB::ALPHA_ROWSET);
+    rowset_meta_pb->set_rowset_state(RowsetStatePB::VISIBLE);
+    rowset_meta_pb->set_start_version(delta.start_version());
+    rowset_meta_pb->set_end_version(delta.end_version());
+    rowset_meta_pb->set_version_hash(delta.version_hash());
+    
+    bool empty = true;
+    int64_t num_rows = 0;
+    int64_t index_size = 0;
+    int64_t data_size = 0;
+    AlphaRowsetExtraMetaPB extra_meta_pb;
+    for (auto& segment_group : delta.segment_group()) {
+        SegmentGroupPB* new_segment_group = extra_meta_pb.add_segment_groups();
+        *new_segment_group = segment_group;
+        if (!segment_group.empty()) {
+            empty = false;
+        }
+        num_rows += segment_group.num_rows();
+        index_size += segment_group.index_size();
+        data_size += segment_group.data_size();
+    }
+    std::string extra_properties;
+    extra_meta_pb.SerializeToString(&extra_properties);
+    rowset_meta_pb->set_extra_properties(extra_properties);
+
+    rowset_meta_pb->set_empty(empty);
+    rowset_meta_pb->set_num_rows(num_rows);
+    rowset_meta_pb->set_data_disk_size(data_size);
+    rowset_meta_pb->set_index_disk_size(index_size);
+    rowset_meta_pb->set_total_disk_size(data_size + index_size);
+    DeletePredicatePB* delete_condition = rowset_meta_pb->mutable_delete_predicate();
+    *delete_condition = delta.delete_condition();
+    rowset_meta_pb->set_creation_time(delta.creation_time());
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus OlapSnapshotConverter::convert_to_rowset_meta(const PPendingDelta& pending_delta, RowsetMetaPB* rowset_meta_pb, 
+        int64_t rowset_id, int64_t tablet_id, int32_t schema_hash) {
+    rowset_meta_pb->set_rowset_id(rowset_id);
+    rowset_meta_pb->set_tablet_id(tablet_id);
+    rowset_meta_pb->set_tablet_schema_hash(schema_hash);
+    rowset_meta_pb->set_rowset_type(RowsetTypePB::ALPHA_ROWSET);
+    rowset_meta_pb->set_rowset_state(RowsetStatePB::COMMITTED);
+    rowset_meta_pb->set_partition_id(pending_delta.partition_id());
+    rowset_meta_pb->set_txn_id(pending_delta.transaction_id());
+    rowset_meta_pb->set_creation_time(pending_delta.creation_time());
+    
+    bool empty = true;
+    int64_t num_rows = 0;
+    int64_t index_size = 0;
+    int64_t data_size = 0;
+    AlphaRowsetExtraMetaPB extra_meta_pb;
+    for (auto& pending_segment_group : pending_delta.pending_segment_group()) {
+        PendingSegmentGroupPB* new_pending_segment_group = extra_meta_pb.add_pending_segment_groups();
+        *new_pending_segment_group = pending_segment_group;
+        if (!pending_segment_group.empty()) {
+            empty = false;
+        }
+        // num_rows += pending_segment_group.num_rows();
+        // index_size += pending_segment_group.index_size();
+        // data_size += pending_segment_group.data_size();
+    }
+    std::string extra_properties;
+    extra_meta_pb.SerializeToString(&extra_properties);
+    rowset_meta_pb->set_extra_properties(extra_properties);
+
+    rowset_meta_pb->set_empty(empty);
+    rowset_meta_pb->set_num_rows(num_rows);
+    rowset_meta_pb->set_data_disk_size(data_size);
+    rowset_meta_pb->set_index_disk_size(index_size);
+    rowset_meta_pb->set_total_disk_size(data_size + index_size);
+
+    DeletePredicatePB* delete_condition = rowset_meta_pb->mutable_delete_predicate();
+    *delete_condition = pending_delta.delete_condition();
+    rowset_meta_pb->set_creation_time(pending_delta.creation_time());
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus OlapSnapshotConverter::to_column_pb(const ColumnMessage& column_msg, ColumnPB* column_pb) {
+    column_pb->set_unique_id(column_msg.unique_id());
+    column_pb->set_name(column_msg.name());
+    column_pb->set_type(column_msg.type());
+    column_pb->set_is_key(column_msg.is_key());
+    column_pb->set_aggregation(column_msg.aggregation());
+    column_pb->set_is_nullable(column_msg.is_allow_null());
+    column_pb->set_default_value(column_msg.default_value());
+    column_pb->set_precision(column_msg.precision());
+    column_pb->set_frac(column_msg.frac());
+    column_pb->set_length(column_msg.length());
+    column_pb->set_index_length(column_msg.index_length());
+    column_pb->set_is_bf_column(column_msg.is_bf_column());
+    // TODO(ygl) calculate column id from column list
+    // column_pb->set_referenced_column_id(column_msg.());
+    column_pb->set_referenced_column(column_msg.referenced_column());
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus OlapSnapshotConverter::to_column_msg(const ColumnPB& column_pb, ColumnMessage* column_msg) {
+    column_msg->set_name(column_pb.name());
+    column_msg->set_type(column_pb.type());
+    column_msg->set_aggregation(column_pb.aggregation());
+    column_msg->set_length(column_pb.length());
+    column_msg->set_is_key(column_pb.is_key());
+    column_msg->set_default_value(column_pb.default_value());
+    column_msg->set_referenced_column(column_pb.referenced_column());
+    column_msg->set_index_length(column_pb.index_length());
+    column_msg->set_precision(column_pb.precision());
+    column_msg->set_frac(column_pb.frac());
+    column_msg->set_is_allow_null(column_pb.is_nullable());
+    column_msg->set_unique_id(column_pb.unique_id());
+    column_msg->set_is_bf_column(column_pb.is_bf_column());
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus OlapSnapshotConverter::to_alter_tablet_pb(const SchemaChangeStatusMessage& schema_change_msg, 
+                                                   AlterTabletPB* alter_tablet_pb) {
+    alter_tablet_pb->set_related_tablet_id(schema_change_msg.related_tablet_id());
+    alter_tablet_pb->set_related_schema_hash(schema_change_msg.related_schema_hash());
+    alter_tablet_pb->set_alter_type(static_cast<AlterTabletType>(schema_change_msg.schema_change_type()));
+    if (schema_change_msg.versions_to_changed().size() == 0) {
+        alter_tablet_pb->set_alter_state(AlterTabletState::ALTER_FINISHED);
+    } else {
+        alter_tablet_pb->set_alter_state(AlterTabletState::ALTER_FAILED);
+    }
+    return OLAP_SUCCESS;
+}
+
+// from olap header to tablet meta
+OLAPStatus OlapSnapshotConverter::to_new_snapshot(const OLAPHeaderMessage& olap_header, const string& old_data_path_prefix, 
+    TabletMetaPB* tablet_meta_pb, const string& new_data_path_prefix, DataDir& data_dir, vector<RowsetMetaPB>* pending_rowsets) {","[{'comment': 'move tablet_meta_pb argument before pending_rowsets', 'commenter': 'kangpinghuang'}]"
669,be/src/olap/olap_snapshot_converter.cpp,"@@ -0,0 +1,367 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""olap/olap_snapshot_converter.h""
+
+#include ""olap/rowset/alpha_rowset.h""
+#include ""olap/rowset/rowset_id_generator.h""
+
+namespace doris {
+
+OLAPStatus OlapSnapshotConverter::to_olap_header(const TabletMetaPB& tablet_meta_pb, OLAPHeaderMessage* olap_header) {
+    olap_header->set_num_rows_per_data_block(tablet_meta_pb.schema().num_rows_per_row_block());
+    olap_header->set_cumulative_layer_point(tablet_meta_pb.cumulative_layer_point());
+    olap_header->set_num_short_key_fields(tablet_meta_pb.schema().num_short_key_columns());
+
+    for (auto& column : tablet_meta_pb.schema().column()) {
+        ColumnMessage* column_msg = olap_header->add_column();
+        to_column_msg(column, column_msg);
+    }
+
+    olap_header->set_creation_time(tablet_meta_pb.creation_time());
+    olap_header->set_data_file_type(DataFileType::COLUMN_ORIENTED_FILE);
+    olap_header->set_next_column_unique_id(tablet_meta_pb.schema().next_column_unique_id());
+    olap_header->set_compress_kind(tablet_meta_pb.schema().compress_kind());
+   
+    olap_header->set_bf_fpp(tablet_meta_pb.schema().bf_fpp());
+    olap_header->set_keys_type(tablet_meta_pb.schema().keys_type());
+
+    for (auto& rs_meta : tablet_meta_pb.rs_metas()) {
+        PDelta* pdelta = olap_header->add_delta();
+        convert_to_pdelta(rs_meta, pdelta);
+    }
+    // not add pending delta, it is usedless in clone or backup restore
+    for (auto& inc_rs_meta : tablet_meta_pb.inc_rs_metas()) {
+        PDelta* pdelta = olap_header->add_incremental_delta();
+        convert_to_pdelta(inc_rs_meta, pdelta);
+    }
+
+    olap_header->set_in_restore_mode(tablet_meta_pb.in_restore_mode());
+    olap_header->set_tablet_id(tablet_meta_pb.tablet_id());
+    olap_header->set_schema_hash(tablet_meta_pb.schema_hash());
+    olap_header->set_shard(tablet_meta_pb.shard_id());
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus OlapSnapshotConverter::to_tablet_meta_pb(const OLAPHeaderMessage& olap_header, TabletMetaPB* tablet_meta_pb, 
+                                                  vector<RowsetMetaPB>* pending_rowsets, DataDir* data_dir) {
+    tablet_meta_pb->set_tablet_id(olap_header.tablet_id());
+    tablet_meta_pb->set_schema_hash(olap_header.schema_hash());
+    tablet_meta_pb->set_shard_id(olap_header.shard());
+    tablet_meta_pb->set_creation_time(olap_header.creation_time());
+    tablet_meta_pb->set_cumulative_layer_point(olap_header.cumulative_layer_point());
+
+    TabletSchemaPB* schema = tablet_meta_pb->mutable_schema();
+    for (auto& column_msg : olap_header.column()) {
+        ColumnPB* column_pb = schema->add_column();
+        to_column_pb(column_msg, column_pb);
+    }
+    schema->set_keys_type(olap_header.keys_type());
+    schema->set_num_short_key_columns(olap_header.num_short_key_fields());
+    schema->set_num_rows_per_row_block(olap_header.num_rows_per_data_block());
+    schema->set_compress_kind(olap_header.compress_kind());
+    schema->set_bf_fpp(olap_header.bf_fpp());
+    schema->set_next_column_unique_id(olap_header.next_column_unique_id());
+
+    for (auto& delta : olap_header.delta()) {
+        RowsetMetaPB* rowset_meta = tablet_meta_pb->add_rs_metas();
+        RowsetId next_id;
+        RETURN_NOT_OK(RowsetIdGenerator::instance()->get_next_id(data_dir, &next_id));
+        convert_to_rowset_meta(delta, rowset_meta, next_id, olap_header.tablet_id(), olap_header.schema_hash());
+    }
+
+    for (auto& inc_delta : olap_header.incremental_delta()) {
+        RowsetMetaPB* rowset_meta = tablet_meta_pb->add_inc_rs_metas();
+        RowsetId next_id;
+        RETURN_NOT_OK(RowsetIdGenerator::instance()->get_next_id(data_dir, &next_id));
+        convert_to_rowset_meta(inc_delta, rowset_meta, next_id, olap_header.tablet_id(), olap_header.schema_hash());
+    }
+
+    for (auto& pending_delta : olap_header.pending_delta()) {
+        RowsetMetaPB rowset_meta;
+        RowsetId next_id;
+        RETURN_NOT_OK(RowsetIdGenerator::instance()->get_next_id(data_dir, &next_id));
+        convert_to_rowset_meta(pending_delta, &rowset_meta, next_id, olap_header.tablet_id(), olap_header.schema_hash());
+        pending_rowsets->push_back(std::move(rowset_meta));
+    }
+    AlterTabletPB* alter_tablet_pb = tablet_meta_pb->mutable_alter_tablet_task();
+    to_alter_tablet_pb(olap_header.schema_change_status(), alter_tablet_pb);
+    tablet_meta_pb->set_in_restore_mode(olap_header.in_restore_mode());
+    tablet_meta_pb->set_tablet_state(TabletStatePB::PB_RUNNING);
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus OlapSnapshotConverter::convert_to_pdelta(const RowsetMetaPB& rowset_meta_pb, PDelta* delta) {
+    delta->set_start_version(rowset_meta_pb.start_version());
+    delta->set_end_version(rowset_meta_pb.end_version());
+    delta->set_version_hash(rowset_meta_pb.version_hash());
+    delta->set_creation_time(rowset_meta_pb.creation_time());
+    AlphaRowsetExtraMetaPB extra_meta_pb;
+    extra_meta_pb.ParseFromString(rowset_meta_pb.extra_properties());
+    
+    for (auto& segment_group : extra_meta_pb.segment_groups()) {
+        SegmentGroupPB* new_segment_group = delta->add_segment_group();
+        *new_segment_group = segment_group;
+    }
+    DeletePredicatePB* delete_condition = delta->mutable_delete_condition();
+    *delete_condition = rowset_meta_pb.delete_predicate();
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus OlapSnapshotConverter::convert_to_ppending_delta(const RowsetMetaPB& rowset_meta_pb, PPendingDelta* pending_delta) {
+    pending_delta->set_partition_id(rowset_meta_pb.partition_id());
+    pending_delta->set_transaction_id(rowset_meta_pb.txn_id());
+    pending_delta->set_creation_time(rowset_meta_pb.creation_time());
+
+    AlphaRowsetExtraMetaPB extra_meta_pb;
+    extra_meta_pb.ParseFromString(rowset_meta_pb.extra_properties());
+    for (auto& pending_segment_group : extra_meta_pb.pending_segment_groups()) {
+        PendingSegmentGroupPB* new_pending_segment_group = pending_delta->add_pending_segment_group();
+        *new_pending_segment_group = pending_segment_group;
+    }
+    DeletePredicatePB* delete_condition = pending_delta->mutable_delete_condition();
+    *delete_condition = rowset_meta_pb.delete_predicate();
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus OlapSnapshotConverter::convert_to_rowset_meta(const PDelta& delta, RowsetMetaPB* rowset_meta_pb, 
+        int64_t rowset_id, int64_t tablet_id, int32_t schema_hash) {
+    rowset_meta_pb->set_rowset_id(rowset_id);
+    rowset_meta_pb->set_tablet_id(tablet_id);
+    rowset_meta_pb->set_tablet_schema_hash(schema_hash);
+    rowset_meta_pb->set_rowset_type(RowsetTypePB::ALPHA_ROWSET);
+    rowset_meta_pb->set_rowset_state(RowsetStatePB::VISIBLE);
+    rowset_meta_pb->set_start_version(delta.start_version());
+    rowset_meta_pb->set_end_version(delta.end_version());
+    rowset_meta_pb->set_version_hash(delta.version_hash());
+    
+    bool empty = true;
+    int64_t num_rows = 0;
+    int64_t index_size = 0;
+    int64_t data_size = 0;
+    AlphaRowsetExtraMetaPB extra_meta_pb;
+    for (auto& segment_group : delta.segment_group()) {
+        SegmentGroupPB* new_segment_group = extra_meta_pb.add_segment_groups();
+        *new_segment_group = segment_group;
+        if (!segment_group.empty()) {
+            empty = false;
+        }
+        num_rows += segment_group.num_rows();
+        index_size += segment_group.index_size();
+        data_size += segment_group.data_size();
+    }
+    std::string extra_properties;
+    extra_meta_pb.SerializeToString(&extra_properties);
+    rowset_meta_pb->set_extra_properties(extra_properties);
+
+    rowset_meta_pb->set_empty(empty);
+    rowset_meta_pb->set_num_rows(num_rows);
+    rowset_meta_pb->set_data_disk_size(data_size);
+    rowset_meta_pb->set_index_disk_size(index_size);
+    rowset_meta_pb->set_total_disk_size(data_size + index_size);
+    DeletePredicatePB* delete_condition = rowset_meta_pb->mutable_delete_predicate();
+    *delete_condition = delta.delete_condition();
+    rowset_meta_pb->set_creation_time(delta.creation_time());
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus OlapSnapshotConverter::convert_to_rowset_meta(const PPendingDelta& pending_delta, RowsetMetaPB* rowset_meta_pb, 
+        int64_t rowset_id, int64_t tablet_id, int32_t schema_hash) {
+    rowset_meta_pb->set_rowset_id(rowset_id);
+    rowset_meta_pb->set_tablet_id(tablet_id);
+    rowset_meta_pb->set_tablet_schema_hash(schema_hash);
+    rowset_meta_pb->set_rowset_type(RowsetTypePB::ALPHA_ROWSET);
+    rowset_meta_pb->set_rowset_state(RowsetStatePB::COMMITTED);
+    rowset_meta_pb->set_partition_id(pending_delta.partition_id());
+    rowset_meta_pb->set_txn_id(pending_delta.transaction_id());
+    rowset_meta_pb->set_creation_time(pending_delta.creation_time());
+    
+    bool empty = true;
+    int64_t num_rows = 0;
+    int64_t index_size = 0;
+    int64_t data_size = 0;
+    AlphaRowsetExtraMetaPB extra_meta_pb;
+    for (auto& pending_segment_group : pending_delta.pending_segment_group()) {
+        PendingSegmentGroupPB* new_pending_segment_group = extra_meta_pb.add_pending_segment_groups();
+        *new_pending_segment_group = pending_segment_group;
+        if (!pending_segment_group.empty()) {
+            empty = false;
+        }
+        // num_rows += pending_segment_group.num_rows();
+        // index_size += pending_segment_group.index_size();
+        // data_size += pending_segment_group.data_size();
+    }
+    std::string extra_properties;
+    extra_meta_pb.SerializeToString(&extra_properties);
+    rowset_meta_pb->set_extra_properties(extra_properties);
+
+    rowset_meta_pb->set_empty(empty);
+    rowset_meta_pb->set_num_rows(num_rows);
+    rowset_meta_pb->set_data_disk_size(data_size);
+    rowset_meta_pb->set_index_disk_size(index_size);
+    rowset_meta_pb->set_total_disk_size(data_size + index_size);
+
+    DeletePredicatePB* delete_condition = rowset_meta_pb->mutable_delete_predicate();
+    *delete_condition = pending_delta.delete_condition();
+    rowset_meta_pb->set_creation_time(pending_delta.creation_time());
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus OlapSnapshotConverter::to_column_pb(const ColumnMessage& column_msg, ColumnPB* column_pb) {
+    column_pb->set_unique_id(column_msg.unique_id());
+    column_pb->set_name(column_msg.name());
+    column_pb->set_type(column_msg.type());
+    column_pb->set_is_key(column_msg.is_key());
+    column_pb->set_aggregation(column_msg.aggregation());
+    column_pb->set_is_nullable(column_msg.is_allow_null());
+    column_pb->set_default_value(column_msg.default_value());
+    column_pb->set_precision(column_msg.precision());
+    column_pb->set_frac(column_msg.frac());
+    column_pb->set_length(column_msg.length());
+    column_pb->set_index_length(column_msg.index_length());
+    column_pb->set_is_bf_column(column_msg.is_bf_column());
+    // TODO(ygl) calculate column id from column list
+    // column_pb->set_referenced_column_id(column_msg.());
+    column_pb->set_referenced_column(column_msg.referenced_column());
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus OlapSnapshotConverter::to_column_msg(const ColumnPB& column_pb, ColumnMessage* column_msg) {
+    column_msg->set_name(column_pb.name());
+    column_msg->set_type(column_pb.type());
+    column_msg->set_aggregation(column_pb.aggregation());
+    column_msg->set_length(column_pb.length());
+    column_msg->set_is_key(column_pb.is_key());
+    column_msg->set_default_value(column_pb.default_value());
+    column_msg->set_referenced_column(column_pb.referenced_column());
+    column_msg->set_index_length(column_pb.index_length());
+    column_msg->set_precision(column_pb.precision());
+    column_msg->set_frac(column_pb.frac());
+    column_msg->set_is_allow_null(column_pb.is_nullable());
+    column_msg->set_unique_id(column_pb.unique_id());
+    column_msg->set_is_bf_column(column_pb.is_bf_column());
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus OlapSnapshotConverter::to_alter_tablet_pb(const SchemaChangeStatusMessage& schema_change_msg, 
+                                                   AlterTabletPB* alter_tablet_pb) {
+    alter_tablet_pb->set_related_tablet_id(schema_change_msg.related_tablet_id());
+    alter_tablet_pb->set_related_schema_hash(schema_change_msg.related_schema_hash());
+    alter_tablet_pb->set_alter_type(static_cast<AlterTabletType>(schema_change_msg.schema_change_type()));
+    if (schema_change_msg.versions_to_changed().size() == 0) {
+        alter_tablet_pb->set_alter_state(AlterTabletState::ALTER_FINISHED);
+    } else {
+        alter_tablet_pb->set_alter_state(AlterTabletState::ALTER_FAILED);
+    }
+    return OLAP_SUCCESS;
+}
+
+// from olap header to tablet meta
+OLAPStatus OlapSnapshotConverter::to_new_snapshot(const OLAPHeaderMessage& olap_header, const string& old_data_path_prefix, 
+    TabletMetaPB* tablet_meta_pb, const string& new_data_path_prefix, DataDir& data_dir, vector<RowsetMetaPB>* pending_rowsets) {
+    RETURN_NOT_OK(to_tablet_meta_pb(olap_header, tablet_meta_pb, pending_rowsets, &data_dir));
+
+    TabletSchema tablet_schema;
+    RETURN_NOT_OK(tablet_schema.init_from_pb(tablet_meta_pb->schema()));
+
+    // convert visible pdelta file to rowsets
+    for (auto& visible_rowset : tablet_meta_pb->rs_metas()) {
+        RowsetMetaSharedPtr alpha_rowset_meta(new AlphaRowsetMeta());
+        alpha_rowset_meta->init_from_pb(visible_rowset);
+        AlphaRowset rowset(&tablet_schema, new_data_path_prefix, &data_dir, alpha_rowset_meta);
+        std::vector<std::string> success_files;
+        RETURN_NOT_OK(rowset.convert_from_old_files(old_data_path_prefix, &success_files));
+    }
+
+    // convert inc delta file to rowsets
+    for (auto& inc_rowset : tablet_meta_pb->inc_rs_metas()) {
+        RowsetMetaSharedPtr alpha_rowset_meta(new AlphaRowsetMeta());
+        alpha_rowset_meta->init_from_pb(inc_rowset);
+        AlphaRowset rowset(&tablet_schema, new_data_path_prefix, &data_dir, alpha_rowset_meta);
+        std::vector<std::string> success_files;
+        RETURN_NOT_OK(rowset.convert_from_old_files(old_data_path_prefix, &success_files));
+    }
+
+    // convert pending delta file to rowsets
+    for (auto& pending_rowset : *pending_rowsets) {
+        RowsetMetaSharedPtr alpha_rowset_meta(new AlphaRowsetMeta());
+        alpha_rowset_meta->init_from_pb(pending_rowset);
+        AlphaRowset rowset(&tablet_schema, new_data_path_prefix, &data_dir, alpha_rowset_meta);
+        std::vector<std::string> success_files;
+        std::string pending_delta_path = old_data_path_prefix + PENDING_DELTA_PREFIX;
+        RETURN_NOT_OK(rowset.convert_from_old_files(pending_delta_path, &success_files));
+    }
+    return OLAP_SUCCESS;
+}
+
+// from tablet meta to olap header
+OLAPStatus OlapSnapshotConverter::to_old_snapshot(const TabletMetaPB& tablet_meta_pb, string& new_data_path_prefix, 
+    OLAPHeaderMessage* olap_header, string& old_data_path_prefix) {","[{'comment': 'move olap_header to last', 'commenter': 'kangpinghuang'}]"
669,be/src/olap/rowset/alpha_rowset.cpp,"@@ -61,12 +61,6 @@ OLAPStatus AlphaRowset::copy(RowsetWriter* dest_rowset_writer) {
 }
 
 OLAPStatus AlphaRowset::remove() {
-    OlapMeta* meta = _data_dir->get_meta();
-    OLAPStatus status = RowsetMetaManager::remove(meta, rowset_id());
-    if (status != OLAP_SUCCESS) {
-        LOG(FATAL) << ""failed to remove meta of rowset_id:"" << rowset_id();
-        return status;
-    }","[{'comment': 'We should make sure that we should remove rowset meta first and then remove related segment groups', 'commenter': 'kangpinghuang'}]"
669,be/src/olap/rowset/alpha_rowset.cpp,"@@ -191,8 +173,8 @@ int64_t AlphaRowset::ref_count() const {
 }
 
 OLAPStatus AlphaRowset::make_snapshot(const std::string& snapshot_path,
-                                      std::vector<std::string>* success_files) {
-    for (auto segment_group : _segment_groups) {
+                                    std::vector<std::string>* success_files) {","[{'comment': 'extra empty, remove it', 'commenter': 'kangpinghuang'}]"
669,be/src/olap/rowset/alpha_rowset.h,"@@ -77,8 +77,14 @@ class AlphaRowset : public Rowset {
     
     int64_t ref_count() const override;
 
-    virtual OLAPStatus make_snapshot(const std::string& snapshot_path,
-                                     std::vector<std::string>* success_files);
+    OLAPStatus make_snapshot(const std::string& snapshot_path,
+                             std::vector<std::string>* success_files);","[{'comment': 'add override to the postfix', 'commenter': 'kangpinghuang'}]"
669,be/src/olap/rowset/rowset_meta.h,"@@ -239,11 +239,12 @@ class RowsetMeta {
     }
 
     std::string rowset_path() {
-        return _rowset_meta_pb.rowset_path();
+        // return _rowset_meta_pb.rowset_path();
+        return """";
     }
 
     void set_rowset_path(std::string rowset_path) {
-        _rowset_meta_pb.set_rowset_path(rowset_path);
+        // _rowset_meta_pb.set_rowset_path(rowset_path);","[{'comment': 'remove these two functions?', 'commenter': 'kangpinghuang'}]"
669,be/src/olap/rowset/rowset_meta_manager.cpp,"@@ -82,6 +82,12 @@ OLAPStatus RowsetMetaManager::save(OlapMeta* meta, int64_t rowset_id, RowsetMeta
     return status;
 }
 
+OLAPStatus RowsetMetaManager::save(OlapMeta* meta, int64_t rowset_id, string& meta_binary) {","[{'comment': 'const string& meta_binary', 'commenter': 'kangpinghuang'}]"
669,be/src/olap/rowset/rowset_meta_manager.h,"@@ -35,6 +37,8 @@ class RowsetMetaManager {
 
     static OLAPStatus save(OlapMeta* meta, int64_t rowset_id, RowsetMetaSharedPtr rowset_meta);
 
+    static OLAPStatus save(OlapMeta* meta, int64_t rowset_id, string& meta_binary);","[{'comment': 'const string& meta_binary', 'commenter': 'kangpinghuang'}]"
669,be/src/olap/rowset/segment_group.cpp,"@@ -730,6 +784,18 @@ OLAPStatus SegmentGroup::remove_old_files(std::vector<std::string>* links_to_rem
     return OLAP_SUCCESS;
 }
 
+OLAPStatus SegmentGroup::remove_new_files(std::vector<std::string>* links_to_remove) {
+    for (int segment_id = 0; segment_id < _num_segments; segment_id++) {
+        std::string data_file_name = construct_data_file_path(segment_id);
+        RETURN_NOT_OK(remove_dir(data_file_name));
+        links_to_remove->push_back(data_file_name);
+        std::string index_file_name = construct_index_file_path(segment_id);
+        RETURN_NOT_OK(remove_dir(index_file_name));
+        links_to_remove->push_back(index_file_name);","[{'comment': 'Is is necessary to print the file path deleted? I think we should add some log', 'commenter': 'kangpinghuang'}]"
669,be/src/olap/snapshot_manager.cpp,"@@ -208,84 +206,114 @@ OLAPStatus SnapshotManager::_create_snapshot_files(
         remove_all_dir(schema_full_path);
     }
     create_dirs(schema_full_path);
-
     path boost_path(snapshot_id_path);
     string snapshot_id = canonical(boost_path).string();
-
-    bool header_locked = false;
-    ref_tablet->obtain_header_rdlock();
-    header_locked = true;
-
-    TabletMeta* new_tablet_meta = nullptr;
     do {
-        // get latest version
-        const RowsetSharedPtr lastest_version = ref_tablet->rowset_with_max_version();
-        if (lastest_version == NULL) {
-            OLAP_LOG_WARNING(""tablet has not any version. [path='%s']"",
-                    ref_tablet->full_name().c_str());
-            res = OLAP_ERR_VERSION_NOT_EXIST;
+        DataDir* data_dir = ref_tablet->data_dir();
+        std::unique_ptr<TabletMeta> new_tablet_meta(new(nothrow) TabletMeta(data_dir));
+        if (new_tablet_meta.get() == NULL) {","[{'comment': 'new_tablet_meta == nullptr', 'commenter': 'kangpinghuang'}]"
669,be/src/olap/snapshot_manager.cpp,"@@ -208,84 +206,114 @@ OLAPStatus SnapshotManager::_create_snapshot_files(
         remove_all_dir(schema_full_path);
     }
     create_dirs(schema_full_path);
-
     path boost_path(snapshot_id_path);
     string snapshot_id = canonical(boost_path).string();
-
-    bool header_locked = false;
-    ref_tablet->obtain_header_rdlock();
-    header_locked = true;
-
-    TabletMeta* new_tablet_meta = nullptr;
     do {
-        // get latest version
-        const RowsetSharedPtr lastest_version = ref_tablet->rowset_with_max_version();
-        if (lastest_version == NULL) {
-            OLAP_LOG_WARNING(""tablet has not any version. [path='%s']"",
-                    ref_tablet->full_name().c_str());
-            res = OLAP_ERR_VERSION_NOT_EXIST;
+        DataDir* data_dir = ref_tablet->data_dir();
+        std::unique_ptr<TabletMeta> new_tablet_meta(new(nothrow) TabletMeta(data_dir));
+        if (new_tablet_meta.get() == NULL) {
+            OLAP_LOG_WARNING(""fail to malloc TabletMeta."");","[{'comment': 'LOG(WARNING)', 'commenter': 'kangpinghuang'}]"
669,be/src/olap/snapshot_manager.cpp,"@@ -208,84 +206,114 @@ OLAPStatus SnapshotManager::_create_snapshot_files(
         remove_all_dir(schema_full_path);
     }
     create_dirs(schema_full_path);
-
     path boost_path(snapshot_id_path);
     string snapshot_id = canonical(boost_path).string();
-
-    bool header_locked = false;
-    ref_tablet->obtain_header_rdlock();
-    header_locked = true;
-
-    TabletMeta* new_tablet_meta = nullptr;
     do {
-        // get latest version
-        const RowsetSharedPtr lastest_version = ref_tablet->rowset_with_max_version();
-        if (lastest_version == NULL) {
-            OLAP_LOG_WARNING(""tablet has not any version. [path='%s']"",
-                    ref_tablet->full_name().c_str());
-            res = OLAP_ERR_VERSION_NOT_EXIST;
+        DataDir* data_dir = ref_tablet->data_dir();
+        std::unique_ptr<TabletMeta> new_tablet_meta(new(nothrow) TabletMeta(data_dir));
+        if (new_tablet_meta.get() == NULL) {
+            OLAP_LOG_WARNING(""fail to malloc TabletMeta."");
+            res = OLAP_ERR_MALLOC_ERROR;
             break;
         }
-
-        // get snapshot version, use request.version if specified
-        int32_t version = lastest_version->end_version();
-        if (request.__isset.version) {
-            if (lastest_version->end_version() < request.version
-                    || (lastest_version->start_version() == lastest_version->end_version()
-                    && lastest_version->end_version() == request.version
-                    && lastest_version->version_hash() != request.version_hash)) {
-                OLAP_LOG_WARNING(""invalid make snapshot request. ""
-                        ""[version=%d version_hash=%ld req_version=%d req_version_hash=%ld]"",
-                        lastest_version->end_version(), lastest_version->version_hash(),
-                        request.version, request.version_hash);
-                res = OLAP_ERR_INPUT_PARAMETER_ERROR;
+        vector<RowsetSharedPtr> consistent_rowsets;
+        if (request.__isset.missing_version) {
+            ReadLock rdlock(ref_tablet->get_header_lock_ptr());
+            for (int64_t missed_version : request.missing_version) {
+                Version version = { missed_version, missed_version };
+                const RowsetSharedPtr rowset = ref_tablet->get_rowset_by_version(version);
+                if (rowset != nullptr) {
+                    consistent_rowsets.push_back(rowset);
+                } else {
+                    LOG(WARNING) << ""failed to find missed version when snapshot. ""
+                                << ""tablet="" << request.tablet_id
+                                << "", schema_hash="" << request.schema_hash
+                                << "", version="" << version.first << ""-"" << version.second;
+                    res = OLAP_ERR_VERSION_NOT_EXIST;
+                    break;
+                }
+            }
+            res = TabletMetaManager::get_header(data_dir, ref_tablet->tablet_id(), ref_tablet->schema_hash(), new_tablet_meta.get());","[{'comment': 'too long', 'commenter': 'kangpinghuang'}]"
669,be/src/olap/snapshot_manager.cpp,"@@ -208,84 +206,114 @@ OLAPStatus SnapshotManager::_create_snapshot_files(
         remove_all_dir(schema_full_path);
     }
     create_dirs(schema_full_path);
-
     path boost_path(snapshot_id_path);
     string snapshot_id = canonical(boost_path).string();
-
-    bool header_locked = false;
-    ref_tablet->obtain_header_rdlock();
-    header_locked = true;
-
-    TabletMeta* new_tablet_meta = nullptr;
     do {
-        // get latest version
-        const RowsetSharedPtr lastest_version = ref_tablet->rowset_with_max_version();
-        if (lastest_version == NULL) {
-            OLAP_LOG_WARNING(""tablet has not any version. [path='%s']"",
-                    ref_tablet->full_name().c_str());
-            res = OLAP_ERR_VERSION_NOT_EXIST;
+        DataDir* data_dir = ref_tablet->data_dir();
+        std::unique_ptr<TabletMeta> new_tablet_meta(new(nothrow) TabletMeta(data_dir));
+        if (new_tablet_meta.get() == NULL) {
+            OLAP_LOG_WARNING(""fail to malloc TabletMeta."");
+            res = OLAP_ERR_MALLOC_ERROR;
             break;
         }
-
-        // get snapshot version, use request.version if specified
-        int32_t version = lastest_version->end_version();
-        if (request.__isset.version) {
-            if (lastest_version->end_version() < request.version
-                    || (lastest_version->start_version() == lastest_version->end_version()
-                    && lastest_version->end_version() == request.version
-                    && lastest_version->version_hash() != request.version_hash)) {
-                OLAP_LOG_WARNING(""invalid make snapshot request. ""
-                        ""[version=%d version_hash=%ld req_version=%d req_version_hash=%ld]"",
-                        lastest_version->end_version(), lastest_version->version_hash(),
-                        request.version, request.version_hash);
-                res = OLAP_ERR_INPUT_PARAMETER_ERROR;
+        vector<RowsetSharedPtr> consistent_rowsets;
+        if (request.__isset.missing_version) {
+            ReadLock rdlock(ref_tablet->get_header_lock_ptr());
+            for (int64_t missed_version : request.missing_version) {
+                Version version = { missed_version, missed_version };
+                const RowsetSharedPtr rowset = ref_tablet->get_rowset_by_version(version);
+                if (rowset != nullptr) {
+                    consistent_rowsets.push_back(rowset);
+                } else {
+                    LOG(WARNING) << ""failed to find missed version when snapshot. ""
+                                << ""tablet="" << request.tablet_id
+                                << "", schema_hash="" << request.schema_hash
+                                << "", version="" << version.first << ""-"" << version.second;
+                    res = OLAP_ERR_VERSION_NOT_EXIST;
+                    break;
+                }
+            }
+            res = TabletMetaManager::get_header(data_dir, ref_tablet->tablet_id(), ref_tablet->schema_hash(), new_tablet_meta.get());
+            if (res != OLAP_SUCCESS) {
+                LOG(WARNING) << ""fail to load header. res="" << res
+                        << "" tablet_id="" << ref_tablet->tablet_id() 
+                        << "" , schema_hash="" << ref_tablet->schema_hash();
+                break;
+            }
+        } else {
+            ReadLock rdlock(ref_tablet->get_header_lock_ptr());
+            // get latest version
+            const RowsetSharedPtr lastest_version = ref_tablet->rowset_with_max_version();
+            if (lastest_version == NULL) {","[{'comment': '== nullptr', 'commenter': 'kangpinghuang'}]"
669,be/src/olap/snapshot_manager.cpp,"@@ -208,84 +206,114 @@ OLAPStatus SnapshotManager::_create_snapshot_files(
         remove_all_dir(schema_full_path);
     }
     create_dirs(schema_full_path);
-
     path boost_path(snapshot_id_path);
     string snapshot_id = canonical(boost_path).string();
-
-    bool header_locked = false;
-    ref_tablet->obtain_header_rdlock();
-    header_locked = true;
-
-    TabletMeta* new_tablet_meta = nullptr;
     do {
-        // get latest version
-        const RowsetSharedPtr lastest_version = ref_tablet->rowset_with_max_version();
-        if (lastest_version == NULL) {
-            OLAP_LOG_WARNING(""tablet has not any version. [path='%s']"",
-                    ref_tablet->full_name().c_str());
-            res = OLAP_ERR_VERSION_NOT_EXIST;
+        DataDir* data_dir = ref_tablet->data_dir();
+        std::unique_ptr<TabletMeta> new_tablet_meta(new(nothrow) TabletMeta(data_dir));
+        if (new_tablet_meta.get() == NULL) {
+            OLAP_LOG_WARNING(""fail to malloc TabletMeta."");
+            res = OLAP_ERR_MALLOC_ERROR;
             break;
         }
-
-        // get snapshot version, use request.version if specified
-        int32_t version = lastest_version->end_version();
-        if (request.__isset.version) {
-            if (lastest_version->end_version() < request.version
-                    || (lastest_version->start_version() == lastest_version->end_version()
-                    && lastest_version->end_version() == request.version
-                    && lastest_version->version_hash() != request.version_hash)) {
-                OLAP_LOG_WARNING(""invalid make snapshot request. ""
-                        ""[version=%d version_hash=%ld req_version=%d req_version_hash=%ld]"",
-                        lastest_version->end_version(), lastest_version->version_hash(),
-                        request.version, request.version_hash);
-                res = OLAP_ERR_INPUT_PARAMETER_ERROR;
+        vector<RowsetSharedPtr> consistent_rowsets;
+        if (request.__isset.missing_version) {
+            ReadLock rdlock(ref_tablet->get_header_lock_ptr());
+            for (int64_t missed_version : request.missing_version) {
+                Version version = { missed_version, missed_version };
+                const RowsetSharedPtr rowset = ref_tablet->get_rowset_by_version(version);
+                if (rowset != nullptr) {
+                    consistent_rowsets.push_back(rowset);
+                } else {
+                    LOG(WARNING) << ""failed to find missed version when snapshot. ""
+                                << ""tablet="" << request.tablet_id
+                                << "", schema_hash="" << request.schema_hash
+                                << "", version="" << version.first << ""-"" << version.second;
+                    res = OLAP_ERR_VERSION_NOT_EXIST;
+                    break;
+                }
+            }
+            res = TabletMetaManager::get_header(data_dir, ref_tablet->tablet_id(), ref_tablet->schema_hash(), new_tablet_meta.get());
+            if (res != OLAP_SUCCESS) {
+                LOG(WARNING) << ""fail to load header. res="" << res
+                        << "" tablet_id="" << ref_tablet->tablet_id() 
+                        << "" , schema_hash="" << ref_tablet->schema_hash();
+                break;
+            }
+        } else {
+            ReadLock rdlock(ref_tablet->get_header_lock_ptr());
+            // get latest version
+            const RowsetSharedPtr lastest_version = ref_tablet->rowset_with_max_version();
+            if (lastest_version == NULL) {
+                OLAP_LOG_WARNING(""tablet has not any version. [path='%s']"",","[{'comment': 'LOG(WARNING)', 'commenter': 'kangpinghuang'}]"
669,be/src/olap/snapshot_manager.cpp,"@@ -208,84 +206,114 @@ OLAPStatus SnapshotManager::_create_snapshot_files(
         remove_all_dir(schema_full_path);
     }
     create_dirs(schema_full_path);
-
     path boost_path(snapshot_id_path);
     string snapshot_id = canonical(boost_path).string();
-
-    bool header_locked = false;
-    ref_tablet->obtain_header_rdlock();
-    header_locked = true;
-
-    TabletMeta* new_tablet_meta = nullptr;
     do {
-        // get latest version
-        const RowsetSharedPtr lastest_version = ref_tablet->rowset_with_max_version();
-        if (lastest_version == NULL) {
-            OLAP_LOG_WARNING(""tablet has not any version. [path='%s']"",
-                    ref_tablet->full_name().c_str());
-            res = OLAP_ERR_VERSION_NOT_EXIST;
+        DataDir* data_dir = ref_tablet->data_dir();
+        std::unique_ptr<TabletMeta> new_tablet_meta(new(nothrow) TabletMeta(data_dir));
+        if (new_tablet_meta.get() == NULL) {
+            OLAP_LOG_WARNING(""fail to malloc TabletMeta."");
+            res = OLAP_ERR_MALLOC_ERROR;
             break;
         }
-
-        // get snapshot version, use request.version if specified
-        int32_t version = lastest_version->end_version();
-        if (request.__isset.version) {
-            if (lastest_version->end_version() < request.version
-                    || (lastest_version->start_version() == lastest_version->end_version()
-                    && lastest_version->end_version() == request.version
-                    && lastest_version->version_hash() != request.version_hash)) {
-                OLAP_LOG_WARNING(""invalid make snapshot request. ""
-                        ""[version=%d version_hash=%ld req_version=%d req_version_hash=%ld]"",
-                        lastest_version->end_version(), lastest_version->version_hash(),
-                        request.version, request.version_hash);
-                res = OLAP_ERR_INPUT_PARAMETER_ERROR;
+        vector<RowsetSharedPtr> consistent_rowsets;
+        if (request.__isset.missing_version) {
+            ReadLock rdlock(ref_tablet->get_header_lock_ptr());
+            for (int64_t missed_version : request.missing_version) {
+                Version version = { missed_version, missed_version };
+                const RowsetSharedPtr rowset = ref_tablet->get_rowset_by_version(version);
+                if (rowset != nullptr) {
+                    consistent_rowsets.push_back(rowset);
+                } else {
+                    LOG(WARNING) << ""failed to find missed version when snapshot. ""
+                                << ""tablet="" << request.tablet_id
+                                << "", schema_hash="" << request.schema_hash
+                                << "", version="" << version.first << ""-"" << version.second;
+                    res = OLAP_ERR_VERSION_NOT_EXIST;
+                    break;
+                }
+            }
+            res = TabletMetaManager::get_header(data_dir, ref_tablet->tablet_id(), ref_tablet->schema_hash(), new_tablet_meta.get());
+            if (res != OLAP_SUCCESS) {
+                LOG(WARNING) << ""fail to load header. res="" << res
+                        << "" tablet_id="" << ref_tablet->tablet_id() 
+                        << "" , schema_hash="" << ref_tablet->schema_hash();
+                break;
+            }
+        } else {
+            ReadLock rdlock(ref_tablet->get_header_lock_ptr());
+            // get latest version
+            const RowsetSharedPtr lastest_version = ref_tablet->rowset_with_max_version();
+            if (lastest_version == NULL) {
+                OLAP_LOG_WARNING(""tablet has not any version. [path='%s']"",
+                        ref_tablet->full_name().c_str());
+                res = OLAP_ERR_VERSION_NOT_EXIST;
+                break;
+            }
+            // get snapshot version, use request.version if specified
+            int32_t version = lastest_version->end_version();
+            if (request.__isset.version) {
+                if (lastest_version->end_version() < request.version
+                        || (lastest_version->start_version() == lastest_version->end_version()
+                        && lastest_version->end_version() == request.version
+                        && lastest_version->version_hash() != request.version_hash)) {
+                    OLAP_LOG_WARNING(""invalid make snapshot request. ""","[{'comment': 'LOG(WARNING)', 'commenter': 'kangpinghuang'}]"
669,be/src/olap/snapshot_manager.cpp,"@@ -208,84 +206,114 @@ OLAPStatus SnapshotManager::_create_snapshot_files(
         remove_all_dir(schema_full_path);
     }
     create_dirs(schema_full_path);
-
     path boost_path(snapshot_id_path);
     string snapshot_id = canonical(boost_path).string();
-
-    bool header_locked = false;
-    ref_tablet->obtain_header_rdlock();
-    header_locked = true;
-
-    TabletMeta* new_tablet_meta = nullptr;
     do {
-        // get latest version
-        const RowsetSharedPtr lastest_version = ref_tablet->rowset_with_max_version();
-        if (lastest_version == NULL) {
-            OLAP_LOG_WARNING(""tablet has not any version. [path='%s']"",
-                    ref_tablet->full_name().c_str());
-            res = OLAP_ERR_VERSION_NOT_EXIST;
+        DataDir* data_dir = ref_tablet->data_dir();
+        std::unique_ptr<TabletMeta> new_tablet_meta(new(nothrow) TabletMeta(data_dir));
+        if (new_tablet_meta.get() == NULL) {
+            OLAP_LOG_WARNING(""fail to malloc TabletMeta."");
+            res = OLAP_ERR_MALLOC_ERROR;
             break;
         }
-
-        // get snapshot version, use request.version if specified
-        int32_t version = lastest_version->end_version();
-        if (request.__isset.version) {
-            if (lastest_version->end_version() < request.version
-                    || (lastest_version->start_version() == lastest_version->end_version()
-                    && lastest_version->end_version() == request.version
-                    && lastest_version->version_hash() != request.version_hash)) {
-                OLAP_LOG_WARNING(""invalid make snapshot request. ""
-                        ""[version=%d version_hash=%ld req_version=%d req_version_hash=%ld]"",
-                        lastest_version->end_version(), lastest_version->version_hash(),
-                        request.version, request.version_hash);
-                res = OLAP_ERR_INPUT_PARAMETER_ERROR;
+        vector<RowsetSharedPtr> consistent_rowsets;
+        if (request.__isset.missing_version) {
+            ReadLock rdlock(ref_tablet->get_header_lock_ptr());
+            for (int64_t missed_version : request.missing_version) {
+                Version version = { missed_version, missed_version };
+                const RowsetSharedPtr rowset = ref_tablet->get_rowset_by_version(version);
+                if (rowset != nullptr) {
+                    consistent_rowsets.push_back(rowset);
+                } else {
+                    LOG(WARNING) << ""failed to find missed version when snapshot. ""
+                                << ""tablet="" << request.tablet_id
+                                << "", schema_hash="" << request.schema_hash
+                                << "", version="" << version.first << ""-"" << version.second;
+                    res = OLAP_ERR_VERSION_NOT_EXIST;
+                    break;
+                }
+            }
+            res = TabletMetaManager::get_header(data_dir, ref_tablet->tablet_id(), ref_tablet->schema_hash(), new_tablet_meta.get());
+            if (res != OLAP_SUCCESS) {
+                LOG(WARNING) << ""fail to load header. res="" << res
+                        << "" tablet_id="" << ref_tablet->tablet_id() 
+                        << "" , schema_hash="" << ref_tablet->schema_hash();
+                break;
+            }
+        } else {
+            ReadLock rdlock(ref_tablet->get_header_lock_ptr());
+            // get latest version
+            const RowsetSharedPtr lastest_version = ref_tablet->rowset_with_max_version();
+            if (lastest_version == NULL) {
+                OLAP_LOG_WARNING(""tablet has not any version. [path='%s']"",
+                        ref_tablet->full_name().c_str());
+                res = OLAP_ERR_VERSION_NOT_EXIST;
+                break;
+            }
+            // get snapshot version, use request.version if specified
+            int32_t version = lastest_version->end_version();
+            if (request.__isset.version) {
+                if (lastest_version->end_version() < request.version
+                        || (lastest_version->start_version() == lastest_version->end_version()
+                        && lastest_version->end_version() == request.version
+                        && lastest_version->version_hash() != request.version_hash)) {
+                    OLAP_LOG_WARNING(""invalid make snapshot request. ""
+                            ""[version=%d version_hash=%ld req_version=%d req_version_hash=%ld]"",
+                            lastest_version->end_version(), lastest_version->version_hash(),
+                            request.version, request.version_hash);
+                    res = OLAP_ERR_INPUT_PARAMETER_ERROR;
+                    break;
+                }
+                version = request.version;
+            }
+            // get shortest version path
+            res = ref_tablet->capture_consistent_rowsets(Version(0, version), &consistent_rowsets);
+            if (res != OLAP_SUCCESS) {
+                OLAP_LOG_WARNING(""fail to select versions to span. [res=%d]"", res);","[{'comment': 'LOG(WARNING)', 'commenter': 'kangpinghuang'}]"
669,be/src/olap/snapshot_manager.cpp,"@@ -208,84 +206,114 @@ OLAPStatus SnapshotManager::_create_snapshot_files(
         remove_all_dir(schema_full_path);
     }
     create_dirs(schema_full_path);
-
     path boost_path(snapshot_id_path);
     string snapshot_id = canonical(boost_path).string();
-
-    bool header_locked = false;
-    ref_tablet->obtain_header_rdlock();
-    header_locked = true;
-
-    TabletMeta* new_tablet_meta = nullptr;
     do {
-        // get latest version
-        const RowsetSharedPtr lastest_version = ref_tablet->rowset_with_max_version();
-        if (lastest_version == NULL) {
-            OLAP_LOG_WARNING(""tablet has not any version. [path='%s']"",
-                    ref_tablet->full_name().c_str());
-            res = OLAP_ERR_VERSION_NOT_EXIST;
+        DataDir* data_dir = ref_tablet->data_dir();
+        std::unique_ptr<TabletMeta> new_tablet_meta(new(nothrow) TabletMeta(data_dir));
+        if (new_tablet_meta.get() == NULL) {
+            OLAP_LOG_WARNING(""fail to malloc TabletMeta."");
+            res = OLAP_ERR_MALLOC_ERROR;
             break;
         }
-
-        // get snapshot version, use request.version if specified
-        int32_t version = lastest_version->end_version();
-        if (request.__isset.version) {
-            if (lastest_version->end_version() < request.version
-                    || (lastest_version->start_version() == lastest_version->end_version()
-                    && lastest_version->end_version() == request.version
-                    && lastest_version->version_hash() != request.version_hash)) {
-                OLAP_LOG_WARNING(""invalid make snapshot request. ""
-                        ""[version=%d version_hash=%ld req_version=%d req_version_hash=%ld]"",
-                        lastest_version->end_version(), lastest_version->version_hash(),
-                        request.version, request.version_hash);
-                res = OLAP_ERR_INPUT_PARAMETER_ERROR;
+        vector<RowsetSharedPtr> consistent_rowsets;
+        if (request.__isset.missing_version) {
+            ReadLock rdlock(ref_tablet->get_header_lock_ptr());
+            for (int64_t missed_version : request.missing_version) {
+                Version version = { missed_version, missed_version };
+                const RowsetSharedPtr rowset = ref_tablet->get_rowset_by_version(version);
+                if (rowset != nullptr) {
+                    consistent_rowsets.push_back(rowset);
+                } else {
+                    LOG(WARNING) << ""failed to find missed version when snapshot. ""
+                                << ""tablet="" << request.tablet_id
+                                << "", schema_hash="" << request.schema_hash
+                                << "", version="" << version.first << ""-"" << version.second;
+                    res = OLAP_ERR_VERSION_NOT_EXIST;
+                    break;
+                }
+            }
+            res = TabletMetaManager::get_header(data_dir, ref_tablet->tablet_id(), ref_tablet->schema_hash(), new_tablet_meta.get());
+            if (res != OLAP_SUCCESS) {
+                LOG(WARNING) << ""fail to load header. res="" << res
+                        << "" tablet_id="" << ref_tablet->tablet_id() 
+                        << "" , schema_hash="" << ref_tablet->schema_hash();
+                break;
+            }
+        } else {
+            ReadLock rdlock(ref_tablet->get_header_lock_ptr());
+            // get latest version
+            const RowsetSharedPtr lastest_version = ref_tablet->rowset_with_max_version();
+            if (lastest_version == NULL) {
+                OLAP_LOG_WARNING(""tablet has not any version. [path='%s']"",
+                        ref_tablet->full_name().c_str());
+                res = OLAP_ERR_VERSION_NOT_EXIST;
+                break;
+            }
+            // get snapshot version, use request.version if specified
+            int32_t version = lastest_version->end_version();
+            if (request.__isset.version) {
+                if (lastest_version->end_version() < request.version
+                        || (lastest_version->start_version() == lastest_version->end_version()
+                        && lastest_version->end_version() == request.version
+                        && lastest_version->version_hash() != request.version_hash)) {
+                    OLAP_LOG_WARNING(""invalid make snapshot request. ""
+                            ""[version=%d version_hash=%ld req_version=%d req_version_hash=%ld]"",
+                            lastest_version->end_version(), lastest_version->version_hash(),
+                            request.version, request.version_hash);
+                    res = OLAP_ERR_INPUT_PARAMETER_ERROR;
+                    break;
+                }
+                version = request.version;
+            }
+            // get shortest version path
+            res = ref_tablet->capture_consistent_rowsets(Version(0, version), &consistent_rowsets);
+            if (res != OLAP_SUCCESS) {
+                OLAP_LOG_WARNING(""fail to select versions to span. [res=%d]"", res);
                 break;
             }
 
-            version = request.version;
-        }
-
-        // get shortest version path
-        vector<RowsetSharedPtr> consistent_rowsets;
-        res = ref_tablet->capture_consistent_rowsets(Version(0, version), &consistent_rowsets);
-        if (res != OLAP_SUCCESS) {
-            OLAP_LOG_WARNING(""fail to select versions to span. [res=%d]"", res);
-            break;
+            res = TabletMetaManager::get_header(data_dir, ref_tablet->tablet_id(), ref_tablet->schema_hash(), new_tablet_meta.get());","[{'comment': 'too long', 'commenter': 'kangpinghuang'}]"
669,be/src/olap/snapshot_manager.cpp,"@@ -208,84 +206,114 @@ OLAPStatus SnapshotManager::_create_snapshot_files(
         remove_all_dir(schema_full_path);
     }
     create_dirs(schema_full_path);
-
     path boost_path(snapshot_id_path);
     string snapshot_id = canonical(boost_path).string();
-
-    bool header_locked = false;
-    ref_tablet->obtain_header_rdlock();
-    header_locked = true;
-
-    TabletMeta* new_tablet_meta = nullptr;
     do {
-        // get latest version
-        const RowsetSharedPtr lastest_version = ref_tablet->rowset_with_max_version();
-        if (lastest_version == NULL) {
-            OLAP_LOG_WARNING(""tablet has not any version. [path='%s']"",
-                    ref_tablet->full_name().c_str());
-            res = OLAP_ERR_VERSION_NOT_EXIST;
+        DataDir* data_dir = ref_tablet->data_dir();
+        std::unique_ptr<TabletMeta> new_tablet_meta(new(nothrow) TabletMeta(data_dir));
+        if (new_tablet_meta.get() == NULL) {
+            OLAP_LOG_WARNING(""fail to malloc TabletMeta."");
+            res = OLAP_ERR_MALLOC_ERROR;
             break;
         }
-
-        // get snapshot version, use request.version if specified
-        int32_t version = lastest_version->end_version();
-        if (request.__isset.version) {
-            if (lastest_version->end_version() < request.version
-                    || (lastest_version->start_version() == lastest_version->end_version()
-                    && lastest_version->end_version() == request.version
-                    && lastest_version->version_hash() != request.version_hash)) {
-                OLAP_LOG_WARNING(""invalid make snapshot request. ""
-                        ""[version=%d version_hash=%ld req_version=%d req_version_hash=%ld]"",
-                        lastest_version->end_version(), lastest_version->version_hash(),
-                        request.version, request.version_hash);
-                res = OLAP_ERR_INPUT_PARAMETER_ERROR;
+        vector<RowsetSharedPtr> consistent_rowsets;
+        if (request.__isset.missing_version) {
+            ReadLock rdlock(ref_tablet->get_header_lock_ptr());
+            for (int64_t missed_version : request.missing_version) {
+                Version version = { missed_version, missed_version };
+                const RowsetSharedPtr rowset = ref_tablet->get_rowset_by_version(version);
+                if (rowset != nullptr) {
+                    consistent_rowsets.push_back(rowset);
+                } else {
+                    LOG(WARNING) << ""failed to find missed version when snapshot. ""
+                                << ""tablet="" << request.tablet_id
+                                << "", schema_hash="" << request.schema_hash
+                                << "", version="" << version.first << ""-"" << version.second;
+                    res = OLAP_ERR_VERSION_NOT_EXIST;
+                    break;
+                }
+            }
+            res = TabletMetaManager::get_header(data_dir, ref_tablet->tablet_id(), ref_tablet->schema_hash(), new_tablet_meta.get());
+            if (res != OLAP_SUCCESS) {
+                LOG(WARNING) << ""fail to load header. res="" << res
+                        << "" tablet_id="" << ref_tablet->tablet_id() 
+                        << "" , schema_hash="" << ref_tablet->schema_hash();
+                break;
+            }
+        } else {
+            ReadLock rdlock(ref_tablet->get_header_lock_ptr());
+            // get latest version
+            const RowsetSharedPtr lastest_version = ref_tablet->rowset_with_max_version();
+            if (lastest_version == NULL) {
+                OLAP_LOG_WARNING(""tablet has not any version. [path='%s']"",
+                        ref_tablet->full_name().c_str());
+                res = OLAP_ERR_VERSION_NOT_EXIST;
+                break;
+            }
+            // get snapshot version, use request.version if specified
+            int32_t version = lastest_version->end_version();
+            if (request.__isset.version) {
+                if (lastest_version->end_version() < request.version
+                        || (lastest_version->start_version() == lastest_version->end_version()
+                        && lastest_version->end_version() == request.version
+                        && lastest_version->version_hash() != request.version_hash)) {
+                    OLAP_LOG_WARNING(""invalid make snapshot request. ""
+                            ""[version=%d version_hash=%ld req_version=%d req_version_hash=%ld]"",
+                            lastest_version->end_version(), lastest_version->version_hash(),
+                            request.version, request.version_hash);
+                    res = OLAP_ERR_INPUT_PARAMETER_ERROR;
+                    break;
+                }
+                version = request.version;
+            }
+            // get shortest version path
+            res = ref_tablet->capture_consistent_rowsets(Version(0, version), &consistent_rowsets);
+            if (res != OLAP_SUCCESS) {
+                OLAP_LOG_WARNING(""fail to select versions to span. [res=%d]"", res);
                 break;
             }
 
-            version = request.version;
-        }
-
-        // get shortest version path
-        vector<RowsetSharedPtr> consistent_rowsets;
-        res = ref_tablet->capture_consistent_rowsets(Version(0, version), &consistent_rowsets);
-        if (res != OLAP_SUCCESS) {
-            OLAP_LOG_WARNING(""fail to select versions to span. [res=%d]"", res);
-            break;
+            res = TabletMetaManager::get_header(data_dir, ref_tablet->tablet_id(), ref_tablet->schema_hash(), new_tablet_meta.get());
+            if (res != OLAP_SUCCESS) {
+                LOG(WARNING) << ""fail to load header. res="" << res
+                        << ""tablet_id="" << ref_tablet->tablet_id() << "", schema_hash="" << ref_tablet->schema_hash();
+                break;
+            }
         }
 
-        DataDir* data_dir = ref_tablet->data_dir();
-        new_tablet_meta = new(nothrow) TabletMeta(data_dir);
-        if (new_tablet_meta == NULL) {
-            OLAP_LOG_WARNING(""fail to malloc TabletMeta."");
-            res = OLAP_ERR_MALLOC_ERROR;
-            break;
+        vector<RowsetMetaSharedPtr> rs_metas;
+        for (auto& rs : consistent_rowsets) {
+            std::vector<std::string> success_files;
+            res = rs->make_snapshot(schema_full_path, &success_files);
+            if (res != OLAP_SUCCESS) { break; }
+            rs_metas.push_back(rs->rowset_meta());
         }
-
-        res = TabletMetaManager::get_header(data_dir, ref_tablet->tablet_id(), ref_tablet->schema_hash(), new_tablet_meta);
         if (res != OLAP_SUCCESS) {
-            LOG(WARNING) << ""fail to load header. res="" << res
-                    << ""tablet_id="" << ref_tablet->tablet_id() << "", schema_hash="" << ref_tablet->schema_hash();
+            LOG(WARNING) << ""fail to create hard link. [path="" << snapshot_id_path << ""]"";
             break;
         }
 
-        ref_tablet->release_header_lock();
-        header_locked = false;
-        update_header_file_info(consistent_rowsets, new_tablet_meta);
-
-        // save new header to snapshot header path
-        res = new_tablet_meta->save(header_path);
-        if (res != OLAP_SUCCESS) {
-            OLAP_LOG_WARNING(""fail to save header. [res=%d tablet_id=%ld, schema_hash=%d, headerpath=%s]"",
-                    res, ref_tablet->tablet_id(), ref_tablet->schema_hash(), header_path.c_str());
-            break;
+        if (request.__isset.missing_version) {
+            new_tablet_meta->revise_inc_rs_metas(rs_metas);
+        } else {
+            new_tablet_meta->revise_rs_metas(rs_metas);
+        }
+        if (snapshot_version < PREFERRED_SNAPSHOT_VERSION) {
+            OlapSnapshotConverter converter;
+            TabletMetaPB tablet_meta_pb;
+            OLAPHeaderMessage olap_header_msg;
+            new_tablet_meta->to_meta_pb(&tablet_meta_pb);
+            converter.to_old_snapshot(tablet_meta_pb, schema_full_path, &olap_header_msg, schema_full_path);
+            // save new header to snapshot header path
+            res = converter.save(header_path, olap_header_msg);
+        } else {
+            res = new_tablet_meta->save(header_path);
         }
 
-        res = _link_index_and_data_files(schema_full_path, ref_tablet, consistent_rowsets);
         if (res != OLAP_SUCCESS) {
-            LOG(WARNING) << ""fail to create hard link. [path="" << snapshot_id_path << ""]"";
             break;
         }
-
+        ","[{'comment': 'extra empty', 'commenter': 'kangpinghuang'}]"
669,be/src/olap/storage_engine.cpp,"@@ -117,8 +119,179 @@ StorageEngine::~StorageEngine() {
     clear();
 }
 
+// convert old tablet and its files to new tablet meta and rowset format
+OLAPStatus StorageEngine::_convert_old_tablet(DataDir* data_dir) {
+    auto convert_tablet_func = [this, data_dir](long tablet_id,
+        long schema_hash, const std::string& value) -> bool {
+        OlapSnapshotConverter converter;
+        // convert olap header and files
+        OLAPHeaderMessage olap_header_msg;
+        TabletMetaPB tablet_meta_pb;
+        vector<RowsetMetaPB> pending_rowsets;
+        bool parsed = olap_header_msg.ParseFromString(value);
+        if (!parsed) {
+            LOG(WARNING) << ""convert olap header to tablet meta failed when load olap header tablet="" 
+                         << tablet_id << ""."" << schema_hash;
+            return false;
+        }
+        string old_data_path_prefix = data_dir->get_absolute_tablet_path(olap_header_msg, true);
+        OLAPStatus status = converter.to_new_snapshot(olap_header_msg, old_data_path_prefix, 
+            &tablet_meta_pb, old_data_path_prefix, *data_dir, &pending_rowsets);
+        if (status != OLAP_SUCCESS) {
+            LOG(WARNING) << ""convert olap header to tablet meta failed when convert header and files tablet="" 
+                         << tablet_id << ""."" << schema_hash;
+            return false;
+        }
+
+        // write pending rowset to olap meta
+        for (auto& rowset_pb : pending_rowsets) {
+            string meta_binary;
+            rowset_pb.SerializeToString(&meta_binary);
+            status = RowsetMetaManager::save(data_dir->get_meta(), rowset_pb.rowset_id() , meta_binary);
+            if (status != OLAP_SUCCESS) {
+                LOG(WARNING) << ""convert olap header to tablet meta failed when save rowset meta tablet="" 
+                             << tablet_id << ""."" << schema_hash;
+                return false;
+            }
+        }
+
+        // write converted tablet meta to olap meta
+        string meta_binary;
+        tablet_meta_pb.SerializeToString(&meta_binary);
+        status = TabletMetaManager::save(data_dir, tablet_meta_pb.tablet_id(), tablet_meta_pb.schema_hash(), meta_binary);
+        if (status != OLAP_SUCCESS) {
+            LOG(WARNING) << ""convert olap header to tablet meta failed when save tablet meta tablet="" 
+                        << tablet_id << ""."" << schema_hash;
+            return false;
+        }
+        return true;
+    };
+    OLAPStatus convert_tablet_status = TabletMetaManager::traverse_headers(data_dir->get_meta(), convert_tablet_func, OLD_HEADER_PREFIX);","[{'comment': 'too long', 'commenter': 'kangpinghuang'}]"
669,be/src/olap/storage_engine.cpp,"@@ -117,8 +119,179 @@ StorageEngine::~StorageEngine() {
     clear();
 }
 
+// convert old tablet and its files to new tablet meta and rowset format
+OLAPStatus StorageEngine::_convert_old_tablet(DataDir* data_dir) {
+    auto convert_tablet_func = [this, data_dir](long tablet_id,
+        long schema_hash, const std::string& value) -> bool {
+        OlapSnapshotConverter converter;
+        // convert olap header and files
+        OLAPHeaderMessage olap_header_msg;
+        TabletMetaPB tablet_meta_pb;
+        vector<RowsetMetaPB> pending_rowsets;
+        bool parsed = olap_header_msg.ParseFromString(value);
+        if (!parsed) {
+            LOG(WARNING) << ""convert olap header to tablet meta failed when load olap header tablet="" 
+                         << tablet_id << ""."" << schema_hash;
+            return false;
+        }
+        string old_data_path_prefix = data_dir->get_absolute_tablet_path(olap_header_msg, true);
+        OLAPStatus status = converter.to_new_snapshot(olap_header_msg, old_data_path_prefix, 
+            &tablet_meta_pb, old_data_path_prefix, *data_dir, &pending_rowsets);
+        if (status != OLAP_SUCCESS) {
+            LOG(WARNING) << ""convert olap header to tablet meta failed when convert header and files tablet="" 
+                         << tablet_id << ""."" << schema_hash;
+            return false;
+        }
+
+        // write pending rowset to olap meta
+        for (auto& rowset_pb : pending_rowsets) {
+            string meta_binary;
+            rowset_pb.SerializeToString(&meta_binary);
+            status = RowsetMetaManager::save(data_dir->get_meta(), rowset_pb.rowset_id() , meta_binary);
+            if (status != OLAP_SUCCESS) {
+                LOG(WARNING) << ""convert olap header to tablet meta failed when save rowset meta tablet="" 
+                             << tablet_id << ""."" << schema_hash;
+                return false;
+            }
+        }
+
+        // write converted tablet meta to olap meta
+        string meta_binary;
+        tablet_meta_pb.SerializeToString(&meta_binary);
+        status = TabletMetaManager::save(data_dir, tablet_meta_pb.tablet_id(), tablet_meta_pb.schema_hash(), meta_binary);
+        if (status != OLAP_SUCCESS) {
+            LOG(WARNING) << ""convert olap header to tablet meta failed when save tablet meta tablet="" 
+                        << tablet_id << ""."" << schema_hash;
+            return false;
+        }
+        return true;
+    };
+    OLAPStatus convert_tablet_status = TabletMetaManager::traverse_headers(data_dir->get_meta(), convert_tablet_func, OLD_HEADER_PREFIX);
+    if (convert_tablet_status != OLAP_SUCCESS) {
+        LOG(WARNING) << ""there is failure when convert old tablet, data dir:"" << data_dir->path();
+        return convert_tablet_status;
+    } else {
+        LOG(INFO) << ""successfully convert old tablet, data dir: "" << data_dir->path();
+    }
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus StorageEngine::_clean_unfinished_converting_data(DataDir* data_dir) {
+    auto clean_unifinished_tablet_meta_func = [this, data_dir](long tablet_id,
+        long schema_hash, const std::string& value) -> bool {
+        TabletMetaManager::remove(data_dir, tablet_id, schema_hash, HEADER_PREFIX);
+        LOG(INFO) << ""successfully clean temp tablet meta for tablet="" 
+                  << tablet_id << ""."" << schema_hash 
+                  << ""from data dir: "" << data_dir->path();
+        return true;
+    };
+    OLAPStatus clean_unfinished_meta_status = TabletMetaManager::traverse_headers(data_dir->get_meta(), 
+        clean_unifinished_tablet_meta_func, HEADER_PREFIX);
+    if (clean_unfinished_meta_status != OLAP_SUCCESS) {
+        // If failed to clean meta just skip the error, there will be useless metas in rocksdb column family
+        LOG(WARNING) << ""there is failure when clean temp tablet meta from data dir:"" << data_dir->path();
+    } else {
+        LOG(INFO) << ""successfully clean temp tablet meta from data dir: "" << data_dir->path();
+    }
+    auto clean_unifinished_rowset_meta_func = [this, data_dir](RowsetId rowset_id, const std::string& value) -> bool {
+        RowsetMetaManager::remove(data_dir->get_meta(), rowset_id);
+        LOG(INFO) << ""successfully clean temp rowset meta for rowset id ="" 
+                  << rowset_id << "" from data dir: "" << data_dir->path();
+        return true;
+    };
+    OLAPStatus clean_unfinished_rowset_meta_status = RowsetMetaManager::traverse_rowset_metas(data_dir->get_meta(), 
+        clean_unifinished_rowset_meta_func);
+    if (clean_unfinished_rowset_meta_status != OLAP_SUCCESS) {
+        // If failed to clean meta just skip the error, there will be useless metas in rocksdb column family
+        LOG(WARNING) << ""there is failure when clean temp rowset meta from data dir:"" << data_dir->path();
+    } else {
+        LOG(INFO) << ""successfully clean temp rowset meta from data dir: "" << data_dir->path();
+    }
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus StorageEngine::_remove_old_meta_and_files(DataDir* data_dir) {
+    // clean old meta(olap header message) 
+    auto clean_old_meta_func = [this, data_dir](long tablet_id,
+        long schema_hash, const std::string& value) -> bool {
+        TabletMetaManager::remove(data_dir, tablet_id, schema_hash, OLD_HEADER_PREFIX);
+        LOG(INFO) << ""successfully clean old tablet meta(olap header) for tablet="" 
+                  << tablet_id << ""."" << schema_hash 
+                  << ""from data dir: "" << data_dir->path();
+        return true;
+    };
+    OLAPStatus clean_old_meta_status = TabletMetaManager::traverse_headers(data_dir->get_meta(), 
+        clean_old_meta_func, OLD_HEADER_PREFIX);
+    if (clean_old_meta_status != OLAP_SUCCESS) {
+        // If failed to clean meta just skip the error, there will be useless metas in rocksdb column family
+        LOG(WARNING) << ""there is failure when clean old tablet meta(olap header) from data dir:"" << data_dir->path();
+    } else {
+        LOG(INFO) << ""successfully clean old tablet meta(olap header) from data dir: "" << data_dir->path();
+    }
+
+    // clean old files because they have hard links in new file name format
+    auto clean_old_files_func = [this, data_dir](long tablet_id,
+        long schema_hash, const std::string& value) -> bool {
+        TabletMetaPB tablet_meta_pb;
+        bool parsed = tablet_meta_pb.ParseFromString(value);
+        if (!parsed) {
+            // if errors when load, just skip it
+            LOG(WARNING) << ""failed to load tablet meta from meta store to tablet="" << tablet_id << ""."" << schema_hash;
+            return true;
+        }
+
+        TabletSchema tablet_schema;
+        tablet_schema.init_from_pb(tablet_meta_pb.schema());
+        string data_path_prefix = data_dir->get_absolute_tablet_path(&tablet_meta_pb, true);
+
+        // convert visible pdelta file to rowsets and remove old files
+        for (auto& visible_rowset : tablet_meta_pb.rs_metas()) {
+            RowsetMetaSharedPtr alpha_rowset_meta(new AlphaRowsetMeta());
+            alpha_rowset_meta->init_from_pb(visible_rowset);
+            AlphaRowset rowset(&tablet_schema, data_path_prefix, data_dir, alpha_rowset_meta);
+            std::vector<std::string> old_files;
+            rowset.remove_old_files(&old_files);","[{'comment': 'old_files is useless? why need it?', 'commenter': 'kangpinghuang'}]"
669,be/src/olap/storage_engine.cpp,"@@ -117,8 +119,179 @@ StorageEngine::~StorageEngine() {
     clear();
 }
 
+// convert old tablet and its files to new tablet meta and rowset format
+OLAPStatus StorageEngine::_convert_old_tablet(DataDir* data_dir) {
+    auto convert_tablet_func = [this, data_dir](long tablet_id,
+        long schema_hash, const std::string& value) -> bool {
+        OlapSnapshotConverter converter;
+        // convert olap header and files
+        OLAPHeaderMessage olap_header_msg;
+        TabletMetaPB tablet_meta_pb;
+        vector<RowsetMetaPB> pending_rowsets;
+        bool parsed = olap_header_msg.ParseFromString(value);
+        if (!parsed) {
+            LOG(WARNING) << ""convert olap header to tablet meta failed when load olap header tablet="" 
+                         << tablet_id << ""."" << schema_hash;
+            return false;
+        }
+        string old_data_path_prefix = data_dir->get_absolute_tablet_path(olap_header_msg, true);
+        OLAPStatus status = converter.to_new_snapshot(olap_header_msg, old_data_path_prefix, 
+            &tablet_meta_pb, old_data_path_prefix, *data_dir, &pending_rowsets);
+        if (status != OLAP_SUCCESS) {
+            LOG(WARNING) << ""convert olap header to tablet meta failed when convert header and files tablet="" 
+                         << tablet_id << ""."" << schema_hash;
+            return false;
+        }
+
+        // write pending rowset to olap meta
+        for (auto& rowset_pb : pending_rowsets) {
+            string meta_binary;
+            rowset_pb.SerializeToString(&meta_binary);
+            status = RowsetMetaManager::save(data_dir->get_meta(), rowset_pb.rowset_id() , meta_binary);
+            if (status != OLAP_SUCCESS) {
+                LOG(WARNING) << ""convert olap header to tablet meta failed when save rowset meta tablet="" 
+                             << tablet_id << ""."" << schema_hash;
+                return false;
+            }
+        }
+
+        // write converted tablet meta to olap meta
+        string meta_binary;
+        tablet_meta_pb.SerializeToString(&meta_binary);
+        status = TabletMetaManager::save(data_dir, tablet_meta_pb.tablet_id(), tablet_meta_pb.schema_hash(), meta_binary);
+        if (status != OLAP_SUCCESS) {
+            LOG(WARNING) << ""convert olap header to tablet meta failed when save tablet meta tablet="" 
+                        << tablet_id << ""."" << schema_hash;
+            return false;
+        }
+        return true;
+    };
+    OLAPStatus convert_tablet_status = TabletMetaManager::traverse_headers(data_dir->get_meta(), convert_tablet_func, OLD_HEADER_PREFIX);
+    if (convert_tablet_status != OLAP_SUCCESS) {
+        LOG(WARNING) << ""there is failure when convert old tablet, data dir:"" << data_dir->path();
+        return convert_tablet_status;
+    } else {
+        LOG(INFO) << ""successfully convert old tablet, data dir: "" << data_dir->path();
+    }
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus StorageEngine::_clean_unfinished_converting_data(DataDir* data_dir) {
+    auto clean_unifinished_tablet_meta_func = [this, data_dir](long tablet_id,
+        long schema_hash, const std::string& value) -> bool {
+        TabletMetaManager::remove(data_dir, tablet_id, schema_hash, HEADER_PREFIX);
+        LOG(INFO) << ""successfully clean temp tablet meta for tablet="" 
+                  << tablet_id << ""."" << schema_hash 
+                  << ""from data dir: "" << data_dir->path();
+        return true;
+    };
+    OLAPStatus clean_unfinished_meta_status = TabletMetaManager::traverse_headers(data_dir->get_meta(), 
+        clean_unifinished_tablet_meta_func, HEADER_PREFIX);
+    if (clean_unfinished_meta_status != OLAP_SUCCESS) {
+        // If failed to clean meta just skip the error, there will be useless metas in rocksdb column family
+        LOG(WARNING) << ""there is failure when clean temp tablet meta from data dir:"" << data_dir->path();
+    } else {
+        LOG(INFO) << ""successfully clean temp tablet meta from data dir: "" << data_dir->path();
+    }
+    auto clean_unifinished_rowset_meta_func = [this, data_dir](RowsetId rowset_id, const std::string& value) -> bool {
+        RowsetMetaManager::remove(data_dir->get_meta(), rowset_id);
+        LOG(INFO) << ""successfully clean temp rowset meta for rowset id ="" 
+                  << rowset_id << "" from data dir: "" << data_dir->path();
+        return true;
+    };
+    OLAPStatus clean_unfinished_rowset_meta_status = RowsetMetaManager::traverse_rowset_metas(data_dir->get_meta(), 
+        clean_unifinished_rowset_meta_func);
+    if (clean_unfinished_rowset_meta_status != OLAP_SUCCESS) {
+        // If failed to clean meta just skip the error, there will be useless metas in rocksdb column family
+        LOG(WARNING) << ""there is failure when clean temp rowset meta from data dir:"" << data_dir->path();
+    } else {
+        LOG(INFO) << ""successfully clean temp rowset meta from data dir: "" << data_dir->path();
+    }
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus StorageEngine::_remove_old_meta_and_files(DataDir* data_dir) {
+    // clean old meta(olap header message) 
+    auto clean_old_meta_func = [this, data_dir](long tablet_id,
+        long schema_hash, const std::string& value) -> bool {
+        TabletMetaManager::remove(data_dir, tablet_id, schema_hash, OLD_HEADER_PREFIX);
+        LOG(INFO) << ""successfully clean old tablet meta(olap header) for tablet="" 
+                  << tablet_id << ""."" << schema_hash 
+                  << ""from data dir: "" << data_dir->path();
+        return true;
+    };
+    OLAPStatus clean_old_meta_status = TabletMetaManager::traverse_headers(data_dir->get_meta(), 
+        clean_old_meta_func, OLD_HEADER_PREFIX);
+    if (clean_old_meta_status != OLAP_SUCCESS) {
+        // If failed to clean meta just skip the error, there will be useless metas in rocksdb column family
+        LOG(WARNING) << ""there is failure when clean old tablet meta(olap header) from data dir:"" << data_dir->path();
+    } else {
+        LOG(INFO) << ""successfully clean old tablet meta(olap header) from data dir: "" << data_dir->path();
+    }
+
+    // clean old files because they have hard links in new file name format
+    auto clean_old_files_func = [this, data_dir](long tablet_id,
+        long schema_hash, const std::string& value) -> bool {
+        TabletMetaPB tablet_meta_pb;
+        bool parsed = tablet_meta_pb.ParseFromString(value);
+        if (!parsed) {
+            // if errors when load, just skip it
+            LOG(WARNING) << ""failed to load tablet meta from meta store to tablet="" << tablet_id << ""."" << schema_hash;
+            return true;
+        }
+
+        TabletSchema tablet_schema;
+        tablet_schema.init_from_pb(tablet_meta_pb.schema());
+        string data_path_prefix = data_dir->get_absolute_tablet_path(&tablet_meta_pb, true);
+
+        // convert visible pdelta file to rowsets and remove old files
+        for (auto& visible_rowset : tablet_meta_pb.rs_metas()) {
+            RowsetMetaSharedPtr alpha_rowset_meta(new AlphaRowsetMeta());
+            alpha_rowset_meta->init_from_pb(visible_rowset);
+            AlphaRowset rowset(&tablet_schema, data_path_prefix, data_dir, alpha_rowset_meta);
+            std::vector<std::string> old_files;
+            rowset.remove_old_files(&old_files);
+        }
+
+        // convert inc delta file to rowsets and remove old files
+        for (auto& inc_rowset : tablet_meta_pb.inc_rs_metas()) {
+            RowsetMetaSharedPtr alpha_rowset_meta(new AlphaRowsetMeta());
+            alpha_rowset_meta->init_from_pb(inc_rowset);
+            AlphaRowset rowset(&tablet_schema, data_path_prefix, data_dir, alpha_rowset_meta);
+            std::vector<std::string> old_files;
+            rowset.remove_old_files(&old_files);
+        }
+        return true;
+    };
+    OLAPStatus clean_old_tablet_status = TabletMetaManager::traverse_headers(data_dir->get_meta(), clean_old_files_func, HEADER_PREFIX);","[{'comment': 'too long', 'commenter': 'kangpinghuang'}]"
669,be/src/olap/storage_engine.cpp,"@@ -117,8 +119,179 @@ StorageEngine::~StorageEngine() {
     clear();
 }
 
+// convert old tablet and its files to new tablet meta and rowset format
+OLAPStatus StorageEngine::_convert_old_tablet(DataDir* data_dir) {
+    auto convert_tablet_func = [this, data_dir](long tablet_id,
+        long schema_hash, const std::string& value) -> bool {
+        OlapSnapshotConverter converter;
+        // convert olap header and files
+        OLAPHeaderMessage olap_header_msg;
+        TabletMetaPB tablet_meta_pb;
+        vector<RowsetMetaPB> pending_rowsets;
+        bool parsed = olap_header_msg.ParseFromString(value);
+        if (!parsed) {
+            LOG(WARNING) << ""convert olap header to tablet meta failed when load olap header tablet="" 
+                         << tablet_id << ""."" << schema_hash;
+            return false;
+        }
+        string old_data_path_prefix = data_dir->get_absolute_tablet_path(olap_header_msg, true);
+        OLAPStatus status = converter.to_new_snapshot(olap_header_msg, old_data_path_prefix, 
+            &tablet_meta_pb, old_data_path_prefix, *data_dir, &pending_rowsets);
+        if (status != OLAP_SUCCESS) {
+            LOG(WARNING) << ""convert olap header to tablet meta failed when convert header and files tablet="" 
+                         << tablet_id << ""."" << schema_hash;
+            return false;
+        }
+
+        // write pending rowset to olap meta
+        for (auto& rowset_pb : pending_rowsets) {
+            string meta_binary;
+            rowset_pb.SerializeToString(&meta_binary);
+            status = RowsetMetaManager::save(data_dir->get_meta(), rowset_pb.rowset_id() , meta_binary);
+            if (status != OLAP_SUCCESS) {
+                LOG(WARNING) << ""convert olap header to tablet meta failed when save rowset meta tablet="" 
+                             << tablet_id << ""."" << schema_hash;
+                return false;
+            }
+        }
+
+        // write converted tablet meta to olap meta
+        string meta_binary;
+        tablet_meta_pb.SerializeToString(&meta_binary);
+        status = TabletMetaManager::save(data_dir, tablet_meta_pb.tablet_id(), tablet_meta_pb.schema_hash(), meta_binary);
+        if (status != OLAP_SUCCESS) {
+            LOG(WARNING) << ""convert olap header to tablet meta failed when save tablet meta tablet="" 
+                        << tablet_id << ""."" << schema_hash;
+            return false;
+        }
+        return true;
+    };
+    OLAPStatus convert_tablet_status = TabletMetaManager::traverse_headers(data_dir->get_meta(), convert_tablet_func, OLD_HEADER_PREFIX);
+    if (convert_tablet_status != OLAP_SUCCESS) {
+        LOG(WARNING) << ""there is failure when convert old tablet, data dir:"" << data_dir->path();
+        return convert_tablet_status;
+    } else {
+        LOG(INFO) << ""successfully convert old tablet, data dir: "" << data_dir->path();
+    }
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus StorageEngine::_clean_unfinished_converting_data(DataDir* data_dir) {
+    auto clean_unifinished_tablet_meta_func = [this, data_dir](long tablet_id,
+        long schema_hash, const std::string& value) -> bool {
+        TabletMetaManager::remove(data_dir, tablet_id, schema_hash, HEADER_PREFIX);
+        LOG(INFO) << ""successfully clean temp tablet meta for tablet="" 
+                  << tablet_id << ""."" << schema_hash 
+                  << ""from data dir: "" << data_dir->path();
+        return true;
+    };
+    OLAPStatus clean_unfinished_meta_status = TabletMetaManager::traverse_headers(data_dir->get_meta(), 
+        clean_unifinished_tablet_meta_func, HEADER_PREFIX);
+    if (clean_unfinished_meta_status != OLAP_SUCCESS) {
+        // If failed to clean meta just skip the error, there will be useless metas in rocksdb column family
+        LOG(WARNING) << ""there is failure when clean temp tablet meta from data dir:"" << data_dir->path();
+    } else {
+        LOG(INFO) << ""successfully clean temp tablet meta from data dir: "" << data_dir->path();
+    }
+    auto clean_unifinished_rowset_meta_func = [this, data_dir](RowsetId rowset_id, const std::string& value) -> bool {
+        RowsetMetaManager::remove(data_dir->get_meta(), rowset_id);
+        LOG(INFO) << ""successfully clean temp rowset meta for rowset id ="" 
+                  << rowset_id << "" from data dir: "" << data_dir->path();
+        return true;
+    };
+    OLAPStatus clean_unfinished_rowset_meta_status = RowsetMetaManager::traverse_rowset_metas(data_dir->get_meta(), 
+        clean_unifinished_rowset_meta_func);
+    if (clean_unfinished_rowset_meta_status != OLAP_SUCCESS) {
+        // If failed to clean meta just skip the error, there will be useless metas in rocksdb column family
+        LOG(WARNING) << ""there is failure when clean temp rowset meta from data dir:"" << data_dir->path();
+    } else {
+        LOG(INFO) << ""successfully clean temp rowset meta from data dir: "" << data_dir->path();
+    }
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus StorageEngine::_remove_old_meta_and_files(DataDir* data_dir) {
+    // clean old meta(olap header message) 
+    auto clean_old_meta_func = [this, data_dir](long tablet_id,
+        long schema_hash, const std::string& value) -> bool {
+        TabletMetaManager::remove(data_dir, tablet_id, schema_hash, OLD_HEADER_PREFIX);
+        LOG(INFO) << ""successfully clean old tablet meta(olap header) for tablet="" 
+                  << tablet_id << ""."" << schema_hash 
+                  << ""from data dir: "" << data_dir->path();
+        return true;
+    };
+    OLAPStatus clean_old_meta_status = TabletMetaManager::traverse_headers(data_dir->get_meta(), 
+        clean_old_meta_func, OLD_HEADER_PREFIX);
+    if (clean_old_meta_status != OLAP_SUCCESS) {
+        // If failed to clean meta just skip the error, there will be useless metas in rocksdb column family
+        LOG(WARNING) << ""there is failure when clean old tablet meta(olap header) from data dir:"" << data_dir->path();
+    } else {
+        LOG(INFO) << ""successfully clean old tablet meta(olap header) from data dir: "" << data_dir->path();
+    }
+
+    // clean old files because they have hard links in new file name format
+    auto clean_old_files_func = [this, data_dir](long tablet_id,
+        long schema_hash, const std::string& value) -> bool {
+        TabletMetaPB tablet_meta_pb;
+        bool parsed = tablet_meta_pb.ParseFromString(value);
+        if (!parsed) {
+            // if errors when load, just skip it
+            LOG(WARNING) << ""failed to load tablet meta from meta store to tablet="" << tablet_id << ""."" << schema_hash;
+            return true;
+        }
+
+        TabletSchema tablet_schema;
+        tablet_schema.init_from_pb(tablet_meta_pb.schema());
+        string data_path_prefix = data_dir->get_absolute_tablet_path(&tablet_meta_pb, true);
+
+        // convert visible pdelta file to rowsets and remove old files
+        for (auto& visible_rowset : tablet_meta_pb.rs_metas()) {
+            RowsetMetaSharedPtr alpha_rowset_meta(new AlphaRowsetMeta());
+            alpha_rowset_meta->init_from_pb(visible_rowset);
+            AlphaRowset rowset(&tablet_schema, data_path_prefix, data_dir, alpha_rowset_meta);
+            std::vector<std::string> old_files;
+            rowset.remove_old_files(&old_files);
+        }
+
+        // convert inc delta file to rowsets and remove old files
+        for (auto& inc_rowset : tablet_meta_pb.inc_rs_metas()) {
+            RowsetMetaSharedPtr alpha_rowset_meta(new AlphaRowsetMeta());
+            alpha_rowset_meta->init_from_pb(inc_rowset);
+            AlphaRowset rowset(&tablet_schema, data_path_prefix, data_dir, alpha_rowset_meta);
+            std::vector<std::string> old_files;
+            rowset.remove_old_files(&old_files);
+        }
+        return true;
+    };
+    OLAPStatus clean_old_tablet_status = TabletMetaManager::traverse_headers(data_dir->get_meta(), clean_old_files_func, HEADER_PREFIX);
+    if (clean_old_tablet_status != OLAP_SUCCESS) {
+        LOG(WARNING) << ""there is failure when loading tablet and clean old files:"" << data_dir->path();
+    } else {
+        LOG(INFO) << ""load rowset from meta finished, data dir: "" << data_dir->path();
+    }
+    return OLAP_SUCCESS;
+}
+
 // TODO(ygl): deal with rowsets and tablets when load failed
 OLAPStatus StorageEngine::_load_data_dir(DataDir* data_dir) {
+    // check if this is an old data path
+    bool is_tablet_convert_finished = false;
+    OLAPStatus res = data_dir->get_meta()->get_tablet_convert_finished(is_tablet_convert_finished);
+    if (res != OLAP_SUCCESS) {
+        LOG(WARNING) << ""get convert flag from meta failed"";
+        return res;
+    }
+
+    if (!is_tablet_convert_finished) {
+        _clean_unfinished_converting_data(data_dir);
+        RETURN_NOT_OK(_convert_old_tablet(data_dir));
+        // TODO(ygl): should load tablet successfully and then set convert flag and clean old files
+        RETURN_NOT_OK(data_dir->get_meta()->set_tablet_convert_finished());
+        // convert may be successfully, but crushed before remove old files","[{'comment': 'crushed -> crashed?', 'commenter': 'kangpinghuang'}]"
669,be/src/olap/storage_engine.cpp,"@@ -117,8 +119,179 @@ StorageEngine::~StorageEngine() {
     clear();
 }
 
+// convert old tablet and its files to new tablet meta and rowset format
+OLAPStatus StorageEngine::_convert_old_tablet(DataDir* data_dir) {
+    auto convert_tablet_func = [this, data_dir](long tablet_id,
+        long schema_hash, const std::string& value) -> bool {
+        OlapSnapshotConverter converter;
+        // convert olap header and files
+        OLAPHeaderMessage olap_header_msg;
+        TabletMetaPB tablet_meta_pb;
+        vector<RowsetMetaPB> pending_rowsets;
+        bool parsed = olap_header_msg.ParseFromString(value);
+        if (!parsed) {
+            LOG(WARNING) << ""convert olap header to tablet meta failed when load olap header tablet="" 
+                         << tablet_id << ""."" << schema_hash;
+            return false;
+        }
+        string old_data_path_prefix = data_dir->get_absolute_tablet_path(olap_header_msg, true);
+        OLAPStatus status = converter.to_new_snapshot(olap_header_msg, old_data_path_prefix, 
+            &tablet_meta_pb, old_data_path_prefix, *data_dir, &pending_rowsets);
+        if (status != OLAP_SUCCESS) {
+            LOG(WARNING) << ""convert olap header to tablet meta failed when convert header and files tablet="" 
+                         << tablet_id << ""."" << schema_hash;
+            return false;
+        }
+
+        // write pending rowset to olap meta
+        for (auto& rowset_pb : pending_rowsets) {
+            string meta_binary;
+            rowset_pb.SerializeToString(&meta_binary);
+            status = RowsetMetaManager::save(data_dir->get_meta(), rowset_pb.rowset_id() , meta_binary);
+            if (status != OLAP_SUCCESS) {
+                LOG(WARNING) << ""convert olap header to tablet meta failed when save rowset meta tablet="" 
+                             << tablet_id << ""."" << schema_hash;
+                return false;
+            }
+        }
+
+        // write converted tablet meta to olap meta
+        string meta_binary;
+        tablet_meta_pb.SerializeToString(&meta_binary);
+        status = TabletMetaManager::save(data_dir, tablet_meta_pb.tablet_id(), tablet_meta_pb.schema_hash(), meta_binary);
+        if (status != OLAP_SUCCESS) {
+            LOG(WARNING) << ""convert olap header to tablet meta failed when save tablet meta tablet="" 
+                        << tablet_id << ""."" << schema_hash;
+            return false;
+        }
+        return true;
+    };
+    OLAPStatus convert_tablet_status = TabletMetaManager::traverse_headers(data_dir->get_meta(), convert_tablet_func, OLD_HEADER_PREFIX);
+    if (convert_tablet_status != OLAP_SUCCESS) {
+        LOG(WARNING) << ""there is failure when convert old tablet, data dir:"" << data_dir->path();
+        return convert_tablet_status;
+    } else {
+        LOG(INFO) << ""successfully convert old tablet, data dir: "" << data_dir->path();
+    }
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus StorageEngine::_clean_unfinished_converting_data(DataDir* data_dir) {
+    auto clean_unifinished_tablet_meta_func = [this, data_dir](long tablet_id,
+        long schema_hash, const std::string& value) -> bool {
+        TabletMetaManager::remove(data_dir, tablet_id, schema_hash, HEADER_PREFIX);
+        LOG(INFO) << ""successfully clean temp tablet meta for tablet="" 
+                  << tablet_id << ""."" << schema_hash 
+                  << ""from data dir: "" << data_dir->path();
+        return true;
+    };
+    OLAPStatus clean_unfinished_meta_status = TabletMetaManager::traverse_headers(data_dir->get_meta(), 
+        clean_unifinished_tablet_meta_func, HEADER_PREFIX);
+    if (clean_unfinished_meta_status != OLAP_SUCCESS) {
+        // If failed to clean meta just skip the error, there will be useless metas in rocksdb column family
+        LOG(WARNING) << ""there is failure when clean temp tablet meta from data dir:"" << data_dir->path();
+    } else {
+        LOG(INFO) << ""successfully clean temp tablet meta from data dir: "" << data_dir->path();
+    }
+    auto clean_unifinished_rowset_meta_func = [this, data_dir](RowsetId rowset_id, const std::string& value) -> bool {
+        RowsetMetaManager::remove(data_dir->get_meta(), rowset_id);
+        LOG(INFO) << ""successfully clean temp rowset meta for rowset id ="" 
+                  << rowset_id << "" from data dir: "" << data_dir->path();
+        return true;
+    };
+    OLAPStatus clean_unfinished_rowset_meta_status = RowsetMetaManager::traverse_rowset_metas(data_dir->get_meta(), 
+        clean_unifinished_rowset_meta_func);
+    if (clean_unfinished_rowset_meta_status != OLAP_SUCCESS) {
+        // If failed to clean meta just skip the error, there will be useless metas in rocksdb column family
+        LOG(WARNING) << ""there is failure when clean temp rowset meta from data dir:"" << data_dir->path();
+    } else {
+        LOG(INFO) << ""successfully clean temp rowset meta from data dir: "" << data_dir->path();
+    }
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus StorageEngine::_remove_old_meta_and_files(DataDir* data_dir) {
+    // clean old meta(olap header message) 
+    auto clean_old_meta_func = [this, data_dir](long tablet_id,
+        long schema_hash, const std::string& value) -> bool {
+        TabletMetaManager::remove(data_dir, tablet_id, schema_hash, OLD_HEADER_PREFIX);
+        LOG(INFO) << ""successfully clean old tablet meta(olap header) for tablet="" 
+                  << tablet_id << ""."" << schema_hash 
+                  << ""from data dir: "" << data_dir->path();
+        return true;
+    };
+    OLAPStatus clean_old_meta_status = TabletMetaManager::traverse_headers(data_dir->get_meta(), 
+        clean_old_meta_func, OLD_HEADER_PREFIX);
+    if (clean_old_meta_status != OLAP_SUCCESS) {
+        // If failed to clean meta just skip the error, there will be useless metas in rocksdb column family
+        LOG(WARNING) << ""there is failure when clean old tablet meta(olap header) from data dir:"" << data_dir->path();
+    } else {
+        LOG(INFO) << ""successfully clean old tablet meta(olap header) from data dir: "" << data_dir->path();
+    }
+
+    // clean old files because they have hard links in new file name format
+    auto clean_old_files_func = [this, data_dir](long tablet_id,
+        long schema_hash, const std::string& value) -> bool {
+        TabletMetaPB tablet_meta_pb;
+        bool parsed = tablet_meta_pb.ParseFromString(value);
+        if (!parsed) {
+            // if errors when load, just skip it
+            LOG(WARNING) << ""failed to load tablet meta from meta store to tablet="" << tablet_id << ""."" << schema_hash;
+            return true;
+        }
+
+        TabletSchema tablet_schema;
+        tablet_schema.init_from_pb(tablet_meta_pb.schema());
+        string data_path_prefix = data_dir->get_absolute_tablet_path(&tablet_meta_pb, true);
+
+        // convert visible pdelta file to rowsets and remove old files
+        for (auto& visible_rowset : tablet_meta_pb.rs_metas()) {
+            RowsetMetaSharedPtr alpha_rowset_meta(new AlphaRowsetMeta());
+            alpha_rowset_meta->init_from_pb(visible_rowset);
+            AlphaRowset rowset(&tablet_schema, data_path_prefix, data_dir, alpha_rowset_meta);
+            std::vector<std::string> old_files;
+            rowset.remove_old_files(&old_files);
+        }
+
+        // convert inc delta file to rowsets and remove old files
+        for (auto& inc_rowset : tablet_meta_pb.inc_rs_metas()) {
+            RowsetMetaSharedPtr alpha_rowset_meta(new AlphaRowsetMeta());
+            alpha_rowset_meta->init_from_pb(inc_rowset);
+            AlphaRowset rowset(&tablet_schema, data_path_prefix, data_dir, alpha_rowset_meta);
+            std::vector<std::string> old_files;
+            rowset.remove_old_files(&old_files);
+        }
+        return true;
+    };
+    OLAPStatus clean_old_tablet_status = TabletMetaManager::traverse_headers(data_dir->get_meta(), clean_old_files_func, HEADER_PREFIX);
+    if (clean_old_tablet_status != OLAP_SUCCESS) {
+        LOG(WARNING) << ""there is failure when loading tablet and clean old files:"" << data_dir->path();
+    } else {
+        LOG(INFO) << ""load rowset from meta finished, data dir: "" << data_dir->path();
+    }
+    return OLAP_SUCCESS;
+}
+
 // TODO(ygl): deal with rowsets and tablets when load failed
 OLAPStatus StorageEngine::_load_data_dir(DataDir* data_dir) {
+    // check if this is an old data path
+    bool is_tablet_convert_finished = false;
+    OLAPStatus res = data_dir->get_meta()->get_tablet_convert_finished(is_tablet_convert_finished);","[{'comment': 'this flag is always true if the be upgrade from old version.Will it cause problems?', 'commenter': 'kangpinghuang'}]"
669,be/src/olap/tablet_meta_manager.cpp,"@@ -78,9 +76,9 @@ OLAPStatus TabletMetaManager::get_json_header(DataDir* store,
 }
 
 OLAPStatus TabletMetaManager::save(DataDir* store,
-        TTabletId tablet_id, TSchemaHash schema_hash, const TabletMeta* tablet_meta) {
+        TTabletId tablet_id, TSchemaHash schema_hash, const TabletMeta* tablet_meta, string header_prefix) {","[{'comment': 'I think header_prefxi should be renamed to meta_prefix or just prefix', 'commenter': 'kangpinghuang'}]"
669,be/src/olap/tablet_meta_manager.h,"@@ -34,13 +38,16 @@ class TabletMetaManager {
     static OLAPStatus get_json_header(DataDir* store, TTabletId tablet_id,
             TSchemaHash schema_hash, std::string* json_header);
 
-    static OLAPStatus save(DataDir* store, TTabletId tablet_id, TSchemaHash schema_hash, const TabletMeta* tablet_meta);
-    static OLAPStatus save(DataDir* store, TTabletId tablet_id, TSchemaHash schema_hash, const std::string& meta_binary);
+    static OLAPStatus save(DataDir* store, TTabletId tablet_id, TSchemaHash schema_hash, 
+                           const TabletMeta* tablet_meta, string header_prefix = ""tablet_meta_"");","[{'comment': 'the default argument is not recommended.\r\nThe same to the following functions', 'commenter': 'kangpinghuang'}]"
687,fe/src/main/java/org/apache/doris/catalog/Catalog.java,"@@ -6065,5 +6079,22 @@ public void replayBackendTabletsInfo(BackendTabletsInfo backendTabletsInfo) {
             replica.setBad(backendTabletsInfo.isBad());
         }
     }
+
+    public List<Long> getBackendIdsByCluster(String clusterName) throws MetaNotFoundException {","[{'comment': 'We already have a method in SystemInfoService', 'commenter': 'morningman'}]"
687,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoadManager.java,"@@ -248,16 +257,20 @@ public void resumeRoutineLoadJob(ResumeRoutineLoadStmt resumeRoutineLoadStmt) th
             throw new DdlException(""There is not routine load job with name "" + resumeRoutineLoadStmt.getName());
         }
         // check auth
-        if (!Catalog.getCurrentCatalog().getAuth().checkTblPriv(ConnectContext.get(),
-                                                                routineLoadJob.getDbFullName(),
-                                                                routineLoadJob.getTableName(),
-                                                                PrivPredicate.LOAD)) {
-            ErrorReport.reportAnalysisException(ErrorCode.ERR_TABLEACCESS_DENIED_ERROR, ""LOAD"",
-                                                ConnectContext.get().getQualifiedUser(),
-                                                ConnectContext.get().getRemoteIP(),
-                                                routineLoadJob.getTableName());
+        try {","[{'comment': 'Check priv does not throw MetaNotFoundException', 'commenter': 'morningman'}]"
687,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoadManager.java,"@@ -304,21 +321,28 @@ public int getClusterIdleSlotNum() {
         }
     }
 
-    public long getMinTaskBeId() throws LoadException {
+    public long getMinTaskBeId(String clusterName) throws LoadException {
+        List<Long> beIdsInCluster = new ArrayList<>();
+        try {","[{'comment': 'Use method in SystemInfoService', 'commenter': 'morningman'}]"
687,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoadTaskInfo.java,"@@ -79,8 +73,16 @@ public long getLoadStartTimeMs() {
     public long getTxnId() {
         return txnId;
     }
-    
-    abstract TRoutineLoadTask createRoutineLoadTask(long beId) throws LoadException, UserException;
+
+    abstract TRoutineLoadTask createRoutineLoadTask() throws LoadException, UserException;
+
+    public void setTxn() throws LabelAlreadyUsedException, BeginTransactionException, AnalysisException {","[{'comment': ""better to name it 'beginTxn'?"", 'commenter': 'morningman'}]"
688,be/src/olap/storage_engine.cpp,"@@ -1021,4 +987,232 @@ OLAPStatus StorageEngine::execute_task(EngineTask* task) {
     }
 }
 
+void StorageEngine::add_check_paths(std::set<std::string> paths) {
+    _check_path_mutex.wrlock();
+    _all_check_paths.insert(paths.begin(), paths.end());
+    _check_path_mutex.unlock();
+}
+
+void StorageEngine::remove_check_paths(std::set<std::string> paths) {
+    _check_path_mutex.wrlock();
+    _remove_check_paths_no_lock(paths);
+    _check_path_mutex.unlock();
+}
+
+void StorageEngine::add_pending_paths(int64_t id, std::set<std::string> paths) {
+    WriteLock wr_lock(&_pending_path_mutex);
+    auto pending_paths= _pending_paths[id];
+    pending_paths.insert(paths.begin(), paths.end());
+}
+
+void StorageEngine::remove_pending_paths(int64_t id) {
+    WriteLock wr_lock(&_pending_path_mutex);
+    _pending_paths.erase(id);
+}
+
+bool StorageEngine::check_path_in_pending_paths(std::string path) {
+    ReadLock rd_lock(&_pending_path_mutex);
+    for (auto id_pending_paths : _pending_paths) {
+        if (id_pending_paths.second.find(path) != id_pending_paths.second.end()) {
+            return true;
+        }
+    }
+    return false;
+}
+
+void StorageEngine::process_garbage_path(std::string path) {
+    if (check_dir_existed(path)) {
+        LOG(INFO) << ""collect garbage dir path:"" << path;
+        OLAPStatus status = remove_all_dir(path);
+        if (status != OLAP_SUCCESS) {
+            LOG(WARNING) << ""remove garbage dir path:"" << path << "" failed"";
+        }
+    }
+}
+
+void StorageEngine::_perform_path_gc(void* arg) {
+    // init the set of valid path
+    // validate the path in data dir
+    LOG(INFO) << ""start to path gc."";
+    int start = 0;
+    int step = config::path_gc_check_step;
+    while (true) {
+        _check_path_mutex.wrlock();
+        for (int index = start; index < start + step;) {
+            auto path_iter = std::next(_all_check_paths.begin(), index);
+            if (path_iter == _all_check_paths.end()) {
+                break;
+            }
+            std::string path = *path_iter;
+            TTabletId tablet_id = -1;
+            TSchemaHash schema_hash = -1;
+            bool is_valid = TabletManager::instance()->get_tablet_id_and_schema_hash_from_path(path,
+                    &tablet_id, &schema_hash);
+            std::set<std::string> paths;
+            paths.insert(path);
+            if (!is_valid) {
+                LOG(WARNING) << ""unknow path:"" << path;
+                _remove_check_paths_no_lock(paths);
+                continue;
+            } else {
+                if (tablet_id >0 && schema_hash >0) {
+                    // tablet schema hash path or rowset file path
+                    TabletSharedPtr tablet = TabletManager::instance()->get_tablet(tablet_id, schema_hash);
+                    if (tablet == nullptr) {
+                        bool exist_in_pending = check_path_in_pending_paths(path);
+                        if (!exist_in_pending) {
+                            process_garbage_path(path);
+                            _remove_check_paths_no_lock(paths);
+                            continue;
+                        }
+                    } else {
+                        bool valid = tablet->check_path(path);
+                        if (!valid) {
+                            bool exist_in_pending = check_path_in_pending_paths(path);
+                            if (!exist_in_pending) {
+                                process_garbage_path(path);
+                                _remove_check_paths_no_lock(paths);
+                                continue;
+                            }
+                        }
+                    }
+                } else if (tablet_id >0 && schema_hash <= 0) {
+                    // tablet id path
+                    if (FileUtils::is_dir(path)) {
+                        bool exist = TabletManager::instance()->check_tablet_id_exist(tablet_id);
+                        if (!exist) {
+                            bool exist_in_pending = check_path_in_pending_paths(path);
+                            if (!exist_in_pending) {
+                                process_garbage_path(path);
+                                _remove_check_paths_no_lock(paths);
+                                continue;
+                            }
+                        }
+                    } else {
+                        LOG(WARNING) << ""unknown path:"" << path;
+                        _remove_check_paths_no_lock(paths);
+                        continue;
+                    }
+                } else {
+                    LOG(WARNING) << ""unknown path:"" << path;
+                    _remove_check_paths_no_lock(paths);
+                    continue;
+                }
+            }
+            ++index;
+        }
+
+        start += step;
+        if (start >= _all_check_paths.size()) {
+            _check_path_mutex.unlock();
+            break;
+        }
+        _check_path_mutex.unlock();
+        sleep(config::path_gc_check_step_interval_ms);
+    }
+    LOG(INFO) << ""finished one time path gc."";
+}
+
+void* StorageEngine::_path_gc_thread_callback(void* arg) {
+#ifdef GOOGLE_PROFILER
+    ProfilerRegisterThread();
+#endif
+
+    LOG(INFO) << ""try to start path gc thread!"";
+    uint32_t interval = config::path_gc_check_interval_second;
+    if (interval <= 0) {
+        LOG(WARNING) << ""path gc thread check interval config is illegal:"" << interval
+            << ""will be forced set to half hour"";
+        interval = 1800; // 0.5 hour
+    }
+
+    while (true) {
+        LOG(INFO) << ""try to perform path gc!"";
+        CgroupsMgr::apply_system_cgroup();","[{'comment': 'no need to use cgroup', 'commenter': 'chaoyli'}, {'comment': 'OK', 'commenter': 'kangpinghuang'}]"
688,be/src/olap/base_compaction.cpp,"@@ -360,7 +364,7 @@ OLAPStatus BaseCompaction::_do_base_compaction(VersionHash new_base_version_hash
 
     Merger merger(_tablet, rs_writer, READER_BASE_COMPACTION);
     res = merger.merge(rs_readers, &merged_rows, &filted_rows);
-
+    StorageEngine::instance()->remove_pending_paths(rs_writer->rowset_id());","[{'comment': 'forget to remove when base compaction failed', 'commenter': 'chaoyli'}]"
688,be/src/olap/cumulative_compaction.cpp,"@@ -378,14 +382,14 @@ OLAPStatus CumulativeCompaction::_do_cumulative_compaction() {
     uint64_t filted_rows = 0;
     res = merger.merge(_rs_readers, &merged_rows, &filted_rows);
     if (res != OLAP_SUCCESS) {
-        OLAP_LOG_WARNING(""failed to do cumulative merge. [tablet=%s; cumulative_version=%d-%d]"",
-                         _tablet->full_name().c_str(),
-                         _cumulative_version.first,
-                         _cumulative_version.second);
+        LOG(WARNING) << ""failed to do cumulative merge. ""
+                     << ""tablet="" << _tablet->full_name()
+                     << "", cumulative_version="" << _cumulative_version.first << ""-"" << _cumulative_version.second;
         return res;
     }
 
     _rowset = _rs_writer->build();
+    StorageEngine::instance()->remove_pending_paths(_rs_writer->rowset_id());","[{'comment': 'forget to remove when base compaction failed', 'commenter': 'chaoyli'}]"
688,be/src/olap/delta_writer.cpp,"@@ -176,6 +176,7 @@ OLAPStatus DeltaWriter::close(google::protobuf::RepeatedPtrField<PTabletInfo>* t
     OLAPStatus res = OLAP_SUCCESS;
     // use rowset meta manager to save meta
     _cur_rowset = _rowset_writer->build();
+    StorageEngine::instance()->remove_pending_paths(_rowset_writer->rowset_id());","[{'comment': 'forget to remove when base compaction failed', 'commenter': 'chaoyli'}]"
688,be/src/olap/storage_engine.cpp,"@@ -1021,4 +987,232 @@ OLAPStatus StorageEngine::execute_task(EngineTask* task) {
     }
 }
 
+void StorageEngine::add_check_paths(std::set<std::string> paths) {","[{'comment': 'std::set<std::string>&', 'commenter': 'chaoyli'}, {'comment': 'ok', 'commenter': 'kangpinghuang'}]"
688,be/src/olap/tablet_manager.h,"@@ -108,7 +108,9 @@ class TabletManager {
     TabletSharedPtr find_best_tablet_to_compaction(CompactionType compaction_type);
 
     // Get tablet pointer
-    TabletSharedPtr get_tablet(TTabletId tablet_id, SchemaHash schema_hash, bool load_tablet = true);
+    TabletSharedPtr get_tablet(TTabletId tablet_id, SchemaHash schema_hash);
+
+    bool get_tablet_id_and_schema_hash_from_path(std::string path, TTabletId* tablet_id, TSchemaHash* schema_hash);","[{'comment': 'std::string&', 'commenter': 'chaoyli'}, {'comment': 'ok', 'commenter': 'kangpinghuang'}]"
688,be/src/olap/tablet.h,"@@ -63,6 +63,8 @@ class Tablet : public std::enable_shared_from_this<Tablet> {
     Tablet(TabletMeta* tablet_meta, DataDir* data_dir);
     ~Tablet();
 
+    static bool is_valid_tablet_path(std::string path);
+","[{'comment': 'std::string&', 'commenter': 'chaoyli'}]"
697,fe/src/main/cup/sql_parser.cup,"@@ -3106,11 +3106,11 @@ type ::=
   | KW_CHAR
   {: RESULT = ScalarType.createCharType(-1); :}
   | KW_DECIMAL LPAREN INTEGER_LITERAL:precision RPAREN
-  {: RESULT = ScalarType.createDecimalType(precision.intValue()); :}
+  {: RESULT = ScalarType.createDecimal_V2Type(precision.intValue()); :}
   | KW_DECIMAL LPAREN INTEGER_LITERAL:precision COMMA INTEGER_LITERAL:scale RPAREN
-  {: RESULT = ScalarType.createDecimalType(precision.intValue(), scale.intValue()); :}
+  {: RESULT = ScalarType.createDecimal_V2Type(precision.intValue(), scale.intValue()); :}","[{'comment': 'in Java, please not use underline in function name', 'commenter': 'imay'}]"
697,be/src/util/string_parser.hpp,"@@ -495,6 +500,193 @@ inline int StringParser::StringParseTraits<__int128>::max_ascii_len() {
     return 39;
 }
 
+inline __int128 StringParser::get_scale_multiplier(int scale) {
+  DCHECK_GE(scale, 0);
+  static const __int128 values[] = {
+      static_cast<__int128>(1ll),
+      static_cast<__int128>(10ll),
+      static_cast<__int128>(100ll),
+      static_cast<__int128>(1000ll),
+      static_cast<__int128>(10000ll),
+      static_cast<__int128>(100000ll),
+      static_cast<__int128>(1000000ll),
+      static_cast<__int128>(10000000ll),
+      static_cast<__int128>(100000000ll),
+      static_cast<__int128>(1000000000ll),
+      static_cast<__int128>(10000000000ll),
+      static_cast<__int128>(100000000000ll),
+      static_cast<__int128>(1000000000000ll),
+      static_cast<__int128>(10000000000000ll),
+      static_cast<__int128>(100000000000000ll),
+      static_cast<__int128>(1000000000000000ll),
+      static_cast<__int128>(10000000000000000ll),
+      static_cast<__int128>(100000000000000000ll),
+      static_cast<__int128>(1000000000000000000ll),
+      static_cast<__int128>(1000000000000000000ll) * 10ll,
+      static_cast<__int128>(1000000000000000000ll) * 100ll,
+      static_cast<__int128>(1000000000000000000ll) * 1000ll,
+      static_cast<__int128>(1000000000000000000ll) * 10000ll,
+      static_cast<__int128>(1000000000000000000ll) * 100000ll,
+      static_cast<__int128>(1000000000000000000ll) * 1000000ll,
+      static_cast<__int128>(1000000000000000000ll) * 10000000ll,
+      static_cast<__int128>(1000000000000000000ll) * 100000000ll,
+      static_cast<__int128>(1000000000000000000ll) * 1000000000ll,
+      static_cast<__int128>(1000000000000000000ll) * 10000000000ll,
+      static_cast<__int128>(1000000000000000000ll) * 100000000000ll,
+      static_cast<__int128>(1000000000000000000ll) * 1000000000000ll,
+      static_cast<__int128>(1000000000000000000ll) * 10000000000000ll,
+      static_cast<__int128>(1000000000000000000ll) * 100000000000000ll,
+      static_cast<__int128>(1000000000000000000ll) * 1000000000000000ll,
+      static_cast<__int128>(1000000000000000000ll) * 10000000000000000ll,
+      static_cast<__int128>(1000000000000000000ll) * 100000000000000000ll,
+      static_cast<__int128>(1000000000000000000ll) * 100000000000000000ll * 10ll,
+      static_cast<__int128>(1000000000000000000ll) * 100000000000000000ll * 100ll,
+      static_cast<__int128>(1000000000000000000ll) * 100000000000000000ll * 1000ll};
+  if (scale >= 0 && scale < 39) return values[scale];
+  return -1;  // Overflow
+}
+
+inline __int128 StringParser::string_to_decimal(const char* s, int len,
+	int type_precision, int type_scale, ParseResult* result) {
+  // Special cases:","[{'comment': 'you should change indent width to 4 spaces', 'commenter': 'imay'}]"
697,be/src/runtime/raw_value.h,"@@ -406,6 +420,10 @@ inline uint32_t RawValue::zlib_crc32(const void* v, const TypeDescriptor& type,
         seed = HashUtil::zlib_crc_hash(&int_val, sizeof(int_val), seed);
         return HashUtil::zlib_crc_hash(&frac_val, sizeof(frac_val), seed);
     }
+
+    case TYPE_DECIMAL_V2: {","[{'comment': 'you should keep this same with TYPE_DECIMAL, because we use this to distribute data', 'commenter': 'imay'}]"
697,be/src/common/daemon.cpp,"@@ -182,6 +183,7 @@ void init_daemon(int argc, char** argv, const std::vector<StorePath>& paths) {
     EncryptionFunctions::init();
     TimestampFunctions::init();
     DecimalOperators::init();
+    Decimal_V2Operators::init();","[{'comment': 'no underline in class name', 'commenter': 'imay'}]"
697,be/src/exec/olap_rewrite_node.cpp,"@@ -179,6 +183,24 @@ bool OlapRewriteNode::copy_one_row(TupleRow* src_row, Tuple* tuple,
             }
             break;
         }
+        case TPrimitiveType::DECIMALV2: {
+            DecimalV2Value* dec_val = (DecimalV2Value*)src_value;
+            DecimalV2Value* dst_val = (DecimalV2Value*)tuple->get_slot(slot_desc->tuple_offset());
+            if (dec_val->scale() > column_type.scale) {","[{'comment': 'can you can `scale` from the DecimalV2', 'commenter': 'imay'}, {'comment': 'Add greater_than_scale function for this', 'commenter': 'lide-reed'}]"
697,be/src/exec/olap_scanner.cpp,"@@ -385,6 +385,15 @@ void OlapScanner::_convert_row_to_tuple(Tuple* tuple) {
             *slot = DecimalValue(int_value, frac_value);
             break;
         }
+        case TYPE_DECIMALV2: {
+            DecimalV2Value *slot = tuple->get_decimalv2_slot(slot_desc->tuple_offset());
+
+            // TODO(lingbin): should remove this assign, use set member function","[{'comment': 'unmatched comment?', 'commenter': 'imay'}]"
697,be/src/exec/olap_scanner.cpp,"@@ -385,6 +385,15 @@ void OlapScanner::_convert_row_to_tuple(Tuple* tuple) {
             *slot = DecimalValue(int_value, frac_value);
             break;
         }
+        case TYPE_DECIMALV2: {
+            DecimalV2Value *slot = tuple->get_decimalv2_slot(slot_desc->tuple_offset());
+
+            // TODO(lingbin): should remove this assign, use set member function
+            int64_t int_value = *(int64_t*)(ptr);
+            int32_t frac_value = *(int32_t*)(ptr + sizeof(int64_t));
+            *slot = DecimalV2Value(int_value, frac_value);","[{'comment': 'can you add a `from_olap_decimal` function to avoid copy?', 'commenter': 'imay'}]"
697,be/src/runtime/decimalv2_value.cpp,"@@ -0,0 +1,415 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""runtime/decimalv2_value.h""
+#include ""util/string_parser.hpp""
+
+#include <algorithm>
+#include <iostream>
+#include <utility>
+
+namespace doris {
+
+const char* DecimalV2Value::_s_llvm_class_name = ""class.doris::DecimalV2Value"";
+
+static inline int128_t abs(const int128_t& x) { return (x < 0) ? -x : x; }
+
+// x>=0 && y>=0
+static int do_add(int128_t x, int128_t y, int128_t* result) {
+    int error = E_DEC_OK;
+    if (DecimalV2Value::MAX_DECIMAL_VALUE - x >= y) {
+        *result = x + y;
+    } else {
+        *result = DecimalV2Value::MAX_DECIMAL_VALUE;
+        error = E_DEC_OVERFLOW;
+        LOG(INFO) << ""overflow (x="" << x << "", y="" << y << "")"";
+    }
+    return error;
+}
+
+// x>=0 && y>=0
+static int do_sub(int128_t x, int128_t y, int128_t* result) {
+    int error = E_DEC_OK;
+    *result = x - y;
+    return error;
+}
+
+// clear leading zero for __int128
+static int clz128(unsigned __int128 v) {
+  if (v == 0) return 128;","[{'comment': 'indent', 'commenter': 'imay'}]"
697,be/src/runtime/decimalv2_value.cpp,"@@ -0,0 +1,415 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""runtime/decimalv2_value.h""
+#include ""util/string_parser.hpp""
+
+#include <algorithm>
+#include <iostream>
+#include <utility>
+
+namespace doris {
+
+const char* DecimalV2Value::_s_llvm_class_name = ""class.doris::DecimalV2Value"";
+
+static inline int128_t abs(const int128_t& x) { return (x < 0) ? -x : x; }
+
+// x>=0 && y>=0
+static int do_add(int128_t x, int128_t y, int128_t* result) {
+    int error = E_DEC_OK;
+    if (DecimalV2Value::MAX_DECIMAL_VALUE - x >= y) {
+        *result = x + y;
+    } else {
+        *result = DecimalV2Value::MAX_DECIMAL_VALUE;
+        error = E_DEC_OVERFLOW;
+        LOG(INFO) << ""overflow (x="" << x << "", y="" << y << "")"";
+    }
+    return error;
+}
+
+// x>=0 && y>=0
+static int do_sub(int128_t x, int128_t y, int128_t* result) {
+    int error = E_DEC_OK;
+    *result = x - y;
+    return error;
+}
+
+// clear leading zero for __int128
+static int clz128(unsigned __int128 v) {
+  if (v == 0) return 128;
+  unsigned __int128 shifted = v >> 64;
+  if (shifted != 0) {
+    return __builtin_clzll(shifted);
+  } else {
+    return __builtin_clzll(v) + 64;
+  }
+}
+
+// x>0 && y>0
+static int do_mul(int128_t x, int128_t y, int128_t* result) {
+    int error = E_DEC_OK;
+
+    // The bits of result as following is 120
+    // clz128((MAX_INT_VALUE * ONE_BILLION + MAX_FRAC_VALUE) * ONE_BILLION) = 8
+    // The bits range of m * n is in (m+n-1 --> m+n)
+    int bits = 128 + 128 - clz128(x) - clz128(y); 
+    if (bits > (120 + 1)) {
+        *result = DecimalV2Value::MAX_DECIMAL_VALUE;
+        LOG(INFO) << ""overflow (x="" << x << "", y="" << y << "")"";
+        error = E_DEC_OVERFLOW;
+        return error;
+    }
+
+    int128_t product = x * y;
+    *result = product / DecimalV2Value::ONE_BILLION;
+
+    // overflow
+    if (*result > DecimalV2Value::MAX_DECIMAL_VALUE) {
+        *result = DecimalV2Value::MAX_DECIMAL_VALUE;
+        LOG(INFO) << ""overflow (x="" << x << "", y="" << y << "")"";
+        error = E_DEC_OVERFLOW;
+        return error;
+    }
+
+    // truncate with round
+    int128_t remainder = product % DecimalV2Value::ONE_BILLION;
+    if (remainder != 0) {
+        error = E_DEC_TRUNCATED;
+        if (remainder >= (DecimalV2Value::ONE_BILLION >> 1)) {
+            *result += 1;
+        }
+        LOG(INFO) << ""truncate (x="" << x << "", y="" << y << "")"" << "", result="" << *result;
+    }
+
+    return error;
+}
+
+// x>0 && y>0
+static int do_div(int128_t x, int128_t y, int128_t* result) {
+    int error = E_DEC_OK;
+    int128_t dividend = x * DecimalV2Value::ONE_BILLION;
+    *result = dividend / y;
+
+    // overflow
+    int128_t remainder = dividend % y;
+    if (remainder != 0) {
+        error = E_DEC_TRUNCATED;
+        if (remainder >= (y >> 1)) {
+            *result += 1;
+        }
+        LOG(INFO) << ""truncate (x="" << x << "", y="" << y << "")"" << "", result="" << *result;
+    }
+
+    return error;
+}
+
+// x>0 && y>0
+static int do_mod(int128_t x, int128_t y, int128_t* result) {
+    int error = E_DEC_OK;
+    *result = x % y;
+    return error;
+}
+
+DecimalV2Value operator+(const DecimalV2Value& v1, const DecimalV2Value& v2) {
+    int128_t result;
+    int128_t x = v1.value();
+    int128_t y = v2.value();
+    if (x == 0) {
+       result = y;
+    } else if (y == 0) {
+       result = x;
+    } else if (x > 0) {
+        if (y > 0) {
+            do_add(x, y, &result);
+        } else {
+            do_sub(x, -y, &result);
+        }
+    } else { // x < 0
+        if (y > 0) {
+            do_sub(y, -x, &result);
+        } else {
+            do_add(-x, -y, &result);
+            result = -result;
+        }
+    }
+
+    return DecimalV2Value(result);
+}
+
+DecimalV2Value operator-(const DecimalV2Value& v1, const DecimalV2Value& v2) {
+    int128_t result;
+    int128_t x = v1.value();
+    int128_t y = v2.value();
+    if (x == 0) {
+       result = -y;
+    } else if (y == 0) {
+       result = x;
+    } else if (x > 0) {
+        if (y > 0) {
+            do_sub(x, y, &result);
+        } else {
+            do_add(x, -y, &result);
+        }
+    } else { // x < 0
+        if (y > 0) {
+            do_add(-x, y, &result);
+            result = -result;
+        } else {
+            do_sub(-x, -y, &result);
+            result = -result;
+        }
+    }
+
+    return DecimalV2Value(result);
+}
+
+DecimalV2Value operator*(const DecimalV2Value& v1, const DecimalV2Value& v2){
+    int128_t result;
+    int128_t x = v1.value();
+    int128_t y = v2.value();
+
+    if (x == 0 || y == 0) return DecimalV2Value(0);
+
+    bool is_positive = (x > 0 && y > 0) || (x < 0 && y < 0);
+
+    do_mul(abs(x), abs(y), &result);
+
+    if (!is_positive) result = -result;
+
+    return DecimalV2Value(result);
+}
+
+DecimalV2Value operator/(const DecimalV2Value& v1, const DecimalV2Value& v2){
+    int128_t result;
+    int128_t x = v1.value();
+    int128_t y = v2.value();
+
+    //todo: return 0 for divide zero 
+    if (x == 0 || y == 0) return DecimalV2Value(0);
+    bool is_positive = (x > 0 && y > 0) || (x < 0 && y < 0);
+    do_div(abs(x), abs(y), &result);
+
+    if (!is_positive) result = -result;
+
+    return DecimalV2Value(result);
+}
+
+DecimalV2Value operator%(const DecimalV2Value& v1, const DecimalV2Value& v2){
+    int128_t result;
+    int128_t x = v1.value();
+    int128_t y = v2.value();
+
+    //todo: return 0 for divide zero 
+    if (x == 0 || y == 0) return DecimalV2Value(0);
+
+    bool is_positive = (x > 0 && y > 0) || (x < 0 && y < 0);
+    do_mod(abs(x), abs(y), &result);
+
+    if (!is_positive) result = -result;
+
+    return DecimalV2Value(result);
+}
+
+std::ostream& operator<<(std::ostream& os, DecimalV2Value const& decimal_value) {
+    return os << decimal_value.to_string();
+}
+
+std::istream& operator>>(std::istream& ism, DecimalV2Value& decimal_value) {
+    std::string str_buff;
+    ism >> str_buff;
+    decimal_value.parse_from_str(str_buff.c_str(), str_buff.size());
+    return ism;
+}
+
+DecimalV2Value operator-(const DecimalV2Value& v) {
+    return DecimalV2Value(-v.value());
+}
+
+DecimalV2Value& DecimalV2Value::operator+=(const DecimalV2Value& other) {
+    *this = *this + other;
+    return *this;
+}
+
+int DecimalV2Value::parse_from_str(const char* decimal_str, int32_t length) {
+    int32_t error = E_DEC_OK;
+    StringParser::ParseResult result = StringParser::PARSE_SUCCESS;
+    
+    _value = StringParser::string_to_decimal(decimal_str, length, 
+		    PRECISION, SCALE, &result);
+
+    if (result != StringParser::PARSE_SUCCESS) { 
+       error = E_DEC_BAD_NUM;
+    }
+    return error;
+}
+
+std::string DecimalV2Value::to_string(int round_scale) const {
+  if (_value == 0) return std::string(1, '0');","[{'comment': 'indent', 'commenter': 'imay'}]"
697,be/src/runtime/decimalv2_value.h,"@@ -0,0 +1,342 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#ifndef DORIS_BE_SRC_RUNTIME_DECIMAL_VALUE_H
+#define DORIS_BE_SRC_RUNTIME_DECIMAL_VALUE_H","[{'comment': '#pragma once', 'commenter': 'imay'}]"
697,fe/src/main/java/org/apache/doris/catalog/Column.java,"@@ -255,7 +255,11 @@ public void checkSchemaChangeAllowed(Column other) throws DdlException {
     public String toSql() {
         StringBuilder sb = new StringBuilder();
         sb.append(""`"").append(name).append(""` "");
-        sb.append(type.toSql()).append("" "");
+        String typeStr = type.toSql();
+        if (type.getPrimitiveType() == PrimitiveType.DECIMALV2) {","[{'comment': 'Why not implement this if{} in type.toSql()?', 'commenter': 'morningman'}]"
697,be/src/udf/udf.h,"@@ -687,6 +689,42 @@ struct DecimalVal : public AnyVal {
 
 };
 
+struct DecimalV2Val : public AnyVal {
+
+    __int128 val;
+
+    // Default value is zero
+    DecimalV2Val() : val(0) {}
+
+    const __int128& value() const { return val; }
+
+    DecimalV2Val(__int128 value) : val(value) {}
+
+    static DecimalV2Val null() {
+        DecimalV2Val result;
+        result.is_null = true;
+        return result;
+    }
+    
+    void set_to_zero() {
+        val = 0;
+    }
+    
+    void set_to_abs_value() {
+        if (val < 0) val = -val;
+    }
+
+    bool operator==(const DecimalV2Val& other) const {
+        return val == other.value();","[{'comment': 'you must check if this is null, then check val field', 'commenter': 'imay'}]"
703,fe/src/main/java/org/apache/doris/analysis/CreateRoutineLoadStmt.java,"@@ -326,6 +335,16 @@ private void checkKafkaCustomProperties() throws AnalysisException {
                 }
             }
         }
+        // check offsets
+        // Todo(ml)
+        final String kafkaOffsetsString = customProperties.get(KAFKA_OFFSETS_PROPERTY);
+        if (kafkaOffsetsString != null) {
+            kafkaOffsets = new ArrayList<>();
+            String[] kafkaOffsetsStringList = customProperties.get(KAFKA_OFFSETS_PROPERTY).split("","");
+            for (String s : kafkaOffsetsStringList) {
+                kafkaOffsets.add(Long.valueOf(s));","[{'comment': 'You should check the number format. And also check whether the number of specified offsets equals to number of specified partitions', 'commenter': 'morningman'}, {'comment': 'I will do it later', 'commenter': 'EmmyMiao87'}]"
703,fe/src/main/java/org/apache/doris/load/routineload/KafkaProgress.java,"@@ -57,7 +58,7 @@ public void setPartitionIdToOffset(Map<Integer, Long> partitionIdToOffset) {
     public void update(RoutineLoadProgress progress) {
         KafkaProgress newProgress = (KafkaProgress) progress;
         newProgress.getPartitionIdToOffset().entrySet().parallelStream()
-                .forEach(entity -> partitionIdToOffset.put(entity.getKey(), entity.getValue()));
+                .forEach(entity -> partitionIdToOffset.put(entity.getKey(), entity.getValue() + 1));","[{'comment': 'Why + 1？', 'commenter': 'morningman'}, {'comment': 'The offset of txn is the end offset while the kafka offset of progress is begin offset . The next task will read data from this begin offset. ', 'commenter': 'EmmyMiao87'}]"
703,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoadJob.java,"@@ -488,72 +502,113 @@ public void plan() throws UserException {
     }
 
     @Override
-    public void beforeAborted(TransactionState txnState, TransactionState.TxnStatusChangeReason txnStatusChangeReason)
+    public void beforeAborted(TransactionState txnState, String txnStatusChangeReason)
             throws AbortTransactionException {
         readLock();
         try {
-            if (txnStatusChangeReason != null) {
-                switch (txnStatusChangeReason) {
-                    case TIMEOUT:
-                    default:
-                        String taskId = txnState.getLabel();
-                        if (routineLoadTaskInfoList.parallelStream().anyMatch(entity -> entity.getId().equals(taskId))) {
-                            throw new AbortTransactionException(
-                                    ""there are task "" + taskId + "" related to this txn, ""
-                                            + ""txn could not be abort"", txnState.getTransactionId());
-                        }
-                        break;
-                }
+            String taskId = txnState.getLabel();
+            if (routineLoadTaskInfoList.parallelStream().anyMatch(entity -> entity.getId().toString().equals(taskId))) {
+                LOG.debug(""there are a txn of routine load task will be aborted"");","[{'comment': '""there are a txn"" ??', 'commenter': 'morningman'}, {'comment': 'And better to add taskid in log', 'commenter': 'morningman'}]"
706,fe/src/main/java/org/apache/doris/optimizer/operator/OptPhysicalHashJoin.java,"@@ -0,0 +1,8 @@
+package org.apache.doris.optimizer.operator;","[{'comment': 'no license header\r\n', 'commenter': 'imay'}]"
706,fe/src/main/java/org/apache/doris/optimizer/OptGroup.java,"@@ -42,4 +51,32 @@ public void addMExpr(MultiExpression mExpr) {
     public String debugString() {
         return """";
     }
+
+    public boolean isImplemented() {","[{'comment': 'setter and getter should be put one line, this can make more code shown in one screen\r\n\r\n', 'commenter': 'imay'}]"
706,fe/src/main/java/org/apache/doris/optimizer/Optimizer.java,"@@ -0,0 +1,43 @@
+package org.apache.doris.optimizer;","[{'comment': 'package under license header', 'commenter': 'imay'}]"
706,fe/src/main/java/org/apache/doris/optimizer/search/TaskStateMachine.java,"@@ -0,0 +1,98 @@
+package org.apache.doris.optimizer.search;
+
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+import com.google.common.base.Preconditions;
+
+/**
+ * Base class for Cascades search task.
+ */
+abstract public class TaskStateMachine {","[{'comment': 'OptTask is better', 'commenter': 'imay'}]"
706,fe/src/main/java/org/apache/doris/optimizer/search/TaskState.java,"@@ -0,0 +1,96 @@
+package org.apache.doris.optimizer.search;","[{'comment': 'package under license header', 'commenter': 'imay'}]"
706,fe/src/main/java/org/apache/doris/optimizer/search/TaskState.java,"@@ -0,0 +1,96 @@
+package org.apache.doris.optimizer.search;
+
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+/**
+ * Base class for task StateMacheine's state.
+ */
+public abstract class TaskState {
+
+    // The followings are all events that StateMachine can use.
+
+    // Current TaskStateMachine is suspended and wait to be resumed
+    // by child TaskStateMachine.
+    private boolean isSuspending;
+
+    // TaskStateMachine has been finished.
+    private boolean isFinished;
+
+    // TaskStateMachine resume from Suspending.
+    private boolean isResuming;
+
+    // TaskStateMachine is running.
+    private boolean isRunning;
+
+    public TaskState() {
+        resetState();
+    }
+
+    public abstract void handle(SchedulerContext sContext);
+
+    public boolean isSuspending() {
+        return isSuspending;
+    }
+
+    public boolean isFinished() {
+        return isFinished;
+    }
+
+    public boolean isResuming() {
+        return isResuming;
+    }
+
+    public boolean isRunning() {","[{'comment': 'put these one line', 'commenter': 'imay'}]"
706,fe/src/main/java/org/apache/doris/optimizer/search/RequestProperty.java,"@@ -0,0 +1,4 @@
+package org.apache.doris.optimizer.search;","[{'comment': 'no license header', 'commenter': 'imay'}]"
706,fe/src/main/java/org/apache/doris/optimizer/rule/transformation/JoinAssociativityRule.java,"@@ -0,0 +1,54 @@
+package org.apache.doris.optimizer.rule.transformation;","[{'comment': 'no license header', 'commenter': 'imay'}]"
706,fe/src/main/java/org/apache/doris/optimizer/OptExpression.java,"@@ -51,6 +51,12 @@ public OptExpression(OptOperator op, OptExpression... inputs) {
         this.inputs = Lists.newArrayList(inputs);
     }
 
+    public OptExpression(OptOperator op, OptExpression firstChild, OptExpression secondChild) {","[{'comment': 'this `public OptExpression(OptOperator op, OptExpression... inputs) {` function can cover this funciton', 'commenter': 'imay'}, {'comment': 'Ok, i will remove it.', 'commenter': 'chenhao7253886'}]"
706,fe/src/main/java/org/apache/doris/optimizer/search/SchedulerContext.java,"@@ -0,0 +1,27 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.optimizer.search;
+
+import org.apache.doris.optimizer.OptMemo;
+
+public interface SchedulerContext {","[{'comment': 'I think there is no need to define an interface', 'commenter': 'imay'}, {'comment': 'Ok, i will delete it.', 'commenter': 'chenhao7253886'}]"
706,fe/src/main/java/org/apache/doris/optimizer/search/SchedulerContextImp.java,"@@ -0,0 +1,47 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.optimizer.search;
+
+import org.apache.doris.optimizer.OptGroup;
+import org.apache.doris.optimizer.OptMemo;
+
+public class SchedulerContextImp implements SchedulerContext {","[{'comment': 'And I think `SearchContext` is a better name', 'commenter': 'imay'}, {'comment': 'OK', 'commenter': 'chenhao7253886'}]"
718,be/src/runtime/tablet_writer_mgr.h,"@@ -92,6 +96,15 @@ class TabletWriterMgr {
         TabletsChannelKeyHasher> _tablets_channels;
 
     Cache* _lastest_success_channel = nullptr;
+
+    std::unordered_map<TabletsChannelKey, time_t,TabletsChannelKeyHasher> _key_time_map;","[{'comment': ""I think it's better to add a `last_updated_time` member in `TabletsChannel `, then there is no need to maintain two map.\r\n\r\nAnd when there is a new request referring to this tablets, then update this field.\r\n\r\nThen we can compare current wall time to this `last_updated_time` to see if it has been exceeded the config timeout.\r\n\r\nAnd I think this timeout can be some shorter time, such as 10 minutes"", 'commenter': 'imay'}, {'comment': 'OK', 'commenter': 'kangkaisen'}]"
718,be/src/runtime/tablet_writer_mgr.cpp,"@@ -311,6 +313,44 @@ Status TabletWriterMgr::cancel(const PTabletWriterCancelRequest& params) {
     {
         std::lock_guard<std::mutex> l(_lock);
         _tablets_channels.erase(key);
+        _key_time_map.erase(key);
+    }
+    return Status::OK;
+}
+
+Status TabletWriterMgr::start_bg_worker() {
+    _tablets_channel_clean_thread = std::thread([this] {
+                _tablets_channel_clean_thread_callback(nullptr);
+            });","[{'comment': 'you need to call `_tablets_channel_clean_thread.detach()` to avoid crush when process exits.', 'commenter': 'imay'}, {'comment': 'OK', 'commenter': 'kangkaisen'}]"
718,be/src/runtime/tablet_writer_mgr.cpp,"@@ -311,6 +313,44 @@ Status TabletWriterMgr::cancel(const PTabletWriterCancelRequest& params) {
     {
         std::lock_guard<std::mutex> l(_lock);
         _tablets_channels.erase(key);
+        _key_time_map.erase(key);
+    }
+    return Status::OK;
+}
+
+Status TabletWriterMgr::start_bg_worker() {
+    _tablets_channel_clean_thread = std::thread([this] {
+                _tablets_channel_clean_thread_callback(nullptr);","[{'comment': '```suggestion\r\n        uint32_t interval = 60;\r\n        while (true) {\r\n            _start_tablets_channel_clean();\r\n            sleep(interval);\r\n        }\r\n```\r\nI think 600 is a bit long', 'commenter': 'imay'}, {'comment': 'OK', 'commenter': 'kangkaisen'}]"
718,be/src/runtime/tablet_writer_mgr.cpp,"@@ -315,6 +324,48 @@ Status TabletWriterMgr::cancel(const PTabletWriterCancelRequest& params) {
     return Status::OK;
 }
 
+Status TabletWriterMgr::start_bg_worker() {
+    _tablets_channel_clean_thread = std::thread(
+         [this] {
+             _tablets_channel_clean_thread_callback(nullptr);
+         });
+    _tablets_channel_clean_thread.detach();
+    return Status::OK;
+}
+
+void* TabletWriterMgr::_tablets_channel_clean_thread_callback(void* arg) {","[{'comment': 'you can remove this function and put this logical to lambda in `start_bg_worker`.', 'commenter': 'imay'}, {'comment': 'done', 'commenter': 'kangkaisen'}]"
718,be/src/runtime/tablet_writer_mgr.cpp,"@@ -315,6 +324,45 @@ Status TabletWriterMgr::cancel(const PTabletWriterCancelRequest& params) {
     return Status::OK;
 }
 
+Status TabletWriterMgr::start_bg_worker() {
+    _tablets_channel_clean_thread = std::thread(
+        [this] {
+            #ifdef GOOGLE_PROFILER
+                ProfilerRegisterThread();
+            #endif
+
+            uint32_t interval = 60;
+            while (true) {
+                _start_tablets_channel_clean();
+                sleep(interval);
+            }
+        });
+    _tablets_channel_clean_thread.detach();
+    return Status::OK;
+}
+
+Status TabletWriterMgr::_start_tablets_channel_clean() {
+    const int32_t max_alive_time = config::streaming_load_rpc_max_alive_time_sec;
+    time_t now = time(nullptr);
+    {
+        std::lock_guard<std::mutex> l(_lock);
+        std::vector<TabletsChannelKey> need_delete_keys;
+
+        for (auto& kv : _tablets_channels) {
+            time_t last_updated_time = kv.second->last_updated_time();
+            if (difftime(now, last_updated_time) >= max_alive_time) {
+                need_delete_keys.emplace_back(kv.first);
+            }
+        }
+
+        for(auto key: need_delete_keys) {","[{'comment': '```suggestion\r\n        for(auto& key: need_delete_keys) {\r\n```', 'commenter': 'imay'}]"
729,be/src/olap/rowset/alpha_rowset_reader.cpp,"@@ -120,9 +180,9 @@ int64_t AlphaRowsetReader::get_filtered_rows() {
 
 OLAPStatus AlphaRowsetReader::_get_next_block(size_t pos, RowBlock** row_block) {
      // get next block
-     ColumnData* column_data = _column_datas[pos].get();
-     OLAPStatus status = column_data->get_next_block(row_block);
-     if (status != OLAP_SUCCESS) {
+    ColumnData* column_data = _column_datas[pos].get();","[{'comment': 'no need to shared_ptr', 'commenter': 'chaoyli'}]"
729,be/src/olap/rowset/alpha_rowset_reader.cpp,"@@ -48,14 +55,67 @@ OLAPStatus AlphaRowsetReader::init(RowsetReaderContext* read_context) {
 }
 
 bool AlphaRowsetReader::has_next() {
-    for (auto& row_block : _row_blocks) {
+    if (_is_first_read) {","[{'comment': 'May be in init() will be ok?', 'commenter': 'chaoyli'}]"
729,be/src/olap/rowset/alpha_rowset_reader.cpp,"@@ -324,13 +325,13 @@ OLAPStatus AlphaRowsetReader::_init_column_datas(RowsetReaderContext* read_conte
 
 OLAPStatus AlphaRowsetReader::_refresh_next_block(size_t pos, RowBlock** next_block) {
     ColumnData* column_data = _column_datas[pos].get();
-    OLAPStatus status = column_data->get_next_block(next_block);
-    if (status == OLAP_ERR_DATA_EOF && _key_range_size > 0) {
+    if (_key_range_size > 0) {
         // currently, SegmentReader can only support filter one key range a time
         // use the next predicate range to get data from segment here
         _key_range_indices[pos]++;
+        OLAPStatus status = OLAP_SUCCESS;
         while (_key_range_indices[pos] < _key_range_size) {
-            status = column_data->prepare_block_read(_current_read_context->lower_bound_keys->at(_key_range_indices[pos]),
+            OLAPStatus status = column_data->prepare_block_read(_current_read_context->lower_bound_keys->at(_key_range_indices[pos]),","[{'comment': 'redundant defintion', 'commenter': 'chaoyli'}]"
729,be/src/olap/rowset/alpha_rowset_reader.cpp,"@@ -324,13 +325,13 @@ OLAPStatus AlphaRowsetReader::_init_column_datas(RowsetReaderContext* read_conte
 
 OLAPStatus AlphaRowsetReader::_refresh_next_block(size_t pos, RowBlock** next_block) {
     ColumnData* column_data = _column_datas[pos].get();","[{'comment': 'use _column_datas[pos] instead', 'commenter': 'chaoyli'}]"
729,be/src/olap/rowset/alpha_rowset_reader.cpp,"@@ -120,9 +141,8 @@ int64_t AlphaRowsetReader::get_filtered_rows() {
 
 OLAPStatus AlphaRowsetReader::_get_next_block(size_t pos, RowBlock** row_block) {
      // get next block","[{'comment': 'You can merge _get_next_block and _refresh_next_block to only on function.', 'commenter': 'chaoyli'}]"
729,be/src/olap/rowset/alpha_rowset_reader.cpp,"@@ -120,9 +141,8 @@ int64_t AlphaRowsetReader::get_filtered_rows() {
 
 OLAPStatus AlphaRowsetReader::_get_next_block(size_t pos, RowBlock** row_block) {
      // get next block
-     ColumnData* column_data = _column_datas[pos].get();
-     OLAPStatus status = column_data->get_next_block(row_block);
-     if (status != OLAP_SUCCESS) {
+    OLAPStatus status = _column_datas[pos]->get_next_block(row_block);
+    if (status != OLAP_SUCCESS) {","[{'comment': 'if is not necessary.', 'commenter': 'chaoyli'}]"
729,be/src/olap/rowset/alpha_rowset_reader.cpp,"@@ -70,11 +91,11 @@ OLAPStatus AlphaRowsetReader::next(RowCursor** row) {
 
 OLAPStatus AlphaRowsetReader::next_block(std::shared_ptr<RowBlock> block) {
     size_t num_rows_in_block = 0;
-    while (has_next() && block->pos() < _num_rows_per_row_block) {
+    while (block->pos() < _num_rows_per_row_block) {
         RowCursor* row_cursor = nullptr;
         OLAPStatus status = next(&row_cursor);","[{'comment': 'OLAPStatus status = next(&_row_cursor)\r\nif (status == OLAP_ERR_DATA_EOF &&  block->pos() > 0) {\r\n    break;\r\n}\r\nreturn status;', 'commenter': 'chaoyli'}]"
729,be/src/olap/rowset/alpha_rowset_reader.cpp,"@@ -190,35 +191,32 @@ OLAPStatus AlphaRowsetReader::_get_next_row_for_singleton_rowset(RowCursor** row
     }
     *row = min_row;
     _row_blocks[min_index]->pos_inc();
-    if (!_row_blocks[min_index]->has_remaining()) {
-        OLAPStatus status = _get_next_block(min_index, &_row_blocks[min_index]);
-        if (status == OLAP_ERR_DATA_EOF) {
-            _row_blocks[min_index] = nullptr;
-            return OLAP_SUCCESS;
-        } else if (status != OLAP_SUCCESS) {
-            LOG(WARNING) << ""_get_next_block failed, status:"" << status;
-            return status;
-        }
-    }
     return OLAP_SUCCESS;
 }
 
 OLAPStatus AlphaRowsetReader::_get_next_row_for_cumulative_rowset(RowCursor** row) {
     size_t pos = 0;
-    (*row) = new RowCursor();
-    (*row)->init(_segment_groups[0]->get_tablet_schema());
-    OLAPStatus status = _get_next_not_filtered_row(pos, row);
-    _row_blocks[pos]->pos_inc();
+    RowCursor* current_row = _row_cursors[pos];
+    OLAPStatus status = OLAP_SUCCESS;
     if (!_row_blocks[pos]->has_remaining()) {
-        OLAPStatus status = _get_next_block(pos, &_row_blocks[pos]);
+        status = _get_next_block(pos, &_row_blocks[pos]);","[{'comment': 'OLAP_ERR_DATA_EOF indicates _row_blocks[pos] is nullptr or not', 'commenter': 'chaoyli'}]"
741,be/src/olap/schema_change.cpp,"@@ -645,18 +645,21 @@ SchemaChangeDirectly::SchemaChangeDirectly(
         _tablet(tablet),
         _row_block_changer(row_block_changer),
         _row_block_allocator(NULL),
-        _src_cursor(NULL) {}
+        _src_cursor(NULL),
+        _dst_cursor(NULL) { }","[{'comment': 'nullptr', 'commenter': 'kangpinghuang'}]"
741,be/src/olap/schema_change.cpp,"@@ -689,6 +692,19 @@ bool SchemaChangeDirectly::process(RowsetReaderSharedPtr rowset_reader, RowsetWr
         }
     }
 
+    if (NULL == _dst_cursor) {","[{'comment': '_dst_cursor == nullptr', 'commenter': 'kangpinghuang'}]"
741,be/src/olap/schema_change.cpp,"@@ -689,6 +692,19 @@ bool SchemaChangeDirectly::process(RowsetReaderSharedPtr rowset_reader, RowsetWr
         }
     }
 
+    if (NULL == _dst_cursor) {
+        _dst_cursor = new(nothrow) RowCursor();
+        if (NULL == _dst_cursor) {","[{'comment': '_dst_cursor == nullptr', 'commenter': 'kangpinghuang'}]"
751,be/src/olap/rowset/alpha_rowset.cpp,"@@ -149,32 +149,15 @@ void AlphaRowset::set_version_and_version_hash(Version version,  VersionHash ver
         return;
     }
 
-    _is_pending_rowset = false;
     AlphaRowsetMetaSharedPtr alpha_rowset_meta =
             std::dynamic_pointer_cast<AlphaRowsetMeta>(_rowset_meta);","[{'comment': 'may be better to clear_load_id() ？', 'commenter': 'chaoyli'}]"
764,be/src/olap/rowset/alpha_rowset_reader.cpp,"@@ -54,65 +54,46 @@ OLAPStatus AlphaRowsetReader::init(RowsetReaderContext* read_context) {
     _dst_cursor = new(std::nothrow)RowCursor();
     _dst_cursor->init(*(_current_read_context->tablet_schema));
     OLAPStatus status = _init_column_datas(read_context);
-    return status;
-}
-
-bool AlphaRowsetReader::has_next() {
-    bool next_flag = false;
-    for (int i = 0; i < _column_datas.size(); ++i) {
-        auto& row_block = _row_blocks[i];
-        if (row_block != nullptr) {
-            if (row_block->has_remaining()) {
-                next_flag = true;
-            } else {
-                OLAPStatus status = _get_next_block(i, &_row_blocks[i]);
-                if (status == OLAP_ERR_DATA_EOF) {
-                    _row_blocks[i] = nullptr;
-                    continue;
-                } else if (status != OLAP_SUCCESS) {
-                    LOG(WARNING) << ""_get_next_block failed, status:"" << status;
-                    return false;
-                } else {
-                    if (_row_blocks[i] != nullptr && _row_blocks[i]->has_remaining()) {
-                        next_flag = true;
-                    }
-                }
-            }
+    if (!_is_cumulative_rowset) {
+        _read_block.reset(new RowBlock(_current_read_context->tablet_schema));
+        if (_read_block == nullptr) {
+            LOG(WARNING) << ""new row block failed in reader"";
+            return OLAP_ERR_MALLOC_ERROR;
         }
-    }
-    return next_flag;
-}
-
-OLAPStatus AlphaRowsetReader::next(RowCursor** row) {
-    OLAPStatus status = OLAP_SUCCESS;
-    if (_is_cumulative_rowset) {
-        status = _get_next_row_for_cumulative_rowset(row);
-    } else {
-        status = _get_next_row_for_singleton_rowset(row);
+        RowBlockInfo block_info;
+        block_info.row_num = _current_read_context->tablet_schema->num_rows_per_row_block();
+        block_info.null_supported = true;
+        _read_block->init(block_info);
+        _dst_cursor = new(std::nothrow)RowCursor();
+        _dst_cursor->init(*(_current_read_context->tablet_schema));
     }
     return status;
 }
 
-OLAPStatus AlphaRowsetReader::next_block(std::shared_ptr<RowBlock> block) {
-    block->clear();
+OLAPStatus AlphaRowsetReader::next_block(RowBlock** block) {
     size_t num_rows_in_block = 0;
-    while (block->pos() < _num_rows_per_row_block) {
-        RowCursor* row_cursor = nullptr;
-        OLAPStatus status = next(&row_cursor);
-        if (status == OLAP_ERR_DATA_EOF && block->pos() > 0) {
-            break;
-        } else if (status != OLAP_SUCCESS) {
-            LOG(WARNING) << ""next block failed.status:"" << status;
-            return status;
+    if (_is_cumulative_rowset) {","[{'comment': ""You can use a function pointer which points to real `next` function, and this pointer can be set in init function. Then  in this function, you don't need this if statement which can reduce CPU execution and  branches."", 'commenter': 'imay'}, {'comment': 'OK, I will add a function pointer to encapsulate it.', 'commenter': 'chaoyli'}]"
764,be/src/olap/rowset/alpha_rowset_reader.cpp,"@@ -54,65 +54,46 @@ OLAPStatus AlphaRowsetReader::init(RowsetReaderContext* read_context) {
     _dst_cursor = new(std::nothrow)RowCursor();
     _dst_cursor->init(*(_current_read_context->tablet_schema));
     OLAPStatus status = _init_column_datas(read_context);
-    return status;
-}
-
-bool AlphaRowsetReader::has_next() {
-    bool next_flag = false;
-    for (int i = 0; i < _column_datas.size(); ++i) {
-        auto& row_block = _row_blocks[i];
-        if (row_block != nullptr) {
-            if (row_block->has_remaining()) {
-                next_flag = true;
-            } else {
-                OLAPStatus status = _get_next_block(i, &_row_blocks[i]);
-                if (status == OLAP_ERR_DATA_EOF) {
-                    _row_blocks[i] = nullptr;
-                    continue;
-                } else if (status != OLAP_SUCCESS) {
-                    LOG(WARNING) << ""_get_next_block failed, status:"" << status;
-                    return false;
-                } else {
-                    if (_row_blocks[i] != nullptr && _row_blocks[i]->has_remaining()) {
-                        next_flag = true;
-                    }
-                }
-            }
+    if (!_is_cumulative_rowset) {","[{'comment': 'Why you check `_is_cumulative_rowset`, and I think you should add comment for it.', 'commenter': 'imay'}, {'comment': 'OK, I have add some comment to elaborate it.', 'commenter': 'chaoyli'}]"
764,be/src/olap/rowset/alpha_rowset_reader.cpp,"@@ -54,66 +48,57 @@ OLAPStatus AlphaRowsetReader::init(RowsetReaderContext* read_context) {
     _dst_cursor = new(std::nothrow)RowCursor();
     _dst_cursor->init(*(_current_read_context->tablet_schema));
     OLAPStatus status = _init_column_datas(read_context);
-    return status;
-}
 
-bool AlphaRowsetReader::has_next() {
-    bool next_flag = false;
-    for (int i = 0; i < _column_datas.size(); ++i) {
-        auto& row_block = _row_blocks[i];
-        if (row_block != nullptr) {
-            if (row_block->has_remaining()) {
-                next_flag = true;
+    Version version = _alpha_rowset_meta->version();
+    _is_singleton_rowset = (version.first == version.second);
+    bool merge = false;
+    /*
+     * For singleton rowset, there exists three situations.
+     *   1. DUP_KEYS tablet has no necessities to merge row in advance.
+     *   2. COMPACTION/CHECKSUM/ALTER_TABLET task has no necessities to
+     *      merge row in advance.
+     *   3. QUERY task will set preaggregation. If preaggregation is
+     *      set to be true, it is necessary to merge row in advance.
+     * For cumulative rowset, there is no necessities to merge row in advance.
+     */
+    if (_is_singleton_rowset) {
+        if (_current_read_context->tablet_schema->keys_type() == DUP_KEYS) {
+            // DUP_KEYS tablet
+            _next_block = &AlphaRowsetReader::_next_block_for_singleton_without_merge;
+        } else {
+            if (_current_read_context->reader_type == READER_QUERY
+                    && _current_read_context->preaggregation) {
+                // QUERY task which set preaggregation to be true.
+                _next_block = &AlphaRowsetReader::_next_block_for_singleton_with_merge;","[{'comment': 'when `preaggregation` is set to true, we read data without merge', 'commenter': 'imay'}, {'comment': 'OK', 'commenter': 'chaoyli'}]"
764,be/src/olap/rowset/alpha_rowset_reader.cpp,"@@ -136,42 +121,82 @@ int64_t AlphaRowsetReader::filtered_rows() {
     return _stats->rows_del_filtered;
 }
 
-OLAPStatus AlphaRowsetReader::_get_next_block(size_t pos, RowBlock** row_block) {
-    // get next block
-    OLAPStatus status = _column_datas[pos]->get_next_block(row_block);
-    if (status == OLAP_ERR_DATA_EOF && _key_range_size > 0) {
-        // reach the end of one predicate
-        // currently, SegmentReader can only support filter one key range a time
-        // refresh the predicate and continue read
-        _key_range_indices[pos]++;
-        OLAPStatus status = OLAP_SUCCESS;
-        while (_key_range_indices[pos] < _key_range_size) {
-            status = _column_datas[pos]->prepare_block_read(
-                    _current_read_context->lower_bound_keys->at(_key_range_indices[pos]),
-                    _current_read_context->is_lower_keys_included->at(_key_range_indices[pos]),
-                    _current_read_context->upper_bound_keys->at(_key_range_indices[pos]),
-                    _current_read_context->is_upper_keys_included->at(_key_range_indices[pos]),
-                    row_block);
-            if (status == OLAP_ERR_DATA_EOF) {
-                _key_range_indices[pos]++;
-                continue;
-            } else if (status != OLAP_SUCCESS) {
-                LOG(WARNING) << ""prepare block read failed"";
-                return status;
-            } else {
-                break;
-            }
+OLAPStatus AlphaRowsetReader::_next_block_for_cumulative_rowset(RowBlock** block) {
+    size_t pos = 0;
+    if (UNLIKELY(_row_blocks.empty() || _row_blocks[pos] == nullptr)) {
+        return OLAP_ERR_DATA_EOF;
+    }
+    OLAPStatus status = OLAP_SUCCESS;
+    if (_row_blocks[pos]->has_remaining()) {
+        *block = _row_blocks[pos];
+    } else {
+        RETURN_NOT_OK(_next_block_for_column_data(pos, &_row_blocks[pos]));
+        *block = _row_blocks[pos];
+    }
+    return status;
+}
+
+OLAPStatus AlphaRowsetReader::_next_block_for_singleton_without_merge(RowBlock** block) {","[{'comment': 'From my point of view, this function can use the same function with `_next_block_for_cumulative_rowset`', 'commenter': 'imay'}, {'comment': 'For singleton rowset, there may be many segment groups.\r\nUpon initialization, many rowblocks will be read in advance.\r\nSo there is necessary to construct a std::vector<RowBlock> _row_blocks to store it.\r\nIf segment groups size is one, the size of std::vector<RowBlock> _row_blocks will to equal one.\r\nUnder this circumstance, and probably this circumstance dominates _next_block_for_cumulative_rowset will be the same with _next_block_for_singleton_without_merge.', 'commenter': 'chaoyli'}, {'comment': 'Why do we read all segment groups when initialization? I think we can read segment group one by one', 'commenter': 'imay'}, {'comment': 'I agree with imay that it is not necessary to read the first block of sement groups when initialization. It can be postponed to the call of _next_block_for_column_data. But I think we can not read segment group one by one when it is the situation for singleton with merge. It can be done for other situations.', 'commenter': 'kangpinghuang'}, {'comment': 'I agree that _next_block_for_cumulative_rowset can be implemented by this function', 'commenter': 'kangpinghuang'}]"
764,be/src/olap/rowset/alpha_rowset_reader.cpp,"@@ -136,42 +121,82 @@ int64_t AlphaRowsetReader::filtered_rows() {
     return _stats->rows_del_filtered;
 }
 
-OLAPStatus AlphaRowsetReader::_get_next_block(size_t pos, RowBlock** row_block) {
-    // get next block
-    OLAPStatus status = _column_datas[pos]->get_next_block(row_block);
-    if (status == OLAP_ERR_DATA_EOF && _key_range_size > 0) {
-        // reach the end of one predicate
-        // currently, SegmentReader can only support filter one key range a time
-        // refresh the predicate and continue read
-        _key_range_indices[pos]++;
-        OLAPStatus status = OLAP_SUCCESS;
-        while (_key_range_indices[pos] < _key_range_size) {
-            status = _column_datas[pos]->prepare_block_read(
-                    _current_read_context->lower_bound_keys->at(_key_range_indices[pos]),
-                    _current_read_context->is_lower_keys_included->at(_key_range_indices[pos]),
-                    _current_read_context->upper_bound_keys->at(_key_range_indices[pos]),
-                    _current_read_context->is_upper_keys_included->at(_key_range_indices[pos]),
-                    row_block);
-            if (status == OLAP_ERR_DATA_EOF) {
-                _key_range_indices[pos]++;
-                continue;
-            } else if (status != OLAP_SUCCESS) {
-                LOG(WARNING) << ""prepare block read failed"";
-                return status;
-            } else {
-                break;
-            }
+OLAPStatus AlphaRowsetReader::_next_block_for_cumulative_rowset(RowBlock** block) {
+    size_t pos = 0;
+    if (UNLIKELY(_row_blocks.empty() || _row_blocks[pos] == nullptr)) {
+        return OLAP_ERR_DATA_EOF;
+    }
+    OLAPStatus status = OLAP_SUCCESS;
+    if (_row_blocks[pos]->has_remaining()) {
+        *block = _row_blocks[pos];
+    } else {
+        RETURN_NOT_OK(_next_block_for_column_data(pos, &_row_blocks[pos]));
+        *block = _row_blocks[pos];
+    }
+    return status;
+}
+
+OLAPStatus AlphaRowsetReader::_next_block_for_singleton_without_merge(RowBlock** block) {
+    // If tablet is a duplicate key tablet, there is no necessity to merge.
+    // If preaggregation is set to be false, there is no necessity to merge.
+    size_t pos = 0;
+    while (pos < _row_blocks.size()) {
+        if (_row_blocks[pos] == nullptr) {","[{'comment': 'There is no need to create so many `_row_blocks`, which is memory consuming. \r\nYou can read data one by one, so one row block is enough', 'commenter': 'imay'}, {'comment': 'In most cases, the size of _row_blocks will be equal to one.\r\nIt will not consume much memory. ', 'commenter': 'chaoyli'}]"
764,be/src/olap/rowset/alpha_rowset_reader.cpp,"@@ -54,66 +48,57 @@ OLAPStatus AlphaRowsetReader::init(RowsetReaderContext* read_context) {
     _dst_cursor = new(std::nothrow)RowCursor();
     _dst_cursor->init(*(_current_read_context->tablet_schema));
     OLAPStatus status = _init_column_datas(read_context);
-    return status;
-}
 
-bool AlphaRowsetReader::has_next() {
-    bool next_flag = false;
-    for (int i = 0; i < _column_datas.size(); ++i) {
-        auto& row_block = _row_blocks[i];
-        if (row_block != nullptr) {
-            if (row_block->has_remaining()) {
-                next_flag = true;
+    Version version = _alpha_rowset_meta->version();
+    _is_singleton_rowset = (version.first == version.second);
+    bool merge = false;
+    /*
+     * For singleton rowset, there exists three situations.
+     *   1. DUP_KEYS tablet has no necessities to merge row in advance.
+     *   2. COMPACTION/CHECKSUM/ALTER_TABLET task has no necessities to
+     *      merge row in advance.
+     *   3. QUERY task will set preaggregation. If preaggregation is
+     *      set to be false, it is necessary to merge row in advance.
+     * For cumulative rowset, there is no necessities to merge row in advance.
+     */
+    if (_is_singleton_rowset) {
+        if (_current_read_context->tablet_schema->keys_type() == DUP_KEYS) {
+            // DUP_KEYS tablet
+            _next_block = &AlphaRowsetReader::_next_block_for_singleton_without_merge;
+        } else {
+            if (_current_read_context->reader_type == READER_QUERY
+                    && !_current_read_context->preaggregation) {
+                // QUERY task which set preaggregation to be true.
+                _next_block = &AlphaRowsetReader::_next_block_for_singleton_with_merge;
+                merge = true;
             } else {
-                OLAPStatus status = _get_next_block(i, &_row_blocks[i]);
-                if (status == OLAP_ERR_DATA_EOF) {
-                    _row_blocks[i] = nullptr;
-                    continue;
-                } else if (status != OLAP_SUCCESS) {
-                    LOG(WARNING) << ""_get_next_block failed, status:"" << status;
-                    return false;
-                } else {
-                    if (_row_blocks[i] != nullptr && _row_blocks[i]->has_remaining()) {
-                        next_flag = true;
-                    }
-                }
+                // COMPACTION/CHECKSUM/ALTER_TABLET task
+                _next_block = &AlphaRowsetReader::_next_block_for_singleton_without_merge;
             }
         }
+    } else {
+        // query task to scan cumulative rowset
+        _next_block = &AlphaRowsetReader::_next_block_for_cumulative_rowset;
     }
-    return next_flag;
-}
 
-OLAPStatus AlphaRowsetReader::next(RowCursor** row) {
-    OLAPStatus status = OLAP_SUCCESS;
-    if (_is_cumulative_rowset) {
-        status = _get_next_row_for_cumulative_rowset(row);
-    } else {
-        status = _get_next_row_for_singleton_rowset(row);
+    if (_is_singleton_rowset && merge) {
+        _read_block.reset(new RowBlock(_current_read_context->tablet_schema));","[{'comment': '```suggestion\r\n        _read_block.reset(new (std::nothrow)RowBlock(_current_read_context->tablet_schema));\r\n```', 'commenter': 'kangpinghuang'}, {'comment': 'OK', 'commenter': 'chaoyli'}]"
764,be/src/olap/rowset/alpha_rowset_reader.cpp,"@@ -54,66 +48,57 @@ OLAPStatus AlphaRowsetReader::init(RowsetReaderContext* read_context) {
     _dst_cursor = new(std::nothrow)RowCursor();
     _dst_cursor->init(*(_current_read_context->tablet_schema));
     OLAPStatus status = _init_column_datas(read_context);
-    return status;
-}
 
-bool AlphaRowsetReader::has_next() {
-    bool next_flag = false;
-    for (int i = 0; i < _column_datas.size(); ++i) {
-        auto& row_block = _row_blocks[i];
-        if (row_block != nullptr) {
-            if (row_block->has_remaining()) {
-                next_flag = true;
+    Version version = _alpha_rowset_meta->version();
+    _is_singleton_rowset = (version.first == version.second);
+    bool merge = false;
+    /*
+     * For singleton rowset, there exists three situations.
+     *   1. DUP_KEYS tablet has no necessities to merge row in advance.
+     *   2. COMPACTION/CHECKSUM/ALTER_TABLET task has no necessities to
+     *      merge row in advance.
+     *   3. QUERY task will set preaggregation. If preaggregation is
+     *      set to be false, it is necessary to merge row in advance.
+     * For cumulative rowset, there is no necessities to merge row in advance.
+     */
+    if (_is_singleton_rowset) {
+        if (_current_read_context->tablet_schema->keys_type() == DUP_KEYS) {
+            // DUP_KEYS tablet
+            _next_block = &AlphaRowsetReader::_next_block_for_singleton_without_merge;
+        } else {
+            if (_current_read_context->reader_type == READER_QUERY
+                    && !_current_read_context->preaggregation) {
+                // QUERY task which set preaggregation to be true.
+                _next_block = &AlphaRowsetReader::_next_block_for_singleton_with_merge;
+                merge = true;
             } else {
-                OLAPStatus status = _get_next_block(i, &_row_blocks[i]);
-                if (status == OLAP_ERR_DATA_EOF) {
-                    _row_blocks[i] = nullptr;
-                    continue;
-                } else if (status != OLAP_SUCCESS) {
-                    LOG(WARNING) << ""_get_next_block failed, status:"" << status;
-                    return false;
-                } else {
-                    if (_row_blocks[i] != nullptr && _row_blocks[i]->has_remaining()) {
-                        next_flag = true;
-                    }
-                }
+                // COMPACTION/CHECKSUM/ALTER_TABLET task
+                _next_block = &AlphaRowsetReader::_next_block_for_singleton_without_merge;
             }
         }
+    } else {
+        // query task to scan cumulative rowset
+        _next_block = &AlphaRowsetReader::_next_block_for_cumulative_rowset;
     }
-    return next_flag;
-}
 
-OLAPStatus AlphaRowsetReader::next(RowCursor** row) {
-    OLAPStatus status = OLAP_SUCCESS;
-    if (_is_cumulative_rowset) {
-        status = _get_next_row_for_cumulative_rowset(row);
-    } else {
-        status = _get_next_row_for_singleton_rowset(row);
+    if (_is_singleton_rowset && merge) {
+        _read_block.reset(new RowBlock(_current_read_context->tablet_schema));
+        if (_read_block == nullptr) {
+            LOG(WARNING) << ""new row block failed in reader"";
+            return OLAP_ERR_MALLOC_ERROR;
+        }
+        RowBlockInfo block_info;
+        block_info.row_num = _current_read_context->tablet_schema->num_rows_per_row_block();
+        block_info.null_supported = true;
+        _read_block->init(block_info);
+        _dst_cursor = new(std::nothrow)RowCursor();","[{'comment': '```suggestion\r\n        _dst_cursor = new (std::nothrow)RowCursor();\r\n```', 'commenter': 'kangpinghuang'}, {'comment': 'OK', 'commenter': 'chaoyli'}]"
764,be/src/olap/rowset/alpha_rowset_reader.cpp,"@@ -136,42 +121,83 @@ int64_t AlphaRowsetReader::filtered_rows() {
     return _stats->rows_del_filtered;
 }
 
-OLAPStatus AlphaRowsetReader::_get_next_block(size_t pos, RowBlock** row_block) {
-    // get next block
-    OLAPStatus status = _column_datas[pos]->get_next_block(row_block);
-    if (status == OLAP_ERR_DATA_EOF && _key_range_size > 0) {
-        // reach the end of one predicate
-        // currently, SegmentReader can only support filter one key range a time
-        // refresh the predicate and continue read
-        _key_range_indices[pos]++;
-        OLAPStatus status = OLAP_SUCCESS;
-        while (_key_range_indices[pos] < _key_range_size) {
-            status = _column_datas[pos]->prepare_block_read(
-                    _current_read_context->lower_bound_keys->at(_key_range_indices[pos]),
-                    _current_read_context->is_lower_keys_included->at(_key_range_indices[pos]),
-                    _current_read_context->upper_bound_keys->at(_key_range_indices[pos]),
-                    _current_read_context->is_upper_keys_included->at(_key_range_indices[pos]),
-                    row_block);
-            if (status == OLAP_ERR_DATA_EOF) {
-                _key_range_indices[pos]++;
-                continue;
-            } else if (status != OLAP_SUCCESS) {
-                LOG(WARNING) << ""prepare block read failed"";
-                return status;
-            } else {
-                break;
-            }
+OLAPStatus AlphaRowsetReader::_next_block_for_cumulative_rowset(RowBlock** block) {
+    size_t pos = 0;","[{'comment': 'I think you can add a DCHECK(_row_blocks.size() == 1);', 'commenter': 'kangpinghuang'}, {'comment': 'You should read code seriously.\r\nI have judge _row_blocks.empty(), which indicates _row_blocks will be empty.', 'commenter': 'chaoyli'}]"
764,be/src/olap/rowset/alpha_rowset_reader.cpp,"@@ -136,42 +121,83 @@ int64_t AlphaRowsetReader::filtered_rows() {
     return _stats->rows_del_filtered;
 }
 
-OLAPStatus AlphaRowsetReader::_get_next_block(size_t pos, RowBlock** row_block) {
-    // get next block
-    OLAPStatus status = _column_datas[pos]->get_next_block(row_block);
-    if (status == OLAP_ERR_DATA_EOF && _key_range_size > 0) {
-        // reach the end of one predicate
-        // currently, SegmentReader can only support filter one key range a time
-        // refresh the predicate and continue read
-        _key_range_indices[pos]++;
-        OLAPStatus status = OLAP_SUCCESS;
-        while (_key_range_indices[pos] < _key_range_size) {
-            status = _column_datas[pos]->prepare_block_read(
-                    _current_read_context->lower_bound_keys->at(_key_range_indices[pos]),
-                    _current_read_context->is_lower_keys_included->at(_key_range_indices[pos]),
-                    _current_read_context->upper_bound_keys->at(_key_range_indices[pos]),
-                    _current_read_context->is_upper_keys_included->at(_key_range_indices[pos]),
-                    row_block);
-            if (status == OLAP_ERR_DATA_EOF) {
-                _key_range_indices[pos]++;
-                continue;
-            } else if (status != OLAP_SUCCESS) {
-                LOG(WARNING) << ""prepare block read failed"";
-                return status;
-            } else {
-                break;
-            }
+OLAPStatus AlphaRowsetReader::_next_block_for_cumulative_rowset(RowBlock** block) {
+    size_t pos = 0;
+    if (UNLIKELY(_row_blocks.empty() || _row_blocks[pos] == nullptr)) {
+        return OLAP_ERR_DATA_EOF;
+    }
+    OLAPStatus status = OLAP_SUCCESS;","[{'comment': 'delete this line and return OLAP_SUCCESS directly in line 136', 'commenter': 'kangpinghuang'}, {'comment': 'OK', 'commenter': 'chaoyli'}]"
764,be/src/olap/rowset/alpha_rowset_reader.cpp,"@@ -136,42 +121,83 @@ int64_t AlphaRowsetReader::filtered_rows() {
     return _stats->rows_del_filtered;
 }
 
-OLAPStatus AlphaRowsetReader::_get_next_block(size_t pos, RowBlock** row_block) {
-    // get next block
-    OLAPStatus status = _column_datas[pos]->get_next_block(row_block);
-    if (status == OLAP_ERR_DATA_EOF && _key_range_size > 0) {
-        // reach the end of one predicate
-        // currently, SegmentReader can only support filter one key range a time
-        // refresh the predicate and continue read
-        _key_range_indices[pos]++;
-        OLAPStatus status = OLAP_SUCCESS;
-        while (_key_range_indices[pos] < _key_range_size) {
-            status = _column_datas[pos]->prepare_block_read(
-                    _current_read_context->lower_bound_keys->at(_key_range_indices[pos]),
-                    _current_read_context->is_lower_keys_included->at(_key_range_indices[pos]),
-                    _current_read_context->upper_bound_keys->at(_key_range_indices[pos]),
-                    _current_read_context->is_upper_keys_included->at(_key_range_indices[pos]),
-                    row_block);
-            if (status == OLAP_ERR_DATA_EOF) {
-                _key_range_indices[pos]++;
-                continue;
-            } else if (status != OLAP_SUCCESS) {
-                LOG(WARNING) << ""prepare block read failed"";
-                return status;
-            } else {
-                break;
-            }
+OLAPStatus AlphaRowsetReader::_next_block_for_cumulative_rowset(RowBlock** block) {
+    size_t pos = 0;
+    if (UNLIKELY(_row_blocks.empty() || _row_blocks[pos] == nullptr)) {
+        return OLAP_ERR_DATA_EOF;
+    }
+    OLAPStatus status = OLAP_SUCCESS;
+    if (_row_blocks[pos]->has_remaining()) {
+        *block = _row_blocks[pos];
+    } else {
+        RETURN_NOT_OK(_next_block_for_column_data(pos, &_row_blocks[pos]));
+        *block = _row_blocks[pos];
+    }
+    return status;
+}
+
+OLAPStatus AlphaRowsetReader::_next_block_for_singleton_without_merge(RowBlock** block) {
+    // If tablet is a duplicate key tablet, there is no necessity to merge.
+    // If preaggregation is set to be false, there is no necessity to merge.","[{'comment': 'I think this comment is opposed to the logic of line 69. pls make the use of preaggregation more clear.\r\nIt is necessary to add a clear comment to describe when preaggregation is set to true or false?', 'commenter': 'kangpinghuang'}, {'comment': 'OK', 'commenter': 'chaoyli'}, {'comment': 'preaggregation is not to suitable to be clarified in this place. It is has be described in another place.', 'commenter': 'chaoyli'}]"
764,be/src/olap/rowset/alpha_rowset_reader.cpp,"@@ -201,24 +229,38 @@ OLAPStatus AlphaRowsetReader::_get_next_row_for_singleton_rowset(RowCursor** row
     return OLAP_SUCCESS;
 }
 
-OLAPStatus AlphaRowsetReader::_get_next_row_for_cumulative_rowset(RowCursor** row) {
-    size_t pos = 0;
-    if (_row_blocks[pos] == nullptr) {
-        return OLAP_ERR_DATA_EOF;
-    }
-    RowCursor* current_row = _row_cursors[pos];
-    OLAPStatus status = OLAP_SUCCESS;
-    if (!_row_blocks[pos]->has_remaining()) {
-        status = _get_next_block(pos, &_row_blocks[pos]);
-        if (status != OLAP_SUCCESS) {
-            LOG(WARNING) << ""_get_next_block failed, status:"" << status;
-            return status;
+OLAPStatus AlphaRowsetReader::_next_block_for_column_data(size_t pos, RowBlock** row_block) {
+    // get next block","[{'comment': 'I think the read logic of first block can be merged in this function', 'commenter': 'kangpinghuang'}, {'comment': 'I will combine function call.\r\nunion_block and merge_block will be remained.', 'commenter': 'chaoyli'}]"
764,be/src/olap/rowset/alpha_rowset_reader.h,"@@ -80,16 +72,32 @@ class AlphaRowsetReader : public RowsetReader {
     AlphaRowsetMeta* _alpha_rowset_meta;
     std::vector<std::shared_ptr<SegmentGroup>> _segment_groups;
     std::vector<std::shared_ptr<ColumnData>> _column_datas;
-    std::vector<RowBlock*> _row_blocks;
+
+    // For singleton Rowset, there are several SegmentGroups
+    // Each of SegmentGroups correponds to a row_block upon scan
+    std::vector<RowBlock*> _row_blocks; 
+    std::unique_ptr<RowBlock> _read_block;
+    OLAPStatus (AlphaRowsetReader::*_next_block)(RowBlock** block) = nullptr;
+
+    // For singleton Rowset, there are several SegmentGroups
+    // Each of SegmentGroups correponds to a row_cursor 
     std::vector<RowCursor*> _row_cursors;
+    RowCursor* _dst_cursor;","[{'comment': 'this cursor should be delete when Desctructor', 'commenter': 'kangpinghuang'}, {'comment': 'I forget to deconstruct it, I will add it. ', 'commenter': 'chaoyli'}]"
764,be/src/olap/schema_change.cpp,"@@ -736,13 +736,9 @@ bool SchemaChangeDirectly::process(RowsetReaderSharedPtr rowset_reader, RowsetWr
     reset_merged_rows();
     reset_filtered_rows();
 
-    std::shared_ptr<RowBlock> ref_row_block(new RowBlock(&(base_tablet->tablet_schema())));
-    RowBlockInfo block_info;
-    block_info.row_num = base_tablet->tablet_schema().num_rows_per_row_block();
-    block_info.null_supported = true;
-    ref_row_block->init(block_info);
-    rowset_reader->next_block(ref_row_block);
-    while (ref_row_block->has_remaining()) {
+    RowBlock* ref_row_block = nullptr;
+    rowset_reader->next_block(&ref_row_block);
+    while (ref_row_block != nullptr && ref_row_block->has_remaining()) {
         // 注意这里强制分配和旧块等大的块(小了可能会存不下)
         if (nullptr == new_row_block","[{'comment': 'new_row_block == nullptr', 'commenter': 'kangpinghuang'}, {'comment': 'OK', 'commenter': 'chaoyli'}]"
772,be/src/agent/task_worker_pool.cpp,"@@ -626,6 +626,10 @@ void TaskWorkerPool::_alter_tablet(
             finish_tablet_infos.push_back(tablet_info);
         }
     }
+    
+    for (auto& tablet_info : finish_tablet_infos) {","[{'comment': 'Debug info is not necessary.', 'commenter': 'chaoyli'}, {'comment': 'OK', 'commenter': 'kangpinghuang'}]"
772,be/src/agent/task_worker_pool.cpp,"@@ -626,6 +626,10 @@ void TaskWorkerPool::_alter_tablet(
             finish_tablet_infos.push_back(tablet_info);
         }
     }
+    
+    for (auto& tablet_info : finish_tablet_infos) {
+        LOG(INFO) << ""finish tablet infos:"" <<  apache::thrift::ThriftDebugString(tablet_info);
+    }","[{'comment': 'It should be debug or vlog or there will be too much logs.', 'commenter': 'yiguolei'}, {'comment': 'ok', 'commenter': 'kangpinghuang'}]"
772,be/src/olap/reader.cpp,"@@ -467,7 +467,7 @@ OLAPStatus Reader::_capture_rs_readers(const ReaderParams& read_params) {
         rs_readers = &read_params.rs_readers;
     } else {
         _tablet->obtain_header_rdlock();
-        _tablet->capture_rs_readers(_version, &_own_rs_readers);
+        RETURN_NOT_OK(_tablet->capture_rs_readers(_version, &_own_rs_readers));","[{'comment': 'lock leak', 'commenter': 'chaoyli'}, {'comment': 'ok', 'commenter': 'kangpinghuang'}]"
772,be/src/olap/rowset/alpha_rowset_reader.cpp,"@@ -234,6 +234,7 @@ OLAPStatus AlphaRowsetReader::_init_column_datas(RowsetReaderContext* read_conte
         _key_range_size = read_context->lower_bound_keys->size();
     }
 
+    LOG(INFO) << ""segment groups size:"" << _segment_groups.size();","[{'comment': 'Debug info is not necessary.', 'commenter': 'chaoyli'}]"
772,be/src/olap/rowset/alpha_rowset_reader.cpp,"@@ -311,8 +317,9 @@ OLAPStatus AlphaRowsetReader::_init_column_datas(RowsetReaderContext* read_conte
             }
         } else {
             status = new_column_data->get_first_row_block(&row_block);
-            if (status != OLAP_SUCCESS) {
-                LOG(WARNING) << ""get first row block failed"";
+            LOG(INFO) << ""get_first_row_block status:"" << status;","[{'comment': 'Debug info is not necessary.', 'commenter': 'chaoyli'}]"
772,be/src/olap/reader.cpp,"@@ -467,7 +467,7 @@ OLAPStatus Reader::_capture_rs_readers(const ReaderParams& read_params) {
         rs_readers = &read_params.rs_readers;
     } else {
         _tablet->obtain_header_rdlock();
-        _tablet->capture_rs_readers(_version, &_own_rs_readers);
+        RETURN_NOT_OK(_tablet->capture_rs_readers(_version, &_own_rs_readers));
         _tablet->release_header_lock();
 ","[{'comment': ""if not return ok, it will return if own_res_readers's size < 1. Is that ok?"", 'commenter': 'yiguolei'}, {'comment': 'Here return not ok means that the reader init failed and the reading will return failed directly. I think it is a right process.', 'commenter': 'kangpinghuang'}]"
772,be/src/olap/rowset/alpha_rowset_reader.cpp,"@@ -285,6 +286,11 @@ OLAPStatus AlphaRowsetReader::_init_column_datas(RowsetReaderContext* read_conte
         _row_cursors.push_back(row_cursor);
 
         RowBlock* row_block = nullptr;
+        if (segment_group->empty()) {
+            LOG(INFO) << ""segment group is empty"";","[{'comment': 'useless log', 'commenter': 'imay'}]"
772,be/src/agent/task_worker_pool.cpp,"@@ -633,8 +631,8 @@ void TaskWorkerPool::_alter_tablet(
         error_msgs.push_back(process_name + "" success"");
         task_status.__set_status_code(TStatusCode::OK);
     } else if (status == DORIS_TASK_REQUEST_ERROR) {
-        OLAP_LOG_WARNING(""alter table request task type invalid. ""
-                         ""signature: %ld"", signature);
+        LOG(WARNING) << ""alter table request task type invalid. ""
+                         ""signature:"" << signature;
         error_msgs.push_back(""alter table request new tablet id or schema count invalid."");","[{'comment': 'missed << ?', 'commenter': 'yiguolei'}]"
772,be/src/olap/reader.cpp,"@@ -467,8 +467,11 @@ OLAPStatus Reader::_capture_rs_readers(const ReaderParams& read_params) {
         rs_readers = &read_params.rs_readers;
     } else {
         _tablet->obtain_header_rdlock();
-        _tablet->capture_rs_readers(_version, &_own_rs_readers);
+        OLAPStatus status = _tablet->capture_rs_readers(_version, &_own_rs_readers);
         _tablet->release_header_lock();
+        if (status != OLAP_SUCCESS) {","[{'comment': 'RETUNR_NOT_OK(status)', 'commenter': 'chaoyli'}]"
773,be/src/http/http_client.cpp,"@@ -39,6 +42,9 @@ Status HttpClient::init(const std::string& url) {
         curl_easy_reset(_curl);
     }
 
+    if(_header_list != nullptr) {
+        curl_slist_free_all(_header_list);","[{'comment': '_header_list = nullptr', 'commenter': 'imay'}]"
773,be/src/http/http_client.h,"@@ -54,6 +54,17 @@ class HttpClient {
         curl_easy_setopt(_curl, CURLOPT_PASSWORD, passwd.c_str());
     }
 
+    void set_content_type(const std::string content_type) {
+        std::string scratch_str = ""Content-Type: "" + content_type;
+        _header_list = curl_slist_append(NULL, scratch_str.c_str());
+        curl_easy_setopt(_curl, CURLOPT_HTTPHEADER, _header_list);
+    }
+
+    void set_post_body(const std::string& post_body) {
+        curl_easy_setopt(_curl, CURLOPT_POSTFIELDS, post_body.c_str());
+        curl_easy_setopt(_curl, CURLOPT_POSTFIELDSIZE, (long)post_body.length());","[{'comment': 'post_body.data(), post_body.size()', 'commenter': 'imay'}]"
773,be/src/http/http_client.h,"@@ -54,6 +54,17 @@ class HttpClient {
         curl_easy_setopt(_curl, CURLOPT_PASSWORD, passwd.c_str());
     }
 
+    void set_content_type(const std::string content_type) {
+        std::string scratch_str = ""Content-Type: "" + content_type;
+        _header_list = curl_slist_append(NULL, scratch_str.c_str());","[{'comment': 'if (_header_list != nullptr) {\r\n      curl_slist_free_all(_header_list);\r\n}', 'commenter': 'imay'}]"
773,be/test/http/http_client_test.cpp,"@@ -114,6 +136,18 @@ TEST_F(HttpClientTest, get_failed) {
     ASSERT_FALSE(st.ok());
 }
 
+TEST_F(HttpClientTest, post_normal) {
+    HttpClient client;
+    auto st = client.init(""http://127.0.0.1:29386/simple_post"");
+    ASSERT_TRUE(st.ok());
+    client.set_method(POST);
+    std::string response;
+    std::string request_body = ""simple post body query"";
+    st = client.execute_post_request(request_body, &response);
+    ASSERT_TRUE(st.ok());
+    ASSERT_EQ(response.length(), request_body.length());","[{'comment': 'ASSERT_STRCMP(.c_str(), .c_str())', 'commenter': 'imay'}]"
777,be/src/olap/data_dir.cpp,"@@ -823,4 +824,174 @@ OLAPStatus DataDir::load() {
     return OLAP_SUCCESS;
 }
 
+void DataDir::add_pending_ids(const std::string& id) {
+    WriteLock wr_lock(&_pending_path_mutex);
+    _pending_path_ids.insert(id);
+}
+
+void DataDir::remove_pending_ids(const std::string& id) {
+    WriteLock wr_lock(&_pending_path_mutex);
+    _pending_path_ids.erase(id);
+}
+
+// path consumer
+void DataDir::perform_path_gc() {
+    // init the set of valid path
+    // validate the path in data dir
+    std::unique_lock<std::mutex> lck(_check_path_mutex);
+    cv.wait(lck, [this]{return _scanned;});
+    _scanned = false;
+    LOG(INFO) << ""start to path gc."";
+    int counter = 0;
+    for (int index = 0; index < _all_check_paths.size();) {
+        ++counter;
+        if (config::path_gc_check_step > 0 && counter % config::path_gc_check_step == 0) {
+            usleep(config::path_gc_check_step_interval_ms * 1000);
+        }
+        auto path_iter = std::next(_all_check_paths.begin(), index);
+        if (path_iter == _all_check_paths.end()) {
+            break;
+        }
+        std::string path = *path_iter;
+        TTabletId tablet_id = -1;
+        TSchemaHash schema_hash = -1;
+        bool is_valid = StorageEngine::instance()->tablet_manager()->get_tablet_id_and_schema_hash_from_path(path,
+                &tablet_id, &schema_hash);
+        std::set<std::string> paths;
+        paths.insert(path);
+        if (!is_valid) {
+            LOG(WARNING) << ""unknow path:"" << path;","[{'comment': '```suggestion\r\n            LOG(WARNING) << ""unknown path:"" << path;\r\n```', 'commenter': 'morningman'}, {'comment': 'ok', 'commenter': 'kangpinghuang'}]"
777,be/src/olap/data_dir.cpp,"@@ -823,4 +824,174 @@ OLAPStatus DataDir::load() {
     return OLAP_SUCCESS;
 }
 
+void DataDir::add_pending_ids(const std::string& id) {
+    WriteLock wr_lock(&_pending_path_mutex);
+    _pending_path_ids.insert(id);
+}
+
+void DataDir::remove_pending_ids(const std::string& id) {
+    WriteLock wr_lock(&_pending_path_mutex);
+    _pending_path_ids.erase(id);
+}
+
+// path consumer
+void DataDir::perform_path_gc() {
+    // init the set of valid path
+    // validate the path in data dir
+    std::unique_lock<std::mutex> lck(_check_path_mutex);
+    cv.wait(lck, [this]{return _scanned;});
+    _scanned = false;
+    LOG(INFO) << ""start to path gc."";
+    int counter = 0;
+    for (int index = 0; index < _all_check_paths.size();) {
+        ++counter;
+        if (config::path_gc_check_step > 0 && counter % config::path_gc_check_step == 0) {
+            usleep(config::path_gc_check_step_interval_ms * 1000);
+        }
+        auto path_iter = std::next(_all_check_paths.begin(), index);
+        if (path_iter == _all_check_paths.end()) {
+            break;
+        }
+        std::string path = *path_iter;
+        TTabletId tablet_id = -1;
+        TSchemaHash schema_hash = -1;
+        bool is_valid = StorageEngine::instance()->tablet_manager()->get_tablet_id_and_schema_hash_from_path(path,
+                &tablet_id, &schema_hash);
+        std::set<std::string> paths;
+        paths.insert(path);
+        if (!is_valid) {
+            LOG(WARNING) << ""unknow path:"" << path;
+            _remove_check_paths_no_lock(paths);
+            continue;
+        } else {
+            if (tablet_id >0 && schema_hash >0) {","[{'comment': '```suggestion\r\n            if (tablet_id > 0 && schema_hash > 0) {\r\n```', 'commenter': 'morningman'}, {'comment': 'ok', 'commenter': 'kangpinghuang'}]"
777,be/src/olap/tablet_manager.cpp,"@@ -628,6 +627,22 @@ bool TabletManager::get_tablet_id_and_schema_hash_from_path(const std::string& p
     return false;
 }
 
+bool TabletManager::get_rowset_id_from_path(const std::string& path, RowsetId* rowset_id) {
+    std::string pattern = ""/data/\\d+/\\d+/\\d+/(\\d+)_.*"";
+    std::regex rgx (pattern.c_str());","[{'comment': 'You should make this static to avoid constructing this multiple times.', 'commenter': 'imay'}, {'comment': 'ok', 'commenter': 'kangpinghuang'}]"
777,be/src/olap/base_compaction.cpp,"@@ -124,7 +124,30 @@ OLAPStatus BaseCompaction::run() {
     // 3. 执行base compaction
     //    执行过程可能会持续比较长时间
     stage_watch.reset();
+    RowsetId rowset_id = 0;
+    RETURN_NOT_OK(_tablet->next_rowset_id(&rowset_id));
+    RowsetWriterContextBuilder context_builder;
+    context_builder.set_rowset_id(rowset_id)","[{'comment': 'Build should have default value, otherwise we can create class instantly. ', 'commenter': 'imay'}, {'comment': 'ok', 'commenter': 'kangpinghuang'}]"
777,be/src/olap/data_dir.h,"@@ -93,6 +94,14 @@ class DataDir {
     // load data from meta and data files
     OLAPStatus load();
 
+    void add_pending_ids(const std::string& id);
+
+    void remove_pending_ids(const std::string& id);
+
+    void perform_path_gc();","[{'comment': 'You should add comment about these two functions, and their relations', 'commenter': 'imay'}, {'comment': 'ok', 'commenter': 'kangpinghuang'}]"
777,be/src/olap/data_dir.cpp,"@@ -823,4 +824,174 @@ OLAPStatus DataDir::load() {
     return OLAP_SUCCESS;
 }
 
+void DataDir::add_pending_ids(const std::string& id) {
+    WriteLock wr_lock(&_pending_path_mutex);
+    _pending_path_ids.insert(id);
+}
+
+void DataDir::remove_pending_ids(const std::string& id) {
+    WriteLock wr_lock(&_pending_path_mutex);
+    _pending_path_ids.erase(id);
+}
+
+// path consumer
+void DataDir::perform_path_gc() {
+    // init the set of valid path
+    // validate the path in data dir
+    std::unique_lock<std::mutex> lck(_check_path_mutex);
+    cv.wait(lck, [this]{return _scanned;});
+    _scanned = false;
+    LOG(INFO) << ""start to path gc."";
+    int counter = 0;
+    for (int index = 0; index < _all_check_paths.size();) {
+        ++counter;
+        if (config::path_gc_check_step > 0 && counter % config::path_gc_check_step == 0) {
+            usleep(config::path_gc_check_step_interval_ms * 1000);
+        }
+        auto path_iter = std::next(_all_check_paths.begin(), index);
+        if (path_iter == _all_check_paths.end()) {
+            break;
+        }
+        std::string path = *path_iter;
+        TTabletId tablet_id = -1;
+        TSchemaHash schema_hash = -1;
+        bool is_valid = StorageEngine::instance()->tablet_manager()->get_tablet_id_and_schema_hash_from_path(path,
+                &tablet_id, &schema_hash);
+        std::set<std::string> paths;
+        paths.insert(path);
+        if (!is_valid) {
+            LOG(WARNING) << ""unknow path:"" << path;
+            _remove_check_paths_no_lock(paths);
+            continue;
+        } else {
+            if (tablet_id >0 && schema_hash >0) {
+                // tablet schema hash path or rowset file path
+                TabletSharedPtr tablet = StorageEngine::instance()->tablet_manager()->get_tablet(tablet_id, schema_hash);
+                if (tablet == nullptr) {
+                    std::string tablet_path_id = TABLET_ID_PREFIX + std::to_string(tablet_id);
+                    bool exist_in_pending = _check_pending_ids(tablet_path_id);
+                    if (!exist_in_pending) {
+                        _process_garbage_path(path);
+                        _remove_check_paths_no_lock(paths);
+                        continue;
+                    }
+                } else {
+                    bool valid = tablet->check_path(path);
+                    if (!valid) {
+                        RowsetId rowset_id = -1;
+                        bool is_rowset_file = StorageEngine::instance()->tablet_manager()
+                                ->get_rowset_id_from_path(path, &rowset_id);
+                        if (is_rowset_file) {
+                            std::string rowset_path_id = ROWSET_ID_PREFIX + std::to_string(rowset_id);
+                            bool exist_in_pending = _check_pending_ids(rowset_path_id);
+                            if (!exist_in_pending) {
+                                _process_garbage_path(path);
+                                _remove_check_paths_no_lock(paths);
+                                continue;
+                            }
+                        }
+                    }
+                }
+            } else if (tablet_id >0 && schema_hash <= 0) {
+                // tablet id path
+                if (FileUtils::is_dir(path)) {
+                    bool exist = StorageEngine::instance()->tablet_manager()->check_tablet_id_exist(tablet_id);
+                    if (!exist) {
+                        std::string tablet_path_id = TABLET_ID_PREFIX + std::to_string(tablet_id);
+                        bool exist_in_pending = _check_pending_ids(tablet_path_id);
+                        if (!exist_in_pending) {
+                            _process_garbage_path(path);
+                            _remove_check_paths_no_lock(paths);
+                            continue;
+                        }
+                    }
+                } else {
+                    LOG(WARNING) << ""unknown path:"" << path;
+                    _remove_check_paths_no_lock(paths);
+                    continue;
+                }
+            } else {
+                LOG(WARNING) << ""unknown path:"" << path;
+                _remove_check_paths_no_lock(paths);
+                continue;
+            }
+        }
+        ++index;
+    }
+    _all_check_paths.clear();
+    LOG(INFO) << ""finished one time path gc."";
+}
+
+// path producer
+void DataDir::perform_path_scan() {
+    std::unique_lock<std::mutex> lck(_check_path_mutex);
+    _scanned = true;
+    LOG(INFO) << ""start to scan data dir path:"" << _path;
+    std::set<std::string> shards;
+    std::string data_path = _path + DATA_PREFIX;
+    if (dir_walk(data_path, &shards, nullptr) != OLAP_SUCCESS) {
+        LOG(WARNING) << ""fail to walk dir. [path="" << data_path << ""]"";
+        lck.unlock();
+        cv.notify_one();
+        return;
+    }
+    for (const auto& shard : shards) {
+        std::string shard_path = data_path + ""/"" + shard;
+        std::set<std::string> tablet_ids;
+        if (dir_walk(shard_path, &tablet_ids, nullptr) != OLAP_SUCCESS) {
+            LOG(WARNING) << ""fail to walk dir. [path="" << shard_path << ""]"";
+            continue;
+        }
+        for (const auto& tablet_id : tablet_ids) {
+            std::string tablet_id_path = shard_path + ""/"" + tablet_id;
+            _all_check_paths.insert(tablet_id_path);
+            std::set<std::string> schema_hashes;
+            if (dir_walk(tablet_id_path, &schema_hashes, nullptr) != OLAP_SUCCESS) {
+                LOG(WARNING) << ""fail to walk dir. [path="" << tablet_id_path << ""]"";
+                continue;
+            }
+            for (const auto& schema_hash : schema_hashes) {
+                std::string tablet_schema_hash_path = tablet_id_path + ""/"" + schema_hash;
+                _all_check_paths.insert(tablet_schema_hash_path);
+                std::set<std::string> rowset_files;
+                if (dir_walk(tablet_schema_hash_path, nullptr, &rowset_files) != OLAP_SUCCESS) {
+                    LOG(WARNING) << ""fail to walk dir. [path="" << tablet_schema_hash_path << ""]"";
+                    continue;
+                }
+                for (const auto& rowset_file : rowset_files) {
+                    std::string rowset_file_path = tablet_schema_hash_path + ""/"" + rowset_file;
+                    _all_check_paths.insert(rowset_file_path);
+                }
+            }
+        }
+    }
+    LOG(INFO) << ""scan data dir path:"" << _path << "" finished. path size:"" << _all_check_paths.size();
+    lck.unlock();
+    cv.notify_one();
+}
+
+void DataDir::_process_garbage_path(const std::string& path) {
+    if (check_dir_existed(path)) {
+        LOG(INFO) << ""collect garbage dir path:"" << path;
+        OLAPStatus status = remove_all_dir(path);
+        if (status != OLAP_SUCCESS) {
+            LOG(WARNING) << ""remove garbage dir path:"" << path << "" failed"";
+        }
+    }
+}
+
+bool DataDir::_check_pending_ids(const std::string& id) {
+    ReadLock rd_lock(&_pending_path_mutex);
+    return _pending_path_ids.find(id) != _pending_path_ids.end();
+}
+
+void DataDir::_remove_check_paths_no_lock(const std::set<std::string> paths) {","[{'comment': 'Arguments should be reference', 'commenter': 'imay'}, {'comment': 'ok', 'commenter': 'kangpinghuang'}]"
777,be/src/olap/data_dir.cpp,"@@ -823,4 +824,174 @@ OLAPStatus DataDir::load() {
     return OLAP_SUCCESS;
 }
 
+void DataDir::add_pending_ids(const std::string& id) {
+    WriteLock wr_lock(&_pending_path_mutex);
+    _pending_path_ids.insert(id);
+}
+
+void DataDir::remove_pending_ids(const std::string& id) {
+    WriteLock wr_lock(&_pending_path_mutex);
+    _pending_path_ids.erase(id);
+}
+
+// path consumer
+void DataDir::perform_path_gc() {","[{'comment': 'You should add unit test for this two function', 'commenter': 'imay'}, {'comment': 'I am trying to fix the unit tests and add the unit test case for these two function later', 'commenter': 'kangpinghuang'}]"
777,be/src/olap/data_dir.cpp,"@@ -823,4 +824,174 @@ OLAPStatus DataDir::load() {
     return OLAP_SUCCESS;
 }
 
+void DataDir::add_pending_ids(const std::string& id) {
+    WriteLock wr_lock(&_pending_path_mutex);
+    _pending_path_ids.insert(id);
+}
+
+void DataDir::remove_pending_ids(const std::string& id) {
+    WriteLock wr_lock(&_pending_path_mutex);
+    _pending_path_ids.erase(id);
+}
+
+// path consumer
+void DataDir::perform_path_gc() {
+    // init the set of valid path
+    // validate the path in data dir
+    std::unique_lock<std::mutex> lck(_check_path_mutex);
+    cv.wait(lck, [this]{return _scanned;});
+    _scanned = false;
+    LOG(INFO) << ""start to path gc."";
+    int counter = 0;
+    for (int index = 0; index < _all_check_paths.size();) {
+        ++counter;
+        if (config::path_gc_check_step > 0 && counter % config::path_gc_check_step == 0) {
+            usleep(config::path_gc_check_step_interval_ms * 1000);
+        }
+        auto path_iter = std::next(_all_check_paths.begin(), index);
+        if (path_iter == _all_check_paths.end()) {
+            break;
+        }
+        std::string path = *path_iter;
+        TTabletId tablet_id = -1;
+        TSchemaHash schema_hash = -1;
+        bool is_valid = StorageEngine::instance()->tablet_manager()->get_tablet_id_and_schema_hash_from_path(path,","[{'comment': ""Instead of using `instance()`, you'd better pass `tablet_manager` to this class"", 'commenter': 'imay'}, {'comment': 'ok', 'commenter': 'kangpinghuang'}]"
777,be/src/olap/tablet_manager.cpp,"@@ -606,8 +605,8 @@ bool TabletManager::get_tablet_id_and_schema_hash_from_path(const std::string& p
     for (auto data_dir : data_dirs) {
         const std::string& data_dir_path = data_dir->path();
         if (path.find(data_dir_path) != std::string::npos) {
-            std::string pattern = data_dir_path + ""/data/\\d+/(\\d+)/?(\\d+)?"";
-            std::regex rgx (pattern.c_str());
+            static std::string pattern = data_dir_path + ""/data/\\d+/(\\d+)/?(\\d+)?"";","[{'comment': ""you can't use `static` here, it would lead a bug"", 'commenter': 'imay'}]"
777,be/src/olap/rowset/rowset_writer_context.h,"@@ -50,6 +50,23 @@ struct RowsetWriterContext {
 
 class RowsetWriterContextBuilder {
 public:
+    RowsetWriterContextBuilder() {","[{'comment': 'I think this builder is useless, `RowsetWriterContext` is enough', 'commenter': 'imay'}]"
777,be/src/olap/data_dir.cpp,"@@ -823,4 +827,174 @@ OLAPStatus DataDir::load() {
     return OLAP_SUCCESS;
 }
 
+void DataDir::add_pending_ids(const std::string& id) {
+    WriteLock wr_lock(&_pending_path_mutex);
+    _pending_path_ids.insert(id);
+}
+
+void DataDir::remove_pending_ids(const std::string& id) {
+    WriteLock wr_lock(&_pending_path_mutex);
+    _pending_path_ids.erase(id);
+}
+
+// path consumer
+void DataDir::perform_path_gc() {
+    // init the set of valid path
+    // validate the path in data dir
+    std::unique_lock<std::mutex> lck(_check_path_mutex);
+    cv.wait(lck, [this]{return _scanned;});
+    _scanned = false;
+    LOG(INFO) << ""start to path gc."";
+    int counter = 0;
+    for (int index = 0; index < _all_check_paths.size();) {
+        ++counter;
+        if (config::path_gc_check_step > 0 && counter % config::path_gc_check_step == 0) {
+            usleep(config::path_gc_check_step_interval_ms * 1000);
+        }
+        auto path_iter = std::next(_all_check_paths.begin(), index);
+        if (path_iter == _all_check_paths.end()) {
+            break;
+        }
+        std::string path = *path_iter;
+        TTabletId tablet_id = -1;
+        TSchemaHash schema_hash = -1;
+        bool is_valid = _tablet_manager->get_tablet_id_and_schema_hash_from_path(path,
+                &tablet_id, &schema_hash);
+        std::set<std::string> paths;
+        paths.insert(path);
+        if (!is_valid) {
+            LOG(WARNING) << ""unknown path:"" << path;
+            _remove_check_paths_no_lock(paths);
+            continue;
+        } else {
+            if (tablet_id > 0 && schema_hash > 0) {
+                // tablet schema hash path or rowset file path
+                TabletSharedPtr tablet = _tablet_manager->get_tablet(tablet_id, schema_hash);
+                if (tablet == nullptr) {
+                    std::string tablet_path_id = TABLET_ID_PREFIX + std::to_string(tablet_id);
+                    bool exist_in_pending = _check_pending_ids(tablet_path_id);
+                    if (!exist_in_pending) {
+                        _process_garbage_path(path);
+                        _remove_check_paths_no_lock(paths);
+                        continue;
+                    }
+                } else {
+                    bool valid = tablet->check_path(path);
+                    if (!valid) {
+                        RowsetId rowset_id = -1;
+                        bool is_rowset_file = _tablet_manager
+                                ->get_rowset_id_from_path(path, &rowset_id);
+                        if (is_rowset_file) {
+                            std::string rowset_path_id = ROWSET_ID_PREFIX + std::to_string(rowset_id);
+                            bool exist_in_pending = _check_pending_ids(rowset_path_id);
+                            if (!exist_in_pending) {
+                                _process_garbage_path(path);
+                                _remove_check_paths_no_lock(paths);
+                                continue;
+                            }
+                        }
+                    }
+                }
+            } else if (tablet_id > 0 && schema_hash <= 0) {
+                // tablet id path
+                if (FileUtils::is_dir(path)) {
+                    bool exist = _tablet_manager->check_tablet_id_exist(tablet_id);
+                    if (!exist) {
+                        std::string tablet_path_id = TABLET_ID_PREFIX + std::to_string(tablet_id);
+                        bool exist_in_pending = _check_pending_ids(tablet_path_id);
+                        if (!exist_in_pending) {
+                            _process_garbage_path(path);
+                            _remove_check_paths_no_lock(paths);
+                            continue;
+                        }
+                    }
+                } else {
+                    LOG(WARNING) << ""unknown path:"" << path;
+                    _remove_check_paths_no_lock(paths);
+                    continue;
+                }
+            } else {
+                LOG(WARNING) << ""unknown path:"" << path;
+                _remove_check_paths_no_lock(paths);
+                continue;
+            }
+        }
+        ++index;
+    }
+    _all_check_paths.clear();
+    LOG(INFO) << ""finished one time path gc."";
+}
+
+// path producer
+void DataDir::perform_path_scan() {
+    std::unique_lock<std::mutex> lck(_check_path_mutex);","[{'comment': '{\r\nunique_lock\r\n}\r\ncv.notify_one()', 'commenter': 'imay'}]"
777,be/src/olap/data_dir.cpp,"@@ -823,4 +827,151 @@ OLAPStatus DataDir::load() {
     return OLAP_SUCCESS;
 }
 
+void DataDir::add_pending_ids(const std::string& id) {
+    WriteLock wr_lock(&_pending_path_mutex);
+    _pending_path_ids.insert(id);
+}
+
+void DataDir::remove_pending_ids(const std::string& id) {
+    WriteLock wr_lock(&_pending_path_mutex);
+    _pending_path_ids.erase(id);
+}
+
+// path consumer
+void DataDir::perform_path_gc() {
+    // init the set of valid path
+    // validate the path in data dir
+    std::unique_lock<std::mutex> lck(_check_path_mutex);
+    cv.wait(lck, [this]{return _scanned;});
+    _scanned = false;
+    LOG(INFO) << ""start to path gc."";
+    int counter = 0;
+    for (auto path_iter = _all_check_paths.begin(); path_iter != _all_check_paths.end(); ++path_iter) {","[{'comment': '```suggestion\r\n    for (auto& path : _all_check_paths) {\r\n```', 'commenter': 'imay'}]"
777,be/src/olap/data_dir.cpp,"@@ -823,4 +827,151 @@ OLAPStatus DataDir::load() {
     return OLAP_SUCCESS;
 }
 
+void DataDir::add_pending_ids(const std::string& id) {
+    WriteLock wr_lock(&_pending_path_mutex);
+    _pending_path_ids.insert(id);
+}
+
+void DataDir::remove_pending_ids(const std::string& id) {
+    WriteLock wr_lock(&_pending_path_mutex);
+    _pending_path_ids.erase(id);
+}
+
+// path consumer
+void DataDir::perform_path_gc() {
+    // init the set of valid path
+    // validate the path in data dir
+    std::unique_lock<std::mutex> lck(_check_path_mutex);
+    cv.wait(lck, [this]{return _scanned;});
+    _scanned = false;
+    LOG(INFO) << ""start to path gc."";
+    int counter = 0;
+    for (auto path_iter = _all_check_paths.begin(); path_iter != _all_check_paths.end(); ++path_iter) {
+        ++counter;
+        if (config::path_gc_check_step > 0 && counter % config::path_gc_check_step == 0) {
+            usleep(config::path_gc_check_step_interval_ms * 1000);
+        }
+        std::string path = *path_iter;
+        TTabletId tablet_id = -1;
+        TSchemaHash schema_hash = -1;
+        bool is_valid = _tablet_manager->get_tablet_id_and_schema_hash_from_path(path,
+                &tablet_id, &schema_hash);
+        if (!is_valid) {
+            LOG(WARNING) << ""unknown path:"" << path;
+            continue;
+        }
+        if (tablet_id > 0 && schema_hash > 0) {
+            // tablet schema hash path or rowset file path
+            TabletSharedPtr tablet = _tablet_manager->get_tablet(tablet_id, schema_hash);
+            if (tablet == nullptr) {
+                std::string tablet_path_id = TABLET_ID_PREFIX + std::to_string(tablet_id);
+                bool exist_in_pending = _check_pending_ids(tablet_path_id);
+                if (!exist_in_pending) {
+                    _process_garbage_path(path);
+                }
+            } else {
+                bool valid = tablet->check_path(path);
+                if (!valid) {
+                    RowsetId rowset_id = -1;
+                    bool is_rowset_file = _tablet_manager->get_rowset_id_from_path(path, &rowset_id);
+                    if (is_rowset_file) {
+                        std::string rowset_path_id = ROWSET_ID_PREFIX + std::to_string(rowset_id);
+                        bool exist_in_pending = _check_pending_ids(rowset_path_id);
+                        if (!exist_in_pending) {
+                            _process_garbage_path(path);
+                        }
+                    }
+                }
+            }
+        } else if (tablet_id > 0 && schema_hash <= 0) {
+            // tablet id path
+            if (!FileUtils::is_dir(path)) {
+                LOG(WARNING) << ""unknown path:"" << path;
+                continue;
+            }
+            bool exist = _tablet_manager->check_tablet_id_exist(tablet_id);
+            if (!exist) {
+                std::string tablet_path_id = TABLET_ID_PREFIX + std::to_string(tablet_id);
+                bool exist_in_pending = _check_pending_ids(tablet_path_id);
+                if (!exist_in_pending) {
+                    _process_garbage_path(path);
+                }
+            }
+        }
+    }
+    _all_check_paths.clear();
+    LOG(INFO) << ""finished one time path gc."";
+}
+
+// path producer
+void DataDir::perform_path_scan() {
+    {
+        std::unique_lock<std::mutex> lck(_check_path_mutex);","[{'comment': 'if (_all_check_paths.size() > 0) {\r\n     return ;\r\n}', 'commenter': 'imay'}]"
778,fe/src/main/java/org/apache/doris/optimizer/OptExpressionWapper.java,"@@ -0,0 +1,18 @@
+package org.apache.doris.optimizer;","[{'comment': 'no license header', 'commenter': 'imay'}]"
778,fe/src/main/java/org/apache/doris/optimizer/base/SearchVariable.java,"@@ -0,0 +1,21 @@
+package org.apache.doris.optimizer.base;","[{'comment': 'license', 'commenter': 'imay'}]"
778,fe/src/main/java/org/apache/doris/optimizer/operator/OptPhysicalUTInternalNode.java,"@@ -0,0 +1,32 @@
+package org.apache.doris.optimizer.operator;","[{'comment': 'license header and you should put this file to fe/src/test', 'commenter': 'imay'}, {'comment': 'license header', 'commenter': 'imay'}]"
800,be/src/util/es_scan_reader.h,"@@ -0,0 +1,72 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#pragma once
+
+#include <string>
+#include ""http/http_client.h""
+
+using std::string;
+
+namespace doris {
+
+class Status;
+
+class ESScanReader {
+private:
+    std::string _target;
+    std::string _user_name;
+    std::string _passwd;
+    std::string _scroll_id;
+    HttpClient _network_client;
+    std::string _index;
+    std::string _type;
+    // push down filter
+    std::string _query;
+    // elaticsearch shards to fetch document
+    std::string _shards;
+    // distinguish the first scroll phase and the following scroll
+    bool _is_first;
+    std::string _init_scroll_url;
+    std::string _next_scroll_url;
+    bool _eos;
+    uint16_t _batch_size;
+
+    std::string *_cached_response = new std::string();
+    
+
+public:
+    static constexpr const char* KEY_USER_NAME = ""user"";
+    static constexpr const char* KEY_PASS_WORD = ""passwd"";
+    static constexpr const char* KEY_INDEX = ""index"";
+    static constexpr const char* KEY_TYPE = ""type"";
+    static constexpr const char* KEY_SHARDS = ""shards"";
+    static constexpr const char* KEY_QUERY = ""query"";
+    static constexpr const char* KEY_BATCH_SIZE = ""batch_size"";
+    ESScanReader(const std::string& target, uint16_t size, std::map<std::string, std::string>& props);","[{'comment': '```suggestion\r\n    ESScanReader(const std::string& target, uint16_t size, const std::map<std::string, std::string>& props);\r\n```', 'commenter': 'imay'}]"
800,be/src/util/es_search_builder.cpp,"@@ -0,0 +1,63 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""es_search_builder.h""
+#include<sstream>
+#include <boost/algorithm/string/join.hpp>
+#include ""common/logging.h""
+#include ""rapidjson/document.h""
+#include ""rapidjson/stringbuffer.h""
+#include ""rapidjson/writer.h""
+
+namespace doris {
+
+SearchRequestBuilder::SearchRequestBuilder() {
+
+}
+
+SearchRequestBuilder::~SearchRequestBuilder() {
+    
+}
+
+std::string SearchRequestBuilder::build() {
+    rapidjson::Document es_query_dsl;
+    rapidjson::Document::AllocatorType &allocator = es_query_dsl.GetAllocator();
+    es_query_dsl.SetObject();
+    if (_fields.size() > 0) {
+        rapidjson::Value source_node(rapidjson::kArrayType);
+        for (auto iter = _fields.cbegin(); iter != _fields.cend(); iter++) {","[{'comment': 'why do you call `cbegin` other `begin`. Could you add some comment for this?', 'commenter': 'imay'}, {'comment': 'I will replace cxxx with xxx', 'commenter': 'wuyunfeng'}]"
800,be/src/util/es_search_builder.h,"@@ -0,0 +1,45 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+
+#pragma once
+#include<string>
+#include<vector>
+
+namespace doris {
+
+class SearchRequestBuilder {
+
+private:","[{'comment': 'We prefer `public` ahead of `private`', 'commenter': 'imay'}]"
800,be/src/util/es_scan_reader.cpp,"@@ -0,0 +1,208 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include <string>
+#include <sstream>
+#include ""es_scan_reader.h""
+#include ""rapidjson/writer.h""
+#include ""rapidjson/document.h""
+#include ""rapidjson/stringbuffer.h""
+#include ""common/logging.h""
+#include ""common/status.h""
+#include <map>
+
+namespace doris {
+const std::string REUQEST_SCROLL_FILTER_PATH = ""filter_path=_scroll_id,hits.hits._source,hits.total,_id,hits.hits._source.fields"";
+const std::string REQUEST_SCROLL_PATH = ""_scroll"";
+const std::string REQUEST_PREFERENCE_PREFIX = ""&preference=shards:"";
+const std::string REQUEST_SEARCH_SCROLL_PATH = ""/_search/scroll"";
+const std::string REQUEST_SEPARATOR = ""/"";
+const std::string REQUEST_SCROLL_TIME = ""5m"";
+
+const char* FIELD_SCROLL_ID = ""_scroll_id"";
+const char* FIELD_HITS = ""hits"";
+const char* FIELD_INNER_HITS = ""hits"";
+const char* FIELD_SOURCE = ""_source"";
+const char* FIELD_TOTAL = ""total"";
+
+ESScanReader::ESScanReader(const std::string& target, uint16_t size,std::map<std::string, std::string>& props) {
+    LOG(INFO) << ""ESScanReader "";
+    _target = target;
+    _batch_size = size;
+    _index = props.at(KEY_INDEX);
+    _type = props.at(KEY_TYPE);
+    if (props.find(KEY_USER_NAME) != props.end()) {
+        _user_name = props.at(KEY_USER_NAME);
+    }
+    if (props.find(KEY_PASS_WORD) != props.end()){
+        _passwd = props.at(KEY_PASS_WORD);
+    }
+    if (props.find(KEY_SHARDS) != props.end()) {
+        _shards = props.at(KEY_SHARDS);
+    }
+    if (props.find(KEY_QUERY) != props.end()) {
+        _query = props.at(KEY_QUERY);
+    }
+    _init_scroll_url = _target + REQUEST_SEPARATOR + _index + REQUEST_SEPARATOR + _type + ""/_search?scroll="" + REQUEST_SCROLL_TIME + REQUEST_PREFERENCE_PREFIX + _shards + ""&"" + REUQEST_SCROLL_FILTER_PATH;
+    _next_scroll_url = _target + REQUEST_SEARCH_SCROLL_PATH + ""?"" + REUQEST_SCROLL_FILTER_PATH;
+    _eos = false;
+}
+
+ESScanReader::~ESScanReader() {
+    if (_cached_response != nullptr) {
+        free(_cached_response);
+        _cached_response = nullptr;
+    }
+}
+
+Status ESScanReader::open() {
+    _is_first = true;
+    RETURN_IF_ERROR(_network_client.init(_init_scroll_url));
+    _network_client.set_basic_auth(_user_name, _passwd);
+    _network_client.set_content_type(""application/json"");
+    // phase open, we cached the first response for `get_next` phase
+    _network_client.execute_post_request(_query, _cached_response);
+    long status = _network_client.get_http_status();
+    if (status != 200) {
+        LOG(WARNING) << ""invalid response http status for open: "" << status;
+        return Status(*_cached_response);
+    }
+    VLOG(1) << ""open _cached response: "" << *_cached_response;
+    rapidjson::Document document_node;","[{'comment': ""You'd better to write a EsResultParser to parse json, and it is easy to do unit test for this parser"", 'commenter': 'imay'}, {'comment': 'ESScanReader only obtain scroll id and hits document size, not responsible for parse json', 'commenter': 'wuyunfeng'}]"
804,be/test/olap/olap_snapshot_converter_test.cpp,"@@ -0,0 +1,240 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include <iostream>
+#include <string>
+#include <sstream>
+#include <fstream>
+
+#include ""gtest/gtest.h""
+#include ""gmock/gmock.h""
+#include ""olap/lru_cache.h""
+#include ""olap/olap_meta.h""
+#include ""olap/olap_snapshot_converter.h""
+#include ""olap/rowset/rowset_meta_manager.h""
+#include ""olap/rowset/alpha_rowset.h""
+#include ""olap/rowset/alpha_rowset_meta.h""
+#include ""olap/txn_manager.h""
+#include ""olap/new_status.h""
+#include <boost/algorithm/string.hpp>
+#include ""boost/filesystem.hpp""
+#include ""json2pb/json_to_pb.h""
+
+#ifndef BE_TEST
+#define BE_TEST
+#endif
+
+using ::testing::_;
+using ::testing::Return;
+using ::testing::SetArgPointee;
+using std::string;
+
+namespace doris {
+
+class OlapSnapshotConverterTest : public testing::Test {
+public:
+    virtual void SetUp() {
+        auto cache = new_lru_cache(config::file_descriptor_cache_capacity);
+        FileHandler::set_fd_cache(cache);
+        string test_engine_data_path = ""./be/test/olap/test_data/converter_test_data/data"";
+        _engine_data_path = ""./be/test/olap/test_data/converter_test_data/tmp"";
+        boost::filesystem::remove_all(_engine_data_path);
+        create_dirs(_engine_data_path);
+        _data_dir = new DataDir(_engine_data_path, 1000000000);
+        _data_dir->init();
+        _meta_path = ""./meta"";
+        string tmp_data_path = _engine_data_path + ""/data""; 
+        if (boost::filesystem::exists(tmp_data_path)) {
+            boost::filesystem::remove_all(tmp_data_path);
+        }
+        copy_dir(test_engine_data_path, tmp_data_path);
+        _tablet_id = 15007;
+        _schema_hash = 368169781;
+        _tablet_data_path = tmp_data_path 
+                + ""/"" + std::to_string(0)
+                + ""/"" + std::to_string(_tablet_id)
+                + ""/"" + std::to_string(_schema_hash);
+        if (boost::filesystem::exists(_meta_path)) {
+            boost::filesystem::remove_all(_meta_path);
+        }
+        ASSERT_TRUE(boost::filesystem::create_directory(_meta_path));
+        ASSERT_TRUE(boost::filesystem::exists(_meta_path));
+        _meta = new(std::nothrow) OlapMeta(_meta_path);
+        ASSERT_NE(nullptr, _meta);
+        OLAPStatus st = _meta->init();
+        ASSERT_TRUE(st == OLAP_SUCCESS);
+    }
+
+    virtual void TearDown() {
+        delete _meta;
+        delete _data_dir;
+        if (boost::filesystem::exists(_meta_path)) {
+            ASSERT_TRUE(boost::filesystem::remove_all(_meta_path));
+        }
+        //if (boost::filesystem::exists(_engine_data_path)) {","[{'comment': 'Not used code can be removed.', 'commenter': 'chaoyli'}, {'comment': ""It's a mistake, it is useful."", 'commenter': 'yiguolei'}]"
806,be/src/olap/txn_manager.cpp,"@@ -95,11 +95,11 @@ OLAPStatus TxnManager::prepare_txn(
         if (load_itr != it->second.end()) {
             // found load for txn,tablet
             // case 1: user commit rowset, then the load id must be equal
-            std::pair<PUniqueId, RowsetSharedPtr>& load_info = load_itr->second;
+            TabletTxnInfo& load_info = load_itr->second;
             // check if load id is equal
-            if (load_info.first.hi() == load_id.hi()
-                && load_info.first.lo() == load_id.lo()
-                && load_info.second != NULL) {
+            if (load_info.load_id.hi() == load_id.hi()
+                && load_info.load_id.lo() == load_id.lo()
+                && load_info.rowset != NULL) {","[{'comment': 'load_info.rowset != nullptr', 'commenter': 'kangpinghuang'}]"
806,be/src/olap/txn_manager.cpp,"@@ -111,7 +111,7 @@ OLAPStatus TxnManager::prepare_txn(
     // not found load id
     // case 1: user start a new txn, rowset_ptr = null
     // case 2: loading txn from meta env
-    std::pair<PUniqueId, RowsetSharedPtr> load_info(load_id, NULL);
+    TabletTxnInfo load_info(load_id, NULL);","[{'comment': 'NULL -> nullptr', 'commenter': 'kangpinghuang'}]"
806,be/src/olap/txn_manager.cpp,"@@ -143,22 +143,22 @@ OLAPStatus TxnManager::commit_txn(
             if (load_itr != it->second.end()) {
                 // found load for txn,tablet
                 // case 1: user commit rowset, then the load id must be equal
-                std::pair<PUniqueId, RowsetSharedPtr>& load_info = load_itr->second;
+                TabletTxnInfo& load_info = load_itr->second;
                 // check if load id is equal
-                if (load_info.first.hi() == load_id.hi()
-                    && load_info.first.lo() == load_id.lo()
-                    && load_info.second != NULL
-                    && load_info.second->rowset_id() == rowset_ptr->rowset_id()) {
+                if (load_info.load_id.hi() == load_id.hi()
+                    && load_info.load_id.lo() == load_id.lo()
+                    && load_info.rowset != NULL","[{'comment': 'null', 'commenter': 'kangpinghuang'}]"
806,be/src/olap/txn_manager.cpp,"@@ -143,22 +143,22 @@ OLAPStatus TxnManager::commit_txn(
             if (load_itr != it->second.end()) {
                 // found load for txn,tablet
                 // case 1: user commit rowset, then the load id must be equal
-                std::pair<PUniqueId, RowsetSharedPtr>& load_info = load_itr->second;
+                TabletTxnInfo& load_info = load_itr->second;
                 // check if load id is equal
-                if (load_info.first.hi() == load_id.hi()
-                    && load_info.first.lo() == load_id.lo()
-                    && load_info.second != NULL
-                    && load_info.second->rowset_id() == rowset_ptr->rowset_id()) {
+                if (load_info.load_id.hi() == load_id.hi()
+                    && load_info.load_id.lo() == load_id.lo()
+                    && load_info.rowset != NULL
+                    && load_info.rowset->rowset_id() == rowset_ptr->rowset_id()) {
                     // find a rowset with same rowset id, then it means a duplicate call
                     LOG(INFO) << ""find transaction exists when add to engine.""
                               << ""partition_id: "" << key.first
                               << "", transaction_id: "" << key.second
                               << "", tablet: "" << tablet_info.to_string();
                     return OLAP_SUCCESS;
-                } else if (load_info.first.hi() == load_id.hi()
-                    && load_info.first.lo() == load_id.lo()
-                    && load_info.second != NULL
-                    && load_info.second->rowset_id() != rowset_ptr->rowset_id()) {
+                } else if (load_info.load_id.hi() == load_id.hi()
+                    && load_info.load_id.lo() == load_id.lo()
+                    && load_info.rowset != NULL","[{'comment': 'NULL', 'commenter': 'kangpinghuang'}]"
806,be/src/olap/txn_manager.cpp,"@@ -270,8 +270,8 @@ OLAPStatus TxnManager::rollback_txn(TPartitionId partition_id, TTransactionId tr
         if (load_itr != it->second.end()) {
             // found load for txn,tablet
             // case 1: user commit rowset, then the load id must be equal
-            std::pair<PUniqueId, RowsetSharedPtr>& load_info = load_itr->second;
-            if (load_info.second != NULL) {
+            TabletTxnInfo& load_info = load_itr->second;
+            if (load_info.rowset != NULL) {","[{'comment': 'NULL', 'commenter': 'kangpinghuang'}]"
806,be/src/olap/txn_manager.cpp,"@@ -304,22 +304,22 @@ OLAPStatus TxnManager::delete_txn(OlapMeta* meta, TPartitionId partition_id, TTr
         if (load_itr != it->second.end()) {
             // found load for txn,tablet
             // case 1: user commit rowset, then the load id must be equal
-            std::pair<PUniqueId, RowsetSharedPtr>& load_info = load_itr->second;
-            if (load_info.second != NULL && meta != NULL) {
-                if (load_info.second->version().first > 0) { 
+            TabletTxnInfo& load_info = load_itr->second;
+            if (load_info.rowset != NULL && meta != NULL) {","[{'comment': 'NULL', 'commenter': 'kangpinghuang'}]"
806,be/src/olap/txn_manager.cpp,"@@ -409,29 +409,38 @@ bool TxnManager::has_committed_txn(TPartitionId partition_id, TTransactionId tra
         if (load_itr != it->second.end()) {
             // found load for txn,tablet
             // case 1: user commit rowset, then the load id must be equal
-            std::pair<PUniqueId, RowsetSharedPtr>& load_info = load_itr->second;
-            if (load_info.second != NULL) {
+            TabletTxnInfo& load_info = load_itr->second;
+            if (load_info.rowset != NULL) {","[{'comment': 'null', 'commenter': 'kangpinghuang'}]"
806,be/src/olap/txn_manager.cpp,"@@ -409,29 +409,38 @@ bool TxnManager::has_committed_txn(TPartitionId partition_id, TTransactionId tra
         if (load_itr != it->second.end()) {
             // found load for txn,tablet
             // case 1: user commit rowset, then the load id must be equal
-            std::pair<PUniqueId, RowsetSharedPtr>& load_info = load_itr->second;
-            if (load_info.second != NULL) {
+            TabletTxnInfo& load_info = load_itr->second;
+            if (load_info.rowset != NULL) {
                 return true;
             }
         }
     }
     return false;
 }
 
-bool TxnManager::get_expire_txns(TTabletId tablet_id, std::vector<int64_t>* transaction_ids) {
-    /*
+bool TxnManager::get_expire_txns(TTabletId tablet_id, SchemaHash schema_hash, std::vector<int64_t>* transaction_ids) {
+    if (transaction_ids == nullptr) {
+        LOG(WARNING) << ""parameter is null when get_expire_txns by tablet"";
+        return false;
+    }
     time_t now = time(NULL);","[{'comment': 'NULL', 'commenter': 'kangpinghuang'}]"
806,be/src/olap/txn_manager.h,"@@ -48,6 +48,22 @@
 
 namespace doris {
 
+struct TabletTxnInfo {
+    PUniqueId load_id;
+    RowsetSharedPtr rowset;
+    int64_t creation_time;
+
+    TabletTxnInfo(
+        PUniqueId load_id,
+        RowsetSharedPtr rowset) :
+        load_id(load_id),
+        rowset(rowset) {
+        creation_time = time(NULL);","[{'comment': 'the name is the same:load_id(load_id)? IS there no problems?\r\nand creation_time can also be put in initialization list,\r\ncreate_time(time(nullptr))', 'commenter': 'kangpinghuang'}]"
832,be/src/exprs/aggregate_functions.cpp,"@@ -1241,6 +1241,38 @@ int64_t AggregateFunctions::hll_algorithm(const doris_udf::StringVal& src) {
     return (int64_t)(estimate + 0.5);
 }
 
+void AggregateFunctions::hll_raw_agg_init(
+        FunctionContext* ctx,
+        StringVal* dst) {
+    hll_union_agg_init(ctx, dst);
+}
+
+void AggregateFunctions::hll_raw_agg_update(
+        FunctionContext* ctx,
+        const StringVal& src,
+        StringVal* dst) {
+    hll_union_agg_update(ctx, src, dst);
+}
+
+void AggregateFunctions::hll_raw_agg_merge(
+        FunctionContext* ctx,
+        const StringVal& src,
+        StringVal* dst) {
+    hll_union_agg_merge(ctx, src, dst);
+}
+
+doris_udf::StringVal AggregateFunctions::hll_raw_agg_finalize(
+        doris_udf::FunctionContext* ctx,
+        const StringVal& src) {
+    DCHECK(!src.is_null);
+    DCHECK_EQ(src.len, HLL_SETS_BYTES_NUM);
+
+    StringVal result(ctx, src.len+1);","[{'comment': '```suggestion\r\n    StringVal result(ctx, src.len + 1);\r\n```', 'commenter': 'imay'}]"
832,be/src/exprs/aggregate_functions.cpp,"@@ -1241,6 +1241,38 @@ int64_t AggregateFunctions::hll_algorithm(const doris_udf::StringVal& src) {
     return (int64_t)(estimate + 0.5);
 }
 
+void AggregateFunctions::hll_raw_agg_init(
+        FunctionContext* ctx,
+        StringVal* dst) {
+    hll_union_agg_init(ctx, dst);
+}","[{'comment': 'All calculations can be based on full type, so dst will be initted to a hll full type .', 'commenter': 'chenhao7253886'}]"
832,be/src/exprs/aggregate_functions.cpp,"@@ -25,6 +23,7 @@
 #include ""runtime/string_value.h""
 #include ""runtime/datetime_value.h""
 #include ""exprs/anyval_util.h""
+#include ""exprs/aggregate_functions.h""","[{'comment': 'You\'d better not move `#include ""exprs/aggregate_functions.h""`, we put this first to make sure that all need includes is written in header files', 'commenter': 'imay'}]"
832,docs/help/Contents/Data Definition/ddl_stmt.md,"@@ -952,38 +955,40 @@
 
       b. 使用数据中的某一列生成hll列
         curl --location-trusted -uname:password -T data http://host/api/test_db/test/_load?label=load_1\&hll=set1,cuid:set2,os
-            \&columns=time,id,name,province,sex,cuid,os
+            \&columns=dt,id,name,province,sex,cuid,os
 
     3. 聚合数据，常用方式3种：（如果不聚合直接对base表查询，速度可能跟直接使用ndv速度差不多）
 
       a. 创建一个rollup，让hll列产生聚合，
-        alter table test add rollup test_rollup(date, set1);
+        alter table test add rollup test_rollup(dt, set1);
         
       b. 创建另外一张专门计算uv的表，然后insert数据）
     
         create table test_uv(
-        time date,
+        dt date,
         uv_set hll hll_union)
         distributed by hash(id) buckets 32;
 
-        insert into test_uv select date, set1 from test;
+        insert into test_uv select dt, set1 from test;
         
       c. 创建另外一张专门计算uv的表，然后insert并通过hll_hash根据test其它非hll列生成hll列
       
         create table test_uv(
-        time date,
+        dt date,
         id_set hll hll_union)
         distributed by hash(id) buckets 32;
         
-        insert into test_uv select date, hll_hash(id) from test;
+        insert into test_uv select dt, hll_hash(id) from test;
             
     4. 查询，hll列不允许直接查询它的原始值，可以通过配套的函数进行查询
     
       a. 求总uv
         select HLL_UNION_AGG(uv_set) from test_uv;
             
       b. 求每一天的uv
-        select HLL_CARDINALITY(uv_set) from test_uv;
+        select dt, HLL_CARDINALITY(uv_set) from test_uv;
+        select dt, HLL_CARDINALITY(uv) from (select dt, HLL_RAW_AGG(set1) as uv from test group by dt) tmp;
+        select dt, HLL_UNION_AGG(set1) as uv from test group by dt;","[{'comment': ""I think that these three queries don't lead to same result. \r\n\r\n1 would return many rows for one `dt` value\r\n2 and 3 queries would group by `dt` column and return one row for each value\r\n\r\nSo, you should split them into different examples"", 'commenter': 'imay'}]"
832,be/src/udf/udf.h,"@@ -762,6 +766,39 @@ struct LargeIntVal : public AnyVal {
     }
 };
 
+struct HllVal : public StringVal {
+
+    HllVal() : StringVal() { }
+
+    void init(FunctionContext* ctx) {
+        len = doris::HLL_COLUMN_DEFAULT_LEN;
+        is_null = false;
+        ptr = ctx->allocate(len);
+        memset(ptr, 0, len);
+        // the HLL type is HLL_DATA_FULL in UDF or UDAF
+        ptr[0] = doris::HllDataType::HLL_DATA_FULL;
+    }
+
+    doris::HllDataType getHllType() {
+        DCHECK(!is_null);
+
+        return (doris::HllDataType)(ptr[0]);
+    }
+
+    uint8_t &getHllData(int idx) const {","[{'comment': ""Don't return a reference value, you can add a function named `setHllData` "", 'commenter': 'imay'}]"
832,be/src/exprs/aggregate_functions.h,"@@ -324,22 +324,45 @@ dst);
 
     //  HLL value type calculate
     //  init sets buffer
-    static void hll_union_agg_init(doris_udf::FunctionContext*, doris_udf::StringVal* slot);
+    static void hll_union_agg_init(doris_udf::FunctionContext*, doris_udf::HllVal* slot);
     // fill all register accroading to hll set type
-    static void hll_union_agg_update(doris_udf::FunctionContext*, const doris_udf::StringVal& src, 
-                                     doris_udf::StringVal* dst);
+    static void hll_union_agg_update(doris_udf::FunctionContext*, const doris_udf::HllVal& src,
+                                     doris_udf::HllVal* dst);
     // merge the register value
     static void hll_union_agg_merge(
                           doris_udf::FunctionContext*,
-                          const doris_udf::StringVal& src,
-                          doris_udf::StringVal* dst);
+                          const doris_udf::HllVal& src,
+                          doris_udf::HllVal* dst);
     // return result
-    static doris_udf::StringVal hll_union_agg_finalize(
+    static doris_udf::BigIntVal hll_union_agg_finalize(
                                             doris_udf::FunctionContext*,
-                                            const doris_udf::StringVal& src);
+                                            const doris_udf::HllVal& src);
     // calculate result
-    static int64_t hll_algorithm(const doris_udf::StringVal& src);
-    static void hll_union_parse_and_cal(HllSetResolver& resolver, StringVal* dst);
+    static int64_t hll_algorithm(uint8_t *pdata, int data_len);
+    static int64_t hll_algorithm(const StringVal &dst) {
+        return hll_algorithm(dst.ptr, dst.len);
+    }
+    static int64_t hll_algorithm(const HllVal &dst) {
+        return hll_algorithm(dst.ptr+1, dst.len-1);","[{'comment': '```suggestion\r\n        return hll_algorithm(dst.ptr + 1, dst.len - 1);\r\n```', 'commenter': 'imay'}]"
832,be/src/udf/udf.h,"@@ -762,6 +766,45 @@ struct LargeIntVal : public AnyVal {
     }
 };
 
+struct HllVal : public StringVal {
+","[{'comment': 'I think HllFullVal is a better name, because your HllVal only represent the Full type.', 'commenter': 'chenhao7253886'}]"
832,be/src/exprs/aggregate_functions.cpp,"@@ -1114,50 +1115,46 @@ StringVal AggregateFunctions::hll_finalize(FunctionContext* ctx, const StringVal
 }
 
     
-void AggregateFunctions::hll_union_agg_init(FunctionContext* ctx, StringVal* dst) {
-    int str_len = std::pow(2, HLL_PRECISION);
-    dst->is_null = false;
-    dst->ptr = ctx->allocate(str_len);
-    dst->len = str_len;
-    memset(dst->ptr, 0, str_len);
+void AggregateFunctions::hll_union_agg_init(FunctionContext* ctx, HllVal* dst) {
+    dst->init(ctx);
 }
 
-void AggregateFunctions::hll_union_parse_and_cal(HllSetResolver& resolver, StringVal* dst) {
+void AggregateFunctions::hll_union_parse_and_cal(HllSetResolver& resolver, HllVal* dst) {
     ","[{'comment': 'hll_union_parse_and_cal can be a member of HllVal， and it support a function that merge other type or src StringVal, so we will get a better encapsulation for hll.', 'commenter': 'chenhao7253886'}]"
832,be/src/exprs/aggregate_functions.cpp,"@@ -1166,32 +1163,26 @@ void AggregateFunctions::hll_union_agg_update(FunctionContext* ctx,
     return ;
 }
 
-void AggregateFunctions::hll_union_agg_merge(FunctionContext* ctx, const StringVal& src,
-                                   StringVal* dst) {
+void AggregateFunctions::hll_union_agg_merge(FunctionContext* ctx, const HllVal& src, HllVal* dst) {
     DCHECK(!dst->is_null);
     DCHECK(!src.is_null);
-    DCHECK_EQ(dst->len, HLL_SETS_BYTES_NUM);
-    DCHECK_EQ(src.len, HLL_SETS_BYTES_NUM);
+    DCHECK_EQ(dst->getHllDataLen(), HLL_SETS_BYTES_NUM);
+    DCHECK_EQ(src.getHllDataLen(), HLL_SETS_BYTES_NUM);
      
     for (int i = 0; i < src.len; ++i) {
-        dst->ptr[i] = std::max(dst->ptr[i], src.ptr[i]);
+        dst->setHllData(i, std::max(dst->getHllData(i), src.getHllData(i)));
     }","[{'comment': 'HLLVal can support a member of merge other HllVal.', 'commenter': 'chenhao7253886'}]"
832,be/src/udf/udf.h,"@@ -762,6 +766,45 @@ struct LargeIntVal : public AnyVal {
     }
 };
 
+struct HllVal : public StringVal {
+
+    HllVal() : StringVal() { }
+
+    void init(FunctionContext* ctx) {
+        len = doris::HLL_COLUMN_DEFAULT_LEN;
+        is_null = false;
+        ptr = ctx->allocate(len);
+        memset(ptr, 0, len);
+        // the HLL type is HLL_DATA_FULL in UDF or UDAF
+        ptr[0] = doris::HllDataType::HLL_DATA_FULL;
+    }
+
+    doris::HllDataType getHllType() {
+        DCHECK(!is_null);
+
+        return (doris::HllDataType)(ptr[0]);
+    }","[{'comment': 'return doris::HllDataType::HLL_DATA_FULL.', 'commenter': 'chenhao7253886'}]"
851,fe/src/main/java/org/apache/doris/qe/Coordinator.java,"@@ -882,12 +889,60 @@ private void computeFragmentHosts() throws Exception {
         }
     }
 
-    //One fragment could only have one HashJoinNode
-    private boolean isColocateJoin(PlanNode node) {
-        if (Config.disable_colocate_join) {
+    private boolean needParallelInstance(PlanNode leftMostNode) {
+        if (getParallelExecInstanceNum() <= SessionVariable.MIN_EXEC_INSTANCE_NUM) {
             return false;
         }
 
+        if (leftMostNode instanceof OlapScanNode) {
+            OlapScanNode olapScanNode = (OlapScanNode) leftMostNode;
+
+            //case 1: the small table of broadcast join need not parallel
+            PlanFragment destFragment =  olapScanNode.getFragment().getDestFragment();
+            if (destFragment != null)  {
+                PlanNode destRootNode = destFragment.getPlanRoot();
+                if (destRootNode instanceof HashJoinNode) {
+                    HashJoinNode joinNode = (HashJoinNode) destRootNode;
+                    if (!joinNode.isShuffleJoin()) {
+                        LOG.debug(""ScanNode {} for fragment {} need not parallel because of broadcast join"",
+                        olapScanNode.getOlapTable().getName(), olapScanNode.getFragmentId());
+                        return false;
+                    }
+                }
+            }
+
+            //case 2: the small table scan  need not parallel
+            long scanDataSize =  Math.round((double) olapScanNode.getCardinality() * olapScanNode.getAvgRowSize());
+            long smallScanSize = Config.parallel_scan_data_size_threshold;
+            if (scanDataSize <= smallScanSize) {
+                LOG.debug(""ScanNode {} for fragment {} need not parallel because of data size {} is small"",
+                olapScanNode.getOlapTable().getName(), olapScanNode.getFragmentId(), scanDataSize);
+                return false;
+            }
+        }
+
+        return true;
+    }
+
+    //get ParallelScanInstanceNum from session config or server config
+    private int getParallelExecInstanceNum() {
+        //cache the result. this method will call many times
+        if (parallelExecInstanceNum > 0) {
+            return parallelExecInstanceNum;
+        }
+
+        int sessionParallelExecInstanceNum = ConnectContext.get().getSessionVariable().getParallelExecInstanceNum();
+        //we prefer session config
+        if (sessionParallelExecInstanceNum > SessionVariable.MIN_EXEC_INSTANCE_NUM) {
+            parallelExecInstanceNum = sessionParallelExecInstanceNum;
+        } else {
+            parallelExecInstanceNum = Config.parallel_fragment_exec_instance_num;
+        }","[{'comment': 'I think that one session variable is enough.', 'commenter': 'imay'}, {'comment': ""Config.parallel_fragment_exec_instance_num is redundant, you can use 'set global variable_name = ...' for all sessions. and i think it is unnecessary to assign value in Else."", 'commenter': 'chenhao7253886'}, {'comment': 'OK', 'commenter': 'kangkaisen'}, {'comment': 'OK', 'commenter': 'kangkaisen'}]"
851,fe/src/main/java/org/apache/doris/qe/Coordinator.java,"@@ -882,12 +889,60 @@ private void computeFragmentHosts() throws Exception {
         }
     }
 
-    //One fragment could only have one HashJoinNode
-    private boolean isColocateJoin(PlanNode node) {
-        if (Config.disable_colocate_join) {
+    private boolean needParallelInstance(PlanNode leftMostNode) {
+        if (getParallelExecInstanceNum() <= SessionVariable.MIN_EXEC_INSTANCE_NUM) {
             return false;
         }
 
+        if (leftMostNode instanceof OlapScanNode) {
+            OlapScanNode olapScanNode = (OlapScanNode) leftMostNode;
+
+            //case 1: the small table of broadcast join need not parallel
+            PlanFragment destFragment =  olapScanNode.getFragment().getDestFragment();
+            if (destFragment != null)  {
+                PlanNode destRootNode = destFragment.getPlanRoot();
+                if (destRootNode instanceof HashJoinNode) {
+                    HashJoinNode joinNode = (HashJoinNode) destRootNode;
+                    if (!joinNode.isShuffleJoin()) {
+                        LOG.debug(""ScanNode {} for fragment {} need not parallel because of broadcast join"",
+                        olapScanNode.getOlapTable().getName(), olapScanNode.getFragmentId());
+                        return false;
+                    }
+                }
+            }
+
+            //case 2: the small table scan  need not parallel
+            long scanDataSize =  Math.round((double) olapScanNode.getCardinality() * olapScanNode.getAvgRowSize());
+            long smallScanSize = Config.parallel_scan_data_size_threshold;
+            if (scanDataSize <= smallScanSize) {
+                LOG.debug(""ScanNode {} for fragment {} need not parallel because of data size {} is small"",
+                olapScanNode.getOlapTable().getName(), olapScanNode.getFragmentId(), scanDataSize);
+                return false;
+            }
+        }
+
+        return true;
+    }
+
+    //get ParallelScanInstanceNum from session config or server config
+    private int getParallelExecInstanceNum() {
+        //cache the result. this method will call many times
+        if (parallelExecInstanceNum > 0) {
+            return parallelExecInstanceNum;
+        }
+
+        int sessionParallelExecInstanceNum = ConnectContext.get().getSessionVariable().getParallelExecInstanceNum();
+        //we prefer session config
+        if (sessionParallelExecInstanceNum > SessionVariable.MIN_EXEC_INSTANCE_NUM) {","[{'comment': 'And do we need to check this value. We should check when we set this session variable', 'commenter': 'imay'}, {'comment': 'The logic is If session config id default value, we use the server config.', 'commenter': 'kangkaisen'}]"
851,fe/src/main/java/org/apache/doris/planner/SingleNodePlanner.java,"@@ -158,15 +158,8 @@ public PlanNode createSingleNodePlan() throws UserException, AnalysisException {
      * Throws a NotImplementedException if plan validation fails.
      */
     public void validatePlan(PlanNode planNode) throws NotImplementedException {
-      if (ctx_.getQueryOptions().isSetMt_dop() && ctx_.getQueryOptions().mt_dop > 0
-          && (planNode instanceof HashJoinNode || planNode instanceof CrossJoinNode)) {
-          throw new NotImplementedException(
-              ""MT_DOP not supported for plans with base table joins or table sinks."");
-      }
-
       // As long as MT_DOP is unset or 0 any join can run in a single-node plan.
-      if (ctx_.isSingleNodeExec() &&
-          (!ctx_.getQueryOptions().isSetMt_dop() || ctx_.getQueryOptions().mt_dop == 0)) {
+      if (ctx_.isSingleNodeExec()) {
           return;","[{'comment': 'validatePlan is unused.', 'commenter': 'chenhao7253886'}, {'comment': 'OK', 'commenter': 'kangkaisen'}]"
851,fe/src/main/java/org/apache/doris/qe/Coordinator.java,"@@ -882,12 +889,60 @@ private void computeFragmentHosts() throws Exception {
         }
     }
 
-    //One fragment could only have one HashJoinNode
-    private boolean isColocateJoin(PlanNode node) {
-        if (Config.disable_colocate_join) {
+    private boolean needParallelInstance(PlanNode leftMostNode) {
+        if (getParallelExecInstanceNum() <= SessionVariable.MIN_EXEC_INSTANCE_NUM) {","[{'comment': 'I prefer 1 to `SessionVariable.MIN_EXEC_INSTANCE_NUM` ', 'commenter': 'imay'}, {'comment': 'OK', 'commenter': 'kangkaisen'}]"
851,fe/src/main/java/org/apache/doris/qe/SessionVariable.java,"@@ -418,7 +414,19 @@ public void setDisableColocateJoin(boolean disableColocateJoin) {
         this.disableColocateJoin = disableColocateJoin;
     }
 
-    // Serialize to thrift object
+    public int getParallelExecInstanceNum() {
+        return parallelExecInstanceNum;
+    }
+
+    public void setParallelExecInstanceNum(int parallelExecInstanceNum) {
+        if (parallelExecInstanceNum < MIN_EXEC_INSTANCE_NUM) {","[{'comment': 'ParallelExecInstanceNum should have a upper boundary，such as 32, 64 or 128. ', 'commenter': 'chenhao7253886'}, {'comment': 'OK', 'commenter': 'kangkaisen'}]"
851,fe/src/main/java/org/apache/doris/qe/Coordinator.java,"@@ -882,12 +889,60 @@ private void computeFragmentHosts() throws Exception {
         }
     }
 
-    //One fragment could only have one HashJoinNode
-    private boolean isColocateJoin(PlanNode node) {
-        if (Config.disable_colocate_join) {
+    private boolean needParallelInstance(PlanNode leftMostNode) {
+        if (getParallelExecInstanceNum() <= SessionVariable.MIN_EXEC_INSTANCE_NUM) {
             return false;
         }
 
+        if (leftMostNode instanceof OlapScanNode) {
+            OlapScanNode olapScanNode = (OlapScanNode) leftMostNode;
+
+            //case 1: the small table of broadcast join need not parallel
+            PlanFragment destFragment =  olapScanNode.getFragment().getDestFragment();
+            if (destFragment != null)  {
+                PlanNode destRootNode = destFragment.getPlanRoot();
+                if (destRootNode instanceof HashJoinNode) {
+                    HashJoinNode joinNode = (HashJoinNode) destRootNode;
+                    if (!joinNode.isShuffleJoin()) {
+                        LOG.debug(""ScanNode {} for fragment {} need not parallel because of broadcast join"",
+                        olapScanNode.getOlapTable().getName(), olapScanNode.getFragmentId());
+                        return false;
+                    }
+                }","[{'comment': ""Does this works? If the fragment is agg(hash(scan1, scan2)), for scan2, the RootNode of `destFragment` is `agg`.\r\n\r\nI think if it isn't easy to match this case, we can return true for it. \r\n\r\nFor broadcast join, I think the bigger influence is that the outer child will be parallel, and making it would need more hash table for inner child. So it's ok to scan parallel here"", 'commenter': 'imay'}, {'comment': ""Yes. It's doesn't work for  agg(hash(scan1, scan2)).\r\n\r\nSo, should we parallel for all query? \r\nIn my test, even if we parallel for small table scan, the performance loss is very little."", 'commenter': 'kangkaisen'}, {'comment': 'For simple, I think we can parallel for all queries. \r\nHowever one thing we should notice is that when we enable fragment parallel, FE would send more `PlanFragmentInstance` than before, which would cost FE more CPU to serialize requests and more network bandwidth to transfer requests. Then as the number of concurrent queries increase, there would be some bottleneck for FE.', 'commenter': 'imay'}, {'comment': '> For simple, I think we can parallel for all queries.\r\n\r\nOK.\r\n\r\n\r\n> which would cost FE more CPU to serialize requests and more network bandwidth to transfer requests.\r\n\r\nYes, this is a side-effect for this improve. But which should be acceptable.\r\n', 'commenter': 'kangkaisen'}]"
872,be/src/runtime/snapshot_loader.cpp,"@@ -951,6 +965,8 @@ Status SnapshotLoader::_replace_tablet_id(
         return Status::OK;
     } else if (_end_with(file_name, "".idx"")
             || _end_with(file_name, "".dat"")) {
+        // currently all files is rowset ids without tabletid
+        /**","[{'comment': 'Codes in annotation may be better to be removed.', 'commenter': 'chaoyli'}]"
872,be/src/runtime/snapshot_loader.cpp,"@@ -518,6 +519,26 @@ Status SnapshotLoader::move(
         return Status(ss.str());
     }
 
+
+    DataDir* store = StorageEngine::instance()->get_store(store_path);
+    if (store == nullptr) {
+        std::stringstream ss;","[{'comment': 'These two dir paths checks should before be used.', 'commenter': 'chaoyli'}]"
872,be/src/olap/tablet_meta.cpp,"@@ -355,12 +355,15 @@ OLAPStatus TabletMeta::add_rs_meta(const RowsetMetaSharedPtr& rs_meta) {
     return OLAP_SUCCESS;
 }
 
-OLAPStatus TabletMeta::delete_rs_meta_by_version(const Version& version) {
+OLAPStatus TabletMeta::delete_rs_meta_by_version(const Version& version, vector<RowsetMetaSharedPtr>* deleted_rs_metas) {
     auto it = _rs_metas.begin();
     while (it != _rs_metas.end()) {
         if ((*it)->version().first == version.first
               && (*it)->version().second == version.second) {
             _rs_metas.erase(it);
+            if (deleted_rs_metas != nullptr) {","[{'comment': 'It has be deconstructed after erasure.', 'commenter': 'chaoyli'}]"
872,be/src/olap/rowset/segment_group.cpp,"@@ -785,8 +785,9 @@ OLAPStatus SegmentGroup::remove_old_files(std::vector<std::string>* links_to_rem
 }
 
 OLAPStatus SegmentGroup::copy_segments_to_path(const std::string& dest_path, int64_t rowset_id) {
-    if (dest_path.empty() || dest_path == _rowset_path_prefix) {
-        return OLAP_SUCCESS;
+    if (dest_path.empty()) {
+        LOG(INFO) << ""dest path is empty, return error"";","[{'comment': 'LOG(WARNING)', 'commenter': 'kangpinghuang'}, {'comment': 'done', 'commenter': 'yiguolei'}]"
872,be/src/olap/snapshot_manager.cpp,"@@ -113,6 +118,119 @@ OLAPStatus SnapshotManager::release_snapshot(const string& snapshot_path) {
     return OLAP_ERR_CE_CMD_PARAMS_ERROR;
 }
 
+
+OLAPStatus SnapshotManager::convert_rowset_ids(DataDir& data_dir, const string& clone_dir, int64_t tablet_id, const int32_t& schema_hash) {
+    OLAPStatus res = OLAP_SUCCESS;   
+    // check clone dir existed
+    if (!check_dir_existed(clone_dir)) {
+        res = OLAP_ERR_DIR_NOT_EXIST;
+        LOG(WARNING) << ""clone dir not existed when convert rowsetids. clone_dir="" << clone_dir.c_str();","[{'comment': '<< clone_dir;', 'commenter': 'kangpinghuang'}, {'comment': 'done', 'commenter': 'yiguolei'}]"
872,be/src/olap/snapshot_manager.h,"@@ -70,6 +70,8 @@ class SnapshotManager {
                 
     static SnapshotManager* instance();
 
+    OLAPStatus convert_rowset_ids(DataDir& data_dir, const string& clone_dir, int64_t tablet_id, const int32_t& schema_hash);","[{'comment': 'int64_t tablet_id, const int32_t schema_hash?\r\n', 'commenter': 'kangpinghuang'}, {'comment': 'yes， schema hash is int32', 'commenter': 'yiguolei'}]"
872,be/src/olap/task/engine_clone_task.cpp,"@@ -957,6 +863,15 @@ OLAPStatus EngineCloneTask::_clone_full_data(TabletSharedPtr tablet, TabletMeta*
     // clone_data to tablet
     OLAPStatus clone_res = tablet->revise_tablet_meta(*cloned_tablet_meta, rowsets_to_clone, versions_to_delete);
     LOG(INFO) << ""finish to full clone. tablet="" << tablet->full_name() << "", res="" << clone_res;
+    // in previous step, copy all files from CLONE_DIR to tablet dir
+    // but some rowset is useless, so that remove them here
+    for (auto& rs_meta_ptr : rs_metas_found_in_src) {
+        RowsetSharedPtr org_rowset(new AlphaRowset(&(cloned_tablet_meta->tablet_schema()), 
+            tablet->tablet_path(), tablet->data_dir(), rs_meta_ptr));
+        if (org_rowset->init() == OLAP_SUCCESS && org_rowset->load() == OLAP_SUCCESS) {
+            org_rowset->remove();","[{'comment': 'validate the return value?', 'commenter': 'kangpinghuang'}, {'comment': 'not validate. because if remove old files failed the clone task should also be success. It will depend on gc thread to recycle the rowset to be deleted.', 'commenter': 'yiguolei'}]"
887,be/src/olap/row_block.cpp,"@@ -38,18 +38,33 @@ namespace doris {
 
 RowBlock::RowBlock(const TabletSchema* schema) :
         _capacity(0),
-        _schema(schema) {
+        _schema(schema),
+        _columns(nullptr) {
+    if (_schema != nullptr) {","[{'comment': 'if _schema is nullptr, all following operations is no-sense.', 'commenter': 'chaoyli'}, {'comment': 'OK', 'commenter': 'kangpinghuang'}]"
887,be/src/olap/row_block.h,"@@ -186,7 +189,8 @@ class RowBlock {
     uint32_t _capacity;
     RowBlockInfo _info;
     const TabletSchema* _schema;     // 内部保存的schema句柄
-
+    const std::vector<TabletColumn>* _columns;","[{'comment': 'Why do you need _columns?', 'commenter': 'chaoyli'}, {'comment': 'OK', 'commenter': 'kangpinghuang'}]"
887,be/src/olap/row_cursor.h,"@@ -73,18 +73,27 @@ class RowCursor {
     OLAPStatus init(const TabletSchema& schema,
                     const std::vector<uint32_t>& columns);
 
+    OLAPStatus init(const std::vector<TabletColumn>& schema,
+                    const std::vector<uint32_t>& columns);
+
     // 用传入的key的size来初始化
     // 目前仅用在拆分key区间的时候
     OLAPStatus init_scan_key(const TabletSchema& schema,
                              const std::vector<std::string>& keys);
 
+    OLAPStatus init_scan_key(const std::vector<TabletColumn>& schema,","[{'comment': 'If you call this function, you will need all columns of schema. There is no necessary to overload with std::vector<TabletColumn>& schema parameter.', 'commenter': 'chaoyli'}, {'comment': 'OK', 'commenter': 'kangpinghuang'}]"
887,be/src/olap/row_cursor.h,"@@ -73,18 +73,27 @@ class RowCursor {
     OLAPStatus init(const TabletSchema& schema,
                     const std::vector<uint32_t>& columns);
 
+    OLAPStatus init(const std::vector<TabletColumn>& schema,
+                    const std::vector<uint32_t>& columns);
+
     // 用传入的key的size来初始化
     // 目前仅用在拆分key区间的时候
     OLAPStatus init_scan_key(const TabletSchema& schema,
                              const std::vector<std::string>& keys);
 
+    OLAPStatus init_scan_key(const std::vector<TabletColumn>& schema,
+                             const std::vector<std::string>& keys);
+
     OLAPStatus init_scan_key(const TabletSchema& schema,
                              const std::vector<size_t>& field_lengths);
 
     //allocate memory for string type, which include char, varchar, hyperloglog
     OLAPStatus allocate_memory_for_string_type(const TabletSchema& schema,
                                                MemPool* mem_pool = nullptr);
- 
+    ","[{'comment': 'The same question', 'commenter': 'chaoyli'}, {'comment': 'OK', 'commenter': 'kangpinghuang'}]"
887,be/src/olap/tablet_meta.cpp,"@@ -264,9 +265,6 @@ OLAPStatus TabletMeta::init_from_pb(const TabletMetaPB& tablet_meta_pb) {
     for (auto& it : tablet_meta_pb.rs_metas()) {
         RowsetMetaSharedPtr rs_meta(new AlphaRowsetMeta());
         rs_meta->init_from_pb(it);
-        if (rs_meta->has_delete_predicate()) {","[{'comment': 'If you remove it, where to add delete predicate?', 'commenter': 'chaoyli'}]"
887,be/src/olap/txn_manager.cpp,"@@ -226,7 +226,7 @@ OLAPStatus TxnManager::publish_txn(OlapMeta* meta, TPartitionId partition_id, TT
         rowset_ptr->set_version_and_version_hash(version, version_hash);
         OLAPStatus save_status = RowsetMetaManager::save(meta,
             rowset_ptr->rowset_id(),
-            rowset_ptr->rowset_meta());","[{'comment': 'Why not SharedPtr is not ok?', 'commenter': 'chaoyli'}, {'comment': 'because save api  just use pointer, no need to manage the object', 'commenter': 'kangpinghuang'}]"
887,be/src/runtime/vectorized_row_batch.h,"@@ -139,7 +139,7 @@ class VectorizedRowBatch {
     void dump_to_row_block(RowBlock* row_block);
 
 private:
-    const TabletSchema* _schema;
+    const std::vector<TabletColumn>* _schema;","[{'comment': 'Only in RowCursor, I must to use std::vector<TabletColumn>. Because there exists a situation it only partial TabletColumn. Otherwise, TabletSchema is only parameter can be imported.', 'commenter': 'chaoyli'}, {'comment': 'OK', 'commenter': 'kangpinghuang'}]"
887,be/test/olap/comparison_predicate_test.cpp,"@@ -99,21 +99,24 @@ public: \
             delete _vectorized_batch; \
         } \
     } \
-    void SetFieldInfo(FieldInfo &field_info, std::string name, \
-            FieldType type, FieldAggregationMethod aggregation, \
-            uint32_t length, bool is_allow_null, bool is_key) { \
-        field_info.name = name; \
-        field_info.type = type; \
-        field_info.aggregation = aggregation; \
-        field_info.length = length; \
-        field_info.is_allow_null = is_allow_null; \
-        field_info.is_key = is_key; \
-        field_info.precision = 1000; \
-        field_info.frac = 10000; \
-        field_info.unique_id = 0; \
-        field_info.is_bf_column = false; \
+    void SetTabletColumn(std::string name, \
+            std::string type, std::string aggregation, \","[{'comment': 'const std::string&', 'commenter': 'chaoyli'}, {'comment': 'OK', 'commenter': 'kangpinghuang'}]"
895,be/src/olap/tablet_manager.cpp,"@@ -1086,8 +1090,9 @@ OLAPStatus TabletManager::_drop_tablet_directly_unlocked(
 
     TabletSharedPtr dropped_tablet = _get_tablet_with_no_lock(tablet_id, schema_hash);
     if (dropped_tablet.get() == NULL) {","[{'comment': 'dropped_tablet == nullptr', 'commenter': 'kangpinghuang'}]"
895,be/src/olap/data_dir.h,"@@ -123,7 +123,7 @@ class DataDir {
     Status _write_cluster_id_to_path(const std::string& path, int32_t cluster_id); 
     OLAPStatus _clean_unfinished_converting_data();
     OLAPStatus _convert_old_tablet();
-    OLAPStatus _remove_old_meta_and_files();
+    OLAPStatus _remove_old_meta_and_files(std::set<int64_t>& tablet_ids);","[{'comment': '```suggestion\r\n    OLAPStatus _remove_old_meta_and_files(const std::set<int64_t>& tablet_ids);\r\n```', 'commenter': 'kangpinghuang'}]"
895,be/src/olap/data_dir.cpp,"@@ -604,18 +605,24 @@ OLAPStatus DataDir::_convert_old_tablet() {
     OLAPStatus convert_tablet_status = TabletMetaManager::traverse_headers(_meta, 
         convert_tablet_func, OLD_HEADER_PREFIX);
     if (convert_tablet_status != OLAP_SUCCESS) {
-        LOG(WARNING) << ""there is failure when convert old tablet, data dir:"" << _path;
+        LOG(FATAL) << ""there is failure when convert old tablet, data dir:"" << _path;
         return convert_tablet_status;
     } else {
         LOG(INFO) << ""successfully convert old tablet, data dir: "" << _path;
     }
     return OLAP_SUCCESS;
 }
 
-OLAPStatus DataDir::_remove_old_meta_and_files() {
+OLAPStatus DataDir::_remove_old_meta_and_files(std::set<int64_t>& tablet_ids) {
     // clean old meta(olap header message) 
-    auto clean_old_meta_func = [this](long tablet_id,
+    auto clean_old_meta_func = [this, &tablet_ids](long tablet_id,","[{'comment': 'use int64_t instead', 'commenter': 'chaoyli'}, {'comment': 'done', 'commenter': 'yiguolei'}]"
895,be/src/olap/data_dir.cpp,"@@ -604,18 +605,24 @@ OLAPStatus DataDir::_convert_old_tablet() {
     OLAPStatus convert_tablet_status = TabletMetaManager::traverse_headers(_meta, 
         convert_tablet_func, OLD_HEADER_PREFIX);
     if (convert_tablet_status != OLAP_SUCCESS) {
-        LOG(WARNING) << ""there is failure when convert old tablet, data dir:"" << _path;
+        LOG(FATAL) << ""there is failure when convert old tablet, data dir:"" << _path;
         return convert_tablet_status;
     } else {
         LOG(INFO) << ""successfully convert old tablet, data dir: "" << _path;
     }
     return OLAP_SUCCESS;
 }
 
-OLAPStatus DataDir::_remove_old_meta_and_files() {
+OLAPStatus DataDir::_remove_old_meta_and_files(std::set<int64_t>& tablet_ids) {
     // clean old meta(olap header message) 
-    auto clean_old_meta_func = [this](long tablet_id,
+    auto clean_old_meta_func = [this, &tablet_ids](long tablet_id,
         long schema_hash, const std::string& value) -> bool {
+        if (tablet_ids.count(tablet_id) == 0) {","[{'comment': 'tablet_ids.find()', 'commenter': 'chaoyli'}, {'comment': 'done', 'commenter': 'yiguolei'}]"
898,be/src/olap/rowset/alpha_rowset_writer.cpp,"@@ -115,38 +116,49 @@ OLAPStatus AlphaRowsetWriter::add_row_block(RowBlock* row_block) {
 }
 
 OLAPStatus AlphaRowsetWriter::add_rowset(RowsetSharedPtr rowset) {
+    _need_column_data_writer = false;
     // this api is for LinkedSchemaChange
     // use create hard link to copy rowset for performance
     // this is feasible because LinkedSchemaChange is done on the same disk
     AlphaRowsetSharedPtr alpha_rowset = std::dynamic_pointer_cast<AlphaRowset>(rowset);
     for (auto& segment_group : alpha_rowset->_segment_groups) {
+        RETURN_NOT_OK(_init());
         RETURN_NOT_OK(segment_group->copy_segments_to_path(_rowset_writer_context.rowset_path_prefix,
                                                            _rowset_writer_context.rowset_id));
         _cur_segment_group->set_empty(segment_group->empty());
         _cur_segment_group->set_num_segments(segment_group->num_segments());
         _cur_segment_group->add_zone_maps_for_linked_schema_change(segment_group->get_zone_maps());
-        RETURN_NOT_OK(_cur_segment_group->load());
+        RETURN_NOT_OK(flush());
         _num_rows_written += alpha_rowset->num_rows();
     }
     return OLAP_SUCCESS;
 }
 
 OLAPStatus AlphaRowsetWriter::flush() {
     if (_writer_state == WRITER_FLUSHED) {
-        LOG(WARNING) << ""writer already flushed."";
         return OLAP_SUCCESS;
     }
     DCHECK(_writer_state == WRITER_INITED);
-    // column_data_writer finalize will call segment_group->set_empty()
-    OLAPStatus status = _column_data_writer->finalize();
+    if (_need_column_data_writer) {
+        // column_data_writer finalize will call segment_group->set_empty()
+        RETURN_NOT_OK(_column_data_writer->finalize());
+    }
     SAFE_DELETE(_column_data_writer);
-    _cur_segment_group->load();
+    RETURN_NOT_OK(_cur_segment_group->load());","[{'comment': 'I think load() should defer to build().\r\nTransaction may be failed after you flush(), and load operation will be no-sense.', 'commenter': 'chaoyli'}]"
906,be/src/olap/tablet_meta.cpp,"@@ -580,16 +582,26 @@ bool TabletMeta::version_for_delete_predicate(const Version& version) {
 }
 
 OLAPStatus TabletMeta::add_alter_task(const AlterTabletTask& alter_task) {
-    _alter_task = alter_task;
-    RETURN_NOT_OK(_alter_task.to_alter_pb(_tablet_meta_pb.mutable_alter_tablet_task()));
-    _has_alter_task = true;
+    WriteLock wrlock(&_meta_lock);
+    AlterTabletTask* alter_tablet_task = new AlterTabletTask();
+    *alter_tablet_task = alter_task;
+    RETURN_NOT_OK(alter_tablet_task->to_alter_pb(_tablet_meta_pb.mutable_alter_tablet_task()));
+    _alter_task.reset(alter_tablet_task);
     return OLAP_SUCCESS;
 }
 
 OLAPStatus TabletMeta::delete_alter_task() {
+    WriteLock wrlock(&_meta_lock);
+    _alter_task.reset();
     _tablet_meta_pb.clear_alter_tablet_task();
-    _has_alter_task = false;
     return OLAP_SUCCESS;
 }
 
+void TabletMeta::set_alter_state(AlterTabletState alter_state) {","[{'comment': 'no place to set alter_state?', 'commenter': 'chaoyli'}]"
906,be/src/olap/delta_writer.cpp,"@@ -81,24 +81,22 @@ OLAPStatus DeltaWriter::init() {
                             _req.partition_id, _req.txn_id,
                             _req.tablet_id, _req.schema_hash, _req.load_id));
         if (_req.need_gen_rollup) {
-            _tablet->obtain_header_rdlock();
-            bool has_alter_task = _tablet->has_alter_task();
-            const AlterTabletTask& alter_task = _tablet->alter_task();
-            AlterTabletState alter_state = alter_task.alter_state();
-            TTabletId new_tablet_id = alter_task.related_tablet_id();
-            TSchemaHash new_schema_hash = alter_task.related_schema_hash();;
-            _tablet->release_header_lock();
-
-            if (has_alter_task && alter_state != ALTER_FAILED) {
-                LOG(INFO) << ""load with schema change."" << ""old_tablet_id: "" << _tablet->tablet_id() << "", ""
-                          << ""old_schema_hash: "" << _tablet->schema_hash() <<  "", ""
-                          << ""new_tablet_id: "" << new_tablet_id << "", ""
-                          << ""new_schema_hash: "" << new_schema_hash << "", ""
-                          << ""transaction_id: "" << _req.txn_id;
-                _new_tablet = StorageEngine::instance()->tablet_manager()->get_tablet(new_tablet_id, new_schema_hash);
-                StorageEngine::instance()->txn_manager()->prepare_txn(
-                                    _req.partition_id, _req.txn_id,
-                                    new_tablet_id, new_schema_hash, _req.load_id);
+            AlterTabletTaskSharedPtr alter_task = _tablet->alter_task();
+            if (alter_task != nullptr) {","[{'comment': 'if (alter_task != nullptr && alter_task->alter_state() != ALTER_FAILED) {', 'commenter': 'chaoyli'}]"
923,be/src/exprs/string_functions.cpp,"@@ -516,17 +516,17 @@ StringVal StringFunctions::concat_ws(
         return strs[0];
     }
 
-    if (strs[0].is_null) {
+    if (strs[0].is_null && 0 == sep.len) {","[{'comment': 'NULL is different with empty string. \r\n\r\nFor NULL, this function return NULL; for empty string, return NOT NULL\r\n\r\nyou can reference https://dev.mysql.com/doc/refman/8.0/en/string-functions.html#function_concat-ws', 'commenter': 'imay'}, {'comment': 'Fix done, Sorry~', 'commenter': 'HangyuanLiu'}]"
923,be/src/exprs/string_functions.cpp,"@@ -516,17 +516,17 @@ StringVal StringFunctions::concat_ws(
         return strs[0];
     }
 
-    if (strs[0].is_null) {
+    if (strs[0].is_null && sep.is_null) {","[{'comment': 'If sep.is_null, function has returned in line 511\r\n', 'commenter': 'imay'}]"
923,be/src/exprs/string_functions.cpp,"@@ -519,14 +510,14 @@ StringVal StringFunctions::concat_ws(
     if (strs[0].is_null) {
         return StringVal::null();
     }
-    int32_t total_size = strs[0].len;
 
+    int32_t total_size = strs[0].len;
     // Loop once to compute the final size and reserve space.
     for (int32_t i = 1; i < num_children; ++i) {
         if (strs[i].is_null) {
             return StringVal::null();
         }
-        total_size += sep.len + strs[i].len;
+        total_size += strs[i].is_null ? 0 : strs[i].len;","[{'comment': 'strs[i].is_null must be false.', 'commenter': 'imay'}]"
923,be/src/exprs/string_functions.cpp,"@@ -519,14 +510,14 @@ StringVal StringFunctions::concat_ws(
     if (strs[0].is_null) {","[{'comment': 'put this to for loop', 'commenter': 'imay'}]"
923,be/src/exprs/string_functions.cpp,"@@ -519,14 +510,14 @@ StringVal StringFunctions::concat_ws(
     if (strs[0].is_null) {
         return StringVal::null();
     }
-    int32_t total_size = strs[0].len;
 
+    int32_t total_size = strs[0].len;","[{'comment': 'int32_t total_size = 0;\r\n\r\nfor (int i = 0; i < num_children; ++i)', 'commenter': 'imay'}]"
923,be/src/exprs/string_functions.cpp,"@@ -535,17 +526,47 @@ StringVal StringFunctions::concat_ws(
     uint8_t* ptr = result.ptr;
 
     // Loop again to append the data.
-    memcpy(ptr, strs[0].ptr, strs[0].len);
-    ptr += strs[0].len;
-    for (int32_t i = 1; i < num_children; ++i) {
-        memcpy(ptr, sep.ptr, sep.len);
-        ptr += sep.len;
+    for (int32_t i = 0; i < num_children; ++i) {
         memcpy(ptr, strs[i].ptr, strs[i].len);
         ptr += strs[i].len;
     }
     return result;
 }
 
+StringVal StringFunctions::concat_ws(
+        FunctionContext* context, const StringVal& sep, 
+        int num_children, const StringVal* strs) {
+    DCHECK_GE(num_children, 1);
+    if (sep.is_null) {
+        return StringVal::null();
+    }
+","[{'comment': 'int total_size = 0;\r\n', 'commenter': 'imay'}]"
923,be/src/exprs/string_functions.cpp,"@@ -535,17 +526,47 @@ StringVal StringFunctions::concat_ws(
     uint8_t* ptr = result.ptr;
 
     // Loop again to append the data.
-    memcpy(ptr, strs[0].ptr, strs[0].len);
-    ptr += strs[0].len;
-    for (int32_t i = 1; i < num_children; ++i) {
-        memcpy(ptr, sep.ptr, sep.len);
-        ptr += sep.len;
+    for (int32_t i = 0; i < num_children; ++i) {
         memcpy(ptr, strs[i].ptr, strs[i].len);
         ptr += strs[i].len;
     }
     return result;
 }
 
+StringVal StringFunctions::concat_ws(
+        FunctionContext* context, const StringVal& sep, 
+        int num_children, const StringVal* strs) {
+    DCHECK_GE(num_children, 1);
+    if (sep.is_null) {
+        return StringVal::null();
+    }
+
+    int32_t total_size = strs[0].is_null ? 0 : strs[0].len;
+    // Loop once to compute the final size and reserve space.
+    for (int32_t i = 1; i < num_children; ++i) {","[{'comment': '```suggestion\r\n    for (int32_t i = 0; i < num_children; ++i) {\r\n```', 'commenter': 'imay'}]"
923,be/src/exprs/string_functions.cpp,"@@ -535,17 +526,47 @@ StringVal StringFunctions::concat_ws(
     uint8_t* ptr = result.ptr;
 
     // Loop again to append the data.
-    memcpy(ptr, strs[0].ptr, strs[0].len);
-    ptr += strs[0].len;
-    for (int32_t i = 1; i < num_children; ++i) {
-        memcpy(ptr, sep.ptr, sep.len);
-        ptr += sep.len;
+    for (int32_t i = 0; i < num_children; ++i) {
         memcpy(ptr, strs[i].ptr, strs[i].len);
         ptr += strs[i].len;
     }
     return result;
 }
 
+StringVal StringFunctions::concat_ws(
+        FunctionContext* context, const StringVal& sep, 
+        int num_children, const StringVal* strs) {
+    DCHECK_GE(num_children, 1);
+    if (sep.is_null) {
+        return StringVal::null();
+    }
+
+    int32_t total_size = strs[0].is_null ? 0 : strs[0].len;
+    // Loop once to compute the final size and reserve space.
+    for (int32_t i = 1; i < num_children; ++i) {
+        total_size += strs[i].is_null ? 0 : (sep.len + strs[i].len);","[{'comment': 'if strs[0] is null, total_size would add a more `sep.len`.\r\n\r\nsame with your data copy logical\r\n', 'commenter': 'imay'}, {'comment': 'computing size should be equal with appending data', 'commenter': 'imay'}]"
923,be/src/exprs/string_functions.cpp,"@@ -535,17 +526,47 @@ StringVal StringFunctions::concat_ws(
     uint8_t* ptr = result.ptr;
 
     // Loop again to append the data.
-    memcpy(ptr, strs[0].ptr, strs[0].len);
-    ptr += strs[0].len;
-    for (int32_t i = 1; i < num_children; ++i) {
-        memcpy(ptr, sep.ptr, sep.len);
-        ptr += sep.len;
+    for (int32_t i = 0; i < num_children; ++i) {
         memcpy(ptr, strs[i].ptr, strs[i].len);
         ptr += strs[i].len;
     }
     return result;
 }
 
+StringVal StringFunctions::concat_ws(
+        FunctionContext* context, const StringVal& sep, 
+        int num_children, const StringVal* strs) {
+    DCHECK_GE(num_children, 1);
+    if (sep.is_null) {
+        return StringVal::null();
+    }
+
+    int32_t total_size = strs[0].is_null ? 0 : strs[0].len;
+    // Loop once to compute the final size and reserve space.
+    for (int32_t i = 1; i < num_children; ++i) {
+        total_size += strs[i].is_null ? 0 : (sep.len + strs[i].len);
+    }
+
+    // TODO pengyubing
+    // StringVal result = StringVal::create_temp_string_val(context, total_size);
+    StringVal result(context, total_size);
+    uint8_t* ptr = result.ptr;
+    bool not_first = false;
+    // Loop again to append the data.
+    for (int32_t i = 0; i < num_children; ++i) {
+        if (!strs[i].is_null) {","[{'comment': '```suggestion\r\n        if (strs[i].is_null) {\r\n            continue;\r\n        }\r\n```', 'commenter': 'imay'}]"
925,be/src/util/es_scroll_parser.cpp,"@@ -0,0 +1,440 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""es_scroll_parser.h""
+
+#include <string>
+#include <boost/algorithm/string.hpp>
+#include <gutil/strings/substitute.h>
+
+#include ""common/logging.h""
+#include ""common/status.h""
+#include ""runtime/mem_pool.h""
+#include ""runtime/mem_tracker.h""
+#include ""util/string_parser.hpp""
+
+namespace doris {
+
+static const char* FIELD_SCROLL_ID = ""_scroll_id"";
+static const char* FIELD_HITS = ""hits"";
+static const char* FIELD_INNER_HITS = ""hits"";
+static const char* FIELD_SOURCE = ""_source"";
+static const char* FIELD_TOTAL = ""total"";
+
+static const string ERROR_INVALID_COL_DATA = ""Data source returned inconsistent column data. ""
+    ""Expected value of type $0 based on column metadata. This likely indicates a ""
+    ""problem with the data source library."";
+static const string ERROR_MEM_LIMIT_EXCEEDED = ""DataSourceScanNode::$0() failed to allocate ""
+    ""$1 bytes for $2."";
+static const string ERROR_COL_DATA_IS_ARRAY = ""Data source returned an array for the type $0""
+    ""based on column metadata."";
+
+ScrollParser::ScrollParser(const std::string& scroll_result) :
+    _scroll_id(""""),
+    _total(0),
+    _size(0),
+    _line_index(0) {
+        parsing(scroll_result);
+}
+
+ScrollParser::~ScrollParser() {
+}
+
+void ScrollParser::parsing(const std::string& scroll_result) {
+    _document_node.Parse(scroll_result.c_str());
+
+    if (!_document_node.HasMember(FIELD_SCROLL_ID)) {
+        LOG(ERROR) << ""maybe not a scroll request"";
+        return;
+    }
+
+    const rapidjson::Value &scroll_node = _document_node[FIELD_SCROLL_ID];
+    _scroll_id = scroll_node.GetString();
+    // { hits: { total : 2, ""hits"" : [ {}, {}, {} ]}}
+    const rapidjson::Value &outer_hits_node = _document_node[FIELD_HITS];
+    const rapidjson::Value &field_total = outer_hits_node[FIELD_TOTAL];
+    _total = field_total.GetInt();
+    if (_total == 0) {
+        return;
+    }
+
+    VLOG(1) << ""es_scan_reader total hits: "" << _total << "" documents"";
+    const rapidjson::Value &inner_hits_node = outer_hits_node[FIELD_INNER_HITS];
+    if (!inner_hits_node.IsArray()) {
+        LOG(ERROR) << ""maybe not a scroll request"";
+        return;
+    }
+
+    rapidjson::Document::AllocatorType& a = _document_node.GetAllocator();
+    _inner_hits_node.CopyFrom(inner_hits_node, a);
+    _size = _inner_hits_node.Size();
+}
+
+int ScrollParser::get_size() {
+    return _size;
+}
+
+const std::string& ScrollParser::get_scroll_id() {
+    return _scroll_id;
+}
+
+int ScrollParser::get_total() {
+    return _total;
+}
+
+Status ScrollParser::fill_tuple(const TupleDescriptor* tuple_desc, 
+            Tuple* tuple, MemPool* tuple_pool, bool* line_eof) {
+    *line_eof = true;
+    if (_size <= 0 || _line_index >= _size) {
+        return Status::OK;
+    }
+
+    const rapidjson::Value& obj = _inner_hits_node[_line_index++];
+    const rapidjson::Value& line = obj[FIELD_SOURCE];
+    if (!line.IsObject()) {
+        return Status(""Parse inner hits failed"");
+    }
+
+    tuple->init(tuple_desc->byte_size());
+    for (int i = 0; i < tuple_desc->slots().size(); ++i) {
+        const SlotDescriptor* slot_desc = tuple_desc->slots()[i];
+
+        if (!slot_desc->is_materialized()) {
+            continue;
+        }
+
+        std::string s(slot_desc->col_name());
+        const char* col_name = s.c_str();
+        rapidjson::Value::ConstMemberIterator itr = line.FindMember(col_name);
+        if (itr == line.MemberEnd()) {
+            tuple->set_null(slot_desc->null_indicator_offset());
+            continue;
+        }
+
+        tuple->set_not_null(slot_desc->null_indicator_offset());
+        const rapidjson::Value &col = line[col_name];
+
+        void* slot = tuple->get_slot(slot_desc->tuple_offset());
+        switch (slot_desc->type().type) {
+            case TYPE_CHAR:
+            case TYPE_VARCHAR: {
+                if (col.IsArray()) {
+                    return Status(strings::Substitute(ERROR_COL_DATA_IS_ARRAY, ""STRING""));
+                }
+                if (!col.IsString()) {
+                    return Status(strings::Substitute(ERROR_INVALID_COL_DATA, ""STRING""));
+                }
+                const std::string& val = col.GetString();
+                size_t val_size = col.GetStringLength();
+                char* buffer = reinterpret_cast<char*>(tuple_pool->try_allocate_unaligned(val_size));
+                if (UNLIKELY(buffer == NULL)) {
+                    string details = strings::Substitute(ERROR_MEM_LIMIT_EXCEEDED, ""MaterializeNextRow"",
+                                val_size, ""string slot"");
+                    return tuple_pool->mem_tracker()->MemLimitExceeded(NULL, details, val_size);
+                }
+                memcpy(buffer, val.data(), val_size);
+                reinterpret_cast<StringValue*>(slot)->ptr = buffer;
+                reinterpret_cast<StringValue*>(slot)->len = val_size;
+                break;
+            }
+
+            case TYPE_TINYINT: {
+                if (col.IsNumber()) {
+                    *reinterpret_cast<int8_t*>(slot) = (int8_t)col.GetInt();
+                    break;
+                }
+
+                if (col.IsArray()) {
+                    return Status(strings::Substitute(ERROR_COL_DATA_IS_ARRAY, ""TINYINT""));
+                }
+
+                if (!col.IsString()) {
+                    return Status(strings::Substitute(ERROR_INVALID_COL_DATA, ""TINYINT""));
+                } 
+
+                const std::string& val = col.GetString();
+                const char* data = val.c_str();
+                size_t len = col.GetStringLength();
+                StringParser::ParseResult result;
+                int8_t v = StringParser::string_to_int<int8_t>(data, len, &result);
+                if (result != StringParser::PARSE_SUCCESS) {
+                    return Status(strings::Substitute(ERROR_INVALID_COL_DATA, ""TINYINT""));
+                }
+                *reinterpret_cast<int8_t*>(slot) = v;
+                break;
+            }
+
+            case TYPE_SMALLINT: {
+                if (col.IsNumber()) {
+                    *reinterpret_cast<int16_t*>(slot) = (int16_t)col.GetInt();
+                    break;
+                }
+
+                if (col.IsArray()) {
+                    return Status(strings::Substitute(ERROR_COL_DATA_IS_ARRAY, ""SMALLINT""));
+                }
+
+                if (!col.IsString()) {
+                    return Status(strings::Substitute(ERROR_INVALID_COL_DATA, ""SMALLINT""));
+                } 
+                    
+                const std::string& val = col.GetString();
+                const char* data = val.c_str();
+                size_t len = col.GetStringLength();
+                StringParser::ParseResult result;
+                int16_t v = StringParser::string_to_int<int16_t>(data, len, &result);
+                if (result != StringParser::PARSE_SUCCESS) {
+                    return Status(strings::Substitute(ERROR_INVALID_COL_DATA, ""SMALLINT""));
+                }
+                *reinterpret_cast<int16_t*>(slot) = v;
+                break;
+            }
+
+            case TYPE_INT: {
+                if (col.IsNumber()) {
+                    *reinterpret_cast<int32_t*>(slot) = (int32_t)col.GetInt();
+                    break;
+                }
+
+                if (col.IsArray()) {
+                    return Status(strings::Substitute(ERROR_COL_DATA_IS_ARRAY, ""INT""));
+                }
+
+                if (!col.IsString()) {
+                    return Status(strings::Substitute(ERROR_INVALID_COL_DATA, ""INT""));
+                } 
+
+                const std::string& val = col.GetString();
+                const char* data = val.c_str();
+                size_t len = col.GetStringLength();
+                StringParser::ParseResult result;
+                int32_t v = StringParser::string_to_int<int32_t>(data, len, &result);
+                if (result != StringParser::PARSE_SUCCESS) {
+                    return Status(strings::Substitute(ERROR_INVALID_COL_DATA, ""INT""));
+                }
+                *reinterpret_cast<int32_t*>(slot) = v;
+                break;
+            }
+
+            case TYPE_BIGINT: {
+                if (col.IsNumber()) {","[{'comment': 'Duplicate code，please use function to dedup', 'commenter': 'imay'}, {'comment': 'Done', 'commenter': 'lide-reed'}]"
925,be/src/exec/es_http_scan_node.cpp,"@@ -0,0 +1,442 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""exec/es_http_scan_node.h""
+
+#include <chrono>
+#include <sstream>
+
+#include ""common/object_pool.h""
+#include ""exprs/expr.h""
+#include ""runtime/runtime_state.h""
+#include ""runtime/row_batch.h""
+#include ""runtime/dpp_sink_internal.h""
+#include ""service/backend_options.h""
+#include ""util/runtime_profile.h""
+#include ""util/es_scan_reader.h""
+#include ""util/es_scroll_query.h""
+#include ""util/es_query_builder.h""
+#include ""exec/es_predicate.h""
+
+namespace doris {
+
+EsHttpScanNode::EsHttpScanNode(
+        ObjectPool* pool, const TPlanNode& tnode, const DescriptorTbl& descs) : 
+            ScanNode(pool, tnode, descs), 
+            _tuple_id(tnode.es_scan_node.tuple_id),
+            _runtime_state(nullptr),
+            _tuple_desc(nullptr),
+            _num_running_scanners(0),
+            _scan_finished(false),
+            _eos(false),
+            _max_buffered_batches(1024),
+            _wait_scanner_timer(nullptr) {
+}
+
+EsHttpScanNode::~EsHttpScanNode() {
+}
+
+Status EsHttpScanNode::init(const TPlanNode& tnode, RuntimeState* state) {
+    RETURN_IF_ERROR(ScanNode::init(tnode));
+
+    // use TEsScanNode
+    _properties = tnode.es_scan_node.properties;
+    return Status::OK;
+}
+
+Status EsHttpScanNode::prepare(RuntimeState* state) {
+    VLOG_QUERY << ""EsHttpScanNode prepare"";
+    RETURN_IF_ERROR(ScanNode::prepare(state));
+
+    _runtime_state = state;
+    _tuple_desc = state->desc_tbl().get_tuple_descriptor(_tuple_id);
+    if (_tuple_desc == nullptr) {
+        std::stringstream ss;
+        ss << ""Failed to get tuple descriptor, _tuple_id="" << _tuple_id;
+        return Status(ss.str());
+    }
+
+    // set up column name vector for ESScrollQueryBuilder
+    for (auto slot_desc : _tuple_desc->slots()) {
+        if (!slot_desc->is_materialized()) {
+            continue;
+        }
+        _column_names.push_back(slot_desc->col_name());
+    }
+
+    _wait_scanner_timer = ADD_TIMER(runtime_profile(), ""WaitScannerTime"");
+
+    return Status::OK;
+}
+
+// build predicate 
+Status EsHttpScanNode::build_conjuncts_list() {
+    Status status = Status::OK;
+    for (int i = 0; i < _conjunct_ctxs.size(); ++i) {
+        EsPredicate* predicate = _pool->add(
+                    new EsPredicate(_conjunct_ctxs[i], _tuple_desc));
+        if (predicate->build_disjuncts_list()) {
+            _predicates.push_back(predicate);
+            _predicate_to_conjunct.push_back(i);
+        } else if (!predicate->get_es_query_status().ok()) {
+            return predicate->get_es_query_status();","[{'comment': 'need some LOG to see why this is not `ok()`', 'commenter': 'imay'}, {'comment': 'Done', 'commenter': 'lide-reed'}]"
925,be/src/exec/es_predicate.cpp,"@@ -0,0 +1,421 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""exec/es_predicate.h""
+
+#include <stdint.h>
+#include <map>
+#include <sstream>
+#include <boost/algorithm/string.hpp>
+#include <gutil/strings/substitute.h>
+
+#include ""common/status.h""
+#include ""common/logging.h""
+#include ""exprs/expr.h""
+#include ""exprs/expr_context.h""
+#include ""exprs/in_predicate.h""
+
+#include ""gen_cpp/PlanNodes_types.h""
+#include ""olap/olap_common.h""
+#include ""olap/utils.h""
+#include ""runtime/client_cache.h""
+#include ""runtime/runtime_state.h""
+#include ""runtime/row_batch.h""
+#include ""runtime/datetime_value.h""
+#include ""runtime/large_int_value.h""
+#include ""runtime/string_value.h""
+#include ""runtime/tuple_row.h""
+
+#include ""service/backend_options.h""
+#include ""util/debug_util.h""
+#include ""util/es_query_builder.h""
+#include ""util/runtime_profile.h""
+
+namespace doris {
+
+using namespace std;
+
+std::string ExtLiteral::value_to_string() {
+    std::stringstream ss;
+    switch (_type) {
+        case TYPE_TINYINT:
+            ss << std::to_string(get_byte());
+            break;
+        case TYPE_SMALLINT:
+            ss << std::to_string(get_short());
+            break;
+        case TYPE_INT:
+            ss << std::to_string(get_int());
+            break;
+        case TYPE_BIGINT:
+            ss << std::to_string(get_long());
+            break;
+        case TYPE_FLOAT:
+            ss << std::to_string(get_float());
+            break;
+        case TYPE_DOUBLE:
+            ss << std::to_string(get_double());
+            break;
+        case TYPE_CHAR:
+        case TYPE_VARCHAR:
+            ss << get_string();
+            break;
+        case TYPE_DATE:
+        case TYPE_DATETIME:
+            ss << get_date_string();
+            break;
+        case TYPE_BOOLEAN:
+            ss << std::to_string(get_bool());
+            break;
+        case TYPE_DECIMAL:
+            ss << get_decimal_string();
+            break;
+        case TYPE_DECIMALV2:
+            ss << get_decimalv2_string();
+            break;
+        case TYPE_LARGEINT:
+            ss << get_largeint_string();
+            break;
+        default:
+            DCHECK(false);
+            break;
+    }
+    return ss.str();
+}
+
+ExtLiteral::~ExtLiteral(){
+}
+
+int8_t ExtLiteral::get_byte() {
+    DCHECK(_type == TYPE_TINYINT);
+    return *(reinterpret_cast<int8_t*>(_value));
+}
+
+int16_t ExtLiteral::get_short() {
+    DCHECK(_type == TYPE_SMALLINT);
+    return *(reinterpret_cast<int16_t*>(_value));
+}
+
+int32_t ExtLiteral::get_int() {
+    DCHECK(_type == TYPE_INT);
+    return *(reinterpret_cast<int32_t*>(_value));
+}
+
+int64_t ExtLiteral::get_long() {
+    DCHECK(_type == TYPE_BIGINT);
+    return *(reinterpret_cast<int64_t*>(_value));
+}
+
+float ExtLiteral::get_float() {
+    DCHECK(_type == TYPE_FLOAT);
+    return *(reinterpret_cast<float*>(_value));
+}
+
+double ExtLiteral::get_double() {
+    DCHECK(_type == TYPE_DOUBLE);
+    return *(reinterpret_cast<double*>(_value));
+}
+
+std::string ExtLiteral::get_string() {
+    DCHECK(_type == TYPE_VARCHAR || _type == TYPE_CHAR);
+    return (reinterpret_cast<StringValue*>(_value))->to_string();
+}
+
+std::string ExtLiteral::get_date_string() {
+    DCHECK(_type == TYPE_DATE || _type == TYPE_DATETIME);
+    DateTimeValue date_value = *reinterpret_cast<DateTimeValue*>(_value);
+    if (_type == TYPE_DATE) {
+        date_value.cast_to_date();
+    }
+
+    char str[MAX_DTVALUE_STR_LEN];
+    date_value.to_string(str);
+    return std::string(str, strlen(str)); 
+}
+
+bool ExtLiteral::get_bool() {
+    DCHECK(_type == TYPE_BOOLEAN);
+    return *(reinterpret_cast<bool*>(_value));
+}
+
+std::string ExtLiteral::get_decimal_string() {
+    DCHECK(_type == TYPE_DECIMAL);
+    return reinterpret_cast<DecimalValue*>(_value)->to_string();
+}
+
+std::string ExtLiteral::get_decimalv2_string() {
+    DCHECK(_type == TYPE_DECIMALV2);
+    return reinterpret_cast<DecimalV2Value*>(_value)->to_string();
+}
+
+std::string ExtLiteral::get_largeint_string() {
+    DCHECK(_type == TYPE_LARGEINT);
+    return LargeIntValue::to_string(*reinterpret_cast<__int128*>(_value));
+}
+
+EsPredicate::EsPredicate(ExprContext* context,
+            const TupleDescriptor* tuple_desc) :
+    _context(context),
+    _disjuncts_num(0),
+    _tuple_desc(tuple_desc),
+    _es_query_status(Status::OK) {
+}
+
+EsPredicate::~EsPredicate() {
+    for(int i=0; i < _disjuncts.size(); i++) {
+        delete _disjuncts[i];
+    }
+    _disjuncts.clear();
+}
+
+bool EsPredicate::build_disjuncts_list() {
+    return build_disjuncts_list(_context->root());
+}
+
+// make sure to build by build_disjuncts_list
+const vector<ExtPredicate*>& EsPredicate::get_predicate_list(){
+    return _disjuncts;
+}
+
+static bool ignore_cast(const SlotDescriptor* slot, const Expr* expr) {
+    if (slot->type().is_date_type() && expr->type().is_date_type()) {
+        return true;
+    }
+    if (slot->type().is_string_type() && expr->type().is_string_type()) {
+        return true;
+    }
+    return false;
+}
+
+static bool is_literal_node(const Expr* expr) {
+    switch (expr->node_type()) {
+        case TExprNodeType::BOOL_LITERAL:
+        case TExprNodeType::INT_LITERAL:
+        case TExprNodeType::LARGE_INT_LITERAL:
+        case TExprNodeType::FLOAT_LITERAL:
+        case TExprNodeType::DECIMAL_LITERAL:
+        case TExprNodeType::STRING_LITERAL:
+        case TExprNodeType::DATE_LITERAL:
+            return true;
+        default:
+            return false;
+    }
+}
+
+bool EsPredicate::build_disjuncts_list(const Expr* conjunct) {","[{'comment': 'why return `bool`? I think `Status` can give caller more information about failure', 'commenter': 'imay'}, {'comment': 'Done', 'commenter': 'lide-reed'}]"
925,be/src/exec/es_predicate.cpp,"@@ -0,0 +1,421 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""exec/es_predicate.h""
+
+#include <stdint.h>
+#include <map>
+#include <sstream>
+#include <boost/algorithm/string.hpp>
+#include <gutil/strings/substitute.h>
+
+#include ""common/status.h""
+#include ""common/logging.h""
+#include ""exprs/expr.h""
+#include ""exprs/expr_context.h""
+#include ""exprs/in_predicate.h""
+
+#include ""gen_cpp/PlanNodes_types.h""
+#include ""olap/olap_common.h""
+#include ""olap/utils.h""
+#include ""runtime/client_cache.h""
+#include ""runtime/runtime_state.h""
+#include ""runtime/row_batch.h""
+#include ""runtime/datetime_value.h""
+#include ""runtime/large_int_value.h""
+#include ""runtime/string_value.h""
+#include ""runtime/tuple_row.h""
+
+#include ""service/backend_options.h""
+#include ""util/debug_util.h""
+#include ""util/es_query_builder.h""
+#include ""util/runtime_profile.h""
+
+namespace doris {
+
+using namespace std;
+
+std::string ExtLiteral::value_to_string() {
+    std::stringstream ss;
+    switch (_type) {
+        case TYPE_TINYINT:
+            ss << std::to_string(get_byte());
+            break;
+        case TYPE_SMALLINT:
+            ss << std::to_string(get_short());
+            break;
+        case TYPE_INT:
+            ss << std::to_string(get_int());
+            break;
+        case TYPE_BIGINT:
+            ss << std::to_string(get_long());
+            break;
+        case TYPE_FLOAT:
+            ss << std::to_string(get_float());
+            break;
+        case TYPE_DOUBLE:
+            ss << std::to_string(get_double());
+            break;
+        case TYPE_CHAR:
+        case TYPE_VARCHAR:
+            ss << get_string();
+            break;
+        case TYPE_DATE:
+        case TYPE_DATETIME:
+            ss << get_date_string();
+            break;
+        case TYPE_BOOLEAN:
+            ss << std::to_string(get_bool());
+            break;
+        case TYPE_DECIMAL:
+            ss << get_decimal_string();
+            break;
+        case TYPE_DECIMALV2:
+            ss << get_decimalv2_string();
+            break;
+        case TYPE_LARGEINT:
+            ss << get_largeint_string();
+            break;
+        default:
+            DCHECK(false);
+            break;
+    }
+    return ss.str();
+}
+
+ExtLiteral::~ExtLiteral(){
+}
+
+int8_t ExtLiteral::get_byte() {
+    DCHECK(_type == TYPE_TINYINT);
+    return *(reinterpret_cast<int8_t*>(_value));
+}
+
+int16_t ExtLiteral::get_short() {
+    DCHECK(_type == TYPE_SMALLINT);
+    return *(reinterpret_cast<int16_t*>(_value));
+}
+
+int32_t ExtLiteral::get_int() {
+    DCHECK(_type == TYPE_INT);
+    return *(reinterpret_cast<int32_t*>(_value));
+}
+
+int64_t ExtLiteral::get_long() {
+    DCHECK(_type == TYPE_BIGINT);
+    return *(reinterpret_cast<int64_t*>(_value));
+}
+
+float ExtLiteral::get_float() {
+    DCHECK(_type == TYPE_FLOAT);
+    return *(reinterpret_cast<float*>(_value));
+}
+
+double ExtLiteral::get_double() {
+    DCHECK(_type == TYPE_DOUBLE);
+    return *(reinterpret_cast<double*>(_value));
+}
+
+std::string ExtLiteral::get_string() {
+    DCHECK(_type == TYPE_VARCHAR || _type == TYPE_CHAR);
+    return (reinterpret_cast<StringValue*>(_value))->to_string();
+}
+
+std::string ExtLiteral::get_date_string() {
+    DCHECK(_type == TYPE_DATE || _type == TYPE_DATETIME);
+    DateTimeValue date_value = *reinterpret_cast<DateTimeValue*>(_value);
+    if (_type == TYPE_DATE) {
+        date_value.cast_to_date();
+    }
+
+    char str[MAX_DTVALUE_STR_LEN];
+    date_value.to_string(str);
+    return std::string(str, strlen(str)); 
+}
+
+bool ExtLiteral::get_bool() {
+    DCHECK(_type == TYPE_BOOLEAN);
+    return *(reinterpret_cast<bool*>(_value));
+}
+
+std::string ExtLiteral::get_decimal_string() {
+    DCHECK(_type == TYPE_DECIMAL);
+    return reinterpret_cast<DecimalValue*>(_value)->to_string();
+}
+
+std::string ExtLiteral::get_decimalv2_string() {
+    DCHECK(_type == TYPE_DECIMALV2);
+    return reinterpret_cast<DecimalV2Value*>(_value)->to_string();
+}
+
+std::string ExtLiteral::get_largeint_string() {
+    DCHECK(_type == TYPE_LARGEINT);
+    return LargeIntValue::to_string(*reinterpret_cast<__int128*>(_value));
+}
+
+EsPredicate::EsPredicate(ExprContext* context,
+            const TupleDescriptor* tuple_desc) :
+    _context(context),
+    _disjuncts_num(0),
+    _tuple_desc(tuple_desc),
+    _es_query_status(Status::OK) {
+}
+
+EsPredicate::~EsPredicate() {
+    for(int i=0; i < _disjuncts.size(); i++) {
+        delete _disjuncts[i];
+    }
+    _disjuncts.clear();
+}
+
+bool EsPredicate::build_disjuncts_list() {
+    return build_disjuncts_list(_context->root());
+}
+
+// make sure to build by build_disjuncts_list
+const vector<ExtPredicate*>& EsPredicate::get_predicate_list(){
+    return _disjuncts;
+}
+
+static bool ignore_cast(const SlotDescriptor* slot, const Expr* expr) {
+    if (slot->type().is_date_type() && expr->type().is_date_type()) {
+        return true;
+    }
+    if (slot->type().is_string_type() && expr->type().is_string_type()) {
+        return true;
+    }
+    return false;
+}
+
+static bool is_literal_node(const Expr* expr) {
+    switch (expr->node_type()) {
+        case TExprNodeType::BOOL_LITERAL:
+        case TExprNodeType::INT_LITERAL:
+        case TExprNodeType::LARGE_INT_LITERAL:
+        case TExprNodeType::FLOAT_LITERAL:
+        case TExprNodeType::DECIMAL_LITERAL:
+        case TExprNodeType::STRING_LITERAL:
+        case TExprNodeType::DATE_LITERAL:
+            return true;
+        default:
+            return false;
+    }
+}
+
+bool EsPredicate::build_disjuncts_list(const Expr* conjunct) {
+    if (TExprNodeType::BINARY_PRED == conjunct->node_type()) {
+        if (conjunct->children().size() != 2) {
+            VLOG(1) << ""get disjuncts fail: number of childs is not 2"";
+            return false;
+        }
+
+        SlotRef* slotRef = nullptr;
+        TExprOpcode::type op;
+        Expr* expr = nullptr;
+        if (TExprNodeType::SLOT_REF == conjunct->get_child(0)->node_type()) {
+            expr = conjunct->get_child(1);
+            slotRef = (SlotRef*)(conjunct->get_child(0));
+            op = conjunct->op();
+        } else if (TExprNodeType::SLOT_REF == conjunct->get_child(1)->node_type()) {
+            expr = conjunct->get_child(0);
+            slotRef = (SlotRef*)(conjunct->get_child(1));
+            op = conjunct->op();
+        } else {
+            VLOG(1) << ""get disjuncts fail: no SLOT_REF child"";
+            return false;
+        }
+
+        const SlotDescriptor* slot_desc = get_slot_desc(slotRef);
+        if (slot_desc == nullptr) {
+            VLOG(1) << ""get disjuncts fail: slot_desc is null"";
+            return false;
+        }
+
+        if (!is_literal_node(expr)) {
+            VLOG(1) << ""get disjuncts fail: expr is not literal type"";
+            return false;
+        }
+
+        ExtLiteral literal(expr->type().type, _context->get_value(expr, NULL));
+        ExtPredicate* predicate = new ExtBinaryPredicate(
+                    TExprNodeType::BINARY_PRED,
+                    slot_desc->col_name(),
+                    slot_desc->type(),
+                    op,
+                    literal);
+
+        _disjuncts.push_back(predicate);
+        return true;
+    }
+    
+    if (is_match_func(conjunct)) {
+
+        Expr* expr = conjunct->get_child(1);
+        ExtLiteral literal(expr->type().type, _context->get_value(expr, NULL));
+        vector<ExtLiteral> query_conditions;
+        query_conditions.emplace_back(literal);
+        vector<ExtColumnDesc> cols; //TODO
+        ExtPredicate* predicate = new ExtFunction(
+                        TExprNodeType::FUNCTION_CALL,
+                        conjunct->fn().name.function_name,
+                        cols,
+                        query_conditions);
+        if (_es_query_status.ok()) {
+            _es_query_status 
+                = BooleanQueryBuilder::check_es_query(*(ExtFunction *)predicate); 
+            if (!_es_query_status.ok()) {
+                return false;
+            }
+        }
+        _disjuncts.push_back(predicate);
+
+        return true;
+    } 
+
+    if (TExprNodeType::FUNCTION_CALL == conjunct->node_type()) {
+        std::string fname = conjunct->fn().name.function_name;
+        if (fname != ""like"") {
+            return false;
+        }
+
+        SlotRef* slotRef = nullptr;
+        Expr* expr = nullptr;
+        if (TExprNodeType::SLOT_REF == conjunct->get_child(0)->node_type()) {
+            expr = conjunct->get_child(1);
+            slotRef = (SlotRef*)(conjunct->get_child(0));
+        } else if (TExprNodeType::SLOT_REF == conjunct->get_child(1)->node_type()) {
+            expr = conjunct->get_child(0);
+            slotRef = (SlotRef*)(conjunct->get_child(1));
+        } else {
+            VLOG(1) << ""get disjuncts fail: no SLOT_REF child"";
+            return false;
+        }
+
+        const SlotDescriptor* slot_desc = get_slot_desc(slotRef);
+        if (slot_desc == nullptr) {
+            VLOG(1) << ""get disjuncts fail: slot_desc is null"";
+            return false;
+        }
+
+        PrimitiveType type = expr->type().type;
+        if (type != TYPE_VARCHAR && type != TYPE_CHAR) {
+            VLOG(1) << ""get disjuncts fail: like value is not a string"";
+            return false;
+        }
+
+        ExtLiteral literal(type, _context->get_value(expr, NULL));
+        ExtPredicate* predicate = new ExtLikePredicate(
+                    TExprNodeType::LIKE_PRED,
+                    slot_desc->col_name(),
+                    slot_desc->type(),
+                    literal);
+
+        _disjuncts.push_back(predicate);
+        return true;
+    }
+      
+    if (TExprNodeType::IN_PRED == conjunct->node_type()) {
+        // the op code maybe FILTER_NEW_IN, it means there is function in list
+        // like col_a in (abs(1))
+        if (TExprOpcode::FILTER_IN != conjunct->op() 
+            && TExprOpcode::FILTER_NOT_IN != conjunct->op()) {
+            return false;
+        }
+
+        vector<ExtLiteral> in_pred_values;
+        const InPredicate* pred = dynamic_cast<const InPredicate*>(conjunct);
+        if (Expr::type_without_cast(pred->get_child(0)) != TExprNodeType::SLOT_REF) {
+            return false;
+        }
+
+        SlotRef* slot_ref = (SlotRef*)(conjunct->get_child(0));
+        const SlotDescriptor* slot_desc = get_slot_desc(slot_ref);
+        if (slot_desc == nullptr) {
+            return false;
+        }
+
+        if (pred->get_child(0)->type().type != slot_desc->type().type) {
+            if (!ignore_cast(slot_desc, pred->get_child(0))) {
+                return false;
+            }
+        }
+
+        HybirdSetBase::IteratorBase* iter = pred->hybird_set()->begin();
+        while (iter->has_next()) {
+            if (nullptr == iter->get_value()) {
+                return false;
+            }
+
+            ExtLiteral literal(slot_desc->type().type, const_cast<void *>(iter->get_value()));
+            in_pred_values.emplace_back(literal);
+            iter->next();
+        }
+
+        ExtPredicate* predicate = new ExtInPredicate(
+                    TExprNodeType::IN_PRED,
+                    pred->is_not_in(),
+                    slot_desc->col_name(),
+                    slot_desc->type(),
+                    in_pred_values);
+        _disjuncts.push_back(predicate);
+
+        return true;
+    } 
+    
+    if (TExprNodeType::COMPOUND_PRED == conjunct->node_type()) {
+        if (TExprOpcode::COMPOUND_OR != conjunct->op()) {
+            VLOG(1) << ""get disjuncts fail: op is not COMPOUND_OR"";
+            return false;
+        }
+        if (!build_disjuncts_list(conjunct->get_child(0))) {
+            return false;
+        }
+        if (!build_disjuncts_list(conjunct->get_child(1))) {
+            return false;
+        }
+
+        return true;
+    }
+
+    // if go to here, report error
+    VLOG(1) << ""get disjuncts fail: node type is "" << conjunct->node_type()
+        << "", should be BINARY_PRED or COMPOUND_PRED"";
+    return false;
+}
+
+bool EsPredicate::is_match_func(const Expr* conjunct) {
+    if (TExprNodeType::FUNCTION_CALL == conjunct->node_type()
+        && conjunct->fn().name.function_name == ""esquery"") {
+            return true;
+    }
+    return false;
+}
+
+const SlotDescriptor* EsPredicate::get_slot_desc(const SlotRef* slotRef) {
+    std::vector<SlotId> slot_ids;
+    slotRef->get_slot_ids(&slot_ids);
+    const SlotDescriptor* slot_desc = nullptr;
+    for (SlotDescriptor* slot : _tuple_desc->slots()) {
+        if (slot->id() == slot_ids[0]) {","[{'comment': 'why do not you use `slotRef->slot_id()`', 'commenter': 'imay'}, {'comment': 'Done', 'commenter': 'lide-reed'}]"
925,be/src/exec/es_http_scan_node.h,"@@ -0,0 +1,112 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#ifndef  BE_EXEC_ES_HTTP_SCAN_NODE_H
+#define  BE_EXEC_ES_HTTP_SCAN_NODE_H
+
+#include <atomic>
+#include <condition_variable>
+#include <map>
+#include <string>
+#include <vector>
+#include <mutex>
+#include <thread>
+#include <future>
+
+#include ""common/status.h""
+#include ""exec/scan_node.h""
+#include ""exec/es_http_scanner.h""
+#include ""gen_cpp/PaloInternalService_types.h""
+
+namespace doris {
+
+class RuntimeState;
+class PartRangeKey;
+class PartitionInfo;
+class EsHttpScanCounter;
+class EsPredicate;
+
+class EsHttpScanNode : public ScanNode {
+public:","[{'comment': 'Why exists two kinds of es scan node, only because the method access ES is different ?   ESScanNode only support single thread scanning but EsHttpScanNode support multi thread scanning, why?', 'commenter': 'chenhao7253886'}, {'comment': 'Due to history cause, It will be merged together in future', 'commenter': 'lide-reed'}]"
925,be/src/exec/es_http_scan_node.cpp,"@@ -0,0 +1,445 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""exec/es_http_scan_node.h""
+
+#include <chrono>
+#include <sstream>
+
+#include ""common/object_pool.h""
+#include ""exec/es/es_predicate.h""
+#include ""exec/es/es_query_builder.h""
+#include ""exec/es/es_scan_reader.h""
+#include ""exec/es/es_scroll_query.h""
+#include ""exprs/expr.h""
+#include ""runtime/runtime_state.h""
+#include ""runtime/row_batch.h""
+#include ""runtime/dpp_sink_internal.h""
+#include ""service/backend_options.h""
+#include ""util/runtime_profile.h""
+
+namespace doris {
+
+EsHttpScanNode::EsHttpScanNode(
+        ObjectPool* pool, const TPlanNode& tnode, const DescriptorTbl& descs) : 
+            ScanNode(pool, tnode, descs), 
+            _tuple_id(tnode.es_scan_node.tuple_id),
+            _runtime_state(nullptr),
+            _tuple_desc(nullptr),
+            _num_running_scanners(0),
+            _scan_finished(false),
+            _eos(false),
+            _max_buffered_batches(1024),
+            _wait_scanner_timer(nullptr) {
+}
+
+EsHttpScanNode::~EsHttpScanNode() {
+}
+
+Status EsHttpScanNode::init(const TPlanNode& tnode, RuntimeState* state) {
+    RETURN_IF_ERROR(ScanNode::init(tnode));
+
+    // use TEsScanNode
+    _properties = tnode.es_scan_node.properties;
+    return Status::OK;
+}
+
+Status EsHttpScanNode::prepare(RuntimeState* state) {
+    VLOG_QUERY << ""EsHttpScanNode prepare"";
+    RETURN_IF_ERROR(ScanNode::prepare(state));
+
+    _runtime_state = state;
+    _tuple_desc = state->desc_tbl().get_tuple_descriptor(_tuple_id);
+    if (_tuple_desc == nullptr) {
+        std::stringstream ss;
+        ss << ""Failed to get tuple descriptor, _tuple_id="" << _tuple_id;
+        return Status(ss.str());
+    }
+
+    // set up column name vector for ESScrollQueryBuilder
+    for (auto slot_desc : _tuple_desc->slots()) {
+        if (!slot_desc->is_materialized()) {
+            continue;","[{'comment': ""This should add a warning log, because the tuple received by BE does't contains unmaterialized slot in normal condition, except it's a bug."", 'commenter': 'chenhao7253886'}, {'comment': 'sometimes slot is not materialized, for example in query `select a from (select * from xxx) yyy`', 'commenter': 'imay'}, {'comment': 'No, it will materialize a in xxx.', 'commenter': 'chenhao7253886'}, {'comment': 'but other column is not materialized', 'commenter': 'imay'}]"
925,be/src/exec/es/es_predicate.cpp,"@@ -0,0 +1,419 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""exec/es/es_predicate.h""
+
+#include <stdint.h>
+#include <map>
+#include <sstream>
+#include <boost/algorithm/string.hpp>
+#include <gutil/strings/substitute.h>
+
+#include ""common/status.h""
+#include ""common/logging.h""
+#include ""exec/es/es_query_builder.h""
+#include ""exprs/expr.h""
+#include ""exprs/expr_context.h""
+#include ""exprs/in_predicate.h""
+
+#include ""gen_cpp/PlanNodes_types.h""
+#include ""olap/olap_common.h""
+#include ""olap/utils.h""
+#include ""runtime/client_cache.h""
+#include ""runtime/runtime_state.h""
+#include ""runtime/row_batch.h""
+#include ""runtime/datetime_value.h""
+#include ""runtime/large_int_value.h""
+#include ""runtime/string_value.h""
+#include ""runtime/tuple_row.h""
+
+#include ""service/backend_options.h""
+#include ""util/debug_util.h""
+#include ""util/runtime_profile.h""
+
+namespace doris {
+
+using namespace std;
+
+std::string ExtLiteral::value_to_string() {
+    std::stringstream ss;
+    switch (_type) {
+        case TYPE_TINYINT:
+            ss << std::to_string(get_byte());
+            break;
+        case TYPE_SMALLINT:
+            ss << std::to_string(get_short());
+            break;
+        case TYPE_INT:
+            ss << std::to_string(get_int());
+            break;
+        case TYPE_BIGINT:
+            ss << std::to_string(get_long());
+            break;
+        case TYPE_FLOAT:
+            ss << std::to_string(get_float());
+            break;
+        case TYPE_DOUBLE:
+            ss << std::to_string(get_double());
+            break;
+        case TYPE_CHAR:
+        case TYPE_VARCHAR:
+            ss << get_string();
+            break;
+        case TYPE_DATE:
+        case TYPE_DATETIME:
+            ss << get_date_string();
+            break;
+        case TYPE_BOOLEAN:
+            ss << std::to_string(get_bool());
+            break;
+        case TYPE_DECIMAL:
+            ss << get_decimal_string();
+            break;
+        case TYPE_DECIMALV2:
+            ss << get_decimalv2_string();
+            break;
+        case TYPE_LARGEINT:
+            ss << get_largeint_string();
+            break;
+        default:
+            DCHECK(false);
+            break;
+    }
+    return ss.str();
+}
+
+ExtLiteral::~ExtLiteral(){
+}
+
+int8_t ExtLiteral::get_byte() {
+    DCHECK(_type == TYPE_TINYINT);
+    return *(reinterpret_cast<int8_t*>(_value));
+}
+
+int16_t ExtLiteral::get_short() {
+    DCHECK(_type == TYPE_SMALLINT);
+    return *(reinterpret_cast<int16_t*>(_value));
+}
+
+int32_t ExtLiteral::get_int() {
+    DCHECK(_type == TYPE_INT);
+    return *(reinterpret_cast<int32_t*>(_value));
+}
+
+int64_t ExtLiteral::get_long() {
+    DCHECK(_type == TYPE_BIGINT);
+    return *(reinterpret_cast<int64_t*>(_value));
+}
+
+float ExtLiteral::get_float() {
+    DCHECK(_type == TYPE_FLOAT);
+    return *(reinterpret_cast<float*>(_value));
+}
+
+double ExtLiteral::get_double() {
+    DCHECK(_type == TYPE_DOUBLE);
+    return *(reinterpret_cast<double*>(_value));
+}
+
+std::string ExtLiteral::get_string() {
+    DCHECK(_type == TYPE_VARCHAR || _type == TYPE_CHAR);
+    return (reinterpret_cast<StringValue*>(_value))->to_string();
+}
+
+std::string ExtLiteral::get_date_string() {
+    DCHECK(_type == TYPE_DATE || _type == TYPE_DATETIME);
+    DateTimeValue date_value = *reinterpret_cast<DateTimeValue*>(_value);
+    if (_type == TYPE_DATE) {
+        date_value.cast_to_date();
+    }
+
+    char str[MAX_DTVALUE_STR_LEN];
+    date_value.to_string(str);
+    return std::string(str, strlen(str)); 
+}
+
+bool ExtLiteral::get_bool() {
+    DCHECK(_type == TYPE_BOOLEAN);
+    return *(reinterpret_cast<bool*>(_value));
+}
+
+std::string ExtLiteral::get_decimal_string() {
+    DCHECK(_type == TYPE_DECIMAL);
+    return reinterpret_cast<DecimalValue*>(_value)->to_string();
+}
+
+std::string ExtLiteral::get_decimalv2_string() {
+    DCHECK(_type == TYPE_DECIMALV2);
+    return reinterpret_cast<DecimalV2Value*>(_value)->to_string();
+}
+
+std::string ExtLiteral::get_largeint_string() {
+    DCHECK(_type == TYPE_LARGEINT);
+    return LargeIntValue::to_string(*reinterpret_cast<__int128*>(_value));
+}
+
+EsPredicate::EsPredicate(ExprContext* context,
+            const TupleDescriptor* tuple_desc) :
+    _context(context),
+    _disjuncts_num(0),
+    _tuple_desc(tuple_desc),
+    _es_query_status(Status::OK) {
+}
+
+EsPredicate::~EsPredicate() {
+    for(int i=0; i < _disjuncts.size(); i++) {
+        delete _disjuncts[i];
+    }
+    _disjuncts.clear();
+}
+
+bool EsPredicate::build_disjuncts_list() {
+    return build_disjuncts_list(_context->root());
+}
+
+// make sure to build by build_disjuncts_list
+const vector<ExtPredicate*>& EsPredicate::get_predicate_list(){
+    return _disjuncts;
+}
+
+static bool ignore_cast(const SlotDescriptor* slot, const Expr* expr) {
+    if (slot->type().is_date_type() && expr->type().is_date_type()) {
+        return true;
+    }
+    if (slot->type().is_string_type() && expr->type().is_string_type()) {
+        return true;
+    }
+    return false;
+}
+
+static bool is_literal_node(const Expr* expr) {
+    switch (expr->node_type()) {
+        case TExprNodeType::BOOL_LITERAL:
+        case TExprNodeType::INT_LITERAL:
+        case TExprNodeType::LARGE_INT_LITERAL:
+        case TExprNodeType::FLOAT_LITERAL:
+        case TExprNodeType::DECIMAL_LITERAL:
+        case TExprNodeType::STRING_LITERAL:
+        case TExprNodeType::DATE_LITERAL:
+            return true;
+        default:
+            return false;
+    }
+}
+
+bool EsPredicate::build_disjuncts_list(const Expr* conjunct) {
+    if (TExprNodeType::BINARY_PRED == conjunct->node_type()) {
+        if (conjunct->children().size() != 2) {
+            VLOG(1) << ""get disjuncts fail: number of childs is not 2"";
+            return false;
+        }
+
+        SlotRef* slotRef = nullptr;
+        TExprOpcode::type op;
+        Expr* expr = nullptr;
+        if (TExprNodeType::SLOT_REF == conjunct->get_child(0)->node_type()) {
+            expr = conjunct->get_child(1);
+            slotRef = (SlotRef*)(conjunct->get_child(0));
+            op = conjunct->op();
+        } else if (TExprNodeType::SLOT_REF == conjunct->get_child(1)->node_type()) {
+            expr = conjunct->get_child(0);
+            slotRef = (SlotRef*)(conjunct->get_child(1));
+            op = conjunct->op();
+        } else {
+            VLOG(1) << ""get disjuncts fail: no SLOT_REF child"";
+            return false;
+        }
+
+        const SlotDescriptor* slot_desc = get_slot_desc(slotRef);
+        if (slot_desc == nullptr) {
+            VLOG(1) << ""get disjuncts fail: slot_desc is null"";
+            return false;
+        }
+
+        if (!is_literal_node(expr)) {
+            VLOG(1) << ""get disjuncts fail: expr is not literal type"";
+            return false;
+        }
+
+        ExtLiteral literal(expr->type().type, _context->get_value(expr, NULL));
+        ExtPredicate* predicate = new ExtBinaryPredicate(
+                    TExprNodeType::BINARY_PRED,
+                    slot_desc->col_name(),
+                    slot_desc->type(),
+                    op,
+                    literal);
+
+        _disjuncts.push_back(predicate);
+        return true;
+    }
+    
+    if (is_match_func(conjunct)) {
+
+        Expr* expr = conjunct->get_child(1);
+        ExtLiteral literal(expr->type().type, _context->get_value(expr, NULL));
+        vector<ExtLiteral> query_conditions;
+        query_conditions.emplace_back(literal);
+        vector<ExtColumnDesc> cols; //TODO
+        ExtPredicate* predicate = new ExtFunction(
+                        TExprNodeType::FUNCTION_CALL,
+                        conjunct->fn().name.function_name,
+                        cols,
+                        query_conditions);
+        if (_es_query_status.ok()) {
+            _es_query_status 
+                = BooleanQueryBuilder::check_es_query(*(ExtFunction *)predicate); 
+            if (!_es_query_status.ok()) {
+                return false;
+            }
+        }
+        _disjuncts.push_back(predicate);
+
+        return true;
+    } 
+
+    if (TExprNodeType::FUNCTION_CALL == conjunct->node_type()) {
+        std::string fname = conjunct->fn().name.function_name;
+        if (fname != ""like"") {
+            return false;
+        }
+
+        SlotRef* slotRef = nullptr;
+        Expr* expr = nullptr;
+        if (TExprNodeType::SLOT_REF == conjunct->get_child(0)->node_type()) {
+            expr = conjunct->get_child(1);
+            slotRef = (SlotRef*)(conjunct->get_child(0));
+        } else if (TExprNodeType::SLOT_REF == conjunct->get_child(1)->node_type()) {
+            expr = conjunct->get_child(0);
+            slotRef = (SlotRef*)(conjunct->get_child(1));
+        } else {
+            VLOG(1) << ""get disjuncts fail: no SLOT_REF child"";
+            return false;
+        }
+
+        const SlotDescriptor* slot_desc = get_slot_desc(slotRef);
+        if (slot_desc == nullptr) {
+            VLOG(1) << ""get disjuncts fail: slot_desc is null"";
+            return false;
+        }
+
+        PrimitiveType type = expr->type().type;
+        if (type != TYPE_VARCHAR && type != TYPE_CHAR) {
+            VLOG(1) << ""get disjuncts fail: like value is not a string"";","[{'comment': ""LikePredicate's child has been checked in FE when analyzing."", 'commenter': 'chenhao7253886'}]"
925,be/src/exec/es/es_predicate.cpp,"@@ -0,0 +1,419 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""exec/es/es_predicate.h""
+
+#include <stdint.h>
+#include <map>
+#include <sstream>
+#include <boost/algorithm/string.hpp>
+#include <gutil/strings/substitute.h>
+
+#include ""common/status.h""
+#include ""common/logging.h""
+#include ""exec/es/es_query_builder.h""
+#include ""exprs/expr.h""
+#include ""exprs/expr_context.h""
+#include ""exprs/in_predicate.h""
+
+#include ""gen_cpp/PlanNodes_types.h""
+#include ""olap/olap_common.h""
+#include ""olap/utils.h""
+#include ""runtime/client_cache.h""
+#include ""runtime/runtime_state.h""
+#include ""runtime/row_batch.h""
+#include ""runtime/datetime_value.h""
+#include ""runtime/large_int_value.h""
+#include ""runtime/string_value.h""
+#include ""runtime/tuple_row.h""
+
+#include ""service/backend_options.h""
+#include ""util/debug_util.h""
+#include ""util/runtime_profile.h""
+
+namespace doris {
+
+using namespace std;
+
+std::string ExtLiteral::value_to_string() {
+    std::stringstream ss;
+    switch (_type) {
+        case TYPE_TINYINT:
+            ss << std::to_string(get_byte());
+            break;
+        case TYPE_SMALLINT:
+            ss << std::to_string(get_short());
+            break;
+        case TYPE_INT:
+            ss << std::to_string(get_int());
+            break;
+        case TYPE_BIGINT:
+            ss << std::to_string(get_long());
+            break;
+        case TYPE_FLOAT:
+            ss << std::to_string(get_float());
+            break;
+        case TYPE_DOUBLE:
+            ss << std::to_string(get_double());
+            break;
+        case TYPE_CHAR:
+        case TYPE_VARCHAR:
+            ss << get_string();
+            break;
+        case TYPE_DATE:
+        case TYPE_DATETIME:
+            ss << get_date_string();
+            break;
+        case TYPE_BOOLEAN:
+            ss << std::to_string(get_bool());
+            break;
+        case TYPE_DECIMAL:
+            ss << get_decimal_string();
+            break;
+        case TYPE_DECIMALV2:
+            ss << get_decimalv2_string();
+            break;
+        case TYPE_LARGEINT:
+            ss << get_largeint_string();
+            break;
+        default:
+            DCHECK(false);
+            break;
+    }
+    return ss.str();
+}
+
+ExtLiteral::~ExtLiteral(){
+}
+
+int8_t ExtLiteral::get_byte() {
+    DCHECK(_type == TYPE_TINYINT);
+    return *(reinterpret_cast<int8_t*>(_value));
+}
+
+int16_t ExtLiteral::get_short() {
+    DCHECK(_type == TYPE_SMALLINT);
+    return *(reinterpret_cast<int16_t*>(_value));
+}
+
+int32_t ExtLiteral::get_int() {
+    DCHECK(_type == TYPE_INT);
+    return *(reinterpret_cast<int32_t*>(_value));
+}
+
+int64_t ExtLiteral::get_long() {
+    DCHECK(_type == TYPE_BIGINT);
+    return *(reinterpret_cast<int64_t*>(_value));
+}
+
+float ExtLiteral::get_float() {
+    DCHECK(_type == TYPE_FLOAT);
+    return *(reinterpret_cast<float*>(_value));
+}
+
+double ExtLiteral::get_double() {
+    DCHECK(_type == TYPE_DOUBLE);
+    return *(reinterpret_cast<double*>(_value));
+}
+
+std::string ExtLiteral::get_string() {
+    DCHECK(_type == TYPE_VARCHAR || _type == TYPE_CHAR);
+    return (reinterpret_cast<StringValue*>(_value))->to_string();
+}
+
+std::string ExtLiteral::get_date_string() {
+    DCHECK(_type == TYPE_DATE || _type == TYPE_DATETIME);
+    DateTimeValue date_value = *reinterpret_cast<DateTimeValue*>(_value);
+    if (_type == TYPE_DATE) {
+        date_value.cast_to_date();
+    }
+
+    char str[MAX_DTVALUE_STR_LEN];
+    date_value.to_string(str);
+    return std::string(str, strlen(str)); 
+}
+
+bool ExtLiteral::get_bool() {
+    DCHECK(_type == TYPE_BOOLEAN);
+    return *(reinterpret_cast<bool*>(_value));
+}
+
+std::string ExtLiteral::get_decimal_string() {
+    DCHECK(_type == TYPE_DECIMAL);
+    return reinterpret_cast<DecimalValue*>(_value)->to_string();
+}
+
+std::string ExtLiteral::get_decimalv2_string() {
+    DCHECK(_type == TYPE_DECIMALV2);
+    return reinterpret_cast<DecimalV2Value*>(_value)->to_string();
+}
+
+std::string ExtLiteral::get_largeint_string() {
+    DCHECK(_type == TYPE_LARGEINT);
+    return LargeIntValue::to_string(*reinterpret_cast<__int128*>(_value));
+}
+
+EsPredicate::EsPredicate(ExprContext* context,
+            const TupleDescriptor* tuple_desc) :
+    _context(context),
+    _disjuncts_num(0),
+    _tuple_desc(tuple_desc),
+    _es_query_status(Status::OK) {
+}
+
+EsPredicate::~EsPredicate() {
+    for(int i=0; i < _disjuncts.size(); i++) {
+        delete _disjuncts[i];
+    }
+    _disjuncts.clear();
+}
+
+bool EsPredicate::build_disjuncts_list() {
+    return build_disjuncts_list(_context->root());
+}
+
+// make sure to build by build_disjuncts_list
+const vector<ExtPredicate*>& EsPredicate::get_predicate_list(){
+    return _disjuncts;
+}
+
+static bool ignore_cast(const SlotDescriptor* slot, const Expr* expr) {
+    if (slot->type().is_date_type() && expr->type().is_date_type()) {
+        return true;
+    }
+    if (slot->type().is_string_type() && expr->type().is_string_type()) {
+        return true;
+    }
+    return false;
+}
+
+static bool is_literal_node(const Expr* expr) {
+    switch (expr->node_type()) {
+        case TExprNodeType::BOOL_LITERAL:
+        case TExprNodeType::INT_LITERAL:
+        case TExprNodeType::LARGE_INT_LITERAL:
+        case TExprNodeType::FLOAT_LITERAL:
+        case TExprNodeType::DECIMAL_LITERAL:
+        case TExprNodeType::STRING_LITERAL:
+        case TExprNodeType::DATE_LITERAL:
+            return true;
+        default:
+            return false;
+    }
+}
+
+bool EsPredicate::build_disjuncts_list(const Expr* conjunct) {
+    if (TExprNodeType::BINARY_PRED == conjunct->node_type()) {
+        if (conjunct->children().size() != 2) {
+            VLOG(1) << ""get disjuncts fail: number of childs is not 2"";
+            return false;
+        }
+
+        SlotRef* slotRef = nullptr;
+        TExprOpcode::type op;
+        Expr* expr = nullptr;
+        if (TExprNodeType::SLOT_REF == conjunct->get_child(0)->node_type()) {
+            expr = conjunct->get_child(1);
+            slotRef = (SlotRef*)(conjunct->get_child(0));
+            op = conjunct->op();
+        } else if (TExprNodeType::SLOT_REF == conjunct->get_child(1)->node_type()) {
+            expr = conjunct->get_child(0);
+            slotRef = (SlotRef*)(conjunct->get_child(1));
+            op = conjunct->op();
+        } else {
+            VLOG(1) << ""get disjuncts fail: no SLOT_REF child"";
+            return false;
+        }
+
+        const SlotDescriptor* slot_desc = get_slot_desc(slotRef);
+        if (slot_desc == nullptr) {
+            VLOG(1) << ""get disjuncts fail: slot_desc is null"";
+            return false;
+        }
+
+        if (!is_literal_node(expr)) {
+            VLOG(1) << ""get disjuncts fail: expr is not literal type"";
+            return false;
+        }
+
+        ExtLiteral literal(expr->type().type, _context->get_value(expr, NULL));
+        ExtPredicate* predicate = new ExtBinaryPredicate(
+                    TExprNodeType::BINARY_PRED,
+                    slot_desc->col_name(),
+                    slot_desc->type(),
+                    op,
+                    literal);
+
+        _disjuncts.push_back(predicate);
+        return true;
+    }
+    
+    if (is_match_func(conjunct)) {
+
+        Expr* expr = conjunct->get_child(1);
+        ExtLiteral literal(expr->type().type, _context->get_value(expr, NULL));
+        vector<ExtLiteral> query_conditions;
+        query_conditions.emplace_back(literal);
+        vector<ExtColumnDesc> cols; //TODO
+        ExtPredicate* predicate = new ExtFunction(
+                        TExprNodeType::FUNCTION_CALL,
+                        conjunct->fn().name.function_name,
+                        cols,
+                        query_conditions);
+        if (_es_query_status.ok()) {
+            _es_query_status 
+                = BooleanQueryBuilder::check_es_query(*(ExtFunction *)predicate); 
+            if (!_es_query_status.ok()) {
+                return false;
+            }
+        }
+        _disjuncts.push_back(predicate);
+
+        return true;
+    } 
+
+    if (TExprNodeType::FUNCTION_CALL == conjunct->node_type()) {
+        std::string fname = conjunct->fn().name.function_name;
+        if (fname != ""like"") {
+            return false;
+        }
+
+        SlotRef* slotRef = nullptr;
+        Expr* expr = nullptr;
+        if (TExprNodeType::SLOT_REF == conjunct->get_child(0)->node_type()) {
+            expr = conjunct->get_child(1);
+            slotRef = (SlotRef*)(conjunct->get_child(0));
+        } else if (TExprNodeType::SLOT_REF == conjunct->get_child(1)->node_type()) {
+            expr = conjunct->get_child(0);
+            slotRef = (SlotRef*)(conjunct->get_child(1));
+        } else {
+            VLOG(1) << ""get disjuncts fail: no SLOT_REF child"";
+            return false;
+        }
+
+        const SlotDescriptor* slot_desc = get_slot_desc(slotRef);
+        if (slot_desc == nullptr) {
+            VLOG(1) << ""get disjuncts fail: slot_desc is null"";
+            return false;
+        }
+
+        PrimitiveType type = expr->type().type;
+        if (type != TYPE_VARCHAR && type != TYPE_CHAR) {
+            VLOG(1) << ""get disjuncts fail: like value is not a string"";
+            return false;
+        }
+
+        ExtLiteral literal(type, _context->get_value(expr, NULL));
+        ExtPredicate* predicate = new ExtLikePredicate(
+                    TExprNodeType::LIKE_PRED,
+                    slot_desc->col_name(),
+                    slot_desc->type(),
+                    literal);
+
+        _disjuncts.push_back(predicate);
+        return true;
+    }
+      
+    if (TExprNodeType::IN_PRED == conjunct->node_type()) {
+        // the op code maybe FILTER_NEW_IN, it means there is function in list
+        // like col_a in (abs(1))
+        if (TExprOpcode::FILTER_IN != conjunct->op() 
+            && TExprOpcode::FILTER_NOT_IN != conjunct->op()) {
+            return false;
+        }
+
+        vector<ExtLiteral> in_pred_values;
+        const InPredicate* pred = dynamic_cast<const InPredicate*>(conjunct);
+        if (Expr::type_without_cast(pred->get_child(0)) != TExprNodeType::SLOT_REF) {
+            return false;
+        }
+
+        SlotRef* slot_ref = (SlotRef*)(conjunct->get_child(0));
+        const SlotDescriptor* slot_desc = get_slot_desc(slot_ref);
+        if (slot_desc == nullptr) {
+            return false;
+        }
+
+        if (pred->get_child(0)->type().type != slot_desc->type().type) {
+            if (!ignore_cast(slot_desc, pred->get_child(0))) {
+                return false;
+            }
+        }
+
+        HybirdSetBase::IteratorBase* iter = pred->hybird_set()->begin();
+        while (iter->has_next()) {
+            if (nullptr == iter->get_value()) {
+                return false;
+            }
+
+            ExtLiteral literal(slot_desc->type().type, const_cast<void *>(iter->get_value()));
+            in_pred_values.emplace_back(literal);
+            iter->next();
+        }
+
+        ExtPredicate* predicate = new ExtInPredicate(
+                    TExprNodeType::IN_PRED,
+                    pred->is_not_in(),
+                    slot_desc->col_name(),
+                    slot_desc->type(),
+                    in_pred_values);
+        _disjuncts.push_back(predicate);
+
+        return true;
+    } 
+    
+    if (TExprNodeType::COMPOUND_PRED == conjunct->node_type()) {
+        if (TExprOpcode::COMPOUND_OR != conjunct->op()) {
+            VLOG(1) << ""get disjuncts fail: op is not COMPOUND_OR"";","[{'comment': ""The conjunct list BE received from FE is Conjunctive normal form, so the relationship between each element is `and`, the disconjunctive normal form is in single element.  it can't express OR relationship right, and can't handle conjuenct containing `AND` and `OR`. "", 'commenter': 'chenhao7253886'}, {'comment': 'Yes, the conjunct here does not been push down to ES now, we will improve it by push such expression to ES in future. ', 'commenter': 'lide-reed'}]"
925,fe/src/main/java/org/apache/doris/catalog/EsTable.java,"@@ -106,8 +114,16 @@ private void validate(Map<String, String> properties) throws DdlException {
                 && !Strings.isNullOrEmpty(properties.get(TYPE).trim())) {
             mappingType = properties.get(TYPE).trim();
         }
+        if (!Strings.isNullOrEmpty(properties.get(TRANSPORT))
+                && !Strings.isNullOrEmpty(properties.get(TRANSPORT).trim())) {
+            transport = properties.get(TRANSPORT).trim();
+            if (!(TRANSPORT_HTTP.equals(transport) || TRANSPORT_THRIFT.equals(transport))) {","[{'comment': 'Properties get same key three times, it better add tmp variable.', 'commenter': 'chenhao7253886'}]"
925,be/src/exec/es/es_scan_reader.cpp,"@@ -0,0 +1,160 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""exec/es/es_scan_reader.h""
+
+#include <map>
+#include <string>
+#include <sstream>
+
+#include ""common/logging.h""
+#include ""common/status.h""
+#include ""exec/es/es_scroll_query.h""
+
+namespace doris {
+const std::string REUQEST_SCROLL_FILTER_PATH = ""filter_path=_scroll_id,hits.hits._source,hits.total,_id,hits.hits._source.fields"";
+const std::string REQUEST_SCROLL_PATH = ""_scroll"";
+const std::string REQUEST_PREFERENCE_PREFIX = ""&preference=_shards:"";
+const std::string REQUEST_SEARCH_SCROLL_PATH = ""/_search/scroll"";
+const std::string REQUEST_SEPARATOR = ""/"";
+const std::string REQUEST_SCROLL_TIME = ""5m"";
+
+ESScanReader::ESScanReader(const std::string& target, const std::map<std::string, std::string>& props) {
+    _target = target;
+    _index = props.at(KEY_INDEX);
+    _type = props.at(KEY_TYPE);
+    if (props.find(KEY_USER_NAME) != props.end()) {
+        _user_name = props.at(KEY_USER_NAME);
+    }
+    if (props.find(KEY_PASS_WORD) != props.end()){
+        _passwd = props.at(KEY_PASS_WORD);
+    }
+    if (props.find(KEY_SHARD) != props.end()) {
+        _shards = props.at(KEY_SHARD);
+    }
+    if (props.find(KEY_QUERY) != props.end()) {
+        _query = props.at(KEY_QUERY);
+    }
+    std::string batch_size_str = props.at(KEY_BATCH_SIZE);
+    _batch_size = atoi(batch_size_str.c_str());
+    _init_scroll_url = _target + REQUEST_SEPARATOR + _index + REQUEST_SEPARATOR + _type + ""/_search?scroll="" + REQUEST_SCROLL_TIME + REQUEST_PREFERENCE_PREFIX + _shards + ""&"" + REUQEST_SCROLL_FILTER_PATH;
+    _next_scroll_url = _target + REQUEST_SEARCH_SCROLL_PATH + ""?"" + REUQEST_SCROLL_FILTER_PATH;
+    _eos = false;
+}
+
+ESScanReader::~ESScanReader() {
+}
+
+Status ESScanReader::open() {
+    _is_first = true;
+    RETURN_IF_ERROR(_network_client.init(_init_scroll_url));
+    _network_client.set_basic_auth(_user_name, _passwd);
+    _network_client.set_content_type(""application/json"");
+    // phase open, we cached the first response for `get_next` phase
+    Status status = _network_client.execute_post_request(_query, &_cached_response);
+    if (!status.ok() || _network_client.get_http_status() != 200) {
+        std::stringstream ss;
+        ss << ""Failed to connect to ES server, errmsg is: "" << status.get_error_msg();
+        LOG(WARNING) << ss.str();
+        return Status(ss.str());
+    }
+    VLOG(1) << ""open _cached response: "" << _cached_response;
+    return Status::OK;
+}
+
+Status ESScanReader::get_next(bool* scan_eos, ScrollParser** parser) {
+    std::string response;
+    ScrollParser* scroll_parser = nullptr;
+    // if is first scroll request, should return the cached response
+    *parser = nullptr;
+    *scan_eos = true;
+    if (_eos) {
+        return Status::OK;
+    }
+
+    if (_is_first) {
+        response = _cached_response;
+        _is_first = false;
+    } else {
+        RETURN_IF_ERROR(_network_client.init(_next_scroll_url));
+        _network_client.set_basic_auth(_user_name, _passwd);
+        _network_client.set_content_type(""application/json"");
+        _network_client.set_timeout_ms(5 * 1000);
+        RETURN_IF_ERROR(_network_client.execute_post_request(
+                        ESScrollQueryBuilder::build_next_scroll_body(_scroll_id, REQUEST_SCROLL_TIME), &response));
+        long status = _network_client.get_http_status();
+        if (status == 404) {
+            LOG(WARNING) << ""request scroll search failure 404["" 
+                         << "", response: "" << (response.empty() ? ""empty response"" : response);
+            return Status(""No search context found for "" + _scroll_id);
+        }
+        if (status != 200) {
+            LOG(WARNING) << ""request scroll search failure["" 
+                         << ""http status"" << status
+                         << "", response: "" << (response.empty() ? ""empty response"" : response);
+            if (status == 404) {
+                    return Status(""No search context found for "" + _scroll_id);
+                }
+            return Status(""request scroll search failure: "" + (response.empty() ? ""empty response"" : response));        
+        }
+    }
+
+    scroll_parser = new ScrollParser();","[{'comment': 'memory leak', 'commenter': 'imay'}, {'comment': 'Done', 'commenter': 'lide-reed'}]"
925,be/src/exec/es_http_scanner.cpp,"@@ -0,0 +1,131 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""exec/es_http_scanner.h""
+
+#include <sstream>
+#include <iostream>
+
+#include ""runtime/descriptors.h""
+#include ""runtime/exec_env.h""
+#include ""runtime/mem_tracker.h""
+#include ""runtime/raw_value.h""
+#include ""runtime/runtime_state.h""
+#include ""runtime/tuple.h""
+#include ""exprs/expr.h""
+
+namespace doris {
+
+EsHttpScanner::EsHttpScanner(
+            RuntimeState* state,
+            RuntimeProfile* profile,
+            TupleId tuple_id,
+            const std::map<std::string, std::string>& properties,
+            const std::vector<ExprContext*>& conjunct_ctxs,
+            EsScanCounter* counter) :
+        _state(state),
+        _profile(profile),
+        _tuple_id(tuple_id),
+        _properties(properties),
+        _conjunct_ctxs(conjunct_ctxs),
+        _next_range(0),
+        _line_eof(false),
+        _batch_eof(false),
+#if BE_TEST
+        _mem_tracker(new MemTracker()),
+        _mem_pool(_mem_tracker.get()),
+#else 
+        _mem_tracker(new MemTracker(-1, ""EsHttp Scanner"", state->instance_mem_tracker())),
+        _mem_pool(_state->instance_mem_tracker()),
+#endif
+        _tuple_desc(nullptr),
+        _counter(counter),
+        _es_reader(nullptr),
+        _es_scroll_parser(nullptr),
+        _rows_read_counter(nullptr),
+        _read_timer(nullptr),
+        _materialize_timer(nullptr) {
+}
+
+EsHttpScanner::~EsHttpScanner() {
+    close();","[{'comment': 'you should delete _es_scroll_parser here, otherwise there is a memory leak', 'commenter': 'imay'}, {'comment': 'Change to use unique_ptr to avoid memory leak', 'commenter': 'lide-reed'}]"
925,be/src/exec/es/es_query_builder.cpp,"@@ -0,0 +1,375 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""exec/es/es_query_builder.h""
+
+#include <boost/algorithm/string/replace.hpp>
+#include ""rapidjson/rapidjson.h""
+#include ""rapidjson/stringbuffer.h""
+#include ""rapidjson/writer.h""
+#include ""common/logging.h""
+
+namespace doris {
+
+ESQueryBuilder::ESQueryBuilder(const std::string& es_query_str) : _es_query_str(es_query_str) {
+
+}
+ESQueryBuilder::ESQueryBuilder(const ExtFunction& es_query) {
+    auto first = es_query.values.front();
+    _es_query_str = first.to_string();
+}
+
+// note: call this function must invoke BooleanQueryBuilder::check_es_query to check validation
+void ESQueryBuilder::to_json(rapidjson::Document* document, rapidjson::Value* query) {
+    rapidjson::Document scratch_document;
+    scratch_document.Parse(_es_query_str.c_str());
+    rapidjson::Document::AllocatorType& allocator = document->GetAllocator();
+    rapidjson::Value query_key;
+    rapidjson::Value query_value;
+    //{ ""term"": { ""dv"": ""2"" } }
+    rapidjson::Value::ConstMemberIterator first = scratch_document.MemberBegin();
+    // deep copy, reference http://rapidjson.org/md_doc_tutorial.html#DeepCopyValue
+    query_key.CopyFrom(first->name, allocator);
+    // if we found one key, then end loop as QueryDSL only support one `query` root
+    query_value.CopyFrom(first->value, allocator);
+    // Move Semantics, reference http://rapidjson.org/md_doc_tutorial.html#MoveSemantics 
+    query->AddMember(query_key, query_value, allocator);
+}
+
+TermQueryBuilder::TermQueryBuilder(const std::string& field, const std::string& term) : _field(field), _term(term) {
+
+}
+
+TermQueryBuilder::TermQueryBuilder(const ExtBinaryPredicate& binary_predicate) {
+    _field =  binary_predicate.col.name;
+    _term = binary_predicate.value.to_string();
+}
+
+void TermQueryBuilder::to_json(rapidjson::Document* document, rapidjson::Value* query) {
+    rapidjson::Document::AllocatorType& allocator = document->GetAllocator();
+    rapidjson::Value term_node(rapidjson::kObjectType);
+    term_node.SetObject();
+    rapidjson::Value field_value(_field.c_str(), allocator);
+    rapidjson::Value term_value(_term.c_str(), allocator);
+    term_node.AddMember(field_value, term_value, allocator);
+    query->AddMember(""term"", term_node, allocator);
+}
+
+RangeQueryBuilder::RangeQueryBuilder(const ExtBinaryPredicate& range_predicate) {
+    _field = range_predicate.col.name;
+    _value = range_predicate.value.to_string();
+    _op = range_predicate.op;
+}
+
+void RangeQueryBuilder::to_json(rapidjson::Document* document, rapidjson::Value* query) {
+    rapidjson::Document::AllocatorType& allocator = document->GetAllocator();
+    rapidjson::Value field_value(_field.c_str(), allocator);
+    rapidjson::Value value(_value.c_str(), allocator);
+    rapidjson::Value op_node(rapidjson::kObjectType);
+    op_node.SetObject();
+    switch (_op) {
+        case TExprOpcode::LT:
+            op_node.AddMember(""lt"", value, allocator);
+            break;
+        case TExprOpcode::LE:
+            op_node.AddMember(""le"", value, allocator);
+            break;
+        case TExprOpcode::GT:
+            op_node.AddMember(""gt"", value, allocator);
+            break;
+        case TExprOpcode::GE:
+            op_node.AddMember(""ge"", value, allocator);
+            break;
+        default:
+            break;
+    }
+    rapidjson::Value field_node(rapidjson::kObjectType);
+    field_node.SetObject();
+    field_node.AddMember(field_value, op_node, allocator);
+    query->AddMember(""range"", field_node, allocator);
+}
+
+void WildCardQueryBuilder::to_json(rapidjson::Document* document, rapidjson::Value* query) {
+    rapidjson::Document::AllocatorType& allocator = document->GetAllocator();
+    rapidjson::Value term_node(rapidjson::kObjectType);
+    term_node.SetObject();
+    rapidjson::Value field_value(_field.c_str(), allocator);
+    rapidjson::Value term_value(_like_value.c_str(), allocator);
+    term_node.AddMember(field_value, term_value, allocator);
+    query->AddMember(""wildcard"", term_node, allocator);
+}
+WildCardQueryBuilder::WildCardQueryBuilder(const ExtLikePredicate& like_predicate) {
+    _like_value = like_predicate.value.to_string();
+    std::replace(_like_value.begin(), _like_value.end(), '_', '?');
+    std::replace(_like_value.begin(), _like_value.end(), '%', '*');
+    _field = like_predicate.col.name;
+}
+
+void TermsInSetQueryBuilder::to_json(rapidjson::Document* document, rapidjson::Value* query) {
+    rapidjson::Document::AllocatorType& allocator = document->GetAllocator();
+    rapidjson::Value terms_node(rapidjson::kObjectType);
+    rapidjson::Value values_node(rapidjson::kArrayType);
+    for (auto value : _values) {
+        rapidjson::Value value_value(value.c_str(), allocator);
+        values_node.PushBack(value_value, allocator);
+    }
+    rapidjson::Value field_value(_field.c_str(), allocator);
+    terms_node.AddMember(field_value, values_node, allocator);
+    query->AddMember(""terms"", terms_node, allocator);
+}
+
+TermsInSetQueryBuilder::TermsInSetQueryBuilder(const ExtInPredicate& in_predicate) {
+    _field = in_predicate.col.name;
+    for (auto value : in_predicate.values) {","[{'comment': 'auto& to avoid copy', 'commenter': 'imay'}, {'comment': 'Done', 'commenter': 'lide-reed'}]"
925,be/src/exec/es/es_predicate.cpp,"@@ -0,0 +1,415 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""exec/es/es_predicate.h""
+
+#include <stdint.h>
+#include <map>
+#include <sstream>
+#include <boost/algorithm/string.hpp>
+#include <gutil/strings/substitute.h>
+
+#include ""common/status.h""
+#include ""common/logging.h""
+#include ""exec/es/es_query_builder.h""
+#include ""exprs/expr.h""
+#include ""exprs/expr_context.h""
+#include ""exprs/in_predicate.h""
+
+#include ""gen_cpp/PlanNodes_types.h""
+#include ""olap/olap_common.h""
+#include ""olap/utils.h""
+#include ""runtime/client_cache.h""
+#include ""runtime/runtime_state.h""
+#include ""runtime/row_batch.h""
+#include ""runtime/datetime_value.h""
+#include ""runtime/large_int_value.h""
+#include ""runtime/string_value.h""
+#include ""runtime/tuple_row.h""
+
+#include ""service/backend_options.h""
+#include ""util/debug_util.h""
+#include ""util/runtime_profile.h""
+
+namespace doris {
+
+using namespace std;
+
+std::string ExtLiteral::value_to_string() {
+    std::stringstream ss;
+    switch (_type) {
+        case TYPE_TINYINT:
+            ss << std::to_string(get_byte());
+            break;
+        case TYPE_SMALLINT:
+            ss << std::to_string(get_short());
+            break;
+        case TYPE_INT:
+            ss << std::to_string(get_int());
+            break;
+        case TYPE_BIGINT:
+            ss << std::to_string(get_long());
+            break;
+        case TYPE_FLOAT:
+            ss << std::to_string(get_float());
+            break;
+        case TYPE_DOUBLE:
+            ss << std::to_string(get_double());
+            break;
+        case TYPE_CHAR:
+        case TYPE_VARCHAR:
+            ss << get_string();
+            break;
+        case TYPE_DATE:
+        case TYPE_DATETIME:
+            ss << get_date_string();
+            break;
+        case TYPE_BOOLEAN:
+            ss << std::to_string(get_bool());
+            break;
+        case TYPE_DECIMAL:
+            ss << get_decimal_string();
+            break;
+        case TYPE_DECIMALV2:
+            ss << get_decimalv2_string();
+            break;
+        case TYPE_LARGEINT:
+            ss << get_largeint_string();
+            break;
+        default:
+            DCHECK(false);
+            break;
+    }
+    return ss.str();
+}
+
+ExtLiteral::~ExtLiteral(){
+}
+
+int8_t ExtLiteral::get_byte() {
+    DCHECK(_type == TYPE_TINYINT);
+    return *(reinterpret_cast<int8_t*>(_value));
+}
+
+int16_t ExtLiteral::get_short() {
+    DCHECK(_type == TYPE_SMALLINT);
+    return *(reinterpret_cast<int16_t*>(_value));
+}
+
+int32_t ExtLiteral::get_int() {
+    DCHECK(_type == TYPE_INT);
+    return *(reinterpret_cast<int32_t*>(_value));
+}
+
+int64_t ExtLiteral::get_long() {
+    DCHECK(_type == TYPE_BIGINT);
+    return *(reinterpret_cast<int64_t*>(_value));
+}
+
+float ExtLiteral::get_float() {
+    DCHECK(_type == TYPE_FLOAT);
+    return *(reinterpret_cast<float*>(_value));
+}
+
+double ExtLiteral::get_double() {
+    DCHECK(_type == TYPE_DOUBLE);
+    return *(reinterpret_cast<double*>(_value));
+}
+
+std::string ExtLiteral::get_string() {
+    DCHECK(_type == TYPE_VARCHAR || _type == TYPE_CHAR);
+    return (reinterpret_cast<StringValue*>(_value))->to_string();
+}
+
+std::string ExtLiteral::get_date_string() {
+    DCHECK(_type == TYPE_DATE || _type == TYPE_DATETIME);
+    DateTimeValue date_value = *reinterpret_cast<DateTimeValue*>(_value);
+    if (_type == TYPE_DATE) {
+        date_value.cast_to_date();
+    }
+
+    char str[MAX_DTVALUE_STR_LEN];
+    date_value.to_string(str);
+    return std::string(str, strlen(str)); 
+}
+
+bool ExtLiteral::get_bool() {
+    DCHECK(_type == TYPE_BOOLEAN);
+    return *(reinterpret_cast<bool*>(_value));
+}
+
+std::string ExtLiteral::get_decimal_string() {
+    DCHECK(_type == TYPE_DECIMAL);
+    return reinterpret_cast<DecimalValue*>(_value)->to_string();
+}
+
+std::string ExtLiteral::get_decimalv2_string() {
+    DCHECK(_type == TYPE_DECIMALV2);
+    return reinterpret_cast<DecimalV2Value*>(_value)->to_string();
+}
+
+std::string ExtLiteral::get_largeint_string() {
+    DCHECK(_type == TYPE_LARGEINT);
+    return LargeIntValue::to_string(*reinterpret_cast<__int128*>(_value));
+}
+
+EsPredicate::EsPredicate(ExprContext* context,
+            const TupleDescriptor* tuple_desc) :
+    _context(context),
+    _disjuncts_num(0),
+    _tuple_desc(tuple_desc),
+    _es_query_status(Status::OK) {
+}
+
+EsPredicate::~EsPredicate() {
+    for(int i=0; i < _disjuncts.size(); i++) {
+        delete _disjuncts[i];
+    }
+    _disjuncts.clear();
+}
+
+Status EsPredicate::build_disjuncts_list() {
+    return build_disjuncts_list(_context->root());
+}
+
+// make sure to build by build_disjuncts_list
+const vector<ExtPredicate*>& EsPredicate::get_predicate_list(){
+    return _disjuncts;
+}
+
+static bool ignore_cast(const SlotDescriptor* slot, const Expr* expr) {
+    if (slot->type().is_date_type() && expr->type().is_date_type()) {
+        return true;
+    }
+    if (slot->type().is_string_type() && expr->type().is_string_type()) {
+        return true;
+    }
+    return false;
+}
+
+static bool is_literal_node(const Expr* expr) {
+    switch (expr->node_type()) {
+        case TExprNodeType::BOOL_LITERAL:
+        case TExprNodeType::INT_LITERAL:
+        case TExprNodeType::LARGE_INT_LITERAL:
+        case TExprNodeType::FLOAT_LITERAL:
+        case TExprNodeType::DECIMAL_LITERAL:
+        case TExprNodeType::STRING_LITERAL:
+        case TExprNodeType::DATE_LITERAL:
+            return true;
+        default:
+            return false;
+    }
+}
+
+Status EsPredicate::build_disjuncts_list(const Expr* conjunct) {
+    if (TExprNodeType::BINARY_PRED == conjunct->node_type()) {
+        if (conjunct->children().size() != 2) {
+            return Status(""build disjuncts failed: number of childs is not 2"");
+        }
+
+        SlotRef* slotRef = nullptr;
+        TExprOpcode::type op;
+        Expr* expr = nullptr;
+        if (TExprNodeType::SLOT_REF == conjunct->get_child(0)->node_type()) {
+            expr = conjunct->get_child(1);
+            slotRef = (SlotRef*)(conjunct->get_child(0));
+            op = conjunct->op();
+        } else if (TExprNodeType::SLOT_REF == conjunct->get_child(1)->node_type()) {
+            expr = conjunct->get_child(0);
+            slotRef = (SlotRef*)(conjunct->get_child(1));
+            op = conjunct->op();
+        } else {
+            return Status(""build disjuncts failed: no SLOT_REF child"");
+        }
+
+        const SlotDescriptor* slot_desc = get_slot_desc(slotRef);
+        if (slot_desc == nullptr) {
+            return Status(""build disjuncts failed: slot_desc is null"");
+        }
+
+        if (!is_literal_node(expr)) {
+            return Status(""build disjuncts failed: expr is not literal type"");
+        }
+
+        ExtLiteral literal(expr->type().type, _context->get_value(expr, NULL));
+        ExtPredicate* predicate = new ExtBinaryPredicate(
+                    TExprNodeType::BINARY_PRED,
+                    slot_desc->col_name(),
+                    slot_desc->type(),
+                    op,
+                    literal);
+
+        _disjuncts.push_back(predicate);
+        return Status::OK;
+    }
+    
+    if (is_match_func(conjunct)) {
+        Expr* expr = conjunct->get_child(1);
+        ExtLiteral literal(expr->type().type, _context->get_value(expr, NULL));
+        vector<ExtLiteral> query_conditions;
+        query_conditions.emplace_back(literal);
+        vector<ExtColumnDesc> cols; //TODO
+        ExtPredicate* predicate = new ExtFunction(
+                        TExprNodeType::FUNCTION_CALL,
+                        conjunct->fn().name.function_name,
+                        cols,
+                        query_conditions);
+        if (_es_query_status.ok()) {
+            _es_query_status 
+                = BooleanQueryBuilder::check_es_query(*(ExtFunction *)predicate); 
+            if (!_es_query_status.ok()) {
+                delete predicate;
+                return _es_query_status;
+            }
+        }
+        _disjuncts.push_back(predicate);
+
+        return Status::OK;
+    } 
+
+    if (TExprNodeType::FUNCTION_CALL == conjunct->node_type()) {
+        std::string fname = conjunct->fn().name.function_name;
+        if (fname != ""like"") {
+            return Status(""build disjuncts failed: function name is not like"");
+        }
+
+        SlotRef* slotRef = nullptr;
+        Expr* expr = nullptr;
+        if (TExprNodeType::SLOT_REF == conjunct->get_child(0)->node_type()) {
+            expr = conjunct->get_child(1);
+            slotRef = (SlotRef*)(conjunct->get_child(0));
+        } else if (TExprNodeType::SLOT_REF == conjunct->get_child(1)->node_type()) {
+            expr = conjunct->get_child(0);
+            slotRef = (SlotRef*)(conjunct->get_child(1));
+        } else {
+            return Status(""build disjuncts failed: no SLOT_REF child"");
+        }
+
+        const SlotDescriptor* slot_desc = get_slot_desc(slotRef);
+        if (slot_desc == nullptr) {
+            return Status(""build disjuncts failed: slot_desc is null"");
+        }
+
+        PrimitiveType type = expr->type().type;
+        if (type != TYPE_VARCHAR && type != TYPE_CHAR) {
+            return Status(""build disjuncts failed: like value is not a string"");
+        }
+
+        ExtLiteral literal(type, _context->get_value(expr, NULL));
+        ExtPredicate* predicate = new ExtLikePredicate(
+                    TExprNodeType::LIKE_PRED,
+                    slot_desc->col_name(),
+                    slot_desc->type(),
+                    literal);
+
+        _disjuncts.push_back(predicate);
+        return Status::OK;
+    }
+      
+    if (TExprNodeType::IN_PRED == conjunct->node_type()) {
+        // the op code maybe FILTER_NEW_IN, it means there is function in list
+        // like col_a in (abs(1))
+        if (TExprOpcode::FILTER_IN != conjunct->op() 
+            && TExprOpcode::FILTER_NOT_IN != conjunct->op()) {
+            return Status(""build disjuncts failed: ""
+                        ""opcode in IN_PRED is neither FILTER_IN nor FILTER_NOT_IN"");
+        }
+
+        vector<ExtLiteral> in_pred_values;
+        const InPredicate* pred = dynamic_cast<const InPredicate*>(conjunct);
+        if (Expr::type_without_cast(pred->get_child(0)) != TExprNodeType::SLOT_REF) {","[{'comment': 'add a function to get `expr` without cast', 'commenter': 'imay'}, {'comment': 'Done', 'commenter': 'lide-reed'}]"
932,be/src/olap/tablet_manager.cpp,"@@ -100,15 +99,22 @@ OLAPStatus TabletManager::add_tablet(TTabletId tablet_id, SchemaHash schema_hash
     if (table_item == nullptr) {
         _tablet_map[tablet_id].table_arr.push_back(tablet);
         _tablet_map[tablet_id].table_arr.sort(_sort_tablet_by_creation_time);
-        _tablet_map_lock.unlock();
         return res;
     }
-    _tablet_map_lock.unlock();
 
     if (!force) {
         if (table_item->tablet_path() == tablet->tablet_path()) {
             LOG(WARNING) << ""add the same tablet twice! tablet_id=""
-                << tablet_id << "" schema_hash="" << tablet_id;
+                         << tablet_id << "" schema_hash="" << tablet_id;
+            return OLAP_ERR_ENGINE_INSERT_EXISTS_TABLE;
+        }
+    }
+
+    // if not force, the data dir should not same
+    if (!force) {
+        if (table_item->data_dir() == tablet->data_dir()) {","[{'comment': 'move these lines to line 110', 'commenter': 'kangpinghuang'}, {'comment': 'done', 'commenter': 'yiguolei'}]"
932,be/src/olap/tablet_manager.cpp,"@@ -81,13 +81,12 @@ TabletManager::TabletManager()
     : _tablet_stat_cache_update_time_ms(0),
       _available_storage_medium_type_count(0) { }
 
-OLAPStatus TabletManager::add_tablet(TTabletId tablet_id, SchemaHash schema_hash,
+OLAPStatus TabletManager::_add_tablet_unlock(TTabletId tablet_id, SchemaHash schema_hash,
                                  const TabletSharedPtr& tablet, bool force) {
     OLAPStatus res = OLAP_SUCCESS;
     VLOG(3) << ""begin to add tablet to TabletManager. ""
             << ""tablet_id="" << tablet_id << "", schema_hash="" << schema_hash
             << "", force="" << force;
-    _tablet_map_lock.wrlock();
 
     TabletSharedPtr table_item;","[{'comment': '```suggestion\r\n    TabletSharedPtr tablet_item = nullptr;\r\n```', 'commenter': 'kangpinghuang'}, {'comment': 'done', 'commenter': 'yiguolei'}]"
932,be/src/olap/tablet_manager.cpp,"@@ -872,30 +882,86 @@ OLAPStatus TabletManager::start_trash_sweep() {
             tablet->delete_expired_inc_rowsets();
         }
     }
-    _tablet_map_lock.unlock();
+    auto it = _shutdown_tablets.begin();
+    for (; it != _shutdown_tablets.end();) { 
+        // check if the meta has the tablet info and its state is shutdown
+        if (it->use_count() > 1) {
+            // it means current tablet is referenced in other thread
+            ++it;
+            continue;
+        }
+        TabletMetaSharedPtr new_tablet_meta(new(nothrow) TabletMeta());
+        if (new_tablet_meta == nullptr) {
+            LOG(WARNING) << ""fail to malloc TabletMeta."";
+            ++it;
+            continue;
+        }
+        OLAPStatus check_st = TabletMetaManager::get_header((*it)->data_dir(), 
+            (*it)->tablet_id(), (*it)->schema_hash(), new_tablet_meta);
+        if (check_st == OLAP_SUCCESS) {
+            if (new_tablet_meta->tablet_state() != TABLET_SHUTDOWN) {
+                LOG(WARNING) << ""tablet's state changed to normal, skip remove dirs""
+                            << "" tablet id = "" << new_tablet_meta->tablet_id()
+                            << "" schema hash = "" << new_tablet_meta->schema_hash();
+                // remove it from list
+                it = _shutdown_tablets.erase(it);
+                continue;
+            }
+            if (check_dir_existed((*it)->tablet_path())) {
+                // take snapshot of tablet meta
+                std::string meta_file = (*it)->tablet_path() + ""/"" + std::to_string((*it)->tablet_id()) + "".hdr"";
+                (*it)->tablet_meta()->save(meta_file);
+                OLAPStatus rm_st = move_to_trash((*it)->tablet_path(), (*it)->tablet_path());","[{'comment': 'add INFO log before move to trash', 'commenter': 'kangpinghuang'}, {'comment': 'done', 'commenter': 'yiguolei'}]"
932,be/src/olap/task/engine_storage_migration_task.cpp,"@@ -122,7 +123,7 @@ OLAPStatus EngineStorageMigrationTask::_storage_medium_migrate(
         // migrate all index and data files but header file
         res = _copy_index_and_data_files(schema_hash_path, tablet, consistent_rowsets);
         if (res != OLAP_SUCCESS) {
-            LOG(WARNING) << ""fail to copy index and data files when migrate. res="" << res;
+            LOG(WARNING) << ""fail to copy index and data files when migrate. res="" <<  res;","[{'comment': 'additional space', 'commenter': 'kangpinghuang'}, {'comment': 'done', 'commenter': 'yiguolei'}]"
932,be/src/olap/tablet_manager.cpp,"@@ -263,11 +301,11 @@ OLAPStatus TabletManager::create_tablet(const TCreateTabletReq& request,
     return res;
 } // create_tablet
 
-
 TabletSharedPtr TabletManager::create_tablet(
         const TCreateTabletReq& request, const bool is_schema_change_tablet,
         const TabletSharedPtr ref_tablet, std::vector<DataDir*> data_dirs) {
-    WriteLock wrlock(&_create_tablet_lock);
+    DCHECK(is_schema_change_tablet && ref_tablet != nullptr);","[{'comment': 'DCHECK((is_schema_change_tablet && ref_tablet != nullptr) || (!is_schema_change_tablet && ref_tablet == nullptr));', 'commenter': 'chaoyli'}]"
932,be/src/olap/tablet_manager.cpp,"@@ -276,6 +314,14 @@ TabletSharedPtr TabletManager::_internal_create_tablet(
         const TCreateTabletReq& request, const bool is_schema_change_tablet,
         const TabletSharedPtr ref_tablet, std::vector<DataDir*> data_dirs) {
     DCHECK((is_schema_change_tablet && ref_tablet != nullptr) || (!is_schema_change_tablet && ref_tablet == nullptr));
+    // check if the tablet with specified tablet id and schema hash already exists","[{'comment': 'DCHECK is duplicated with create_tablet', 'commenter': 'chaoyli'}]"
932,be/src/olap/tablet_manager.cpp,"@@ -132,11 +134,44 @@ OLAPStatus TabletManager::add_tablet(TTabletId tablet_id, SchemaHash schema_hash
     bool keep_files = force ? true : false;
     if (force || (new_version > old_version
             || (new_version == old_version && new_time > old_time))) {
-        drop_tablet(tablet_id, schema_hash, keep_files);
-        _tablet_map_lock.wrlock();
+        // check if new tablet's meta is in store and add new tablet's meta to meta store
+        TabletMetaSharedPtr new_tablet_meta(new(nothrow) TabletMeta());
+        if (new_tablet_meta == nullptr) {
+            LOG(WARNING) << ""fail to malloc TabletMeta."";
+            return OLAP_ERR_MALLOC_ERROR;
+        }
+        OLAPStatus check_st = TabletMetaManager::get_header(tablet->data_dir(), 
+            tablet->tablet_id(), tablet->schema_hash(), new_tablet_meta);
+        if (check_st == OLAP_ERR_META_KEY_NOT_FOUND) {
+            res = TabletMetaManager::save(tablet->data_dir(), 
+                tablet->tablet_id(), tablet->schema_hash(), tablet->tablet_meta());
+            if (res != OLAP_SUCCESS) {
+                LOG(WARNING) << ""failed to save new tablet's meta to meta store"" 
+                             << "" tablet_id = "" << tablet_id
+                             << "" schema_hash = "" << schema_hash;
+                return res;
+            }
+        }
+        // if the new tablet is fresher than current one
+        // then delete current one and add new one
+        res = _drop_tablet_unlock(tablet_id, schema_hash, keep_files);
+        if (res != OLAP_SUCCESS) {
+            LOG(WARNING) << ""failed to drop old tablet when add new tablet""
+                         << "" tablet_id = "" << tablet_id
+                         << "" schema_hash = "" << schema_hash;
+            return res;
+        }
+        // Register tablet into StorageEngine, so that we can manage tablet from
+        // the perspective of root path.
+        // Example: unregister all tables when a bad disk found.
+        res = tablet->register_tablet_into_dir();","[{'comment': 'Why has you drop tablet, and register tablet into dir?', 'commenter': 'chaoyli'}]"
932,be/src/olap/tablet_manager.cpp,"@@ -132,11 +134,44 @@ OLAPStatus TabletManager::add_tablet(TTabletId tablet_id, SchemaHash schema_hash
     bool keep_files = force ? true : false;
     if (force || (new_version > old_version
             || (new_version == old_version && new_time > old_time))) {
-        drop_tablet(tablet_id, schema_hash, keep_files);
-        _tablet_map_lock.wrlock();
+        // check if new tablet's meta is in store and add new tablet's meta to meta store
+        TabletMetaSharedPtr new_tablet_meta(new(nothrow) TabletMeta());
+        if (new_tablet_meta == nullptr) {
+            LOG(WARNING) << ""fail to malloc TabletMeta."";
+            return OLAP_ERR_MALLOC_ERROR;
+        }
+        OLAPStatus check_st = TabletMetaManager::get_header(tablet->data_dir(), 
+            tablet->tablet_id(), tablet->schema_hash(), new_tablet_meta);
+        if (check_st == OLAP_ERR_META_KEY_NOT_FOUND) {","[{'comment': 'Why we need to save meta?', 'commenter': 'chaoyli'}]"
933,be/src/olap/rowset/rowset.h,"@@ -113,13 +113,23 @@ class Rowset : public std::enable_shared_from_this<Rowset> {
 
     virtual int64_t txn_id() const = 0;
     
+    // flag for push delete rowset
     virtual bool delete_flag() = 0;
 
     virtual bool check_path(const std::string& path) = 0;
 
+    bool need_delete_file() {
+        return _need_delete_file;
+    }
+
+    void set_need_delete_file(bool need_delete_file) {
+        _need_delete_file = need_delete_file;
+    }
+","[{'comment': 'add a check here. if it is set need deleted it could not be reverted.', 'commenter': 'yiguolei'}]"
933,be/src/olap/storage_engine.cpp,"@@ -677,17 +675,19 @@ OLAPStatus StorageEngine::_do_sweep(
 
 void StorageEngine::start_delete_unused_rowset() {
     _gc_mutex.lock();
-
-    auto it = _unused_rowsets.begin();
-    for (; it != _unused_rowsets.end();) { 
-        if (it->second->in_use()) {
+    LOG(INFO) << ""start to delete unused rowset."";","[{'comment': 'Log is non-sense when has no unused rowset.', 'commenter': 'chaoyli'}]"
933,be/src/olap/storage_engine.cpp,"@@ -677,17 +675,19 @@ OLAPStatus StorageEngine::_do_sweep(
 
 void StorageEngine::start_delete_unused_rowset() {
     _gc_mutex.lock();
-
-    auto it = _unused_rowsets.begin();
-    for (; it != _unused_rowsets.end();) { 
-        if (it->second->in_use()) {
+    LOG(INFO) << ""start to delete unused rowset."";
+    for (auto it = _unused_rowsets.begin(); it != _unused_rowsets.end();) {
+        if (it->second.use_count() != 1) {
             ++it;
-        } else {
-            it->second->remove();
-            _unused_rowsets.erase(it);
+        } else if (it->second->need_delete_file()){
+            LOG(INFO) << ""start to remove rowset:"" << it->second->rowset_id()
+                    << "", version:"" << it->second->version().first << ""-"" << it->second->version().second;
+            OLAPStatus status = it->second->remove();","[{'comment': 'if failed, you can not remove it from unused rowset.', 'commenter': 'chaoyli'}]"
933,be/src/olap/rowset/rowset.h,"@@ -113,13 +113,26 @@ class Rowset : public std::enable_shared_from_this<Rowset> {
 
     virtual int64_t txn_id() const = 0;
     
+    // flag for push delete rowset
     virtual bool delete_flag() = 0;
 
     virtual bool check_path(const std::string& path) = 0;
 
+    bool need_delete_file() {
+        return _need_delete_file;","[{'comment': 'if _need_delete_file = false and rowset shared_ptr use_count() == 1 will happen meanwhile?', 'commenter': 'chaoyli'}]"
934,fe/src/main/java/org/apache/doris/transaction/GlobalTransactionMgr.java,"@@ -360,39 +360,18 @@ public void commitTransaction(long dbId, long transactionId, List<TabletCommitIn
                 }
             }
         }
-        
+
+        // before state transform
+        transactionState.beforeStateTransform(TransactionStatus.COMMITTED);
+        // transaction state transform
         writeLock();
         try {
-            // transaction state is modified during check if the transaction could committed
-            if (transactionState.getTransactionStatus() != TransactionStatus.PREPARE) {
-                return;
-            }
-            // 4. update transaction state version
-            transactionState.setCommitTime(System.currentTimeMillis());
-            transactionState.setTransactionStatus(TransactionStatus.COMMITTED);
-            transactionState.setErrorReplicas(errorReplicaIds);
-            for (long tableId : tableToPartition.keySet()) {
-                TableCommitInfo tableCommitInfo = new TableCommitInfo(tableId);
-                for (long partitionId : tableToPartition.get(tableId)) {
-                    OlapTable table = (OlapTable) db.getTable(tableId);
-                    Partition partition = table.getPartition(partitionId);
-                    PartitionCommitInfo partitionCommitInfo = new PartitionCommitInfo(partitionId,
-                                                                                      partition.getNextVersion(),
-                                                                                      partition.getNextVersionHash());
-                    tableCommitInfo.addPartitionCommitInfo(partitionCommitInfo);
-                }
-                transactionState.putIdToTableCommitInfo(tableId, tableCommitInfo);
-            }
-            // 5. persistent transactionState
-            unprotectUpsertTransactionState(transactionState);
-            
-            // add publish version tasks. set task to null as a placeholder.
-            // tasks will be created when publishing version.
-            for (long backendId : totalInvolvedBackends) {
-                transactionState.addPublishVersionTask(backendId, null);
-            }
+            unprotectedCommitTransaction(transactionState, errorReplicaIds, tableToPartition, totalInvolvedBackends,
+                                         db);
         } finally {
             writeUnlock();
+            // after state transform
+            transactionState.afterStateTransform(TransactionStatus.COMMITTED);","[{'comment': 'If fail to commit, call `afterStateTransform` with COMMITTED is OK? ', 'commenter': 'imay'}, {'comment': 'I will add a boolean variable to judge correctly commit txn', 'commenter': 'EmmyMiao87'}]"
941,be/src/olap/tablet.cpp,"@@ -206,6 +206,24 @@ OLAPStatus Tablet::revise_tablet_meta(const TabletMeta& tablet_meta,
         _tablet_meta = new_tablet_meta;
     } while (0);
 
+    for (auto& version : versions_to_delete) {
+        auto it = _rs_version_map.find(version);
+        _rs_version_map.erase(it);","[{'comment': '_inc_rs_version_map need also erase it?', 'commenter': 'kangpinghuang'}, {'comment': 'I have missed to handle _inc_rs_version_map.', 'commenter': 'chaoyli'}]"
941,be/src/olap/tablet.cpp,"@@ -206,6 +206,24 @@ OLAPStatus Tablet::revise_tablet_meta(const TabletMeta& tablet_meta,
         _tablet_meta = new_tablet_meta;
     } while (0);
 
+    for (auto& version : versions_to_delete) {
+        auto it = _rs_version_map.find(version);
+        _rs_version_map.erase(it);
+    }
+
+    for (auto& rs_meta : rowsets_to_clone) {
+        Version version = { rs_meta->start_version(), rs_meta->end_version() };
+        RowsetSharedPtr rowset(new AlphaRowset(&_schema, _tablet_path, _data_dir, rs_meta));
+        _rs_version_map[version] = rowset;","[{'comment': 'I think rowset should be added to the map after init and you should return failure if init failed.', 'commenter': 'kangpinghuang'}, {'comment': 'OK', 'commenter': 'chaoyli'}]"
950,be/src/olap/tablet_manager.cpp,"@@ -151,17 +151,11 @@ OLAPStatus TabletManager::_add_tablet_unlock(TTabletId tablet_id, SchemaHash sch
 } // add_tablet
 
 OLAPStatus TabletManager::_add_tablet_to_map(TTabletId tablet_id, SchemaHash schema_hash,
-                                 const TabletSharedPtr& tablet, bool keep_files, bool drop_old) {
+                                 const TabletSharedPtr& tablet, bool update_meta, 
+                                 bool keep_files, bool drop_old) {","[{'comment': 'add a comment to explain the drop_old and update_meta', 'commenter': 'kangpinghuang'}]"
966,be/src/olap/push_handler.cpp,"@@ -117,7 +117,7 @@ OLAPStatus PushHandler::_do_streaming_ingestion(
       // if related tablet not exists, only push current tablet
       if (NULL == related_tablet.get()) {","[{'comment': 'related_tablet == nullptr', 'commenter': 'kangpinghuang'}]"
966,be/src/olap/schema_change.cpp,"@@ -1201,7 +1201,9 @@ OLAPStatus SchemaChangeHandler::process_alter_tablet(AlterTabletType type,
     base_tablet->obtain_push_lock();
     base_tablet->obtain_header_wrlock();
     new_tablet->obtain_header_wrlock();
-    _save_alter_state(ALTER_PREPARED, base_tablet, new_tablet);
+    // store schema change information into tablet header","[{'comment': '```suggestion\r\n    // store schema change information into tablet meta\r\n```', 'commenter': 'kangpinghuang'}]"
966,be/test/olap/tablet_mgr_test.cpp,"@@ -129,6 +130,10 @@ TEST_F(TabletMgrTest, CreateTablet) {
     // check dir exist
     bool dir_exist = check_dir_existed(tablet->tablet_path());
     ASSERT_TRUE(dir_exist);
+    // check meta has this tablet
+    TabletMetaSharedPtr new_tablet_meta(new TabletMeta());","[{'comment': '```suggestion\r\n    TabletMetaSharedPtr new_tablet_meta(new(std::nothrow) TabletMeta());\r\n```', 'commenter': 'kangpinghuang'}]"
967,docs/documentation/cn/administrator-guide/load-data/routine-load-manual.md,"@@ -0,0 +1,138 @@
+# 例行导入使用手册
+
+例行导入（Routine Load）功能为用户提供了一种自动从指定数据源进行数据导入的功能。本文档主要介绍该功能的实现原理、使用方式以及最佳实践。
+
+## 名词解释
+
+* FE：Frontend，Doris 的前端节点。负责元数据管理和请求接入。
+* BE：Backend，Doris 的后端节点。负责查询执行和数据存储。
+* RoutineLoadJob：用户提交的一个例行导入作业。
+* JobScheduler：例行导入作业调度器，用于调度和拆分一个 RoutineLoadJob 为多个 Task。
+* Task：RoutineLoadJob 被 JobScheduler 根据规则拆分的子任务。
+* TaskScheduler：任务调度器。用于调度 Task 的执行。
+
+## 原理
+
+```              
+         +---------+
+         |  Client |
+         +----+----+
+              |
++-----------------------------+
+| FE          |               |
+| +-----------v------------+  |
+| |                        |  |
+| |   Routine Load Job     |  |
+| |                        |  |
+| +---+--------+--------+--+  |
+|     |        |        |     |
+| +---v--+ +---v--+ +---v--+  |
+| | task | | task | | task |  |
+| +--+---+ +---+--+ +---+--+  |
+|    |         |        |     |
++-----------------------------+
+     |         |        |
+     v         v        v
+ +---+--+   +--+---+   ++-----+
+ |  BE  |   |  BE  |   |  BE  |
+ +------+   +------+   +------+
+
+```
+
+如上图，Client 向 FE 提交一个例行导入作业。FE 通过 JobScheduler 将一个导入作业拆分成若干个 Task。每个 Task 负责导入指定的一部分数据。Task 被 TaskScheduler 分配到指定的 BE 上执行。在 BE 上，一个 Task 被视为一个普通的导入任务，通过 Stream Load 的导入机制进行导入。导入完成后，向 FE 汇报。FE 中的 JobScheduler 根据汇报结果，继续生成后续新的 Task，或者对失败的 Task 进行重试。整个例行导入作业通过不断的产生新的 Task，来完成数据不间断的导入。
+
+## Kafka 例行导入
+
+当前我们仅支持从 Kafka 系统进行例行导入。该部分会详细介绍 Kafka 例行导入使用方式和最佳实践。
+
+### 使用限制
+
+1. 仅支持无认证的 Kafka 访问。
+2. 支持的消息格式为 csv 文本格式。每一个 message 为一行，且行尾**不包含**换行符。
+3. 仅支持 Kafka 0.10.0.0(含) 以上版本。
+
+### 创建例行导入任务
+
+创建例行导入任务的的详细语法可以参照 [这里]()。或者连接到 Doris 后，执行 `HELP ROUTINE LOAD;` 查看语法帮助。这里主要详细介绍，创建作业时的注意事项。
+
+* columns_mapping
+
+    columns_mapping 主要用于指定表结构和 message 中的列映射关系，以及一些列的转换。如果不指定，Doris 会默认 message 中的列和表结构的列按顺序一一对应。虽然在正常情况下，如果源数据正好一一对应，则不指定也可以进行正常的数据导入。但是我们依然强烈建议用户**显示的指定列映射关系**。这样当表结构发生变化（比如增加一个 nullable 的列），或者源文件发生变化（比如增加了一列）时，导入任务依然可以继续进行。否则，当发生上述变动后，因为列映射关系不再一一对应，导入将报错。
+    
+    在 columns_mapping 中我们同样可以使用一些内置函数进行列的转换。但需要注意函数对应的实际列类型。举例说明：
+    
+    假设用户需要导入只包含 `k1` 一列的表，列类型为 int。并且需要将源文件中的 null 值转换为 0。该功能可以通过 `ifnull` 函数实现。正确是的使用方式如下：
+    
+    `COLUMNS (xx, k1=ifnull(xx, ""3""))`
+    
+    注意这里我们使用 `""3""` 而不是 `3`。因为对于导入任务来说，源数据中的列类型都为 `varchar`，所以这里 `xx` 虚拟列的类型也为 `varchar`。所以我们需要使用 `""3""` 来进行对应的匹配，否则 `ifnull` 函数无法找到参数为 `(varchar, int)` 的函数签名，将出现错误。
+
+* desired\_concurrent\_number
+
+    desired_concurrent_number 用于指定一个例行作业期望的并发度。即一个作业，最多有多少 task 同时在执行。对于 Kafka 导入而言，当前的实际并发度计算如下：
+    
+    `Min(partition num / 3, desired_concurrent_number, alive_backend_num)`","[{'comment': '```suggestion\r\n    `Min(partition num / 3, desired_concurrent_number, alive_backend_num, DEFAULT_TASK_MAX_CONCURRENT_NUM)`\r\n    其中DEFAULT_TASK_MAX_CONCURRENT_NUM是系统的一个默认的最大并发数限制。\r\n```', 'commenter': 'EmmyMiao87'}]"
967,docs/documentation/cn/administrator-guide/load-data/routine-load-manual.md,"@@ -0,0 +1,138 @@
+# 例行导入使用手册
+
+例行导入（Routine Load）功能为用户提供了一种自动从指定数据源进行数据导入的功能。本文档主要介绍该功能的实现原理、使用方式以及最佳实践。
+
+## 名词解释
+
+* FE：Frontend，Doris 的前端节点。负责元数据管理和请求接入。
+* BE：Backend，Doris 的后端节点。负责查询执行和数据存储。
+* RoutineLoadJob：用户提交的一个例行导入作业。
+* JobScheduler：例行导入作业调度器，用于调度和拆分一个 RoutineLoadJob 为多个 Task。
+* Task：RoutineLoadJob 被 JobScheduler 根据规则拆分的子任务。
+* TaskScheduler：任务调度器。用于调度 Task 的执行。
+
+## 原理
+
+```              
+         +---------+
+         |  Client |
+         +----+----+
+              |
++-----------------------------+
+| FE          |               |
+| +-----------v------------+  |
+| |                        |  |
+| |   Routine Load Job     |  |
+| |                        |  |
+| +---+--------+--------+--+  |
+|     |        |        |     |
+| +---v--+ +---v--+ +---v--+  |
+| | task | | task | | task |  |
+| +--+---+ +---+--+ +---+--+  |
+|    |         |        |     |
++-----------------------------+
+     |         |        |
+     v         v        v
+ +---+--+   +--+---+   ++-----+
+ |  BE  |   |  BE  |   |  BE  |
+ +------+   +------+   +------+
+
+```
+
+如上图，Client 向 FE 提交一个例行导入作业。FE 通过 JobScheduler 将一个导入作业拆分成若干个 Task。每个 Task 负责导入指定的一部分数据。Task 被 TaskScheduler 分配到指定的 BE 上执行。在 BE 上，一个 Task 被视为一个普通的导入任务，通过 Stream Load 的导入机制进行导入。导入完成后，向 FE 汇报。FE 中的 JobScheduler 根据汇报结果，继续生成后续新的 Task，或者对失败的 Task 进行重试。整个例行导入作业通过不断的产生新的 Task，来完成数据不间断的导入。
+
+## Kafka 例行导入
+
+当前我们仅支持从 Kafka 系统进行例行导入。该部分会详细介绍 Kafka 例行导入使用方式和最佳实践。
+
+### 使用限制
+
+1. 仅支持无认证的 Kafka 访问。
+2. 支持的消息格式为 csv 文本格式。每一个 message 为一行，且行尾**不包含**换行符。
+3. 仅支持 Kafka 0.10.0.0(含) 以上版本。
+
+### 创建例行导入任务
+
+创建例行导入任务的的详细语法可以参照 [这里]()。或者连接到 Doris 后，执行 `HELP ROUTINE LOAD;` 查看语法帮助。这里主要详细介绍，创建作业时的注意事项。
+
+* columns_mapping
+
+    columns_mapping 主要用于指定表结构和 message 中的列映射关系，以及一些列的转换。如果不指定，Doris 会默认 message 中的列和表结构的列按顺序一一对应。虽然在正常情况下，如果源数据正好一一对应，则不指定也可以进行正常的数据导入。但是我们依然强烈建议用户**显示的指定列映射关系**。这样当表结构发生变化（比如增加一个 nullable 的列），或者源文件发生变化（比如增加了一列）时，导入任务依然可以继续进行。否则，当发生上述变动后，因为列映射关系不再一一对应，导入将报错。
+    
+    在 columns_mapping 中我们同样可以使用一些内置函数进行列的转换。但需要注意函数对应的实际列类型。举例说明：
+    
+    假设用户需要导入只包含 `k1` 一列的表，列类型为 int。并且需要将源文件中的 null 值转换为 0。该功能可以通过 `ifnull` 函数实现。正确是的使用方式如下：
+    
+    `COLUMNS (xx, k1=ifnull(xx, ""3""))`
+    
+    注意这里我们使用 `""3""` 而不是 `3`。因为对于导入任务来说，源数据中的列类型都为 `varchar`，所以这里 `xx` 虚拟列的类型也为 `varchar`。所以我们需要使用 `""3""` 来进行对应的匹配，否则 `ifnull` 函数无法找到参数为 `(varchar, int)` 的函数签名，将出现错误。
+
+* desired\_concurrent\_number
+
+    desired_concurrent_number 用于指定一个例行作业期望的并发度。即一个作业，最多有多少 task 同时在执行。对于 Kafka 导入而言，当前的实际并发度计算如下：
+    
+    `Min(partition num / 3, desired_concurrent_number, alive_backend_num)`
+
+    其中 partition 值订阅的 Kafka topic 的 partition数量。`alive_backend_num` 是当前正常的 BE 节点数。
+    ","[{'comment': 'Maybe add a topic of `高级调优方式` is better.', 'commenter': 'EmmyMiao87'}]"
967,docs/documentation/cn/administrator-guide/load-data/routine-load-manual.md,"@@ -0,0 +1,138 @@
+# 例行导入使用手册
+
+例行导入（Routine Load）功能为用户提供了一种自动从指定数据源进行数据导入的功能。本文档主要介绍该功能的实现原理、使用方式以及最佳实践。
+
+## 名词解释
+
+* FE：Frontend，Doris 的前端节点。负责元数据管理和请求接入。
+* BE：Backend，Doris 的后端节点。负责查询执行和数据存储。
+* RoutineLoadJob：用户提交的一个例行导入作业。
+* JobScheduler：例行导入作业调度器，用于调度和拆分一个 RoutineLoadJob 为多个 Task。
+* Task：RoutineLoadJob 被 JobScheduler 根据规则拆分的子任务。
+* TaskScheduler：任务调度器。用于调度 Task 的执行。
+
+## 原理
+
+```              
+         +---------+
+         |  Client |
+         +----+----+
+              |
++-----------------------------+
+| FE          |               |
+| +-----------v------------+  |
+| |                        |  |
+| |   Routine Load Job     |  |
+| |                        |  |
+| +---+--------+--------+--+  |
+|     |        |        |     |
+| +---v--+ +---v--+ +---v--+  |
+| | task | | task | | task |  |
+| +--+---+ +---+--+ +---+--+  |
+|    |         |        |     |
++-----------------------------+
+     |         |        |
+     v         v        v
+ +---+--+   +--+---+   ++-----+
+ |  BE  |   |  BE  |   |  BE  |
+ +------+   +------+   +------+
+
+```
+
+如上图，Client 向 FE 提交一个例行导入作业。FE 通过 JobScheduler 将一个导入作业拆分成若干个 Task。每个 Task 负责导入指定的一部分数据。Task 被 TaskScheduler 分配到指定的 BE 上执行。在 BE 上，一个 Task 被视为一个普通的导入任务，通过 Stream Load 的导入机制进行导入。导入完成后，向 FE 汇报。FE 中的 JobScheduler 根据汇报结果，继续生成后续新的 Task，或者对失败的 Task 进行重试。整个例行导入作业通过不断的产生新的 Task，来完成数据不间断的导入。
+
+## Kafka 例行导入
+
+当前我们仅支持从 Kafka 系统进行例行导入。该部分会详细介绍 Kafka 例行导入使用方式和最佳实践。
+
+### 使用限制
+
+1. 仅支持无认证的 Kafka 访问。
+2. 支持的消息格式为 csv 文本格式。每一个 message 为一行，且行尾**不包含**换行符。
+3. 仅支持 Kafka 0.10.0.0(含) 以上版本。
+
+### 创建例行导入任务
+
+创建例行导入任务的的详细语法可以参照 [这里]()。或者连接到 Doris 后，执行 `HELP ROUTINE LOAD;` 查看语法帮助。这里主要详细介绍，创建作业时的注意事项。
+
+* columns_mapping
+
+    columns_mapping 主要用于指定表结构和 message 中的列映射关系，以及一些列的转换。如果不指定，Doris 会默认 message 中的列和表结构的列按顺序一一对应。虽然在正常情况下，如果源数据正好一一对应，则不指定也可以进行正常的数据导入。但是我们依然强烈建议用户**显示的指定列映射关系**。这样当表结构发生变化（比如增加一个 nullable 的列），或者源文件发生变化（比如增加了一列）时，导入任务依然可以继续进行。否则，当发生上述变动后，因为列映射关系不再一一对应，导入将报错。
+    
+    在 columns_mapping 中我们同样可以使用一些内置函数进行列的转换。但需要注意函数对应的实际列类型。举例说明：
+    
+    假设用户需要导入只包含 `k1` 一列的表，列类型为 int。并且需要将源文件中的 null 值转换为 0。该功能可以通过 `ifnull` 函数实现。正确是的使用方式如下：
+    
+    `COLUMNS (xx, k1=ifnull(xx, ""3""))`
+    
+    注意这里我们使用 `""3""` 而不是 `3`。因为对于导入任务来说，源数据中的列类型都为 `varchar`，所以这里 `xx` 虚拟列的类型也为 `varchar`。所以我们需要使用 `""3""` 来进行对应的匹配，否则 `ifnull` 函数无法找到参数为 `(varchar, int)` 的函数签名，将出现错误。
+
+* desired\_concurrent\_number
+
+    desired_concurrent_number 用于指定一个例行作业期望的并发度。即一个作业，最多有多少 task 同时在执行。对于 Kafka 导入而言，当前的实际并发度计算如下：
+    
+    `Min(partition num / 3, desired_concurrent_number, alive_backend_num)`
+
+    其中 partition 值订阅的 Kafka topic 的 partition数量。`alive_backend_num` 是当前正常的 BE 节点数。
+    
+* max\_batch\_interval/max\_batch\_rows/max\_batch\_size
+
+    这三个参数用于控制单个任务的执行时间。其中任意一个阈值达到，则任务结束。其中 `max_batch_rows` 用于记录从 Kafka 中读取到的数据行数。`max_batch_size` 用于记录从 Kafka 中读取到的数据量，单位是字节。目前一个任务的消费速率大约为 5-10MB/s。
+    
+    那么假设一行数据 500B，用户希望每 100MB 或 10 秒为一个 task。100MB 的预期处理时间是 10-20 秒，对应的行数约为 200000 行。则一个合理的配置为：
+    
+    ```
+    ""max_batch_interval"" = ""10"",
+    ""max_batch_rows"" = ""200000"",
+    ""max_batch_size"" = ""104857600""
+    ```
+    
+    以上示例中的参数也是这些配置的默认参数。
+    
+* max\_error\_number
+
+    `max_error_number` 用于控制错误率。在错误率过高的时候，作业会自动暂停。因为整个作业是面向数据流的，因为数据流的无边界性，我们无法像其他导入任务一样，通过一个错误比例来计算错误率。因此这里提供了一种新的计算方式，来计算数据流中的错误比例。
+    
+    我们设定了一个采样窗口。窗口的大小为 `max_batch_rows * 10`。在一个采样窗口内，如果错误行数超过 `max_error_number`，则作业被暂定。如果没有超过，则下一个窗口重新开始计算错误行数。
+    
+    我们假设 `max_error_number` 为 200000，则窗口大小为 2000000。设 `max_error_number` 为 20000，即用户预期每 2000000 行的错误行为 20000。即错误率为 1%。但是因为不是每批次任务正好消费 200000 行，所以窗口的实际范围是 [2000000, 2200000]，即有 10% 的统计误差。
+    
+    错误行不包括通过 where 条件过滤掉的行。但是包括没有对应的 Doris 表中的分区的行。
+    
+* data\_source\_properties
+
+    `data_source_properties` 中可以指定消费具体的 Kakfa partition。如果不指定，则默认消费所订阅的 topic 的所有 partition。
+    
+    注意，当显示的指定了 partition，则导入作业不会再动态的检测 Kafka partition 的变化。如果没有指定，则会根据 kafka partition 的变化，动态消费 partition。
+
+### 查看导入作业状态
+
+查看作业状态的具体命令和示例可以通过 `help show routine load;` 命令查看。
+
+查看任务运行状态的具体命令和示例可以通过 `help show routine load task;` 命令查看。
+
+只能查看当前正在运行中的任务，已结束和为开始的任务无法查看。","[{'comment': '```suggestion\r\n只能查看当前正在运行中的任务，已结束和未开始的任务无法查看。\r\n```', 'commenter': 'EmmyMiao87'}]"
967,docs/documentation/cn/administrator-guide/load-data/routine-load-manual.md,"@@ -0,0 +1,138 @@
+# 例行导入使用手册
+
+例行导入（Routine Load）功能为用户提供了一种自动从指定数据源进行数据导入的功能。本文档主要介绍该功能的实现原理、使用方式以及最佳实践。
+
+## 名词解释
+
+* FE：Frontend，Doris 的前端节点。负责元数据管理和请求接入。
+* BE：Backend，Doris 的后端节点。负责查询执行和数据存储。
+* RoutineLoadJob：用户提交的一个例行导入作业。
+* JobScheduler：例行导入作业调度器，用于调度和拆分一个 RoutineLoadJob 为多个 Task。
+* Task：RoutineLoadJob 被 JobScheduler 根据规则拆分的子任务。
+* TaskScheduler：任务调度器。用于调度 Task 的执行。
+
+## 原理
+
+```              
+         +---------+
+         |  Client |
+         +----+----+
+              |
++-----------------------------+
+| FE          |               |
+| +-----------v------------+  |
+| |                        |  |
+| |   Routine Load Job     |  |
+| |                        |  |
+| +---+--------+--------+--+  |
+|     |        |        |     |
+| +---v--+ +---v--+ +---v--+  |
+| | task | | task | | task |  |
+| +--+---+ +---+--+ +---+--+  |
+|    |         |        |     |
++-----------------------------+
+     |         |        |
+     v         v        v
+ +---+--+   +--+---+   ++-----+
+ |  BE  |   |  BE  |   |  BE  |
+ +------+   +------+   +------+
+
+```
+
+如上图，Client 向 FE 提交一个例行导入作业。FE 通过 JobScheduler 将一个导入作业拆分成若干个 Task。每个 Task 负责导入指定的一部分数据。Task 被 TaskScheduler 分配到指定的 BE 上执行。在 BE 上，一个 Task 被视为一个普通的导入任务，通过 Stream Load 的导入机制进行导入。导入完成后，向 FE 汇报。FE 中的 JobScheduler 根据汇报结果，继续生成后续新的 Task，或者对失败的 Task 进行重试。整个例行导入作业通过不断的产生新的 Task，来完成数据不间断的导入。
+
+## Kafka 例行导入
+
+当前我们仅支持从 Kafka 系统进行例行导入。该部分会详细介绍 Kafka 例行导入使用方式和最佳实践。
+
+### 使用限制
+
+1. 仅支持无认证的 Kafka 访问。
+2. 支持的消息格式为 csv 文本格式。每一个 message 为一行，且行尾**不包含**换行符。
+3. 仅支持 Kafka 0.10.0.0(含) 以上版本。
+
+### 创建例行导入任务
+
+创建例行导入任务的的详细语法可以参照 [这里]()。或者连接到 Doris 后，执行 `HELP ROUTINE LOAD;` 查看语法帮助。这里主要详细介绍，创建作业时的注意事项。
+
+* columns_mapping
+
+    columns_mapping 主要用于指定表结构和 message 中的列映射关系，以及一些列的转换。如果不指定，Doris 会默认 message 中的列和表结构的列按顺序一一对应。虽然在正常情况下，如果源数据正好一一对应，则不指定也可以进行正常的数据导入。但是我们依然强烈建议用户**显示的指定列映射关系**。这样当表结构发生变化（比如增加一个 nullable 的列），或者源文件发生变化（比如增加了一列）时，导入任务依然可以继续进行。否则，当发生上述变动后，因为列映射关系不再一一对应，导入将报错。
+    
+    在 columns_mapping 中我们同样可以使用一些内置函数进行列的转换。但需要注意函数对应的实际列类型。举例说明：
+    
+    假设用户需要导入只包含 `k1` 一列的表，列类型为 int。并且需要将源文件中的 null 值转换为 0。该功能可以通过 `ifnull` 函数实现。正确是的使用方式如下：
+    
+    `COLUMNS (xx, k1=ifnull(xx, ""3""))`
+    
+    注意这里我们使用 `""3""` 而不是 `3`。因为对于导入任务来说，源数据中的列类型都为 `varchar`，所以这里 `xx` 虚拟列的类型也为 `varchar`。所以我们需要使用 `""3""` 来进行对应的匹配，否则 `ifnull` 函数无法找到参数为 `(varchar, int)` 的函数签名，将出现错误。
+
+* desired\_concurrent\_number
+
+    desired_concurrent_number 用于指定一个例行作业期望的并发度。即一个作业，最多有多少 task 同时在执行。对于 Kafka 导入而言，当前的实际并发度计算如下：
+    
+    `Min(partition num / 3, desired_concurrent_number, alive_backend_num)`
+
+    其中 partition 值订阅的 Kafka topic 的 partition数量。`alive_backend_num` 是当前正常的 BE 节点数。
+    
+* max\_batch\_interval/max\_batch\_rows/max\_batch\_size
+
+    这三个参数用于控制单个任务的执行时间。其中任意一个阈值达到，则任务结束。其中 `max_batch_rows` 用于记录从 Kafka 中读取到的数据行数。`max_batch_size` 用于记录从 Kafka 中读取到的数据量，单位是字节。目前一个任务的消费速率大约为 5-10MB/s。
+    
+    那么假设一行数据 500B，用户希望每 100MB 或 10 秒为一个 task。100MB 的预期处理时间是 10-20 秒，对应的行数约为 200000 行。则一个合理的配置为：
+    
+    ```
+    ""max_batch_interval"" = ""10"",
+    ""max_batch_rows"" = ""200000"",
+    ""max_batch_size"" = ""104857600""
+    ```
+    
+    以上示例中的参数也是这些配置的默认参数。
+    
+* max\_error\_number
+
+    `max_error_number` 用于控制错误率。在错误率过高的时候，作业会自动暂停。因为整个作业是面向数据流的，因为数据流的无边界性，我们无法像其他导入任务一样，通过一个错误比例来计算错误率。因此这里提供了一种新的计算方式，来计算数据流中的错误比例。
+    
+    我们设定了一个采样窗口。窗口的大小为 `max_batch_rows * 10`。在一个采样窗口内，如果错误行数超过 `max_error_number`，则作业被暂定。如果没有超过，则下一个窗口重新开始计算错误行数。
+    
+    我们假设 `max_error_number` 为 200000，则窗口大小为 2000000。设 `max_error_number` 为 20000，即用户预期每 2000000 行的错误行为 20000。即错误率为 1%。但是因为不是每批次任务正好消费 200000 行，所以窗口的实际范围是 [2000000, 2200000]，即有 10% 的统计误差。
+    
+    错误行不包括通过 where 条件过滤掉的行。但是包括没有对应的 Doris 表中的分区的行。
+    
+* data\_source\_properties
+
+    `data_source_properties` 中可以指定消费具体的 Kakfa partition。如果不指定，则默认消费所订阅的 topic 的所有 partition。
+    
+    注意，当显示的指定了 partition，则导入作业不会再动态的检测 Kafka partition 的变化。如果没有指定，则会根据 kafka partition 的变化，动态消费 partition。
+
+### 查看导入作业状态
+
+查看作业状态的具体命令和示例可以通过 `help show routine load;` 命令查看。
+
+查看任务运行状态的具体命令和示例可以通过 `help show routine load task;` 命令查看。
+
+只能查看当前正在运行中的任务，已结束和为开始的任务无法查看。
+
+### 作业控制
+    
+用户可以通过 `STOP/PAUSE/RESUME` 三个命令来控制作业的停止，暂停和重启。可以通过 `help stop routine load;`, `help pause routine load;` 以及 `help resume routine load;` 三个命令查看帮助和示例。
+
+## 其他说明
+
+1. 例行导入作业和 ALTER TABLE 操作的关系
+
+    * 例行导入不会阻塞 SCHEMA CHANGE 和 ROLLUP 操作。但是注意如果 SCHEMA CHANGE 完成后，列映射关系无法匹配，则会导致例行作业暂定。建议通过在例行导入作业中显式指定列映射关系，以及通过增加 Nullable 列或带 Default 值的列来减少这类问题。","[{'comment': '```suggestion\r\n    * 例行导入不会阻塞 SCHEMA CHANGE 和 ROLLUP 操作。但是注意如果 SCHEMA CHANGE 完成后，列映射关系无法匹配，则会导致作业的错误数据激增，最终导致作业暂定。建议通过在例行导入作业中显式指定列映射关系，以及通过增加 Nullable 列或带 Default 值的列来减少这类问题。\r\n```', 'commenter': 'EmmyMiao87'}]"
973,be/src/agent/task_worker_pool.cpp,"@@ -481,15 +481,23 @@ void* TaskWorkerPool::_drop_tablet_worker_thread_callback(void* arg_this) {
         vector<string> error_msgs;
         TStatus task_status;
         AgentStatus status = DORIS_SUCCESS;
-        OLAPStatus drop_status = StorageEngine::instance()->tablet_manager()->drop_tablet(drop_tablet_req.tablet_id, drop_tablet_req.schema_hash);
-        if (drop_status != OLAP_SUCCESS && drop_status != OLAP_ERR_TABLE_NOT_FOUND) {
-            status = DORIS_ERROR;
-        }
-        if (status != DORIS_SUCCESS) {
-            OLAP_LOG_WARNING(
-                ""drop table failed! signature: %ld"", agent_task_req.signature);
-            error_msgs.push_back(""drop table failed!"");
-            status_code = TStatusCode::RUNTIME_ERROR;
+        TabletSharedPtr dropped_tablet = StorageEngine::instance()->tablet_manager()->get_tablet(
+            drop_tablet_req.tablet_id, drop_tablet_req.schema_hash);
+        if (dropped_tablet != nullptr) {
+            OLAPStatus drop_status = StorageEngine::instance()->tablet_manager()->drop_tablet(
+                drop_tablet_req.tablet_id, drop_tablet_req.schema_hash);
+            if (drop_status != OLAP_SUCCESS && drop_status != OLAP_ERR_TABLE_NOT_FOUND) {
+                status = DORIS_ERROR;
+            }
+            if (status != DORIS_SUCCESS) {
+                OLAP_LOG_WARNING(","[{'comment': 'LOG(WARNING)', 'commenter': 'kangpinghuang'}]"
973,be/src/olap/txn_manager.cpp,"@@ -359,6 +359,33 @@ void TxnManager::get_tablet_related_txns(TabletSharedPtr tablet, int64_t* partit
     }
 }
 
+// force drop all txns related with the tablet
+// maybe lock error, because not get txn lock before remove from meta
+void TxnManager::force_rollback_tablet_related_txns(OlapMeta* meta, TTabletId tablet_id, SchemaHash schema_hash) {
+    TabletInfo tablet_info(tablet_id, schema_hash);
+    WriteLock txn_wrlock(&_txn_map_lock);
+    for (auto& it : _txn_tablet_map) {
+        auto load_itr = it.second.find(tablet_info);
+        if (load_itr != it.second.end()) {
+            TabletTxnInfo& load_info = load_itr->second;
+            if (load_info.rowset != nullptr && meta != nullptr) {","[{'comment': 'move meta != nullptr to the line 365', 'commenter': 'kangpinghuang'}]"
974,docs/documentation/cn/administrator-guide/load-data/routine-load-manual.md,"@@ -53,28 +63,29 @@
 
 ### 创建例行导入任务
 
-创建例行导入任务的的详细语法可以参照 [这里]()。或者连接到 Doris 后，执行 `HELP ROUTINE LOAD;` 查看语法帮助。这里主要详细介绍，创建作业时的注意事项。
+创建例行导入任务的的详细语法可以参照 [这里]()。或者连接到 Doris 后，执行 `HELP CREATE ROUTINE LOAD;` 查看语法帮助。这里主要详细介绍，创建作业时的注意事项。
 
 * columns_mapping
 
-    columns_mapping 主要用于指定表结构和 message 中的列映射关系，以及一些列的转换。如果不指定，Doris 会默认 message 中的列和表结构的列按顺序一一对应。虽然在正常情况下，如果源数据正好一一对应，则不指定也可以进行正常的数据导入。但是我们依然强烈建议用户**显示的指定列映射关系**。这样当表结构发生变化（比如增加一个 nullable 的列），或者源文件发生变化（比如增加了一列）时，导入任务依然可以继续进行。否则，当发生上述变动后，因为列映射关系不再一一对应，导入将报错。
+    `columns_mapping` 主要用于指定表结构和 message 中的列映射关系，以及一些列的转换。如果不指定，Doris 会默认 message 中的列和表结构的列按顺序一一对应。虽然在正常情况下，如果源数据正好一一对应，则不指定也可以进行正常的数据导入。但是我们依然强烈建议用户**显式的指定列映射关系**。这样当表结构发生变化（比如增加一个 nullable 的列），或者源文件发生变化（比如增加了一列）时，导入任务依然可以继续进行。否则，当发生上述变动后，因为列映射关系不再一一对应，导入将报错。
     
-    在 columns_mapping 中我们同样可以使用一些内置函数进行列的转换。但需要注意函数对应的实际列类型。举例说明：
+    在 `columns_mapping` 中我们同样可以使用一些内置函数进行列的转换。但需要注意函数参数对应的实际列类型。举例说明：
     
-    假设用户需要导入只包含 `k1` 一列的表，列类型为 int。并且需要将源文件中的 null 值转换为 0。该功能可以通过 `ifnull` 函数实现。正确是的使用方式如下：
+    假设用户需要导入只包含 `k1` 一列的表，列类型为 `int`。并且需要将源文件中的 null 值转换为 0。该功能可以通过 `ifnull` 函数实现。正确是的使用方式如下：
     
     `COLUMNS (xx, k1=ifnull(xx, ""3""))`
     
-    注意这里我们使用 `""3""` 而不是 `3`。因为对于导入任务来说，源数据中的列类型都为 `varchar`，所以这里 `xx` 虚拟列的类型也为 `varchar`。所以我们需要使用 `""3""` 来进行对应的匹配，否则 `ifnull` 函数无法找到参数为 `(varchar, int)` 的函数签名，将出现错误。
+    注意这里我们使用 `""3""` 而不是 `3`，虽然 `k1` 的类型为 `int`。因为对于导入任务来说，源数据中的列类型都为 `varchar`，所以这里 `xx` 虚拟列的类型也为 `varchar`。所以我们需要使用 `""3""` 来进行对应的匹配，否则 `ifnull` 函数无法找到参数为 `(varchar, int)` 的函数签名，将出现错误。
 
 * desired\_concurrent\_number
 
-    desired_concurrent_number 用于指定一个例行作业期望的并发度。即一个作业，最多有多少 task 同时在执行。对于 Kafka 导入而言，当前的实际并发度计算如下：
+    `desired_concurrent_number` 用于指定一个例行作业期望的并发度。即一个作业，最多有多少 task 同时在执行。对于 Kafka 导入而言，当前的实际并发度计算如下：
+    
+    `Min(partition num / 3, desired_concurrent_number, alive_backend_num, Config.max_routine_load_task_concurrrent_num)`
     
-    `Min(partition num / 3, desired_concurrent_number, alive_backend_num, DEFAULT_TASK_MAX_CONCURRENT_NUM)`
-    其中DEFAULT_TASK_MAX_CONCURRENT_NUM是系统的一个默认的最大并发数限制。
+    其中 `Config.max_routine_load_task_concurrrent_num` 是系统的一个默认的最大并发数限制。这是一个 FE 参数，可以动态调整。默认为 5。","[{'comment': '```suggestion\r\n    其中 `Config.max_routine_load_task_concurrrent_num` 是系统的一个默认的最大并发数限制。这是一个 FE 配置，可以通过改配置调整。默认为 5。\r\n```', 'commenter': 'EmmyMiao87'}]"
974,docs/documentation/cn/administrator-guide/load-data/routine-load-manual.md,"@@ -94,41 +105,41 @@
 
     `max_error_number` 用于控制错误率。在错误率过高的时候，作业会自动暂停。因为整个作业是面向数据流的，因为数据流的无边界性，我们无法像其他导入任务一样，通过一个错误比例来计算错误率。因此这里提供了一种新的计算方式，来计算数据流中的错误比例。
     
-    我们设定了一个采样窗口。窗口的大小为 `max_batch_rows * 10`。在一个采样窗口内，如果错误行数超过 `max_error_number`，则作业被暂定。如果没有超过，则下一个窗口重新开始计算错误行数。
+    我们设定了一个采样窗口。窗口的大小为 `max_batch_rows * 10`。在一个采样窗口内，如果错误行数超过 `max_error_number`，则作业被暂停。如果没有超过，则下一个窗口重新开始计算错误行数。
     
-    我们假设 `max_error_number` 为 200000，则窗口大小为 2000000。设 `max_error_number` 为 20000，即用户预期每 2000000 行的错误行为 20000。即错误率为 1%。但是因为不是每批次任务正好消费 200000 行，所以窗口的实际范围是 [2000000, 2200000]，即有 10% 的统计误差。
+    我们假设 `max_batch_rows` 为 200000，则窗口大小为 2000000。设 `max_error_number` 为 20000，即用户预期每 2000000 行的错误行为 20000。即错误率为 1%。但是因为不是每批次任务正好消费 200000 行，所以窗口的实际范围是 [2000000, 2200000]，即有 10% 的统计误差。
     
     错误行不包括通过 where 条件过滤掉的行。但是包括没有对应的 Doris 表中的分区的行。
     
 * data\_source\_properties
 
     `data_source_properties` 中可以指定消费具体的 Kakfa partition。如果不指定，则默认消费所订阅的 topic 的所有 partition。
     
-    注意，当显示的指定了 partition，则导入作业不会再动态的检测 Kafka partition 的变化。如果没有指定，则会根据 kafka partition 的变化，动态消费 partition。
+    注意，当显式的指定了 partition，则导入作业不会再动态的检测 Kafka partition 的变化。如果没有指定，则会根据 kafka partition 的变化，动态调整需要消费 partition。","[{'comment': '```suggestion\r\n    注意，当显式的指定了 partition，则导入作业不会再动态的检测 Kafka partition 的变化。如果没有指定，则会根据 kafka partition 的变化，动态调整需要消费的 partition。\r\n```', 'commenter': 'EmmyMiao87'}]"
974,fe/src/main/java/org/apache/doris/analysis/CreateRoutineLoadStmt.java,"@@ -279,7 +280,7 @@ private void checkJobProperties() throws AnalysisException {
         }
 
         desiredConcurrentNum = ((Long) Util.getLongPropertyOrDefault(jobProperties.get(DESIRED_CONCURRENT_NUMBER_PROPERTY),
-                RoutineLoadJob.DEFAULT_TASK_MAX_CONCURRENT_NUM, DESIRED_CONCURRENT_NUMBER_PRED,
+                Config.max_routine_load_task_concurrent_num, DESIRED_CONCURRENT_NUMBER_PRED,","[{'comment': 'Maybe max_concurrent_num_of_per_rl_task is better? ', 'commenter': 'EmmyMiao87'}]"
974,fe/src/main/java/org/apache/doris/planner/PartitionColumnFilter.java,"@@ -22,12 +22,13 @@
 import org.apache.doris.catalog.Column;
 import org.apache.doris.catalog.PartitionKey;
 import org.apache.doris.common.AnalysisException;
+
 import com.google.common.collect.BoundType;
 import com.google.common.collect.Lists;
 import com.google.common.collect.Range;
 
-import org.apache.logging.log4j.Logger;
 import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;","[{'comment': 'Move Logger in front of LogManager is better', 'commenter': 'EmmyMiao87'}]"
1003,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoadScheduler.java,"@@ -30,58 +35,96 @@
 public class RoutineLoadScheduler extends Daemon {
 
     private static final Logger LOG = LogManager.getLogger(RoutineLoadScheduler.class);
+    private static final int DEFAULT_INTERVAL_SECONDS = 10;
+
+    private RoutineLoadManager routineLoadManager;
 
-    private RoutineLoadManager routineLoadManager = Catalog.getInstance().getRoutineLoadManager();
+    @VisibleForTesting
+    public RoutineLoadScheduler() {
+        super();
+        routineLoadManager = Catalog.getInstance().getRoutineLoadManager();
+    }
+
+    public RoutineLoadScheduler(RoutineLoadManager routineLoadManager) {
+        super(""Routine load"", DEFAULT_INTERVAL_SECONDS * 1000);
+        this.routineLoadManager = routineLoadManager;
+    }
 
     @Override
     protected void runOneCycle() {
         try {
             process();
         } catch (Throwable e) {
-            LOG.error(""failed to schedule jobs with error massage {}"", e.getMessage(), e);
+            LOG.warn(""Failed to process one round of RoutineLoadScheduler with error message {}"", e.getMessage(), e);","[{'comment': '{} placeholder is only one, but appear to params.\r\n\r\ne.getMeesage() maybe NULL, will throws NullPointerExcpetion again. This RoutineLoadScheduler will never schedule as expected.', 'commenter': 'wuyunfeng'}, {'comment': 'It will not throw `NullPointerExcpetion `. It will print `with error message null`', 'commenter': 'EmmyMiao87'}, {'comment': 'You are right. I will fix it.', 'commenter': 'EmmyMiao87'}]"
1003,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoadScheduler.java,"@@ -30,58 +35,96 @@
 public class RoutineLoadScheduler extends Daemon {
 
     private static final Logger LOG = LogManager.getLogger(RoutineLoadScheduler.class);
+    private static final int DEFAULT_INTERVAL_SECONDS = 10;
+
+    private RoutineLoadManager routineLoadManager;
 
-    private RoutineLoadManager routineLoadManager = Catalog.getInstance().getRoutineLoadManager();
+    @VisibleForTesting
+    public RoutineLoadScheduler() {
+        super();
+        routineLoadManager = Catalog.getInstance().getRoutineLoadManager();
+    }
+
+    public RoutineLoadScheduler(RoutineLoadManager routineLoadManager) {
+        super(""Routine load"", DEFAULT_INTERVAL_SECONDS * 1000);
+        this.routineLoadManager = routineLoadManager;
+    }
 
     @Override
     protected void runOneCycle() {
         try {
             process();
         } catch (Throwable e) {
-            LOG.error(""failed to schedule jobs with error massage {}"", e.getMessage(), e);
+            LOG.warn(""Failed to process one round of RoutineLoadScheduler with error message {}"", e.getMessage(), e);
         }
     }
 
-    private void process() {
+    private void process() throws UserException {
         // update
-        routineLoadManager.rescheduleRoutineLoadJob();
+        routineLoadManager.updateRoutineLoadJob();
         // get need schedule routine jobs
         List<RoutineLoadJob> routineLoadJobList = null;
         try {
             routineLoadJobList = getNeedScheduleRoutineJobs();
         } catch (LoadException e) {
-            LOG.error(""failed to get need schedule routine jobs"");
+            LOG.warn(""failed to get need schedule routine jobs"", e);","[{'comment': 'no placeholder {} ? may be this line shoud replaced with Failed to get needed routine schedule jobs.', 'commenter': 'wuyunfeng'}, {'comment': 'I use the function warn(String, Throwable). It can print warn log and throwable at the same time', 'commenter': 'EmmyMiao87'}]"
1003,fe/src/main/java/org/apache/doris/transaction/PublishVersionDaemon.java,"@@ -57,7 +58,7 @@ protected void runOneCycle() {
         }
     }
     
-    private void publishVersion() {
+    private void publishVersion() throws UserException {","[{'comment': 'UserException ......', 'commenter': 'wuyunfeng'}]"
1003,fe/src/main/java/org/apache/doris/qe/ShowExecutor.java,"@@ -793,37 +800,98 @@ private void handleShowLoadWarningsFromURL(ShowLoadWarningsStmt showWarningsStmt
 
     private void handleShowRoutineLoad() throws AnalysisException {
         ShowRoutineLoadStmt showRoutineLoadStmt = (ShowRoutineLoadStmt) stmt;
+        List<List<String>> rows = Lists.newArrayList();
+        // if job exists
+        List<RoutineLoadJob> routineLoadJobList;
+        try {
+            routineLoadJobList =
+                    Catalog.getCurrentCatalog().getRoutineLoadManager().getJob(showRoutineLoadStmt.getDbFullName(),","[{'comment': 'new line for = ?', 'commenter': 'wuyunfeng'}, {'comment': 'I will change it', 'commenter': 'EmmyMiao87'}]"
1003,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoadJob.java,"@@ -301,332 +388,771 @@ public int getSizeOfRoutineLoadTaskInfoList() {
         } finally {
             readUnlock();
         }
-
-    }
-
-    public List<RoutineLoadTaskInfo> getNeedScheduleTaskInfoList() {
-        return needScheduleTaskInfoList;
     }
 
-    public void updateState(JobState jobState) {
-        writeLock();
-        try {
-            state = jobState;
-        } finally {
-            writeUnlock();
-        }
-    }
-
-    public List<RoutineLoadTaskInfo> processTimeoutTasks() {
-        List<RoutineLoadTaskInfo> result = new ArrayList<>();
+    // only check loading task
+    public void processTimeoutTasks() {
         writeLock();
         try {
             List<RoutineLoadTaskInfo> runningTasks = new ArrayList<>(routineLoadTaskInfoList);
-            runningTasks.removeAll(needScheduleTaskInfoList);
-
             for (RoutineLoadTaskInfo routineLoadTaskInfo : runningTasks) {
-                if ((System.currentTimeMillis() - routineLoadTaskInfo.getLoadStartTimeMs())
-                        > DEFAULT_TASK_TIMEOUT_SECONDS * 1000) {
-                    String oldSignature = routineLoadTaskInfo.getId();
-                    // abort txn if not committed
-                    try {
-                        Catalog.getCurrentGlobalTransactionMgr()
-                                .abortTransaction(routineLoadTaskInfo.getTxnId(), ""routine load task of txn was timeout"");
-                    } catch (UserException e) {
-                        if (e.getMessage().contains(""committed"")) {
-                            LOG.debug(""txn of task {} has been committed, timeout task has been ignored"", oldSignature);
-                            continue;
-                        }
-                    }
-
-                    try {
-                        result.add(reNewTask(routineLoadTaskInfo));
-                        LOG.debug(""Task {} was ran more then {} minutes. It was removed and rescheduled"",
-                                  oldSignature, DEFAULT_TASK_TIMEOUT_SECONDS);
-                    } catch (UserException e) {
-                        state = JobState.CANCELLED;
-                        // TODO(ml): edit log
-                        LOG.warn(""failed to renew a routine load task in job {} with error message {}"", id, e.getMessage());
-                    }
+                if (routineLoadTaskInfo.isRunning()
+                        && ((System.currentTimeMillis() - routineLoadTaskInfo.getExecuteStartTimeMs())
+                        > maxBatchIntervalS * 2 * 1000)) {
+                    RoutineLoadTaskInfo newTask = unprotectRenewTask(routineLoadTaskInfo);
+                    Catalog.getCurrentCatalog().getRoutineLoadTaskScheduler().addTaskInQueue(newTask);
                 }
             }
         } finally {
             writeUnlock();
         }
-        return result;
     }
 
-    abstract List<RoutineLoadTaskInfo> divideRoutineLoadJob(int currentConcurrentTaskNum);
+    abstract void divideRoutineLoadJob(int currentConcurrentTaskNum) throws UserException;
 
     public int calculateCurrentConcurrentTaskNum() throws MetaNotFoundException {
         return 0;
     }
 
-    @Override
-    public void write(DataOutput out) throws IOException {
-        // TODO(ml)
-    }
-
-    @Override
-    public void readFields(DataInput in) throws IOException {
-        // TODO(ml)
-    }
-
-
-    public void removeNeedScheduleTask(RoutineLoadTaskInfo routineLoadTaskInfo) {
-        writeLock();
+    public Map<Long, Integer> getBeIdToConcurrentTaskNum() {
+        Map<Long, Integer> beIdConcurrentTasksNum = Maps.newHashMap();
+        readLock();
         try {
-            needScheduleTaskInfoList.remove(routineLoadTaskInfo);
+            for (RoutineLoadTaskInfo routineLoadTaskInfo : routineLoadTaskInfoList) {
+                if (routineLoadTaskInfo.getBeId() != -1L) {
+                    long beId = routineLoadTaskInfo.getBeId();
+                    if (beIdConcurrentTasksNum.containsKey(beId)) {
+                        beIdConcurrentTasksNum.put(beId, beIdConcurrentTasksNum.get(beId) + 1);
+                    } else {
+                        beIdConcurrentTasksNum.put(beId, 1);
+                    }
+                }
+            }
+            return beIdConcurrentTasksNum;
         } finally {
-            writeUnlock();
+            readUnlock();
         }
     }
 
-    abstract void updateProgress(RoutineLoadProgress progress);
-
-    public boolean containsTask(String taskId) {
+    public boolean containsTask(UUID taskId) {
         readLock();
         try {
-            return routineLoadTaskInfoList.parallelStream()
+            return routineLoadTaskInfoList.stream()
                     .anyMatch(entity -> entity.getId().equals(taskId));
         } finally {
             readUnlock();
         }
     }
 
     // All of private method could not be call without lock
-    private void checkStateTransform(RoutineLoadJob.JobState desireState)
-            throws UnsupportedOperationException {
+    private void checkStateTransform(RoutineLoadJob.JobState desireState) throws UserException {
         switch (state) {
             case RUNNING:
                 if (desireState == JobState.NEED_SCHEDULE) {
-                    throw new UnsupportedOperationException(""Could not transform "" + state + "" to "" + desireState);
+                    throw new DdlException(""Could not transform "" + state + "" to "" + desireState);
                 }
                 break;
             case PAUSED:
                 if (desireState == JobState.PAUSED) {
-                    throw new UnsupportedOperationException(""Could not transform "" + state + "" to "" + desireState);
+                    throw new DdlException(""Could not transform "" + state + "" to "" + desireState);
                 }
                 break;
             case STOPPED:
             case CANCELLED:
-                throw new UnsupportedOperationException(""Could not transfrom "" + state + "" to "" + desireState);
+                throw new DdlException(""Could not transform "" + state + "" to "" + desireState);
             default:
                 break;
         }
     }
 
-    private void loadTxnCommit(TLoadTxnCommitRequest request) throws TException {
-        FrontendServiceImpl frontendService = new FrontendServiceImpl(ExecuteEnv.getInstance());
-        frontendService.loadTxnCommit(request);
+    // if rate of error data is more then max_filter_ratio, pause job
+    protected void updateProgress(RLTaskTxnCommitAttachment attachment) throws UserException {
+        updateNumOfData(attachment.getTotalRows(), attachment.getFilteredRows(), attachment.getUnselectedRows(),
+                attachment.getReceivedBytes(), attachment.getTaskExecutionTimeMs(),
+                false /* not replay */);
     }
 
-    private void updateNumOfData(int numOfErrorData, int numOfTotalData) {
-        currentErrorNum += numOfErrorData;
-        currentTotalNum += numOfTotalData;
-        if (currentTotalNum > BASE_OF_ERROR_RATE) {
-            if (currentErrorNum > maxErrorNum) {
-                LOG.info(""current error num {} of job {} is more then max error num {}. begin to pause job"",
-                         currentErrorNum, id, maxErrorNum);
-                // remove all of task in jobs and change job state to paused
-                executePause(""current error num of job is more then max error num"");
+    private void updateNumOfData(long numOfTotalRows, long numOfErrorRows, long unselectedRows, long receivedBytes,
+            long taskExecutionTime, boolean isReplay) throws UserException {
+        this.totalRows += numOfTotalRows;
+        this.errorRows += numOfErrorRows;
+        this.unselectedRows += unselectedRows;
+        this.receivedBytes += receivedBytes;
+        this.totalTaskExcutionTimeMs += taskExecutionTime;
+
+        if (MetricRepo.isInit.get()) {
+            MetricRepo.COUNTER_ROUTINE_LOAD_ROWS.increase(numOfTotalRows);
+            MetricRepo.COUNTER_ROUTINE_LOAD_ERROR_ROWS.increase(numOfErrorRows);
+            MetricRepo.COUNTER_ROUTINE_LOAD_RECEIVED_BYTES.increase(receivedBytes);
+        }
+
+        // check error rate
+        currentErrorRows += numOfErrorRows;
+        currentTotalRows += numOfTotalRows;
+        if (currentTotalRows > maxBatchRows * 10) {
+            if (currentErrorRows > maxErrorNum) {
+                LOG.info(new LogBuilder(LogKey.ROUTINE_LOAD_JOB, id)
+                                 .add(""current_total_rows"", currentTotalRows)
+                                 .add(""current_error_rows"", currentErrorRows)
+                                 .add(""max_error_num"", maxErrorNum)
+                                 .add(""msg"", ""current error rows is more then max error num, begin to pause job"")
+                                 .build());
+                // if this is a replay thread, the update state should already be replayed by OP_CHANGE_ROUTINE_LOAD_JOB
+                if (!isReplay) {
+                    // remove all of task in jobs and change job state to paused
+                    updateState(JobState.PAUSED, ""current error rows of job is more then max error num"", isReplay);
+                }
             }
 
+            if (LOG.isDebugEnabled()) {
+                LOG.debug(new LogBuilder(LogKey.ROUTINE_LOAD_JOB, id)
+                                  .add(""current_total_rows"", currentTotalRows)
+                                  .add(""current_error_rows"", currentErrorRows)
+                                  .add(""max_error_num"", maxErrorNum)
+                                  .add(""msg"", ""reset current total rows and current error rows ""
+                                          + ""when current total rows is more then base"")
+                                  .build());
+            }
             // reset currentTotalNum and currentErrorNum
-            currentErrorNum = 0;
-            currentTotalNum = 0;
-        } else if (currentErrorNum > maxErrorNum) {
-            LOG.info(""current error num {} of job {} is more then max error num {}. begin to pause job"",
-                     currentErrorNum, id, maxErrorNum);
-            // remove all of task in jobs and change job state to paused
-            executePause(""current error num is more then max error num"");
+            currentErrorRows = 0;
+            currentTotalRows = 0;
+        } else if (currentErrorRows > maxErrorNum) {
+            LOG.info(new LogBuilder(LogKey.ROUTINE_LOAD_JOB, id)
+                             .add(""current_total_rows"", currentTotalRows)
+                             .add(""current_error_rows"", currentErrorRows)
+                             .add(""max_error_num"", maxErrorNum)
+                             .add(""msg"", ""current error rows is more then max error rows, begin to pause job"")
+                             .build());
+            if (!isReplay) {
+                // remove all of task in jobs and change job state to paused
+                updateState(JobState.PAUSED, ""current error rows is more then max error num"", isReplay);
+            }
             // reset currentTotalNum and currentErrorNum
-            currentErrorNum = 0;
-            currentTotalNum = 0;
+            currentErrorRows = 0;
+            currentTotalRows = 0;
         }
     }
 
-    abstract RoutineLoadTaskInfo reNewTask(RoutineLoadTaskInfo routineLoadTaskInfo) throws AnalysisException,
-            LabelAlreadyUsedException, BeginTransactionException;
+    protected void replayUpdateProgress(RLTaskTxnCommitAttachment attachment) {
+        try {
+            updateNumOfData(attachment.getTotalRows(), attachment.getFilteredRows(), attachment.getUnselectedRows(),
+                    attachment.getReceivedBytes(), attachment.getTaskExecutionTimeMs(), true /* is replay */);
+        } catch (UserException e) {
+            LOG.error(""should not happen"", e);
+        }
+    }
 
-    @Override
-    public void beforeAborted(TransactionState txnState, TransactionState.TxnStatusChangeReason txnStatusChangeReason)
-            throws AbortTransactionException {
-        readLock();
+    abstract RoutineLoadTaskInfo unprotectRenewTask(RoutineLoadTaskInfo routineLoadTaskInfo);
+
+    public void initPlanner() throws UserException {
+        StreamLoadTask streamLoadTask = StreamLoadTask.fromRoutineLoadJob(this);
+        Database db = Catalog.getCurrentCatalog().getDb(dbId);
+        if (db == null) {
+            throw new MetaNotFoundException(""db "" + dbId + "" does not exist"");
+        }
+        planner = new StreamLoadPlanner(db, (OlapTable) db.getTable(this.tableId), streamLoadTask);
+    }
+
+    public TExecPlanFragmentParams plan() throws UserException {
+        Preconditions.checkNotNull(planner);
+        Database db = Catalog.getCurrentCatalog().getDb(dbId);
+        if (db == null) {
+            throw new MetaNotFoundException(""db "" + dbId + "" does not exist"");
+        }
+        db.readLock();
         try {
-            if (txnStatusChangeReason != null) {
-                switch (txnStatusChangeReason) {
-                    case TIMEOUT:
-                        String taskId = txnState.getLabel();
-                        if (routineLoadTaskInfoList.parallelStream().anyMatch(entity -> entity.getId().equals(taskId))) {
-                            throw new AbortTransactionException(
-                                    ""there are task "" + taskId + "" related to this txn, ""
-                                            + ""txn could not be abort"", txnState.getTransactionId());
-                        }
-                        break;
-                }
-            }
+            return planner.plan();
         } finally {
-            readUnlock();
+            db.readUnlock();
         }
     }
 
+    // if task not exists, before aborted will throw exception
+    // if task pass the checker, job lock will be locked
+    // if tryLock timeout, txn will be aborted by timeout progress.
+    // *** Please do not call before individually. It must be combined use with after ***
     @Override
-    public void onCommitted(TransactionState txnState) {
-        // step0: get progress from transaction state
-        RLTaskTxnCommitAttachment rlTaskTxnCommitAttachment = (RLTaskTxnCommitAttachment) txnState.getTxnCommitAttachment();
-
-        writeLock();
-        try {
-            // step1: find task in job
-            Optional<RoutineLoadTaskInfo> routineLoadTaskInfoOptional =
-                    routineLoadTaskInfoList.parallelStream()
-                            .filter(entity -> entity.getId().equals(txnState.getLabel())).findFirst();
-            RoutineLoadTaskInfo routineLoadTaskInfo = routineLoadTaskInfoOptional.get();
+    public void beforeAborted(TransactionState txnState) throws TransactionException {
+        LOG.debug(new LogBuilder(LogKey.ROUINTE_LOAD_TASK, txnState.getLabel())
+                          .add(""txn_state"", txnState)
+                          .add(""msg"", ""task before aborted"")
+                          .build());
+        executeBeforeCheck(txnState, TransactionStatus.ABORTED);
+    }
 
-            // step2: update job progress
-            updateProgress(rlTaskTxnCommitAttachment.getProgress());
+    // if task not exists, before aborted will throw exception
+    // if task pass the checker, lock job will be locked
+    // if tryLock timeout, txn will be aborted by timeout progress.
+    // *** Please do not call before individually. It must be combined use with after ***
+    @Override
+    public void beforeCommitted(TransactionState txnState) throws TransactionException {
+        LOG.debug(new LogBuilder(LogKey.ROUINTE_LOAD_TASK, txnState.getLabel())","[{'comment': '\r\n```suggestion\r\n    if (LOG.isDebugEnabled()) \r\n        LOG.debug(new LogBuilder(LogKey.ROUINTE_LOAD_TASK, txnState.getLabel())\r\n```', 'commenter': 'imay'}]"
1003,fe/src/main/java/org/apache/doris/load/routineload/RoutineLoadJob.java,"@@ -301,332 +388,771 @@ public int getSizeOfRoutineLoadTaskInfoList() {
         } finally {
             readUnlock();
         }
-
-    }
-
-    public List<RoutineLoadTaskInfo> getNeedScheduleTaskInfoList() {
-        return needScheduleTaskInfoList;
     }
 
-    public void updateState(JobState jobState) {
-        writeLock();
-        try {
-            state = jobState;
-        } finally {
-            writeUnlock();
-        }
-    }
-
-    public List<RoutineLoadTaskInfo> processTimeoutTasks() {
-        List<RoutineLoadTaskInfo> result = new ArrayList<>();
+    // only check loading task
+    public void processTimeoutTasks() {
         writeLock();
         try {
             List<RoutineLoadTaskInfo> runningTasks = new ArrayList<>(routineLoadTaskInfoList);
-            runningTasks.removeAll(needScheduleTaskInfoList);
-
             for (RoutineLoadTaskInfo routineLoadTaskInfo : runningTasks) {
-                if ((System.currentTimeMillis() - routineLoadTaskInfo.getLoadStartTimeMs())
-                        > DEFAULT_TASK_TIMEOUT_SECONDS * 1000) {
-                    String oldSignature = routineLoadTaskInfo.getId();
-                    // abort txn if not committed
-                    try {
-                        Catalog.getCurrentGlobalTransactionMgr()
-                                .abortTransaction(routineLoadTaskInfo.getTxnId(), ""routine load task of txn was timeout"");
-                    } catch (UserException e) {
-                        if (e.getMessage().contains(""committed"")) {
-                            LOG.debug(""txn of task {} has been committed, timeout task has been ignored"", oldSignature);
-                            continue;
-                        }
-                    }
-
-                    try {
-                        result.add(reNewTask(routineLoadTaskInfo));
-                        LOG.debug(""Task {} was ran more then {} minutes. It was removed and rescheduled"",
-                                  oldSignature, DEFAULT_TASK_TIMEOUT_SECONDS);
-                    } catch (UserException e) {
-                        state = JobState.CANCELLED;
-                        // TODO(ml): edit log
-                        LOG.warn(""failed to renew a routine load task in job {} with error message {}"", id, e.getMessage());
-                    }
+                if (routineLoadTaskInfo.isRunning()
+                        && ((System.currentTimeMillis() - routineLoadTaskInfo.getExecuteStartTimeMs())
+                        > maxBatchIntervalS * 2 * 1000)) {
+                    RoutineLoadTaskInfo newTask = unprotectRenewTask(routineLoadTaskInfo);
+                    Catalog.getCurrentCatalog().getRoutineLoadTaskScheduler().addTaskInQueue(newTask);
                 }
             }
         } finally {
             writeUnlock();
         }
-        return result;
     }
 
-    abstract List<RoutineLoadTaskInfo> divideRoutineLoadJob(int currentConcurrentTaskNum);
+    abstract void divideRoutineLoadJob(int currentConcurrentTaskNum) throws UserException;
 
     public int calculateCurrentConcurrentTaskNum() throws MetaNotFoundException {
         return 0;
     }
 
-    @Override
-    public void write(DataOutput out) throws IOException {
-        // TODO(ml)
-    }
-
-    @Override
-    public void readFields(DataInput in) throws IOException {
-        // TODO(ml)
-    }
-
-
-    public void removeNeedScheduleTask(RoutineLoadTaskInfo routineLoadTaskInfo) {
-        writeLock();
+    public Map<Long, Integer> getBeIdToConcurrentTaskNum() {
+        Map<Long, Integer> beIdConcurrentTasksNum = Maps.newHashMap();
+        readLock();
         try {
-            needScheduleTaskInfoList.remove(routineLoadTaskInfo);
+            for (RoutineLoadTaskInfo routineLoadTaskInfo : routineLoadTaskInfoList) {
+                if (routineLoadTaskInfo.getBeId() != -1L) {
+                    long beId = routineLoadTaskInfo.getBeId();
+                    if (beIdConcurrentTasksNum.containsKey(beId)) {
+                        beIdConcurrentTasksNum.put(beId, beIdConcurrentTasksNum.get(beId) + 1);
+                    } else {
+                        beIdConcurrentTasksNum.put(beId, 1);
+                    }
+                }
+            }
+            return beIdConcurrentTasksNum;
         } finally {
-            writeUnlock();
+            readUnlock();
         }
     }
 
-    abstract void updateProgress(RoutineLoadProgress progress);
-
-    public boolean containsTask(String taskId) {
+    public boolean containsTask(UUID taskId) {
         readLock();
         try {
-            return routineLoadTaskInfoList.parallelStream()
+            return routineLoadTaskInfoList.stream()
                     .anyMatch(entity -> entity.getId().equals(taskId));
         } finally {
             readUnlock();
         }
     }
 
     // All of private method could not be call without lock
-    private void checkStateTransform(RoutineLoadJob.JobState desireState)
-            throws UnsupportedOperationException {
+    private void checkStateTransform(RoutineLoadJob.JobState desireState) throws UserException {
         switch (state) {
             case RUNNING:
                 if (desireState == JobState.NEED_SCHEDULE) {
-                    throw new UnsupportedOperationException(""Could not transform "" + state + "" to "" + desireState);
+                    throw new DdlException(""Could not transform "" + state + "" to "" + desireState);
                 }
                 break;
             case PAUSED:
                 if (desireState == JobState.PAUSED) {
-                    throw new UnsupportedOperationException(""Could not transform "" + state + "" to "" + desireState);
+                    throw new DdlException(""Could not transform "" + state + "" to "" + desireState);
                 }
                 break;
             case STOPPED:
             case CANCELLED:
-                throw new UnsupportedOperationException(""Could not transfrom "" + state + "" to "" + desireState);
+                throw new DdlException(""Could not transform "" + state + "" to "" + desireState);
             default:
                 break;
         }
     }
 
-    private void loadTxnCommit(TLoadTxnCommitRequest request) throws TException {
-        FrontendServiceImpl frontendService = new FrontendServiceImpl(ExecuteEnv.getInstance());
-        frontendService.loadTxnCommit(request);
+    // if rate of error data is more then max_filter_ratio, pause job
+    protected void updateProgress(RLTaskTxnCommitAttachment attachment) throws UserException {
+        updateNumOfData(attachment.getTotalRows(), attachment.getFilteredRows(), attachment.getUnselectedRows(),
+                attachment.getReceivedBytes(), attachment.getTaskExecutionTimeMs(),
+                false /* not replay */);
     }
 
-    private void updateNumOfData(int numOfErrorData, int numOfTotalData) {
-        currentErrorNum += numOfErrorData;
-        currentTotalNum += numOfTotalData;
-        if (currentTotalNum > BASE_OF_ERROR_RATE) {
-            if (currentErrorNum > maxErrorNum) {
-                LOG.info(""current error num {} of job {} is more then max error num {}. begin to pause job"",
-                         currentErrorNum, id, maxErrorNum);
-                // remove all of task in jobs and change job state to paused
-                executePause(""current error num of job is more then max error num"");
+    private void updateNumOfData(long numOfTotalRows, long numOfErrorRows, long unselectedRows, long receivedBytes,
+            long taskExecutionTime, boolean isReplay) throws UserException {
+        this.totalRows += numOfTotalRows;
+        this.errorRows += numOfErrorRows;
+        this.unselectedRows += unselectedRows;
+        this.receivedBytes += receivedBytes;
+        this.totalTaskExcutionTimeMs += taskExecutionTime;
+
+        if (MetricRepo.isInit.get()) {
+            MetricRepo.COUNTER_ROUTINE_LOAD_ROWS.increase(numOfTotalRows);
+            MetricRepo.COUNTER_ROUTINE_LOAD_ERROR_ROWS.increase(numOfErrorRows);
+            MetricRepo.COUNTER_ROUTINE_LOAD_RECEIVED_BYTES.increase(receivedBytes);
+        }
+
+        // check error rate
+        currentErrorRows += numOfErrorRows;
+        currentTotalRows += numOfTotalRows;
+        if (currentTotalRows > maxBatchRows * 10) {
+            if (currentErrorRows > maxErrorNum) {
+                LOG.info(new LogBuilder(LogKey.ROUTINE_LOAD_JOB, id)
+                                 .add(""current_total_rows"", currentTotalRows)
+                                 .add(""current_error_rows"", currentErrorRows)
+                                 .add(""max_error_num"", maxErrorNum)
+                                 .add(""msg"", ""current error rows is more then max error num, begin to pause job"")
+                                 .build());
+                // if this is a replay thread, the update state should already be replayed by OP_CHANGE_ROUTINE_LOAD_JOB
+                if (!isReplay) {
+                    // remove all of task in jobs and change job state to paused
+                    updateState(JobState.PAUSED, ""current error rows of job is more then max error num"", isReplay);
+                }
             }
 
+            if (LOG.isDebugEnabled()) {
+                LOG.debug(new LogBuilder(LogKey.ROUTINE_LOAD_JOB, id)
+                                  .add(""current_total_rows"", currentTotalRows)
+                                  .add(""current_error_rows"", currentErrorRows)
+                                  .add(""max_error_num"", maxErrorNum)
+                                  .add(""msg"", ""reset current total rows and current error rows ""
+                                          + ""when current total rows is more then base"")
+                                  .build());
+            }
             // reset currentTotalNum and currentErrorNum
-            currentErrorNum = 0;
-            currentTotalNum = 0;
-        } else if (currentErrorNum > maxErrorNum) {
-            LOG.info(""current error num {} of job {} is more then max error num {}. begin to pause job"",
-                     currentErrorNum, id, maxErrorNum);
-            // remove all of task in jobs and change job state to paused
-            executePause(""current error num is more then max error num"");
+            currentErrorRows = 0;
+            currentTotalRows = 0;
+        } else if (currentErrorRows > maxErrorNum) {
+            LOG.info(new LogBuilder(LogKey.ROUTINE_LOAD_JOB, id)
+                             .add(""current_total_rows"", currentTotalRows)
+                             .add(""current_error_rows"", currentErrorRows)
+                             .add(""max_error_num"", maxErrorNum)
+                             .add(""msg"", ""current error rows is more then max error rows, begin to pause job"")
+                             .build());
+            if (!isReplay) {
+                // remove all of task in jobs and change job state to paused
+                updateState(JobState.PAUSED, ""current error rows is more then max error num"", isReplay);
+            }
             // reset currentTotalNum and currentErrorNum
-            currentErrorNum = 0;
-            currentTotalNum = 0;
+            currentErrorRows = 0;
+            currentTotalRows = 0;
         }
     }
 
-    abstract RoutineLoadTaskInfo reNewTask(RoutineLoadTaskInfo routineLoadTaskInfo) throws AnalysisException,
-            LabelAlreadyUsedException, BeginTransactionException;
+    protected void replayUpdateProgress(RLTaskTxnCommitAttachment attachment) {
+        try {
+            updateNumOfData(attachment.getTotalRows(), attachment.getFilteredRows(), attachment.getUnselectedRows(),
+                    attachment.getReceivedBytes(), attachment.getTaskExecutionTimeMs(), true /* is replay */);
+        } catch (UserException e) {
+            LOG.error(""should not happen"", e);
+        }
+    }
 
-    @Override
-    public void beforeAborted(TransactionState txnState, TransactionState.TxnStatusChangeReason txnStatusChangeReason)
-            throws AbortTransactionException {
-        readLock();
+    abstract RoutineLoadTaskInfo unprotectRenewTask(RoutineLoadTaskInfo routineLoadTaskInfo);
+
+    public void initPlanner() throws UserException {
+        StreamLoadTask streamLoadTask = StreamLoadTask.fromRoutineLoadJob(this);
+        Database db = Catalog.getCurrentCatalog().getDb(dbId);
+        if (db == null) {
+            throw new MetaNotFoundException(""db "" + dbId + "" does not exist"");
+        }
+        planner = new StreamLoadPlanner(db, (OlapTable) db.getTable(this.tableId), streamLoadTask);
+    }
+
+    public TExecPlanFragmentParams plan() throws UserException {
+        Preconditions.checkNotNull(planner);
+        Database db = Catalog.getCurrentCatalog().getDb(dbId);
+        if (db == null) {
+            throw new MetaNotFoundException(""db "" + dbId + "" does not exist"");
+        }
+        db.readLock();
         try {
-            if (txnStatusChangeReason != null) {
-                switch (txnStatusChangeReason) {
-                    case TIMEOUT:
-                        String taskId = txnState.getLabel();
-                        if (routineLoadTaskInfoList.parallelStream().anyMatch(entity -> entity.getId().equals(taskId))) {
-                            throw new AbortTransactionException(
-                                    ""there are task "" + taskId + "" related to this txn, ""
-                                            + ""txn could not be abort"", txnState.getTransactionId());
-                        }
-                        break;
-                }
-            }
+            return planner.plan();
         } finally {
-            readUnlock();
+            db.readUnlock();
         }
     }
 
+    // if task not exists, before aborted will throw exception
+    // if task pass the checker, job lock will be locked
+    // if tryLock timeout, txn will be aborted by timeout progress.
+    // *** Please do not call before individually. It must be combined use with after ***
     @Override
-    public void onCommitted(TransactionState txnState) {
-        // step0: get progress from transaction state
-        RLTaskTxnCommitAttachment rlTaskTxnCommitAttachment = (RLTaskTxnCommitAttachment) txnState.getTxnCommitAttachment();
-
-        writeLock();
-        try {
-            // step1: find task in job
-            Optional<RoutineLoadTaskInfo> routineLoadTaskInfoOptional =
-                    routineLoadTaskInfoList.parallelStream()
-                            .filter(entity -> entity.getId().equals(txnState.getLabel())).findFirst();
-            RoutineLoadTaskInfo routineLoadTaskInfo = routineLoadTaskInfoOptional.get();
+    public void beforeAborted(TransactionState txnState) throws TransactionException {
+        LOG.debug(new LogBuilder(LogKey.ROUINTE_LOAD_TASK, txnState.getLabel())
+                          .add(""txn_state"", txnState)
+                          .add(""msg"", ""task before aborted"")
+                          .build());
+        executeBeforeCheck(txnState, TransactionStatus.ABORTED);
+    }
 
-            // step2: update job progress
-            updateProgress(rlTaskTxnCommitAttachment.getProgress());
+    // if task not exists, before aborted will throw exception
+    // if task pass the checker, lock job will be locked
+    // if tryLock timeout, txn will be aborted by timeout progress.
+    // *** Please do not call before individually. It must be combined use with after ***
+    @Override
+    public void beforeCommitted(TransactionState txnState) throws TransactionException {
+        LOG.debug(new LogBuilder(LogKey.ROUINTE_LOAD_TASK, txnState.getLabel())
+                          .add(""txn_state"", txnState)
+                          .add(""msg"", ""task before committed"")
+                          .build());
+        executeBeforeCheck(txnState, TransactionStatus.COMMITTED);
+    }
 
-            // step3: remove task in agentTaskQueue
-            AgentTaskQueue.removeTask(rlTaskTxnCommitAttachment.getBackendId(), TTaskType.STREAM_LOAD,
-                                      rlTaskTxnCommitAttachment.getTaskSignature());
+    /*
+     * try lock the write lock.
+     * Make sure lock is released if any exception being thrown
+     */
+    private void executeBeforeCheck(TransactionState txnState, TransactionStatus transactionStatus)
+            throws TransactionException {
+        if (!tryWriteLock(2000, TimeUnit.MILLISECONDS)) {
+            // The lock of job has been locked by another thread more then timeout seconds.
+            // The commit txn by thread2 will be failed after waiting for timeout seconds.
+            // Maybe thread1 hang on somewhere
+            LOG.warn(new LogBuilder(LogKey.ROUINTE_LOAD_TASK, txnState.getLabel()).add(""job_id"", id).add(""txn_status"",
+                    transactionStatus).add(""error_msg"",
+                            ""txn could not be transformed while waiting for timeout of routine load job""));
+            throw new TransactionException(""txn "" + txnState.getTransactionId() + ""could not be "" + transactionStatus
+                    + ""while waiting for timeout of routine load job."");
+        }
 
-            // step4: if rate of error data is more then max_filter_ratio, pause job
-            updateNumOfData(rlTaskTxnCommitAttachment.getNumOfErrorData(), rlTaskTxnCommitAttachment.getNumOfTotalData());
+        // task already pass the checker
+        try {
+            // check if task has been aborted
+            Optional<RoutineLoadTaskInfo> routineLoadTaskInfoOptional =
+                    routineLoadTaskInfoList.stream()
+                            .filter(entity -> entity.getTxnId() == txnState.getTransactionId()).findFirst();
+            if (!routineLoadTaskInfoOptional.isPresent()) {
+                throw new TransactionException(""txn "" + txnState.getTransactionId()
+                                                       + "" could not be "" + transactionStatus
+                                                       + "" while task "" + txnState.getLabel() + "" has been aborted."");
+            }
+        } catch (TransactionException e) {","[{'comment': 'other exception will cause your `lock` is locked all the time.\r\n\r\nyou can set a boolean to true if check ok, and unlock write lock in finally block if this boolean is false', 'commenter': 'imay'}]"
1003,be/src/runtime/routine_load/data_consumer.cpp,"@@ -0,0 +1,224 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""runtime/routine_load/data_consumer.h""
+
+#include <algorithm>
+#include <functional>
+#include <string>
+#include <vector>
+
+#include ""common/status.h""
+#include ""service/backend_options.h""
+#include ""util/defer_op.h""
+#include ""util/stopwatch.hpp""
+#include ""util/uid_util.h""
+
+namespace doris {
+
+// init kafka consumer will only set common configs such as
+// brokers, groupid
+Status KafkaDataConsumer::init(StreamLoadContext* ctx) {
+    std::unique_lock<std::mutex> l(_lock); 
+    if (_init) {
+        // this consumer has already been initialized.
+        return Status::OK;
+    }
+
+    RdKafka::Conf *conf = RdKafka::Conf::create(RdKafka::Conf::CONF_GLOBAL);
+    
+    // conf has to be deleted finally
+    auto conf_deleter = [conf] () { delete conf; };
+    DeferOp delete_conf(std::bind<void>(conf_deleter));
+
+    std::stringstream ss;
+    ss << BackendOptions::get_localhost() << ""_"";
+    std::string group_id = ss.str() + UniqueId().to_string();
+    LOG(INFO) << ""init kafka consumer with group id: "" << group_id;
+
+    std::string errstr;
+    auto set_conf = [&conf, &errstr](const std::string& conf_key, const std::string& conf_val) {
+        if (conf->set(conf_key, conf_val, errstr) != RdKafka::Conf::CONF_OK) {
+            std::stringstream ss;
+            ss << ""failed to set '"" << conf_key << ""'"";
+            LOG(WARNING) << ss.str();
+            return Status(ss.str());
+        }
+        VLOG(3) << ""set "" << conf_key << "": "" << conf_val;
+        return Status::OK;
+    };
+
+    RETURN_IF_ERROR(set_conf(""metadata.broker.list"", ctx->kafka_info->brokers));
+    RETURN_IF_ERROR(set_conf(""group.id"", group_id));
+    RETURN_IF_ERROR(set_conf(""enable.partition.eof"", ""false""));
+    RETURN_IF_ERROR(set_conf(""enable.auto.offset.store"", ""false""));
+    // TODO: set it larger than 0 after we set rd_kafka_conf_set_stats_cb()
+    RETURN_IF_ERROR(set_conf(""statistics.interval.ms"", ""0""));
+    RETURN_IF_ERROR(set_conf(""auto.offset.reset"", ""error""));
+
+    if (conf->set(""event_cb"", &_k_event_cb, errstr) != RdKafka::Conf::CONF_OK) {
+        std::stringstream ss;
+        ss << ""failed to set 'event_cb'"";
+        LOG(WARNING) << ss.str();
+        return Status(ss.str());
+    }
+
+    // create consumer
+    _k_consumer = RdKafka::KafkaConsumer::create(conf, errstr); 
+    if (!_k_consumer) {
+        LOG(WARNING) << ""failed to create kafka consumer"";
+        return Status(""failed to create kafka consumer"");
+    }
+
+    VLOG(3) << ""finished to init kafka consumer. "" << ctx->brief();
+
+    _init = true;
+    return Status::OK;
+}
+
+Status KafkaDataConsumer::assign_topic_partitions(
+        const std::map<int32_t, int64_t>& begin_partition_offset,
+        const std::string& topic,
+        StreamLoadContext* ctx) {
+
+    DCHECK(_k_consumer);
+    // create TopicPartitions
+    std::stringstream ss;
+    std::vector<RdKafka::TopicPartition*> topic_partitions;
+    for (auto& entry : begin_partition_offset) {
+        RdKafka::TopicPartition* tp1 = RdKafka::TopicPartition::create(
+                topic, entry.first, entry.second);
+        topic_partitions.push_back(tp1);
+        ss << ""["" << entry.first << "": "" << entry.second << ""] "";
+    }
+
+    LOG(INFO) << ""consumer: "" << _id << "", grp: "" << _grp_id
+            << "" assign topic partitions: "" << topic << "", "" << ss.str();
+
+    // delete TopicPartition finally
+    auto tp_deleter = [&topic_partitions] () {
+            std::for_each(topic_partitions.begin(), topic_partitions.end(),
+                    [](RdKafka::TopicPartition* tp1) { delete tp1; });
+    };
+    DeferOp delete_tp(std::bind<void>(tp_deleter));
+
+    // assign partition
+    RdKafka::ErrorCode err = _k_consumer->assign(topic_partitions);
+    if (err) {
+        LOG(WARNING) << ""failed to assign topic partitions: "" << ctx->brief(true)
+                << "", err: "" << RdKafka::err2str(err);
+        return Status(""failed to assign topic partitions"");
+    }
+
+    return Status::OK;
+}
+
+Status KafkaDataConsumer::group_consume(
+        BlockingQueue<RdKafka::Message*>* queue,
+        int64_t max_running_time_ms) {
+    _last_visit_time = time(nullptr);
+    int64_t left_time = max_running_time_ms;
+    LOG(INFO) << ""start kafka consumer: "" << _id << "", grp: "" << _grp_id
+            << "", max running time(ms): "" << left_time;
+
+    int64_t received_rows = 0;
+    int64_t put_rows = 0;
+    Status st = Status::OK;
+    MonotonicStopWatch consumer_watch;
+    MonotonicStopWatch watch;
+    watch.start();
+    while (true) {
+        {
+            std::unique_lock<std::mutex> l(_lock);
+            if (_cancelled) { break; }
+        }
+
+        if (left_time <= 0) { break; }
+
+        bool done = false;
+        // consume 1 message at a time
+        consumer_watch.start();
+        RdKafka::Message *msg = _k_consumer->consume(1000 /* timeout, ms */);
+        consumer_watch.stop();
+        switch (msg->err()) {
+            case RdKafka::ERR_NO_ERROR:
+                if (!queue->blocking_put(msg)) {
+                    // queue is shutdown
+                    done = true;
+                } else {
+                    ++put_rows;
+                }
+                ++received_rows;
+                break;
+            case RdKafka::ERR__TIMED_OUT:","[{'comment': ' Does this `msg` need freed ?', 'commenter': 'imay'}, {'comment': 'Yes, it need to be deleted', 'commenter': 'morningman'}]"
1003,be/src/runtime/routine_load/data_consumer_group.cpp,"@@ -0,0 +1,181 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+
+#include ""runtime/routine_load/data_consumer_group.h""
+#include ""runtime/routine_load/kafka_consumer_pipe.h""
+
+namespace doris {
+
+Status KafkaDataConsumerGroup::assign_topic_partitions(StreamLoadContext* ctx) {
+    DCHECK(ctx->kafka_info);
+    DCHECK(_consumers.size() >= 1);
+
+    // divide partitions
+    int consumer_size = _consumers.size();
+    std::vector<std::map<int32_t, int64_t>> divide_parts(consumer_size);
+    int i = 0;
+    for (auto& kv : ctx->kafka_info->begin_offset) {
+        int idx = i % consumer_size;
+        divide_parts[idx].emplace(kv.first, kv.second);
+        i++;
+    }
+
+    // assign partitions to consumers equally
+    for (int i = 0; i < consumer_size; ++i) {
+        RETURN_IF_ERROR(std::static_pointer_cast<KafkaDataConsumer>(_consumers[i])->assign_topic_partitions(
+                divide_parts[i], ctx->kafka_info->topic, ctx));
+    }
+
+    return Status::OK;
+}
+
+Status KafkaDataConsumerGroup::start_all(StreamLoadContext* ctx) {
+    Status result_st = Status::OK;
+    // start all consumers
+    for(auto& consumer : _consumers) {
+        if (!_thread_pool.offer(
+            boost::bind<void>(&KafkaDataConsumerGroup::actual_consume, this, consumer, &_queue, ctx->max_interval_s * 1000,
+            [this, &result_st] (const Status& st) { 
+                std::unique_lock<std::mutex> lock(_mutex);
+                _counter--;
+                if (_counter == 0) {
+                    _queue.shutdown();
+                    LOG(INFO) << ""all consumers are finished. shutdown queue. group id: "" << _grp_id;
+                } 
+                if (result_st.ok() && !st.ok()) {
+                    result_st = st;
+                }
+            }))) {
+
+            LOG(WARNING) << ""failed to submit data consumer: "" << consumer->id() << "", group id: "" << _grp_id;
+            return Status(""failed to submit data consumer"");
+        } else {
+            VLOG(1) << ""submit a data consumer: "" << consumer->id() << "", group id: "" << _grp_id;
+        }
+    }
+
+    // consuming from queue and put data to stream load pipe
+    int64_t left_time = ctx->max_interval_s * 1000;
+    int64_t left_rows = ctx->max_batch_rows;
+    int64_t left_bytes = ctx->max_batch_size;
+
+    std::shared_ptr<KafkaConsumerPipe> kafka_pipe = std::static_pointer_cast<KafkaConsumerPipe>(ctx->body_sink);
+
+    LOG(INFO) << ""start consumer group: "" << _grp_id
+        << "". max time(ms): "" << left_time
+        << "", batch rows: "" << left_rows
+        << "", batch size: "" << left_bytes
+        << "". "" << ctx->brief();
+
+    // copy one
+    std::map<int32_t, int64_t> cmt_offset = ctx->kafka_info->cmt_offset;
+
+    MonotonicStopWatch watch;
+    watch.start();
+    Status st;
+    bool eos = false;
+    while (true) {
+        if (eos || left_time <= 0 || left_rows <= 0 || left_bytes <=0) {
+            LOG(INFO) << ""consumer group done: "" << _grp_id
+                    << "". consume time(ms)="" << ctx->max_interval_s * 1000 - left_time
+                    << "", received rows="" << ctx->max_batch_rows - left_rows
+                    << "", received bytes="" << ctx->max_batch_size - left_bytes
+                    << "", eos: "" << eos
+                    << "", blocking get time(us): "" << _queue.total_get_wait_time() / 1000
+                    << "", blocking put time(us): "" << _queue.total_put_wait_time() / 1000;
+            
+            // shutdown queue
+            _queue.shutdown();
+            // cancel all consumers
+            for(auto& consumer : _consumers) { consumer->cancel(ctx); }
+            // clean the msgs left in queue
+            while(true) {","[{'comment': 'put this to destructor', 'commenter': 'imay'}, {'comment': 'ok', 'commenter': 'morningman'}]"
1003,fe/src/main/java/org/apache/doris/catalog/Catalog.java,"@@ -1385,24 +1394,27 @@ public long loadMasterInfo(DataInputStream dis, long checksum) throws IOExceptio
     }
 
     public long loadFrontends(DataInputStream dis, long checksum) throws IOException {
-        int size = dis.readInt();
-        long newChecksum = checksum ^ size;
-        for (int i = 0; i < size; i++) {
-            Frontend fe = Frontend.read(dis);
-            replayAddFrontend(fe);
-        }
-
-        size = dis.readInt();
-        newChecksum ^= size;
-        for (int i = 0; i < size; i++) {
-            if (Catalog.getCurrentCatalogJournalVersion() < FeMetaVersion.VERSION_41) {
+        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_22) {","[{'comment': 'why convert with VERSION_22?', 'commenter': 'imay'}, {'comment': 'I just move this judgement inside the loadFrontends() method.', 'commenter': 'morningman'}]"
1003,fe/src/main/java/org/apache/doris/catalog/Catalog.java,"@@ -1688,10 +1700,13 @@ public long loadAccessService(DataInputStream dis, long checksum) throws IOExcep
     }
 
     public long loadTransactionState(DataInputStream dis, long checksum) throws IOException {
-        int size = dis.readInt();
-        long newChecksum = checksum ^ size;
-        globalTransactionMgr.readFields(dis);
-        return newChecksum;
+        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_45) {","[{'comment': 'why convert with VERSION_45', 'commenter': 'imay'}, {'comment': 'I just move this judgement inside the loadTransactionState() method.', 'commenter': 'morningman'}]"
1003,fe/src/main/java/org/apache/doris/transaction/TransactionState.java,"@@ -334,14 +280,62 @@ public void setTransactionStatus(TransactionStatus transactionStatus, TxnStatusC
             if (MetricRepo.isInit.get()) {
                 MetricRepo.COUNTER_TXN_FAILED.increase(1L);
             }
-            if (txnStateChangeListener != null) {
-                txnStateChangeListener.onAborted(this, txnStatusChangeReason);
+        }
+    }
+
+    public void beforeStateTransform(TransactionStatus transactionStatus) throws TransactionException {
+        // before status changed
+        TxnStateChangeListener listener = Catalog.getCurrentGlobalTransactionMgr()
+                .getListenerRegistry().getListener(listenerId);
+        if (listener != null) {","[{'comment': 'If Job is removed, listener will be null', 'commenter': 'imay'}]"
1003,fe/src/main/java/org/apache/doris/analysis/CreateRoutineLoadStmt.java,"@@ -153,122 +178,129 @@ public int getDesiredConcurrentNum() {
         return desiredConcurrentNum;
     }
 
-    public int getMaxErrorNum() {
+    public long getMaxErrorNum() {
         return maxErrorNum;
     }
 
-    public String getKafkaEndpoint() {
-        return kafkaEndpoint;
+    public long getMaxBatchIntervalS() {
+        return maxBatchIntervalS;
+    }
+
+    public long getMaxBatchRows() {
+        return maxBatchRows;
+    }
+
+    public long getMaxBatchSize() {
+        return maxBatchSizeBytes;
+    }
+
+    public String getKafkaBrokerList() {
+        return kafkaBrokerList;
     }
 
     public String getKafkaTopic() {
         return kafkaTopic;
     }
 
-    public List<Integer> getKafkaPartitions() {
-        return kafkaPartitions;
+    public List<Pair<Integer, Long>> getKafkaPartitionOffsets() {
+        return kafkaPartitionOffsets;
     }
 
     @Override
-    public void analyze(Analyzer analyzer) throws AnalysisException, UserException {
+    public void analyze(Analyzer analyzer) throws UserException {
         super.analyze(analyzer);
+        // check dbName and tableName
+        checkDBTable(analyzer);
         // check name","[{'comment': ""The checkDBTable throws AnalysisException, but analyze does't catch it or throws it too."", 'commenter': 'chenhao7253886'}, {'comment': 'It will throw analysisException to outside.', 'commenter': 'EmmyMiao87'}]"
1003,fe/src/main/java/org/apache/doris/analysis/CreateRoutineLoadStmt.java,"@@ -153,122 +178,129 @@ public int getDesiredConcurrentNum() {
         return desiredConcurrentNum;
     }
 
-    public int getMaxErrorNum() {
+    public long getMaxErrorNum() {
         return maxErrorNum;
     }
 
-    public String getKafkaEndpoint() {
-        return kafkaEndpoint;
+    public long getMaxBatchIntervalS() {
+        return maxBatchIntervalS;
+    }
+
+    public long getMaxBatchRows() {
+        return maxBatchRows;
+    }
+
+    public long getMaxBatchSize() {
+        return maxBatchSizeBytes;
+    }
+
+    public String getKafkaBrokerList() {
+        return kafkaBrokerList;
     }
 
     public String getKafkaTopic() {
         return kafkaTopic;
     }
 
-    public List<Integer> getKafkaPartitions() {
-        return kafkaPartitions;
+    public List<Pair<Integer, Long>> getKafkaPartitionOffsets() {
+        return kafkaPartitionOffsets;
     }
 
     @Override
-    public void analyze(Analyzer analyzer) throws AnalysisException, UserException {
+    public void analyze(Analyzer analyzer) throws UserException {
         super.analyze(analyzer);
+        // check dbName and tableName
+        checkDBTable(analyzer);
         // check name
         FeNameFormat.checkCommonName(NAME_TYPE, name);
-        // check dbName and tableName
-        checkDBTableName();
         // check load properties include column separator etc.
         checkLoadProperties(analyzer);
-        // check routine load properties include desired concurrent number etc.
-        checkRoutineLoadProperties();
-        // check custom properties
-        checkCustomProperties();
+        // check routine load job properties include desired concurrent number etc.
+        checkJobProperties();
+        // check data source properties
+        checkDataSourceProperties();
     }
 
-    private void checkDBTableName() throws AnalysisException {
-        if (Strings.isNullOrEmpty(dbTableName.getDb())) {
-            String dbName = ConnectContext.get().getDatabase();
-            if (Strings.isNullOrEmpty(dbName)) {
-                throw new AnalysisException(""please choose a database first"");
-            }
-            dbTableName.setDb(dbName);
-        }
-        if (Strings.isNullOrEmpty(dbTableName.getTbl())) {
-            throw new AnalysisException(""empty table name in create routine load statement"");
+    public void checkDBTable(Analyzer analyzer) throws AnalysisException {
+        labelName.analyze(analyzer);
+        dbName = labelName.getDbName();
+        name = labelName.getLabelName();
+        if (Strings.isNullOrEmpty(tableName)) {
+            throw new AnalysisException(""Table name should not be null"");
         }","[{'comment': ""db and table is't checked if it exists? "", 'commenter': 'chenhao7253886'}, {'comment': 'It will be check in `RoutineLoadManager.fromCreateStmt()`', 'commenter': 'EmmyMiao87'}]"
1003,docs/documentation/cn/administrator-guide/load-data/routine-load-manual.md,"@@ -0,0 +1,155 @@
+# 例行导入使用手册
+
+例行导入（Routine Load）功能为用户提供了一种自动从指定数据源进行数据导入的功能。
+
+本文档主要介绍该功能的实现原理、使用方式以及最佳实践。
+
+## 名词解释
+
+* FE：Frontend，Doris 的前端节点。负责元数据管理和请求接入。
+* BE：Backend，Doris 的后端节点。负责查询执行和数据存储。
+* RoutineLoadJob：用户提交的一个例行导入作业。
+* JobScheduler：例行导入作业调度器，用于调度和拆分一个 RoutineLoadJob 为多个 Task。
+* Task：RoutineLoadJob 被 JobScheduler 根据规则拆分的子任务。
+* TaskScheduler：任务调度器。用于调度 Task 的执行。
+
+## 原理
+
+```              
+         +---------+
+         |  Client |
+         +----+----+
+              |
++-----------------------------+
+| FE          |               |
+| +-----------v------------+  |
+| |                        |  |
+| |   Routine Load Job     |  |
+| |                        |  |
+| +---+--------+--------+--+  |
+|     |        |        |     |
+| +---v--+ +---v--+ +---v--+  |
+| | task | | task | | task |  |
+| +--+---+ +---+--+ +---+--+  |
+|    |         |        |     |
++-----------------------------+
+     |         |        |
+     v         v        v
+ +---+--+   +--+---+   ++-----+
+ |  BE  |   |  BE  |   |  BE  |
+ +------+   +------+   +------+
+
+```
+
+如上图，Client 向 FE 提交一个例行导入作业。
+
+FE 通过 JobScheduler 将一个导入作业拆分成若干个 Task。每个 Task 负责导入指定的一部分数据。Task 被 TaskScheduler 分配到指定的 BE 上执行。
+
+在 BE 上，一个 Task 被视为一个普通的导入任务，通过 Stream Load 的导入机制进行导入。导入完成后，向 FE 汇报。
+
+FE 中的 JobScheduler 根据汇报结果，继续生成后续新的 Task，或者对失败的 Task 进行重试。
+
+整个例行导入作业通过不断的产生新的 Task，来完成数据不间断的导入。
+
+## Kafka 例行导入
+
+当前我们仅支持从 Kafka 系统进行例行导入。该部分会详细介绍 Kafka 例行导入使用方式和最佳实践。
+
+### 使用限制
+
+1. 仅支持无认证的 Kafka 访问。
+2. 支持的消息格式为 csv 文本格式。每一个 message 为一行，且行尾**不包含**换行符。
+3. 仅支持 Kafka 0.10.0.0(含) 以上版本。
+
+### 创建例行导入任务
+
+创建例行导入任务的的详细语法可以参照 [这里]()。或者连接到 Doris 后，执行 `HELP CREATE ROUTINE LOAD;` 查看语法帮助。这里主要详细介绍，创建作业时的注意事项。
+
+* columns_mapping
+
+    `columns_mapping` 主要用于指定表结构和 message 中的列映射关系，以及一些列的转换。如果不指定，Doris 会默认 message 中的列和表结构的列按顺序一一对应。虽然在正常情况下，如果源数据正好一一对应，则不指定也可以进行正常的数据导入。但是我们依然强烈建议用户**显式的指定列映射关系**。这样当表结构发生变化（比如增加一个 nullable 的列），或者源文件发生变化（比如增加了一列）时，导入任务依然可以继续进行。否则，当发生上述变动后，因为列映射关系不再一一对应，导入将报错。
+    
+    在 `columns_mapping` 中我们同样可以使用一些内置函数进行列的转换。但需要注意函数参数对应的实际列类型。举例说明：
+    
+    假设用户需要导入只包含 `k1` 一列的表，列类型为 `int`。并且需要将源文件中的 null 值转换为 0。该功能可以通过 `ifnull` 函数实现。正确是的使用方式如下：
+    
+    `COLUMNS (xx, k1=ifnull(xx, ""3""))`
+    
+    注意这里我们使用 `""3""` 而不是 `3`，虽然 `k1` 的类型为 `int`。因为对于导入任务来说，源数据中的列类型都为 `varchar`，所以这里 `xx` 虚拟列的类型也为 `varchar`。所以我们需要使用 `""3""` 来进行对应的匹配，否则 `ifnull` 函数无法找到参数为 `(varchar, int)` 的函数签名，将出现错误。","[{'comment': ""firstly, it does't exist ifnull function which params are `(varchar, int)`, secondly because `xx` is varchar and function will match  ifnull(varchar, varchar). if it can't match right symbol, it's a function match bug."", 'commenter': 'chenhao7253886'}]"
1003,docs/documentation/cn/administrator-guide/load-data/routine-load-manual.md,"@@ -0,0 +1,155 @@
+# 例行导入使用手册
+
+例行导入（Routine Load）功能为用户提供了一种自动从指定数据源进行数据导入的功能。
+
+本文档主要介绍该功能的实现原理、使用方式以及最佳实践。
+
+## 名词解释
+
+* FE：Frontend，Doris 的前端节点。负责元数据管理和请求接入。
+* BE：Backend，Doris 的后端节点。负责查询执行和数据存储。
+* RoutineLoadJob：用户提交的一个例行导入作业。
+* JobScheduler：例行导入作业调度器，用于调度和拆分一个 RoutineLoadJob 为多个 Task。
+* Task：RoutineLoadJob 被 JobScheduler 根据规则拆分的子任务。
+* TaskScheduler：任务调度器。用于调度 Task 的执行。
+
+## 原理
+
+```              
+         +---------+
+         |  Client |
+         +----+----+
+              |
++-----------------------------+
+| FE          |               |
+| +-----------v------------+  |
+| |                        |  |
+| |   Routine Load Job     |  |
+| |                        |  |
+| +---+--------+--------+--+  |
+|     |        |        |     |
+| +---v--+ +---v--+ +---v--+  |
+| | task | | task | | task |  |
+| +--+---+ +---+--+ +---+--+  |
+|    |         |        |     |
++-----------------------------+
+     |         |        |
+     v         v        v
+ +---+--+   +--+---+   ++-----+
+ |  BE  |   |  BE  |   |  BE  |
+ +------+   +------+   +------+
+
+```
+
+如上图，Client 向 FE 提交一个例行导入作业。
+
+FE 通过 JobScheduler 将一个导入作业拆分成若干个 Task。每个 Task 负责导入指定的一部分数据。Task 被 TaskScheduler 分配到指定的 BE 上执行。
+
+在 BE 上，一个 Task 被视为一个普通的导入任务，通过 Stream Load 的导入机制进行导入。导入完成后，向 FE 汇报。
+
+FE 中的 JobScheduler 根据汇报结果，继续生成后续新的 Task，或者对失败的 Task 进行重试。
+
+整个例行导入作业通过不断的产生新的 Task，来完成数据不间断的导入。
+
+## Kafka 例行导入
+
+当前我们仅支持从 Kafka 系统进行例行导入。该部分会详细介绍 Kafka 例行导入使用方式和最佳实践。
+
+### 使用限制
+
+1. 仅支持无认证的 Kafka 访问。
+2. 支持的消息格式为 csv 文本格式。每一个 message 为一行，且行尾**不包含**换行符。
+3. 仅支持 Kafka 0.10.0.0(含) 以上版本。
+
+### 创建例行导入任务
+
+创建例行导入任务的的详细语法可以参照 [这里]()。或者连接到 Doris 后，执行 `HELP CREATE ROUTINE LOAD;` 查看语法帮助。这里主要详细介绍，创建作业时的注意事项。
+
+* columns_mapping
+
+    `columns_mapping` 主要用于指定表结构和 message 中的列映射关系，以及一些列的转换。如果不指定，Doris 会默认 message 中的列和表结构的列按顺序一一对应。虽然在正常情况下，如果源数据正好一一对应，则不指定也可以进行正常的数据导入。但是我们依然强烈建议用户**显式的指定列映射关系**。这样当表结构发生变化（比如增加一个 nullable 的列），或者源文件发生变化（比如增加了一列）时，导入任务依然可以继续进行。否则，当发生上述变动后，因为列映射关系不再一一对应，导入将报错。
+    
+    在 `columns_mapping` 中我们同样可以使用一些内置函数进行列的转换。但需要注意函数参数对应的实际列类型。举例说明：
+    
+    假设用户需要导入只包含 `k1` 一列的表，列类型为 `int`。并且需要将源文件中的 null 值转换为 0。该功能可以通过 `ifnull` 函数实现。正确是的使用方式如下：
+    
+    `COLUMNS (xx, k1=ifnull(xx, ""3""))`
+    
+    注意这里我们使用 `""3""` 而不是 `3`，虽然 `k1` 的类型为 `int`。因为对于导入任务来说，源数据中的列类型都为 `varchar`，所以这里 `xx` 虚拟列的类型也为 `varchar`。所以我们需要使用 `""3""` 来进行对应的匹配，否则 `ifnull` 函数无法找到参数为 `(varchar, int)` 的函数签名，将出现错误。
+
+* desired\_concurrent\_number
+
+    `desired_concurrent_number` 用于指定一个例行作业期望的并发度。即一个作业，最多有多少 task 同时在执行。对于 Kafka 导入而言，当前的实际并发度计算如下：
+    
+    `Min(partition num / 3, desired_concurrent_number, alive_backend_num, Config.max_routine_load_task_concurrrent_num)`
+    ","[{'comment': 'the result maybe zero? if partition num smaller than 3.', 'commenter': 'chenhao7253886'}, {'comment': 'You are right. If current_concurrent_number is 0, the job will be rescheduled later.', 'commenter': 'EmmyMiao87'}]"
1003,fe/src/main/java/org/apache/doris/common/util/Daemon.java,"@@ -106,8 +106,8 @@ public void run() {
         while (!isStop.get()) {
             try {
                 runOneCycle();
-            } catch (Exception e) {
-                LOG.error(""exception: "", e);
+            } catch (Throwable e) {","[{'comment': ""Why catch throwable. because derived class such as Error should't be catched. we are only catch exceptions except RuntimeException."", 'commenter': 'chenhao7253886'}, {'comment': 'If there are Error thrown by Daemon, the daemon thread will be exited while fe is running correctly. No one know this error when thread has been exited.', 'commenter': 'EmmyMiao87'}]"
1037,be/src/olap/snapshot_manager.cpp,"@@ -155,6 +155,24 @@ OLAPStatus SnapshotManager::convert_rowset_ids(DataDir& data_dir, const string&
     new_tablet_meta_pb.set_schema_hash(schema_hash);
     TabletSchema tablet_schema;
     RETURN_NOT_OK(tablet_schema.init_from_pb(new_tablet_meta_pb.schema()));
+
+    RowsetId max_rowset_id = 0;
+    for (auto& visible_rowset : cloned_tablet_meta_pb.rs_metas()) {
+        if (visible_rowset.rowset_id() > max_rowset_id) {
+            max_rowset_id = visible_rowset.rowset_id();
+        }
+    }
+
+    for (auto& inc_rowset : cloned_tablet_meta_pb.inc_rs_metas()) {
+        if (inc_rowset.rowset_id() > max_rowset_id) {
+            max_rowset_id = inc_rowset.rowset_id();
+        }
+    }
+    OLAPStatus set_id_st = data_dir.set_next_id(max_rowset_id);","[{'comment': 'RETURN_NOT_OK(data_dir.set_next_id(max_rowset_id));', 'commenter': 'chaoyli'}]"
1039,samples/stream_load/java/DorisStreamLoad.java,"@@ -0,0 +1,124 @@
+import org.apache.commons.codec.binary.Base64;","[{'comment': 'license header', 'commenter': 'imay'}, {'comment': 'OK', 'commenter': 'kangkaisen'}]"
1064,be/src/exprs/string_functions.h,"@@ -137,8 +141,37 @@ class StringFunctions {
     static void parse_url_close(
         doris_udf::FunctionContext*,
         doris_udf::FunctionContext::FunctionStateScope);
-};
 
+
+    static doris_udf::StringVal money_format(doris_udf::FunctionContext* context,
+                                                 const doris_udf::DoubleVal& v);
+
+    static doris_udf::StringVal money_format(doris_udf::FunctionContext* context,
+                                                 const doris_udf::DecimalVal& v);
+
+    static doris_udf::StringVal money_format(doris_udf::FunctionContext* context,
+                                                 const doris_udf::BigIntVal& v);
+
+    static doris_udf::StringVal money_format(doris_udf::FunctionContext* context,
+                                                 const doris_udf::LargeIntVal& v);
+
+    struct comma_moneypunct : std::moneypunct<char> {
+        pattern do_pos_format() const override { return {{none, sign, none, value}}; }
+        pattern do_neg_format() const override { return {{none, sign, none, value}}; }
+        int do_frac_digits() const override { return 2; }
+        char_type do_thousands_sep() const override { return ','; }
+        string_type do_grouping() const override { return ""\003""; }
+        string_type do_negative_sign() const override { return ""-""; }
+    };
+
+    static StringVal do_money_format(FunctionContext *context, const std::string& v) {
+        std::locale comma_locale(std::locale(), new comma_moneypunct());","[{'comment': 'who can delete `new comma_moneypunct`? If there is no body, this memory will leak\r\n\r\nAnd I think use a stack variable is enough, \r\n\r\n```suggestion\r\n       comman_cmoneypunct moneypunct;\r\n        std::locale comma_locale(std::locale(), &moneypunc);\r\n```', 'commenter': 'imay'}, {'comment': 'OK', 'commenter': 'kangkaisen'}]"
1064,be/src/exprs/string_functions.h,"@@ -137,8 +141,37 @@ class StringFunctions {
     static void parse_url_close(
         doris_udf::FunctionContext*,
         doris_udf::FunctionContext::FunctionStateScope);
-};
 
+
+    static doris_udf::StringVal money_format(doris_udf::FunctionContext* context,
+                                                 const doris_udf::DoubleVal& v);
+
+    static doris_udf::StringVal money_format(doris_udf::FunctionContext* context,
+                                                 const doris_udf::DecimalVal& v);
+
+    static doris_udf::StringVal money_format(doris_udf::FunctionContext* context,
+                                                 const doris_udf::BigIntVal& v);
+
+    static doris_udf::StringVal money_format(doris_udf::FunctionContext* context,
+                                                 const doris_udf::LargeIntVal& v);
+
+    struct comma_moneypunct : std::moneypunct<char> {","[{'comment': 'class name should be camel-case\r\n```suggestion\r\n    struct CommaMoneypunct : std::moneypunct<char> {\r\n```', 'commenter': 'imay'}, {'comment': 'OK', 'commenter': 'kangkaisen'}]"
1064,be/test/exprs/string_functions_test.cpp,"@@ -0,0 +1,111 @@
+// Licensed to the Apache Software Foundation (ASF) under one","[{'comment': 'you should add run this test to run-ut.sh file to make regression test can run this test', 'commenter': 'imay'}, {'comment': 'OK', 'commenter': 'kangkaisen'}]"
1064,docs/documentation/cn/sql-reference/sql-functions/string-functions/money_format.md,"@@ -0,0 +1,34 @@
+# money_format
+
+## Syntax
+
+money_format(Number)","[{'comment': '```suggestion\r\nVARCHAR money_format(Number)\r\n```', 'commenter': 'imay'}, {'comment': 'OK', 'commenter': 'kangkaisen'}]"
1064,be/src/exprs/string_functions.cpp,"@@ -707,4 +708,45 @@ StringVal StringFunctions::parse_url_key(
     return result_sv;
 }
 
+StringVal StringFunctions::money_format(FunctionContext* context, const DoubleVal& v) {
+    if (v.is_null) {
+        return StringVal::null();
+    }
+
+    double v_cent= MathFunctions::my_double_round(v.val, 2, false, false) * 100;
+    return do_money_format(context, std::to_string(v_cent));
+}
+
+StringVal StringFunctions::money_format(FunctionContext *context, const DecimalVal &v) {
+    if (v.is_null) {
+        return StringVal::null();
+    }
+
+    DecimalValue rounded;
+    DecimalValue::from_decimal_val(v).round(&rounded, 2, HALF_UP);
+    DecimalValue tmp(std::string(""100""));
+    DecimalValue result = rounded * tmp;
+    return do_money_format(context, result.to_string());
+}
+
+StringVal StringFunctions::money_format(FunctionContext *context, const BigIntVal &v) {
+    if (v.is_null) {
+        return StringVal::null();
+    }
+
+    std::string cent_money = std::to_string(v.val) + std::string(""00"");
+    return do_money_format(context, cent_money);
+}
+
+StringVal StringFunctions::money_format(FunctionContext *context, const LargeIntVal &v) {
+    if (v.is_null) {
+        return StringVal::null();
+    }
+
+    std::stringstream ss;
+    ss << v.val;
+    std::string cent_money = ss.str() + std::string(""00"");","[{'comment': '```suggestion\r\n    ss << v.val << "".00"";\r\n    std::string cent_money = ss.str();\r\n```', 'commenter': 'imay'}, {'comment': 'OK', 'commenter': 'kangkaisen'}]"
1064,be/test/exprs/string_functions_test.cpp,"@@ -0,0 +1,131 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#include ""exprs/string_functions.h""
+#include ""util/logging.h""
+#include ""exprs/anyval_util.h""
+#include <iostream>
+#include <string>
+
+#include <gtest/gtest.h>
+
+namespace doris {
+
+    class StringFunctionsTest : public testing::Test {","[{'comment': 'there is no indent here. It is different with Java', 'commenter': 'imay'}, {'comment': 'OK', 'commenter': 'kangkaisen'}]"
1070,be/src/exec/olap_table_info.h,"@@ -105,12 +105,21 @@ struct OlapTablePartition {
 class OlapTablePartKeyComparator {
 public:
     OlapTablePartKeyComparator(SlotDescriptor* slot_desc) : _slot_desc(slot_desc) { }
+    // return true if lhs < rhs
+    // 'nullptr' is max value, but 'null' is min value
     bool operator()(const Tuple* lhs, const Tuple* rhs) const {
         if (lhs == nullptr) {
             return false;
         } else if (rhs == nullptr) {
             return true;
         }
+
+        if (lhs->is_null(_slot_desc->null_indicator_offset())) {
+            return true;","[{'comment': 'if rhs->is_null() is false, this will return false', 'commenter': 'imay'}, {'comment': 'Changed', 'commenter': 'morningman'}]"
1070,be/src/exec/olap_table_info.h,"@@ -105,12 +105,25 @@ struct OlapTablePartition {
 class OlapTablePartKeyComparator {
 public:
     OlapTablePartKeyComparator(SlotDescriptor* slot_desc) : _slot_desc(slot_desc) { }
+    // return true if lhs < rhs
+    // 'nullptr' is max value, but 'null' is min value
     bool operator()(const Tuple* lhs, const Tuple* rhs) const {
         if (lhs == nullptr) {
             return false;
         } else if (rhs == nullptr) {
             return true;
         }
+
+        bool lhs_null = lhs->is_null(_slot_desc->null_indicator_offset());
+        bool rhs_null = rhs->is_null(_slot_desc->null_indicator_offset());
+        if (lhs_null && rhs_null) {
+            return false;
+        } else if (lhs_null) {
+            return true;
+        } else if (rhs_null) {
+            return false;
+        }
+","[{'comment': 'It exists some redundant if. you can change to \r\n\r\nif (rhs_null) {\r\n    return false;\r\n} else if (lhs_null) {\r\n   return true;\r\n}\r\n \r\nor \r\n\r\nif (lhs_null || rhs_null) {\r\n    return rhs_null? false : true;\r\n}', 'commenter': 'chenhao7253886'}]"
1075,be/src/olap/tablet_schema.cpp,"@@ -67,6 +73,9 @@ OLAPStatus TabletColumn::to_schema_pb(ColumnPB* column) {
     column->set_type(FieldInfo::get_string_by_field_type(_type));
     column->set_is_key(_is_key);
     column->set_is_nullable(_is_nullable);
+    if (_has_default_value) {","[{'comment': 'duplicate with line 83', 'commenter': 'kangpinghuang'}]"
1076,fe/src/main/java/org/apache/doris/load/loadv2/LoadManager.java,"@@ -0,0 +1,104 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ *
+ */
+
+package org.apache.doris.load.loadv2;
+
+import org.apache.doris.analysis.LoadStmt;
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.catalog.Database;
+import org.apache.doris.common.Config;
+import org.apache.doris.common.DdlException;
+import org.apache.doris.task.MasterTaskExecutor;
+
+import com.google.common.collect.Maps;
+
+import java.util.List;
+import java.util.Map;
+import java.util.concurrent.locks.ReentrantReadWriteLock;
+import java.util.stream.Collectors;
+
+/*
+This class contains all of broker and mini load job(v2).
+ */
+public class LoadManager {
+    private Map<Long, LoadJob> idToLoadJob = Maps.newConcurrentMap();
+    private MasterTaskExecutor pendingTaskExecutor =
+            new MasterTaskExecutor(Config.load_pending_thread_num_high_priority);
+    private MasterTaskExecutor loadingTaskExecutor = new MasterTaskExecutor(10);
+
+    private ReentrantReadWriteLock lock = new ReentrantReadWriteLock(true);
+
+    /*
+    Only broker stmt will call this method now.
+     */
+    public void createLoadJob(LoadStmt stmt) throws DdlException {
+        checkDb(stmt.getLabel().getDbName());
+        if (stmt.getBrokerDesc() != null) {","[{'comment': '```suggestion\r\n        if (stmt.getBrokerDesc() == null) {\r\n            throw new Exception()\r\n        }\r\n```', 'commenter': 'imay'}]"
1076,fe/src/main/java/org/apache/doris/load/loadv2/LoadPendingTask.java,"@@ -0,0 +1,58 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ *
+ */
+
+package org.apache.doris.load.loadv2;
+
+import org.apache.doris.common.UserException;
+import org.apache.doris.common.util.LogBuilder;
+import org.apache.doris.common.util.LogKey;
+import org.apache.doris.task.MasterTask;
+
+import org.apache.log4j.LogManager;
+import org.apache.log4j.Logger;
+
+public abstract class LoadPendingTask extends MasterTask {
+
+    private static final Logger LOG = LogManager.getLogger(LoadPendingTask.class);
+
+    protected LoadTaskCallback loadTaskCallback;
+    protected LoadPendingTaskAttachment attachment;
+
+    public LoadPendingTask(LoadTaskCallback loadTaskCallback) {
+        this.loadTaskCallback = loadTaskCallback;
+    }
+
+    @Override
+    protected void exec() {
+        try {
+            // execute pending task
+            executeTask();
+            // callback on pending task finished
+            loadTaskCallback.onPendingTaskFinished(attachment);
+        } catch (Exception e) {
+            LOG.warn(new LogBuilder(LogKey.LOAD_JOB, loadTaskCallback.getCallbackId())
+                             .add(""error_msg"", ""Failed to execute load pending task"").build(), e);
+            // callback on pending task failed
+            loadTaskCallback.onPendingTaskFailed(e.getMessage());","[{'comment': 'I think calling callback in finally bock is better', 'commenter': 'imay'}]"
1076,fe/src/main/java/org/apache/doris/load/loadv2/LoadLoadingTask.java,"@@ -0,0 +1,167 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ *
+ */
+
+package org.apache.doris.load.loadv2;
+
+import org.apache.doris.analysis.BrokerDesc;
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.catalog.Database;
+import org.apache.doris.catalog.OlapTable;
+import org.apache.doris.common.Status;
+import org.apache.doris.common.UserException;
+import org.apache.doris.common.util.LogBuilder;
+import org.apache.doris.common.util.LogKey;
+import org.apache.doris.load.BrokerFileGroup;
+import org.apache.doris.qe.Coordinator;
+import org.apache.doris.qe.QeProcessorImpl;
+import org.apache.doris.task.MasterTask;
+import org.apache.doris.thrift.TBrokerFileStatus;
+import org.apache.doris.thrift.TQueryType;
+import org.apache.doris.thrift.TUniqueId;
+import org.apache.doris.transaction.TabletCommitInfo;
+
+import com.google.common.collect.Maps;
+
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.List;
+import java.util.Map;
+import java.util.UUID;
+
+public class LoadLoadingTask extends MasterTask {
+    private static final Logger LOG = LogManager.getLogger(LoadLoadingTask.class);
+
+    private final Database db;
+    private final OlapTable table;
+    private final BrokerDesc brokerDesc;
+    private final List<BrokerFileGroup> fileGroups;
+    private final long jobDeadlineMs;
+    private final long execMemLimit;
+    private final long txnId;
+    private final LoadTaskCallback callback;
+    private boolean isFinished = false;
+
+    private LoadingTaskPlanner planner;
+
+    private LoadLoadingTaskAttachment attachment;
+    private String errMsg;
+
+    public LoadLoadingTask(Database db, OlapTable table,
+                           BrokerDesc brokerDesc, List<BrokerFileGroup> fileGroups,
+                           long jobDeadlineMs, long execMemLimit, long txnId, LoadTaskCallback callback) {
+        this.signature = Catalog.getInstance().getNextId();
+        this.db = db;
+        this.table = table;
+        this.brokerDesc = brokerDesc;
+        this.fileGroups = fileGroups;
+        this.jobDeadlineMs = jobDeadlineMs;
+        this.execMemLimit = execMemLimit;
+        this.txnId = txnId;
+        this.callback = callback;
+    }
+
+    public void init(List<List<TBrokerFileStatus>> fileStatusList, int fileNum) throws UserException {","[{'comment': 'caller should hold the db.lock', 'commenter': 'imay'}, {'comment': 'The db lock has been held in the BrokerLoadJob.', 'commenter': 'EmmyMiao87'}]"
1076,fe/src/main/java/org/apache/doris/load/loadv2/BrokerLoadingTaskAttachment.java,"@@ -0,0 +1,58 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ *
+ */
+
+package org.apache.doris.load.loadv2;
+
+import org.apache.doris.transaction.TabletCommitInfo;
+
+import java.util.List;
+import java.util.Map;
+
+public class BrokerLoadingTaskAttachment implements LoadLoadingTaskAttachment {
+
+    private Map<String, Long> fileMap;","[{'comment': 'Do we need thie `fileMap`', 'commenter': 'imay'}, {'comment': 'Plz add comment to explain the key and value of the map', 'commenter': 'morningman'}, {'comment': 'I will remove it later.', 'commenter': 'EmmyMiao87'}]"
1076,fe/src/main/java/org/apache/doris/load/loadv2/LoadManager.java,"@@ -0,0 +1,104 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ *
+ */
+
+package org.apache.doris.load.loadv2;
+
+import org.apache.doris.analysis.LoadStmt;
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.catalog.Database;
+import org.apache.doris.common.Config;
+import org.apache.doris.common.DdlException;
+import org.apache.doris.task.MasterTaskExecutor;
+
+import com.google.common.collect.Maps;
+
+import java.util.List;
+import java.util.Map;
+import java.util.concurrent.locks.ReentrantReadWriteLock;
+import java.util.stream.Collectors;
+
+/*
+This class contains all of broker and mini load job(v2).
+ */
+public class LoadManager {
+    private Map<Long, LoadJob> idToLoadJob = Maps.newConcurrentMap();
+    private MasterTaskExecutor pendingTaskExecutor =
+            new MasterTaskExecutor(Config.load_pending_thread_num_high_priority);
+    private MasterTaskExecutor loadingTaskExecutor = new MasterTaskExecutor(10);
+
+    private ReentrantReadWriteLock lock = new ReentrantReadWriteLock(true);","[{'comment': 'What is this lock for?', 'commenter': 'morningman'}, {'comment': 'To protect the atomicity of (idToLoadJob, dbIdToLabelToLoadJobs)', 'commenter': 'EmmyMiao87'}]"
1076,fe/src/main/java/org/apache/doris/load/loadv2/LoadScheduler.java,"@@ -0,0 +1,90 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ *
+ */
+
+package org.apache.doris.load.loadv2;
+
+import org.apache.doris.common.AnalysisException;
+import org.apache.doris.common.LabelAlreadyUsedException;
+import org.apache.doris.common.util.Daemon;
+import org.apache.doris.common.util.LogBuilder;
+import org.apache.doris.common.util.LogKey;
+import org.apache.doris.load.FailMsg;
+import org.apache.doris.transaction.BeginTransactionException;
+
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.List;
+
+/*
+LoadScheduler will schedule the pending LoadJob which belongs to LoadManager.
+Every pending job will be allocated one pending task.
+The pre-loading checking will be executed by pending task.
+If there is a pending task belong to pending job, job will not be rescheduled.
+ */
+public class LoadScheduler extends Daemon {
+
+    private static final Logger LOG = LogManager.getLogger(LoadScheduler.class);
+
+    private LoadManager loadManager;
+
+    public LoadScheduler(LoadManager loadManager) {
+        super();","[{'comment': 'Better adding a thread name for debugging easily', 'commenter': 'morningman'}]"
1076,fe/src/main/java/org/apache/doris/load/loadv2/LoadTaskCallback.java,"@@ -0,0 +1,33 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ *
+ */
+
+package org.apache.doris.load.loadv2;
+
+public interface LoadTaskCallback {","[{'comment': 'wen can make this callback more general.\r\n\r\nI think that define only `onTaskFinished` and `onTaskFailed` is enough and give job more options', 'commenter': 'imay'}]"
1076,fe/src/main/java/org/apache/doris/load/loadv2/LoadTask.java,"@@ -0,0 +1,74 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ *
+ */
+
+package org.apache.doris.load.loadv2;
+
+import org.apache.doris.common.UserException;
+import org.apache.doris.common.util.LogBuilder;
+import org.apache.doris.common.util.LogKey;
+import org.apache.doris.task.MasterTask;
+
+import org.apache.log4j.LogManager;
+import org.apache.log4j.Logger;
+
+public abstract class LoadTask extends MasterTask {
+
+    private static final Logger LOG = LogManager.getLogger(LoadTask.class);
+
+    protected LoadTaskCallback callback;
+    protected TaskAttachment attachment;
+    protected boolean isFinished = false;
+
+    public LoadTask(LoadTaskCallback callback) {
+        this.callback = callback;
+    }
+
+    @Override
+    protected void exec() {
+        Exception exception = null;
+        try {
+            // execute pending task
+            executeTask();
+            isFinished = true;
+            // callback on pending task finished
+            callback.onTaskFinished(attachment);
+        } catch (Exception e) {
+            exception = e;
+            LOG.warn(new LogBuilder(LogKey.LOAD_JOB, callback.getCallbackId())
+                             .add(""error_msg"", ""Failed to execute load task"").build(), e);
+        } finally {
+            if (!isFinished) {
+                // callback on pending task failed
+                callback.onTaskFailed(exception.getMessage());","[{'comment': 'If exception != null\r\n\r\n', 'commenter': 'imay'}]"
1076,fe/src/main/java/org/apache/doris/load/loadv2/BrokerLoadJob.java,"@@ -0,0 +1,229 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ *
+ */
+
+package org.apache.doris.load.loadv2;
+
+
+import org.apache.doris.analysis.BrokerDesc;
+import org.apache.doris.analysis.DataDescription;
+import org.apache.doris.analysis.LoadStmt;
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.catalog.Database;
+import org.apache.doris.catalog.OlapTable;
+import org.apache.doris.common.Config;
+import org.apache.doris.common.DdlException;
+import org.apache.doris.common.MetaNotFoundException;
+import org.apache.doris.common.UserException;
+import org.apache.doris.common.util.LogBuilder;
+import org.apache.doris.common.util.LogKey;
+import org.apache.doris.load.BrokerFileGroup;
+import org.apache.doris.load.FailMsg;
+import org.apache.doris.load.PullLoadSourceInfo;
+
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.List;
+import java.util.Map;
+
+public class BrokerLoadJob extends LoadJob {
+
+    private static final Logger LOG = LogManager.getLogger(BrokerLoadJob.class);
+
+    private BrokerDesc brokerDesc;
+    // include broker desc and data desc
+    private PullLoadSourceInfo dataSourceInfo = new PullLoadSourceInfo();
+
+    public BrokerLoadJob(long dbId, String label, BrokerDesc brokerDesc) {
+        super(dbId, label);
+        this.timeoutSecond = Config.pull_load_task_default_timeout_second;
+        this.brokerDesc = brokerDesc;
+    }
+
+    public static BrokerLoadJob fromLoadStmt(LoadStmt stmt) throws DdlException {
+        // get db id
+        String dbName = stmt.getLabel().getDbName();
+        Database db = Catalog.getInstance().getDb(stmt.getLabel().getDbName());
+        if (db == null) {
+            throw new DdlException(""Database["" + dbName + ""] does not exist"");
+        }
+
+        // create job
+        BrokerLoadJob brokerLoadJob = new BrokerLoadJob(db.getId(), stmt.getLabel().getLabelName(),
+                                                        stmt.getBrokerDesc());
+        brokerLoadJob.setJobProperties(stmt.getProperties());
+        brokerLoadJob.checkDataSourceInfo(db, stmt.getDataDescriptions());
+        brokerLoadJob.setDataSourceInfo(db, stmt.getDataDescriptions());
+        return brokerLoadJob;
+    }
+
+    private void setDataSourceInfo(Database db, List<DataDescription> dataDescriptions) throws DdlException {
+        for (DataDescription dataDescription : dataDescriptions) {
+            BrokerFileGroup fileGroup = new BrokerFileGroup(dataDescription);
+            fileGroup.parse(db);
+            dataSourceInfo.addFileGroup(fileGroup);
+        }
+    }
+
+    @Override
+    public void executeScheduleJob() {
+        LoadTask task = new BrokerLoadPendingTask(this, dataSourceInfo.getIdToFileGroups(), brokerDesc);
+        tasks.add(task);
+        Catalog.getCurrentCatalog().getLoadManager().submitTask(task);
+    }
+
+    @Override
+    public void onTaskFinished(TaskAttachment attachment) {
+        if (attachment instanceof BrokerPendingTaskAttachment) {
+            onPendingTaskFinished((BrokerPendingTaskAttachment) attachment);
+        } else if (attachment instanceof BrokerLoadingTaskAttachment) {
+            onLoadingTaskFinished((BrokerLoadingTaskAttachment) attachment);
+        }
+    }
+
+    @Override
+    public void onTaskFailed(String errMsg) {
+        updateState(JobState.CANCELLED, FailMsg.CancelType.LOAD_RUN_FAIL, errMsg);
+    }
+
+    /**
+     * step1: divide job into loading task
+     * step2: init the plan of task
+     * step3: submit tasks into loadingTaskExecutor
+     * @param attachment BrokerPendingTaskAttachment
+     */
+    public void onPendingTaskFinished(BrokerPendingTaskAttachment attachment) {","[{'comment': 'private?', 'commenter': 'imay'}]"
1076,fe/src/main/java/org/apache/doris/load/loadv2/BrokerLoadJob.java,"@@ -0,0 +1,229 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ *
+ */
+
+package org.apache.doris.load.loadv2;
+
+
+import org.apache.doris.analysis.BrokerDesc;
+import org.apache.doris.analysis.DataDescription;
+import org.apache.doris.analysis.LoadStmt;
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.catalog.Database;
+import org.apache.doris.catalog.OlapTable;
+import org.apache.doris.common.Config;
+import org.apache.doris.common.DdlException;
+import org.apache.doris.common.MetaNotFoundException;
+import org.apache.doris.common.UserException;
+import org.apache.doris.common.util.LogBuilder;
+import org.apache.doris.common.util.LogKey;
+import org.apache.doris.load.BrokerFileGroup;
+import org.apache.doris.load.FailMsg;
+import org.apache.doris.load.PullLoadSourceInfo;
+
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.List;
+import java.util.Map;
+
+public class BrokerLoadJob extends LoadJob {
+
+    private static final Logger LOG = LogManager.getLogger(BrokerLoadJob.class);
+
+    private BrokerDesc brokerDesc;
+    // include broker desc and data desc
+    private PullLoadSourceInfo dataSourceInfo = new PullLoadSourceInfo();
+
+    public BrokerLoadJob(long dbId, String label, BrokerDesc brokerDesc) {
+        super(dbId, label);
+        this.timeoutSecond = Config.pull_load_task_default_timeout_second;
+        this.brokerDesc = brokerDesc;
+    }
+
+    public static BrokerLoadJob fromLoadStmt(LoadStmt stmt) throws DdlException {
+        // get db id
+        String dbName = stmt.getLabel().getDbName();
+        Database db = Catalog.getInstance().getDb(stmt.getLabel().getDbName());
+        if (db == null) {
+            throw new DdlException(""Database["" + dbName + ""] does not exist"");
+        }
+
+        // create job
+        BrokerLoadJob brokerLoadJob = new BrokerLoadJob(db.getId(), stmt.getLabel().getLabelName(),
+                                                        stmt.getBrokerDesc());
+        brokerLoadJob.setJobProperties(stmt.getProperties());
+        brokerLoadJob.checkDataSourceInfo(db, stmt.getDataDescriptions());
+        brokerLoadJob.setDataSourceInfo(db, stmt.getDataDescriptions());
+        return brokerLoadJob;
+    }
+
+    private void setDataSourceInfo(Database db, List<DataDescription> dataDescriptions) throws DdlException {
+        for (DataDescription dataDescription : dataDescriptions) {
+            BrokerFileGroup fileGroup = new BrokerFileGroup(dataDescription);
+            fileGroup.parse(db);
+            dataSourceInfo.addFileGroup(fileGroup);
+        }
+    }
+
+    @Override
+    public void executeScheduleJob() {
+        LoadTask task = new BrokerLoadPendingTask(this, dataSourceInfo.getIdToFileGroups(), brokerDesc);
+        tasks.add(task);
+        Catalog.getCurrentCatalog().getLoadManager().submitTask(task);
+    }
+
+    @Override
+    public void onTaskFinished(TaskAttachment attachment) {","[{'comment': 'comment your state changes and what they do', 'commenter': 'imay'}]"
1076,fe/src/main/java/org/apache/doris/load/loadv2/BrokerLoadJob.java,"@@ -0,0 +1,229 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ *
+ */
+
+package org.apache.doris.load.loadv2;
+
+
+import org.apache.doris.analysis.BrokerDesc;
+import org.apache.doris.analysis.DataDescription;
+import org.apache.doris.analysis.LoadStmt;
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.catalog.Database;
+import org.apache.doris.catalog.OlapTable;
+import org.apache.doris.common.Config;
+import org.apache.doris.common.DdlException;
+import org.apache.doris.common.MetaNotFoundException;
+import org.apache.doris.common.UserException;
+import org.apache.doris.common.util.LogBuilder;
+import org.apache.doris.common.util.LogKey;
+import org.apache.doris.load.BrokerFileGroup;
+import org.apache.doris.load.FailMsg;
+import org.apache.doris.load.PullLoadSourceInfo;
+
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.List;
+import java.util.Map;
+
+public class BrokerLoadJob extends LoadJob {
+
+    private static final Logger LOG = LogManager.getLogger(BrokerLoadJob.class);
+
+    private BrokerDesc brokerDesc;
+    // include broker desc and data desc
+    private PullLoadSourceInfo dataSourceInfo = new PullLoadSourceInfo();
+
+    public BrokerLoadJob(long dbId, String label, BrokerDesc brokerDesc) {
+        super(dbId, label);
+        this.timeoutSecond = Config.pull_load_task_default_timeout_second;
+        this.brokerDesc = brokerDesc;
+    }
+
+    public static BrokerLoadJob fromLoadStmt(LoadStmt stmt) throws DdlException {
+        // get db id
+        String dbName = stmt.getLabel().getDbName();
+        Database db = Catalog.getInstance().getDb(stmt.getLabel().getDbName());
+        if (db == null) {
+            throw new DdlException(""Database["" + dbName + ""] does not exist"");
+        }
+
+        // create job
+        BrokerLoadJob brokerLoadJob = new BrokerLoadJob(db.getId(), stmt.getLabel().getLabelName(),
+                                                        stmt.getBrokerDesc());
+        brokerLoadJob.setJobProperties(stmt.getProperties());
+        brokerLoadJob.checkDataSourceInfo(db, stmt.getDataDescriptions());
+        brokerLoadJob.setDataSourceInfo(db, stmt.getDataDescriptions());
+        return brokerLoadJob;
+    }
+
+    private void setDataSourceInfo(Database db, List<DataDescription> dataDescriptions) throws DdlException {
+        for (DataDescription dataDescription : dataDescriptions) {
+            BrokerFileGroup fileGroup = new BrokerFileGroup(dataDescription);
+            fileGroup.parse(db);
+            dataSourceInfo.addFileGroup(fileGroup);
+        }
+    }
+
+    @Override
+    public void executeScheduleJob() {
+        LoadTask task = new BrokerLoadPendingTask(this, dataSourceInfo.getIdToFileGroups(), brokerDesc);
+        tasks.add(task);
+        Catalog.getCurrentCatalog().getLoadManager().submitTask(task);
+    }
+
+    @Override
+    public void onTaskFinished(TaskAttachment attachment) {
+        if (attachment instanceof BrokerPendingTaskAttachment) {
+            onPendingTaskFinished((BrokerPendingTaskAttachment) attachment);
+        } else if (attachment instanceof BrokerLoadingTaskAttachment) {
+            onLoadingTaskFinished((BrokerLoadingTaskAttachment) attachment);
+        }
+    }
+
+    @Override
+    public void onTaskFailed(String errMsg) {
+        updateState(JobState.CANCELLED, FailMsg.CancelType.LOAD_RUN_FAIL, errMsg);
+    }
+
+    /**
+     * step1: divide job into loading task
+     * step2: init the plan of task
+     * step3: submit tasks into loadingTaskExecutor
+     * @param attachment BrokerPendingTaskAttachment
+     */
+    public void onPendingTaskFinished(BrokerPendingTaskAttachment attachment) {
+        // TODO(ml): check if task has been cancelled
+        Database db = null;
+        try {
+            getDb();
+        } catch (MetaNotFoundException e) {
+            LOG.warn(new LogBuilder(LogKey.LOAD_JOB, id)
+                             .add(""database_id"", dbId)
+                             .add(""error_msg"", ""Failed to divide job into loading task when db not found."")
+                             .build(), e);
+            updateState(JobState.CANCELLED, FailMsg.CancelType.ETL_RUN_FAIL, ""db does not exist. id: "" + dbId);
+            return;
+        }
+
+        // divide job into broker loading task by table
+        db.readLock();
+        try {
+            for (Map.Entry<Long, List<BrokerFileGroup>> entry :
+                    dataSourceInfo.getIdToFileGroups().entrySet()) {
+                long tableId = entry.getKey();
+                OlapTable table = (OlapTable) db.getTable(tableId);
+                if (table == null) {
+                    LOG.warn(new LogBuilder(LogKey.LOAD_JOB, id)
+                                     .add(""database_id"", dbId)
+                                     .add(""table_id"", tableId)
+                                     .add(""error_msg"", ""Failed to divide job into loading task when table not found"")
+                                     .build());
+                    updateState(JobState.CANCELLED, FailMsg.CancelType.ETL_RUN_FAIL,
+                                ""Unknown table("" + tableId + "") in database("" + db.getFullName() + "")"");
+                    return;
+                }
+
+                // Generate loading task and init the plan of task
+                LoadLoadingTask task = new LoadLoadingTask(db, table, brokerDesc,
+                                                           entry.getValue(), getDeadlineMs(), execMemLimit,
+                                                           transactionId, this);
+                task.init(attachment.getFileStatusByTable(tableId),
+                          attachment.getFileNumByTable(tableId));
+                // Add tasks into list and pool
+                tasks.add(task);
+                Catalog.getCurrentCatalog().getLoadManager().submitTask(task);","[{'comment': 'Can you pass loadManager when this object is created?\r\nwe should avoid using getInstance() in our code', 'commenter': 'imay'}]"
1076,fe/src/main/java/org/apache/doris/load/loadv2/BrokerLoadJob.java,"@@ -0,0 +1,229 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ *
+ */
+
+package org.apache.doris.load.loadv2;
+
+
+import org.apache.doris.analysis.BrokerDesc;
+import org.apache.doris.analysis.DataDescription;
+import org.apache.doris.analysis.LoadStmt;
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.catalog.Database;
+import org.apache.doris.catalog.OlapTable;
+import org.apache.doris.common.Config;
+import org.apache.doris.common.DdlException;
+import org.apache.doris.common.MetaNotFoundException;
+import org.apache.doris.common.UserException;
+import org.apache.doris.common.util.LogBuilder;
+import org.apache.doris.common.util.LogKey;
+import org.apache.doris.load.BrokerFileGroup;
+import org.apache.doris.load.FailMsg;
+import org.apache.doris.load.PullLoadSourceInfo;
+
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.List;
+import java.util.Map;
+
+public class BrokerLoadJob extends LoadJob {
+
+    private static final Logger LOG = LogManager.getLogger(BrokerLoadJob.class);
+
+    private BrokerDesc brokerDesc;
+    // include broker desc and data desc
+    private PullLoadSourceInfo dataSourceInfo = new PullLoadSourceInfo();
+
+    public BrokerLoadJob(long dbId, String label, BrokerDesc brokerDesc) {
+        super(dbId, label);
+        this.timeoutSecond = Config.pull_load_task_default_timeout_second;
+        this.brokerDesc = brokerDesc;
+    }
+
+    public static BrokerLoadJob fromLoadStmt(LoadStmt stmt) throws DdlException {
+        // get db id
+        String dbName = stmt.getLabel().getDbName();
+        Database db = Catalog.getInstance().getDb(stmt.getLabel().getDbName());
+        if (db == null) {
+            throw new DdlException(""Database["" + dbName + ""] does not exist"");
+        }
+
+        // create job
+        BrokerLoadJob brokerLoadJob = new BrokerLoadJob(db.getId(), stmt.getLabel().getLabelName(),
+                                                        stmt.getBrokerDesc());
+        brokerLoadJob.setJobProperties(stmt.getProperties());
+        brokerLoadJob.checkDataSourceInfo(db, stmt.getDataDescriptions());
+        brokerLoadJob.setDataSourceInfo(db, stmt.getDataDescriptions());
+        return brokerLoadJob;
+    }
+
+    private void setDataSourceInfo(Database db, List<DataDescription> dataDescriptions) throws DdlException {
+        for (DataDescription dataDescription : dataDescriptions) {
+            BrokerFileGroup fileGroup = new BrokerFileGroup(dataDescription);
+            fileGroup.parse(db);
+            dataSourceInfo.addFileGroup(fileGroup);
+        }
+    }
+
+    @Override
+    public void executeScheduleJob() {
+        LoadTask task = new BrokerLoadPendingTask(this, dataSourceInfo.getIdToFileGroups(), brokerDesc);
+        tasks.add(task);
+        Catalog.getCurrentCatalog().getLoadManager().submitTask(task);
+    }
+
+    @Override
+    public void onTaskFinished(TaskAttachment attachment) {
+        if (attachment instanceof BrokerPendingTaskAttachment) {
+            onPendingTaskFinished((BrokerPendingTaskAttachment) attachment);
+        } else if (attachment instanceof BrokerLoadingTaskAttachment) {
+            onLoadingTaskFinished((BrokerLoadingTaskAttachment) attachment);
+        }","[{'comment': 'Do you have some `state` or something to indicate whether or not this call is valid in your state machine?\r\n\r\nFor example this function can be called concurrently? or called multiple times for same task?', 'commenter': 'imay'}, {'comment': 'Situation 1 (BrokerPendingTask ):There is only one pending task in BrokerLoad. So this function will not be called concurrently when pending task.\r\nSituation 2 (LoadingTask): I add the writelock before onLoadingTaskFinished. So the loading task will be serial executed.\r\n\r\n', 'commenter': 'EmmyMiao87'}]"
1076,fe/src/main/java/org/apache/doris/load/loadv2/BrokerLoadJob.java,"@@ -0,0 +1,229 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ *
+ */
+
+package org.apache.doris.load.loadv2;
+
+
+import org.apache.doris.analysis.BrokerDesc;
+import org.apache.doris.analysis.DataDescription;
+import org.apache.doris.analysis.LoadStmt;
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.catalog.Database;
+import org.apache.doris.catalog.OlapTable;
+import org.apache.doris.common.Config;
+import org.apache.doris.common.DdlException;
+import org.apache.doris.common.MetaNotFoundException;
+import org.apache.doris.common.UserException;
+import org.apache.doris.common.util.LogBuilder;
+import org.apache.doris.common.util.LogKey;
+import org.apache.doris.load.BrokerFileGroup;
+import org.apache.doris.load.FailMsg;
+import org.apache.doris.load.PullLoadSourceInfo;
+
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.List;
+import java.util.Map;
+
+public class BrokerLoadJob extends LoadJob {
+
+    private static final Logger LOG = LogManager.getLogger(BrokerLoadJob.class);
+
+    private BrokerDesc brokerDesc;
+    // include broker desc and data desc
+    private PullLoadSourceInfo dataSourceInfo = new PullLoadSourceInfo();
+
+    public BrokerLoadJob(long dbId, String label, BrokerDesc brokerDesc) {
+        super(dbId, label);
+        this.timeoutSecond = Config.pull_load_task_default_timeout_second;
+        this.brokerDesc = brokerDesc;
+    }
+
+    public static BrokerLoadJob fromLoadStmt(LoadStmt stmt) throws DdlException {
+        // get db id
+        String dbName = stmt.getLabel().getDbName();
+        Database db = Catalog.getInstance().getDb(stmt.getLabel().getDbName());
+        if (db == null) {
+            throw new DdlException(""Database["" + dbName + ""] does not exist"");
+        }
+
+        // create job
+        BrokerLoadJob brokerLoadJob = new BrokerLoadJob(db.getId(), stmt.getLabel().getLabelName(),
+                                                        stmt.getBrokerDesc());
+        brokerLoadJob.setJobProperties(stmt.getProperties());
+        brokerLoadJob.checkDataSourceInfo(db, stmt.getDataDescriptions());
+        brokerLoadJob.setDataSourceInfo(db, stmt.getDataDescriptions());
+        return brokerLoadJob;
+    }
+
+    private void setDataSourceInfo(Database db, List<DataDescription> dataDescriptions) throws DdlException {
+        for (DataDescription dataDescription : dataDescriptions) {
+            BrokerFileGroup fileGroup = new BrokerFileGroup(dataDescription);
+            fileGroup.parse(db);
+            dataSourceInfo.addFileGroup(fileGroup);
+        }
+    }
+
+    @Override
+    public void executeScheduleJob() {
+        LoadTask task = new BrokerLoadPendingTask(this, dataSourceInfo.getIdToFileGroups(), brokerDesc);
+        tasks.add(task);
+        Catalog.getCurrentCatalog().getLoadManager().submitTask(task);
+    }
+
+    @Override
+    public void onTaskFinished(TaskAttachment attachment) {
+        if (attachment instanceof BrokerPendingTaskAttachment) {
+            onPendingTaskFinished((BrokerPendingTaskAttachment) attachment);
+        } else if (attachment instanceof BrokerLoadingTaskAttachment) {
+            onLoadingTaskFinished((BrokerLoadingTaskAttachment) attachment);
+        }
+    }
+
+    @Override
+    public void onTaskFailed(String errMsg) {
+        updateState(JobState.CANCELLED, FailMsg.CancelType.LOAD_RUN_FAIL, errMsg);
+    }
+
+    /**
+     * step1: divide job into loading task
+     * step2: init the plan of task
+     * step3: submit tasks into loadingTaskExecutor
+     * @param attachment BrokerPendingTaskAttachment
+     */
+    public void onPendingTaskFinished(BrokerPendingTaskAttachment attachment) {
+        // TODO(ml): check if task has been cancelled
+        Database db = null;
+        try {
+            getDb();
+        } catch (MetaNotFoundException e) {
+            LOG.warn(new LogBuilder(LogKey.LOAD_JOB, id)
+                             .add(""database_id"", dbId)
+                             .add(""error_msg"", ""Failed to divide job into loading task when db not found."")
+                             .build(), e);
+            updateState(JobState.CANCELLED, FailMsg.CancelType.ETL_RUN_FAIL, ""db does not exist. id: "" + dbId);
+            return;
+        }
+
+        // divide job into broker loading task by table
+        db.readLock();
+        try {
+            for (Map.Entry<Long, List<BrokerFileGroup>> entry :
+                    dataSourceInfo.getIdToFileGroups().entrySet()) {
+                long tableId = entry.getKey();
+                OlapTable table = (OlapTable) db.getTable(tableId);
+                if (table == null) {
+                    LOG.warn(new LogBuilder(LogKey.LOAD_JOB, id)
+                                     .add(""database_id"", dbId)
+                                     .add(""table_id"", tableId)
+                                     .add(""error_msg"", ""Failed to divide job into loading task when table not found"")
+                                     .build());
+                    updateState(JobState.CANCELLED, FailMsg.CancelType.ETL_RUN_FAIL,
+                                ""Unknown table("" + tableId + "") in database("" + db.getFullName() + "")"");
+                    return;
+                }
+
+                // Generate loading task and init the plan of task
+                LoadLoadingTask task = new LoadLoadingTask(db, table, brokerDesc,
+                                                           entry.getValue(), getDeadlineMs(), execMemLimit,
+                                                           transactionId, this);
+                task.init(attachment.getFileStatusByTable(tableId),
+                          attachment.getFileNumByTable(tableId));
+                // Add tasks into list and pool
+                tasks.add(task);
+                Catalog.getCurrentCatalog().getLoadManager().submitTask(task);
+            }
+        } catch (UserException e) {
+            updateState(JobState.CANCELLED, FailMsg.CancelType.ETL_RUN_FAIL, ""failed to "" + e.getMessage());
+        } finally {
+            db.readUnlock();
+        }
+        loadStartTimestamp = System.currentTimeMillis();
+    }
+
+    public void onLoadingTaskFinished(BrokerLoadingTaskAttachment attachment) {
+        // TODO(ml): check if task has been cancelled
+        boolean commitTxn = false;
+        writeLock();
+        try {
+            // update loading status
+            updateLoadingStatus(attachment);
+
+            // begin commit txn when all of loading tasks have been finished
+            if (tasks.size() == tasks.stream()
+                    .filter(entity -> entity.isFinished()).count()) {","[{'comment': 'why not use a set to record which task has finished?', 'commenter': 'imay'}]"
1076,fe/src/main/java/org/apache/doris/load/loadv2/BrokerLoadJob.java,"@@ -0,0 +1,229 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ *
+ */
+
+package org.apache.doris.load.loadv2;
+
+
+import org.apache.doris.analysis.BrokerDesc;
+import org.apache.doris.analysis.DataDescription;
+import org.apache.doris.analysis.LoadStmt;
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.catalog.Database;
+import org.apache.doris.catalog.OlapTable;
+import org.apache.doris.common.Config;
+import org.apache.doris.common.DdlException;
+import org.apache.doris.common.MetaNotFoundException;
+import org.apache.doris.common.UserException;
+import org.apache.doris.common.util.LogBuilder;
+import org.apache.doris.common.util.LogKey;
+import org.apache.doris.load.BrokerFileGroup;
+import org.apache.doris.load.FailMsg;
+import org.apache.doris.load.PullLoadSourceInfo;
+
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.List;
+import java.util.Map;
+
+public class BrokerLoadJob extends LoadJob {
+
+    private static final Logger LOG = LogManager.getLogger(BrokerLoadJob.class);
+
+    private BrokerDesc brokerDesc;
+    // include broker desc and data desc
+    private PullLoadSourceInfo dataSourceInfo = new PullLoadSourceInfo();
+
+    public BrokerLoadJob(long dbId, String label, BrokerDesc brokerDesc) {
+        super(dbId, label);
+        this.timeoutSecond = Config.pull_load_task_default_timeout_second;
+        this.brokerDesc = brokerDesc;
+    }
+
+    public static BrokerLoadJob fromLoadStmt(LoadStmt stmt) throws DdlException {
+        // get db id
+        String dbName = stmt.getLabel().getDbName();
+        Database db = Catalog.getInstance().getDb(stmt.getLabel().getDbName());
+        if (db == null) {
+            throw new DdlException(""Database["" + dbName + ""] does not exist"");
+        }
+
+        // create job
+        BrokerLoadJob brokerLoadJob = new BrokerLoadJob(db.getId(), stmt.getLabel().getLabelName(),
+                                                        stmt.getBrokerDesc());
+        brokerLoadJob.setJobProperties(stmt.getProperties());
+        brokerLoadJob.checkDataSourceInfo(db, stmt.getDataDescriptions());
+        brokerLoadJob.setDataSourceInfo(db, stmt.getDataDescriptions());
+        return brokerLoadJob;
+    }
+
+    private void setDataSourceInfo(Database db, List<DataDescription> dataDescriptions) throws DdlException {
+        for (DataDescription dataDescription : dataDescriptions) {
+            BrokerFileGroup fileGroup = new BrokerFileGroup(dataDescription);
+            fileGroup.parse(db);
+            dataSourceInfo.addFileGroup(fileGroup);
+        }
+    }
+
+    @Override
+    public void executeScheduleJob() {
+        LoadTask task = new BrokerLoadPendingTask(this, dataSourceInfo.getIdToFileGroups(), brokerDesc);
+        tasks.add(task);
+        Catalog.getCurrentCatalog().getLoadManager().submitTask(task);
+    }
+
+    @Override
+    public void onTaskFinished(TaskAttachment attachment) {
+        if (attachment instanceof BrokerPendingTaskAttachment) {
+            onPendingTaskFinished((BrokerPendingTaskAttachment) attachment);
+        } else if (attachment instanceof BrokerLoadingTaskAttachment) {
+            onLoadingTaskFinished((BrokerLoadingTaskAttachment) attachment);
+        }
+    }
+
+    @Override
+    public void onTaskFailed(String errMsg) {
+        updateState(JobState.CANCELLED, FailMsg.CancelType.LOAD_RUN_FAIL, errMsg);
+    }
+
+    /**
+     * step1: divide job into loading task
+     * step2: init the plan of task
+     * step3: submit tasks into loadingTaskExecutor
+     * @param attachment BrokerPendingTaskAttachment
+     */
+    public void onPendingTaskFinished(BrokerPendingTaskAttachment attachment) {
+        // TODO(ml): check if task has been cancelled
+        Database db = null;
+        try {
+            getDb();
+        } catch (MetaNotFoundException e) {
+            LOG.warn(new LogBuilder(LogKey.LOAD_JOB, id)
+                             .add(""database_id"", dbId)
+                             .add(""error_msg"", ""Failed to divide job into loading task when db not found."")
+                             .build(), e);
+            updateState(JobState.CANCELLED, FailMsg.CancelType.ETL_RUN_FAIL, ""db does not exist. id: "" + dbId);
+            return;
+        }
+
+        // divide job into broker loading task by table
+        db.readLock();
+        try {
+            for (Map.Entry<Long, List<BrokerFileGroup>> entry :
+                    dataSourceInfo.getIdToFileGroups().entrySet()) {
+                long tableId = entry.getKey();
+                OlapTable table = (OlapTable) db.getTable(tableId);
+                if (table == null) {
+                    LOG.warn(new LogBuilder(LogKey.LOAD_JOB, id)
+                                     .add(""database_id"", dbId)
+                                     .add(""table_id"", tableId)
+                                     .add(""error_msg"", ""Failed to divide job into loading task when table not found"")
+                                     .build());
+                    updateState(JobState.CANCELLED, FailMsg.CancelType.ETL_RUN_FAIL,
+                                ""Unknown table("" + tableId + "") in database("" + db.getFullName() + "")"");
+                    return;
+                }
+
+                // Generate loading task and init the plan of task
+                LoadLoadingTask task = new LoadLoadingTask(db, table, brokerDesc,
+                                                           entry.getValue(), getDeadlineMs(), execMemLimit,
+                                                           transactionId, this);
+                task.init(attachment.getFileStatusByTable(tableId),
+                          attachment.getFileNumByTable(tableId));
+                // Add tasks into list and pool
+                tasks.add(task);
+                Catalog.getCurrentCatalog().getLoadManager().submitTask(task);
+            }
+        } catch (UserException e) {
+            updateState(JobState.CANCELLED, FailMsg.CancelType.ETL_RUN_FAIL, ""failed to "" + e.getMessage());
+        } finally {
+            db.readUnlock();
+        }
+        loadStartTimestamp = System.currentTimeMillis();
+    }
+
+    public void onLoadingTaskFinished(BrokerLoadingTaskAttachment attachment) {
+        // TODO(ml): check if task has been cancelled
+        boolean commitTxn = false;
+        writeLock();
+        try {
+            // update loading status
+            updateLoadingStatus(attachment);
+
+            // begin commit txn when all of loading tasks have been finished
+            if (tasks.size() == tasks.stream()
+                    .filter(entity -> entity.isFinished()).count()) {
+                // check data quality
+                if (!checkDataQuality()) {
+                    unprotectedUpdateState(JobState.CANCELLED, FailMsg.CancelType.ETL_QUALITY_UNSATISFIED,
+                                           QUALITY_FAIL_MSG);
+                } else {
+                    commitTxn = true;
+                }
+            }
+        } finally {
+            writeUnlock();
+        }
+
+        Database db = null;
+        try {
+            getDb();","[{'comment': 'db = getDb()?', 'commenter': 'imay'}]"
1076,fe/src/main/java/org/apache/doris/load/loadv2/LoadJobScheduler.java,"@@ -22,69 +22,71 @@
 
 import org.apache.doris.common.AnalysisException;
 import org.apache.doris.common.Config;
+import org.apache.doris.common.DdlException;
 import org.apache.doris.common.LabelAlreadyUsedException;
 import org.apache.doris.common.util.Daemon;
 import org.apache.doris.common.util.LogBuilder;
 import org.apache.doris.common.util.LogKey;
 import org.apache.doris.load.FailMsg;
 import org.apache.doris.transaction.BeginTransactionException;
 
+import com.google.common.collect.Queues;
+
 import org.apache.logging.log4j.LogManager;
 import org.apache.logging.log4j.Logger;
 
 import java.util.List;
+import java.util.concurrent.LinkedBlockingQueue;
 
 /**
  * LoadScheduler will schedule the pending LoadJob which belongs to LoadManager.
  * The function of scheduleJob, which is used to submit tasks, will be called in LoadScheduler.
  * The status of LoadJob will be changed to loading after LoadScheduler.
  */
-public class LoadScheduler extends Daemon {
-
-    private static final Logger LOG = LogManager.getLogger(LoadScheduler.class);
+public class LoadJobScheduler extends Daemon {
 
-    private LoadManager loadManager;
+    private static final Logger LOG = LogManager.getLogger(LoadJobScheduler.class);
+    private LinkedBlockingQueue<LoadJob> needScheduleJobs = Queues.newLinkedBlockingQueue();
 
-    public LoadScheduler(LoadManager loadManager) {
-        super(""Load scheduler"", Config.load_checker_interval_second * 1000L);
-        this.loadManager = loadManager;
+    public LoadJobScheduler() {
+        super(""Load job scheduler"", Config.load_checker_interval_second * 1000);
     }
 
     @Override
     protected void runOneCycle() {
         try {
             process();
         } catch (Throwable e) {
-            LOG.warn(""Failed to process one round of RoutineLoadScheduler with error message {}"", e.getMessage(), e);
+            LOG.warn(""Failed to process one round of LoadJobScheduler with error message {}"", e.getMessage(), e);
         }
     }
 
-    private void process() {
-        // fetch all of pending job without pending task in loadManager
-        List<LoadJob> loadJobList = loadManager.getLoadJobByState(JobState.PENDING);
+    private void process() throws InterruptedException {
+        while (!needScheduleJobs.isEmpty()) {","[{'comment': 'why not use timeout poll\r\n\r\nhttps://docs.oracle.com/javase/7/docs/api/java/util/concurrent/BlockingQueue.html#poll(long,%20java.util.concurrent.TimeUnit)', 'commenter': 'imay'}]"
1076,fe/src/main/java/org/apache/doris/load/loadv2/BrokerLoadJob.java,"@@ -0,0 +1,265 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ *
+ */
+
+package org.apache.doris.load.loadv2;
+
+
+import org.apache.doris.analysis.BrokerDesc;
+import org.apache.doris.analysis.DataDescription;
+import org.apache.doris.analysis.LoadStmt;
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.catalog.Database;
+import org.apache.doris.catalog.OlapTable;
+import org.apache.doris.common.Config;
+import org.apache.doris.common.DdlException;
+import org.apache.doris.common.UserException;
+import org.apache.doris.common.util.LogBuilder;
+import org.apache.doris.common.util.LogKey;
+import org.apache.doris.load.BrokerFileGroup;
+import org.apache.doris.load.FailMsg;
+import org.apache.doris.load.PullLoadSourceInfo;
+
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.List;
+import java.util.Map;
+
+/**
+ * There are 3 steps in BrokerLoadJob: BrokerPendingTask, LoadLoadingTask, CommitAndPublishTxn.
+ * Step1: BrokerPendingTask will be created on method of executeScheduleJob.
+ * Step2: LoadLoadingTasks will be created by the method of onTaskFinished when BrokerPendingTask is finished.
+ * Step3: CommitAndPublicTxn will be called by the method of onTaskFinished when all of LoadLoadingTasks are finished.
+ */
+public class BrokerLoadJob extends LoadJob {
+
+    private static final Logger LOG = LogManager.getLogger(BrokerLoadJob.class);
+
+    private BrokerDesc brokerDesc;
+    // include broker desc and data desc
+    private PullLoadSourceInfo dataSourceInfo = new PullLoadSourceInfo();
+
+    // it will be set to true when pending task finished
+    private boolean isLoading = false;
+
+    public BrokerLoadJob(long dbId, String label, BrokerDesc brokerDesc) {
+        super(dbId, label);
+        this.timeoutSecond = Config.pull_load_task_default_timeout_second;
+        this.brokerDesc = brokerDesc;
+    }
+
+    public static BrokerLoadJob fromLoadStmt(LoadStmt stmt) throws DdlException {
+        // get db id
+        String dbName = stmt.getLabel().getDbName();
+        Database db = Catalog.getCurrentCatalog().getDb(stmt.getLabel().getDbName());
+        if (db == null) {
+            throw new DdlException(""Database["" + dbName + ""] does not exist"");
+        }
+
+        // create job
+        BrokerLoadJob brokerLoadJob = new BrokerLoadJob(db.getId(), stmt.getLabel().getLabelName(),
+                                                        stmt.getBrokerDesc());
+        brokerLoadJob.setJobProperties(stmt.getProperties());
+        brokerLoadJob.checkDataSourceInfo(db, stmt.getDataDescriptions());
+        brokerLoadJob.setDataSourceInfo(db, stmt.getDataDescriptions());
+        return brokerLoadJob;
+    }
+
+    private void setDataSourceInfo(Database db, List<DataDescription> dataDescriptions) throws DdlException {
+        for (DataDescription dataDescription : dataDescriptions) {
+            BrokerFileGroup fileGroup = new BrokerFileGroup(dataDescription);
+            fileGroup.parse(db);
+            dataSourceInfo.addFileGroup(fileGroup);
+        }
+    }
+
+    @Override
+    public void executeScheduleJob() {
+        LoadTask task = new BrokerLoadPendingTask(this, dataSourceInfo.getIdToFileGroups(), brokerDesc);
+        tasks.add(task);
+        Catalog.getCurrentCatalog().getLoadTaskScheduler().submit(task);
+    }
+
+    /**
+     * Situation1: When attachment is instance of BrokerPendingTaskAttachment, this method is called by broker pending task.
+     * LoadLoadingTask will be created after BrokerPendingTask is finished.
+     * Situation2: When attachment is instance of BrokerLoadingTaskAttachment, this method is called by LoadLoadingTask.
+     * CommitTxn will be called after all of LoadingTasks are finished.
+     *
+     * @param attachment
+     */
+    @Override
+    public void onTaskFinished(TaskAttachment attachment) {
+        if (attachment instanceof BrokerPendingTaskAttachment) {
+            onPendingTaskFinished((BrokerPendingTaskAttachment) attachment);
+        } else if (attachment instanceof BrokerLoadingTaskAttachment) {
+            onLoadingTaskFinished((BrokerLoadingTaskAttachment) attachment);
+        }
+    }
+
+    @Override
+    public void onTaskFailed(String errMsg) {
+        cancelJobWithoutCheck(FailMsg.CancelType.LOAD_RUN_FAIL, errMsg);
+    }
+
+    /**
+     * step1: divide job into loading task
+     * step2: init the plan of task
+     * step3: submit tasks into loadingTaskExecutor
+     * @param attachment BrokerPendingTaskAttachment
+     */
+    private void onPendingTaskFinished(BrokerPendingTaskAttachment attachment) {
+        writeLock();
+        try {
+            // check if job has been cancelled
+            if (isFinished()) {
+                LOG.warn(new LogBuilder(LogKey.LOAD_JOB, id)
+                                 .add(""error_msg"", ""this task will be ignored when job is finished"")
+                                 .build());
+                return;
+            }
+            if (isLoading) {
+                LOG.warn(new LogBuilder(LogKey.LOAD_JOB, id)
+                                 .add(""error_msg"", ""this is a duplicated callback of pending task ""
+                                         + ""when broker already has loading task"")
+                                 .build());
+                return;
+            }
+            isLoading = true;
+        } finally {
+            writeUnlock();
+        }
+
+        Database db = null;
+        try {
+            db = getDb();
+            allocateLoadingTask(db, attachment);
+        } catch (UserException e) {
+            LOG.warn(new LogBuilder(LogKey.LOAD_JOB, id)
+                             .add(""database_id"", dbId)
+                             .add(""error_msg"", ""Failed to divide job into loading task."")
+                             .build(), e);
+            cancelJobWithoutCheck(FailMsg.CancelType.ETL_RUN_FAIL, e.getMessage());
+            return;
+        }
+
+        loadStartTimestamp = System.currentTimeMillis();
+    }
+
+    private void allocateLoadingTask(Database db, BrokerPendingTaskAttachment attachment) throws UserException {
+        // divide job into broker loading task by table
+        db.readLock();
+        try {
+            for (Map.Entry<Long, List<BrokerFileGroup>> entry :
+                    dataSourceInfo.getIdToFileGroups().entrySet()) {
+                long tableId = entry.getKey();
+                OlapTable table = (OlapTable) db.getTable(tableId);
+                if (table == null) {
+                    LOG.warn(new LogBuilder(LogKey.LOAD_JOB, id)
+                                     .add(""database_id"", dbId)
+                                     .add(""table_id"", tableId)
+                                     .add(""error_msg"", ""Failed to divide job into loading task when table not found"")
+                                     .build());
+                    cancelJobWithoutCheck(FailMsg.CancelType.ETL_RUN_FAIL,
+                                          ""Unknown table("" + tableId + "") in database("" + db.getFullName() + "")"");
+                    return;","[{'comment': 'throw Exception?', 'commenter': 'imay'}]"
1076,fe/src/main/java/org/apache/doris/load/loadv2/BrokerLoadJob.java,"@@ -0,0 +1,265 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ *
+ */
+
+package org.apache.doris.load.loadv2;
+
+
+import org.apache.doris.analysis.BrokerDesc;
+import org.apache.doris.analysis.DataDescription;
+import org.apache.doris.analysis.LoadStmt;
+import org.apache.doris.catalog.Catalog;
+import org.apache.doris.catalog.Database;
+import org.apache.doris.catalog.OlapTable;
+import org.apache.doris.common.Config;
+import org.apache.doris.common.DdlException;
+import org.apache.doris.common.UserException;
+import org.apache.doris.common.util.LogBuilder;
+import org.apache.doris.common.util.LogKey;
+import org.apache.doris.load.BrokerFileGroup;
+import org.apache.doris.load.FailMsg;
+import org.apache.doris.load.PullLoadSourceInfo;
+
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.List;
+import java.util.Map;
+
+/**
+ * There are 3 steps in BrokerLoadJob: BrokerPendingTask, LoadLoadingTask, CommitAndPublishTxn.
+ * Step1: BrokerPendingTask will be created on method of executeScheduleJob.
+ * Step2: LoadLoadingTasks will be created by the method of onTaskFinished when BrokerPendingTask is finished.
+ * Step3: CommitAndPublicTxn will be called by the method of onTaskFinished when all of LoadLoadingTasks are finished.
+ */
+public class BrokerLoadJob extends LoadJob {
+
+    private static final Logger LOG = LogManager.getLogger(BrokerLoadJob.class);
+
+    private BrokerDesc brokerDesc;
+    // include broker desc and data desc
+    private PullLoadSourceInfo dataSourceInfo = new PullLoadSourceInfo();
+
+    // it will be set to true when pending task finished
+    private boolean isLoading = false;
+
+    public BrokerLoadJob(long dbId, String label, BrokerDesc brokerDesc) {
+        super(dbId, label);
+        this.timeoutSecond = Config.pull_load_task_default_timeout_second;
+        this.brokerDesc = brokerDesc;
+    }
+
+    public static BrokerLoadJob fromLoadStmt(LoadStmt stmt) throws DdlException {
+        // get db id
+        String dbName = stmt.getLabel().getDbName();
+        Database db = Catalog.getCurrentCatalog().getDb(stmt.getLabel().getDbName());
+        if (db == null) {
+            throw new DdlException(""Database["" + dbName + ""] does not exist"");
+        }
+
+        // create job
+        BrokerLoadJob brokerLoadJob = new BrokerLoadJob(db.getId(), stmt.getLabel().getLabelName(),
+                                                        stmt.getBrokerDesc());
+        brokerLoadJob.setJobProperties(stmt.getProperties());
+        brokerLoadJob.checkDataSourceInfo(db, stmt.getDataDescriptions());
+        brokerLoadJob.setDataSourceInfo(db, stmt.getDataDescriptions());
+        return brokerLoadJob;
+    }
+
+    private void setDataSourceInfo(Database db, List<DataDescription> dataDescriptions) throws DdlException {
+        for (DataDescription dataDescription : dataDescriptions) {
+            BrokerFileGroup fileGroup = new BrokerFileGroup(dataDescription);
+            fileGroup.parse(db);
+            dataSourceInfo.addFileGroup(fileGroup);
+        }
+    }
+
+    @Override
+    public void executeScheduleJob() {
+        LoadTask task = new BrokerLoadPendingTask(this, dataSourceInfo.getIdToFileGroups(), brokerDesc);
+        tasks.add(task);
+        Catalog.getCurrentCatalog().getLoadTaskScheduler().submit(task);
+    }
+
+    /**
+     * Situation1: When attachment is instance of BrokerPendingTaskAttachment, this method is called by broker pending task.
+     * LoadLoadingTask will be created after BrokerPendingTask is finished.
+     * Situation2: When attachment is instance of BrokerLoadingTaskAttachment, this method is called by LoadLoadingTask.
+     * CommitTxn will be called after all of LoadingTasks are finished.
+     *
+     * @param attachment
+     */
+    @Override
+    public void onTaskFinished(TaskAttachment attachment) {
+        if (attachment instanceof BrokerPendingTaskAttachment) {
+            onPendingTaskFinished((BrokerPendingTaskAttachment) attachment);
+        } else if (attachment instanceof BrokerLoadingTaskAttachment) {
+            onLoadingTaskFinished((BrokerLoadingTaskAttachment) attachment);
+        }
+    }
+
+    @Override
+    public void onTaskFailed(String errMsg) {
+        cancelJobWithoutCheck(FailMsg.CancelType.LOAD_RUN_FAIL, errMsg);
+    }
+
+    /**
+     * step1: divide job into loading task
+     * step2: init the plan of task
+     * step3: submit tasks into loadingTaskExecutor
+     * @param attachment BrokerPendingTaskAttachment
+     */
+    private void onPendingTaskFinished(BrokerPendingTaskAttachment attachment) {
+        writeLock();
+        try {
+            // check if job has been cancelled
+            if (isFinished()) {
+                LOG.warn(new LogBuilder(LogKey.LOAD_JOB, id)
+                                 .add(""error_msg"", ""this task will be ignored when job is finished"")
+                                 .build());
+                return;
+            }
+            if (isLoading) {
+                LOG.warn(new LogBuilder(LogKey.LOAD_JOB, id)
+                                 .add(""error_msg"", ""this is a duplicated callback of pending task ""
+                                         + ""when broker already has loading task"")
+                                 .build());
+                return;
+            }
+            isLoading = true;
+        } finally {
+            writeUnlock();
+        }
+
+        Database db = null;
+        try {
+            db = getDb();
+            allocateLoadingTask(db, attachment);
+        } catch (UserException e) {
+            LOG.warn(new LogBuilder(LogKey.LOAD_JOB, id)
+                             .add(""database_id"", dbId)
+                             .add(""error_msg"", ""Failed to divide job into loading task."")
+                             .build(), e);
+            cancelJobWithoutCheck(FailMsg.CancelType.ETL_RUN_FAIL, e.getMessage());
+            return;
+        }
+
+        loadStartTimestamp = System.currentTimeMillis();
+    }
+
+    private void allocateLoadingTask(Database db, BrokerPendingTaskAttachment attachment) throws UserException {","[{'comment': '```suggestion\r\n    private void createLoadingTask(Database db, BrokerPendingTaskAttachment attachment) throws UserException {\r\n```', 'commenter': 'imay'}]"
1076,fe/src/main/java/org/apache/doris/load/loadv2/LoadJobScheduler.java,"@@ -0,0 +1,92 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ *
+ */
+
+package org.apache.doris.load.loadv2;
+
+import org.apache.doris.common.AnalysisException;
+import org.apache.doris.common.Config;
+import org.apache.doris.common.DdlException;
+import org.apache.doris.common.LabelAlreadyUsedException;
+import org.apache.doris.common.util.Daemon;
+import org.apache.doris.common.util.LogBuilder;
+import org.apache.doris.common.util.LogKey;
+import org.apache.doris.load.FailMsg;
+import org.apache.doris.transaction.BeginTransactionException;
+
+import com.google.common.collect.Queues;
+
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.List;
+import java.util.concurrent.LinkedBlockingQueue;
+
+/**
+ * LoadScheduler will schedule the pending LoadJob which belongs to LoadManager.
+ * The function of scheduleJob, which is used to submit tasks, will be called in LoadScheduler.
+ * The status of LoadJob will be changed to loading after LoadScheduler.
+ */
+public class LoadJobScheduler extends Daemon {
+
+    private static final Logger LOG = LogManager.getLogger(LoadJobScheduler.class);
+    private LinkedBlockingQueue<LoadJob> needScheduleJobs = Queues.newLinkedBlockingQueue();
+
+    public LoadJobScheduler() {
+        super(""Load job scheduler"", Config.load_checker_interval_second * 1000);
+    }
+
+    @Override
+    protected void runOneCycle() {
+        try {
+            process();
+        } catch (Throwable e) {
+            LOG.warn(""Failed to process one round of LoadJobScheduler with error message {}"", e.getMessage(), e);
+        }
+    }
+
+    private void process() throws InterruptedException {
+        while (!needScheduleJobs.isEmpty()) {
+            // take one load job from queue
+            LoadJob loadJob = needScheduleJobs.take();
+
+            // schedule job
+            try {
+                loadJob.scheduleJob();","[{'comment': 'scheduler call `scheduleJob`? a little strange', 'commenter': 'imay'}]"
1076,fe/src/main/java/org/apache/doris/load/loadv2/BrokerLoadPendingTask.java,"@@ -0,0 +1,85 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ *
+ */
+
+package org.apache.doris.load.loadv2;
+
+import org.apache.doris.analysis.BrokerDesc;
+import org.apache.doris.common.UserException;
+import org.apache.doris.common.util.BrokerUtil;
+import org.apache.doris.common.util.LogBuilder;
+import org.apache.doris.common.util.LogKey;
+import org.apache.doris.load.BrokerFileGroup;
+import org.apache.doris.thrift.TBrokerFileStatus;
+
+import com.google.common.collect.Lists;
+
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.util.List;
+import java.util.Map;
+
+public class BrokerLoadPendingTask extends LoadTask {
+
+    private static final Logger LOG = LogManager.getLogger(BrokerLoadPendingTask.class);
+
+    private Map<Long, List<BrokerFileGroup>> tableToBrokerFileList;
+    private BrokerDesc brokerDesc;
+
+    public BrokerLoadPendingTask(LoadTaskCallback loadTaskCallback,
+                                 Map<Long, List<BrokerFileGroup>> tableToBrokerFileList,
+                                 BrokerDesc brokerDesc) {
+        super(loadTaskCallback);
+        this.attachment = new BrokerPendingTaskAttachment();
+        this.tableToBrokerFileList = tableToBrokerFileList;
+        this.brokerDesc = brokerDesc;
+    }
+
+    @Override
+    void executeTask() throws UserException {
+        getAllFileStatus();
+    }
+
+    private void getAllFileStatus()
+            throws UserException {
+        for (Map.Entry<Long, List<BrokerFileGroup>> entry : tableToBrokerFileList.entrySet()) {
+            long tableId = entry.getKey();
+
+            List<List<TBrokerFileStatus>> fileStatusList = Lists.newArrayList();
+            List<BrokerFileGroup> fileGroups = entry.getValue();
+            for (BrokerFileGroup fileGroup : fileGroups) {
+                List<TBrokerFileStatus> fileStatuses = Lists.newArrayList();
+                for (String path : fileGroup.getFilePathes()) {
+                    BrokerUtil.parseBrokerFile(path, brokerDesc, fileStatuses);
+                }
+                fileStatusList.add(fileStatuses);
+                for (TBrokerFileStatus fstatus : fileStatuses) {
+                    if (LOG.isDebugEnabled()) {","[{'comment': 'put this judge out of `for` block? ', 'commenter': 'imay'}]"
1079,be/src/util/system_metrics.cpp,"@@ -435,4 +435,65 @@ void SystemMetrics::_update_fd_metrics() {
     fclose(fp);
 }
 
+int64_t SystemMetrics::get_max_io_util(
+        const std::map<std::string, int64_t>& lst_value, int64_t interval_sec) {
+    int64_t max = 0;
+    for (auto& it : _disk_metrics) {
+        int64_t cur = it.second->io_time_ms.value();
+        const auto& find = lst_value.find(it.first);","[{'comment': 'find return a temporary variable, do not use reference', 'commenter': 'imay'}, {'comment': 'ok', 'commenter': 'morningman'}]"
1079,be/src/util/system_metrics.h,"@@ -40,6 +41,19 @@ class SystemMetrics {
     // update metrics
     void update();
 
+    void get_disks_io_time(std::map<std::string, int64_t>& map);
+    int64_t get_max_io_util(
+        const std::map<std::string, int64_t>& lst_value, int64_t interval_sec);
+
+    void get_network_traffic(","[{'comment': 'output variable use pointer', 'commenter': 'imay'}, {'comment': 'there is not output variables in parameter list', 'commenter': 'morningman'}]"
1079,be/src/util/system_metrics.cpp,"@@ -435,4 +435,65 @@ void SystemMetrics::_update_fd_metrics() {
     fclose(fp);
 }
 
+int64_t SystemMetrics::get_max_io_util(
+        const std::map<std::string, int64_t>& lst_value, int64_t interval_sec) {
+    int64_t max = 0;
+    for (auto& it : _disk_metrics) {
+        int64_t cur = it.second->io_time_ms.value();
+        const auto& find = lst_value.find(it.first);
+        if (find == lst_value.end()) {
+            continue;
+        }
+        int64_t incr = cur - find->second;
+        if (incr > max) max = incr;
+    }
+    return max / interval_sec / 10;
+}
+
+void SystemMetrics::get_disks_io_time(std::map<std::string, int64_t>& map) {
+    map.clear();","[{'comment': 'I think that caller clear is better ', 'commenter': 'imay'}, {'comment': 'This method is MEAN to reset the map, so I think clear inside the method is ok', 'commenter': 'morningman'}]"
1079,be/src/common/daemon.cpp,"@@ -111,17 +112,27 @@ void* memory_maintenance_thread(void* dummy) {
  * this thread will calculate some metrics at a fix interval(15 sec)
  * 1. push bytes per second
  * 2. scan bytes per second
+ * 3. max io util of all disks
+ * 4. max network send bytes rate
+ * 5. max network receive bytes rate
  */
 void* calculate_metrics(void* dummy) {
     int64_t last_ts = -1L;
     int64_t lst_push_bytes = -1;
     int64_t lst_query_bytes = -1;
+    std::map<std::string, int64_t> lst_disks_io_time;
+    std::map<std::string, int64_t> lst_net_send_bytes;
+    std::map<std::string, int64_t> lst_net_receive_bytes;","[{'comment': 'you can put these map in `while` block to avoid clear', 'commenter': 'imay'}, {'comment': 'The values in these maps need to be used at next loop, so they can not be put in the while block.', 'commenter': 'morningman'}]"
1098,be/src/olap/tablet_meta.cpp,"@@ -181,6 +187,41 @@ OLAPStatus TabletMeta::create_from_file(const std::string& file_path) {
     return init_from_pb(tablet_meta_pb);
 }
 
+OLAPStatus TabletMeta::reset_tablet_uid(const std::string& file_path) {
+    OLAPStatus res = OLAP_SUCCESS;
+    TabletMeta tmp_tablet_meta;
+    if ((res = tmp_tablet_meta.create_from_file(file_path)) != OLAP_SUCCESS) {
+        LOG(WARNING) << ""fail to load tablet meta from file""
+                     << "", meta_file="" << file_path;
+        return res;
+    }
+    TabletMetaPB tmp_tablet_meta_pb;
+    res = tmp_tablet_meta.to_meta_pb(&tmp_tablet_meta_pb);
+    if (res != OLAP_SUCCESS) {
+        LOG(WARNING) << ""fail to serialize tablet meta to pb object. "" 
+                     << "" , meta_file="" << file_path;
+        return res;
+    }
+    *(tmp_tablet_meta_pb.mutable_tablet_uid()) = TabletUid().to_proto();
+    res = save(file_path, tmp_tablet_meta_pb);
+    if (res != OLAP_SUCCESS) {
+        LOG(WARNING) << ""fail to save tablet meta pb to "" ","[{'comment': 'LOG(FATAL)', 'commenter': 'chaoyli'}]"
1098,be/src/olap/tablet_meta.cpp,"@@ -216,8 +257,28 @@ OLAPStatus TabletMeta::save(const string& file_path, TabletMetaPB& tablet_meta_p
 }
 
 OLAPStatus TabletMeta::save_meta(DataDir* data_dir) {
+    WriteLock wrlock(&_meta_lock);
+    return _save_meta(data_dir);
+}
+
+OLAPStatus TabletMeta::_save_meta(DataDir* data_dir) {
     string meta_binary;
     serialize(&meta_binary);","[{'comment': 'serialize(&meta_binary) should be put after check logic codes', 'commenter': 'chaoyli'}]"
1098,be/src/olap/tablet_meta.cpp,"@@ -587,6 +653,37 @@ bool TabletMeta::version_for_delete_predicate(const Version& version) {
     return false;
 }
 
+OLAPStatus TabletMeta::get_next_rowset_id(RowsetId* gen_rowset_id, DataDir* data_dir) {
+    WriteLock wrlock(&_meta_lock);
+    if (_next_rowset_id >= _end_rowset_id) {
+        ++_next_rowset_id;
+        _end_rowset_id = _next_rowset_id + _batch_interval;
+        RETURN_NOT_OK(_save_meta(data_dir));
+    }
+    *gen_rowset_id = _next_rowset_id;
+    ++_next_rowset_id;
+    return OLAP_SUCCESS;
+}
+
+OLAPStatus TabletMeta::set_next_rowset_id(RowsetId new_rowset_id, DataDir* data_dir) {
+    WriteLock wrlock(&_meta_lock);
+    // must be < not <=
+    if (new_rowset_id < _next_rowset_id) {
+        return OLAP_SUCCESS;
+    }
+    if (new_rowset_id >= _end_rowset_id) {
+        _end_rowset_id = new_rowset_id + _batch_interval;
+        RETURN_NOT_OK(_save_meta(data_dir));
+    }
+    _next_rowset_id = new_rowset_id + 1;
+    return OLAP_SUCCESS;
+}
+
+RowsetId TabletMeta::get_previous_rowset_id() {","[{'comment': 'get_next_rowset_id() may be better?', 'commenter': 'chaoyli'}]"
1098,be/src/olap/data_dir.h,"@@ -89,15 +89,15 @@ class DataDir {
     void find_tablet_in_trash(int64_t tablet_id, std::vector<std::string>* paths);
 
     static std::string get_root_path_from_schema_hash_path_in_trash(const std::string& schema_hash_dir_in_trash);
-
+/*","[{'comment': 'remove it', 'commenter': 'chaoyli'}]"
1101,fe/src/main/java/org/apache/doris/load/Load.java,"@@ -734,6 +735,14 @@ public static void checkAndCreateSource(Database db, DataDescription dataDescrip
                 throw new DdlException(""Column has no default value. column: "" + columnName);
             }
 
+            // check negative for sum aggreate type","[{'comment': 'if (dataDescriptior.isNegative()) {\r\n}', 'commenter': 'imay'}]"
1107,docker/Dockerfile,"@@ -46,22 +46,16 @@ ARG CMAKE_DOWNLOAD_URL=https://cmake.org/files/v3.12/cmake-${CMAKE_VERSION}.tar.
 RUN mkdir -p /tmp/cmake && curl -fsSL -o /tmp/cmake.tar.gz ${CMAKE_DOWNLOAD_URL} \
     && tar -zxf /tmp/cmake.tar.gz -C /tmp/cmake --strip-components=1 \
     && cd /tmp/cmake \
-    && ./bootstrap --prefix=/usr/local \
-    && make -j$[$(nproc)/4+1] \
-    && make install \
+    && ./bootstrap --system-curl \
+    && gmake -j$[$(nproc)/4+1] \
+    && gmake install \
     && rm -rf /tmp/cmake.tar.gz \
     && rm -rf /tmp/cmake
 
 # install jdk
+COPY ./jdk.rpm ./","[{'comment': 'This operator needs to depend the local file which is not good for others', 'commenter': 'EmmyMiao87'}]"
1107,docker/Dockerfile,"@@ -107,5 +101,4 @@ RUN cd ${DEFAULT_DIR}/incubator-doris && /bin/bash thirdparty/build-thirdparty.s
     && mv ${DEFAULT_DIR}/incubator-doris/thirdparty/installed ${DEFAULT_DIR}/thirdparty/ \
     && rm -rf ${DEFAULT_DIR}/incubator-doris
 
-ENV DORIS_THIRDPARTY /var/local/thirdparty","[{'comment': 'why you drop this line?', 'commenter': 'imay'}]"
1107,thirdparty/build-thirdparty.sh,"@@ -517,6 +517,27 @@ build_librdkafka() {
     make -j$PARALLEL && make install
 }
 
+# arrow
+build_arrow() {
+    check_if_source_exist $ARROW_SOURCE
+
+    cd $TP_SOURCE_DIR/$ARROW_SOURCE/cpp
+
+    mkdir release && cd release && \
+    cmake .. -DARROW_PARQUET=ON -DARROW_OPTIONAL_INSTALL=ON -DBOOST_ROOT=$TP_INSTALL_DIR && \","[{'comment': '```suggestion\r\n    $CMAKE_CMD .. -DARROW_PARQUET=ON -DARROW_OPTIONAL_INSTALL=ON -DBOOST_ROOT=$TP_INSTALL_DIR && \\\r\n```\r\n\r\n', 'commenter': 'imay'}]"
1107,thirdparty/build-thirdparty.sh,"@@ -517,6 +517,27 @@ build_librdkafka() {
     make -j$PARALLEL && make install
 }
 
+# arrow
+build_arrow() {
+    check_if_source_exist $ARROW_SOURCE
+
+    cd $TP_SOURCE_DIR/$ARROW_SOURCE/cpp
+
+    mkdir release && cd release && \
+    cmake .. -DARROW_PARQUET=ON -DARROW_OPTIONAL_INSTALL=ON -DBOOST_ROOT=$TP_INSTALL_DIR && \
+    make -j$PARALLEL parquet && make install && \
+    cp -r /usr/local/include/arrow $TP_INSTALL_DIR/include/ && \","[{'comment': 'you can use -DCMAKE_INSTALL_PREFIX=$TP_INSTALL_DIR to avoid copy install yourself', 'commenter': 'imay'}]"
1107,be/src/util/debug_util.cpp,"@@ -65,12 +65,12 @@ namespace doris {
     }
 
 THRIFT_ENUM_OUTPUT_FN(TExprOpcode);
-THRIFT_ENUM_OUTPUT_FN(TAggregationOp);
-THRIFT_ENUM_OUTPUT_FN(THdfsFileFormat);
-THRIFT_ENUM_OUTPUT_FN(THdfsCompression);
-THRIFT_ENUM_OUTPUT_FN(TStmtType);
-THRIFT_ENUM_OUTPUT_FN(QueryState);
-THRIFT_ENUM_OUTPUT_FN(TAgentServiceVersion);
+//THRIFT_ENUM_OUTPUT_FN(TAggregationOp);","[{'comment': 'why comment this macro define?', 'commenter': 'imay'}]"
1113,fe/src/main/java/org/apache/doris/load/loadv2/LoadManager.java,"@@ -118,10 +129,53 @@ public void processTimeoutJobs() {
         idToLoadJob.values().stream().forEach(entity -> entity.processTimeout());
     }
 
+    public List<List<Comparable>> getLoadJobInfosByDb(long dbId, String labelValue,
+                                                      boolean accurateMatch, List<String> statesValue) {
+        LinkedList<List<Comparable>> loadJobInfos = new LinkedList<List<Comparable>>();
+        if (!dbIdToLabelToLoadJobs.containsKey(dbId)) {
+            return loadJobInfos;
+        }
+
+        List<JobState> states = Lists.newArrayList();
+        if (statesValue == null || statesValue.size() == 0) {
+            states.addAll(EnumSet.allOf(JobState.class));
+        } else {
+            for (String stateValue : statesValue) {
+                try {
+                    states.add(JobState.valueOf(stateValue));
+                } catch (IllegalArgumentException e) {
+                    // ignore this state
+                }
+            }
+        }
+
+        Map<String, List<LoadJob>> labelToLoadJobs = dbIdToLabelToLoadJobs.get(dbId);","[{'comment': 'you should hold lock when use this `labelToLoadJobs `, or other thread will change it concurrently', 'commenter': 'imay'}]"
1113,fe/src/main/java/org/apache/doris/load/loadv2/LoadManager.java,"@@ -118,10 +129,53 @@ public void processTimeoutJobs() {
         idToLoadJob.values().stream().forEach(entity -> entity.processTimeout());
     }
 
+    public List<List<Comparable>> getLoadJobInfosByDb(long dbId, String labelValue,
+                                                      boolean accurateMatch, List<String> statesValue) {
+        LinkedList<List<Comparable>> loadJobInfos = new LinkedList<List<Comparable>>();
+        if (!dbIdToLabelToLoadJobs.containsKey(dbId)) {
+            return loadJobInfos;
+        }
+
+        List<JobState> states = Lists.newArrayList();
+        if (statesValue == null || statesValue.size() == 0) {
+            states.addAll(EnumSet.allOf(JobState.class));
+        } else {
+            for (String stateValue : statesValue) {
+                try {
+                    states.add(JobState.valueOf(stateValue));
+                } catch (IllegalArgumentException e) {
+                    // ignore this state
+                }
+            }
+        }
+
+        Map<String, List<LoadJob>> labelToLoadJobs = dbIdToLabelToLoadJobs.get(dbId);
+        if (accurateMatch) {
+            if (!labelToLoadJobs.containsKey(labelValue)) {
+                return loadJobInfos;
+            }
+            loadJobInfos.addAll(labelToLoadJobs.get(labelValue).stream()
+                                        .filter(entity -> states.contains(entity.getState()))
+                                        .map(entity -> entity.getShowInfo()).collect(Collectors.toList()));
+            return loadJobInfos;
+        }
+        List<LoadJob> loadJobList = Lists.newArrayList();
+        for (Map.Entry<String, List<LoadJob>> entry : labelToLoadJobs.entrySet()) {
+            if (entry.getKey().contains(labelValue)) {
+                loadJobList.addAll(entry.getValue());","[{'comment': ""why don't do state check here?"", 'commenter': 'imay'}]"
1113,fe/src/main/java/org/apache/doris/load/loadv2/LoadJob.java,"@@ -83,6 +89,7 @@
     protected long transactionId;
     protected FailMsg failMsg;
     protected List<LoadTask> tasks = Lists.newArrayList();
+    protected List<Long> finishedTaskIds = Lists.newArrayList();","[{'comment': 'why list? I think set is more suitable', 'commenter': 'imay'}]"
1113,fe/src/main/java/org/apache/doris/load/loadv2/LoadManager.java,"@@ -118,10 +129,57 @@ public void processTimeoutJobs() {
         idToLoadJob.values().stream().forEach(entity -> entity.processTimeout());
     }
 
+    public List<List<Comparable>> getLoadJobInfosByDb(long dbId, String labelValue,
+                                                      boolean accurateMatch, List<String> statesValue) {
+        LinkedList<List<Comparable>> loadJobInfos = new LinkedList<List<Comparable>>();
+        if (!dbIdToLabelToLoadJobs.containsKey(dbId)) {
+            return loadJobInfos;
+        }
+
+        List<JobState> states = Lists.newArrayList();","[{'comment': 'Is set better than list?', 'commenter': 'imay'}]"
1113,fe/src/main/java/org/apache/doris/load/loadv2/LoadManager.java,"@@ -118,10 +129,57 @@ public void processTimeoutJobs() {
         idToLoadJob.values().stream().forEach(entity -> entity.processTimeout());
     }
 
+    public List<List<Comparable>> getLoadJobInfosByDb(long dbId, String labelValue,","[{'comment': 'add some comment for this function? Let people known what does this function do without reading code', 'commenter': 'imay'}]"
1126,fe/src/main/java/org/apache/doris/analysis/Expr.java,"@@ -1547,4 +1547,15 @@ public static Expr readIn(DataInput in) throws IOException {
     public boolean supportSerializable() {
         return false;
     }
+
+    /**
+     * For calculating expr.
+     * @return value returned can't be null, if this and it's children are't constant expr, return this.
+     * @throws AnalysisException
+     */
+    public Expr getResultValue() throws AnalysisException {
+        final Expr newExpr = ExpressionFunctions.INSTANCE.evalExpr(this);
+        return newExpr != null? newExpr : this;","[{'comment': '```suggestion\r\n        return newExpr != null ? newExpr : this;\r\n```', 'commenter': 'imay'}, {'comment': 'Ok. i will fix it.', 'commenter': 'chenhao7253886'}]"
1126,fe/src/main/java/org/apache/doris/analysis/CastExpr.java,"@@ -248,4 +248,33 @@ public boolean canHashPartition() {
         return false;
     }
 
+    @Override
+    public Expr getResultValue() throws AnalysisException {
+        final Expr value = children.get(0).getResultValue();
+        if (!(value instanceof LiteralExpr)) {
+            return this;
+        }
+
+        final Expr targetExpr = castTo((LiteralExpr)value);
+        return targetExpr;
+    }
+
+    private Expr castTo(LiteralExpr value) throws AnalysisException {","[{'comment': 'use `LiteralExpr.uncheckCast`', 'commenter': 'imay'}, {'comment': ""This can't use uncheckCast, it is used by impliclit cast and it will return new CastExpr for no supporting type, so it will cause dead loop."", 'commenter': 'chenhao7253886'}]"
1126,fe/src/main/java/org/apache/doris/analysis/ExpressionFunctions.java,"@@ -0,0 +1,224 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+package org.apache.doris.analysis;
+
+import com.google.common.base.Joiner;
+import com.google.common.base.Preconditions;
+import com.google.common.collect.ImmutableMultimap;
+import com.google.common.collect.ImmutableSet;
+import com.google.common.collect.Lists;
+import org.apache.doris.catalog.Function;
+import org.apache.doris.catalog.ScalarType;
+import org.apache.doris.catalog.Type;
+import org.apache.doris.common.AnalysisException;
+import org.apache.doris.rewrite.FEFunction;
+import org.apache.doris.rewrite.FEFunctions;
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.lang.reflect.InvocationTargetException;
+import java.lang.reflect.Method;
+import java.util.*;
+
+public enum ExpressionFunctions {
+    INSTANCE;
+
+    private static final Logger LOG = LogManager.getLogger(ExpressionFunctions.class);
+    private ImmutableMultimap<String, FEFunctionInvoker> functions;
+    // For most build-in functions, it will return NullLiteral when params contain NullLiteral.
+    // But a few functions need to handle NullLiteral differently, such as ""if"". It need to add
+    // an attribute to LiteralExpr to mark null and check the attribute to decide whether to
+    // replace the result with NullLiteral when function finished. It leaves to be realized.
+    // TODO chenhao16.
+    private ImmutableSet<String> nonNullResultWithNullParamFunctions;
+
+    private ExpressionFunctions() {
+        registerFunctions();
+    }
+
+    public Expr evalExpr(Expr constExpr) {
+        if (constExpr instanceof ArithmeticExpr
+                || constExpr instanceof FunctionCallExpr) {
+            Function fn = constExpr.getFn();
+            // unused cast
+            if (fn == null) {","[{'comment': 'unused fn?', 'commenter': 'imay'}, {'comment': 'Ok, i will fix it.', 'commenter': 'chenhao7253886'}]"
1131,docs/help/Contents/Administration/admin_stmt.md,"@@ -23,15 +23,15 @@
             ALTER SYSTEM SET LOAD ERRORS HUB PROPERTIES (""key"" = ""value""[, ...]);
 
     说明：
-        1) host 可以使主机名或者ip地址
+        1) host 可以是主机名或者ip地址
         2) heartbeat_port 为该节点的心跳端口
         3) 增加和删除节点为同步操作。这两种操作不考虑节点上已有的数据，节点直接从元数据中删除，请谨慎使用。
         4) 节点下线操作用于安全下线节点。该操作为异步操作。如果成功，节点最终会从元数据中删除。如果失败，则不会完成下线。
         5) 可以手动取消节点下线操作。详见 CANCEL DECOMMISSION
         6) Load error hub:
             当前支持两种类型的 Hub：Mysql 和 Broker。需在 PROPERTIES 中指定 ""type"" = ""mysql"" 或 ""type"" = ""broker""。
             如果需要删除当前的 load error hub，可以将 type 设为 null。
-            1) 当使用 Mysql 类型时，导入时产生的错误信息将会插入到指定的 mysql 库表中，之后可以通过 show load warnings 语句直接查看错误信息。
+            1) 当使用 Mysql 类型时，导入时产生的错误信息将会插入到指定的 mysql 库表中，之后可以通过 show load warnings; 语句直接查看错误信息。","[{'comment': 'Semicolon is ambiguous her. It is better to remove it.', 'commenter': 'chaoyli'}, {'comment': 'ok', 'commenter': 'mengqinghuan'}]"
1134,be/src/exec/broker_scan_node.cpp,"@@ -407,7 +407,6 @@ void BrokerScanNode::scanner_worker(int start_idx, int length) {
     }
 
     // Update stats
-    // _runtime_state->update_num_rows_load_success(counter.num_rows_returned);","[{'comment': 'You should remove `update_num_rows_load_success ` in RuntimeState, and its corresponding variables', 'commenter': 'imay'}]"
1138,fe/src/main/java/org/apache/doris/catalog/Tablet.java,"@@ -51,6 +51,7 @@
         HEALTHY,
         REPLICA_MISSING, // not enough alive replica num
         VERSION_INCOMPLETE, // alive replica num is enough, but version is missing
+        REPLICA_UNAVAILABLE, // replica is alive and version complete, but some BE of replicas may under decommission","[{'comment': 'change a name, what about RELOCATING?', 'commenter': 'imay'}]"
1139,fe/src/main/java/org/apache/doris/clone/ColocateTableBalancer.java,"@@ -507,111 +508,133 @@ private Long selectCloneBackendIdForRemove(com.google.common.collect.Table<Long,
     }
 
     /**
-     * balance after new backend added
-     *
-     * 1 compute the the number of bucket seqs need to move from the each old backend and
-     *   the number of bucket seqs need to move to the each new backend
-     * 2 select the clone target Backend for the new Replica
-     * 3 mark colocate group balancing
-     * 4 add a Migration Job
-     * 5 update the ColocateTableIndex's backendsPerBucketSeq
+     * 1 compute which bucket seq need to migrate, the migrate source backend, the migrate target backend
+     * 2 mark colocate group balancing in colocate meta
+     * 3 update colcate backendsPerBucketSeq meta
      *
      * For example:
-     * There are 3 backend and 4 tablet, and replicateNum is 3.
-     *
-     * the mapping from tablet to backend to is following:
-     *
-     * tablet1 : [1, 2, 3]
-     * tablet2 : [2, 1, 3]
-     * tablet3 : [3, 2, 1]
-     * tablet4 : [1, 2, 3]
-     *
-     * After Adding a new backend:
-     *
-     * the needMoveBucketSeqs  = 4 * 3 / (3 + 1) = 3
-     * the bucketSeqsPerNewBackend = 3 / 1 = 1
      *
-     * After balancing, the mapping from tablet to backend to is following:
-     *
-     * tablet1 : [4, 2, 3]
-     * tablet2 : [4, 1, 3]
-     * tablet3 : [4, 2, 1]
-     * tablet4 : [1, 2, 3]
+     * the old backendsPerBucketSeq is:
+     * [[1, 2, 3], [4, 1, 2], [3, 4, 1], [2, 3, 4], [1, 2, 3]]
+     * 
+     * after we add two new backends: [5, 6]
+     * 
+     * the balanced backendsPerBucketSeq will become:
+     * [[5, 6, 3], [6, 1, 2], [5, 4, 1], [2, 3, 4], [1, 2, 3]]
      *
      */
     private void balanceForBackendAdded(Long groupId, Database db, List<Long> addedBackendIds) {
         ColocateTableIndex colocateIndex = Catalog.getCurrentColocateIndex();
-        com.google.common.collect.Table<Long, Integer, Long> newGroup2BackendsPerBucketSeq = HashBasedTable.create();
-
         List<List<Long>> backendsPerBucketSeq = colocateIndex.getBackendsPerBucketSeq(groupId);
-        int replicateNum = backendsPerBucketSeq.get(0).size();
-        Set<Long> allGroupBackendIds = colocateIndex.getBackendsByGroup(groupId);
-
-        List<List<Long>> newBackendsPerBucketSeq = deepCopy(backendsPerBucketSeq);
-
-        int needMoveBucketSeqs = backendsPerBucketSeq.size() * replicateNum / (allGroupBackendIds.size() + addedBackendIds.size());
-        int bucketSeqsPerNewBackend = needMoveBucketSeqs / addedBackendIds.size();
-        LOG.info(""for colocate group {}, needMoveBucketSeqs : {} , bucketSeqPerNewBackend: {}"", groupId, needMoveBucketSeqs, bucketSeqsPerNewBackend);
 
         db.readLock();
         try {
-            List<Long> allTableIds = colocateIndex.getAllTableIds(groupId);
-            for (long tableId : allTableIds) {
-                OlapTable olapTable = (OlapTable) db.getTable(tableId);
-                for (Partition partition : olapTable.getPartitions()) {
-                    replicateNum = olapTable.getPartitionInfo().getReplicationNum(partition.getId());
-                    for (MaterializedIndex index : partition.getMaterializedIndices()) {
-                        List<Tablet> tablets = index.getTablets();
-                        for (int i = 0; i < tablets.size() && i < needMoveBucketSeqs; i++) {
-                            Tablet tablet = tablets.get(i);
-                            List<Replica> replicas = tablet.getReplicas();
-                            List<Long> sortedReplicaIds = sortReplicaId(replicas);
-                            //always delete replica which id is minimum
-                            Replica deleteReplica = tablet.getReplicaById(sortedReplicaIds.get(0));
+            List<List<Long>> newBackendsPerBucketSeq = balance(backendsPerBucketSeq, addedBackendIds);
+            markGroupBalancing(groupId);
+            persistBackendsToBucketSeqMeta(groupId, newBackendsPerBucketSeq);
+        } catch (Exception e) {
+            LOG.error(e.getMessage(), e);
+        } finally {
+            db.readUnlock();
+        }
+    }
 
-                            long tabletSizeB = deleteReplica.getDataSize() * partition.getMaterializedIndices().size()
-                             * olapTable.getPartitions().size() * allTableIds.size();
-                            CloneTabletInfo tabletInfo = new CloneTabletInfo(db.getId(), tableId, partition.getId(),
-                            index.getId(), tablet.getId(), (short) replicateNum, (short) replicateNum,
-                            tabletSizeB, tablet.getBackendIds());
+    /**
+     * Returns a map that the key is backend id, the value is backend counter
+     * The map will sort by counter in descending order
+     *
+     * @param backends the backend id list
+     * @return a descending sorted map
+     */
+    private static Map<Long, Long> getBackendCounter(List<Long> backends) {
+        Map<Long, Long> backendCounter = backends.stream()
+                .collect(Collectors.groupingBy(Function.identity(), Collectors.counting()));
 
-                            Long cloneReplicaBackendId = newGroup2BackendsPerBucketSeq.get(groupId, i);
-                            if (cloneReplicaBackendId == null) {
-                                // select dest backend
-                                cloneReplicaBackendId = addedBackendIds.get(i % addedBackendIds.size());
-                                newGroup2BackendsPerBucketSeq.put(groupId, i, cloneReplicaBackendId);
-                            }
+        return backendCounter.entrySet().stream().sorted(Collections.reverseOrder(Map.Entry.comparingByValue()))
+                .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue, (e1, e2) -> e1, LinkedHashMap::new));
+    }
 
-                            if (!colocateIndex.isGroupBalancing(groupId)) {
-                                colocateIndex.markGroupBalancing(groupId);
-                                ColocatePersistInfo info = ColocatePersistInfo.CreateForMarkBalancing(groupId);
-                                Catalog.getInstance().getEditLog().logColocateMarkBalancing(info);
-                            }
+    /**
+     * balance the bucket seq according to the new backends
+     *
+     * @param backendsPerBucketSeq the mapping from bucket seq to backend
+     * @param newBackends the new backends need to balance
+     * @return the balanced mapping from bucket seq to backend
+     */
+    public static List<List<Long>> balance(List<List<Long>> backendsPerBucketSeq, List<Long> newBackends) {
+        int replicateNum = backendsPerBucketSeq.get(0).size();
+        Set<Long> groupBackendSet = backendsPerBucketSeq.stream().flatMap(List::stream).collect(Collectors.toSet());
+        List<Long> groupBackendList = backendsPerBucketSeq.stream().flatMap(List::stream).collect(Collectors.toList());
+        Map<Long, Long> sortedBackendCounter = getBackendCounter(groupBackendList);
+
+        int allBackendSize = groupBackendSet.size() + newBackends.size();
+        //all backend should keep at least one replica","[{'comment': '```suggestion\r\n        // all backend should keep at least one replica\r\n```', 'commenter': 'imay'}, {'comment': 'done', 'commenter': 'kangkaisen'}]"
1139,fe/src/main/java/org/apache/doris/clone/ColocateTableBalancer.java,"@@ -507,111 +508,133 @@ private Long selectCloneBackendIdForRemove(com.google.common.collect.Table<Long,
     }
 
     /**
-     * balance after new backend added
-     *
-     * 1 compute the the number of bucket seqs need to move from the each old backend and
-     *   the number of bucket seqs need to move to the each new backend
-     * 2 select the clone target Backend for the new Replica
-     * 3 mark colocate group balancing
-     * 4 add a Migration Job
-     * 5 update the ColocateTableIndex's backendsPerBucketSeq
+     * 1 compute which bucket seq need to migrate, the migrate source backend, the migrate target backend
+     * 2 mark colocate group balancing in colocate meta
+     * 3 update colcate backendsPerBucketSeq meta
      *
      * For example:
-     * There are 3 backend and 4 tablet, and replicateNum is 3.
-     *
-     * the mapping from tablet to backend to is following:
-     *
-     * tablet1 : [1, 2, 3]
-     * tablet2 : [2, 1, 3]
-     * tablet3 : [3, 2, 1]
-     * tablet4 : [1, 2, 3]
-     *
-     * After Adding a new backend:
-     *
-     * the needMoveBucketSeqs  = 4 * 3 / (3 + 1) = 3
-     * the bucketSeqsPerNewBackend = 3 / 1 = 1
      *
-     * After balancing, the mapping from tablet to backend to is following:
-     *
-     * tablet1 : [4, 2, 3]
-     * tablet2 : [4, 1, 3]
-     * tablet3 : [4, 2, 1]
-     * tablet4 : [1, 2, 3]
+     * the old backendsPerBucketSeq is:
+     * [[1, 2, 3], [4, 1, 2], [3, 4, 1], [2, 3, 4], [1, 2, 3]]
+     * 
+     * after we add two new backends: [5, 6]
+     * 
+     * the balanced backendsPerBucketSeq will become:
+     * [[5, 6, 3], [6, 1, 2], [5, 4, 1], [2, 3, 4], [1, 2, 3]]
      *
      */
     private void balanceForBackendAdded(Long groupId, Database db, List<Long> addedBackendIds) {
         ColocateTableIndex colocateIndex = Catalog.getCurrentColocateIndex();
-        com.google.common.collect.Table<Long, Integer, Long> newGroup2BackendsPerBucketSeq = HashBasedTable.create();
-
         List<List<Long>> backendsPerBucketSeq = colocateIndex.getBackendsPerBucketSeq(groupId);
-        int replicateNum = backendsPerBucketSeq.get(0).size();
-        Set<Long> allGroupBackendIds = colocateIndex.getBackendsByGroup(groupId);
-
-        List<List<Long>> newBackendsPerBucketSeq = deepCopy(backendsPerBucketSeq);
-
-        int needMoveBucketSeqs = backendsPerBucketSeq.size() * replicateNum / (allGroupBackendIds.size() + addedBackendIds.size());
-        int bucketSeqsPerNewBackend = needMoveBucketSeqs / addedBackendIds.size();
-        LOG.info(""for colocate group {}, needMoveBucketSeqs : {} , bucketSeqPerNewBackend: {}"", groupId, needMoveBucketSeqs, bucketSeqsPerNewBackend);
 
         db.readLock();
         try {
-            List<Long> allTableIds = colocateIndex.getAllTableIds(groupId);
-            for (long tableId : allTableIds) {
-                OlapTable olapTable = (OlapTable) db.getTable(tableId);
-                for (Partition partition : olapTable.getPartitions()) {
-                    replicateNum = olapTable.getPartitionInfo().getReplicationNum(partition.getId());
-                    for (MaterializedIndex index : partition.getMaterializedIndices()) {
-                        List<Tablet> tablets = index.getTablets();
-                        for (int i = 0; i < tablets.size() && i < needMoveBucketSeqs; i++) {
-                            Tablet tablet = tablets.get(i);
-                            List<Replica> replicas = tablet.getReplicas();
-                            List<Long> sortedReplicaIds = sortReplicaId(replicas);
-                            //always delete replica which id is minimum
-                            Replica deleteReplica = tablet.getReplicaById(sortedReplicaIds.get(0));
+            List<List<Long>> newBackendsPerBucketSeq = balance(backendsPerBucketSeq, addedBackendIds);
+            markGroupBalancing(groupId);
+            persistBackendsToBucketSeqMeta(groupId, newBackendsPerBucketSeq);
+        } catch (Exception e) {
+            LOG.error(e.getMessage(), e);
+        } finally {
+            db.readUnlock();
+        }
+    }
 
-                            long tabletSizeB = deleteReplica.getDataSize() * partition.getMaterializedIndices().size()
-                             * olapTable.getPartitions().size() * allTableIds.size();
-                            CloneTabletInfo tabletInfo = new CloneTabletInfo(db.getId(), tableId, partition.getId(),
-                            index.getId(), tablet.getId(), (short) replicateNum, (short) replicateNum,
-                            tabletSizeB, tablet.getBackendIds());
+    /**
+     * Returns a map that the key is backend id, the value is backend counter
+     * The map will sort by counter in descending order
+     *
+     * @param backends the backend id list
+     * @return a descending sorted map
+     */
+    private static Map<Long, Long> getBackendCounter(List<Long> backends) {
+        Map<Long, Long> backendCounter = backends.stream()
+                .collect(Collectors.groupingBy(Function.identity(), Collectors.counting()));
 
-                            Long cloneReplicaBackendId = newGroup2BackendsPerBucketSeq.get(groupId, i);
-                            if (cloneReplicaBackendId == null) {
-                                // select dest backend
-                                cloneReplicaBackendId = addedBackendIds.get(i % addedBackendIds.size());
-                                newGroup2BackendsPerBucketSeq.put(groupId, i, cloneReplicaBackendId);
-                            }
+        return backendCounter.entrySet().stream().sorted(Collections.reverseOrder(Map.Entry.comparingByValue()))
+                .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue, (e1, e2) -> e1, LinkedHashMap::new));
+    }
 
-                            if (!colocateIndex.isGroupBalancing(groupId)) {
-                                colocateIndex.markGroupBalancing(groupId);
-                                ColocatePersistInfo info = ColocatePersistInfo.CreateForMarkBalancing(groupId);
-                                Catalog.getInstance().getEditLog().logColocateMarkBalancing(info);
-                            }
+    /**
+     * balance the bucket seq according to the new backends
+     *
+     * @param backendsPerBucketSeq the mapping from bucket seq to backend
+     * @param newBackends the new backends need to balance
+     * @return the balanced mapping from bucket seq to backend
+     */
+    public static List<List<Long>> balance(List<List<Long>> backendsPerBucketSeq, List<Long> newBackends) {
+        int replicateNum = backendsPerBucketSeq.get(0).size();
+        Set<Long> groupBackendSet = backendsPerBucketSeq.stream().flatMap(List::stream).collect(Collectors.toSet());
+        List<Long> groupBackendList = backendsPerBucketSeq.stream().flatMap(List::stream).collect(Collectors.toList());
+        Map<Long, Long> sortedBackendCounter = getBackendCounter(groupBackendList);
+
+        int allBackendSize = groupBackendSet.size() + newBackends.size();
+        //all backend should keep at least one replica
+        int avgReplicaNum = Math.max(groupBackendList.size() / allBackendSize, 1);
+        //the most balance case: all replica in all bucket seq have different backend
+        int needBalanceNum = Math.min(avgReplicaNum * newBackends.size(), groupBackendList.size() - groupBackendSet.size());","[{'comment': 'Why `needBalanceNum` can be computed like this? Can you write some comment?', 'commenter': 'imay'}, {'comment': 'has comment', 'commenter': 'kangkaisen'}]"
1139,fe/src/main/java/org/apache/doris/clone/ColocateTableBalancer.java,"@@ -507,111 +508,133 @@ private Long selectCloneBackendIdForRemove(com.google.common.collect.Table<Long,
     }
 
     /**
-     * balance after new backend added
-     *
-     * 1 compute the the number of bucket seqs need to move from the each old backend and
-     *   the number of bucket seqs need to move to the each new backend
-     * 2 select the clone target Backend for the new Replica
-     * 3 mark colocate group balancing
-     * 4 add a Migration Job
-     * 5 update the ColocateTableIndex's backendsPerBucketSeq
+     * 1 compute which bucket seq need to migrate, the migrate source backend, the migrate target backend
+     * 2 mark colocate group balancing in colocate meta
+     * 3 update colcate backendsPerBucketSeq meta
      *
      * For example:
-     * There are 3 backend and 4 tablet, and replicateNum is 3.
-     *
-     * the mapping from tablet to backend to is following:
-     *
-     * tablet1 : [1, 2, 3]
-     * tablet2 : [2, 1, 3]
-     * tablet3 : [3, 2, 1]
-     * tablet4 : [1, 2, 3]
-     *
-     * After Adding a new backend:
-     *
-     * the needMoveBucketSeqs  = 4 * 3 / (3 + 1) = 3
-     * the bucketSeqsPerNewBackend = 3 / 1 = 1
      *
-     * After balancing, the mapping from tablet to backend to is following:
-     *
-     * tablet1 : [4, 2, 3]
-     * tablet2 : [4, 1, 3]
-     * tablet3 : [4, 2, 1]
-     * tablet4 : [1, 2, 3]
+     * the old backendsPerBucketSeq is:
+     * [[1, 2, 3], [4, 1, 2], [3, 4, 1], [2, 3, 4], [1, 2, 3]]
+     * 
+     * after we add two new backends: [5, 6]
+     * 
+     * the balanced backendsPerBucketSeq will become:
+     * [[5, 6, 3], [6, 1, 2], [5, 4, 1], [2, 3, 4], [1, 2, 3]]
      *
      */
     private void balanceForBackendAdded(Long groupId, Database db, List<Long> addedBackendIds) {
         ColocateTableIndex colocateIndex = Catalog.getCurrentColocateIndex();
-        com.google.common.collect.Table<Long, Integer, Long> newGroup2BackendsPerBucketSeq = HashBasedTable.create();
-
         List<List<Long>> backendsPerBucketSeq = colocateIndex.getBackendsPerBucketSeq(groupId);
-        int replicateNum = backendsPerBucketSeq.get(0).size();
-        Set<Long> allGroupBackendIds = colocateIndex.getBackendsByGroup(groupId);
-
-        List<List<Long>> newBackendsPerBucketSeq = deepCopy(backendsPerBucketSeq);
-
-        int needMoveBucketSeqs = backendsPerBucketSeq.size() * replicateNum / (allGroupBackendIds.size() + addedBackendIds.size());
-        int bucketSeqsPerNewBackend = needMoveBucketSeqs / addedBackendIds.size();
-        LOG.info(""for colocate group {}, needMoveBucketSeqs : {} , bucketSeqPerNewBackend: {}"", groupId, needMoveBucketSeqs, bucketSeqsPerNewBackend);
 
         db.readLock();
         try {
-            List<Long> allTableIds = colocateIndex.getAllTableIds(groupId);
-            for (long tableId : allTableIds) {
-                OlapTable olapTable = (OlapTable) db.getTable(tableId);
-                for (Partition partition : olapTable.getPartitions()) {
-                    replicateNum = olapTable.getPartitionInfo().getReplicationNum(partition.getId());
-                    for (MaterializedIndex index : partition.getMaterializedIndices()) {
-                        List<Tablet> tablets = index.getTablets();
-                        for (int i = 0; i < tablets.size() && i < needMoveBucketSeqs; i++) {
-                            Tablet tablet = tablets.get(i);
-                            List<Replica> replicas = tablet.getReplicas();
-                            List<Long> sortedReplicaIds = sortReplicaId(replicas);
-                            //always delete replica which id is minimum
-                            Replica deleteReplica = tablet.getReplicaById(sortedReplicaIds.get(0));
+            List<List<Long>> newBackendsPerBucketSeq = balance(backendsPerBucketSeq, addedBackendIds);
+            markGroupBalancing(groupId);
+            persistBackendsToBucketSeqMeta(groupId, newBackendsPerBucketSeq);
+        } catch (Exception e) {
+            LOG.error(e.getMessage(), e);
+        } finally {
+            db.readUnlock();
+        }
+    }
 
-                            long tabletSizeB = deleteReplica.getDataSize() * partition.getMaterializedIndices().size()
-                             * olapTable.getPartitions().size() * allTableIds.size();
-                            CloneTabletInfo tabletInfo = new CloneTabletInfo(db.getId(), tableId, partition.getId(),
-                            index.getId(), tablet.getId(), (short) replicateNum, (short) replicateNum,
-                            tabletSizeB, tablet.getBackendIds());
+    /**
+     * Returns a map that the key is backend id, the value is backend counter
+     * The map will sort by counter in descending order
+     *
+     * @param backends the backend id list
+     * @return a descending sorted map
+     */
+    private static Map<Long, Long> getBackendCounter(List<Long> backends) {
+        Map<Long, Long> backendCounter = backends.stream()
+                .collect(Collectors.groupingBy(Function.identity(), Collectors.counting()));
 
-                            Long cloneReplicaBackendId = newGroup2BackendsPerBucketSeq.get(groupId, i);
-                            if (cloneReplicaBackendId == null) {
-                                // select dest backend
-                                cloneReplicaBackendId = addedBackendIds.get(i % addedBackendIds.size());
-                                newGroup2BackendsPerBucketSeq.put(groupId, i, cloneReplicaBackendId);
-                            }
+        return backendCounter.entrySet().stream().sorted(Collections.reverseOrder(Map.Entry.comparingByValue()))
+                .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue, (e1, e2) -> e1, LinkedHashMap::new));
+    }
 
-                            if (!colocateIndex.isGroupBalancing(groupId)) {
-                                colocateIndex.markGroupBalancing(groupId);
-                                ColocatePersistInfo info = ColocatePersistInfo.CreateForMarkBalancing(groupId);
-                                Catalog.getInstance().getEditLog().logColocateMarkBalancing(info);
-                            }
+    /**
+     * balance the bucket seq according to the new backends
+     *
+     * @param backendsPerBucketSeq the mapping from bucket seq to backend
+     * @param newBackends the new backends need to balance
+     * @return the balanced mapping from bucket seq to backend
+     */
+    public static List<List<Long>> balance(List<List<Long>> backendsPerBucketSeq, List<Long> newBackends) {
+        int replicateNum = backendsPerBucketSeq.get(0).size();
+        Set<Long> groupBackendSet = backendsPerBucketSeq.stream().flatMap(List::stream).collect(Collectors.toSet());
+        List<Long> groupBackendList = backendsPerBucketSeq.stream().flatMap(List::stream).collect(Collectors.toList());
+        Map<Long, Long> sortedBackendCounter = getBackendCounter(groupBackendList);
+
+        int allBackendSize = groupBackendSet.size() + newBackends.size();
+        //all backend should keep at least one replica
+        int avgReplicaNum = Math.max(groupBackendList.size() / allBackendSize, 1);
+        //the most balance case: all replica in all bucket seq have different backend
+        int needBalanceNum = Math.min(avgReplicaNum * newBackends.size(), groupBackendList.size() - groupBackendSet.size());
+
+        LOG.info(""avg ReplicaNum: "" + avgReplicaNum);
+        LOG.info(""need BalanceNum: "" + needBalanceNum);
+
+        int hasBalancedNum = 0;
+        //keep which BucketSeq will migrate to the new target backend
+        Map<Long, List<Integer>> targetBackendsToBucketSeqs = Maps.newHashMap();
+
+        while (hasBalancedNum < needBalanceNum) {
+            for(Map.Entry<Long, Long> beckendCounter: sortedBackendCounter.entrySet()) {
+                long count = beckendCounter.getValue();
+                long sourceBackend = beckendCounter.getKey();
+
+                //new backend should not as sourceBackend
+                if (newBackends.contains(sourceBackend)) {","[{'comment': 'does this situation exist? If this existed, is `int allBackendSize = groupBackendSet.size() + newBackends.size();` right?\r\n', 'commenter': 'imay'}, {'comment': 'has comment', 'commenter': 'kangkaisen'}]"
1139,fe/src/main/java/org/apache/doris/clone/ColocateTableBalancer.java,"@@ -507,111 +508,133 @@ private Long selectCloneBackendIdForRemove(com.google.common.collect.Table<Long,
     }
 
     /**
-     * balance after new backend added
-     *
-     * 1 compute the the number of bucket seqs need to move from the each old backend and
-     *   the number of bucket seqs need to move to the each new backend
-     * 2 select the clone target Backend for the new Replica
-     * 3 mark colocate group balancing
-     * 4 add a Migration Job
-     * 5 update the ColocateTableIndex's backendsPerBucketSeq
+     * 1 compute which bucket seq need to migrate, the migrate source backend, the migrate target backend
+     * 2 mark colocate group balancing in colocate meta
+     * 3 update colcate backendsPerBucketSeq meta
      *
      * For example:
-     * There are 3 backend and 4 tablet, and replicateNum is 3.
-     *
-     * the mapping from tablet to backend to is following:
-     *
-     * tablet1 : [1, 2, 3]
-     * tablet2 : [2, 1, 3]
-     * tablet3 : [3, 2, 1]
-     * tablet4 : [1, 2, 3]
-     *
-     * After Adding a new backend:
-     *
-     * the needMoveBucketSeqs  = 4 * 3 / (3 + 1) = 3
-     * the bucketSeqsPerNewBackend = 3 / 1 = 1
      *
-     * After balancing, the mapping from tablet to backend to is following:
-     *
-     * tablet1 : [4, 2, 3]
-     * tablet2 : [4, 1, 3]
-     * tablet3 : [4, 2, 1]
-     * tablet4 : [1, 2, 3]
+     * the old backendsPerBucketSeq is:
+     * [[1, 2, 3], [4, 1, 2], [3, 4, 1], [2, 3, 4], [1, 2, 3]]
+     * 
+     * after we add two new backends: [5, 6]
+     * 
+     * the balanced backendsPerBucketSeq will become:
+     * [[5, 6, 3], [6, 1, 2], [5, 4, 1], [2, 3, 4], [1, 2, 3]]
      *
      */
     private void balanceForBackendAdded(Long groupId, Database db, List<Long> addedBackendIds) {
         ColocateTableIndex colocateIndex = Catalog.getCurrentColocateIndex();
-        com.google.common.collect.Table<Long, Integer, Long> newGroup2BackendsPerBucketSeq = HashBasedTable.create();
-
         List<List<Long>> backendsPerBucketSeq = colocateIndex.getBackendsPerBucketSeq(groupId);
-        int replicateNum = backendsPerBucketSeq.get(0).size();
-        Set<Long> allGroupBackendIds = colocateIndex.getBackendsByGroup(groupId);
-
-        List<List<Long>> newBackendsPerBucketSeq = deepCopy(backendsPerBucketSeq);
-
-        int needMoveBucketSeqs = backendsPerBucketSeq.size() * replicateNum / (allGroupBackendIds.size() + addedBackendIds.size());
-        int bucketSeqsPerNewBackend = needMoveBucketSeqs / addedBackendIds.size();
-        LOG.info(""for colocate group {}, needMoveBucketSeqs : {} , bucketSeqPerNewBackend: {}"", groupId, needMoveBucketSeqs, bucketSeqsPerNewBackend);
 
         db.readLock();
         try {
-            List<Long> allTableIds = colocateIndex.getAllTableIds(groupId);
-            for (long tableId : allTableIds) {
-                OlapTable olapTable = (OlapTable) db.getTable(tableId);
-                for (Partition partition : olapTable.getPartitions()) {
-                    replicateNum = olapTable.getPartitionInfo().getReplicationNum(partition.getId());
-                    for (MaterializedIndex index : partition.getMaterializedIndices()) {
-                        List<Tablet> tablets = index.getTablets();
-                        for (int i = 0; i < tablets.size() && i < needMoveBucketSeqs; i++) {
-                            Tablet tablet = tablets.get(i);
-                            List<Replica> replicas = tablet.getReplicas();
-                            List<Long> sortedReplicaIds = sortReplicaId(replicas);
-                            //always delete replica which id is minimum
-                            Replica deleteReplica = tablet.getReplicaById(sortedReplicaIds.get(0));
+            List<List<Long>> newBackendsPerBucketSeq = balance(backendsPerBucketSeq, addedBackendIds);
+            markGroupBalancing(groupId);
+            persistBackendsToBucketSeqMeta(groupId, newBackendsPerBucketSeq);
+        } catch (Exception e) {
+            LOG.error(e.getMessage(), e);
+        } finally {
+            db.readUnlock();
+        }
+    }
 
-                            long tabletSizeB = deleteReplica.getDataSize() * partition.getMaterializedIndices().size()
-                             * olapTable.getPartitions().size() * allTableIds.size();
-                            CloneTabletInfo tabletInfo = new CloneTabletInfo(db.getId(), tableId, partition.getId(),
-                            index.getId(), tablet.getId(), (short) replicateNum, (short) replicateNum,
-                            tabletSizeB, tablet.getBackendIds());
+    /**
+     * Returns a map that the key is backend id, the value is backend counter
+     * The map will sort by counter in descending order
+     *
+     * @param backends the backend id list
+     * @return a descending sorted map
+     */
+    private static Map<Long, Long> getBackendCounter(List<Long> backends) {
+        Map<Long, Long> backendCounter = backends.stream()
+                .collect(Collectors.groupingBy(Function.identity(), Collectors.counting()));
 
-                            Long cloneReplicaBackendId = newGroup2BackendsPerBucketSeq.get(groupId, i);
-                            if (cloneReplicaBackendId == null) {
-                                // select dest backend
-                                cloneReplicaBackendId = addedBackendIds.get(i % addedBackendIds.size());
-                                newGroup2BackendsPerBucketSeq.put(groupId, i, cloneReplicaBackendId);
-                            }
+        return backendCounter.entrySet().stream().sorted(Collections.reverseOrder(Map.Entry.comparingByValue()))
+                .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue, (e1, e2) -> e1, LinkedHashMap::new));
+    }
 
-                            if (!colocateIndex.isGroupBalancing(groupId)) {
-                                colocateIndex.markGroupBalancing(groupId);
-                                ColocatePersistInfo info = ColocatePersistInfo.CreateForMarkBalancing(groupId);
-                                Catalog.getInstance().getEditLog().logColocateMarkBalancing(info);
-                            }
+    /**
+     * balance the bucket seq according to the new backends
+     *
+     * @param backendsPerBucketSeq the mapping from bucket seq to backend
+     * @param newBackends the new backends need to balance
+     * @return the balanced mapping from bucket seq to backend
+     */
+    public static List<List<Long>> balance(List<List<Long>> backendsPerBucketSeq, List<Long> newBackends) {
+        int replicateNum = backendsPerBucketSeq.get(0).size();
+        Set<Long> groupBackendSet = backendsPerBucketSeq.stream().flatMap(List::stream).collect(Collectors.toSet());
+        List<Long> groupBackendList = backendsPerBucketSeq.stream().flatMap(List::stream).collect(Collectors.toList());
+        Map<Long, Long> sortedBackendCounter = getBackendCounter(groupBackendList);
+
+        int allBackendSize = groupBackendSet.size() + newBackends.size();
+        //all backend should keep at least one replica
+        int avgReplicaNum = Math.max(groupBackendList.size() / allBackendSize, 1);
+        //the most balance case: all replica in all bucket seq have different backend
+        int needBalanceNum = Math.min(avgReplicaNum * newBackends.size(), groupBackendList.size() - groupBackendSet.size());
+
+        LOG.info(""avg ReplicaNum: "" + avgReplicaNum);
+        LOG.info(""need BalanceNum: "" + needBalanceNum);
+
+        int hasBalancedNum = 0;
+        //keep which BucketSeq will migrate to the new target backend
+        Map<Long, List<Integer>> targetBackendsToBucketSeqs = Maps.newHashMap();
+
+        while (hasBalancedNum < needBalanceNum) {
+            for(Map.Entry<Long, Long> beckendCounter: sortedBackendCounter.entrySet()) {
+                long count = beckendCounter.getValue();
+                long sourceBackend = beckendCounter.getKey();
+
+                //new backend should not as sourceBackend
+                if (newBackends.contains(sourceBackend)) {
+                    continue;
+                }
 
-                            //update ColocateTableIndex backendsPerBucketSeq
-                            List<Long> backends = newBackendsPerBucketSeq.get(i);
-                            backends.remove(deleteReplica.getBackendId());
-                            if (!backends.contains(cloneReplicaBackendId)) {
-                                backends.add(cloneReplicaBackendId);
-                            }
+                if (count > avgReplicaNum && hasBalancedNum < needBalanceNum) {","[{'comment': 'if hasBalancedNum < needBalanceNum we can break this for block', 'commenter': 'imay'}, {'comment': ""I don't get this point."", 'commenter': 'kangkaisen'}, {'comment': 'Sorry for my mistake. what I want to express is when `hasBalancedNum ` >= `needBalanceNum ` we can break all the `while` loop.', 'commenter': 'imay'}, {'comment': 'OK. I see. done.', 'commenter': 'kangkaisen'}]"
1139,fe/src/main/java/org/apache/doris/clone/ColocateTableBalancer.java,"@@ -507,111 +508,133 @@ private Long selectCloneBackendIdForRemove(com.google.common.collect.Table<Long,
     }
 
     /**
-     * balance after new backend added
-     *
-     * 1 compute the the number of bucket seqs need to move from the each old backend and
-     *   the number of bucket seqs need to move to the each new backend
-     * 2 select the clone target Backend for the new Replica
-     * 3 mark colocate group balancing
-     * 4 add a Migration Job
-     * 5 update the ColocateTableIndex's backendsPerBucketSeq
+     * 1 compute which bucket seq need to migrate, the migrate source backend, the migrate target backend
+     * 2 mark colocate group balancing in colocate meta
+     * 3 update colcate backendsPerBucketSeq meta
      *
      * For example:
-     * There are 3 backend and 4 tablet, and replicateNum is 3.
-     *
-     * the mapping from tablet to backend to is following:
-     *
-     * tablet1 : [1, 2, 3]
-     * tablet2 : [2, 1, 3]
-     * tablet3 : [3, 2, 1]
-     * tablet4 : [1, 2, 3]
-     *
-     * After Adding a new backend:
-     *
-     * the needMoveBucketSeqs  = 4 * 3 / (3 + 1) = 3
-     * the bucketSeqsPerNewBackend = 3 / 1 = 1
      *
-     * After balancing, the mapping from tablet to backend to is following:
-     *
-     * tablet1 : [4, 2, 3]
-     * tablet2 : [4, 1, 3]
-     * tablet3 : [4, 2, 1]
-     * tablet4 : [1, 2, 3]
+     * the old backendsPerBucketSeq is:
+     * [[1, 2, 3], [4, 1, 2], [3, 4, 1], [2, 3, 4], [1, 2, 3]]
+     * 
+     * after we add two new backends: [5, 6]
+     * 
+     * the balanced backendsPerBucketSeq will become:
+     * [[5, 6, 3], [6, 1, 2], [5, 4, 1], [2, 3, 4], [1, 2, 3]]
      *
      */
     private void balanceForBackendAdded(Long groupId, Database db, List<Long> addedBackendIds) {
         ColocateTableIndex colocateIndex = Catalog.getCurrentColocateIndex();
-        com.google.common.collect.Table<Long, Integer, Long> newGroup2BackendsPerBucketSeq = HashBasedTable.create();
-
         List<List<Long>> backendsPerBucketSeq = colocateIndex.getBackendsPerBucketSeq(groupId);
-        int replicateNum = backendsPerBucketSeq.get(0).size();
-        Set<Long> allGroupBackendIds = colocateIndex.getBackendsByGroup(groupId);
-
-        List<List<Long>> newBackendsPerBucketSeq = deepCopy(backendsPerBucketSeq);
-
-        int needMoveBucketSeqs = backendsPerBucketSeq.size() * replicateNum / (allGroupBackendIds.size() + addedBackendIds.size());
-        int bucketSeqsPerNewBackend = needMoveBucketSeqs / addedBackendIds.size();
-        LOG.info(""for colocate group {}, needMoveBucketSeqs : {} , bucketSeqPerNewBackend: {}"", groupId, needMoveBucketSeqs, bucketSeqsPerNewBackend);
 
         db.readLock();
         try {
-            List<Long> allTableIds = colocateIndex.getAllTableIds(groupId);
-            for (long tableId : allTableIds) {
-                OlapTable olapTable = (OlapTable) db.getTable(tableId);
-                for (Partition partition : olapTable.getPartitions()) {
-                    replicateNum = olapTable.getPartitionInfo().getReplicationNum(partition.getId());
-                    for (MaterializedIndex index : partition.getMaterializedIndices()) {
-                        List<Tablet> tablets = index.getTablets();
-                        for (int i = 0; i < tablets.size() && i < needMoveBucketSeqs; i++) {
-                            Tablet tablet = tablets.get(i);
-                            List<Replica> replicas = tablet.getReplicas();
-                            List<Long> sortedReplicaIds = sortReplicaId(replicas);
-                            //always delete replica which id is minimum
-                            Replica deleteReplica = tablet.getReplicaById(sortedReplicaIds.get(0));
+            List<List<Long>> newBackendsPerBucketSeq = balance(backendsPerBucketSeq, addedBackendIds);
+            markGroupBalancing(groupId);
+            persistBackendsToBucketSeqMeta(groupId, newBackendsPerBucketSeq);
+        } catch (Exception e) {
+            LOG.error(e.getMessage(), e);
+        } finally {
+            db.readUnlock();
+        }
+    }
 
-                            long tabletSizeB = deleteReplica.getDataSize() * partition.getMaterializedIndices().size()
-                             * olapTable.getPartitions().size() * allTableIds.size();
-                            CloneTabletInfo tabletInfo = new CloneTabletInfo(db.getId(), tableId, partition.getId(),
-                            index.getId(), tablet.getId(), (short) replicateNum, (short) replicateNum,
-                            tabletSizeB, tablet.getBackendIds());
+    /**
+     * Returns a map that the key is backend id, the value is backend counter
+     * The map will sort by counter in descending order
+     *
+     * @param backends the backend id list
+     * @return a descending sorted map
+     */
+    private static Map<Long, Long> getBackendCounter(List<Long> backends) {
+        Map<Long, Long> backendCounter = backends.stream()
+                .collect(Collectors.groupingBy(Function.identity(), Collectors.counting()));
 
-                            Long cloneReplicaBackendId = newGroup2BackendsPerBucketSeq.get(groupId, i);
-                            if (cloneReplicaBackendId == null) {
-                                // select dest backend
-                                cloneReplicaBackendId = addedBackendIds.get(i % addedBackendIds.size());
-                                newGroup2BackendsPerBucketSeq.put(groupId, i, cloneReplicaBackendId);
-                            }
+        return backendCounter.entrySet().stream().sorted(Collections.reverseOrder(Map.Entry.comparingByValue()))
+                .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue, (e1, e2) -> e1, LinkedHashMap::new));
+    }
 
-                            if (!colocateIndex.isGroupBalancing(groupId)) {
-                                colocateIndex.markGroupBalancing(groupId);
-                                ColocatePersistInfo info = ColocatePersistInfo.CreateForMarkBalancing(groupId);
-                                Catalog.getInstance().getEditLog().logColocateMarkBalancing(info);
-                            }
+    /**
+     * balance the bucket seq according to the new backends
+     *
+     * @param backendsPerBucketSeq the mapping from bucket seq to backend
+     * @param newBackends the new backends need to balance
+     * @return the balanced mapping from bucket seq to backend
+     */
+    public static List<List<Long>> balance(List<List<Long>> backendsPerBucketSeq, List<Long> newBackends) {
+        int replicateNum = backendsPerBucketSeq.get(0).size();
+        Set<Long> groupBackendSet = backendsPerBucketSeq.stream().flatMap(List::stream).collect(Collectors.toSet());
+        List<Long> groupBackendList = backendsPerBucketSeq.stream().flatMap(List::stream).collect(Collectors.toList());
+        Map<Long, Long> sortedBackendCounter = getBackendCounter(groupBackendList);
+
+        int allBackendSize = groupBackendSet.size() + newBackends.size();
+        //all backend should keep at least one replica
+        int avgReplicaNum = Math.max(groupBackendList.size() / allBackendSize, 1);
+        //the most balance case: all replica in all bucket seq have different backend
+        int needBalanceNum = Math.min(avgReplicaNum * newBackends.size(), groupBackendList.size() - groupBackendSet.size());
+
+        LOG.info(""avg ReplicaNum: "" + avgReplicaNum);
+        LOG.info(""need BalanceNum: "" + needBalanceNum);
+
+        int hasBalancedNum = 0;
+        //keep which BucketSeq will migrate to the new target backend
+        Map<Long, List<Integer>> targetBackendsToBucketSeqs = Maps.newHashMap();
+
+        while (hasBalancedNum < needBalanceNum) {
+            for(Map.Entry<Long, Long> beckendCounter: sortedBackendCounter.entrySet()) {
+                long count = beckendCounter.getValue();
+                long sourceBackend = beckendCounter.getKey();
+
+                //new backend should not as sourceBackend
+                if (newBackends.contains(sourceBackend)) {
+                    continue;
+                }
 
-                            //update ColocateTableIndex backendsPerBucketSeq
-                            List<Long> backends = newBackendsPerBucketSeq.get(i);
-                            backends.remove(deleteReplica.getBackendId());
-                            if (!backends.contains(cloneReplicaBackendId)) {
-                                backends.add(cloneReplicaBackendId);
-                            }
+                if (count > avgReplicaNum && hasBalancedNum < needBalanceNum) {
+                    Long targetBackend = newBackends.get (hasBalancedNum % newBackends.size());
+
+                    List<Integer> targetIndexes = IntStream.range(0, groupBackendList.size()).boxed()","[{'comment': 'I think changing `targetIndexes` to `sourceIndexes` is better for reader', 'commenter': 'imay'}, {'comment': 'done', 'commenter': 'kangkaisen'}]"
1139,fe/src/main/java/org/apache/doris/clone/ColocateTableBalancer.java,"@@ -507,111 +508,133 @@ private Long selectCloneBackendIdForRemove(com.google.common.collect.Table<Long,
     }
 
     /**
-     * balance after new backend added
-     *
-     * 1 compute the the number of bucket seqs need to move from the each old backend and
-     *   the number of bucket seqs need to move to the each new backend
-     * 2 select the clone target Backend for the new Replica
-     * 3 mark colocate group balancing
-     * 4 add a Migration Job
-     * 5 update the ColocateTableIndex's backendsPerBucketSeq
+     * 1 compute which bucket seq need to migrate, the migrate source backend, the migrate target backend
+     * 2 mark colocate group balancing in colocate meta
+     * 3 update colcate backendsPerBucketSeq meta
      *
      * For example:
-     * There are 3 backend and 4 tablet, and replicateNum is 3.
-     *
-     * the mapping from tablet to backend to is following:
-     *
-     * tablet1 : [1, 2, 3]
-     * tablet2 : [2, 1, 3]
-     * tablet3 : [3, 2, 1]
-     * tablet4 : [1, 2, 3]
-     *
-     * After Adding a new backend:
-     *
-     * the needMoveBucketSeqs  = 4 * 3 / (3 + 1) = 3
-     * the bucketSeqsPerNewBackend = 3 / 1 = 1
      *
-     * After balancing, the mapping from tablet to backend to is following:
-     *
-     * tablet1 : [4, 2, 3]
-     * tablet2 : [4, 1, 3]
-     * tablet3 : [4, 2, 1]
-     * tablet4 : [1, 2, 3]
+     * the old backendsPerBucketSeq is:
+     * [[1, 2, 3], [4, 1, 2], [3, 4, 1], [2, 3, 4], [1, 2, 3]]
+     * 
+     * after we add two new backends: [5, 6]
+     * 
+     * the balanced backendsPerBucketSeq will become:
+     * [[5, 6, 3], [6, 1, 2], [5, 4, 1], [2, 3, 4], [1, 2, 3]]
      *
      */
     private void balanceForBackendAdded(Long groupId, Database db, List<Long> addedBackendIds) {
         ColocateTableIndex colocateIndex = Catalog.getCurrentColocateIndex();
-        com.google.common.collect.Table<Long, Integer, Long> newGroup2BackendsPerBucketSeq = HashBasedTable.create();
-
         List<List<Long>> backendsPerBucketSeq = colocateIndex.getBackendsPerBucketSeq(groupId);
-        int replicateNum = backendsPerBucketSeq.get(0).size();
-        Set<Long> allGroupBackendIds = colocateIndex.getBackendsByGroup(groupId);
-
-        List<List<Long>> newBackendsPerBucketSeq = deepCopy(backendsPerBucketSeq);
-
-        int needMoveBucketSeqs = backendsPerBucketSeq.size() * replicateNum / (allGroupBackendIds.size() + addedBackendIds.size());
-        int bucketSeqsPerNewBackend = needMoveBucketSeqs / addedBackendIds.size();
-        LOG.info(""for colocate group {}, needMoveBucketSeqs : {} , bucketSeqPerNewBackend: {}"", groupId, needMoveBucketSeqs, bucketSeqsPerNewBackend);
 
         db.readLock();
         try {
-            List<Long> allTableIds = colocateIndex.getAllTableIds(groupId);
-            for (long tableId : allTableIds) {
-                OlapTable olapTable = (OlapTable) db.getTable(tableId);
-                for (Partition partition : olapTable.getPartitions()) {
-                    replicateNum = olapTable.getPartitionInfo().getReplicationNum(partition.getId());
-                    for (MaterializedIndex index : partition.getMaterializedIndices()) {
-                        List<Tablet> tablets = index.getTablets();
-                        for (int i = 0; i < tablets.size() && i < needMoveBucketSeqs; i++) {
-                            Tablet tablet = tablets.get(i);
-                            List<Replica> replicas = tablet.getReplicas();
-                            List<Long> sortedReplicaIds = sortReplicaId(replicas);
-                            //always delete replica which id is minimum
-                            Replica deleteReplica = tablet.getReplicaById(sortedReplicaIds.get(0));
+            List<List<Long>> newBackendsPerBucketSeq = balance(backendsPerBucketSeq, addedBackendIds);
+            markGroupBalancing(groupId);
+            persistBackendsToBucketSeqMeta(groupId, newBackendsPerBucketSeq);
+        } catch (Exception e) {
+            LOG.error(e.getMessage(), e);
+        } finally {
+            db.readUnlock();
+        }
+    }
 
-                            long tabletSizeB = deleteReplica.getDataSize() * partition.getMaterializedIndices().size()
-                             * olapTable.getPartitions().size() * allTableIds.size();
-                            CloneTabletInfo tabletInfo = new CloneTabletInfo(db.getId(), tableId, partition.getId(),
-                            index.getId(), tablet.getId(), (short) replicateNum, (short) replicateNum,
-                            tabletSizeB, tablet.getBackendIds());
+    /**
+     * Returns a map that the key is backend id, the value is backend counter
+     * The map will sort by counter in descending order
+     *
+     * @param backends the backend id list
+     * @return a descending sorted map
+     */
+    private static Map<Long, Long> getBackendCounter(List<Long> backends) {
+        Map<Long, Long> backendCounter = backends.stream()
+                .collect(Collectors.groupingBy(Function.identity(), Collectors.counting()));
 
-                            Long cloneReplicaBackendId = newGroup2BackendsPerBucketSeq.get(groupId, i);
-                            if (cloneReplicaBackendId == null) {
-                                // select dest backend
-                                cloneReplicaBackendId = addedBackendIds.get(i % addedBackendIds.size());
-                                newGroup2BackendsPerBucketSeq.put(groupId, i, cloneReplicaBackendId);
-                            }
+        return backendCounter.entrySet().stream().sorted(Collections.reverseOrder(Map.Entry.comparingByValue()))
+                .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue, (e1, e2) -> e1, LinkedHashMap::new));
+    }
 
-                            if (!colocateIndex.isGroupBalancing(groupId)) {
-                                colocateIndex.markGroupBalancing(groupId);
-                                ColocatePersistInfo info = ColocatePersistInfo.CreateForMarkBalancing(groupId);
-                                Catalog.getInstance().getEditLog().logColocateMarkBalancing(info);
-                            }
+    /**
+     * balance the bucket seq according to the new backends
+     *
+     * @param backendsPerBucketSeq the mapping from bucket seq to backend
+     * @param newBackends the new backends need to balance
+     * @return the balanced mapping from bucket seq to backend
+     */
+    public static List<List<Long>> balance(List<List<Long>> backendsPerBucketSeq, List<Long> newBackends) {
+        int replicateNum = backendsPerBucketSeq.get(0).size();
+        Set<Long> groupBackendSet = backendsPerBucketSeq.stream().flatMap(List::stream).collect(Collectors.toSet());
+        List<Long> groupBackendList = backendsPerBucketSeq.stream().flatMap(List::stream).collect(Collectors.toList());
+        Map<Long, Long> sortedBackendCounter = getBackendCounter(groupBackendList);
+
+        int allBackendSize = groupBackendSet.size() + newBackends.size();
+        //all backend should keep at least one replica
+        int avgReplicaNum = Math.max(groupBackendList.size() / allBackendSize, 1);
+        //the most balance case: all replica in all bucket seq have different backend
+        int needBalanceNum = Math.min(avgReplicaNum * newBackends.size(), groupBackendList.size() - groupBackendSet.size());
+
+        LOG.info(""avg ReplicaNum: "" + avgReplicaNum);
+        LOG.info(""need BalanceNum: "" + needBalanceNum);
+
+        int hasBalancedNum = 0;
+        //keep which BucketSeq will migrate to the new target backend
+        Map<Long, List<Integer>> targetBackendsToBucketSeqs = Maps.newHashMap();
+
+        while (hasBalancedNum < needBalanceNum) {
+            for(Map.Entry<Long, Long> beckendCounter: sortedBackendCounter.entrySet()) {
+                long count = beckendCounter.getValue();
+                long sourceBackend = beckendCounter.getKey();
+
+                //new backend should not as sourceBackend
+                if (newBackends.contains(sourceBackend)) {
+                    continue;
+                }
 
-                            //update ColocateTableIndex backendsPerBucketSeq
-                            List<Long> backends = newBackendsPerBucketSeq.get(i);
-                            backends.remove(deleteReplica.getBackendId());
-                            if (!backends.contains(cloneReplicaBackendId)) {
-                                backends.add(cloneReplicaBackendId);
-                            }
+                if (count > avgReplicaNum && hasBalancedNum < needBalanceNum) {
+                    Long targetBackend = newBackends.get (hasBalancedNum % newBackends.size());
+
+                    List<Integer> targetIndexes = IntStream.range(0, groupBackendList.size()).boxed()
+                            .filter(i -> groupBackendList.get(i).equals(sourceBackend))
+                            .collect(Collectors.toList());
+
+                    for(int targetIndex: targetIndexes) {
+                        int targetBucketSeq = targetIndex / replicateNum;","[{'comment': 'so this `targetBucketSeq ` to `sourceBucketSeq`', 'commenter': 'imay'}, {'comment': 'done', 'commenter': 'kangkaisen'}]"
1139,fe/src/main/java/org/apache/doris/clone/ColocateTableBalancer.java,"@@ -507,111 +508,133 @@ private Long selectCloneBackendIdForRemove(com.google.common.collect.Table<Long,
     }
 
     /**
-     * balance after new backend added
-     *
-     * 1 compute the the number of bucket seqs need to move from the each old backend and
-     *   the number of bucket seqs need to move to the each new backend
-     * 2 select the clone target Backend for the new Replica
-     * 3 mark colocate group balancing
-     * 4 add a Migration Job
-     * 5 update the ColocateTableIndex's backendsPerBucketSeq
+     * 1 compute which bucket seq need to migrate, the migrate source backend, the migrate target backend
+     * 2 mark colocate group balancing in colocate meta
+     * 3 update colcate backendsPerBucketSeq meta
      *
      * For example:
-     * There are 3 backend and 4 tablet, and replicateNum is 3.
-     *
-     * the mapping from tablet to backend to is following:
-     *
-     * tablet1 : [1, 2, 3]
-     * tablet2 : [2, 1, 3]
-     * tablet3 : [3, 2, 1]
-     * tablet4 : [1, 2, 3]
-     *
-     * After Adding a new backend:
-     *
-     * the needMoveBucketSeqs  = 4 * 3 / (3 + 1) = 3
-     * the bucketSeqsPerNewBackend = 3 / 1 = 1
      *
-     * After balancing, the mapping from tablet to backend to is following:
-     *
-     * tablet1 : [4, 2, 3]
-     * tablet2 : [4, 1, 3]
-     * tablet3 : [4, 2, 1]
-     * tablet4 : [1, 2, 3]
+     * the old backendsPerBucketSeq is:
+     * [[1, 2, 3], [4, 1, 2], [3, 4, 1], [2, 3, 4], [1, 2, 3]]
+     * 
+     * after we add two new backends: [5, 6]
+     * 
+     * the balanced backendsPerBucketSeq will become:
+     * [[5, 6, 3], [6, 1, 2], [5, 4, 1], [2, 3, 4], [1, 2, 3]]
      *
      */
     private void balanceForBackendAdded(Long groupId, Database db, List<Long> addedBackendIds) {
         ColocateTableIndex colocateIndex = Catalog.getCurrentColocateIndex();
-        com.google.common.collect.Table<Long, Integer, Long> newGroup2BackendsPerBucketSeq = HashBasedTable.create();
-
         List<List<Long>> backendsPerBucketSeq = colocateIndex.getBackendsPerBucketSeq(groupId);
-        int replicateNum = backendsPerBucketSeq.get(0).size();
-        Set<Long> allGroupBackendIds = colocateIndex.getBackendsByGroup(groupId);
-
-        List<List<Long>> newBackendsPerBucketSeq = deepCopy(backendsPerBucketSeq);
-
-        int needMoveBucketSeqs = backendsPerBucketSeq.size() * replicateNum / (allGroupBackendIds.size() + addedBackendIds.size());
-        int bucketSeqsPerNewBackend = needMoveBucketSeqs / addedBackendIds.size();
-        LOG.info(""for colocate group {}, needMoveBucketSeqs : {} , bucketSeqPerNewBackend: {}"", groupId, needMoveBucketSeqs, bucketSeqsPerNewBackend);
 
         db.readLock();
         try {
-            List<Long> allTableIds = colocateIndex.getAllTableIds(groupId);
-            for (long tableId : allTableIds) {
-                OlapTable olapTable = (OlapTable) db.getTable(tableId);
-                for (Partition partition : olapTable.getPartitions()) {
-                    replicateNum = olapTable.getPartitionInfo().getReplicationNum(partition.getId());
-                    for (MaterializedIndex index : partition.getMaterializedIndices()) {
-                        List<Tablet> tablets = index.getTablets();
-                        for (int i = 0; i < tablets.size() && i < needMoveBucketSeqs; i++) {
-                            Tablet tablet = tablets.get(i);
-                            List<Replica> replicas = tablet.getReplicas();
-                            List<Long> sortedReplicaIds = sortReplicaId(replicas);
-                            //always delete replica which id is minimum
-                            Replica deleteReplica = tablet.getReplicaById(sortedReplicaIds.get(0));
+            List<List<Long>> newBackendsPerBucketSeq = balance(backendsPerBucketSeq, addedBackendIds);
+            markGroupBalancing(groupId);
+            persistBackendsToBucketSeqMeta(groupId, newBackendsPerBucketSeq);
+        } catch (Exception e) {
+            LOG.error(e.getMessage(), e);
+        } finally {
+            db.readUnlock();
+        }
+    }
 
-                            long tabletSizeB = deleteReplica.getDataSize() * partition.getMaterializedIndices().size()
-                             * olapTable.getPartitions().size() * allTableIds.size();
-                            CloneTabletInfo tabletInfo = new CloneTabletInfo(db.getId(), tableId, partition.getId(),
-                            index.getId(), tablet.getId(), (short) replicateNum, (short) replicateNum,
-                            tabletSizeB, tablet.getBackendIds());
+    /**
+     * Returns a map that the key is backend id, the value is backend counter
+     * The map will sort by counter in descending order
+     *
+     * @param backends the backend id list
+     * @return a descending sorted map
+     */
+    private static Map<Long, Long> getBackendCounter(List<Long> backends) {
+        Map<Long, Long> backendCounter = backends.stream()
+                .collect(Collectors.groupingBy(Function.identity(), Collectors.counting()));
 
-                            Long cloneReplicaBackendId = newGroup2BackendsPerBucketSeq.get(groupId, i);
-                            if (cloneReplicaBackendId == null) {
-                                // select dest backend
-                                cloneReplicaBackendId = addedBackendIds.get(i % addedBackendIds.size());
-                                newGroup2BackendsPerBucketSeq.put(groupId, i, cloneReplicaBackendId);
-                            }
+        return backendCounter.entrySet().stream().sorted(Collections.reverseOrder(Map.Entry.comparingByValue()))
+                .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue, (e1, e2) -> e1, LinkedHashMap::new));","[{'comment': 'Hash map is not ordered, I think you what you need is List<Map.Entry<Long, Long>>', 'commenter': 'imay'}, {'comment': 'The input order and out order for `LinkedHashMap` is the same.', 'commenter': 'kangkaisen'}, {'comment': 'OK, I see', 'commenter': 'imay'}]"
1140,docs/documentation/cn/extending-doris/user-defined-function.md,"@@ -0,0 +1,83 @@
+# USER DEFINED FUNCTION
+
+用户可以通过UDF机制来扩展Doris的能力。通过这篇文档，用户能够创建自己的UDF。
+
+## 编写UDF函数
+
+在使用UDF之前，用户需要先在Doris的UDF框架下，编写自己的UDF函数。在`be/src/udf_samples/udf_sample.h|cpp`文件中是一个简单的UDF Demo。
+
+编写一个UDF函数需要以下几个步骤
+
+### 编写函数
+
+创建对应的头文件、CPP文件，在CPP文件中实现你需要的逻辑。CPP文件中的实现函数格式与UDF的对应关系。
+
+#### 非可变参数
+
+对于非可变参数的UDF，那么两者之间的对应关系很直接。
+比如`INT MyADD(INT, INT)`的UDF就会对应`IntVal AddUdf(FunctionContext* context, const IntVal& arg1, const IntVal& arg2)`
+
+1. `AddUdf`可以为任意的名字，只要创建UDF的时候指定即可
+2. 实现函数中的第一个参数永远是`FunctionContext*`。实现者可以通过这个结构体获得一些查询相关的内容，以及申请一些需要使用的内存。具体使用的接口可以参考`udf/udf.h`中的定义。
+3. 实现函数中从第二个参数开始需要与UDF的参数一一对应，比如`IntVal`对应`INT`类型。这部分的类型都要使用`const`引用
+4. 返回参数与UDF的参数的类型要相对应
+
+#### 可变参数
+
+对于可变参数，可以参见一下例子，UDF`String md5sum(String, ...)`对应的","[{'comment': '一下 -> 以下', 'commenter': 'morningman'}, {'comment': 'OK ', 'commenter': 'imay'}]"
1140,docs/documentation/cn/extending-doris/user-defined-function.md,"@@ -0,0 +1,83 @@
+# USER DEFINED FUNCTION
+
+用户可以通过UDF机制来扩展Doris的能力。通过这篇文档，用户能够创建自己的UDF。
+
+## 编写UDF函数
+
+在使用UDF之前，用户需要先在Doris的UDF框架下，编写自己的UDF函数。在`be/src/udf_samples/udf_sample.h|cpp`文件中是一个简单的UDF Demo。
+
+编写一个UDF函数需要以下几个步骤
+
+### 编写函数
+
+创建对应的头文件、CPP文件，在CPP文件中实现你需要的逻辑。CPP文件中的实现函数格式与UDF的对应关系。
+
+#### 非可变参数
+
+对于非可变参数的UDF，那么两者之间的对应关系很直接。
+比如`INT MyADD(INT, INT)`的UDF就会对应`IntVal AddUdf(FunctionContext* context, const IntVal& arg1, const IntVal& arg2)`
+
+1. `AddUdf`可以为任意的名字，只要创建UDF的时候指定即可","[{'comment': '这里每个语句有些有句号有些没有句号，统一一下吧。下同', 'commenter': 'morningman'}, {'comment': 'OK', 'commenter': 'imay'}]"
1140,docs/documentation/cn/extending-doris/user-defined-function.md,"@@ -0,0 +1,83 @@
+# USER DEFINED FUNCTION
+
+用户可以通过UDF机制来扩展Doris的能力。通过这篇文档，用户能够创建自己的UDF。
+
+## 编写UDF函数
+
+在使用UDF之前，用户需要先在Doris的UDF框架下，编写自己的UDF函数。在`be/src/udf_samples/udf_sample.h|cpp`文件中是一个简单的UDF Demo。
+
+编写一个UDF函数需要以下几个步骤
+
+### 编写函数
+
+创建对应的头文件、CPP文件，在CPP文件中实现你需要的逻辑。CPP文件中的实现函数格式与UDF的对应关系。
+
+#### 非可变参数
+
+对于非可变参数的UDF，那么两者之间的对应关系很直接。
+比如`INT MyADD(INT, INT)`的UDF就会对应`IntVal AddUdf(FunctionContext* context, const IntVal& arg1, const IntVal& arg2)`
+
+1. `AddUdf`可以为任意的名字，只要创建UDF的时候指定即可
+2. 实现函数中的第一个参数永远是`FunctionContext*`。实现者可以通过这个结构体获得一些查询相关的内容，以及申请一些需要使用的内存。具体使用的接口可以参考`udf/udf.h`中的定义。
+3. 实现函数中从第二个参数开始需要与UDF的参数一一对应，比如`IntVal`对应`INT`类型。这部分的类型都要使用`const`引用
+4. 返回参数与UDF的参数的类型要相对应
+
+#### 可变参数
+
+对于可变参数，可以参见一下例子，UDF`String md5sum(String, ...)`对应的
+实现函数是`StringVal md5sumUdf(FunctionContext* ctx, int num_args, const StringVal* args)`
+
+1. `md5sumUdf`这个也是可以任意改变的，创建的时候指定即可
+2. 第一个参数与非可变参数函数一样，传入的是一个`FunctionContext*`
+3. 可变参数部分由两部分组成，首先会传入一个整数，说明后面还有几个参数。后面传入的是一个可变参数部分的数组
+
+#### 类型对应关系
+
+|UDF Type|Argument Type|
+|----|---------|
+|TinyInt|TinyIntVal|
+|SmallInt|SmallIntVal|
+|Int|IntVal|
+|BigInt|BigIntVal|
+|LargeInt|LargeIntVal|
+|Float|FloatVal|
+|Double|DoubleVal|
+|Date|DateTimeVal|
+|Datetime|DateTimeVal|
+|Char|StringVal|
+|Varchar|StringVal|
+|Decimal|DecimalVal|
+
+### 修改CMakeLists.txt
+
+在`be/src/udf_samples/CMakeLists.txt`增加对应的动态库创建描述，类似于`add_library(udfsample SHARED udf_sample.cpp)`。这个描述增加了一个`udfsample`动态库。后面需要写上涉及的所有源文件（不包含头文件）。
+
+### 执行编译
+
+在最外部执行`sh build.sh`就可以生成对应的动态库。生成的动态库的位置，位于`be/build/src/udf_samples/`下。比如`udfsample`就会生成一个文件位于`be/build/src/udf_samples/libudfsample.so`
+
+## 创建UDF函数
+
+通过上述的步骤后，你可以得到一个动态库。你需要将这个动态库放到一个能够通过HTTP协议访问到的位置。然后执行创建UDF函数在Doris系统内部创建一个UDF，你需要拥有AMDIN权限才能够完成这个操作。
+
+```
+CREATE [AGGREGATE] FUNCTION 
+	name ([argtype][,...])
+	[RETURNS] rettype
+	PROPERTIES ([""key""=""value""][,...])
+```
+说明：","[{'comment': '这里可以加一个，更多帮助请参阅 `HELP CREATE FUNCTION;`', 'commenter': 'morningman'}, {'comment': 'OK', 'commenter': 'imay'}]"
1140,docs/documentation/cn/extending-doris/user-defined-function.md,"@@ -0,0 +1,83 @@
+# USER DEFINED FUNCTION
+
+用户可以通过UDF机制来扩展Doris的能力。通过这篇文档，用户能够创建自己的UDF。
+
+## 编写UDF函数
+
+在使用UDF之前，用户需要先在Doris的UDF框架下，编写自己的UDF函数。在`be/src/udf_samples/udf_sample.h|cpp`文件中是一个简单的UDF Demo。
+
+编写一个UDF函数需要以下几个步骤
+
+### 编写函数
+
+创建对应的头文件、CPP文件，在CPP文件中实现你需要的逻辑。CPP文件中的实现函数格式与UDF的对应关系。
+
+#### 非可变参数
+
+对于非可变参数的UDF，那么两者之间的对应关系很直接。
+比如`INT MyADD(INT, INT)`的UDF就会对应`IntVal AddUdf(FunctionContext* context, const IntVal& arg1, const IntVal& arg2)`
+
+1. `AddUdf`可以为任意的名字，只要创建UDF的时候指定即可
+2. 实现函数中的第一个参数永远是`FunctionContext*`。实现者可以通过这个结构体获得一些查询相关的内容，以及申请一些需要使用的内存。具体使用的接口可以参考`udf/udf.h`中的定义。
+3. 实现函数中从第二个参数开始需要与UDF的参数一一对应，比如`IntVal`对应`INT`类型。这部分的类型都要使用`const`引用
+4. 返回参数与UDF的参数的类型要相对应
+
+#### 可变参数
+
+对于可变参数，可以参见一下例子，UDF`String md5sum(String, ...)`对应的
+实现函数是`StringVal md5sumUdf(FunctionContext* ctx, int num_args, const StringVal* args)`
+
+1. `md5sumUdf`这个也是可以任意改变的，创建的时候指定即可
+2. 第一个参数与非可变参数函数一样，传入的是一个`FunctionContext*`
+3. 可变参数部分由两部分组成，首先会传入一个整数，说明后面还有几个参数。后面传入的是一个可变参数部分的数组
+
+#### 类型对应关系
+
+|UDF Type|Argument Type|
+|----|---------|
+|TinyInt|TinyIntVal|
+|SmallInt|SmallIntVal|
+|Int|IntVal|
+|BigInt|BigIntVal|
+|LargeInt|LargeIntVal|
+|Float|FloatVal|
+|Double|DoubleVal|
+|Date|DateTimeVal|
+|Datetime|DateTimeVal|
+|Char|StringVal|
+|Varchar|StringVal|
+|Decimal|DecimalVal|
+
+### 修改CMakeLists.txt
+
+在`be/src/udf_samples/CMakeLists.txt`增加对应的动态库创建描述，类似于`add_library(udfsample SHARED udf_sample.cpp)`。这个描述增加了一个`udfsample`动态库。后面需要写上涉及的所有源文件（不包含头文件）。
+
+### 执行编译
+
+在最外部执行`sh build.sh`就可以生成对应的动态库。生成的动态库的位置，位于`be/build/src/udf_samples/`下。比如`udfsample`就会生成一个文件位于`be/build/src/udf_samples/libudfsample.so`
+
+## 创建UDF函数
+
+通过上述的步骤后，你可以得到一个动态库。你需要将这个动态库放到一个能够通过HTTP协议访问到的位置。然后执行创建UDF函数在Doris系统内部创建一个UDF，你需要拥有AMDIN权限才能够完成这个操作。
+
+```
+CREATE [AGGREGATE] FUNCTION 
+	name ([argtype][,...])
+	[RETURNS] rettype
+	PROPERTIES ([""key""=""value""][,...])
+```
+说明：
+
+1. PROPERTIES中""symbol""表示的是，执行入口函数的对应symbol，这个参数是必须设定。你可以通过`nm`命令来获得对应的symbol，比如`nm libudfsample.so | grep AddUdf`获得到的`_ZN9doris_udf6AddUdfEPNS_15FunctionContextERKNS_6IntValES4_`就是对应的symbol。
+2. PROPERTIES中""object_file""表示的是从哪里能够下载到对应的动态库，这个参数是必须设定的。
+3. name: 一个function是要归属于某个DB的，name的形式为`dbName`.`funcName`。当`dbName`没有明确指定的时候，就是使用当前session所在的db作为`dbName`.
+
+## 使用UDF
+
+UDF的使用与普通的函数方式一致，唯一的区别在于，内置函数的作用域是全局的，而UDF的作用域是DB内部。当链接session位于数据内部时，直接使用UDF名字会在当前DB内部查找对应的UDF。否则用户需要显示的指定UDF的数据库名字，例如`dbName`.`funcName`。
+
+
+## 删除UDF函数
+
+当你不再需要UDF函数时，你可以通过下述命令来删除一个UDF函数, 可以参考 `DROP FUNCTION`","[{'comment': '可以参考 `DROP FUNCTION` -> 可以参考 `HELP DROP FUNCTION`', 'commenter': 'morningman'}, {'comment': '这里加HELP不太好，未来这个文档可能在PDF中出现，那么可能是另外一个章节的链接。所以还是不加HELP比较好', 'commenter': 'imay'}]"
1140,docs/documentation/cn/sql-reference/sql-statements/create-function.md,"@@ -0,0 +1,70 @@
+# CREATE FUNCTION
+
+## Syntax
+
+```
+CREATE [AGGREGATE] FUNCTION function_name
+    (arg_type [, ...])
+    RETURNS ret_type
+    [INTERMEDIATE inter_type]
+    [PROPERTIES (""key"" = ""value"" [, ...]) ]
+```
+
+## Description
+
+此语句创建一个自定义函数。执行此命令需要用户拥有 `ADMIN` 权限。
+
+如果 `function_name` 中包含了数据库名字，那么这个自定义函数会创建在对应的数据库中，否则这个函数将会创建在当前会话所在的数据库。新函数的名字与参数不能够与当前命名空间中已存在的函数相同，否则会创建失败。但是只有名字相同，参数不同是能够创建成果的。","[{'comment': '能够创建成果的。-> 能够创建成功的。', 'commenter': 'morningman'}, {'comment': 'OK', 'commenter': 'imay'}]"
1140,docs/documentation/cn/sql-reference/sql-statements/create-function.md,"@@ -0,0 +1,70 @@
+# CREATE FUNCTION
+
+## Syntax
+
+```
+CREATE [AGGREGATE] FUNCTION function_name
+    (arg_type [, ...])
+    RETURNS ret_type
+    [INTERMEDIATE inter_type]
+    [PROPERTIES (""key"" = ""value"" [, ...]) ]
+```
+
+## Description
+
+此语句创建一个自定义函数。执行此命令需要用户拥有 `ADMIN` 权限。
+
+如果 `function_name` 中包含了数据库名字，那么这个自定义函数会创建在对应的数据库中，否则这个函数将会创建在当前会话所在的数据库。新函数的名字与参数不能够与当前命名空间中已存在的函数相同，否则会创建失败。但是只有名字相同，参数不同是能够创建成果的。
+
+## Parameters
+
+> `AGGREGATE`: 如果有此项，表示的是创建的函数是一个聚合函数，否则创建的是一个标量函数
+>
+> `function_name`: 要创建函数的名字, 可以包含数据库的名字。比如：`db1.my_func`
+>
+> `arg_type`: 函数的参数类型，与建表时定义的类型一致。变长参数时可以使用`, ...`来表示，如果是变长类型，那么变长部分参数的类型与最后一个费变长参数类型一致","[{'comment': '最后一个费变长 -> 最后一个非变长', 'commenter': 'morningman'}]"
1140,docs/documentation/cn/extending-doris/user-defined-function.md,"@@ -0,0 +1,83 @@
+# USER DEFINED FUNCTION","[{'comment': '建议文档中加入对 function 权限的说明，比如需要有 db 的select 权限才可以使用 function 等。', 'commenter': 'morningman'}, {'comment': 'OK', 'commenter': 'imay'}]"
1143,fe/src/main/java/org/apache/doris/clone/TabletScheduler.java,"@@ -833,7 +833,8 @@ private RootPathLoadStatistic chooseAvailableDestPath(TabletSchedCtx tabletCtx)
 
             List<RootPathLoadStatistic> resultPaths = Lists.newArrayList();
             BalanceStatus st = bes.isFit(tabletCtx.getTabletSize(), tabletCtx.getStorageMedium(),
-                    resultPaths, true /* is supplement */);
+                    resultPaths, tabletCtx.getTabletStatus() != TabletStatus.REPLICA_RELOCATING
+                    /* if REPLICA_RELOCATING, than it is not a supplement task */);","[{'comment': '```suggestion\r\n                    /* if REPLICA_RELOCATING, then it is not a supplement task */);\r\n```', 'commenter': 'imay'}]"
1143,fe/src/test/java/org/apache/doris/clone/RootPathLoadStatisticTest.java,"@@ -0,0 +1,32 @@
+package org.apache.doris.clone;","[{'comment': 'licence header', 'commenter': 'imay'}]"
1144,thirdparty/vars.sh,"@@ -194,8 +194,8 @@ LEVELDB_SOURCE=leveldb-1.20
 LEVELDB_MD5SUM=""298b5bddf12c675d6345784261302252""
 
 # brpc
-BRPC_DOWNLOAD=""https://github.com/brpc/brpc/archive/v0.9.0.tar.gz""
-BRPC_NAME=brpc-0.9.0.tar.gz
+BRPC_DOWNLOAD=""https://github.com/brpc/brpc/archive/incubator-brpc-0.9.0.tar.gz""","[{'comment': '```suggestion\r\nBRPC_DOWNLOAD=""https://github.com/apache/incubator-brpc/archive/v0.9.0.tar.gz""\r\n```', 'commenter': 'imay'}]"
1144,thirdparty/vars.sh,"@@ -211,5 +211,36 @@ LIBRDKAFKA_NAME=librdkafka-0.11.6-RC5.tar.gz
 LIBRDKAFKA_SOURCE=librdkafka-0.11.6-RC5
 LIBRDKAFKA_MD5SUM=""2e4ecef2df277e55a0144eb6d185e18a""
 
+# zstd
+ZSTD_DOWNLOAD=""https://github.com/facebook/zstd/archive/v1.3.7.tar.gz""
+ZSTD_NAME=zstd-1.3.7.tar.gz
+ZSTD_SOURCE=zstd-1.3.7
+ZSTD_MD5SUM=""9b89923a360ac85a3b8076fdf495318d""
+
+# uriparser
+URIPARSER_DOWNLOAD=""https://github.com/uriparser/uriparser/archive/63384be4fb8197264c55ff53a135110ecd5bd8c4.tar.gz""","[{'comment': 'use a release download link', 'commenter': 'imay'}, {'comment': 'ok ', 'commenter': 'worker24h'}]"
1144,thirdparty/vars.sh,"@@ -211,5 +211,36 @@ LIBRDKAFKA_NAME=librdkafka-0.11.6-RC5.tar.gz
 LIBRDKAFKA_SOURCE=librdkafka-0.11.6-RC5
 LIBRDKAFKA_MD5SUM=""2e4ecef2df277e55a0144eb6d185e18a""
 
+# zstd
+ZSTD_DOWNLOAD=""https://github.com/facebook/zstd/archive/v1.3.7.tar.gz""
+ZSTD_NAME=zstd-1.3.7.tar.gz
+ZSTD_SOURCE=zstd-1.3.7
+ZSTD_MD5SUM=""9b89923a360ac85a3b8076fdf495318d""
+
+# uriparser
+URIPARSER_DOWNLOAD=""https://github.com/uriparser/uriparser/archive/uriparser-0.9.2.tar.gz""
+URIPARSER_NAME=uriparser-0.9.2.tar.gz
+URIPARSER_SOURCE=uriparser-uriparser-0.9.2","[{'comment': '```suggestion\r\nURIPARSER_SOURCE=uriparser-0.9.2\r\n```', 'commenter': 'imay'}]"
1144,thirdparty/build-thirdparty.sh,"@@ -511,12 +516,41 @@ build_librdkafka() {
     cd $TP_SOURCE_DIR/$LIBRDKAFKA_SOURCE
 
     CPPFLAGS=""-I${TP_INCLUDE_DIR}"" \
-    LDFLAGS=""-L${TP_LIB_DIR}""
+    LDFLAGS=""-L${TP_LIB_DIR}"" \
     CFLAGS=""-fPIC"" \
     ./configure --prefix=$TP_INSTALL_DIR --enable-static --disable-ssl --disable-sasl
     make -j$PARALLEL && make install
 }
 
+build_arrow() {
+    check_if_source_exist $ARROW_SOURCE
+    cd $TP_SOURCE_DIR/$ARROW_SOURCE/cpp && mkdir -p release && cd release
+    export ARROW_BROTLI_URL=${TP_SOURCE_DIR}/${BROTLI_NAME}
+    export ARROW_DOUBLE_CONVERSION_URL=${TP_SOURCE_DIR}/${DOUBLE_CONVERSION_NAME}
+    export ARROW_GLOG_URL=${TP_SOURCE_DIR}/${GLOG_NAME}
+    export ARROW_LZ4_URL=${TP_SOURCE_DIR}/${LZ4_NAME}
+    export ARROW_URIPARSER_URL=${TP_SOURCE_DIR}/${URIPARSER_NAME}
+    export ARROW_ZSTD_URL=${TP_SOURCE_DIR}/${ZSTD_NAME}
+    
+    cmake -DARROW_PARQUET=ON -DARROW_IPC=OFF -DARROW_BUILD_SHARED=OFF \
+    -DCMAKE_INSTALL_PREFIX=$TP_INSTALL_DIR \
+    -DARROW_BOOST_USE_SHARED=OFF -DBoost_NO_BOOST_CMAKE=ON -DBOOST_ROOT=$TP_INSTALL_DIR \
+    -Dgflags_ROOT=$TP_INSTALL_DIR/ \
+    -DSnappy_ROOT=$TP_INSTALL_DIR/ \
+    -DGLOG_ROOT=$TP_INSTALL_DIR/ \
+    -DLZ4_ROOT=$TP_INSTALL_DIR/ \
+    -DThrift_ROOT=$TP_INSTALL_DIR/ ..
+
+    make -j$PARALLEL && make install
+    #copy dep libs
+    cp -rf ./jemalloc_ep-prefix/src/jemalloc_ep/dist/lib/libjemalloc_pic.a $TP_INSTALL_DIR/lib64/libjemalloc.a
+    cp -rf ./brotli_ep/src/brotli_ep-install/lib/libbrotlienc.a $TP_INSTALL_DIR/lib64/libbrotlienc.a
+    cp -rf ./brotli_ep/src/brotli_ep-install/lib/libbrotlidec.a $TP_INSTALL_DIR/lib64/libbrotlidec.a
+    cp -rf ./brotli_ep/src/brotli_ep-install/lib/libbrotlicommon.a $TP_INSTALL_DIR/lib64/libbrotlicommon.a
+    cp -rf ./zstd_ep-install/lib64/libzstd.a $TP_INSTALL_DIR/lib64/libzstd.a
+    cp -rf ./double-conversion_ep/src/double-conversion_ep/lib/libdouble-conversion.a $TP_INSTALL_DIR/lib64/libdouble-conversion.a","[{'comment': ""uriparser lib isn't copied"", 'commenter': 'imay'}]"
1146,be/src/olap/olap_engine.cpp,"@@ -835,6 +840,10 @@ OLAPStatus OLAPEngine::get_tables_by_id(
                 it = table_list->erase(it);
                 continue;
             }
+        } else if ((*it)->is_used()) {","[{'comment': ""why not check if `(*it)->is_bad()`\r\n\r\nI'm afraid when tablet is not bad and it is not used, you erase it. which is not what we want"", 'commenter': 'imay'}, {'comment': 'For now, `tablet is not used` means the store which this tablet on is bad(IO error). So I think using `(*it)->is_used()` is OK', 'commenter': 'morningman'}]"
1147,be/src/olap/tablet_manager.cpp,"@@ -239,6 +239,15 @@ void TabletManager::cancel_unfinished_schema_change() {
             }
 
             AlterTabletTaskSharedPtr new_alter_task = new_tablet->alter_task();
+            if (new_alter_task != nullptr 
+                && (new_alter_task->related_tablet_id() != tablet->tablet_id() 
+                    || new_alter_task->related_schema_hash() != tablet->schema_hash())) {
+                LOG(WARNING) << ""base tablet "" << tablet->full_name()
+                             << ""new tablet "" << new_tablet->full_name()","[{'comment': '```suggestion\r\n                             << "" new tablet "" << new_tablet->full_name()\r\n```\r\nadd a space. The same the the following line.', 'commenter': 'kangpinghuang'}]"
1147,be/src/olap/tablet_manager.cpp,"@@ -258,18 +267,28 @@ void TabletManager::cancel_unfinished_schema_change() {
                            << "", base_tablet="" << tablet->full_name();
                 return;
             }
-
-            res = new_tablet->set_alter_state(ALTER_FAILED);
-            if (res != OLAP_SUCCESS) {
-                LOG(FATAL) << ""fail to save new tablet meta. res="" << res
-                           << "", new_tablet="" << new_tablet->full_name();
-                return;
-            }
-            res = new_tablet->save_meta();
-            if (res != OLAP_SUCCESS) {
-                LOG(FATAL) << ""fail to save new tablet meta. res="" << res
-                           << "", new_tablet="" << new_tablet->full_name();
-                return;
+            if (new_alter_task == nullptr 
+                && new_tablet->creation_time() < tablet->creation_time()) {
+                // case 1: create new tablet and save meta successfully, but failed to save alter state in base tablet
+                // case 2: during clear stage, clear base successfully, but faile dto clear new tablet","[{'comment': '```suggestion\r\n                // case 2: during clear stage, clear base successfully, but failed to clear new tablet\r\n```', 'commenter': 'kangpinghuang'}]"
1147,be/src/olap/tablet_manager.cpp,"@@ -258,18 +267,28 @@ void TabletManager::cancel_unfinished_schema_change() {
                            << "", base_tablet="" << tablet->full_name();
                 return;
             }
-
-            res = new_tablet->set_alter_state(ALTER_FAILED);
-            if (res != OLAP_SUCCESS) {
-                LOG(FATAL) << ""fail to save new tablet meta. res="" << res
-                           << "", new_tablet="" << new_tablet->full_name();
-                return;
-            }
-            res = new_tablet->save_meta();
-            if (res != OLAP_SUCCESS) {
-                LOG(FATAL) << ""fail to save new tablet meta. res="" << res
-                           << "", new_tablet="" << new_tablet->full_name();
-                return;
+            if (new_alter_task == nullptr 
+                && new_tablet->creation_time() < tablet->creation_time()) {
+                // case 1: create new tablet and save meta successfully, but failed to save alter state in base tablet
+                // case 2: during clear stage, clear base successfully, but faile dto clear new tablet
+                LOG(WARNING) << ""base tablet's alter task is null, skip set state""
+                             << "" base_tablet="" << new_tablet->full_name()
+                             << "" create_time="" << new_tablet->creation_time()
+                             << "" new_tablet="" << tablet->full_name()
+                             << "" create_time="" << tablet->creation_time();
+            } else {
+                res = new_tablet->set_alter_state(ALTER_FAILED);
+                if (res != OLAP_SUCCESS) {
+                    LOG(FATAL) << ""fail to save new tablet meta. res="" << res","[{'comment': 'modify the failure message. I think failed to set the alter state is better.', 'commenter': 'kangpinghuang'}]"
1147,be/src/olap/tablet_manager.cpp,"@@ -258,18 +267,28 @@ void TabletManager::cancel_unfinished_schema_change() {
                            << "", base_tablet="" << tablet->full_name();
                 return;
             }
-
-            res = new_tablet->set_alter_state(ALTER_FAILED);
-            if (res != OLAP_SUCCESS) {
-                LOG(FATAL) << ""fail to save new tablet meta. res="" << res
-                           << "", new_tablet="" << new_tablet->full_name();
-                return;
-            }
-            res = new_tablet->save_meta();
-            if (res != OLAP_SUCCESS) {
-                LOG(FATAL) << ""fail to save new tablet meta. res="" << res
-                           << "", new_tablet="" << new_tablet->full_name();
-                return;
+            if (new_alter_task == nullptr 
+                && new_tablet->creation_time() < tablet->creation_time()) {
+                // case 1: create new tablet and save meta successfully, but failed to save alter state in base tablet
+                // case 2: during clear stage, clear base successfully, but faile dto clear new tablet
+                LOG(WARNING) << ""base tablet's alter task is null, skip set state""
+                             << "" base_tablet="" << new_tablet->full_name()
+                             << "" create_time="" << new_tablet->creation_time()
+                             << "" new_tablet="" << tablet->full_name()
+                             << "" create_time="" << tablet->creation_time();
+            } else {
+                res = new_tablet->set_alter_state(ALTER_FAILED);","[{'comment': 'what if new_alter_task == nullptr?', 'commenter': 'kangpinghuang'}, {'comment': 'fatal error', 'commenter': 'yiguolei'}]"
1147,be/src/olap/tablet_manager.cpp,"@@ -239,6 +239,15 @@ void TabletManager::cancel_unfinished_schema_change() {
             }
 
             AlterTabletTaskSharedPtr new_alter_task = new_tablet->alter_task();
+            if (new_alter_task != nullptr 
+                && (new_alter_task->related_tablet_id() != tablet->tablet_id() 
+                    || new_alter_task->related_schema_hash() != tablet->schema_hash())) {","[{'comment': 'indent with the last line', 'commenter': 'kangpinghuang'}]"
1151,docs/documentation/cn/administrator-guide/doris-on-es/doris-on-es.md,"@@ -0,0 +1,200 @@
+# Doris-On-ES使用手册","[{'comment': 'put it to  docs/documentation/cn/extending-doris', 'commenter': 'imay'}, {'comment': 'OK, Please review again.', 'commenter': 'wuyunfeng'}]"
1151,docs/documentation/cn/administrator-guide/extending-doris/doris-on-es.md,"@@ -0,0 +1,205 @@
+# Doris-On-ES使用手册
+
+Doris-On-ES将Doris的分布式查询规划能力和ES(Elasticsearch)的全文检索能力相结合，提供更完善的OLAP分析场景解决方案：
+ 
+ 1. ES中的多index分布式Join查询
+ 2. Doris和ES中的表联合查询，更复杂的全文检索过滤
+ 3. ES keyword类型字段的聚合查询：适用于index 频繁发生变化、单个分片文档数量千万级以上且该字段基数(cardinality)非常大
+
+本文档主要介绍该功能的实现原理、使用方式等。
+
+## 名词解释
+
+* FE：Frontend，Doris 的前端节点。负责元数据管理和请求接入。
+* BE：Backend，Doris 的后端节点。负责查询执行和数据存储。
+* Elasticsearch(ES)：目前最流行的开源分布式搜索引擎。
+* DataNode：ES的数据存储与计算节点。
+* MasterNode：ES的Master节点，管理元数据、节点、数据分布等。
+* scroll：ES内置的数据集游标特性，用来对数据进行流式扫描和过滤。
+
+
+## 如何使用
+
+### 创建外表
+
+```
+CREATE EXTERNAL TABLE `es_table` (
+  `id` bigint(20) COMMENT """",
+  `k1` bigint(20) COMMENT """",
+  `k2` datetime COMMENT """",
+  `k3` varchar(20) COMMENT """",
+  `k4` varchar(100) COMMENT """",
+  `k5` float COMMENT """"
+) ENGINE=ELASTICSEARCH
+PARTITION BY RANGE(`id`)
+()
+PROPERTIES (
+""host"" = ""host"",
+""user"" = ""xxxx"",
+""password"" = ""xxxx"",
+""index"" = ""tindex”,
+""type"" = ""doc""
+);
+```
+
+参数说明：
+
+参数 | 说明
+---|---
+host | ES集群连接地址，可指定一个或多个，Doris通过这个地址获取到ES版本号、index的shard分布信息
+user | 开启basic认证的ES集群的用户名，需要确保该用户有访问: /、/_cluster/state、_nodes/http等路径权限","[{'comment': '这里 下滑线 _ 需要加 反斜杠 \\ ，否则会被认为是斜体', 'commenter': 'morningman'}, {'comment': '下同', 'commenter': 'morningman'}, {'comment': 'hi，morningman. please review this change', 'commenter': 'wuyunfeng'}]"
1151,docs/documentation/cn/extending-doris/doris-on-es/doris-on-es.md,"@@ -0,0 +1,205 @@
+# Doris-On-ES使用手册
+
+Doris-On-ES将Doris的分布式查询规划能力和ES(Elasticsearch)的全文检索能力相结合，提供更完善的OLAP分析场景解决方案：
+ 
+ 1. ES中的多index分布式Join查询
+ 2. Doris和ES中的表联合查询，更复杂的全文检索过滤
+ 3. ES keyword类型字段的聚合查询：适用于index 频繁发生变化、单个分片文档数量千万级以上且该字段基数(cardinality)非常大
+
+本文档主要介绍该功能的实现原理、使用方式等。
+
+## 名词解释
+
+* FE：Frontend，Doris 的前端节点。负责元数据管理和请求接入。
+* BE：Backend，Doris 的后端节点。负责查询执行和数据存储。
+* Elasticsearch(ES)：目前最流行的开源分布式搜索引擎。
+* DataNode：ES的数据存储与计算节点。
+* MasterNode：ES的Master节点，管理元数据、节点、数据分布等。
+* scroll：ES内置的数据集游标特性，用来对数据进行流式扫描和过滤。
+
+
+## 如何使用
+
+### 创建外表
+
+```
+CREATE EXTERNAL TABLE `es_table` (
+  `id` bigint(20) COMMENT """",
+  `k1` bigint(20) COMMENT """",
+  `k2` datetime COMMENT """",
+  `k3` varchar(20) COMMENT """",
+  `k4` varchar(100) COMMENT """",
+  `k5` float COMMENT """"
+) ENGINE=ELASTICSEARCH
+PARTITION BY RANGE(`id`)
+()
+PROPERTIES (
+""host"" = ""host"",
+""user"" = ""xxxx"",
+""password"" = ""xxxx"",
+""index"" = ""tindex”,
+""type"" = ""doc""
+);
+```
+
+参数说明：
+
+参数 | 说明
+---|---
+host | ES集群连接地址，可指定一个或多个，Doris通过这个地址获取到ES版本号、index的shard分布信息
+user | 开启basic认证的ES集群的用户名，需要确保该用户有访问: /、/\_cluster/state、/\_nodes/http等路径权限和对index的读权限
+password | 对应用户的密码信息
+index | Doris中的表对应的ES的index名字，可以是alias
+type | 指定index的type，默认是_doc
+transport | 内部保留，默认为http
+
+### 查询
+
+#### 基本条件过滤
+
+```
+select * from es_table where k1 > 1000 and k3 ='term' or k4 like 'fu*z_'
+```
+
+#### 扩展的esquery sql语法
+通过`esquery`函数将一些无法用sql表述的ES query如match、geoshape等下推给ES进行过滤处理，`esquery`的第一个列名参数用于关联`index`，第二个参数是ES的基本`Query DSL`的json表述，使用花括号`{}`包含，json的`root key`有且只能有一个，如match、geo_shape、bool等
+
+match查询：
+
+```
+select * from es_table where esquery('k4', '{","[{'comment': ""```suggestion\r\nselect * from es_table where esquery(k4, '{\r\n```"", 'commenter': 'imay'}]"
1151,docs/documentation/cn/extending-doris/doris-on-es/doris-on-es.md,"@@ -0,0 +1,205 @@
+# Doris-On-ES使用手册
+
+Doris-On-ES将Doris的分布式查询规划能力和ES(Elasticsearch)的全文检索能力相结合，提供更完善的OLAP分析场景解决方案：
+ 
+ 1. ES中的多index分布式Join查询
+ 2. Doris和ES中的表联合查询，更复杂的全文检索过滤
+ 3. ES keyword类型字段的聚合查询：适用于index 频繁发生变化、单个分片文档数量千万级以上且该字段基数(cardinality)非常大
+
+本文档主要介绍该功能的实现原理、使用方式等。
+
+## 名词解释
+
+* FE：Frontend，Doris 的前端节点。负责元数据管理和请求接入。
+* BE：Backend，Doris 的后端节点。负责查询执行和数据存储。
+* Elasticsearch(ES)：目前最流行的开源分布式搜索引擎。
+* DataNode：ES的数据存储与计算节点。
+* MasterNode：ES的Master节点，管理元数据、节点、数据分布等。
+* scroll：ES内置的数据集游标特性，用来对数据进行流式扫描和过滤。
+
+
+## 如何使用
+
+### 创建外表
+
+```
+CREATE EXTERNAL TABLE `es_table` (
+  `id` bigint(20) COMMENT """",
+  `k1` bigint(20) COMMENT """",
+  `k2` datetime COMMENT """",
+  `k3` varchar(20) COMMENT """",
+  `k4` varchar(100) COMMENT """",
+  `k5` float COMMENT """"
+) ENGINE=ELASTICSEARCH
+PARTITION BY RANGE(`id`)
+()
+PROPERTIES (
+""host"" = ""host"",","[{'comment': 'host:port', 'commenter': 'imay'}]"
1151,docs/documentation/cn/extending-doris/doris-on-es/doris-on-es.md,"@@ -0,0 +1,205 @@
+# Doris-On-ES使用手册
+
+Doris-On-ES将Doris的分布式查询规划能力和ES(Elasticsearch)的全文检索能力相结合，提供更完善的OLAP分析场景解决方案：
+ 
+ 1. ES中的多index分布式Join查询
+ 2. Doris和ES中的表联合查询，更复杂的全文检索过滤
+ 3. ES keyword类型字段的聚合查询：适用于index 频繁发生变化、单个分片文档数量千万级以上且该字段基数(cardinality)非常大
+
+本文档主要介绍该功能的实现原理、使用方式等。
+
+## 名词解释
+
+* FE：Frontend，Doris 的前端节点。负责元数据管理和请求接入。
+* BE：Backend，Doris 的后端节点。负责查询执行和数据存储。
+* Elasticsearch(ES)：目前最流行的开源分布式搜索引擎。
+* DataNode：ES的数据存储与计算节点。
+* MasterNode：ES的Master节点，管理元数据、节点、数据分布等。
+* scroll：ES内置的数据集游标特性，用来对数据进行流式扫描和过滤。
+
+
+## 如何使用
+
+### 创建外表
+
+```
+CREATE EXTERNAL TABLE `es_table` (
+  `id` bigint(20) COMMENT """",
+  `k1` bigint(20) COMMENT """",
+  `k2` datetime COMMENT """",
+  `k3` varchar(20) COMMENT """",
+  `k4` varchar(100) COMMENT """",
+  `k5` float COMMENT """"
+) ENGINE=ELASTICSEARCH
+PARTITION BY RANGE(`id`)
+()
+PROPERTIES (
+""host"" = ""host"",
+""user"" = ""xxxx"",
+""password"" = ""xxxx"",
+""index"" = ""tindex”,
+""type"" = ""doc""
+);
+```
+
+参数说明：
+
+参数 | 说明
+---|---
+host | ES集群连接地址，可指定一个或多个，Doris通过这个地址获取到ES版本号、index的shard分布信息
+user | 开启basic认证的ES集群的用户名，需要确保该用户有访问: /、/\_cluster/state、/\_nodes/http等路径权限和对index的读权限
+password | 对应用户的密码信息
+index | Doris中的表对应的ES的index名字，可以是alias
+type | 指定index的type，默认是_doc
+transport | 内部保留，默认为http
+
+### 查询
+
+#### 基本条件过滤
+
+```
+select * from es_table where k1 > 1000 and k3 ='term' or k4 like 'fu*z_'
+```
+
+#### 扩展的esquery sql语法
+通过`esquery`函数将一些无法用sql表述的ES query如match、geoshape等下推给ES进行过滤处理，`esquery`的第一个列名参数用于关联`index`，第二个参数是ES的基本`Query DSL`的json表述，使用花括号`{}`包含，json的`root key`有且只能有一个，如match、geo_shape、bool等
+
+match查询：
+
+```
+select * from es_table where esquery('k4', '{
+        ""match"": {
+           ""k4"": ""doris on elasticsearch""
+        }
+    }');
+```
+geo相关查询：
+
+```
+select * from es_table where esquery('k4', '{
+      ""geo_shape"": {
+         ""location"": {
+            ""shape"": {
+               ""type"": ""envelope"",
+               ""coordinates"": [
+                  [
+                     13,
+                     53
+                  ],
+                  [
+                     14,
+                     52
+                  ]
+               ]
+            },
+            ""relation"": ""within""
+         }
+      }
+   }');
+```
+
+bool查询：
+
+```
+select * from es_table where esquery('k4', ' {
+         ""bool"": {
+            ""must"": [
+               {
+                  ""terms"": {
+                     ""k1"": [
+                        11,
+                        12
+                     ]
+                  }
+               },
+               {
+                  ""terms"": {
+                     ""k2"": [
+                        100
+                     ]
+                  }
+               }
+            ]
+         }
+      }');
+```
+
+
+
+## 原理
+
+```              
++----------------------------------------------+
+|                                              |
+| Doris      +------------------+              |
+|            |       FE         +--------------+-------+
+|            |                  |  Request Shard Location
+|            +--+-------------+-+              |       |
+|               ^             ^                |       |
+|               |             |                |       |
+|  +-------------------+ +------------------+  |       |
+|  |            |      | |    |             |  |       |
+|  | +----------+----+ | | +--+-----------+ |  |       |
+|  | |      BE       | | | |      BE      | |  |       |
+|  | +---------------+ | | +--------------+ |  |       |
++----------------------------------------------+       |
+   |        |          | |        |         |          |
+   |        |          | |        |         |          |
+   |    HTTP SCROLL    | |    HTTP SCROLL   |          |
++-----------+---------------------+------------+       |
+|  |        v          | |        v         |  |       |
+|  | +------+--------+ | | +------+-------+ |  |       |
+|  | |               | | | |              | |  |       |
+|  | |   DataNode    | | | |   DataNode   +<-----------+
+|  | |               | | | |              | |  |       |
+|  | |               +<--------------------------------+
+|  | +---------------+ | | |--------------| |  |       |
+|  +-------------------+ +------------------+  |       |
+|   Same Physical Node                         |       |
+|                                              |       |
+|           +-----------------------+          |       |
+|           |                       |          |       |
+|           |      MasterNode       +<-----------------+
+| ES        |                       |          |
+|           +-----------------------+          |
++----------------------------------------------+
+
+
+```
+
+1. 创建ES外表后，FE会请求建表指定的主机，获取所有节点的HTTP端口信息以及index的shard分布信息等，如果请求失败会顺序遍历host列表直至成功或完全失败
+
+2. 查询时，会根据FE得到的一些节点信息和index的元数据信息，生成查询计划并发给对应的BE节点
+
+3. BE节点会根据`就近原则`即优先请求本地部署的ES节点，BE通过`HTTP Scroll`方式流式的从ES index的每个分片中并发的获取数据
+
+4. 计算完结果后，返回给client端
+
+## Push-Down operations
+`Doris On Elasticsearch`一个重要的功能就是过滤条件的下推: 过滤条件下推给ES，这样只有真正满足条件的数据才会被返回，能够显著的提高查询性能和降低Doris和Elasticsearch的CPU、memory、IO利用率
+
+下面的操作符(Operators)会被优化成如下下推filters:
+
+| SQL syntax  | ES 5.x+ syntax | 
+|-------|:---:|
+| =   | term query|
+| in  | terms query   |
+| > , < , >= , ⇐  | range   |
+| and  | bool.filter   |
+| or  | bool.should   |
+| not  | bool.must_not   |
+| not in  | bool.must_not + terms  |
+| esquery  | ES Query DSL   |
+
+
+## 其他说明
+
+1. ES的版本要求
+
+    ES主版本大于5，ES在2.x之前和5.x之后数据的扫描方式不同，目前支持5.x之后的
+2. 是否支持X-Pack认证的ES集群
+
+    支持所有使用http basic认证方式的ES集群","[{'comment': '```suggestion\r\n    支持所有使用HTTP basic认证方式的ES集群\r\n```', 'commenter': 'imay'}]"
1154,fe/src/main/java/org/apache/doris/load/loadv2/LoadJob.java,"@@ -102,9 +107,10 @@
 
     protected ReentrantReadWriteLock lock = new ReentrantReadWriteLock(true);
 
-    public LoadJob(long dbId, String label) {
+    public LoadJob(long dbId, String label, List<DataDescription> dataDescriptions) {","[{'comment': 'Does `mini_load` have `dataDescriptions`?', 'commenter': 'imay'}]"
1154,fe/src/main/java/org/apache/doris/load/loadv2/LoadJob.java,"@@ -303,18 +317,54 @@ public void cancelJobWithoutCheck(FailMsg failMsg) {
     public void cancelJob(FailMsg failMsg) throws DdlException {
         writeLock();
         try {
+            // check
             if (isCommitting) {
                 LOG.warn(new LogBuilder(LogKey.LOAD_JOB, id)
                                  .add(""error_msg"", ""The txn which belongs to job is committing. ""
                                          + ""The job could not be cancelled in this step"").build());
                 throw new DdlException(""Job could not be cancelled while txn is committing"");
             }
+            if (isFinished()) {
+                LOG.warn(new LogBuilder(LogKey.LOAD_JOB, id)
+                                 .add(""state"", state)
+                                 .add(""error_msg"", ""Job could not be cancelled when job is finished or cancelled"")
+                                 .build());
+                throw new DdlException(""Job could not be cancelled when job is finished or cancelled"");
+            }
+
+            checkAuth();
             executeCancel(failMsg);
         } finally {
             writeUnlock();
         }
     }
 
+    public void checkAuth() throws DdlException {","[{'comment': 'It is not a good idea that use `tableNames` to infer whether it is a `CANCEL` command\r\n\r\nyou can change the function to \r\n\r\n```suggestion\r\n    public void checkAuth(Set<String> tableNames) throws DdlException {\r\n```', 'commenter': 'imay'}, {'comment': 'Whether it is a CANCEL command, the tableNames is the required param of `checkAuth`. So maybe getting it inside of function is better?', 'commenter': 'EmmyMiao87'}, {'comment': 'It is better that `checkAuth` is a private method. If this is a public function, some caller may forget call this function before call other method', 'commenter': 'imay'}]"
1154,fe/src/main/java/org/apache/doris/load/loadv2/LoadManager.java,"@@ -101,6 +101,34 @@ private void addLoadJob(LoadJob loadJob) {
         loadJobScheduler.submitJob(loadJob);
     }
 
+    public void cancelLoadJob(CancelLoadStmt stmt) throws DdlException {
+        Database db = Catalog.getInstance().getDb(stmt.getDbName());
+        if (db == null) {
+            throw new DdlException(""Db does not exist. name: "" + stmt.getDbName());
+        }
+
+        LoadJob loadJob = null;
+        readLock();
+        try {
+            if (!dbIdToLabelToLoadJobs.containsKey(db.getId())) {
+                throw new DdlException(""Load job does not exist"");
+            }
+            if (!dbIdToLabelToLoadJobs.get(db.getId()).containsKey(stmt.getLabel())) {
+                throw new DdlException(""Load job does not exist"");
+            }
+            List<LoadJob> loadJobList = dbIdToLabelToLoadJobs.get(db.getId()).get(stmt.getLabel());","[{'comment': 'you execute `containKey` or `get` multiple times. please use a local variable to avoid that ', 'commenter': 'imay'}]"
1154,fe/src/main/java/org/apache/doris/load/loadv2/LoadManager.java,"@@ -113,10 +114,11 @@ public void cancelLoadJob(CancelLoadStmt stmt) throws DdlException {
             if (!dbIdToLabelToLoadJobs.containsKey(db.getId())) {
                 throw new DdlException(""Load job does not exist"");
             }
-            if (!dbIdToLabelToLoadJobs.get(db.getId()).containsKey(stmt.getLabel())) {
+            Map<String, List<LoadJob>> labelToLoadJobs = dbIdToLabelToLoadJobs.get(db.getId());
+            if (!labelToLoadJobs.containsKey(stmt.getLabel())) {
                 throw new DdlException(""Load job does not exist"");
             }
-            List<LoadJob> loadJobList = dbIdToLabelToLoadJobs.get(db.getId()).get(stmt.getLabel());
+            List<LoadJob> loadJobList = labelToLoadJobs.get(stmt.getLabel());","[{'comment': ""why don't you get and check if the result is null"", 'commenter': 'imay'}, {'comment': 'Because I think `containsKey` is more graceful then checking the result is null is more ', 'commenter': 'EmmyMiao87'}, {'comment': ""I don't agree with you, you get it twice. And in some cases, If another thread modify this map, your check can't make sure you can get a not null value"", 'commenter': 'imay'}, {'comment': 'Why you two hold opposite opinions /(ㄒoㄒ)/~~ @morningman ', 'commenter': 'EmmyMiao87'}, {'comment': 'I do not get it twice. The containsKey does not get the value, it just hash the key. Also, I add the read lock whether get and check null or containsKey', 'commenter': 'EmmyMiao87'}, {'comment': ""Hash the key is the first operation, `containsKey` will also check if two keys are equal, because hash of two keys may equal. And these operation's cost is less than a `get` operation.\r\n\r\nYes, in this case you add a readLock. But what I want to say is that it is good habit to check get's value "", 'commenter': 'imay'}, {'comment': 'Yes, you are right. If I only check the key, the containsKey is better. If I need to check and get in concurrent situation, the get and check null is better.', 'commenter': 'EmmyMiao87'}]"
1159,be/src/olap/olap_snapshot_converter.cpp,"@@ -497,4 +502,21 @@ OLAPStatus OlapSnapshotConverter::save(const string& file_path, const OLAPHeader
     return OLAP_SUCCESS;
 }
 
+void OlapSnapshotConverter::_modify_old_segment_group_id(RowsetMetaPB& rowset_meta) {
+    if (!rowset_meta.has_alpha_rowset_extra_meta_pb()) {
+        return;
+    }
+    AlphaRowsetExtraMetaPB* alpha_rowset_extra_meta_pb = rowset_meta.mutable_alpha_rowset_extra_meta_pb();
+    for (auto& segment_group_pb : alpha_rowset_extra_meta_pb->segment_groups()) {
+        if (segment_group_pb.segment_group_id() == -1) {
+            // check if segment groups size == 1
+            if (alpha_rowset_extra_meta_pb->segment_groups().size() != 1) {
+                LOG(FATAL) << ""the rowset has a segment group's id == -1 but it contains more than one segment group""
+                           << "" it should not happen"";
+            }
+            (const_cast<SegmentGroupPB&>(segment_group_pb)).set_segment_group_id(0);","[{'comment': 'do you rename the old file name to new file name when segment group id is -1?', 'commenter': 'kangpinghuang'}]"
1159,be/src/olap/olap_snapshot_converter.cpp,"@@ -417,6 +420,7 @@ OLAPStatus OlapSnapshotConverter::to_new_snapshot(const OLAPHeaderMessage& olap_
         RETURN_NOT_OK(rowset.init());
         std::vector<std::string> success_files;
         RETURN_NOT_OK(rowset.convert_from_old_files(old_data_path_prefix, &success_files));","[{'comment': 'I think add a judgement in _construct_file_name to handle segment_group_id = -1', 'commenter': 'chaoyli'}]"
1160,be/src/olap/segment_group.cpp,"@@ -568,6 +568,7 @@ OLAPStatus SegmentGroup::add_short_key(const RowCursor& short_key, const uint32_
             _check_io_error(res);
             return res;
         }
+        _file_created=true;","[{'comment': '```suggestion\r\n        _file_created = true;\r\n```', 'commenter': 'imay'}]"
1173,gensrc/thrift/PaloBrokerService.thrift,"@@ -126,10 +127,17 @@ struct TBrokerPReadRequest {
     4: required i64 length;
 }
 
+enum EWhence {","[{'comment': 'why EWhence, change to TSeekWhence\r\n\r\nT means thrift', 'commenter': 'imay'}, {'comment': 'i had changed', 'commenter': 'worker24h'}]"
1173,be/src/exec/scanner_interface.h,"@@ -0,0 +1,42 @@
+","[{'comment': 'license header', 'commenter': 'imay'}, {'comment': 'i had add', 'commenter': 'worker24h'}]"
1173,be/test/exec/broker_scanner_test.cpp,"@@ -98,7 +98,7 @@ void BrokerScannerTest::init_desc_table() {
             TTypeNode node;
             node.__set_type(TTypeNodeType::SCALAR);
             TScalarType scalar_type;
-            scalar_type.__set_type(TPrimitiveType::INT);","[{'comment': 'why you change this type???', 'commenter': 'imay'}]"
1173,fe/src/test/java/org/apache/doris/analysis/DataDescriptionTest.java,"@@ -54,20 +51,20 @@ public void setUp() {
     @Test
     public void testNormal() throws AnalysisException {
         DataDescription desc = new DataDescription(""testTable"", null, Lists.newArrayList(""abc.txt""),
-                null, null, false, null);
+                null, null, ""csv"", false, null);","[{'comment': ""you don't have to change this file, DataDescription has two constructor to comply with it"", 'commenter': 'imay'}, {'comment': 'Ok', 'commenter': 'worker24h'}]"
1173,gensrc/thrift/PaloBrokerService.thrift,"@@ -126,10 +127,17 @@ struct TBrokerPReadRequest {
     4: required i64 length;
 }
 
+enum TSeekWhence {
+    TSEEK_SET = 0;
+    TSEEK_CUR = 1;
+    TSEEK_END = 2;
+}
+
 struct TBrokerSeekRequest {
     1: required TBrokerVersion version;
     2: required TBrokerFD fd;
     3: required i64 offset;
+    4: required TSeekWhence whence;","[{'comment': 'new field should be optional', 'commenter': 'imay'}, {'comment': 'ok', 'commenter': 'worker24h'}]"
1173,gensrc/thrift/PaloBrokerService.thrift,"@@ -126,10 +127,17 @@ struct TBrokerPReadRequest {
     4: required i64 length;
 }
 
+enum TSeekWhence {","[{'comment': ""And you should change function seek's response with offset like fseek's response"", 'commenter': 'imay'}]"
1173,gensrc/thrift/PaloBrokerService.thrift,"@@ -67,6 +67,7 @@ struct TBrokerListResponse {
 struct TBrokerOpenReaderResponse {
     1: required TBrokerOperationStatus opStatus;
     2: optional TBrokerFD fd;
+    3: optional i64 size; //file size","[{'comment': 'Do we need this size? ', 'commenter': 'imay'}, {'comment': 'When read a parquet file by RPC, libparquet.a must kown the file size so that it can parse file. for example: When it read footer information.', 'commenter': 'worker24h'}]"
1173,fs_brokers/apache_hdfs_broker/src/main/java/org/apache/doris/broker/hdfs/FileSystemManager.java,"@@ -404,7 +403,7 @@ public boolean checkPathExist(String path, Map<String, String> properties) {
         }
     }
     
-    public TBrokerFD openReader(String clientId, String path, long startOffset, Map<String, String> properties) {
+    public TBrokerOpenReaderResponse openReader(String clientId, String path, long startOffset, Map<String, String> properties) {","[{'comment': ""this is a bad modification.\r\n\r\nIn this class, we can't return a RPC's response. We should handle RPC in RPC layer."", 'commenter': 'imay'}]"
1173,fe/src/main/java/org/apache/doris/load/BrokerFileGroup.java,"@@ -228,6 +231,8 @@ public void write(DataOutput out) throws IOException {
         Text.writeString(out, valueSeparator);
         // lineDelimiter
         Text.writeString(out, lineDelimiter);
+        // format type
+        Text.writeString(out, fileFormat);","[{'comment': ""you can't write this value without change META_VERSION. this can make frontend unable to load old image.\r\n\r\nNow, we don't persist this value until we have enough meta change."", 'commenter': 'imay'}, {'comment': 'ok', 'commenter': 'worker24h'}]"
1173,fe/src/main/java/org/apache/doris/catalog/BrokerTable.java,"@@ -222,7 +237,7 @@ public void readFields(DataInput in) throws IOException {
         }
         columnSeparator = Text.readString(in);
         lineDelimiter = Text.readString(in);
-
+        fileFormat = Text.readString(in);","[{'comment': 'Should not write format here', 'commenter': 'imay'}, {'comment': 'OK', 'commenter': 'worker24h'}]"
1173,be/test/exec/CMakeLists.txt,"@@ -35,11 +35,13 @@ ADD_BE_TEST(partitioned_hash_table_test)
 #ADD_BE_TEST(csv_scanner_test)
 #ADD_BE_TEST(csv_scan_node_test)
 # ADD_BE_TEST(csv_scan_bench_test)
+ADD_BE_TEST(parquet_scanner_test)
 ADD_BE_TEST(plain_text_line_reader_uncompressed_test)
 ADD_BE_TEST(plain_text_line_reader_gzip_test)
 ADD_BE_TEST(plain_text_line_reader_bzip_test)
 ADD_BE_TEST(plain_text_line_reader_lz4frame_test)
 ADD_BE_TEST(plain_text_line_reader_lzop_test)
+#ADD_BE_TEST(broker_parquet_reader_test)","[{'comment': 'why you not compile this UT', 'commenter': 'imay'}, {'comment': 'The ut depend on local real environment. for exmaple:    \r\n std::string path = ""hdfs://A.B.C.D:8020/user/hive/warehouse/test.db/parquet_test/000000_0"";\r\n', 'commenter': 'worker24h'}]"
1173,be/src/exec/broker_reader.cpp,"@@ -43,7 +43,7 @@ BrokerReader::BrokerReader(
             _path(path),
             _cur_offset(start_offset),
             _is_fd_valid(false),
-            _eof(false),
+            _file_size(0),","[{'comment': ""why remove _eof? \r\nI think you'd better not change old code if you have sufficient reason, because this can make review work hard and easy to lead error"", 'commenter': 'imay'}, {'comment': 'I think that the member **_eof**  is not useful. Because it has exist the input pararm eof of the function **read** ', 'commenter': 'worker24h'}]"
1173,be/src/exec/broker_reader.cpp,"@@ -156,24 +161,40 @@ Status BrokerReader::read(uint8_t* buf, size_t* buf_len, bool* eof) {
 
     if (response.opStatus.statusCode == TBrokerOperationStatusCode::END_OF_FILE) {
         // read the end of broker's file
-        *eof = _eof = true;
+        *bytes_read = 0;
         return Status::OK;
     } else if (response.opStatus.statusCode != TBrokerOperationStatusCode::OK) {
         std::stringstream ss;
-        ss << ""Read from broker failed, broker:"" << broker_addr 
+        ss << ""Read from broker failed, broker:"" << broker_addr
             << "" failed:"" << response.opStatus.message;
         LOG(WARNING) << ss.str();
         return Status(ss.str());
     }
 
-    *buf_len = response.data.size();
-    memcpy(buf, response.data.data(), *buf_len);
-    _cur_offset += *buf_len; 
-    *eof = false;
+    *bytes_read = response.data.size();
+    memcpy(out, response.data.data(), *bytes_read);
+    _cur_offset += *bytes_read;","[{'comment': ""and you don't set eof to false?"", 'commenter': 'imay'}]"
1173,be/src/exec/broker_scanner.h,"@@ -48,40 +49,26 @@ class MemTracker;
 class RuntimeProfile;
 class StreamLoadPipe;
 
-struct BrokerScanCounter {
-    BrokerScanCounter() :
-        num_rows_total(0),
-        // num_rows_returned(0),
-        num_rows_filtered(0),
-        num_rows_unselected(0) {
-    }
-    
-    int64_t num_rows_total; // total read rows (read from source)
-    // int64_t num_rows_returned;  // qualified rows (match the dest schema)
-    int64_t num_rows_filtered;  // unqualified rows (unmatch the dest schema, or no partition)
-    int64_t num_rows_unselected; // rows filterd by predicates
-};
-
 // Broker scanner convert the data read from broker to doris's tuple.
-class BrokerScanner {
+class BrokerScanner : public ScannerInterface {
 public:
     BrokerScanner(
         RuntimeState* state,
         RuntimeProfile* profile,
         const TBrokerScanRangeParams& params, 
         const std::vector<TBrokerRangeDesc>& ranges,
         const std::vector<TNetworkAddress>& broker_addresses,
-        BrokerScanCounter* counter);
+        ScannerCounter* counter);
     ~BrokerScanner();
 
-    // Open this scanner, will initialize informtion need to 
-    Status open();
+    // Open this scanner, will initialize information need to
+    virtual Status open();
 
-    // Get next tuple 
-    Status get_next(Tuple* tuple, MemPool* tuple_pool, bool* eof);
+    // Get next tuple
+    virtual Status get_next(Tuple* tuple, MemPool* tuple_pool, bool* eof);","[{'comment': '```suggestion\r\n   Status get_next(Tuple* tuple, MemPool* tuple_pool, bool* eof) override;\r\n```', 'commenter': 'imay'}, {'comment': 'ok i had changed.', 'commenter': 'worker24h'}]"
1173,be/src/exec/broker_scanner.h,"@@ -48,40 +49,26 @@ class MemTracker;
 class RuntimeProfile;
 class StreamLoadPipe;
 
-struct BrokerScanCounter {
-    BrokerScanCounter() :
-        num_rows_total(0),
-        // num_rows_returned(0),
-        num_rows_filtered(0),
-        num_rows_unselected(0) {
-    }
-    
-    int64_t num_rows_total; // total read rows (read from source)
-    // int64_t num_rows_returned;  // qualified rows (match the dest schema)
-    int64_t num_rows_filtered;  // unqualified rows (unmatch the dest schema, or no partition)
-    int64_t num_rows_unselected; // rows filterd by predicates
-};
-
 // Broker scanner convert the data read from broker to doris's tuple.
-class BrokerScanner {
+class BrokerScanner : public ScannerInterface {
 public:
     BrokerScanner(
         RuntimeState* state,
         RuntimeProfile* profile,
         const TBrokerScanRangeParams& params, 
         const std::vector<TBrokerRangeDesc>& ranges,
         const std::vector<TNetworkAddress>& broker_addresses,
-        BrokerScanCounter* counter);
+        ScannerCounter* counter);
     ~BrokerScanner();
 
-    // Open this scanner, will initialize informtion need to 
-    Status open();
+    // Open this scanner, will initialize information need to
+    virtual Status open();
 
-    // Get next tuple 
-    Status get_next(Tuple* tuple, MemPool* tuple_pool, bool* eof);
+    // Get next tuple
+    virtual Status get_next(Tuple* tuple, MemPool* tuple_pool, bool* eof);
 
     // Close this scanner
-    void close();
+    virtual void close();","[{'comment': '```suggestion\r\n   void close() override;\r\n```', 'commenter': 'imay'}, {'comment': 'ok i had changed.', 'commenter': 'worker24h'}]"
1173,be/src/exec/broker_scanner.h,"@@ -48,40 +49,26 @@ class MemTracker;
 class RuntimeProfile;
 class StreamLoadPipe;
 
-struct BrokerScanCounter {
-    BrokerScanCounter() :
-        num_rows_total(0),
-        // num_rows_returned(0),
-        num_rows_filtered(0),
-        num_rows_unselected(0) {
-    }
-    
-    int64_t num_rows_total; // total read rows (read from source)
-    // int64_t num_rows_returned;  // qualified rows (match the dest schema)
-    int64_t num_rows_filtered;  // unqualified rows (unmatch the dest schema, or no partition)
-    int64_t num_rows_unselected; // rows filterd by predicates
-};
-
 // Broker scanner convert the data read from broker to doris's tuple.
-class BrokerScanner {
+class BrokerScanner : public ScannerInterface {
 public:
     BrokerScanner(
         RuntimeState* state,
         RuntimeProfile* profile,
         const TBrokerScanRangeParams& params, 
         const std::vector<TBrokerRangeDesc>& ranges,
         const std::vector<TNetworkAddress>& broker_addresses,
-        BrokerScanCounter* counter);
+        ScannerCounter* counter);
     ~BrokerScanner();
 
-    // Open this scanner, will initialize informtion need to 
-    Status open();
+    // Open this scanner, will initialize information need to
+    virtual Status open();","[{'comment': '```suggestion\r\n    Status open() override;\r\n```', 'commenter': 'imay'}, {'comment': 'ok i had changed.', 'commenter': 'worker24h'}]"
1173,be/src/exec/ifile_reader.h,"@@ -0,0 +1,38 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// ""License""); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+#pragma once
+
+#include <stdint.h>
+
+#include ""common/status.h""
+
+namespace doris {
+class Tuple;
+class SlotDescriptor;
+class MemPool;
+class IFileReader {","[{'comment': 'what does this class want to do?', 'commenter': 'imay'}, {'comment': 'It is used parse parquet file. We will write the data of parquet file  to tuple-object so that i have define a interface class.', 'commenter': 'worker24h'}]"
