Pull,Path,Diff_hunk,Comment
7,flink-connector-netty/pom.xml,"@@ -0,0 +1,87 @@
+<?xml version=""1.0"" encoding=""UTF-8""?>
+<!--
+  ~ Licensed to the Apache Software Foundation (ASF) under one or more
+  ~ contributor license agreements.  See the NOTICE file distributed with
+  ~ this work for additional information regarding copyright ownership.
+  ~ The ASF licenses this file to You under the Apache License, Version 2.0
+  ~ (the ""License""); you may not use this file except in compliance with
+  ~ the License.  You may obtain a copy of the License at
+  ~
+  ~    http://www.apache.org/licenses/LICENSE-2.0
+  ~
+  ~ Unless required by applicable law or agreed to in writing, software
+  ~ distributed under the License is distributed on an ""AS IS"" BASIS,
+  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  ~ See the License for the specific language governing permissions and
+  ~ limitations under the License.
+  -->
+<project xmlns=""http://maven.apache.org/POM/4.0.0""
+         xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
+         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
+  <modelVersion>4.0.0</modelVersion>
+
+  <parent>
+    <groupId>org.apache.bahir</groupId>
+    <artifactId>bahir-flink_parent_2.11</artifactId>
+    <version>1.0.0-SNAPSHOT</version>
+    <relativePath>..</relativePath>
+  </parent>
+
+  <artifactId>flink-connector-netty_2.11</artifactId>
+  <name>flink-connector-netty</name>
+  <version>1.0.0-SNAPSHOT</version>
+  <packaging>jar</packaging>
+
+  <dependencies>
+    <dependency>
+      <groupId>io.netty</groupId>
+      <artifactId>netty-all</artifactId>
+      <version>4.1.5.Final</version>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.flink</groupId>
+      <artifactId>flink-table_${scala.binary.version}</artifactId>
+      <version>${flink.version}</version>
+    </dependency>","[{'comment': ""I don't think its a good idea to add the Table API here as a dependency just for one connector example.\n\nFlink Table could grow quite big in the future, and users of the connector will not execute a single line of the table API.\n"", 'commenter': 'rmetzger'}]"
7,flink-connector-netty/src/main/scala/org/apache/flink/streaming/connectors/netty/example/NettyUtil.scala,"@@ -0,0 +1,160 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.streaming.connectors.netty.example
+
+import java.io.{BufferedReader, InputStreamReader}
+import java.net._
+
+import org.apache.commons.lang3.SystemUtils
+import org.mortbay.util.MultiException
+import org.slf4j.LoggerFactory
+
+import scala.collection.JavaConverters._
+
+/**
+ * Netty Utility class for start netty service and retry tcp port
+ */
+object NettyUtil {
+  private lazy val logger = LoggerFactory.getLogger(getClass)
+
+  /** find local inet addresses */
+  def findLocalInetAddress(): InetAddress = {
+
+    val address = InetAddress.getLocalHost
+    address.isLoopbackAddress match {
+      case true =>
+        // Address resolves to something like 127.0.1.1, which happens on Debian; try to find
+        // a better address using the local network interfaces
+        // getNetworkInterfaces returns ifs in reverse order compared to ifconfig output order
+        // on unix-like system. On windows, it returns in index order.
+        // It's more proper to pick ip address following system output order.
+        val activeNetworkIFs = NetworkInterface.getNetworkInterfaces.asScala.toSeq
+        val reOrderedNetworkIFs = SystemUtils.IS_OS_WINDOWS match {
+          case true => activeNetworkIFs
+          case false => activeNetworkIFs.reverse
+        }
+
+        reOrderedNetworkIFs.find { ni: NetworkInterface =>
+          val addr = ni.getInetAddresses.asScala.toSeq.filterNot { addr =>
+            addr.isLinkLocalAddress || addr.isLoopbackAddress
+          }
+          addr.nonEmpty
+        } match {
+          case Some(ni) =>
+            val addr = ni.getInetAddresses.asScala.toSeq.filterNot { inet =>
+              inet.isLinkLocalAddress || inet.isLoopbackAddress
+            }
+            val address = addr.find(_.isInstanceOf[Inet4Address]).getOrElse(addr.head).getAddress
+            // because of Inet6Address.toHostName may add interface at the end if it knows about it
+            InetAddress.getByAddress(address)
+          case None => address
+        }
+      case false => address
+    }
+  }
+
+  /** start service, if port is collision, retry 128 times */
+  def startServiceOnPort[T](
+    startPort: Int,
+    startService: Int => T,
+    maxRetries: Int = 128,
+    serviceName: String = """"): T = {
+
+    if (startPort != 0 && (startPort < 1024 || startPort > 65536)) {
+      throw new Exception(""startPort should be between 1024 and 65535 (inclusive), "" +
+        ""or 0 for a random free port."")
+    }
+
+    val serviceString = if (serviceName.isEmpty) """" else s"" '$serviceName'""
+    for (offset <- 0 to maxRetries) {
+      // Do not increment port if startPort is 0, which is treated as a special port
+      val tryPort = if (startPort == 0) {
+        startPort
+      } else {
+        // If the new port wraps around, do not try a privilege port
+        ((startPort + offset - 1024) % (65536 - 1024)) + 1024
+      }
+
+      try {
+        val result = startService(tryPort)
+        logger.info(s""Successfully started service$serviceString, result:$result."")
+        return result
+      } catch {
+        case e: Exception if isBindCollision(e) =>
+          if (offset >= maxRetries) {
+            val exceptionMessage = s""${e.getMessage}: Service$serviceString failed after "" +
+              s""$maxRetries retries! Consider explicitly setting the appropriate port for the "" +
+              s""service$serviceString (for example spark.ui.port for SparkUI) to an available "" +
+              ""port or increasing spark.port.maxRetries.""","[{'comment': ""The exception message doesn't seem to be relevant to Flink.\n\nIn general, it seems that the code here seems to be copied from Apache Spark: https://github.com/apache/spark/blob/39755169fb5bb07332eef263b4c18ede1528812d/core/src/main/scala/org/apache/spark/util/Utils.scala#L2172\n\nCan you add comments to the code copied from other projects?\n"", 'commenter': 'rmetzger'}, {'comment': '@rmetzger OK, added\n', 'commenter': 'shijinkui'}, {'comment': ""@rmetzger I've added example to TcpReceiverSource and HttpReceiverSource. Also there are two full example in test package `TcpSourceExample` and `StreamSqlExample`\n"", 'commenter': 'shijinkui'}]"
8,flink-connector-akka/src/main/java/org/apache/flink/streaming/connectors/akka/utils/ReceiverActor.java,"@@ -0,0 +1,122 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.streaming.connectors.akka.utils;
+
+import akka.actor.ActorRef;
+import akka.actor.ActorSelection;
+import akka.actor.UntypedActor;
+import org.apache.flink.api.java.tuple.Tuple2;
+import org.apache.flink.streaming.api.functions.source.SourceFunction.SourceContext;
+
+import java.util.Iterator;
+
+/**
+ * Generalized receiver actor which receives messages
+ * from the feeder or publisher actor.
+ */
+public class ReceiverActor extends UntypedActor {
+  // --- Fields set by the constructor
+  private final SourceContext<Object> ctx;
+
+  private final String urlOfPublisher;
+
+  private final boolean autoAck;
+
+  // --- Runtime fields
+  private ActorSelection remotePublisher;
+
+  public ReceiverActor(SourceContext<Object> ctx,
+            String urlOfPublisher,
+            boolean autoAck) {
+    this.ctx = ctx;
+    this.urlOfPublisher = urlOfPublisher;
+    this.autoAck = autoAck;
+  }
+
+  @Override
+  public void preStart() throws Exception {
+    remotePublisher = getContext().actorSelection(urlOfPublisher);
+    remotePublisher.tell(new SubscribeReceiver(getSelf()), getSelf());
+  }
+
+  @SuppressWarnings(""unchecked"")
+  @Override
+  public void onReceive(Object message)
+    throws Exception {
+    if (message instanceof Iterable) {
+      collect((Iterable<Object>) message);
+    } else if (message instanceof byte[]) {
+      byte[] messageBytes = (byte[]) message;
+      collect(messageBytes);
+    } else if (message instanceof Tuple2) {
+      Tuple2<Object, Long> messageTuple = (Tuple2<Object, Long>) message;
+      collect(messageTuple.f0, messageTuple.f1);
+    } else {
+      collect(message);
+    }
+
+    if (autoAck) {
+      getSender().tell(""ack"", getSelf());
+    }
+  }
+
+  /**
+   * To handle {@link Iterable} data
+   *
+   * @param data data received from feeder actor
+   */
+  private void collect(Iterable<Object> data) {
+    Iterator<Object> iterator = data.iterator();
+    while (iterator.hasNext()) {
+      ctx.collect(iterator.next());
+    }
+  }
+
+  /**
+   * To handle byte array data
+   *
+   * @param bytes data received from feeder actor
+   */
+  private void collect(byte[] bytes) {","[{'comment': 'What is the purpose of this message? There is no special treatment for byte arrays', 'commenter': 'rmetzger'}]"
8,flink-connector-akka/src/main/java/org/apache/flink/streaming/connectors/akka/AkkaSource.java,"@@ -0,0 +1,147 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.streaming.connectors.akka;
+
+import akka.actor.ActorRef;
+import akka.actor.ActorSystem;
+import akka.actor.PoisonPill;
+import akka.actor.Props;
+import com.typesafe.config.Config;
+import com.typesafe.config.ConfigFactory;
+import org.apache.flink.api.common.functions.RuntimeContext;
+import org.apache.flink.api.common.functions.StoppableFunction;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.streaming.api.functions.source.RichSourceFunction;
+import org.apache.flink.streaming.api.functions.source.SourceFunction;
+import org.apache.flink.streaming.api.operators.StreamingRuntimeContext;
+import org.apache.flink.streaming.connectors.akka.utils.ReceiverActor;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * Implementation of {@link SourceFunction} specialized to read messages
+ * from Akka actors.
+ */
+public class AkkaSource extends RichSourceFunction<Object>
+  implements StoppableFunction {
+
+  private static final Logger LOG = LoggerFactory.getLogger(AkkaSource.class);
+
+  private static final long serialVersionUID = 1L;
+
+  // --- Fields set by the constructor
+
+  private final Class<?> classForActor;
+
+  private final String actorName;
+
+  private final String urlOfPublisher;
+
+  // --- Runtime fields
+  private transient ActorSystem receiverActorSystem;
+  private transient ActorRef receiverActor;
+  private transient Object waitLock;
+  private transient boolean running = true;
+
+  protected transient boolean autoAck;
+
+  /**
+   * Creates {@link AkkaSource} for Streaming
+   *
+   * @param actorName Receiver Actor name
+   * @param urlOfPublisher tcp url of the publisher or feeder actor
+   */
+  public AkkaSource(String actorName,
+          String urlOfPublisher) {
+    super();
+    this.classForActor = ReceiverActor.class;
+    this.actorName = actorName;
+    this.urlOfPublisher = urlOfPublisher;
+  }
+
+  @Override
+  public void open(Configuration parameters) throws Exception {
+    waitLock = new Object();
+    receiverActorSystem = createDefaultActorSystem();
+
+    RuntimeContext runtimeContext = getRuntimeContext();
+    if (runtimeContext instanceof StreamingRuntimeContext
+      && ((StreamingRuntimeContext) runtimeContext).isCheckpointingEnabled()) {
+      autoAck = false;
+    } else {
+      autoAck = true;
+    }
+  }
+
+  @Override
+  public void run(SourceFunction.SourceContext<Object> ctx) throws Exception {
+    LOG.info(""Starting the Receiver actor {}"", actorName);
+    receiverActor = receiverActorSystem.actorOf(
+      Props.create(classForActor, ctx, urlOfPublisher, autoAck), actorName);
+
+    running = true;
+    LOG.info(""Started the Receiver actor {} successfully"", actorName);
+
+    while (running) {
+      synchronized (waitLock) {","[{'comment': ""I'm not an akka expert, but can't we call `receiverActorSystem.awaitTermination();` instead here?"", 'commenter': 'rmetzger'}]"
8,flink-connector-akka/src/main/java/org/apache/flink/streaming/connectors/akka/AkkaSource.java,"@@ -0,0 +1,147 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.streaming.connectors.akka;
+
+import akka.actor.ActorRef;
+import akka.actor.ActorSystem;
+import akka.actor.PoisonPill;
+import akka.actor.Props;
+import com.typesafe.config.Config;
+import com.typesafe.config.ConfigFactory;
+import org.apache.flink.api.common.functions.RuntimeContext;
+import org.apache.flink.api.common.functions.StoppableFunction;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.streaming.api.functions.source.RichSourceFunction;
+import org.apache.flink.streaming.api.functions.source.SourceFunction;
+import org.apache.flink.streaming.api.operators.StreamingRuntimeContext;
+import org.apache.flink.streaming.connectors.akka.utils.ReceiverActor;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * Implementation of {@link SourceFunction} specialized to read messages
+ * from Akka actors.
+ */
+public class AkkaSource extends RichSourceFunction<Object>
+  implements StoppableFunction {
+
+  private static final Logger LOG = LoggerFactory.getLogger(AkkaSource.class);
+
+  private static final long serialVersionUID = 1L;
+
+  // --- Fields set by the constructor
+
+  private final Class<?> classForActor;
+
+  private final String actorName;
+
+  private final String urlOfPublisher;
+
+  // --- Runtime fields
+  private transient ActorSystem receiverActorSystem;
+  private transient ActorRef receiverActor;
+  private transient Object waitLock;
+  private transient boolean running = true;
+
+  protected transient boolean autoAck;
+
+  /**
+   * Creates {@link AkkaSource} for Streaming
+   *
+   * @param actorName Receiver Actor name
+   * @param urlOfPublisher tcp url of the publisher or feeder actor
+   */
+  public AkkaSource(String actorName,
+          String urlOfPublisher) {
+    super();
+    this.classForActor = ReceiverActor.class;
+    this.actorName = actorName;
+    this.urlOfPublisher = urlOfPublisher;
+  }
+
+  @Override
+  public void open(Configuration parameters) throws Exception {
+    waitLock = new Object();
+    receiverActorSystem = createDefaultActorSystem();
+
+    RuntimeContext runtimeContext = getRuntimeContext();
+    if (runtimeContext instanceof StreamingRuntimeContext
+      && ((StreamingRuntimeContext) runtimeContext).isCheckpointingEnabled()) {
+      autoAck = false;
+    } else {
+      autoAck = true;
+    }
+  }
+
+  @Override
+  public void run(SourceFunction.SourceContext<Object> ctx) throws Exception {
+    LOG.info(""Starting the Receiver actor {}"", actorName);
+    receiverActor = receiverActorSystem.actorOf(
+      Props.create(classForActor, ctx, urlOfPublisher, autoAck), actorName);
+
+    running = true;
+    LOG.info(""Started the Receiver actor {} successfully"", actorName);
+
+    while (running) {
+      synchronized (waitLock) {
+        waitLock.wait(100L);
+      }
+    }
+  }
+
+  @Override
+  public void close() {
+    this.running = false;
+    LOG.info(""Closing source"");
+    if (receiverActorSystem != null) {
+      receiverActor.tell(PoisonPill.getInstance(), ActorRef.noSender());
+      receiverActorSystem.shutdown();
+      receiverActorSystem.awaitTermination();
+    }
+    synchronized (waitLock) {
+      waitLock.notify();
+    }
+  }
+
+  @Override
+  public void cancel() {
+    LOG.info(""Cancelling akka source"");
+    close();
+  }
+
+  @Override
+  public void stop() {
+    LOG.info(""Stopping akka source"");
+    close();
+  }
+
+  /**
+   * Creates an actor system with default configurations for Receiver actor.
+   *
+   * @return Actor System instance with default configurations
+   */
+  private ActorSystem createDefaultActorSystem() {
+    String defaultActorSystemName = ""receiver-actor-system"";
+
+    String configString = ""akka.actor.provider = \""akka.remote.RemoteActorRefProvider\""\n"" +
+      ""akka.remote.enabled-transports = [\""akka.remote.netty.tcp\""]"";
+    Config defaultConfig = ConfigFactory.parseString(configString);","[{'comment': 'I wonder if we should allow users of the source to add more configuration options.', 'commenter': 'rmetzger'}]"
8,flink-connector-akka/src/main/java/org/apache/flink/streaming/connectors/akka/AkkaSource.java,"@@ -0,0 +1,147 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.streaming.connectors.akka;
+
+import akka.actor.ActorRef;
+import akka.actor.ActorSystem;
+import akka.actor.PoisonPill;
+import akka.actor.Props;
+import com.typesafe.config.Config;
+import com.typesafe.config.ConfigFactory;
+import org.apache.flink.api.common.functions.RuntimeContext;
+import org.apache.flink.api.common.functions.StoppableFunction;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.streaming.api.functions.source.RichSourceFunction;
+import org.apache.flink.streaming.api.functions.source.SourceFunction;
+import org.apache.flink.streaming.api.operators.StreamingRuntimeContext;
+import org.apache.flink.streaming.connectors.akka.utils.ReceiverActor;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * Implementation of {@link SourceFunction} specialized to read messages
+ * from Akka actors.
+ */
+public class AkkaSource extends RichSourceFunction<Object>
+  implements StoppableFunction {
+
+  private static final Logger LOG = LoggerFactory.getLogger(AkkaSource.class);
+
+  private static final long serialVersionUID = 1L;
+
+  // --- Fields set by the constructor
+
+  private final Class<?> classForActor;
+
+  private final String actorName;
+
+  private final String urlOfPublisher;
+
+  // --- Runtime fields
+  private transient ActorSystem receiverActorSystem;
+  private transient ActorRef receiverActor;
+  private transient Object waitLock;
+  private transient boolean running = true;
+
+  protected transient boolean autoAck;
+
+  /**
+   * Creates {@link AkkaSource} for Streaming
+   *
+   * @param actorName Receiver Actor name
+   * @param urlOfPublisher tcp url of the publisher or feeder actor
+   */
+  public AkkaSource(String actorName,
+          String urlOfPublisher) {
+    super();
+    this.classForActor = ReceiverActor.class;
+    this.actorName = actorName;
+    this.urlOfPublisher = urlOfPublisher;
+  }
+
+  @Override
+  public void open(Configuration parameters) throws Exception {
+    waitLock = new Object();
+    receiverActorSystem = createDefaultActorSystem();
+
+    RuntimeContext runtimeContext = getRuntimeContext();
+    if (runtimeContext instanceof StreamingRuntimeContext
+      && ((StreamingRuntimeContext) runtimeContext).isCheckpointingEnabled()) {
+      autoAck = false;
+    } else {
+      autoAck = true;","[{'comment': 'Why is the acking dependent on the checkpointing?\r\nMaybe it would make sense to allow the user to configure this independently.', 'commenter': 'rmetzger'}]"
8,flink-connector-akka/src/main/java/org/apache/flink/streaming/connectors/akka/AkkaSource.java,"@@ -0,0 +1,174 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.streaming.connectors.akka;
+
+import akka.actor.ActorRef;
+import akka.actor.ActorSystem;
+import akka.actor.PoisonPill;
+import akka.actor.Props;
+import com.typesafe.config.Config;
+import com.typesafe.config.ConfigFactory;
+import org.apache.flink.api.common.functions.StoppableFunction;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.streaming.api.functions.source.RichSourceFunction;
+import org.apache.flink.streaming.api.functions.source.SourceFunction;
+import org.apache.flink.streaming.connectors.akka.utils.ReceiverActor;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.Map;
+import java.util.Properties;
+
+/**
+ * Implementation of {@link SourceFunction} specialized to read messages
+ * from Akka actors.
+ */
+public class AkkaSource extends RichSourceFunction<Object>
+  implements StoppableFunction {
+
+  private static final Logger LOG = LoggerFactory.getLogger(AkkaSource.class);
+
+  private static final long serialVersionUID = 1L;
+
+  // --- Fields set by the constructor
+
+  private final Class<?> classForActor;
+
+  private final String actorName;
+
+  private final String urlOfPublisher;
+
+  // --- Runtime fields
+  private transient ActorSystem receiverActorSystem;
+  private transient ActorRef receiverActor;
+  private transient Object waitLock;
+  private transient boolean running = true;
+
+  protected transient boolean autoAck;
+
+  /**
+   * Creates {@link AkkaSource} for Streaming
+   *
+   * @param actorName Receiver Actor name
+   * @param urlOfPublisher tcp url of the publisher or feeder actor
+   */
+  public AkkaSource(String actorName,
+          String urlOfPublisher) {
+    super();
+    this.classForActor = ReceiverActor.class;
+    this.actorName = actorName;
+    this.urlOfPublisher = urlOfPublisher;
+  }
+
+  @Override
+  public void open(Configuration parameters) throws Exception {
+    Properties customProperties = new Properties();
+    parameters.addAllToProperties(customProperties);
+
+    waitLock = new Object();
+    receiverActorSystem = createDefaultActorSystem(customProperties);
+
+    if (customProperties.containsKey(""akka.remote.auto-ack"") &&
+      customProperties.getProperty(""akka.remote.auto-ack"").equals(""on"")) {
+      autoAck = true;
+    } else {
+      autoAck = false;
+    }
+  }
+
+  @Override
+  public void run(SourceFunction.SourceContext<Object> ctx) throws Exception {
+    LOG.info(""Starting the Receiver actor {}"", actorName);
+    receiverActor = receiverActorSystem.actorOf(
+      Props.create(classForActor, ctx, urlOfPublisher, autoAck), actorName);
+
+    running = true;
+    LOG.info(""Started the Receiver actor {} successfully"", actorName);
+    receiverActorSystem.awaitTermination();
+
+    while (running) {
+      synchronized (waitLock) {
+        waitLock.wait(100L);","[{'comment': ""I don't think the wait lock is needed anymore when doing `awaitTermination()`."", 'commenter': 'rmetzger'}]"
8,flink-connector-akka/README.md,"@@ -0,0 +1,45 @@
+# Flink Akka connector
+
+This connector provides a sink to [Akka](http://akka.io/) source actors in an ActorSystem.
+To use this connector, add the following dependency to your project:
+
+
+    <dependency>
+      <groupId>org.apache.bahir</groupId>
+      <artifactId>flink-connector-akka_2.11</artifactId>
+      <version>1.0.0-SNAPSHOT</version>
+    </dependency>
+    
+*Version Compatibility*: This module is compatible with Akka 2.0+.
+    
+## Configuration
+    
+The configurations for the Receiver Actor System in Flink Akka connector can be created using the `Configuration (org.apache.flink.configuration.Configuration)` object in Flink.
+    
+To enable acknowledgements, the custom configuration `akka.remote.auto-ack` can be used.
+
+The user can set any of the default configuration allowed by Akka as well as custom configuration allowed by the connector.
+   
+A sample configuration can be defined as follows:
+    
+    Configuration configuration = new Configuration();
+    configuration.setString(""akka.loglevel"", ""INFO"");
+    configuration.setString(""akka.actor.provider"", ""akka.remote.RemoteActorRefProvider"");
+    configuration.setString(""akka.remote.netty.tcp.hostname"", ""127.0.0.1"");
+    configuration.setString(""akka.remote.enabled-transports"", ""[akka.remote.netty.tcp]"");
+    configuration.setString(""akka.remote.netty.tcp.port"", ""5150"");
+    configuration.setString(""akka.remote.log-sent-messages"", ""on"");
+    configuration.setString(""akka.remote.log-received-messages"", ""on"");
+    configuration.setString(""akka.remote.auto-ack"", ""on"");    ","[{'comment': 'How can a user pass the `configuration` to the Akka source? Afaik its not possible because the open(Configuration c) is not really supported in the DataStream API of Flink.', 'commenter': 'rmetzger'}]"
8,flink-connector-akka/src/main/java/org/apache/flink/streaming/connectors/akka/utils/ReceiverActor.java,"@@ -0,0 +1,113 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.streaming.connectors.akka.utils;
+
+import akka.actor.ActorRef;
+import akka.actor.ActorSelection;
+import akka.actor.UntypedActor;
+import org.apache.flink.api.java.tuple.Tuple2;
+import org.apache.flink.streaming.api.functions.source.SourceFunction.SourceContext;
+
+import java.util.Iterator;
+
+/**
+ * Generalized receiver actor which receives messages
+ * from the feeder or publisher actor.
+ */
+public class ReceiverActor extends UntypedActor {
+  // --- Fields set by the constructor
+  private final SourceContext<Object> ctx;
+
+  private final String urlOfPublisher;
+
+  private final boolean autoAck;
+
+  // --- Runtime fields
+  private ActorSelection remotePublisher;
+
+  public ReceiverActor(SourceContext<Object> ctx,
+            String urlOfPublisher,
+            boolean autoAck) {
+    this.ctx = ctx;
+    this.urlOfPublisher = urlOfPublisher;
+    this.autoAck = autoAck;
+  }
+
+  @Override
+  public void preStart() throws Exception {
+    remotePublisher = getContext().actorSelection(urlOfPublisher);
+    remotePublisher.tell(new SubscribeReceiver(getSelf()), getSelf());
+  }
+
+  @SuppressWarnings(""unchecked"")
+  @Override
+  public void onReceive(Object message)
+    throws Exception {
+    if (message instanceof Iterable) {
+      collect((Iterable<Object>) message);
+    } else if (message instanceof byte[]) {","[{'comment': ""I don't understand this case. You are casing an Object into a byte[], but collect() expects only a Object."", 'commenter': 'rmetzger'}]"
14,dev/release-build.sh,"@@ -0,0 +1,330 @@
+#!/usr/bin/env bash
+
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the ""License""); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+function exit_with_usage {
+  cat << EOF
+
+release-build - Creates build distributions from a git commit hash or from HEAD.
+
+SYNOPSIS
+
+usage: release-build.sh [--release-prepare | --release-publish | --release-snapshot]
+
+DESCRIPTION
+
+Use maven infrastructure to create a project release package and publish
+to staging release location (https://dist.apache.org/repos/dist/dev/bahir-flink)
+and maven staging release repository.
+
+--release-prepare --releaseVersion=""1.0"" --developmentVersion=""1.1-SNAPSHOT"" [--releaseRc=""rc1""] [--tag=""v1.0""] [--gitCommitHash=""a874b73""]
+This form execute maven release:prepare and upload the release candidate distribution
+to the staging release location.
+
+--release-publish --gitCommitHash=""a874b73""
+Publish the maven artifacts of a release to the Apache staging maven repository.
+Note that this will publish both Scala 2.10 and 2.11 artifacts.
+
+--release-snapshot [--gitCommitHash=""a874b73""]
+Publish the maven snapshot artifacts to Apache snapshots maven repository
+Note that this will publish both Scala 2.10 and 2.11 artifacts.
+
+OPTIONS
+
+--releaseVersion     - Release identifier used when publishing
+--developmentVersion - Release identifier used for next development cycle
+--releaseRc          - Release RC identifier used when publishing, default 'rc1'
+--tag                - Release Tag identifier used when taging the release, default 'v$releaseVersion'
+--gitCommitHash      - Release tag or commit to build from, default master HEAD
+--dryRun             - Dry run only, mostly used for testing.
+
+A GPG passphrase is expected as an environment variable
+
+GPG_PASSPHRASE - Passphrase for GPG key used to sign release
+
+EXAMPLES
+
+release-build.sh --release-prepare --releaseVersion=""1.0"" --developmentVersion=""1.1-SNAPSHOT""
+release-build.sh --release-prepare --releaseVersion=""1.0"" --developmentVersion=""1.1-SNAPSHOT"" --releaseRc=""rc1"" --tag=""v1.0""
+release-build.sh --release-prepare --releaseVersion=""1.0"" --developmentVersion=""1.1-SNAPSHOT"" --releaseRc=""rc1"" --tag=""v1.0""  --gitCommitHash=""a874b73"" --dryRun
+
+release-build.sh --release-publish --gitCommitHash=""a874b73""
+release-build.sh --release-publish --gitTag=""v1.0rc1""
+
+release-build.sh --release-snapshot
+release-build.sh --release-snapshot --gitCommitHash=""a874b73""
+
+EOF
+  exit 1
+}
+
+set -e
+
+if [ $# -eq 0 ]; then
+  exit_with_usage
+fi
+
+
+# Process each provided argument configuration
+while [ ""${1+defined}"" ]; do
+  IFS=""="" read -ra PARTS <<< ""$1""
+  case ""${PARTS[0]}"" in
+    --release-prepare)
+      GOAL=""release-prepare""
+      RELEASE_PREPARE=true
+      shift
+      ;;
+    --release-publish)
+      GOAL=""release-publish""
+      RELEASE_PUBLISH=true
+      shift
+      ;;
+    --release-snapshot)
+      GOAL=""release-snapshot""
+      RELEASE_SNAPSHOT=true
+      shift
+      ;;
+    --gitCommitHash)
+      GIT_REF=""${PARTS[1]}""
+      shift
+      ;;
+    --gitTag)
+      GIT_TAG=""${PARTS[1]}""
+      shift
+      ;;
+    --releaseVersion)
+      RELEASE_VERSION=""${PARTS[1]}""
+      shift
+      ;;
+    --developmentVersion)
+      DEVELOPMENT_VERSION=""${PARTS[1]}""
+      shift
+      ;;
+    --releaseRc)
+      RELEASE_RC=""${PARTS[1]}""
+      shift
+      ;;
+    --tag)
+      RELEASE_TAG=""${PARTS[1]}""
+      shift
+      ;;
+    --dryRun)
+      DRY_RUN=""-DdryRun=true""
+      shift
+      ;;
+
+    *help* | -h)
+      exit_with_usage
+     exit 0
+     ;;
+    -*)
+     echo ""Error: Unknown option: $1"" >&2
+     exit 1
+     ;;
+    *)  # No more options
+     break
+     ;;
+  esac
+done
+
+
+if [[ -z ""$GPG_PASSPHRASE"" ]]; then
+    echo 'The environment variable GPG_PASSPHRASE is not set. Enter the passphrase to'
+    echo 'unlock the GPG signing key that will be used to sign the release!'
+    echo
+    stty -echo && printf ""GPG passphrase: "" && read GPG_PASSPHRASE && printf '\n' && stty echo
+  fi
+
+if [[ ""$RELEASE_PREPARE"" == ""true"" && -z ""$RELEASE_VERSION"" ]]; then
+    echo ""ERROR: --releaseVersion must be passed as an argument to run this script""
+    exit_with_usage
+fi
+
+if [[ ""$RELEASE_PREPARE"" == ""true"" && -z ""$DEVELOPMENT_VERSION"" ]]; then
+    echo ""ERROR: --developmentVersion must be passed as an argument to run this script""
+    exit_with_usage
+fi
+
+if [[ ""$RELEASE_PUBLISH"" == ""true""  ]]; then
+    if [[ ""$GIT_REF"" && ""$GIT_TAG"" ]]; then
+        echo ""ERROR: Only one argumented permitted when publishing : --gitCommitHash or --gitTag""
+        exit_with_usage
+    fi
+    if [[ -z ""$GIT_REF"" && -z ""$GIT_TAG"" ]]; then
+        echo ""ERROR: --gitCommitHash OR --gitTag must be passed as an argument to run this script""
+        exit_with_usage
+    fi
+fi
+
+if [[ ""$RELEASE_PUBLISH"" == ""true"" && ""$DRY_RUN"" ]]; then
+    echo ""ERROR: --dryRun not supported for --release-publish""
+    exit_with_usage
+fi
+
+if [[ ""$RELEASE_SNAPSHOT"" == ""true"" && ""$DRY_RUN"" ]]; then
+    echo ""ERROR: --dryRun not supported for --release-publish""
+    exit_with_usage
+fi
+
+# Commit ref to checkout when building
+GIT_REF=${GIT_REF:-master}
+if [[ ""$RELEASE_PUBLISH"" == ""true"" && ""$GIT_TAG"" ]]; then
+    GIT_REF=""tags/$GIT_TAG""
+fi
+
+BASE_DIR=$(pwd)
+
+MVN=""mvn""
+PUBLISH_PROFILES=""-Pdistribution""
+
+if [ -z ""$RELEASE_RC"" ]; then
+  RELEASE_RC=""rc1""
+fi
+
+if [ -z ""$RELEASE_TAG"" ]; then
+  RELEASE_TAG=""v$RELEASE_VERSION-$RELEASE_RC""
+fi
+
+RELEASE_STAGING_LOCATION=""https://dist.apache.org/repos/dist/dev/bahir-flink/""","[{'comment': 'The release svn location has the pattern of one folder per project, so I would say we would need to have something like bahir/bahir-flink, and probably also change bahir to be bahir/bahir-spark', 'commenter': 'lresende'}, {'comment': 'Ah, I see. +1 to this pattern, changing.', 'commenter': 'tzulitai'}]"
14,dev/release-build.sh,"@@ -0,0 +1,330 @@
+#!/usr/bin/env bash
+
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the ""License""); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+function exit_with_usage {
+  cat << EOF
+
+release-build - Creates build distributions from a git commit hash or from HEAD.
+
+SYNOPSIS
+
+usage: release-build.sh [--release-prepare | --release-publish | --release-snapshot]
+
+DESCRIPTION
+
+Use maven infrastructure to create a project release package and publish
+to staging release location (https://dist.apache.org/repos/dist/dev/bahir-flink)
+and maven staging release repository.
+
+--release-prepare --releaseVersion=""1.0"" --developmentVersion=""1.1-SNAPSHOT"" [--releaseRc=""rc1""] [--tag=""v1.0""] [--gitCommitHash=""a874b73""]
+This form execute maven release:prepare and upload the release candidate distribution
+to the staging release location.
+
+--release-publish --gitCommitHash=""a874b73""
+Publish the maven artifacts of a release to the Apache staging maven repository.
+Note that this will publish both Scala 2.10 and 2.11 artifacts.
+
+--release-snapshot [--gitCommitHash=""a874b73""]
+Publish the maven snapshot artifacts to Apache snapshots maven repository
+Note that this will publish both Scala 2.10 and 2.11 artifacts.
+
+OPTIONS
+
+--releaseVersion     - Release identifier used when publishing
+--developmentVersion - Release identifier used for next development cycle
+--releaseRc          - Release RC identifier used when publishing, default 'rc1'
+--tag                - Release Tag identifier used when taging the release, default 'v$releaseVersion'
+--gitCommitHash      - Release tag or commit to build from, default master HEAD
+--dryRun             - Dry run only, mostly used for testing.
+
+A GPG passphrase is expected as an environment variable
+
+GPG_PASSPHRASE - Passphrase for GPG key used to sign release
+
+EXAMPLES
+
+release-build.sh --release-prepare --releaseVersion=""1.0"" --developmentVersion=""1.1-SNAPSHOT""
+release-build.sh --release-prepare --releaseVersion=""1.0"" --developmentVersion=""1.1-SNAPSHOT"" --releaseRc=""rc1"" --tag=""v1.0""
+release-build.sh --release-prepare --releaseVersion=""1.0"" --developmentVersion=""1.1-SNAPSHOT"" --releaseRc=""rc1"" --tag=""v1.0""  --gitCommitHash=""a874b73"" --dryRun
+
+release-build.sh --release-publish --gitCommitHash=""a874b73""
+release-build.sh --release-publish --gitTag=""v1.0rc1""
+
+release-build.sh --release-snapshot
+release-build.sh --release-snapshot --gitCommitHash=""a874b73""
+
+EOF
+  exit 1
+}
+
+set -e
+
+if [ $# -eq 0 ]; then
+  exit_with_usage
+fi
+
+
+# Process each provided argument configuration
+while [ ""${1+defined}"" ]; do
+  IFS=""="" read -ra PARTS <<< ""$1""
+  case ""${PARTS[0]}"" in
+    --release-prepare)
+      GOAL=""release-prepare""
+      RELEASE_PREPARE=true
+      shift
+      ;;
+    --release-publish)
+      GOAL=""release-publish""
+      RELEASE_PUBLISH=true
+      shift
+      ;;
+    --release-snapshot)
+      GOAL=""release-snapshot""
+      RELEASE_SNAPSHOT=true
+      shift
+      ;;
+    --gitCommitHash)
+      GIT_REF=""${PARTS[1]}""
+      shift
+      ;;
+    --gitTag)
+      GIT_TAG=""${PARTS[1]}""
+      shift
+      ;;
+    --releaseVersion)
+      RELEASE_VERSION=""${PARTS[1]}""
+      shift
+      ;;
+    --developmentVersion)
+      DEVELOPMENT_VERSION=""${PARTS[1]}""
+      shift
+      ;;
+    --releaseRc)
+      RELEASE_RC=""${PARTS[1]}""
+      shift
+      ;;
+    --tag)
+      RELEASE_TAG=""${PARTS[1]}""
+      shift
+      ;;
+    --dryRun)
+      DRY_RUN=""-DdryRun=true""
+      shift
+      ;;
+
+    *help* | -h)
+      exit_with_usage
+     exit 0
+     ;;
+    -*)
+     echo ""Error: Unknown option: $1"" >&2
+     exit 1
+     ;;
+    *)  # No more options
+     break
+     ;;
+  esac
+done
+
+
+if [[ -z ""$GPG_PASSPHRASE"" ]]; then
+    echo 'The environment variable GPG_PASSPHRASE is not set. Enter the passphrase to'
+    echo 'unlock the GPG signing key that will be used to sign the release!'
+    echo
+    stty -echo && printf ""GPG passphrase: "" && read GPG_PASSPHRASE && printf '\n' && stty echo
+  fi
+
+if [[ ""$RELEASE_PREPARE"" == ""true"" && -z ""$RELEASE_VERSION"" ]]; then
+    echo ""ERROR: --releaseVersion must be passed as an argument to run this script""
+    exit_with_usage
+fi
+
+if [[ ""$RELEASE_PREPARE"" == ""true"" && -z ""$DEVELOPMENT_VERSION"" ]]; then
+    echo ""ERROR: --developmentVersion must be passed as an argument to run this script""
+    exit_with_usage
+fi
+
+if [[ ""$RELEASE_PUBLISH"" == ""true""  ]]; then
+    if [[ ""$GIT_REF"" && ""$GIT_TAG"" ]]; then
+        echo ""ERROR: Only one argumented permitted when publishing : --gitCommitHash or --gitTag""
+        exit_with_usage
+    fi
+    if [[ -z ""$GIT_REF"" && -z ""$GIT_TAG"" ]]; then
+        echo ""ERROR: --gitCommitHash OR --gitTag must be passed as an argument to run this script""
+        exit_with_usage
+    fi
+fi
+
+if [[ ""$RELEASE_PUBLISH"" == ""true"" && ""$DRY_RUN"" ]]; then
+    echo ""ERROR: --dryRun not supported for --release-publish""
+    exit_with_usage
+fi
+
+if [[ ""$RELEASE_SNAPSHOT"" == ""true"" && ""$DRY_RUN"" ]]; then
+    echo ""ERROR: --dryRun not supported for --release-publish""
+    exit_with_usage
+fi
+
+# Commit ref to checkout when building
+GIT_REF=${GIT_REF:-master}
+if [[ ""$RELEASE_PUBLISH"" == ""true"" && ""$GIT_TAG"" ]]; then
+    GIT_REF=""tags/$GIT_TAG""
+fi
+
+BASE_DIR=$(pwd)
+
+MVN=""mvn""
+PUBLISH_PROFILES=""-Pdistribution""
+
+if [ -z ""$RELEASE_RC"" ]; then
+  RELEASE_RC=""rc1""
+fi
+
+if [ -z ""$RELEASE_TAG"" ]; then
+  RELEASE_TAG=""v$RELEASE_VERSION-$RELEASE_RC""
+fi
+
+RELEASE_STAGING_LOCATION=""https://dist.apache.org/repos/dist/dev/bahir-flink/""
+
+
+echo ""  ""
+echo ""-------------------------------------------------------------""
+echo ""------- Release preparation with the following parameters ---""
+echo ""-------------------------------------------------------------""
+echo ""Executing           ==> $GOAL""
+echo ""Git reference       ==> $GIT_REF""
+echo ""release version     ==> $RELEASE_VERSION""
+echo ""development version ==> $DEVELOPMENT_VERSION""
+echo ""rc                  ==> $RELEASE_RC""
+echo ""tag                 ==> $RELEASE_TAG""
+if [ ""$DRY_RUN"" ]; then
+   echo ""dry run ?           ==> true""
+fi
+echo ""  ""
+echo ""Deploying to :""
+echo $RELEASE_STAGING_LOCATION
+echo ""  ""
+
+function checkout_code {
+    # Checkout code
+    rm -rf target
+    mkdir target
+    cd target
+    rm -rf bahir-flink
+    git clone https://git-wip-us.apache.org/repos/asf/bahir-flink.git
+    cd bahir-flink
+    git checkout $GIT_REF
+    git_hash=`git rev-parse --short HEAD`
+    echo ""Checked out Bahir Flink git hash $git_hash""
+
+    git clean -d -f -x
+    #rm .gitignore
+    #rm -rf .git
+
+    cd ""$BASE_DIR"" #return to base dir
+}
+
+if [[ ""$RELEASE_PREPARE"" == ""true"" ]]; then
+    echo ""Preparing release $RELEASE_VERSION""
+    # Checkout code
+    checkout_code
+    cd target/bahir-flink
+
+    # Build and prepare the release
+    $MVN $PUBLISH_PROFILES release:clean release:prepare $DRY_RUN -Darguments=""-Dgpg.passphrase=\""$GPG_PASSPHRASE\"" -DskipTests"" -DreleaseVersion=""$RELEASE_VERSION"" -DdevelopmentVersion=""$DEVELOPMENT_VERSION"" -Dtag=""$RELEASE_TAG""
+
+    cd .. #exit bahir-flink
+
+    if [ -z ""$DRY_RUN"" ]; then
+        svn co $RELEASE_STAGING_LOCATION svn-bahir-flink
+        mkdir -p svn-bahir-flink/$RELEASE_VERSION-$RELEASE_RC
+        cp bahir-flink/distribution/target/*.tar.gz svn-bahir-flink/$RELEASE_VERSION-$RELEASE_RC/
+        cp bahir-flink/distribution/target/*.zip    svn-bahir-flink/$RELEASE_VERSION-$RELEASE_RC/
+
+        cd svn-bahir-flink/$RELEASE_VERSION-$RELEASE_RC/
+        for i in *.zip *.gz; do gpg --output $i.asc --detach-sig --armor $i; done
+        for i in *.zip *.gz; do openssl md5 -hex $i | sed 's/MD5(\([^)]*\))= \([0-9a-f]*\)/\2 *\1/' > $i.md5; done
+
+        cd .. #exit $RELEASE_VERSION-$RELEASE_RC/
+
+        svn add $RELEASE_VERSION-$RELEASE_RC/
+        svn ci -m""Apache Bahir Flink Extensions $RELEASE_VERSION-$RELEASE_RC""","[{'comment': 'How about change this to:\r\n""Apache Bahir extensions for Apache Flink ....""\r\n\r\nWhich will make things following the same pattern as the download page (recent suggested changes)', 'commenter': 'lresende'}, {'comment': 'That sounds better, will change this!', 'commenter': 'tzulitai'}]"
17,flink-connector-kudu/README.md,"@@ -0,0 +1,75 @@
+# Flink Kudu Connector
+This connector provides a source and sink to [Apache Kudu](http://kudu.apache.org/)™
+To use this connector, add the following dependency to your project:
+
+```
+<dependency>
+  <groupId>es.accenture</groupId>
+  <artifactId>flink-kudu-connector</artifactId>
+  <version>1.0</version>
+</dependency>
+```
+
+
+Data flows patterns:","[{'comment': 'I think there\'s a type in ""flows"": its probably called ""Data flow patterns"".', 'commenter': 'rmetzger'}]"
17,flink-connector-kudu/README.md,"@@ -0,0 +1,75 @@
+# Flink Kudu Connector
+This connector provides a source and sink to [Apache Kudu](http://kudu.apache.org/)™
+To use this connector, add the following dependency to your project:
+
+```
+<dependency>
+  <groupId>es.accenture</groupId>
+  <artifactId>flink-kudu-connector</artifactId>
+  <version>1.0</version>
+</dependency>
+```
+
+
+Data flows patterns:
+* Batch
+  * Kudu -> DataSet\<RowSerializable\> -> Kudu
+  * Kudu -> DataSet\<RowSerializable\> -> other source
+  * Other source -> DataSet\<RowSerializable\> -> other source
+* Stream
+  * Other source -> DataStream \<RowSerializable\> -> Kudu
+
+
+```java
+
+/* Batch mode - DataSet API -*/
+
+DataSet<RowSerializable> input = KuduInputBuilder.build(TABLE_SOURCE, KUDU_MASTER)
+               
+// DataSet operations --> .map(), .filter(), reduce(), etc.
+//result = input.map(...)
+
+result.output(new KuduOutputFormat(KUDU_MASTER, TABLE_SINK, columnNames, KuduOutputFormat.CREATE));
+
+KuduInputBuilder.env.execute();","[{'comment': 'Why do you recommend accessing the execution environment from the `KuduInputBuilder`? ', 'commenter': 'rmetzger'}]"
17,flink-connector-kudu/pom.xml,"@@ -0,0 +1,294 @@
+<?xml version=""1.0"" encoding=""UTF-8""?>
+<project xmlns=""http://maven.apache.org/POM/4.0.0""
+         xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
+         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
+    <modelVersion>4.0.0</modelVersion>
+    <groupId>es.accenture</groupId>","[{'comment': 'the group id has to be `org.apache.bahir`.', 'commenter': 'rmetzger'}, {'comment': ""The whole maven file doesn't seem to be integrated into Bahir's module structure.\r\n\r\nThe licenses, developers an scm sections are not needed because they should be inherited from bahir-flink's parent pom."", 'commenter': 'rmetzger'}]"
17,flink-connector-kudu/src/main/java/es/accenture/flink/Sink/KuduOutputFormat.java,"@@ -0,0 +1,186 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package es.accenture.flink.Sink;","[{'comment': 'this has to be changed to the `org.apache.bahir` package.', 'commenter': 'rmetzger'}]"
17,flink-connector-kudu/src/main/java/es/accenture/flink/Sink/KuduOutputFormat.java,"@@ -0,0 +1,186 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package es.accenture.flink.Sink;
+
+import es.accenture.flink.Utils.Exceptions.KuduClientException;
+import es.accenture.flink.Utils.Exceptions.KuduTableException;
+import es.accenture.flink.Utils.RowSerializable;
+import es.accenture.flink.Utils.Utils;
+import org.apache.flink.api.common.io.RichOutputFormat;
+import org.apache.flink.configuration.Configuration;
+import org.apache.kudu.client.*;
+import org.apache.log4j.Logger;
+
+import java.io.IOException;
+
+
+public class KuduOutputFormat extends RichOutputFormat<RowSerializable> {
+
+    private String host, tableName;
+    private Integer tableMode;
+    private String[] fieldsNames;
+    private transient Utils utils;
+
+    //Kudu variables
+    private transient KuduTable table;
+
+    //Modes
+    public static final Integer CREATE = 1;
+    public static final Integer APPEND = 2;
+    public static final Integer OVERRIDE = 3;
+
+
+    //LOG4J
+    private final static Logger logger = Logger.getLogger(KuduOutputFormat.class);
+    private static final Object lock = new Object();
+    /**
+     * Builder to use when you want to create a new table
+     *
+     * @param host        Kudu host
+     * @param tableName   Kudu table name
+     * @param fieldsNames List of column names in the table to be created
+     * @param tableMode   Way to operate with table (CREATE, APPEND, OVERRIDE)
+     */
+    public KuduOutputFormat(String host, String tableName, String[] fieldsNames, Integer tableMode) throws KuduException, KuduTableException, KuduClientException {
+        if (tableMode == null || ((!tableMode.equals(CREATE)) && (!tableMode.equals(APPEND)) && (!tableMode.equals(OVERRIDE)))) {
+            throw new IllegalArgumentException(""ERROR: Param \""tableMode\"" not valid (null or empty)"");
+
+        } else if (!(tableMode.equals(CREATE) || tableMode.equals(APPEND) || tableMode.equals(OVERRIDE))) {
+            throw new IllegalArgumentException(""ERROR: Param \""tableMode\"" not valid (must be CREATE, APPEND or OVERRIDE)"");
+
+        } else if (tableMode.equals(CREATE)) {
+            if (fieldsNames == null || fieldsNames.length == 0)
+                throw new IllegalArgumentException(""ERROR: Missing param \""fieldNames\"". Can't create a table without column names"");
+
+        } else if (host == null || host.isEmpty()) {
+            throw new IllegalArgumentException(""ERROR: Param \""host\"" not valid (null or empty)"");
+
+        } else if (tableName == null || tableName.isEmpty()) {
+            throw new IllegalArgumentException(""ERROR: Param \""tableName\"" not valid (null or empty)"");
+        }
+
+        this.host = host;
+        this.tableName = tableName;
+        this.fieldsNames = fieldsNames;
+        this.tableMode = tableMode;
+
+    }
+
+    /**
+     * Builder to be used when using an existing table
+     *
+     * @param host      Kudu host
+     * @param tableName Kudu table name to be used
+     * @param tableMode Way to operate with table (CREATE, APPEND, OVERRIDE)
+     * @throws KuduClientException In case of exception caused by Kudu Client
+     * @throws KuduTableException In case of exception caused by Kudu Tablet
+     * @throws KuduException In case of exception caused by Kudu
+     */
+    public KuduOutputFormat(String host, String tableName, Integer tableMode) throws KuduException, KuduTableException, KuduClientException {
+        if (tableMode == null || ((!tableMode.equals(CREATE)) && (!tableMode.equals(APPEND)) && (!tableMode.equals(OVERRIDE)))) {
+            throw new IllegalArgumentException(""ERROR: Param \""tableMode\"" not valid (null or empty)"");
+
+        } else if (tableMode.equals(CREATE)) {
+            throw new IllegalArgumentException(""ERROR: Param \""tableMode\"" can't be CREATE if missing \""fieldNames\"". Use other builder for this mode"");
+
+        } else if (!(tableMode.equals(APPEND) || tableMode.equals(OVERRIDE))) {
+            throw new IllegalArgumentException(""ERROR: Param \""tableMode\"" not valid (must be APPEND or OVERRIDE)"");
+
+        } else if (host == null || host.isEmpty()) {
+            throw new IllegalArgumentException(""ERROR: Param \""host\"" not valid (null or empty)"");
+
+        } else if (tableName == null || tableName.isEmpty()) {
+            throw new IllegalArgumentException(""ERROR: Param \""tableName\"" not valid (null or empty)"");
+        }
+
+        this.host = host;
+        this.tableName = tableName;
+        this.tableMode = tableMode;
+
+    }
+
+
+    @Override
+    public void configure(Configuration configuration) {
+
+    }
+
+    @Override
+    public void open(int i, int i1) throws IOException {
+
+        // Establish connection with Kudu
+        this.utils = new Utils(host);
+        if(this.utils.getClient().tableExists(tableName)){
+            logger.info(""Mode is CREATE and table already exist. Changed mode to APPEND. Warning, parallelism may be less efficient"");
+            tableMode = APPEND;
+        }
+
+        // Case APPEND (or OVERRIDE), with builder without column names, because otherwise it throws a NullPointerException
+        if(tableMode.equals(APPEND) || tableMode.equals(OVERRIDE)) {
+            this.table = utils.useTable(tableName, tableMode);
+
+            if (fieldsNames == null || fieldsNames.length == 0) {
+                fieldsNames = utils.getNamesOfColumns(table);
+            } else {
+                // When column names provided, and table exists, must check if column names match
+                utils.checkNamesOfColumns(utils.getNamesOfColumns(this.table), fieldsNames);
+            }
+
+        }
+
+    }
+
+    @Override
+    public void close() throws IOException {
+        this.utils.getClient().close();
+    }
+
+    /**
+     * It's responsible to insert a row into the indicated table by the builder (Batch)
+     *
+     * @param row   Data of a row to insert
+     * */
+    @Override
+    public void writeRecord(RowSerializable row) throws IOException {
+
+        if(tableMode.equals(CREATE)){
+            if (!utils.getClient().tableExists(tableName)) {
+                createTable(utils, tableName, fieldsNames, row);
+            }else{
+                this.table = utils.getClient().openTable(tableName);
+            }
+        }
+        if(table!=null)
+            utils.insert(table, row, fieldsNames);","[{'comment': 'This looks like inconsistent style. I would suggest to use {} around all statements.', 'commenter': 'rmetzger'}]"
17,flink-connector-kudu/src/main/java/es/accenture/flink/Sink/KuduOutputFormat.java,"@@ -0,0 +1,186 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package es.accenture.flink.Sink;
+
+import es.accenture.flink.Utils.Exceptions.KuduClientException;
+import es.accenture.flink.Utils.Exceptions.KuduTableException;
+import es.accenture.flink.Utils.RowSerializable;
+import es.accenture.flink.Utils.Utils;
+import org.apache.flink.api.common.io.RichOutputFormat;
+import org.apache.flink.configuration.Configuration;
+import org.apache.kudu.client.*;
+import org.apache.log4j.Logger;
+
+import java.io.IOException;
+
+
+public class KuduOutputFormat extends RichOutputFormat<RowSerializable> {
+
+    private String host, tableName;
+    private Integer tableMode;
+    private String[] fieldsNames;
+    private transient Utils utils;
+
+    //Kudu variables
+    private transient KuduTable table;
+
+    //Modes
+    public static final Integer CREATE = 1;
+    public static final Integer APPEND = 2;
+    public static final Integer OVERRIDE = 3;
+
+
+    //LOG4J
+    private final static Logger logger = Logger.getLogger(KuduOutputFormat.class);
+    private static final Object lock = new Object();
+    /**
+     * Builder to use when you want to create a new table
+     *
+     * @param host        Kudu host
+     * @param tableName   Kudu table name
+     * @param fieldsNames List of column names in the table to be created
+     * @param tableMode   Way to operate with table (CREATE, APPEND, OVERRIDE)
+     */
+    public KuduOutputFormat(String host, String tableName, String[] fieldsNames, Integer tableMode) throws KuduException, KuduTableException, KuduClientException {
+        if (tableMode == null || ((!tableMode.equals(CREATE)) && (!tableMode.equals(APPEND)) && (!tableMode.equals(OVERRIDE)))) {
+            throw new IllegalArgumentException(""ERROR: Param \""tableMode\"" not valid (null or empty)"");
+
+        } else if (!(tableMode.equals(CREATE) || tableMode.equals(APPEND) || tableMode.equals(OVERRIDE))) {
+            throw new IllegalArgumentException(""ERROR: Param \""tableMode\"" not valid (must be CREATE, APPEND or OVERRIDE)"");
+
+        } else if (tableMode.equals(CREATE)) {
+            if (fieldsNames == null || fieldsNames.length == 0)
+                throw new IllegalArgumentException(""ERROR: Missing param \""fieldNames\"". Can't create a table without column names"");
+
+        } else if (host == null || host.isEmpty()) {
+            throw new IllegalArgumentException(""ERROR: Param \""host\"" not valid (null or empty)"");
+
+        } else if (tableName == null || tableName.isEmpty()) {
+            throw new IllegalArgumentException(""ERROR: Param \""tableName\"" not valid (null or empty)"");
+        }
+
+        this.host = host;
+        this.tableName = tableName;
+        this.fieldsNames = fieldsNames;
+        this.tableMode = tableMode;
+
+    }
+
+    /**
+     * Builder to be used when using an existing table
+     *
+     * @param host      Kudu host
+     * @param tableName Kudu table name to be used
+     * @param tableMode Way to operate with table (CREATE, APPEND, OVERRIDE)
+     * @throws KuduClientException In case of exception caused by Kudu Client
+     * @throws KuduTableException In case of exception caused by Kudu Tablet
+     * @throws KuduException In case of exception caused by Kudu
+     */
+    public KuduOutputFormat(String host, String tableName, Integer tableMode) throws KuduException, KuduTableException, KuduClientException {
+        if (tableMode == null || ((!tableMode.equals(CREATE)) && (!tableMode.equals(APPEND)) && (!tableMode.equals(OVERRIDE)))) {
+            throw new IllegalArgumentException(""ERROR: Param \""tableMode\"" not valid (null or empty)"");
+
+        } else if (tableMode.equals(CREATE)) {
+            throw new IllegalArgumentException(""ERROR: Param \""tableMode\"" can't be CREATE if missing \""fieldNames\"". Use other builder for this mode"");
+
+        } else if (!(tableMode.equals(APPEND) || tableMode.equals(OVERRIDE))) {
+            throw new IllegalArgumentException(""ERROR: Param \""tableMode\"" not valid (must be APPEND or OVERRIDE)"");
+
+        } else if (host == null || host.isEmpty()) {
+            throw new IllegalArgumentException(""ERROR: Param \""host\"" not valid (null or empty)"");
+
+        } else if (tableName == null || tableName.isEmpty()) {
+            throw new IllegalArgumentException(""ERROR: Param \""tableName\"" not valid (null or empty)"");
+        }
+
+        this.host = host;
+        this.tableName = tableName;
+        this.tableMode = tableMode;
+
+    }
+
+
+    @Override
+    public void configure(Configuration configuration) {
+
+    }
+
+    @Override
+    public void open(int i, int i1) throws IOException {
+
+        // Establish connection with Kudu
+        this.utils = new Utils(host);
+        if(this.utils.getClient().tableExists(tableName)){
+            logger.info(""Mode is CREATE and table already exist. Changed mode to APPEND. Warning, parallelism may be less efficient"");
+            tableMode = APPEND;
+        }
+
+        // Case APPEND (or OVERRIDE), with builder without column names, because otherwise it throws a NullPointerException
+        if(tableMode.equals(APPEND) || tableMode.equals(OVERRIDE)) {
+            this.table = utils.useTable(tableName, tableMode);
+
+            if (fieldsNames == null || fieldsNames.length == 0) {
+                fieldsNames = utils.getNamesOfColumns(table);
+            } else {
+                // When column names provided, and table exists, must check if column names match
+                utils.checkNamesOfColumns(utils.getNamesOfColumns(this.table), fieldsNames);
+            }
+
+        }
+
+    }
+
+    @Override
+    public void close() throws IOException {
+        this.utils.getClient().close();
+    }
+
+    /**
+     * It's responsible to insert a row into the indicated table by the builder (Batch)
+     *
+     * @param row   Data of a row to insert
+     * */
+    @Override
+    public void writeRecord(RowSerializable row) throws IOException {
+
+        if(tableMode.equals(CREATE)){
+            if (!utils.getClient().tableExists(tableName)) {
+                createTable(utils, tableName, fieldsNames, row);
+            }else{
+                this.table = utils.getClient().openTable(tableName);
+            }
+        }
+        if(table!=null)
+            utils.insert(table, row, fieldsNames);
+        //logger.info(""Inserted the Row: | "" + utils.printRow(row) + ""at the table \"""" + this.tableName + ""\"""");","[{'comment': 'Why is this commented out?', 'commenter': 'rmetzger'}]"
17,flink-connector-kudu/src/main/java/es/accenture/flink/Sink/KuduOutputFormat.java,"@@ -0,0 +1,186 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package es.accenture.flink.Sink;
+
+import es.accenture.flink.Utils.Exceptions.KuduClientException;
+import es.accenture.flink.Utils.Exceptions.KuduTableException;
+import es.accenture.flink.Utils.RowSerializable;
+import es.accenture.flink.Utils.Utils;
+import org.apache.flink.api.common.io.RichOutputFormat;
+import org.apache.flink.configuration.Configuration;
+import org.apache.kudu.client.*;
+import org.apache.log4j.Logger;
+
+import java.io.IOException;
+
+
+public class KuduOutputFormat extends RichOutputFormat<RowSerializable> {
+
+    private String host, tableName;
+    private Integer tableMode;
+    private String[] fieldsNames;
+    private transient Utils utils;
+
+    //Kudu variables
+    private transient KuduTable table;
+
+    //Modes
+    public static final Integer CREATE = 1;
+    public static final Integer APPEND = 2;
+    public static final Integer OVERRIDE = 3;
+
+
+    //LOG4J
+    private final static Logger logger = Logger.getLogger(KuduOutputFormat.class);
+    private static final Object lock = new Object();
+    /**
+     * Builder to use when you want to create a new table
+     *
+     * @param host        Kudu host
+     * @param tableName   Kudu table name
+     * @param fieldsNames List of column names in the table to be created
+     * @param tableMode   Way to operate with table (CREATE, APPEND, OVERRIDE)
+     */
+    public KuduOutputFormat(String host, String tableName, String[] fieldsNames, Integer tableMode) throws KuduException, KuduTableException, KuduClientException {
+        if (tableMode == null || ((!tableMode.equals(CREATE)) && (!tableMode.equals(APPEND)) && (!tableMode.equals(OVERRIDE)))) {
+            throw new IllegalArgumentException(""ERROR: Param \""tableMode\"" not valid (null or empty)"");
+
+        } else if (!(tableMode.equals(CREATE) || tableMode.equals(APPEND) || tableMode.equals(OVERRIDE))) {
+            throw new IllegalArgumentException(""ERROR: Param \""tableMode\"" not valid (must be CREATE, APPEND or OVERRIDE)"");
+
+        } else if (tableMode.equals(CREATE)) {
+            if (fieldsNames == null || fieldsNames.length == 0)
+                throw new IllegalArgumentException(""ERROR: Missing param \""fieldNames\"". Can't create a table without column names"");
+
+        } else if (host == null || host.isEmpty()) {
+            throw new IllegalArgumentException(""ERROR: Param \""host\"" not valid (null or empty)"");
+
+        } else if (tableName == null || tableName.isEmpty()) {
+            throw new IllegalArgumentException(""ERROR: Param \""tableName\"" not valid (null or empty)"");
+        }
+
+        this.host = host;
+        this.tableName = tableName;
+        this.fieldsNames = fieldsNames;
+        this.tableMode = tableMode;
+
+    }
+
+    /**
+     * Builder to be used when using an existing table
+     *
+     * @param host      Kudu host
+     * @param tableName Kudu table name to be used
+     * @param tableMode Way to operate with table (CREATE, APPEND, OVERRIDE)
+     * @throws KuduClientException In case of exception caused by Kudu Client
+     * @throws KuduTableException In case of exception caused by Kudu Tablet
+     * @throws KuduException In case of exception caused by Kudu
+     */
+    public KuduOutputFormat(String host, String tableName, Integer tableMode) throws KuduException, KuduTableException, KuduClientException {
+        if (tableMode == null || ((!tableMode.equals(CREATE)) && (!tableMode.equals(APPEND)) && (!tableMode.equals(OVERRIDE)))) {
+            throw new IllegalArgumentException(""ERROR: Param \""tableMode\"" not valid (null or empty)"");
+
+        } else if (tableMode.equals(CREATE)) {
+            throw new IllegalArgumentException(""ERROR: Param \""tableMode\"" can't be CREATE if missing \""fieldNames\"". Use other builder for this mode"");
+
+        } else if (!(tableMode.equals(APPEND) || tableMode.equals(OVERRIDE))) {
+            throw new IllegalArgumentException(""ERROR: Param \""tableMode\"" not valid (must be APPEND or OVERRIDE)"");
+
+        } else if (host == null || host.isEmpty()) {
+            throw new IllegalArgumentException(""ERROR: Param \""host\"" not valid (null or empty)"");
+
+        } else if (tableName == null || tableName.isEmpty()) {
+            throw new IllegalArgumentException(""ERROR: Param \""tableName\"" not valid (null or empty)"");
+        }
+
+        this.host = host;
+        this.tableName = tableName;
+        this.tableMode = tableMode;
+
+    }
+
+
+    @Override
+    public void configure(Configuration configuration) {
+
+    }
+
+    @Override
+    public void open(int i, int i1) throws IOException {
+
+        // Establish connection with Kudu
+        this.utils = new Utils(host);
+        if(this.utils.getClient().tableExists(tableName)){
+            logger.info(""Mode is CREATE and table already exist. Changed mode to APPEND. Warning, parallelism may be less efficient"");
+            tableMode = APPEND;
+        }
+
+        // Case APPEND (or OVERRIDE), with builder without column names, because otherwise it throws a NullPointerException
+        if(tableMode.equals(APPEND) || tableMode.equals(OVERRIDE)) {
+            this.table = utils.useTable(tableName, tableMode);
+
+            if (fieldsNames == null || fieldsNames.length == 0) {
+                fieldsNames = utils.getNamesOfColumns(table);
+            } else {
+                // When column names provided, and table exists, must check if column names match
+                utils.checkNamesOfColumns(utils.getNamesOfColumns(this.table), fieldsNames);
+            }
+
+        }
+
+    }
+
+    @Override
+    public void close() throws IOException {
+        this.utils.getClient().close();
+    }
+
+    /**
+     * It's responsible to insert a row into the indicated table by the builder (Batch)
+     *
+     * @param row   Data of a row to insert
+     * */
+    @Override
+    public void writeRecord(RowSerializable row) throws IOException {
+
+        if(tableMode.equals(CREATE)){
+            if (!utils.getClient().tableExists(tableName)) {","[{'comment': ""How expensive is this `tableExists()` call? I assume it'll be executed on each record to write."", 'commenter': 'rmetzger'}]"
17,flink-connector-kudu/src/main/java/es/accenture/flink/Sink/KuduSink.java,"@@ -0,0 +1,105 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package es.accenture.flink.Sink;
+
+import es.accenture.flink.Utils.Exceptions.KuduClientException;
+import es.accenture.flink.Utils.RowSerializable;
+import es.accenture.flink.Utils.Utils;
+import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;
+import org.apache.kudu.client.KuduTable;
+import org.apache.log4j.Logger;
+
+import java.io.IOException;
+
+public class KuduSink extends RichSinkFunction<RowSerializable>{
+
+    private String host, tableName;
+    private String [] fieldsNames;
+    private transient Utils utils;
+
+    //Kudu variables
+    private transient KuduTable table;
+
+    // LOG4J
+
+    private final static Logger logger = Logger.getLogger(KuduSink.class);
+
+    /**
+     * Builder to use when you want to create a new table
+     *
+     * @param host          Kudu host
+     * @param tableName     Kudu table name
+     * @param fieldsNames   List of column names in the table to be created
+     * @throws KuduClientException In case of exception caused by Kudu Client
+     */
+    public KuduSink (String host, String tableName, String [] fieldsNames) throws KuduClientException {
+","[{'comment': 'Empty line', 'commenter': 'rmetzger'}]"
17,flink-connector-kudu/src/main/java/es/accenture/flink/Sink/KuduSink.java,"@@ -0,0 +1,105 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package es.accenture.flink.Sink;
+
+import es.accenture.flink.Utils.Exceptions.KuduClientException;
+import es.accenture.flink.Utils.RowSerializable;
+import es.accenture.flink.Utils.Utils;
+import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;
+import org.apache.kudu.client.KuduTable;
+import org.apache.log4j.Logger;
+
+import java.io.IOException;
+
+public class KuduSink extends RichSinkFunction<RowSerializable>{
+
+    private String host, tableName;
+    private String [] fieldsNames;
+    private transient Utils utils;
+
+    //Kudu variables
+    private transient KuduTable table;
+
+    // LOG4J
+
+    private final static Logger logger = Logger.getLogger(KuduSink.class);
+
+    /**
+     * Builder to use when you want to create a new table
+     *
+     * @param host          Kudu host
+     * @param tableName     Kudu table name
+     * @param fieldsNames   List of column names in the table to be created
+     * @throws KuduClientException In case of exception caused by Kudu Client
+     */
+    public KuduSink (String host, String tableName, String [] fieldsNames) throws KuduClientException {
+
+        if (host == null || host.isEmpty()) {
+            throw new IllegalArgumentException(""ERROR: Param \""host\"" not valid (null or empty)"");
+
+        } else if (tableName == null || tableName.isEmpty()) {
+            throw new IllegalArgumentException(""ERROR: Param \""tableName\"" not valid (null or empty)"");
+
+        }
+        this.host = host;
+        this.tableName = tableName;
+        this.fieldsNames = fieldsNames;
+    }
+
+    /**
+     * Builder to be used when using an existing table
+     *
+     * @param host          Kudu host
+     * @param tableName     Kudu table name
+     * @throws KuduClientException In case of exception caused by Kudu Client
+     */
+    public KuduSink (String host, String tableName) throws KuduClientException {
+
+        if (host == null || host.isEmpty()) {
+            throw new IllegalArgumentException(""ERROR: Param \""host\"" not valid (null or empty)"");
+
+        } else if (tableName == null || tableName.isEmpty()) {
+            throw new IllegalArgumentException(""ERROR: Param \""tableName\"" not valid (null or empty)"");
+        }
+
+        this.host = host;
+        this.tableName = tableName;
+    }
+
+    /**
+     * It's responsible to insert a row into the indicated table by the builder (Streaming)
+     *
+     * @param row   Data of a row to insert
+     */
+    @Override
+    public void invoke(RowSerializable row) throws IOException {
+
+        // Establish connection with Kudu
+        if (this.utils == null)
+            this.utils = new Utils(host);
+
+        if (this.table == null)
+            this.table = this.utils.useTable(tableName, fieldsNames, row);
+
+
+        // Make the insert into the table
+        utils.insert(table, row, fieldsNames);
+
+        logger.info(""Inserted the Row: | "" + utils.printRow(row) + ""at the table \"""" + this.tableName + ""\"""");","[{'comment': 'We suggest to use parameterized messages in log4j for performance reasons: https://logging.apache.org/log4j/2.0/manual/messages.html\r\nHere, you\'ll concat 5 strings, even if logging is disabled.\r\n\r\nAlso, there\'s a space missing in + ""at the table"".', 'commenter': 'rmetzger'}]"
17,flink-connector-kudu/src/main/java/es/accenture/flink/Sources/KuduInputBuilder.java,"@@ -0,0 +1,39 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package es.accenture.flink.Sources;
+
+import es.accenture.flink.Utils.RowSerializable;
+import org.apache.flink.api.common.typeinfo.TypeInformation;
+import org.apache.flink.api.java.DataSet;
+import org.apache.flink.api.java.ExecutionEnvironment;
+
+public class KuduInputBuilder {
+
+
+    public static ExecutionEnvironment env = null;","[{'comment': 'What is this static variable used for?', 'commenter': 'rmetzger'}]"
17,flink-connector-kudu/src/main/java/es/accenture/flink/Sources/KuduInputFormat.java,"@@ -0,0 +1,340 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package es.accenture.flink.Sources;
+
+import es.accenture.flink.Utils.RowSerializable;
+import org.apache.flink.api.common.io.InputFormat;
+import org.apache.flink.api.common.io.LocatableInputSplitAssigner;
+import org.apache.flink.api.common.io.statistics.BaseStatistics;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.core.io.InputSplitAssigner;
+import org.apache.kudu.client.*;
+import org.apache.log4j.Logger;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+/**
+ * {@link InputFormat} subclass that wraps the access for KuduTables.
+ */
+public class KuduInputFormat implements InputFormat<RowSerializable, KuduInputSplit> {
+
+    private String KUDU_MASTER;
+    private String TABLE_NAME;
+
+    private transient KuduTable table = null;
+    private transient KuduScanner scanner = null;
+    private transient KuduClient client = null;
+
+    private transient RowResultIterator results = null;
+    private List<RowSerializable> rows = null;
+    private List<KuduScanToken> tokens = null;
+    private boolean endReached = false;
+    private int scannedRows = 0;
+
+    private static final Logger LOG = Logger.getLogger(KuduInputFormat.class);
+
+    private List<String> projectColumns;
+
+    /**
+     * Constructor of class KuduInputFormat
+     * @param tableName Name of the Kudu table in which we are going to read
+     * @param IP Kudu-master server's IP direction
+     */
+    public KuduInputFormat(String tableName, String IP){
+        LOG.info(""1. CONSTRUCTOR"");","[{'comment': 'Looks like a debugging message.', 'commenter': 'rmetzger'}]"
17,flink-connector-kudu/src/main/java/es/accenture/flink/Sources/KuduInputFormat.java,"@@ -0,0 +1,340 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package es.accenture.flink.Sources;
+
+import es.accenture.flink.Utils.RowSerializable;
+import org.apache.flink.api.common.io.InputFormat;
+import org.apache.flink.api.common.io.LocatableInputSplitAssigner;
+import org.apache.flink.api.common.io.statistics.BaseStatistics;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.core.io.InputSplitAssigner;
+import org.apache.kudu.client.*;
+import org.apache.log4j.Logger;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+/**
+ * {@link InputFormat} subclass that wraps the access for KuduTables.
+ */
+public class KuduInputFormat implements InputFormat<RowSerializable, KuduInputSplit> {
+
+    private String KUDU_MASTER;
+    private String TABLE_NAME;
+
+    private transient KuduTable table = null;
+    private transient KuduScanner scanner = null;
+    private transient KuduClient client = null;
+
+    private transient RowResultIterator results = null;
+    private List<RowSerializable> rows = null;
+    private List<KuduScanToken> tokens = null;
+    private boolean endReached = false;
+    private int scannedRows = 0;
+
+    private static final Logger LOG = Logger.getLogger(KuduInputFormat.class);
+
+    private List<String> projectColumns;
+
+    /**
+     * Constructor of class KuduInputFormat
+     * @param tableName Name of the Kudu table in which we are going to read
+     * @param IP Kudu-master server's IP direction
+     */
+    public KuduInputFormat(String tableName, String IP){
+        LOG.info(""1. CONSTRUCTOR"");
+        KUDU_MASTER = IP;
+        TABLE_NAME = tableName;","[{'comment': 'Why are these fields all-uppercase?', 'commenter': 'rmetzger'}]"
17,flink-connector-kudu/src/main/java/es/accenture/flink/Sources/KuduInputFormat.java,"@@ -0,0 +1,340 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package es.accenture.flink.Sources;
+
+import es.accenture.flink.Utils.RowSerializable;
+import org.apache.flink.api.common.io.InputFormat;
+import org.apache.flink.api.common.io.LocatableInputSplitAssigner;
+import org.apache.flink.api.common.io.statistics.BaseStatistics;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.core.io.InputSplitAssigner;
+import org.apache.kudu.client.*;
+import org.apache.log4j.Logger;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+/**
+ * {@link InputFormat} subclass that wraps the access for KuduTables.
+ */
+public class KuduInputFormat implements InputFormat<RowSerializable, KuduInputSplit> {
+
+    private String KUDU_MASTER;
+    private String TABLE_NAME;
+
+    private transient KuduTable table = null;
+    private transient KuduScanner scanner = null;
+    private transient KuduClient client = null;
+
+    private transient RowResultIterator results = null;
+    private List<RowSerializable> rows = null;
+    private List<KuduScanToken> tokens = null;
+    private boolean endReached = false;
+    private int scannedRows = 0;
+
+    private static final Logger LOG = Logger.getLogger(KuduInputFormat.class);
+
+    private List<String> projectColumns;
+
+    /**
+     * Constructor of class KuduInputFormat
+     * @param tableName Name of the Kudu table in which we are going to read
+     * @param IP Kudu-master server's IP direction
+     */
+    public KuduInputFormat(String tableName, String IP){
+        LOG.info(""1. CONSTRUCTOR"");
+        KUDU_MASTER = IP;
+        TABLE_NAME = tableName;
+
+    }
+
+    /**
+     * Returns an instance of Scan that retrieves the required subset of records from the Kudu table.
+     * @return The appropriate instance of Scan for this usecase.
+     */
+    private KuduScanner getScanner(){
+        return this.scanner;
+    }
+
+    /**
+     * What table is to be read.
+     * Per instance of a TableInputFormat derivative only a single tablename is possible.
+     * @return The name of the table
+     */
+    public String getTableName(){
+        return TABLE_NAME;
+    }
+
+    /**
+     * @return A list of rows ({@link RowSerializable}) from the Kudu table
+     */
+    public List<RowSerializable> getRows(){
+        return this.rows;
+    }
+
+    /**
+     * The output from Kudu is always an instance of {@link RowResult}.
+     * This method is to copy the data in the RowResult instance into the required {@link RowSerializable}
+     * @param rowResult The Result instance from Kudu that needs to be converted
+     * @return The appropriate instance of {@link RowSerializable} that contains the needed information.
+     */
+    private RowSerializable RowResultToRowSerializable(RowResult rowResult) throws IllegalAccessException {
+        RowSerializable row = new RowSerializable(rowResult.getColumnProjection().getColumnCount());
+        for (int i=0; i<rowResult.getColumnProjection().getColumnCount(); i++){
+            switch(rowResult.getColumnType(i).getDataType()){
+                case INT8:
+                    row.setField(i, rowResult.getByte(i));
+                    break;
+                case INT16:
+                    row.setField(i, rowResult.getShort(i));
+                    break;
+                case INT32:
+                    row.setField(i, rowResult.getInt(i));
+                    break;
+                case INT64:
+                    row.setField(i, rowResult.getLong(i));
+                    break;
+                case FLOAT:
+                    row.setField(i, rowResult.getFloat(i));
+                    break;
+                case DOUBLE:
+                    row.setField(i, rowResult.getDouble(i));
+                    break;
+                case STRING:
+                    row.setField(i, rowResult.getString(i));
+                    break;
+                case BOOL:
+                    row.setField(i, rowResult.getBoolean(i));
+                    break;
+                case BINARY:
+                    row.setField(i, rowResult.getBinary(i));
+                    break;
+            }
+        }
+        return row;
+    }
+
+    /**
+     * Creates a object and opens the {@link KuduTable} connection.
+     * These are opened here because they are needed in the createInputSplits
+     * which is called before the openInputFormat method.
+     *
+     * @param parameters The configuration that is to be used
+     * @see Configuration
+     */
+","[{'comment': 'empty line', 'commenter': 'rmetzger'}]"
17,flink-connector-kudu/src/main/java/es/accenture/flink/Sources/KuduInputFormat.java,"@@ -0,0 +1,340 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package es.accenture.flink.Sources;
+
+import es.accenture.flink.Utils.RowSerializable;
+import org.apache.flink.api.common.io.InputFormat;
+import org.apache.flink.api.common.io.LocatableInputSplitAssigner;
+import org.apache.flink.api.common.io.statistics.BaseStatistics;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.core.io.InputSplitAssigner;
+import org.apache.kudu.client.*;
+import org.apache.log4j.Logger;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+/**
+ * {@link InputFormat} subclass that wraps the access for KuduTables.
+ */
+public class KuduInputFormat implements InputFormat<RowSerializable, KuduInputSplit> {
+
+    private String KUDU_MASTER;
+    private String TABLE_NAME;
+
+    private transient KuduTable table = null;
+    private transient KuduScanner scanner = null;
+    private transient KuduClient client = null;
+
+    private transient RowResultIterator results = null;
+    private List<RowSerializable> rows = null;
+    private List<KuduScanToken> tokens = null;
+    private boolean endReached = false;
+    private int scannedRows = 0;
+
+    private static final Logger LOG = Logger.getLogger(KuduInputFormat.class);
+
+    private List<String> projectColumns;
+
+    /**
+     * Constructor of class KuduInputFormat
+     * @param tableName Name of the Kudu table in which we are going to read
+     * @param IP Kudu-master server's IP direction
+     */
+    public KuduInputFormat(String tableName, String IP){
+        LOG.info(""1. CONSTRUCTOR"");
+        KUDU_MASTER = IP;
+        TABLE_NAME = tableName;
+
+    }
+
+    /**
+     * Returns an instance of Scan that retrieves the required subset of records from the Kudu table.
+     * @return The appropriate instance of Scan for this usecase.
+     */
+    private KuduScanner getScanner(){
+        return this.scanner;
+    }
+
+    /**
+     * What table is to be read.
+     * Per instance of a TableInputFormat derivative only a single tablename is possible.
+     * @return The name of the table
+     */
+    public String getTableName(){
+        return TABLE_NAME;
+    }
+
+    /**
+     * @return A list of rows ({@link RowSerializable}) from the Kudu table
+     */
+    public List<RowSerializable> getRows(){
+        return this.rows;
+    }
+
+    /**
+     * The output from Kudu is always an instance of {@link RowResult}.
+     * This method is to copy the data in the RowResult instance into the required {@link RowSerializable}
+     * @param rowResult The Result instance from Kudu that needs to be converted
+     * @return The appropriate instance of {@link RowSerializable} that contains the needed information.
+     */
+    private RowSerializable RowResultToRowSerializable(RowResult rowResult) throws IllegalAccessException {
+        RowSerializable row = new RowSerializable(rowResult.getColumnProjection().getColumnCount());
+        for (int i=0; i<rowResult.getColumnProjection().getColumnCount(); i++){
+            switch(rowResult.getColumnType(i).getDataType()){
+                case INT8:
+                    row.setField(i, rowResult.getByte(i));
+                    break;
+                case INT16:
+                    row.setField(i, rowResult.getShort(i));
+                    break;
+                case INT32:
+                    row.setField(i, rowResult.getInt(i));
+                    break;
+                case INT64:
+                    row.setField(i, rowResult.getLong(i));
+                    break;
+                case FLOAT:
+                    row.setField(i, rowResult.getFloat(i));
+                    break;
+                case DOUBLE:
+                    row.setField(i, rowResult.getDouble(i));
+                    break;
+                case STRING:
+                    row.setField(i, rowResult.getString(i));
+                    break;
+                case BOOL:
+                    row.setField(i, rowResult.getBoolean(i));
+                    break;
+                case BINARY:
+                    row.setField(i, rowResult.getBinary(i));
+                    break;
+            }
+        }
+        return row;
+    }
+
+    /**
+     * Creates a object and opens the {@link KuduTable} connection.
+     * These are opened here because they are needed in the createInputSplits
+     * which is called before the openInputFormat method.
+     *
+     * @param parameters The configuration that is to be used
+     * @see Configuration
+     */
+
+    @Override
+    public void configure(Configuration parameters) {
+        LOG.info(""2. CONFIGURE"");","[{'comment': 'This log statement is also probably a wip-leftover.', 'commenter': 'rmetzger'}]"
17,flink-connector-kudu/src/main/java/es/accenture/flink/Sources/KuduInputFormat.java,"@@ -0,0 +1,340 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package es.accenture.flink.Sources;
+
+import es.accenture.flink.Utils.RowSerializable;
+import org.apache.flink.api.common.io.InputFormat;
+import org.apache.flink.api.common.io.LocatableInputSplitAssigner;
+import org.apache.flink.api.common.io.statistics.BaseStatistics;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.core.io.InputSplitAssigner;
+import org.apache.kudu.client.*;
+import org.apache.log4j.Logger;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+/**
+ * {@link InputFormat} subclass that wraps the access for KuduTables.
+ */
+public class KuduInputFormat implements InputFormat<RowSerializable, KuduInputSplit> {
+
+    private String KUDU_MASTER;
+    private String TABLE_NAME;
+
+    private transient KuduTable table = null;
+    private transient KuduScanner scanner = null;
+    private transient KuduClient client = null;
+
+    private transient RowResultIterator results = null;
+    private List<RowSerializable> rows = null;
+    private List<KuduScanToken> tokens = null;
+    private boolean endReached = false;
+    private int scannedRows = 0;
+
+    private static final Logger LOG = Logger.getLogger(KuduInputFormat.class);
+
+    private List<String> projectColumns;
+
+    /**
+     * Constructor of class KuduInputFormat
+     * @param tableName Name of the Kudu table in which we are going to read
+     * @param IP Kudu-master server's IP direction
+     */
+    public KuduInputFormat(String tableName, String IP){
+        LOG.info(""1. CONSTRUCTOR"");
+        KUDU_MASTER = IP;
+        TABLE_NAME = tableName;
+
+    }
+
+    /**
+     * Returns an instance of Scan that retrieves the required subset of records from the Kudu table.
+     * @return The appropriate instance of Scan for this usecase.
+     */
+    private KuduScanner getScanner(){
+        return this.scanner;
+    }
+
+    /**
+     * What table is to be read.
+     * Per instance of a TableInputFormat derivative only a single tablename is possible.
+     * @return The name of the table
+     */
+    public String getTableName(){
+        return TABLE_NAME;
+    }
+
+    /**
+     * @return A list of rows ({@link RowSerializable}) from the Kudu table
+     */
+    public List<RowSerializable> getRows(){
+        return this.rows;
+    }
+
+    /**
+     * The output from Kudu is always an instance of {@link RowResult}.
+     * This method is to copy the data in the RowResult instance into the required {@link RowSerializable}
+     * @param rowResult The Result instance from Kudu that needs to be converted
+     * @return The appropriate instance of {@link RowSerializable} that contains the needed information.
+     */
+    private RowSerializable RowResultToRowSerializable(RowResult rowResult) throws IllegalAccessException {
+        RowSerializable row = new RowSerializable(rowResult.getColumnProjection().getColumnCount());
+        for (int i=0; i<rowResult.getColumnProjection().getColumnCount(); i++){
+            switch(rowResult.getColumnType(i).getDataType()){
+                case INT8:
+                    row.setField(i, rowResult.getByte(i));
+                    break;
+                case INT16:
+                    row.setField(i, rowResult.getShort(i));
+                    break;
+                case INT32:
+                    row.setField(i, rowResult.getInt(i));
+                    break;
+                case INT64:
+                    row.setField(i, rowResult.getLong(i));
+                    break;
+                case FLOAT:
+                    row.setField(i, rowResult.getFloat(i));
+                    break;
+                case DOUBLE:
+                    row.setField(i, rowResult.getDouble(i));
+                    break;
+                case STRING:
+                    row.setField(i, rowResult.getString(i));
+                    break;
+                case BOOL:
+                    row.setField(i, rowResult.getBoolean(i));
+                    break;
+                case BINARY:
+                    row.setField(i, rowResult.getBinary(i));
+                    break;
+            }
+        }
+        return row;
+    }
+
+    /**
+     * Creates a object and opens the {@link KuduTable} connection.
+     * These are opened here because they are needed in the createInputSplits
+     * which is called before the openInputFormat method.
+     *
+     * @param parameters The configuration that is to be used
+     * @see Configuration
+     */
+
+    @Override
+    public void configure(Configuration parameters) {
+        LOG.info(""2. CONFIGURE"");
+        LOG.info(""Initializing KUDU Configuration..."");
+
+        String kuduMaster = System.getProperty(
+                ""kuduMaster"", KUDU_MASTER);
+
+        this.client  = new KuduClient.KuduClientBuilder(kuduMaster).build();
+
+        String tablename = System.getProperty(
+                ""tableName"", TABLE_NAME);
+
+        table = createTable(tablename);
+        if (table != null) {
+            scanner = client.newScannerBuilder(table)
+                    .setProjectedColumnNames(projectColumns)
+                    .build();
+        }
+
+    }
+
+    /**
+     * Create an {@link KuduTable} instance and set it into this format
+     */
+
+    private KuduTable createTable(String TABLE_NAME) {
+
+        LOG.info(""OPENTABLE"");
+
+        try {
+            table = client.openTable(TABLE_NAME);
+        } catch (Exception e) {
+            throw new RuntimeException(""Could not obtain the table "" + TABLE_NAME + "" from master"", e);
+        }
+        projectColumns = new ArrayList<>();
+        for (int i = 0; i < table.getSchema().getColumnCount(); i++) {
+            projectColumns.add(this.table.getSchema().getColumnByIndex(i).getName());
+        }
+        return table;
+    }
+
+    /**
+     * Create an {@link KuduTable} instance and set it into this format
+     */
+
+    @Override
+    public void open(KuduInputSplit split) throws IOException {
+
+
+        LOG.info(""SPLIT ""+split.getSplitNumber()+"" PASANDO POR 5. OPEN"");","[{'comment': '2 empty lines', 'commenter': 'rmetzger'}]"
17,flink-connector-kudu/src/main/java/es/accenture/flink/Sources/KuduInputFormat.java,"@@ -0,0 +1,340 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package es.accenture.flink.Sources;
+
+import es.accenture.flink.Utils.RowSerializable;
+import org.apache.flink.api.common.io.InputFormat;
+import org.apache.flink.api.common.io.LocatableInputSplitAssigner;
+import org.apache.flink.api.common.io.statistics.BaseStatistics;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.core.io.InputSplitAssigner;
+import org.apache.kudu.client.*;
+import org.apache.log4j.Logger;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+/**
+ * {@link InputFormat} subclass that wraps the access for KuduTables.
+ */
+public class KuduInputFormat implements InputFormat<RowSerializable, KuduInputSplit> {
+
+    private String KUDU_MASTER;
+    private String TABLE_NAME;
+
+    private transient KuduTable table = null;
+    private transient KuduScanner scanner = null;
+    private transient KuduClient client = null;
+
+    private transient RowResultIterator results = null;
+    private List<RowSerializable> rows = null;
+    private List<KuduScanToken> tokens = null;
+    private boolean endReached = false;
+    private int scannedRows = 0;
+
+    private static final Logger LOG = Logger.getLogger(KuduInputFormat.class);
+
+    private List<String> projectColumns;
+
+    /**
+     * Constructor of class KuduInputFormat
+     * @param tableName Name of the Kudu table in which we are going to read
+     * @param IP Kudu-master server's IP direction
+     */
+    public KuduInputFormat(String tableName, String IP){
+        LOG.info(""1. CONSTRUCTOR"");
+        KUDU_MASTER = IP;
+        TABLE_NAME = tableName;
+
+    }
+
+    /**
+     * Returns an instance of Scan that retrieves the required subset of records from the Kudu table.
+     * @return The appropriate instance of Scan for this usecase.
+     */
+    private KuduScanner getScanner(){
+        return this.scanner;
+    }
+
+    /**
+     * What table is to be read.
+     * Per instance of a TableInputFormat derivative only a single tablename is possible.
+     * @return The name of the table
+     */
+    public String getTableName(){
+        return TABLE_NAME;
+    }
+
+    /**
+     * @return A list of rows ({@link RowSerializable}) from the Kudu table
+     */
+    public List<RowSerializable> getRows(){
+        return this.rows;
+    }
+
+    /**
+     * The output from Kudu is always an instance of {@link RowResult}.
+     * This method is to copy the data in the RowResult instance into the required {@link RowSerializable}
+     * @param rowResult The Result instance from Kudu that needs to be converted
+     * @return The appropriate instance of {@link RowSerializable} that contains the needed information.
+     */
+    private RowSerializable RowResultToRowSerializable(RowResult rowResult) throws IllegalAccessException {
+        RowSerializable row = new RowSerializable(rowResult.getColumnProjection().getColumnCount());
+        for (int i=0; i<rowResult.getColumnProjection().getColumnCount(); i++){
+            switch(rowResult.getColumnType(i).getDataType()){
+                case INT8:
+                    row.setField(i, rowResult.getByte(i));
+                    break;
+                case INT16:
+                    row.setField(i, rowResult.getShort(i));
+                    break;
+                case INT32:
+                    row.setField(i, rowResult.getInt(i));
+                    break;
+                case INT64:
+                    row.setField(i, rowResult.getLong(i));
+                    break;
+                case FLOAT:
+                    row.setField(i, rowResult.getFloat(i));
+                    break;
+                case DOUBLE:
+                    row.setField(i, rowResult.getDouble(i));
+                    break;
+                case STRING:
+                    row.setField(i, rowResult.getString(i));
+                    break;
+                case BOOL:
+                    row.setField(i, rowResult.getBoolean(i));
+                    break;
+                case BINARY:
+                    row.setField(i, rowResult.getBinary(i));
+                    break;
+            }
+        }
+        return row;
+    }
+
+    /**
+     * Creates a object and opens the {@link KuduTable} connection.
+     * These are opened here because they are needed in the createInputSplits
+     * which is called before the openInputFormat method.
+     *
+     * @param parameters The configuration that is to be used
+     * @see Configuration
+     */
+
+    @Override
+    public void configure(Configuration parameters) {
+        LOG.info(""2. CONFIGURE"");
+        LOG.info(""Initializing KUDU Configuration..."");
+
+        String kuduMaster = System.getProperty(
+                ""kuduMaster"", KUDU_MASTER);
+
+        this.client  = new KuduClient.KuduClientBuilder(kuduMaster).build();
+
+        String tablename = System.getProperty(
+                ""tableName"", TABLE_NAME);
+
+        table = createTable(tablename);
+        if (table != null) {
+            scanner = client.newScannerBuilder(table)
+                    .setProjectedColumnNames(projectColumns)
+                    .build();
+        }
+
+    }
+
+    /**
+     * Create an {@link KuduTable} instance and set it into this format
+     */
+
+    private KuduTable createTable(String TABLE_NAME) {
+
+        LOG.info(""OPENTABLE"");
+
+        try {
+            table = client.openTable(TABLE_NAME);
+        } catch (Exception e) {
+            throw new RuntimeException(""Could not obtain the table "" + TABLE_NAME + "" from master"", e);
+        }
+        projectColumns = new ArrayList<>();
+        for (int i = 0; i < table.getSchema().getColumnCount(); i++) {
+            projectColumns.add(this.table.getSchema().getColumnByIndex(i).getName());
+        }
+        return table;
+    }
+
+    /**
+     * Create an {@link KuduTable} instance and set it into this format
+     */
+
+    @Override
+    public void open(KuduInputSplit split) throws IOException {
+
+
+        LOG.info(""SPLIT ""+split.getSplitNumber()+"" PASANDO POR 5. OPEN"");
+        if (table == null) {
+            throw new IOException(""The Kudu table has not been opened!"");
+        }
+
+        LOG.info(""Opening split..."");
+
+        KuduScanToken.KuduScanTokenBuilder builder = client.newScanTokenBuilder(this.table)
+                .setProjectedColumnNames(this.projectColumns);
+
+        this.tokens = builder.build();
+
+        endReached = false;
+        scannedRows = 0;
+
+        try {
+            LOG.info(""SPLIT NUMBER ""+split.getSplitNumber());
+            scanner = tokens.get(split.getSplitNumber()).intoScanner(client);
+        } catch (Exception e) {
+            e.printStackTrace();","[{'comment': 'This will be logged to standard out', 'commenter': 'rmetzger'}]"
17,flink-connector-kudu/src/main/java/es/accenture/flink/Utils/CreateKuduTable.java,"@@ -0,0 +1,54 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package es.accenture.flink.Utils;
+
+import org.apache.kudu.ColumnSchema;
+import org.apache.kudu.Schema;
+import org.apache.kudu.Type;
+import org.apache.kudu.client.CreateTableOptions;
+import org.apache.kudu.client.KuduClient;
+
+import java.util.ArrayList;
+import java.util.List;
+
+
+public class CreateKuduTable {
+    public static void main(String[] args) {
+
+        String tableName = """"; // TODO insert table name
+        String host = ""localhost"";
+
+        KuduClient client = new KuduClient.KuduClientBuilder(host).build();
+        try {
+            List<ColumnSchema> columns = new ArrayList(2);
+            columns.add(new ColumnSchema.ColumnSchemaBuilder(""valueInt"", Type.INT32)
+                    .key(true)
+                    .build());
+            columns.add(new ColumnSchema.ColumnSchemaBuilder(""valueString"", Type.STRING)
+                    .build());
+            List<String> rangeKeys = new ArrayList<>();
+            rangeKeys.add(""valueInt"");
+            Schema schema = new Schema(columns);
+            client.createTable(tableName, schema,
+                    new CreateTableOptions().setRangePartitionColumns(rangeKeys).addHashPartitions(rangeKeys, 4));
+            System.out.println(""Table \"""" + tableName + ""\"" created succesfully"");
+        } catch (Exception e) {","[{'comment': 'I would suggest to just let the main method throw exceptions.', 'commenter': 'rmetzger'}]"
17,flink-connector-kudu/src/main/java/es/accenture/flink/Utils/Utils.java,"@@ -0,0 +1,447 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package es.accenture.flink.Utils;
+
+import es.accenture.flink.Sink.KuduOutputFormat;
+import es.accenture.flink.Utils.Exceptions.KuduClientException;
+import es.accenture.flink.Utils.Exceptions.KuduTableException;
+import org.apache.kudu.ColumnSchema;
+import org.apache.kudu.ColumnSchema.ColumnSchemaBuilder;
+import org.apache.kudu.Schema;
+import org.apache.kudu.Type;
+import org.apache.kudu.client.*;
+import org.apache.log4j.Logger;
+
+import java.util.ArrayList;
+import java.util.List;
+
+public class Utils {
+
+    //Kudu variables
+    private KuduClient client;
+    private KuduSession session;
+
+    // LOG4J
+    private final static Logger logger = Logger.getLogger(Utils.class);
+
+    /**
+     * Builder Util Class which creates a Kudu client and log in to be able to perform operations later
+     * @param host Kudu's host
+     * @throws KuduClientException In case of exception caused by Kudu Client
+     */
+    public Utils(String host) throws KuduClientException {
+        this.client = new KuduClient.KuduClientBuilder(host).build();
+        if (client == null){
+            throw new KuduClientException(""ERROR: param \""host\"" not valid, can't establish connection"");
+        }
+        this.session = this.client.newSession();
+    }
+
+    /**
+     * Return an instance of the table indicated in the settings
+     *
+     * In case that the table exists, return an instance of the table
+     * In case that the table doesn't exist, create a new table with the data provided and return an instance
+     * In both cases,takes into account the way of the table to perfom some operations or others
+     *
+     *     If the mode is CREATE:
+     *
+     *         If the table exists: return error (Can not create table that already exists)
+     *         If the table doesn't exist and  the list of column names has not been provided: return error
+     *         If the table doesn't exist and  the list of column names has been provided: create a new table with data provided and return an instance
+     *
+     *    If the mode is APPEND:
+     *
+     *        If the table exists: return the instance in the table
+     *        If the table doesn't exist: return error
+     *
+     *    If the mode is OVERRIDE:
+     *
+     *        If the table exist: delete all rows of this table and return an instance of it
+     *        If the table doesn't exist: return error
+     *
+     *
+     * @param tableName             Table name to use
+     * @param tableMode             Operations mode for operate with the table (CREATE, APPEND, OVERRIDE)
+     * @return                      Instance of the table indicated
+     * @throws KuduTableException   In case of can't access to a table o can't create it (wrong params or not existing table)
+     * @throws KuduException        In case of error of Kudu
+     */
+    public KuduTable useTable(String tableName, Integer tableMode) throws KuduTableException, KuduException {
+        KuduTable table;
+
+        if (tableMode == KuduOutputFormat.CREATE) {
+            logger.error(""Bad call method, use useTable(String tableName, String [] fieldsNames, RowSerializable row) instead"");
+            table = null;
+        }else if (tableMode == KuduOutputFormat.APPEND) {
+            logger.info(""Modo APPEND"");
+            try {
+                if (client.tableExists(tableName)) {
+                    //logger.info(""SUCCESS: There is the table with the name \"""" + tableName + ""\"""");
+                    table = client.openTable(tableName);
+                } else {
+                    logger.error(""ERROR: The table doesn't exist"");
+                    throw new KuduTableException(""ERROR: The table doesn't exist, so can't do APPEND operation"");
+                }
+            } catch (Exception e) {
+                throw new KuduTableException(""ERROR: param \""host\"" not valid, can't establish connection"");
+            }
+        }else if (tableMode == KuduOutputFormat.OVERRIDE) {
+            logger.info(""Modo OVERRIDE"");
+            try {
+                if (client.tableExists(tableName)) {
+                    logger.info(""SUCCESS: There is the table with the name \"""" + tableName + ""\"". Emptying the table"");
+                    clearTable(tableName);
+                    table = client.openTable(tableName);
+                } else {
+                    logger.error(""ERROR: The table doesn't exist"");
+                    throw new KuduTableException(""ERROR: The table doesn't exist, so can't do OVERRIDE operation"");
+                }
+            } catch (Exception e) {
+                throw new KuduTableException(""ERROR: param \""host\"" not valid, can't establish connection"");
+            }
+        }else {
+            throw new KuduTableException(""ERROR: Incorrect parameters, please check the constructor method. Incorrect \""tableMode\"" parameter."");
+        }
+        return table;
+    }
+
+    /**
+     * Returns an instance of the table requested in parameters
+     * If the table exists, returns an instance of the table
+     * If the table doesn't exist, creates a new table with the data provided and returns an instance
+     *
+     * @param tableName     Table name to use
+     * @param fieldsNames   List of names of columns of the table (to create table)
+     * @param row           List of values to insert a row in the table (to know the types of columns)
+     * @return              Instance of the table indicated
+     * @throws IllegalArgumentException In case of wrong parameters
+     * @throws KuduException    In case of exception caused by Kudu
+     */
+    public KuduTable useTable(String tableName, String [] fieldsNames, RowSerializable row) throws IllegalArgumentException, KuduException {
+        KuduTable table;
+
+        if (client.tableExists(tableName)){
+            logger.info(""The table exists"");
+            table = client.openTable(tableName);
+        } else {
+            if (tableName == null || tableName.equals("""")) {
+                throw new IllegalArgumentException(""ERROR: Incorrect parameters, please check the constructor method. Incorrect \""tableName\"" parameter."");
+
+            } else if (fieldsNames == null || fieldsNames[0].isEmpty()) {
+                throw new IllegalArgumentException(""ERROR: Incorrect parameters, please check the constructor method. Missing \""fields\"" parameter."");
+
+            } else if (row == null){
+                throw new IllegalArgumentException(""ERROR: Incorrect parameters, please check the constructor method. Incorrect \""row\"" parameter."");
+
+            } else {
+                logger.info(""The table doesn't exist"");
+                table = createTable(tableName, fieldsNames, row);
+            }
+        }
+        return table;
+    }
+    /**
+     * Create a new Kudu table and return the instance of this table
+     *
+     * @param tableName     name of the table to create
+     * @param fieldsNames   list name columns of the table
+     * @param row           list of values to insert a row in the table( to know the types of columns)
+     * @return              instance of the table indicated
+     * @throws KuduException In case of exception caused by Kudu
+     */
+    public KuduTable createTable (String tableName, String [] fieldsNames, RowSerializable row) throws KuduException {
+
+        if(client.tableExists(tableName))
+            return client.openTable(tableName);
+
+
+        List<ColumnSchema> columns = new ArrayList<ColumnSchema>();
+        List<String> rangeKeys = new ArrayList<String>(); // Primary key
+        rangeKeys.add(fieldsNames[0]);
+
+        logger.info(""Creating the table \"""" + tableName + ""\""..."");
+        for (int i = 0; i < fieldsNames.length; i++){
+            ColumnSchema col;
+            String colName = fieldsNames[i];
+            Type colType = getRowsPositionType(i, row);
+
+            if (colName.equals(fieldsNames[0])) {
+                col = new ColumnSchemaBuilder(colName, colType).key(true).build();
+                columns.add(0, col);//To create the table, the key must be the first in the column list otherwise it will give a failure
+            } else {
+                col = new ColumnSchemaBuilder(colName, colType).build();
+                columns.add(col);
+            }
+        }
+        Schema schema = new Schema(columns);
+
+        if(!client.tableExists(tableName))
+            client.createTable(tableName, schema, new CreateTableOptions().setRangePartitionColumns(rangeKeys).addHashPartitions(rangeKeys, 4));
+        //logger.info(""SUCCESS: The table has been created successfully"");
+
+
+        return client.openTable(tableName);
+    }
+    /**
+     * Delete the indicated table
+     *
+     * @param tableName name table to delete
+     */
+    public void deleteTable (String tableName){
+
+        logger.info(""Deleting the table \"""" + tableName + ""\""..."");
+        try {
+            if(client.tableExists(tableName)) {
+                client.deleteTable(tableName);
+                logger.info(""SUCCESS: Table deleted successfully"");
+            }
+        } catch (KuduException e) {
+            logger.error(""The table \"""" + tableName  +""\"" doesn't exist, so can't be deleted."", e);
+        }
+    }
+
+    /**
+     * Return the type of the value of the position ""pos"", like the class object ""Type""
+     *
+     * @param pos   Row position
+     * @param row   list of values to insert a row in the table
+     * @return      element type ""pos""-esimo of ""row""
+     */
+    public Type getRowsPositionType (int pos, RowSerializable row){
+        Type colType = null;
+        switch(row.productElement(pos).getClass().getName()){
+            case ""java.lang.String"":
+                colType = Type.STRING;
+                break;
+            case ""java.lang.Integer"":
+                colType = Type.INT32;
+                break;
+            case ""java.lang.Boolean"":
+                colType = Type.BOOL;
+                break;
+            default:
+                break;
+        }
+        return colType;
+    }
+
+    /**
+     * Return a list with all rows of the indicated table
+     *
+     * @param tableName Table name to read
+     * @return          List of rows in the table(object Row)
+     * @throws KuduException In case of exception caused by Kudu
+     */
+    public List<RowSerializable> readTable (String tableName) throws KuduException {
+
+        KuduTable table = client.openTable(tableName);
+        KuduScanner scanner = client.newScannerBuilder(table).build();
+        //Obtain the column name list
+        String[] columnsNames = getNamesOfColumns(table);
+        //The list return all rows
+        List<RowSerializable> rowsList = new ArrayList<>();
+
+        int posRow = 0;
+        while (scanner.hasMoreRows()) {
+            for (RowResult row : scanner.nextRows()) { //Get the rows
+                RowSerializable rowToInsert = new RowSerializable(columnsNames.length);
+                for (String col : columnsNames) { //For each column, it's type determined and this is how to read it
+
+                    String colType = row.getColumnType(col).getName();
+                    switch (colType) {
+                        case ""string"":
+                            rowToInsert.setField(posRow, row.getString(col));
+                            posRow++;
+                            break;
+                        case ""int32"":
+                            rowToInsert.setField(posRow, row.getInt(col));
+                            posRow++;
+                            break;
+                        case ""bool"":
+                            rowToInsert.setField(posRow, row.getBoolean(col));
+                            posRow++;
+                            break;
+                        default:
+                            break;
+                    }
+                }
+                rowsList.add(rowToInsert);
+                posRow = 0;
+            }
+        }
+        return rowsList;
+    }
+
+
+
+    /**
+     * Return a list with all rows of the indicated table
+     *
+     * @param tableName Table name to read
+     * @throws KuduException In case of exception caused by Kudu
+     */
+    public void readTablePrint (String tableName) throws KuduException {
+        KuduTable table = client.openTable(tableName);
+        KuduScanner scanner = client.newScannerBuilder(table).build();
+        int cont = 0;
+        try {
+            while (scanner.hasMoreRows()) {
+                RowResultIterator results = scanner.nextRows();
+                while (results.hasNext()) {
+                    RowResult result = results.next();
+                    System.out.println(result.rowToString());
+                    cont++;
+                }
+            }
+            System.out.println(""Number of rows: "" + cont);
+        } catch (Exception e) {","[{'comment': 'The exception handling needs improvement', 'commenter': 'rmetzger'}]"
17,flink-connector-kudu/src/main/java/es/accenture/flink/Utils/Utils.java,"@@ -0,0 +1,447 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package es.accenture.flink.Utils;
+
+import es.accenture.flink.Sink.KuduOutputFormat;
+import es.accenture.flink.Utils.Exceptions.KuduClientException;
+import es.accenture.flink.Utils.Exceptions.KuduTableException;
+import org.apache.kudu.ColumnSchema;
+import org.apache.kudu.ColumnSchema.ColumnSchemaBuilder;
+import org.apache.kudu.Schema;
+import org.apache.kudu.Type;
+import org.apache.kudu.client.*;
+import org.apache.log4j.Logger;
+
+import java.util.ArrayList;
+import java.util.List;
+
+public class Utils {
+
+    //Kudu variables
+    private KuduClient client;
+    private KuduSession session;
+
+    // LOG4J
+    private final static Logger logger = Logger.getLogger(Utils.class);
+
+    /**
+     * Builder Util Class which creates a Kudu client and log in to be able to perform operations later
+     * @param host Kudu's host
+     * @throws KuduClientException In case of exception caused by Kudu Client
+     */
+    public Utils(String host) throws KuduClientException {
+        this.client = new KuduClient.KuduClientBuilder(host).build();
+        if (client == null){
+            throw new KuduClientException(""ERROR: param \""host\"" not valid, can't establish connection"");
+        }
+        this.session = this.client.newSession();
+    }
+
+    /**
+     * Return an instance of the table indicated in the settings
+     *
+     * In case that the table exists, return an instance of the table
+     * In case that the table doesn't exist, create a new table with the data provided and return an instance
+     * In both cases,takes into account the way of the table to perfom some operations or others
+     *
+     *     If the mode is CREATE:
+     *
+     *         If the table exists: return error (Can not create table that already exists)
+     *         If the table doesn't exist and  the list of column names has not been provided: return error
+     *         If the table doesn't exist and  the list of column names has been provided: create a new table with data provided and return an instance
+     *
+     *    If the mode is APPEND:
+     *
+     *        If the table exists: return the instance in the table
+     *        If the table doesn't exist: return error
+     *
+     *    If the mode is OVERRIDE:
+     *
+     *        If the table exist: delete all rows of this table and return an instance of it
+     *        If the table doesn't exist: return error
+     *
+     *
+     * @param tableName             Table name to use
+     * @param tableMode             Operations mode for operate with the table (CREATE, APPEND, OVERRIDE)
+     * @return                      Instance of the table indicated
+     * @throws KuduTableException   In case of can't access to a table o can't create it (wrong params or not existing table)
+     * @throws KuduException        In case of error of Kudu
+     */
+    public KuduTable useTable(String tableName, Integer tableMode) throws KuduTableException, KuduException {
+        KuduTable table;
+
+        if (tableMode == KuduOutputFormat.CREATE) {
+            logger.error(""Bad call method, use useTable(String tableName, String [] fieldsNames, RowSerializable row) instead"");
+            table = null;
+        }else if (tableMode == KuduOutputFormat.APPEND) {
+            logger.info(""Modo APPEND"");
+            try {
+                if (client.tableExists(tableName)) {
+                    //logger.info(""SUCCESS: There is the table with the name \"""" + tableName + ""\"""");
+                    table = client.openTable(tableName);
+                } else {
+                    logger.error(""ERROR: The table doesn't exist"");
+                    throw new KuduTableException(""ERROR: The table doesn't exist, so can't do APPEND operation"");
+                }
+            } catch (Exception e) {
+                throw new KuduTableException(""ERROR: param \""host\"" not valid, can't establish connection"");
+            }
+        }else if (tableMode == KuduOutputFormat.OVERRIDE) {
+            logger.info(""Modo OVERRIDE"");
+            try {
+                if (client.tableExists(tableName)) {
+                    logger.info(""SUCCESS: There is the table with the name \"""" + tableName + ""\"". Emptying the table"");
+                    clearTable(tableName);
+                    table = client.openTable(tableName);
+                } else {
+                    logger.error(""ERROR: The table doesn't exist"");
+                    throw new KuduTableException(""ERROR: The table doesn't exist, so can't do OVERRIDE operation"");
+                }
+            } catch (Exception e) {
+                throw new KuduTableException(""ERROR: param \""host\"" not valid, can't establish connection"");
+            }
+        }else {
+            throw new KuduTableException(""ERROR: Incorrect parameters, please check the constructor method. Incorrect \""tableMode\"" parameter."");
+        }
+        return table;
+    }
+
+    /**
+     * Returns an instance of the table requested in parameters
+     * If the table exists, returns an instance of the table
+     * If the table doesn't exist, creates a new table with the data provided and returns an instance
+     *
+     * @param tableName     Table name to use
+     * @param fieldsNames   List of names of columns of the table (to create table)
+     * @param row           List of values to insert a row in the table (to know the types of columns)
+     * @return              Instance of the table indicated
+     * @throws IllegalArgumentException In case of wrong parameters
+     * @throws KuduException    In case of exception caused by Kudu
+     */
+    public KuduTable useTable(String tableName, String [] fieldsNames, RowSerializable row) throws IllegalArgumentException, KuduException {
+        KuduTable table;
+
+        if (client.tableExists(tableName)){
+            logger.info(""The table exists"");
+            table = client.openTable(tableName);
+        } else {
+            if (tableName == null || tableName.equals("""")) {
+                throw new IllegalArgumentException(""ERROR: Incorrect parameters, please check the constructor method. Incorrect \""tableName\"" parameter."");
+
+            } else if (fieldsNames == null || fieldsNames[0].isEmpty()) {
+                throw new IllegalArgumentException(""ERROR: Incorrect parameters, please check the constructor method. Missing \""fields\"" parameter."");
+
+            } else if (row == null){
+                throw new IllegalArgumentException(""ERROR: Incorrect parameters, please check the constructor method. Incorrect \""row\"" parameter."");
+
+            } else {
+                logger.info(""The table doesn't exist"");
+                table = createTable(tableName, fieldsNames, row);
+            }
+        }
+        return table;
+    }
+    /**
+     * Create a new Kudu table and return the instance of this table
+     *
+     * @param tableName     name of the table to create
+     * @param fieldsNames   list name columns of the table
+     * @param row           list of values to insert a row in the table( to know the types of columns)
+     * @return              instance of the table indicated
+     * @throws KuduException In case of exception caused by Kudu
+     */
+    public KuduTable createTable (String tableName, String [] fieldsNames, RowSerializable row) throws KuduException {
+
+        if(client.tableExists(tableName))
+            return client.openTable(tableName);
+
+
+        List<ColumnSchema> columns = new ArrayList<ColumnSchema>();
+        List<String> rangeKeys = new ArrayList<String>(); // Primary key
+        rangeKeys.add(fieldsNames[0]);
+
+        logger.info(""Creating the table \"""" + tableName + ""\""..."");
+        for (int i = 0; i < fieldsNames.length; i++){
+            ColumnSchema col;
+            String colName = fieldsNames[i];
+            Type colType = getRowsPositionType(i, row);
+
+            if (colName.equals(fieldsNames[0])) {
+                col = new ColumnSchemaBuilder(colName, colType).key(true).build();
+                columns.add(0, col);//To create the table, the key must be the first in the column list otherwise it will give a failure
+            } else {
+                col = new ColumnSchemaBuilder(colName, colType).build();
+                columns.add(col);
+            }
+        }
+        Schema schema = new Schema(columns);
+
+        if(!client.tableExists(tableName))
+            client.createTable(tableName, schema, new CreateTableOptions().setRangePartitionColumns(rangeKeys).addHashPartitions(rangeKeys, 4));
+        //logger.info(""SUCCESS: The table has been created successfully"");
+
+
+        return client.openTable(tableName);
+    }
+    /**
+     * Delete the indicated table
+     *
+     * @param tableName name table to delete
+     */
+    public void deleteTable (String tableName){
+
+        logger.info(""Deleting the table \"""" + tableName + ""\""..."");
+        try {
+            if(client.tableExists(tableName)) {
+                client.deleteTable(tableName);
+                logger.info(""SUCCESS: Table deleted successfully"");
+            }
+        } catch (KuduException e) {
+            logger.error(""The table \"""" + tableName  +""\"" doesn't exist, so can't be deleted."", e);
+        }
+    }
+
+    /**
+     * Return the type of the value of the position ""pos"", like the class object ""Type""
+     *
+     * @param pos   Row position
+     * @param row   list of values to insert a row in the table
+     * @return      element type ""pos""-esimo of ""row""
+     */
+    public Type getRowsPositionType (int pos, RowSerializable row){
+        Type colType = null;
+        switch(row.productElement(pos).getClass().getName()){
+            case ""java.lang.String"":
+                colType = Type.STRING;
+                break;
+            case ""java.lang.Integer"":
+                colType = Type.INT32;
+                break;
+            case ""java.lang.Boolean"":
+                colType = Type.BOOL;
+                break;
+            default:
+                break;
+        }
+        return colType;
+    }
+
+    /**
+     * Return a list with all rows of the indicated table
+     *
+     * @param tableName Table name to read
+     * @return          List of rows in the table(object Row)
+     * @throws KuduException In case of exception caused by Kudu
+     */
+    public List<RowSerializable> readTable (String tableName) throws KuduException {
+
+        KuduTable table = client.openTable(tableName);
+        KuduScanner scanner = client.newScannerBuilder(table).build();
+        //Obtain the column name list
+        String[] columnsNames = getNamesOfColumns(table);
+        //The list return all rows
+        List<RowSerializable> rowsList = new ArrayList<>();
+
+        int posRow = 0;
+        while (scanner.hasMoreRows()) {
+            for (RowResult row : scanner.nextRows()) { //Get the rows
+                RowSerializable rowToInsert = new RowSerializable(columnsNames.length);
+                for (String col : columnsNames) { //For each column, it's type determined and this is how to read it
+
+                    String colType = row.getColumnType(col).getName();
+                    switch (colType) {
+                        case ""string"":
+                            rowToInsert.setField(posRow, row.getString(col));
+                            posRow++;
+                            break;
+                        case ""int32"":
+                            rowToInsert.setField(posRow, row.getInt(col));
+                            posRow++;
+                            break;
+                        case ""bool"":
+                            rowToInsert.setField(posRow, row.getBoolean(col));
+                            posRow++;
+                            break;
+                        default:
+                            break;
+                    }
+                }
+                rowsList.add(rowToInsert);
+                posRow = 0;
+            }
+        }
+        return rowsList;
+    }
+
+
+
+    /**
+     * Return a list with all rows of the indicated table
+     *
+     * @param tableName Table name to read
+     * @throws KuduException In case of exception caused by Kudu
+     */
+    public void readTablePrint (String tableName) throws KuduException {
+        KuduTable table = client.openTable(tableName);
+        KuduScanner scanner = client.newScannerBuilder(table).build();
+        int cont = 0;
+        try {
+            while (scanner.hasMoreRows()) {
+                RowResultIterator results = scanner.nextRows();
+                while (results.hasNext()) {
+                    RowResult result = results.next();
+                    System.out.println(result.rowToString());
+                    cont++;
+                }
+            }
+            System.out.println(""Number of rows: "" + cont);
+        } catch (Exception e) {
+            e.printStackTrace();
+        } finally {
+            try {
+                client.shutdown();
+            } catch (Exception e) {
+                e.printStackTrace();
+            }
+        }
+    }
+
+    /**
+     * Returns a representation on the table screen of a table
+     *
+     * @param row   row to show
+     * @return      a string containing the data of the row indicated in the parameter
+     */
+    public String printRow (RowSerializable row){
+        String res = """";
+        for(int i = 0; i< row.productArity(); i++){
+            res += (row.productElement(i) + "" | "");","[{'comment': 'I would recommend using a StringBuilder here ', 'commenter': 'rmetzger'}]"
17,flink-connector-kudu/src/main/java/es/accenture/flink/Utils/Utils.java,"@@ -0,0 +1,447 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package es.accenture.flink.Utils;
+
+import es.accenture.flink.Sink.KuduOutputFormat;
+import es.accenture.flink.Utils.Exceptions.KuduClientException;
+import es.accenture.flink.Utils.Exceptions.KuduTableException;
+import org.apache.kudu.ColumnSchema;
+import org.apache.kudu.ColumnSchema.ColumnSchemaBuilder;
+import org.apache.kudu.Schema;
+import org.apache.kudu.Type;
+import org.apache.kudu.client.*;
+import org.apache.log4j.Logger;
+
+import java.util.ArrayList;
+import java.util.List;
+
+public class Utils {
+
+    //Kudu variables
+    private KuduClient client;
+    private KuduSession session;
+
+    // LOG4J
+    private final static Logger logger = Logger.getLogger(Utils.class);
+
+    /**
+     * Builder Util Class which creates a Kudu client and log in to be able to perform operations later
+     * @param host Kudu's host
+     * @throws KuduClientException In case of exception caused by Kudu Client
+     */
+    public Utils(String host) throws KuduClientException {
+        this.client = new KuduClient.KuduClientBuilder(host).build();
+        if (client == null){
+            throw new KuduClientException(""ERROR: param \""host\"" not valid, can't establish connection"");
+        }
+        this.session = this.client.newSession();
+    }
+
+    /**
+     * Return an instance of the table indicated in the settings
+     *
+     * In case that the table exists, return an instance of the table
+     * In case that the table doesn't exist, create a new table with the data provided and return an instance
+     * In both cases,takes into account the way of the table to perfom some operations or others
+     *
+     *     If the mode is CREATE:
+     *
+     *         If the table exists: return error (Can not create table that already exists)
+     *         If the table doesn't exist and  the list of column names has not been provided: return error
+     *         If the table doesn't exist and  the list of column names has been provided: create a new table with data provided and return an instance
+     *
+     *    If the mode is APPEND:
+     *
+     *        If the table exists: return the instance in the table
+     *        If the table doesn't exist: return error
+     *
+     *    If the mode is OVERRIDE:
+     *
+     *        If the table exist: delete all rows of this table and return an instance of it
+     *        If the table doesn't exist: return error
+     *
+     *
+     * @param tableName             Table name to use
+     * @param tableMode             Operations mode for operate with the table (CREATE, APPEND, OVERRIDE)
+     * @return                      Instance of the table indicated
+     * @throws KuduTableException   In case of can't access to a table o can't create it (wrong params or not existing table)
+     * @throws KuduException        In case of error of Kudu
+     */
+    public KuduTable useTable(String tableName, Integer tableMode) throws KuduTableException, KuduException {
+        KuduTable table;
+
+        if (tableMode == KuduOutputFormat.CREATE) {
+            logger.error(""Bad call method, use useTable(String tableName, String [] fieldsNames, RowSerializable row) instead"");
+            table = null;
+        }else if (tableMode == KuduOutputFormat.APPEND) {
+            logger.info(""Modo APPEND"");
+            try {
+                if (client.tableExists(tableName)) {
+                    //logger.info(""SUCCESS: There is the table with the name \"""" + tableName + ""\"""");
+                    table = client.openTable(tableName);
+                } else {
+                    logger.error(""ERROR: The table doesn't exist"");
+                    throw new KuduTableException(""ERROR: The table doesn't exist, so can't do APPEND operation"");
+                }
+            } catch (Exception e) {
+                throw new KuduTableException(""ERROR: param \""host\"" not valid, can't establish connection"");
+            }
+        }else if (tableMode == KuduOutputFormat.OVERRIDE) {
+            logger.info(""Modo OVERRIDE"");
+            try {
+                if (client.tableExists(tableName)) {
+                    logger.info(""SUCCESS: There is the table with the name \"""" + tableName + ""\"". Emptying the table"");
+                    clearTable(tableName);
+                    table = client.openTable(tableName);
+                } else {
+                    logger.error(""ERROR: The table doesn't exist"");
+                    throw new KuduTableException(""ERROR: The table doesn't exist, so can't do OVERRIDE operation"");
+                }
+            } catch (Exception e) {
+                throw new KuduTableException(""ERROR: param \""host\"" not valid, can't establish connection"");
+            }
+        }else {
+            throw new KuduTableException(""ERROR: Incorrect parameters, please check the constructor method. Incorrect \""tableMode\"" parameter."");
+        }
+        return table;
+    }
+
+    /**
+     * Returns an instance of the table requested in parameters
+     * If the table exists, returns an instance of the table
+     * If the table doesn't exist, creates a new table with the data provided and returns an instance
+     *
+     * @param tableName     Table name to use
+     * @param fieldsNames   List of names of columns of the table (to create table)
+     * @param row           List of values to insert a row in the table (to know the types of columns)
+     * @return              Instance of the table indicated
+     * @throws IllegalArgumentException In case of wrong parameters
+     * @throws KuduException    In case of exception caused by Kudu
+     */
+    public KuduTable useTable(String tableName, String [] fieldsNames, RowSerializable row) throws IllegalArgumentException, KuduException {
+        KuduTable table;
+
+        if (client.tableExists(tableName)){
+            logger.info(""The table exists"");
+            table = client.openTable(tableName);
+        } else {
+            if (tableName == null || tableName.equals("""")) {
+                throw new IllegalArgumentException(""ERROR: Incorrect parameters, please check the constructor method. Incorrect \""tableName\"" parameter."");
+
+            } else if (fieldsNames == null || fieldsNames[0].isEmpty()) {
+                throw new IllegalArgumentException(""ERROR: Incorrect parameters, please check the constructor method. Missing \""fields\"" parameter."");
+
+            } else if (row == null){
+                throw new IllegalArgumentException(""ERROR: Incorrect parameters, please check the constructor method. Incorrect \""row\"" parameter."");
+
+            } else {
+                logger.info(""The table doesn't exist"");
+                table = createTable(tableName, fieldsNames, row);
+            }
+        }
+        return table;
+    }
+    /**
+     * Create a new Kudu table and return the instance of this table
+     *
+     * @param tableName     name of the table to create
+     * @param fieldsNames   list name columns of the table
+     * @param row           list of values to insert a row in the table( to know the types of columns)
+     * @return              instance of the table indicated
+     * @throws KuduException In case of exception caused by Kudu
+     */
+    public KuduTable createTable (String tableName, String [] fieldsNames, RowSerializable row) throws KuduException {
+
+        if(client.tableExists(tableName))
+            return client.openTable(tableName);
+
+
+        List<ColumnSchema> columns = new ArrayList<ColumnSchema>();
+        List<String> rangeKeys = new ArrayList<String>(); // Primary key
+        rangeKeys.add(fieldsNames[0]);
+
+        logger.info(""Creating the table \"""" + tableName + ""\""..."");
+        for (int i = 0; i < fieldsNames.length; i++){
+            ColumnSchema col;
+            String colName = fieldsNames[i];
+            Type colType = getRowsPositionType(i, row);
+
+            if (colName.equals(fieldsNames[0])) {
+                col = new ColumnSchemaBuilder(colName, colType).key(true).build();
+                columns.add(0, col);//To create the table, the key must be the first in the column list otherwise it will give a failure
+            } else {
+                col = new ColumnSchemaBuilder(colName, colType).build();
+                columns.add(col);
+            }
+        }
+        Schema schema = new Schema(columns);
+
+        if(!client.tableExists(tableName))
+            client.createTable(tableName, schema, new CreateTableOptions().setRangePartitionColumns(rangeKeys).addHashPartitions(rangeKeys, 4));
+        //logger.info(""SUCCESS: The table has been created successfully"");
+
+
+        return client.openTable(tableName);
+    }
+    /**
+     * Delete the indicated table
+     *
+     * @param tableName name table to delete
+     */
+    public void deleteTable (String tableName){
+
+        logger.info(""Deleting the table \"""" + tableName + ""\""..."");
+        try {
+            if(client.tableExists(tableName)) {
+                client.deleteTable(tableName);
+                logger.info(""SUCCESS: Table deleted successfully"");
+            }
+        } catch (KuduException e) {
+            logger.error(""The table \"""" + tableName  +""\"" doesn't exist, so can't be deleted."", e);
+        }
+    }
+
+    /**
+     * Return the type of the value of the position ""pos"", like the class object ""Type""
+     *
+     * @param pos   Row position
+     * @param row   list of values to insert a row in the table
+     * @return      element type ""pos""-esimo of ""row""
+     */
+    public Type getRowsPositionType (int pos, RowSerializable row){
+        Type colType = null;
+        switch(row.productElement(pos).getClass().getName()){
+            case ""java.lang.String"":
+                colType = Type.STRING;
+                break;
+            case ""java.lang.Integer"":
+                colType = Type.INT32;
+                break;
+            case ""java.lang.Boolean"":
+                colType = Type.BOOL;
+                break;
+            default:
+                break;
+        }
+        return colType;
+    }
+
+    /**
+     * Return a list with all rows of the indicated table
+     *
+     * @param tableName Table name to read
+     * @return          List of rows in the table(object Row)
+     * @throws KuduException In case of exception caused by Kudu
+     */
+    public List<RowSerializable> readTable (String tableName) throws KuduException {
+
+        KuduTable table = client.openTable(tableName);
+        KuduScanner scanner = client.newScannerBuilder(table).build();
+        //Obtain the column name list
+        String[] columnsNames = getNamesOfColumns(table);
+        //The list return all rows
+        List<RowSerializable> rowsList = new ArrayList<>();
+
+        int posRow = 0;
+        while (scanner.hasMoreRows()) {
+            for (RowResult row : scanner.nextRows()) { //Get the rows
+                RowSerializable rowToInsert = new RowSerializable(columnsNames.length);
+                for (String col : columnsNames) { //For each column, it's type determined and this is how to read it
+
+                    String colType = row.getColumnType(col).getName();
+                    switch (colType) {
+                        case ""string"":
+                            rowToInsert.setField(posRow, row.getString(col));
+                            posRow++;
+                            break;
+                        case ""int32"":
+                            rowToInsert.setField(posRow, row.getInt(col));
+                            posRow++;
+                            break;
+                        case ""bool"":
+                            rowToInsert.setField(posRow, row.getBoolean(col));
+                            posRow++;
+                            break;
+                        default:
+                            break;
+                    }
+                }
+                rowsList.add(rowToInsert);
+                posRow = 0;
+            }
+        }
+        return rowsList;
+    }
+
+
+
+    /**
+     * Return a list with all rows of the indicated table
+     *
+     * @param tableName Table name to read
+     * @throws KuduException In case of exception caused by Kudu
+     */
+    public void readTablePrint (String tableName) throws KuduException {
+        KuduTable table = client.openTable(tableName);
+        KuduScanner scanner = client.newScannerBuilder(table).build();
+        int cont = 0;
+        try {
+            while (scanner.hasMoreRows()) {
+                RowResultIterator results = scanner.nextRows();
+                while (results.hasNext()) {
+                    RowResult result = results.next();
+                    System.out.println(result.rowToString());
+                    cont++;
+                }
+            }
+            System.out.println(""Number of rows: "" + cont);
+        } catch (Exception e) {
+            e.printStackTrace();
+        } finally {
+            try {
+                client.shutdown();
+            } catch (Exception e) {
+                e.printStackTrace();
+            }
+        }
+    }
+
+    /**
+     * Returns a representation on the table screen of a table
+     *
+     * @param row   row to show
+     * @return      a string containing the data of the row indicated in the parameter
+     */
+    public String printRow (RowSerializable row){
+        String res = """";
+        for(int i = 0; i< row.productArity(); i++){
+            res += (row.productElement(i) + "" | "");
+        }
+        return res;
+    }
+
+    /**
+     * Deelte all rows of the table until empty
+     *
+     * @param tableName  table name to empty
+     * @throws KuduException In case of exception caused by Kudu
+     */
+    public void clearTable (String tableName) throws KuduException {
+        KuduTable table = client.openTable(tableName);
+        List<RowSerializable> rowsList = readTable(tableName);
+
+        String primaryKey = table.getSchema().getPrimaryKeyColumns().get(0).getName();
+        List<Delete> deletes = new ArrayList<>();
+        for(RowSerializable row : rowsList){
+            Delete d = table.newDelete();
+            switch(getRowsPositionType(0, row).getName()){
+                case ""string"":
+                    d.getRow().addString(primaryKey, (String) row.productElement(0));
+                    break;
+
+                case ""int32"":
+                    d.getRow().addInt(primaryKey, (Integer) row.productElement(0));
+                    break;
+
+                case ""bool"":
+                    d.getRow().addBoolean(primaryKey, (Boolean) row.productElement(0));
+                    break;
+
+                default:
+                    break;
+            }
+            deletes.add(d);
+        }
+        for(Delete d : deletes){
+            session.apply(d);
+        }
+        logger.info(""SUCCESS: The table has been emptied successfully"");
+    }
+
+    /**
+     * Return a list of columns names in a table
+     *
+     * @param table table instance
+     * @return      List of column names in the table indicated in the parameter
+     */
+    public String [] getNamesOfColumns (KuduTable table){
+        List<ColumnSchema> columns = table.getSchema().getColumns();
+        List<String> columnsNames = new ArrayList<>(); //  List of column names
+        for (ColumnSchema schema : columns) {
+            columnsNames.add(schema.getName());
+        }
+        String [] array = new String[columnsNames.size()];
+        array = columnsNames.toArray(array);
+        return array;
+    }
+
+    public boolean checkNamesOfColumns(String [] tableNames, String [] providedNames) throws KuduTableException{
+        boolean res = false;
+        if(tableNames.length != providedNames.length){
+            res = false;
+        } else{
+            for (int i = 0; i < tableNames.length; i++) {
+                res = tableNames[i].equals(providedNames[i]) ? true : false;
+            }
+        }
+        if(!res){
+            throw new KuduTableException(""ERROR: The table column names and the provided column names don't match"");
+        }
+        return res;
+    }
+
+    public void insert (KuduTable table, RowSerializable row, String [] fieldsNames) throws KuduException, NullPointerException {
+
+        Insert insert;
+        try{
+            insert = table.newInsert();
+        } catch (NullPointerException e){
+            throw new NullPointerException(""Error encountered at opening/creating table"");","[{'comment': ""I don't think this re-throw adds any value.\r\nIf this is a protection against table being null, I would do an explicit null check."", 'commenter': 'rmetzger'}]"
17,flink-connector-kudu/src/test/java/es/accenture/flink/Job/JobBatchSinkTest.java,"@@ -0,0 +1,158 @@
+package es.accenture.flink.Job;","[{'comment': 'the license header is missing + wrong package.', 'commenter': 'rmetzger'}]"
17,flink-connector-kudu/src/test/java/es/accenture/flink/Job/JobBatchSinkTest.java,"@@ -0,0 +1,158 @@
+package es.accenture.flink.Job;
+
+import es.accenture.flink.Sink.KuduOutputFormat;
+import es.accenture.flink.Sources.KuduInputFormat;
+import es.accenture.flink.Utils.Exceptions.KuduClientException;
+import es.accenture.flink.Utils.Exceptions.KuduTableException;
+import es.accenture.flink.Utils.RowSerializable;
+import es.accenture.flink.Utils.Utils;
+import org.apache.flink.api.common.typeinfo.TypeInformation;
+import org.apache.flink.api.java.DataSet;
+import org.apache.flink.api.java.ExecutionEnvironment;
+import org.apache.kudu.client.KuduClient;
+import org.apache.kudu.client.KuduException;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+public class JobBatchSinkTest {
+
+    /*class vars*/
+    private String KUDU_MASTER;
+    private String TABLE_NAME;
+    private String TABLE_NAME2;
+    private String[] columnNames;
+    private Utils utils;
+    private KuduClient client;
+    private ExecutionEnvironment env;
+    private boolean singleton = true;
+    private DataSet<RowSerializable> input = null;
+    private Integer MODE;
+
+
+    /**
+     * Function to set program's variables
+     * @throws Exception excepcion
+     */
+    @Before
+    public void setUp() throws Exception {
+        KUDU_MASTER = System.getProperty(""kuduMaster"", ""localhost"");
+        TABLE_NAME = System.getProperty(""tableName"", ""Table_1"");
+        TABLE_NAME2 = System.getProperty(""tableName"", ""Table_2"");
+        MODE = KuduOutputFormat.CREATE;
+        columnNames= new String[2];
+        columnNames[0] = ""key"";
+        columnNames[1] = ""value"";
+        utils=new Utils(""localhost"");
+        client = new KuduClient.KuduClientBuilder(KUDU_MASTER).build();
+
+
+    }","[{'comment': 'empty lines.', 'commenter': 'rmetzger'}]"
31,flink-connector-kudu/dockers/docker-compose.yml,"@@ -0,0 +1,77 @@
+","[{'comment': 'Could you please add Apache License header.', 'commenter': 'lresende'}, {'comment': 'added', 'commenter': 'eskabetxe'}]"
31,flink-connector-kudu/dockers/role/Dockerfile,"@@ -0,0 +1,24 @@
+FROM bitnami/minideb:jessie","[{'comment': 'Could you please add Apache License header.', 'commenter': 'lresende'}, {'comment': 'done', 'commenter': 'eskabetxe'}]"
31,flink-connector-kudu/dockers/role/docker-entrypoint.sh,"@@ -0,0 +1,53 @@
+#!/bin/bash","[{'comment': 'Could you please add Apache License header.', 'commenter': 'lresende'}, {'comment': 'done', 'commenter': 'eskabetxe'}]"
31,flink-connector-kudu/dockers/run_kudu_tests.sh,"@@ -0,0 +1,52 @@
+#!/bin/bash","[{'comment': 'Could you please add Apache License header.', 'commenter': 'lresende'}, {'comment': 'done', 'commenter': 'eskabetxe'}]"
31,flink-connector-kudu/dockers/start-images.sh,"@@ -0,0 +1,27 @@
+#!/usr/bin/env bash","[{'comment': 'Could you please add Apache License header.', 'commenter': 'lresende'}, {'comment': 'done', 'commenter': 'eskabetxe'}]"
31,flink-connector-kudu/dockers/stop-images.sh,"@@ -0,0 +1,17 @@
+#!/usr/bin/env bash","[{'comment': 'Could you please add Apache License header.', 'commenter': 'lresende'}, {'comment': 'done', 'commenter': 'eskabetxe'}]"
31,flink-connector-kudu/src/main/java/org/apache/flink/streaming/connectors/kudu/KuduOutputFormat.java,"@@ -0,0 +1,110 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.flink.streaming.connectors.kudu;
+
+import org.apache.flink.api.common.io.OutputFormat;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.streaming.connectors.kudu.connector.KuduConnector;
+import org.apache.flink.streaming.connectors.kudu.connector.KuduRow;
+import org.apache.flink.streaming.connectors.kudu.connector.KuduTableInfo;
+import org.apache.flink.util.Preconditions;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+
+public class KuduOutputFormat<OUT extends KuduRow> implements OutputFormat<OUT> {
+
+    private static final Logger LOG = LoggerFactory.getLogger(KuduOutputFormat.class);
+
+    private String kuduMasters;
+    private KuduTableInfo tableInfo;
+    private KuduConnector.Consistency consistency;
+    private KuduConnector.WriteMode writeMode;
+
+    private transient KuduConnector tableContext;
+
+
+    public KuduOutputFormat(String kuduMasters, KuduTableInfo tableInfo) {
+        Preconditions.checkNotNull(kuduMasters,""kuduMasters could not be null"");
+        this.kuduMasters = kuduMasters;
+
+        Preconditions.checkNotNull(tableInfo,""tableInfo could not be null"");
+        this.tableInfo = tableInfo;
+        this.consistency = KuduConnector.Consistency.STRONG;
+        this.writeMode = KuduConnector.WriteMode.UPSERT;
+    }
+
+    public KuduOutputFormat<OUT> withEventualConsistency() {
+        this.consistency = KuduConnector.Consistency.EVENTUAL;
+        return this;
+    }
+
+    public KuduOutputFormat<OUT> withStrongConsistency() {
+        this.consistency = KuduConnector.Consistency.STRONG;
+        return this;
+    }
+
+    public KuduOutputFormat<OUT> withUpsertWriteMode() {
+        this.writeMode = KuduConnector.WriteMode.UPSERT;
+        return this;
+    }
+
+    public KuduOutputFormat<OUT> withInsertWriteMode() {
+        this.writeMode = KuduConnector.WriteMode.INSERT;
+        return this;
+    }
+
+    public KuduOutputFormat<OUT> withUpdateWriteMode() {
+        this.writeMode = KuduConnector.WriteMode.UPDATE;
+        return this;
+    }
+
+    @Override
+    public void configure(Configuration parameters) {","[{'comment': 'Are we not supporting configuring the extension? ', 'commenter': 'lresende'}, {'comment': 'the configuration is done by the methods above. \r\nthe only configuration possible is setting the writeMode or consistence and if you have multiple output format, this configuration can be different between outputs\r\n', 'commenter': 'eskabetxe'}]"
31,pom.xml,"@@ -274,6 +275,7 @@
               <exclude>**/dependency-reduced-pom.xml</exclude>
               <exclude>**/target/**</exclude>
               <exclude>**/README.md</exclude>
+              <exclude>**/dockers/**</exclude>","[{'comment': 'This should not be required as all the files inside dockers seems to accept the Apache License header', 'commenter': 'lresende'}, {'comment': 'removed', 'commenter': 'eskabetxe'}]"
57,flink-connector-redis/src/test/java/org/apache/flink/streaming/connectors/redis/common/config/JedisClusterConfigTest.java,"@@ -46,4 +48,20 @@ public void shouldThrowIllegalArgumentExceptionIfNodeValuesAreEmpty(){
             .setNodes(set)
             .build();
     }
+
+    @Test
+    public void shouldSetPasswordSuccessfully() {","[{'comment': 'could you add a test with no password setted.', 'commenter': 'eskabetxe'}, {'comment': 'I added the test.', 'commenter': 'liketic'}]"
82,flink-connector-kudu/src/main/java/org/apache/flink/connectors/kudu/table/KuduTableSource.java,"@@ -109,12 +156,167 @@ public boolean isLimitPushedDown() {
         for (int i = 0; i < ints.length; i++) {
             fieldNames[i] = prevFieldNames.get(ints[i]);
         }
-        return new KuduTableSource(configBuilder, tableInfo, flinkSchema, fieldNames);
+        return new KuduTableSource(configBuilder, tableInfo, flinkSchema, predicates, fieldNames);
+    }
+
+    @Override
+    public TableSource<Row> applyPredicate(List<Expression> predicates) {
+        // try to convert Flink filter expressions to Kudu Filter Info
+        List<KuduFilterInfo> kuduPredicates = new ArrayList<>(predicates.size());
+        List<Expression> unsupportedExpressions = new ArrayList<>(predicates.size());
+        for (Expression pred : predicates) {
+            KuduFilterInfo kuduPred = toKuduFilterInfo(pred);
+            if (kuduPred != null) {
+                LOG.info(""Predicate [{}] converted into KuduFilterInfo [{}] and pushed into "" +
+                    ""KuduTable [{}]."", pred, kuduPred, tableInfo.getName());
+                kuduPredicates.add(kuduPred);
+            } else {
+                unsupportedExpressions.add(pred);
+                LOG.info(""Predicate [{}] could not be pushed into KuduFilterInfo for KuduTable [{}]."",
+                    pred, tableInfo.getName());
+            }
+        }
+        // update list of Flink expressions to unsupported expressions
+        predicates.clear();
+        predicates.addAll(unsupportedExpressions);
+        return new KuduTableSource(configBuilder, tableInfo, flinkSchema, kuduPredicates, projectedFields);
+    }
+
+    /**
+     * Converts Flink Expression to KuduFilterInfo.
+     */
+    @Nullable
+    private KuduFilterInfo toKuduFilterInfo(Expression predicate) {","[{'comment': 'Could we move all the logic translating between Expression to KuduFilterInfo to a Utility class? That would leave the source cleaner.', 'commenter': 'gyfora'}, {'comment': 'done', 'commenter': 'sebastianliu'}]"
82,flink-connector-kudu/src/main/java/org/apache/flink/connectors/kudu/table/KuduTableSource.java,"@@ -109,12 +156,167 @@ public boolean isLimitPushedDown() {
         for (int i = 0; i < ints.length; i++) {
             fieldNames[i] = prevFieldNames.get(ints[i]);
         }
-        return new KuduTableSource(configBuilder, tableInfo, flinkSchema, fieldNames);
+        return new KuduTableSource(configBuilder, tableInfo, flinkSchema, predicates, fieldNames);
+    }
+
+    @Override
+    public TableSource<Row> applyPredicate(List<Expression> predicates) {
+        // try to convert Flink filter expressions to Kudu Filter Info
+        List<KuduFilterInfo> kuduPredicates = new ArrayList<>(predicates.size());
+        List<Expression> unsupportedExpressions = new ArrayList<>(predicates.size());
+        for (Expression pred : predicates) {
+            KuduFilterInfo kuduPred = toKuduFilterInfo(pred);
+            if (kuduPred != null) {
+                LOG.info(""Predicate [{}] converted into KuduFilterInfo [{}] and pushed into "" +
+                    ""KuduTable [{}]."", pred, kuduPred, tableInfo.getName());
+                kuduPredicates.add(kuduPred);
+            } else {
+                unsupportedExpressions.add(pred);
+                LOG.info(""Predicate [{}] could not be pushed into KuduFilterInfo for KuduTable [{}]."",
+                    pred, tableInfo.getName());
+            }
+        }
+        // update list of Flink expressions to unsupported expressions
+        predicates.clear();","[{'comment': ""Tha javadocs of applyPredicate specifies that we should remove the applicable expressions from the list only.\r\nI know it's not a large difference but maybe it would be better to use an iterator and remove the expression if pushed down."", 'commenter': 'gyfora'}, {'comment': 'Fixed. This has been changed to use iterator. Thx for this reminder', 'commenter': 'sebastianliu'}]"
82,flink-connector-kudu/src/main/java/org/apache/flink/connectors/kudu/table/KuduTableSource.java,"@@ -17,55 +17,106 @@
 
 package org.apache.flink.connectors.kudu.table;
 
+import org.apache.flink.api.common.io.InputFormat;
+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;
 import org.apache.flink.api.common.typeinfo.TypeInformation;
 import org.apache.flink.connectors.kudu.batch.KuduRowInputFormat;
+import org.apache.flink.connectors.kudu.connector.KuduFilterInfo;
 import org.apache.flink.connectors.kudu.connector.KuduTableInfo;
 import org.apache.flink.connectors.kudu.connector.reader.KuduReaderConfig;
-import org.apache.flink.streaming.api.datastream.DataStream;
-import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
 import org.apache.flink.table.api.DataTypes;
 import org.apache.flink.table.api.TableSchema;
+import org.apache.flink.table.expressions.Expression;
+import org.apache.flink.table.planner.expressions.Attribute;
+import org.apache.flink.table.planner.expressions.BinaryComparison;
+import org.apache.flink.table.planner.expressions.EqualTo;
+import org.apache.flink.table.planner.expressions.GreaterThan;
+import org.apache.flink.table.planner.expressions.GreaterThanOrEqual;
+import org.apache.flink.table.planner.expressions.IsNotNull;
+import org.apache.flink.table.planner.expressions.IsNull;
+import org.apache.flink.table.planner.expressions.LessThan;
+import org.apache.flink.table.planner.expressions.LessThanOrEqual;
+import org.apache.flink.table.planner.expressions.Literal;
+import org.apache.flink.table.planner.expressions.UnaryExpression;
+import org.apache.flink.table.sources.FilterableTableSource;
+import org.apache.flink.table.sources.InputFormatTableSource;
 import org.apache.flink.table.sources.LimitableTableSource;
 import org.apache.flink.table.sources.ProjectableTableSource;
-import org.apache.flink.table.sources.StreamTableSource;
 import org.apache.flink.table.sources.TableSource;
 import org.apache.flink.table.types.DataType;
 import org.apache.flink.table.types.logical.RowType;
-import org.apache.flink.table.types.utils.TypeConversions;
 import org.apache.flink.types.Row;
 
 import org.apache.flink.shaded.guava18.com.google.common.collect.Lists;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
+import javax.annotation.Nullable;
+import java.util.ArrayList;
 import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashSet;
 import java.util.List;
+import java.util.Set;
 
-public class KuduTableSource implements StreamTableSource<Row>, LimitableTableSource<Row>, ProjectableTableSource<Row> {
+public class KuduTableSource extends InputFormatTableSource<Row> implements
+    LimitableTableSource<Row>, ProjectableTableSource<Row>, FilterableTableSource<Row> {
+
+    private static final Logger LOG = LoggerFactory.getLogger(KuduTableSource.class);
+
+    @SuppressWarnings(""unchecked"")
+    private static final Set<BasicTypeInfo> VALID_LITERAL_TYPE = new HashSet() {{","[{'comment': 'Should probably go to a Utility class with all the filter logic', 'commenter': 'gyfora'}, {'comment': 'Moved to KuduTableUtils', 'commenter': 'sebastianliu'}]"
82,flink-connector-kudu/src/test/java/org/apache/flink/connectors/kudu/table/KuduTableSourceTest.java,"@@ -0,0 +1,73 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.flink.connectors.kudu.table;
+
+import org.apache.flink.api.java.DataSet;
+import org.apache.flink.api.java.ExecutionEnvironment;
+import org.apache.flink.connectors.kudu.connector.KuduTableInfo;
+import org.apache.flink.connectors.kudu.connector.KuduTestBase;
+import org.apache.flink.table.api.Table;
+import org.apache.flink.table.api.java.BatchTableEnvironment;
+import org.apache.flink.types.Row;
+import org.junit.jupiter.api.BeforeEach;
+import org.junit.jupiter.api.Test;
+
+import java.util.List;
+
+import static org.junit.Assert.assertEquals;
+
+public class KuduTableSourceTest extends KuduTestBase {
+    private BatchTableEnvironment tableEnv;
+    private KuduCatalog catalog;
+
+    @BeforeEach
+    public void init() {
+        KuduTableInfo tableInfo = booksTableInfo(""books"", true);
+        setUpDatabase(tableInfo);
+        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
+        tableEnv = KuduTableTestUtils.createBatchTableEnvWithBlinkPlannerBatchMode(env);
+        catalog = new KuduCatalog(harness.getMasterAddressesAsString());
+        tableEnv.registerCatalog(""kudu"", catalog);
+        tableEnv.useCatalog(""kudu"");
+    }
+
+    @Test
+    public void testFullScan() throws Exception {
+        Table table = tableEnv.sqlQuery(""select * from books order by id"");
+        DataSet<Row> dataSet = tableEnv.toDataSet(table, Row.class);
+        List<Row> result = dataSet.collect();
+        // check result
+        assertEquals(5, result.size());
+        assertEquals(""1001,Java for dummies,Tan Ah Teck,11.11,11"",
+            result.get(0).toString());
+        tableEnv.sqlUpdate(""DROP TABLE books"");
+    }
+
+    @Test
+    public void testScanWithProjectionAndFilter() throws Exception {","[{'comment': 'With this test we cannot be sure that the filters were actually pushed down and it works correctly.\r\n\r\nWe should also add a unit test style test that validates the KuduFilterInfos created and the KuduTableSource directly.\r\nThings like the applyPredicate or isFilterPushed down and all the rest.\r\n\r\nThe problem here is that we add a lot of different filtering logic that can easily break the output of SQL queries if incorrect, so we really want to make sure that it is tested.', 'commenter': 'gyfora'}, {'comment': 'Good point. I have changed this test to be an integration test and added a new unit test style test for KuduTableSource. Thx ', 'commenter': 'sebastianliu'}]"
82,flink-connector-kudu/src/main/java/org/apache/flink/connectors/kudu/table/KuduTableFactory.java,"@@ -65,6 +65,7 @@
     public static final String KUDU_HASH_COLS = ""kudu.hash-columns"";
     public static final String KUDU_PRIMARY_KEY_COLS = ""kudu.primary-key-columns"";
     public static final String KUDU_REPLICAS = ""kudu.replicas"";
+    public static final String KUDU_IS_BOUNDED = ""kudu.is-bounded"";","[{'comment': 'Why are we introducing the is-bounded flag? As far as I can tell this only affects the isBounded() method of the KuduTableSource and not the actual reading of the data.\r\n\r\nThe Kudu reading logic at the moment reads the table contents at the start of the job so it is always bounded, I think we should not touch this logic now. \r\n\r\nWhat do you think?', 'commenter': 'gyfora'}, {'comment': 'For the `is-bounded` flag, it really just control the `isBounded()` method in \r\n`KuduTableSource` which inherit from `StreamTableSource`. The main reason I add this is because there may be some users who want to use this kudu connector in a Flink Batch SQL job. And this is usually used under the Blink Batch mode. I also added a few notes to the README file. In addition, I keep the default behavior of this `is-bounded` flag, which is false in `StreamTableSource`.  :)', 'commenter': 'sebastianliu'}, {'comment': 'already remove this unnecessary flag', 'commenter': 'sebastianliu'}]"
82,flink-connector-kudu/src/main/java/org/apache/flink/connectors/kudu/table/KuduTableSource.java,"@@ -109,12 +147,41 @@ public boolean isLimitPushedDown() {
         for (int i = 0; i < ints.length; i++) {
             fieldNames[i] = prevFieldNames.get(ints[i]);
         }
-        return new KuduTableSource(configBuilder, tableInfo, flinkSchema, fieldNames);
+        return new KuduTableSource(configBuilder, tableInfo, flinkSchema, bounded, predicates, fieldNames);
+    }
+
+    @Override
+    public TableSource<Row> applyPredicate(List<Expression> predicates) {
+        List<KuduFilterInfo> kuduPredicates = new ArrayList<>();
+        ListIterator<Expression> predicatesIter = predicates.listIterator();
+        while(predicatesIter.hasNext()) {
+            Expression predicate = predicatesIter.next();
+            KuduFilterInfo kuduPred = toKuduFilterInfo(predicate);","[{'comment': 'Would be nicer to return Optional here but I leave this up to you', 'commenter': 'gyfora'}, {'comment': 'Good suggestion, has changed to use `Optional` var', 'commenter': 'sebastianliu'}]"
82,flink-connector-kudu/src/main/java/org/apache/flink/connectors/kudu/table/KuduTableSource.java,"@@ -109,12 +147,41 @@ public boolean isLimitPushedDown() {
         for (int i = 0; i < ints.length; i++) {
             fieldNames[i] = prevFieldNames.get(ints[i]);
         }
-        return new KuduTableSource(configBuilder, tableInfo, flinkSchema, fieldNames);
+        return new KuduTableSource(configBuilder, tableInfo, flinkSchema, bounded, predicates, fieldNames);
+    }
+
+    @Override
+    public TableSource<Row> applyPredicate(List<Expression> predicates) {
+        List<KuduFilterInfo> kuduPredicates = new ArrayList<>();
+        ListIterator<Expression> predicatesIter = predicates.listIterator();
+        while(predicatesIter.hasNext()) {
+            Expression predicate = predicatesIter.next();
+            KuduFilterInfo kuduPred = toKuduFilterInfo(predicate);
+            if (kuduPred != null) {
+                LOG.info(""Predicate [{}] converted into KuduFilterInfo and pushed into "" +","[{'comment': 'Should we use debug logging here?', 'commenter': 'gyfora'}, {'comment': 'fixed', 'commenter': 'sebastianliu'}]"
82,flink-connector-kudu/src/main/java/org/apache/flink/connectors/kudu/table/KuduTableSource.java,"@@ -109,12 +147,41 @@ public boolean isLimitPushedDown() {
         for (int i = 0; i < ints.length; i++) {
             fieldNames[i] = prevFieldNames.get(ints[i]);
         }
-        return new KuduTableSource(configBuilder, tableInfo, flinkSchema, fieldNames);
+        return new KuduTableSource(configBuilder, tableInfo, flinkSchema, bounded, predicates, fieldNames);
+    }
+
+    @Override
+    public TableSource<Row> applyPredicate(List<Expression> predicates) {
+        List<KuduFilterInfo> kuduPredicates = new ArrayList<>();
+        ListIterator<Expression> predicatesIter = predicates.listIterator();
+        while(predicatesIter.hasNext()) {
+            Expression predicate = predicatesIter.next();
+            KuduFilterInfo kuduPred = toKuduFilterInfo(predicate);
+            if (kuduPred != null) {
+                LOG.info(""Predicate [{}] converted into KuduFilterInfo and pushed into "" +
+                    ""KuduTable [{}]."", predicate, tableInfo.getName());
+                kuduPredicates.add(kuduPred);
+                predicatesIter.remove();
+            } else {
+                LOG.info(""Predicate [{}] could not be pushed into KuduFilterInfo for KuduTable [{}]."",","[{'comment': 'debug log maybe?', 'commenter': 'gyfora'}]"
82,flink-connector-kudu/src/main/java/org/apache/flink/connectors/kudu/table/KuduTableSource.java,"@@ -109,12 +147,41 @@ public boolean isLimitPushedDown() {
         for (int i = 0; i < ints.length; i++) {
             fieldNames[i] = prevFieldNames.get(ints[i]);
         }
-        return new KuduTableSource(configBuilder, tableInfo, flinkSchema, fieldNames);
+        return new KuduTableSource(configBuilder, tableInfo, flinkSchema, bounded, predicates, fieldNames);
+    }
+
+    @Override
+    public TableSource<Row> applyPredicate(List<Expression> predicates) {
+        List<KuduFilterInfo> kuduPredicates = new ArrayList<>();
+        ListIterator<Expression> predicatesIter = predicates.listIterator();
+        while(predicatesIter.hasNext()) {
+            Expression predicate = predicatesIter.next();
+            KuduFilterInfo kuduPred = toKuduFilterInfo(predicate);
+            if (kuduPred != null) {
+                LOG.info(""Predicate [{}] converted into KuduFilterInfo and pushed into "" +
+                    ""KuduTable [{}]."", predicate, tableInfo.getName());
+                kuduPredicates.add(kuduPred);
+                predicatesIter.remove();
+            } else {
+                LOG.info(""Predicate [{}] could not be pushed into KuduFilterInfo for KuduTable [{}]."",
+                    predicate, tableInfo.getName());
+            }
+        }
+        return new KuduTableSource(configBuilder, tableInfo, flinkSchema, bounded, kuduPredicates, projectedFields);
     }
 
     @Override
     public String explainSource() {
-        return ""KuduStreamTableSource[schema="" + Arrays.toString(getTableSchema().getFieldNames())
-                + (projectedFields != null ?"", projectFields="" + Arrays.toString(projectedFields) + ""]"" : ""]"");
+        return ""KuduTableSource[schema="" + Arrays.toString(getTableSchema().getFieldNames()) +
+            "", filter="" + predicateString() +
+            (projectedFields != null ?"", projectFields="" + Arrays.toString(projectedFields) + ""]"" : ""]"");
+    }
+
+    private String predicateString() {
+        if (predicates == null || predicates.size() == 0) {
+            return ""FALSE"";","[{'comment': 'Maybe instead of false, something like No Filters / predicates?', 'commenter': 'gyfora'}]"
103,flink-connector-kudu/src/main/java/org/apache/flink/connectors/kudu/streaming/KuduSink.java,"@@ -97,6 +97,21 @@ public void invoke(IN value) throws Exception {
         }
     }
 
+    /**
+     * new invoke api
+     * @param value input
+     * @param context runtime context
+     * @throws Exception
+     */
+    @Override
+    public void invoke(IN value, Context context) throws Exception {","[{'comment': 'Code duplication, this should invoke the other method or vice versa', 'commenter': 'gyfora'}, {'comment': 'done', 'commenter': 'collabH'}]"
103,flink-connector-kudu/src/main/java/org/apache/flink/connectors/kudu/table/utils/KuduTableUtils.java,"@@ -79,10 +80,19 @@ public static KuduTableInfo createTableInfo(String tableName, TableSchema schema
             ColumnSchemasFactory schemasFactory = () -> toKuduConnectorColumns(columns, keyColumns);
             List<String> hashColumns = getHashColumns(props);
             int replicas = Optional.ofNullable(props.get(KuduTableFactory.KUDU_REPLICAS)).map(Integer::parseInt).orElse(1);
+            // if hash partitions nums not exists,default 1;
+            int hashPartitionNums = Optional.ofNullable(props.get(KUDU_HASH_PARTITION_NUMS)).map(Integer::parseInt).orElse(3);
+            // if table owner is null,default 'admin';
+            String tableOwner = Optional.ofNullable(props.get(KUDU_TABLE_OWNER)).orElse(""admin"");
+
+            PartialRow lowerRow = new PartialRow(new Schema(schemasFactory.getColumnSchemas()));
+            PartialRow upper = new PartialRow(new Schema(schemasFactory.getColumnSchemas()));
+
 
             CreateTableOptionsFactory optionsFactory = () -> new CreateTableOptions()
                     .setNumReplicas(replicas)
-                    .addHashPartitions(hashColumns, replicas * 2);
+                    .addHashPartitions(hashColumns, hashPartitionNums)","[{'comment': 'Why did we change the default from replicas * 2 to hardcoded 3?', 'commenter': 'gyfora'}, {'comment': ""Instead of hard coding 3, I provide a parameter 'kudu. Hash-partition-nums' to support the number of buckets configured in kudu's hash partition"", 'commenter': 'collabH'}, {'comment': 'What I mean is that previously the default number of hashpartitions was replicas * 2, now if the new option is not specified it falls back to a default of 3', 'commenter': 'gyfora'}, {'comment': ""Got it. I've fixed it"", 'commenter': 'collabH'}]"
103,flink-connector-kudu/src/main/java/org/apache/flink/connectors/kudu/table/utils/KuduTableUtils.java,"@@ -78,11 +79,20 @@ public static KuduTableInfo createTableInfo(String tableName, TableSchema schema
             List<String> keyColumns = getPrimaryKeyColumns(props, schema);
             ColumnSchemasFactory schemasFactory = () -> toKuduConnectorColumns(columns, keyColumns);
             List<String> hashColumns = getHashColumns(props);
-            int replicas = Optional.ofNullable(props.get(KuduTableFactory.KUDU_REPLICAS)).map(Integer::parseInt).orElse(1);
+            int replicas = Optional.ofNullable(props.get(KuduTableFactory.KUDU_REPLICAS)).map(Integer::parseInt).orElse(3);","[{'comment': 'why are we changing the default number of replicas from 1 to 3 ?', 'commenter': 'eskabetxe'}, {'comment': 'because the number of copies of the previous version was 3.', 'commenter': 'collabH'}]"
103,flink-connector-kudu/src/test/java/org/apache/flink/connectors/kudu/table/NewTablePropertiesTest.java,"@@ -0,0 +1,44 @@
+package org.apache.flink.connectors.kudu.table;
+
+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
+import org.apache.flink.table.api.EnvironmentSettings;
+import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;
+import org.apache.flink.table.catalog.ObjectPath;
+import org.apache.flink.table.catalog.exceptions.TableNotExistException;
+import org.junit.jupiter.api.BeforeEach;
+import org.junit.jupiter.api.Test;
+
+/**","[{'comment': 'could you remove this type of javadoc', 'commenter': 'eskabetxe'}, {'comment': 'done', 'commenter': 'collabH'}]"
113,flink-connector-pinot/pom.xml,"@@ -0,0 +1,213 @@
+<?xml version=""1.0"" encoding=""UTF-8""?>
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+  http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+<project xmlns=""http://maven.apache.org/POM/4.0.0""
+         xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
+         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"">
+
+    <modelVersion>4.0.0</modelVersion>
+
+    <parent>
+        <groupId>org.apache.bahir</groupId>
+        <artifactId>bahir-flink-parent_2.11</artifactId>
+        <version>1.1-SNAPSHOT</version>
+        <relativePath>..</relativePath>
+    </parent>
+
+    <artifactId>flink-connector-pinot_2.11</artifactId>
+    <name>flink-connector-pinot</name>
+
+
+    <packaging>jar</packaging>
+
+    <!-- Allow users to pass custom connector versions -->
+    <properties>
+        <pinot.version>0.6.0</pinot.version>
+
+        <!-- Flink version -->
+        <flink.version>1.12.0</flink.version>","[{'comment': 'we should update the flink version on parent\r\n', 'commenter': 'eskabetxe'}, {'comment': 'Totally right, this gets obsolete once #115 was merged', 'commenter': 'mschroederi'}]"
113,flink-connector-pinot/src/main/java/org/apache/flink/streaming/connectors/pinot/PinotControllerApi.java,"@@ -0,0 +1,304 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.streaming.connectors.pinot;
+
+import com.fasterxml.jackson.databind.JsonNode;
+import org.apache.flink.streaming.connectors.pinot.exceptions.PinotControllerApiException;
+import org.apache.http.HttpEntity;
+import org.apache.http.StatusLine;
+import org.apache.http.client.methods.*;
+import org.apache.http.entity.ContentType;
+import org.apache.http.entity.StringEntity;
+import org.apache.http.impl.client.CloseableHttpClient;
+import org.apache.http.impl.client.HttpClients;
+import org.apache.http.util.EntityUtils;
+import org.apache.pinot.spi.config.table.TableConfig;
+import org.apache.pinot.spi.data.Schema;
+import org.apache.pinot.spi.utils.JsonUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import static org.apache.flink.util.Preconditions.checkNotNull;
+
+/**
+ * Helpers to interact with the Pinot controller via its public API.
+ */
+public class PinotControllerApi {
+
+    private static final Logger LOG = LoggerFactory.getLogger(PinotControllerApi.class);
+    protected final String controllerHost;
+    protected final String controllerHostPort;
+
+    /**
+     * @param controllerHost Pinot controller's host
+     * @param controllerPort Pinot controller's port
+     */
+    public PinotControllerApi(String controllerHost, String controllerPort) {
+        this.controllerHost = checkNotNull(controllerHost);","[{'comment': 'this could be removed, its only used to generate the controolerHostPort no?', 'commenter': 'eskabetxe'}, {'comment': ""You're right, thanks for noting"", 'commenter': 'mschroederi'}]"
113,flink-connector-pinot/src/main/java/org/apache/flink/streaming/connectors/pinot/PinotControllerApi.java,"@@ -0,0 +1,304 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.streaming.connectors.pinot;
+
+import com.fasterxml.jackson.databind.JsonNode;
+import org.apache.flink.streaming.connectors.pinot.exceptions.PinotControllerApiException;
+import org.apache.http.HttpEntity;
+import org.apache.http.StatusLine;
+import org.apache.http.client.methods.*;
+import org.apache.http.entity.ContentType;
+import org.apache.http.entity.StringEntity;
+import org.apache.http.impl.client.CloseableHttpClient;
+import org.apache.http.impl.client.HttpClients;
+import org.apache.http.util.EntityUtils;
+import org.apache.pinot.spi.config.table.TableConfig;
+import org.apache.pinot.spi.data.Schema;
+import org.apache.pinot.spi.utils.JsonUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import static org.apache.flink.util.Preconditions.checkNotNull;
+
+/**
+ * Helpers to interact with the Pinot controller via its public API.
+ */
+public class PinotControllerApi {
+
+    private static final Logger LOG = LoggerFactory.getLogger(PinotControllerApi.class);
+    protected final String controllerHost;
+    protected final String controllerHostPort;
+
+    /**
+     * @param controllerHost Pinot controller's host
+     * @param controllerPort Pinot controller's port
+     */
+    public PinotControllerApi(String controllerHost, String controllerPort) {
+        this.controllerHost = checkNotNull(controllerHost);
+        checkNotNull(controllerPort);
+        this.controllerHostPort = String.format(""http://%s:%s"", controllerHost, controllerPort);
+    }
+
+    /**
+     * Issues a request to the Pinot controller API.
+     *
+     * @param request Request to issue
+     * @return Api response
+     * @throws IOException
+     */
+    private ApiResponse execute(HttpRequestBase request) throws IOException {
+        ApiResponse result;
+
+        try (CloseableHttpClient httpClient = HttpClients.createDefault();
+             CloseableHttpResponse response = httpClient.execute(request)) {
+
+
+            String body = EntityUtils.toString(response.getEntity());","[{'comment': 'if fail to connect to pinot this will fail no?', 'commenter': 'eskabetxe'}, {'comment': '@mschroederi I am also interested what happens in the error case. In general it should be safe to fail the Flink job if the connection to pinot cannot be established.', 'commenter': 'fapaul'}]"
113,flink-connector-pinot/src/main/java/org/apache/flink/streaming/connectors/pinot/committer/PinotSinkGlobalCommitter.java,"@@ -0,0 +1,428 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.streaming.connectors.pinot.committer;
+
+import org.apache.flink.api.connector.sink.GlobalCommitter;
+import org.apache.flink.streaming.connectors.pinot.PinotControllerApi;
+import org.apache.flink.streaming.connectors.pinot.filesystem.FileSystemAdapter;
+import org.apache.pinot.common.segment.ReadMode;
+import org.apache.pinot.core.indexsegment.generator.SegmentGeneratorConfig;
+import org.apache.pinot.core.indexsegment.immutable.ImmutableSegment;
+import org.apache.pinot.core.indexsegment.immutable.ImmutableSegmentLoader;
+import org.apache.pinot.core.segment.creator.SegmentIndexCreationDriver;
+import org.apache.pinot.core.segment.creator.impl.SegmentIndexCreationDriverImpl;
+import org.apache.pinot.core.segment.name.SegmentNameGenerator;
+import org.apache.pinot.spi.config.table.TableConfig;
+import org.apache.pinot.spi.data.Schema;
+import org.apache.pinot.spi.data.readers.FileFormat;
+import org.apache.pinot.tools.admin.command.UploadSegmentCommand;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.File;
+import java.io.IOException;
+import java.nio.file.Files;
+import java.util.ArrayList;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Set;
+import java.util.concurrent.*;
+
+import static org.apache.flink.util.Preconditions.checkNotNull;
+
+/**
+ * Global committer takes committables from {@link org.apache.flink.streaming.connectors.pinot.writer.PinotSinkWriter},
+ * generates segments and pushed them to the Pinot controller.
+ * Note: We use a custom multithreading approach to parallelize the segment creation and upload to
+ * overcome the performance limitations resulting from using a {@link GlobalCommitter} always
+ * running at a parallelism of 1.
+ */
+public class PinotSinkGlobalCommitter implements GlobalCommitter<PinotSinkCommittable, PinotSinkGlobalCommittable> {
+
+    private static final Logger LOG = LoggerFactory.getLogger(PinotSinkGlobalCommitter.class);
+
+    private final String pinotControllerHost;
+    private final String pinotControllerPort;
+    private final String tableName;
+    private final SegmentNameGenerator segmentNameGenerator;
+    private final String tempDirPrefix;
+    private final FileSystemAdapter fsAdapter;
+    private final String timeColumnName;
+    private final TimeUnit segmentTimeUnit;
+
+    /**
+     * @param pinotControllerHost  Host of the Pinot controller
+     * @param pinotControllerPort  Port of the Pinot controller
+     * @param tableName            Target table's name
+     * @param segmentNameGenerator Pinot segment name generator
+     * @param fsAdapter            Adapter for interacting with the shared file system
+     * @param timeColumnName       Name of the column containing the timestamp
+     * @param segmentTimeUnit      Unit of the time column
+     */
+    public PinotSinkGlobalCommitter(String pinotControllerHost, String pinotControllerPort, String tableName, SegmentNameGenerator segmentNameGenerator, String tempDirPrefix, FileSystemAdapter fsAdapter, String timeColumnName, TimeUnit segmentTimeUnit) {
+        this.pinotControllerHost = checkNotNull(pinotControllerHost);
+        this.pinotControllerPort = checkNotNull(pinotControllerPort);
+        this.tableName = checkNotNull(tableName);
+        this.segmentNameGenerator = checkNotNull(segmentNameGenerator);
+        this.tempDirPrefix = checkNotNull(tempDirPrefix);
+        this.fsAdapter = checkNotNull(fsAdapter);
+        this.timeColumnName = checkNotNull(timeColumnName);
+        this.segmentTimeUnit = checkNotNull(segmentTimeUnit);
+    }
+
+    /**
+     * Identifies global committables that need to be re-committed from a list of recovered committables.
+     *
+     * @param globalCommittables List of global committables that are checked for required re-commit
+     * @return List of global committable that need to be re-committed
+     * @throws IOException
+     */
+    @Override
+    public List<PinotSinkGlobalCommittable> filterRecoveredCommittables(List<PinotSinkGlobalCommittable> globalCommittables) throws IOException {
+        PinotControllerApi controllerApi = new PinotControllerApi(this.pinotControllerHost, this.pinotControllerPort);
+        List<PinotSinkGlobalCommittable> committablesToRetry = new ArrayList<>();
+
+        for (PinotSinkGlobalCommittable globalCommittable : globalCommittables) {
+            CommitStatus commitStatus = this.getCommitStatus(globalCommittable);
+
+            if (commitStatus.getMissingSegmentNames().isEmpty()) {
+                // All segments were already committed. Thus, we do not need to retry the commit.
+                continue;
+            }
+
+            for (String existingSegment : commitStatus.getExistingSegmentNames()) {
+                // Some but not all segments were already committed. As we cannot assure the data
+                // files containing the same data as originally when recovering from failure,
+                // we delete the already committed segments in order to recommit them later on.
+                controllerApi.deleteSegment(tableName, existingSegment);
+            }
+            committablesToRetry.add(globalCommittable);
+        }
+
+        return committablesToRetry;
+    }
+
+    /**
+     * Combines multiple {@link PinotSinkCommittable}s into one {@link PinotSinkGlobalCommittable}
+     * by finding the minimum and maximum timestamps from the provided {@link PinotSinkCommittable}s.
+     *
+     * @param committables Committables created by {@link org.apache.flink.streaming.connectors.pinot.writer.PinotSinkWriter}
+     * @return Global committer committable
+     */
+    @Override
+    public PinotSinkGlobalCommittable combine(List<PinotSinkCommittable> committables) {
+        List<String> dataFilePaths = new ArrayList<>();
+        long minTimestamp = Long.MAX_VALUE;
+        long maxTimestamp = Long.MIN_VALUE;
+
+        // Extract all data file paths and the overall minimum and maximum timestamps
+        // from all committables
+        for (PinotSinkCommittable committable : committables) {
+            dataFilePaths.add(committable.getDataFilePath());
+            minTimestamp = Long.min(minTimestamp, committable.getMinTimestamp());
+            maxTimestamp = Long.max(maxTimestamp, committable.getMaxTimestamp());
+        }
+
+        LOG.info(""Combined {} committables into one global committable"", committables.size());
+        return new PinotSinkGlobalCommittable(dataFilePaths, minTimestamp, maxTimestamp);
+    }
+
+    /**
+     * Copies data files from shared filesystem to the local filesystem, generates segments with names
+     * according to the segment naming schema and finally pushes the segments to the Pinot cluster.
+     * Before pushing a segment it is checked whether there already exists a segment with that name
+     * in the Pinot cluster by calling the Pinot controller. In case there is one, it gets deleted.
+     *
+     * @param globalCommittables List of global committables
+     * @return Global committables whose commit failed
+     * @throws IOException
+     */
+    @Override
+    public List<PinotSinkGlobalCommittable> commit(List<PinotSinkGlobalCommittable> globalCommittables) throws IOException {
+        // Retrieve the Pinot table schema and the Pinot table config from the Pinot controller
+        PinotControllerApi controllerApi = new PinotControllerApi(this.pinotControllerHost, this.pinotControllerPort);
+        Schema tableSchema = controllerApi.getSchema(this.tableName);
+        TableConfig tableConfig = controllerApi.getTableConfig(this.tableName);
+
+        // List of failed global committables that can be retried later on
+        List<PinotSinkGlobalCommittable> failedCommits = new ArrayList<>();
+
+        for (PinotSinkGlobalCommittable globalCommittable : globalCommittables) {
+            // Make sure to remove all previously committed segments in globalCommittable
+            // when recovering from failure
+            CommitStatus commitStatus = this.getCommitStatus(globalCommittable);
+            for (String existingSegment : commitStatus.getExistingSegmentNames()) {
+                // Some but not all segments were already committed. As we cannot assure the data
+                // files containing the same data as originally when recovering from failure,
+                // we delete the already committed segments in order to recommit them later on.
+                controllerApi.deleteSegment(tableName, existingSegment);
+            }
+
+            // We use a thread pool in order to parallelize the segment creation and segment upload
+            ExecutorService pool = Executors.newCachedThreadPool();
+            Set<Future<Boolean>> resultFutures = new HashSet<>();
+
+            // Commit all segments in globalCommittable
+            int sequenceId = 0;
+            for (String dataFilePath : globalCommittable.getDataFilePaths()) {
+                // Get segment names with increasing sequenceIds
+                String segmentName = this.getSegmentName(globalCommittable, sequenceId++);
+                // Segment committer handling the whole commit process for a single segment
+                Callable<Boolean> segmentCommitter = new SegmentCommitter(
+                        this.pinotControllerHost, this.pinotControllerPort, this.tempDirPrefix,
+                        this.fsAdapter, dataFilePath, segmentName, tableSchema, tableConfig,
+                        this.timeColumnName, this.segmentTimeUnit
+                );
+                // Submits the segment committer to the thread pool
+                resultFutures.add(pool.submit(segmentCommitter));
+            }
+
+            try {
+                for (Future<Boolean> wasSuccessful : resultFutures) {
+                    // In case any of the segment commits wasn't successful we mark the whole
+                    // globalCommittable as failed
+                    if (!wasSuccessful.get()) {
+                        failedCommits.add(globalCommittable);
+                        // Once any of the commits failed, we do not need to check the remaining
+                        // ones, as we try to commit the globalCommittable next time
+                        break;
+                    }
+                }
+            } catch (Exception e) {
+                // In case of an exception mark the whole globalCommittable as failed
+                failedCommits.add(globalCommittable);
+                LOG.error(e.getMessage());
+                e.printStackTrace();
+            }
+        }
+
+        // Return failed commits so that they can be retried later on
+        return failedCommits;
+    }
+
+    /**
+     * Empty method.
+     */
+    @Override
+    public void endOfInput() {
+    }
+
+    /**
+     * Empty method, as we do not open any connections.
+     */
+    @Override
+    public void close() {
+    }
+
+    /**
+     * Helper method for generating segment names using the segment name generator.
+     *
+     * @param globalCommittable Global committable the segment name shall be generated from
+     * @param sequenceId        Incrementing counter
+     * @return generated segment name
+     */
+    private String getSegmentName(PinotSinkGlobalCommittable globalCommittable, int sequenceId) {
+        return this.segmentNameGenerator.generateSegmentName(sequenceId, globalCommittable.getMinTimestamp(), globalCommittable.getMaxTimestamp());
+    }
+
+    /**
+     * Evaluates the status of already uploaded segments by requesting segment metadata from the
+     * Pinot controller.
+     *
+     * @param globalCommittable Global committable whose commit status gets evaluated
+     * @return Commit status
+     * @throws IOException
+     */
+    private CommitStatus getCommitStatus(PinotSinkGlobalCommittable globalCommittable) throws IOException {
+        PinotControllerApi controllerApi = new PinotControllerApi(this.pinotControllerHost, this.pinotControllerPort);
+
+        List<String> existingSegmentNames = new ArrayList<>();
+        List<String> missingSegmentNames = new ArrayList<>();
+
+        // For all segment names that will be used to submit new segments, check whether the segment
+        // name already exists for the target table
+        for (int sequenceId = 0; sequenceId < globalCommittable.getDataFilePaths().size(); sequenceId++) {
+            String segmentName = this.getSegmentName(globalCommittable, sequenceId);
+            if (controllerApi.tableHasSegment(this.tableName, segmentName)) {
+                // Segment name already exists
+                existingSegmentNames.add(segmentName);
+            } else {
+                // Segment name does not exist yet
+                missingSegmentNames.add(segmentName);
+            }
+        }
+
+        return new CommitStatus(existingSegmentNames, missingSegmentNames);
+    }
+
+    /**
+     * Wrapper for existing and missing segments in the Pinot cluster.
+     */
+    static class CommitStatus {
+        private final List<String> existingSegmentNames;
+        private final List<String> missingSegmentNames;
+
+        public CommitStatus(List<String> existingSegmentNames, List<String> missingSegmentNames) {
+            this.existingSegmentNames = existingSegmentNames;
+            this.missingSegmentNames = missingSegmentNames;
+        }
+
+        public List<String> getExistingSegmentNames() {
+            return existingSegmentNames;
+        }
+
+        public List<String> getMissingSegmentNames() {
+            return missingSegmentNames;
+        }
+    }
+
+    /**
+     * Helper class for committing a single segment. Downloads a data file from the shared filesystem,
+     * generates a segment from the data file and uploads segment to the Pinot controller.
+     */
+    static class SegmentCommitter implements Callable<Boolean> {
+
+        private static final Logger LOG = LoggerFactory.getLogger(SegmentCommitter.class);
+
+        final String pinotControllerHost;
+        final String pinotControllerPort;
+        final String tempDirPrefix;
+        final FileSystemAdapter fsAdapter;
+        final String dataFilePath;
+        final String segmentName;
+        final Schema tableSchema;
+        final TableConfig tableConfig;
+        final String timeColumnName;
+        final TimeUnit segmentTimeUnit;
+
+        /**
+         * @param pinotControllerHost Host of the Pinot controller
+         * @param pinotControllerPort Port of the Pinot controller
+         * @param fsAdapter           Filesystem adapter used to load data files from the shared file system
+         * @param dataFilePath        Data file to load from the shared file system
+         * @param segmentName         Name of the segment to create and commit
+         * @param tableSchema         Pinot table schema
+         * @param tableConfig         Pinot table config
+         * @param timeColumnName      Name of the column containing the timestamp
+         * @param segmentTimeUnit     Unit of the time column
+         */
+        public SegmentCommitter(String pinotControllerHost, String pinotControllerPort, String tempDirPrefix, FileSystemAdapter fsAdapter, String dataFilePath, String segmentName, Schema tableSchema, TableConfig tableConfig, String timeColumnName, TimeUnit segmentTimeUnit) {
+            this.pinotControllerHost = pinotControllerHost;
+            this.pinotControllerPort = pinotControllerPort;
+            this.tempDirPrefix = tempDirPrefix;
+            this.fsAdapter = fsAdapter;
+            this.dataFilePath = dataFilePath;
+            this.segmentName = segmentName;
+            this.tableSchema = tableSchema;
+            this.tableConfig = tableConfig;
+            this.timeColumnName = timeColumnName;
+            this.segmentTimeUnit = segmentTimeUnit;
+        }
+
+        /**
+         * Downloads a segment from the shared file system via {@code fsAdapter}, generates a segment
+         * and finally uploads the segment to the Pinot controller
+         *
+         * @return True if the commit succeeded
+         */
+        @Override
+        public Boolean call() {
+            try {
+                // Download data file from the shared filesystem
+                LOG.info(""Downloading data file {} from shared file system..."", dataFilePath);
+                File segmentData = fsAdapter.copyToLocalFile(dataFilePath);
+                LOG.info(""Successfully downloaded data file {} from shared file system"", dataFilePath);
+
+                File segmentFile = Files.createTempDirectory(this.tempDirPrefix).toFile();
+                LOG.info(""Creating segment in "" + segmentFile.getAbsolutePath());
+
+                // Creates a segment with name `segmentName` in `segmentFile`
+                this.generateSegment(segmentData, segmentFile, true);
+
+                // Uploads the recently created segment to the Pinot controller
+                this.uploadSegment(segmentFile);
+
+                // Commit successful
+                return true;
+            } catch (IOException e) {
+                e.printStackTrace();","[{'comment': 'set this to log', 'commenter': 'eskabetxe'}, {'comment': 'done', 'commenter': 'mschroederi'}]"
113,flink-connector-pinot/src/main/java/org/apache/flink/streaming/connectors/pinot/committer/PinotSinkGlobalCommitter.java,"@@ -0,0 +1,428 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.streaming.connectors.pinot.committer;
+
+import org.apache.flink.api.connector.sink.GlobalCommitter;
+import org.apache.flink.streaming.connectors.pinot.PinotControllerApi;
+import org.apache.flink.streaming.connectors.pinot.filesystem.FileSystemAdapter;
+import org.apache.pinot.common.segment.ReadMode;
+import org.apache.pinot.core.indexsegment.generator.SegmentGeneratorConfig;
+import org.apache.pinot.core.indexsegment.immutable.ImmutableSegment;
+import org.apache.pinot.core.indexsegment.immutable.ImmutableSegmentLoader;
+import org.apache.pinot.core.segment.creator.SegmentIndexCreationDriver;
+import org.apache.pinot.core.segment.creator.impl.SegmentIndexCreationDriverImpl;
+import org.apache.pinot.core.segment.name.SegmentNameGenerator;
+import org.apache.pinot.spi.config.table.TableConfig;
+import org.apache.pinot.spi.data.Schema;
+import org.apache.pinot.spi.data.readers.FileFormat;
+import org.apache.pinot.tools.admin.command.UploadSegmentCommand;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.File;
+import java.io.IOException;
+import java.nio.file.Files;
+import java.util.ArrayList;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Set;
+import java.util.concurrent.*;
+
+import static org.apache.flink.util.Preconditions.checkNotNull;
+
+/**
+ * Global committer takes committables from {@link org.apache.flink.streaming.connectors.pinot.writer.PinotSinkWriter},
+ * generates segments and pushed them to the Pinot controller.
+ * Note: We use a custom multithreading approach to parallelize the segment creation and upload to
+ * overcome the performance limitations resulting from using a {@link GlobalCommitter} always
+ * running at a parallelism of 1.
+ */
+public class PinotSinkGlobalCommitter implements GlobalCommitter<PinotSinkCommittable, PinotSinkGlobalCommittable> {
+
+    private static final Logger LOG = LoggerFactory.getLogger(PinotSinkGlobalCommitter.class);
+
+    private final String pinotControllerHost;
+    private final String pinotControllerPort;
+    private final String tableName;
+    private final SegmentNameGenerator segmentNameGenerator;
+    private final String tempDirPrefix;
+    private final FileSystemAdapter fsAdapter;
+    private final String timeColumnName;
+    private final TimeUnit segmentTimeUnit;
+
+    /**
+     * @param pinotControllerHost  Host of the Pinot controller
+     * @param pinotControllerPort  Port of the Pinot controller
+     * @param tableName            Target table's name
+     * @param segmentNameGenerator Pinot segment name generator
+     * @param fsAdapter            Adapter for interacting with the shared file system
+     * @param timeColumnName       Name of the column containing the timestamp
+     * @param segmentTimeUnit      Unit of the time column
+     */
+    public PinotSinkGlobalCommitter(String pinotControllerHost, String pinotControllerPort, String tableName, SegmentNameGenerator segmentNameGenerator, String tempDirPrefix, FileSystemAdapter fsAdapter, String timeColumnName, TimeUnit segmentTimeUnit) {
+        this.pinotControllerHost = checkNotNull(pinotControllerHost);
+        this.pinotControllerPort = checkNotNull(pinotControllerPort);
+        this.tableName = checkNotNull(tableName);
+        this.segmentNameGenerator = checkNotNull(segmentNameGenerator);
+        this.tempDirPrefix = checkNotNull(tempDirPrefix);
+        this.fsAdapter = checkNotNull(fsAdapter);
+        this.timeColumnName = checkNotNull(timeColumnName);
+        this.segmentTimeUnit = checkNotNull(segmentTimeUnit);
+    }
+
+    /**
+     * Identifies global committables that need to be re-committed from a list of recovered committables.
+     *
+     * @param globalCommittables List of global committables that are checked for required re-commit
+     * @return List of global committable that need to be re-committed
+     * @throws IOException
+     */
+    @Override
+    public List<PinotSinkGlobalCommittable> filterRecoveredCommittables(List<PinotSinkGlobalCommittable> globalCommittables) throws IOException {
+        PinotControllerApi controllerApi = new PinotControllerApi(this.pinotControllerHost, this.pinotControllerPort);
+        List<PinotSinkGlobalCommittable> committablesToRetry = new ArrayList<>();
+
+        for (PinotSinkGlobalCommittable globalCommittable : globalCommittables) {
+            CommitStatus commitStatus = this.getCommitStatus(globalCommittable);
+
+            if (commitStatus.getMissingSegmentNames().isEmpty()) {
+                // All segments were already committed. Thus, we do not need to retry the commit.
+                continue;
+            }
+
+            for (String existingSegment : commitStatus.getExistingSegmentNames()) {
+                // Some but not all segments were already committed. As we cannot assure the data
+                // files containing the same data as originally when recovering from failure,
+                // we delete the already committed segments in order to recommit them later on.
+                controllerApi.deleteSegment(tableName, existingSegment);
+            }
+            committablesToRetry.add(globalCommittable);
+        }
+
+        return committablesToRetry;
+    }
+
+    /**
+     * Combines multiple {@link PinotSinkCommittable}s into one {@link PinotSinkGlobalCommittable}
+     * by finding the minimum and maximum timestamps from the provided {@link PinotSinkCommittable}s.
+     *
+     * @param committables Committables created by {@link org.apache.flink.streaming.connectors.pinot.writer.PinotSinkWriter}
+     * @return Global committer committable
+     */
+    @Override
+    public PinotSinkGlobalCommittable combine(List<PinotSinkCommittable> committables) {
+        List<String> dataFilePaths = new ArrayList<>();
+        long minTimestamp = Long.MAX_VALUE;
+        long maxTimestamp = Long.MIN_VALUE;
+
+        // Extract all data file paths and the overall minimum and maximum timestamps
+        // from all committables
+        for (PinotSinkCommittable committable : committables) {
+            dataFilePaths.add(committable.getDataFilePath());
+            minTimestamp = Long.min(minTimestamp, committable.getMinTimestamp());
+            maxTimestamp = Long.max(maxTimestamp, committable.getMaxTimestamp());
+        }
+
+        LOG.info(""Combined {} committables into one global committable"", committables.size());
+        return new PinotSinkGlobalCommittable(dataFilePaths, minTimestamp, maxTimestamp);
+    }
+
+    /**
+     * Copies data files from shared filesystem to the local filesystem, generates segments with names
+     * according to the segment naming schema and finally pushes the segments to the Pinot cluster.
+     * Before pushing a segment it is checked whether there already exists a segment with that name
+     * in the Pinot cluster by calling the Pinot controller. In case there is one, it gets deleted.
+     *
+     * @param globalCommittables List of global committables
+     * @return Global committables whose commit failed
+     * @throws IOException
+     */
+    @Override
+    public List<PinotSinkGlobalCommittable> commit(List<PinotSinkGlobalCommittable> globalCommittables) throws IOException {
+        // Retrieve the Pinot table schema and the Pinot table config from the Pinot controller
+        PinotControllerApi controllerApi = new PinotControllerApi(this.pinotControllerHost, this.pinotControllerPort);
+        Schema tableSchema = controllerApi.getSchema(this.tableName);
+        TableConfig tableConfig = controllerApi.getTableConfig(this.tableName);
+
+        // List of failed global committables that can be retried later on
+        List<PinotSinkGlobalCommittable> failedCommits = new ArrayList<>();
+
+        for (PinotSinkGlobalCommittable globalCommittable : globalCommittables) {
+            // Make sure to remove all previously committed segments in globalCommittable
+            // when recovering from failure
+            CommitStatus commitStatus = this.getCommitStatus(globalCommittable);
+            for (String existingSegment : commitStatus.getExistingSegmentNames()) {
+                // Some but not all segments were already committed. As we cannot assure the data
+                // files containing the same data as originally when recovering from failure,
+                // we delete the already committed segments in order to recommit them later on.
+                controllerApi.deleteSegment(tableName, existingSegment);
+            }
+
+            // We use a thread pool in order to parallelize the segment creation and segment upload
+            ExecutorService pool = Executors.newCachedThreadPool();
+            Set<Future<Boolean>> resultFutures = new HashSet<>();
+
+            // Commit all segments in globalCommittable
+            int sequenceId = 0;
+            for (String dataFilePath : globalCommittable.getDataFilePaths()) {
+                // Get segment names with increasing sequenceIds
+                String segmentName = this.getSegmentName(globalCommittable, sequenceId++);
+                // Segment committer handling the whole commit process for a single segment
+                Callable<Boolean> segmentCommitter = new SegmentCommitter(
+                        this.pinotControllerHost, this.pinotControllerPort, this.tempDirPrefix,
+                        this.fsAdapter, dataFilePath, segmentName, tableSchema, tableConfig,
+                        this.timeColumnName, this.segmentTimeUnit
+                );
+                // Submits the segment committer to the thread pool
+                resultFutures.add(pool.submit(segmentCommitter));
+            }
+
+            try {
+                for (Future<Boolean> wasSuccessful : resultFutures) {
+                    // In case any of the segment commits wasn't successful we mark the whole
+                    // globalCommittable as failed
+                    if (!wasSuccessful.get()) {
+                        failedCommits.add(globalCommittable);
+                        // Once any of the commits failed, we do not need to check the remaining
+                        // ones, as we try to commit the globalCommittable next time
+                        break;
+                    }
+                }
+            } catch (Exception e) {
+                // In case of an exception mark the whole globalCommittable as failed
+                failedCommits.add(globalCommittable);
+                LOG.error(e.getMessage());
+                e.printStackTrace();
+            }
+        }
+
+        // Return failed commits so that they can be retried later on
+        return failedCommits;
+    }
+
+    /**
+     * Empty method.
+     */
+    @Override
+    public void endOfInput() {
+    }
+
+    /**
+     * Empty method, as we do not open any connections.
+     */
+    @Override
+    public void close() {
+    }
+
+    /**
+     * Helper method for generating segment names using the segment name generator.
+     *
+     * @param globalCommittable Global committable the segment name shall be generated from
+     * @param sequenceId        Incrementing counter
+     * @return generated segment name
+     */
+    private String getSegmentName(PinotSinkGlobalCommittable globalCommittable, int sequenceId) {
+        return this.segmentNameGenerator.generateSegmentName(sequenceId, globalCommittable.getMinTimestamp(), globalCommittable.getMaxTimestamp());
+    }
+
+    /**
+     * Evaluates the status of already uploaded segments by requesting segment metadata from the
+     * Pinot controller.
+     *
+     * @param globalCommittable Global committable whose commit status gets evaluated
+     * @return Commit status
+     * @throws IOException
+     */
+    private CommitStatus getCommitStatus(PinotSinkGlobalCommittable globalCommittable) throws IOException {
+        PinotControllerApi controllerApi = new PinotControllerApi(this.pinotControllerHost, this.pinotControllerPort);
+
+        List<String> existingSegmentNames = new ArrayList<>();
+        List<String> missingSegmentNames = new ArrayList<>();
+
+        // For all segment names that will be used to submit new segments, check whether the segment
+        // name already exists for the target table
+        for (int sequenceId = 0; sequenceId < globalCommittable.getDataFilePaths().size(); sequenceId++) {
+            String segmentName = this.getSegmentName(globalCommittable, sequenceId);
+            if (controllerApi.tableHasSegment(this.tableName, segmentName)) {
+                // Segment name already exists
+                existingSegmentNames.add(segmentName);
+            } else {
+                // Segment name does not exist yet
+                missingSegmentNames.add(segmentName);
+            }
+        }
+
+        return new CommitStatus(existingSegmentNames, missingSegmentNames);
+    }
+
+    /**
+     * Wrapper for existing and missing segments in the Pinot cluster.
+     */
+    static class CommitStatus {
+        private final List<String> existingSegmentNames;
+        private final List<String> missingSegmentNames;
+
+        public CommitStatus(List<String> existingSegmentNames, List<String> missingSegmentNames) {
+            this.existingSegmentNames = existingSegmentNames;
+            this.missingSegmentNames = missingSegmentNames;
+        }
+
+        public List<String> getExistingSegmentNames() {
+            return existingSegmentNames;
+        }
+
+        public List<String> getMissingSegmentNames() {
+            return missingSegmentNames;
+        }
+    }
+
+    /**
+     * Helper class for committing a single segment. Downloads a data file from the shared filesystem,
+     * generates a segment from the data file and uploads segment to the Pinot controller.
+     */
+    static class SegmentCommitter implements Callable<Boolean> {
+
+        private static final Logger LOG = LoggerFactory.getLogger(SegmentCommitter.class);
+
+        final String pinotControllerHost;
+        final String pinotControllerPort;
+        final String tempDirPrefix;
+        final FileSystemAdapter fsAdapter;
+        final String dataFilePath;
+        final String segmentName;
+        final Schema tableSchema;
+        final TableConfig tableConfig;
+        final String timeColumnName;
+        final TimeUnit segmentTimeUnit;
+
+        /**
+         * @param pinotControllerHost Host of the Pinot controller
+         * @param pinotControllerPort Port of the Pinot controller
+         * @param fsAdapter           Filesystem adapter used to load data files from the shared file system
+         * @param dataFilePath        Data file to load from the shared file system
+         * @param segmentName         Name of the segment to create and commit
+         * @param tableSchema         Pinot table schema
+         * @param tableConfig         Pinot table config
+         * @param timeColumnName      Name of the column containing the timestamp
+         * @param segmentTimeUnit     Unit of the time column
+         */
+        public SegmentCommitter(String pinotControllerHost, String pinotControllerPort, String tempDirPrefix, FileSystemAdapter fsAdapter, String dataFilePath, String segmentName, Schema tableSchema, TableConfig tableConfig, String timeColumnName, TimeUnit segmentTimeUnit) {
+            this.pinotControllerHost = pinotControllerHost;
+            this.pinotControllerPort = pinotControllerPort;
+            this.tempDirPrefix = tempDirPrefix;
+            this.fsAdapter = fsAdapter;
+            this.dataFilePath = dataFilePath;
+            this.segmentName = segmentName;
+            this.tableSchema = tableSchema;
+            this.tableConfig = tableConfig;
+            this.timeColumnName = timeColumnName;
+            this.segmentTimeUnit = segmentTimeUnit;
+        }
+
+        /**
+         * Downloads a segment from the shared file system via {@code fsAdapter}, generates a segment
+         * and finally uploads the segment to the Pinot controller
+         *
+         * @return True if the commit succeeded
+         */
+        @Override
+        public Boolean call() {
+            try {
+                // Download data file from the shared filesystem
+                LOG.info(""Downloading data file {} from shared file system..."", dataFilePath);
+                File segmentData = fsAdapter.copyToLocalFile(dataFilePath);
+                LOG.info(""Successfully downloaded data file {} from shared file system"", dataFilePath);
+
+                File segmentFile = Files.createTempDirectory(this.tempDirPrefix).toFile();
+                LOG.info(""Creating segment in "" + segmentFile.getAbsolutePath());
+
+                // Creates a segment with name `segmentName` in `segmentFile`
+                this.generateSegment(segmentData, segmentFile, true);
+
+                // Uploads the recently created segment to the Pinot controller
+                this.uploadSegment(segmentFile);
+
+                // Commit successful
+                return true;
+            } catch (IOException e) {
+                e.printStackTrace();
+                LOG.error(e.getMessage());
+
+                // Commit failed
+                return false;
+            }
+        }
+
+        /**
+         * Creates a segment from the given parameters.
+         * This method was adapted from {@link org.apache.pinot.tools.admin.command.CreateSegmentCommand}.
+         *
+         * @param dataFile                  File containing the JSON data
+         * @param outDir                    Segment target path
+         * @param _postCreationVerification Verify segment after generation
+         */
+        public void generateSegment(File dataFile, File outDir, Boolean _postCreationVerification) {
+            SegmentGeneratorConfig segmentGeneratorConfig = new SegmentGeneratorConfig(tableConfig, tableSchema);
+            segmentGeneratorConfig.setSegmentName(segmentName);
+            segmentGeneratorConfig.setSegmentTimeUnit(segmentTimeUnit);
+            segmentGeneratorConfig.setTimeColumnName(timeColumnName);
+            segmentGeneratorConfig.setInputFilePath(dataFile.getPath());
+            segmentGeneratorConfig.setFormat(FileFormat.JSON);
+            segmentGeneratorConfig.setOutDir(outDir.getPath());
+            segmentGeneratorConfig.setTableName(tableConfig.getTableName());
+
+            try {
+                SegmentIndexCreationDriver driver = new SegmentIndexCreationDriverImpl();
+                driver.init(segmentGeneratorConfig);
+                driver.build();
+                File indexDir = new File(outDir, segmentName);
+                LOG.info(""Successfully created segment: {} in directory: {}"", segmentName, indexDir);
+                if (_postCreationVerification) {
+                    LOG.info(""Verifying the segment by loading it"");
+                    ImmutableSegment segment = ImmutableSegmentLoader.load(indexDir, ReadMode.mmap);
+                    LOG.info(""Successfully loaded segment: {} of size: {} bytes"", segmentName,
+                            segment.getSegmentSizeBytes());
+                    segment.destroy();
+                }
+            } catch (Exception e) {
+                e.printStackTrace();","[{'comment': 'log this, or remove', 'commenter': 'eskabetxe'}, {'comment': 'done', 'commenter': 'mschroederi'}]"
113,flink-connector-pinot/src/main/java/org/apache/flink/streaming/connectors/pinot/writer/PinotWriterSegment.java,"@@ -0,0 +1,148 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.streaming.connectors.pinot.writer;
+
+import org.apache.flink.streaming.connectors.pinot.committer.PinotSinkCommittable;
+import org.apache.flink.streaming.connectors.pinot.external.JsonSerializer;
+import org.apache.flink.streaming.connectors.pinot.filesystem.FileSystemAdapter;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.File;
+import java.io.IOException;
+import java.io.Serializable;
+import java.nio.charset.Charset;
+import java.nio.file.Files;
+import java.nio.file.Path;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.stream.Collectors;
+
+import static org.apache.flink.util.Preconditions.checkArgument;
+import static org.apache.flink.util.Preconditions.checkNotNull;
+
+/**
+ * A {@link PinotWriterSegment} represents exactly one segment that can be found in the Pinot
+ * cluster once the commit has been completed.
+ *
+ * @param <IN> Type of incoming elements
+ */
+public class PinotWriterSegment<IN> implements Serializable {
+
+    private static final Logger LOG = LoggerFactory.getLogger(""PinotWriterSegment"");
+
+    private final int maxRowsPerSegment;
+    private final String tempDirPrefix;
+    private final JsonSerializer<IN> jsonSerializer;
+    private final FileSystemAdapter fsAdapter;
+
+    private boolean acceptsElements = true;
+
+    private final List<IN> elements;
+    private File dataFile;
+    private long minTimestamp = Long.MAX_VALUE;
+    private long maxTimestamp = Long.MIN_VALUE;
+
+    /**
+     * @param maxRowsPerSegment Maximum number of rows to be stored within a Pinot segment
+     * @param tempDirPrefix     Prefix for temp directories used
+     * @param jsonSerializer    Serializer used to convert elements to JSON
+     * @param fsAdapter         Filesystem adapter used to save files for sharing files across nodes
+     */
+    protected PinotWriterSegment(int maxRowsPerSegment, String tempDirPrefix, JsonSerializer<IN> jsonSerializer, FileSystemAdapter fsAdapter) {
+        checkArgument(maxRowsPerSegment > 0L);
+        this.maxRowsPerSegment = maxRowsPerSegment;
+        this.tempDirPrefix = checkNotNull(tempDirPrefix);
+        this.jsonSerializer = checkNotNull(jsonSerializer);
+        this.fsAdapter = checkNotNull(fsAdapter);
+        this.elements = new ArrayList<>();
+    }
+
+    /**
+     * Takes elements and stores them in memory until either {@link #maxRowsPerSegment} is reached
+     * or {@link #prepareCommit} is called.
+     *
+     * @param element   Object from upstream task
+     * @param timestamp Timestamp assigned to element
+     * @throws IOException
+     */
+    public void write(IN element, long timestamp) throws IOException {
+        if (!this.acceptsElements()) {
+            throw new IllegalStateException(""This PinotSegmentWriter does not accept any elements anymore."");
+        }
+        this.elements.add(element);
+        this.minTimestamp = Long.min(this.minTimestamp, timestamp);
+        this.maxTimestamp = Long.max(this.maxTimestamp, timestamp);
+
+        // Writes elements to local filesystem once the maximum number of items is reached
+        if (this.elements.size() == this.maxRowsPerSegment) {","[{'comment': 'why we write to local?', 'commenter': 'eskabetxe'}, {'comment': ""The current version of the `FileSystemAdapter` only supports copying local files to the shared FS and vice versa which should allow maximum flexibility when it comes to extensibility.\r\nI nonetheless see your point that we can avoid unnecessary I/O here. Therefore, I'd suggest to change the `FileSystemAdapter`'s API to accept a list of serialized elements to write to the shared filesystem. Those adapters that need a local file to upload could first write to local disk before uploading and all others could directly write to the respective filesystem."", 'commenter': 'mschroederi'}]"
113,flink-connector-pinot/src/test/java/org/apache/flink/streaming/connectors/pinot/emulator/PinotEmulatorManager.java,"@@ -0,0 +1,341 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.streaming.connectors.pinot.emulator;
+
+import com.github.dockerjava.api.DockerClient;","[{'comment': 'could you use testcontainers?', 'commenter': 'eskabetxe'}, {'comment': 'Great advice, the manual testing setup was replaced by testcontainers', 'commenter': 'mschroederi'}]"
113,flink-connector-pinot/README.md,"@@ -0,0 +1,60 @@
+# Flink Pinot Connector
+
+This connector provides a source and sink to [Apache Pinot](http://pinot.apache.org/)™.  ","[{'comment': '```suggestion\r\nThis connector provides a sink to [Apache Pinot](http://pinot.apache.org/)™.  \r\n```', 'commenter': 'fapaul'}]"
113,flink-connector-pinot/README.md,"@@ -0,0 +1,60 @@
+# Flink Pinot Connector
+
+This connector provides a source and sink to [Apache Pinot](http://pinot.apache.org/)™.  
+To use this connector, add the following dependency to your project:
+
+    <dependency>
+      <groupId>org.apache.bahir</groupId>
+      <artifactId>flink-connector-pinot_2.11</artifactId>
+      <version>1.1-SNAPSHOT</version>
+    </dependency>
+
+*Version Compatibility*: This module is compatible with Pinot 0.6.0.
+
+Note that the streaming connectors are not part of the binary distribution of Flink. You need to link them into your job jar for cluster execution.","[{'comment': 'I am missing in the readme a few of the design decisions.\r\n\r\n- How does streaming/batch mode work?\r\n- At least once delivery?\r\n- Core architecture overview (Commiter, GlobalCommitter)\r\n\r\nYou can look at https://github.com/apache/bahir-flink/blob/521df6c23f0a2a788d0853a260d31caaca33f31b/flink-connector-influxdb2/README.md to get some ideas although I do think not all of that is necessary ;)', 'commenter': 'fapaul'}]"
113,flink-connector-pinot/src/main/java/org/apache/flink/streaming/connectors/pinot/PinotControllerApi.java,"@@ -0,0 +1,218 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.streaming.connectors.pinot;
+
+import org.apache.flink.streaming.connectors.pinot.exceptions.PinotControllerApiException;
+import org.apache.http.StatusLine;
+import org.apache.http.client.methods.*;
+import org.apache.http.entity.ContentType;
+import org.apache.http.entity.StringEntity;
+import org.apache.http.impl.client.CloseableHttpClient;
+import org.apache.http.impl.client.HttpClients;
+import org.apache.http.util.EntityUtils;
+import org.apache.pinot.spi.config.table.TableConfig;
+import org.apache.pinot.spi.data.Schema;
+import org.apache.pinot.spi.utils.JsonUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+
+import static org.apache.flink.util.Preconditions.checkNotNull;
+
+/**
+ * Helpers to interact with the Pinot controller via its public API.
+ */
+public class PinotControllerApi {
+
+    private static final Logger LOG = LoggerFactory.getLogger(PinotControllerApi.class);
+    protected final String controllerHostPort;
+
+    /**
+     * @param controllerHost Pinot controller's host
+     * @param controllerPort Pinot controller's port
+     */
+    public PinotControllerApi(String controllerHost, String controllerPort) {
+        checkNotNull(controllerHost);
+        checkNotNull(controllerPort);
+        this.controllerHostPort = String.format(""http://%s:%s"", controllerHost, controllerPort);
+    }
+
+    /**
+     * Issues a request to the Pinot controller API.
+     *
+     * @param request Request to issue
+     * @return Api response
+     * @throws IOException
+     */
+    private ApiResponse execute(HttpRequestBase request) throws IOException {
+        ApiResponse result;
+
+        try (CloseableHttpClient httpClient = HttpClients.createDefault();","[{'comment': 'WDYT of making the `PinotControllerApi` closeable? This way we can only instantiate one `CloseableHttpClient` in the constructor and do not have to create one for every request.', 'commenter': 'fapaul'}]"
113,flink-connector-pinot/src/main/java/org/apache/flink/streaming/connectors/pinot/PinotControllerApi.java,"@@ -0,0 +1,218 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.streaming.connectors.pinot;
+
+import org.apache.flink.streaming.connectors.pinot.exceptions.PinotControllerApiException;
+import org.apache.http.StatusLine;
+import org.apache.http.client.methods.*;
+import org.apache.http.entity.ContentType;
+import org.apache.http.entity.StringEntity;
+import org.apache.http.impl.client.CloseableHttpClient;
+import org.apache.http.impl.client.HttpClients;
+import org.apache.http.util.EntityUtils;
+import org.apache.pinot.spi.config.table.TableConfig;
+import org.apache.pinot.spi.data.Schema;
+import org.apache.pinot.spi.utils.JsonUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+
+import static org.apache.flink.util.Preconditions.checkNotNull;
+
+/**
+ * Helpers to interact with the Pinot controller via its public API.
+ */
+public class PinotControllerApi {
+
+    private static final Logger LOG = LoggerFactory.getLogger(PinotControllerApi.class);
+    protected final String controllerHostPort;
+
+    /**
+     * @param controllerHost Pinot controller's host
+     * @param controllerPort Pinot controller's port
+     */
+    public PinotControllerApi(String controllerHost, String controllerPort) {
+        checkNotNull(controllerHost);
+        checkNotNull(controllerPort);
+        this.controllerHostPort = String.format(""http://%s:%s"", controllerHost, controllerPort);
+    }
+
+    /**
+     * Issues a request to the Pinot controller API.
+     *
+     * @param request Request to issue
+     * @return Api response
+     * @throws IOException
+     */
+    private ApiResponse execute(HttpRequestBase request) throws IOException {
+        ApiResponse result;
+
+        try (CloseableHttpClient httpClient = HttpClients.createDefault();
+             CloseableHttpResponse response = httpClient.execute(request)) {
+
+            String body = EntityUtils.toString(response.getEntity());
+            result = new ApiResponse(response.getStatusLine(), body);
+        }
+
+        return result;
+    }
+
+    /**
+     * Issues a POST request to the Pinot controller API.
+     *
+     * @param path Path to POST to
+     * @param body Request's body
+     * @return API response
+     * @throws IOException
+     */
+    protected ApiResponse post(String path, String body) throws IOException {","[{'comment': 'I do not like the inheritance model here. WDYT about splitting this class into two classes? One holding the http client and exposing the `post`, `get` and `delete` method and the other class only holding production code like `tableHasSegment` etc. The basic http class is then also usable for testing and you can pass it to the `PinotTestHelper` to implement more API calls.', 'commenter': 'fapaul'}]"
113,flink-connector-pinot/src/main/java/org/apache/flink/streaming/connectors/pinot/PinotControllerApi.java,"@@ -0,0 +1,218 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.streaming.connectors.pinot;
+
+import org.apache.flink.streaming.connectors.pinot.exceptions.PinotControllerApiException;
+import org.apache.http.StatusLine;
+import org.apache.http.client.methods.*;
+import org.apache.http.entity.ContentType;
+import org.apache.http.entity.StringEntity;
+import org.apache.http.impl.client.CloseableHttpClient;
+import org.apache.http.impl.client.HttpClients;
+import org.apache.http.util.EntityUtils;
+import org.apache.pinot.spi.config.table.TableConfig;
+import org.apache.pinot.spi.data.Schema;
+import org.apache.pinot.spi.utils.JsonUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+
+import static org.apache.flink.util.Preconditions.checkNotNull;
+
+/**
+ * Helpers to interact with the Pinot controller via its public API.
+ */
+public class PinotControllerApi {
+
+    private static final Logger LOG = LoggerFactory.getLogger(PinotControllerApi.class);
+    protected final String controllerHostPort;
+
+    /**
+     * @param controllerHost Pinot controller's host
+     * @param controllerPort Pinot controller's port
+     */
+    public PinotControllerApi(String controllerHost, String controllerPort) {
+        checkNotNull(controllerHost);
+        checkNotNull(controllerPort);
+        this.controllerHostPort = String.format(""http://%s:%s"", controllerHost, controllerPort);
+    }
+
+    /**
+     * Issues a request to the Pinot controller API.
+     *
+     * @param request Request to issue
+     * @return Api response
+     * @throws IOException
+     */
+    private ApiResponse execute(HttpRequestBase request) throws IOException {
+        ApiResponse result;
+
+        try (CloseableHttpClient httpClient = HttpClients.createDefault();
+             CloseableHttpResponse response = httpClient.execute(request)) {
+
+            String body = EntityUtils.toString(response.getEntity());
+            result = new ApiResponse(response.getStatusLine(), body);
+        }
+
+        return result;
+    }
+
+    /**
+     * Issues a POST request to the Pinot controller API.
+     *
+     * @param path Path to POST to
+     * @param body Request's body
+     * @return API response
+     * @throws IOException
+     */
+    protected ApiResponse post(String path, String body) throws IOException {
+        HttpPost httppost = new HttpPost(this.controllerHostPort + path);
+        httppost.setEntity(new StringEntity(body, ContentType.APPLICATION_JSON));
+        LOG.info(""Posting string entity {} to {}"", body, path);","[{'comment': 'Should be debug log.', 'commenter': 'fapaul'}]"
113,flink-connector-pinot/src/main/java/org/apache/flink/streaming/connectors/pinot/PinotControllerApi.java,"@@ -0,0 +1,218 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.streaming.connectors.pinot;
+
+import org.apache.flink.streaming.connectors.pinot.exceptions.PinotControllerApiException;
+import org.apache.http.StatusLine;
+import org.apache.http.client.methods.*;
+import org.apache.http.entity.ContentType;
+import org.apache.http.entity.StringEntity;
+import org.apache.http.impl.client.CloseableHttpClient;
+import org.apache.http.impl.client.HttpClients;
+import org.apache.http.util.EntityUtils;
+import org.apache.pinot.spi.config.table.TableConfig;
+import org.apache.pinot.spi.data.Schema;
+import org.apache.pinot.spi.utils.JsonUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+
+import static org.apache.flink.util.Preconditions.checkNotNull;
+
+/**
+ * Helpers to interact with the Pinot controller via its public API.
+ */
+public class PinotControllerApi {
+
+    private static final Logger LOG = LoggerFactory.getLogger(PinotControllerApi.class);
+    protected final String controllerHostPort;
+
+    /**
+     * @param controllerHost Pinot controller's host
+     * @param controllerPort Pinot controller's port
+     */
+    public PinotControllerApi(String controllerHost, String controllerPort) {
+        checkNotNull(controllerHost);
+        checkNotNull(controllerPort);
+        this.controllerHostPort = String.format(""http://%s:%s"", controllerHost, controllerPort);
+    }
+
+    /**
+     * Issues a request to the Pinot controller API.
+     *
+     * @param request Request to issue
+     * @return Api response
+     * @throws IOException
+     */
+    private ApiResponse execute(HttpRequestBase request) throws IOException {
+        ApiResponse result;
+
+        try (CloseableHttpClient httpClient = HttpClients.createDefault();
+             CloseableHttpResponse response = httpClient.execute(request)) {
+
+            String body = EntityUtils.toString(response.getEntity());
+            result = new ApiResponse(response.getStatusLine(), body);
+        }
+
+        return result;
+    }
+
+    /**
+     * Issues a POST request to the Pinot controller API.
+     *
+     * @param path Path to POST to
+     * @param body Request's body
+     * @return API response
+     * @throws IOException
+     */
+    protected ApiResponse post(String path, String body) throws IOException {
+        HttpPost httppost = new HttpPost(this.controllerHostPort + path);
+        httppost.setEntity(new StringEntity(body, ContentType.APPLICATION_JSON));
+        LOG.info(""Posting string entity {} to {}"", body, path);
+        return this.execute(httppost);
+    }
+
+    /**
+     * Issues a GET request to the Pinot controller API.
+     *
+     * @param path Path to GET from
+     * @return API response
+     * @throws IOException
+     */
+    protected ApiResponse get(String path) throws IOException {
+        HttpGet httpget = new HttpGet(this.controllerHostPort + path);
+        LOG.info(""Sending GET request to {}"", path);","[{'comment': 'Should be debug log.', 'commenter': 'fapaul'}]"
113,flink-connector-pinot/src/main/java/org/apache/flink/streaming/connectors/pinot/PinotControllerApi.java,"@@ -0,0 +1,218 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.streaming.connectors.pinot;
+
+import org.apache.flink.streaming.connectors.pinot.exceptions.PinotControllerApiException;
+import org.apache.http.StatusLine;
+import org.apache.http.client.methods.*;
+import org.apache.http.entity.ContentType;
+import org.apache.http.entity.StringEntity;
+import org.apache.http.impl.client.CloseableHttpClient;
+import org.apache.http.impl.client.HttpClients;
+import org.apache.http.util.EntityUtils;
+import org.apache.pinot.spi.config.table.TableConfig;
+import org.apache.pinot.spi.data.Schema;
+import org.apache.pinot.spi.utils.JsonUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+
+import static org.apache.flink.util.Preconditions.checkNotNull;
+
+/**
+ * Helpers to interact with the Pinot controller via its public API.
+ */
+public class PinotControllerApi {
+
+    private static final Logger LOG = LoggerFactory.getLogger(PinotControllerApi.class);
+    protected final String controllerHostPort;
+
+    /**
+     * @param controllerHost Pinot controller's host
+     * @param controllerPort Pinot controller's port
+     */
+    public PinotControllerApi(String controllerHost, String controllerPort) {
+        checkNotNull(controllerHost);
+        checkNotNull(controllerPort);
+        this.controllerHostPort = String.format(""http://%s:%s"", controllerHost, controllerPort);
+    }
+
+    /**
+     * Issues a request to the Pinot controller API.
+     *
+     * @param request Request to issue
+     * @return Api response
+     * @throws IOException
+     */
+    private ApiResponse execute(HttpRequestBase request) throws IOException {
+        ApiResponse result;
+
+        try (CloseableHttpClient httpClient = HttpClients.createDefault();
+             CloseableHttpResponse response = httpClient.execute(request)) {
+
+            String body = EntityUtils.toString(response.getEntity());
+            result = new ApiResponse(response.getStatusLine(), body);
+        }
+
+        return result;
+    }
+
+    /**
+     * Issues a POST request to the Pinot controller API.
+     *
+     * @param path Path to POST to
+     * @param body Request's body
+     * @return API response
+     * @throws IOException
+     */
+    protected ApiResponse post(String path, String body) throws IOException {
+        HttpPost httppost = new HttpPost(this.controllerHostPort + path);
+        httppost.setEntity(new StringEntity(body, ContentType.APPLICATION_JSON));
+        LOG.info(""Posting string entity {} to {}"", body, path);
+        return this.execute(httppost);
+    }
+
+    /**
+     * Issues a GET request to the Pinot controller API.
+     *
+     * @param path Path to GET from
+     * @return API response
+     * @throws IOException
+     */
+    protected ApiResponse get(String path) throws IOException {
+        HttpGet httpget = new HttpGet(this.controllerHostPort + path);
+        LOG.info(""Sending GET request to {}"", path);
+        return this.execute(httpget);
+    }
+
+    /**
+     * Issues a DELETE request to the Pinot controller API.
+     *
+     * @param path Path to issue DELETE request to
+     * @return API response
+     * @throws IOException
+     */
+    protected ApiResponse delete(String path) throws IOException {
+        HttpDelete httpdelete = new HttpDelete(this.controllerHostPort + path);
+        LOG.info(""Sending DELETE request to {}"", path);","[{'comment': 'debug log', 'commenter': 'fapaul'}]"
113,flink-connector-pinot/src/main/java/org/apache/flink/streaming/connectors/pinot/PinotControllerApi.java,"@@ -0,0 +1,218 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.streaming.connectors.pinot;
+
+import org.apache.flink.streaming.connectors.pinot.exceptions.PinotControllerApiException;
+import org.apache.http.StatusLine;
+import org.apache.http.client.methods.*;
+import org.apache.http.entity.ContentType;
+import org.apache.http.entity.StringEntity;
+import org.apache.http.impl.client.CloseableHttpClient;
+import org.apache.http.impl.client.HttpClients;
+import org.apache.http.util.EntityUtils;
+import org.apache.pinot.spi.config.table.TableConfig;
+import org.apache.pinot.spi.data.Schema;
+import org.apache.pinot.spi.utils.JsonUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+
+import static org.apache.flink.util.Preconditions.checkNotNull;
+
+/**
+ * Helpers to interact with the Pinot controller via its public API.
+ */
+public class PinotControllerApi {
+
+    private static final Logger LOG = LoggerFactory.getLogger(PinotControllerApi.class);
+    protected final String controllerHostPort;
+
+    /**
+     * @param controllerHost Pinot controller's host
+     * @param controllerPort Pinot controller's port
+     */
+    public PinotControllerApi(String controllerHost, String controllerPort) {
+        checkNotNull(controllerHost);
+        checkNotNull(controllerPort);
+        this.controllerHostPort = String.format(""http://%s:%s"", controllerHost, controllerPort);
+    }
+
+    /**
+     * Issues a request to the Pinot controller API.
+     *
+     * @param request Request to issue
+     * @return Api response
+     * @throws IOException
+     */
+    private ApiResponse execute(HttpRequestBase request) throws IOException {
+        ApiResponse result;
+
+        try (CloseableHttpClient httpClient = HttpClients.createDefault();
+             CloseableHttpResponse response = httpClient.execute(request)) {
+
+            String body = EntityUtils.toString(response.getEntity());
+            result = new ApiResponse(response.getStatusLine(), body);
+        }
+
+        return result;
+    }
+
+    /**
+     * Issues a POST request to the Pinot controller API.
+     *
+     * @param path Path to POST to
+     * @param body Request's body
+     * @return API response
+     * @throws IOException
+     */
+    protected ApiResponse post(String path, String body) throws IOException {
+        HttpPost httppost = new HttpPost(this.controllerHostPort + path);
+        httppost.setEntity(new StringEntity(body, ContentType.APPLICATION_JSON));
+        LOG.info(""Posting string entity {} to {}"", body, path);
+        return this.execute(httppost);
+    }
+
+    /**
+     * Issues a GET request to the Pinot controller API.
+     *
+     * @param path Path to GET from
+     * @return API response
+     * @throws IOException
+     */
+    protected ApiResponse get(String path) throws IOException {
+        HttpGet httpget = new HttpGet(this.controllerHostPort + path);
+        LOG.info(""Sending GET request to {}"", path);
+        return this.execute(httpget);
+    }
+
+    /**
+     * Issues a DELETE request to the Pinot controller API.
+     *
+     * @param path Path to issue DELETE request to
+     * @return API response
+     * @throws IOException
+     */
+    protected ApiResponse delete(String path) throws IOException {
+        HttpDelete httpdelete = new HttpDelete(this.controllerHostPort + path);
+        LOG.info(""Sending DELETE request to {}"", path);
+        return this.execute(httpdelete);
+    }
+
+    /**
+     * Checks whether the provided segment name is registered with the given table.
+     *
+     * @param tableName   Target table's name
+     * @param segmentName Segment name to check
+     * @return True if segment with the provided name exists
+     * @throws IOException
+     */
+    public boolean tableHasSegment(String tableName, String segmentName) throws IOException {
+        ApiResponse res = this.get(String.format(""/tables/%s/%s/metadata"", tableName, segmentName));
+
+        if (res.statusLine.getStatusCode() == 200) {
+            // A segment named `segmentName` exists within the table named `tableName`
+            return true;
+        }
+        if (res.statusLine.getStatusCode() == 404) {
+            // There is no such segment named `segmentName` within the table named `tableName`
+            // (or the table named `tableName` does not exist)
+            return false;
+        }
+
+        // Received an unexpected status code
+        throw new PinotControllerApiException(res.responseBody);
+    }
+
+    /**
+     * Deletes a segment from a table.
+     *
+     * @param tableName   Target table's name
+     * @param segmentName Identifies the segment to delete
+     * @throws IOException
+     */
+    public void deleteSegment(String tableName, String segmentName) throws IOException {
+        ApiResponse res = this.delete(String.format(""/tables/%s/%s"", tableName, segmentName));
+
+        if (res.statusLine.getStatusCode() != 200) {
+            LOG.error(""Could not delete segment {} from table {}. Pinot controller returned: {}"", tableName, segmentName, res.responseBody);
+            throw new PinotControllerApiException(res.responseBody);
+        }
+    }
+
+    /**
+     * Fetches a Pinot table's schema via the Pinot controller API.
+     *
+     * @param tableName Target table's name
+     * @return Pinot table schema
+     * @throws IOException
+     */
+    public Schema getSchema(String tableName) throws IOException {
+        Schema schema;
+        ApiResponse res = this.get(String.format(""/tables/%s/schema"", tableName));
+        LOG.info(""Get schema request for table {} returned {}"", tableName, res.responseBody);
+
+        if (res.statusLine.getStatusCode() != 200) {
+            throw new PinotControllerApiException(res.responseBody);
+        }
+
+        try {
+            schema = JsonUtils.stringToObject(res.responseBody, Schema.class);
+        } catch (Exception e) {
+            throw new IllegalStateException(""Caught exception while reading schema from Pinot Controller's response: "" + res.responseBody, e);
+        }
+        LOG.info(""Retrieved schema: {}"", schema.toSingleLineJsonString());","[{'comment': 'debug log', 'commenter': 'fapaul'}]"
113,flink-connector-pinot/src/main/java/org/apache/flink/streaming/connectors/pinot/PinotControllerApi.java,"@@ -0,0 +1,218 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.streaming.connectors.pinot;
+
+import org.apache.flink.streaming.connectors.pinot.exceptions.PinotControllerApiException;
+import org.apache.http.StatusLine;
+import org.apache.http.client.methods.*;
+import org.apache.http.entity.ContentType;
+import org.apache.http.entity.StringEntity;
+import org.apache.http.impl.client.CloseableHttpClient;
+import org.apache.http.impl.client.HttpClients;
+import org.apache.http.util.EntityUtils;
+import org.apache.pinot.spi.config.table.TableConfig;
+import org.apache.pinot.spi.data.Schema;
+import org.apache.pinot.spi.utils.JsonUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+
+import static org.apache.flink.util.Preconditions.checkNotNull;
+
+/**
+ * Helpers to interact with the Pinot controller via its public API.
+ */
+public class PinotControllerApi {
+
+    private static final Logger LOG = LoggerFactory.getLogger(PinotControllerApi.class);
+    protected final String controllerHostPort;
+
+    /**
+     * @param controllerHost Pinot controller's host
+     * @param controllerPort Pinot controller's port
+     */
+    public PinotControllerApi(String controllerHost, String controllerPort) {
+        checkNotNull(controllerHost);
+        checkNotNull(controllerPort);
+        this.controllerHostPort = String.format(""http://%s:%s"", controllerHost, controllerPort);
+    }
+
+    /**
+     * Issues a request to the Pinot controller API.
+     *
+     * @param request Request to issue
+     * @return Api response
+     * @throws IOException
+     */
+    private ApiResponse execute(HttpRequestBase request) throws IOException {
+        ApiResponse result;
+
+        try (CloseableHttpClient httpClient = HttpClients.createDefault();
+             CloseableHttpResponse response = httpClient.execute(request)) {
+
+            String body = EntityUtils.toString(response.getEntity());
+            result = new ApiResponse(response.getStatusLine(), body);
+        }
+
+        return result;
+    }
+
+    /**
+     * Issues a POST request to the Pinot controller API.
+     *
+     * @param path Path to POST to
+     * @param body Request's body
+     * @return API response
+     * @throws IOException
+     */
+    protected ApiResponse post(String path, String body) throws IOException {
+        HttpPost httppost = new HttpPost(this.controllerHostPort + path);
+        httppost.setEntity(new StringEntity(body, ContentType.APPLICATION_JSON));
+        LOG.info(""Posting string entity {} to {}"", body, path);
+        return this.execute(httppost);
+    }
+
+    /**
+     * Issues a GET request to the Pinot controller API.
+     *
+     * @param path Path to GET from
+     * @return API response
+     * @throws IOException
+     */
+    protected ApiResponse get(String path) throws IOException {
+        HttpGet httpget = new HttpGet(this.controllerHostPort + path);
+        LOG.info(""Sending GET request to {}"", path);
+        return this.execute(httpget);
+    }
+
+    /**
+     * Issues a DELETE request to the Pinot controller API.
+     *
+     * @param path Path to issue DELETE request to
+     * @return API response
+     * @throws IOException
+     */
+    protected ApiResponse delete(String path) throws IOException {
+        HttpDelete httpdelete = new HttpDelete(this.controllerHostPort + path);
+        LOG.info(""Sending DELETE request to {}"", path);
+        return this.execute(httpdelete);
+    }
+
+    /**
+     * Checks whether the provided segment name is registered with the given table.
+     *
+     * @param tableName   Target table's name
+     * @param segmentName Segment name to check
+     * @return True if segment with the provided name exists
+     * @throws IOException
+     */
+    public boolean tableHasSegment(String tableName, String segmentName) throws IOException {
+        ApiResponse res = this.get(String.format(""/tables/%s/%s/metadata"", tableName, segmentName));
+
+        if (res.statusLine.getStatusCode() == 200) {
+            // A segment named `segmentName` exists within the table named `tableName`
+            return true;
+        }
+        if (res.statusLine.getStatusCode() == 404) {
+            // There is no such segment named `segmentName` within the table named `tableName`
+            // (or the table named `tableName` does not exist)
+            return false;
+        }
+
+        // Received an unexpected status code
+        throw new PinotControllerApiException(res.responseBody);
+    }
+
+    /**
+     * Deletes a segment from a table.
+     *
+     * @param tableName   Target table's name
+     * @param segmentName Identifies the segment to delete
+     * @throws IOException
+     */
+    public void deleteSegment(String tableName, String segmentName) throws IOException {
+        ApiResponse res = this.delete(String.format(""/tables/%s/%s"", tableName, segmentName));
+
+        if (res.statusLine.getStatusCode() != 200) {
+            LOG.error(""Could not delete segment {} from table {}. Pinot controller returned: {}"", tableName, segmentName, res.responseBody);
+            throw new PinotControllerApiException(res.responseBody);
+        }
+    }
+
+    /**
+     * Fetches a Pinot table's schema via the Pinot controller API.
+     *
+     * @param tableName Target table's name
+     * @return Pinot table schema
+     * @throws IOException
+     */
+    public Schema getSchema(String tableName) throws IOException {
+        Schema schema;
+        ApiResponse res = this.get(String.format(""/tables/%s/schema"", tableName));
+        LOG.info(""Get schema request for table {} returned {}"", tableName, res.responseBody);
+
+        if (res.statusLine.getStatusCode() != 200) {
+            throw new PinotControllerApiException(res.responseBody);
+        }
+
+        try {
+            schema = JsonUtils.stringToObject(res.responseBody, Schema.class);
+        } catch (Exception e) {
+            throw new IllegalStateException(""Caught exception while reading schema from Pinot Controller's response: "" + res.responseBody, e);
+        }
+        LOG.info(""Retrieved schema: {}"", schema.toSingleLineJsonString());
+        return schema;
+    }
+
+    /**
+     * Fetches a Pinot table's configuration via the Pinot controller API.
+     *
+     * @param tableName Target table's name
+     * @return Pinot table configuration
+     * @throws IOException
+     */
+    public TableConfig getTableConfig(String tableName) throws IOException {
+        TableConfig tableConfig;
+        ApiResponse res = this.get(String.format(""/tables/%s"", tableName));
+        LOG.info(""Get table config request for table {} returned {}"", tableName, res.responseBody);
+
+        try {
+            String tableConfigAsJson = JsonUtils.stringToJsonNode(res.responseBody).get(""OFFLINE"").toString();
+            tableConfig = JsonUtils.stringToObject(tableConfigAsJson, TableConfig.class);
+        } catch (Exception e) {
+            throw new IllegalStateException(""Caught exception while reading table config from Pinot Controller's response: "" + res.responseBody, e);","[{'comment': 'Why is this not a `PinotControllerApiException`?', 'commenter': 'fapaul'}]"
113,flink-connector-pinot/src/main/java/org/apache/flink/streaming/connectors/pinot/PinotControllerApi.java,"@@ -0,0 +1,218 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.streaming.connectors.pinot;
+
+import org.apache.flink.streaming.connectors.pinot.exceptions.PinotControllerApiException;
+import org.apache.http.StatusLine;
+import org.apache.http.client.methods.*;
+import org.apache.http.entity.ContentType;
+import org.apache.http.entity.StringEntity;
+import org.apache.http.impl.client.CloseableHttpClient;
+import org.apache.http.impl.client.HttpClients;
+import org.apache.http.util.EntityUtils;
+import org.apache.pinot.spi.config.table.TableConfig;
+import org.apache.pinot.spi.data.Schema;
+import org.apache.pinot.spi.utils.JsonUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+
+import static org.apache.flink.util.Preconditions.checkNotNull;
+
+/**
+ * Helpers to interact with the Pinot controller via its public API.
+ */
+public class PinotControllerApi {
+
+    private static final Logger LOG = LoggerFactory.getLogger(PinotControllerApi.class);
+    protected final String controllerHostPort;
+
+    /**
+     * @param controllerHost Pinot controller's host
+     * @param controllerPort Pinot controller's port
+     */
+    public PinotControllerApi(String controllerHost, String controllerPort) {
+        checkNotNull(controllerHost);
+        checkNotNull(controllerPort);
+        this.controllerHostPort = String.format(""http://%s:%s"", controllerHost, controllerPort);
+    }
+
+    /**
+     * Issues a request to the Pinot controller API.
+     *
+     * @param request Request to issue
+     * @return Api response
+     * @throws IOException
+     */
+    private ApiResponse execute(HttpRequestBase request) throws IOException {
+        ApiResponse result;
+
+        try (CloseableHttpClient httpClient = HttpClients.createDefault();
+             CloseableHttpResponse response = httpClient.execute(request)) {
+
+            String body = EntityUtils.toString(response.getEntity());
+            result = new ApiResponse(response.getStatusLine(), body);
+        }
+
+        return result;
+    }
+
+    /**
+     * Issues a POST request to the Pinot controller API.
+     *
+     * @param path Path to POST to
+     * @param body Request's body
+     * @return API response
+     * @throws IOException
+     */
+    protected ApiResponse post(String path, String body) throws IOException {
+        HttpPost httppost = new HttpPost(this.controllerHostPort + path);
+        httppost.setEntity(new StringEntity(body, ContentType.APPLICATION_JSON));
+        LOG.info(""Posting string entity {} to {}"", body, path);
+        return this.execute(httppost);
+    }
+
+    /**
+     * Issues a GET request to the Pinot controller API.
+     *
+     * @param path Path to GET from
+     * @return API response
+     * @throws IOException
+     */
+    protected ApiResponse get(String path) throws IOException {
+        HttpGet httpget = new HttpGet(this.controllerHostPort + path);
+        LOG.info(""Sending GET request to {}"", path);
+        return this.execute(httpget);
+    }
+
+    /**
+     * Issues a DELETE request to the Pinot controller API.
+     *
+     * @param path Path to issue DELETE request to
+     * @return API response
+     * @throws IOException
+     */
+    protected ApiResponse delete(String path) throws IOException {
+        HttpDelete httpdelete = new HttpDelete(this.controllerHostPort + path);
+        LOG.info(""Sending DELETE request to {}"", path);
+        return this.execute(httpdelete);
+    }
+
+    /**
+     * Checks whether the provided segment name is registered with the given table.
+     *
+     * @param tableName   Target table's name
+     * @param segmentName Segment name to check
+     * @return True if segment with the provided name exists
+     * @throws IOException
+     */
+    public boolean tableHasSegment(String tableName, String segmentName) throws IOException {
+        ApiResponse res = this.get(String.format(""/tables/%s/%s/metadata"", tableName, segmentName));
+
+        if (res.statusLine.getStatusCode() == 200) {
+            // A segment named `segmentName` exists within the table named `tableName`
+            return true;
+        }
+        if (res.statusLine.getStatusCode() == 404) {
+            // There is no such segment named `segmentName` within the table named `tableName`
+            // (or the table named `tableName` does not exist)
+            return false;
+        }
+
+        // Received an unexpected status code
+        throw new PinotControllerApiException(res.responseBody);
+    }
+
+    /**
+     * Deletes a segment from a table.
+     *
+     * @param tableName   Target table's name
+     * @param segmentName Identifies the segment to delete
+     * @throws IOException
+     */
+    public void deleteSegment(String tableName, String segmentName) throws IOException {
+        ApiResponse res = this.delete(String.format(""/tables/%s/%s"", tableName, segmentName));
+
+        if (res.statusLine.getStatusCode() != 200) {
+            LOG.error(""Could not delete segment {} from table {}. Pinot controller returned: {}"", tableName, segmentName, res.responseBody);
+            throw new PinotControllerApiException(res.responseBody);
+        }
+    }
+
+    /**
+     * Fetches a Pinot table's schema via the Pinot controller API.
+     *
+     * @param tableName Target table's name
+     * @return Pinot table schema
+     * @throws IOException
+     */
+    public Schema getSchema(String tableName) throws IOException {
+        Schema schema;
+        ApiResponse res = this.get(String.format(""/tables/%s/schema"", tableName));
+        LOG.info(""Get schema request for table {} returned {}"", tableName, res.responseBody);
+
+        if (res.statusLine.getStatusCode() != 200) {
+            throw new PinotControllerApiException(res.responseBody);
+        }
+
+        try {
+            schema = JsonUtils.stringToObject(res.responseBody, Schema.class);
+        } catch (Exception e) {
+            throw new IllegalStateException(""Caught exception while reading schema from Pinot Controller's response: "" + res.responseBody, e);
+        }
+        LOG.info(""Retrieved schema: {}"", schema.toSingleLineJsonString());
+        return schema;
+    }
+
+    /**
+     * Fetches a Pinot table's configuration via the Pinot controller API.
+     *
+     * @param tableName Target table's name
+     * @return Pinot table configuration
+     * @throws IOException
+     */
+    public TableConfig getTableConfig(String tableName) throws IOException {
+        TableConfig tableConfig;
+        ApiResponse res = this.get(String.format(""/tables/%s"", tableName));
+        LOG.info(""Get table config request for table {} returned {}"", tableName, res.responseBody);","[{'comment': 'debug log', 'commenter': 'fapaul'}]"
113,flink-connector-pinot/src/main/java/org/apache/flink/streaming/connectors/pinot/PinotControllerApi.java,"@@ -0,0 +1,218 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.streaming.connectors.pinot;
+
+import org.apache.flink.streaming.connectors.pinot.exceptions.PinotControllerApiException;
+import org.apache.http.StatusLine;
+import org.apache.http.client.methods.*;
+import org.apache.http.entity.ContentType;
+import org.apache.http.entity.StringEntity;
+import org.apache.http.impl.client.CloseableHttpClient;
+import org.apache.http.impl.client.HttpClients;
+import org.apache.http.util.EntityUtils;
+import org.apache.pinot.spi.config.table.TableConfig;
+import org.apache.pinot.spi.data.Schema;
+import org.apache.pinot.spi.utils.JsonUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+
+import static org.apache.flink.util.Preconditions.checkNotNull;
+
+/**
+ * Helpers to interact with the Pinot controller via its public API.
+ */
+public class PinotControllerApi {
+
+    private static final Logger LOG = LoggerFactory.getLogger(PinotControllerApi.class);
+    protected final String controllerHostPort;
+
+    /**
+     * @param controllerHost Pinot controller's host
+     * @param controllerPort Pinot controller's port
+     */
+    public PinotControllerApi(String controllerHost, String controllerPort) {
+        checkNotNull(controllerHost);
+        checkNotNull(controllerPort);
+        this.controllerHostPort = String.format(""http://%s:%s"", controllerHost, controllerPort);
+    }
+
+    /**
+     * Issues a request to the Pinot controller API.
+     *
+     * @param request Request to issue
+     * @return Api response
+     * @throws IOException
+     */
+    private ApiResponse execute(HttpRequestBase request) throws IOException {
+        ApiResponse result;
+
+        try (CloseableHttpClient httpClient = HttpClients.createDefault();
+             CloseableHttpResponse response = httpClient.execute(request)) {
+
+            String body = EntityUtils.toString(response.getEntity());
+            result = new ApiResponse(response.getStatusLine(), body);
+        }
+
+        return result;
+    }
+
+    /**
+     * Issues a POST request to the Pinot controller API.
+     *
+     * @param path Path to POST to
+     * @param body Request's body
+     * @return API response
+     * @throws IOException
+     */
+    protected ApiResponse post(String path, String body) throws IOException {
+        HttpPost httppost = new HttpPost(this.controllerHostPort + path);
+        httppost.setEntity(new StringEntity(body, ContentType.APPLICATION_JSON));
+        LOG.info(""Posting string entity {} to {}"", body, path);
+        return this.execute(httppost);
+    }
+
+    /**
+     * Issues a GET request to the Pinot controller API.
+     *
+     * @param path Path to GET from
+     * @return API response
+     * @throws IOException
+     */
+    protected ApiResponse get(String path) throws IOException {
+        HttpGet httpget = new HttpGet(this.controllerHostPort + path);
+        LOG.info(""Sending GET request to {}"", path);
+        return this.execute(httpget);
+    }
+
+    /**
+     * Issues a DELETE request to the Pinot controller API.
+     *
+     * @param path Path to issue DELETE request to
+     * @return API response
+     * @throws IOException
+     */
+    protected ApiResponse delete(String path) throws IOException {
+        HttpDelete httpdelete = new HttpDelete(this.controllerHostPort + path);
+        LOG.info(""Sending DELETE request to {}"", path);
+        return this.execute(httpdelete);
+    }
+
+    /**
+     * Checks whether the provided segment name is registered with the given table.
+     *
+     * @param tableName   Target table's name
+     * @param segmentName Segment name to check
+     * @return True if segment with the provided name exists
+     * @throws IOException
+     */
+    public boolean tableHasSegment(String tableName, String segmentName) throws IOException {
+        ApiResponse res = this.get(String.format(""/tables/%s/%s/metadata"", tableName, segmentName));
+
+        if (res.statusLine.getStatusCode() == 200) {
+            // A segment named `segmentName` exists within the table named `tableName`
+            return true;
+        }
+        if (res.statusLine.getStatusCode() == 404) {
+            // There is no such segment named `segmentName` within the table named `tableName`
+            // (or the table named `tableName` does not exist)
+            return false;
+        }
+
+        // Received an unexpected status code
+        throw new PinotControllerApiException(res.responseBody);
+    }
+
+    /**
+     * Deletes a segment from a table.
+     *
+     * @param tableName   Target table's name
+     * @param segmentName Identifies the segment to delete
+     * @throws IOException
+     */
+    public void deleteSegment(String tableName, String segmentName) throws IOException {
+        ApiResponse res = this.delete(String.format(""/tables/%s/%s"", tableName, segmentName));
+
+        if (res.statusLine.getStatusCode() != 200) {
+            LOG.error(""Could not delete segment {} from table {}. Pinot controller returned: {}"", tableName, segmentName, res.responseBody);
+            throw new PinotControllerApiException(res.responseBody);
+        }
+    }
+
+    /**
+     * Fetches a Pinot table's schema via the Pinot controller API.
+     *
+     * @param tableName Target table's name
+     * @return Pinot table schema
+     * @throws IOException
+     */
+    public Schema getSchema(String tableName) throws IOException {
+        Schema schema;
+        ApiResponse res = this.get(String.format(""/tables/%s/schema"", tableName));
+        LOG.info(""Get schema request for table {} returned {}"", tableName, res.responseBody);
+
+        if (res.statusLine.getStatusCode() != 200) {
+            throw new PinotControllerApiException(res.responseBody);
+        }
+
+        try {
+            schema = JsonUtils.stringToObject(res.responseBody, Schema.class);
+        } catch (Exception e) {
+            throw new IllegalStateException(""Caught exception while reading schema from Pinot Controller's response: "" + res.responseBody, e);
+        }
+        LOG.info(""Retrieved schema: {}"", schema.toSingleLineJsonString());
+        return schema;
+    }
+
+    /**
+     * Fetches a Pinot table's configuration via the Pinot controller API.
+     *
+     * @param tableName Target table's name
+     * @return Pinot table configuration
+     * @throws IOException
+     */
+    public TableConfig getTableConfig(String tableName) throws IOException {
+        TableConfig tableConfig;
+        ApiResponse res = this.get(String.format(""/tables/%s"", tableName));
+        LOG.info(""Get table config request for table {} returned {}"", tableName, res.responseBody);
+
+        try {
+            String tableConfigAsJson = JsonUtils.stringToJsonNode(res.responseBody).get(""OFFLINE"").toString();
+            tableConfig = JsonUtils.stringToObject(tableConfigAsJson, TableConfig.class);
+        } catch (Exception e) {
+            throw new IllegalStateException(""Caught exception while reading table config from Pinot Controller's response: "" + res.responseBody, e);
+        }
+        LOG.info(""Retrieved table config: {}"", tableConfig.toJsonString());","[{'comment': 'debug log', 'commenter': 'fapaul'}]"
113,flink-connector-pinot/src/main/java/org/apache/flink/streaming/connectors/pinot/committer/PinotSinkGlobalCommitter.java,"@@ -0,0 +1,426 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.streaming.connectors.pinot.committer;
+
+import org.apache.flink.api.connector.sink.GlobalCommitter;
+import org.apache.flink.streaming.connectors.pinot.PinotControllerApi;
+import org.apache.flink.streaming.connectors.pinot.filesystem.FileSystemAdapter;
+import org.apache.pinot.common.segment.ReadMode;
+import org.apache.pinot.core.indexsegment.generator.SegmentGeneratorConfig;
+import org.apache.pinot.core.indexsegment.immutable.ImmutableSegment;
+import org.apache.pinot.core.indexsegment.immutable.ImmutableSegmentLoader;
+import org.apache.pinot.core.segment.creator.SegmentIndexCreationDriver;
+import org.apache.pinot.core.segment.creator.impl.SegmentIndexCreationDriverImpl;
+import org.apache.pinot.core.segment.name.SegmentNameGenerator;
+import org.apache.pinot.spi.config.table.TableConfig;
+import org.apache.pinot.spi.data.Schema;
+import org.apache.pinot.spi.data.readers.FileFormat;
+import org.apache.pinot.tools.admin.command.UploadSegmentCommand;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.File;
+import java.io.IOException;
+import java.nio.file.Files;
+import java.util.ArrayList;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Set;
+import java.util.concurrent.*;
+
+import static org.apache.flink.util.Preconditions.checkNotNull;
+
+/**
+ * Global committer takes committables from {@link org.apache.flink.streaming.connectors.pinot.writer.PinotSinkWriter},
+ * generates segments and pushed them to the Pinot controller.
+ * Note: We use a custom multithreading approach to parallelize the segment creation and upload to
+ * overcome the performance limitations resulting from using a {@link GlobalCommitter} always
+ * running at a parallelism of 1.
+ */
+public class PinotSinkGlobalCommitter implements GlobalCommitter<PinotSinkCommittable, PinotSinkGlobalCommittable> {
+
+    private static final Logger LOG = LoggerFactory.getLogger(PinotSinkGlobalCommitter.class);
+
+    private final String pinotControllerHost;
+    private final String pinotControllerPort;
+    private final String tableName;
+    private final SegmentNameGenerator segmentNameGenerator;
+    private final String tempDirPrefix;
+    private final FileSystemAdapter fsAdapter;
+    private final String timeColumnName;
+    private final TimeUnit segmentTimeUnit;
+
+    /**
+     * @param pinotControllerHost  Host of the Pinot controller
+     * @param pinotControllerPort  Port of the Pinot controller
+     * @param tableName            Target table's name
+     * @param segmentNameGenerator Pinot segment name generator
+     * @param fsAdapter            Adapter for interacting with the shared file system
+     * @param timeColumnName       Name of the column containing the timestamp
+     * @param segmentTimeUnit      Unit of the time column
+     */
+    public PinotSinkGlobalCommitter(String pinotControllerHost, String pinotControllerPort, String tableName, SegmentNameGenerator segmentNameGenerator, String tempDirPrefix, FileSystemAdapter fsAdapter, String timeColumnName, TimeUnit segmentTimeUnit) {
+        this.pinotControllerHost = checkNotNull(pinotControllerHost);
+        this.pinotControllerPort = checkNotNull(pinotControllerPort);
+        this.tableName = checkNotNull(tableName);
+        this.segmentNameGenerator = checkNotNull(segmentNameGenerator);
+        this.tempDirPrefix = checkNotNull(tempDirPrefix);
+        this.fsAdapter = checkNotNull(fsAdapter);
+        this.timeColumnName = checkNotNull(timeColumnName);
+        this.segmentTimeUnit = checkNotNull(segmentTimeUnit);
+    }
+
+    /**
+     * Identifies global committables that need to be re-committed from a list of recovered committables.
+     *
+     * @param globalCommittables List of global committables that are checked for required re-commit
+     * @return List of global committable that need to be re-committed
+     * @throws IOException
+     */
+    @Override
+    public List<PinotSinkGlobalCommittable> filterRecoveredCommittables(List<PinotSinkGlobalCommittable> globalCommittables) throws IOException {
+        PinotControllerApi controllerApi = new PinotControllerApi(this.pinotControllerHost, this.pinotControllerPort);","[{'comment': 'We should instantiate the controller API in the constructor.', 'commenter': 'fapaul'}, {'comment': 'We should instantiate the controller API in the constructor.', 'commenter': 'fapaul'}, {'comment': 'We should instantiate the controller API in the constructor.', 'commenter': 'fapaul'}, {'comment': 'You should instantiate the controller API in the constructor.', 'commenter': 'fapaul'}]"
113,flink-connector-pinot/src/main/java/org/apache/flink/streaming/connectors/pinot/committer/PinotSinkGlobalCommitter.java,"@@ -0,0 +1,426 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.streaming.connectors.pinot.committer;
+
+import org.apache.flink.api.connector.sink.GlobalCommitter;
+import org.apache.flink.streaming.connectors.pinot.PinotControllerApi;
+import org.apache.flink.streaming.connectors.pinot.filesystem.FileSystemAdapter;
+import org.apache.pinot.common.segment.ReadMode;
+import org.apache.pinot.core.indexsegment.generator.SegmentGeneratorConfig;
+import org.apache.pinot.core.indexsegment.immutable.ImmutableSegment;
+import org.apache.pinot.core.indexsegment.immutable.ImmutableSegmentLoader;
+import org.apache.pinot.core.segment.creator.SegmentIndexCreationDriver;
+import org.apache.pinot.core.segment.creator.impl.SegmentIndexCreationDriverImpl;
+import org.apache.pinot.core.segment.name.SegmentNameGenerator;
+import org.apache.pinot.spi.config.table.TableConfig;
+import org.apache.pinot.spi.data.Schema;
+import org.apache.pinot.spi.data.readers.FileFormat;
+import org.apache.pinot.tools.admin.command.UploadSegmentCommand;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.File;
+import java.io.IOException;
+import java.nio.file.Files;
+import java.util.ArrayList;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Set;
+import java.util.concurrent.*;
+
+import static org.apache.flink.util.Preconditions.checkNotNull;
+
+/**
+ * Global committer takes committables from {@link org.apache.flink.streaming.connectors.pinot.writer.PinotSinkWriter},
+ * generates segments and pushed them to the Pinot controller.
+ * Note: We use a custom multithreading approach to parallelize the segment creation and upload to
+ * overcome the performance limitations resulting from using a {@link GlobalCommitter} always
+ * running at a parallelism of 1.
+ */
+public class PinotSinkGlobalCommitter implements GlobalCommitter<PinotSinkCommittable, PinotSinkGlobalCommittable> {
+
+    private static final Logger LOG = LoggerFactory.getLogger(PinotSinkGlobalCommitter.class);
+
+    private final String pinotControllerHost;
+    private final String pinotControllerPort;
+    private final String tableName;
+    private final SegmentNameGenerator segmentNameGenerator;
+    private final String tempDirPrefix;
+    private final FileSystemAdapter fsAdapter;
+    private final String timeColumnName;
+    private final TimeUnit segmentTimeUnit;
+
+    /**
+     * @param pinotControllerHost  Host of the Pinot controller
+     * @param pinotControllerPort  Port of the Pinot controller
+     * @param tableName            Target table's name
+     * @param segmentNameGenerator Pinot segment name generator
+     * @param fsAdapter            Adapter for interacting with the shared file system
+     * @param timeColumnName       Name of the column containing the timestamp
+     * @param segmentTimeUnit      Unit of the time column
+     */
+    public PinotSinkGlobalCommitter(String pinotControllerHost, String pinotControllerPort, String tableName, SegmentNameGenerator segmentNameGenerator, String tempDirPrefix, FileSystemAdapter fsAdapter, String timeColumnName, TimeUnit segmentTimeUnit) {
+        this.pinotControllerHost = checkNotNull(pinotControllerHost);
+        this.pinotControllerPort = checkNotNull(pinotControllerPort);
+        this.tableName = checkNotNull(tableName);
+        this.segmentNameGenerator = checkNotNull(segmentNameGenerator);
+        this.tempDirPrefix = checkNotNull(tempDirPrefix);
+        this.fsAdapter = checkNotNull(fsAdapter);
+        this.timeColumnName = checkNotNull(timeColumnName);
+        this.segmentTimeUnit = checkNotNull(segmentTimeUnit);
+    }
+
+    /**
+     * Identifies global committables that need to be re-committed from a list of recovered committables.
+     *
+     * @param globalCommittables List of global committables that are checked for required re-commit
+     * @return List of global committable that need to be re-committed
+     * @throws IOException
+     */
+    @Override
+    public List<PinotSinkGlobalCommittable> filterRecoveredCommittables(List<PinotSinkGlobalCommittable> globalCommittables) throws IOException {
+        PinotControllerApi controllerApi = new PinotControllerApi(this.pinotControllerHost, this.pinotControllerPort);
+        List<PinotSinkGlobalCommittable> committablesToRetry = new ArrayList<>();
+
+        for (PinotSinkGlobalCommittable globalCommittable : globalCommittables) {
+            CommitStatus commitStatus = this.getCommitStatus(globalCommittable);
+
+            if (commitStatus.getMissingSegmentNames().isEmpty()) {
+                // All segments were already committed. Thus, we do not need to retry the commit.
+                continue;
+            }
+
+            for (String existingSegment : commitStatus.getExistingSegmentNames()) {
+                // Some but not all segments were already committed. As we cannot assure the data
+                // files containing the same data as originally when recovering from failure,
+                // we delete the already committed segments in order to recommit them later on.
+                controllerApi.deleteSegment(tableName, existingSegment);
+            }
+            committablesToRetry.add(globalCommittable);
+        }
+
+        return committablesToRetry;
+    }
+
+    /**
+     * Combines multiple {@link PinotSinkCommittable}s into one {@link PinotSinkGlobalCommittable}
+     * by finding the minimum and maximum timestamps from the provided {@link PinotSinkCommittable}s.
+     *
+     * @param committables Committables created by {@link org.apache.flink.streaming.connectors.pinot.writer.PinotSinkWriter}
+     * @return Global committer committable
+     */
+    @Override
+    public PinotSinkGlobalCommittable combine(List<PinotSinkCommittable> committables) {
+        List<String> dataFilePaths = new ArrayList<>();
+        long minTimestamp = Long.MAX_VALUE;
+        long maxTimestamp = Long.MIN_VALUE;
+
+        // Extract all data file paths and the overall minimum and maximum timestamps
+        // from all committables
+        for (PinotSinkCommittable committable : committables) {
+            dataFilePaths.add(committable.getDataFilePath());
+            minTimestamp = Long.min(minTimestamp, committable.getMinTimestamp());
+            maxTimestamp = Long.max(maxTimestamp, committable.getMaxTimestamp());
+        }
+
+        LOG.info(""Combined {} committables into one global committable"", committables.size());","[{'comment': 'debug log', 'commenter': 'fapaul'}]"
114,flink-connector-influxdb2/src/main/java/org/apache/flink/streaming/connectors/influxdb/sink/commiter/InfluxDBCommitter.java,"@@ -0,0 +1,95 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements. See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership. The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License. You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.flink.streaming.connectors.influxdb.sink.commiter;
+
+import static org.apache.flink.streaming.connectors.influxdb.sink.InfluxDBSinkOptions.getInfluxDBClient;
+import static org.apache.flink.streaming.connectors.influxdb.sink.InfluxDBSinkOptions.writeDataPointCheckpoint;
+
+import com.influxdb.client.InfluxDBClient;
+import com.influxdb.client.WriteApi;
+import com.influxdb.client.domain.WritePrecision;
+import com.influxdb.client.write.Point;
+import java.util.Collections;
+import java.util.List;
+import java.util.Optional;
+import java.util.Properties;
+import lombok.SneakyThrows;
+import lombok.extern.slf4j.Slf4j;
+import org.apache.flink.api.connector.sink.Committer;
+
+/**
+ * The InfluxDBCommitter implements the {@link Committer} interface The InfluxDBCommitter is called
+ * whenever a checkpoint is set by Flink. When this class is called it writes a checkpoint data
+ * point in InfluxDB. The checkpoint data point uses the latest written record timestamp.
+ */
+@Slf4j
+public final class InfluxDBCommitter implements Committer<Long> {
+
+    private final InfluxDBClient influxDBClient;
+    private final boolean writeCheckpoint;
+
+    public InfluxDBCommitter(final Properties properties) {
+        this.influxDBClient = getInfluxDBClient(properties);
+        this.writeCheckpoint = writeDataPointCheckpoint(properties);
+    }
+
+    /**
+     * This method is called only when a checkpoint is set and writes a checkpoint data point into
+     * InfluxDB. The {@link
+     * org.apache.flink.streaming.connectors.influxdb.sink.writer.InfluxDBWriter} prepares the
+     * commit and fills the commitable list with the latest timestamp. If the list contains a single
+     * element it will be used as the timestamp of the datapoint. Otherwise when no timestamp is
+     * provided, InfluxDB will use the current timestamp (UTC) of the host machine.
+     *
+     * <p>
+     *
+     * @param committables Contains the latest written timestamp.
+     * @return Empty list
+     * @see <a
+     *     href=https://docs.influxdata.com/influxdb/v2.0/reference/syntax/line-protocol/#timestamp></a>
+     */
+    @SneakyThrows
+    @Override
+    public List<Long> commit(final List<Long> committables) {
+        if (this.writeCheckpoint) {
+            log.debug(""A checkpoint is set."");
+            Optional<Long> lastTimestamp = Optional.empty();
+            if (committables.size() >= 1) {
+                lastTimestamp = Optional.ofNullable(committables.get(committables.size() - 1));
+            }
+            this.writeCheckpointDataPoint(lastTimestamp);
+        }
+        return Collections.emptyList();
+    }
+
+    @Override
+    public void close() {
+        this.influxDBClient.close();
+        log.debug(""Closing the committer."");
+    }
+
+    private void writeCheckpointDataPoint(final Optional<Long> timestamp) {
+        try (final WriteApi writeApi = this.influxDBClient.getWriteApi()) {
+            final Point point = new Point(""checkpoint"");
+            point.addField(""checkpoint"", ""flink"");
+            timestamp.ifPresent(aTime -> point.time(aTime, WritePrecision.NS));
+            writeApi.writePoint(point);
+            log.debug(""Checkpoint data point write at {}"", point.toLineProtocol());
+        }
+    }","[{'comment': ""@Aheise here we are opening a connection to InfluxDB and use the try with resources statement since the `WriteApi` interface extends the `AutoCloseable`. The statement doesn't catch any exceptions yet. We were wondering how to catch the exceptions and how to handle them correctly?\r\nHere is a list of InfluxDB-Client-Java [exceptions](https://github.com/influxdata/influxdb-client-java/tree/master/client-core/src/main/java/com/influxdb/exceptions)."", 'commenter': 'raminqaf'}]"
114,flink-connector-influxdb2/pom.xml,"@@ -0,0 +1,202 @@
+<?xml version=""1.0"" encoding=""UTF-8""?>
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+<project xmlns=""http://maven.apache.org/POM/4.0.0""
+  xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
+  xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
+
+  <modelVersion>4.0.0</modelVersion>
+
+  <parent>
+    <groupId>org.apache.bahir</groupId>
+    <artifactId>bahir-flink-parent_2.11</artifactId>
+    <version>1.1-SNAPSHOT</version>
+    <relativePath>..</relativePath>
+  </parent>
+
+  <artifactId>flink-connector-influxdb2_2.12</artifactId>","[{'comment': 'this should have the same scala version from parent.\r\n', 'commenter': 'eskabetxe'}]"
114,flink-connector-influxdb2/pom.xml,"@@ -0,0 +1,202 @@
+<?xml version=""1.0"" encoding=""UTF-8""?>
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+<project xmlns=""http://maven.apache.org/POM/4.0.0""
+  xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
+  xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
+
+  <modelVersion>4.0.0</modelVersion>
+
+  <parent>
+    <groupId>org.apache.bahir</groupId>
+    <artifactId>bahir-flink-parent_2.11</artifactId>
+    <version>1.1-SNAPSHOT</version>
+    <relativePath>..</relativePath>
+  </parent>
+
+  <artifactId>flink-connector-influxdb2_2.12</artifactId>
+  <name>flink-connector-influxdb2</name>
+
+  <packaging>jar</packaging>
+
+  <properties>
+    <influxdbClient.version>2.0.0</influxdbClient.version>
+    <flink.new.version>1.12.2</flink.new.version>","[{'comment': 'we should update the flink version on parent', 'commenter': 'eskabetxe'}]"
114,flink-connector-influxdb2/pom.xml,"@@ -0,0 +1,202 @@
+<?xml version=""1.0"" encoding=""UTF-8""?>
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+<project xmlns=""http://maven.apache.org/POM/4.0.0""
+  xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
+  xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
+
+  <modelVersion>4.0.0</modelVersion>
+
+  <parent>
+    <groupId>org.apache.bahir</groupId>
+    <artifactId>bahir-flink-parent_2.11</artifactId>
+    <version>1.1-SNAPSHOT</version>
+    <relativePath>..</relativePath>
+  </parent>
+
+  <artifactId>flink-connector-influxdb2_2.12</artifactId>
+  <name>flink-connector-influxdb2</name>
+
+  <packaging>jar</packaging>
+
+  <properties>
+    <influxdbClient.version>2.0.0</influxdbClient.version>
+    <flink.new.version>1.12.2</flink.new.version>
+    <lombok.version>1.18.10</lombok.version>
+    <scala.binary.new.version>2.11</scala.binary.new.version>
+    <spotless.version>2.7.0</spotless.version>
+    <druid.version>0.13.0-incubating</druid.version>
+    <!--  Test Properties  -->
+    <testcontainers.version>1.15.2</testcontainers.version>
+    <hamcrest.version>2.2</hamcrest.version>
+    <google.http.client.version>1.39.0</google.http.client.version>
+  </properties>
+
+  <dependencies>
+
+    <!-- Flink  -->
+    <dependency>
+      <groupId>org.apache.flink</groupId>
+      <artifactId>flink-core</artifactId>
+      <version>${flink.new.version}</version>
+    </dependency>
+
+
+    <dependency>
+      <groupId>org.apache.flink</groupId>
+      <artifactId>flink-connector-base</artifactId>
+      <version>${flink.new.version}</version>
+    </dependency>
+
+
+    <dependency>
+      <groupId>org.projectlombok</groupId>
+      <artifactId>lombok</artifactId>
+      <version>${lombok.version}</version>
+    </dependency>
+
+    <!-- InfluxDB  -->
+
+    <dependency>
+      <groupId>com.influxdb</groupId>
+      <artifactId>influxdb-client-java</artifactId>
+      <version>${influxdbClient.version}</version>
+    </dependency>
+
+    <!-- InfluxDB Line Protocol Parser by Apache Druid -->
+
+    <dependency>
+      <groupId>org.apache.druid.extensions</groupId>
+      <artifactId>druid-influx-extensions</artifactId>
+      <version>${druid.version}</version>
+    </dependency>
+
+    <!-- Logging -->
+    <dependency>
+      <groupId>org.apache.logging.log4j</groupId>
+      <artifactId>log4j-api</artifactId>
+      <version>${log4j2.version}</version>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.logging.log4j</groupId>
+      <artifactId>log4j-core</artifactId>
+      <version>${log4j2.version}</version>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.logging.log4j</groupId>
+      <artifactId>log4j-slf4j-impl</artifactId>
+      <version>${log4j2.version}</version>
+    </dependency>
+
+    <!-- Flink Test Utils -->
+
+    <dependency>
+      <groupId>org.apache.flink</groupId>
+      <artifactId>flink-test-utils_${scala.binary.new.version}</artifactId>
+      <version>${flink.new.version}</version>
+      <scope>test</scope>
+    </dependency>
+
+    <dependency>
+      <groupId>org.apache.flink</groupId>
+      <artifactId>flink-streaming-java_${scala.binary.new.version}</artifactId>
+      <version>${flink.new.version}</version>
+      <scope>test</scope>
+      <classifier>tests</classifier>
+    </dependency>
+
+    <!-- Test container -->
+
+    <dependency>
+      <groupId>org.testcontainers</groupId>
+      <artifactId>testcontainers</artifactId>
+      <version>${testcontainers.version}</version>
+      <scope>test</scope>
+    </dependency>
+
+    <dependency>
+      <groupId>org.testcontainers</groupId>
+      <artifactId>junit-jupiter</artifactId>
+      <version>${testcontainers.version}</version>
+      <scope>test</scope>
+    </dependency>
+
+    <dependency>
+      <groupId>org.testcontainers</groupId>
+      <artifactId>influxdb</artifactId>
+      <version>${testcontainers.version}</version>
+      <scope>test</scope>
+    </dependency>
+
+    <dependency>
+      <groupId>com.google.http-client</groupId>
+      <artifactId>google-http-client</artifactId>
+      <version>${google.http.client.version}</version>
+      <scope>test</scope>
+    </dependency>
+
+  </dependencies>
+
+  <build>
+    <plugins>
+      <plugin>
+        <groupId>org.apache.maven.plugins</groupId>
+        <artifactId>maven-jar-plugin</artifactId>
+        <executions>
+          <execution>
+            <goals>
+              <goal>test-jar</goal>
+            </goals>
+          </execution>
+        </executions>
+      </plugin>
+
+      <plugin>","[{'comment': 'the use of this kind of plugins should be discussed, and used on parent', 'commenter': 'eskabetxe'}]"
114,flink-connector-influxdb2/src/main/java/org/apache/flink/streaming/connectors/influxdb/common/InfluxParser.java,"@@ -0,0 +1,140 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements. See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership. The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License. You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.flink.streaming.connectors.influxdb.common;
+
+import java.text.ParseException;
+import java.util.List;
+import java.util.regex.Pattern;
+import javax.annotation.Nullable;
+import org.antlr.v4.runtime.ANTLRInputStream;
+import org.antlr.v4.runtime.CharStream;
+import org.antlr.v4.runtime.CommonTokenStream;
+import org.antlr.v4.runtime.TokenStream;
+import org.apache.druid.data.input.influx.InfluxLineProtocolLexer;
+import org.apache.druid.data.input.influx.InfluxLineProtocolParser;
+import org.apache.druid.data.input.influx.InfluxLineProtocolParser.TimestampContext;
+
+/**
+ * This is an InfluxDB line protocol parser.
+ *
+ * @see <a href=https://docs.influxdata.com/influxdb/v2.0/reference/syntax/line-protocol/"">Line
+ *     Protocol</a>
+ * @see <a
+ *     href=https://github.com/apache/druid/blob/master/extensions-contrib/influx-extensions/src/main/java/org/apache/druid/data/input/influx/InfluxParser.java>
+ *     Apache Druid InfluxDB Parser </a>
+ */
+public class InfluxParser {
+    private static final Pattern BACKSLASH_PATTERN = Pattern.compile(""\\\\\"""");
+    private static final Pattern IDENTIFIER_PATTERN = Pattern.compile(""\\\\([,= ])"");
+
+    @Nullable
+    public DataPoint parseToDataPoint(final String input) throws ParseException {
+        final CharStream charStream = new ANTLRInputStream(input);
+        final InfluxLineProtocolLexer lexer = new InfluxLineProtocolLexer(charStream);
+        final TokenStream tokenStream = new CommonTokenStream(lexer);
+        final InfluxLineProtocolParser parser = new InfluxLineProtocolParser(tokenStream);
+
+        final List<InfluxLineProtocolParser.LineContext> lines = parser.lines().line();
+        if (parser.getNumberOfSyntaxErrors() != 0) {
+            throw new ParseException(""Unable to parse line."", 0);
+        }
+        if (lines.size() != 1) {
+            throw new ParseException(
+                    ""Multiple lines present; unable to parse more than one per record."", 0);
+        }
+
+        final InfluxLineProtocolParser.LineContext line = lines.get(0);
+        final String measurement = this.parseIdentifier(line.identifier());
+
+        final Number timestamp = this.parseTimestamp(line.timestamp());
+
+        final DataPoint out = new DataPoint(measurement, timestamp);
+
+        if (line.tag_set() != null) {
+            line.tag_set().tag_pair().forEach(t -> this.parseTag(t, out));
+        }
+
+        line.field_set().field_pair().forEach(t -> this.parseField(t, out));
+
+        return out;
+    }
+
+    private void parseTag(final InfluxLineProtocolParser.Tag_pairContext tag, final DataPoint out) {
+        final String key = this.parseIdentifier(tag.identifier(0));
+        final String value = this.parseIdentifier(tag.identifier(1));
+        out.addTag(key, value);
+    }
+
+    private void parseField(
+            final InfluxLineProtocolParser.Field_pairContext field, final DataPoint out) {
+        final String key = this.parseIdentifier(field.identifier());
+        final InfluxLineProtocolParser.Field_valueContext valueContext = field.field_value();
+        final Object value;
+        if (valueContext.NUMBER() != null) {
+            value = this.parseNumber(valueContext.NUMBER().getText());
+        } else if (valueContext.BOOLEAN() != null) {
+            value = this.parseBool(valueContext.BOOLEAN().getText());
+        } else {
+            value = this.parseQuotedString(valueContext.QUOTED_STRING().getText());
+        }
+        out.addField(key, value);
+    }
+
+    private Object parseQuotedString(final String text) {
+        return BACKSLASH_PATTERN.matcher(text.substring(1, text.length() - 1)).replaceAll(""\"""");
+    }
+
+    private Object parseNumber(final String raw) {
+        if (raw.endsWith(""i"")) {
+            return Long.valueOf(raw.substring(0, raw.length() - 1));
+        }
+
+        return new Double(raw);
+    }
+
+    private Object parseBool(final String raw) {
+        final char first = raw.charAt(0);
+        if (first == 't' || first == 'T') {","[{'comment': ""this could be reduce to \r\nreturn (first == 't' || first == 'T')"", 'commenter': 'eskabetxe'}]"
114,pom.xml,"@@ -93,7 +94,7 @@
 
     <slf4j.version>1.7.16</slf4j.version>
     <log4j.version>1.2.17</log4j.version>
-    <log4j2.version>2.13.3</log4j2.version>
+    <log4j2.version>2.12.1</log4j2.version>","[{'comment': 'why you lower the log4j2 version?', 'commenter': 'eskabetxe'}, {'comment': ""We were struggling with this issue in our logs: https://issues.apache.org/jira/browse/LOG4J2-2901\r\nThe logs were not printed on the console and got this error:\r\n`ERROR StatusLogger Reconfiguration failed: No configuration found for '2aae9190' at 'null' in 'null'`"", 'commenter': 'raminqaf'}]"
114,pom.xml,"@@ -75,6 +75,7 @@
     <module>flink-connector-akka</module>
     <module>flink-connector-flume</module>
     <module>flink-connector-influxdb</module>
+    <module>flink-connector-influxdb2</module>","[{'comment': 'why create another module?\r\nthe first one could not be updated?', 'commenter': 'eskabetxe'}, {'comment': ""Unfortunately not. Since there are breaking changes in InfluxDB 2.x. Even the java client is written from scratch and it's not compatible with version 1.7 anymore. More information [here](https://github.com/influxdata/influxdb-client-java#note-use-this-client-library-with-influxdb-2x-and-influxdb-18-see-details-for-connecting-to-influxdb-17-or-earlier-instances-use-the-influxdb-java-client-library). We created a separate module to avoid a situation where other users still rely on InfluxDB 1.x connectors."", 'commenter': 'raminqaf'}]"
114,flink-connector-influxdb2/src/main/java/org/apache/flink/streaming/connectors/influxdb/source/InfluxDBSourceBuilder.java,"@@ -0,0 +1,136 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements. See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership. The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License. You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.flink.streaming.connectors.influxdb.source;
+
+import static lombok.Lombok.checkNotNull;
+
+import java.util.Properties;
+import org.apache.flink.streaming.connectors.influxdb.source.reader.deserializer.InfluxDBDataPointDeserializer;
+
+public final class InfluxDBSourceBuilder<OUT> {
+
+    private InfluxDBDataPointDeserializer<OUT> deserializationSchema;
+    // Configurations
+    private final Properties properties;
+
+    InfluxDBSourceBuilder() {
+        this.deserializationSchema = null;
+        this.properties = new Properties();
+    }
+
+    /**
+     * Sets the {@link InfluxDBDataPointDeserializer deserializer} of the {@link
+     * org.apache.flink.streaming.connectors.influxdb.common.DataPoint DataPoint} for the
+     * InfluxDBSource.
+     *
+     * @param dataPointDeserializer the deserializer for InfluxDB {@link
+     *     org.apache.flink.streaming.connectors.influxdb.common.DataPoint DataPoint}.
+     * @return this InfluxDBSourceBuilder.
+     */
+    public InfluxDBSourceBuilder<OUT> setDeserializer(
+            final InfluxDBDataPointDeserializer<OUT> dataPointDeserializer) {
+        this.deserializationSchema = dataPointDeserializer;
+        return this;
+    }
+
+    /**
+     * Sets the enqueue wait time, i.e., the time out of this InfluxDBSource.
+     *
+     * @param timeOut the enqueue wait time to use for this InfluxDBSource.
+     * @return this InfluxDBSourceBuilder.
+     */
+    public InfluxDBSourceBuilder<OUT> setEnqueueWaitTime(final long timeOut) {
+        return this.setProperty(
+                InfluxDBSourceOptions.ENQUEUE_WAIT_TIME.key(), String.valueOf(timeOut));
+    }
+
+    /**
+     * Sets the ingest queue capacity of this InfluxDBSource.
+     *
+     * @param capacity the capacity to use for this InfluxDBSource.
+     * @return this InfluxDBSourceBuilder.
+     */
+    public InfluxDBSourceBuilder<OUT> setIngestQueueCapacity(final int capacity) {
+        return this.setProperty(
+                InfluxDBSourceOptions.INGEST_QUEUE_CAPACITY.key(), String.valueOf(capacity));
+    }
+
+    /**
+     * Sets the maximum number of lines that should be parsed per HTTP request for this
+     * InfluxDBSource.
+     *
+     * @param max the maximum number of lines to use for this InfluxDBSource.
+     * @return this InfluxDBSourceBuilder.
+     */
+    public InfluxDBSourceBuilder<OUT> setMaximumLinesPerRequest(final int max) {
+        return this.setProperty(
+                InfluxDBSourceOptions.MAXIMUM_LINES_PER_REQUEST.key(), String.valueOf(max));
+    }
+
+    /**
+     * Sets the TCP port on which the split reader's HTTP server of this InfluxDBSource is running
+     * on.
+     *
+     * @param port the port to use for this InfluxDBSource.
+     * @return this InfluxDBSourceBuilder.
+     */
+    public InfluxDBSourceBuilder<OUT> setPort(final int port) {
+        return this.setProperty(InfluxDBSourceOptions.PORT.key(), String.valueOf(port));
+    }
+
+    /**
+     * Set arbitrary properties for the InfluxDBSource. The valid keys can be found in {@link
+     * InfluxDBSourceOptions}.
+     *
+     * @param properties the properties to set for the InfluxDBSource.
+     * @return this InfluxDBSourceBuilder.
+     */
+    public InfluxDBSourceBuilder<OUT> setProperties(final Properties properties) {
+        this.properties.putAll(properties);
+        return this;
+    }
+
+    /**
+     * Build the {@link InfluxDBSource}.
+     *
+     * @return a InfluxDBSource with the settings made for this builder.
+     */
+    public InfluxDBSource<OUT> build() {
+        this.sanityCheck();
+        return new InfluxDBSource<>(this.properties, this.deserializationSchema);
+    }
+
+    // ------------- private helpers  --------------
+    /**
+     * Set an arbitrary property for the InfluxDBSource. The valid keys can be found in {@link
+     * InfluxDBSourceOptions}.
+     *
+     * @param key the key of the property.
+     * @param value the value of the property.
+     * @return this InfluxDBSourceBuilder.
+     */
+    private InfluxDBSourceBuilder<OUT> setProperty(final String key, final String value) {
+        this.properties.setProperty(key, value);
+        return this;
+    }
+
+    private void sanityCheck() {
+        checkNotNull(","[{'comment': 'why not use Preconditions.checkNotNull from flink ?', 'commenter': 'eskabetxe'}]"
114,flink-connector-influxdb2/pom.xml,"@@ -0,0 +1,202 @@
+<?xml version=""1.0"" encoding=""UTF-8""?>
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+<project xmlns=""http://maven.apache.org/POM/4.0.0""
+  xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
+  xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
+
+  <modelVersion>4.0.0</modelVersion>
+
+  <parent>
+    <groupId>org.apache.bahir</groupId>
+    <artifactId>bahir-flink-parent_2.11</artifactId>
+    <version>1.1-SNAPSHOT</version>
+    <relativePath>..</relativePath>
+  </parent>
+
+  <artifactId>flink-connector-influxdb2_2.12</artifactId>
+  <name>flink-connector-influxdb2</name>
+
+  <packaging>jar</packaging>
+
+  <properties>
+    <influxdbClient.version>2.0.0</influxdbClient.version>
+    <flink.new.version>1.12.2</flink.new.version>
+    <lombok.version>1.18.10</lombok.version>
+    <scala.binary.new.version>2.11</scala.binary.new.version>
+    <spotless.version>2.7.0</spotless.version>
+    <druid.version>0.13.0-incubating</druid.version>
+    <!--  Test Properties  -->
+    <testcontainers.version>1.15.2</testcontainers.version>
+    <hamcrest.version>2.2</hamcrest.version>
+    <google.http.client.version>1.39.0</google.http.client.version>
+  </properties>
+
+  <dependencies>
+
+    <!-- Flink  -->
+    <dependency>
+      <groupId>org.apache.flink</groupId>
+      <artifactId>flink-core</artifactId>
+      <version>${flink.new.version}</version>
+    </dependency>
+
+
+    <dependency>
+      <groupId>org.apache.flink</groupId>
+      <artifactId>flink-connector-base</artifactId>
+      <version>${flink.new.version}</version>
+    </dependency>
+
+
+    <dependency>
+      <groupId>org.projectlombok</groupId>","[{'comment': 'I see that you use this for getter and log4j, as the use of this library could be a barrier for new committers, could you delombok that uses', 'commenter': 'eskabetxe'}]"
114,flink-connector-influxdb2/README.md,"@@ -0,0 +1,202 @@
+# Flink InfluxDB Connector
+
+This connector provides a Source that parses the [InfluxDB Line Protocol](https://docs.influxdata.com/influxdb/v2.0/reference/syntax/line-protocol/) and a Sink that can write to [InfluxDB](https://www.influxdata.com/). The Source implements the unified [Data Source API](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/sources.html). Our sink implements the unified [Sink API](https://cwiki.apache.org/confluence/display/FLINK/FLIP-143%3A+Unified+Sink+API#FLIP143:UnifiedSinkAPI-SinkAPI).","[{'comment': 'Add CDC to description.', 'commenter': 'AHeise'}, {'comment': ""I'd probably enhance the description to make clear why Telegraf was used. Maybe a even a small diagram\r\n```\r\nsensor -> telegraf -> [ influx CDC source -> ... -> influx sink ] -> influx db\r\n```"", 'commenter': 'AHeise'}]"
114,flink-connector-influxdb2/README.md,"@@ -0,0 +1,202 @@
+# Flink InfluxDB Connector
+
+This connector provides a Source that parses the [InfluxDB Line Protocol](https://docs.influxdata.com/influxdb/v2.0/reference/syntax/line-protocol/) and a Sink that can write to [InfluxDB](https://www.influxdata.com/). The Source implements the unified [Data Source API](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/sources.html). Our sink implements the unified [Sink API](https://cwiki.apache.org/confluence/display/FLINK/FLIP-143%3A+Unified+Sink+API#FLIP143:UnifiedSinkAPI-SinkAPI).
+
+## Installation
+
+To use this connector, add the following dependency to your project:
+
+```xml=
+<dependency>
+  <groupId>org.apache.bahir</groupId>
+  <artifactId>flink-connector-influxdb2_2.12</artifactId>
+  <version>1.1-SNAPSHOT</version>
+</dependency>
+```
+
+Note that the streaming connectors are not part of the binary distribution of Flink. You need to link them into your job jar for cluster execution. See how to link with them for cluster execution [here](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/project-configuration.html#adding-connector-and-library-dependencies).
+
+## Compatibility
+
+This module is compatible with InfluxDB 2.x and InfluxDB 1.8+. See more information [here](https://github.com/influxdata/influxdb-client-java#influxdb-client-java).
+
+## Source
+
+The Source accepts data in the form of the [Line Protocol](https://docs.influxdata.com/influxdb/v2.0/reference/syntax/line-protocol/). One HTTP server starts per SplitReader that parses HTTP requests to our Data Point class. That Data Point instance is deserialized by a user-provided implementation of our InfluxDBDataPointDeserializer and send to the next Flink operator.","[{'comment': 'Not sure if users know what a SplitReader is. How about replacing it with source instance?', 'commenter': 'AHeise'}]"
114,flink-connector-influxdb2/README.md,"@@ -0,0 +1,202 @@
+# Flink InfluxDB Connector
+
+This connector provides a Source that parses the [InfluxDB Line Protocol](https://docs.influxdata.com/influxdb/v2.0/reference/syntax/line-protocol/) and a Sink that can write to [InfluxDB](https://www.influxdata.com/). The Source implements the unified [Data Source API](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/sources.html). Our sink implements the unified [Sink API](https://cwiki.apache.org/confluence/display/FLINK/FLIP-143%3A+Unified+Sink+API#FLIP143:UnifiedSinkAPI-SinkAPI).
+
+## Installation
+
+To use this connector, add the following dependency to your project:
+
+```xml=
+<dependency>
+  <groupId>org.apache.bahir</groupId>
+  <artifactId>flink-connector-influxdb2_2.12</artifactId>
+  <version>1.1-SNAPSHOT</version>
+</dependency>
+```
+
+Note that the streaming connectors are not part of the binary distribution of Flink. You need to link them into your job jar for cluster execution. See how to link with them for cluster execution [here](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/project-configuration.html#adding-connector-and-library-dependencies).","[{'comment': '`link` -> `shade`', 'commenter': 'AHeise'}]"
114,flink-connector-influxdb2/README.md,"@@ -0,0 +1,202 @@
+# Flink InfluxDB Connector
+
+This connector provides a Source that parses the [InfluxDB Line Protocol](https://docs.influxdata.com/influxdb/v2.0/reference/syntax/line-protocol/) and a Sink that can write to [InfluxDB](https://www.influxdata.com/). The Source implements the unified [Data Source API](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/sources.html). Our sink implements the unified [Sink API](https://cwiki.apache.org/confluence/display/FLINK/FLIP-143%3A+Unified+Sink+API#FLIP143:UnifiedSinkAPI-SinkAPI).
+
+## Installation
+
+To use this connector, add the following dependency to your project:
+
+```xml=
+<dependency>
+  <groupId>org.apache.bahir</groupId>
+  <artifactId>flink-connector-influxdb2_2.12</artifactId>
+  <version>1.1-SNAPSHOT</version>
+</dependency>
+```
+
+Note that the streaming connectors are not part of the binary distribution of Flink. You need to link them into your job jar for cluster execution. See how to link with them for cluster execution [here](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/project-configuration.html#adding-connector-and-library-dependencies).
+
+## Compatibility
+
+This module is compatible with InfluxDB 2.x and InfluxDB 1.8+. See more information [here](https://github.com/influxdata/influxdb-client-java#influxdb-client-java).
+
+## Source
+
+The Source accepts data in the form of the [Line Protocol](https://docs.influxdata.com/influxdb/v2.0/reference/syntax/line-protocol/). One HTTP server starts per SplitReader that parses HTTP requests to our Data Point class. That Data Point instance is deserialized by a user-provided implementation of our InfluxDBDataPointDeserializer and send to the next Flink operator.
+
+When using Telegraf, you have two choices to configure it. You can either configure its [InfluxDB v2 output plugin](https://docs.influxdata.com/telegraf/v1.17/plugins/#influxdb_v2) for writing to the running HTTP servers or use its [HTTP output plugin](https://docs.influxdata.com/telegraf/v1.17/plugins/#http) for that:
+
+```toml
+[[outputs.influxdb_v2]]
+  urls = [""http://task-manager-1:8000"", ""http:/task-manager-2:8000""]
+
+# or
+
+[[outputs.http]]
+  url = ""http://task-manager-1:8000/api/v2/write""
+  method = ""POST""
+  data_format = ""influx""
+```
+
+![Source Architecture](media/source-architecture.png)","[{'comment': '👍 ', 'commenter': 'AHeise'}]"
114,flink-connector-influxdb2/README.md,"@@ -0,0 +1,202 @@
+# Flink InfluxDB Connector
+
+This connector provides a Source that parses the [InfluxDB Line Protocol](https://docs.influxdata.com/influxdb/v2.0/reference/syntax/line-protocol/) and a Sink that can write to [InfluxDB](https://www.influxdata.com/). The Source implements the unified [Data Source API](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/sources.html). Our sink implements the unified [Sink API](https://cwiki.apache.org/confluence/display/FLINK/FLIP-143%3A+Unified+Sink+API#FLIP143:UnifiedSinkAPI-SinkAPI).
+
+## Installation
+
+To use this connector, add the following dependency to your project:
+
+```xml=
+<dependency>
+  <groupId>org.apache.bahir</groupId>
+  <artifactId>flink-connector-influxdb2_2.12</artifactId>
+  <version>1.1-SNAPSHOT</version>
+</dependency>
+```
+
+Note that the streaming connectors are not part of the binary distribution of Flink. You need to link them into your job jar for cluster execution. See how to link with them for cluster execution [here](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/project-configuration.html#adding-connector-and-library-dependencies).
+
+## Compatibility
+
+This module is compatible with InfluxDB 2.x and InfluxDB 1.8+. See more information [here](https://github.com/influxdata/influxdb-client-java#influxdb-client-java).
+
+## Source
+
+The Source accepts data in the form of the [Line Protocol](https://docs.influxdata.com/influxdb/v2.0/reference/syntax/line-protocol/). One HTTP server starts per SplitReader that parses HTTP requests to our Data Point class. That Data Point instance is deserialized by a user-provided implementation of our InfluxDBDataPointDeserializer and send to the next Flink operator.
+
+When using Telegraf, you have two choices to configure it. You can either configure its [InfluxDB v2 output plugin](https://docs.influxdata.com/telegraf/v1.17/plugins/#influxdb_v2) for writing to the running HTTP servers or use its [HTTP output plugin](https://docs.influxdata.com/telegraf/v1.17/plugins/#http) for that:
+
+```toml
+[[outputs.influxdb_v2]]
+  urls = [""http://task-manager-1:8000"", ""http:/task-manager-2:8000""]
+
+# or
+
+[[outputs.http]]
+  url = ""http://task-manager-1:8000/api/v2/write""
+  method = ""POST""
+  data_format = ""influx""
+```
+
+![Source Architecture](media/source-architecture.png)
+
+### Usage
+
+```java=
+InfluxDBSource<Long> influxDBSource = InfluxBSource.<Long>builder()","[{'comment': 'Maybe builder should be untyped (`?`) until the user sets the deserializer?', 'commenter': 'AHeise'}]"
114,flink-connector-influxdb2/README.md,"@@ -0,0 +1,202 @@
+# Flink InfluxDB Connector
+
+This connector provides a Source that parses the [InfluxDB Line Protocol](https://docs.influxdata.com/influxdb/v2.0/reference/syntax/line-protocol/) and a Sink that can write to [InfluxDB](https://www.influxdata.com/). The Source implements the unified [Data Source API](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/sources.html). Our sink implements the unified [Sink API](https://cwiki.apache.org/confluence/display/FLINK/FLIP-143%3A+Unified+Sink+API#FLIP143:UnifiedSinkAPI-SinkAPI).
+
+## Installation
+
+To use this connector, add the following dependency to your project:
+
+```xml=
+<dependency>
+  <groupId>org.apache.bahir</groupId>
+  <artifactId>flink-connector-influxdb2_2.12</artifactId>
+  <version>1.1-SNAPSHOT</version>
+</dependency>
+```
+
+Note that the streaming connectors are not part of the binary distribution of Flink. You need to link them into your job jar for cluster execution. See how to link with them for cluster execution [here](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/project-configuration.html#adding-connector-and-library-dependencies).
+
+## Compatibility
+
+This module is compatible with InfluxDB 2.x and InfluxDB 1.8+. See more information [here](https://github.com/influxdata/influxdb-client-java#influxdb-client-java).
+
+## Source
+
+The Source accepts data in the form of the [Line Protocol](https://docs.influxdata.com/influxdb/v2.0/reference/syntax/line-protocol/). One HTTP server starts per SplitReader that parses HTTP requests to our Data Point class. That Data Point instance is deserialized by a user-provided implementation of our InfluxDBDataPointDeserializer and send to the next Flink operator.
+
+When using Telegraf, you have two choices to configure it. You can either configure its [InfluxDB v2 output plugin](https://docs.influxdata.com/telegraf/v1.17/plugins/#influxdb_v2) for writing to the running HTTP servers or use its [HTTP output plugin](https://docs.influxdata.com/telegraf/v1.17/plugins/#http) for that:
+
+```toml
+[[outputs.influxdb_v2]]
+  urls = [""http://task-manager-1:8000"", ""http:/task-manager-2:8000""]
+
+# or
+
+[[outputs.http]]
+  url = ""http://task-manager-1:8000/api/v2/write""
+  method = ""POST""
+  data_format = ""influx""
+```
+
+![Source Architecture](media/source-architecture.png)
+
+### Usage
+
+```java=
+InfluxDBSource<Long> influxDBSource = InfluxBSource.<Long>builder()
+        .setDeserializer(new TestDeserializer())
+        .build()
+        
+// ...
+
+/**
+ * Implementation of InfluxDBDataPointDeserializer interface
+ * (dataPoint) -----> (element)
+ *  test,longValue=1 fieldKey=""fieldValue"" -----------> 1L
+ *  test,longValue=2 fieldKey=""fieldValue"" -----------> 2L
+ *  test,longValue=3 fieldKey=""fieldValue"" -----------> 3L
+ */
+class TestDeserializer implements InfluxDBDataPointDeserializer<Long> {
+    @Override
+    public Long deserialize(final DataPoint dataPoint) {
+        return (Long) dataPoint.getField(""longValue"");
+    }
+}
+```
+
+
+### Options
+
+| Option            | Description     | Default Value     |
+| ----------------- |-----------------|:-----------------:|
+| ENQUEUE_WAIT_TIME | The time out in seconds for enqueuing an HTTP request to the queue. | 5 |
+| INGEST_QUEUE_CAPACITY | Size of queue that buffers HTTP requests data points before fetching. | 1000 |
+| MAXIMUM_LINES_PER_REQUEST | The maximum number of lines that should be parsed per HTTP request. | 10000 |
+| PORT | TCP port on which the split reader's HTTP server is running on. | 8000 |","[{'comment': '👍 ', 'commenter': 'AHeise'}]"
114,flink-connector-influxdb2/README.md,"@@ -0,0 +1,202 @@
+# Flink InfluxDB Connector
+
+This connector provides a Source that parses the [InfluxDB Line Protocol](https://docs.influxdata.com/influxdb/v2.0/reference/syntax/line-protocol/) and a Sink that can write to [InfluxDB](https://www.influxdata.com/). The Source implements the unified [Data Source API](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/sources.html). Our sink implements the unified [Sink API](https://cwiki.apache.org/confluence/display/FLINK/FLIP-143%3A+Unified+Sink+API#FLIP143:UnifiedSinkAPI-SinkAPI).
+
+## Installation
+
+To use this connector, add the following dependency to your project:
+
+```xml=
+<dependency>
+  <groupId>org.apache.bahir</groupId>
+  <artifactId>flink-connector-influxdb2_2.12</artifactId>
+  <version>1.1-SNAPSHOT</version>
+</dependency>
+```
+
+Note that the streaming connectors are not part of the binary distribution of Flink. You need to link them into your job jar for cluster execution. See how to link with them for cluster execution [here](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/project-configuration.html#adding-connector-and-library-dependencies).
+
+## Compatibility
+
+This module is compatible with InfluxDB 2.x and InfluxDB 1.8+. See more information [here](https://github.com/influxdata/influxdb-client-java#influxdb-client-java).
+
+## Source
+
+The Source accepts data in the form of the [Line Protocol](https://docs.influxdata.com/influxdb/v2.0/reference/syntax/line-protocol/). One HTTP server starts per SplitReader that parses HTTP requests to our Data Point class. That Data Point instance is deserialized by a user-provided implementation of our InfluxDBDataPointDeserializer and send to the next Flink operator.
+
+When using Telegraf, you have two choices to configure it. You can either configure its [InfluxDB v2 output plugin](https://docs.influxdata.com/telegraf/v1.17/plugins/#influxdb_v2) for writing to the running HTTP servers or use its [HTTP output plugin](https://docs.influxdata.com/telegraf/v1.17/plugins/#http) for that:
+
+```toml
+[[outputs.influxdb_v2]]
+  urls = [""http://task-manager-1:8000"", ""http:/task-manager-2:8000""]
+
+# or
+
+[[outputs.http]]
+  url = ""http://task-manager-1:8000/api/v2/write""
+  method = ""POST""
+  data_format = ""influx""
+```
+
+![Source Architecture](media/source-architecture.png)
+
+### Usage
+
+```java=
+InfluxDBSource<Long> influxDBSource = InfluxBSource.<Long>builder()
+        .setDeserializer(new TestDeserializer())
+        .build()
+        
+// ...
+
+/**
+ * Implementation of InfluxDBDataPointDeserializer interface
+ * (dataPoint) -----> (element)
+ *  test,longValue=1 fieldKey=""fieldValue"" -----------> 1L
+ *  test,longValue=2 fieldKey=""fieldValue"" -----------> 2L
+ *  test,longValue=3 fieldKey=""fieldValue"" -----------> 3L
+ */
+class TestDeserializer implements InfluxDBDataPointDeserializer<Long> {
+    @Override
+    public Long deserialize(final DataPoint dataPoint) {
+        return (Long) dataPoint.getField(""longValue"");
+    }
+}
+```
+
+
+### Options
+
+| Option            | Description     | Default Value     |
+| ----------------- |-----------------|:-----------------:|
+| ENQUEUE_WAIT_TIME | The time out in seconds for enqueuing an HTTP request to the queue. | 5 |
+| INGEST_QUEUE_CAPACITY | Size of queue that buffers HTTP requests data points before fetching. | 1000 |
+| MAXIMUM_LINES_PER_REQUEST | The maximum number of lines that should be parsed per HTTP request. | 10000 |
+| PORT | TCP port on which the split reader's HTTP server is running on. | 8000 |
+
+### Supported Data Types in Field Set
+
+| Field Set     | Support       | 
+| ------------- |:-------------:| 
+|    Float      | ✅            |
+|    Integer    | ✅            |
+|    UInteger   | ❌            |","[{'comment': 'UInt limitation comes directly from Druid right? Could you check if there is a ticket and link it?', 'commenter': 'AHeise'}]"
114,flink-connector-influxdb2/README.md,"@@ -0,0 +1,202 @@
+# Flink InfluxDB Connector
+
+This connector provides a Source that parses the [InfluxDB Line Protocol](https://docs.influxdata.com/influxdb/v2.0/reference/syntax/line-protocol/) and a Sink that can write to [InfluxDB](https://www.influxdata.com/). The Source implements the unified [Data Source API](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/sources.html). Our sink implements the unified [Sink API](https://cwiki.apache.org/confluence/display/FLINK/FLIP-143%3A+Unified+Sink+API#FLIP143:UnifiedSinkAPI-SinkAPI).
+
+## Installation
+
+To use this connector, add the following dependency to your project:
+
+```xml=
+<dependency>
+  <groupId>org.apache.bahir</groupId>
+  <artifactId>flink-connector-influxdb2_2.12</artifactId>
+  <version>1.1-SNAPSHOT</version>
+</dependency>
+```
+
+Note that the streaming connectors are not part of the binary distribution of Flink. You need to link them into your job jar for cluster execution. See how to link with them for cluster execution [here](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/project-configuration.html#adding-connector-and-library-dependencies).
+
+## Compatibility
+
+This module is compatible with InfluxDB 2.x and InfluxDB 1.8+. See more information [here](https://github.com/influxdata/influxdb-client-java#influxdb-client-java).
+
+## Source
+
+The Source accepts data in the form of the [Line Protocol](https://docs.influxdata.com/influxdb/v2.0/reference/syntax/line-protocol/). One HTTP server starts per SplitReader that parses HTTP requests to our Data Point class. That Data Point instance is deserialized by a user-provided implementation of our InfluxDBDataPointDeserializer and send to the next Flink operator.
+
+When using Telegraf, you have two choices to configure it. You can either configure its [InfluxDB v2 output plugin](https://docs.influxdata.com/telegraf/v1.17/plugins/#influxdb_v2) for writing to the running HTTP servers or use its [HTTP output plugin](https://docs.influxdata.com/telegraf/v1.17/plugins/#http) for that:
+
+```toml
+[[outputs.influxdb_v2]]
+  urls = [""http://task-manager-1:8000"", ""http:/task-manager-2:8000""]
+
+# or
+
+[[outputs.http]]
+  url = ""http://task-manager-1:8000/api/v2/write""
+  method = ""POST""
+  data_format = ""influx""
+```
+
+![Source Architecture](media/source-architecture.png)
+
+### Usage
+
+```java=
+InfluxDBSource<Long> influxDBSource = InfluxBSource.<Long>builder()
+        .setDeserializer(new TestDeserializer())
+        .build()
+        
+// ...
+
+/**
+ * Implementation of InfluxDBDataPointDeserializer interface
+ * (dataPoint) -----> (element)
+ *  test,longValue=1 fieldKey=""fieldValue"" -----------> 1L
+ *  test,longValue=2 fieldKey=""fieldValue"" -----------> 2L
+ *  test,longValue=3 fieldKey=""fieldValue"" -----------> 3L
+ */
+class TestDeserializer implements InfluxDBDataPointDeserializer<Long> {
+    @Override
+    public Long deserialize(final DataPoint dataPoint) {
+        return (Long) dataPoint.getField(""longValue"");
+    }
+}
+```
+
+
+### Options
+
+| Option            | Description     | Default Value     |
+| ----------------- |-----------------|:-----------------:|
+| ENQUEUE_WAIT_TIME | The time out in seconds for enqueuing an HTTP request to the queue. | 5 |
+| INGEST_QUEUE_CAPACITY | Size of queue that buffers HTTP requests data points before fetching. | 1000 |
+| MAXIMUM_LINES_PER_REQUEST | The maximum number of lines that should be parsed per HTTP request. | 10000 |
+| PORT | TCP port on which the split reader's HTTP server is running on. | 8000 |
+
+### Supported Data Types in Field Set
+
+| Field Set     | Support       | 
+| ------------- |:-------------:| 
+|    Float      | ✅            |
+|    Integer    | ✅            |
+|    UInteger   | ❌            |
+|    String     | ✅            |
+|    Boolean    | ✅            |
+
+See InfluxDB field set value [data type](https://docs.influxdata.com/influxdb/cloud/reference/syntax/line-protocol/#field-set).
+ 
+
+## Sink
+
+The Sink writes data points to InfluxDB using the [InfluxDB Java Client](https://github.com/influxdata/influxdb-client-java). You provide the connection information (URL, username, password, bucket, and organization) and an implementation of `InfluxDBSchemaSerializer<IN>` generic interface. The implementation of the interface overrides the `serialize(IN element, Context context)` function. This function serializes incoming Flink elements of type `IN` to [Point](https://github.com/influxdata/influxdb-client-java/blob/master/client/src/main/java/com/influxdb/client/write/Point.java) objects.
+
+It is possible to write multiple data points to InfluxDB simultaneously by separating each point with a new line. Batching data points in this manner results in much higher performance. The batch size can be set through the `WRITE_BUFFER_SIZE` option. By default, the buffer size is set to 1000 and can be changed to any value using the `setWriteBufferSize(final int bufferSize)` of the Sink builder class.
+
+It is possible to write checkpoint data points to InfluxDB whenever Flink sets a checkpoint. To enable this functionality, you need to set the `WRITE_DATA_POINT_CHECKPOINT` flag to true (default is false). The checkpoint data point looks as follow:
+```text
+checkpoint checkpoint=flink <timestamp>
+```
+The timestamp refers to the latest element that Flink serializes.
+
+### Usage
+
+```java=
+// The InfluxDB Sink uses the build pattern to create a Sink object
+InfluxDBSink<Long> influxDBSink = InfluxDBSink.<Long>builder()","[{'comment': 'Similar idea: leave `builder()` untyped?', 'commenter': 'AHeise'}]"
114,flink-connector-influxdb2/README.md,"@@ -0,0 +1,202 @@
+# Flink InfluxDB Connector
+
+This connector provides a Source that parses the [InfluxDB Line Protocol](https://docs.influxdata.com/influxdb/v2.0/reference/syntax/line-protocol/) and a Sink that can write to [InfluxDB](https://www.influxdata.com/). The Source implements the unified [Data Source API](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/sources.html). Our sink implements the unified [Sink API](https://cwiki.apache.org/confluence/display/FLINK/FLIP-143%3A+Unified+Sink+API#FLIP143:UnifiedSinkAPI-SinkAPI).
+
+## Installation
+
+To use this connector, add the following dependency to your project:
+
+```xml=
+<dependency>
+  <groupId>org.apache.bahir</groupId>
+  <artifactId>flink-connector-influxdb2_2.12</artifactId>
+  <version>1.1-SNAPSHOT</version>
+</dependency>
+```
+
+Note that the streaming connectors are not part of the binary distribution of Flink. You need to link them into your job jar for cluster execution. See how to link with them for cluster execution [here](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/project-configuration.html#adding-connector-and-library-dependencies).
+
+## Compatibility
+
+This module is compatible with InfluxDB 2.x and InfluxDB 1.8+. See more information [here](https://github.com/influxdata/influxdb-client-java#influxdb-client-java).
+
+## Source
+
+The Source accepts data in the form of the [Line Protocol](https://docs.influxdata.com/influxdb/v2.0/reference/syntax/line-protocol/). One HTTP server starts per SplitReader that parses HTTP requests to our Data Point class. That Data Point instance is deserialized by a user-provided implementation of our InfluxDBDataPointDeserializer and send to the next Flink operator.
+
+When using Telegraf, you have two choices to configure it. You can either configure its [InfluxDB v2 output plugin](https://docs.influxdata.com/telegraf/v1.17/plugins/#influxdb_v2) for writing to the running HTTP servers or use its [HTTP output plugin](https://docs.influxdata.com/telegraf/v1.17/plugins/#http) for that:
+
+```toml
+[[outputs.influxdb_v2]]
+  urls = [""http://task-manager-1:8000"", ""http:/task-manager-2:8000""]
+
+# or
+
+[[outputs.http]]
+  url = ""http://task-manager-1:8000/api/v2/write""
+  method = ""POST""
+  data_format = ""influx""
+```
+
+![Source Architecture](media/source-architecture.png)
+
+### Usage
+
+```java=
+InfluxDBSource<Long> influxDBSource = InfluxBSource.<Long>builder()
+        .setDeserializer(new TestDeserializer())
+        .build()
+        
+// ...
+
+/**
+ * Implementation of InfluxDBDataPointDeserializer interface
+ * (dataPoint) -----> (element)
+ *  test,longValue=1 fieldKey=""fieldValue"" -----------> 1L
+ *  test,longValue=2 fieldKey=""fieldValue"" -----------> 2L
+ *  test,longValue=3 fieldKey=""fieldValue"" -----------> 3L
+ */
+class TestDeserializer implements InfluxDBDataPointDeserializer<Long> {
+    @Override
+    public Long deserialize(final DataPoint dataPoint) {
+        return (Long) dataPoint.getField(""longValue"");
+    }
+}
+```
+
+
+### Options
+
+| Option            | Description     | Default Value     |
+| ----------------- |-----------------|:-----------------:|
+| ENQUEUE_WAIT_TIME | The time out in seconds for enqueuing an HTTP request to the queue. | 5 |
+| INGEST_QUEUE_CAPACITY | Size of queue that buffers HTTP requests data points before fetching. | 1000 |
+| MAXIMUM_LINES_PER_REQUEST | The maximum number of lines that should be parsed per HTTP request. | 10000 |
+| PORT | TCP port on which the split reader's HTTP server is running on. | 8000 |
+
+### Supported Data Types in Field Set
+
+| Field Set     | Support       | 
+| ------------- |:-------------:| 
+|    Float      | ✅            |
+|    Integer    | ✅            |
+|    UInteger   | ❌            |
+|    String     | ✅            |
+|    Boolean    | ✅            |
+
+See InfluxDB field set value [data type](https://docs.influxdata.com/influxdb/cloud/reference/syntax/line-protocol/#field-set).
+ 
+
+## Sink
+
+The Sink writes data points to InfluxDB using the [InfluxDB Java Client](https://github.com/influxdata/influxdb-client-java). You provide the connection information (URL, username, password, bucket, and organization) and an implementation of `InfluxDBSchemaSerializer<IN>` generic interface. The implementation of the interface overrides the `serialize(IN element, Context context)` function. This function serializes incoming Flink elements of type `IN` to [Point](https://github.com/influxdata/influxdb-client-java/blob/master/client/src/main/java/com/influxdb/client/write/Point.java) objects.
+
+It is possible to write multiple data points to InfluxDB simultaneously by separating each point with a new line. Batching data points in this manner results in much higher performance. The batch size can be set through the `WRITE_BUFFER_SIZE` option. By default, the buffer size is set to 1000 and can be changed to any value using the `setWriteBufferSize(final int bufferSize)` of the Sink builder class.
+
+It is possible to write checkpoint data points to InfluxDB whenever Flink sets a checkpoint. To enable this functionality, you need to set the `WRITE_DATA_POINT_CHECKPOINT` flag to true (default is false). The checkpoint data point looks as follow:
+```text
+checkpoint checkpoint=flink <timestamp>
+```
+The timestamp refers to the latest element that Flink serializes.
+
+### Usage
+
+```java=
+// The InfluxDB Sink uses the build pattern to create a Sink object
+InfluxDBSink<Long> influxDBSink = InfluxDBSink.<Long>builder()
+        .setInfluxDBSchemaSerializer(new TestSerializer())
+        .setInfluxDBUrl(getUrl())           // http://localhost:8086
+        .setInfluxDBUsername(getUsername()) // admin
+        .setInfluxDBPassword(getPassword()) // admin
+        .setInfluxDBBucket(getBucket())     // default
+        .setInfluxDBOrganization(getOrg())  // influxdata
+        .build();
+        
+// ...
+
+/**
+ * Implementation of InfluxDBSchemaSerializer interface
+ * (element) -----> (dataPoint)
+ *  1L -----------> test,longValue=1 fieldKey=""fieldValue""
+ *  2L -----------> test,longValue=2 fieldKey=""fieldValue""
+ *  3L -----------> test,longValue=3 fieldKey=""fieldValue""           
+ */
+class TestSerializer implements InfluxDBSchemaSerializer<Long> {
+
+    @Override
+    public Point serialize(Long element, Context context) {
+        final Point dataPoint = new Point(""test"");
+        dataPoint.addTag(""longValue"", String.valueOf(element));
+        dataPoint.addField(""fieldKey"", ""fieldValue"");
+        return dataPoint;
+    }
+}
+```
+
+### Options
+
+| Option            | Description   | Default Value   |
+| ----------------- |-----------------|:-----------------:|
+| WRITE_DATA_POINT_CHECKPOINT | Determines if the checkpoint data point should be written to InfluxDB or not. | false |
+| WRITE_BUFFER_SIZE | Number of elements to buffer the data before writing them to InfluxDB. | 1000 |
+| INFLUXDB_URL | InfluxDB Connection URL. | ❌ |
+| INFLUXDB_USERNAME | InfluxDB username. | ❌ |
+| INFLUXDB_PASSWORD | InfluxDB password. | ❌ |
+| INFLUXDB_BUCKET | InfluxDB bucket. | ❌ |
+| INFLUXDB_ORGANIZATION | InfluxDB organization. | ❌ |","[{'comment': ':+1:', 'commenter': 'AHeise'}]"
114,flink-connector-influxdb2/src/main/java/org/apache/flink/streaming/connectors/influxdb/common/DataPoint.java,"@@ -0,0 +1,101 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements. See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership. The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License. You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.flink.streaming.connectors.influxdb.common;
+
+import com.influxdb.Arguments;
+import com.influxdb.client.domain.WritePrecision;
+import com.influxdb.client.write.Point;
+import java.util.HashMap;
+import java.util.Map;
+import javax.annotation.Nullable;
+import lombok.Getter;
+
+/**
+ * InfluxDB data point class.
+ *
+ * <p>{@link InfluxParser} parses line protocol into this data point representation.","[{'comment': ""I'd add an example/pattern of a datapoint here."", 'commenter': 'AHeise'}]"
114,flink-connector-influxdb2/src/main/java/org/apache/flink/streaming/connectors/influxdb/common/InfluxParser.java,"@@ -0,0 +1,140 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements. See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership. The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License. You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.flink.streaming.connectors.influxdb.common;
+
+import java.text.ParseException;
+import java.util.List;
+import java.util.regex.Pattern;
+import javax.annotation.Nullable;
+import org.antlr.v4.runtime.ANTLRInputStream;
+import org.antlr.v4.runtime.CharStream;
+import org.antlr.v4.runtime.CommonTokenStream;
+import org.antlr.v4.runtime.TokenStream;
+import org.apache.druid.data.input.influx.InfluxLineProtocolLexer;
+import org.apache.druid.data.input.influx.InfluxLineProtocolParser;
+import org.apache.druid.data.input.influx.InfluxLineProtocolParser.TimestampContext;
+
+/**
+ * This is an InfluxDB line protocol parser.
+ *
+ * @see <a href=https://docs.influxdata.com/influxdb/v2.0/reference/syntax/line-protocol/"">Line
+ *     Protocol</a>
+ * @see <a
+ *     href=https://github.com/apache/druid/blob/master/extensions-contrib/influx-extensions/src/main/java/org/apache/druid/data/input/influx/InfluxParser.java>
+ *     Apache Druid InfluxDB Parser </a>
+ */
+public class InfluxParser {
+    private static final Pattern BACKSLASH_PATTERN = Pattern.compile(""\\\\\"""");
+    private static final Pattern IDENTIFIER_PATTERN = Pattern.compile(""\\\\([,= ])"");
+
+    @Nullable
+    public DataPoint parseToDataPoint(final String input) throws ParseException {
+        final CharStream charStream = new ANTLRInputStream(input);
+        final InfluxLineProtocolLexer lexer = new InfluxLineProtocolLexer(charStream);
+        final TokenStream tokenStream = new CommonTokenStream(lexer);
+        final InfluxLineProtocolParser parser = new InfluxLineProtocolParser(tokenStream);
+
+        final List<InfluxLineProtocolParser.LineContext> lines = parser.lines().line();
+        if (parser.getNumberOfSyntaxErrors() != 0) {
+            throw new ParseException(""Unable to parse line."", 0);
+        }
+        if (lines.size() != 1) {
+            throw new ParseException(
+                    ""Multiple lines present; unable to parse more than one per record."", 0);
+        }
+
+        final InfluxLineProtocolParser.LineContext line = lines.get(0);
+        final String measurement = this.parseIdentifier(line.identifier());
+
+        final Number timestamp = this.parseTimestamp(line.timestamp());
+
+        final DataPoint out = new DataPoint(measurement, timestamp);
+
+        if (line.tag_set() != null) {
+            line.tag_set().tag_pair().forEach(t -> this.parseTag(t, out));
+        }
+
+        line.field_set().field_pair().forEach(t -> this.parseField(t, out));
+
+        return out;
+    }
+
+    private void parseTag(final InfluxLineProtocolParser.Tag_pairContext tag, final DataPoint out) {
+        final String key = this.parseIdentifier(tag.identifier(0));
+        final String value = this.parseIdentifier(tag.identifier(1));
+        out.addTag(key, value);
+    }
+
+    private void parseField(
+            final InfluxLineProtocolParser.Field_pairContext field, final DataPoint out) {
+        final String key = this.parseIdentifier(field.identifier());
+        final InfluxLineProtocolParser.Field_valueContext valueContext = field.field_value();
+        final Object value;
+        if (valueContext.NUMBER() != null) {
+            value = this.parseNumber(valueContext.NUMBER().getText());
+        } else if (valueContext.BOOLEAN() != null) {
+            value = this.parseBool(valueContext.BOOLEAN().getText());
+        } else {
+            value = this.parseQuotedString(valueContext.QUOTED_STRING().getText());
+        }
+        out.addField(key, value);
+    }
+
+    private Object parseQuotedString(final String text) {
+        return BACKSLASH_PATTERN.matcher(text.substring(1, text.length() - 1)).replaceAll(""\"""");
+    }
+
+    private Object parseNumber(final String raw) {
+        if (raw.endsWith(""i"")) {
+            return Long.valueOf(raw.substring(0, raw.length() - 1));
+        }
+
+        return new Double(raw);","[{'comment': '`Double.valueOf`', 'commenter': 'AHeise'}]"
114,flink-connector-influxdb2/src/main/java/org/apache/flink/streaming/connectors/influxdb/common/InfluxParser.java,"@@ -0,0 +1,140 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements. See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership. The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License. You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.flink.streaming.connectors.influxdb.common;
+
+import java.text.ParseException;
+import java.util.List;
+import java.util.regex.Pattern;
+import javax.annotation.Nullable;
+import org.antlr.v4.runtime.ANTLRInputStream;
+import org.antlr.v4.runtime.CharStream;
+import org.antlr.v4.runtime.CommonTokenStream;
+import org.antlr.v4.runtime.TokenStream;
+import org.apache.druid.data.input.influx.InfluxLineProtocolLexer;
+import org.apache.druid.data.input.influx.InfluxLineProtocolParser;
+import org.apache.druid.data.input.influx.InfluxLineProtocolParser.TimestampContext;
+
+/**
+ * This is an InfluxDB line protocol parser.
+ *
+ * @see <a href=https://docs.influxdata.com/influxdb/v2.0/reference/syntax/line-protocol/"">Line
+ *     Protocol</a>
+ * @see <a
+ *     href=https://github.com/apache/druid/blob/master/extensions-contrib/influx-extensions/src/main/java/org/apache/druid/data/input/influx/InfluxParser.java>
+ *     Apache Druid InfluxDB Parser </a>
+ */
+public class InfluxParser {
+    private static final Pattern BACKSLASH_PATTERN = Pattern.compile(""\\\\\"""");
+    private static final Pattern IDENTIFIER_PATTERN = Pattern.compile(""\\\\([,= ])"");
+
+    @Nullable
+    public DataPoint parseToDataPoint(final String input) throws ParseException {
+        final CharStream charStream = new ANTLRInputStream(input);
+        final InfluxLineProtocolLexer lexer = new InfluxLineProtocolLexer(charStream);
+        final TokenStream tokenStream = new CommonTokenStream(lexer);
+        final InfluxLineProtocolParser parser = new InfluxLineProtocolParser(tokenStream);","[{'comment': 'This seems to be rather costly for each line. Could you please double-check if there is a way to reuse the parser with existing Druid parser?', 'commenter': 'AHeise'}]"
114,flink-connector-influxdb2/src/main/java/org/apache/flink/streaming/connectors/influxdb/common/InfluxParser.java,"@@ -0,0 +1,140 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements. See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership. The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License. You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.flink.streaming.connectors.influxdb.common;
+
+import java.text.ParseException;
+import java.util.List;
+import java.util.regex.Pattern;
+import javax.annotation.Nullable;
+import org.antlr.v4.runtime.ANTLRInputStream;
+import org.antlr.v4.runtime.CharStream;
+import org.antlr.v4.runtime.CommonTokenStream;
+import org.antlr.v4.runtime.TokenStream;
+import org.apache.druid.data.input.influx.InfluxLineProtocolLexer;
+import org.apache.druid.data.input.influx.InfluxLineProtocolParser;
+import org.apache.druid.data.input.influx.InfluxLineProtocolParser.TimestampContext;
+
+/**
+ * This is an InfluxDB line protocol parser.
+ *
+ * @see <a href=https://docs.influxdata.com/influxdb/v2.0/reference/syntax/line-protocol/"">Line
+ *     Protocol</a>
+ * @see <a
+ *     href=https://github.com/apache/druid/blob/master/extensions-contrib/influx-extensions/src/main/java/org/apache/druid/data/input/influx/InfluxParser.java>
+ *     Apache Druid InfluxDB Parser </a>
+ */
+public class InfluxParser {
+    private static final Pattern BACKSLASH_PATTERN = Pattern.compile(""\\\\\"""");
+    private static final Pattern IDENTIFIER_PATTERN = Pattern.compile(""\\\\([,= ])"");
+
+    @Nullable
+    public DataPoint parseToDataPoint(final String input) throws ParseException {
+        final CharStream charStream = new ANTLRInputStream(input);
+        final InfluxLineProtocolLexer lexer = new InfluxLineProtocolLexer(charStream);
+        final TokenStream tokenStream = new CommonTokenStream(lexer);
+        final InfluxLineProtocolParser parser = new InfluxLineProtocolParser(tokenStream);
+
+        final List<InfluxLineProtocolParser.LineContext> lines = parser.lines().line();
+        if (parser.getNumberOfSyntaxErrors() != 0) {
+            throw new ParseException(""Unable to parse line."", 0);
+        }
+        if (lines.size() != 1) {
+            throw new ParseException(
+                    ""Multiple lines present; unable to parse more than one per record."", 0);
+        }
+
+        final InfluxLineProtocolParser.LineContext line = lines.get(0);
+        final String measurement = this.parseIdentifier(line.identifier());
+
+        final Number timestamp = this.parseTimestamp(line.timestamp());
+
+        final DataPoint out = new DataPoint(measurement, timestamp);
+
+        if (line.tag_set() != null) {
+            line.tag_set().tag_pair().forEach(t -> this.parseTag(t, out));
+        }
+
+        line.field_set().field_pair().forEach(t -> this.parseField(t, out));
+
+        return out;
+    }
+
+    private void parseTag(final InfluxLineProtocolParser.Tag_pairContext tag, final DataPoint out) {
+        final String key = this.parseIdentifier(tag.identifier(0));
+        final String value = this.parseIdentifier(tag.identifier(1));
+        out.addTag(key, value);
+    }
+
+    private void parseField(
+            final InfluxLineProtocolParser.Field_pairContext field, final DataPoint out) {
+        final String key = this.parseIdentifier(field.identifier());
+        final InfluxLineProtocolParser.Field_valueContext valueContext = field.field_value();
+        final Object value;
+        if (valueContext.NUMBER() != null) {
+            value = this.parseNumber(valueContext.NUMBER().getText());
+        } else if (valueContext.BOOLEAN() != null) {
+            value = this.parseBool(valueContext.BOOLEAN().getText());
+        } else {
+            value = this.parseQuotedString(valueContext.QUOTED_STRING().getText());
+        }
+        out.addField(key, value);
+    }
+
+    private Object parseQuotedString(final String text) {
+        return BACKSLASH_PATTERN.matcher(text.substring(1, text.length() - 1)).replaceAll(""\"""");
+    }
+
+    private Object parseNumber(final String raw) {
+        if (raw.endsWith(""i"")) {
+            return Long.valueOf(raw.substring(0, raw.length() - 1));
+        }
+
+        return new Double(raw);
+    }
+
+    private Object parseBool(final String raw) {
+        final char first = raw.charAt(0);
+        if (first == 't' || first == 'T') {
+            return ""true"";
+        } else {
+            return ""false"";
+        }
+    }
+
+    private String parseIdentifier(final InfluxLineProtocolParser.IdentifierContext ctx) {
+        if (ctx.BOOLEAN() != null || ctx.NUMBER() != null) {
+            return ctx.getText();
+        }
+
+        return IDENTIFIER_PATTERN.matcher(ctx.IDENTIFIER_STRING().getText()).replaceAll(""$1"");
+    }
+
+    private Number parseTimestamp(@Nullable final TimestampContext timestamp) {","[{'comment': 'Long?', 'commenter': 'AHeise'}]"
114,flink-connector-influxdb2/src/main/java/org/apache/flink/streaming/connectors/influxdb/common/DataPoint.java,"@@ -0,0 +1,101 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements. See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership. The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License. You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.flink.streaming.connectors.influxdb.common;
+
+import com.influxdb.Arguments;
+import com.influxdb.client.domain.WritePrecision;
+import com.influxdb.client.write.Point;
+import java.util.HashMap;
+import java.util.Map;
+import javax.annotation.Nullable;
+import lombok.Getter;
+
+/**
+ * InfluxDB data point class.
+ *
+ * <p>{@link InfluxParser} parses line protocol into this data point representation.
+ */
+public final class DataPoint {
+    @Getter private final String name;
+    private final Map<String, String> tags = new HashMap();
+    private final Map<String, Object> fields = new HashMap();
+    @Getter private final Number timestamp;","[{'comment': '`Long`?', 'commenter': 'AHeise'}]"
114,flink-connector-influxdb2/src/main/java/org/apache/flink/streaming/connectors/influxdb/common/InfluxParser.java,"@@ -0,0 +1,140 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements. See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership. The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License. You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.flink.streaming.connectors.influxdb.common;
+
+import java.text.ParseException;
+import java.util.List;
+import java.util.regex.Pattern;
+import javax.annotation.Nullable;
+import org.antlr.v4.runtime.ANTLRInputStream;
+import org.antlr.v4.runtime.CharStream;
+import org.antlr.v4.runtime.CommonTokenStream;
+import org.antlr.v4.runtime.TokenStream;
+import org.apache.druid.data.input.influx.InfluxLineProtocolLexer;
+import org.apache.druid.data.input.influx.InfluxLineProtocolParser;
+import org.apache.druid.data.input.influx.InfluxLineProtocolParser.TimestampContext;
+
+/**
+ * This is an InfluxDB line protocol parser.
+ *
+ * @see <a href=https://docs.influxdata.com/influxdb/v2.0/reference/syntax/line-protocol/"">Line
+ *     Protocol</a>
+ * @see <a
+ *     href=https://github.com/apache/druid/blob/master/extensions-contrib/influx-extensions/src/main/java/org/apache/druid/data/input/influx/InfluxParser.java>
+ *     Apache Druid InfluxDB Parser </a>
+ */
+public class InfluxParser {
+    private static final Pattern BACKSLASH_PATTERN = Pattern.compile(""\\\\\"""");
+    private static final Pattern IDENTIFIER_PATTERN = Pattern.compile(""\\\\([,= ])"");
+
+    @Nullable
+    public DataPoint parseToDataPoint(final String input) throws ParseException {
+        final CharStream charStream = new ANTLRInputStream(input);
+        final InfluxLineProtocolLexer lexer = new InfluxLineProtocolLexer(charStream);
+        final TokenStream tokenStream = new CommonTokenStream(lexer);
+        final InfluxLineProtocolParser parser = new InfluxLineProtocolParser(tokenStream);
+
+        final List<InfluxLineProtocolParser.LineContext> lines = parser.lines().line();
+        if (parser.getNumberOfSyntaxErrors() != 0) {
+            throw new ParseException(""Unable to parse line."", 0);
+        }
+        if (lines.size() != 1) {
+            throw new ParseException(
+                    ""Multiple lines present; unable to parse more than one per record."", 0);
+        }
+
+        final InfluxLineProtocolParser.LineContext line = lines.get(0);
+        final String measurement = this.parseIdentifier(line.identifier());
+
+        final Number timestamp = this.parseTimestamp(line.timestamp());
+
+        final DataPoint out = new DataPoint(measurement, timestamp);
+
+        if (line.tag_set() != null) {
+            line.tag_set().tag_pair().forEach(t -> this.parseTag(t, out));
+        }
+
+        line.field_set().field_pair().forEach(t -> this.parseField(t, out));
+
+        return out;
+    }
+
+    private void parseTag(final InfluxLineProtocolParser.Tag_pairContext tag, final DataPoint out) {
+        final String key = this.parseIdentifier(tag.identifier(0));
+        final String value = this.parseIdentifier(tag.identifier(1));
+        out.addTag(key, value);
+    }
+
+    private void parseField(
+            final InfluxLineProtocolParser.Field_pairContext field, final DataPoint out) {
+        final String key = this.parseIdentifier(field.identifier());
+        final InfluxLineProtocolParser.Field_valueContext valueContext = field.field_value();
+        final Object value;
+        if (valueContext.NUMBER() != null) {
+            value = this.parseNumber(valueContext.NUMBER().getText());
+        } else if (valueContext.BOOLEAN() != null) {
+            value = this.parseBool(valueContext.BOOLEAN().getText());
+        } else {
+            value = this.parseQuotedString(valueContext.QUOTED_STRING().getText());
+        }
+        out.addField(key, value);
+    }
+
+    private Object parseQuotedString(final String text) {
+        return BACKSLASH_PATTERN.matcher(text.substring(1, text.length() - 1)).replaceAll(""\"""");
+    }
+
+    private Object parseNumber(final String raw) {
+        if (raw.endsWith(""i"")) {
+            return Long.valueOf(raw.substring(0, raw.length() - 1));
+        }
+
+        return new Double(raw);
+    }
+
+    private Object parseBool(final String raw) {","[{'comment': 'Should probably by `Boolean`.', 'commenter': 'AHeise'}]"
114,flink-connector-influxdb2/src/main/java/org/apache/flink/streaming/connectors/influxdb/common/InfluxParser.java,"@@ -0,0 +1,140 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements. See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership. The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License. You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.flink.streaming.connectors.influxdb.common;
+
+import java.text.ParseException;
+import java.util.List;
+import java.util.regex.Pattern;
+import javax.annotation.Nullable;
+import org.antlr.v4.runtime.ANTLRInputStream;
+import org.antlr.v4.runtime.CharStream;
+import org.antlr.v4.runtime.CommonTokenStream;
+import org.antlr.v4.runtime.TokenStream;
+import org.apache.druid.data.input.influx.InfluxLineProtocolLexer;
+import org.apache.druid.data.input.influx.InfluxLineProtocolParser;
+import org.apache.druid.data.input.influx.InfluxLineProtocolParser.TimestampContext;
+
+/**
+ * This is an InfluxDB line protocol parser.
+ *
+ * @see <a href=https://docs.influxdata.com/influxdb/v2.0/reference/syntax/line-protocol/"">Line
+ *     Protocol</a>
+ * @see <a
+ *     href=https://github.com/apache/druid/blob/master/extensions-contrib/influx-extensions/src/main/java/org/apache/druid/data/input/influx/InfluxParser.java>
+ *     Apache Druid InfluxDB Parser </a>
+ */
+public class InfluxParser {
+    private static final Pattern BACKSLASH_PATTERN = Pattern.compile(""\\\\\"""");
+    private static final Pattern IDENTIFIER_PATTERN = Pattern.compile(""\\\\([,= ])"");
+
+    @Nullable
+    public DataPoint parseToDataPoint(final String input) throws ParseException {
+        final CharStream charStream = new ANTLRInputStream(input);
+        final InfluxLineProtocolLexer lexer = new InfluxLineProtocolLexer(charStream);
+        final TokenStream tokenStream = new CommonTokenStream(lexer);
+        final InfluxLineProtocolParser parser = new InfluxLineProtocolParser(tokenStream);
+
+        final List<InfluxLineProtocolParser.LineContext> lines = parser.lines().line();
+        if (parser.getNumberOfSyntaxErrors() != 0) {
+            throw new ParseException(""Unable to parse line."", 0);
+        }
+        if (lines.size() != 1) {
+            throw new ParseException(
+                    ""Multiple lines present; unable to parse more than one per record."", 0);
+        }
+
+        final InfluxLineProtocolParser.LineContext line = lines.get(0);
+        final String measurement = this.parseIdentifier(line.identifier());
+
+        final Number timestamp = this.parseTimestamp(line.timestamp());
+
+        final DataPoint out = new DataPoint(measurement, timestamp);
+
+        if (line.tag_set() != null) {
+            line.tag_set().tag_pair().forEach(t -> this.parseTag(t, out));
+        }
+
+        line.field_set().field_pair().forEach(t -> this.parseField(t, out));
+
+        return out;
+    }
+
+    private void parseTag(final InfluxLineProtocolParser.Tag_pairContext tag, final DataPoint out) {
+        final String key = this.parseIdentifier(tag.identifier(0));
+        final String value = this.parseIdentifier(tag.identifier(1));
+        out.addTag(key, value);
+    }
+
+    private void parseField(
+            final InfluxLineProtocolParser.Field_pairContext field, final DataPoint out) {
+        final String key = this.parseIdentifier(field.identifier());
+        final InfluxLineProtocolParser.Field_valueContext valueContext = field.field_value();
+        final Object value;
+        if (valueContext.NUMBER() != null) {
+            value = this.parseNumber(valueContext.NUMBER().getText());
+        } else if (valueContext.BOOLEAN() != null) {
+            value = this.parseBool(valueContext.BOOLEAN().getText());
+        } else {
+            value = this.parseQuotedString(valueContext.QUOTED_STRING().getText());
+        }
+        out.addField(key, value);
+    }
+
+    private Object parseQuotedString(final String text) {
+        return BACKSLASH_PATTERN.matcher(text.substring(1, text.length() - 1)).replaceAll(""\"""");
+    }
+
+    private Object parseNumber(final String raw) {","[{'comment': '`Number`', 'commenter': 'AHeise'}]"
114,flink-connector-influxdb2/src/main/java/org/apache/flink/streaming/connectors/influxdb/common/InfluxParser.java,"@@ -0,0 +1,140 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements. See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership. The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License. You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.flink.streaming.connectors.influxdb.common;
+
+import java.text.ParseException;
+import java.util.List;
+import java.util.regex.Pattern;
+import javax.annotation.Nullable;
+import org.antlr.v4.runtime.ANTLRInputStream;
+import org.antlr.v4.runtime.CharStream;
+import org.antlr.v4.runtime.CommonTokenStream;
+import org.antlr.v4.runtime.TokenStream;
+import org.apache.druid.data.input.influx.InfluxLineProtocolLexer;
+import org.apache.druid.data.input.influx.InfluxLineProtocolParser;
+import org.apache.druid.data.input.influx.InfluxLineProtocolParser.TimestampContext;
+
+/**
+ * This is an InfluxDB line protocol parser.
+ *
+ * @see <a href=https://docs.influxdata.com/influxdb/v2.0/reference/syntax/line-protocol/"">Line
+ *     Protocol</a>
+ * @see <a
+ *     href=https://github.com/apache/druid/blob/master/extensions-contrib/influx-extensions/src/main/java/org/apache/druid/data/input/influx/InfluxParser.java>
+ *     Apache Druid InfluxDB Parser </a>
+ */
+public class InfluxParser {
+    private static final Pattern BACKSLASH_PATTERN = Pattern.compile(""\\\\\"""");
+    private static final Pattern IDENTIFIER_PATTERN = Pattern.compile(""\\\\([,= ])"");
+
+    @Nullable
+    public DataPoint parseToDataPoint(final String input) throws ParseException {
+        final CharStream charStream = new ANTLRInputStream(input);
+        final InfluxLineProtocolLexer lexer = new InfluxLineProtocolLexer(charStream);
+        final TokenStream tokenStream = new CommonTokenStream(lexer);
+        final InfluxLineProtocolParser parser = new InfluxLineProtocolParser(tokenStream);
+
+        final List<InfluxLineProtocolParser.LineContext> lines = parser.lines().line();
+        if (parser.getNumberOfSyntaxErrors() != 0) {
+            throw new ParseException(""Unable to parse line."", 0);
+        }
+        if (lines.size() != 1) {
+            throw new ParseException(
+                    ""Multiple lines present; unable to parse more than one per record."", 0);
+        }
+
+        final InfluxLineProtocolParser.LineContext line = lines.get(0);
+        final String measurement = this.parseIdentifier(line.identifier());
+
+        final Number timestamp = this.parseTimestamp(line.timestamp());
+
+        final DataPoint out = new DataPoint(measurement, timestamp);
+
+        if (line.tag_set() != null) {
+            line.tag_set().tag_pair().forEach(t -> this.parseTag(t, out));
+        }
+
+        line.field_set().field_pair().forEach(t -> this.parseField(t, out));
+
+        return out;
+    }
+
+    private void parseTag(final InfluxLineProtocolParser.Tag_pairContext tag, final DataPoint out) {
+        final String key = this.parseIdentifier(tag.identifier(0));
+        final String value = this.parseIdentifier(tag.identifier(1));
+        out.addTag(key, value);
+    }
+
+    private void parseField(
+            final InfluxLineProtocolParser.Field_pairContext field, final DataPoint out) {
+        final String key = this.parseIdentifier(field.identifier());
+        final InfluxLineProtocolParser.Field_valueContext valueContext = field.field_value();
+        final Object value;
+        if (valueContext.NUMBER() != null) {
+            value = this.parseNumber(valueContext.NUMBER().getText());
+        } else if (valueContext.BOOLEAN() != null) {
+            value = this.parseBool(valueContext.BOOLEAN().getText());
+        } else {
+            value = this.parseQuotedString(valueContext.QUOTED_STRING().getText());
+        }
+        out.addField(key, value);
+    }
+
+    private Object parseQuotedString(final String text) {","[{'comment': '`String`', 'commenter': 'AHeise'}]"
114,flink-connector-influxdb2/src/main/java/org/apache/flink/streaming/connectors/influxdb/sink/InfluxDBSink.java,"@@ -0,0 +1,90 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements. See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership. The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License. You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.flink.streaming.connectors.influxdb.sink;
+
+import com.influxdb.client.write.Point;
+import java.util.List;
+import java.util.Optional;
+import java.util.Properties;
+import lombok.Getter;
+import org.apache.flink.api.connector.sink.Committer;
+import org.apache.flink.api.connector.sink.GlobalCommitter;
+import org.apache.flink.api.connector.sink.Sink;
+import org.apache.flink.api.connector.sink.SinkWriter;
+import org.apache.flink.core.io.SimpleVersionedSerializer;
+import org.apache.flink.streaming.connectors.influxdb.sink.commiter.InfluxDBCommittableSerializer;
+import org.apache.flink.streaming.connectors.influxdb.sink.commiter.InfluxDBCommitter;
+import org.apache.flink.streaming.connectors.influxdb.sink.writer.InfluxDBPointSerializer;
+import org.apache.flink.streaming.connectors.influxdb.sink.writer.InfluxDBSchemaSerializer;
+import org.apache.flink.streaming.connectors.influxdb.sink.writer.InfluxDBWriter;
+
+public final class InfluxDBSink<IN> implements Sink<IN, Long, Point, Void> {","[{'comment': 'Add docs + usage example (recycle from readme).', 'commenter': 'AHeise'}]"
121,flink-connector-redis/src/main/java/org/apache/flink/streaming/connectors/redis/common/config/FlinkJedisClusterConfig.java,"@@ -179,25 +185,68 @@ public Builder setPassword(String password) {
             return this;
         }
 
+        /**
+         * Sets value for the {@code testOnBorrow} configuration attribute
+         * for pools to be created with this configuration instance.
+         *
+         * @param testOnBorrow Whether objects borrowed from the pool will be validated before being returned
+         * @return Builder itself
+         */
+        public Builder setTestOnBorrow(boolean testOnBorrow) {
+            this.testOnBorrow = testOnBorrow;
+            return this;
+        }
+
+        /**
+         * Sets value for the {@code testOnReturn} configuration attribute
+         * for pools to be created with this configuration instance.
+         *
+         * @param testOnReturn Whether objects borrowed from the pool will be validated when they are returned to the pool
+         * @return Builder itself
+         */
+        public Builder setTestOnReturn(boolean testOnReturn) {
+            this.testOnReturn = testOnReturn;
+            return this;
+        }
+
+        /**
+         * Sets value for the {@code testWhileIdle} configuration attribute
+         * for pools to be created with this configuration instance.
+         *
+         * Setting this to true will also set default idle-testing parameters provided in Jedis
+         * @see redis.clients.jedis.JedisPoolConfig
+         *
+         * @param testWhileIdle Whether objects sitting idle in the pool will be validated by the idle object evictor
+         * @return Builder itself
+         */
+        public Builder setTestWhileIdle(boolean testWhileIdle) {
+            this.testWhileIdle = testWhileIdle;
+            return this;
+        }
+
         /**
          * Builds JedisClusterConfig.
          *
          * @return JedisClusterConfig
          */
         public FlinkJedisClusterConfig build() {
-            return new FlinkJedisClusterConfig(nodes, timeout, maxRedirections, maxTotal, maxIdle, minIdle, password);
+            return new FlinkJedisClusterConfig(nodes, timeout, maxRedirections, maxTotal, maxIdle, minIdle, password, testOnBorrow, testOnReturn, testWhileIdle);
         }
     }
 
     @Override
     public String toString() {
         return ""FlinkJedisClusterConfig{"" +
-            ""nodes="" + nodes +
-            "", timeout="" + connectionTimeout +
-            "", maxRedirections="" + maxRedirections +
-            "", maxTotal="" + maxTotal +
-            "", maxIdle="" + maxIdle +
-            "", minIdle="" + minIdle +
-            '}';
+          ""nodes="" + nodes +
+          "", maxRedirections="" + maxRedirections +
+          "", maxTotal="" + maxTotal +
+          "", maxIdle="" + maxIdle +
+          "", minIdle="" + minIdle +
+          "", connectionTimeout="" + connectionTimeout +
+          "", password='"" + password + '\'' +","[{'comment': ""remove `'\\''`"", 'commenter': 'YikSanChan'}]"
121,flink-connector-redis/src/main/java/org/apache/flink/streaming/connectors/redis/common/config/FlinkJedisPoolConfig.java,"@@ -188,27 +196,69 @@ public Builder setPassword(String password) {
             return this;
         }
 
+        /**
+         * Sets value for the {@code testOnBorrow} configuration attribute
+         * for pools to be created with this configuration instance.
+         *
+         * @param testOnBorrow Whether objects borrowed from the pool will be validated before being returned
+         * @return Builder itself
+         */
+        public Builder setTestOnBorrow(boolean testOnBorrow) {
+            this.testOnBorrow = testOnBorrow;
+            return this;
+        }
+
+        /**
+         * Sets value for the {@code testOnReturn} configuration attribute
+         * for pools to be created with this configuration instance.
+         *
+         * @param testOnReturn Whether objects borrowed from the pool will be validated when they are returned to the pool
+         * @return Builder itself
+         */
+        public Builder setTestOnReturn(boolean testOnReturn) {
+            this.testOnReturn = testOnReturn;
+            return this;
+        }
+
+        /**
+         * Sets value for the {@code testWhileIdle} configuration attribute
+         * for pools to be created with this configuration instance.
+         *
+         * Setting this to true will also set default idle-testing parameters provided in Jedis
+         * @see redis.clients.jedis.JedisPoolConfig
+         *
+         * @param testWhileIdle Whether objects sitting idle in the pool will be validated by the idle object evictor
+         * @return Builder itself
+         */
+        public Builder setTestWhileIdle(boolean testWhileIdle) {
+            this.testWhileIdle = testWhileIdle;
+            return this;
+        }
 
         /**
          * Builds JedisPoolConfig.
          *
          * @return JedisPoolConfig
          */
         public FlinkJedisPoolConfig build() {
-            return new FlinkJedisPoolConfig(host, port, timeout, password, database, maxTotal, maxIdle, minIdle);
+            return new FlinkJedisPoolConfig(host, port, timeout, password, database, maxTotal, maxIdle, minIdle, testOnBorrow, testOnReturn, testWhileIdle);
         }
     }
 
     @Override
     public String toString() {
         return ""FlinkJedisPoolConfig{"" +
-            ""host='"" + host + '\'' +
-            "", port="" + port +
-            "", timeout="" + connectionTimeout +
-            "", database="" + database +
-            "", maxTotal="" + maxTotal +
-            "", maxIdle="" + maxIdle +
-            "", minIdle="" + minIdle +
-            '}';
+          ""host='"" + host + '\'' +
+          "", port="" + port +
+          "", database="" + database +
+          "", maxTotal="" + maxTotal +
+          "", maxIdle="" + maxIdle +
+          "", minIdle="" + minIdle +
+          "", connectionTimeout="" + connectionTimeout +
+          "", password='"" + password + '\'' +","[{'comment': 'ditto', 'commenter': 'YikSanChan'}]"
121,flink-connector-redis/src/main/java/org/apache/flink/streaming/connectors/redis/common/config/FlinkJedisSentinelConfig.java,"@@ -223,27 +230,71 @@ public Builder setMinIdle(int minIdle) {
             return this;
         }
 
+        /**
+         * Sets value for the {@code testOnBorrow} configuration attribute
+         * for pools to be created with this configuration instance.
+         *
+         * @param testOnBorrow Whether objects borrowed from the pool will be validated before being returned
+         * @return Builder itself
+         */
+        public Builder setTestOnBorrow(boolean testOnBorrow) {
+            this.testOnBorrow = testOnBorrow;
+            return this;
+        }
+
+        /**
+         * Sets value for the {@code testOnReturn} configuration attribute
+         * for pools to be created with this configuration instance.
+         *
+         * @param testOnReturn Whether objects borrowed from the pool will be validated when they are returned to the pool
+         * @return Builder itself
+         */
+        public Builder setTestOnReturn(boolean testOnReturn) {
+            this.testOnReturn = testOnReturn;
+            return this;
+        }
+
+        /**
+         * Sets value for the {@code testWhileIdle} configuration attribute
+         * for pools to be created with this configuration instance.
+         *
+         * Setting this to true will also set default idle-testing parameters provided in Jedis
+         * @see redis.clients.jedis.JedisPoolConfig
+         *
+         * @param testWhileIdle Whether objects sitting idle in the pool will be validated by the idle object evictor
+         * @return Builder itself
+         */
+        public Builder setTestWhileIdle(boolean testWhileIdle) {
+            this.testWhileIdle = testWhileIdle;
+            return this;
+        }
+
         /**
          * Builds JedisSentinelConfig.
          *
          * @return JedisSentinelConfig
          */
         public FlinkJedisSentinelConfig build(){
             return new FlinkJedisSentinelConfig(masterName, sentinels, connectionTimeout, soTimeout,
-                password, database, maxTotal, maxIdle, minIdle);
+                password, database, maxTotal, maxIdle, minIdle, testOnBorrow, testOnReturn, testWhileIdle);
         }
     }
 
     @Override
     public String toString() {
         return ""FlinkJedisSentinelConfig{"" +
-            ""masterName='"" + masterName + '\'' +
-            "", connectionTimeout="" + connectionTimeout +
-            "", soTimeout="" + soTimeout +
-            "", database="" + database +
-            "", maxTotal="" + maxTotal +
-            "", maxIdle="" + maxIdle +
-            "", minIdle="" + minIdle +
-            '}';
+          ""masterName='"" + masterName + '\'' +","[{'comment': 'ditto', 'commenter': 'YikSanChan'}]"
121,flink-connector-redis/src/main/java/org/apache/flink/streaming/connectors/redis/common/config/FlinkJedisSentinelConfig.java,"@@ -223,27 +230,71 @@ public Builder setMinIdle(int minIdle) {
             return this;
         }
 
+        /**
+         * Sets value for the {@code testOnBorrow} configuration attribute
+         * for pools to be created with this configuration instance.
+         *
+         * @param testOnBorrow Whether objects borrowed from the pool will be validated before being returned
+         * @return Builder itself
+         */
+        public Builder setTestOnBorrow(boolean testOnBorrow) {
+            this.testOnBorrow = testOnBorrow;
+            return this;
+        }
+
+        /**
+         * Sets value for the {@code testOnReturn} configuration attribute
+         * for pools to be created with this configuration instance.
+         *
+         * @param testOnReturn Whether objects borrowed from the pool will be validated when they are returned to the pool
+         * @return Builder itself
+         */
+        public Builder setTestOnReturn(boolean testOnReturn) {
+            this.testOnReturn = testOnReturn;
+            return this;
+        }
+
+        /**
+         * Sets value for the {@code testWhileIdle} configuration attribute
+         * for pools to be created with this configuration instance.
+         *
+         * Setting this to true will also set default idle-testing parameters provided in Jedis
+         * @see redis.clients.jedis.JedisPoolConfig
+         *
+         * @param testWhileIdle Whether objects sitting idle in the pool will be validated by the idle object evictor
+         * @return Builder itself
+         */
+        public Builder setTestWhileIdle(boolean testWhileIdle) {
+            this.testWhileIdle = testWhileIdle;
+            return this;
+        }
+
         /**
          * Builds JedisSentinelConfig.
          *
          * @return JedisSentinelConfig
          */
         public FlinkJedisSentinelConfig build(){
             return new FlinkJedisSentinelConfig(masterName, sentinels, connectionTimeout, soTimeout,
-                password, database, maxTotal, maxIdle, minIdle);
+                password, database, maxTotal, maxIdle, minIdle, testOnBorrow, testOnReturn, testWhileIdle);
         }
     }
 
     @Override
     public String toString() {
         return ""FlinkJedisSentinelConfig{"" +
-            ""masterName='"" + masterName + '\'' +
-            "", connectionTimeout="" + connectionTimeout +
-            "", soTimeout="" + soTimeout +
-            "", database="" + database +
-            "", maxTotal="" + maxTotal +
-            "", maxIdle="" + maxIdle +
-            "", minIdle="" + minIdle +
-            '}';
+          ""masterName='"" + masterName + '\'' +
+          "", sentinels="" + sentinels +
+          "", soTimeout="" + soTimeout +
+          "", database="" + database +
+          "", maxTotal="" + maxTotal +
+          "", maxIdle="" + maxIdle +
+          "", minIdle="" + minIdle +
+          "", connectionTimeout="" + connectionTimeout +
+          "", password='"" + password + '\'' +","[{'comment': 'ditto', 'commenter': 'YikSanChan'}]"
121,flink-connector-redis/src/main/java/org/apache/flink/streaming/connectors/redis/common/container/RedisCommandsContainerBuilder.java,"@@ -64,9 +65,17 @@ public static RedisCommandsContainer build(FlinkJedisPoolConfig jedisPoolConfig)
 
         GenericObjectPoolConfig genericObjectPoolConfig = getGenericObjectPoolConfig(jedisPoolConfig);
 
+        if (jedisPoolConfig.getTestWhileIdle()) {
+            // default parameters from redis.clients.jedis.JedisPoolConfig
+            genericObjectPoolConfig.setTestWhileIdle(true);
+            genericObjectPoolConfig.setMinEvictableIdleTimeMillis(60000);
+            genericObjectPoolConfig.setTimeBetweenEvictionRunsMillis(30000);
+            genericObjectPoolConfig.setNumTestsPerEvictionRun(-1);
+        }","[{'comment': 'This is no longer needed since they are covered by `getGenericObjectPoolConfig`', 'commenter': 'YikSanChan'}]"
121,flink-connector-redis/src/main/java/org/apache/flink/streaming/connectors/redis/common/container/RedisCommandsContainerBuilder.java,"@@ -82,12 +91,20 @@ public static RedisCommandsContainer build(FlinkJedisClusterConfig jedisClusterC
 
         GenericObjectPoolConfig genericObjectPoolConfig = getGenericObjectPoolConfig(jedisClusterConfig);
 
+        if (jedisClusterConfig.getTestWhileIdle()) {
+            // default parameters from redis.clients.jedis.JedisPoolConfig
+            genericObjectPoolConfig.setTestWhileIdle(true);
+            genericObjectPoolConfig.setMinEvictableIdleTimeMillis(60000);
+            genericObjectPoolConfig.setTimeBetweenEvictionRunsMillis(30000);
+            genericObjectPoolConfig.setNumTestsPerEvictionRun(-1);
+        }","[{'comment': 'ditto', 'commenter': 'YikSanChan'}]"
121,flink-connector-redis/src/main/java/org/apache/flink/streaming/connectors/redis/common/container/RedisCommandsContainerBuilder.java,"@@ -103,18 +120,29 @@ public static RedisCommandsContainer build(FlinkJedisSentinelConfig jedisSentine
 
         GenericObjectPoolConfig genericObjectPoolConfig = getGenericObjectPoolConfig(jedisSentinelConfig);
 
+        if (jedisSentinelConfig.getTestWhileIdle()) {
+            // default parameters from redis.clients.jedis.JedisPoolConfig
+            genericObjectPoolConfig.setTestWhileIdle(true);
+            genericObjectPoolConfig.setMinEvictableIdleTimeMillis(60000);
+            genericObjectPoolConfig.setTimeBetweenEvictionRunsMillis(30000);
+            genericObjectPoolConfig.setNumTestsPerEvictionRun(-1);
+        }","[{'comment': 'ditto', 'commenter': 'YikSanChan'}]"
121,flink-connector-redis/src/main/java/org/apache/flink/streaming/connectors/redis/common/config/FlinkJedisConfigBase.java,"@@ -91,6 +99,18 @@ public int getMinIdle() {
         return minIdle;
     }
 
+    public boolean isTestOnBorrow() {","[{'comment': 'we have the getTestOnBorrow could we use that and eliminate this', 'commenter': 'eskabetxe'}]"
130,.github/workflows/maven-ci.yml,"@@ -45,4 +45,4 @@ jobs:
       run: ./dev/change-scala-version.sh ${{ matrix.scala-version }}
       shell: bash
     - name: Build with flink ${{ matrix.flink-version }}
-      run: mvn -B clean verify -Dscala-${{ matrix.scala-version }} -Dflink.version=${{ matrix.flink-version }}","[{'comment': 'why?', 'commenter': 'eskabetxe'}]"
130,flink-connector-redis/src/test/java/org/apache/flink/streaming/connectors/redis/RedisSinkITCase.java,"@@ -77,14 +77,20 @@ public void testRedisListDataType() throws Exception {
     @Test
     public void testRedisSetDataType() throws Exception {
         DataStreamSource<Tuple2<String, String>> source = env.addSource(new TestSourceFunction());
-        RedisSink<Tuple2<String, String>> redisSink = new RedisSink<>(jedisPoolConfig,
+        RedisSink<Tuple2<String, String>> redisSaddSink = new RedisSink<>(jedisPoolConfig,
             new RedisCommandMapper(RedisCommand.SADD));
 
-        source.addSink(redisSink);
-        env.execute(""Test Redis Set Data Type"");
+        source.addSink(redisSaddSink);
+        env.execute(""Test SADD"");
 
         assertEquals(NUM_ELEMENTS, jedis.scard(REDIS_KEY));
 
+        RedisSink<Tuple2<String, String>> redisSremSink = new RedisSink<>(jedisPoolConfig,","[{'comment': 'could we add a new test only to remove?', 'commenter': 'eskabetxe'}, {'comment': ""\r\n\r\n\r\n> Could you separate in two commits one for each command added?\r\n\r\nSorry, It's a mistake, I'm new to github. 😅\r\nI will close this PR and create a new one in future.\r\n\r\nThanks for review."", 'commenter': 'wyxpku'}]"
136,flink-connector-redis/src/test/java/org/apache/flink/streaming/connectors/redis/common/RedisSinkZIncrByTest.java,"@@ -1,121 +0,0 @@
-/*","[{'comment': 'why we delete this test?', 'commenter': 'eskabetxe'}]"
136,flink-connector-activemq/pom.xml,"@@ -35,7 +35,7 @@ under the License.
 
     <!-- Allow users to pass custom connector versions -->
     <properties>
-        <activemq.version>5.15.10</activemq.version>
+        <activemq.version>5.16.2</activemq.version>","[{'comment': 'is this mandatory for flink update?\r\nif not will be better in other commit, only with this update (and related)', 'commenter': 'eskabetxe'}]"
136,flink-connector-flume/pom.xml,"@@ -36,7 +36,7 @@ under the License.
 	<!-- Allow users to pass custom connector versions -->
 	<properties>
 		<flume-ng.version>1.9.0</flume-ng.version>
-		<testcontainers.version>1.15.3</testcontainers.version>
+		<testcontainers.version>1.16.2</testcontainers.version>","[{'comment': 'is this mandatory for flink update?\r\nif not will be better in other commit, only with this update (and related)', 'commenter': 'eskabetxe'}]"
136,flink-connector-activemq/pom.xml,"@@ -101,14 +101,14 @@ under the License.
         <dependency>
             <groupId>org.powermock</groupId>
             <artifactId>powermock-module-junit4</artifactId>
-            <version>1.5.5</version>
+            <version>2.0.9</version>","[{'comment': 'is this mandatory for flink update?\r\nif not will be better in other commit, only for powermock update (and related)', 'commenter': 'eskabetxe'}]"
136,flink-connector-influxdb2/pom.xml,"@@ -39,7 +39,7 @@ under the License.
     <influxdbClient.version>2.0.0</influxdbClient.version>
     <druid.version>0.13.0-incubating</druid.version>
     <!--  Test Properties  -->
-    <testcontainers.version>1.15.2</testcontainers.version>
+    <testcontainers.version>1.16.2</testcontainers.version>","[{'comment': 'is this mandatory for flink update?\r\nif not will be better in other commit, only with this update (and related)', 'commenter': 'eskabetxe'}]"
136,flink-connector-redis/pom.xml,"@@ -34,7 +34,7 @@ under the License.
     <packaging>jar</packaging>
 
     <properties>
-        <jedis.version>2.9.0</jedis.version>
+        <jedis.version>3.7.1</jedis.version>","[{'comment': 'is this mandatory for flink update?\r\nif not will be better in other commit, only with this update (and related)', 'commenter': 'eskabetxe'}]"
136,flink-connector-redis/src/test/java/org/apache/flink/streaming/connectors/redis/RedisDescriptorTest.java,"@@ -51,23 +46,26 @@ public void testRedisDescriptor() throws Exception {
 
         EnvironmentSettings settings = EnvironmentSettings
                 .newInstance()
-                .useOldPlanner()
                 .inStreamingMode()
                 .build();
         StreamTableEnvironment tableEnvironment = StreamTableEnvironment.create(env, settings);
         tableEnvironment.registerDataStream(""t1"", source, ""k, v"");
 
-        Redis redis = new Redis()
+        /*Redis redis = new Redis()","[{'comment': 'this comment could be removed no?', 'commenter': 'eskabetxe'}]"
156,flink-connector-redis/src/main/java/org/apache/flink/streaming/connectors/redis/common/container/RedisClusterContainer.java,"@@ -46,15 +46,7 @@ public RedisClusterContainer(JedisCluster jedisCluster) {
         this.jedisCluster = jedisCluster;
     }
 
-    @Override
-    public void open() throws Exception {
-
-        // echo() tries to open a connection and echos back the
-        // message passed as argument. Here we use it to monitor
-        // if we can communicate with the cluster.
-
-        jedisCluster.echo(""Test"");
-    }
+    public void open() {}","[{'comment': 'Could you leave de override?\r\n\r\nthe ""communication test"" is not needed anymore?', 'commenter': 'eskabetxe'}, {'comment': 're:override, sure, let me do that!\r\n\r\nI\'ll take a look at the ""communication test""\r\n', 'commenter': 'srmrz'}, {'comment': '@eskabetxe, added override here [94e2078](https://github.com/apache/bahir-flink/pull/156/commits/94e2078feec7b98d22a083b23c9a826a836c6d80)\r\n\r\nMind pointing me at the communication test?', 'commenter': 'srmrz'}, {'comment': '@srmrz ""jedisCluster.echo(""Test"");""\r\nI don\'t know if is needed.. but for the comment its testing the communication with cluster..\r\nI don\'t write this code, I\'m trying to understand if removing that code could have some side effect..\r\n', 'commenter': 'eskabetxe'}, {'comment': ""@eskabetxe I see! \r\n\r\n> I don't know if is needed\r\n\r\nMy take is that it isn't needed - the constructor takes a `JedisCluster`. \r\n\r\nAs part of the initialization of `JedisCluster` , it contacts the cluster:\r\n\r\nhttps://github.com/redis/jedis/blob/379bbed0ee363bfc9f31e4591f5d46c28976cff5/src/main/java/redis/clients/jedis/UnifiedJedis.java#L105-L107\r\n\r\nhttps://github.com/redis/jedis/blob/379bbed0ee363bfc9f31e4591f5d46c28976cff5/src/main/java/redis/clients/jedis/providers/ClusterConnectionProvider.java#L27\r\n\r\nhttps://github.com/redis/jedis/blob/379bbed0ee363bfc9f31e4591f5d46c28976cff5/src/main/java/redis/clients/jedis/providers/ClusterConnectionProvider.java#L36-L49\r\n\r\nThat runs commands in the cluster. So if there are problems contacting the cluster, we'll know before we even construct a `RedisClusterContainer`"", 'commenter': 'srmrz'}, {'comment': 'Checking in on this. Let me know if there are other questions', 'commenter': 'srmrz'}]"
166,flink-connector-kudu/src/main/java/org/apache/flink/connectors/kudu/format/AbstractKuduInputFormat.java,"@@ -113,10 +120,7 @@ public void close() throws IOException {
                 e.printStackTrace();","[{'comment': 'O know this is unrelated, but can we change this to a log?', 'commenter': 'eskabetxe'}, {'comment': 'sure.', 'commenter': 'collabH'}]"
