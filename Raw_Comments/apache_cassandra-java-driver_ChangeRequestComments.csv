Pull,Path,Diff_hunk,Comment
718,driver-core/src/main/java/com/datastax/driver/core/Connection.java,"@@ -243,7 +246,8 @@ private static String extractMessage(Throwable t) {
                         // Testing for a specific string is a tad fragile but well, we don't have much choice
                         // C* 2.1 reports a server error instead of protocol error, see CASSANDRA-9451
                         if ((error.code == ExceptionCode.PROTOCOL_ERROR || error.code == ExceptionCode.SERVER_ERROR) &&
-                                error.message.contains(""Invalid or unsupported protocol version""))
+                                error.message.contains(""Invalid or unsupported protocol version"") ||","[{'comment': 'I think the precedence of boolean operators would require parentheses here:\n\n```\n(\n    error.code == ExceptionCode.PROTOCOL_ERROR || \n    error.code == ExceptionCode.SERVER_ERROR\n) && (\n    error.message.contains(""Invalid or unsupported protocol version"") || \n    error.message.contains(""Beta version of the protocol used"")\n)\n```\n\nOr is it enough to detect the `""Beta version of the protocol used""` message, regardless of the error code?\n', 'commenter': 'adutra'}, {'comment': ""You're right, it's much better to check error code. \n"", 'commenter': 'ifesdjeen'}]"
718,driver-core/src/main/java/com/datastax/driver/core/Connection.java,"@@ -1319,6 +1327,10 @@ protected void initChannel(SocketChannel channel) throws Exception {
             pipeline.addLast(""frameDecoder"", new Frame.Decoder());
             pipeline.addLast(""frameEncoder"", frameEncoder);
 
+            if (useBeta) {
+                pipeline.addLast(new Frame.BetaVersionFlagAdder());","[{'comment': ""Very minor suggestions:\n1. I don't know the practical implications of _not_ giving a name to a handler, but since all the other handlers are given names in this method, I'd suggest we name this new one too.\n2. Since this handler is stateless, couldn't we make it a singleton, just like `Frame.Encoder`? Also it would be good to annotate it with `@ChannelHandler.Sharable`.\n"", 'commenter': 'adutra'}]"
718,driver-core/src/main/java/com/datastax/driver/core/Frame.java,"@@ -175,7 +178,10 @@ static int lengthFor(ProtocolVersion version) {
                 return set;
             }
 
-            static int serialize(EnumSet<Flag> flags) {
+            static int serialize(EnumSet<Flag> flags, ProtocolVersion protocolVersion) {
+                if (protocolVersion.toInt() < ProtocolVersion.V5.toInt() && flags.contains(USE_BETA))","[{'comment': 'I think we have a problem here. `Frame.BetaVersionFlagAdder` is executed _after_  `Frame.Encoder` so the flag will never be set at this point. Maybe a better approach is to move this check to `Frame.BetaVersionFlagAdder`.\n', 'commenter': 'adutra'}, {'comment': 'Actually it is not a good idea to throw exceptions from channel handlers as these would defunct the connection and throw a rather cryptic error message (""Error writing"").\nSo instead I would suggest moving the check to `Frame.BetaVersionFlagAdder` but in a more lenient version:\n\n``` java\nstatic class BetaVersionFlagAdder extends MessageToMessageEncoder<Frame> {\n    @Override\n    protected void encode(ChannelHandlerContext ctx, Frame frame, List<Object> out) throws Exception {\n        ProtocolVersion protocolVersion = frame.header.version;\n        if (protocolVersion.toInt() >= ProtocolVersion.V5.toInt())\n            frame.header.flags.add(USE_BETA);\n        out.add(frame);\n    }\n}\n```\n\nOne might wonder why the check is required at all, since this handler is only added to the pipeline if the user requests the `USE_BETA` flag explicitly. It is still required in the case of renegotiation: e.g. the driver tries `V5` + `USE_BETA`, gets a `PROTOCOL_ERROR` because the server only handles `V4`, and then retries with `V4`: in this case, because `USE_BETA` is not supported in `V4`, the handler should check the version and refrain itself from adding the flag.\n', 'commenter': 'adutra'}]"
718,driver-core/src/main/java/com/datastax/driver/core/ProtocolOptions.java,"@@ -231,4 +232,23 @@ public AuthProvider getAuthProvider() {
         return authProvider;
     }
 
+    /**
+     * Allows client to connect to server using latest development protocol version,
+     * which is currently in beta.
+     *
+     * @param useBeta
+     * @return this {@code ProtocolOptions} object.
+     */
+    public ProtocolOptions setAllowBetaProtocolVersions(boolean useBeta) {
+        this.useBeta = useBeta;
+        return this;
+    }
+
+    /**
+     * Whether or not connecting to server with protocol version which is currently in beta is allowed.
+     * @return {@code true} if connecting with beta protocol version is allowed, {@code false} otherwise.
+     */
+    public boolean getAllowBetaProtocolVersions() {","[{'comment': 'The correct name would be `isAllowBetaProtocolVersions()`.\n', 'commenter': 'adutra'}, {'comment': ""Sorry, didn't notice / know about this convention.\n"", 'commenter': 'ifesdjeen'}]"
718,driver-core/src/main/java/com/datastax/driver/core/ProtocolOptions.java,"@@ -231,4 +232,23 @@ public AuthProvider getAuthProvider() {
         return authProvider;
     }
 
+    /**
+     * Allows client to connect to server using latest development protocol version,
+     * which is currently in beta.
+     *
+     * @param useBeta
+     * @return this {@code ProtocolOptions} object.
+     */
+    public ProtocolOptions setAllowBetaProtocolVersions(boolean useBeta) {","[{'comment': ""I don't think we can change this value once the `Cluster` is created, because we would have to remove the `Frame.BetaVersionFlagAdder` handler from all channels. So I'd suggest we make this field `final` and remove this setter.\n"", 'commenter': 'adutra'}, {'comment': ""true, I've only added it here because I wanted to reduce the amount of constructors. But you're right, it's better to do it here.\n"", 'commenter': 'ifesdjeen'}]"
718,driver-core/src/main/java/com/datastax/driver/core/Cluster.java,"@@ -744,6 +745,22 @@ public Builder withPort(int port) {
         }
 
         /**
+         * Allows client to connect to server using latest development protocol version,
+         * which is currently in beta.
+         * <p/>
+         * Use with caution, refer to the server and protocol documentation for the details
+         * on latest protocol version.
+         *
+         * @param allowBetaProtocolVersions {@code true} if connecting with beta protocol version should be allowed,
+         *                                  {@code false} otherwise
+         * @return this Builder.
+         */
+        public Builder setAllowBetaProtocolVersions(boolean allowBetaProtocolVersions) {","[{'comment': 'Following the example of other boolean properties in this builder, I would suggest turning this method into a more simple `allowBetaProtocolVersions()` that would internally set the corresponding flag to `true` (with no possibility of turning back to `false`).\n', 'commenter': 'adutra'}, {'comment': 'Also, as a convenience for our users, we might want to cross-check invalid situations such as:\n\n``` java\nCluster cluster = Cluster.builder()\n        .addContactPoint(...)\n        .withProtocolVersion(V4)\n        .setAllowBetaProtocolVersions(true)\n        .build();\n```\n\n...and throw an `IllegalArgumentException` right away instead of letting the user proceed.\n', 'commenter': 'adutra'}]"
718,driver-core/src/main/java/com/datastax/driver/core/Cluster.java,"@@ -744,6 +745,22 @@ public Builder withPort(int port) {
         }
 
         /**
+         * Allows client to connect to server using latest development protocol version,
+         * which is currently in beta.","[{'comment': 'Maybe add here some explanation about what are the implications of calling this method (i.e. the flag `USE_BETA` would be added to all outgoing messages) and also warn that it only makes sense for protocol versions >= 5.\n', 'commenter': 'adutra'}]"
718,driver-core/src/main/java/com/datastax/driver/core/Cluster.java,"@@ -784,6 +803,11 @@ public Builder withMaxSchemaAgreementWaitSeconds(int maxSchemaAgreementWaitSecon
          * (the default), then the native protocol version 1 will be use for the lifetime
          * of the Cluster instance.
          * <p/>
+         * With {@link ProtocolOptions#setAllowBetaProtocolVersions(boolean)}, it is","[{'comment': 'If the referenced method is deleted as I suggested, please delete this paragraph as well (or rephrase accordingly).\n', 'commenter': 'adutra'}]"
718,driver-core/src/test/java/com/datastax/driver/core/ProtocolBetaVersionTest.java,"@@ -0,0 +1,123 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.core;
+
+import com.datastax.driver.core.exceptions.NoHostAvailableException;
+import com.datastax.driver.core.exceptions.UnsupportedProtocolVersionException;
+import org.testng.annotations.Test;
+
+import static com.datastax.driver.core.ProtocolVersion.V4;
+import static com.datastax.driver.core.ProtocolVersion.V5;
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.assertj.core.api.Assertions.fail;
+
+/**
+ * Tests for the new USE_BETA flag introduced in protocol v5
+ * and Cassandra 3.10.
+ */
+@CCMConfig(createCluster = false, version = ""3.10"")","[{'comment': 'This test requires  version 3.10 _precisely_, just as `ProtocolV1Test` requires 1.2.19.\n\nUntil 3.10 is released, here is what you should do to make the tests pass:\n\n```\ncd /path/to/cassandra/local/git/repo\ngit fetch\ngit checkout 12142-trunk\nant build\ncd ~/.ccm/repository\nln -s /path/to/cassandra/local/git/repo 3.10\n```\n\nNote that you will have to delete the symlink after 3.10 is released.\n', 'commenter': 'adutra'}, {'comment': 'Since the branch is merged, one can already use the snapshot version or trunk, so you can do something like:\n\n```\ncd /tmp\nwget http://cassci.datastax.com/job/trunk/lastSuccessfulBuild/artifact/build/apache-cassandra-3.10-SNAPSHOT-src.tar.gz\ntar -xvf apache-cassandra-3.10-SNAPSHOT-bin.tar.gz\ncd ~/.ccm/repository\nln -s /tmp/apache-cassandra-3.10-SNAPSHOT-bin/ 3.10\n```\n\n(with auto-deleteâ„¢)\n', 'commenter': 'ifesdjeen'}]"
718,driver-core/src/test/java/com/datastax/driver/core/ProtocolBetaVersionTest.java,"@@ -15,48 +15,109 @@
  */
 package com.datastax.driver.core;
 
-import org.junit.experimental.runners.Enclosed;
-import org.junit.runner.RunWith;
+import com.datastax.driver.core.exceptions.NoHostAvailableException;
+import com.datastax.driver.core.exceptions.UnsupportedProtocolVersionException;
 import org.testng.annotations.Test;
 
-import static org.junit.Assert.assertEquals;
+import static com.datastax.driver.core.ProtocolVersion.V4;
+import static com.datastax.driver.core.ProtocolVersion.V5;
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.assertj.core.api.Assertions.fail;
 
-@RunWith(Enclosed.class)
-public class ProtocolBetaVersionTest {
+/**
+ * Tests for the new USE_BETA flag introduced in protocol v5
+ * and Cassandra 3.10.
+ */
+@CCMConfig(createCluster = false, version = ""3.10"")
+public class ProtocolBetaVersionTest extends CCMTestsSupport {
 
-    public static class TestAutoUpgrade extends BetaVersionTest {
-        @Override
-        public Cluster.Builder createClusterBuilder() {
-            return super.createClusterBuilder().setAllowBetaProtocolVersions(false);
-        }
+    /**
+     * Verifies that the driver can connect to 3.10 with the following combination of options:
+     * Version V5
+     * Flag SET
+     * Expected version: V5
+     *
+     * @jira_ticket JAVA-1248
+     */
+    @Test(groups = ""short"")
+    public void should_connect_with_beta_when_beta_version_explicitly_required_and_flag_set() throws Exception {","[{'comment': 'very good test, along with unset flag one.\n', 'commenter': 'ifesdjeen'}]"
718,driver-core/src/main/java/com/datastax/driver/core/ProtocolOptions.java,"@@ -135,11 +136,30 @@ public ProtocolOptions(int port) {
      *                        the Cassandra nodes.
      */
     public ProtocolOptions(int port, ProtocolVersion protocolVersion, int maxSchemaAgreementWaitSeconds, SSLOptions sslOptions, AuthProvider authProvider) {
+        this(port, protocolVersion, maxSchemaAgreementWaitSeconds, sslOptions, authProvider, false);
+    }
+
+    /**
+     * Creates a new {@code ProtocolOptions} instance using the provided port
+     * and SSL context.
+     *
+     * @param port            the port to use for the binary protocol.
+     * @param protocolVersion the protocol version to use. This can be {@code null}, in which case the
+     *                        version used will be the biggest version supported by the <em>first</em> node the driver connects to.
+     *                        See {@link Cluster.Builder#withProtocolVersion} for more details.
+     * @param sslOptions      the SSL options to use. Use {@code null} if SSL is not
+     *                        to be used.
+     * @param authProvider    the {@code AuthProvider} to use for authentication against
+     *                        the Cassandra nodes.
+     * @param useBeta         whether or not this client is allowed to connect to servers with protocol versions that are in beta.
+     */
+    public ProtocolOptions(int port, ProtocolVersion protocolVersion, int maxSchemaAgreementWaitSeconds, SSLOptions sslOptions, AuthProvider authProvider, boolean useBeta) {","[{'comment': ""Those constructors are really annoying because each new setting requires a new constructor. I wonder if we shouldn't introduce a builder pattern, turn this constructor private and mark the other as `@Deprecated`. This way the private constructor is not part of the API and can be modified any time without risking to break backwards compatibility.\n"", 'commenter': 'adutra'}, {'comment': ""I'm not sure, it sounds a bit overkill. By hiding the constructors we also prevent extension of the class (that's an edge case, but we've had users extend these classes to add their own options).\nOn the other hand I get your point about extensibility. But we rarely add new protocol options.\n"", 'commenter': 'olim7t'}, {'comment': ""Fair enough, let's keep it that way.\n"", 'commenter': 'adutra'}]"
718,driver-core/src/main/java/com/datastax/driver/core/ProtocolOptions.java,"@@ -231,4 +251,11 @@ public AuthProvider getAuthProvider() {
         return authProvider;
     }
 
+    /**
+     * Whether or not connecting to server with protocol version which is currently in beta is allowed.
+     * @return {@code true} if connecting with beta protocol version is allowed, {@code false} otherwise.
+     */
+    public boolean isAllowBetaProtocolVersions() {","[{'comment': ""I'm not a big proponent of `isXxx` in all cases, here I think `allowsBetaProtocolVersions` would be easier to read.\n"", 'commenter': 'olim7t'}, {'comment': ""Good point. I'm pretty sure @adutra mentioned it in a different context`allowsBetaProtocolVersions`, sorry I missed it.\n"", 'commenter': 'ifesdjeen'}]"
718,driver-core/src/main/java/com/datastax/driver/core/Frame.java,"@@ -328,4 +332,16 @@ protected void encode(ChannelHandlerContext ctx, Frame frame, List<Object> out)
             }
         }
     }
+
+    @ChannelHandler.Sharable
+    static class BetaVersionFlagAdder extends MessageToMessageEncoder<Frame> {
+        @Override
+        protected void encode(ChannelHandlerContext ctx, Frame frame, List<Object> out) throws Exception {
+            ProtocolVersion protocolVersion = frame.header.version;
+            // Check for the protocol version, if the downgrade has occurred during renegotiation
+            if (protocolVersion.toInt() >= ProtocolVersion.V5.toInt())
+                frame.header.flags.add(Header.Flag.USE_BETA);","[{'comment': ""I don't understand what's happening here: it seems like there's a case where outgoing frames did not contain the beta flag and we add it on the fly. How can that be? Either the user set the beta flag in their config and it should already be set on the frame, or they didn't and forcing it transparently sounds wrong (user wouldn't realize they're in beta).\n"", 'commenter': 'olim7t'}, {'comment': 'Possibly related to this: how does auto-negotiation behave if\n- driver is an older version that supports a beta v5\n- Cassandra is a newer version that has the complete v5\n- you set allowBetas = false\n\nCould negotiation somehow succeed and result in v5/no beta? That would be bad because the driver could fail on some of the not-yet-implemented-on-the-client v5 features.\n\nI feel like the driver should have the notion of whether its latest supported version is fully implemented or not (and not even allow non-beta attempts if it is not).\n', 'commenter': 'olim7t'}, {'comment': '> Possibly related to this: how does auto-negotiation behave if\n\nThis is what we currently have. The initial patch was adding `beta` to the protocol version, although we discussed and decided to go the protocol-negotiation path. \n\n> outgoing frames did not contain the beta flag and we add it on the fly\n\nThat was a bit easier than having ProtocolOptions (or that flag) passed through to `ProtocolEncoder` (since it\'s currently static). \n\nWhat we can do is adding a ""soft"" coding to fix both issues. If we know that driver supports v5 only in beta, we\'d throw a configuration exception in cluster builder. Same simplification can be done in `ProtocolEncoder`, where we can just hardwire beta flag for all v5 messages on creation instead of having it added dynamically.\n\ncc @adutra \n', 'commenter': 'ifesdjeen'}, {'comment': '> Could negotiation somehow succeed and result in v5/no beta? That would be bad because the driver could fail on some of the not-yet-implemented-on-the-client v5 features.\n\nIndeed you have a point here.\n\nSome background first: so far we\'ve been working on the following assumptions:\n1. The driver should have `Protocol.NEWEST_SUPPORTED` pointing to the highest version available, even if it is a beta one.\n2. Suppose that current driver has versions: X beta, X-1 stable\n3. `Protocol.NEWEST_SUPPORTED` points to X, which is beta\n4. Driver requests connection with `Protocol.NEWEST_SUPPORTED` = X (beta)\n\nExpected Results:\n\n<table>\n<tr>\n<th></th>\n<th>1. Without USE_BETA flag</th>\n<th>2. With USE_BETA flag</th>\n</tr>\n<tr>\n<th>A. X is beta server-side</th>\n<td>Connects with X-1 (after renegotiation) <br/> :white_check_mark:</td>\n<td>Connects with X <br/> :white_check_mark:</td>\n</tr>\n<tr>\n<th>B. X is not beta server-side</th>\n<td>Connects with X <br/> :x:</td>\n<td>Connects with X <br/> :warning:</td>\n</tr>\n</table>\n\n\nAs we can see, our initial assumptions had problems in some cases: if the server is newer and has X stable, our driver would most likely fail to connect if `USE_BETA` was not requested (case B1). It would probably also fail to connect even if the flag was requested (case B2), but that\'s more acceptable because `USE_BETA` implies ""Bad Things â„¢ï¸  can happen"".\n\nSo I would suggest the following improvements:\n1. It would indeed be better if the driver completely prevent B1 from happening at all. That would mean the driver would have to store information about beta versions:\n   1. `Protocol.NEWEST_SUPPORTED` would be the last stable (V4)\n   2. `Protocol.NEWEST_BETA` would be the last beta, or null\n   3. If the user does not specify a version and do not call `allowBetaVersions()`, use `Protocol.NEWEST_SUPPORTED`\n   4. If the user does not specify a version and _does_ call `allowBetaVersions()`, use `Protocol.NEWEST_BETA`, if not null, or `Protocol.NEWEST_SUPPORTED` otherwise\n2. `BetaVersionFlagAdder` might become obsolete in this new design; the flag could be added in `ProtocolEncoder` and `com.datastax.driver.core.Message.Request` could convey the request for the flag, pretty much like `isTracingRequested()`.\n\nAn alternative solution would be to have the _server_ reject B1 and B2 on the grounds that from its point of view, X is not beta. That would also simplify things a lot, but I don\'t know if it is still time to amend the code server-side.\n', 'commenter': 'adutra'}, {'comment': '> our driver would most likely fail to connect if USE_BETA was not requested (case B1)\n\nI\'m not sure how the current version of the PR behaves, but what I would expect is ""connect with X-1"" (the highest stable supported by both sides).\n\n> It would probably also fail to connect even if the flag was requested (case B2), but that\'s more acceptable\n\nAgreed. That also applies to the case where the server is on a _different_ X-beta -- i.e. more beta features were added to X since the driver started supporting it (see my [comment on 10786](https://issues.apache.org/jira/browse/CASSANDRA-10786?focusedCommentId=15344506&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15344506)).\n\n> Request could convey the request for the flag, pretty much like isTracingRequested()\n\nYes, that would avoid the additional handler.\n\n> If the user does not specify a version and do not call allowBetaVersions(), use Protocol.NEWEST_SUPPORTED\n\nWouldn\'t it be slightly simpler if we didn\'t allow auto-negotiation with beta versions?\nIf you want to use a beta, you would have to specify the version as well (we could even have an alternate method `Cluster.Builder#withBetaProtocolVersion(ProtocolVersion)`).\nWith auto-negotiation, you would always end up with a stable.\n', 'commenter': 'olim7t'}, {'comment': ""> Wouldn't it be slightly simpler if we didn't allow auto-negotiation with beta versions?\n> If you want to use a beta, you would have to specify the version as well (we could even have an alternate method `Cluster.Builder#withBetaProtocolVersion(ProtocolVersion)`).\n\nYes we could, and that was close to what @ifesdjeen had in his early versions. I found it a bit unfriendly as that would throw an `UnsupportedProtocolVersionException` back to the user in case A1 in my table above. So I suggested instead the opposite as I thought it would be interesting for users to simply declare that they are willing to use beta versions (and cope with the consequences), without actually specifying the version. This way renegotiation would take care of downgrading the version gracefully if need be.\n\nBut long story short, I'm fine if we decide to go down the path you suggested, specially if it simplifies the code.\n"", 'commenter': 'adutra'}, {'comment': ""Considering this from the Cassandra ticket:\n\n> This is primarily useful for driver authors to start work against a new protocol version when the work on that spans multiple releases. Users would not generally be expected to utilize this flag, although it could potentially be used to offer early feedback on new protocol features.\n\nIt's fair to assume that clients requesting USE_BETA will always have a specific version in mind, so negotiation is not needed. I vote for taking the simple path and not allowing negotiation with betas.\n\n> that would throw [...] in case A1 in my table above\n\nEven with the above suggestion it shouldn't. If X is beta server-side and the driver requests negotiation (hence without USE_BETA), I would expect X-1 as a result. In other words, negotiation would result in the highest common stable.\n"", 'commenter': 'olim7t'}]"
718,driver-core/src/main/java/com/datastax/driver/core/Cluster.java,"@@ -808,6 +837,12 @@ public Builder withMaxSchemaAgreementWaitSeconds(int maxSchemaAgreementWaitSecon
          * @return this Builder.
          */
         public Builder withProtocolVersion(ProtocolVersion version) {
+            if (allowBetaProtocolVersion)
+                throw new IllegalStateException(""Can not set the version explicitly if `allowBetaProtocolVersion` was used."");
+            if (version.toInt() > ProtocolVersion.NEWEST_SUPPORTED.toInt())","[{'comment': 'Small nit: I would prefer `version.compareTo(ProtocolVersion.NEWEST_SUPPORTED) > 0`\n', 'commenter': 'olim7t'}]"
718,driver-core/src/test/java/com/datastax/driver/core/ProtocolBetaVersionTest.java,"@@ -0,0 +1,152 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.core;
+
+import org.testng.annotations.Test;
+
+import static com.datastax.driver.core.ProtocolVersion.V4;
+import static com.datastax.driver.core.ProtocolVersion.V5;
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.assertj.core.api.Assertions.fail;
+
+/**
+ * Tests for the new USE_BETA flag introduced in protocol v5
+ * and Cassandra 3.10.
+ */
+@CCMConfig(version = ""3.10"")","[{'comment': ""Unfortunately this doesn't work at the moment since 3.10 isn't released (yet) and the `cassandra-3.10` branch is gone and the tag isn't there yet.  However, ðŸ‘ on leaving it as is as it will eventually work and we can verify manually until then."", 'commenter': 'tolbertam'}]"
732,upgrade_guide/migrating_from_astyanax/configuration/README.md,"@@ -0,0 +1,243 @@
+# Configuration
+
+## How Configuring the Java driver works
+
+The two basic components in the Java driver are the `Cluster` and the `Session`.
+The `Cluster` is the object to create first and on which to will apply all ","[{'comment': 'This needs to be rephrased: ""on which to will apply""\n', 'commenter': 'adutra'}]"
732,upgrade_guide/migrating_from_astyanax/configuration/README.md,"@@ -0,0 +1,243 @@
+# Configuration
+
+## How Configuring the Java driver works
+
+The two basic components in the Java driver are the `Cluster` and the `Session`.
+The `Cluster` is the object to create first and on which to will apply all 
+global configuration options. Connecting to the `Cluster` creates a
+`Session`. Queries are executed through the `Session`.
+
+The `Cluster` object then is to be viewed as the equivalent of the `AstyanaxContext`
+object. â€™Startingâ€™ an `AstyanaxContext` object typically returns a `Keyspace`
+object, the `Keyspace` object is the equivalent of the *Java driver*â€™s `Session`.
+
+Configuring a `Cluster` works with the *Builder* pattern. The `Builder` takes all
+the configurations into account before building the `Cluster`.
+
+Following are some examples of the most important configurations that were 
+possible with *Astyanax* and how to translate them into *DataStax Java driver*
+configurations.
+
+## Connection pools
+
+Configuration of connection pools in *Astyanax* are made through the
+`ConnectionPoolConfigurationImpl`. This object gathers important configurations
+that the *Java driver* has categorized in multiple *Option* and *Policy* kinds.
+
+### Connections pools internals
+Everything concerning the internal pools of connections to the *Cassandra nodes*
+will be gathered in the Java driver in the `PoolingOptions` :
+
+*Astyanax*:
+
+```java
+ConnectionPoolConfigurationImpl cpool =
+       new ConnectionPoolConfigurationImpl(""myConnectionPool"")
+               .setInitConnsPerHost(2)
+               .setMaxConnsPerHost(3)
+```
+
+*Java driver*:
+
+```java
+PoolingOptions poolingOptions =
+       new PoolingOptions()
+           .setMaxRequestsPerConnection(HostDistance.LOCAL, 1024)
+           .setCoreConnectionsPerHost(HostDistance.LOCAL, 2)
+           .setMaxConnectionsPerHost(HostDistance.LOCAL, 3);
+```
+
+Note that the *Java driver* allows multiple simultaneous requests on one single
+connection, as it is based upon the *Native protocol*, an asynchronous binary
+protocol that can handle up to 32768 simultaneous requests.
+
+### Timeouts
+
+Timeouts concerning requests, or connections will be part of the `SocketOptions`.
+
+*Astyanax*:
+
+```java
+ConnectionPoolConfigurationImpl cpool =
+       new ConnectionPoolConfigurationImpl(""myConnectionPool"")
+               .setSocketTimeout(3000)
+               .setConnectTimeout(3000)
+```
+
+*Java Driver :*
+
+```java
+SocketOptions so =
+       new SocketOptions()
+           .setReadTimeoutMillis(3000)
+           .setConnectTimeoutMillis(3000);
+```
+
+## Load Balancing
+Both *Astyanax* and the *Java driver* connect to multiple nodes of the *Cassandra*
+cluster. Distributing requests through all the nodes plays an important role in 
+the good operation of the `Cluster` and for best performances. With *Astyanax*, 
+requests (or â€œoperationsâ€) can be sent directly to replicas that have a copy of 
+the data targeted by the *â€œRow keyâ€* specified in the operation. Since the *Thrift* API is
+low-level, it forces the user to provide *Row keys*, known as the `TokenAware` 
+connection pool type. This setting is also present in the *Java driver*, however 
+the configuration is different and provides more options to tweak.
+
+Load balancing in the *Java driver* is a *Policy*, it is a class that will be
+plugged in the *Java driver*â€™s code and the Driver will call its methods when it 
+needs to. The *Java driver* comes with a preset of specific load balancing policies. 
+Hereâ€™s an equivalent code :
+
+*Astyanax*:
+
+```java
+final ConnectionPoolType poolType = ConnectionPoolType.TOKEN_AWARE;
+final NodeDiscoveryType discType = NodeDiscoveryType.RING_DESCRIBE;
+ConnectionPoolConfigurationImpl cpool =
+       new ConnectionPoolConfigurationImpl(""myConnectionPool"")
+               .setLocalDatacenter(""myDC"")
+AstyanaxConfigurationImpl aconf =
+       new AstyanaxConfigurationImpl()
+               .setConnectionPoolType(poolType)
+               .setDiscoveryType(discType)
+```
+
+*Java driver*:
+
+```java
+LoadBalancingPolicy lbp = new TokenAwarePolicy(
+       DCAwareRoundRobinPolicy.builder()
+       .withLocalDc(""myDC"")
+       .build()
+);
+```
+
+*By default* the *Java driver* will instantiate the exact Load balancing policy 
+shown above, with the `LocalDC` being the DC of the first host the driver connects 
+to. So to get the same behaviour than the *TokenAware* pool type of *Astyanax*, 
+users shouldnâ€™t need to specify a load balancing policy since the default one 
+should cover it.
+
+Important : Note that since *CQL* is an abstraction of the Cassandraâ€™s architecture, a simple 
+query needs to have the *Row key* specified explicitly on a `Statement` in order 
+to benefit from the *TokenAware* routing (the *Row key* in the *Java driver* is 
+referenced as *Routing Key*), unlike the *Astyanax* driver. 
+Some differences occur related to the different kinds of `Statements` the *Java
+driver* provides. Please see [this link](../../../manual/statements) for specific information.
+
+Custom load balancing policies can easily be implemented by users, and supplied to 
+the *Java driver* for specific use cases. All information necessary is available
+in the [Load balaning policies docs](../../../manual/load_balancing).
+
+## Consistency levels
+Consistency levels can be set per-statement, or globally through the `QueryOptions`.
+
+*Astyanax*:
+
+```java
+AstyanaxConfigurationImpl aconf =
+       new AstyanaxConfigurationImpl()
+               .setDefaultReadConsistencyLevel(ConsistencyLevel.CL*ALL)
+               .setDefaultWriteConsistencyLevel(ConsistencyLevel.CL*ALL)
+```
+
+*Java driver*:
+
+```java
+QueryOptions qo = new QueryOptions().setConsistencyLevel(ConsistencyLevel.ALL);
+```
+
+Since the *Java driver* only executes *CQL* statements, which can be either reads
+or writes to *Cassandra*, it is not possible to globally configure the
+Consistency Level for only reads or only writes. To do so, since the Consistency 
+Level can be set per-statement, you can either set it on every statement, or use 
+`PreparedStatements` (if queries are to be repeated with different values): in
+this case, setting the CL on the `PreparedStatement`, causes the `BoundStatements` to 
+inherit the CL from the prepared statements they were prepared from. More
+informations about how `Statement`s work in the *Java driver* are detailed
+in the [â€œQueries and Resultâ€ section](../queries_and_results/).
+
+
+## Authentication
+
+Authentication settings are managed by the `AuthProvider` class in the *Java driver*.
+It can be highly customizable, but also comes with default simple implementations :
+
+*Astyanax*:
+
+```java
+AuthenticationCredentials authCreds = new SimpleAuthenticationCredentials(""username"", ""password"");
+ConnectionPoolConfigurationImpl cpool =
+       new ConnectionPoolConfigurationImpl(""myConnectionPool"")
+               .setAuthenticationCredentials(authCreds)
+```
+
+*Java driver*:
+
+```java
+AuthProvider authProvider = new PlainTextAuthProvider(""username"", ""password"");
+```
+
+The class `AuthProvider` can be easily implemented to suit the userâ€™s needs,
+documentation about the classes needed is [available there](../../../manual/auth/).
+
+## Hosts and ports
+
+Setting the â€œseedsâ€ or first hosts to connect to can be done directly on the 
+Cluster configuration Builder :
+
+*Astyanax*:
+
+```java
+ConnectionPoolConfigurationImpl cpool =
+       new ConnectionPoolConfigurationImpl(""myConnectionPool"")
+               .setSeeds(""127.0.0.1"")
+               .setPort(9160)
+```
+
+*Java driver*:
+
+```java
+Cluster cluster = Cluster.builder()
+       .addContactPoint(""127.0.0.1"")
+       .withPort(9042)
+```
+
+The *Java driver* by default connects to port *9042*, hence you can supply only
+host names with the `addContactPoints(String...)` method. Note that the contact
+points are only the entry points to the `Cluster` for the *Automatic discovery
+phase*, the driver will connect to all hosts of the cluster, in the DC of the","[{'comment': ""That depends on the LBP, it's not necessarily true for all setups.\n"", 'commenter': 'adutra'}]"
732,upgrade_guide/migrating_from_astyanax/language_level_changes/README.md,"@@ -0,0 +1,75 @@
+# Language change : from Thrift to CQL
+The data model changes when using *CQL* (Cassandra Query Language).
+*CQL* is providing an abstraction of the low-level data stored in *Cassandra*, in
+opposition to *Thrift* that aims to expose the low-level data structure directly.
+[But note that this changes with Cassandra 3â€™s new storage engine.](http://www.datastax.com/2015/12/storage-engine-30)
+
+*Thrift* exposes *Keyspaces*, and these *Keyspaces* contain *Column Families*. A
+*ColumnFamily* contains *Rows* in which each *Row* has a list of an arbitrary number
+of column-values. With *CQL*, the data is **tabular**, *ColumnFamily* gets viewed
+as a *Table*, the **Table Rows** get a **fixed and finite number of named columns**.
+*Thrift*â€™s columns inside the *Rows* get distributed in a tabular way through the
+_Table Rows_. See the following figure :
+
+```ditaa
+                                                     Thrift
+             /-                                                                                          -\
+             |                                                                                            |
+             |  /------------\              /---------------+---------------+---------------+---------+   |
+             |  |    cRED    |              |cFA0 Col : 1   |     Col : 2   |     Col : 3   |         |   |
+             |  | Key : 1    | ---------->  +---------------+---------------+---------------+ ...     |   +--> One Thrift
+             |  |            |              |c1AB Val : 'a' |     Val : 'b' |     Val : 'c' |         |   |    ROW
+             |  \------------/              \---------------+---------------+---------------+---------+   |
+             |                                                                                            |
+One Thrift   |                                                                                           -/
+COLUMNFAMILY |          
+             |  
+             |  /------------\              /---------------+---------------+---------+
+             |  |            |              |     Col : 1   |     Col : 2   |         |
+             |  | Key : 2    | ---------->  +---------------+---------------+ ...     |
+             |  |            |              |     Val : 'a' |     Val : 'b' |         |
+             |  \------------/              \---------------+---------------+---------+
+             |  
+             \- 
+
+
+                      -----------------------------------------------------------------------
+
+
+                                                     CQL
+                                                     
+             /-                              
+             |                               
+             |  /--------------------+---------------------------------+-----------------------------\","[{'comment': ""This is not the way I would intuitively represent a CQL row. It would be more like this:\n\n<table>\n<tr><th>key</th><th>column: 1</th><th>column: 2</th><th>column: 3</th></tr>\n<tr><td>cRED   1</td><td>'a'</td><td>'b'</td><td>'c'</td></tr>\n<tr><td>cRED   2</td><td>'a'</td><td>'b'</td><td>null</td></tr>\n</table>   \n"", 'commenter': 'adutra'}, {'comment': 'Agreed, pushed a revised version thanks. However, it is strange that cqlsh does not represent things this way, but the way I had done it before...\n', 'commenter': 'newkek'}]"
732,upgrade_guide/migrating_from_astyanax/language_level_changes/README.md,"@@ -0,0 +1,75 @@
+# Language change : from Thrift to CQL
+The data model changes when using *CQL* (Cassandra Query Language).
+*CQL* is providing an abstraction of the low-level data stored in *Cassandra*, in
+opposition to *Thrift* that aims to expose the low-level data structure directly.
+[But note that this changes with Cassandra 3â€™s new storage engine.](http://www.datastax.com/2015/12/storage-engine-30)
+
+*Thrift* exposes *Keyspaces*, and these *Keyspaces* contain *Column Families*. A
+*ColumnFamily* contains *Rows* in which each *Row* has a list of an arbitrary number
+of column-values. With *CQL*, the data is **tabular**, *ColumnFamily* gets viewed
+as a *Table*, the **Table Rows** get a **fixed and finite number of named columns**.
+*Thrift*â€™s columns inside the *Rows* get distributed in a tabular way through the
+_Table Rows_. See the following figure :
+
+```ditaa
+                                                     Thrift
+             /-                                                                                          -\
+             |                                                                                            |
+             |  /------------\              /---------------+---------------+---------------+---------+   |
+             |  |    cRED    |              |cFA0 Col : 1   |     Col : 2   |     Col : 3   |         |   |
+             |  | Key : 1    | ---------->  +---------------+---------------+---------------+ ...     |   +--> One Thrift
+             |  |            |              |c1AB Val : 'a' |     Val : 'b' |     Val : 'c' |         |   |    ROW
+             |  \------------/              \---------------+---------------+---------------+---------+   |
+             |                                                                                            |
+One Thrift   |                                                                                           -/
+COLUMNFAMILY |          
+             |  
+             |  /------------\              /---------------+---------------+---------+
+             |  |            |              |     Col : 1   |     Col : 2   |         |
+             |  | Key : 2    | ---------->  +---------------+---------------+ ...     |
+             |  |            |              |     Val : 'a' |     Val : 'b' |         |
+             |  \------------/              \---------------+---------------+---------+
+             |  
+             \- 
+
+
+                      -----------------------------------------------------------------------
+
+
+                                                     CQL
+                                                     
+             /-                              
+             |                               
+             |  /--------------------+---------------------------------+-----------------------------\
+             |  |       key          |              column             |            value            |  
+             |  +--------------------+---------------------------------+-----------------------------+
+             |  | cRED   1           |  cFA0          1                |   c1AB      'a'             |
+             |  +--------------------+---------------------------------+-----------------------------+ -\
+             |  | cRED   1           |                2                |             'b'             |  +--> One CQL
+   One CQL   |  +--------------------+---------------------------------+-----------------------------+ -/    ROW
+   TABLE     |  | cRED   1           |                3                |             'c'             |
+             |  +--------------------+---------------------------------+-----------------------------+
+             |  | cRED  ...          |               ...               |             ...             |
+             |  +--------------------+---------------------------------+-----------------------------+
+             |  |        2           |                1                |             'a'             |
+             |  +--------------------+---------------------------------+-----------------------------+
+             |  |        2           |                2                |             'b'             |
+             |  +--------------------+---------------------------------+-----------------------------+
+             |  |       ...          |               ...               |             ...             |
+             |  +--------------------+---------------------------------+-----------------------------+
+             \-
+```
+
+Some of the columns of a *CQL Table* have a special role that is specifically
+related to the *Cassandra* architecture. Indeed, the *Row key* of the *Thrift Row*,
+becomes the *Partition Key* in the *CQL Table*, and can be composed of 1 or multiple
+*CQL columns* (the key column in Figure 1). The *â€œColumnâ€* part of the Column-value
+component in a *Thrift Row*, becomes the *Clustering ColumnKey* in *CQL*, and can
+also be composed of multiple columns (in the figure, column1 is the only column 
+composing the *Clustering ColumnKey*).
+
+Here is the basic architectural concept of *CQL*, this post will not dive more","[{'comment': ""This should be rephrased as it's not a blog post anymore.\n"", 'commenter': 'adutra'}]"
732,upgrade_guide/migrating_from_astyanax/language_level_changes/README.md,"@@ -0,0 +1,66 @@
+# Language change : from Thrift to CQL
+The data model changes when using *CQL* (Cassandra Query Language).
+*CQL* is providing an abstraction of the low-level data stored in *Cassandra*, in
+opposition to *Thrift* that aims to expose the low-level data structure directly.
+[But note that this changes with Cassandra 3â€™s new storage engine.](http://www.datastax.com/2015/12/storage-engine-30)
+
+*Thrift* exposes *Keyspaces*, and these *Keyspaces* contain *Column Families*. A
+*ColumnFamily* contains *Rows* in which each *Row* has a list of an arbitrary number
+of column-values. With *CQL*, the data is **tabular**, *ColumnFamily* gets viewed
+as a *Table*, the **Table Rows** get a **fixed and finite number of named columns**.
+*Thrift*â€™s columns inside the *Rows* get distributed in a tabular way through the
+_Table Rows_. See the following figure :
+
+```ditaa
+                                                     Thrift
+             /-                                                                                          -\
+             |                                                                                            |
+             |  /------------\              /---------------+---------------+---------------+---------+   |
+             |  |    cRED    |              |      Col 1    |      Col 2    |      Col 3    |         |   |
+             |  | Key : 1    | ---------->  +---------------+---------------+---------------+ ...     |   +--> One Thrift
+             |  |            |              |c1AB Val : 'a' |cFA0 Val : 'b' |     Val : 'c' |         |   |    ROW
+             |  \------------/              \---------------+---------------+---------------+---------+   |
+             |                                                                                            |
+One Thrift   |                                                                                           -/
+COLUMNFAMILY |          
+             |  
+             |  /------------\              /---------------+---------------+---------+
+             |  |            |              |     Col 1     |     Col 4     |         |
+             |  | Key : 2    | ---------->  +---------------+---------------+ ...     |
+             |  |            |              |     Val : 'a' |     Val : 'b' |         |
+             |  \------------/              \---------------+---------------+---------+
+             |  
+             \- 
+
+
+                      -----------------------------------------------------------------------
+
+
+                                                     CQL
+                                                     
+             /-                              
+             |                               
+             |  /--------------------+---------------------------+-----------------------+----------------------+-----------------------------\
+             |  |       key          |           Col1            |         Col2          |         Col3         |            Col4             |  
+             |  +--------------------+---------------------------+-----------------------+----------------------+-----------------------------+ -\
+             |  | cRED   1           |   c1AB     'a'            | cFA0     'b'          |          'c'         |             null            |  +--> One CQL
+   One CQL   |  +--------------------+---------------------------+-----------------------+----------------------+-----------------------------+ -/    ROW
+   TABLE     |  |        2           |             'a'           |          null         |          null        |             'b'             |
+             |  +--------------------+---------------------------+-----------------------+----------------------+-----------------------------+
+             |  |       ...          |            ...            |          ...          |          ...         |             ...             |
+             |  +--------------------+---------------------------+-----------------------+----------------------+-----------------------------+
+             \-
+```
+
+Some of the columns of a *CQL Table* have a special role that is specifically
+related to the *Cassandra* architecture. Indeed, the *Row key* of the *Thrift Row*,
+becomes the *Partition Key* in the *CQL Table*, and can be composed of 1 or multiple
+*CQL columns* (the key column in Figure 1). The *â€œColumnâ€* part of the Column-value
+component in a *Thrift Row*, becomes the *Clustering ColumnKey* in *CQL*, and can
+also be composed of multiple columns (in the figure, column1 is the only column 
+composing the *Clustering ColumnKey*).","[{'comment': 'In the thrift example, are ""col1, col2, col3"" the cell names, and ""a, b, c"" the cell values? If so, I don\'t agree with the second part of the schema, cell names should translate to different values of the clustering column.\nSee the ""Dynamic Column family"" example in [this blog post](http://www.datastax.com/dev/blog/thrift-to-cql3): the thrift cell names (timestamps) become the _values_ of the ""time"" CQL column. The text says\n\n> map [...] the second component [of the CQL primary key, i.e. the clustering column] (time) to the internal cell name. And the last CQL3 column (url) will be mapped to the cell value""\n\nIt gets more complicated with multiple clustering columns and multiple non-primary key rows (see the ""Composite"" and ""Non-compact tables"" examples respectively), but the _value_ of the CQL clustering column always goes into the _name_ of the internal cell.\n\nI think the example should also illustrate a case where a single Thrift wide row translates to multiple CQL rows, it seems like that\'s a common hurdle for people coming from Thrift.\n', 'commenter': 'olim7t'}, {'comment': ""That was tripping me up as well, initially I was conflating how things are stored on disk with the chart here.    I think it would be more clear if the chart represented the mental models for Thrift and CQL and Thrift has 1 row per Partition with the cell names like:\n\n`'a:col2', 'a:col3', 'b:col2', 'b:col3'`\n\nWhere the CQL chart would represent multiple rows within a partition with each Row being 1 line each within a partition, where thrift is 1 line of rows overall within the partition.\n"", 'commenter': 'tolbertam'}, {'comment': 'Initially the schema was like this: https://gist.github.com/newkek/4a0cbe91577886383aaa9ef89701cf03\nI changed it to this version after a discussion with @adutra, I think it kind of represents more ""the concept"" even though it\'s not the exact memory representation. I don\'t mind changing it back as I believe it is rather subjective. Also noticing that I updated the schema without updating the explanation so actually currently both don\'t match.\n', 'commenter': 'newkek'}, {'comment': ""I like the current version compared to the original diagram.  The two suggestions I would have:\n- The column/cell names in the thrift diagram should be clusteringkeyvalue:columnname (i.e. a:col2, a:col3 and so on).  The clustering key shouldn't be it's own column.\n- I think it would be good to show what it would look like though if you had multiple rows per partition.  The current diagram only has 1 cql row/unique cluster per partition, it would be good to demonstrate what it would look like if there were at least 2.  (i.e. have two rows for key=1 where key=1,col1=a and key=1,col1=b).  Hopefully that makes sense.\n"", 'commenter': 'tolbertam'}]"
732,upgrade_guide/migrating_from_astyanax/configuration/README.md,"@@ -0,0 +1,242 @@
+# Configuration
+
+## How Configuring the Java driver works
+
+The two basic components in the Java driver are the `Cluster` and the `Session`.
+The `Cluster` is the object to create first, and on to which apply all
+global configuration options. Connecting to the `Cluster` creates a","[{'comment': 'suggestion: ""on to which all global configuration options apply""\n', 'commenter': 'olim7t'}]"
732,upgrade_guide/migrating_from_astyanax/README.md,"@@ -0,0 +1,10 @@
+# Migrating from Astyanax
+
+This section is a guide for users previously using *Astyanax* and looking for
+migrating to the *DataStax Java driver*.
+
+See the child pages for more information:
+
+* [Changes at the language level](language_level_changes/)
+* [Migrating Astyanax configurations to DataStax Java driver configurations](configuration/)
+* [Querying and retrieving results comparisons.](queries_and_result/)","[{'comment': 'This should be queries_and_results I think\n', 'commenter': 'tolbertam'}]"
732,upgrade_guide/migrating_from_astyanax/configuration/README.md,"@@ -0,0 +1,242 @@
+# Configuration
+
+## How Configuring the Java driver works
+
+The two basic components in the Java driver are the `Cluster` and the `Session`.
+The `Cluster` is the object to create first, and on to which apply all
+global configuration options. Connecting to the `Cluster` creates a
+`Session`. Queries are executed through the `Session`.
+
+The `Cluster` object then is to be viewed as the equivalent of the `AstyanaxContext`
+object. â€™Startingâ€™ an `AstyanaxContext` object typically returns a `Keyspace`","[{'comment': '[typo maniac warning] Surrounding ""Starting"" with two closing quotes looks ugly. If you use plain quotes in the source (`\'Starting\'`), documentor will do the right thing and generate an opening and closing quote. This works with double quotes as well.\n', 'commenter': 'olim7t'}]"
732,upgrade_guide/migrating_from_astyanax/language_level_changes/README.md,"@@ -0,0 +1,66 @@
+# Language change : from Thrift to CQL
+The data model changes when using *CQL* (Cassandra Query Language).
+*CQL* is providing an abstraction of the low-level data stored in *Cassandra*, in
+opposition to *Thrift* that aims to expose the low-level data structure directly.
+[But note that this changes with Cassandra 3â€™s new storage engine.](http://www.datastax.com/2015/12/storage-engine-30)
+
+*Thrift* exposes *Keyspaces*, and these *Keyspaces* contain *Column Families*. A
+*ColumnFamily* contains *Rows* in which each *Row* has a list of an arbitrary number
+of column-values. With *CQL*, the data is **tabular**, *ColumnFamily* gets viewed
+as a *Table*, the **Table Rows** get a **fixed and finite number of named columns**.
+*Thrift*â€™s columns inside the *Rows* get distributed in a tabular way through the
+_Table Rows_. See the following figure :
+
+```ditaa
+                                                     Thrift
+             /-                                                                                          -\
+             |                                                                                            |
+             |  /------------\              /---------------+---------------+---------------+---------+   |
+             |  |    cRED    |              |      Col 1    |      Col 2    |      Col 3    |         |   |
+             |  | Key : 1    | ---------->  +---------------+---------------+---------------+ ...     |   +--> One Thrift
+             |  |            |              |c1AB Val : 'a' |cFA0 Val : 'b' |     Val : 'c' |         |   |    ROW
+             |  \------------/              \---------------+---------------+---------------+---------+   |
+             |                                                                                            |
+One Thrift   |                                                                                           -/
+COLUMNFAMILY |          
+             |  
+             |  /------------\              /---------------+---------------+---------+
+             |  |            |              |     Col 1     |     Col 4     |         |
+             |  | Key : 2    | ---------->  +---------------+---------------+ ...     |
+             |  |            |              |     Val : 'a' |     Val : 'b' |         |
+             |  \------------/              \---------------+---------------+---------+
+             |  
+             \- 
+
+
+                      -----------------------------------------------------------------------
+
+
+                                                     CQL
+                                                     
+             /-                              
+             |                               
+             |  /--------------------+---------------------------+-----------------------+----------------------+-----------------------------\
+             |  |       key          |           Col1            |         Col2          |         Col3         |            Col4             |  
+             |  +--------------------+---------------------------+-----------------------+----------------------+-----------------------------+ -\
+             |  | cRED   1           |   c1AB     'a'            | cFA0     'b'          |          'c'         |             null            |  +--> One CQL
+   One CQL   |  +--------------------+---------------------------+-----------------------+----------------------+-----------------------------+ -/    ROW
+   TABLE     |  |        2           |             'a'           |          null         |          null        |             'b'             |
+             |  +--------------------+---------------------------+-----------------------+----------------------+-----------------------------+
+             |  |       ...          |            ...            |          ...          |          ...         |             ...             |
+             |  +--------------------+---------------------------+-----------------------+----------------------+-----------------------------+
+             \-
+```
+
+Some of the columns of a *CQL Table* have a special role that is specifically
+related to the *Cassandra* architecture. Indeed, the *Row key* of the *Thrift Row*,
+becomes the *Partition Key* in the *CQL Table*, and can be composed of 1 or multiple
+*CQL columns* (the key column in Figure 1). The *â€œColumnâ€* part of the Column-value
+component in a *Thrift Row*, becomes the *Clustering ColumnKey* in *CQL*, and can
+also be composed of multiple columns (in the figure, column1 is the only column 
+composing the *Clustering ColumnKey*).
+
+Here is the basic architectural concept of *CQL*, a detailed explanation and *CQL*
+examples can be found in this article : [http://www.planetcassandra.org/making-the-change-from-thrift-to-cql/].","[{'comment': 'This appears somewhat weird in the generated docs:\n\n![image](https://cloud.githubusercontent.com/assets/6889771/18524994/6dd4be12-7a80-11e6-9ae9-1a7e95b231c9.png)\n', 'commenter': 'tolbertam'}]"
732,upgrade_guide/migrating_from_astyanax/configuration/README.md,"@@ -0,0 +1,242 @@
+# Configuration
+
+## How Configuring the Java driver works
+
+The two basic components in the Java driver are the `Cluster` and the `Session`.
+The `Cluster` is the object to create first, and on to which apply all
+global configuration options. Connecting to the `Cluster` creates a
+`Session`. Queries are executed through the `Session`.
+
+The `Cluster` object then is to be viewed as the equivalent of the `AstyanaxContext`
+object. â€™Startingâ€™ an `AstyanaxContext` object typically returns a `Keyspace`
+object, the `Keyspace` object is the equivalent of the *Java driver*â€™s `Session`.
+
+Configuring a `Cluster` works with the *Builder* pattern. The `Builder` takes all
+the configurations into account before building the `Cluster`.
+
+Following are some examples of the most important configurations that were 
+possible with *Astyanax* and how to translate them into *DataStax Java driver*
+configurations.
+
+## Connection pools
+
+Configuration of connection pools in *Astyanax* are made through the
+`ConnectionPoolConfigurationImpl`. This object gathers important configurations
+that the *Java driver* has categorized in multiple *Option* and *Policy* kinds.
+
+### Connections pools internals
+Everything concerning the internal pools of connections to the *Cassandra nodes*
+will be gathered in the Java driver in the `PoolingOptions` :","[{'comment': 'Link to ../../../manual/pooling\n', 'commenter': 'olim7t'}]"
732,upgrade_guide/migrating_from_astyanax/configuration/README.md,"@@ -0,0 +1,242 @@
+# Configuration
+
+## How Configuring the Java driver works
+
+The two basic components in the Java driver are the `Cluster` and the `Session`.
+The `Cluster` is the object to create first, and on to which apply all
+global configuration options. Connecting to the `Cluster` creates a
+`Session`. Queries are executed through the `Session`.
+
+The `Cluster` object then is to be viewed as the equivalent of the `AstyanaxContext`
+object. â€™Startingâ€™ an `AstyanaxContext` object typically returns a `Keyspace`
+object, the `Keyspace` object is the equivalent of the *Java driver*â€™s `Session`.
+
+Configuring a `Cluster` works with the *Builder* pattern. The `Builder` takes all
+the configurations into account before building the `Cluster`.
+
+Following are some examples of the most important configurations that were 
+possible with *Astyanax* and how to translate them into *DataStax Java driver*
+configurations.
+
+## Connection pools
+
+Configuration of connection pools in *Astyanax* are made through the
+`ConnectionPoolConfigurationImpl`. This object gathers important configurations
+that the *Java driver* has categorized in multiple *Option* and *Policy* kinds.
+
+### Connections pools internals
+Everything concerning the internal pools of connections to the *Cassandra nodes*
+will be gathered in the Java driver in the `PoolingOptions` :
+
+*Astyanax*:
+
+```java
+ConnectionPoolConfigurationImpl cpool =
+       new ConnectionPoolConfigurationImpl(""myConnectionPool"")
+               .setInitConnsPerHost(2)
+               .setMaxConnsPerHost(3)
+```
+
+*Java driver*:
+
+```java
+PoolingOptions poolingOptions =
+       new PoolingOptions()
+           .setMaxRequestsPerConnection(HostDistance.LOCAL, 1024)
+           .setCoreConnectionsPerHost(HostDistance.LOCAL, 2)
+           .setMaxConnectionsPerHost(HostDistance.LOCAL, 3);
+```
+
+Note that the *Java driver* allows multiple simultaneous requests on one single
+connection, as it is based upon the *Native protocol*, an asynchronous binary
+protocol that can handle up to 32768 simultaneous requests.","[{'comment': 'Link ""native protocol"" to ../../../manual/native_protocol\n', 'commenter': 'olim7t'}, {'comment': ""To my comment about not changing options, I think it would be good emphasize that configuring pooling with the java driver is less important because it allows multiple requests on a connection.   There shouldn't be a compelling reason to increase the number of connections in the general case except for very high throughputs (can link to ../../../manual/pooling/#tuning-protocol-v3-for-very-high-throughputs).\n"", 'commenter': 'tolbertam'}]"
732,upgrade_guide/migrating_from_astyanax/configuration/README.md,"@@ -0,0 +1,242 @@
+# Configuration
+
+## How Configuring the Java driver works
+
+The two basic components in the Java driver are the `Cluster` and the `Session`.
+The `Cluster` is the object to create first, and on to which apply all
+global configuration options. Connecting to the `Cluster` creates a
+`Session`. Queries are executed through the `Session`.
+
+The `Cluster` object then is to be viewed as the equivalent of the `AstyanaxContext`
+object. â€™Startingâ€™ an `AstyanaxContext` object typically returns a `Keyspace`
+object, the `Keyspace` object is the equivalent of the *Java driver*â€™s `Session`.
+
+Configuring a `Cluster` works with the *Builder* pattern. The `Builder` takes all
+the configurations into account before building the `Cluster`.
+
+Following are some examples of the most important configurations that were 
+possible with *Astyanax* and how to translate them into *DataStax Java driver*
+configurations.
+
+## Connection pools
+
+Configuration of connection pools in *Astyanax* are made through the
+`ConnectionPoolConfigurationImpl`. This object gathers important configurations
+that the *Java driver* has categorized in multiple *Option* and *Policy* kinds.
+
+### Connections pools internals
+Everything concerning the internal pools of connections to the *Cassandra nodes*
+will be gathered in the Java driver in the `PoolingOptions` :
+
+*Astyanax*:
+
+```java
+ConnectionPoolConfigurationImpl cpool =
+       new ConnectionPoolConfigurationImpl(""myConnectionPool"")
+               .setInitConnsPerHost(2)
+               .setMaxConnsPerHost(3)
+```
+
+*Java driver*:
+
+```java
+PoolingOptions poolingOptions =
+       new PoolingOptions()
+           .setMaxRequestsPerConnection(HostDistance.LOCAL, 1024)
+           .setCoreConnectionsPerHost(HostDistance.LOCAL, 2)
+           .setMaxConnectionsPerHost(HostDistance.LOCAL, 3);
+```
+
+Note that the *Java driver* allows multiple simultaneous requests on one single
+connection, as it is based upon the *Native protocol*, an asynchronous binary
+protocol that can handle up to 32768 simultaneous requests.
+
+### Timeouts
+
+Timeouts concerning requests, or connections will be part of the `SocketOptions`.
+
+*Astyanax*:
+
+```java
+ConnectionPoolConfigurationImpl cpool =
+       new ConnectionPoolConfigurationImpl(""myConnectionPool"")
+               .setSocketTimeout(3000)
+               .setConnectTimeout(3000)
+```
+
+*Java Driver :*
+
+```java
+SocketOptions so =
+       new SocketOptions()
+           .setReadTimeoutMillis(3000)
+           .setConnectTimeoutMillis(3000);
+```
+
+## Load Balancing
+Both *Astyanax* and the *Java driver* connect to multiple nodes of the *Cassandra*
+cluster. Distributing requests through all the nodes plays an important role in 
+the good operation of the `Cluster` and for best performances. With *Astyanax*, 
+requests (or â€œoperationsâ€) can be sent directly to replicas that have a copy of 
+the data targeted by the *â€œRow keyâ€* specified in the operation. Since the *Thrift* API is
+low-level, it forces the user to provide *Row keys*, known as the `TokenAware` 
+connection pool type. This setting is also present in the *Java driver*, however 
+the configuration is different and provides more options to tweak.
+
+Load balancing in the *Java driver* is a *Policy*, it is a class that will be
+plugged in the *Java driver*â€™s code and the Driver will call its methods when it 
+needs to. The *Java driver* comes with a preset of specific load balancing policies. 
+Hereâ€™s an equivalent code :
+
+*Astyanax*:
+
+```java
+final ConnectionPoolType poolType = ConnectionPoolType.TOKEN_AWARE;
+final NodeDiscoveryType discType = NodeDiscoveryType.RING_DESCRIBE;
+ConnectionPoolConfigurationImpl cpool =
+       new ConnectionPoolConfigurationImpl(""myConnectionPool"")
+               .setLocalDatacenter(""myDC"")
+AstyanaxConfigurationImpl aconf =
+       new AstyanaxConfigurationImpl()
+               .setConnectionPoolType(poolType)
+               .setDiscoveryType(discType)
+```
+
+*Java driver*:
+
+```java
+LoadBalancingPolicy lbp = new TokenAwarePolicy(
+       DCAwareRoundRobinPolicy.builder()
+       .withLocalDc(""myDC"")
+       .build()
+);
+```
+
+*By default* the *Java driver* will instantiate the exact Load balancing policy 
+shown above, with the `LocalDC` being the DC of the first host the driver connects 
+to. So to get the same behaviour than the *TokenAware* pool type of *Astyanax*, 
+users shouldnâ€™t need to specify a load balancing policy since the default one 
+should cover it.
+
+Important : Note that since *CQL* is an abstraction of the Cassandraâ€™s architecture, a simple 
+query needs to have the *Row key* specified explicitly on a `Statement` in order 
+to benefit from the *TokenAware* routing (the *Row key* in the *Java driver* is 
+referenced as *Routing Key*), unlike the *Astyanax* driver. 
+Some differences occur related to the different kinds of `Statements` the *Java
+driver* provides. Please see [this link](../../../manual/statements) for specific information.","[{'comment': 'It would be more appropriate to link to `../../../manual/load_balancing/#token-aware-policy`, it details how to set the routing key for each statement type.\n', 'commenter': 'olim7t'}]"
732,upgrade_guide/migrating_from_astyanax/configuration/README.md,"@@ -0,0 +1,242 @@
+# Configuration
+
+## How Configuring the Java driver works
+
+The two basic components in the Java driver are the `Cluster` and the `Session`.
+The `Cluster` is the object to create first, and on to which apply all
+global configuration options. Connecting to the `Cluster` creates a
+`Session`. Queries are executed through the `Session`.
+
+The `Cluster` object then is to be viewed as the equivalent of the `AstyanaxContext`
+object. â€™Startingâ€™ an `AstyanaxContext` object typically returns a `Keyspace`
+object, the `Keyspace` object is the equivalent of the *Java driver*â€™s `Session`.
+
+Configuring a `Cluster` works with the *Builder* pattern. The `Builder` takes all
+the configurations into account before building the `Cluster`.
+
+Following are some examples of the most important configurations that were 
+possible with *Astyanax* and how to translate them into *DataStax Java driver*
+configurations.
+
+## Connection pools
+
+Configuration of connection pools in *Astyanax* are made through the
+`ConnectionPoolConfigurationImpl`. This object gathers important configurations
+that the *Java driver* has categorized in multiple *Option* and *Policy* kinds.
+
+### Connections pools internals
+Everything concerning the internal pools of connections to the *Cassandra nodes*
+will be gathered in the Java driver in the `PoolingOptions` :
+
+*Astyanax*:
+
+```java
+ConnectionPoolConfigurationImpl cpool =
+       new ConnectionPoolConfigurationImpl(""myConnectionPool"")
+               .setInitConnsPerHost(2)
+               .setMaxConnsPerHost(3)
+```
+
+*Java driver*:
+
+```java
+PoolingOptions poolingOptions =
+       new PoolingOptions()
+           .setMaxRequestsPerConnection(HostDistance.LOCAL, 1024)
+           .setCoreConnectionsPerHost(HostDistance.LOCAL, 2)
+           .setMaxConnectionsPerHost(HostDistance.LOCAL, 3);
+```
+
+Note that the *Java driver* allows multiple simultaneous requests on one single
+connection, as it is based upon the *Native protocol*, an asynchronous binary
+protocol that can handle up to 32768 simultaneous requests.
+
+### Timeouts
+
+Timeouts concerning requests, or connections will be part of the `SocketOptions`.
+
+*Astyanax*:
+
+```java
+ConnectionPoolConfigurationImpl cpool =
+       new ConnectionPoolConfigurationImpl(""myConnectionPool"")
+               .setSocketTimeout(3000)
+               .setConnectTimeout(3000)
+```
+
+*Java Driver :*
+
+```java
+SocketOptions so =
+       new SocketOptions()
+           .setReadTimeoutMillis(3000)
+           .setConnectTimeoutMillis(3000);
+```
+
+## Load Balancing
+Both *Astyanax* and the *Java driver* connect to multiple nodes of the *Cassandra*
+cluster. Distributing requests through all the nodes plays an important role in 
+the good operation of the `Cluster` and for best performances. With *Astyanax*, 
+requests (or â€œoperationsâ€) can be sent directly to replicas that have a copy of 
+the data targeted by the *â€œRow keyâ€* specified in the operation. Since the *Thrift* API is
+low-level, it forces the user to provide *Row keys*, known as the `TokenAware` 
+connection pool type. This setting is also present in the *Java driver*, however 
+the configuration is different and provides more options to tweak.
+
+Load balancing in the *Java driver* is a *Policy*, it is a class that will be
+plugged in the *Java driver*â€™s code and the Driver will call its methods when it 
+needs to. The *Java driver* comes with a preset of specific load balancing policies. 
+Hereâ€™s an equivalent code :
+
+*Astyanax*:
+
+```java
+final ConnectionPoolType poolType = ConnectionPoolType.TOKEN_AWARE;
+final NodeDiscoveryType discType = NodeDiscoveryType.RING_DESCRIBE;
+ConnectionPoolConfigurationImpl cpool =
+       new ConnectionPoolConfigurationImpl(""myConnectionPool"")
+               .setLocalDatacenter(""myDC"")
+AstyanaxConfigurationImpl aconf =
+       new AstyanaxConfigurationImpl()
+               .setConnectionPoolType(poolType)
+               .setDiscoveryType(discType)
+```
+
+*Java driver*:
+
+```java
+LoadBalancingPolicy lbp = new TokenAwarePolicy(
+       DCAwareRoundRobinPolicy.builder()
+       .withLocalDc(""myDC"")
+       .build()
+);
+```
+
+*By default* the *Java driver* will instantiate the exact Load balancing policy 
+shown above, with the `LocalDC` being the DC of the first host the driver connects 
+to. So to get the same behaviour than the *TokenAware* pool type of *Astyanax*, 
+users shouldnâ€™t need to specify a load balancing policy since the default one 
+should cover it.
+
+Important : Note that since *CQL* is an abstraction of the Cassandraâ€™s architecture, a simple 
+query needs to have the *Row key* specified explicitly on a `Statement` in order 
+to benefit from the *TokenAware* routing (the *Row key* in the *Java driver* is 
+referenced as *Routing Key*), unlike the *Astyanax* driver. 
+Some differences occur related to the different kinds of `Statements` the *Java
+driver* provides. Please see [this link](../../../manual/statements) for specific information.
+
+Custom load balancing policies can easily be implemented by users, and supplied to 
+the *Java driver* for specific use cases. All information necessary is available
+in the [Load balaning policies docs](../../../manual/load_balancing).
+
+## Consistency levels
+Consistency levels can be set per-statement, or globally through the `QueryOptions`.
+
+*Astyanax*:
+
+```java
+AstyanaxConfigurationImpl aconf =
+       new AstyanaxConfigurationImpl()
+               .setDefaultReadConsistencyLevel(ConsistencyLevel.CL*ALL)
+               .setDefaultWriteConsistencyLevel(ConsistencyLevel.CL*ALL)
+```
+
+*Java driver*:
+
+```java
+QueryOptions qo = new QueryOptions().setConsistencyLevel(ConsistencyLevel.ALL);
+```
+
+Since the *Java driver* only executes *CQL* statements, which can be either reads
+or writes to *Cassandra*, it is not possible to globally configure the
+Consistency Level for only reads or only writes. To do so, since the Consistency 
+Level can be set per-statement, you can either set it on every statement, or use 
+`PreparedStatements` (if queries are to be repeated with different values): in
+this case, setting the CL on the `PreparedStatement`, causes the `BoundStatements` to 
+inherit the CL from the prepared statements they were prepared from. More
+informations about how `Statement`s work in the *Java driver* are detailed
+in the [â€œQueries and Resultâ€ section](../queries_and_results/).
+
+
+## Authentication
+
+Authentication settings are managed by the `AuthProvider` class in the *Java driver*.
+It can be highly customizable, but also comes with default simple implementations :","[{'comment': '[typo maniac warning] remove space before column\n', 'commenter': 'olim7t'}]"
732,upgrade_guide/migrating_from_astyanax/configuration/README.md,"@@ -0,0 +1,242 @@
+# Configuration
+
+## How Configuring the Java driver works
+
+The two basic components in the Java driver are the `Cluster` and the `Session`.
+The `Cluster` is the object to create first, and on to which apply all
+global configuration options. Connecting to the `Cluster` creates a
+`Session`. Queries are executed through the `Session`.
+
+The `Cluster` object then is to be viewed as the equivalent of the `AstyanaxContext`
+object. â€™Startingâ€™ an `AstyanaxContext` object typically returns a `Keyspace`
+object, the `Keyspace` object is the equivalent of the *Java driver*â€™s `Session`.
+
+Configuring a `Cluster` works with the *Builder* pattern. The `Builder` takes all
+the configurations into account before building the `Cluster`.
+
+Following are some examples of the most important configurations that were 
+possible with *Astyanax* and how to translate them into *DataStax Java driver*
+configurations.
+
+## Connection pools
+
+Configuration of connection pools in *Astyanax* are made through the
+`ConnectionPoolConfigurationImpl`. This object gathers important configurations
+that the *Java driver* has categorized in multiple *Option* and *Policy* kinds.
+
+### Connections pools internals
+Everything concerning the internal pools of connections to the *Cassandra nodes*
+will be gathered in the Java driver in the `PoolingOptions` :
+
+*Astyanax*:
+
+```java
+ConnectionPoolConfigurationImpl cpool =
+       new ConnectionPoolConfigurationImpl(""myConnectionPool"")
+               .setInitConnsPerHost(2)
+               .setMaxConnsPerHost(3)
+```
+
+*Java driver*:
+
+```java
+PoolingOptions poolingOptions =
+       new PoolingOptions()
+           .setMaxRequestsPerConnection(HostDistance.LOCAL, 1024)
+           .setCoreConnectionsPerHost(HostDistance.LOCAL, 2)
+           .setMaxConnectionsPerHost(HostDistance.LOCAL, 3);
+```
+
+Note that the *Java driver* allows multiple simultaneous requests on one single
+connection, as it is based upon the *Native protocol*, an asynchronous binary
+protocol that can handle up to 32768 simultaneous requests.
+
+### Timeouts
+
+Timeouts concerning requests, or connections will be part of the `SocketOptions`.
+
+*Astyanax*:
+
+```java
+ConnectionPoolConfigurationImpl cpool =
+       new ConnectionPoolConfigurationImpl(""myConnectionPool"")
+               .setSocketTimeout(3000)
+               .setConnectTimeout(3000)
+```
+
+*Java Driver :*
+
+```java
+SocketOptions so =
+       new SocketOptions()
+           .setReadTimeoutMillis(3000)
+           .setConnectTimeoutMillis(3000);
+```
+
+## Load Balancing
+Both *Astyanax* and the *Java driver* connect to multiple nodes of the *Cassandra*
+cluster. Distributing requests through all the nodes plays an important role in 
+the good operation of the `Cluster` and for best performances. With *Astyanax*, 
+requests (or â€œoperationsâ€) can be sent directly to replicas that have a copy of 
+the data targeted by the *â€œRow keyâ€* specified in the operation. Since the *Thrift* API is
+low-level, it forces the user to provide *Row keys*, known as the `TokenAware` 
+connection pool type. This setting is also present in the *Java driver*, however 
+the configuration is different and provides more options to tweak.
+
+Load balancing in the *Java driver* is a *Policy*, it is a class that will be
+plugged in the *Java driver*â€™s code and the Driver will call its methods when it 
+needs to. The *Java driver* comes with a preset of specific load balancing policies. 
+Hereâ€™s an equivalent code :
+
+*Astyanax*:
+
+```java
+final ConnectionPoolType poolType = ConnectionPoolType.TOKEN_AWARE;
+final NodeDiscoveryType discType = NodeDiscoveryType.RING_DESCRIBE;
+ConnectionPoolConfigurationImpl cpool =
+       new ConnectionPoolConfigurationImpl(""myConnectionPool"")
+               .setLocalDatacenter(""myDC"")
+AstyanaxConfigurationImpl aconf =
+       new AstyanaxConfigurationImpl()
+               .setConnectionPoolType(poolType)
+               .setDiscoveryType(discType)
+```
+
+*Java driver*:
+
+```java
+LoadBalancingPolicy lbp = new TokenAwarePolicy(
+       DCAwareRoundRobinPolicy.builder()
+       .withLocalDc(""myDC"")
+       .build()
+);
+```
+
+*By default* the *Java driver* will instantiate the exact Load balancing policy 
+shown above, with the `LocalDC` being the DC of the first host the driver connects 
+to. So to get the same behaviour than the *TokenAware* pool type of *Astyanax*, 
+users shouldnâ€™t need to specify a load balancing policy since the default one 
+should cover it.
+
+Important : Note that since *CQL* is an abstraction of the Cassandraâ€™s architecture, a simple 
+query needs to have the *Row key* specified explicitly on a `Statement` in order 
+to benefit from the *TokenAware* routing (the *Row key* in the *Java driver* is 
+referenced as *Routing Key*), unlike the *Astyanax* driver. 
+Some differences occur related to the different kinds of `Statements` the *Java
+driver* provides. Please see [this link](../../../manual/statements) for specific information.
+
+Custom load balancing policies can easily be implemented by users, and supplied to 
+the *Java driver* for specific use cases. All information necessary is available
+in the [Load balaning policies docs](../../../manual/load_balancing).
+
+## Consistency levels
+Consistency levels can be set per-statement, or globally through the `QueryOptions`.
+
+*Astyanax*:
+
+```java
+AstyanaxConfigurationImpl aconf =
+       new AstyanaxConfigurationImpl()
+               .setDefaultReadConsistencyLevel(ConsistencyLevel.CL*ALL)
+               .setDefaultWriteConsistencyLevel(ConsistencyLevel.CL*ALL)
+```
+
+*Java driver*:
+
+```java
+QueryOptions qo = new QueryOptions().setConsistencyLevel(ConsistencyLevel.ALL);
+```
+
+Since the *Java driver* only executes *CQL* statements, which can be either reads
+or writes to *Cassandra*, it is not possible to globally configure the
+Consistency Level for only reads or only writes. To do so, since the Consistency 
+Level can be set per-statement, you can either set it on every statement, or use 
+`PreparedStatements` (if queries are to be repeated with different values): in
+this case, setting the CL on the `PreparedStatement`, causes the `BoundStatements` to 
+inherit the CL from the prepared statements they were prepared from. More
+informations about how `Statement`s work in the *Java driver* are detailed
+in the [â€œQueries and Resultâ€ section](../queries_and_results/).
+
+
+## Authentication
+
+Authentication settings are managed by the `AuthProvider` class in the *Java driver*.
+It can be highly customizable, but also comes with default simple implementations :
+
+*Astyanax*:
+
+```java
+AuthenticationCredentials authCreds = new SimpleAuthenticationCredentials(""username"", ""password"");
+ConnectionPoolConfigurationImpl cpool =
+       new ConnectionPoolConfigurationImpl(""myConnectionPool"")
+               .setAuthenticationCredentials(authCreds)
+```
+
+*Java driver*:
+
+```java
+AuthProvider authProvider = new PlainTextAuthProvider(""username"", ""password"");
+```
+
+The class `AuthProvider` can be easily implemented to suit the userâ€™s needs,
+documentation about the classes needed is [available there](../../../manual/auth/).
+
+## Hosts and ports
+
+Setting the â€œseedsâ€ or first hosts to connect to can be done directly on the 
+Cluster configuration Builder :
+
+*Astyanax*:
+
+```java
+ConnectionPoolConfigurationImpl cpool =
+       new ConnectionPoolConfigurationImpl(""myConnectionPool"")
+               .setSeeds(""127.0.0.1"")
+               .setPort(9160)
+```
+
+*Java driver*:
+
+```java
+Cluster cluster = Cluster.builder()
+       .addContactPoint(""127.0.0.1"")
+       .withPort(9042)
+```
+
+The *Java driver* by default connects to port *9042*, hence you can supply only
+host names with the `addContactPoints(String...)` method. Note that the contact
+points are only the entry points to the `Cluster` for the *Automatic discovery
+phase*.
+
+## Building the Cluster
+With all options previously presented, one may configure and create the
+`Cluster` object this way :
+
+*Java driver*:
+
+```java
+Cluster cluster = Cluster.builder()
+       .addContactPoint(""127.0.0.1"")
+       .withAuthProvider(authProvider)
+       .withLoadBalancingPolicy(lbp)
+       .withSocketOptions(so)
+       .withPoolingOptions(poolingOptions)
+       .withQueryOptions(qo)
+       .build();
+Session session = cluster.connect();
+```
+
+## Best Practices
+
+A few best practices are summed up in [this blog post](http://www.datastax.com/dev/blog/4-simple-rules-when-using-the-datastax-drivers-for-cassandra).
+
+Concerning connection pools, the Java driverâ€™s default settings should allow 
+most of the users to get the best out of the driver in terms of throughput, 
+they have been thoroughly tested and tweaked to accommodate the usersâ€™ needs. 
+If one still wishes to change those, first [Monitoring the pools](../../../manual/pooling/#monitoring-and-tuning-the-pool) is
+advised, then a [deep dive in the Pools management mechanism](../../../manual/pooling/) should
+provide enough insight.
+
+A lot more options are available in the different `XxxxOption`s classes, policies are
+also highly customizable since the base drivers implementations can easily be 
+extended and implement users specific actions.","[{'comment': 'suggestion: user-specific actions\n', 'commenter': 'olim7t'}]"
732,upgrade_guide/migrating_from_astyanax/configuration/README.md,"@@ -0,0 +1,242 @@
+# Configuration
+
+## How Configuring the Java driver works
+
+The two basic components in the Java driver are the `Cluster` and the `Session`.
+The `Cluster` is the object to create first, and on to which apply all
+global configuration options. Connecting to the `Cluster` creates a
+`Session`. Queries are executed through the `Session`.
+
+The `Cluster` object then is to be viewed as the equivalent of the `AstyanaxContext`
+object. â€™Startingâ€™ an `AstyanaxContext` object typically returns a `Keyspace`
+object, the `Keyspace` object is the equivalent of the *Java driver*â€™s `Session`.
+
+Configuring a `Cluster` works with the *Builder* pattern. The `Builder` takes all
+the configurations into account before building the `Cluster`.
+
+Following are some examples of the most important configurations that were 
+possible with *Astyanax* and how to translate them into *DataStax Java driver*
+configurations.
+
+## Connection pools
+
+Configuration of connection pools in *Astyanax* are made through the
+`ConnectionPoolConfigurationImpl`. This object gathers important configurations
+that the *Java driver* has categorized in multiple *Option* and *Policy* kinds.
+
+### Connections pools internals
+Everything concerning the internal pools of connections to the *Cassandra nodes*
+will be gathered in the Java driver in the `PoolingOptions` :
+
+*Astyanax*:
+
+```java
+ConnectionPoolConfigurationImpl cpool =
+       new ConnectionPoolConfigurationImpl(""myConnectionPool"")
+               .setInitConnsPerHost(2)
+               .setMaxConnsPerHost(3)
+```
+
+*Java driver*:
+
+```java
+PoolingOptions poolingOptions =
+       new PoolingOptions()
+           .setMaxRequestsPerConnection(HostDistance.LOCAL, 1024)
+           .setCoreConnectionsPerHost(HostDistance.LOCAL, 2)
+           .setMaxConnectionsPerHost(HostDistance.LOCAL, 3);
+```
+
+Note that the *Java driver* allows multiple simultaneous requests on one single
+connection, as it is based upon the *Native protocol*, an asynchronous binary
+protocol that can handle up to 32768 simultaneous requests.
+
+### Timeouts
+
+Timeouts concerning requests, or connections will be part of the `SocketOptions`.
+
+*Astyanax*:
+
+```java
+ConnectionPoolConfigurationImpl cpool =
+       new ConnectionPoolConfigurationImpl(""myConnectionPool"")
+               .setSocketTimeout(3000)
+               .setConnectTimeout(3000)
+```
+
+*Java Driver :*
+
+```java
+SocketOptions so =
+       new SocketOptions()
+           .setReadTimeoutMillis(3000)
+           .setConnectTimeoutMillis(3000);
+```
+
+## Load Balancing
+Both *Astyanax* and the *Java driver* connect to multiple nodes of the *Cassandra*
+cluster. Distributing requests through all the nodes plays an important role in 
+the good operation of the `Cluster` and for best performances. With *Astyanax*, 
+requests (or â€œoperationsâ€) can be sent directly to replicas that have a copy of 
+the data targeted by the *â€œRow keyâ€* specified in the operation. Since the *Thrift* API is
+low-level, it forces the user to provide *Row keys*, known as the `TokenAware` 
+connection pool type. This setting is also present in the *Java driver*, however 
+the configuration is different and provides more options to tweak.
+
+Load balancing in the *Java driver* is a *Policy*, it is a class that will be
+plugged in the *Java driver*â€™s code and the Driver will call its methods when it 
+needs to. The *Java driver* comes with a preset of specific load balancing policies. 
+Hereâ€™s an equivalent code :
+
+*Astyanax*:
+
+```java
+final ConnectionPoolType poolType = ConnectionPoolType.TOKEN_AWARE;
+final NodeDiscoveryType discType = NodeDiscoveryType.RING_DESCRIBE;
+ConnectionPoolConfigurationImpl cpool =
+       new ConnectionPoolConfigurationImpl(""myConnectionPool"")
+               .setLocalDatacenter(""myDC"")
+AstyanaxConfigurationImpl aconf =
+       new AstyanaxConfigurationImpl()
+               .setConnectionPoolType(poolType)
+               .setDiscoveryType(discType)
+```
+
+*Java driver*:
+
+```java
+LoadBalancingPolicy lbp = new TokenAwarePolicy(
+       DCAwareRoundRobinPolicy.builder()
+       .withLocalDc(""myDC"")
+       .build()
+);
+```
+
+*By default* the *Java driver* will instantiate the exact Load balancing policy 
+shown above, with the `LocalDC` being the DC of the first host the driver connects 
+to. So to get the same behaviour than the *TokenAware* pool type of *Astyanax*, 
+users shouldnâ€™t need to specify a load balancing policy since the default one 
+should cover it.
+
+Important : Note that since *CQL* is an abstraction of the Cassandraâ€™s architecture, a simple 
+query needs to have the *Row key* specified explicitly on a `Statement` in order 
+to benefit from the *TokenAware* routing (the *Row key* in the *Java driver* is 
+referenced as *Routing Key*), unlike the *Astyanax* driver. 
+Some differences occur related to the different kinds of `Statements` the *Java
+driver* provides. Please see [this link](../../../manual/statements) for specific information.
+
+Custom load balancing policies can easily be implemented by users, and supplied to 
+the *Java driver* for specific use cases. All information necessary is available
+in the [Load balaning policies docs](../../../manual/load_balancing).
+
+## Consistency levels
+Consistency levels can be set per-statement, or globally through the `QueryOptions`.
+
+*Astyanax*:
+
+```java
+AstyanaxConfigurationImpl aconf =
+       new AstyanaxConfigurationImpl()
+               .setDefaultReadConsistencyLevel(ConsistencyLevel.CL*ALL)
+               .setDefaultWriteConsistencyLevel(ConsistencyLevel.CL*ALL)
+```
+
+*Java driver*:
+
+```java
+QueryOptions qo = new QueryOptions().setConsistencyLevel(ConsistencyLevel.ALL);
+```
+
+Since the *Java driver* only executes *CQL* statements, which can be either reads
+or writes to *Cassandra*, it is not possible to globally configure the
+Consistency Level for only reads or only writes. To do so, since the Consistency 
+Level can be set per-statement, you can either set it on every statement, or use 
+`PreparedStatements` (if queries are to be repeated with different values): in
+this case, setting the CL on the `PreparedStatement`, causes the `BoundStatements` to 
+inherit the CL from the prepared statements they were prepared from. More
+informations about how `Statement`s work in the *Java driver* are detailed
+in the [â€œQueries and Resultâ€ section](../queries_and_results/).
+
+
+## Authentication
+
+Authentication settings are managed by the `AuthProvider` class in the *Java driver*.
+It can be highly customizable, but also comes with default simple implementations :
+
+*Astyanax*:
+
+```java
+AuthenticationCredentials authCreds = new SimpleAuthenticationCredentials(""username"", ""password"");
+ConnectionPoolConfigurationImpl cpool =
+       new ConnectionPoolConfigurationImpl(""myConnectionPool"")
+               .setAuthenticationCredentials(authCreds)
+```
+
+*Java driver*:
+
+```java
+AuthProvider authProvider = new PlainTextAuthProvider(""username"", ""password"");
+```
+
+The class `AuthProvider` can be easily implemented to suit the userâ€™s needs,
+documentation about the classes needed is [available there](../../../manual/auth/).
+
+## Hosts and ports
+
+Setting the â€œseedsâ€ or first hosts to connect to can be done directly on the 
+Cluster configuration Builder :
+
+*Astyanax*:
+
+```java
+ConnectionPoolConfigurationImpl cpool =
+       new ConnectionPoolConfigurationImpl(""myConnectionPool"")
+               .setSeeds(""127.0.0.1"")
+               .setPort(9160)
+```
+
+*Java driver*:
+
+```java
+Cluster cluster = Cluster.builder()
+       .addContactPoint(""127.0.0.1"")
+       .withPort(9042)
+```
+
+The *Java driver* by default connects to port *9042*, hence you can supply only
+host names with the `addContactPoints(String...)` method. Note that the contact
+points are only the entry points to the `Cluster` for the *Automatic discovery
+phase*.
+
+## Building the Cluster
+With all options previously presented, one may configure and create the
+`Cluster` object this way :
+
+*Java driver*:
+
+```java
+Cluster cluster = Cluster.builder()
+       .addContactPoint(""127.0.0.1"")
+       .withAuthProvider(authProvider)
+       .withLoadBalancingPolicy(lbp)
+       .withSocketOptions(so)
+       .withPoolingOptions(poolingOptions)
+       .withQueryOptions(qo)
+       .build();
+Session session = cluster.connect();
+```
+
+## Best Practices
+
+A few best practices are summed up in [this blog post](http://www.datastax.com/dev/blog/4-simple-rules-when-using-the-datastax-drivers-for-cassandra).
+
+Concerning connection pools, the Java driverâ€™s default settings should allow 
+most of the users to get the best out of the driver in terms of throughput, 
+they have been thoroughly tested and tweaked to accommodate the usersâ€™ needs. 
+If one still wishes to change those, first [Monitoring the pools](../../../manual/pooling/#monitoring-and-tuning-the-pool) is
+advised, then a [deep dive in the Pools management mechanism](../../../manual/pooling/) should
+provide enough insight.
+
+A lot more options are available in the different `XxxxOption`s classes, policies are
+also highly customizable since the base drivers implementations can easily be ","[{'comment': 'only one driver?\n', 'commenter': 'olim7t'}]"
732,upgrade_guide/migrating_from_astyanax/queries_and_results/README.md,"@@ -0,0 +1,106 @@
+# Queries and Results
+There are many ressources such as [this post][planetCCqlLink] or [this post][dsBlogCqlLink] to learn
+how to transform previous Thrift operations to CQL queries.
+ 
+The *Java driver* executes CQL queries through the `Session`. 
+The queries can either be simple *CQL* Strings or represented in the form of 
+`Statement`s. The driver offers 4 kinds of statements, `SimpleStatement`, 
+`Prepared/BoundStatement`, `BuiltStatement`, `BatchStatement`. All necessary 
+information can be [found here](../../../manual/statements/) about the natures of the different
+`Statement`s.
+
+As explained in [this documentation section](../../../manual/#running-queries),
+results of a *CQL* query will be in the form of *Rows* from *Tables*, composed 
+of fixed set of columns, each with a type and a name. The driver exposes the 
+set of *Rows* returned from a query as a ResultSet, thus containing *Rows* on 
+which `getXXX()` can be called. Here are simple examples of translation from 
+*Astyanax* to *Java driver* in querying and retrieving query results.
+
+## Single column
+
+*Astyanax*:
+
+```java
+ColumnFamily<String, String> CF_STANDARD1 = new ColumnFamily<String, String>(""cf1"", StringSerializer.get(), StringSerializer.get(). StringSerializer.get());
+
+Column<String> result = keyspace.prepareQuery(CF_STANDARD1)
+    .getKey(""1"")
+    .getColumn(""3"")
+    .execute().getResult();
+String value = result.getStringValue();
+```
+
+*Java driver*:
+
+```
+Row row = session.execute(""SELECT value FROM table1 WHERE key = '1' AND column1 = '3'"").one();
+String value = row.getString(""value"");
+```
+
+## All columns
+
+*Astyanax*: 
+
+```java
+ColumnList<String> columns;
+int pagesize = 10;
+RowQuery<String, String> query = keyspace
+       .prepareQuery(CF_STANDARD1)
+       .getKey(""1"")
+       .autoPaginate(true)
+       .withColumnRange(new RangeBuilder().setLimit(pagesize).build());
+
+while (!(columns = query.execute().getResult()).isEmpty()) {
+   for (Column<String> c : columns) {
+       String value = c.getStringValue();
+   }
+}
+```
+
+*Java driver*:
+
+```java
+ResultSet rs = session.execute(""SELECT value FROM table1 WHERE key = '1'"");
+for (Row row : rs) {
+   String value = row.getString(""value"");
+}
+```
+
+## Column range
+
+*Astyanax*:
+
+```java
+ColumnList<String> result;
+result = keyspace.prepareQuery(CF_STANDARD1)
+       .getKey(""1"")
+       .withColumnRange(new RangeBuilder().setStart(""3"").setEnd(""5"").setMaxSize(100).build())
+       .execute().getResult();
+
+Iterator<Column<String>> it = result.iterator();
+while (it.hasNext()) {
+   Column<String> col = it.next();
+   String value = col.getStringValue();
+}
+```
+
+*Java driver*:
+
+```java
+ResultSet rs = session.execute(""SELECT value FROM table1 WHERE key = '1'"" +
+       "" AND column1 > '3'"" +
+       "" AND column1 < '5'"");
+for (Row row : rs) {
+   String value = row.getString(""value"");
+}
+```","[{'comment': 'ðŸ‘ love those examples\n', 'commenter': 'olim7t'}]"
732,upgrade_guide/migrating_from_astyanax/configuration/README.md,"@@ -0,0 +1,242 @@
+# Configuration
+
+## How Configuring the Java driver works
+
+The two basic components in the Java driver are the `Cluster` and the `Session`.
+The `Cluster` is the object to create first, and on to which apply all
+global configuration options. Connecting to the `Cluster` creates a
+`Session`. Queries are executed through the `Session`.
+
+The `Cluster` object then is to be viewed as the equivalent of the `AstyanaxContext`
+object. â€™Startingâ€™ an `AstyanaxContext` object typically returns a `Keyspace`
+object, the `Keyspace` object is the equivalent of the *Java driver*â€™s `Session`.
+
+Configuring a `Cluster` works with the *Builder* pattern. The `Builder` takes all
+the configurations into account before building the `Cluster`.
+
+Following are some examples of the most important configurations that were 
+possible with *Astyanax* and how to translate them into *DataStax Java driver*
+configurations.
+
+## Connection pools
+
+Configuration of connection pools in *Astyanax* are made through the
+`ConnectionPoolConfigurationImpl`. This object gathers important configurations
+that the *Java driver* has categorized in multiple *Option* and *Policy* kinds.
+
+### Connections pools internals
+Everything concerning the internal pools of connections to the *Cassandra nodes*
+will be gathered in the Java driver in the `PoolingOptions` :
+
+*Astyanax*:
+
+```java
+ConnectionPoolConfigurationImpl cpool =
+       new ConnectionPoolConfigurationImpl(""myConnectionPool"")
+               .setInitConnsPerHost(2)
+               .setMaxConnsPerHost(3)
+```
+
+*Java driver*:
+
+```java
+PoolingOptions poolingOptions =
+       new PoolingOptions()
+           .setMaxRequestsPerConnection(HostDistance.LOCAL, 1024)
+           .setCoreConnectionsPerHost(HostDistance.LOCAL, 2)","[{'comment': 'I think it would be better to use `setConnectionsPerHost(HostDistance.LOCAL, 2, 3)`, since I think we should encourage using that method.\n', 'commenter': 'tolbertam'}]"
732,upgrade_guide/migrating_from_astyanax/configuration/README.md,"@@ -0,0 +1,242 @@
+# Configuration
+
+## How Configuring the Java driver works
+
+The two basic components in the Java driver are the `Cluster` and the `Session`.
+The `Cluster` is the object to create first, and on to which apply all
+global configuration options. Connecting to the `Cluster` creates a
+`Session`. Queries are executed through the `Session`.
+
+The `Cluster` object then is to be viewed as the equivalent of the `AstyanaxContext`
+object. â€™Startingâ€™ an `AstyanaxContext` object typically returns a `Keyspace`
+object, the `Keyspace` object is the equivalent of the *Java driver*â€™s `Session`.
+
+Configuring a `Cluster` works with the *Builder* pattern. The `Builder` takes all
+the configurations into account before building the `Cluster`.
+
+Following are some examples of the most important configurations that were 
+possible with *Astyanax* and how to translate them into *DataStax Java driver*
+configurations.
+
+## Connection pools
+
+Configuration of connection pools in *Astyanax* are made through the
+`ConnectionPoolConfigurationImpl`. This object gathers important configurations
+that the *Java driver* has categorized in multiple *Option* and *Policy* kinds.
+
+### Connections pools internals
+Everything concerning the internal pools of connections to the *Cassandra nodes*
+will be gathered in the Java driver in the `PoolingOptions` :
+
+*Astyanax*:
+
+```java
+ConnectionPoolConfigurationImpl cpool =
+       new ConnectionPoolConfigurationImpl(""myConnectionPool"")
+               .setInitConnsPerHost(2)
+               .setMaxConnsPerHost(3)
+```
+
+*Java driver*:
+
+```java
+PoolingOptions poolingOptions =
+       new PoolingOptions()
+           .setMaxRequestsPerConnection(HostDistance.LOCAL, 1024)
+           .setCoreConnectionsPerHost(HostDistance.LOCAL, 2)
+           .setMaxConnectionsPerHost(HostDistance.LOCAL, 3);
+```
+
+Note that the *Java driver* allows multiple simultaneous requests on one single
+connection, as it is based upon the *Native protocol*, an asynchronous binary
+protocol that can handle up to 32768 simultaneous requests.
+
+### Timeouts
+
+Timeouts concerning requests, or connections will be part of the `SocketOptions`.
+
+*Astyanax*:
+
+```java
+ConnectionPoolConfigurationImpl cpool =
+       new ConnectionPoolConfigurationImpl(""myConnectionPool"")
+               .setSocketTimeout(3000)
+               .setConnectTimeout(3000)
+```
+
+*Java Driver :*
+
+```java
+SocketOptions so =","[{'comment': 'would be good to add note that timeouts should not be changed unless you are changing the timeouts in cassandra.yaml.\n', 'commenter': 'tolbertam'}]"
732,upgrade_guide/migrating_from_astyanax/configuration/README.md,"@@ -0,0 +1,242 @@
+# Configuration
+
+## How Configuring the Java driver works
+
+The two basic components in the Java driver are the `Cluster` and the `Session`.
+The `Cluster` is the object to create first, and on to which apply all
+global configuration options. Connecting to the `Cluster` creates a
+`Session`. Queries are executed through the `Session`.
+
+The `Cluster` object then is to be viewed as the equivalent of the `AstyanaxContext`
+object. â€™Startingâ€™ an `AstyanaxContext` object typically returns a `Keyspace`
+object, the `Keyspace` object is the equivalent of the *Java driver*â€™s `Session`.
+
+Configuring a `Cluster` works with the *Builder* pattern. The `Builder` takes all
+the configurations into account before building the `Cluster`.
+
+Following are some examples of the most important configurations that were ","[{'comment': 'It would be good to add a general comment that you should depend on the default configuration unless you have a good reason.  It\'d be nice to have a sentence after each section saying ""You shouldn\'t need to configure this unless...."".  I think users often change a lot of configuration options because they see it in examples, but in reality they shouldn\'t need to.\n', 'commenter': 'tolbertam'}]"
732,upgrade_guide/migrating_from_astyanax/configuration/README.md,"@@ -0,0 +1,242 @@
+# Configuration
+
+## How Configuring the Java driver works
+
+The two basic components in the Java driver are the `Cluster` and the `Session`.
+The `Cluster` is the object to create first, and on to which apply all
+global configuration options. Connecting to the `Cluster` creates a
+`Session`. Queries are executed through the `Session`.
+
+The `Cluster` object then is to be viewed as the equivalent of the `AstyanaxContext`
+object. â€™Startingâ€™ an `AstyanaxContext` object typically returns a `Keyspace`
+object, the `Keyspace` object is the equivalent of the *Java driver*â€™s `Session`.
+
+Configuring a `Cluster` works with the *Builder* pattern. The `Builder` takes all
+the configurations into account before building the `Cluster`.
+
+Following are some examples of the most important configurations that were 
+possible with *Astyanax* and how to translate them into *DataStax Java driver*
+configurations.
+
+## Connection pools
+
+Configuration of connection pools in *Astyanax* are made through the
+`ConnectionPoolConfigurationImpl`. This object gathers important configurations
+that the *Java driver* has categorized in multiple *Option* and *Policy* kinds.
+
+### Connections pools internals
+Everything concerning the internal pools of connections to the *Cassandra nodes*
+will be gathered in the Java driver in the `PoolingOptions` :
+
+*Astyanax*:
+
+```java
+ConnectionPoolConfigurationImpl cpool =
+       new ConnectionPoolConfigurationImpl(""myConnectionPool"")
+               .setInitConnsPerHost(2)
+               .setMaxConnsPerHost(3)
+```
+
+*Java driver*:
+
+```java
+PoolingOptions poolingOptions =
+       new PoolingOptions()
+           .setMaxRequestsPerConnection(HostDistance.LOCAL, 1024)
+           .setCoreConnectionsPerHost(HostDistance.LOCAL, 2)
+           .setMaxConnectionsPerHost(HostDistance.LOCAL, 3);
+```
+
+Note that the *Java driver* allows multiple simultaneous requests on one single
+connection, as it is based upon the *Native protocol*, an asynchronous binary
+protocol that can handle up to 32768 simultaneous requests.
+
+### Timeouts
+
+Timeouts concerning requests, or connections will be part of the `SocketOptions`.
+
+*Astyanax*:
+
+```java
+ConnectionPoolConfigurationImpl cpool =
+       new ConnectionPoolConfigurationImpl(""myConnectionPool"")
+               .setSocketTimeout(3000)
+               .setConnectTimeout(3000)
+```
+
+*Java Driver :*
+
+```java
+SocketOptions so =
+       new SocketOptions()
+           .setReadTimeoutMillis(3000)
+           .setConnectTimeoutMillis(3000);
+```
+
+## Load Balancing
+Both *Astyanax* and the *Java driver* connect to multiple nodes of the *Cassandra*
+cluster. Distributing requests through all the nodes plays an important role in 
+the good operation of the `Cluster` and for best performances. With *Astyanax*, 
+requests (or â€œoperationsâ€) can be sent directly to replicas that have a copy of 
+the data targeted by the *â€œRow keyâ€* specified in the operation. Since the *Thrift* API is
+low-level, it forces the user to provide *Row keys*, known as the `TokenAware` 
+connection pool type. This setting is also present in the *Java driver*, however 
+the configuration is different and provides more options to tweak.
+
+Load balancing in the *Java driver* is a *Policy*, it is a class that will be
+plugged in the *Java driver*â€™s code and the Driver will call its methods when it 
+needs to. The *Java driver* comes with a preset of specific load balancing policies. 
+Hereâ€™s an equivalent code :
+
+*Astyanax*:
+
+```java
+final ConnectionPoolType poolType = ConnectionPoolType.TOKEN_AWARE;
+final NodeDiscoveryType discType = NodeDiscoveryType.RING_DESCRIBE;
+ConnectionPoolConfigurationImpl cpool =
+       new ConnectionPoolConfigurationImpl(""myConnectionPool"")
+               .setLocalDatacenter(""myDC"")
+AstyanaxConfigurationImpl aconf =
+       new AstyanaxConfigurationImpl()
+               .setConnectionPoolType(poolType)
+               .setDiscoveryType(discType)
+```
+
+*Java driver*:
+
+```java
+LoadBalancingPolicy lbp = new TokenAwarePolicy(
+       DCAwareRoundRobinPolicy.builder()
+       .withLocalDc(""myDC"")
+       .build()
+);
+```
+
+*By default* the *Java driver* will instantiate the exact Load balancing policy 
+shown above, with the `LocalDC` being the DC of the first host the driver connects 
+to. So to get the same behaviour than the *TokenAware* pool type of *Astyanax*, 
+users shouldnâ€™t need to specify a load balancing policy since the default one 
+should cover it.
+
+Important : Note that since *CQL* is an abstraction of the Cassandraâ€™s architecture, a simple 
+query needs to have the *Row key* specified explicitly on a `Statement` in order 
+to benefit from the *TokenAware* routing (the *Row key* in the *Java driver* is 
+referenced as *Routing Key*), unlike the *Astyanax* driver. 
+Some differences occur related to the different kinds of `Statements` the *Java
+driver* provides. Please see [this link](../../../manual/statements) for specific information.
+
+Custom load balancing policies can easily be implemented by users, and supplied to 
+the *Java driver* for specific use cases. All information necessary is available
+in the [Load balaning policies docs](../../../manual/load_balancing).
+
+## Consistency levels
+Consistency levels can be set per-statement, or globally through the `QueryOptions`.
+
+*Astyanax*:
+
+```java
+AstyanaxConfigurationImpl aconf =
+       new AstyanaxConfigurationImpl()
+               .setDefaultReadConsistencyLevel(ConsistencyLevel.CL*ALL)
+               .setDefaultWriteConsistencyLevel(ConsistencyLevel.CL*ALL)
+```
+
+*Java driver*:
+
+```java
+QueryOptions qo = new QueryOptions().setConsistencyLevel(ConsistencyLevel.ALL);
+```
+
+Since the *Java driver* only executes *CQL* statements, which can be either reads
+or writes to *Cassandra*, it is not possible to globally configure the
+Consistency Level for only reads or only writes. To do so, since the Consistency 
+Level can be set per-statement, you can either set it on every statement, or use 
+`PreparedStatements` (if queries are to be repeated with different values): in
+this case, setting the CL on the `PreparedStatement`, causes the `BoundStatements` to 
+inherit the CL from the prepared statements they were prepared from. More
+informations about how `Statement`s work in the *Java driver* are detailed
+in the [â€œQueries and Resultâ€ section](../queries_and_results/).","[{'comment': 'Should be ""Queries and Results""\n', 'commenter': 'tolbertam'}]"
732,upgrade_guide/migrating_from_astyanax/language_level_changes/README.md,"@@ -0,0 +1,66 @@
+# Language change : from Thrift to CQL
+The data model changes when using *CQL* (Cassandra Query Language).
+*CQL* is providing an abstraction of the low-level data stored in *Cassandra*, in
+opposition to *Thrift* that aims to expose the low-level data structure directly.
+[But note that this changes with Cassandra 3â€™s new storage engine.](http://www.datastax.com/2015/12/storage-engine-30)
+
+*Thrift* exposes *Keyspaces*, and these *Keyspaces* contain *Column Families*. A
+*ColumnFamily* contains *Rows* in which each *Row* has a list of an arbitrary number
+of column-values. With *CQL*, the data is **tabular**, *ColumnFamily* gets viewed
+as a *Table*, the **Table Rows** get a **fixed and finite number of named columns**.","[{'comment': ""In one case Table Rows is bolded for emphasis, and in another case it's underlined, is that intentional?\n"", 'commenter': 'tolbertam'}]"
732,upgrade_guide/migrating_from_astyanax/queries_and_results/README.md,"@@ -0,0 +1,106 @@
+# Queries and Results
+There are many ressources such as [this post][planetCCqlLink] or [this post][dsBlogCqlLink] to learn","[{'comment': 'ressources -> resources\n', 'commenter': 'tolbertam'}]"
732,upgrade_guide/migrating_from_astyanax/queries_and_results/README.md,"@@ -0,0 +1,106 @@
+# Queries and Results
+There are many ressources such as [this post][planetCCqlLink] or [this post][dsBlogCqlLink] to learn
+how to transform previous Thrift operations to CQL queries.
+ 
+The *Java driver* executes CQL queries through the `Session`. 
+The queries can either be simple *CQL* Strings or represented in the form of 
+`Statement`s. The driver offers 4 kinds of statements, `SimpleStatement`, 
+`Prepared/BoundStatement`, `BuiltStatement`, `BatchStatement`. All necessary ","[{'comment': ', and BatchStatement\n', 'commenter': 'tolbertam'}]"
732,upgrade_guide/migrating_from_astyanax/queries_and_results/README.md,"@@ -0,0 +1,106 @@
+# Queries and Results
+There are many ressources such as [this post][planetCCqlLink] or [this post][dsBlogCqlLink] to learn
+how to transform previous Thrift operations to CQL queries.
+ 
+The *Java driver* executes CQL queries through the `Session`. 
+The queries can either be simple *CQL* Strings or represented in the form of 
+`Statement`s. The driver offers 4 kinds of statements, `SimpleStatement`, 
+`Prepared/BoundStatement`, `BuiltStatement`, `BatchStatement`. All necessary 
+information can be [found here](../../../manual/statements/) about the natures of the different","[{'comment': 'natures -> nature\n', 'commenter': 'tolbertam'}]"
732,upgrade_guide/migrating_from_astyanax/queries_and_results/README.md,"@@ -0,0 +1,106 @@
+# Queries and Results
+There are many ressources such as [this post][planetCCqlLink] or [this post][dsBlogCqlLink] to learn
+how to transform previous Thrift operations to CQL queries.
+ 
+The *Java driver* executes CQL queries through the `Session`. 
+The queries can either be simple *CQL* Strings or represented in the form of 
+`Statement`s. The driver offers 4 kinds of statements, `SimpleStatement`, 
+`Prepared/BoundStatement`, `BuiltStatement`, `BatchStatement`. All necessary 
+information can be [found here](../../../manual/statements/) about the natures of the different
+`Statement`s.
+
+As explained in [this documentation section](../../../manual/#running-queries),","[{'comment': ""suggest renaming 'this documentation section' 'the 'running queries' section'\n"", 'commenter': 'tolbertam'}]"
747,driver-core/src/main/java/com/datastax/driver/core/RequestHandler.java,"@@ -382,23 +404,11 @@ private void retry(final boolean retryCurrent, ConsistencyLevel newConsistencyLe
             if (newConsistencyLevel != null)
                 this.retryConsistencyLevel = newConsistencyLevel;
 
-            // We should not retry on the current thread as this will be an IO thread.
-            manager.executor().execute(new Runnable() {
-                @Override
-                public void run() {
-                    if (queryStateRef.get().isCancelled())
-                        return;
-                    try {
-                        if (retryCurrent) {
-                            if (query(h))
-                                return;
-                        }
-                        sendRequest();
-                    } catch (Exception e) {
-                        setFinalException(null, new DriverInternalError(""Unexpected exception while retrying query"", e));
-                    }
-                }
-            });
+            if (queryStateRef.get().isCancelled())
+                return;
+
+            if (!retryCurrent || !query(h))","[{'comment': 'So retries are now executed on Netty I/O threads?\n', 'commenter': 'adutra'}, {'comment': ""Yes. There's nothing blocking or compute-intensive on that path anymore: if the pool is busy or the keyspace needs to be changed, that will be handled asynchronously via the future returned from borrow. So I figured the extra indirection wasn't worth it anymore.\n"", 'commenter': 'olim7t'}, {'comment': 'Nice,  I remember seeing retries fill up the Cluster executor if your pool timeout > 0 on retries and pool was full.\n', 'commenter': 'tolbertam'}]"
747,driver-core/src/main/java/com/datastax/driver/core/RequestHandler.java,"@@ -261,12 +264,18 @@ private RetryPolicy retryPolicy() {
                 logger.trace(""[{}] Starting"", id);
         }
 
-        void sendRequest() {
+        void findNextHostAndQuery() {
             try {
                 Host host;
                 while (!isDone.get() && (host = queryPlan.next()) != null && !queryStateRef.get().isCancelled()) {
+
+                    HostConnectionPool pool = manager.pools.get(host);","[{'comment': 'What is the motivation for checking the pool here _and_ inside the `query` method?\n', 'commenter': 'adutra'}, {'comment': ""Yes that's overkill, I'll get rid of this one.\n"", 'commenter': 'olim7t'}]"
747,driver-core/src/main/java/com/datastax/driver/core/Connection.java,"@@ -94,8 +96,10 @@
 
     private final AtomicReference<Owner> ownerRef = new AtomicReference<Owner>();
 
+    private final ListenableFuture<Connection> thisFuture;
+","[{'comment': 'minor nit: one extra blank line here.\n', 'commenter': 'adutra'}, {'comment': 'removed\n', 'commenter': 'olim7t'}]"
747,driver-core/src/main/java/com/datastax/driver/core/PoolingOptions.java,"@@ -400,30 +406,48 @@ public PoolingOptions setIdleTimeoutSeconds(int idleTimeoutSeconds) {
     }
 
     /**
-     * Returns the timeout when trying to acquire a connection from a host's pool.
-     *
-     * @return the timeout.
+     * @deprecated see {@link #setPoolTimeoutMillis(int)}. This method always returns 0.
      */
+    @Deprecated
     public int getPoolTimeoutMillis() {
-        return poolTimeoutMillis;
+        return 0;
+    }
+
+    /**
+     * @deprecated the connection pool does not use a timeout anymore, incoming requests are now throttled with a
+     * threshold on the {@link #setMaxQueueSize(int)}  queue size}. This method has no effect.
+     */
+    @Deprecated
+    public PoolingOptions setPoolTimeoutMillis(int poolTimeoutMillis) {
+        return this;
     }
 
     /**
-     * Sets the timeout when trying to acquire a connection from a host's pool.
+     * Returns the maximum number of requests that get enqueued if no connection is available.
+     *
+     * @return the maximum queue size.
+     */
+    public int getMaxQueueSize() {","[{'comment': 'Would it be worth giving this parameter a more meaningful name, e.g. `maxPendingConnectionRequests`, and making it clear in the javadocs that this number is meant _per connection pool_ (and not global)?\n', 'commenter': 'adutra'}, {'comment': 'I wouldn\'t use ""request"" because it\'s too similar to ""network request"", but if you can think of something else I\'m open to suggestions, it\'s true that maxQueueSize is not great.\n\nI added a sentence to the javadoc of the setter as you suggested.\n', 'commenter': 'olim7t'}, {'comment': ""I think the javadoc on the setter is really good.  I can't think of a better name other than `setMaxQueueSize` (but I am bad at coming up with names :) ).\n"", 'commenter': 'tolbertam'}, {'comment': '`maxConnectionWaitQueueSize`? Otherwise I throw in the towel too.\n', 'commenter': 'adutra'}]"
747,driver-core/src/main/java/com/datastax/driver/core/HostConnectionPool.java,"@@ -384,7 +309,32 @@ public void returnConnection(Connection connection) {
             if (connection.maxAvailableStreams() < minAllowedStreams) {
                 replaceConnection(connection);
             } else {
-                signalAvailableConnection();
+                dequeue(connection);
+            }
+        }
+    }
+
+    private void dequeue(Connection connection) {
+        while (!pendingBorrows.isEmpty()) {
+
+            // Try to acquire the right to use the connection
+            while (true) {
+                int inFlight = connection.inFlight.get();
+                if (inFlight >= Math.min(connection.maxAvailableStreams(), options().getMaxRequestsPerConnection(hostDistance))) {
+                    // Connection is full again, stop dequeuing
+                    return;
+                }
+                if (connection.inFlight.compareAndSet(inFlight, inFlight + 1))","[{'comment': 'Nice, I like this approach compared to the previous `signalAvailableConnection()` approach.    \n\nIt would be good to add a comment here why `inFlight` is incremented as it was initially confusing to me, but now I see that the previous implementation would increment inFlight as soon as it got the connection in `waitForConnection`.  It seems more natural in the previous implementation, but it also makes sense here (just took me a moment to get it).   It is done here instead in the different places where the behavior is handling when the Future completes and provides the connection (which at that point you could be outside scope of HostConnectionPool).  \n\nMy current interpretation is:\n\n`if (connection.inFlight.compareAndSet(inFlight, inFlight + 1))`: Increment inFlight on the connection to account for a new borrow request.\n', 'commenter': 'tolbertam'}, {'comment': ""ðŸ‘ for commenting a bit more this line's intention, it made me pause for a couple seconds too.\n"", 'commenter': 'adutra'}, {'comment': 'I tried to explain it in the comment above the `while` loop: a successful increment is the precondition that gives you the ""right"" to use the connection. I\'ll see how I can explain it better.\n', 'commenter': 'olim7t'}]"
747,driver-core/src/main/java/com/datastax/driver/core/HostConnectionPool.java,"@@ -62,9 +59,8 @@
     @VisibleForTesting
     final Set<Connection> trash = new CopyOnWriteArraySet<Connection>();
 
-    private volatile int waiter = 0;
-    private final Lock waitLock = new ReentrantLock(true);
-    private final Condition hasAvailableConnection = waitLock.newCondition();
+    private final Queue<SettableFuture<Connection>> pendingBorrows = new ConcurrentLinkedQueue<SettableFuture<Connection>>();
+    private final AtomicInteger pendingBorrowCount = new AtomicInteger();","[{'comment': ""Why not use `pendingBorrows.size()` instead of a separate counter?  I think it is because `ConcurrentLinkedQueue.size()` is not constant time, but was just wondering.   As we check this to determine whether or not we've reached `maxQueueSize` i worry about the count getting off somehow, but i see it is only incremented and decremented in 2 occurrences, so I am probably overworrying :)\n"", 'commenter': 'tolbertam'}, {'comment': 'It also reminded me some bad memories from `EventDebouncer` :) but indeed I think the counter is required because `size()` is O(N). I think the counter and the queue could be out of sync momentarily but I did not find any situation where this would lead to a bad behavior of the pool.\n', 'commenter': 'adutra'}, {'comment': ""Yes, it's because `size()` is not constant.\n"", 'commenter': 'olim7t'}]"
747,driver-core/src/main/java/com/datastax/driver/core/PoolingOptions.java,"@@ -400,30 +406,48 @@ public PoolingOptions setIdleTimeoutSeconds(int idleTimeoutSeconds) {
     }
 
     /**
-     * Returns the timeout when trying to acquire a connection from a host's pool.
-     *
-     * @return the timeout.
+     * @deprecated see {@link #setPoolTimeoutMillis(int)}. This method always returns 0.
      */
+    @Deprecated
     public int getPoolTimeoutMillis() {
-        return poolTimeoutMillis;
+        return 0;
+    }
+
+    /**
+     * @deprecated the connection pool does not use a timeout anymore, incoming requests are now throttled with a
+     * threshold on the {@link #setMaxQueueSize(int)}  queue size}. This method has no effect.","[{'comment': 'dangling `}`, `{@link #setMaxQueueSize(int)}  queue size-->}<--` \n', 'commenter': 'tolbertam'}, {'comment': 'fixed\n', 'commenter': 'olim7t'}]"
747,driver-core/src/main/java/com/datastax/driver/core/PoolingOptions.java,"@@ -151,7 +157,7 @@
     private volatile int maxRequestsPerConnectionRemote = UNSET;
 
     private volatile int idleTimeoutSeconds = DEFAULT_IDLE_TIMEOUT_SECONDS;
-    private volatile int poolTimeoutMillis = DEFAULT_POOL_TIMEOUT_MILLIS;
+    private volatile int maxQueueSize = DEFAULT_MAX_QUEUE_SIZE;","[{'comment': 'Thinking out loud, one small concern I have about using a queue instead of a timeout is that since there is no timeout on awaiting an available connection now it could be possible that a Future returned from `executeAsync` may never be completed if for some reason there is a bug in `HostConnectionPool`.  \n\nWell, I suppose that possibility has always existed, there would need to be a bug in the driver for it to be a problem, which we would then have to fix :).    If a user wanted to be paranoid about it, they could timeout their Futures themselves (which is a good practice anyways).\n', 'commenter': 'tolbertam'}]"
747,driver-core/src/main/java/com/datastax/driver/core/HostConnectionPool.java,"@@ -612,8 +562,9 @@ public final CloseFuture closeAsync() {
 
         phase.set(Phase.CLOSING);
 
-        // Wake up all threads that wait
-        signalAllAvailableConnection();
+        for (SettableFuture<Connection> pendingBorrow : pendingBorrows) {","[{'comment': ""It looks like we use the pool phase to protect us from enqueuing borrows for a Pool that is already closed, but isn't there a possibility that a request may be added to `pendingBorrows` afterwards?\n\nThe possibility seems very slim, but couldn't the following happen:\n1. thread-X: Enters `borrowConnection`, pool phase is `READY` so code proceeds.\n2. thread-Y: Enters `closeAsync`, sets phase to `CLOSING`, fails futures in pendingBorrows.\n3. thread-X: Notices `connections.isEmpty`evaluates to true, spawns new connection and calls `enqueue`.  Or any other condition that causes `enqueue` to be called.\n\nThere is now a future in `pendingBorrows` which never completes right?  That window seems really tiny but it does seem possible.  Might be able to reproduce it in a debugger and adding some breakpoints.\n"", 'commenter': 'tolbertam'}, {'comment': ""Yes, I agree there's a race there.\n"", 'commenter': 'olim7t'}]"
747,driver-core/src/main/java/com/datastax/driver/core/HostConnectionPool.java,"@@ -276,93 +264,30 @@ public Connection borrowConnection(long timeout, TimeUnit unit) throws Connectio
                 maybeSpawnNewConnection();
         }
 
-        leastBusy.setKeyspace(manager.poolsState.keyspace);
-        return leastBusy;
-    }
-
-    private void awaitAvailableConnection(long timeout, TimeUnit unit) throws InterruptedException {
-        waitLock.lock();
-        waiter++;
-        try {
-            hasAvailableConnection.await(timeout, unit);
-        } finally {
-            waiter--;
-            waitLock.unlock();
-        }
-    }
-
-    private void signalAvailableConnection() {
-        // Quick check if it's worth signaling to avoid locking
-        if (waiter == 0)
-            return;
-
-        waitLock.lock();
-        try {
-            hasAvailableConnection.signal();
-        } finally {
-            waitLock.unlock();
-        }
+        return leastBusy.setKeyspaceAsync(manager.poolsState.keyspace);
     }
 
-    private void signalAllAvailableConnection() {
-        // Quick check if it's worth signaling to avoid locking
-        if (waiter == 0)
-            return;
-
-        waitLock.lock();
-        try {
-            hasAvailableConnection.signalAll();
-        } finally {
-            waitLock.unlock();
+    private ListenableFuture<Connection> enqueue(int maxQueueSize) {
+        if (maxQueueSize == 0) {
+            return Futures.immediateFailedFuture(new ConnectionException(host.getSocketAddress(),
+                    ""Pool is busy (no available connection and the max queue size is 0)""));
         }
-    }
-
-    private Connection waitForConnection(long timeout, TimeUnit unit) throws ConnectionException, TimeoutException {
-        if (timeout == 0)
-            throw new TimeoutException(""All connections are busy and pool timeout is 0"");
-
-        long start = System.nanoTime();
-        long remaining = timeout;
-        do {
-            try {
-                awaitAvailableConnection(remaining, unit);
-            } catch (InterruptedException e) {
-                Thread.currentThread().interrupt();
-                // If we're interrupted fine, check if there is a connection available but stop waiting otherwise
-                timeout = 0; // this will make us stop the loop if we don't get a connection right away
-            }
 
-            if (isClosed())
-                throw new ConnectionException(host.getSocketAddress(), ""Pool is shutdown"");
-
-            int minInFlight = Integer.MAX_VALUE;
-            Connection leastBusy = null;
-            for (Connection connection : connections) {
-                int inFlight = connection.inFlight.get();
-                if (inFlight < minInFlight) {
-                    minInFlight = inFlight;
-                    leastBusy = connection;
-                }
+        while (true) {
+            int count = pendingBorrowCount.get();
+            if (count >= maxQueueSize) {
+                return Futures.immediateFailedFuture(new ConnectionException(host.getSocketAddress(),
+                        ""Pool is busy (no available connection and the queue has reached its max size "" + maxQueueSize
+                                + "" )""));","[{'comment': ""There's an extra space here, i.e. : `reached its max size 256 )`\n"", 'commenter': 'tolbertam'}]"
747,driver-core/src/main/java/com/datastax/driver/core/HostConnectionPool.java,"@@ -384,7 +309,32 @@ public void returnConnection(Connection connection) {
             if (connection.maxAvailableStreams() < minAllowedStreams) {
                 replaceConnection(connection);
             } else {
-                signalAvailableConnection();
+                dequeue(connection);
+            }
+        }
+    }
+
+    private void dequeue(Connection connection) {
+        while (!pendingBorrows.isEmpty()) {
+
+            // Try to acquire the right to use the connection
+            while (true) {
+                int inFlight = connection.inFlight.get();
+                if (inFlight >= Math.min(connection.maxAvailableStreams(), options().getMaxRequestsPerConnection(hostDistance))) {
+                    // Connection is full again, stop dequeuing
+                    return;
+                }
+                if (connection.inFlight.compareAndSet(inFlight, inFlight + 1))
+                    break;
+            }
+
+            SettableFuture<Connection> pendingBorrow = pendingBorrows.poll();
+            if (pendingBorrow == null) {
+                // Another thread has emptied the queue since our last check, restore the count
+                connection.inFlight.decrementAndGet();
+            } else {
+                pendingBorrowCount.decrementAndGet();","[{'comment': 'I observed in testing that `totalInFlight` was becoming offset and over time become a negative number.  I think it is because `totalInFlight` is not incremented here.\n', 'commenter': 'tolbertam'}]"
747,driver-core/src/main/java/com/datastax/driver/core/RequestHandler.java,"@@ -278,48 +279,64 @@ void sendRequest() {
         }
 
         private boolean query(final Host host) {
-            HostConnectionPool currentPool = manager.pools.get(host);
-            if (currentPool == null || currentPool.isClosed())
+            HostConnectionPool pool = manager.pools.get(host);
+            if (pool == null || pool.isClosed())
                 return false;
 
+            if (logger.isTraceEnabled())
+                logger.trace(""[{}] Querying node {}"", id, host);
+
             if (allowSpeculativeExecutions && nextExecutionScheduled.compareAndSet(false, true))
                 scheduleExecution(speculativeExecutionPlan.nextExecution(host));
 
-            Connection connection = null;
-            try {
-                connection = currentPool.borrowConnection(manager.configuration().getPoolingOptions().getPoolTimeoutMillis(), TimeUnit.MILLISECONDS);
-                if (current != null) {
-                    if (triedHosts == null)
-                        triedHosts = new CopyOnWriteArrayList<Host>();
-                    triedHosts.add(current);
+            ListenableFuture<Connection> connectionFuture = pool.borrowConnection(
+                    manager.configuration().getPoolingOptions().getMaxQueueSize());
+            Futures.addCallback(connectionFuture, new FutureCallback<Connection>() {
+                @Override
+                public void onSuccess(Connection connection) {
+                    if (current != null) {
+                        if (triedHosts == null)
+                            triedHosts = new CopyOnWriteArrayList<Host>();
+                        triedHosts.add(current);
+                    }
+                    current = host;
+                    try {
+                        write(connection, SpeculativeExecution.this);
+                    } catch (ConnectionException e) {
+                        // If we have any problem with the connection, move to the next node.
+                        if (metricsEnabled())
+                            metrics().getErrorMetrics().getConnectionErrors().inc();
+                        if (connection != null)
+                            connection.release();
+                        logError(host.getSocketAddress(), e);
+                        findNextHostAndQuery();
+                    } catch (BusyConnectionException e) {
+                        // The pool shouldn't have give us a busy connection unless we've maxed up the pool, so move on to the next host.
+                        connection.release();
+                        logError(host.getSocketAddress(), e);
+                        findNextHostAndQuery();
+                    } catch (RuntimeException e) {
+                        if (connection != null)
+                            connection.release();
+                        logger.error(""Unexpected error while querying "" + host.getAddress(), e);
+                        logError(host.getSocketAddress(), e);
+                        findNextHostAndQuery();
+                    }
                 }
-                current = host;
-                write(connection, this);
-                return true;
-            } catch (ConnectionException e) {
-                // If we have any problem with the connection, move to the next node.
-                if (metricsEnabled())
-                    metrics().getErrorMetrics().getConnectionErrors().inc();
-                if (connection != null)
-                    connection.release();
-                logError(host.getSocketAddress(), e);
-                return false;
-            } catch (BusyConnectionException e) {
-                // The pool shouldn't have give us a busy connection unless we've maxed up the pool, so move on to the next host.
-                connection.release();
-                logError(host.getSocketAddress(), e);
-                return false;
-            } catch (TimeoutException e) {
-                // We timeout, log it but move to the next node.
-                logError(host.getSocketAddress(), new DriverException(""Timeout while trying to acquire available connection (you may want to increase the driver number of per-host connections)"", e));
-                return false;
-            } catch (RuntimeException e) {
-                if (connection != null)
-                    connection.release();
-                logger.error(""Unexpected error while querying "" + host.getAddress(), e);
-                logError(host.getSocketAddress(), e);
-                return false;
-            }
+
+                @Override
+                public void onFailure(Throwable t) {
+                    if (t instanceof TimeoutException) {
+                        logError(host.getSocketAddress(),","[{'comment': 'Is `TimeoutException` possible?  I noticed the `Pool is Busy` exception is a `ConnectionException`.  \n', 'commenter': 'tolbertam'}]"
747,driver-core/src/main/java/com/datastax/driver/core/exceptions/BusyPoolException.java,"@@ -0,0 +1,80 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.core.exceptions;
+
+import com.datastax.driver.core.HostDistance;
+import com.datastax.driver.core.Statement;
+
+import java.net.InetAddress;
+import java.net.InetSocketAddress;
+
+/**
+ * Indicates that a connection pool has run out of available connections.
+ * <p/>
+ * This happens if the pool has no connections (for example if it's currently reconnecting to its host), or if all
+ * connections have reached their maximum number of in flight queries. The query will be retried on the next host in the
+ * {@link com.datastax.driver.core.policies.LoadBalancingPolicy#newQueryPlan(String, Statement) query plan}.
+ * <p/>
+ * This exception is a symptom that the driver is experiencing a high workload. If it happens regularly on all hosts,
+ * you should consider tuning one (or a combination of) the following pooling options:
+ * <ul>
+ * <li>{@link com.datastax.driver.core.PoolingOptions#setMaxRequestsPerConnection(HostDistance, int)}: maximum number of
+ * requests per connection;</li>
+ * <li>{@link com.datastax.driver.core.PoolingOptions#setMaxConnectionsPerHost(HostDistance, int)}: maximum number of
+ * connections in the pool;</li>
+ * <li>{@link com.datastax.driver.core.PoolingOptions#setMaxQueueSize(int)}: maximum number of enqueued requests before
+ * this exception is thrown.</li>
+ * </ul>
+ */
+public class BusyPoolException extends DriverException implements CoordinatorException {
+
+    private static final long serialVersionUID = 0;
+
+    private final InetSocketAddress address;
+    private final int queueSize;
+
+    public BusyPoolException(InetSocketAddress address, int queueSize) {
+        this(address, queueSize, null);
+    }
+
+    private BusyPoolException(InetSocketAddress address, int queueSize, Throwable cause) {
+        super(buildMessage(address, queueSize), cause);
+        this.address = address;
+        this.queueSize = queueSize;
+    }
+
+    private static String buildMessage(InetSocketAddress address, int queueSize) {
+        return String.format(""[%s] Pool is busy (no available connection and the queue has reached its max size %d"",","[{'comment': 'Missing closing `)`\n', 'commenter': 'tolbertam'}]"
747,driver-core/src/main/java/com/datastax/driver/core/PoolingOptions.java,"@@ -415,7 +415,7 @@ public int getPoolTimeoutMillis() {
 
     /**
      * @deprecated the connection pool does not use a timeout anymore, incoming requests are now throttled with a
-     * threshold on the {@link #setMaxQueueSize(int)}  queue size}. This method has no effect.
+     * threshold on the {@link #setMaxQueueSize(int) queue size}. This method has no effect.","[{'comment': 'Should the closing `}` be after `(int)`?\n', 'commenter': 'tolbertam'}, {'comment': ""it's a named link\n"", 'commenter': 'olim7t'}, {'comment': ""oh, I didn't even know that was a thing you could do, nice!\n"", 'commenter': 'tolbertam'}]"
747,driver-core/src/main/java/com/datastax/driver/core/HostConnectionPool.java,"@@ -285,6 +285,13 @@ private PoolingOptions options() {
 
         SettableFuture<Connection> future = SettableFuture.create();
         pendingBorrows.add(future);
+
+        // If we raced with shutdown, make sure the future will be completed. This has no effect if it was properly
+        // handled in closeAsync.
+        if (phase.get() == Phase.CLOSING) {","[{'comment': 'Nice, for some reason I thought handling this would be much more complex, but checking the state of the pool after adding the future makes perfect sense.\n', 'commenter': 'tolbertam'}]"
748,driver-core/src/test/java/com/datastax/driver/core/FrameLengthTest.java,"@@ -0,0 +1,128 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.core;
+
+import com.datastax.driver.core.exceptions.NoHostAvailableException;
+import com.datastax.driver.core.querybuilder.Insert;
+import com.datastax.driver.core.schemabuilder.Create;
+import com.datastax.driver.core.schemabuilder.SchemaBuilder;
+import org.apache.log4j.Level;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import org.testng.annotations.Test;
+
+import java.nio.ByteBuffer;
+import java.util.Random;
+
+import static com.datastax.driver.core.TestUtils.ipOfNode;
+import static com.datastax.driver.core.TestUtils.waitForUp;
+import static com.datastax.driver.core.querybuilder.QueryBuilder.*;
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.assertj.core.api.Fail.fail;
+
+@CCMConfig(numberOfNodes = 2)
+public class FrameLengthTest extends CCMTestsSupport {
+
+    static {
+        // Set max frame size to 1MB to make it easier to manifest frame length error.
+        System.setProperty(""com.datastax.driver.NATIVE_TRANSPORT_MAX_FRAME_SIZE_IN_MB"", ""1"");
+    }
+
+    Logger logger = LoggerFactory.getLogger(FrameLengthTest.class);
+
+    private static final String tableName = ""blob_table"";
+    private static final int colCount = 256;
+    private static final int rowsPerPartitionCount = 4;
+    private static final int partitionCount = 1;
+    private static final int bytesPerCol = 1024;
+
+    @Override
+    public void onTestContextInitialized() {
+        logger.info(""Creating table {} with {} {}-byte blob columns"", tableName, colCount, bytesPerCol);
+        Random random = new Random();
+        // Create table
+        Create create = SchemaBuilder.createTable(tableName).addPartitionKey(""k"", DataType.cint()).addClusteringColumn(""c"", DataType.cint());
+        for (int i = 0; i < colCount; i++) {
+            create.addColumn(""col"" + i, DataType.blob());
+        }
+        execute(create.getQueryString());
+
+        // build prepared statement.
+        Insert insert = insertInto(tableName).value(""k"", bindMarker()).value(""c"", bindMarker());
+        for (int i = 0; i < colCount; i++) {
+            insert = insert.value(""col"" + i, bindMarker());
+        }
+
+        PreparedStatement prepared = session().prepare(insert);
+
+        // Insert rows.
+        logger.info(""Inserting data for {} partitions."", partitionCount);
+        for (int i = 0; i < partitionCount; i++) {
+            logger.info(""Inserting {} rows in partition {}"", rowsPerPartitionCount, i);
+            for (int r = 0; r < rowsPerPartitionCount; r++) {
+                BoundStatement stmt = prepared.bind();
+                stmt.setInt(""k"", i);
+                stmt.setInt(""c"", r);
+                for (int c = 0; c < colCount; c++) {
+                    byte[] b = new byte[bytesPerCol];
+                    random.nextBytes(b);
+                    ByteBuffer in = ByteBuffer.wrap(b);
+                    stmt.setBytes(""col"" + c, in);
+                }
+                session().execute(stmt);
+            }
+        }
+        logger.info(""Done loading {}"", tableName);
+    }
+
+    /**
+     * Validates that if a frame is received that exceeds NATIVE_TRANSPORT_MAX_FRAME_SIZE_IN_MB that
+     * the driver is able to recover and reconnect to each host and make subsequent queries.  It
+     * configures NATIVE_TRANSPORT_MAX_FRAME_SIZE_IN_MB to 1 MB to make the error easier to reproduce.
+     *
+     * @jira_ticket JAVA-1292
+     * @jira_ticket JAVA-1293
+     * @test_category connection
+     */
+    @Test(groups = ""isolated"")
+    public void should_throw_exception_when_frame_exceeds_configured_max() {
+        // Capture the connection logger to check for TooLongFrameException.
+        org.apache.log4j.Logger connectionLogger = org.apache.log4j.Logger.getLogger(Connection.class);
+        Level originalLevel = connectionLogger.getLevel();
+        connectionLogger.setLevel(Level.DEBUG);
+        MemoryAppender logs = new MemoryAppender();
+        connectionLogger.addAppender(logs);
+        try {
+            session().execute(select().from(tableName).where(eq(""k"", 0)));
+            fail(""Exception expected"");
+        } catch (NoHostAvailableException e) {
+            // Both hosts should have been tried.
+            assertThat(e.getErrors()).hasSize(2);
+        } finally {
+            // Check that TooLongFrameException is encountered as it isn't raised to the NoHostAvailableException.
+            assertThat(logs.get()).contains(""io.netty.handler.codec.TooLongFrameException"");
+            connectionLogger.setLevel(originalLevel);
+            connectionLogger.removeAppender(logs);
+        }
+        // Connection will be lost for both hosts, but they should be able to reconnect.
+        waitForUp(ipOfNode(1), cluster(), 10);","[{'comment': 'When this exception is encountered a `DriverInternalError` is raised, which causes the connection to be defuncted and the request retried on the next host, which will also be defuncted.  Is this the right behavior?  It might be better to fail the request, closed the connection (nor not?) and not try on the next host.\n', 'commenter': 'tolbertam'}, {'comment': ""Yes, it would be better to fail the request (i.e. rethrow the error to the client) to stop retries. With your fix the connection is still working, so I don't see any reason to mark it defunct.\n"", 'commenter': 'olim7t'}, {'comment': ""ðŸ‘ , I'll see if I can find a good way to make it fail and not retry.\n"", 'commenter': 'tolbertam'}, {'comment': ""There are a few things to overcome around that:\n1. If we hit a frame too large, the decoder discards all readable bytes up until the frame length.  However, if readable bytes < frame length, all bytes are discarded and the Decoder enters a state where it will throw away bytes up to the frame size.  This works well, however in the parent `Frame.Decoder` it checks the protocol version and opcode every time regardless of whether or not we are in this state and we can't detect the current state in the subclass `DecoderForStreamIdSize` (from what I can tell).  Will keep looking into that.  EDIT:  We could disable failFast, which will cause the decoder to not fail until we've consumed the entire frame.  EDIT 2: That won't work either since the frame bytes get skipped until the frame is done.\n2. From what I can tell, there's no where else in code where an exception raised while Decoding does anything but close the connection.   We can handle this by checking the exception cause in `Connection.Dispatcher.handlerCaught` and then notifying the appropriate handler (we'd have to parse the stream id to do that, but I can overcome that) and not defuncting the connection.\n"", 'commenter': 'tolbertam'}, {'comment': '1. Need to decide whether or not this exception should be passed to `RetryPolicy#onRequestError`.  I would think not, since the default policy retries all request errors, and we likely would not want this ever to be retried.\n', 'commenter': 'tolbertam'}, {'comment': ""I think I can work around the first problem by doing the following:\n1. After the first message, we assume the version of all frames will be the same on the same connection.  This way we don't have to parse version every time.\n2. Check the opcode after creating the frame.  The Frame body is not parsed until later anyways, so this should be ok, the disadvantage of this is that we may fail later on in the process. I.E. if the payload is malformed and the full frame length is never received, the request would just time out instead.  That might not be too bad if the data is bad anyways.\n"", 'commenter': 'tolbertam'}, {'comment': ""That seems to work, but may not be the best route.   I'll push something up for review.\n"", 'commenter': 'tolbertam'}]"
748,driver-core/src/test/java/com/datastax/driver/core/FrameLengthTest.java,"@@ -0,0 +1,128 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.core;
+
+import com.datastax.driver.core.exceptions.NoHostAvailableException;
+import com.datastax.driver.core.querybuilder.Insert;
+import com.datastax.driver.core.schemabuilder.Create;
+import com.datastax.driver.core.schemabuilder.SchemaBuilder;
+import org.apache.log4j.Level;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import org.testng.annotations.Test;
+
+import java.nio.ByteBuffer;
+import java.util.Random;
+
+import static com.datastax.driver.core.TestUtils.ipOfNode;
+import static com.datastax.driver.core.TestUtils.waitForUp;
+import static com.datastax.driver.core.querybuilder.QueryBuilder.*;
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.assertj.core.api.Fail.fail;
+
+@CCMConfig(numberOfNodes = 2)
+public class FrameLengthTest extends CCMTestsSupport {
+
+    static {
+        // Set max frame size to 1MB to make it easier to manifest frame length error.
+        System.setProperty(""com.datastax.driver.NATIVE_TRANSPORT_MAX_FRAME_SIZE_IN_MB"", ""1"");","[{'comment': 'this gets evaluated earlier then I would have expected and breaks other tests.  Will fix.\n', 'commenter': 'tolbertam'}]"
748,driver-core/src/main/java/com/datastax/driver/core/Frame.java,"@@ -201,19 +201,32 @@ protected void decode(ChannelHandlerContext ctx, ByteBuf buffer, List<Object> ou
             // version first bit is the ""direction"" of the frame (request or response)
             version = version & 0x7F;
 
-            DecoderForStreamIdSize decoder = (version >= 3) ? decoderV3 : decoderV1;
+            DecoderForStreamIdSize decoder = decoderForStreamIdSize.get();
+            if (decoder == null) {","[{'comment': ""Needed because we can't be sure the protocol version the server is using until we actually get something from it.  Although I'm not sure an `AtomicReference` is necessary here, i'd expect `decode` will only called one at a time and from the same i/o thread, wdyt?  In general could there be a better approach than this?\n"", 'commenter': 'tolbertam'}, {'comment': 'I think the only change required here is to remove the `static` modifier so that the decoders are not shared among connections.\n\nThe following makes your test pass as well:\n\n``` java\n    static final class Decoder extends ByteToMessageDecoder {\n        private final DecoderForStreamIdSize decoderV1 = new DecoderForStreamIdSize(1);\n        private final DecoderForStreamIdSize decoderV3 = new DecoderForStreamIdSize(2);\n\n        @Override\n        protected void decode(ChannelHandlerContext ctx, ByteBuf buffer, List<Object> out) throws Exception {\n            if (buffer.readableBytes() < 1)\n                return;\n\n            int version = buffer.getByte(0);\n            // version first bit is the ""direction"" of the frame (request or response)\n            version = version & 0x7F;\n\n            DecoderForStreamIdSize decoder = (version >= 3) ? decoderV3 : decoderV1;\n            Object frame = decoder.decode(ctx, buffer);\n            if (frame != null)\n                out.add(frame);\n        }\n...\n```\n', 'commenter': 'adutra'}, {'comment': ""That will work in the general case, the main reason for restricting to one `DecoderForStreamIdSize` is that if you reach the frame length limit, the `DecoderForStreamIdSize` instance tracks the remaining bytes to consume and discards them until remnant of the frame is discarded.  Although in general you should only expect frames sharing the same protocol version, and if that didn't happen the code I have here would fail in unexpected ways anyways, so maybe the best solution is to do as you suggest.\n"", 'commenter': 'tolbertam'}, {'comment': ""Hmm I see. Conceptually speaking, you are right that `Decoder` should instantiate only one `DecoderForStreamIdSize` and use it consistently, so I'm ok with your approach as well. \n"", 'commenter': 'adutra'}, {'comment': ""> you are right that Decoder should instantiate only one DecoderForStreamIdSize and use it consistently, so I'm ok with your approach as well.\n\nSounds good, I think if we can count on C\\* always sending the same protocol version on a connection this approach works, but added an assertion just to see if that would ever be a problem (it helped me capture the bug i had previously where I passed in the protocolVersion originally).   Wasn't sure if using an `AtomicReference` was required or not.  I'd expect decode to only ever be called one at a time, but wasn't sure if that was a guarantee netty makes.\n"", 'commenter': 'tolbertam'}, {'comment': ""The decoder is a `ChannelHandler`, so as you noted previously `decode` will always be invoked from the same thread (the channel's). You're safe using a plain private field without any synchronization.\n"", 'commenter': 'olim7t'}, {'comment': ""> You're safe using a plain private field without any synchronization.\n\nawesome, I thought that was the case but wasn't completely sure,  I'll nix the `AtomicReference`.\n"", 'commenter': 'tolbertam'}]"
748,driver-core/src/main/java/com/datastax/driver/core/Frame.java,"@@ -201,19 +201,32 @@ protected void decode(ChannelHandlerContext ctx, ByteBuf buffer, List<Object> ou
             // version first bit is the ""direction"" of the frame (request or response)
             version = version & 0x7F;
 
-            DecoderForStreamIdSize decoder = (version >= 3) ? decoderV3 : decoderV1;
+            DecoderForStreamIdSize decoder = decoderForStreamIdSize.get();
+            if (decoder == null) {
+                decoder = new DecoderForStreamIdSize(version, version >= 3 ? 2 : 1);
+                if (!decoderForStreamIdSize.compareAndSet(null, decoder)) {
+                    decoder = decoderForStreamIdSize.get();
+                }
+            }
+
+            // the version in the message should match the version this Decoder is configured for.
+            assert decoder.protocolVersion == version;
+
             Object frame = decoder.decode(ctx, buffer);
             if (frame != null)
                 out.add(frame);
         }
 
         static class DecoderForStreamIdSize extends LengthFieldBasedFrameDecoder {
-            private static final int MAX_FRAME_LENGTH = 256 * 1024 * 1024; // 256 MB
+            // The maximum response frame length allowed.  Note that C* does not currently restrict the length of its responses (CASSANDRA-12630).
+            private static final int MAX_FRAME_LENGTH = SystemProperties.getInt(""com.datastax.driver.NATIVE_TRANSPORT_MAX_FRAME_SIZE_IN_MB"", 256) * 1024 * 1024; // 256 MB","[{'comment': 'This could also go into `ProtocolOptions`, however the annoying thing with that class is that every new field requires a new `Cluster.Builder` method (there is no `withProtocolOptions()`). Changing the max frame size is an edge case, so I guess this can stay as a system property.\n', 'commenter': 'olim7t'}, {'comment': 'Sounds good ðŸ‘ , I agree that it would be unlikely that this will be a configuration users will very rarely touch.\n', 'commenter': 'tolbertam'}]"
748,driver-core/src/main/java/com/datastax/driver/core/Frame.java,"@@ -189,54 +190,62 @@ Frame with(ByteBuf newBody) {
     }
 
     static final class Decoder extends ByteToMessageDecoder {
-        static final DecoderForStreamIdSize decoderV1 = new DecoderForStreamIdSize(1);
-        static final DecoderForStreamIdSize decoderV3 = new DecoderForStreamIdSize(2);
+        private DecoderForStreamIdSize decoder;
 
         @Override
         protected void decode(ChannelHandlerContext ctx, ByteBuf buffer, List<Object> out) throws Exception {
             if (buffer.readableBytes() < 1)
                 return;
 
-            int version = buffer.getByte(buffer.readerIndex());
-            // version first bit is the ""direction"" of the frame (request or response)
-            version = version & 0x7F;
+            // Initialize sub decoder on first message.  No synchronization needed as
+            // decode is always called from same thread.
+            if (decoder == null) {
+                int version = buffer.getByte(buffer.readerIndex());
+                // version first bit is the ""direction"" of the frame (request or response)
+                version = version & 0x7F;
+                decoder = new DecoderForStreamIdSize(version, version >= 3 ? 2 : 1);
+            }
 
-            DecoderForStreamIdSize decoder = (version >= 3) ? decoderV3 : decoderV1;
             Object frame = decoder.decode(ctx, buffer);
             if (frame != null)
                 out.add(frame);
         }
 
         static class DecoderForStreamIdSize extends LengthFieldBasedFrameDecoder {
-            private static final int MAX_FRAME_LENGTH = 256 * 1024 * 1024; // 256 MB
-            private final int opcodeOffset;
+            // The maximum response frame length allowed.  Note that C* does not currently restrict the length of its responses (CASSANDRA-12630).
+            private static final int MAX_FRAME_LENGTH = SystemProperties.getInt(""com.datastax.driver.NATIVE_TRANSPORT_MAX_FRAME_SIZE_IN_MB"", 256) * 1024 * 1024; // 256 MB
+            private final int protocolVersion;
 
-            DecoderForStreamIdSize(int streamIdSize) {
+            DecoderForStreamIdSize(int protocolVersion, int streamIdSize) {
                 super(MAX_FRAME_LENGTH, /*lengthOffset=*/ 3 + streamIdSize, 4, 0, 0, true);
-                this.opcodeOffset = 2 + streamIdSize;
+                this.protocolVersion = protocolVersion;
             }
 
             @Override
             protected Object decode(ChannelHandlerContext ctx, ByteBuf buffer) throws Exception {
+                // Capture current index in case we need to get the stream id.
+                // If a TooLongFrameException is thrown the readerIndex will advance to the end of
+                // the buffer (or past the frame) so we need the position as we entered this method.
+                int curIndex = buffer.readerIndex();
                 try {
-                    if (buffer.readableBytes() < opcodeOffset + 1)
-                        return null;
-
-                    // Validate the opcode (this will throw if it's not a response)
-                    Message.Response.Type.fromOpcode(buffer.getByte(buffer.readerIndex() + opcodeOffset));
-
                     ByteBuf frame = (ByteBuf) super.decode(ctx, buffer);
                     if (frame == null) {
                         return null;
                     }
                     // Do not deallocate `frame` just yet, because it is stored as Frame.body and will be used
                     // in Message.ProtocolDecoder or Frame.Decompressor if compression is enabled (we deallocate
                     // it there).
-                    return Frame.create(frame);
+                    Frame theFrame = Frame.create(frame);
+                    // Validate the opcode (this will throw if it's not a response)
+                    Message.Response.Type.fromOpcode(theFrame.header.opcode);","[{'comment': ""One difference w/ previous implementation was it previously parsed the opcode before calling `super.decode`, this wouldn't work if you were in the middle of discarding a frame because the previous frame bytes would have already been discarded.   This would explain why you previously would get an exception like:\n\n```\nio.netty.handler.codec.DecoderException: com.datastax.driver.core.exceptions.DriverInternalError: Unknown response opcode XX\n```\n\nafter a `TooLongFrameException` because the opcode bytes would have been previously discarded.\n\nThis means that if you get an invalid opcode, it now won't defunct the connection until we've received the whole frame.  I don't think that is a problem, but thought I'd point out the difference.\n"", 'commenter': 'tolbertam'}]"
748,driver-core/src/main/java/com/datastax/driver/core/exceptions/TooLongFrameException.java,"@@ -0,0 +1,46 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.core.exceptions;
+
+/**
+ * Indicates that the response frame for a request exceeded
+ * {@link com.datastax.driver.core.Frame.Decoder.DecoderForStreamIdSize#MAX_FRAME_LENGTH}
+ * (default: 256MB, configurable via com.datastax.driver.NATIVE_TRANSPORT_MAX_FRAME_SIZE_IN_MB
+ * system property) and thus was not parsed.
+ */
+public class TooLongFrameException extends DriverException {","[{'comment': ""Shouldn't this be considered a subtype of `DriverInternalError`?\n"", 'commenter': 'adutra'}, {'comment': ""I considered that, but this comment in the `DriverInternalError` documentation scared me away:\n\n```\n This should never be raised and indicates a bug (either in the driver or in Cassandra).\n```\n\nI didn't consider this to be a bug so I put it at the `DriverException` level instead, but I can be convinced to put it under `DriverInternalError` too.\n"", 'commenter': 'tolbertam'}, {'comment': 'Ok makes sense to not inherit from DIE, thanks for clarifying.\n', 'commenter': 'adutra'}, {'comment': ""This will be exposed to the client (as the `DefaultResultSetFuture`'s cause, or rethrown from the sync API), so definite +1 for `DriverException`.\nFor the same reason, I think it should have a message, and it's probably not necessary to nest the cause (which is a Netty type so not part of our API, and doesn't bring any additional info).\n"", 'commenter': 'olim7t'}, {'comment': ""`FrameTooLongException` sounds more natural, WDYT? (think of `ClassNotFoundException`).\n\nI know it will look weird in the code where we interact with the Netty exception, but I'd take that for a better name in our public API.\n"", 'commenter': 'olim7t'}, {'comment': ""`FrameTooLongException` sounds more natural to me as well, i'll also not pass the netty exception in the cause.\n"", 'commenter': 'tolbertam'}]"
748,driver-core/src/main/java/com/datastax/driver/core/Connection.java,"@@ -993,14 +994,15 @@ protected void channelRead0(ChannelHandlerContext ctx, Message.Response response
             ResponseHandler handler = pending.remove(streamId);
             streamIdHandler.release(streamId);
             if (handler == null) {
-                /**
+                /*
                  * During normal operation, we should not receive responses for which we don't have a handler. There is
                  * two cases however where this can happen:
                  *   1) The connection has been defuncted due to some internal error and we've raced between removing the
                  *      handler and actually closing the connection; since the original error has been logged, we're fine
                  *      ignoring this completely.
                  *   2) This request has timed out. In that case, we've already switched to another host (or errored out
                  *      to the user). So log it for debugging purpose, but it's fine ignoring otherwise.
+                 *   3) The connection encountered a frame too large to process.","[{'comment': ""In case of frame too large, wouldn't `exceptionCaught()` be called instead of `channelRead0()`?\n"", 'commenter': 'adutra'}, {'comment': ""You are right, I was more commenting on the fact that this is a scenario where there is no handler, but it does seem wrong for this to be here since when this happens it'd be handled in `exceptionCaught()` like you said.   I'll fix this ðŸ‘ \n"", 'commenter': 'tolbertam'}]"
748,driver-core/src/main/java/com/datastax/driver/core/Frame.java,"@@ -189,54 +190,62 @@ Frame with(ByteBuf newBody) {
     }
 
     static final class Decoder extends ByteToMessageDecoder {
-        static final DecoderForStreamIdSize decoderV1 = new DecoderForStreamIdSize(1);
-        static final DecoderForStreamIdSize decoderV3 = new DecoderForStreamIdSize(2);
+        private DecoderForStreamIdSize decoder;
 
         @Override
         protected void decode(ChannelHandlerContext ctx, ByteBuf buffer, List<Object> out) throws Exception {
             if (buffer.readableBytes() < 1)
                 return;
 
-            int version = buffer.getByte(buffer.readerIndex());
-            // version first bit is the ""direction"" of the frame (request or response)
-            version = version & 0x7F;
+            // Initialize sub decoder on first message.  No synchronization needed as
+            // decode is always called from same thread.
+            if (decoder == null) {
+                int version = buffer.getByte(buffer.readerIndex());
+                // version first bit is the ""direction"" of the frame (request or response)
+                version = version & 0x7F;
+                decoder = new DecoderForStreamIdSize(version, version >= 3 ? 2 : 1);
+            }
 
-            DecoderForStreamIdSize decoder = (version >= 3) ? decoderV3 : decoderV1;
             Object frame = decoder.decode(ctx, buffer);
             if (frame != null)
                 out.add(frame);
         }
 
         static class DecoderForStreamIdSize extends LengthFieldBasedFrameDecoder {
-            private static final int MAX_FRAME_LENGTH = 256 * 1024 * 1024; // 256 MB
-            private final int opcodeOffset;
+            // The maximum response frame length allowed.  Note that C* does not currently restrict the length of its responses (CASSANDRA-12630).
+            private static final int MAX_FRAME_LENGTH = SystemProperties.getInt(""com.datastax.driver.NATIVE_TRANSPORT_MAX_FRAME_SIZE_IN_MB"", 256) * 1024 * 1024; // 256 MB
+            private final int protocolVersion;
 
-            DecoderForStreamIdSize(int streamIdSize) {
+            DecoderForStreamIdSize(int protocolVersion, int streamIdSize) {
                 super(MAX_FRAME_LENGTH, /*lengthOffset=*/ 3 + streamIdSize, 4, 0, 0, true);
-                this.opcodeOffset = 2 + streamIdSize;
+                this.protocolVersion = protocolVersion;
             }
 
             @Override
             protected Object decode(ChannelHandlerContext ctx, ByteBuf buffer) throws Exception {
+                // Capture current index in case we need to get the stream id.
+                // If a TooLongFrameException is thrown the readerIndex will advance to the end of
+                // the buffer (or past the frame) so we need the position as we entered this method.
+                int curIndex = buffer.readerIndex();
                 try {
-                    if (buffer.readableBytes() < opcodeOffset + 1)
-                        return null;
-
-                    // Validate the opcode (this will throw if it's not a response)
-                    Message.Response.Type.fromOpcode(buffer.getByte(buffer.readerIndex() + opcodeOffset));
-
                     ByteBuf frame = (ByteBuf) super.decode(ctx, buffer);
                     if (frame == null) {
                         return null;
                     }
                     // Do not deallocate `frame` just yet, because it is stored as Frame.body and will be used
                     // in Message.ProtocolDecoder or Frame.Decompressor if compression is enabled (we deallocate
                     // it there).
-                    return Frame.create(frame);
+                    Frame theFrame = Frame.create(frame);
+                    // Validate the opcode (this will throw if it's not a response)
+                    Message.Response.Type.fromOpcode(theFrame.header.opcode);
+                    return theFrame;
                 } catch (CorruptedFrameException e) {
                     throw new DriverInternalError(e);
-                } catch (TooLongFrameException e) {
-                    throw new DriverInternalError(e);
+                } catch (io.netty.handler.codec.TooLongFrameException e) {
+                    int streamId = protocolVersion > 2 ?
+                            buffer.getShort(curIndex + 2) :
+                            buffer.getByte(curIndex + 2);
+                    throw new TooLongFrameException(streamId, e);","[{'comment': ""We catch a subclass of `DecoderException`, wrap it in our own type which will itself be wrapped into another `DecoderException` by the calling Netty code. In `exceptionCaught` we unwrap the outer one to rethrow our exception to the client.\nThat's a bit convoluted but I think it's acceptable for this edge case. I've thought of rethrowing Netty's `TooLongFrameException` directly, but that means we'd have to store the streamId elsewhere (maybe as a context attribute) which is a bit less clean.\nSo no suggestion really but I just wanted to mention it :)\n"", 'commenter': 'olim7t'}, {'comment': ""I agree 100% with you there, it is very convoluted and this was the part of the change I was least comfortable with.   I didn't even catch the detail that `TooLongFrameException` was a subclass of `DecoderException` so it is even stranger than I had thought :).\n"", 'commenter': 'tolbertam'}]"
757,driver-core/src/main/java/com/datastax/driver/core/CodecRegistry.java,"@@ -142,28 +152,30 @@
     private static final Logger logger = LoggerFactory.getLogger(CodecRegistry.class);
 
     @SuppressWarnings(""unchecked"")
-    private static final ImmutableSet<TypeCodec<?>> PRIMITIVE_CODECS = ImmutableSet.of(
-            TypeCodec.blob(),
-            TypeCodec.cboolean(),
-            TypeCodec.smallInt(),
-            TypeCodec.tinyInt(),
-            TypeCodec.cint(),
-            TypeCodec.bigint(),
-            TypeCodec.counter(),
-            TypeCodec.cdouble(),
-            TypeCodec.cfloat(),
-            TypeCodec.varint(),
-            TypeCodec.decimal(),
-            TypeCodec.varchar(), // must be declared before AsciiCodec so it gets chosen when CQL type not available
-            TypeCodec.ascii(),
-            TypeCodec.timestamp(),
-            TypeCodec.date(),
-            TypeCodec.time(),
-            TypeCodec.uuid(), // must be declared before TimeUUIDCodec so it gets chosen when CQL type not available
-            TypeCodec.timeUUID(),
-            TypeCodec.inet(),
-            TypeCodec.duration()
-    );
+    private static final ImmutableMap<DataType, TypeCodec<?>> PRIMITIVE_CODECS = ImmutableMap.copyOf(
+            new LinkedHashMap<DataType, TypeCodec<?>>() {
+        {
+            put(TypeCodec.blob().getCqlType(), TypeCodec.blob());
+            put(TypeCodec.cboolean().getCqlType(), TypeCodec.cboolean());
+            put(TypeCodec.smallInt().getCqlType(), TypeCodec.smallInt());
+            put(TypeCodec.tinyInt().getCqlType(), TypeCodec.tinyInt());
+            put(TypeCodec.cint().getCqlType(), TypeCodec.cint());
+            put(TypeCodec.bigint().getCqlType(), TypeCodec.bigint());
+            put(TypeCodec.counter().getCqlType(), TypeCodec.counter());
+            put(TypeCodec.cdouble().getCqlType(), TypeCodec.cdouble());
+            put(TypeCodec.cfloat().getCqlType(), TypeCodec.cfloat());
+            put(TypeCodec.varint().getCqlType(), TypeCodec.varint());
+            put(TypeCodec.decimal().getCqlType(), TypeCodec.decimal());
+            put(TypeCodec.varchar().getCqlType(), TypeCodec.varchar()); // must be declared before AsciiCodec so it gets chosen when CQL type not available
+            put(TypeCodec.ascii().getCqlType(), TypeCodec.ascii());
+            put(TypeCodec.timestamp().getCqlType(), TypeCodec.timestamp());
+            put(TypeCodec.date().getCqlType(), TypeCodec.date());
+            put(TypeCodec.time().getCqlType(), TypeCodec.time());
+            put(TypeCodec.uuid().getCqlType(), TypeCodec.uuid()); // must be declared before TimeUUIDCodec so it gets chosen when CQL type not available
+            put(TypeCodec.timeUUID().getCqlType(), TypeCodec.timeUUID());
+            put(TypeCodec.inet().getCqlType(), TypeCodec.inet());","[{'comment': 'With the recent merge of JAVA-1347, this should have `duration` as well.', 'commenter': 'olim7t'}]"
757,driver-core/src/main/java/com/datastax/driver/core/CodecRegistry.java,"@@ -294,7 +306,7 @@ public void onRemoval(RemovalNotification<CacheKey, TypeCodec<?>> notification)
      * Creates a new instance initialized with built-in codecs for all the base CQL types.
      */
     public CodecRegistry() {
-        this.codecs = new CopyOnWriteArrayList<TypeCodec<?>>(PRIMITIVE_CODECS);
+        this.codecs = new CopyOnWriteArrayList<TypeCodec<?>>();","[{'comment': ""Putting this here because I can't comment on unmodified lines: the doc of the `codecs` field (line 297) is now outdated:\r\n```java\r\n/**\r\n * The list of registered codecs.\r\n * This list is initialized with the built-in codecs;\r\n * User-defined codecs are appended to the list.\r\n */\r\nprivate final CopyOnWriteArrayList<TypeCodec<?>> codecs;\r\n```"", 'commenter': 'olim7t'}]"
757,driver-core/src/main/java/com/datastax/driver/core/CodecRegistry.java,"@@ -476,9 +488,14 @@ public CodecRegistry register(Iterable<? extends TypeCodec<?>> codecs) {
         checkNotNull(cqlType, ""Parameter cqlType cannot be null"");
         if (logger.isTraceEnabled())
             logger.trace(""Querying cache for codec [{} <-> {}]"", toString(cqlType), toString(javaType));
-        CacheKey cacheKey = new CacheKey(cqlType, javaType);
         try {
-            TypeCodec<?> codec = cache.get(cacheKey);
+            TypeCodec<?> codec = PRIMITIVE_CODECS.get(cqlType);
+            if (codec != null && (javaType == null || codec.accepts(javaType))) {
+                logger.trace(""Returning cached codec {}"", codec);","[{'comment': 'Trivial: we could log ""Returning primitive codec {}"" here, to distinguish from the other case.', 'commenter': 'olim7t'}]"
760,driver-core/src/main/java/com/datastax/driver/core/ThreadingOptions.java,"@@ -0,0 +1,143 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.core;
+
+import com.google.common.util.concurrent.ThreadFactoryBuilder;
+import io.netty.util.concurrent.DefaultThreadFactory;
+
+import java.util.concurrent.*;
+
+/**
+ * A set of hooks that allow clients to customize the driver's internal executors.
+ * <p/>
+ * The methods in this class are invoked when the cluster initializes. To customize the behavior, extend the class and
+ * override the appropriate methods.
+ * <p/>
+ * This is mainly intended to allow customization and instrumentation of driver threads. Keep the executors separate, as
+ * using a common one could introduce unintended consequences like deadlocks (we're working to simplify the driver's
+ * architecture and reduce the number of executors in a future release). The default implementations use unbounded
+ * queues, which is appropriate when the driver is properly configured; the only reason you would want to use bounded
+ * queues is to limit memory consumption in case of a bug or bad configuration. In that case, make sure to use a
+ * {@link RejectedExecutionHandler} that throws, such as {@link java.util.concurrent.ThreadPoolExecutor.AbortPolicy}; a
+ * blocking handler could introduce deadlocks.
+ * <p/>
+ * Netty uses a separate pool for I/O operations, that can be configured via {@link NettyOptions}.
+ */
+public class ThreadingOptions {
+    // Kept for backward compatibility, but this should be customized via this class now
+    private static final int NON_BLOCKING_EXECUTOR_SIZE = SystemProperties.getInt(
+            ""com.datastax.driver.NON_BLOCKING_EXECUTOR_SIZE"", Runtime.getRuntime().availableProcessors());
+    private static final int DEFAULT_THREAD_KEEP_ALIVE_SECONDS = 30;
+
+    /**
+     * Builds a thread factory for the threads created by a given executor.
+     * <p/>
+     * This is used by the default implementations in this class, and also internally to create the Netty I/O pool.
+     *
+     * @param clusterName  the name of the cluster, as specified by
+     *                     {@link com.datastax.driver.core.Cluster.Builder#withClusterName(String)}.
+     * @param executorName a name that identifies the executor.
+     * @return the thread factory.
+     */
+    public ThreadFactory threadFactory(String clusterName, String executorName) {
+        return new ThreadFactoryBuilder()
+                .setNameFormat(clusterName + ""-"" + executorName + ""-%d"")
+                // Back with Netty's thread factory in order to create FastThreadLocalThread instances. This allows
+                // an optimization around ThreadLocals (we could use DefaultThreadFactory directly but it creates
+                // slightly different thread names, so keep we keep a ThreadFactoryBuilder wrapper for backward
+                // compatibility).
+                .setThreadFactory(new DefaultThreadFactory(""ignored name""))
+                .build();
+    }
+
+    /**
+     * Builds the generic administration executor, used for tasks such as triggering registered
+     * {@link SchemaChangeListener}s, reacting to node state changes, and metadata updates.
+     * <p/>
+     * The default implementation sets the pool size to the number of available cores.
+     *
+     * @param clusterName the name of the cluster, as specified by
+     *                    {@link com.datastax.driver.core.Cluster.Builder#withClusterName(String)}.
+     * @return the executor.
+     */
+    public ThreadPoolExecutor executor(String clusterName) {","[{'comment': ""Couldn't we give it a more meaningful name? Suggestions: `mainExecutor`, `mainWorkerPool` etc...\n"", 'commenter': 'adutra'}]"
760,driver-core/src/main/java/com/datastax/driver/core/ThreadingOptions.java,"@@ -0,0 +1,143 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.core;
+
+import com.google.common.util.concurrent.ThreadFactoryBuilder;
+import io.netty.util.concurrent.DefaultThreadFactory;
+
+import java.util.concurrent.*;
+
+/**
+ * A set of hooks that allow clients to customize the driver's internal executors.
+ * <p/>
+ * The methods in this class are invoked when the cluster initializes. To customize the behavior, extend the class and
+ * override the appropriate methods.
+ * <p/>
+ * This is mainly intended to allow customization and instrumentation of driver threads. Keep the executors separate, as
+ * using a common one could introduce unintended consequences like deadlocks (we're working to simplify the driver's
+ * architecture and reduce the number of executors in a future release). The default implementations use unbounded
+ * queues, which is appropriate when the driver is properly configured; the only reason you would want to use bounded
+ * queues is to limit memory consumption in case of a bug or bad configuration. In that case, make sure to use a
+ * {@link RejectedExecutionHandler} that throws, such as {@link java.util.concurrent.ThreadPoolExecutor.AbortPolicy}; a
+ * blocking handler could introduce deadlocks.
+ * <p/>
+ * Netty uses a separate pool for I/O operations, that can be configured via {@link NettyOptions}.
+ */
+public class ThreadingOptions {
+    // Kept for backward compatibility, but this should be customized via this class now
+    private static final int NON_BLOCKING_EXECUTOR_SIZE = SystemProperties.getInt(
+            ""com.datastax.driver.NON_BLOCKING_EXECUTOR_SIZE"", Runtime.getRuntime().availableProcessors());
+    private static final int DEFAULT_THREAD_KEEP_ALIVE_SECONDS = 30;
+
+    /**
+     * Builds a thread factory for the threads created by a given executor.
+     * <p/>
+     * This is used by the default implementations in this class, and also internally to create the Netty I/O pool.
+     *
+     * @param clusterName  the name of the cluster, as specified by
+     *                     {@link com.datastax.driver.core.Cluster.Builder#withClusterName(String)}.
+     * @param executorName a name that identifies the executor.
+     * @return the thread factory.
+     */
+    public ThreadFactory threadFactory(String clusterName, String executorName) {
+        return new ThreadFactoryBuilder()
+                .setNameFormat(clusterName + ""-"" + executorName + ""-%d"")
+                // Back with Netty's thread factory in order to create FastThreadLocalThread instances. This allows
+                // an optimization around ThreadLocals (we could use DefaultThreadFactory directly but it creates
+                // slightly different thread names, so keep we keep a ThreadFactoryBuilder wrapper for backward
+                // compatibility).
+                .setThreadFactory(new DefaultThreadFactory(""ignored name""))
+                .build();
+    }
+
+    /**
+     * Builds the generic administration executor, used for tasks such as triggering registered
+     * {@link SchemaChangeListener}s, reacting to node state changes, and metadata updates.","[{'comment': ""It is also used for speculative executions and retries (before JAVA-893), not sure it's worth mentioning.\n"", 'commenter': 'adutra'}]"
760,driver-core/src/main/java/com/datastax/driver/core/ThreadingOptions.java,"@@ -0,0 +1,143 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.core;
+
+import com.google.common.util.concurrent.ThreadFactoryBuilder;
+import io.netty.util.concurrent.DefaultThreadFactory;
+
+import java.util.concurrent.*;
+
+/**
+ * A set of hooks that allow clients to customize the driver's internal executors.
+ * <p/>
+ * The methods in this class are invoked when the cluster initializes. To customize the behavior, extend the class and
+ * override the appropriate methods.
+ * <p/>
+ * This is mainly intended to allow customization and instrumentation of driver threads. Keep the executors separate, as
+ * using a common one could introduce unintended consequences like deadlocks (we're working to simplify the driver's
+ * architecture and reduce the number of executors in a future release). The default implementations use unbounded
+ * queues, which is appropriate when the driver is properly configured; the only reason you would want to use bounded
+ * queues is to limit memory consumption in case of a bug or bad configuration. In that case, make sure to use a
+ * {@link RejectedExecutionHandler} that throws, such as {@link java.util.concurrent.ThreadPoolExecutor.AbortPolicy}; a
+ * blocking handler could introduce deadlocks.
+ * <p/>
+ * Netty uses a separate pool for I/O operations, that can be configured via {@link NettyOptions}.
+ */
+public class ThreadingOptions {
+    // Kept for backward compatibility, but this should be customized via this class now
+    private static final int NON_BLOCKING_EXECUTOR_SIZE = SystemProperties.getInt(
+            ""com.datastax.driver.NON_BLOCKING_EXECUTOR_SIZE"", Runtime.getRuntime().availableProcessors());
+    private static final int DEFAULT_THREAD_KEEP_ALIVE_SECONDS = 30;
+
+    /**
+     * Builds a thread factory for the threads created by a given executor.
+     * <p/>
+     * This is used by the default implementations in this class, and also internally to create the Netty I/O pool.
+     *
+     * @param clusterName  the name of the cluster, as specified by
+     *                     {@link com.datastax.driver.core.Cluster.Builder#withClusterName(String)}.
+     * @param executorName a name that identifies the executor.
+     * @return the thread factory.
+     */
+    public ThreadFactory threadFactory(String clusterName, String executorName) {
+        return new ThreadFactoryBuilder()
+                .setNameFormat(clusterName + ""-"" + executorName + ""-%d"")
+                // Back with Netty's thread factory in order to create FastThreadLocalThread instances. This allows
+                // an optimization around ThreadLocals (we could use DefaultThreadFactory directly but it creates
+                // slightly different thread names, so keep we keep a ThreadFactoryBuilder wrapper for backward
+                // compatibility).
+                .setThreadFactory(new DefaultThreadFactory(""ignored name""))
+                .build();
+    }
+
+    /**
+     * Builds the generic administration executor, used for tasks such as triggering registered","[{'comment': 'Nit: ""generic administration"" sounds a bit like a bag for any kind of tasks, I\'d suggest ""main internal executor"" or something in the style.\n', 'commenter': 'adutra'}]"
760,driver-core/src/main/java/com/datastax/driver/core/Cluster.java,"@@ -1362,16 +1374,17 @@ synchronized void init() {
 
             this.configuration.register(this);
 
-            this.executorQueue = new LinkedBlockingQueue<Runnable>();
-            this.executor = makeExecutor(NON_BLOCKING_EXECUTOR_SIZE, ""worker"", executorQueue);
-            this.blockingExecutorQueue = new LinkedBlockingQueue<Runnable>();
-            this.blockingExecutor = makeExecutor(2, ""blocking-task-worker"", blockingExecutorQueue);
-            this.reconnectionExecutor = new ScheduledThreadPoolExecutor(2, threadFactory(""reconnection""));
-            // scheduledTasksExecutor is used to process C* notifications. So having it mono-threaded ensures notifications are
-            // applied in the order received.
-            this.scheduledTasksExecutor = new ScheduledThreadPoolExecutor(1, threadFactory(""scheduled-task-worker""));
-
-            this.reaper = new ConnectionReaper(this);
+            ThreadingOptions threadingOptions = this.configuration.getThreadingOptions();
+            ThreadPoolExecutor tmpExecutor = threadingOptions.executor(clusterName);
+            this.executorQueue = tmpExecutor.getQueue();
+            this.executor = MoreExecutors.listeningDecorator(tmpExecutor);","[{'comment': ""I think that ideally, `ThreadingOptions` should return `Listening(Scheduled)ExecutorService` instances. Returning `(Scheduled)ThreadPoolExecutor` sounds like a half-way solution. To give users total flexibility, we need to be able to operate on any `ExecutorService` regardless of the implementation. \n\nBut if I follow you correctly, you need `ThreadPoolExecutor` instances so that you can access their underlying queue.\n\nHowever such queues seem only used for metrics anyway, so I'd suggest that we simply stop monitoring these altogether, on the grounds that now that they are customizable, it's up to the user to monitor them. Alternatively, we could expose new methods in `ThreadingOptions` that would return the queues to monitor, but to me it seems overkill.\n"", 'commenter': 'adutra'}, {'comment': 'I think the need for monitoring thread pool queues is even greater now that the users have control over configuring them, so I think it would be good to keep it (the cost is low anyways).   Also think we should not remove the metrics in a patch release as it could break some users existing monitoring (i.e. if they were alerting on these metrics, although that likelihood seems low).\n', 'commenter': 'tolbertam'}, {'comment': ""As discussed offline, I agree that we could use `*Service` interfaces, and `instanceof` checks and casts internally to access the queues.\nI don't see any value in exposing the Guava wrappers. Implementations may choose to return them, when we call `MoreExecutors.listeningDecorator` internally it will be a no-op.\n"", 'commenter': 'olim7t'}]"
760,driver-core/src/main/java/com/datastax/driver/core/ThreadingOptions.java,"@@ -0,0 +1,143 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.core;
+
+import com.google.common.util.concurrent.ThreadFactoryBuilder;
+import io.netty.util.concurrent.DefaultThreadFactory;
+
+import java.util.concurrent.*;
+
+/**
+ * A set of hooks that allow clients to customize the driver's internal executors.
+ * <p/>
+ * The methods in this class are invoked when the cluster initializes. To customize the behavior, extend the class and
+ * override the appropriate methods.
+ * <p/>
+ * This is mainly intended to allow customization and instrumentation of driver threads. Keep the executors separate, as","[{'comment': 'Nit: I would be even more explicit here and state stg like ""each executorX method must return a newly-allocated thread pool; shared thread pools should be avoided as these could introduce etc..""\n', 'commenter': 'adutra'}, {'comment': ""BTW I wonder if all methods in this class shouldn't be prefixed with `create` or `new` to make it crystal clear that each invocation should return a newly-allocated object.\n"", 'commenter': 'adutra'}]"
760,driver-core/src/main/java/com/datastax/driver/core/ThreadingOptions.java,"@@ -0,0 +1,143 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.core;
+
+import com.google.common.util.concurrent.ThreadFactoryBuilder;
+import io.netty.util.concurrent.DefaultThreadFactory;
+
+import java.util.concurrent.*;
+
+/**
+ * A set of hooks that allow clients to customize the driver's internal executors.
+ * <p/>
+ * The methods in this class are invoked when the cluster initializes. To customize the behavior, extend the class and
+ * override the appropriate methods.
+ * <p/>
+ * This is mainly intended to allow customization and instrumentation of driver threads. Keep the executors separate, as
+ * using a common one could introduce unintended consequences like deadlocks (we're working to simplify the driver's
+ * architecture and reduce the number of executors in a future release). The default implementations use unbounded
+ * queues, which is appropriate when the driver is properly configured; the only reason you would want to use bounded
+ * queues is to limit memory consumption in case of a bug or bad configuration. In that case, make sure to use a
+ * {@link RejectedExecutionHandler} that throws, such as {@link java.util.concurrent.ThreadPoolExecutor.AbortPolicy}; a
+ * blocking handler could introduce deadlocks.
+ * <p/>
+ * Netty uses a separate pool for I/O operations, that can be configured via {@link NettyOptions}.
+ */
+public class ThreadingOptions {
+    // Kept for backward compatibility, but this should be customized via this class now
+    private static final int NON_BLOCKING_EXECUTOR_SIZE = SystemProperties.getInt(
+            ""com.datastax.driver.NON_BLOCKING_EXECUTOR_SIZE"", Runtime.getRuntime().availableProcessors());
+    private static final int DEFAULT_THREAD_KEEP_ALIVE_SECONDS = 30;
+
+    /**
+     * Builds a thread factory for the threads created by a given executor.
+     * <p/>
+     * This is used by the default implementations in this class, and also internally to create the Netty I/O pool.
+     *
+     * @param clusterName  the name of the cluster, as specified by
+     *                     {@link com.datastax.driver.core.Cluster.Builder#withClusterName(String)}.
+     * @param executorName a name that identifies the executor.
+     * @return the thread factory.
+     */
+    public ThreadFactory threadFactory(String clusterName, String executorName) {
+        return new ThreadFactoryBuilder()
+                .setNameFormat(clusterName + ""-"" + executorName + ""-%d"")
+                // Back with Netty's thread factory in order to create FastThreadLocalThread instances. This allows
+                // an optimization around ThreadLocals (we could use DefaultThreadFactory directly but it creates
+                // slightly different thread names, so keep we keep a ThreadFactoryBuilder wrapper for backward
+                // compatibility).
+                .setThreadFactory(new DefaultThreadFactory(""ignored name""))
+                .build();
+    }
+
+    /**
+     * Builds the generic administration executor, used for tasks such as triggering registered
+     * {@link SchemaChangeListener}s, reacting to node state changes, and metadata updates.
+     * <p/>
+     * The default implementation sets the pool size to the number of available cores.
+     *
+     * @param clusterName the name of the cluster, as specified by
+     *                    {@link com.datastax.driver.core.Cluster.Builder#withClusterName(String)}.
+     * @return the executor.
+     */
+    public ThreadPoolExecutor executor(String clusterName) {
+        ThreadPoolExecutor executor = new ThreadPoolExecutor(
+                NON_BLOCKING_EXECUTOR_SIZE, NON_BLOCKING_EXECUTOR_SIZE,
+                DEFAULT_THREAD_KEEP_ALIVE_SECONDS, TimeUnit.SECONDS,
+                new LinkedBlockingQueue<Runnable>(),
+                threadFactory(clusterName, ""worker""));
+        executor.allowCoreThreadTimeOut(true);
+        return executor;
+    }
+
+    /**
+     * Builds the executor used to block on new connections before they are added to a pool.
+     * <p/>
+     * The default implementation uses 2 threads.
+     *
+     * @param clusterName the name of the cluster, as specified by
+     *                    {@link com.datastax.driver.core.Cluster.Builder#withClusterName(String)}.
+     * @return the executor.
+     */
+    public ThreadPoolExecutor blockingExecutor(String clusterName) {","[{'comment': 'Same here, the name is a bit vague, why not `connectionCreationExecutor`?\n', 'commenter': 'adutra'}, {'comment': 'I like that the names are 1:1 with the names in `Cluster.manager`.  I expect the people who will use this will will need to look at the source code anyways so it makes sense to sync them up.  I think in a future major release where we refactor our thread pools I agree it would be good to make the names more explicit.\n', 'commenter': 'tolbertam'}, {'comment': ""Yes, I thought about changing the names but I agree it's better to stay in line with the code and especially the metrics (which we can't change).\n"", 'commenter': 'olim7t'}]"
760,driver-core/src/main/java/com/datastax/driver/core/ThreadingOptions.java,"@@ -0,0 +1,143 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.core;
+
+import com.google.common.util.concurrent.ThreadFactoryBuilder;
+import io.netty.util.concurrent.DefaultThreadFactory;
+
+import java.util.concurrent.*;
+
+/**
+ * A set of hooks that allow clients to customize the driver's internal executors.
+ * <p/>
+ * The methods in this class are invoked when the cluster initializes. To customize the behavior, extend the class and
+ * override the appropriate methods.
+ * <p/>
+ * This is mainly intended to allow customization and instrumentation of driver threads. Keep the executors separate, as
+ * using a common one could introduce unintended consequences like deadlocks (we're working to simplify the driver's
+ * architecture and reduce the number of executors in a future release). The default implementations use unbounded
+ * queues, which is appropriate when the driver is properly configured; the only reason you would want to use bounded
+ * queues is to limit memory consumption in case of a bug or bad configuration. In that case, make sure to use a
+ * {@link RejectedExecutionHandler} that throws, such as {@link java.util.concurrent.ThreadPoolExecutor.AbortPolicy}; a
+ * blocking handler could introduce deadlocks.
+ * <p/>
+ * Netty uses a separate pool for I/O operations, that can be configured via {@link NettyOptions}.
+ */
+public class ThreadingOptions {
+    // Kept for backward compatibility, but this should be customized via this class now
+    private static final int NON_BLOCKING_EXECUTOR_SIZE = SystemProperties.getInt(
+            ""com.datastax.driver.NON_BLOCKING_EXECUTOR_SIZE"", Runtime.getRuntime().availableProcessors());
+    private static final int DEFAULT_THREAD_KEEP_ALIVE_SECONDS = 30;
+
+    /**
+     * Builds a thread factory for the threads created by a given executor.
+     * <p/>
+     * This is used by the default implementations in this class, and also internally to create the Netty I/O pool.
+     *
+     * @param clusterName  the name of the cluster, as specified by
+     *                     {@link com.datastax.driver.core.Cluster.Builder#withClusterName(String)}.
+     * @param executorName a name that identifies the executor.
+     * @return the thread factory.
+     */
+    public ThreadFactory threadFactory(String clusterName, String executorName) {
+        return new ThreadFactoryBuilder()
+                .setNameFormat(clusterName + ""-"" + executorName + ""-%d"")
+                // Back with Netty's thread factory in order to create FastThreadLocalThread instances. This allows
+                // an optimization around ThreadLocals (we could use DefaultThreadFactory directly but it creates
+                // slightly different thread names, so keep we keep a ThreadFactoryBuilder wrapper for backward
+                // compatibility).
+                .setThreadFactory(new DefaultThreadFactory(""ignored name""))
+                .build();
+    }
+
+    /**
+     * Builds the generic administration executor, used for tasks such as triggering registered
+     * {@link SchemaChangeListener}s, reacting to node state changes, and metadata updates.
+     * <p/>
+     * The default implementation sets the pool size to the number of available cores.
+     *
+     * @param clusterName the name of the cluster, as specified by
+     *                    {@link com.datastax.driver.core.Cluster.Builder#withClusterName(String)}.
+     * @return the executor.
+     */
+    public ThreadPoolExecutor executor(String clusterName) {
+        ThreadPoolExecutor executor = new ThreadPoolExecutor(
+                NON_BLOCKING_EXECUTOR_SIZE, NON_BLOCKING_EXECUTOR_SIZE,
+                DEFAULT_THREAD_KEEP_ALIVE_SECONDS, TimeUnit.SECONDS,
+                new LinkedBlockingQueue<Runnable>(),
+                threadFactory(clusterName, ""worker""));
+        executor.allowCoreThreadTimeOut(true);
+        return executor;
+    }
+
+    /**
+     * Builds the executor used to block on new connections before they are added to a pool.
+     * <p/>
+     * The default implementation uses 2 threads.
+     *
+     * @param clusterName the name of the cluster, as specified by
+     *                    {@link com.datastax.driver.core.Cluster.Builder#withClusterName(String)}.
+     * @return the executor.
+     */
+    public ThreadPoolExecutor blockingExecutor(String clusterName) {
+        ThreadPoolExecutor executor = new ThreadPoolExecutor(
+                2, 2,
+                DEFAULT_THREAD_KEEP_ALIVE_SECONDS, TimeUnit.SECONDS,
+                new LinkedBlockingQueue<Runnable>(),
+                threadFactory(clusterName, ""worker""));
+        executor.allowCoreThreadTimeOut(true);
+        return executor;
+    }
+
+    /**
+     * Builds the executor when reconnection attempts will be scheduled.
+     * <p/>
+     * The default implementation uses 2 threads.
+     *
+     * @param clusterName the name of the cluster, as specified by
+     *                    {@link com.datastax.driver.core.Cluster.Builder#withClusterName(String)}.
+     * @return the executor.
+     */
+    public ScheduledThreadPoolExecutor reconnectionExecutor(String clusterName) {
+        return new ScheduledThreadPoolExecutor(2, threadFactory(clusterName, ""reconnection""));
+    }
+
+    /**
+     * Builds the executor to handle host state notifications from Cassandra.","[{'comment': ""It's also used for debouncing of such notification events, and for idle connections cleanup (again, not sure it's worth mentioning)\n"", 'commenter': 'adutra'}]"
760,driver-core/src/main/java/com/datastax/driver/core/ThreadingOptions.java,"@@ -0,0 +1,143 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.core;
+
+import com.google.common.util.concurrent.ThreadFactoryBuilder;
+import io.netty.util.concurrent.DefaultThreadFactory;
+
+import java.util.concurrent.*;
+
+/**","[{'comment': 'It would be good to use strong language discouraging the use of this, maybe indicate it is for expert use only as the default options will be sensible for most cases.  Sometimes users add configuration for everything just because the option to is there, but this is one of those where they should not be overriding behavior unless they have a really good reason.\n', 'commenter': 'tolbertam'}, {'comment': 'I\'m a bit wary of ""expert use"" comments in general. It implies that we divide users into two groups, but how does someone know if they have the ""expert"" badge?\nI\'d rather say something along the lines of ""things will break badly if this is misconfigured"".\n', 'commenter': 'olim7t'}, {'comment': ""Maybe 'expert use only' isn't the right verbiage, but I think it would be good to have something strong and delineating this configuration from others as users shouldn't be using this without good reason.  `things will break badly if this is misconfigured` sounds good to me.\n"", 'commenter': 'tolbertam'}]"
760,driver-core/src/main/java/com/datastax/driver/core/Metrics.java,"@@ -262,6 +251,24 @@ void shutdown() {
             jmxReporter.stop();
     }
 
+    private static Gauge<Integer> buildQueueSizeGauge(final ExecutorService executor) {
+        if (executor instanceof ThreadPoolExecutor) {","[{'comment': ""I think this doesn't work for executor and blockingExecutor because `listeningDecorator` returns a `ListeningDecorator` which isn't an instance of `ThreadPoolExecutor`.  Noticed that it was returning `-1` for `blocking-executor-queue-depth` and `executor-queue-depth`\n"", 'commenter': 'tolbertam'}]"
771,clirr-ignores.xml,"@@ -147,4 +147,18 @@
         <justification>False positive, the enclosing class is package-private so this was never exposed</justification>
     </difference>
 
+    <difference>
+        <differenceType>7012</differenceType> <!-- method added to interface -->
+        <className>com/datastax/driver/mapping/annotations/Table</className>
+        <method>java.lang.String[] transientProperties()</method>
+        <justification>False positive, it's an annotation and the new method has a default value</justification>
+    </difference>
+
+    <difference>
+        <differenceType>7012</differenceType> <!-- method added to interface -->
+        <className>com/datastax/driver/mapping/annotations/UDT</className>
+        <method>java.lang.String[] transientProperties()</method>
+        <justification>False positive, it's an annotation and the new method has a default value</justification>
+    </difference>
+","[{'comment': 'ðŸ‘ ', 'commenter': 'olim7t'}]"
771,driver-mapping/src/main/java/com/datastax/driver/mapping/MappingManager.java,"@@ -137,7 +172,7 @@ public void onUserTypeChanged(UserType current, UserType previous) {
                     for (Class<?> udtClass : udtClasses) {
                         // try to register an updated version of the previous codec
                         try {
-                            getUDTCodec(udtClass);
+                            getUDTCodec(udtClass, defaultConfiguration); // TODO - this will update with default configuration even though previously loaded with a custom one. Solution may be to pass the MappedUDTCodec it's configuration on creation time, and then at this point we are able to get it from the previous instance","[{'comment': 'I agree. Instead of `udtClasses`, we can store the whole codecs in a `deletedCodecs` set, and add a way to access the configuration in `MappedUDTCodec` (package-private getter or direct field access).', 'commenter': 'olim7t'}]"
771,driver-mapping/src/main/java/com/datastax/driver/mapping/MapperConfiguration.java,"@@ -0,0 +1,336 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.mapping;
+
+import java.util.Collection;
+import java.util.HashSet;
+
+/**
+ * The configuration of the mapper.
+ * It configures the following categories:
+ * Property scan configurations:
+ * <ul>
+ * <li>Transient property names (added on top of properties annotated with {@code Transient} or
+ * property names that appear in {@code Table} or {@code UDT} annotations.</li>
+ * <li>Property scan scope: whether or not to scan properties by getters/setters, whether or not
+ * to scan properties by class fields.</li>
+ * <li>Property mapping strategy: whether to scan fields that are not blacklisted (opt-out), meaning
+ * that unless a property is explicitly annotated with {@code Transient} it will be mapped, or to scan
+ * fields that are not white-listed (opt-in), meaning that unless a property is explicitly annotated
+ * with either {@code PartitionKey} or {@code ClusteringColumn} or {@code Column} or
+ * {@code com.datastax.driver.mapping.annotations.Field} or {@code Computed} it won't be mapped.</li>
+ * <li>Hierarchy scan strategy: defines how to scan for properties in parent classes - if
+ * hierarchyScanEnabled set to false - parent classes will not be scanned at all.
+ * if scanOnlyAnnotatedClasses set to true, only parents annotated with table classes
+ * ({@code Table}, {@code UDT}, {@code Accessor}) will be scanned. And lastly - deepestAllowedAncestor
+ * will define the deepest parent class to scan - which default to {@code Object.class}</li>","[{'comment': ""These detailed explanations are great. I think they would be better located on each corresponding setter's javadocs in `PropertyScanConfiguration` (this is likely the first thing a developer will use in their IDE)."", 'commenter': 'olim7t'}]"
771,driver-mapping/src/main/java/com/datastax/driver/mapping/MapperConfiguration.java,"@@ -0,0 +1,336 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.mapping;
+
+import java.util.Collection;
+import java.util.HashSet;
+
+/**
+ * The configuration of the mapper.
+ * It configures the following categories:
+ * Property scan configurations:
+ * <ul>
+ * <li>Transient property names (added on top of properties annotated with {@code Transient} or
+ * property names that appear in {@code Table} or {@code UDT} annotations.</li>
+ * <li>Property scan scope: whether or not to scan properties by getters/setters, whether or not
+ * to scan properties by class fields.</li>
+ * <li>Property mapping strategy: whether to scan fields that are not blacklisted (opt-out), meaning
+ * that unless a property is explicitly annotated with {@code Transient} it will be mapped, or to scan
+ * fields that are not white-listed (opt-in), meaning that unless a property is explicitly annotated
+ * with either {@code PartitionKey} or {@code ClusteringColumn} or {@code Column} or
+ * {@code com.datastax.driver.mapping.annotations.Field} or {@code Computed} it won't be mapped.</li>
+ * <li>Hierarchy scan strategy: defines how to scan for properties in parent classes - if
+ * hierarchyScanEnabled set to false - parent classes will not be scanned at all.
+ * if scanOnlyAnnotatedClasses set to true, only parents annotated with table classes
+ * ({@code Table}, {@code UDT}, {@code Accessor}) will be scanned. And lastly - deepestAllowedAncestor
+ * will define the deepest parent class to scan - which default to {@code Object.class}</li>
+ * </ul>
+ * This is also where you get the configured settings, though those cannot be changed
+ * (they are set during the built of the Mapper object).
+ */
+public class MapperConfiguration {
+
+    private PropertyScanConfiguration propertyScanConfiguration;","[{'comment': 'The configuration is shared across multiple threads, so every mutable field (here and inside `PropertyScanConfiguration` itself) should be volatile.', 'commenter': 'olim7t'}, {'comment': ""Yeah i'll will.\r\n\r\nMaybe it's worth mentioning this case:\r\n\r\n```\r\nMapperConfiguration config = ...\r\nMapper<User> userMapper1 = manager.mapper(User.class, config);\r\nconfig.changeSomething();\r\nMapper<User> userMapper2 = manager.mapper(User.class, config);\r\n// at this point we should make sure that userMapper1 has no direct references to config fields,\r\n// otherwise it would make it change it's config from outside and after creation\r\n```\r\n\r\nSo for this end each configuration class implements a [copy constructor](https://github.com/datastax/java-driver/blob/6677829beec3d67020c5d24cbf4f752689146dd5/driver-mapping/src/main/java/com/datastax/driver/mapping/MapperConfiguration.java#L52) and on mapper creation [we clone the given config instance](https://github.com/datastax/java-driver/blob/6677829beec3d67020c5d24cbf4f752689146dd5/driver-mapping/src/main/java/com/datastax/driver/mapping/MappingManager.java#L312).\r\n\r\nI think if we are to extend the usage of the config class in the future it may be a bit hard to maintain since the developer must remember it or fields will not be copied. If we want to avoid it at the price of a bit efficiency loss we can use some 3rd party lib to deep copy the object only on mapper creation and then we can remove those constructors.\r\n\r\nThoughts?"", 'commenter': 'avivcarmis'}, {'comment': ""On the other hand, if I set a global configuration and then change something on it, I kind of expect the change to be reflected on every mapper, currently with the copy this would not be the case.\r\nMaybe we're overthinking this. All the configuration we have so far is stuff you set once at startup, and don't change at runtime. We could make the configuration classes immutable and avoid the copies.\r\n\r\n> we can use some 3rd party lib to deep copy the object\r\n\r\nWe avoid dependencies as much as possible. It leads to dependency hell if users already use another version of the library in their application. Guava is a recent example (JAVA-1328)."", 'commenter': 'olim7t'}, {'comment': ""ðŸ‘ on making the configuration immutable.  I think changing the config at runtime doesn't offer a lot of value and also adds uncertainty."", 'commenter': 'tolbertam'}]"
771,driver-mapping/src/main/java/com/datastax/driver/mapping/MapperConfiguration.java,"@@ -0,0 +1,336 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.mapping;
+
+import java.util.Collection;
+import java.util.HashSet;
+
+/**
+ * The configuration of the mapper.
+ * It configures the following categories:
+ * Property scan configurations:
+ * <ul>
+ * <li>Transient property names (added on top of properties annotated with {@code Transient} or
+ * property names that appear in {@code Table} or {@code UDT} annotations.</li>
+ * <li>Property scan scope: whether or not to scan properties by getters/setters, whether or not
+ * to scan properties by class fields.</li>
+ * <li>Property mapping strategy: whether to scan fields that are not blacklisted (opt-out), meaning
+ * that unless a property is explicitly annotated with {@code Transient} it will be mapped, or to scan
+ * fields that are not white-listed (opt-in), meaning that unless a property is explicitly annotated
+ * with either {@code PartitionKey} or {@code ClusteringColumn} or {@code Column} or
+ * {@code com.datastax.driver.mapping.annotations.Field} or {@code Computed} it won't be mapped.</li>
+ * <li>Hierarchy scan strategy: defines how to scan for properties in parent classes - if
+ * hierarchyScanEnabled set to false - parent classes will not be scanned at all.
+ * if scanOnlyAnnotatedClasses set to true, only parents annotated with table classes
+ * ({@code Table}, {@code UDT}, {@code Accessor}) will be scanned. And lastly - deepestAllowedAncestor
+ * will define the deepest parent class to scan - which default to {@code Object.class}</li>
+ * </ul>
+ * This is also where you get the configured settings, though those cannot be changed
+ * (they are set during the built of the Mapper object).
+ */
+public class MapperConfiguration {
+
+    private PropertyScanConfiguration propertyScanConfiguration;
+
+    public MapperConfiguration() {
+        this.propertyScanConfiguration = new PropertyScanConfiguration();
+    }
+
+    public MapperConfiguration(MapperConfiguration toCopy) {
+        this.propertyScanConfiguration = new PropertyScanConfiguration(toCopy.propertyScanConfiguration);
+    }
+
+    /**
+     * Returns the property scanning configuration
+     *
+     * @return the property scanning configuration
+     */
+    public PropertyScanConfiguration getPropertyScanConfiguration() {
+        return propertyScanConfiguration;
+    }
+
+    /**
+     * Sets the property scanning configuration
+     *
+     * @param propertyScanConfiguration property scanning configuration to use
+     * @return the MapperConfiguration to enable builder pattern
+     */
+    public MapperConfiguration setPropertyScanConfiguration(PropertyScanConfiguration propertyScanConfiguration) {
+        this.propertyScanConfiguration = propertyScanConfiguration;
+        return this;
+    }
+
+    public static class PropertyScanConfiguration {","[{'comment': ""Do you mind moving this to a top-level class? Historically the driver codebase has been using inner classes a lot, but I must say I'm not a fan of it, it creates very long source files."", 'commenter': 'olim7t'}, {'comment': 'Sure!', 'commenter': 'avivcarmis'}]"
771,driver-mapping/src/main/java/com/datastax/driver/mapping/MappingManager.java,"@@ -213,17 +288,34 @@ public Session getSession() {
      * @return the accessor object for class {@code klass}.
      */
     public <T> T createAccessor(Class<T> klass) {
-        return getAccessor(klass);
+        return getAccessor(klass, defaultConfiguration);
+    }
+
+    /**
+     * Creates an accessor object based on the provided interface (that must be annotated by
+     * a {@link Accessor} annotation) with given {@code MapperConfiguration}.
+     * <p/>
+     * The {@code MappingManager} only ever keep one Accessor for each class, and so calling this
+     * method multiple time on the same class will always return the same object.
+     *
+     * @param <T>   the type of the accessor class.
+     * @param klass the (annotated) class for which to create an accessor object.
+     * @param configuration the configuration to be used.
+     * @return the accessor object for class {@code klass}.
+     */
+    public <T> T createAccessor(Class<T> klass, MapperConfiguration configuration) {
+        return getAccessor(klass, configuration);
     }
 
     @SuppressWarnings(""unchecked"")
-    private <T> Mapper<T> getMapper(Class<T> klass) {
+    private <T> Mapper<T> getMapper(Class<T> klass, MapperConfiguration configuration) {
+        configuration = new MapperConfiguration(configuration);","[{'comment': ""The configuration is not used in the map key. So if you do this:\r\n```java\r\nMapper<User> userMapper1 = manager.mapper(User.class, config1);\r\nMapper<User> userMapper2 = manager.mapper(User.class, config2);\r\n```\r\nThen you get the same object and userMapper2 uses config1.\r\n\r\nGranted, this is a weird use case, so I'm not sure we want to go through the trouble of modifying the map key. But this should at least be documented in the javadoc, and maybe we should log a warning or throw an error on the second call (that forces us to store the configuration in the mapper, and implement `equals` on all configuration classes)."", 'commenter': 'olim7t'}, {'comment': ""I agree.\r\nWe can easily implement a key containing both class and config with `equals` method and make sure it won't happen.\r\n\r\nWhichever we like better."", 'commenter': 'avivcarmis'}, {'comment': ""> We can easily implement a key containing both class and config with equals method and make sure it won't happen.\r\n\r\n+1 (a bit tedious for sure)."", 'commenter': 'adutra'}, {'comment': 'This means all configuration classes must implement equals and hashcode. In hindsight I would be fine with just documenting it.', 'commenter': 'olim7t'}]"
771,driver-mapping/src/main/java/com/datastax/driver/mapping/MapperConfiguration.java,"@@ -0,0 +1,336 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.mapping;
+
+import java.util.Collection;
+import java.util.HashSet;
+
+/**
+ * The configuration of the mapper.
+ * It configures the following categories:
+ * Property scan configurations:
+ * <ul>
+ * <li>Transient property names (added on top of properties annotated with {@code Transient} or
+ * property names that appear in {@code Table} or {@code UDT} annotations.</li>
+ * <li>Property scan scope: whether or not to scan properties by getters/setters, whether or not
+ * to scan properties by class fields.</li>
+ * <li>Property mapping strategy: whether to scan fields that are not blacklisted (opt-out), meaning
+ * that unless a property is explicitly annotated with {@code Transient} it will be mapped, or to scan
+ * fields that are not white-listed (opt-in), meaning that unless a property is explicitly annotated
+ * with either {@code PartitionKey} or {@code ClusteringColumn} or {@code Column} or
+ * {@code com.datastax.driver.mapping.annotations.Field} or {@code Computed} it won't be mapped.</li>
+ * <li>Hierarchy scan strategy: defines how to scan for properties in parent classes - if
+ * hierarchyScanEnabled set to false - parent classes will not be scanned at all.
+ * if scanOnlyAnnotatedClasses set to true, only parents annotated with table classes
+ * ({@code Table}, {@code UDT}, {@code Accessor}) will be scanned. And lastly - deepestAllowedAncestor
+ * will define the deepest parent class to scan - which default to {@code Object.class}</li>
+ * </ul>
+ * This is also where you get the configured settings, though those cannot be changed
+ * (they are set during the built of the Mapper object).
+ */
+public class MapperConfiguration {
+
+    private PropertyScanConfiguration propertyScanConfiguration;
+
+    public MapperConfiguration() {
+        this.propertyScanConfiguration = new PropertyScanConfiguration();
+    }
+
+    public MapperConfiguration(MapperConfiguration toCopy) {
+        this.propertyScanConfiguration = new PropertyScanConfiguration(toCopy.propertyScanConfiguration);
+    }
+
+    /**
+     * Returns the property scanning configuration
+     *
+     * @return the property scanning configuration
+     */
+    public PropertyScanConfiguration getPropertyScanConfiguration() {
+        return propertyScanConfiguration;
+    }
+
+    /**
+     * Sets the property scanning configuration
+     *
+     * @param propertyScanConfiguration property scanning configuration to use
+     * @return the MapperConfiguration to enable builder pattern
+     */
+    public MapperConfiguration setPropertyScanConfiguration(PropertyScanConfiguration propertyScanConfiguration) {
+        this.propertyScanConfiguration = propertyScanConfiguration;
+        return this;
+    }
+
+    public static class PropertyScanConfiguration {
+
+        private Collection<String> excludedProperties;
+
+        private PropertyScanScope propertyScanScope;
+
+        private PropertyMappingStrategy propertyMappingStrategy;
+
+        private HierarchyScanStrategy hierarchyScanStrategy;
+
+        public PropertyScanConfiguration() {
+            this.excludedProperties = new HashSet<String>();
+            this.propertyScanScope = new PropertyScanScope();
+            this.propertyMappingStrategy = PropertyMappingStrategy.BLACK_LIST;
+            this.hierarchyScanStrategy = new HierarchyScanStrategy();
+        }
+
+        public PropertyScanConfiguration(PropertyScanConfiguration toCopy) {
+            this.excludedProperties = toCopy.excludedProperties;
+            this.propertyScanScope = new PropertyScanScope(toCopy.propertyScanScope);
+            this.propertyMappingStrategy = toCopy.propertyMappingStrategy;
+            this.hierarchyScanStrategy = new HierarchyScanStrategy(toCopy.hierarchyScanStrategy);
+        }
+
+        /**
+         * Returns a collection of properties to exclude from mapping","[{'comment': ""1. I would suggest to complete the documentation by stating that properties defined here will be transient on a per-mapper basis; if the user wants a more fine-grained tuning, there is also the `transientProperties` attribute in `@Table` and `@UDT`.\r\n2. I would suggest to normalize the namings here. Since it's already called `transientProperties` in the annotations and we cannot change that, why not call this method `getTransientProperties()` instead? Same for some local variables in `AnnotationParser`, these are currently called `classLevelTransients`, which is a bit confusing."", 'commenter': 'adutra'}]"
771,driver-mapping/src/main/java/com/datastax/driver/mapping/MapperConfiguration.java,"@@ -0,0 +1,336 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.mapping;
+
+import java.util.Collection;
+import java.util.HashSet;
+
+/**
+ * The configuration of the mapper.
+ * It configures the following categories:
+ * Property scan configurations:
+ * <ul>
+ * <li>Transient property names (added on top of properties annotated with {@code Transient} or
+ * property names that appear in {@code Table} or {@code UDT} annotations.</li>
+ * <li>Property scan scope: whether or not to scan properties by getters/setters, whether or not
+ * to scan properties by class fields.</li>
+ * <li>Property mapping strategy: whether to scan fields that are not blacklisted (opt-out), meaning
+ * that unless a property is explicitly annotated with {@code Transient} it will be mapped, or to scan
+ * fields that are not white-listed (opt-in), meaning that unless a property is explicitly annotated
+ * with either {@code PartitionKey} or {@code ClusteringColumn} or {@code Column} or
+ * {@code com.datastax.driver.mapping.annotations.Field} or {@code Computed} it won't be mapped.</li>
+ * <li>Hierarchy scan strategy: defines how to scan for properties in parent classes - if
+ * hierarchyScanEnabled set to false - parent classes will not be scanned at all.
+ * if scanOnlyAnnotatedClasses set to true, only parents annotated with table classes
+ * ({@code Table}, {@code UDT}, {@code Accessor}) will be scanned. And lastly - deepestAllowedAncestor
+ * will define the deepest parent class to scan - which default to {@code Object.class}</li>
+ * </ul>
+ * This is also where you get the configured settings, though those cannot be changed
+ * (they are set during the built of the Mapper object).
+ */
+public class MapperConfiguration {
+
+    private PropertyScanConfiguration propertyScanConfiguration;
+
+    public MapperConfiguration() {
+        this.propertyScanConfiguration = new PropertyScanConfiguration();
+    }
+
+    public MapperConfiguration(MapperConfiguration toCopy) {
+        this.propertyScanConfiguration = new PropertyScanConfiguration(toCopy.propertyScanConfiguration);
+    }
+
+    /**
+     * Returns the property scanning configuration
+     *
+     * @return the property scanning configuration
+     */
+    public PropertyScanConfiguration getPropertyScanConfiguration() {
+        return propertyScanConfiguration;
+    }
+
+    /**
+     * Sets the property scanning configuration
+     *
+     * @param propertyScanConfiguration property scanning configuration to use
+     * @return the MapperConfiguration to enable builder pattern
+     */
+    public MapperConfiguration setPropertyScanConfiguration(PropertyScanConfiguration propertyScanConfiguration) {
+        this.propertyScanConfiguration = propertyScanConfiguration;
+        return this;
+    }
+
+    public static class PropertyScanConfiguration {
+
+        private Collection<String> excludedProperties;
+
+        private PropertyScanScope propertyScanScope;
+
+        private PropertyMappingStrategy propertyMappingStrategy;
+
+        private HierarchyScanStrategy hierarchyScanStrategy;
+
+        public PropertyScanConfiguration() {
+            this.excludedProperties = new HashSet<String>();
+            this.propertyScanScope = new PropertyScanScope();
+            this.propertyMappingStrategy = PropertyMappingStrategy.BLACK_LIST;
+            this.hierarchyScanStrategy = new HierarchyScanStrategy();
+        }
+
+        public PropertyScanConfiguration(PropertyScanConfiguration toCopy) {
+            this.excludedProperties = toCopy.excludedProperties;
+            this.propertyScanScope = new PropertyScanScope(toCopy.propertyScanScope);
+            this.propertyMappingStrategy = toCopy.propertyMappingStrategy;
+            this.hierarchyScanStrategy = new HierarchyScanStrategy(toCopy.hierarchyScanStrategy);
+        }
+
+        /**
+         * Returns a collection of properties to exclude from mapping
+         *
+         * @return a collection of properties to exclude from mapping
+         */
+        public Collection<String> getExcludedProperties() {
+            return excludedProperties;
+        }
+
+        /**
+         * Sets a collection of properties to exclude from mapping
+         *
+         * @param excludedProperties a collection of properties to exclude from mapping
+         * @return the PropertyScanConfiguration to enable builder pattern
+         */
+        public PropertyScanConfiguration setExcludedProperties(Collection<String> excludedProperties) {
+            this.excludedProperties = excludedProperties;
+            return this;
+        }
+
+        /**
+         * Returns the property scan scope settings
+         *
+         * @return the property scan scope settings
+         */
+        public PropertyScanScope getPropertyScanScope() {
+            return propertyScanScope;
+        }
+
+        /**
+         * Sets the property scan scope settings
+         *
+         * @param propertyScanScope property scan scope settings object to use
+         * @return the PropertyScanConfiguration to enable builder pattern
+         */
+        public PropertyScanConfiguration setPropertyScanScope(PropertyScanScope propertyScanScope) {
+            this.propertyScanScope = propertyScanScope;
+            return this;
+        }
+
+        /**
+         * Returns the PropertyMappingStrategy of the mapper.
+         * e.g. whether to scan fields that are not blacklisted (opt-out), meaning
+         * that unless a property is explicitly annotated with {@code Transient} it will be mapped, or to scan
+         * fields that are not white-listed (opt-in), meaning that unless a property is explicitly annotated
+         * with either {@code PartitionKey} or {@code ClusteringColumn} or {@code Column} or
+         * {@code com.datastax.driver.mapping.annotations.Field} or {@code Computed} it won't be mapped.
+         *
+         * @return the PropertyMappingStrategy of the mapper
+         */
+        public PropertyMappingStrategy getPropertyMappingStrategy() {
+            return propertyMappingStrategy;
+        }
+
+        /**
+         * Sets the PropertyMappingStrategy of the mapper.
+         *
+         * @param propertyMappingStrategy PropertyMappingStrategy to use
+         * @return the PropertyScanConfiguration to enable builder pattern
+         */
+        public PropertyScanConfiguration setPropertyMappingStrategy(PropertyMappingStrategy propertyMappingStrategy) {
+            this.propertyMappingStrategy = propertyMappingStrategy;
+            return this;
+        }
+
+        /**
+         * Returns the HierarchyScanStrategy settings of the mapper
+         *
+         * @return the HierarchyScanStrategy settings of the mapper
+         */
+        public HierarchyScanStrategy getHierarchyScanStrategy() {
+            return hierarchyScanStrategy;
+        }
+
+        /**
+         * Sets the HierarchyScanStrategy settings of the mapper
+         *
+         * @param hierarchyScanStrategy HierarchyScanStrategy settings to use
+         * @return the PropertyScanConfiguration to enable builder pattern
+         */
+        public PropertyScanConfiguration setHierarchyScanStrategy(HierarchyScanStrategy hierarchyScanStrategy) {
+            this.hierarchyScanStrategy = hierarchyScanStrategy;
+            return this;
+        }
+
+    }
+
+    public static class PropertyScanScope {
+
+        private boolean scanFields;
+
+        private boolean scanGetters;
+
+        public PropertyScanScope() {
+            this.scanFields = true;
+            this.scanGetters = true;
+        }
+
+        public PropertyScanScope(PropertyScanScope toCopy) {
+            this.scanFields = toCopy.scanFields;
+            this.scanGetters = toCopy.scanGetters;
+        }
+
+        /**
+         * Returns whether or not the scope include class fields
+         *
+         * @return whether or not the scope include class fields
+         */
+        public boolean isScanFields() {
+            return scanFields;
+        }
+
+        /**
+         * Set whether or not the scope include class fields
+         *
+         * @param scanFields    whether or not the scope include class fields
+         * @return the PropertyScanScope to enable builder pattern
+         */
+        public PropertyScanScope setScanFields(boolean scanFields) {
+            this.scanFields = scanFields;
+            return this;
+        }
+
+        /**
+         * Returns whether or not the scope include class getters
+         *
+         * @return whether or not the scope include class getters
+         */
+        public boolean isScanGetters() {
+            return scanGetters;
+        }
+
+        /**
+         * Set whether or not the scope include class getters
+         *
+         * @param scanGetters whether or not the scope include class getters
+         * @return the PropertyScanScope to enable builder pattern
+         */
+        public PropertyScanScope setScanGetters(boolean scanGetters) {
+            this.scanGetters = scanGetters;
+            return this;
+        }
+
+    }
+
+    public static class HierarchyScanStrategy {
+
+        private boolean hierarchyScanEnabled;
+
+        private boolean scanOnlyAnnotatedClasses;
+
+        private Class<?> deepestAllowedAncestor;
+
+        public HierarchyScanStrategy() {
+            this.hierarchyScanEnabled = true;
+            this.scanOnlyAnnotatedClasses = false;
+            this.deepestAllowedAncestor = Object.class;
+        }
+
+        public HierarchyScanStrategy(HierarchyScanStrategy toCopy) {
+            this.hierarchyScanEnabled = toCopy.hierarchyScanEnabled;
+            this.scanOnlyAnnotatedClasses = toCopy.scanOnlyAnnotatedClasses;
+            this.deepestAllowedAncestor = toCopy.deepestAllowedAncestor;
+        }
+
+        /**
+         * Returns whether or not the strategy allows scanning of parent classes
+         *
+         * @return whether or not the strategy allows scanning of parent classes
+         */
+        public boolean isHierarchyScanEnabled() {
+            return hierarchyScanEnabled;
+        }
+
+        /**
+         * Sets whether or not the strategy allows scanning of parent classes
+         *
+         * @param hierarchyScanEnabled whether or not the strategy allows scanning of parent classes
+         * @return the HierarchyScanStrategy to enable builder pattern
+         */
+        public HierarchyScanStrategy setHierarchyScanEnabled(boolean hierarchyScanEnabled) {
+            this.hierarchyScanEnabled = hierarchyScanEnabled;
+            return this;
+        }
+
+        /**
+         * Returns whether or not the strategy allows scanning of all parent classes or only those
+         * annotated with {@code Table} or {@code UDT} or {@code Accessor}
+         *
+         * @return whether or not the strategy allows scanning of all parent classes or only those
+         * annotated with {@code Table} or {@code UDT} or {@code Accessor}
+         */
+        public boolean isScanOnlyAnnotatedClasses() {
+            return scanOnlyAnnotatedClasses;
+        }
+
+        /**
+         * Sets whether or not the strategy allows scanning of all parent classes or only those
+         * annotated with {@code Table} or {@code UDT} or {@code Accessor}
+         *
+         * @param scanOnlyAnnotatedClasses whether or not the strategy allows scanning of
+         *                                 all parent classes or only those annotated with
+         *                                 {@code Table} or {@code UDT} or {@code Accessor}
+         * @return the HierarchyScanStrategy to enable builder pattern
+         */
+        public HierarchyScanStrategy setScanOnlyAnnotatedClasses(boolean scanOnlyAnnotatedClasses) {
+            this.scanOnlyAnnotatedClasses = scanOnlyAnnotatedClasses;
+            return this;
+        }
+
+        /**
+         * Returns the deepest parent class to the strategy allows to scan
+         * (which default to {@code Object.class})
+         *
+         * @return the deepest parent class to the strategy allows to scan
+         * (which default to {@code Object.class})
+         */
+        public Class<?> getDeepestAllowedAncestor() {
+            return deepestAllowedAncestor;
+        }
+
+        /**
+         * Sets the deepest parent class to the strategy allows to scan
+         *
+         * @param deepestAllowedAncestor the deepest parent class to the strategy allows to scan
+         * @return the HierarchyScanStrategy to enable builder pattern
+         */
+        public HierarchyScanStrategy setDeepestAllowedAncestor(Class<?> deepestAllowedAncestor) {
+            this.deepestAllowedAncestor = deepestAllowedAncestor;
+            return this;
+        }
+
+    }
+
+    public enum PropertyMappingStrategy {BLACK_LIST, WHITE_LIST}","[{'comment': ""I would prefer `OPT_IN` and `OPT_OUT`, but if I'm the only one to ask that, it's fine to keep it the way it is."", 'commenter': 'adutra'}]"
771,driver-mapping/src/main/java/com/datastax/driver/mapping/PropertyMapper.java,"@@ -121,15 +132,27 @@ boolean isComputed() {
     }
 
     boolean isTransient() {
-        return hasAnnotation(Transient.class) ||
-                // If a property is both annotated and declared as transient in the class annotation, the property
-                // annotations take precedence (the property will not be transient)
-                classLevelTransients.contains(propertyName)
-                        && !hasAnnotation(PartitionKey.class)
-                        && !hasAnnotation(ClusteringColumn.class)
-                        && !hasAnnotation(Column.class)
-                        && !hasAnnotation(com.datastax.driver.mapping.annotations.Field.class)
-                        && !hasAnnotation(Computed.class);
+        // JAVA-1310: Make transient properties configurable at mapper level
+        // (should properties be transient by default or not)
+        switch (mapperConfiguration.getPropertyScanConfiguration().getPropertyMappingStrategy()) {
+            case BLACK_LIST:
+                return hasAnnotation(Transient.class) ||","[{'comment': ""There is a pull request ( #750 ) that suggests to also consider fields annotated with the Java keyword `transient` â€“ as they are probably not meant to be serialized nor persisted in any form. I think it's a valid point but the author has not accepted the CLA, so if you want to consider that here, I wouldn't be against it."", 'commenter': 'adutra'}]"
771,driver-mapping/src/main/java/com/datastax/driver/mapping/PropertyMapper.java,"@@ -40,6 +41,14 @@
  */
 class PropertyMapper {
 
+    private static final Set<Class<? extends Annotation>> COLUMN_ANNOTATIONS = ImmutableSet.of(","[{'comment': ""Instead of defining this here, I'd suggest you to reuse `AnnotationParser.VALID_COLUMN_ANNOTATIONS` and `AnnotationParser.VALID_FIELD_ANNOTATIONS` by making them package-private and combining them together."", 'commenter': 'adutra'}]"
771,driver-mapping/src/main/java/com/datastax/driver/mapping/MapperConfiguration.java,"@@ -0,0 +1,336 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.mapping;
+
+import java.util.Collection;
+import java.util.HashSet;
+
+/**
+ * The configuration of the mapper.
+ * It configures the following categories:
+ * Property scan configurations:
+ * <ul>
+ * <li>Transient property names (added on top of properties annotated with {@code Transient} or
+ * property names that appear in {@code Table} or {@code UDT} annotations.</li>
+ * <li>Property scan scope: whether or not to scan properties by getters/setters, whether or not
+ * to scan properties by class fields.</li>
+ * <li>Property mapping strategy: whether to scan fields that are not blacklisted (opt-out), meaning
+ * that unless a property is explicitly annotated with {@code Transient} it will be mapped, or to scan
+ * fields that are not white-listed (opt-in), meaning that unless a property is explicitly annotated
+ * with either {@code PartitionKey} or {@code ClusteringColumn} or {@code Column} or
+ * {@code com.datastax.driver.mapping.annotations.Field} or {@code Computed} it won't be mapped.</li>
+ * <li>Hierarchy scan strategy: defines how to scan for properties in parent classes - if
+ * hierarchyScanEnabled set to false - parent classes will not be scanned at all.
+ * if scanOnlyAnnotatedClasses set to true, only parents annotated with table classes
+ * ({@code Table}, {@code UDT}, {@code Accessor}) will be scanned. And lastly - deepestAllowedAncestor
+ * will define the deepest parent class to scan - which default to {@code Object.class}</li>
+ * </ul>
+ * This is also where you get the configured settings, though those cannot be changed
+ * (they are set during the built of the Mapper object).
+ */
+public class MapperConfiguration {
+
+    private PropertyScanConfiguration propertyScanConfiguration;
+
+    public MapperConfiguration() {
+        this.propertyScanConfiguration = new PropertyScanConfiguration();
+    }
+
+    public MapperConfiguration(MapperConfiguration toCopy) {
+        this.propertyScanConfiguration = new PropertyScanConfiguration(toCopy.propertyScanConfiguration);
+    }
+
+    /**
+     * Returns the property scanning configuration
+     *
+     * @return the property scanning configuration
+     */
+    public PropertyScanConfiguration getPropertyScanConfiguration() {
+        return propertyScanConfiguration;
+    }
+
+    /**
+     * Sets the property scanning configuration
+     *
+     * @param propertyScanConfiguration property scanning configuration to use
+     * @return the MapperConfiguration to enable builder pattern
+     */
+    public MapperConfiguration setPropertyScanConfiguration(PropertyScanConfiguration propertyScanConfiguration) {
+        this.propertyScanConfiguration = propertyScanConfiguration;
+        return this;
+    }
+
+    public static class PropertyScanConfiguration {
+
+        private Collection<String> excludedProperties;
+
+        private PropertyScanScope propertyScanScope;
+
+        private PropertyMappingStrategy propertyMappingStrategy;
+
+        private HierarchyScanStrategy hierarchyScanStrategy;
+
+        public PropertyScanConfiguration() {
+            this.excludedProperties = new HashSet<String>();
+            this.propertyScanScope = new PropertyScanScope();
+            this.propertyMappingStrategy = PropertyMappingStrategy.BLACK_LIST;
+            this.hierarchyScanStrategy = new HierarchyScanStrategy();
+        }
+
+        public PropertyScanConfiguration(PropertyScanConfiguration toCopy) {
+            this.excludedProperties = toCopy.excludedProperties;
+            this.propertyScanScope = new PropertyScanScope(toCopy.propertyScanScope);
+            this.propertyMappingStrategy = toCopy.propertyMappingStrategy;
+            this.hierarchyScanStrategy = new HierarchyScanStrategy(toCopy.hierarchyScanStrategy);
+        }
+
+        /**
+         * Returns a collection of properties to exclude from mapping
+         *
+         * @return a collection of properties to exclude from mapping
+         */
+        public Collection<String> getExcludedProperties() {
+            return excludedProperties;
+        }
+
+        /**
+         * Sets a collection of properties to exclude from mapping
+         *
+         * @param excludedProperties a collection of properties to exclude from mapping
+         * @return the PropertyScanConfiguration to enable builder pattern
+         */
+        public PropertyScanConfiguration setExcludedProperties(Collection<String> excludedProperties) {","[{'comment': ""Related to [Olivier's comment about immutability](https://github.com/olim7t), Unless I missed a previous discussion rather than setters, I would prefer we use a builder strategy for `PropertyScanConfiguration` and `MappingConfiguration`.  Once intialized it would be nice if the configuration would not change and we use a builder strategy for a lot of other modes of configuration."", 'commenter': 'tolbertam'}]"
771,driver-mapping/src/test/java/com/datastax/driver/mapping/MapperConfigurationHierarchyScanStrategyTest.java,"@@ -0,0 +1,180 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.mapping;
+
+import com.datastax.driver.core.CCMTestsSupport;
+import com.datastax.driver.mapping.annotations.Column;
+import com.datastax.driver.mapping.annotations.PartitionKey;
+import com.datastax.driver.mapping.annotations.Table;
+import org.testng.annotations.BeforeClass;
+import org.testng.annotations.Test;
+
+import static org.assertj.core.api.Assertions.assertThat;
+
+/**
+ * Test for JAVA-1310 - validate ability configure ancestor property scanning:
+ * - disable
+ * - force class annotations
+ * - configure max depth ancestor
+ */
+public class MapperConfigurationHierarchyScanStrategyTest extends CCMTestsSupport {
+
+    @Override
+    public void onTestContextInitialized() {
+        execute(""CREATE TABLE foo (k int primary key, v int)"");
+        execute(""INSERT INTO foo (k, v) VALUES (1, 1)"");
+    }
+
+    @Test(groups = ""short"")
+    public void should_not_inherit_properties() {
+        MappingManager mappingManager = new MappingManager(session());
+        MapperConfiguration conf = new MapperConfiguration();
+        MapperConfiguration.PropertyScanConfiguration scanConf = new MapperConfiguration.PropertyScanConfiguration();
+        MapperConfiguration.HierarchyScanStrategy scanStrategy = new MapperConfiguration.HierarchyScanStrategy();
+        scanStrategy.setHierarchyScanEnabled(false);
+        scanConf.setHierarchyScanStrategy(scanStrategy);
+        conf.setPropertyScanConfiguration(scanConf);
+        mappingManager.mapper(Foo1.class, conf);
+    }
+
+    @Table(name = ""foo"")
+    public static class Boo1 {
+
+        @Column(name = ""notAColumn"")
+        private int notAColumn;
+
+    }
+
+    @Table(name = ""foo"")
+    public static class Foo1 extends Boo1 {
+
+        @PartitionKey
+        private int k;
+
+        public int getK() {
+            return k;
+        }
+
+        public void setK(int k) {
+            this.k = k;
+        }
+
+    }
+
+    @Test(groups = ""short"")
+    public void should_inherit_only_boo() {
+        MappingManager mappingManager = new MappingManager(session());
+        MapperConfiguration conf = new MapperConfiguration();
+        MapperConfiguration.PropertyScanConfiguration scanConf = new MapperConfiguration.PropertyScanConfiguration();
+        MapperConfiguration.HierarchyScanStrategy scanStrategy = new MapperConfiguration.HierarchyScanStrategy();
+        scanStrategy.setDeepestAllowedAncestor(Boo2.class);
+        scanConf.setHierarchyScanStrategy(scanStrategy);
+        conf.setPropertyScanConfiguration(scanConf);
+        Mapper<Foo2> mapper = mappingManager.mapper(Foo2.class, conf);
+        assertThat(mapper.get(1).getV()).isEqualTo(1);
+    }
+
+    @Table(name = ""foo"")
+    public static class Goo2 {
+
+        @Column(name = ""notAColumn"")
+        private int notAColumn;
+
+    }
+
+    @Table(name = ""foo"")
+    public static class Boo2 extends Goo2 {
+
+        private int v;
+
+        public int getV() {
+            return v;
+        }
+
+        public void setV(int v) {
+            this.v = v;
+        }
+
+    }
+
+    @Table(name = ""foo"")
+    public static class Foo2 extends Boo2 {
+
+        @PartitionKey
+        private int k;
+
+        public int getK() {
+            return k;
+        }
+
+        public void setK(int k) {
+            this.k = k;
+        }
+
+    }
+
+    @Test(groups = ""short"")
+    public void ignore_non_annotated_classes() {","[{'comment': 'Small nit:  should be `should_ignored_non_annotated_classes()`', 'commenter': 'tolbertam'}]"
772,driver-mapping/src/main/java/com/datastax/driver/mapping/MappingManager.java,"@@ -218,57 +219,33 @@ public Session getSession() {
 
     @SuppressWarnings(""unchecked"")
     private <T> Mapper<T> getMapper(Class<T> klass) {
-        Mapper<T> mapper = (Mapper<T>) mappers.get(klass);
-        if (mapper == null) {
-            synchronized (mappers) {
-                mapper = (Mapper<T>) mappers.get(klass);
-                if (mapper == null) {
-                    EntityMapper<T> entityMapper = AnnotationParser.parseEntity(klass, ReflectionMapper.factory(), this);
-                    mapper = new Mapper<T>(this, klass, entityMapper);
-                    Map<Class<?>, Mapper<?>> newMappers = new HashMap<Class<?>, Mapper<?>>(mappers);
-                    newMappers.put(klass, mapper);
-                    mappers = newMappers;
-                }
-            }
+        if (!mappers.containsKey(klass)) {
+            EntityMapper<T> entityMapper = AnnotationParser.parseEntity(klass, ReflectionMapper.factory(), this);
+            Mapper<T> mapper = new Mapper<T>(this, klass, entityMapper);
+            mappers.putIfAbsent(klass, mapper);
         }
-        return mapper;
+        return (Mapper<T>) mappers.get(klass);","[{'comment': 'I might be nitpicking but I think the following would be still more efficient:\r\n\r\n```java\r\n        Mapper<T> mapper = (Mapper<T>) mappers.get(klass);\r\n        if (mapper == null) {\r\n            EntityMapper<T> entityMapper = ...;\r\n            mapper = new Mapper<T>(this, klass, entityMapper);\r\n            Mapper<T> old = (Mapper<T>) mappers.putIfAbsent(klass, mapper);\r\n            if (old != null)\r\n                mapper = old;\r\n        }\r\n        return mapper;\r\n```\r\n... because in the nominal case (i.e. there is already an entry in the map) there would be just one access to the map instead of two.', 'commenter': 'adutra'}, {'comment': ""Yeah, i'll change it.\r\nI'm actually re-thinking that whole PR, these entire parts ([1](https://github.com/datastax/java-driver/blob/41d3d72c23f149b13303074046f72877461a3cd7/driver-mapping/src/main/java/com/datastax/driver/mapping/MappingManager.java#L82), [2](https://github.com/datastax/java-driver/blob/41d3d72c23f149b13303074046f72877461a3cd7/driver-mapping/src/main/java/com/datastax/driver/mapping/MappingManager.java#L96), etc...) may not work.\r\n\r\nI think the safest tweak will be to go back to the [original ](https://github.com/datastax/java-driver/blob/3.x/driver-mapping/src/main/java/com/datastax/driver/mapping/MappingManager.java#L37) implementation and add\r\n`final Object lock = new Object()`  for each of those maps and synchronize them when constructing. Less pretty, but less error prone i guess.\r\nWhat do you think?"", 'commenter': 'avivcarmis'}, {'comment': '@adutra', 'commenter': 'avivcarmis'}, {'comment': ""@avivcarmis `onTableRemoved` and `onTableChanged` should work on a best-effort basis. Honestly, if people out there are crazy enough to alter their schema while clients are issuing requests, it's their problem. I would actually suggest that we get rid of the `synchronized` blocks in the places you spotted and clearly specify in the documentation that concurrent modifications of the schema could end up in unexpected results."", 'commenter': 'adutra'}]"
776,driver-core/src/main/java/com/datastax/driver/core/DataType.java,"@@ -759,4 +772,16 @@ public String toString() {
             return String.format(""'%s'"", customClassName);
         }
     }
+
+    /**
+     */
+    public static class DurationType extends CustomType {","[{'comment': 'Just a reminder to either fill in or remove the javadoc comment', 'commenter': 'olim7t'}]"
776,driver-core/src/main/java/com/datastax/driver/core/Duration.java,"@@ -0,0 +1,587 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.core;
+
+import com.datastax.driver.core.exceptions.InvalidTypeException;
+import com.google.common.base.Objects;
+
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+import static com.google.common.base.Preconditions.checkArgument;
+
+
+/**
+ * Represents a duration. A duration stores separately months, days, and seconds due to the fact that
+ * the number of days in a month varies, and a day can have 23 or 25 hours if a daylight saving is involved.
+ */
+public final class Duration {
+
+    static final long NANOS_PER_MICRO = 1000L;
+    static final long NANOS_PER_MILLI = 1000 * NANOS_PER_MICRO;
+    static final long NANOS_PER_SECOND = 1000 * NANOS_PER_MILLI;
+    static final long NANOS_PER_MINUTE = 60 * NANOS_PER_SECOND;
+    static final long NANOS_PER_HOUR = 60 * NANOS_PER_MINUTE;
+    static final int DAYS_PER_WEEK = 7;
+    static final int MONTHS_PER_YEAR = 12;
+
+    /**
+     * The Regexp used to parse the duration provided as String.
+     */
+    private static final Pattern STANDARD_PATTERN =
+            Pattern.compile(""\\G(\\d+)(y|Y|mo|MO|mO|Mo|w|W|d|D|h|H|s|S|ms|MS|mS|Ms|us|US|uS|Us|Âµs|ÂµS|ns|NS|nS|Ns|m|M)"");
+
+    /**
+     * The Regexp used to parse the duration when provided in the ISO 8601 format with designators.
+     */
+    private static final Pattern ISO8601_PATTERN =
+            Pattern.compile(""P((\\d+)Y)?((\\d+)M)?((\\d+)D)?(T((\\d+)H)?((\\d+)M)?((\\d+)S)?)?"");
+
+    /**
+     * The Regexp used to parse the duration when provided in the ISO 8601 format with designators.
+     */
+    private static final Pattern ISO8601_WEEK_PATTERN = Pattern.compile(""P(\\d+)W"");
+
+    /**
+     * The Regexp used to parse the duration when provided in the ISO 8601 alternative format.
+     */
+    private static final Pattern ISO8601_ALTERNATIVE_PATTERN =
+            Pattern.compile(""P(\\d{4})-(\\d{2})-(\\d{2})T(\\d{2}):(\\d{2}):(\\d{2})"");
+
+    /**
+     * The number of months.
+     */
+    private final int months;
+
+    /**
+     * The number of days.
+     */
+    private final int days;
+
+    /**
+     * The number of nanoseconds.
+     */
+    private final long nanoseconds;
+
+    /**
+     * Creates a duration. A duration can be negative.
+     * In this case all the non zero values must be negatives.
+     *
+     * @param months      the number of months
+     * @param days        the number of days
+     * @param nanoseconds the number of nanoseconds
+     */
+    private Duration(int months, int days, long nanoseconds) {
+        // Makes sure that all the values are negatives if one of them is
+        assert (months >= 0 && days >= 0 && nanoseconds >= 0)
+                || ((months <= 0 && days <= 0 && nanoseconds <= 0));","[{'comment': 'We should throw an `IllegalArgumentException` rather than using an assertion, this is called directly from `newInstance` which is public.', 'commenter': 'olim7t'}]"
776,driver-core/src/main/java/com/datastax/driver/core/Duration.java,"@@ -0,0 +1,587 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.core;
+
+import com.datastax.driver.core.exceptions.InvalidTypeException;
+import com.google.common.base.Objects;
+
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+import static com.google.common.base.Preconditions.checkArgument;
+
+
+/**
+ * Represents a duration. A duration stores separately months, days, and seconds due to the fact that
+ * the number of days in a month varies, and a day can have 23 or 25 hours if a daylight saving is involved.
+ */
+public final class Duration {
+
+    static final long NANOS_PER_MICRO = 1000L;
+    static final long NANOS_PER_MILLI = 1000 * NANOS_PER_MICRO;
+    static final long NANOS_PER_SECOND = 1000 * NANOS_PER_MILLI;
+    static final long NANOS_PER_MINUTE = 60 * NANOS_PER_SECOND;
+    static final long NANOS_PER_HOUR = 60 * NANOS_PER_MINUTE;
+    static final int DAYS_PER_WEEK = 7;
+    static final int MONTHS_PER_YEAR = 12;
+
+    /**
+     * The Regexp used to parse the duration provided as String.
+     */
+    private static final Pattern STANDARD_PATTERN =
+            Pattern.compile(""\\G(\\d+)(y|Y|mo|MO|mO|Mo|w|W|d|D|h|H|s|S|ms|MS|mS|Ms|us|US|uS|Us|Âµs|ÂµS|ns|NS|nS|Ns|m|M)"");
+
+    /**
+     * The Regexp used to parse the duration when provided in the ISO 8601 format with designators.
+     */
+    private static final Pattern ISO8601_PATTERN =
+            Pattern.compile(""P((\\d+)Y)?((\\d+)M)?((\\d+)D)?(T((\\d+)H)?((\\d+)M)?((\\d+)S)?)?"");
+
+    /**
+     * The Regexp used to parse the duration when provided in the ISO 8601 format with designators.
+     */
+    private static final Pattern ISO8601_WEEK_PATTERN = Pattern.compile(""P(\\d+)W"");
+
+    /**
+     * The Regexp used to parse the duration when provided in the ISO 8601 alternative format.
+     */
+    private static final Pattern ISO8601_ALTERNATIVE_PATTERN =
+            Pattern.compile(""P(\\d{4})-(\\d{2})-(\\d{2})T(\\d{2}):(\\d{2}):(\\d{2})"");
+
+    /**
+     * The number of months.
+     */
+    private final int months;
+
+    /**
+     * The number of days.
+     */
+    private final int days;
+
+    /**
+     * The number of nanoseconds.
+     */
+    private final long nanoseconds;
+
+    /**
+     * Creates a duration. A duration can be negative.
+     * In this case all the non zero values must be negatives.
+     *
+     * @param months      the number of months
+     * @param days        the number of days
+     * @param nanoseconds the number of nanoseconds
+     */
+    private Duration(int months, int days, long nanoseconds) {
+        // Makes sure that all the values are negatives if one of them is
+        assert (months >= 0 && days >= 0 && nanoseconds >= 0)
+                || ((months <= 0 && days <= 0 && nanoseconds <= 0));
+
+        this.months = months;
+        this.days = days;
+        this.nanoseconds = nanoseconds;
+    }
+
+    public static Duration newInstance(int months, int days, long nanoseconds) {","[{'comment': 'Needs a javadoc with an explanation on positive/negative arguments.', 'commenter': 'olim7t'}]"
776,driver-core/src/main/java/com/datastax/driver/core/Duration.java,"@@ -0,0 +1,587 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.core;
+
+import com.datastax.driver.core.exceptions.InvalidTypeException;
+import com.google.common.base.Objects;
+
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+import static com.google.common.base.Preconditions.checkArgument;
+
+
+/**
+ * Represents a duration. A duration stores separately months, days, and seconds due to the fact that
+ * the number of days in a month varies, and a day can have 23 or 25 hours if a daylight saving is involved.
+ */
+public final class Duration {
+
+    static final long NANOS_PER_MICRO = 1000L;
+    static final long NANOS_PER_MILLI = 1000 * NANOS_PER_MICRO;
+    static final long NANOS_PER_SECOND = 1000 * NANOS_PER_MILLI;
+    static final long NANOS_PER_MINUTE = 60 * NANOS_PER_SECOND;
+    static final long NANOS_PER_HOUR = 60 * NANOS_PER_MINUTE;
+    static final int DAYS_PER_WEEK = 7;
+    static final int MONTHS_PER_YEAR = 12;
+
+    /**
+     * The Regexp used to parse the duration provided as String.
+     */
+    private static final Pattern STANDARD_PATTERN =
+            Pattern.compile(""\\G(\\d+)(y|Y|mo|MO|mO|Mo|w|W|d|D|h|H|s|S|ms|MS|mS|Ms|us|US|uS|Us|Âµs|ÂµS|ns|NS|nS|Ns|m|M)"");
+
+    /**
+     * The Regexp used to parse the duration when provided in the ISO 8601 format with designators.
+     */
+    private static final Pattern ISO8601_PATTERN =
+            Pattern.compile(""P((\\d+)Y)?((\\d+)M)?((\\d+)D)?(T((\\d+)H)?((\\d+)M)?((\\d+)S)?)?"");
+
+    /**
+     * The Regexp used to parse the duration when provided in the ISO 8601 format with designators.
+     */
+    private static final Pattern ISO8601_WEEK_PATTERN = Pattern.compile(""P(\\d+)W"");
+
+    /**
+     * The Regexp used to parse the duration when provided in the ISO 8601 alternative format.
+     */
+    private static final Pattern ISO8601_ALTERNATIVE_PATTERN =
+            Pattern.compile(""P(\\d{4})-(\\d{2})-(\\d{2})T(\\d{2}):(\\d{2}):(\\d{2})"");
+
+    /**
+     * The number of months.
+     */
+    private final int months;
+
+    /**
+     * The number of days.
+     */
+    private final int days;
+
+    /**
+     * The number of nanoseconds.
+     */
+    private final long nanoseconds;
+
+    /**
+     * Creates a duration. A duration can be negative.
+     * In this case all the non zero values must be negatives.
+     *
+     * @param months      the number of months
+     * @param days        the number of days
+     * @param nanoseconds the number of nanoseconds
+     */
+    private Duration(int months, int days, long nanoseconds) {
+        // Makes sure that all the values are negatives if one of them is
+        assert (months >= 0 && days >= 0 && nanoseconds >= 0)
+                || ((months <= 0 && days <= 0 && nanoseconds <= 0));
+
+        this.months = months;
+        this.days = days;
+        this.nanoseconds = nanoseconds;
+    }
+
+    public static Duration newInstance(int months, int days, long nanoseconds) {
+        return new Duration(months, days, nanoseconds);
+    }
+
+    /**
+     * Converts a <code>String</code> into a duration.
+     * <p>The accepted formats are:
+     * <ul>
+     * <li>multiple digits followed by a time unit like: 12h30m where the time unit can be:
+     * <ul>
+     * <li>{@code y}: years</li>
+     * <li>{@code m}: months</li>
+     * <li>{@code w}: weeks</li>
+     * <li>{@code d}: days</li>
+     * <li>{@code h}: hours</li>
+     * <li>{@code m}: minutes</li>
+     * <li>{@code s}: seconds</li>
+     * <li>{@code ms}: milliseconds</li>
+     * <li>{@code us} or {@code Âµs}: microseconds</li>
+     * <li>{@code ns}: nanoseconds</li>
+     * </ul>
+     * </li>
+     * <li>ISO 8601 format:  P[n]Y[n]M[n]DT[n]H[n]M[n]S or P[n]W</li>
+     * <li>ISO 8601 alternative format: P[YYYY]-[MM]-[DD]T[hh]:[mm]:[ss]</li>
+     * </ul>
+     *
+     * @param input the <code>String</code> to convert
+     * @return a number of nanoseconds","[{'comment': 'Returns a Duration rather.', 'commenter': 'olim7t'}]"
776,driver-core/src/main/java/com/datastax/driver/core/Duration.java,"@@ -0,0 +1,587 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.core;
+
+import com.datastax.driver.core.exceptions.InvalidTypeException;
+import com.google.common.base.Objects;
+
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+import static com.google.common.base.Preconditions.checkArgument;
+
+
+/**
+ * Represents a duration. A duration stores separately months, days, and seconds due to the fact that
+ * the number of days in a month varies, and a day can have 23 or 25 hours if a daylight saving is involved.
+ */
+public final class Duration {
+
+    static final long NANOS_PER_MICRO = 1000L;
+    static final long NANOS_PER_MILLI = 1000 * NANOS_PER_MICRO;
+    static final long NANOS_PER_SECOND = 1000 * NANOS_PER_MILLI;
+    static final long NANOS_PER_MINUTE = 60 * NANOS_PER_SECOND;
+    static final long NANOS_PER_HOUR = 60 * NANOS_PER_MINUTE;
+    static final int DAYS_PER_WEEK = 7;
+    static final int MONTHS_PER_YEAR = 12;
+
+    /**
+     * The Regexp used to parse the duration provided as String.
+     */
+    private static final Pattern STANDARD_PATTERN =
+            Pattern.compile(""\\G(\\d+)(y|Y|mo|MO|mO|Mo|w|W|d|D|h|H|s|S|ms|MS|mS|Ms|us|US|uS|Us|Âµs|ÂµS|ns|NS|nS|Ns|m|M)"");
+
+    /**
+     * The Regexp used to parse the duration when provided in the ISO 8601 format with designators.
+     */
+    private static final Pattern ISO8601_PATTERN =
+            Pattern.compile(""P((\\d+)Y)?((\\d+)M)?((\\d+)D)?(T((\\d+)H)?((\\d+)M)?((\\d+)S)?)?"");
+
+    /**
+     * The Regexp used to parse the duration when provided in the ISO 8601 format with designators.
+     */
+    private static final Pattern ISO8601_WEEK_PATTERN = Pattern.compile(""P(\\d+)W"");
+
+    /**
+     * The Regexp used to parse the duration when provided in the ISO 8601 alternative format.
+     */
+    private static final Pattern ISO8601_ALTERNATIVE_PATTERN =
+            Pattern.compile(""P(\\d{4})-(\\d{2})-(\\d{2})T(\\d{2}):(\\d{2}):(\\d{2})"");
+
+    /**
+     * The number of months.
+     */
+    private final int months;
+
+    /**
+     * The number of days.
+     */
+    private final int days;
+
+    /**
+     * The number of nanoseconds.
+     */
+    private final long nanoseconds;
+
+    /**
+     * Creates a duration. A duration can be negative.
+     * In this case all the non zero values must be negatives.
+     *
+     * @param months      the number of months
+     * @param days        the number of days
+     * @param nanoseconds the number of nanoseconds
+     */
+    private Duration(int months, int days, long nanoseconds) {
+        // Makes sure that all the values are negatives if one of them is
+        assert (months >= 0 && days >= 0 && nanoseconds >= 0)
+                || ((months <= 0 && days <= 0 && nanoseconds <= 0));
+
+        this.months = months;
+        this.days = days;
+        this.nanoseconds = nanoseconds;
+    }
+
+    public static Duration newInstance(int months, int days, long nanoseconds) {
+        return new Duration(months, days, nanoseconds);
+    }
+
+    /**
+     * Converts a <code>String</code> into a duration.
+     * <p>The accepted formats are:
+     * <ul>
+     * <li>multiple digits followed by a time unit like: 12h30m where the time unit can be:
+     * <ul>
+     * <li>{@code y}: years</li>
+     * <li>{@code m}: months</li>
+     * <li>{@code w}: weeks</li>
+     * <li>{@code d}: days</li>
+     * <li>{@code h}: hours</li>
+     * <li>{@code m}: minutes</li>
+     * <li>{@code s}: seconds</li>
+     * <li>{@code ms}: milliseconds</li>
+     * <li>{@code us} or {@code Âµs}: microseconds</li>
+     * <li>{@code ns}: nanoseconds</li>
+     * </ul>
+     * </li>
+     * <li>ISO 8601 format:  P[n]Y[n]M[n]DT[n]H[n]M[n]S or P[n]W</li>
+     * <li>ISO 8601 alternative format: P[YYYY]-[MM]-[DD]T[hh]:[mm]:[ss]</li>
+     * </ul>
+     *
+     * @param input the <code>String</code> to convert
+     * @return a number of nanoseconds
+     */
+    public static Duration from(String input) {
+        boolean isNegative = input.startsWith(""-"");
+        String source = isNegative ? input.substring(1) : input;
+
+        if (source.startsWith(""P"")) {
+            if (source.endsWith(""W""))
+                return parseIso8601WeekFormat(isNegative, source);
+
+            if (source.contains(""-""))
+                return parseIso8601AlternativeFormat(isNegative, source);
+
+            return parseIso8601Format(isNegative, source);
+        }
+        return parseStandardFormat(isNegative, source);
+    }
+
+    private static Duration parseIso8601Format(boolean isNegative, String source) {
+        Matcher matcher = ISO8601_PATTERN.matcher(source);
+        if (!matcher.matches())
+            throw new InvalidTypeException(String.format(""Unable to convert '%s' to a duration"", source));
+
+        Builder builder = new Builder(isNegative);
+        if (matcher.group(1) != null)
+            builder.addYears(groupAsLong(matcher, 2));
+
+        if (matcher.group(3) != null)
+            builder.addMonths(groupAsLong(matcher, 4));
+
+        if (matcher.group(5) != null)
+            builder.addDays(groupAsLong(matcher, 6));
+
+        // Checks if the String contains time information
+        if (matcher.group(7) != null) {
+            if (matcher.group(8) != null)
+                builder.addHours(groupAsLong(matcher, 9));
+
+            if (matcher.group(10) != null)
+                builder.addMinutes(groupAsLong(matcher, 11));
+
+            if (matcher.group(12) != null)
+                builder.addSeconds(groupAsLong(matcher, 13));
+        }
+        return builder.build();
+    }
+
+    private static Duration parseIso8601AlternativeFormat(boolean isNegative, String source) {
+        Matcher matcher = ISO8601_ALTERNATIVE_PATTERN.matcher(source);
+        if (!matcher.matches())
+            throw new InvalidTypeException(String.format(""Unable to convert '%s' to a duration"", source));
+
+        return new Builder(isNegative).addYears(groupAsLong(matcher, 1))
+                .addMonths(groupAsLong(matcher, 2))
+                .addDays(groupAsLong(matcher, 3))
+                .addHours(groupAsLong(matcher, 4))
+                .addMinutes(groupAsLong(matcher, 5))
+                .addSeconds(groupAsLong(matcher, 6))
+                .build();
+    }
+
+    private static Duration parseIso8601WeekFormat(boolean isNegative, String source) {
+        Matcher matcher = ISO8601_WEEK_PATTERN.matcher(source);
+        if (!matcher.matches())
+            throw new InvalidTypeException(String.format(""Unable to convert '%s' to a duration"", source));
+
+        return new Builder(isNegative).addWeeks(groupAsLong(matcher, 1))
+                .build();
+    }
+
+    private static Duration parseStandardFormat(boolean isNegative, String source) {
+        Matcher matcher = STANDARD_PATTERN.matcher(source);
+        if (!matcher.find())
+            throw new InvalidTypeException(String.format(""Unable to convert '%s' to a duration"", source));
+
+        Builder builder = new Builder(isNegative);
+        boolean done;
+
+        do {
+            long number = groupAsLong(matcher, 1);
+            String symbol = matcher.group(2);
+            add(builder, number, symbol);
+            done = matcher.end() == source.length();
+        }
+        while (matcher.find());
+
+        if (!done)
+            throw new InvalidTypeException(String.format(""Unable to convert '%s' to a duration"", source));
+
+        return builder.build();
+    }
+
+    private static long groupAsLong(Matcher matcher, int group) {
+        return Long.parseLong(matcher.group(group));
+    }
+
+    private static Builder add(Builder builder, long number, String symbol) {
+        String s = symbol.toLowerCase();
+        if (s.equals(""y"")) {
+            return builder.addYears(number);
+        } else if (s.equals(""mo"")) {
+            return builder.addMonths(number);
+        } else if (s.equals(""w"")) {
+            return builder.addWeeks(number);
+        } else if (s.equals(""d"")) {
+            return builder.addDays(number);
+        } else if (s.equals(""h"")) {
+            return builder.addHours(number);
+        } else if (s.equals(""m"")) {
+            return builder.addMinutes(number);
+        } else if (s.equals(""s"")) {
+            return builder.addSeconds(number);
+        } else if (s.equals(""ms"")) {
+            return builder.addMillis(number);
+        } else if (s.equals(""us"") || s.equals(""Âµs"")) {
+            return builder.addMicros(number);
+        } else if (s.equals(""ns"")) {
+            return builder.addNanos(number);
+        }
+        throw new InvalidTypeException(String.format(""Unknown duration symbol '%s'"", symbol));","[{'comment': ""I wonder if `InvalidTypeException` is the best error to throw here, it's closely related to the codec system but this class is not. Maybe `IllegalArgumentException` would be more appropriate."", 'commenter': 'olim7t'}]"
776,driver-core/src/main/java/com/datastax/driver/core/DataType.java,"@@ -759,4 +772,16 @@ public String toString() {
             return String.format(""'%s'"", customClassName);
         }
     }
+
+    /**
+     */
+    public static class DurationType extends CustomType {
+
+        private static final DurationType instance = new DurationType();
+
+        private DurationType() {
+            super(Name.CUSTOM, ""org.apache.cassandra.db.marshal.DurationType"");
+        }","[{'comment': 'Nit: we could override `toString()` here, as this produces a slightly better output, for example in `TableMetadata.toString()` (although the full custom type name is valid and entirely compatible).', 'commenter': 'olim7t'}]"
776,driver-core/src/main/java/com/datastax/driver/core/DataType.java,"@@ -503,6 +503,19 @@ public static CollectionType frozenMap(DataType keyType, DataType valueType) {
     }
 
     /**
+     * Returns a Duration type, introduced in Cassandra 3.10.
+     * <p/>
+     * Note that a Duration type does not have a native representation in CQL, and
+     * technically, is merely a special {@link DataType#custom(String) custom type}
+     * from the driver's point of view.
+     *
+     * @return the Duration type.
+     */
+    public static DurationType duration() {
+        return DurationType.instance;","[{'comment': 'We might want to add a special case in `DataType.decode()` to return this singleton as well.\r\n\r\nCurrently it returns a `CustomType` instance that is not a `DurationType`. It appears for example in the column definitions of a prepared statement.', 'commenter': 'olim7t'}, {'comment': 'Good catch, thanks.', 'commenter': 'adutra'}]"
776,driver-core/src/main/java/com/datastax/driver/core/DataTypeCqlNameParser.java,"@@ -65,6 +65,8 @@
                     .put(""timeuuid"", timeuuid())
                     .put(""tinyint"", tinyint())
                     .put(""smallint"", smallint())
+                    // duration is not really a native CQL type, but appears as so in system tables
+                    .put(""duration"", duration())","[{'comment': 'It turns out it will be in the system tables in C* 4.x (v5 protocol), but is a custom type in v4 protocol.', 'commenter': 'stamhankar999'}, {'comment': ""Nice, I remember that it was a similar story with UDTs being a custom type between a later version of 2.0 and 2.1 (with protocol v3).  I'm glad that is the case."", 'commenter': 'tolbertam'}]"
778,driver-mapping/src/main/java/com/datastax/driver/mapping/configuration/naming/NamingConvention.java,"@@ -0,0 +1,88 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.mapping.configuration.naming;
+
+import java.util.List;
+
+/**
+ * Represent a naming convention (e.g. snake_case, camelCase, etc...) to be used when
+ * auto-translating java property names to cassandra column names and vise-versa.","[{'comment': 'Small typo: ""vice versa""', 'commenter': 'adutra'}]"
778,driver-mapping/src/main/java/com/datastax/driver/mapping/configuration/naming/NamingStrategy.java,"@@ -0,0 +1,130 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.mapping.configuration.naming;
+
+public class NamingStrategy {
+
+    private boolean enabled;
+
+    private NamingConvention javaConvention;
+
+    private NamingConvention cassandraConvention;
+
+    public NamingStrategy(NamingConvention javaConvention, NamingConvention cassandraConvention) {
+        setJavaConvention(javaConvention);","[{'comment': ""Don't call public setters from a constructor, assign to fields directly."", 'commenter': 'adutra'}]"
778,driver-mapping/src/main/java/com/datastax/driver/mapping/configuration/naming/NamingStrategy.java,"@@ -0,0 +1,130 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.mapping.configuration.naming;
+
+public class NamingStrategy {
+
+    private boolean enabled;
+
+    private NamingConvention javaConvention;
+
+    private NamingConvention cassandraConvention;
+
+    public NamingStrategy(NamingConvention javaConvention, NamingConvention cassandraConvention) {
+        setJavaConvention(javaConvention);
+        setCassandraConvention(cassandraConvention);
+        enable();
+    }
+
+    public NamingStrategy(NamingStrategy toCopy) {
+        this.enabled = toCopy.enabled;
+        this.javaConvention = toCopy.javaConvention;
+        this.cassandraConvention = toCopy.cassandraConvention;
+    }
+
+    /**
+     * Receive a String input assumed to be in the Java naming convention configured,
+     * returns the String translation to the Cassandra naming convention.
+     *
+     * @param input value to translate
+     * @return Cassandra naming convention translation of the input
+     */
+    public String toCassandra(String input) {
+        if (!enabled || javaConvention.equals(cassandraConvention)) {
+            return input;
+        }
+        return cassandraConvention.join(javaConvention.split(input));
+    }
+
+    /**
+     * Receive a String input assumed to be in the Cassandra naming convention configured,
+     * returns the String translation to the Java naming convention.
+     *
+     * @param input value to translate
+     * @return Java naming convention translation of the input
+     */
+    public String toJava(String input) {","[{'comment': 'This is not covered by tests currently.', 'commenter': 'adutra'}, {'comment': 'Nor is it used anywhere in the production code. Do we really need it?', 'commenter': 'olim7t'}, {'comment': 'I went for designing an independent interface for handling translation of naming conventions to it seemed trivial to implement both ways. If we prefer to avoid unused code we can remove it. If in the future we find a need to translate Cassandra column names to java fields, as you can see, the implementation is 3 lines.\r\nSo i think we can go for removing right now.', 'commenter': 'avivcarmis'}, {'comment': '> So i think we can go for removing right now.\r\n\r\nYes please. We need to be a bit conservative at this stage because once a line of code is contributed to the driver, it means it needs to be maintained :)', 'commenter': 'adutra'}]"
778,driver-mapping/src/main/java/com/datastax/driver/mapping/configuration/naming/NamingStrategy.java,"@@ -0,0 +1,130 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.mapping.configuration.naming;
+
+public class NamingStrategy {
+
+    private boolean enabled;
+
+    private NamingConvention javaConvention;
+
+    private NamingConvention cassandraConvention;
+
+    public NamingStrategy(NamingConvention javaConvention, NamingConvention cassandraConvention) {
+        setJavaConvention(javaConvention);
+        setCassandraConvention(cassandraConvention);
+        enable();
+    }
+
+    public NamingStrategy(NamingStrategy toCopy) {
+        this.enabled = toCopy.enabled;
+        this.javaConvention = toCopy.javaConvention;
+        this.cassandraConvention = toCopy.cassandraConvention;
+    }
+
+    /**
+     * Receive a String input assumed to be in the Java naming convention configured,
+     * returns the String translation to the Cassandra naming convention.
+     *
+     * @param input value to translate
+     * @return Cassandra naming convention translation of the input
+     */
+    public String toCassandra(String input) {
+        if (!enabled || javaConvention.equals(cassandraConvention)) {
+            return input;
+        }
+        return cassandraConvention.join(javaConvention.split(input));
+    }
+
+    /**
+     * Receive a String input assumed to be in the Cassandra naming convention configured,
+     * returns the String translation to the Java naming convention.
+     *
+     * @param input value to translate
+     * @return Java naming convention translation of the input
+     */
+    public String toJava(String input) {
+        if (!enabled || javaConvention.equals(cassandraConvention)) {
+            return input;
+        }
+        return javaConvention.join(cassandraConvention.split(input));
+    }
+
+    /**
+     * Enables the naming strategy, allowing it to auto translate properties that haven't been
+     * explicitly translated.
+     */
+    public void enable() {","[{'comment': ""Sorry but isn't this going a bit too far? Why would someone enable then disable the naming strategy? IMO it should be always enabled."", 'commenter': 'adutra'}, {'comment': ""Let's discuss [below](https://github.com/datastax/java-driver/pull/778#discussion-diff-101866824R22)."", 'commenter': 'avivcarmis'}]"
778,driver-mapping/src/test/java/com/datastax/driver/mapping/PropertyNamingStrategyTest.java,"@@ -0,0 +1,314 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.mapping;
+
+import com.datastax.driver.core.CCMTestsSupport;
+import com.datastax.driver.mapping.annotations.Column;
+import com.datastax.driver.mapping.annotations.PartitionKey;
+import com.datastax.driver.mapping.annotations.Table;
+import com.datastax.driver.mapping.configuration.MapperConfiguration;
+import com.datastax.driver.mapping.configuration.naming.CommonNamingConventions;
+import org.testng.annotations.Test;
+
+import static org.assertj.core.api.Assertions.assertThat;
+
+/**
+ * Test for JAVA-1316 - validate ability to automatically translate property names
+ * across different naming conventions.
+ */
+@SuppressWarnings(""unused"")
+public class PropertyNamingStrategyTest extends CCMTestsSupport {","[{'comment': 'The following conventions seem untested: UpperCamelCase, UpperSnakeCase, LowerKebabCase, UpperKebabCase.\r\n\r\nI have a mixed feeling about the ""kebab"" variants. \r\n\r\nFirst of all, this ""kebab"" name: as per [this SO question](http://stackoverflow.com/questions/11273282/whats-the-name-for-hyphen-separated-case/12273101) it seems we could call it ""lisp-case"" or ""spinal-case"" as well. If for nothing else, I\'d rather pay hommage to a beautiful programming language than to some greasy â€“ yet savory â€“ meat skewers :)\r\n\r\nSecondly, as you said, the kebab variants are not possible in Java. They _could_ be possible in Cassandra, provided that the column name is double-quoted: `""MY-COLUMN""` is a valid CQL identifier. This should be explicitly tested for.\r\n\r\nI\'d suggest here the use of a `@DataProvider` annotation that would test all the possible valid combinations of Java + Cassandra naming conventions.', 'commenter': 'adutra'}, {'comment': ""Yeah i followed [Jackson](https://github.com/FasterXML/jackson-databind/blob/master/src/main/java/com/fasterxml/jackson/databind/PropertyNamingStrategy.java#L77) on this one and wondered if you'll like it or not. (:\r\nBTW Guava called it [hyphen](https://github.com/google/guava/blob/master/guava/src/com/google/common/base/CaseFormat.java#L35).\r\nI think we can go for `UPPER-LIPS-CASE` and `lower-lisp-case`."", 'commenter': 'avivcarmis'}, {'comment': '> I think we can go for UPPER-LIPS-CASE and lower-lisp-case\r\n\r\nYes â€“ provided you spell it LISP, not LIPS :D', 'commenter': 'adutra'}]"
778,driver-mapping/src/main/java/com/datastax/driver/mapping/PropertyMapper.java,"@@ -144,20 +170,26 @@ private String inferColumnName() {
         if (isComputed()) {
             return annotation(Computed.class).value();
         }
-        boolean caseSensitive = false;
-        String columnName = propertyName;
+        String columnName = mapperConfiguration.getNamingStrategy().toCassandra(propertyName);
         if (hasAnnotation(Column.class)) {
             Column column = annotation(Column.class);
-            caseSensitive = column.caseSensitive();
-            if (!column.name().isEmpty())
+            if (!column.name().isEmpty()) {
                 columnName = column.name();
-        } else if (hasAnnotation(com.datastax.driver.mapping.annotations.Field.class)) {
+            }
+            else if (column.caseSensitive()) {
+                columnName = propertyName;
+            }
+        }
+        else if (hasAnnotation(com.datastax.driver.mapping.annotations.Field.class)) {
             com.datastax.driver.mapping.annotations.Field udtField = annotation(com.datastax.driver.mapping.annotations.Field.class);
-            caseSensitive = udtField.caseSensitive();
-            if (!udtField.name().isEmpty())
+            if (!udtField.name().isEmpty()) {
                 columnName = udtField.name();
+            }
+            else if (udtField.caseSensitive()) {
+                columnName = propertyName;
+            }
         }
-        return caseSensitive ? Metadata.quote(columnName) : columnName.toLowerCase();
+        return columnName.matches(UNQUOTED_COLUMN_NAME_REGEXP) ? columnName : Metadata.quote(columnName);","[{'comment': 'I wonder if we could quote everything by default here, I think these names are only used for internal requests.', 'commenter': 'olim7t'}, {'comment': ""I initially tried to as i wrote in my first comment, but it broke some other places within the mapper (i can look them up although i can't recall right now) that were expecting lower cases not to be quoted."", 'commenter': 'avivcarmis'}]"
778,driver-mapping/src/main/java/com/datastax/driver/mapping/ReflectionUtils.java,"@@ -120,16 +117,18 @@
                 throw Throwables.propagate(e);
             }
             for (PropertyDescriptor property : beanInfo.getPropertyDescriptors()) {
-                properties.put(property.getName(), property);
+                if (!properties.containsKey(property.getName())) {
+                    properties.put(property.getName(), property);
+                }","[{'comment': 'Is this fix related to JAVA-1310 or 1316?', 'commenter': 'olim7t'}]"
778,driver-mapping/src/main/java/com/datastax/driver/mapping/configuration/naming/NamingStrategy.java,"@@ -0,0 +1,130 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.mapping.configuration.naming;
+
+public class NamingStrategy {
+
+    private boolean enabled;
+
+    private NamingConvention javaConvention;
+
+    private NamingConvention cassandraConvention;","[{'comment': 'I suggest making these fields final, and removing the getters and setters. The naming strategy is something that users will set at the beginning, and never worry about later. I see little value in accessing the conventions, let alone changing them at runtime.', 'commenter': 'olim7t'}, {'comment': 'So we can remove setters and `enable`, `disable` methods.\r\nOne thing is design how should users disable naming strategy all together (in case they have their cassandra column names exactly like their java fields). As you suggested below we probably want to define a NamingStrategy singleton to bypass naming. Only question is whether this is a trivial approach for the user, i.e.:\r\n\r\n`config.setNamingStrategy(NamingStrategy.PASS_THROUGH);`\r\n\r\nWhat do you think?', 'commenter': 'avivcarmis'}, {'comment': '> disable naming strategy all together\r\n\r\nI don\'t think users would actually want that, there should always be a naming strategy in place.\r\n\r\nThe question is: what should be the _default_ strategy? I think we need to prioritize continuity and favor a strategy that actually yields the same results as currently, _without any need to customize the default settings_. In other words: favor convention over configuration. So I think indeed we need a ""passthrough"" strategy as our default.', 'commenter': 'adutra'}, {'comment': '@adutra To my understanding the default today is just converting to lower-case (i.e. converting `myXmlParser` simply to `myxmlparser`), this means that the default should be [`LowerCamelCase`](https://github.com/datastax/java-driver/blob/4a300e68d1a4b82fade18351a416fa5233f885ab/driver-mapping/src/main/java/com/datastax/driver/mapping/configuration/naming/CommonNamingConventions.java#L33) to [`LowerCase`](https://github.com/datastax/java-driver/blob/4a300e68d1a4b82fade18351a416fa5233f885ab/driver-mapping/src/main/java/com/datastax/driver/mapping/configuration/naming/CommonNamingConventions.java#L176) which will provide the same result. if we pass through we are to output `myXmlParser` which might break some code for migrating developers. Right?', 'commenter': 'avivcarmis'}]"
778,driver-mapping/src/main/java/com/datastax/driver/mapping/configuration/naming/NamingConvention.java,"@@ -0,0 +1,88 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.mapping.configuration.naming;
+
+import java.util.List;
+
+/**
+ * Represent a naming convention (e.g. snake_case, camelCase, etc...) to be used when
+ * auto-translating java property names to cassandra column names and vise-versa.
+ *
+ * This interface may be implemented to define custom naming convention.
+ */
+public interface NamingConvention {
+
+    /**
+     * Receive a property name value and returns an ordered list of Word objects.
+     * Each word contains a String value and a boolean indicating whether or not
+     * the value is an abbreviation (In most cases could not be determined).
+     * Quick examples:
+     * <ul>
+     * <li>Let's consider lowerCamelCase convention and input = ""myXMLParser"",
+     * then the output should be:
+     * [
+     *   word{value = ""my"", isAbbreviation = false},
+     *   word{value = ""xml"", isAbbreviation = true},
+     *   word{value = ""parser"", isAbbreviation = false}
+     * ]</li>
+     * <li>Let's consider lower_snake_case convention and input = ""myXMLParser"",
+     * then the output may be (since there's no trivial way to determine xml
+     * to an abbreviation):
+     * [
+     *   word{value = ""my"", isAbbreviation = false},
+     *   word{value = ""xml"", isAbbreviation = false},
+     *   word{value = ""parser"", isAbbreviation = false}
+     * ]</li>
+     * </ul>
+     *
+     * @param input value to split
+     * @return an ordered list of split Word objects
+     */
+    List<Word> split(String input);
+
+    /**
+     * Receive an ordered list of Word objects and returns a result property name.
+     * Quick examples:
+     * <ul>
+     * <li>Let's consider lowerCamelCase convention with upperCaseAbbreviations set
+     * to false, and input = [
+     *   word{value = ""my"", isAbbreviation = false},
+     *   word{value = ""xml"", isAbbreviation = true},
+     *   word{value = ""parser"", isAbbreviation = false}
+     * ]
+     * then the output should be ""myXmlParser"".</li>
+     * <li>Let's consider upperCamelCase convention with upperCaseAbbreviations set
+     * to true, and input = [
+     *   word{value = ""my"", isAbbreviation = false},
+     *   word{value = ""xml"", isAbbreviation = true},
+     *   word{value = ""parser"", isAbbreviation = false}
+     * ]
+     * then the output should be ""MyXMLParser"".</li>
+     * </ul>
+     *
+     * @param input list to translate
+     * @return the result property name
+     */
+    String join(List<Word> input);
+
+    /**
+     * @param other NamingConvention to test
+     * @return whether or not the given naming convention returns exactly
+     * the same output as this
+     */
+    boolean equals(NamingConvention other);","[{'comment': 'It seems that this is only used to optimize the conversions in `NamingStrategy` when the two conventions are the same.\r\nWe could instead have a `PassThroughNamingStrategy` that contains `null` conventions and overrides `toCassandra` (and `toJava` if we keep it). It could even be a unique instance of an anonymous class exposed as a singleton, such as `NamingStrategy.PASS_THROUGH`.\r\nThis way we get rid of `equals` in all the implementations.', 'commenter': 'olim7t'}, {'comment': ""Let's discuss [above](https://github.com/datastax/java-driver/pull/778#discussion-diff-101866824R22)."", 'commenter': 'avivcarmis'}]"
778,driver-mapping/src/main/java/com/datastax/driver/mapping/configuration/naming/CommonNamingConventions.java,"@@ -0,0 +1,296 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.mapping.configuration.naming;
+
+import java.util.*;
+
+/**
+ * Implementations of industry common naming conventions
+ */
+public class CommonNamingConventions {
+
+    /**
+     * E.g. myXmlParser / myXMLParser
+     * When constructing, upperCaseAbbreviations may be used to decide:
+     * <ul>
+     * <li>{@code false} - myXmlParser (default)</li>
+     * <li>{@code true} - myXMLParser</li>
+     * </ul>
+     */
+    public static class LowerCamelCase extends CamelCase {","[{'comment': ""More documentation on these classes would be appreciated, and also on the constructors as some of them aren't clear to me (i.e. It isn't immediately clear to me what `ignorablePrefixes` means)"", 'commenter': 'tolbertam'}]"
778,driver-mapping/src/test/java/com/datastax/driver/mapping/PropertyNamingStrategyTest.java,"@@ -0,0 +1,314 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.mapping;
+
+import com.datastax.driver.core.CCMTestsSupport;
+import com.datastax.driver.mapping.annotations.Column;
+import com.datastax.driver.mapping.annotations.PartitionKey;
+import com.datastax.driver.mapping.annotations.Table;
+import com.datastax.driver.mapping.configuration.MapperConfiguration;
+import com.datastax.driver.mapping.configuration.naming.CommonNamingConventions;
+import org.testng.annotations.Test;
+
+import static org.assertj.core.api.Assertions.assertThat;
+
+/**
+ * Test for JAVA-1316 - validate ability to automatically translate property names
+ * across different naming conventions.
+ */
+@SuppressWarnings(""unused"")
+public class PropertyNamingStrategyTest extends CCMTestsSupport {
+
+    @Override
+    public void onTestContextInitialized() {
+        createTable(""camel_case_table"", ""myXmlParser"", ""other"");
+        createTable(""snake_case_table"", ""my_xml_parser"", ""other"");
+        createTable(""lower_case_table"", ""myxmlparser"", ""other"");
+    }
+
+    private void createTable(String tableName, String propertyOne, String propertyTwo) {
+        execute(String.format(""CREATE TABLE %s (\""%s\"" int primary key, \""%s\"" int)"", tableName, propertyOne, propertyTwo));
+        execute(String.format(""INSERT INTO %s (\""%s\"", \""%s\"") VALUES (1, 2)"", tableName, propertyOne, propertyTwo));
+    }
+
+    @Test(groups = ""short"")
+    public void camel_case_to_camel_case() {
+        MapperConfiguration conf = new MapperConfiguration();
+        conf.getNamingStrategy()
+                .setJavaConvention(new CommonNamingConventions.LowerCamelCase())
+                .setCassandraConvention(new CommonNamingConventions.LowerCamelCase());
+        MappingManager mappingManager = new MappingManager(session(), conf);
+        Mapper<CamelCaseToCamelCase> mapper = mappingManager.mapper(CamelCaseToCamelCase.class);
+        assertThat(mapper.get(1).getMyXmlParser()).isEqualTo(1);
+        assertThat(mapper.get(1).getOther()).isEqualTo(2);
+    }
+
+    @Table(name = ""camel_case_table"")
+    public static class CamelCaseToCamelCase {
+
+        @PartitionKey
+        private int myXmlParser;
+
+        private int other;
+
+        public int getMyXmlParser() {
+            return myXmlParser;
+        }
+
+        public void setMyXmlParser(int myXmlParser) {
+            this.myXmlParser = myXmlParser;
+        }
+
+        public int getOther() {
+            return other;
+        }
+
+        public void setOther(int other) {
+            this.other = other;
+        }
+
+    }
+
+    @Test(groups = ""short"")
+    public void camel_case_to_snake_case() {
+        MapperConfiguration conf = new MapperConfiguration();
+        conf.getNamingStrategy()
+                .setJavaConvention(new CommonNamingConventions.LowerCamelCase())
+                .setCassandraConvention(new CommonNamingConventions.LowerSnakeCase());
+        MappingManager mappingManager = new MappingManager(session(), conf);
+        Mapper<CamelCaseToSnakeCase> mapper = mappingManager.mapper(CamelCaseToSnakeCase.class);
+        assertThat(mapper.get(1).getMyXmlParser()).isEqualTo(1);
+        assertThat(mapper.get(1).getOther()).isEqualTo(2);
+    }
+
+    @Table(name = ""snake_case_table"")
+    public static class CamelCaseToSnakeCase {
+
+        @PartitionKey
+        private int myXmlParser;
+
+        private int other;
+
+        public int getMyXmlParser() {
+            return myXmlParser;
+        }
+
+        public void setMyXmlParser(int myXmlParser) {
+            this.myXmlParser = myXmlParser;
+        }
+
+        public int getOther() {
+            return other;
+        }
+
+        public void setOther(int other) {
+            this.other = other;
+        }
+
+    }
+
+    @Test(groups = ""short"")
+    public void camel_case_to_lower_case() {
+        MapperConfiguration conf = new MapperConfiguration();
+        conf.getNamingStrategy()
+                .setJavaConvention(new CommonNamingConventions.LowerCamelCase())
+                .setCassandraConvention(new CommonNamingConventions.LowerCase());
+        MappingManager mappingManager = new MappingManager(session(), conf);
+        Mapper<CamelCaseToLowerCase> mapper = mappingManager.mapper(CamelCaseToLowerCase.class);
+        assertThat(mapper.get(1).getMyXmlParser()).isEqualTo(1);
+        assertThat(mapper.get(1).getOther()).isEqualTo(2);
+    }
+
+    @Table(name = ""lower_case_table"")
+    public static class CamelCaseToLowerCase {
+
+        @PartitionKey
+        private int myXmlParser;
+
+        private int other;
+
+        public int getMyXmlParser() {
+            return myXmlParser;
+        }
+
+        public void setMyXmlParser(int myXmlParser) {
+            this.myXmlParser = myXmlParser;
+        }
+
+        public int getOther() {
+            return other;
+        }
+
+        public void setOther(int other) {
+            this.other = other;
+        }
+
+    }
+
+    @Test(groups = ""short"")
+    public void camel_case_abbreviation_to_camel_case() {
+        MapperConfiguration conf = new MapperConfiguration();
+        conf.getNamingStrategy()
+                .setJavaConvention(new CommonNamingConventions.LowerCamelCase(true))
+                .setCassandraConvention(new CommonNamingConventions.LowerCamelCase());
+        MappingManager mappingManager = new MappingManager(session(), conf);
+        Mapper<CamelCaseAbbreviationToCamelCase> mapper = mappingManager.mapper(CamelCaseAbbreviationToCamelCase.class);
+        assertThat(mapper.get(1).getMyXMLParser()).isEqualTo(1);
+        assertThat(mapper.get(1).getOther()).isEqualTo(2);
+    }
+
+    @Table(name = ""camel_case_table"")
+    public static class CamelCaseAbbreviationToCamelCase {
+
+        @PartitionKey
+        private int myXMLParser;
+
+        private int other;
+
+        public int getMyXMLParser() {
+            return myXMLParser;
+        }
+
+        public void setMyXMLParser(int myXMLParser) {
+            this.myXMLParser = myXMLParser;
+        }
+
+        public int getOther() {
+            return other;
+        }
+
+        public void setOther(int other) {
+            this.other = other;
+        }
+
+    }
+
+    @Test(groups = ""short"")
+    public void snake_case_to_camel_case() {
+        MapperConfiguration conf = new MapperConfiguration();
+        conf.getNamingStrategy()
+                .setJavaConvention(new CommonNamingConventions.LowerSnakeCase())
+                .setCassandraConvention(new CommonNamingConventions.LowerCamelCase());
+        MappingManager mappingManager = new MappingManager(session(), conf);
+        Mapper<SnakeCaseToCamelCase> mapper = mappingManager.mapper(SnakeCaseToCamelCase.class);
+        assertThat(mapper.get(1).getMy_xml_parser()).isEqualTo(1);
+        assertThat(mapper.get(1).getOther()).isEqualTo(2);
+    }
+
+    @Table(name = ""camel_case_table"")
+    public static class SnakeCaseToCamelCase {
+
+        @PartitionKey
+        private int my_xml_parser;
+
+        private int other;
+
+        public int getMy_xml_parser() {
+            return my_xml_parser;
+        }
+
+        public void setMy_xml_parser(int my_xml_parser) {
+            this.my_xml_parser = my_xml_parser;
+        }
+
+        public int getOther() {
+            return other;
+        }
+
+        public void setOther(int other) {
+            this.other = other;
+        }
+
+    }
+
+    @Test(groups = ""short"")
+    public void test_overrides() {
+        MapperConfiguration conf = new MapperConfiguration();
+        conf.getNamingStrategy()
+                .setJavaConvention(new CommonNamingConventions.LowerSnakeCase())
+                .setCassandraConvention(new CommonNamingConventions.LowerCamelCase());
+        MappingManager mappingManager = new MappingManager(session(), conf);
+        Mapper<TestOverrides> mapper = mappingManager.mapper(TestOverrides.class);
+        assertThat(mapper.get(1).getMyXmlParser()).isEqualTo(1);
+        assertThat(mapper.get(1).getGoo()).isEqualTo(2);
+    }
+
+    @Table(name = ""camel_case_table"")
+    public static class TestOverrides {
+
+        @PartitionKey
+        @Column(caseSensitive = true)
+        private int myXmlParser;
+
+        @Column(name = ""other"")
+        private int goo;
+
+        public int getMyXmlParser() {
+            return myXmlParser;
+        }
+
+        public void setMyXmlParser(int myXmlParser) {
+            this.myXmlParser = myXmlParser;
+        }
+
+        public int getGoo() {
+            return goo;
+        }
+
+        public void setGoo(int goo) {
+            this.goo = goo;
+        }
+
+    }@Test(groups = ""short"")
+
+    public void test_recommended_settings() {
+        MapperConfiguration conf = new MapperConfiguration();
+        conf.getNamingStrategy()
+                .setJavaConvention(new CommonNamingConventions.LowerCamelCase(""m"", ""_""))
+                .setCassandraConvention(new CommonNamingConventions.LowerSnakeCase());
+        MappingManager mappingManager = new MappingManager(session(), conf);
+        Mapper<TestRecommendedSettings> mapper = mappingManager.mapper(TestRecommendedSettings.class);
+        assertThat(mapper.get(1).getmMyXmlParser()).isEqualTo(1);
+        assertThat(mapper.get(1).get_other()).isEqualTo(2);
+    }
+
+    @Table(name = ""snake_case_table"")
+    public static class TestRecommendedSettings {
+
+        @PartitionKey
+        private int mMyXmlParser;","[{'comment': 'Sort of confused how this works.  Since the Java Convention is `LowerCamelCase` (`new CommonNamingConventions.LowerCamelCase(""m"", ""_"")`) should this be `mmyXmlParser`, as the first `m` would be stripped and `myXmlParser` is `LowerCamelCase` (vs `MyXmlParser` which would be `UpperCamelCase`)?', 'commenter': 'tolbertam'}, {'comment': 'So the corresponding table `snake_case_table` contain the fields `my_xml_parser` and `other`. If I am a [sworn android developer](http://source.android.com/source/code-style.html#follow-field-naming-conventions), i\'d probably put `m` in the beginning of all my private fields, or i may have any other blind faith [like putting `_`](http://stackoverflow.com/questions/1899683/is-there-a-standard-in-java-for-underscore-in-front-of-variable-or-class-nam) as i personally like. But there many other examples. Anyways, in this test we want to tell the mapper - each time you spot `m` or `_` in the beginning of a field name (or a getter/setter), strip it down. then convert to the output naming conversion.\r\nSo what the mapper does here is actually receive `mMyXmlParser` as input, strip it to `myXmlParser`, and then transform to snake case (the output strategy) which results in `my_xml_parser`. Same for `_other` field.\r\n\r\nPractically, you should use this option only if you do have a constant prefix you use, and only for the one you use. I don\'t think anyone actually uses both ""_"" and ""m"" as prefixes but the test covers them both. although [some extreme conventions](https://en.wikipedia.org/wiki/Hungarian_notation) have many different prefixes depending on the type of the variable, and we can support it too.', 'commenter': 'avivcarmis'}]"
779,changelog/README.md,"@@ -6,6 +6,7 @@
 - [bug] JAVA-1351: Include Custom Payload in Request.copy.
 - [bug] JAVA-1346: Reset heartbeat only on client reads (not writes).
 - [improvement] JAVA-866: Support tuple notation in QueryBuilder.eq/in.
+- [improvement] JAVA-1357: Document that getReplicas only returns replicas of the last token in range.","[{'comment': 'This should now be placed under 3.0.7.', 'commenter': 'adutra'}]"
782,driver-core/src/main/java/com/datastax/driver/core/Connection.java,"@@ -1311,7 +1311,8 @@ protected void initChannel(SocketChannel channel) throws Exception {
             ChannelPipeline pipeline = channel.pipeline();
 
             if (sslOptions != null) {
-                pipeline.addLast(""ssl"", sslOptions.newSSLHandler(channel));
+                InetSocketAddress addr = new InetSocketAddress(connection.address.getAddress(), connection.address.getPort());","[{'comment': 'Why not `InetSocketAddress addr = connection.address`?', 'commenter': 'olim7t'}, {'comment': ""Ah, I didn't realize InetSocketAddress' fields are 'immutable' so I took this approach to avoid possible side effects of state change. You are right though, it's unnecessary."", 'commenter': 'wimtie'}]"
782,driver-core/src/main/java/com/datastax/driver/core/AddrInfoSSLOptions.java,"@@ -0,0 +1,27 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.core;
+
+import java.net.InetSocketAddress;
+
+import io.netty.channel.socket.SocketChannel;
+import io.netty.handler.ssl.SslHandler;
+
+public interface AddrInfoSSLOptions extends SSLOptions {","[{'comment': '`AddressAwareSSLOptions` is a clearer name imo.', 'commenter': 'adutra'}, {'comment': 'Please add a javadoc explaining the reason of this child interface, a link to the Jira ticket (JAVA-1364) and a warning that this interface might be deleted in the next major release of the driver, and merged up into `SSLOptions`.', 'commenter': 'adutra'}]"
782,driver-core/src/main/java/com/datastax/driver/core/Connection.java,"@@ -1311,7 +1311,13 @@ protected void initChannel(SocketChannel channel) throws Exception {
             ChannelPipeline pipeline = channel.pipeline();
 
             if (sslOptions != null) {
-                pipeline.addLast(""ssl"", sslOptions.newSSLHandler(channel));
+                if (sslOptions instanceof AddrInfoSSLOptions) {
+                    InetSocketAddress addr = new InetSocketAddress(connection.address.getAddress(), connection.address.getPort());
+                    pipeline.addLast(""ssl"", ((AddrInfoSSLOptions)sslOptions).newSSLHandler(channel, addr));
+                } else {
+                    // Legacy compatibility
+                    pipeline.addLast(""ssl"",sslOptions.newSSLHandler(channel));","[{'comment': 'Currently, this would throw `UnsupportedOperationException` and break working code (see my other comments).', 'commenter': 'adutra'}]"
782,driver-core/src/main/java/com/datastax/driver/core/JdkSSLOptions.java,"@@ -50,17 +53,36 @@ protected JdkSSLOptions(SSLContext context, String[] cipherSuites) {
         this.cipherSuites = cipherSuites;
     }
 
+    /**
+     * Creates an SSLHandler for a connection.
+     * You might want to override this (and newSSLEngine) if you need to fine-tune the engine's configuration
+     * (for example enabling hostname verification).
+     * 
+     * @param channel the Netty channel for that connection.
+     * @param address the remote address of the connection.
+     */
     @Override
-    public SslHandler newSSLHandler(SocketChannel channel) {
+    public SslHandler newSSLHandler(SocketChannel channel, InetSocketAddress address) {
         SSLEngine engine = newSSLEngine(channel);","[{'comment': 'Here we should call `newSSLEngine(channel, address)`.', 'commenter': 'adutra'}]"
782,driver-core/src/main/java/com/datastax/driver/core/JdkSSLOptions.java,"@@ -50,17 +53,36 @@ protected JdkSSLOptions(SSLContext context, String[] cipherSuites) {
         this.cipherSuites = cipherSuites;
     }
 
+    /**
+     * Creates an SSLHandler for a connection.
+     * You might want to override this (and newSSLEngine) if you need to fine-tune the engine's configuration
+     * (for example enabling hostname verification).
+     * 
+     * @param channel the Netty channel for that connection.
+     * @param address the remote address of the connection.
+     */
     @Override
-    public SslHandler newSSLHandler(SocketChannel channel) {
+    public SslHandler newSSLHandler(SocketChannel channel, InetSocketAddress address) {
         SSLEngine engine = newSSLEngine(channel);
         return new SslHandler(engine);
     }
 
     /**
+     * Creates an SSLHandler for a connection.
+     * Deprecated: code uses newSSLHandler(SocketChannel channel, InetSocketAddress address) instead.
+     * @param channel the Netty channel for that connection.
+     */
+    @Deprecated","[{'comment': 'The deprecation should be declared in `SSLOptions`. No javadoc needed here.', 'commenter': 'adutra'}]"
782,driver-core/src/main/java/com/datastax/driver/core/JdkSSLOptions.java,"@@ -50,17 +53,36 @@ protected JdkSSLOptions(SSLContext context, String[] cipherSuites) {
         this.cipherSuites = cipherSuites;
     }
 
+    /**
+     * Creates an SSLHandler for a connection.
+     * You might want to override this (and newSSLEngine) if you need to fine-tune the engine's configuration
+     * (for example enabling hostname verification).
+     * 
+     * @param channel the Netty channel for that connection.
+     * @param address the remote address of the connection.
+     */
     @Override
-    public SslHandler newSSLHandler(SocketChannel channel) {
+    public SslHandler newSSLHandler(SocketChannel channel, InetSocketAddress address) {
         SSLEngine engine = newSSLEngine(channel);
         return new SslHandler(engine);
     }
 
     /**
+     * Creates an SSLHandler for a connection.
+     * Deprecated: code uses newSSLHandler(SocketChannel channel, InetSocketAddress address) instead.
+     * @param channel the Netty channel for that connection.
+     */
+    @Deprecated
+    @Override
+    public SslHandler newSSLHandler(SocketChannel channel) {
+        throw new UnsupportedOperationException(""This is a placeholder, please use newSSLHandler(SocketChannel channel, InetSocketAddress address)"");","[{'comment': 'This will break existing code. Instead we should keep the old implementation.', 'commenter': 'adutra'}]"
782,driver-core/src/main/java/com/datastax/driver/core/JdkSSLOptions.java,"@@ -74,6 +96,25 @@ protected SSLEngine newSSLEngine(SocketChannel channel) {
         return engine;
     }
 
+    /**
+     * Creates an SSL engine each time a connection is established.
+     * <p/>
+     * <p/>
+     * You might want to override this if you need to fine-tune the engine's configuration
+     * (for example enabling hostname verification).
+     *
+     * @param channel the Netty channel for that connection.
+     * @param address remote address for the connection
+     * @return the engine.
+     */
+    protected SSLEngine newSSLEngine(SocketChannel channel, InetSocketAddress address) {
+        SSLEngine engine = context.createSSLEngine(address.getHostName(), address.getPort());","[{'comment': 'It might be a better idea to make `newSSLEngine(SocketChannel channel)` delegate to `newSSLEngine(SocketChannel channel, InetSocketAddress address)` and test the nullity of parameter `address`. This would reduce the amount of duplicated code.', 'commenter': 'adutra'}]"
782,driver-core/src/main/java/com/datastax/driver/core/NettySSLOptions.java,"@@ -37,7 +39,13 @@ public NettySSLOptions(SslContext context) {
     }
 
     @Override
+    public SslHandler newSSLHandler(SocketChannel channel, InetSocketAddress address) {
+        return context.newHandler(channel.alloc(), address.getHostName(), address.getPort());
+    }
+
+    @Override
     public SslHandler newSSLHandler(SocketChannel channel) {
-        return context.newHandler(channel.alloc());","[{'comment': 'Same here, the current implementation should be kept as is to avoid breaking user code.', 'commenter': 'adutra'}]"
782,driver-core/src/main/java/com/datastax/driver/core/JdkSSLOptions.java,"@@ -50,17 +53,36 @@ protected JdkSSLOptions(SSLContext context, String[] cipherSuites) {
         this.cipherSuites = cipherSuites;
     }
 
+    /**
+     * Creates an SSLHandler for a connection.
+     * You might want to override this (and newSSLEngine) if you need to fine-tune the engine's configuration
+     * (for example enabling hostname verification).
+     * 
+     * @param channel the Netty channel for that connection.
+     * @param address the remote address of the connection.
+     */
     @Override
-    public SslHandler newSSLHandler(SocketChannel channel) {
+    public SslHandler newSSLHandler(SocketChannel channel, InetSocketAddress address) {
         SSLEngine engine = newSSLEngine(channel);
         return new SslHandler(engine);
     }
 
     /**
+     * Creates an SSLHandler for a connection.
+     * Deprecated: code uses newSSLHandler(SocketChannel channel, InetSocketAddress address) instead.
+     * @param channel the Netty channel for that connection.
+     */
+    @Deprecated
+    @Override
+    public SslHandler newSSLHandler(SocketChannel channel) {
+        throw new UnsupportedOperationException(""This is a placeholder, please use newSSLHandler(SocketChannel channel, InetSocketAddress address)"");
+    }
+
+    /**
      * Creates an SSL engine each time a connection is established.
      * <p/>
      * <p/>
-     * You might want to override this if you need to fine-tune the engine's configuration
+     * You might want to override this (and newSSLHandler) if you need to fine-tune the engine's configuration","[{'comment': 'nit: `(and its {@link #newSSLEngine(SocketChannel, InetSocketAddress) overloaded method})`', 'commenter': 'adutra'}]"
782,driver-core/src/main/java/com/datastax/driver/core/Connection.java,"@@ -1311,7 +1311,13 @@ protected void initChannel(SocketChannel channel) throws Exception {
             ChannelPipeline pipeline = channel.pipeline();
 
             if (sslOptions != null) {
-                pipeline.addLast(""ssl"", sslOptions.newSSLHandler(channel));
+                if (sslOptions instanceof AddrInfoSSLOptions) {
+                    InetSocketAddress addr = new InetSocketAddress(connection.address.getAddress(), connection.address.getPort());","[{'comment': 'This seems unnecessary, `connection.address` is already an `InetSocketAddress`, so the following should suffice:\r\n\r\n```java\r\npipeline.addLast(""ssl"", sslOptions.newSSLHandler(channel, connection.address));\r\n```', 'commenter': 'adutra'}]"
782,driver-core/src/main/java/com/datastax/driver/core/JdkSSLOptions.java,"@@ -50,17 +53,36 @@ protected JdkSSLOptions(SSLContext context, String[] cipherSuites) {
         this.cipherSuites = cipherSuites;
     }
 
+    /**","[{'comment': 'Javadoc here is unnecessary, this is an implementation method.', 'commenter': 'adutra'}]"
784,driver-core/src/main/java/com/datastax/driver/core/GuavaCompatibility.java,"@@ -0,0 +1,215 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.core;
+
+import com.datastax.driver.core.exceptions.DriverInternalError;
+import com.google.common.io.Resources;
+import com.google.common.reflect.TypeToken;
+import com.google.common.util.concurrent.*;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import java.io.InputStream;
+import java.net.URL;
+import java.util.Enumeration;
+import java.util.concurrent.Executor;
+import java.util.jar.Attributes;
+import java.util.jar.Manifest;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+/**
+ * A compatibility layer to support a wide range of Guava versions.
+ * <p>
+ * The driver is compatible with Guava 16.0.1 or higher, but Guava 20 introduced incompatible breaking changes in its
+ * API, that could in turn be breaking for legacy driver clients if we simply upgraded our dependency. We don't want to
+ * increment our major version ""just"" for Guava (we have other changes planned).
+ * <p>
+ * Therefore we depend on Guava 19, which has both the deprecated and the new APIs, and detect the actual version at
+ * runtime in order to call the relevant methods.
+ * <p>
+ * This is a hack, and might not work with subsequent Guava releases; the real fix is to stop exposing Guava in our
+ * public API. We'll address that in version 4 of the driver.
+ */
+public abstract class GuavaCompatibility {
+
+    private static final Logger logger = LoggerFactory.getLogger(GuavaCompatibility.class);
+    private static final Pattern GA_VERSION_EXTRACTOR = Pattern.compile(""(\\d+\\.\\d+\\.\\d+).*"");
+
+    public static final GuavaCompatibility INSTANCE = selectImplementation();
+
+    public static void init() {
+        // dummy method to force initialization of the class
+    }
+
+    public abstract <V> ListenableFuture<V> withFallback(ListenableFuture<? extends V> input,
+                                                         AsyncFunction<Throwable, V> fallback);
+
+    public abstract <V> ListenableFuture<V> withFallback(ListenableFuture<? extends V> input,
+                                                         AsyncFunction<Throwable, V> fallback, Executor executor);
+
+    public abstract <I, O> ListenableFuture<O> transformAsync(ListenableFuture<I> input,
+                                                              AsyncFunction<? super I, ? extends O> function);
+
+    public abstract <I, O> ListenableFuture<O> transformAsync(ListenableFuture<I> input,
+                                                              AsyncFunction<? super I, ? extends O> function,
+                                                              Executor executor);
+
+    public abstract boolean isSupertypeOf(TypeToken<?> target, TypeToken<?> argument);
+
+    public abstract Executor sameThreadExecutor();
+
+    private static GuavaCompatibility selectImplementation() {
+        String fullVersion = getBundleVersion(loadGuavaManifest());
+        // Get rid of potential rc qualifier, as it could throw off the lexical comparisons
+        String version = stripQualifiers(fullVersion);
+        if (version.compareTo(""16.0.1"") < 0) {
+            throw new DriverInternalError(String.format(
+                    ""Detected incompatible version of Guava in the classpath (%s). "" +
+                            ""You need 16.0.1 or higher."", fullVersion));
+        } else if (version.compareTo(""19.0"") < 0) {
+            logger.info(""Detected Guava {} in the classpath, using pre-19 compatibility layer"", fullVersion);
+            return new Version18OrLower();
+        } else {
+            logger.info(""Detected Guava {} in the classpath, using 19+ compatibility layer"", fullVersion);
+            return new Version19OrHigher();
+        }
+    }
+
+    private static Manifest loadGuavaManifest() {
+        InputStream in = null;
+        try {
+            Enumeration<URL> resources = Resources.class.getClassLoader()
+                    .getResources(""META-INF/MANIFEST.MF"");
+            while (resources.hasMoreElements()) {
+                in = resources.nextElement().openStream();
+                Manifest manifest = new Manifest(in);
+                Attributes mainAttributes = manifest.getMainAttributes();
+                String symbolicName = mainAttributes.getValue(""Bundle-SymbolicName"");
+                if (""com.google.guava"".equals(symbolicName)) {
+                    return manifest;
+                }
+            }
+            throw new DriverInternalError(""Error while looking up Guava manifest: "" +
+                    ""no manifest with symbolic name 'com.google.guava' found in classpath."");
+        } catch (Exception e) {
+            throw new DriverInternalError(""Error while looking up Guava manifest"", e);
+        } finally {
+            if (in != null) {
+                try {
+                    in.close();
+                } catch (IOException e) {
+                    // ignore
+                }
+            }
+        }
+    }
+
+    private static String getBundleVersion(Manifest manifest) {
+        return manifest.getMainAttributes().getValue(""Bundle-Version"");
+    }
+
+    private static String stripQualifiers(String fullVersion) {
+        Matcher matcher = GA_VERSION_EXTRACTOR.matcher(fullVersion);
+        if (matcher.matches()) {
+            return matcher.group(1);
+        } else {
+            throw new DriverInternalError(String.format(""Could not strip qualifiers from full Guava version %s"", fullVersion));
+        }
+    }
+
+    private static class Version18OrLower extends GuavaCompatibility {","[{'comment': 'Nit: adding `@SuppressWarnings(""deprecation"")` here would spare us a couple warnings.', 'commenter': 'adutra'}]"
784,driver-core/src/main/java/com/datastax/driver/core/GuavaCompatibility.java,"@@ -0,0 +1,215 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.core;
+
+import com.datastax.driver.core.exceptions.DriverInternalError;
+import com.google.common.io.Resources;
+import com.google.common.reflect.TypeToken;
+import com.google.common.util.concurrent.*;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import java.io.InputStream;
+import java.net.URL;
+import java.util.Enumeration;
+import java.util.concurrent.Executor;
+import java.util.jar.Attributes;
+import java.util.jar.Manifest;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+/**
+ * A compatibility layer to support a wide range of Guava versions.
+ * <p>
+ * The driver is compatible with Guava 16.0.1 or higher, but Guava 20 introduced incompatible breaking changes in its
+ * API, that could in turn be breaking for legacy driver clients if we simply upgraded our dependency. We don't want to
+ * increment our major version ""just"" for Guava (we have other changes planned).
+ * <p>
+ * Therefore we depend on Guava 19, which has both the deprecated and the new APIs, and detect the actual version at
+ * runtime in order to call the relevant methods.
+ * <p>
+ * This is a hack, and might not work with subsequent Guava releases; the real fix is to stop exposing Guava in our
+ * public API. We'll address that in version 4 of the driver.
+ */
+public abstract class GuavaCompatibility {
+
+    private static final Logger logger = LoggerFactory.getLogger(GuavaCompatibility.class);
+    private static final Pattern GA_VERSION_EXTRACTOR = Pattern.compile(""(\\d+\\.\\d+\\.\\d+).*"");
+
+    public static final GuavaCompatibility INSTANCE = selectImplementation();
+
+    public static void init() {
+        // dummy method to force initialization of the class
+    }
+
+    public abstract <V> ListenableFuture<V> withFallback(ListenableFuture<? extends V> input,
+                                                         AsyncFunction<Throwable, V> fallback);
+
+    public abstract <V> ListenableFuture<V> withFallback(ListenableFuture<? extends V> input,
+                                                         AsyncFunction<Throwable, V> fallback, Executor executor);
+
+    public abstract <I, O> ListenableFuture<O> transformAsync(ListenableFuture<I> input,
+                                                              AsyncFunction<? super I, ? extends O> function);
+
+    public abstract <I, O> ListenableFuture<O> transformAsync(ListenableFuture<I> input,
+                                                              AsyncFunction<? super I, ? extends O> function,
+                                                              Executor executor);
+
+    public abstract boolean isSupertypeOf(TypeToken<?> target, TypeToken<?> argument);
+
+    public abstract Executor sameThreadExecutor();
+
+    private static GuavaCompatibility selectImplementation() {
+        String fullVersion = getBundleVersion(loadGuavaManifest());
+        // Get rid of potential rc qualifier, as it could throw off the lexical comparisons
+        String version = stripQualifiers(fullVersion);
+        if (version.compareTo(""16.0.1"") < 0) {
+            throw new DriverInternalError(String.format(
+                    ""Detected incompatible version of Guava in the classpath (%s). "" +
+                            ""You need 16.0.1 or higher."", fullVersion));
+        } else if (version.compareTo(""19.0"") < 0) {
+            logger.info(""Detected Guava {} in the classpath, using pre-19 compatibility layer"", fullVersion);
+            return new Version18OrLower();
+        } else {
+            logger.info(""Detected Guava {} in the classpath, using 19+ compatibility layer"", fullVersion);
+            return new Version19OrHigher();
+        }
+    }
+
+    private static Manifest loadGuavaManifest() {
+        InputStream in = null;
+        try {
+            Enumeration<URL> resources = Resources.class.getClassLoader()","[{'comment': '`Resources` is itself part of Guava, and is marked `@Beta`. Why not use `GuavaCompatibility.class.getClassLoader()`?', 'commenter': 'adutra'}, {'comment': ""I hadn't noticed it was beta. I think it's better to use a Guava class to make sure it's the classloader that loaded Guava, I've changed it to `Preconditions`."", 'commenter': 'olim7t'}]"
784,driver-core/src/main/java/com/datastax/driver/core/GuavaCompatibility.java,"@@ -0,0 +1,215 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.core;
+
+import com.datastax.driver.core.exceptions.DriverInternalError;
+import com.google.common.io.Resources;
+import com.google.common.reflect.TypeToken;
+import com.google.common.util.concurrent.*;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import java.io.InputStream;
+import java.net.URL;
+import java.util.Enumeration;
+import java.util.concurrent.Executor;
+import java.util.jar.Attributes;
+import java.util.jar.Manifest;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+/**
+ * A compatibility layer to support a wide range of Guava versions.
+ * <p>
+ * The driver is compatible with Guava 16.0.1 or higher, but Guava 20 introduced incompatible breaking changes in its
+ * API, that could in turn be breaking for legacy driver clients if we simply upgraded our dependency. We don't want to
+ * increment our major version ""just"" for Guava (we have other changes planned).
+ * <p>
+ * Therefore we depend on Guava 19, which has both the deprecated and the new APIs, and detect the actual version at
+ * runtime in order to call the relevant methods.
+ * <p>
+ * This is a hack, and might not work with subsequent Guava releases; the real fix is to stop exposing Guava in our
+ * public API. We'll address that in version 4 of the driver.
+ */
+public abstract class GuavaCompatibility {
+
+    private static final Logger logger = LoggerFactory.getLogger(GuavaCompatibility.class);
+    private static final Pattern GA_VERSION_EXTRACTOR = Pattern.compile(""(\\d+\\.\\d+\\.\\d+).*"");
+
+    public static final GuavaCompatibility INSTANCE = selectImplementation();
+
+    public static void init() {
+        // dummy method to force initialization of the class
+    }
+
+    public abstract <V> ListenableFuture<V> withFallback(ListenableFuture<? extends V> input,
+                                                         AsyncFunction<Throwable, V> fallback);
+
+    public abstract <V> ListenableFuture<V> withFallback(ListenableFuture<? extends V> input,
+                                                         AsyncFunction<Throwable, V> fallback, Executor executor);
+
+    public abstract <I, O> ListenableFuture<O> transformAsync(ListenableFuture<I> input,
+                                                              AsyncFunction<? super I, ? extends O> function);
+
+    public abstract <I, O> ListenableFuture<O> transformAsync(ListenableFuture<I> input,
+                                                              AsyncFunction<? super I, ? extends O> function,
+                                                              Executor executor);
+
+    public abstract boolean isSupertypeOf(TypeToken<?> target, TypeToken<?> argument);
+
+    public abstract Executor sameThreadExecutor();
+
+    private static GuavaCompatibility selectImplementation() {
+        String fullVersion = getBundleVersion(loadGuavaManifest());
+        // Get rid of potential rc qualifier, as it could throw off the lexical comparisons
+        String version = stripQualifiers(fullVersion);
+        if (version.compareTo(""16.0.1"") < 0) {
+            throw new DriverInternalError(String.format(
+                    ""Detected incompatible version of Guava in the classpath (%s). "" +
+                            ""You need 16.0.1 or higher."", fullVersion));
+        } else if (version.compareTo(""19.0"") < 0) {
+            logger.info(""Detected Guava {} in the classpath, using pre-19 compatibility layer"", fullVersion);
+            return new Version18OrLower();
+        } else {
+            logger.info(""Detected Guava {} in the classpath, using 19+ compatibility layer"", fullVersion);
+            return new Version19OrHigher();
+        }
+    }
+
+    private static Manifest loadGuavaManifest() {
+        InputStream in = null;
+        try {
+            Enumeration<URL> resources = Resources.class.getClassLoader()
+                    .getResources(""META-INF/MANIFEST.MF"");
+            while (resources.hasMoreElements()) {
+                in = resources.nextElement().openStream();
+                Manifest manifest = new Manifest(in);
+                Attributes mainAttributes = manifest.getMainAttributes();
+                String symbolicName = mainAttributes.getValue(""Bundle-SymbolicName"");
+                if (""com.google.guava"".equals(symbolicName)) {
+                    return manifest;
+                }
+            }
+            throw new DriverInternalError(""Error while looking up Guava manifest: "" +
+                    ""no manifest with symbolic name 'com.google.guava' found in classpath."");
+        } catch (Exception e) {
+            throw new DriverInternalError(""Error while looking up Guava manifest"", e);
+        } finally {
+            if (in != null) {
+                try {
+                    in.close();
+                } catch (IOException e) {
+                    // ignore
+                }
+            }
+        }
+    }
+
+    private static String getBundleVersion(Manifest manifest) {
+        return manifest.getMainAttributes().getValue(""Bundle-Version"");
+    }
+
+    private static String stripQualifiers(String fullVersion) {
+        Matcher matcher = GA_VERSION_EXTRACTOR.matcher(fullVersion);
+        if (matcher.matches()) {
+            return matcher.group(1);
+        } else {
+            throw new DriverInternalError(String.format(""Could not strip qualifiers from full Guava version %s"", fullVersion));
+        }
+    }
+
+    private static class Version18OrLower extends GuavaCompatibility {
+
+        @Override
+        public <V> ListenableFuture<V> withFallback(ListenableFuture<? extends V> input,
+                                                    final AsyncFunction<Throwable, V> fallback) {
+            return Futures.withFallback(input, new FutureFallback<V>() {
+                @Override
+                public ListenableFuture<V> create(Throwable t) throws Exception {
+                    return fallback.apply(t);
+                }
+            });
+        }
+
+        @Override
+        public <V> ListenableFuture<V> withFallback(ListenableFuture<? extends V> input,
+                                                    final AsyncFunction<Throwable, V> fallback,
+                                                    Executor executor) {
+            return Futures.withFallback(input, new FutureFallback<V>() {
+                @Override
+                public ListenableFuture<V> create(Throwable t) throws Exception {
+                    return fallback.apply(t);
+                }
+            }, executor);
+        }
+
+        @Override
+        public <I, O> ListenableFuture<O> transformAsync(ListenableFuture<I> input, AsyncFunction<? super I, ? extends O> function) {
+            return Futures.transform(input, function);
+        }
+
+        @Override
+        public <I, O> ListenableFuture<O> transformAsync(ListenableFuture<I> input, AsyncFunction<? super I, ? extends O> function, Executor executor) {
+            return Futures.transform(input, function, executor);","[{'comment': 'Found one occurrence of this method in `com.datastax.driver.core.AsyncQueryTest#connectAndQuery`', 'commenter': 'adutra'}]"
784,driver-core/src/main/java/com/datastax/driver/core/utils/MoreObjects.java,"@@ -0,0 +1,34 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.core.utils;
+
+import java.util.Arrays;
+
+/**
+ * Driver-specific implementation of utility object methods.
+ * <p>
+ * They are available in some versions of Java/Guava, but not across all versions ranges supported by the driver, hence
+ * the custom implementation.
+ */
+public class MoreObjects {","[{'comment': ""Since it's public, maybe we could add javadocs for methods in this class, even if I concede that their purposes are pretty obvious."", 'commenter': 'adutra'}]"
784,driver-core/src/test/java/com/datastax/driver/core/policies/LimitingLoadBalancingPolicy.java,"@@ -40,8 +40,8 @@
 public class LimitingLoadBalancingPolicy extends DelegatingLoadBalancingPolicy {","[{'comment': 'The import `import com.google.common.collect.Sets` is unused.', 'commenter': 'adutra'}]"
791,driver-core/src/test/java/com/datastax/driver/core/CCMBridge.java,"@@ -941,6 +980,12 @@ public void run() {
             return ccm;
         }
 
+        private static boolean isThriftSupported(boolean dse, VersionNumber version) {
+            return
+                    (!dse && version.compareTo(VersionNumber.parse(""3.10"")) < 0)","[{'comment': 'I think C* 4.0 will be the first version without thrift.  It will still be present in 3.10, but not in trunk.', 'commenter': 'tolbertam'}]"
791,driver-core/src/test/java/com/datastax/driver/core/CCMBridge.java,"@@ -941,6 +978,10 @@ public void run() {
             return ccm;
         }
 
+        private static boolean isThriftSupported(boolean dse, VersionNumber version) {
+            return dse || version.compareTo(VersionNumber.parse(""4.0"")) < 0;","[{'comment': 'Just curious, but what is the significance of DSE here?  Although all versions of DSE support Thrift at the moment, i imagine that will change too.  That being said I think it is fine this way for now.', 'commenter': 'tolbertam'}, {'comment': ""I'm just assuming that all versions of DSE support thrift for now, because I don't know when C* 4.0 will get pulled in DSE."", 'commenter': 'adutra'}]"
794,driver-core/src/main/java/com/datastax/driver/core/Cluster.java,"@@ -2201,7 +2201,14 @@ public PreparedStatement addPrepared(PreparedStatement stmt) {
 
                 // The one object in the cache will get GCed once it's not referenced by the client anymore since we use a weak reference.
                 // So we need to make sure that the instance we do return to the user is the one that is in the cache.
-                return previous;
+                if (previous.getPreparedId().getResultMetadataId().equals(stmt.getPreparedId().metadata)) {","[{'comment': 'This should be ` if (previous.getPreparedId().getResultMetadataId().equals(stmt.getPreparedId().getResultMetadataId()))`.', 'commenter': 'adutra'}, {'comment': 'The warning line 2199 should imo only be printed if we are in this side of the if/else block.', 'commenter': 'adutra'}]"
794,driver-core/src/main/java/com/datastax/driver/core/PreparedId.java,"@@ -24,16 +24,33 @@
     final MD5Digest id;
 
     final ColumnDefinitions metadata;
-    final ColumnDefinitions resultSetMetadata;
+
+    private volatile MD5Digest resultMetadataId;
+    private volatile ColumnDefinitions resultSetMetadata;
 
     final int[] routingKeyIndexes;
     final ProtocolVersion protocolVersion;
 
-    PreparedId(MD5Digest id, ColumnDefinitions metadata, ColumnDefinitions resultSetMetadata, int[] routingKeyIndexes, ProtocolVersion protocolVersion) {
+    PreparedId(MD5Digest id, MD5Digest resultMetadataId, ColumnDefinitions metadata, ColumnDefinitions resultSetMetadata, int[] routingKeyIndexes, ProtocolVersion protocolVersion) {
         this.id = id;
+        this.resultMetadataId = resultMetadataId;
         this.metadata = metadata;
         this.resultSetMetadata = resultSetMetadata;
         this.routingKeyIndexes = routingKeyIndexes;
         this.protocolVersion = protocolVersion;
     }
+
+    public void swap(MD5Digest metadataId, ColumnDefinitions resultSetMetadata)","[{'comment': 'This swap is not atomic, we should either synchronize on `this` or create an `AtomicReference` to hold a value object containing both data.', 'commenter': 'adutra'}]"
794,driver-core/src/main/java/com/datastax/driver/core/Responses.java,"@@ -380,6 +383,14 @@ static Metadata decode(ByteBuf body, boolean withPkIndices, ProtocolVersion prot
                     EnumSet<Flag> flags = Flag.deserialize(body.readInt());
                     int columnCount = body.readInt();
 
+                    MD5Digest resultMetadataId = null;
+                    if (flags.contains(Flag.METADATA_CHANGED))
+                    {
+                        assert protocolVersion == ProtocolVersion.V5 : ""MetadataChanged flag is supported starting from v5"";","[{'comment': 'Nit: `MetadataChanged` -> `METADATA_CHANGED`', 'commenter': 'adutra'}]"
794,driver-core/src/main/java/com/datastax/driver/core/Cluster.java,"@@ -2208,16 +2208,35 @@ public void ensurePoolsSizing() {
         }
 
         public PreparedStatement addPrepared(PreparedStatement stmt) {","[{'comment': ""Do we still need to change this method?\r\nIt's not very important to handle race conditions here; even if we store an instance that has the wrong `resultSetMetadata`, the server will inform us in the first response and we'll mutate the instance then."", 'commenter': 'olim7t'}, {'comment': ""Indeed, but at least the new version would not print a log warning about re-preparing the same statement, if it's being re-prepared because the resultset metadata has changed."", 'commenter': 'adutra'}, {'comment': ""> if it's being re-prepared because the resultset metadata has changed.\r\n\r\nThis is no longer the case. When the resultset metadata has changed, we don't re-prepare and re-add the statement, only mutate it in place.\r\nSo the only way `addPrepared` is called twice is if the client explicitly called `session.prepare` twice with the same statement. Whether the resultset metadata changed or not between the two calls is irrelevant I think, this is still an anti-pattern."", 'commenter': 'olim7t'}]"
794,driver-core/src/main/java/com/datastax/driver/core/SessionManager.java,"@@ -567,16 +566,23 @@ else if (fetchSize != Integer.MAX_VALUE)
             request = new Requests.Query(qString, options, statement.isTracing());
         } else if (statement instanceof BoundStatement) {
             BoundStatement bs = (BoundStatement) statement;
-            if (!cluster.manager.preparedQueries.containsKey(bs.statement.getPreparedId().id)) {
+            if (!cluster.manager.preparedQueries.containsKey(bs.statement.getPreparedId().boundValuesMetadata.id)) {
                 throw new InvalidQueryException(String.format(""Tried to execute unknown prepared query : %s. ""
-                        + ""You may have used a PreparedStatement that was created with another Cluster instance."", bs.statement.getPreparedId().id));
+                        + ""You may have used a PreparedStatement that was created with another Cluster instance."", bs.statement.getPreparedId().boundValuesMetadata.id));
             }
             if (protocolVersion.compareTo(ProtocolVersion.V4) < 0)
                 bs.ensureAllSet();
-            boolean skipMetadata = protocolVersion != ProtocolVersion.V1 && bs.statement.getPreparedId().resultSetMetadata != null;
-            Requests.QueryProtocolOptions options = new Requests.QueryProtocolOptions(Message.Request.Type.EXECUTE, consistency, Arrays.asList(bs.wrapper.values), Collections.<String, ByteBuffer>emptyMap(),
-                    skipMetadata, fetchSize, usedPagingState, serialConsistency, defaultTimestamp);
-            request = new Requests.Execute(bs.statement.getPreparedId().id, options, statement.isTracing());
+
+            // skip resultset metadata if version > 1 (otherwise this feature is not supported)
+            // and if we already have metadata for the prepared statement being executed.
+            boolean skipMetadata = protocolVersion != ProtocolVersion.V1 && bs.statement.getPreparedId().resultSetMetadata.variables != null;","[{'comment': ""Isn't the driver supposed to always receive results metadata after a `PREPARE` message? I was wondering if the check for `bs.statement.getPreparedId().resultSetMetadata.variables != null` wasn't redundant."", 'commenter': 'newkek'}]"
794,driver-core/src/main/java/com/datastax/driver/core/Responses.java,"@@ -380,6 +384,17 @@ static Metadata decode(ByteBuf body, boolean withPkIndices, ProtocolVersion prot
                     EnumSet<Flag> flags = Flag.deserialize(body.readInt());
                     int columnCount = body.readInt();
 
+                    ByteBuffer state = null;
+                    if (flags.contains(Flag.HAS_MORE_PAGES))
+                        state = CBUtil.readValue(body);
+
+                    MD5Digest resultMetadataId = null;
+                    if (flags.contains(Flag.METADATA_CHANGED)) {
+                        assert protocolVersion == ProtocolVersion.V5 : ""METADATA_CHANGED flag is supported starting from v5"";","[{'comment': ""Is it possible that we would ever receive from Cassandra the `METADATA_CHANGED` with protocol < V5? Maybe that check isn't necessary?"", 'commenter': 'newkek'}, {'comment': 'This is the point of the assertion: we can not receive it. If we somehow did - throw assertion error.', 'commenter': 'ifesdjeen'}]"
794,driver-core/src/main/java/com/datastax/driver/core/Cluster.java,"@@ -1388,7 +1388,7 @@ private static String generateClusterName() {
         // new one join the cluster).
         // Note: we could move this down to the session level, but since prepared statement are global to a node,
         // this would yield a slightly less clear behavior.
-        ConcurrentMap<MD5Digest, PreparedStatement> preparedQueries;
+        public ConcurrentMap<MD5Digest, PreparedStatement> preparedQueries;","[{'comment': 'Looks unrelated to the latest change, is it a leftover?', 'commenter': 'olim7t'}]"
794,driver-core/src/main/java/com/datastax/driver/core/Cluster.java,"@@ -2218,14 +2218,15 @@ public void ensurePoolsSizing() {
         }
 
         public PreparedStatement addPrepared(PreparedStatement stmt) {
-            PreparedStatement previous = preparedQueries.putIfAbsent(stmt.getPreparedId().id, stmt);
+            PreparedStatement previous = preparedQueries.putIfAbsent(stmt.getPreparedId().boundValuesMetadata.id, stmt);
             if (previous != null) {
                 logger.warn(""Re-preparing already prepared query is generally an anti-pattern and will likely affect performance. ""
                         + ""Consider preparing the statement only once. Query='{}'"", stmt.getQueryString());
 
                 // The one object in the cache will get GCed once it's not referenced by the client anymore since we use a weak reference.
                 // So we need to make sure that the instance we do return to the user is the one that is in the cache.
-                return previous;
+                if (com.google.common.base.Objects.equal(previous.getPreparedId().resultSetMetadata.id, stmt.getPreparedId().resultSetMetadata.id))
+                    return previous;","[{'comment': ""I don't think it's the right way to fix the issue you described. We're returning the correct PreparedStatement to the caller of this method, but leaving the cached instance with the old result metadata. Rather it should be:\r\n```java\r\nprevious.getPreparedId().resultSetMetadata = stmt.getPreparedId().resultSetMetadata;\r\nreturn previous;\r\n```"", 'commenter': 'olim7t'}]"
794,driver-core/src/main/java/com/datastax/driver/core/ArrayBackedResultSet.java,"@@ -62,15 +62,34 @@ static ArrayBackedResultSet fromMessage(Responses.Result msg, SessionManager ses
 
                 ColumnDefinitions columnDefs;
                 if (r.metadata.columns == null) {
+                    // if resultset metadata is not present, this means that
+                    // the statement being executed is a BoundStatement (EXECUTE request),
+                    // not a RegularStatement (QUERY request), because the driver
+                    // only ever sets the flag SKIP_METADATA to true for bound statements,
+                    // never for regular ones.
                     Statement actualStatement = statement;
                     if (statement instanceof StatementWrapper) {
                         actualStatement = ((StatementWrapper) statement).getWrappedStatement();
                     }
-                    assert actualStatement instanceof BoundStatement;
-                    columnDefs = ((BoundStatement) actualStatement).statement.getPreparedId().resultSetMetadata;
+                    assert statement instanceof BoundStatement;
+                    columnDefs = ((BoundStatement) statement).statement.getPreparedId().resultSetMetadata.variables;","[{'comment': 'Should be `actualStatement` on those two lines.', 'commenter': 'olim7t'}]"
794,driver-core/src/main/java/com/datastax/driver/core/ArrayBackedResultSet.java,"@@ -62,15 +62,34 @@ static ArrayBackedResultSet fromMessage(Responses.Result msg, SessionManager ses
 
                 ColumnDefinitions columnDefs;
                 if (r.metadata.columns == null) {
+                    // if resultset metadata is not present, this means that
+                    // the statement being executed is a BoundStatement (EXECUTE request),
+                    // not a RegularStatement (QUERY request), because the driver
+                    // only ever sets the flag SKIP_METADATA to true for bound statements,
+                    // never for regular ones.
                     Statement actualStatement = statement;
                     if (statement instanceof StatementWrapper) {
                         actualStatement = ((StatementWrapper) statement).getWrappedStatement();
                     }
-                    assert actualStatement instanceof BoundStatement;
-                    columnDefs = ((BoundStatement) actualStatement).statement.getPreparedId().resultSetMetadata;
+                    assert statement instanceof BoundStatement;
+                    columnDefs = ((BoundStatement) statement).statement.getPreparedId().resultSetMetadata.variables;
                     assert columnDefs != null;
                 } else {
+                    // metadata is always present for regular statements (QUERY requests);
+                    // it might also be present for bound statements (EXECUTE requests)
+                    // if the metadata has changed server-side.
                     columnDefs = r.metadata.columns;
+                    if (statement instanceof BoundStatement) {","[{'comment': 'We should unwrap like in the other branch.', 'commenter': 'olim7t'}]"
794,driver-core/src/main/java/com/datastax/driver/core/ProtocolFeature.java,"@@ -0,0 +1,46 @@
+/*
+ * Copyright (C) 2012-2017 DataStax Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.driver.core;
+
+/**
+ * A listing of features that may or not apply to a given {@link ProtocolVersion}.
+ */
+enum ProtocolFeature {
+
+    /**
+     * The capability of updating a prepared statement if the result's metadata changes at runtime (for example, if the
+     * query is a {@code SELECT *} and the table is altered).
+     */
+    PREPARED_METADATA_CHANGES,
+    //
+    ;
+
+    /**
+     * Determines whether or not the input version supports ths feature.
+     *
+     * @param version the version to test against.
+     * @return true if supported, false otherwise.
+     */
+    boolean isSupportedBy(ProtocolVersion version) {
+        switch (this) {
+            case PREPARED_METADATA_CHANGES:
+                return version == ProtocolVersion.V5;","[{'comment': 'Might be worth checking for `>`, will be hard to catch when  we bump the version again...', 'commenter': 'ifesdjeen'}]"
796,driver-core/src/main/java/com/datastax/driver/core/RemoteEndpointAwareSSLOptions.java,"@@ -0,0 +1,62 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.core;
+
+import io.netty.channel.socket.SocketChannel;
+import io.netty.handler.ssl.SslHandler;
+
+import java.net.InetSocketAddress;
+
+/**
+ * Child interface to {@link SSLOptions} with the possibility to pass remote endpoint data
+ * when instantiating {@link SslHandler}s.
+ * <p/>
+ * This is needed when e.g. hostname verification is required.
+ * See <a href=""https://datastax-oss.atlassian.net/browse/JAVA-1364"">JAVA-1364</a> for details.
+ * <p/>
+ * The reason this is a child interface is to keep {@link SSLOptions} backwards-compatible.
+ * This interface may be be merged into {@link SSLOptions} in a later major release.
+ *
+ * @see <a href=""https://datastax-oss.atlassian.net/browse/JAVA-1364"">JAVA-1364</a>
+ * @since 3.2.0
+ */
+public interface RemoteEndpointAwareSSLOptions extends SSLOptions {
+
+    /**
+     * {@inheritDoc}
+     *
+     * @deprecated use {@link #newSSLHandler(SocketChannel, InetSocketAddress)} instead.
+     */
+    @Deprecated
+    @Override
+    SslHandler newSSLHandler(SocketChannel channel);
+
+    /**
+     * Creates a new SSL handler for the given Netty channel and the given remote remoteEndpoint.","[{'comment': '> remote remoteEndpoint\r\n\r\ntypo?', 'commenter': 'olim7t'}]"
796,driver-core/src/main/java/com/datastax/driver/core/JdkSSLOptions.java,"@@ -52,7 +53,12 @@ protected JdkSSLOptions(SSLContext context, String[] cipherSuites) {
 
     @Override
     public SslHandler newSSLHandler(SocketChannel channel) {
-        SSLEngine engine = newSSLEngine(channel);
+        return newSSLHandler(channel, null);","[{'comment': 'Now that the class implements `RemoteEndpointAwareSSLOptions`, this method should never get called by our code. We could throw an `UnsupportedOperationException` or `AssertionError` here to ensure that this is the case.', 'commenter': 'olim7t'}, {'comment': 'Agreed.\r\n\r\nHowever your remark made me think of the following situation: If someone is extending `JdkSSLOptions` and overriding this method, or the method `newSSLEngine`, he will experience runtime problems because his overridden method will not be called anymore. Maybe something worth mentioning in the upgrade guide?', 'commenter': 'adutra'}, {'comment': ""I'm even wondering if this wouldn't be a showstopper actually. I think it would be safer to create subclasses, i.e. `RemoteEndpointAwareJdkSSLOptions extends JdkSSLOptions`."", 'commenter': 'adutra'}]"
796,driver-core/src/main/java/com/datastax/driver/core/RemoteEndpointAwareSSLOptions.java,"@@ -0,0 +1,62 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.core;
+
+import io.netty.channel.socket.SocketChannel;
+import io.netty.handler.ssl.SslHandler;
+
+import java.net.InetSocketAddress;
+
+/**
+ * Child interface to {@link SSLOptions} with the possibility to pass remote endpoint data
+ * when instantiating {@link SslHandler}s.
+ * <p/>
+ * This is needed when e.g. hostname verification is required.
+ * See <a href=""https://datastax-oss.atlassian.net/browse/JAVA-1364"">JAVA-1364</a> for details.
+ * <p/>
+ * The reason this is a child interface is to keep {@link SSLOptions} backwards-compatible.
+ * This interface may be be merged into {@link SSLOptions} in a later major release.
+ *
+ * @see <a href=""https://datastax-oss.atlassian.net/browse/JAVA-1364"">JAVA-1364</a>
+ * @since 3.2.0
+ */
+public interface RemoteEndpointAwareSSLOptions extends SSLOptions {
+
+    /**
+     * {@inheritDoc}
+     *
+     * @deprecated use {@link #newSSLHandler(SocketChannel, InetSocketAddress)} instead.
+     */
+    @Deprecated
+    @Override
+    SslHandler newSSLHandler(SocketChannel channel);","[{'comment': 'I would go as far as to deprecate it in the parent interface as well, with a comment that points to this interface. Any new implementation of the SSL options should use this interface.', 'commenter': 'olim7t'}]"
796,manual/ssl/README.md,"@@ -178,15 +177,16 @@ SslContextBuilder builder = SslContextBuilder
   // only if you use client authentication
   .keyManager(new File(""client.crt""), new File(""client.key""));
 
-SSLOptions sslOptions = new NettySSLOptions(builder.build());
+SSLOptions sslOptions = new RemoteEndpointAwareNettySSLOptions(builder.build());
 
 Cluster cluster = Cluster.builder()
   .addContactPoint(""127.0.0.1"")
   .withSSL(sslOptions)
   .build();
 ```
 
-[SSLOptions]: http://docs.datastax.com/en/drivers/java/3.0/com/datastax/driver/core/SSLOptions.html
-[JdkSSLOptions]: http://docs.datastax.com/en/drivers/java/3.0/com/datastax/driver/core/JdkSSLOptions.html
-[NettySSLOptions]: http://docs.datastax.com/en/drivers/java/3.0/com/datastax/driver/core/NettySSLOptions.html
-[NettyOptions]: http://docs.datastax.com/en/drivers/java/3.0/com/datastax/driver/core/NettyOptions.html
+[RemoteEndpointAwareSSLOptions]:      http://docs.datastax.com/en/drivers/java/3.0/com/datastax/driver/core/RemoteEndpointAwareSSLOptions.html","[{'comment': 'All these urls should be updated to 3.2, but I guess we can do that later when releasing 3.2.0 (I bet many other manual pages will have to be updated as well).', 'commenter': 'adutra'}]"
803,driver-mapping/src/main/java/com/datastax/driver/mapping/MappingManager.java,"@@ -220,15 +222,11 @@ public Session getSession() {
     private <T> Mapper<T> getMapper(Class<T> klass) {
         Mapper<T> mapper = (Mapper<T>) mappers.get(klass);
         if (mapper == null) {
-            synchronized (mappers) {
-                mapper = (Mapper<T>) mappers.get(klass);
-                if (mapper == null) {
-                    EntityMapper<T> entityMapper = AnnotationParser.parseEntity(klass, this);
-                    mapper = new Mapper<T>(this, klass, entityMapper);
-                    Map<Class<?>, Mapper<?>> newMappers = new HashMap<Class<?>, Mapper<?>>(mappers);
-                    newMappers.put(klass, mapper);
-                    mappers = newMappers;
-                }
+            EntityMapper<T> entityMapper = AnnotationParser.parseEntity(klass, this);
+            mapper = new Mapper<T>(this, klass, entityMapper);
+            Mapper<T> old = (Mapper<T>) mappers.putIfAbsent(klass, mapper);
+            if (old != null) {
+                mapper = old;","[{'comment': ""This is not exactly the same as the previous version, now we create a new instance each time, even if the entry existed in the map.\r\nFor `getMapper` and `getAccessor` it doesn't matter that much, because they're only called directly by the client (and in general they will probably only do it once), but `getUDTCodec` is also called internally, each time a new class with an UDT field is parsed.\r\nChecking the map first is simple enough, so I think we should do it in all cases:\r\n```\r\nMapper<T> old = (Mapper<T>) mappers.get(klass);\r\nif (old != null) {\r\n    return old;\r\n}\r\nEntityMapper<T> entityMapper = AnnotationParser.parseEntity(klass, this);\r\nmapper = new Mapper<T>(this, klass, entityMapper);\r\nold = (Mapper<T>) mappers.putIfAbsent(klass, mapper);\r\nif (old != null) {\r\n    mapper = old;\r\n}\r\nreturn mapper;\r\n```\r\nIt can still create an unnecessary instance, but the window is much shorter."", 'commenter': 'olim7t'}, {'comment': 'ðŸ¼  my bad, I was looking at the diff view on github and I only focused on the changed lines.', 'commenter': 'olim7t'}]"
805,driver-core/src/test/java/com/datastax/driver/core/CCMBridge.java,"@@ -40,7 +40,11 @@
 
     private static final Logger logger = LoggerFactory.getLogger(CCMBridge.class);
 
-    private static final String CASSANDRA_VERSION;
+    private static final String INPUT_CASSANDRA_VERSION;","[{'comment': 'This is actually only used in the static block below and could be turned into a local variable.', 'commenter': 'adutra'}]"
805,driver-core/src/test/java/com/datastax/driver/core/CCMBridge.java,"@@ -144,7 +148,7 @@
     private static final String CCM_COMMAND;
 
     static {
-        CASSANDRA_VERSION = System.getProperty(""cassandra.version"");
+        INPUT_CASSANDRA_VERSION = System.getProperty(""cassandra.version"");","[{'comment': 'What about distinguishing two different system properties for default versions to use: one for OSS C*, one for DSE? \r\n\r\nOne could e.g. specify that tests requiring DSE should by default use 5.0.3, but tests requiring OSS C* should rather use 3.10.\r\n\r\nIn this case, the `dse` system property is useless and the mapping from a DSE version to an OSS C* one should only be used when determining the corresponding C* version of a DSE version (which might be different than the ""global"" C* version).\r\n\r\nI can give it a try myself if you don\'t mind.', 'commenter': 'adutra'}, {'comment': ""I didn't want to change the existing system properties, I'll think on this for a bit"", 'commenter': 'tolbertam'}, {'comment': ""Discussion for system properties is on the `consistentTestVersioning-adu` branch didn't want to address that here."", 'commenter': 'tolbertam'}]"
805,driver-core/src/test/java/com/datastax/driver/core/CCMBridge.java,"@@ -40,7 +40,11 @@
 
     private static final Logger logger = LoggerFactory.getLogger(CCMBridge.class);
 
-    private static final String CASSANDRA_VERSION;
+    private static final String INPUT_CASSANDRA_VERSION;
+
+    private static final VersionNumber CASSANDRA_VERSION_NUMBER;","[{'comment': 'How about calling these constants `GLOBAL_XXX` to match their getter method names?', 'commenter': 'adutra'}]"
805,driver-core/src/test/java/com/datastax/driver/core/CCMAccess.java,"@@ -34,11 +34,18 @@
     /**
      * Returns the Cassandra version of this CCM cluster.
      * <p/>
-     * By default the version is equal to {@link CCMBridge#getCassandraVersion()}.
      *
      * @return The version of this CCM cluster.
      */
-    VersionNumber getVersion();
+    VersionNumber getCassandraVersion();
+
+    /**
+     * Returns the DSE version of this CCM cluster.
+     * <p/>
+     *
+     * @return The version of this CCM cluster.
+     */
+    VersionNumber getDSEVersion();","[{'comment': 'Not sure how much it makes sense to have a CCM cluster instance reporting two different versions. A CCMAccess is a bridge to either a DSE cluster or a C* cluster, not both.', 'commenter': 'adutra'}, {'comment': 'Oh my bad, I see it now, getCassandraVersion() is filled automatically with the corresponding C* version if this instance is a DSE cluster, correct? If so, please disregard my previous comment.', 'commenter': 'adutra'}, {'comment': ""ðŸ‘ in the case the dse version is provided, the cassandra version is just the derived cassandra verson from dse for deriving test behavior based on that version.  I'll update the comments to reflect that."", 'commenter': 'tolbertam'}]"
805,driver-core/src/test/java/com/datastax/driver/core/CCMAccess.java,"@@ -236,4 +243,14 @@
      */
     void waitForDown(int node);
 
+    /**
+     * @return The desired target protocol version based on the 'cassandra.version' System property.
+     */
+    ProtocolVersion getDesiredProtocolVersion();","[{'comment': 'Suggestion: replace ""desired"" â€“ its meaning is quite unclear here â€“ with ""target"", e.g. `getTargetProtocolVersion` os even simply `getProtocolVersion`.\r\nThe javadocs need to be updated, here the returned version is the version that matches this specific instance, not the global system property.', 'commenter': 'adutra'}, {'comment': 'I would describe it as the highest version common to the driver and the server, is that accurate?', 'commenter': 'olim7t'}, {'comment': 'Yep, that is the intent of this', 'commenter': 'tolbertam'}, {'comment': ""Ah I see that the documentation doesn't completely reflect that, i'll fix that up:\r\n\r\n```\r\n The target protocol version based on the 'cassandra.version' System property.\r\n```"", 'commenter': 'tolbertam'}]"
805,driver-core/src/test/java/com/datastax/driver/core/CCMBridge.java,"@@ -724,6 +694,26 @@ public void waitForDown(int node) {
         TestUtils.waitUntilPortIsDown(addressOfNode(node));
     }
 
+    @Override
+    public ProtocolVersion getDesiredProtocolVersion() {
+        VersionNumber version = getCassandraVersion();","[{'comment': ""this is only checking cassandra versions, shouldn't it be checking dse versions as well? Anyway as per my previous comment I think a single CCMBridge instance should carry only one version, and a boolean flag to indicate if it is a DSE cluster or not."", 'commenter': 'adutra'}, {'comment': 'For now the Cassandra version (which is the compatible cassandra version when DSE version is provided) is adequate for determining the protocol version.', 'commenter': 'tolbertam'}]"
805,driver-core/src/test/java/com/datastax/driver/core/CCMBridge.java,"@@ -840,41 +830,26 @@ public Builder notStarted() {
         }
 
         /**
-         * Sets this cluster to be a DSE cluster (defaults to {@link #isDSE()} if this is never called).
-         */
-        public Builder withDSE() {
-            this.isDSE = true;
-            return this;
-        }
-
-        /**
-         * Sets this cluster to be a non-DSE cluster (defaults to {@link #isDSE()} if this is never called).
+         * The Cassandra version to use.
          */
-        public Builder withoutDSE() {
-            this.isDSE = false;
+        public Builder withCassandraVersion(VersionNumber versionNumber) {","[{'comment': 'Again, not sure this makes sense, since `withCassandraVersion` and `withDSEVersion` are mutually exclusive (see assertions) why not keep just one method `withVersion`?', 'commenter': 'adutra'}, {'comment': ""Agreed, reverting this back as it isn't necessary in this particular case."", 'commenter': 'tolbertam'}]"
805,driver-core/src/test/java/com/datastax/driver/core/CCMBridge.java,"@@ -1075,16 +1105,16 @@ private String randomizePorts(CharSequence str) {
         }
 
         @Override
-        @SuppressWarnings(""SimplifiableIfStatement"")
         public boolean equals(Object o) {
-            // do not include cluster name and start, only","[{'comment': 'I think we should keep the comment about cluster name because indeed a cluster name is unique, but two CCM instances should still be considered equal if their characteristics are identical. As for the `start` property, I am not sure anymore, but I think we can include it the computation of equals() and hashcode().', 'commenter': 'adutra'}, {'comment': 'Good catch will fix', 'commenter': 'tolbertam'}, {'comment': ""Hrmm it doesn't look like cluster name is even part of the builder anymore since this is randomly generated, so maybe that comment isn't relevant anymore?"", 'commenter': 'tolbertam'}, {'comment': 'Ah right, silly me :)', 'commenter': 'adutra'}]"
805,driver-core/src/test/java/com/datastax/driver/core/TestUtils.java,"@@ -838,7 +811,7 @@ public static long getFreeMemoryMB() {
 
     /**
      * Helper for generating a DynamicCompositeType {@link ByteBuffer} from the given parameters.
-     *
+     * <p>","[{'comment': 'This is minor but I think the convention is to use `<p/>`', 'commenter': 'adutra'}, {'comment': ""Ah, good catch, IntelliJ's autoformatter behaves differently between when you have JDK 8 configured and otherwise.  For some reason with JDK 8 it omits `/`.  I'll fix it."", 'commenter': 'tolbertam'}]"
818,driver-core/src/test/java/com/datastax/driver/core/ClusterInitTest.java,"@@ -116,7 +117,9 @@ public void should_handle_failing_or_missing_contact_points() throws UnknownHost
             // - 0 or 1 for the missing host. We can't know for sure because contact points are randomized. If it's tried
             //   before the live host there will be a connection attempt, otherwise it will be removed directly because
             //   it's not in the live host's system.peers.
+            //noinspection ResultOfMethodCallIgnored
             verify(socketOptions, atLeast(6)).getKeepAlive();
+            //noinspection ResultOfMethodCallIgnored
             verify(socketOptions, atMost(7)).getKeepAlive();","[{'comment': ""These are IntelliJ IDEA warnings, I'd rather avoid IDE-specific stuff in the source code as much as possible (I use IDEA as well, but if someone added random comments for another IDE I would find it annoying)."", 'commenter': 'olim7t'}, {'comment': ""I know but there is no standard equivalent for this warning, it's specific to IntelliJ. In hindsight, I think the right choice is to remove these comments altogether."", 'commenter': 'adutra'}]"
818,driver-core/src/test/java/com/datastax/driver/core/HostAssert.java,"@@ -141,31 +142,37 @@ public void onDown(Host host) {
         return this;
     }
 
+    @SuppressWarnings(""deprecation"")
     public HostAssert hasWorkload(String workload) {
         assertThat(actual.getDseWorkload()).isNotNull().isEqualTo(workload);
         return this;
     }","[{'comment': 'If the methods are deprecated, do we still need to test them?', 'commenter': 'olim7t'}, {'comment': '1) They are only deprecated in OSS driver\r\n2) Not sure if, because some method is deprecated, it should not be tested.', 'commenter': 'adutra'}, {'comment': ""OK, I'm fine leaving it as is for now."", 'commenter': 'olim7t'}]"
818,driver-core/src/test/java/com/datastax/driver/core/TestUtils.java,"@@ -129,13 +129,13 @@ public static void setValue(SettableByIndexData<?> data, int i, DataType type, O
                 data.setUUID(i, (UUID) value);
                 break;
             case LIST:
-                data.setList(i, (List) value);
+                data.setList(i, (List<?>) value);","[{'comment': 'ðŸ‘ ', 'commenter': 'olim7t'}]"
818,driver-core/src/test/java/com/datastax/driver/core/policies/IdempotenceAwareRetryPolicyIntegrationTest.java,"@@ -34,6 +34,7 @@
 /**
  * Integration test with an IdempotenceAwareRetryPolicy.
  */
+@SuppressWarnings(""deprecation"")
 public class IdempotenceAwareRetryPolicyIntegrationTest extends AbstractRetryPolicyIntegrationTest {","[{'comment': ""I don't get any deprecation warnings here on 3.0.x. They do happen in 3.1.x though, the policy was deprecated (should we still test it?)."", 'commenter': 'olim7t'}, {'comment': 'Good catch, I will remove it from the 3.0.x and see if this test could be removed altogether in 3.1.x.', 'commenter': 'adutra'}]"
818,driver-dist/pom.xml,"@@ -70,44 +70,6 @@
                         </goals>
                         <configuration>
                             <includeDependencySources>true</includeDependencySources>
-                            <!-- optional dependencies from other modules (must be explicitly declared here in order to be correctly resolved) -->
-                            <additionalDependencies>
-                                <additionalDependency>
-                                    <groupId>org.xerial.snappy</groupId>
-                                    <artifactId>snappy-java</artifactId>
-                                    <version>${snappy.version}</version>
-                                </additionalDependency>
-                                <additionalDependency>
-                                    <groupId>net.jpountz.lz4</groupId>
-                                    <artifactId>lz4</artifactId>
-                                    <version>${lz4.version}</version>
-                                </additionalDependency>
-                                <additionalDependency>
-                                    <groupId>org.hdrhistogram</groupId>
-                                    <artifactId>HdrHistogram</artifactId>
-                                    <version>${hdr.version}</version>
-                                </additionalDependency>
-                                <additionalDependency>
-                                    <groupId>com.fasterxml.jackson.core</groupId>
-                                    <artifactId>jackson-core</artifactId>
-                                    <version>${jackson.version}</version>
-                                </additionalDependency>
-                                <additionalDependency>
-                                    <groupId>com.fasterxml.jackson.core</groupId>
-                                    <artifactId>jackson-databind</artifactId>
-                                    <version>${jackson.version}</version>
-                                </additionalDependency>
-                                <additionalDependency>
-                                    <groupId>joda-time</groupId>
-                                    <artifactId>joda-time</artifactId>
-                                    <version>${joda.version}</version>
-                                </additionalDependency>
-                                <additionalDependency>
-                                    <groupId>javax.json</groupId>
-                                    <artifactId>javax.json-api</artifactId>
-                                    <version>${jsr353-api.version}</version>
-                                </additionalDependency>
-                            </additionalDependencies>","[{'comment': 'For some reason this change never made it to the 3.1.x branch, will fix when we merge downstream.', 'commenter': 'olim7t'}, {'comment': ""Sorry I don't get it, how could it have made it to the 3.1.x branch if it isn't merged in 3.0.x yet?"", 'commenter': 'adutra'}, {'comment': 'Sorry I meant the initial addition of the `<additionalDependencies>` block (which you moved in this commit).', 'commenter': 'olim7t'}]"
821,driver-core/pom.xml,"@@ -224,7 +224,7 @@
                                     JNR does not provide OSGi bundles, so exclude it; the driver can live without it
                                     Explicitly import javax.security.cert because it's required by Netty, but Netty has been explicitly excluded
                                     -->
-                                    <![CDATA[com.google.common.*;version=""[16.0.1,21)"",!jnr.*,!io.netty.*,javax.security.cert,*]]></Import-Package>
+                                    <![CDATA[com.google.common.*;version=""[16.0.1,22)"",!jnr.*,!io.netty.*,javax.security.cert,*]]></Import-Package>","[{'comment': ""I see that sometimes we use `com.google.common.*` and sometimes `com.google.common*`. Both seem to work but for consistency I'd suggest using one or the other. (Disclaimer: I might be the culprit here.)"", 'commenter': 'adutra'}, {'comment': ""ðŸ‘ on making it consistent, didn't even notice that,  which would you prefer?  I like `com.google.common.*` i think."", 'commenter': 'tolbertam'}, {'comment': 'Same here.', 'commenter': 'adutra'}, {'comment': 'Fixed', 'commenter': 'tolbertam'}]"
827,driver-core/src/test/java/com/datastax/driver/core/HostConnectionPoolTest.java,"@@ -1242,8 +1241,9 @@ public void should_wait_on_connection_if_zero_core_connections() throws Exceptio
 
             // Should create up to core connections.
             verify(factory, timeout(readTimeout).times(1)).open(any(HostConnectionPool.class));
-
+            Uninterruptibles.getUninterruptibly(request.connectionFuture, 5, TimeUnit.SECONDS);","[{'comment': ""This line shouldn't be needed anymore since the completion of requestInitialized depends on connectionFuture."", 'commenter': 'tolbertam'}]"
827,driver-mapping/src/test/java/com/datastax/driver/mapping/MapperSaveNullFieldsTest.java,"@@ -40,6 +41,7 @@ public void setup() {
         mapper = new MappingManager(session()).mapper(User.class);
     }
 
+    @CassandraVersion(""2.2.0"")","[{'comment': ""Should be 2.1.0 right?  The problem only happens on 1.2/2.0 because client timestamps aren't used (happens in event when both inserts are done in same millisecond)"", 'commenter': 'tolbertam'}]"
827,driver-core/src/test/java/com/datastax/driver/core/HostConnectionPoolTest.java,"@@ -290,7 +290,7 @@ public void should_adjust_connection_keyspace_on_dequeue_if_pool_state_is_differ
             int count = 0;
             for (MockRequest queuedRequest : queuedRequests) {
                 try {
-                    Uninterruptibles.getUninterruptibly(queuedRequest.connectionFuture, 5, TimeUnit.SECONDS);
+                    Uninterruptibles.getUninterruptibly(queuedRequest.connectionFuture, 10, TimeUnit.SECONDS);","[{'comment': 'Just for context for anyone else looking, this was needed because of [JAVA-1371](https://datastax-oss.atlassian.net/browse/JAVA-1371).  Previously an exception would be thrown nearly right away, after 1371 the future now fails after 5 seconds.  With only waiting up to 5 seconds, this creates a very small window for waiting to timeout before the future fails.', 'commenter': 'tolbertam'}]"
831,driver-core/src/main/java/com/datastax/driver/core/SessionManager.java,"@@ -370,14 +370,17 @@ public void onFailure(Throwable t) {
                         if (t instanceof UnsupportedProtocolVersionException) {
                             cluster.manager.logUnsupportedVersionProtocol(host, ((UnsupportedProtocolVersionException) t).getUnsupportedVersion());
                             cluster.manager.triggerOnDown(host, false);
+                            future.set(false);
                         } else if (t instanceof ClusterNameMismatchException) {
                             ClusterNameMismatchException e = (ClusterNameMismatchException) t;
                             cluster.manager.logClusterNameMismatch(host, e.expectedClusterName, e.actualClusterName);
                             cluster.manager.triggerOnDown(host, false);
+                            future.set(false);
                         } else {
-                            logger.warn(""Error creating pool to "" + host, t);
+                            logger.error(""Error creating pool to "" + host, t);","[{'comment': ""Ah interesting, so previously for any Error (like OOME as documented in HostConnectionPool), we wouldn't propagate?"", 'commenter': 'tolbertam'}, {'comment': 'Correct, and the pool creation future would complete normally (although with `false` instead of `true`).\r\n\r\nI suppose it was so by design, on the assumption that the failure is transient and the failed host/pool would eventually get created later, so no need to stop the whole session initialization process. \r\n\r\nThe change I made here indeed modifies this behavior. My main intent was to account for `AuthenticationException` because when it happens, it usually happens for all hosts, and the session gets created with no pools at all and the user gets a cryptic `BusyPoolException` when he tries to use the session. \r\n\r\nBut you are right concerning OOME: it was being swollen here as well.', 'commenter': 'adutra'}, {'comment': 'I refactored this portion a bit to make things clearer wrt to what exception should be propagated and why.', 'commenter': 'adutra'}]"
831,driver-core/src/test/java/com/datastax/driver/core/AuthenticationTest.java,"@@ -132,4 +137,29 @@ public void run() {
         }, 2000);
     }
 
+    /**
+     * Ensures that the driver throws an AuthenticationException
+     * when an authentication error occurs during connection pool initialization.
+     *
+     * @jira_ticket JAVA-1431
+     */
+    @Test(groups = ""short"")
+    @CCMConfig(dirtiesContext = true)
+    public void should_not_connect_with_wrong_credentials() throws InterruptedException {
+        PlainTextAuthProvider authProvider = new PlainTextAuthProvider(""cassandra"", ""cassandra"");
+        Cluster cluster = Cluster.builder()
+            .addContactPoints(getContactPoints())
+            .withPort(ccm().getBinaryPort())
+            .withAuthProvider(authProvider)
+            .build();
+        cluster.init();
+        authProvider.setPassword(""wrong"");","[{'comment': ""One concern, what if you have 1 node in the cluster which for whatever reason you can't authenticate with?  In this case, you wouldn't be able to establish a Session.\r\n\r\nOne possibility that I could think of is if for whatever reason a node you are trying to authenticate with can't access the replicas owning data for that user id in the system_auth table and thus can't authenticate the connection.   This seems very unlikely, but in such a case it seems like it could be a bigger problem than the one this ticket attempts to solve.\r\n\r\nI think triggering the host down and starting reconnection and not propagating the exception up to the user might be a good happy medium as you can then establish connections to the rest of the nodes (in this case).  wdyt?"", 'commenter': 'tolbertam'}, {'comment': '> I think triggering the host down and starting reconnection and not propagating the exception up to the user might be a good happy medium as you can then establish connections to the rest of the nodes (in this case). wdyt?\r\n\r\n`AuthenticationException` is a strange kind of error, because it can be specific to a host in some cases (like in your example), or cluster-wise in some others; besides, it can be sometimes temporary (like when a user changes his password), sometimes permanent (wrong credentials).\r\n\r\nBut you might be right, I tested your suggestion and it seems a good comprise. On one side, it does not prevent the session from being created, which is good in your scenario. But on the other side, it also improves the user experience by having the session throw NHAE instead of `BusyPoolException` if the error happened cluster-wise.\r\n\r\nSo yes, I think you have a very good point, I will amend the code accordingly, thanks!\r\n', 'commenter': 'adutra'}]"
831,driver-core/src/main/java/com/datastax/driver/core/SessionManager.java,"@@ -431,7 +437,7 @@ CloseFuture removePool(Host host) {
         }
 
         // Wait pool creation before removing, so we don't lose connectivity
-        ListenableFuture<?> allPoolsCreatedFuture = Futures.successfulAsList(poolCreatedFutures);
+        ListenableFuture<?> allPoolsCreatedFuture = Futures.allAsList(poolCreatedFutures);","[{'comment': 'Again, required for errors to be propagated.', 'commenter': 'adutra'}, {'comment': ""There's no way to get a failed future from maybeAddPool, so the two are equivalent."", 'commenter': 'olim7t'}, {'comment': 'Now there is one, see [here](https://github.com/datastax/java-driver/blob/71c23f8b33dc5bd5cc66f3220ee119f1437b106e/driver-core/src/main/java/com/datastax/driver/core/SessionManager.java#L383-L384).', 'commenter': 'adutra'}]"
831,driver-core/src/main/java/com/datastax/driver/core/SessionManager.java,"@@ -377,7 +378,11 @@ public void onFailure(Throwable t) {
                         } else {
                             logger.warn(""Error creating pool to "" + host, t);
                         }
-                        future.set(false);
+                        if (t instanceof Error) {","[{'comment': ""Special-case  `java.lang.Error` otherwise it's not propagated."", 'commenter': 'adutra'}]"
831,driver-core/src/main/java/com/datastax/driver/core/Connection.java,"@@ -193,8 +193,8 @@ public void operationComplete(ChannelFuture future) throws Exception {
                     future.setException(t);
                 } else {
                     // Defunct to ensure that the error will be signaled (marking the host down)
-                    Exception e = (t instanceof ConnectionException || t instanceof DriverException || t instanceof InterruptedException)
-                            ? (Exception) t
+                    Throwable e = (t instanceof ConnectionException || t instanceof DriverException || t instanceof InterruptedException || t instanceof Error)","[{'comment': 'Related to propagation of `java.lang.Error`.', 'commenter': 'adutra'}]"
831,driver-core/src/test/java/com/datastax/driver/core/MemoryAppender.java,"@@ -95,4 +96,33 @@ public String getNext() {
         nextLogIdx += next.length();
         return next;
     }
+
+    public MemoryAppender enableFor(Class<?> logger) {","[{'comment': ""Unrelated, but a small refactoring I've been willing to do for a long time."", 'commenter': 'adutra'}]"
831,driver-core/pom.xml,"@@ -161,6 +161,40 @@
             <scope>test</scope>
         </dependency>
 
+        <dependency>
+            <groupId>org.jboss.byteman</groupId>
+            <artifactId>byteman</artifactId>
+            <scope>test</scope>
+            <version>${byteman.version}</version>
+        </dependency>
+
+        <dependency>
+            <groupId>org.jboss.byteman</groupId>
+            <artifactId>byteman-submit</artifactId>
+            <scope>test</scope>
+            <version>${byteman.version}</version>
+        </dependency>
+
+        <dependency>
+            <groupId>org.jboss.byteman</groupId>
+            <artifactId>byteman-install</artifactId>
+            <scope>test</scope>
+            <version>${byteman.version}</version>
+        </dependency>
+
+        <dependency>","[{'comment': 'small nit: bmunit depends on byteman-install, submit and version so you should only need to list the byteman-bmunit dependency,', 'commenter': 'tolbertam'}]"
831,driver-core/src/test/java/com/datastax/driver/core/SessionErrorTest.java,"@@ -0,0 +1,89 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.core;
+
+import org.apache.log4j.Level;
+import org.jboss.byteman.contrib.bmunit.BMNGListener;
+import org.jboss.byteman.contrib.bmunit.BMRule;
+import org.jboss.byteman.contrib.bmunit.BMUnitConfig;
+import org.testng.annotations.BeforeMethod;
+import org.testng.annotations.Listeners;
+import org.testng.annotations.Test;
+
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.assertj.core.api.Assertions.fail;
+
+/**
+ * Simple test of the Sessions methods against a one node cluster.
+ */
+@BMUnitConfig(loadDirectory = ""target/test-classes"")","[{'comment': 'This is really cool, looking forward to finding more fun use cases for byteman.', 'commenter': 'tolbertam'}]"
831,driver-core/src/test/java/com/datastax/driver/core/SessionErrorTest.java,"@@ -0,0 +1,89 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.core;
+
+import org.apache.log4j.Level;
+import org.jboss.byteman.contrib.bmunit.BMNGListener;
+import org.jboss.byteman.contrib.bmunit.BMRule;
+import org.jboss.byteman.contrib.bmunit.BMUnitConfig;
+import org.testng.annotations.BeforeMethod;
+import org.testng.annotations.Listeners;
+import org.testng.annotations.Test;
+
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.assertj.core.api.Assertions.fail;
+
+/**
+ * Simple test of the Sessions methods against a one node cluster.
+ */
+@BMUnitConfig(loadDirectory = ""target/test-classes"")
+@Listeners(BMNGListener.class)
+@CCMConfig(createCluster = false)
+public class SessionErrorTest extends CCMTestsSupport {
+
+    private Cluster cluster1;
+    private Cluster cluster2;
+
+    @BeforeMethod
+    public void setUp() throws Exception {
+        cluster1 = register(createClusterBuilder()
+                .addContactPointsWithPorts(ccm().addressOfNode(1)).build()).init();
+        cluster2 = register(createClusterBuilder()
+                .addContactPointsWithPorts(ccm().addressOfNode(1)).build()).init();
+    }
+
+    @Test","[{'comment': ""Do we intend for these to run as part of our test harness?  Since the Test annotation lacks a group definition they aren't running."", 'commenter': 'tolbertam'}]"
831,driver-core/src/test/java/com/datastax/driver/core/SessionErrorTest.java,"@@ -0,0 +1,89 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.core;
+
+import org.apache.log4j.Level;
+import org.jboss.byteman.contrib.bmunit.BMNGListener;
+import org.jboss.byteman.contrib.bmunit.BMRule;
+import org.jboss.byteman.contrib.bmunit.BMUnitConfig;
+import org.testng.annotations.BeforeMethod;
+import org.testng.annotations.Listeners;
+import org.testng.annotations.Test;
+
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.assertj.core.api.Assertions.fail;
+
+/**
+ * Simple test of the Sessions methods against a one node cluster.
+ */
+@BMUnitConfig(loadDirectory = ""target/test-classes"")
+@Listeners(BMNGListener.class)
+@CCMConfig(createCluster = false)
+public class SessionErrorTest extends CCMTestsSupport {","[{'comment': ""It would be nice if this used Scassandra so we could run through it a bit faster (an actual cassandra node isn't needed i think)."", 'commenter': 'tolbertam'}]"
831,driver-core/src/main/java/com/datastax/driver/core/SessionManager.java,"@@ -376,8 +376,14 @@ public void onFailure(Throwable t) {
                             cluster.manager.triggerOnDown(host, false);
                         } else {
                             logger.warn(""Error creating pool to "" + host, t);
+                            cluster.manager.triggerOnDown(host, true);","[{'comment': ""I don't think this is necessary. It should be the conviction policy's job to mark the node down if there are no more connections to it.\r\nIf there are still active connections (for example from another session, or the control connection) then the host should stay up."", 'commenter': 'olim7t'}]"
831,driver-core/src/main/java/com/datastax/driver/core/SessionManager.java,"@@ -431,7 +439,7 @@ CloseFuture removePool(Host host) {
         }
 
         // Wait pool creation before removing, so we don't lose connectivity
-        ListenableFuture<?> allPoolsCreatedFuture = Futures.successfulAsList(poolCreatedFutures);
+        ListenableFuture<?> allPoolsCreatedFuture = Futures.allAsList(poolCreatedFutures);","[{'comment': 'I have noticed that `void updateCreatedPools(Host h)` catches `InterruptedException` and `ExecutionException` and [does things](https://github.com/datastax/java-driver/pull/831/files#diff-8ccd18d8cb39074bf83554358d002ceeR476) if those exceptions were to be thrown. And I wonder if now that we use `allAsList()` it is something to be worried about in this method, too if that makes any sense?', 'commenter': 'newkek'}, {'comment': 'There is nothing to worry about it. Only `java.lang.Error` instances can be propagated up here, all the rest is unchanged.', 'commenter': 'adutra'}]"
832,driver-core/src/main/java/com/datastax/driver/core/querybuilder/Select.java,"@@ -95,7 +95,7 @@ StringBuilder buildQueryString(List<Object> variables, CodecRegistry codecRegist
             Utils.joinAndAppend(builder, codecRegistry, "" AND "", where.clauses, variables);
         }
 
-        if (orderings != null) {
+        if (orderings != null && !orderings.isEmpty()) {","[{'comment': ""I don't think we should silently ignore if the user calls QueryBuilder methods that are not valid to create a CQL query. If the argument in the `orderBy` method is not correct for a CQL query we should throw an exception. As it is done for other query builder methods, like `limit()` if the argument is not valid for Cassandra (limit < 0), we should not ignore it when creating the query"", 'commenter': 'newkek'}, {'comment': ""You've certainly fixed the issue as reported in JIRA, thanks for that.  My only feedback would be that when the person writing code against the querybuilder doesn't know exactly what the end user is going to give them as a query, not accepting an empty list of orderings can make for some ugly builder code:\r\n\r\nhttps://gist.github.com/podnov/b67f7e57cf702df579b68b67dde839c7\r\n\r\nI'm sure there are other ways to write this, but the ones immediately apparent to me don't seem terribly straight forward."", 'commenter': 'podnov'}]"
839,driver-mapping/src/main/java/com/datastax/driver/mapping/Mapper.java,"@@ -861,7 +865,7 @@ public void resetDefaultDeleteOptions() {
      */
     public static abstract class Option {
 
-        enum Type {TTL, TIMESTAMP, CL, TRACING, SAVE_NULL_FIELDS, IF_NOT_EXISTS}
+        enum Type {TTL, TIMESTAMP, CL, TRACING, SAVE_NULL_FIELDS, SAVE_NULL_FIELDS_AS_UNSET, IF_NOT_EXISTS}","[{'comment': 'It looks like SAVE_NULL_FIELDS_AS_UNSET is no longer used so this can be removed', 'commenter': 'tolbertam'}]"
839,manual/object_mapper/using/README.md,"@@ -106,13 +106,13 @@ mapper.save(new User(userId, ""helloworld""),
 Some options don't apply to all operations:
 
 <table border=""1"" style=""text-align:center; width:100%;margin-bottom:1em;"">
-    <tr> <td><b>Option</b></td>    <td><b>save/saveQuery</b></td> <td><b>get/getQuery</b></td> <td><b>delete/deleteQuery</b></td></tr>
-    <tr> <td>Ttl</td>              <td>yes</td>                   <td>no</td>                  <td>no</td> </tr>
-    <tr> <td>Timestamp</td>        <td>yes</td>                   <td>no</td>                  <td>yes</td> </tr>
-    <tr> <td>ConsistencyLevel</td> <td>yes</td>                   <td>yes</td>                 <td>yes</td> </tr>
-    <tr> <td>Tracing</td>          <td>yes</td>                   <td>yes</td>                 <td>yes</td> </tr>
-    <tr> <td>SaveNullFields</td>   <td>yes</td>                   <td>no</td>                  <td>no</td> </tr>
-    <tr> <td>IfNotExists</td>      <td>yes</td>                   <td>no</td>                  <td>no</td> </tr>
+    <tr> <td><b>Option</b></td>         <td><b>save/saveQuery</b></td> <td><b>get/getQuery</b></td> <td><b>delete/deleteQuery</b></td></tr>","[{'comment': 'As far as I can tell, with SaveNullFieldsAsUnset no longer needed this file is unchanged (other than the spacing), so this file change should be backed out.', 'commenter': 'tolbertam'}]"
839,driver-mapping/src/test/java/com/datastax/driver/mapping/MapperSaveNullFields22Test.java,"@@ -0,0 +1,154 @@
+/*
+ *      Copyright (C) 2012-2015 DataStax Inc.
+ *
+ *   Licensed under the Apache License, Version 2.0 (the ""License"");
+ *   you may not use this file except in compliance with the License.
+ *   You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *   Unless required by applicable law or agreed to in writing, software
+ *   distributed under the License is distributed on an ""AS IS"" BASIS,
+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *   See the License for the specific language governing permissions and
+ *   limitations under the License.
+ */
+package com.datastax.driver.mapping;
+
+import com.datastax.driver.core.BoundStatement;
+import com.datastax.driver.core.CCMTestsSupport;
+import com.datastax.driver.core.utils.CassandraVersion;
+import com.datastax.driver.mapping.Mapper.Option;
+import com.datastax.driver.mapping.annotations.PartitionKey;
+import com.datastax.driver.mapping.annotations.Table;
+import org.testng.annotations.BeforeMethod;
+import org.testng.annotations.Test;
+
+import static org.assertj.core.api.Assertions.assertThat;
+
+@SuppressWarnings(""unused"")
+public class MapperSaveNullFields22Test extends CCMTestsSupport {","[{'comment': '`@CassandraVersion(""2.2.0"")` just sets the minimum version requirement of the test, so `MapperSaveNullFieldsTest` will fail when run against >= C* 2.2.  We have an internal ci test matrix that runs against all of the latest cassandra versions (1.2.19, 2.0.17, 2.1.17, 2.2.9, 3.0.13 and 3.10 at the moment).\r\n\r\nI think what we could do instead is change `MapperSaveNullFieldsTest#should_save_null_fields` to change the criteria of the validation based on the protocol version in use, using something like this to get at the protocol version:\r\n\r\n```java\r\n        boolean unsetSupported = session().getCluster().getConfiguration().getProtocolOptions()\r\n                .getProtocolVersion().compareTo(ProtocolVersion.V4) >= 0;\r\n```\r\n\r\nand then we could do something like this in `should_save_null_fields`:\r\n\r\n```java\r\n        if (nullName && !saveExpected) {\r\n            if (unsetSupported) { // column should be present if unset supported and value should be unset\r\n                assertThat(queryString).as(description).contains(""name"");\r\n                assertThat(!bs.isSet(""name""));\r\n            } else {\r\n                assertThat(queryString).as(description).doesNotContain(""name"");\r\n            }\r\n        } else {\r\n            assertThat(queryString).as(description).contains(""name"");\r\n        }\r\n\r\n        if (nullPhone && !saveExpected) {\r\n            if (unsetSupported) {\r\n                assertThat(queryString).as(description).contains(""phone"");\r\n                assertThat(!bs.isSet(""phone""));\r\n            } else {\r\n                assertThat(queryString).as(description).doesNotContain(""phone"");\r\n            }\r\n        } else {\r\n            assertThat(queryString).as(description).contains(""phone"");\r\n        }\r\n```\r\n\r\nI concede that is a bit complex, but it\'ll make it so we can use the existing tests to validate both cases.', 'commenter': 'tolbertam'}]"
839,driver-mapping/src/main/java/com/datastax/driver/mapping/Mapper.java,"@@ -216,25 +217,26 @@ public Statement saveQuery(T entity, Option... options) {
     }
 
     private ListenableFuture<BoundStatement> saveQueryAsync(T entity, final EnumMap<Option.Type, Option> options) {
-        final Map<AliasedMappedProperty, Object> values = new HashMap<AliasedMappedProperty, Object>();
-        boolean saveNullFields = shouldSaveNullFields(options);
+        final Map<AliasedMappedProperty, Object> columnToValue = new HashMap<AliasedMappedProperty, Object>();
+        final boolean useUnsetForNullValue = !shouldSaveNullFields(options) && manager.protocolVersionAsInt >= 4;
+        final boolean includeColumnsWithNullValue = shouldSaveNullFields(options) || useUnsetForNullValue;","[{'comment': ""> It's not possible to use the modifiesQueryString method to without some serious rewriting/refactor. The reason being that when generating the prepared statement in QueryType.makePreparedQueryString you need the values for the columns so you can omit the columns with null values when generating the statement to prepare to support <ProtocolV4.\r\n\r\nIt seems like `modifiesQueryString` wasn't relevant anyways.  This is where the magic is happening, very clever!     ðŸ‘ "", 'commenter': 'tolbertam'}]"
841,manual/tuples/README.md,"@@ -1,5 +1,77 @@
-## Tuples
+## Using Tuples with the Java driver
 
-*Coming soon... In the meantime, see the javadoc for [TupleType].*
+Cassandra allows to use `tuple` data types [in tables](https://docs.datastax.com/en/cql/3.1/cql/cql_reference/tupleType.html):
+
+```
+CREATE TABLE ks.collect_things (
+  pk int,
+  ck1 text,
+  ck2 text,
+  v tuple<int, text, float>,
+  PRIMARY KEY (pk, ck1, ck2)
+);
+```
+
+### Fetching Tuples from Rows results
+
+The DataStax Java driver exposes a special `TupleValue` class to handle such columns. `TupleValue` extends `GettableByIndexData` class, which allows to call specific `get...(int)` methods on a returned `TupleValue`:","[{'comment': 'Would be good to link to `TupleValue` & `GettableByIndexData` javadoc on the first reference to them.', 'commenter': 'tolbertam'}]"
841,manual/tuples/README.md,"@@ -1,5 +1,77 @@
-## Tuples
+## Using Tuples with the Java driver
 
-*Coming soon... In the meantime, see the javadoc for [TupleType].*
+Cassandra allows to use `tuple` data types [in tables](https://docs.datastax.com/en/cql/3.1/cql/cql_reference/tupleType.html):","[{'comment': 'They are also allowed in user types.', 'commenter': 'adutra'}]"
841,manual/tuples/README.md,"@@ -1,5 +1,77 @@
-## Tuples
+## Using Tuples with the Java driver
 
-*Coming soon... In the meantime, see the javadoc for [TupleType].*
+Cassandra allows to use `tuple` data types [in tables](https://docs.datastax.com/en/cql/3.1/cql/cql_reference/tupleType.html):
+
+```
+CREATE TABLE ks.collect_things (
+  pk int,
+  ck1 text,
+  ck2 text,
+  v tuple<int, text, float>,
+  PRIMARY KEY (pk, ck1, ck2)
+);
+```
+
+### Fetching Tuples from Rows results
+
+The DataStax Java driver exposes a special `TupleValue` class to handle such columns. `TupleValue` extends `GettableByIndexData` class, which allows to call specific `get...(int)` methods on a returned `TupleValue`:
+
+```java
+Row row = session.execute(""SELECT v FROM ks.collect_things WHERE pk = 1"").one();
+
+TupleValue tupleValue = row.getTupleValue(""v"");
+
+int firstValueInTuple = tupleValue.getInt(0);
+
+String secondValueInTuple = tupleValue.getString(1);
+
+Float thirdValueInTuple = tupleValue.getFloat(2);
+```
+
+### Using tuples as statement parameters
+
+A prepared statement may be containing a Tuple as a query parameter. In such case, users will need to create or gather a `TupleType` first, in order to be able to create a `TupleValue` to bind:","[{'comment': 'Nit: ""may contain"" ... ""In such cases,...""', 'commenter': 'adutra'}]"
841,manual/tuples/README.md,"@@ -1,5 +1,77 @@
-## Tuples
+## Using Tuples with the Java driver
 
-*Coming soon... In the meantime, see the javadoc for [TupleType].*
+Cassandra allows to use `tuple` data types [in tables](https://docs.datastax.com/en/cql/3.1/cql/cql_reference/tupleType.html):
+
+```
+CREATE TABLE ks.collect_things (
+  pk int,
+  ck1 text,
+  ck2 text,
+  v tuple<int, text, float>,
+  PRIMARY KEY (pk, ck1, ck2)
+);
+```
+
+### Fetching Tuples from Rows results
+
+The DataStax Java driver exposes a special `TupleValue` class to handle such columns. `TupleValue` extends `GettableByIndexData` class, which allows to call specific `get...(int)` methods on a returned `TupleValue`:
+
+```java
+Row row = session.execute(""SELECT v FROM ks.collect_things WHERE pk = 1"").one();
+
+TupleValue tupleValue = row.getTupleValue(""v"");
+
+int firstValueInTuple = tupleValue.getInt(0);
+
+String secondValueInTuple = tupleValue.getString(1);
+
+Float thirdValueInTuple = tupleValue.getFloat(2);
+```
+
+### Using tuples as statement parameters
+
+A prepared statement may be containing a Tuple as a query parameter. In such case, users will need to create or gather a `TupleType` first, in order to be able to create a `TupleValue` to bind:
+
+```java
+PreparedStatement ps = session.prepare(""INSERT INTO ks.collect_things (pk, ck1, ck2, v) VALUES (:pk, :ck1, :ck2, :v)"");
+
+TupleType tupleType = cluster.getMetadata().newTupleType(DataType.cint(), DataType.text(), DataType.cfloat());
+
+BoundStatement bs = ps.bind();
+bs.setInt(""pk"", 1);
+bs.setString(""ck1"", ""1"");
+bs.setString(""ck2"", ""1"");
+bs.setTupleValue(""v"", tupleType.newValue(1, ""hello"", 2.3f));
+
+session.execute(bs);
+```
+
+#### More use cases
+
+Users can also define single-usage tuples in _SELECT_ queries with the `IN` keyword, usually for tables with composite clustering keys, in that case a tuple will be usable the same way it was for prepared statements parameters:","[{'comment': 'This is called a ""multi-column IN restriction"" â€“ maybe it\'s worth mentioning this term somewhere.', 'commenter': 'adutra'}]"
841,manual/tuples/README.md,"@@ -1,5 +1,87 @@
-## Tuples
+## Using Tuples with the Java driver
 
-*Coming soon... In the meantime, see the javadoc for [TupleType].*
+Cassandra allows to use `tuple` data types [in tables and user-defined types](https://docs.datastax.com/en/cql/3.1/cql/cql_reference/tupleType.html):
 
-[TupleType]: http://docs.datastax.com/en/drivers/java/3.0/com/datastax/driver/core/TupleType.html
\ No newline at end of file
+```
+CREATE TABLE ks.collect_things (
+  pk int,
+  ck1 text,
+  ck2 text,
+  v tuple<int, text, float>,
+  PRIMARY KEY (pk, ck1, ck2)
+);
+```
+
+### Fetching Tuples from Rows results
+
+The DataStax Java driver exposes a special [TupleValue] class to handle such columns. 
+[TupleValue] extends [GettableByIndexData] class, which allows to call specific `get...(int)` ","[{'comment': ""nit: GettableByIndexData is an implementation detail, not sure if it's relevant to mention it here. All the user needs to know is that there are getter methods."", 'commenter': 'olim7t'}]"
841,manual/tuples/README.md,"@@ -1,5 +1,87 @@
-## Tuples
+## Using Tuples with the Java driver
 
-*Coming soon... In the meantime, see the javadoc for [TupleType].*
+Cassandra allows to use `tuple` data types [in tables and user-defined types](https://docs.datastax.com/en/cql/3.1/cql/cql_reference/tupleType.html):
 
-[TupleType]: http://docs.datastax.com/en/drivers/java/3.0/com/datastax/driver/core/TupleType.html
\ No newline at end of file
+```
+CREATE TABLE ks.collect_things (
+  pk int,
+  ck1 text,
+  ck2 text,
+  v tuple<int, text, float>,
+  PRIMARY KEY (pk, ck1, ck2)
+);
+```
+
+### Fetching Tuples from Rows results
+
+The DataStax Java driver exposes a special [TupleValue] class to handle such columns. 
+[TupleValue] extends [GettableByIndexData] class, which allows to call specific `get...(int)` 
+methods on a returned [TupleValue]:
+
+```java
+Row row = session.execute(""SELECT v FROM ks.collect_things WHERE pk = 1"").one();
+
+TupleValue tupleValue = row.getTupleValue(""v"");
+
+int firstValueInTuple = tupleValue.getInt(0);
+
+String secondValueInTuple = tupleValue.getString(1);
+
+Float thirdValueInTuple = tupleValue.getFloat(2);
+```
+
+### Using tuples as statement parameters
+
+A prepared statement may contain a Tuple as a query parameter. In such cases, users 
+will need to create or gather a [TupleType] first, in order to be able to create a [TupleValue] 
+to bind:
+
+```java
+PreparedStatement ps = session.prepare(""INSERT INTO ks.collect_things (pk, ck1, ck2, v) VALUES (:pk, :ck1, :ck2, :v)"");
+
+TupleType tupleType = cluster.getMetadata().newTupleType(DataType.cint(), DataType.text(), DataType.cfloat());
+
+BoundStatement bs = ps.bind();
+bs.setInt(""pk"", 1);
+bs.setString(""ck1"", ""1"");
+bs.setString(""ck2"", ""1"");
+bs.setTupleValue(""v"", tupleType.newValue(1, ""hello"", 2.3f));","[{'comment': 'Would be useful to mention that `newValue(Object...)` follows the same rules as `new SimpleStatement(String, Object...)`, i.e. type inference with some ambiguities (like numeric literals always interpreted as `int`).\r\nAnother way to create a tuple value -- and the workaround to solve those ambiguities -- is the parameterless `newValue()` followed by setter calls.', 'commenter': 'olim7t'}]"
842,driver-core/src/main/java/com/datastax/driver/core/ExecutionInfo.java,"@@ -84,9 +85,9 @@ ExecutionInfo withIncomingPayload(Map<String, ByteBuffer> incomingPayload) {
      * <li>if a host is tried by the driver but is dead or in
      * error, that host is recorded and the query is retried;</li>
      * <li>on a timeout or unavailable exception, some
-     * {@link com.datastax.driver.core.policies.RetryPolicy} may retry the","[{'comment': 'Nit: we usually do not add an import for a class that is only referenced through javadocs, to avoid creating a compile-time dependency between classes that are otherwise unrelated.', 'commenter': 'adutra'}, {'comment': ""I've removed this.\r\n"", 'commenter': 'GregBestland'}]"
842,driver-core/src/main/java/com/datastax/driver/core/ExecutionInfo.java,"@@ -112,8 +113,24 @@ public Host getQueriedHost() {
     }
 
     /**
+     * The speculative execution that completed this query.
+     * <p>
+     * 1 represents the initial, regular execution of the query, 2 represents the first speculative
+     * execution, etc.
+     * <p>
+     * Note that this is different from the number of <em>started</em> executions. For example, if
+     * one speculative execution was triggered, but the initial execution eventually completed
+     * first, this will be 1.","[{'comment': ""Wouldn't users wish they could have a more complete vision? I.e know exactly how many spec execs were triggered, and which one completed and which ones were canceled)?"", 'commenter': 'adutra'}, {'comment': ""It's more complicated... The dilemma with ExecutionInfo is that it's updated concurrently, so it boils down to how much info we want to provide vs the overhead of tracking this info."", 'commenter': 'olim7t'}, {'comment': ""I agree it would be nice to add the number some context onto at least which hosts were queried with speculative. I also understand why you avoided making this change as it's not trivial.  Mostly I think it's confusing to the user.\r\n\r\nI think we need to be more explicit about the information we are returning on the ExecutionInfo. Without knowing how we handle speculative execution, I can see users being confused as to why speculative executions executed on hosts aren't listed in the triedhosts list for instance. Perhaps we could provide seperate specific speculative execution related information with regards to things like tried hosts. \r\n\r\n"", 'commenter': 'GregBestland'}, {'comment': 'Tracking the total number of executions sounds reasonable and is not too hard to do, adding it.', 'commenter': 'olim7t'}]"
842,driver-core/src/main/java/com/datastax/driver/core/RequestHandler.java,"@@ -111,7 +111,7 @@ private void startNewExecution() {
             return;
 
         Message.Request request = callback.request();
-        int position = executionCount.incrementAndGet();
+        int position = executionIndex.getAndIncrement();","[{'comment': ""Changed this because it's more consistent with the way things are exposed in ExecutionInfo.\r\nThe initial execution will now have a position of 0 instead of 1. This was only exposed in debug logs so I don't think it's an issue."", 'commenter': 'olim7t'}]"
842,driver-core/src/main/java/com/datastax/driver/core/ExecutionInfo.java,"@@ -112,6 +116,33 @@ public Host getQueriedHost() {
     }","[{'comment': 'The javadocs for `getQueriedHost()` should be amended: `... This is a shortcut for {@code getTriedHosts().get(getTriedHosts().size() - 1)}.`\r\n', 'commenter': 'adutra'}]"
842,driver-core/src/main/java/com/datastax/driver/core/ExecutionInfo.java,"@@ -87,7 +88,10 @@ ExecutionInfo withIncomingPayload(Map<String, ByteBuffer> incomingPayload) {
      * {@link com.datastax.driver.core.policies.RetryPolicy} may retry the
      * query on the same host, so the same host might appear twice.</li>
      * <li>if {@link com.datastax.driver.core.policies.SpeculativeExecutionPolicy speculative executions}
-     * are enabled, other hosts might have been tried speculatively as well.</li>
+     * are enabled, this will also contain hosts that were tried by other executions (however, note that
+     * this only contains hosts for which a response was received; if an execution is waiting for a response","[{'comment': 'This is misleading: there is only ever at most one response received (either a result or an error); other tried hosts, if any, can only be listed here if they timed out with no response at all. I suggest: ""this only contains the queried host â€“ i.e. the host who replied with the final response â€“ and hosts that failed to reply before the request timed out; if an execution is waiting...""', 'commenter': 'adutra'}, {'comment': 'Besides, the queried host, according to `getQueriedHost()`, is always the last one. We could stress this fact as well.', 'commenter': 'adutra'}, {'comment': 'There can also be hosts that replied with an error response for which the retry policy decided to retry.', 'commenter': 'olim7t'}]"
842,driver-core/src/main/java/com/datastax/driver/core/ExecutionInfo.java,"@@ -87,7 +88,10 @@ ExecutionInfo withIncomingPayload(Map<String, ByteBuffer> incomingPayload) {
      * {@link com.datastax.driver.core.policies.RetryPolicy} may retry the
      * query on the same host, so the same host might appear twice.</li>","[{'comment': ""Just as a reminder, we might want to change the return type in version 4.0 to `Set<Host>`, because it doesn't really make sense to return a `List` since we introduced retries on the same host. Or better yet, some sort of `Map<SpeculativeExecution, Host>`. And even better yet, a custom structure `RequestTimeline` that would record all the relevant events that lead from the request submission to request completion (e.g. at instant T0 request was submitted, at instant T1 spec exec 1 was fired, at instant T2 normal exec timed out, etc...) â€“ in other words, some sort of client-side query trace."", 'commenter': 'adutra'}, {'comment': ""> RequestTimeline\r\n\r\nThat would be **great** it's kind of what I was imagining when opening the ticket"", 'commenter': 'newkek'}, {'comment': ""However we'll need to consider the cost of this"", 'commenter': 'newkek'}, {'comment': 'Yes, consider the cost and compare it to the value for users. The question is, what percentage of users are interested in a detailed timeline of executions and retries, and how would they use it in their application?\r\nFor debugging => maybe trace logs are good enough\r\nTo compute metrics => we can expose those metrics ourselves, in a less invasive way (make them optional)', 'commenter': 'olim7t'}]"
858,driver-core/src/main/java/com/datastax/driver/core/Metrics.java,"@@ -219,6 +229,15 @@ public Errors getErrorMetrics() {
     }
 
     /**
+     * Returns the total number of in flight requests to Cassandra hosts.
+     *
+     * @return The total number of in flight requests to Cassandra hosts.
+     */
+    public Gauge<Integer> getInFlightRequests() {","[{'comment': ""Isn't this a bit too coarse-grained? I wonder if it would be better to return a `MetricSet` instead, and return the number of inflight requests on a per-host basis. This would allow users to detect slow hosts (and besides, this is exactly what has been suggested recently as the best available metric to measure host performance â€“ much better than request latencies for instance)."", 'commenter': 'adutra'}, {'comment': ""It's a good suggestion that several folks actually prefer as well on our side here! Let me update the PR accordingly."", 'commenter': 'mfiguiere'}, {'comment': ""+1, from my experience most metrics would be more useful if they were exposed per host, I'll keep this in mind for the next major revision."", 'commenter': 'olim7t'}]"
858,driver-core/src/main/java/com/datastax/driver/core/Metrics.java,"@@ -79,6 +79,16 @@ public Integer getValue() {
             return value;
         }
     });
+    private final Gauge<Integer> inFlightRequests = registry.register(""inflight-requests"", new Gauge<Integer>() {
+        @Override
+        public Integer getValue() {
+            int value = 0;
+            for (SessionManager session : manager.sessions)
+                for (HostConnectionPool pool : session.pools.values())
+                    value += pool.totalInFlight.get();
+            return value;
+        }
+    });","[{'comment': 'lgtm.\r\nMy only remark is that the sum is not done atomically, but there would be no reasonable way to do it, and this should provide a good enough approximation.', 'commenter': 'olim7t'}]"
863,driver-core/src/main/java/com/datastax/driver/core/DirectedGraph.java,"@@ -65,16 +67,21 @@ void addEdge(V from, V to) {
 
         Queue<V> queue = new LinkedList<V>();
 
-        for (Map.Entry<V, Integer> entry : vertices.entrySet()) {
-            if (entry.getValue() == 0)
-                queue.add(entry.getKey());
+        // Sort vertices so order of evaluation is always the same (instead of depending on undefined map order behavior)
+        List<V> orderedVertices = new ArrayList<V>(vertices.keySet());
+        Collections.sort(orderedVertices, comparator);","[{'comment': 'Not too concerned about performance as number of UDTs should be rather small, and we only sort at top level and within adjacency lists.', 'commenter': 'tolbertam'}]"
863,driver-core/src/main/java/com/datastax/driver/core/SchemaParser.java,"@@ -663,6 +663,24 @@ else if (targetType == AGGREGATE)
             return whereClause;
         }
 
+        // Used by maybeSortUdts to sort at each dependency group alphabetically.
+        private static final Comparator<Row> sortByTypeName = new Comparator<Row>() {
+            @Override
+            public int compare(Row o1, Row o2) {
+                String type1 = o1.getString(UserType.TYPE_NAME);
+                String type2 = o2.getString(UserType.TYPE_NAME);
+
+                if (type1 == null && type2 == null) {","[{'comment': 'Technically these should never be null, could remove check if you think it is being overcautious', 'commenter': 'tolbertam'}]"
863,driver-core/src/main/java/com/datastax/driver/core/DirectedGraph.java,"@@ -33,8 +33,10 @@
     final Map<V, Integer> vertices;
     final Multimap<V, V> adjacencyList;
     boolean wasSorted;
+    final Comparator<V> comparator;
 
-    DirectedGraph(List<V> vertices) {
+    DirectedGraph(Comparator<V> comparator, List<V> vertices) {","[{'comment': ""I considered having the `Comparator` be optional, but since this is a internal class that is only used for this one purpose, figured i'd leave it as is."", 'commenter': 'tolbertam'}]"
863,driver-core/src/test/java/com/datastax/driver/core/DirectedGraphTest.java,"@@ -58,24 +68,80 @@ public void should_sort_complex_graph() {
         g.addEdge(""D"", ""B"");
         g.addEdge(""B"", ""A"");
 
-        // Topological sort order should be : GH,FE,D,CB,A
+        // Topological sort order should be : GH,E,F,D,BC,A
         // There's no guarantee on the order within the same level, so we use sublists:
         List<String> sorted = g.topologicalSort();
+        System.out.println(sorted);
+        assertThat(sorted.subList(0, 2))
+                .containsExactly(""G"", ""H"");
+        assertThat(sorted.subList(2, 3))
+                .containsExactly(""E"");
+        assertThat(sorted.subList(3, 4))
+                .containsExactly(""F"");
+        assertThat(sorted.subList(4, 5))
+                .containsExactly(""D"");
+        assertThat(sorted.subList(5, 7))
+                .containsExactly(""B"", ""C"");
+        assertThat(sorted.subList(7, 8))
+                .containsExactly(""A"");
+    }
+
+    @Test(groups = ""unit"")
+    public void should_sort_complex_custom_comparator() {
+        // Version of should_sort_complex_graph using a custom comparator based on ordering largest values first.
+        // This is counter to how hashmaps should usually behave, so this should help ensure that the comparator is
+        // being used.
+        Comparator<Integer> highFirst = new Comparator<Integer>() {
+            @Override
+            public int compare(Integer o1, Integer o2) {
+                return o2 - o1;
+            }
+        };
+
+        // sort graph and use a alphaComparator that favors larger values ordered first.
+        //         7   6
+        //        / \ /\
+        //       5   | 10
+        //        \ /  /
+        //         9  /
+        //        / \/
+        //        1  2
+        //        |
+        //        0
+        DirectedGraph<Integer> g = new DirectedGraph<Integer>(highFirst, 0, 1, 2, 9, 10, 5, 6, 7);
+        g.addEdge(7, 5);
+        g.addEdge(6, 10);
+        g.addEdge(7, 9);
+        g.addEdge(5, 9);
+        g.addEdge(6, 9);
+        g.addEdge(9, 2);
+        g.addEdge(10, 2);
+        g.addEdge(9, 1);
+        g.addEdge(1, 0);
+
+        // Topological sort order should be : [7,6],[5],[10],[2,1],[0]
+        // There's no guarantee on the order within the same level, so we use sublists:
+        List<Integer> sorted = g.topologicalSort();
+        System.out.println(sorted);
         assertThat(sorted.subList(0, 2))
-                .contains(""G"", ""H"");
-        assertThat(sorted.subList(2, 4))
-                .contains(""F"", ""E"");
+                .containsExactly(7, 6);
+        // 5 comes before 10 even though they appear at the same depth.  This happens because 5's (7) dependency","[{'comment': ""Note this detail, since 7's dependents (5,9) are evaluated first, 5 comes before 10, even though they are the same depth.   I don't expect this to bother anyone, but I thought I'd expound on that detail."", 'commenter': 'tolbertam'}]"
863,driver-core/src/test/java/com/datastax/driver/core/DirectedGraphTest.java,"@@ -58,24 +68,80 @@ public void should_sort_complex_graph() {
         g.addEdge(""D"", ""B"");
         g.addEdge(""B"", ""A"");
 
-        // Topological sort order should be : GH,FE,D,CB,A
+        // Topological sort order should be : GH,E,F,D,BC,A
         // There's no guarantee on the order within the same level, so we use sublists:
         List<String> sorted = g.topologicalSort();
+        System.out.println(sorted);
+        assertThat(sorted.subList(0, 2))
+                .containsExactly(""G"", ""H"");
+        assertThat(sorted.subList(2, 3))
+                .containsExactly(""E"");
+        assertThat(sorted.subList(3, 4))
+                .containsExactly(""F"");
+        assertThat(sorted.subList(4, 5))
+                .containsExactly(""D"");
+        assertThat(sorted.subList(5, 7))
+                .containsExactly(""B"", ""C"");
+        assertThat(sorted.subList(7, 8))
+                .containsExactly(""A"");
+    }
+
+    @Test(groups = ""unit"")
+    public void should_sort_complex_custom_comparator() {
+        // Version of should_sort_complex_graph using a custom comparator based on ordering largest values first.
+        // This is counter to how hashmaps should usually behave, so this should help ensure that the comparator is
+        // being used.
+        Comparator<Integer> highFirst = new Comparator<Integer>() {
+            @Override
+            public int compare(Integer o1, Integer o2) {
+                return o2 - o1;
+            }
+        };
+
+        // sort graph and use a alphaComparator that favors larger values ordered first.
+        //         7   6
+        //        / \ /\
+        //       5   | 10
+        //        \ /  /
+        //         9  /
+        //        / \/
+        //        1  2
+        //        |
+        //        0
+        DirectedGraph<Integer> g = new DirectedGraph<Integer>(highFirst, 0, 1, 2, 9, 10, 5, 6, 7);
+        g.addEdge(7, 5);
+        g.addEdge(6, 10);
+        g.addEdge(7, 9);
+        g.addEdge(5, 9);
+        g.addEdge(6, 9);
+        g.addEdge(9, 2);
+        g.addEdge(10, 2);
+        g.addEdge(9, 1);
+        g.addEdge(1, 0);
+
+        // Topological sort order should be : [7,6],[5],[10],[2,1],[0]
+        // There's no guarantee on the order within the same level, so we use sublists:
+        List<Integer> sorted = g.topologicalSort();
+        System.out.println(sorted);","[{'comment': 'Need to remove these', 'commenter': 'tolbertam'}]"
863,driver-core/src/main/java/com/datastax/driver/core/KeyspaceMetadata.java,"@@ -262,21 +286,61 @@ public String exportAsString() {
 
         sb.append(asCQLQuery()).append('\n');
 
-        for (UserType udt : userTypes.values())
+        // include types, tables, views, functions and aggregates, each ordered by name, with one small exception
+        // being that user types are ordered topologically and then by name within same level.
+        for (UserType udt : getSortedUserTypes())
             sb.append('\n').append(udt.exportAsString()).append('\n');
 
-        for (TableMetadata tm : tables.values())
+        for (AbstractTableMetadata tm : ImmutableSortedSet.orderedBy(AbstractTableMetadata.byNameComparator).addAll(tables.values()).build())","[{'comment': ""ImmutableSortedSet is not necessary, but it's nice because it makes it easy to make a copy of a collection and sort it."", 'commenter': 'tolbertam'}]"
863,driver-core/src/main/java/com/datastax/driver/core/KeyspaceMetadata.java,"@@ -262,21 +286,61 @@ public String exportAsString() {
 
         sb.append(asCQLQuery()).append('\n');
 
-        for (UserType udt : userTypes.values())
+        // include types, tables, views, functions and aggregates, each ordered by name, with one small exception
+        // being that user types are ordered topologically and then by name within same level.
+        for (UserType udt : getSortedUserTypes())
             sb.append('\n').append(udt.exportAsString()).append('\n');
 
-        for (TableMetadata tm : tables.values())
+        for (AbstractTableMetadata tm : ImmutableSortedSet.orderedBy(AbstractTableMetadata.byNameComparator).addAll(tables.values()).build())
             sb.append('\n').append(tm.exportAsString()).append('\n');
 
-        for (FunctionMetadata fm : functions.values())
+        for (FunctionMetadata fm : ImmutableSortedSet.orderedBy(functionByName).addAll(functions.values()).build())
             sb.append('\n').append(fm.exportAsString()).append('\n');
 
-        for (AggregateMetadata am : aggregates.values())
+        for (AggregateMetadata am : ImmutableSortedSet.orderedBy(aggregateByName).addAll(aggregates.values()).build())
             sb.append('\n').append(am.exportAsString()).append('\n');
 
         return sb.toString();
     }
 
+    private List<UserType> getSortedUserTypes() {
+        // rebuilds dependency tree of user types so they may be sorted within each dependency level.
+        List<UserType> unsortedTypes = new ArrayList<UserType>(userTypes.values());
+        DirectedGraph<UserType> graph = new DirectedGraph<UserType>(typeByName, unsortedTypes);
+        for (UserType from : unsortedTypes) {
+            for (UserType to : unsortedTypes) {
+                if (from != to && dependsOn(to, from))
+                    graph.addEdge(from, to);
+            }
+        }
+        return graph.topologicalSort();
+    }
+
+    private boolean dependsOn(UserType udt1, UserType udt2) {","[{'comment': ""It is kind of annoying that we have to duplicate this code from `V3SchemaParser` but it's necessary because here we are working on the UDT's directly, where in the other code we're working on the Row."", 'commenter': 'tolbertam'}]"
863,driver-core/src/main/java/com/datastax/driver/core/TableMetadata.java,"@@ -384,12 +385,30 @@ public String exportAsString() {
 
         sb.append(super.exportAsString());
 
-        for (IndexMetadata index : indexes.values()) {
-            sb.append('\n').append(index.asCQLQuery());
+        if (!indexes.isEmpty()) {
+            sb.append('\n');","[{'comment': ""We have to be stingy with newlines here because `KeyspaceMetadata` adds one after the tables total metadata, so we don't want duplicate newlines."", 'commenter': 'tolbertam'}]"
863,driver-core/src/main/java/com/datastax/driver/core/AbstractTableMetadata.java,"@@ -240,7 +248,7 @@ protected StringBuilder appendOptions(StringBuilder sb, boolean formatted) {
         sb.append("" WITH "");
         if (options.isCompactStorage())
             and(sb.append(""COMPACT STORAGE""), formatted);
-        if (!Iterables.all(clusteringOrder, isAscending))
+        if (!clusteringOrder.isEmpty())","[{'comment': ""This change isn't necessary, but I noticed this is what the python driver does, and we are already pretty aggressive about including options which the user wouldn't (usually) specify themselves."", 'commenter': 'tolbertam'}]"
863,driver-core/src/test/java/com/datastax/driver/core/ExportAsStringTest2x.java,"@@ -0,0 +1,108 @@
+/*
+ * Copyright (C) 2012-2017 DataStax Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.driver.core;
+
+import com.datastax.driver.core.schemabuilder.SchemaBuilder;
+import com.datastax.driver.core.utils.CassandraVersion;
+import com.google.common.collect.ImmutableMap;
+import java.util.Map;
+import org.testng.SkipException;
+import org.testng.annotations.Test;
+
+import static com.datastax.driver.core.ExportAsStringTest.getExpectedCqlString;
+import static org.assertj.core.api.Assertions.assertThat;
+
+@CassandraVersion(""2.0"")
+public class ExportAsStringTest2x extends CCMTestsSupport {
+
+    /**
+     * A version of {@link ExportAsStringTest} for C* 2.0 and 2.1 clusters.
+     */
+    @Test(groups = ""short"")
+    public void create_schema_and_ensure_exported_cql_is_expected() {","[{'comment': 'This is a duplicate of `ExportAsStringTest`, but for 2.0 and 2.1 only.  Separated them out before `ExportAsStringTest` requires UDFs to be enabled, but i just realized we already have selective logic to remove that configuration for older versions..will unify.', 'commenter': 'tolbertam'}]"
863,driver-core/src/main/java/com/datastax/driver/core/FunctionMetadata.java,"@@ -174,11 +174,9 @@ private String asCQLQuery(boolean formatted) {
                 first = false;
             else
                 sb.append(',');
-            TableMetadata.newLine(sb, formatted);","[{'comment': 'cqlsh puts all arguments on the same line as `CREATE FUNCTION`, ', 'commenter': 'tolbertam'}]"
863,driver-core/src/main/java/com/datastax/driver/core/FunctionMetadata.java,"@@ -190,7 +188,7 @@ private String asCQLQuery(boolean formatted) {
 
         TableMetadata.spaceOrNewLine(sb, formatted)
                 .append(""RETURNS "")
-                .append(returnType);
+                .append(returnType.asFunctionParameterString());","[{'comment': ""Needed to remove `frozen` from return type (which isn't valid)"", 'commenter': 'tolbertam'}]"
863,driver-core/src/test/java/com/datastax/driver/core/DirectedGraphTest.java,"@@ -58,24 +68,78 @@ public void should_sort_complex_graph() {
         g.addEdge(""D"", ""B"");
         g.addEdge(""B"", ""A"");
 
-        // Topological sort order should be : GH,FE,D,CB,A
+        // Topological sort order should be : GH,E,F,D,BC,A
         // There's no guarantee on the order within the same level, so we use sublists:","[{'comment': 'Is that still relevant?', 'commenter': 'olim7t'}, {'comment': 'You are right, there *is* a guarantee that it is ordered the same way.   I should update the test to no longer check on sublists and validate it as a whole, will do that.', 'commenter': 'tolbertam'}]"
863,driver-core/src/main/java/com/datastax/driver/core/KeyspaceMetadata.java,"@@ -262,21 +286,61 @@ public String exportAsString() {
 
         sb.append(asCQLQuery()).append('\n');
 
-        for (UserType udt : userTypes.values())
+        // include types, tables, views, functions and aggregates, each ordered by name, with one small exception
+        // being that user types are ordered topologically and then by name within same level.
+        for (UserType udt : getSortedUserTypes())","[{'comment': ""If we re-sort when exporting as CQL, do we still need the changes from the first commit? Having a consistent order matters mostly for `exportAsString`, for a programmatic traversal I don't think it's a big deal if it differs across VMs."", 'commenter': 'olim7t'}, {'comment': ""I was unsure if maintaining the topographical order in `KeyspaceMetadata.userTypes` was important or not.  If it's not it would simplify some code / remove some semi-duplication, if agree I'll remove it."", 'commenter': 'tolbertam'}, {'comment': ""There already isn't a consistent ordering in the collection itself depending on the order of UDT creation (if created within the same Cluster instance), but they are always ordered topologically (just not consistently).  I think it is not important for it to be sorted at that time, but only when exporting it to cql.  I'll add a separate commit that backs that out, but we can always remove that commit if we want to keep it.   I think the order isn't significant (nor is it documented) for the actual collection members, but it is significant when exporting as a cql string because if it's not ordered correctly it's not valid."", 'commenter': 'tolbertam'}, {'comment': 'Actually it does appear to be relevant.   When connecting and building metadata, if a UDT depends on another UDT, it needs to be built in order, otherwise:\r\n\r\n```\r\ncom.datastax.driver.core.exceptions.UnresolvedUserTypeException: Cannot resolve user type unresolved_user_type_test.""C""\r\n\tat com.datastax.driver.core.DataTypeCqlNameParser.parse(DataTypeCqlNameParser.java:149)\r\n\tat com.datastax.driver.core.DataTypeCqlNameParser.parse(DataTypeCqlNameParser.java:120)\r\n\tat com.datastax.driver.core.UserType.build(UserType.java:93)\r\n\tat com.datastax.driver.core.SchemaParser.buildUserTypes(SchemaParser.java:196)\r\n\tat com.datastax.driver.core.SchemaParser.buildKeyspaces(SchemaParser.java:127)\r\n\tat com.datastax.driver.core.SchemaParser.refresh(SchemaParser.java:64)\r\n\tat com.datastax.driver.core.ControlConnection.refreshSchema(ControlConnection.java:336)\r\n\tat com.datastax.driver.core.ControlConnection.tryConnect(ControlConnection.java:276)\r\n\tat com.datastax.driver.core.ControlConnection.reconnectInternal(ControlConnection.java:201)\r\n\tat com.datastax.driver.core.ControlConnection.connect(ControlConnection.java:79)\r\n\tat com.datastax.driver.core.Cluster$Manager.negotiateProtocolVersionAndConnect(Cluster.java:1600)\r\n\tat com.datastax.driver.core.Cluster$Manager.init(Cluster.java:1518)\r\n\tat com.datastax.driver.core.Cluster.getMetadata(Cluster.java:399)\r\n\tat com.datastax.driver.core.UnresolvedUserTypeTest.should_resolve_nested_user_types(UnresolvedUserTypeTest.java:114)\r\n```', 'commenter': 'tolbertam'}, {'comment': ""Going to leave that as is for now for the preceding comments reason, but let me know if you'd like me to adjust it.   "", 'commenter': 'tolbertam'}]"
863,driver-core/src/test/java/com/datastax/driver/core/UnresolvedUserTypeTest.java,"@@ -28,6 +28,42 @@
 @CassandraVersion(""3.0"")
 public class UnresolvedUserTypeTest extends CCMTestsSupport {
 
+    private static final String keyspace = ""unresolved_user_type_test"";","[{'comment': 'Those are constants, the names should be capitalized.', 'commenter': 'olim7t'}]"
863,driver-core/src/test/java/com/datastax/driver/core/UnresolvedUserTypeTest.java,"@@ -28,6 +28,42 @@
 @CassandraVersion(""3.0"")
 public class UnresolvedUserTypeTest extends CCMTestsSupport {
 
+    private static final String keyspace = ""unresolved_user_type_test"";
+
+    private static final String expectedSchema = ""CREATE KEYSPACE unresolved_user_type_test WITH REPLICATION = { 'class' : 'org.apache.cassandra.locator.SimpleStrategy', 'replication_factor': '1' } AND DURABLE_WRITES = true;\n"" +","[{'comment': 'Maybe reuse the keyspace name constant here for consistency.', 'commenter': 'olim7t'}]"
876,clirr-ignores.xml,"@@ -147,4 +147,19 @@
         <justification>False positive, the enclosing class is package-private so this was never exposed</justification>
     </difference>
 
+    <difference>
+        <differenceType>7004</differenceType>
+        <className>com/datastax/driver/core/AbstractSession</className>
+        <method>com.google.common.util.concurrent.ListenableFuture prepareAsync(java.lang.String, java.util.Map)</method>
+        <to>com.google.common.util.concurrent.ListenableFuture prepareAsync(java.lang.String, java.lang.String, java.util.Map)</to>
+        <justification>Protected method, only used internally</justification>
+    </difference>
+
+    <difference>","[{'comment': ""Figured this would be ok, but thought i'd point it out just in case."", 'commenter': 'tolbertam'}]"
876,driver-core/src/main/java/com/datastax/driver/core/Cluster.java,"@@ -2262,12 +2262,12 @@ private Connection prepareAllQueries(Host host, Connection reusedConnection) thr
                 // used for preparing it. However, since we are likely that all prepared query belong to only a handful
                 // of different keyspace (possibly only one), and to avoid setting the current keyspace more than needed,
                 // we first sort the query per keyspace.
-                SetMultimap<String, String> perKeyspace = HashMultimap.create();
+                SetMultimap<String, PreparedStatement> perKeyspace = HashMultimap.create();","[{'comment': ""Could also change this such that if a PreparedStatement has a keyspace set explicitly, prepare it with default session since it doesn't matter what the session keyspace state is in this case."", 'commenter': 'tolbertam'}]"
876,driver-core/src/main/java/com/datastax/driver/core/RegularStatement.java,"@@ -190,6 +189,47 @@ public boolean hasValues() {
     }
 
     /**
+     * Returns the keyspace this query operates on.
+     * <p/>
+     * Unless the keyspace has been explicitly set through {@link #setKeyspace},
+     * this method will return {@code null} to avoid having to parse the query
+     * string.
+     *
+     * @return the keyspace set through {@link #setKeyspace} if such keyspace was
+     * set, {@code null} otherwise.
+     * @see Statement#getKeyspace
+     */
+    @Override
+    public String getKeyspace() {","[{'comment': ""Added default implementation to avoid clirr errors, we could also consider simply defining keyspace in `RegularStatement` since I think most implementations do something similar and we can handle the few cases that don't."", 'commenter': 'tolbertam'}]"
876,driver-core/src/main/java/com/datastax/driver/core/Requests.java,"@@ -240,9 +241,13 @@ public String toString() {
 
         static void serialize(EnumSet<QueryFlag> flags, ByteBuf dest, ProtocolVersion version) {
             int i = 0;
-            for (QueryFlag flag : flags)
-                i |= 1 << flag.ordinal();
-            if (version.compareTo(ProtocolVersion.V5) >= 0) {
+            boolean isV5 = version.compareTo(ProtocolVersion.V5) >= 0;
+            for (QueryFlag flag : flags) {
+                // don't include flag if would make value width greater than int and protocol < 5
+                if (flag.ordinal() < QueryFlag.KEYSPACE.ordinal() || isV5)","[{'comment': ""One oddity, if you set the keyspace flag and using protocol v4, C* 4.0+ will fail since it will still attempt to parse the keyspace.   Because of this figured i'd add a check here to not add the flag if protocol is less than v5."", 'commenter': 'tolbertam'}]"
876,driver-core/src/main/java/com/datastax/driver/core/Requests.java,"@@ -569,27 +591,42 @@ public void encode(Prepare msg, ByteBuf dest, ProtocolVersion version) {
                 CBUtil.writeLongString(msg.query, dest);
 
                 if (version.compareTo(ProtocolVersion.V5) >= 0) {
-                    // Write empty flags for now, to communicate that no keyspace is being set.
-                    dest.writeInt(0);
+                    // if keyspace is present write 0x1 for prepare flags.
+                    if (msg.keyspace != null) {
+                        dest.writeInt(0x01);
+                        CBUtil.writeString(msg.keyspace, dest);
+                    } else {
+                        dest.writeInt(0x00);
+                    }
                 }
             }
 
             @Override
             public int encodedSize(Prepare msg, ProtocolVersion version) {
-                return CBUtil.sizeOfLongString(msg.query);
+                int size = CBUtil.sizeOfLongString(msg.query);
+
+                if (version.compareTo(ProtocolVersion.V5) >= 0 && msg.flags.contains(QueryFlag.KEYSPACE))
+                    size += CBUtil.sizeOfString(msg.keyspace);
+                return size;
             }
         };
 
+        private final EnumSet<QueryFlag> flags = EnumSet.noneOf(QueryFlag.class);","[{'comment': ""Will get rid of this since it's unused.   Originally i used it, but later noticed that QueryFlag.KEYSPACE (0x80) != 0x1 which is what prepare uses for keyspace "", 'commenter': 'tolbertam'}]"
876,driver-core/src/main/java/com/datastax/driver/core/querybuilder/BuiltStatement.java,"@@ -77,7 +77,7 @@
 
     private final List<ColumnMetadata> partitionKey;
     private final List<Object> routingKeyValues;
-    final String keyspace;
+    String keyspace;","[{'comment': ""I wonder if we should consider differentiating between keyspace provided with `BuildStatement` (i.e. what comes from `insertInto(keyspace, table)` and otherwise in some way, as it is redundant to send the keyspace with the statement if it's already set in the query."", 'commenter': 'tolbertam'}, {'comment': 'Had separate discussion that we were ok with this.', 'commenter': 'tolbertam'}]"
876,driver-core/src/main/java/com/datastax/driver/core/Cluster.java,"@@ -2272,12 +2272,14 @@ private Connection prepareAllQueries(Host host, Connection reusedConnection) thr
 
                 for (String keyspace : perKeyspace.keySet()) {
                     // Empty string mean no particular keyspace to set
-                    if (!keyspace.isEmpty())
+                    // Optimization: Only change keyspace for older protocol versions as newer protocols allow
+                    // specifying keyspace on prepared statement.
+                    if (protocolVersion().compareTo(ProtocolVersion.V5) < 0 && !keyspace.isEmpty())
                         connection.setKeyspace(keyspace);
 
                     List<Connection.Future> futures = new ArrayList<Connection.Future>(preparedQueries.size());
                     for (String query : perKeyspace.get(keyspace)) {
-                        futures.add(connection.write(new Requests.Prepare(query)));
+                        futures.add(connection.write(new Requests.Prepare(query, keyspace)));","[{'comment': ""One thing here, since we can't distinguish whether the keyspace was established via the session keyspace or set on the statement explicitly we always pass it here.  I don't consider this that big of a deal due to the infrequency of prepared statement preparations."", 'commenter': 'tolbertam'}]"
876,.travis.yml,"@@ -1,7 +1,6 @@
 language: java
 jdk:
-- openjdk6
-- oraclejdk7
+- openjdk7","[{'comment': ""not related to pr, but new travis default image (ubuntu trusty) does not have openjdk6 and oraclejdk7 was replaced by openjdk7.   Since we have jenkins and appveyor builds running for java 6 i think it's ok to rely on them for that. "", 'commenter': 'tolbertam'}]"
876,driver-core/src/main/java/com/datastax/driver/core/RequestHandler.java,"@@ -637,7 +637,7 @@ public void onSet(Connection connection, Message.Response response, long latency
                                                 + ""Seeing this message a few times is fine, but seeing it a lot may be source of performance problems"",","[{'comment': ""Need to add special case at line 626 to allow case where keyspaces don't match if protocol version >= 5"", 'commenter': 'tolbertam'}]"
876,driver-core/src/main/java/com/datastax/driver/core/RegularStatement.java,"@@ -190,6 +190,44 @@ public boolean hasValues() {
     }
 
     /**
+     * Returns the keyspace this query operates on.
+     * <p/>
+     * Unless the keyspace has been explicitly set through {@link #setKeyspace},
+     * this method will return {@code null} to avoid having to parse the query
+     * string.
+     *
+     * @return the keyspace set through {@link #setKeyspace} if such keyspace was
+     * set, {@code null} otherwise.
+     * @see Statement#getKeyspace
+     */
+    @Override
+    public String getKeyspace() {
+        return null;
+    }
+
+    /**
+     * Sets the keyspace this query operates on.
+     * <p/>
+     * This method allows you to manually provide a keyspace for this query.  It is used for the following:
+     * <ol>
+     * <li>To indicate to cassandra what keyspace the statement is applicable to (protocol V5+ only).  This is useful
+     * when the query does not provide an explicit keyspace and you want to override the session's keyspace.</li>
+     * <li>By {@link com.datastax.driver.core.policies.TokenAwarePolicy}</li> to help identify which
+     * replicas are applicable to send this statement to.</li>
+     * </ol>
+     * Do note that if the query does not use a fully qualified keyspace, then
+     * you do not need to set the keyspace through this method as the
+     * currently logged in keyspace will be used if it is set.
+     *
+     * @param keyspace the name of the keyspace this query operates on.
+     * @return this {@code SimpleStatement} object.
+     * @see Statement#getKeyspace
+     */
+    public RegularStatement setKeyspace(String keyspace) {
+        throw new UnsupportedOperationException(""No concrete implementation of setKeyspace defined for "" + this.getClass().getName());","[{'comment': ""It sucks that we have to resort to that. I understand that the goal is to avoid a breaking API change, but we're only moving the issue from compile time to run time. On the one hand, it seems reasonable to assume that custom `RegularStatement` implementations should be rare in the field, on the other we could push down the method to each subclass, WDYT?"", 'commenter': 'olim7t'}, {'comment': 'i had kind of waffled back and forth on that.  I ultimately did this to avoid adding a clirr exception, but I do like the idea of making it abstract to force implementation.  Will change it.', 'commenter': 'tolbertam'}, {'comment': 'To be clear, I think we must avoid the Clirr exception. By ""push down"" I meant removing it entirely from the abstract class, not making it abstract.', 'commenter': 'olim7t'}, {'comment': ""I see what you are saying now, that makes complete sense to me.  `setKeyspace` is not really something used within the driver itself (afaict), so there's really no need to be part of `RegularStatement`.  Instead it is only used by client APIs, who have a more concrete implementation, like `BuiltStatement` for example."", 'commenter': 'tolbertam'}]"
876,driver-core/src/main/java/com/datastax/driver/core/SessionManager.java,"@@ -202,7 +202,12 @@ public Cluster getCluster() {
                         switch (rm.kind) {
                             case PREPARED:
                                 Responses.Result.Prepared pmsg = (Responses.Result.Prepared) rm;
-                                PreparedStatement stmt = DefaultPreparedStatement.fromMessage(pmsg, cluster, query, poolsState.keyspace);
+                                String keyspaceToUse = poolsState.keyspace;
+                                if (keyspace != null) {
+                                    // TODO: possibly check protocol version here and if not supported throw an exception.
+                                    keyspaceToUse = keyspace;","[{'comment': ""Yes, we should do that check. Otherwise we might end up storing a corrupted prepared statement if you prepare with an old protocol version and a routing keyspace that doesn't match the query string."", 'commenter': 'olim7t'}, {'comment': 'will do', 'commenter': 'tolbertam'}, {'comment': 'One thing about this is that for this scenario to happen, the statement has to be successfully prepared by the server, which means the table and query criteria etc have to match for this query for the target keyspace, but not necessarily completely, so I agree we should still have this check.', 'commenter': 'tolbertam'}, {'comment': 'Just had a thought, we could handle this better by checking the poolState before sending the request to the server to ensure the keyspaces match ahead of time.  I think we should still check here too just in case for whatever reason the pool state changes during the request (which is unlikely).', 'commenter': 'tolbertam'}, {'comment': 'we could do this in general (not just for prepared statements) so at least we get the error before the query is sent and with a more appropriate error.', 'commenter': 'tolbertam'}, {'comment': 'ðŸ‘  for the change.\r\n\r\nAs to:\r\n> we could handle this better by checking the poolState before sending the request to the server to ensure the keyspaces match ahead of time\r\n\r\nIt\'s not possible if the query string uses a qualified table name, since we don\'t parse it.\r\n```\r\nsession.execute(""USE ks1"");\r\n// session\'s keyspace does not match the statement\'s keyspace, yet the query is valid\r\nsession.execute(new SimpleStatement(""SELECT * FROM ks2.bar"").setKeyspace(ks2));\r\n```', 'commenter': 'olim7t'}]"
876,driver-core/src/test/java/com/datastax/driver/core/PreparedStatementMultiNodeTest.java,"@@ -0,0 +1,230 @@
+/*
+ * Copyright (C) 2012-2017 DataStax Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.driver.core;
+
+import com.datastax.driver.core.exceptions.DriverInternalError;
+import com.datastax.driver.core.utils.CassandraVersion;
+import org.testng.annotations.Test;
+
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.testng.Assert.fail;
+
+@CCMConfig(numberOfNodes = 2, dirtiesContext = true)
+@CreateCCM(CreateCCM.TestMode.PER_METHOD)
+public class PreparedStatementMultiNodeTest extends CCMTestsSupport {
+
+    private static final String keyspace2 = TestUtils.generateIdentifier(""ks_"");
+    private static final String keyspace3 = TestUtils.generateIdentifier(""ks_"");
+
+    @Override
+    public void onTestContextInitialized() {
+        execute(
+                String.format(TestUtils.CREATE_KEYSPACE_SIMPLE_FORMAT, keyspace2, 2),
+                String.format(""CREATE TABLE %s.users(id int, id2 int, name text, primary key (id, id2))"", keyspace2),
+                String.format(""INSERT INTO %s.users(id, id2, name) VALUES (2, 2, 'test2')"", keyspace2),
+                String.format(TestUtils.CREATE_KEYSPACE_SIMPLE_FORMAT, keyspace3, 2),
+                String.format(""CREATE TABLE %s.users(id int, id2 int, name text, primary key (id, id2))"", keyspace3),
+                String.format(""INSERT INTO %s.users(id, id2, name) VALUES (3, 3, 'test3')"", keyspace3)
+        );
+    }
+
+    @Test(groups = ""long"")
+    @CassandraVersion(""4.0.0"")
+    public void should_be_able_to_execute_statement_on_restarted_node_reprepare_on_up_set_keyspace() {
+        // validate that if a node is restarted we should be able to execute a bound statement against it
+        // as the statement is reprepared when the node comes back up.
+        // uses the Protocol V5 per-query keyspace strategy.
+        executeAfterNodeBroughtBackUp(true, true);
+    }
+
+    @Test(groups = ""long"")
+    @CassandraVersion(""4.0.0"")
+    public void should_be_able_to_execute_statement_on_restarted_node_set_keyspace() {
+        // validate that if a node is restarted we should be able to execute a bound statement against it
+        // as the statement is reprepared when we receive an 'unprepared' response.
+        // uses the Protocol V5 per-query keyspace strategy.
+        executeAfterNodeBroughtBackUp(false, true);
+    }
+
+    @Test(groups = ""long"")
+    public void should_be_able_to_execute_statement_on_restarted_node_reprepare_on_up_use_keyspace() {
+        // validate that if a node is restarted we should be able to execute a bound statement against it
+        // as the statement is reprepared when the node comes back up.
+        // uses the ""USE keyspace"" strategy which is an anti-pattern.
+        executeAfterNodeBroughtBackUp(true, false);
+    }
+
+    @Test(groups = ""long"")
+    public void should_be_able_to_execute_statement_on_restarted_node_use_keyspace() {
+        // validate that if a node is restarted we should be able to execute a bound statement against it
+        // as the statement is reprepared when we receive an 'unprepared' response.
+        // uses the Protocol V5 per-query keyspace strategy.
+        executeAfterNodeBroughtBackUp(false, false);
+    }
+
+    public void executeAfterNodeBroughtBackUp(boolean reprepareOnUp, boolean setKeyspace) {
+        QueryOptions queryOptions = new QueryOptions().setReprepareOnUp(reprepareOnUp).setPrepareOnAllHosts(false);
+        Cluster.Builder builder = createClusterBuilderNoDebouncing()
+                .withNettyOptions(TestUtils.nonQuietClusterCloseOptions)
+                .addContactPointsWithPorts(getContactPointsWithPorts())
+                .withPort(ccm().getBinaryPort())
+                .withQueryOptions(queryOptions)
+                .withLoadBalancingPolicy(new SortingLoadBalancingPolicy());
+
+        // TODO, remove this when V5 is no longer beta.","[{'comment': 'You can add an assertion so that we get a reminder:\r\n```\r\nassert ProtocolVersion.NEWEST_BETA == ProtocolVersion.V5\r\n```', 'commenter': 'olim7t'}]"
876,driver-core/src/test/java/com/datastax/driver/core/PreparedStatementMultiNodeTest.java,"@@ -0,0 +1,230 @@
+/*
+ * Copyright (C) 2012-2017 DataStax Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.driver.core;
+
+import com.datastax.driver.core.exceptions.DriverInternalError;
+import com.datastax.driver.core.utils.CassandraVersion;
+import org.testng.annotations.Test;
+
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.testng.Assert.fail;
+
+@CCMConfig(numberOfNodes = 2, dirtiesContext = true)
+@CreateCCM(CreateCCM.TestMode.PER_METHOD)
+public class PreparedStatementMultiNodeTest extends CCMTestsSupport {
+
+    private static final String keyspace2 = TestUtils.generateIdentifier(""ks_"");
+    private static final String keyspace3 = TestUtils.generateIdentifier(""ks_"");
+
+    @Override
+    public void onTestContextInitialized() {
+        execute(
+                String.format(TestUtils.CREATE_KEYSPACE_SIMPLE_FORMAT, keyspace2, 2),
+                String.format(""CREATE TABLE %s.users(id int, id2 int, name text, primary key (id, id2))"", keyspace2),
+                String.format(""INSERT INTO %s.users(id, id2, name) VALUES (2, 2, 'test2')"", keyspace2),
+                String.format(TestUtils.CREATE_KEYSPACE_SIMPLE_FORMAT, keyspace3, 2),
+                String.format(""CREATE TABLE %s.users(id int, id2 int, name text, primary key (id, id2))"", keyspace3),
+                String.format(""INSERT INTO %s.users(id, id2, name) VALUES (3, 3, 'test3')"", keyspace3)
+        );
+    }
+
+    @Test(groups = ""long"")
+    @CassandraVersion(""4.0.0"")
+    public void should_be_able_to_execute_statement_on_restarted_node_reprepare_on_up_set_keyspace() {
+        // validate that if a node is restarted we should be able to execute a bound statement against it
+        // as the statement is reprepared when the node comes back up.
+        // uses the Protocol V5 per-query keyspace strategy.
+        executeAfterNodeBroughtBackUp(true, true);
+    }
+
+    @Test(groups = ""long"")
+    @CassandraVersion(""4.0.0"")
+    public void should_be_able_to_execute_statement_on_restarted_node_set_keyspace() {
+        // validate that if a node is restarted we should be able to execute a bound statement against it
+        // as the statement is reprepared when we receive an 'unprepared' response.
+        // uses the Protocol V5 per-query keyspace strategy.
+        executeAfterNodeBroughtBackUp(false, true);
+    }
+
+    @Test(groups = ""long"")
+    public void should_be_able_to_execute_statement_on_restarted_node_reprepare_on_up_use_keyspace() {
+        // validate that if a node is restarted we should be able to execute a bound statement against it
+        // as the statement is reprepared when the node comes back up.
+        // uses the ""USE keyspace"" strategy which is an anti-pattern.
+        executeAfterNodeBroughtBackUp(true, false);
+    }
+
+    @Test(groups = ""long"")
+    public void should_be_able_to_execute_statement_on_restarted_node_use_keyspace() {
+        // validate that if a node is restarted we should be able to execute a bound statement against it
+        // as the statement is reprepared when we receive an 'unprepared' response.
+        // uses the Protocol V5 per-query keyspace strategy.
+        executeAfterNodeBroughtBackUp(false, false);
+    }
+
+    public void executeAfterNodeBroughtBackUp(boolean reprepareOnUp, boolean setKeyspace) {
+        QueryOptions queryOptions = new QueryOptions().setReprepareOnUp(reprepareOnUp).setPrepareOnAllHosts(false);
+        Cluster.Builder builder = createClusterBuilderNoDebouncing()
+                .withNettyOptions(TestUtils.nonQuietClusterCloseOptions)
+                .addContactPointsWithPorts(getContactPointsWithPorts())
+                .withPort(ccm().getBinaryPort())
+                .withQueryOptions(queryOptions)
+                .withLoadBalancingPolicy(new SortingLoadBalancingPolicy());
+
+        // TODO, remove this when V5 is no longer beta.
+        if (setKeyspace) {
+            builder = builder.allowBetaProtocolVersion();
+        }
+
+        Cluster cluster = builder.build();
+        try {
+            Session session = cluster.connect();
+
+            PreparedStatement prep2;
+            PreparedStatement prep3;
+
+            if (setKeyspace) {
+                prep2 = session.prepare(new SimpleStatement(""SELECT * from users"").setKeyspace(keyspace2));
+                prep3 = session.prepare(new SimpleStatement(""SELECT * from users"").setKeyspace(keyspace3));
+            } else {
+                // Execute use keyspace before preparing.  This is an anti-pattern but works nonetheless.
+                session.execute(""USE "" + keyspace2);
+                prep2 = session.prepare(new SimpleStatement(""SELECT * from users""));
+                session.execute(""USE "" + keyspace3);
+                prep3 = session.prepare(new SimpleStatement(""SELECT * from users""));
+            }
+
+            Host host1 = TestUtils.findHost(cluster, 1);
+            Host host2 = TestUtils.findHost(cluster, 2);
+
+            // Execute queries 10 times, should always hit host1 due to sorting policy.
+            for (int i = 0; i < 10; i++) {","[{'comment': 'Why 10 times?', 'commenter': 'olim7t'}, {'comment': 'arbitrary, just wanted to hit all hosts.  Will reduce.', 'commenter': 'tolbertam'}, {'comment': ""But doesn't the sorting policy guarantee that you'll always hit host1?"", 'commenter': 'olim7t'}, {'comment': ""ah that's right.  I'm probably using the policy incorrectly then (or not, i need to revisit the tests). I'll look into adjusting one way or another."", 'commenter': 'tolbertam'}]"
876,driver-core/src/test/java/com/datastax/driver/core/PreparedStatementMultiNodeTest.java,"@@ -0,0 +1,230 @@
+/*
+ * Copyright (C) 2012-2017 DataStax Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.driver.core;
+
+import com.datastax.driver.core.exceptions.DriverInternalError;
+import com.datastax.driver.core.utils.CassandraVersion;
+import org.testng.annotations.Test;
+
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.testng.Assert.fail;
+
+@CCMConfig(numberOfNodes = 2, dirtiesContext = true)
+@CreateCCM(CreateCCM.TestMode.PER_METHOD)
+public class PreparedStatementMultiNodeTest extends CCMTestsSupport {
+
+    private static final String keyspace2 = TestUtils.generateIdentifier(""ks_"");
+    private static final String keyspace3 = TestUtils.generateIdentifier(""ks_"");
+
+    @Override
+    public void onTestContextInitialized() {
+        execute(
+                String.format(TestUtils.CREATE_KEYSPACE_SIMPLE_FORMAT, keyspace2, 2),
+                String.format(""CREATE TABLE %s.users(id int, id2 int, name text, primary key (id, id2))"", keyspace2),
+                String.format(""INSERT INTO %s.users(id, id2, name) VALUES (2, 2, 'test2')"", keyspace2),
+                String.format(TestUtils.CREATE_KEYSPACE_SIMPLE_FORMAT, keyspace3, 2),
+                String.format(""CREATE TABLE %s.users(id int, id2 int, name text, primary key (id, id2))"", keyspace3),
+                String.format(""INSERT INTO %s.users(id, id2, name) VALUES (3, 3, 'test3')"", keyspace3)
+        );
+    }
+
+    @Test(groups = ""long"")
+    @CassandraVersion(""4.0.0"")
+    public void should_be_able_to_execute_statement_on_restarted_node_reprepare_on_up_set_keyspace() {","[{'comment': 'The method names are really hard to understand. ""be able to"" can generally be removed without losing anything. Also, the fact that reprepare on up is enabled or not is a detail, especially with recent Cassandra version where it will make no difference; the two cases could be done in the same method.\r\n\r\nSo my suggestion would be:\r\n```java\r\n    @Test(groups = ""long"")\r\n    @CassandraVersion(""4.0.0"")\r\n    public void should_execute_statement_with_per_query_keyspace_on_restarted_node() {\r\n        executeAfterNodeBroughtBackUp(true, true);\r\n        executeAfterNodeBroughtBackUp(false, true);\r\n    }\r\n\r\n    @Test(groups = ""long"")\r\n    public void should_execute_statement_with_session_keyspace_on_restarted_node() {\r\n        executeAfterNodeBroughtBackUp(true, false);\r\n        executeAfterNodeBroughtBackUp(false, false);\r\n    }\r\n```\r\n', 'commenter': 'olim7t'}]"
876,driver-core/src/main/java/com/datastax/driver/core/SessionManager.java,"@@ -149,12 +149,25 @@ public void run() {
     }
 
     @Override
-    protected ListenableFuture<PreparedStatement> prepareAsync(String query, Map<String, ByteBuffer> customPayload) {
-        Requests.Prepare request = new Requests.Prepare(query);
+    protected ListenableFuture<PreparedStatement> prepareAsync(String query, String keyspace, Map<String, ByteBuffer> customPayload) {
+        try {
+            checkCanSetKeyspace(keyspace);
+        } catch (UnsupportedFeatureException ufe) {
+            return Futures.immediateFailedFuture(ufe);
+        }
+        Requests.Prepare request = new Requests.Prepare(query, keyspace);
         request.setCustomPayload(customPayload);
         Connection.Future future = new Connection.Future(request);
         execute(future, Statement.DEFAULT);
-        return toPreparedStatement(query, future);
+        return toPreparedStatement(query, keyspace, future);
+    }
+
+    private void checkCanSetKeyspace(String keyspace) {
+        if (keyspace != null && !keyspace.equals(poolsState.keyspace) && cluster.manager.protocolVersion().compareTo(ProtocolVersion.V5) < 0) {","[{'comment': 'Should I remove the condition that checks if the keyspaces are different?  Maybe we should also throw an exception when using `setKeyspace` at < V5?', 'commenter': 'tolbertam'}, {'comment': ""Just remembered that this is imperfect because it's possible `setKeyspace` was used for something else (like routing).  I think i'll remove this check then"", 'commenter': 'tolbertam'}, {'comment': ""Yes I think that's in line with my previous comment about qualified table names."", 'commenter': 'olim7t'}]"
876,driver-core/src/main/java/com/datastax/driver/core/SessionManager.java,"@@ -202,7 +215,16 @@ public Cluster getCluster() {
                         switch (rm.kind) {
                             case PREPARED:
                                 Responses.Result.Prepared pmsg = (Responses.Result.Prepared) rm;
-                                PreparedStatement stmt = DefaultPreparedStatement.fromMessage(pmsg, cluster, query, poolsState.keyspace);
+                                String keyspaceToUse = poolsState.keyspace;
+                                if (keyspace != null && !keyspace.equals(keyspaceToUse)) {","[{'comment': 'This seems very unlikely now that we check before sending the query.', 'commenter': 'tolbertam'}]"
903,driver-core/pom.xml,"@@ -229,6 +229,8 @@
                             <instructions>
                                 <!-- JNR does not provide OSGi bundles, so exclude it; the driver can live without it -->
                                 <Import-Package><![CDATA[com.google.common.*;version=""[16.0.1,22)"",!jnr.*,*]]></Import-Package>
+                                <!-- Give the ability to discover and load netty-transport-native-epoll classes -->
+                                <DynamicImport-Package>*</DynamicImport-Package>","[{'comment': ""I have limited knowledge of OSGI, but from what I can see in the [Netty PR](https://github.com/netty/netty/pull/5326/files), this is necessary to load native libraries. Is there any way we could target this more accurately? The `*` seems very generic (although I can't really tell the consequences)."", 'commenter': 'olim7t'}, {'comment': ""Native library is a corner case. DynamicImport-Package has the following main meaning: within OSGi container you have multiple bundles (JARs), each one is loaded by it's own classloader. OSGi controls visibility of java packages between bundles based on declaration in bundles manifests. Bundle can declare some packages as exported (Export-Package), it means that they can be used by other bundles. From consumer side - bundle declares what packages it needs using Import-Package. Usually import packages are generated automatically by maven-bungle-plugin based on imports in code. If a class is not a part of bundle itself, not a part of base JDK packages (in OSGI there is a boot delegation logic) and not declared as imported - it will not be resolved. \r\nThere are cases when plain import-export logic is not enough - classes are not declared in code explicitly, the most common cause is Class.forName usage when class is loaded by name (as we have in com.datastax.driver.core.NettyUtil class). One possible way to deal with such dynamic class loading is to add DynamicImport-Package declaration. It allows to resolve class from another bundle if it is not declared explicitly as imported. Only exported packages can be imported via dynamic imports, so it does not break encapsulation.\r\nImport-Package and DynamicImport-Package are also resolved at different moments of bundle lifecycle. Import-Package is resolved when a bundle is installed (and if a package is not present and not declared as optional - the resolving will fail). DynamicImport-Package is used in runtime, when an actual code tries to load classes and other ways do not work.\r\nIn theory we can try to limit DynamicImport-Package scope to  `io.netty.channel.epoll.*` but I did not test such option to be sure. Regarding consequences of using `*`  - actually there is no large difference because DynamicImport is used only as a last chance to load a class if other mechanisms do not work (the resolving order is a part of OSGi specification), so for existing working logic there is no large difference.\r\n\r\n"", 'commenter': 'netudima'}]"
905,driver-core/src/main/java/com/datastax/driver/core/Metadata.java,"@@ -125,27 +132,36 @@ static String handleId(String id) {
         if (id == null)
             return null;
 
-        if (isAlphanumeric(id))
+        boolean isAlphanumericLowCase = true;
+        boolean isAlphanumeric = true;
+        for (int i = 0; i < id.length(); i++) {
+            char c = id.charAt(i);
+            if (c >= 65 && c <= 90) { // A-Z
+                isAlphanumericLowCase = false;
+            } else {
+                if (!(","[{'comment': 'Nit: you can use `else if` here.', 'commenter': 'olim7t'}, {'comment': 'yes, agree, will update with the change', 'commenter': 'netudima'}]"
905,driver-core/src/main/java/com/datastax/driver/core/Metadata.java,"@@ -274,7 +290,7 @@ public static String quote(String id) {
      * CQL keyword, {@code false} otherwise.
      */
     public static boolean isReservedCqlKeyword(String id) {
-        return id != null && RESERVED_KEYWORDS.contains(id.toLowerCase());
+        return id != null && RESERVED_KEYWORDS.contains(id);","[{'comment': ""Unfortunately this won't work for mixed-case keywords, as in `Local_Quorum`. I know this is a silly edge case, but it's possible so we have to handle it all the way."", 'commenter': 'olim7t'}, {'comment': 'yes, agree, missed the case, will think how is better to deal with it', 'commenter': 'netudima'}, {'comment': ""To expand on my suggestion in the review summary, we can try using this method:\r\n```java\r\nprivate static int caseInsensitiveHash(String s) {\r\n    int hash = 17;\r\n    for (int i = 0; i < s.length(); i++) {\r\n        char c = s.charAt(i);\r\n        if (c >= 65 && c <= 90) { // A-Z\r\n            c += 32;\r\n        }\r\n        hash = 31 * hash + c;\r\n    }\r\n    return hash;\r\n```\r\nWe turn `RESERVED_KEYWORDS` into a map of caseInsensitiveHash => lower-case keyword.\r\nThen `isReservedKeywords` can be implemented as:\r\n```java\r\nif (id == null) {\r\n  return false;\r\n}\r\nint hash = caseInsensitiveHash(id);\r\nString keyword = RESERVED_KEYWORDS.get(hash);\r\nreturn keyword != null && id.toLowerCase().equals(keyword);\r\n```\r\nAgain I'm not sure if this will improve performance so we'll have to benchmark it."", 'commenter': 'olim7t'}, {'comment': 'Thank you for the details, I got the idea. Checking different options with JMH, will publish results soon.', 'commenter': 'netudima'}, {'comment': 'Some initial results:\r\n\r\nhttps://gist.github.com/netudima/6c60455a370de4d44bb0879f36f4641b \r\n```\r\n# JMH version: 1.19\r\n# VM version: JDK 1.8.0_151, VM 25.151-b12\r\n# VM invoker: C:\\Program Files\\Java\\jre1.8.0_151\\bin\\java.exe\r\n# VM options: <none>\r\n# Warmup: 10 iterations, 1 s each\r\n# Measurement: 30 iterations, 1 s each\r\n# Timeout: 10 min per iteration\r\n# Threads: 2 threads, will synchronize iterations\r\n# Benchmark mode: Average time, time/op\r\n\r\nBenchmark                                                                      Mode  Cnt    Score   Error  Units\r\nMetadataBenchmark.equalsWrapper_NonReservedKeywordMixedCase                    avgt   30   35.298 Â± 1.840  ns/op\r\nMetadataBenchmark.equalsWrapper_NonReservedKeywordUpperCase                    avgt   30   39.861 Â± 0.864  ns/op\r\nMetadataBenchmark.equalsWrapper_ReservedKeywordMixedCase                       avgt   30   66.610 Â± 1.391  ns/op\r\nMetadataBenchmark.equalsWrapper_reservedKeywordUpperCase                       avgt   30   92.498 Â± 1.685  ns/op\r\nMetadataBenchmark.fullyUpperOrFullyLowerCaseCheck_NonReservedKeywordMixedCase  avgt   30   40.995 Â± 0.725  ns/op\r\nMetadataBenchmark.fullyUpperOrFullyLowerCaseCheck_NonReservedKeywordUpperCase  avgt   30   30.344 Â± 0.538  ns/op\r\nMetadataBenchmark.fullyUpperOrFullyLowerCaseCheck_ReservedKeywordMixedCase     avgt   30   70.420 Â± 3.180  ns/op\r\nMetadataBenchmark.fullyUpperOrFullyLowerCaseCheck_ReservedKeywordUpperCase     avgt   30   20.327 Â± 0.635  ns/op\r\nMetadataBenchmark.toLowCase_NonReservedKeywordMixedCase                        avgt   30  130.456 Â± 2.344  ns/op\r\nMetadataBenchmark.toLowCase_NonReservedKeywordUpperCase                        avgt   30  161.837 Â± 2.163  ns/op\r\nMetadataBenchmark.toLowCase_ReservedKeywordMixedCase                           avgt   30  128.490 Â± 4.897  ns/op\r\nMetadataBenchmark.toLowCase_reservedKeywordUpperCase                           avgt   30  142.862 Â± 2.836  ns/op\r\n```\r\n\r\nWhere:\r\n\r\n1) equalsWrapper - wrap String with case-insensitive equals and hashcode\r\n```java\r\n\r\nprivate static final Set<IgnoreCaseWrapper> RESERVED_KEYWORDS_WITH_CUSTOM_EQUAL = ignoreCaseImmutableSet(RESERVED_KEYWORDS);\r\n\r\nprivate static IgnoreCaseWrapper ignoreCase(String str) {\r\n    return new IgnoreCaseWrapper(str);\r\n}\r\n\r\nprivate static class IgnoreCaseWrapper {\r\n    private final String str;\r\n\r\n    private IgnoreCaseWrapper(String str) {\r\n        this.str = str;\r\n    }\r\n\r\n    @Override\r\n    public boolean equals(Object o) {\r\n        if (this == o) return true;\r\n        if (!(o instanceof IgnoreCaseWrapper)) return false;\r\n        IgnoreCaseWrapper anotherWrapper = (IgnoreCaseWrapper) o;\r\n\r\n        if (str.length() != anotherWrapper.str.length()) return false;\r\n\r\n        String anotherString = ((IgnoreCaseWrapper) o).str;\r\n\r\n        for (int i = 0; i < str.length(); i++) {\r\n            char c1 = str.charAt(i);\r\n            char c2 = anotherString.charAt(i);\r\n            if (c1 == c2) {\r\n                continue;\r\n            }\r\n            char low1 = toLowerCaseAscii(c1);\r\n            char low2 = toLowerCaseAscii(c2);\r\n            if (low1 == low2) {\r\n                continue;\r\n            }\r\n            return false;\r\n        }\r\n        return true;\r\n    }\r\n\r\n    private static char toLowerCaseAscii(char c) {\r\n        if (c >= 65 && c <= 90) { // A-Z\r\n            c += 32; // convert to low case\r\n        }\r\n        return c;\r\n    }\r\n\r\n    @Override\r\n    public int hashCode() {\r\n        int hashCode = 17;\r\n        for (int i = 0; i < str.length(); i++) {\r\n            char c = toLowerCaseAscii(str.charAt(i));\r\n            hashCode = 31 * hashCode + c;\r\n        }\r\n        return hashCode;\r\n    }\r\n}\r\n```\r\n\r\n2) fullyUpperOrFullyLowerCaseCheck - check does String contain only Upper case or only Lower case symbols and use plain set contains check in this case, use equalsWrapper otherwise.\r\n```java\r\npublic static boolean isReservedCqlKeyword(String id) {\r\n    if(id == null) return false;\r\n    if (isFullyUpperOrFullyLowerCase(id)) {\r\n        return RESERVED_KEYWORDS.contains(id);\r\n    }\r\n    return RESERVED_KEYWORDS_WITH_CUSTOM_EQUAL.contains(ignoreCase(id));\r\n}\r\n\r\nprivate static boolean isFullyUpperOrFullyLowerCase(String s) {\r\n    boolean isAlphanumericLowCase = true;\r\n    boolean isAlphanumericUpperCase = true;\r\n    for (int i = 0; i < s.length(); i++) {\r\n        char c = s.charAt(i);\r\n        if (c >= 65 && c <= 90) { // A-Z\r\n            isAlphanumericLowCase = false;\r\n            if (!isAlphanumericUpperCase) {\r\n                break;\r\n            }\r\n        } else if (c >= 97 && c <= 122) { // a-z\r\n            isAlphanumericUpperCase = false;\r\n            if (!isAlphanumericLowCase) {\r\n                break;\r\n            }\r\n        } else if (!(\r\n                (c >= 48 && c <= 57) // 0-9\r\n                        || (c == 95) // _ (underscore)\r\n        )) {\r\n            isAlphanumericLowCase = false;\r\n            isAlphanumericUpperCase = false;\r\n            break;\r\n        }\r\n    }\r\n\r\n    return isAlphanumericLowCase || isAlphanumericUpperCase;\r\n}\r\n```\r\n3) toLowCase - the current behavior with toLowerCase conversion\r\n```java\r\npublic static boolean isReservedCqlKeyword(String id) {\r\n    return id != null && RESERVED_KEYWORDS_OLD.contains(id.toLowerCase());\r\n}\r\n```', 'commenter': 'netudima'}]"
905,driver-core/src/main/java/com/datastax/driver/core/Metadata.java,"@@ -125,27 +132,36 @@ static String handleId(String id) {
         if (id == null)
             return null;
 
-        if (isAlphanumeric(id))
+        boolean isAlphanumericLowCase = true;
+        boolean isAlphanumeric = true;
+        for (int i = 0; i < id.length(); i++) {
+            char c = id.charAt(i);
+            if (c >= 65 && c <= 90) { // A-Z
+                isAlphanumericLowCase = false;
+            } else {
+                if (!(
+                        (c >= 48 && c <= 57) // 0-9
+                                || (c == 95) // _ (underscore)
+                                || (c >= 97 && c <= 122) // a-z
+                )) {
+                    isAlphanumeric = false;
+                    isAlphanumericLowCase = false;
+                    break;
+                }
+            }
+        }
+
+        if (isAlphanumericLowCase) {
+            return id;
+        }","[{'comment': 'ðŸ‘  agreed, nice improvement.\r\nEspecially since hopefully 99% of people use lower case.', 'commenter': 'olim7t'}]"
905,driver-core/src/main/java/com/datastax/driver/core/Metadata.java,"@@ -289,8 +350,9 @@ public static String quote(String id) {
      * @return {@code true} if the given identifier is a known reserved
      * CQL keyword, {@code false} otherwise.
      */
+
     public static boolean isReservedCqlKeyword(String id) {
-        return id != null && RESERVED_KEYWORDS.contains(id);
+        return id != null && IGNORE_CASE_RESERVED_KEYWORDS.contains(ignoreCase(id));","[{'comment': ""I agree that a wrapper is cleaner, but since we're trying to squeeze the last bit of performance, I wonder if we should avoid this extra object instantiation for each method call. That's why I suggested a separate method for the hash and a map for the reserved keywords. WDYT?"", 'commenter': 'olim7t'}, {'comment': ""yes, I thought about the extra object allocation. The problem is what in this case:\r\n```java\r\nint hash = caseInsensitiveHash(id);\r\nString keyword = RESERVED_KEYWORDS.get(hash);\r\n```\r\nwe also have a allocation - auto-boxing of primitive int. There is a pool of pre-allocated objects in Integer.valueOf ([-128 ; 127]) but in cause of hashes when values are spread across int range there is no large benefit from it. We can use a special map with primitive int keys to avoid it - will be ok to use https://netty.io/4.0/api/io/netty/util/collection/IntObjectMap.html in the driver code (there are other libraries like Trove but Netty is already present as a dependency)?\r\n\r\nRegarding:\r\n```java \r\nString keyword = RESERVED_KEYWORDS.get(hash)\r\n```\r\nDo you suggest here to use 'perfect hash functions' without collisions on the constant set of keywords?\r\n"", 'commenter': 'netudima'}, {'comment': ""> we also have a allocation - auto-boxing of primitive int\r\n\r\nGood point.\r\n\r\n> Do you suggest here to use 'perfect hash functions' without collisions on the constant set of keywords?\r\n\r\nI hadn't completely thought this through, we'd need a multimap (map of lists) to handle collisions. At this point I'm not sure if this is worth exploring."", 'commenter': 'olim7t'}, {'comment': 'Have implemented a non-wrap option using io.netty.util.collection.IntObjectHashMap.\r\nAlso because there is a granular control on comparison logic for this way - used char[] to avoid boundary checks in equals logic (saves about 10 ns).\r\n\r\nResults:\r\n\r\n```\r\n# JMH version: 1.19\r\n# VM version: JDK 1.8.0_151, VM 25.151-b12\r\n# VM invoker: C:\\Program Files\\Java\\jre1.8.0_151\\bin\\java.exe\r\n# VM options: <none>\r\n# Warmup: 5 iterations, 1 s each\r\n# Measurement: 30 iterations, 1 s each\r\n# Timeout: 10 min per iteration\r\n# Threads: 2 threads, will synchronize iterations\r\n# Benchmark mode: Average time, time/op\r\n\r\nBenchmark                                                    Mode  Cnt    Score   Error  Units\r\nMetadataBenchmark.equalsWrapper_NonReservedKeywordMixedCase  avgt   30   49.738 Â± 1.313  ns/op\r\nMetadataBenchmark.equalsWrapper_NonReservedKeywordUpperCase  avgt   30   59.430 Â± 3.230  ns/op\r\nMetadataBenchmark.equalsWrapper_ReservedKeywordMixedCase     avgt   30   75.837 Â± 4.706  ns/op\r\nMetadataBenchmark.equalsWrapper_reservedKeywordUpperCase     avgt   30  100.812 Â± 2.402  ns/op\r\n\r\nMetadataBenchmark.hashCheck_NonReservedKeywordMixedCase      avgt   30   45.817 Â± 1.289  ns/op\r\nMetadataBenchmark.hashCheck_NonReservedKeywordUpperCase      avgt   30   47.084 Â± 1.760  ns/op\r\nMetadataBenchmark.hashCheck_ReservedKeywordMixedCase         avgt   30   72.546 Â± 1.198  ns/op\r\nMetadataBenchmark.hashCheck_reservedKeywordUpperCase         avgt   30   93.308 Â± 1.769  ns/op\r\n```\r\n\r\nCode:\r\n\r\n```java\r\nimport io.netty.util.collection.IntObjectHashMap;\r\nimport io.netty.util.collection.IntObjectMap;\r\n\r\nprivate static final IntObjectMap<List<char[]>> KEYWORD_HASHES = new IntObjectHashMap<List<char[]>>();\r\nstatic {\r\n    for (String keyword : RESERVED_KEYWORDS) {\r\n        char[] keywordAsCharArray = keyword.toLowerCase().toCharArray();\r\n        int hash = ignoreCaseHashCode(keyword);\r\n        List<char[]> keywordsPerHash = KEYWORD_HASHES.get(hash);\r\n        if (keywordsPerHash == null) {\r\n            keywordsPerHash = new ArrayList<char[]>();\r\n            keywordsPerHash.add(keywordAsCharArray);\r\n            KEYWORD_HASHES.put(hash, keywordsPerHash);\r\n        } else {\r\n            keywordsPerHash.add(keywordAsCharArray);\r\n        }\r\n    }\r\n}\r\n    \r\n// keyword is expected as a second argument always in low case\r\nprivate static boolean equalsIgnoreCaseAscii(String str1, char[] str2LowCase) {\r\n    if (str1.length() != str2LowCase.length) return false;\r\n\r\n    for (int i = 0; i < str1.length(); i++) {\r\n        char c1 = str1.charAt(i);\r\n        char c2Low = str2LowCase[i];\r\n        if (c1 == c2Low) {\r\n            continue;\r\n        }\r\n        char low1 = toLowerCaseAscii(c1);\r\n        if (low1 == c2Low) {\r\n            continue;\r\n        }\r\n        return false;\r\n    }\r\n    return true;\r\n}\r\n\r\nprivate static char toLowerCaseAscii(char c) {\r\n    if (c >= 65 && c <= 90) { // A-Z\r\n        c = (char) (c ^ 0x20); // convert to lower case\r\n    }\r\n    return c;\r\n}\r\n\r\nprivate static int ignoreCaseHashCode(String str) {\r\n    int hashCode = 17;\r\n    for (int i = 0; i < str.length(); i++) {\r\n        char c = toLowerCaseAscii(str.charAt(i));\r\n        hashCode = 31 * hashCode + c;\r\n    }\r\n    return hashCode;\r\n}\r\n\r\npublic static boolean isReservedCqlKeyword(String id) {\r\n    if(id == null) {\r\n        return false;\r\n    }\r\n    int hash = ignoreCaseHashCode(id);\r\n    List<char[]> keywords = KEYWORD_HASHES.get(hash);\r\n    if (keywords == null) {\r\n        return false;\r\n    } else {\r\n        for (char[] keyword : keywords) {\r\n            if (equalsIgnoreCaseAscii(id, keyword)) {\r\n                return true;\r\n            }\r\n        }\r\n        return false;\r\n    }\r\n}\r\n```', 'commenter': 'netudima'}, {'comment': 'That is the preference for the last 2 options (ignore case wrapper vs hashes in primitive int map from netty-common)?', 'commenter': 'netudima'}, {'comment': ""The primitive int map seems a bit faster, let's go with that.\r\n> used char[] to avoid boundary checks in equals logic\r\n\r\nDo you mean the checks made by `charAt(i)`?"", 'commenter': 'olim7t'}, {'comment': 'yes, that logic\r\nok, thank you, I will update the patch with the primitive int map option.', 'commenter': 'netudima'}]"
915,driver-core/src/main/java/com/datastax/driver/core/Cluster.java,"@@ -1295,6 +1296,24 @@ public Builder withNettyOptions(NettyOptions nettyOptions) {
             return this;
         }
 
+        /**
+         * Sets whether or not the <code>NO_COMPACT</code> startup option should be present and set to true.
+         * <p>
+         * When this option is supplied, <code>SELECT</code>, <code>UPDATE</code>, <code>DELETE</code> and
+         * <code>BATCH</code> statements on <code>COMPACT STORAGE</code> tables function in ""compatibility"" mode which
+         * allows seeing these tables as if they were ""regular"" CQL tables.
+         * <p>
+         * This option only effects interactions with tables using <code>COMPACT STORAGE<code>.
+         *
+         * @param noCompact Whether or not NO_COMPACT should be configured on connections.
+         * @return this builder.
+         * @see <a href=""https://issues.apache.org/jira/browse/CASSANDRA-10857"">CASSANDRA-10857</a>
+         */
+        public Builder withNoCompact(boolean noCompact) {","[{'comment': ""Wasn't sure if I should have done it this way or to simply have `withNoCompact()` which enables the flag.  The advantage of doing it this way if you can disable the option after the fact, but the downside is you have to provide the boolean value."", 'commenter': 'tolbertam'}, {'comment': ""I'd prefer the no-argument version, in order to follow the same pattern as `withoutMetrics`."", 'commenter': 'olim7t'}]"
915,driver-core/src/main/java/com/datastax/driver/core/ProtocolOptions.java,"@@ -135,11 +137,30 @@ public ProtocolOptions(int port) {
      *                        the Cassandra nodes.
      */
     public ProtocolOptions(int port, ProtocolVersion protocolVersion, int maxSchemaAgreementWaitSeconds, SSLOptions sslOptions, AuthProvider authProvider) {
+        this(port, protocolVersion, maxSchemaAgreementWaitSeconds, sslOptions, authProvider, false);
+    }
+
+    /**
+     * Creates a new {@code ProtocolOptions} instance using the provided port
+     * and SSL context.
+     *
+     * @param port            the port to use for the binary protocol.
+     * @param protocolVersion the protocol version to use. This can be {@code null}, in which case the
+     *                        version used will be the biggest version supported by the <em>first</em> node the driver connects to.
+     *                        See {@link Cluster.Builder#withProtocolVersion} for more details.
+     * @param sslOptions      the SSL options to use. Use {@code null} if SSL is not
+     *                        to be used.
+     * @param authProvider    the {@code AuthProvider} to use for authentication against
+     *                        the Cassandra nodes.
+     * @param noCompact       whether or not to include the NO_COMPACT startup option.
+     */
+    public ProtocolOptions(int port, ProtocolVersion protocolVersion, int maxSchemaAgreementWaitSeconds, SSLOptions sslOptions, AuthProvider authProvider, boolean noCompact) {","[{'comment': 'Created additional constructor for backwards compatibility,  but may be overkill.', 'commenter': 'tolbertam'}, {'comment': 'lgtm ðŸ‘ ', 'commenter': 'olim7t'}]"
915,driver-core/src/main/java/com/datastax/driver/core/Cluster.java,"@@ -1295,6 +1296,24 @@ public Builder withNettyOptions(NettyOptions nettyOptions) {
             return this;
         }
 
+        /**","[{'comment': 'Following the [python driver implementation](https://github.com/datastax/python-driver/pull/846), NO_COMPACT is not enabled by default.', 'commenter': 'tolbertam'}]"
919,integration-tests/src/test/java/com/datastax/oss/driver/api/core/cql/PerRequestKeyspaceIT.java,"@@ -35,7 +35,7 @@
  * test against a local build, run with
  *
  * <pre>
- *   -Dccm.cassandraVersion=4.0.0 -Dccm.cassandraDirectory=/path/to/cassandra -Ddatastax-java-driver.protocol.version=V5
+ *   -Dccm.version=4.0.0 -Dccm.directory=/path/to/cassandra -Ddatastax-java-driver.protocol.version=V5","[{'comment': 'Kudos for thinking of updating this ðŸ‘ \r\nThere is a similar comment in `PreparedStatementInvalidationIT`.', 'commenter': 'olim7t'}, {'comment': 'Ah yes, will adjust that too.', 'commenter': 'tolbertam'}]"
919,changelog/README.md,"@@ -17,6 +17,8 @@
 - [improvement] JAVA-1662: Raise default request timeout
 - [improvement] JAVA-1566: Enforce API rules automatically
 - [bug] JAVA-1584: Validate that no bound values are unset in protocol v3
+- [improvement] JAVA-1707: Test infrastructure for running DSE clusters with CCM
+","[{'comment': 'Could you move this to the beginning of the changelog, and also change the commit message to the imperative form (see [contribution guidelines](https://github.com/datastax/java-driver/blob/4.x/CONTRIBUTING.md#commits))?', 'commenter': 'olim7t'}, {'comment': 'Forgot about that change, will do!', 'commenter': 'tolbertam'}]"
919,test-infra/src/main/java/com/datastax/oss/driver/api/testinfra/ccm/CcmBridge.java,"@@ -278,17 +332,20 @@ public static Builder builder() {
   public static class Builder {
     private int[] nodes = {1};
     private final Map<String, Object> cassandraConfiguration = new LinkedHashMap<>();
+    private final Map<String, Object> dseConfiguration = new LinkedHashMap<>();
     private final List<String> jvmArgs = new ArrayList<>();
     private String ipPrefix = ""127.0.0."";
-    private CassandraVersion cassandraVersion = CcmBridge.DEFAULT_CASSANDRA_VERSION;
-    private String cassandraDirectory = CcmBridge.DEFAULT_CASSANDRA_DIRECTORY;
+    private CassandraVersion version = CcmBridge.DEFAULT_CASSANDRA_VERSION;
+    private String directory = CcmBridge.DEFAULT_CASSANDRA_DIRECTORY;","[{'comment': 'The builder does not provide any way to change those two fields.\r\nIf there is a case for controlling them programatically, we should have builder methods. If system properties are enough, we should get rid of all the fields and rely exclusively on the two constants.\r\nNo strong feeling but I would say the latter, wdyt?\r\n\r\nEDIT: same for `dse` actually', 'commenter': 'olim7t'}, {'comment': ':+1: on the latter.  Originally I wanted to make the version/directory/dse enablement API configurable, but I think it would be better to avoid tests that use and rely on specifying versions, and rather just only run them when that version is configured.    Will fix.', 'commenter': 'tolbertam'}]"
919,test-infra/src/main/java/com/datastax/oss/driver/api/testinfra/ccm/CcmBridge.java,"@@ -49,29 +50,39 @@
 
   private static final Logger logger = LoggerFactory.getLogger(CcmBridge.class);
 
-  private final CassandraVersion cassandraVersion;
-  private final String cassandraDirectory;
+  private final CassandraVersion version;
+  private final String directory;
+  private final boolean dse;
 
   private final int[] nodes;
 
-  private final Path directory;
+  private final Path configDirectory;
 
   private final AtomicBoolean started = new AtomicBoolean();
 
   private final AtomicBoolean created = new AtomicBoolean();
 
   private final String ipPrefix;
 
-  private final Map<String, Object> initialConfiguration;
+  private final Map<String, Object> cassandraConfiguration;
+  private final Map<String, Object> dseConfiguration;
   private final List<String> createOptions;
+  private final List<String> dseWorkloads;
 
   private final String jvmArgs;
 
   public static final CassandraVersion DEFAULT_CASSANDRA_VERSION =
-      CassandraVersion.parse(System.getProperty(""ccm.cassandraVersion"", ""3.11.0""));
+      CassandraVersion.parse(System.getProperty(""ccm.version"", ""3.11.0""));
 
-  public static final String DEFAULT_CASSANDRA_DIRECTORY =
-      System.getProperty(""ccm.cassandraDirectory"");
+  public static final String DEFAULT_CASSANDRA_DIRECTORY = System.getProperty(""ccm.directory"");
+
+  public static final Boolean DEFAULT_DSE_ENABLEMENT;
+
+  static {
+    String dseProperty = System.getProperty(""ccm.dse"");
+    DEFAULT_DSE_ENABLEMENT =
+        dseProperty != null && (dseProperty.isEmpty() || Boolean.parseBoolean(dseProperty));
+  }","[{'comment': 'I think this can be simplified as:\r\n```java\r\npublic static final boolean DEFAULT_DSE_ENABLEMENT = Boolean.getBoolean(""ccm.dse"");\r\n```', 'commenter': 'olim7t'}]"
919,test-infra/src/main/java/com/datastax/oss/driver/api/testinfra/ccm/CcmBridge.java,"@@ -139,18 +155,45 @@ private CcmBridge(
       allJvmArgs.append(quote);
     }
     this.jvmArgs = allJvmArgs.toString();
+    this.dseWorkloads = dseWorkloads;
+  }
+
+  public Optional<CassandraVersion> getDseVersion() {
+    return dse ? Optional.of(version) : Optional.empty();
   }
 
   public CassandraVersion getCassandraVersion() {
-    return cassandraVersion;
+    if (!dse) {
+      return version;
+    } else {
+      CassandraVersion cassandraVersion = version;","[{'comment': 'Why the extra local variable?', 'commenter': 'olim7t'}]"
919,test-infra/src/main/java/com/datastax/oss/driver/api/testinfra/ccm/CcmBridge.java,"@@ -139,18 +155,45 @@ private CcmBridge(
       allJvmArgs.append(quote);
     }
     this.jvmArgs = allJvmArgs.toString();
+    this.dseWorkloads = dseWorkloads;
+  }
+
+  public Optional<CassandraVersion> getDseVersion() {
+    return dse ? Optional.of(version) : Optional.empty();
   }
 
   public CassandraVersion getCassandraVersion() {
-    return cassandraVersion;
+    if (!dse) {
+      return version;
+    } else {
+      CassandraVersion cassandraVersion = version;
+      if (cassandraVersion.getMajor() == 6) {
+        // 6.0 = 4.0.x
+        return CassandraVersion.parse(""4.0.0"");
+      } else if (cassandraVersion.getMajor() == 5) {
+        if (cassandraVersion.getMinor() == 0) {
+          // 5.0 = 3.0.x
+          return CassandraVersion.parse(""3.0.15"");
+        } else {
+          // 5.1 == 3.10
+          return CassandraVersion.parse(""3.10"");
+        }
+      } else {
+        // assume anything older is 2.1.19
+        return CassandraVersion.parse(""2.1.19"");
+      }","[{'comment': 'No big deal but I think it would be slightly more readable using constants and comparisons:\r\n\r\n```java\r\nCassandraVersion stableVersion = version.nextStable();\r\nif (stableVersion.compareTo(V6_0_0) >= 0) {\r\n  return CassandraVersion.parse(""4.0.0"");\r\n} else if (stableVersion.compareTo(V5_1_0) >= 0) {\r\n  return CassandraVersion.parse(""3.10"");\r\n} else if (stableVersion.compareTo(V5_0_0) >= 0) {\r\n  return CassandraVersion.parse(""3.0.15"");\r\n} else {\r\n  return CassandraVersion.parse(""2.1.19"");\r\n}\r\n```\r\nAnd then the comments are not needed.', 'commenter': 'olim7t'}]"
919,test-infra/src/main/java/com/datastax/oss/driver/api/testinfra/ccm/BaseCcmRule.java,"@@ -20,20 +20,18 @@
 import com.datastax.oss.driver.api.core.ProtocolVersion;
 import com.datastax.oss.driver.api.testinfra.CassandraRequirement;
 import com.datastax.oss.driver.api.testinfra.CassandraResourceRule;
-import com.datastax.oss.driver.api.testinfra.ccm.CcmBridge;
+import com.datastax.oss.driver.api.testinfra.DseRequirement;
+import java.util.Optional;
 import org.junit.AssumptionViolatedException;
 import org.junit.runner.Description;
 import org.junit.runners.model.Statement;
 
 public abstract class BaseCcmRule extends CassandraResourceRule {
 
-  protected final CcmBridge ccmBridge;
+  final CcmBridge ccmBridge;","[{'comment': 'why?', 'commenter': 'olim7t'}, {'comment': ""This is an artifact of my initial stab (which required accessing ccm bridge in the subclasses), that's no longer needed so I should revert this change."", 'commenter': 'tolbertam'}]"
919,test-infra/src/main/java/com/datastax/oss/driver/api/testinfra/ccm/BaseCcmRule.java,"@@ -88,33 +89,86 @@ public void evaluate() throws Throwable {
         // if the test version exceeds the maximum configured one, fail out.
         CassandraVersion maxVersion = CassandraVersion.parse(cassandraRequirement.max());
 
-        if (maxVersion.compareTo(cassandraVersion) <= 0) {
+        if (maxVersion.compareTo(ccmBridge.getCassandraVersion()) <= 0) {
           return new Statement() {
 
             @Override
-            public void evaluate() throws Throwable {
+            public void evaluate() {
               throw new AssumptionViolatedException(
                   ""Test requires C* less than ""
                       + maxVersion
                       + "" but ""
-                      + cassandraVersion
+                      + ccmBridge.getCassandraVersion()
                       + "" is configured.  Description: ""
                       + cassandraRequirement.description());
             }
           };
         }
       }
     }
+
+    DseRequirement dseRequirement = description.getAnnotation(DseRequirement.class);
+    if (dseRequirement != null) {
+      Optional<CassandraVersion> dseVersionOption = ccmBridge.getDseVersion();
+      if (!dseVersionOption.isPresent()) {
+        throw new AssumptionViolatedException(""Test Requires DSE but C* is configured."");","[{'comment': 'Why can we throw directly here, but in the other cases we have to build a statement?', 'commenter': 'olim7t'}, {'comment': 'ah yes, this looks like a bug, will fix.', 'commenter': 'tolbertam'}]"
919,test-infra/src/main/java/com/datastax/oss/driver/api/testinfra/ccm/BaseCcmRule.java,"@@ -59,24 +57,27 @@ protected void after() {
 
   @Override
   public Statement apply(Statement base, Description description) {
+    // If test is annotated with CassandraRequirement or DseRequirement, ensure configured CCM
+    // cluster meets those requirements.
     CassandraRequirement cassandraRequirement =
         description.getAnnotation(CassandraRequirement.class);
 
     if (cassandraRequirement != null) {
       // if the configured cassandra cassandraRequirement exceeds the one being used skip this test.
       if (!cassandraRequirement.min().isEmpty()) {
         CassandraVersion minVersion = CassandraVersion.parse(cassandraRequirement.min());
-        if (minVersion.compareTo(cassandraVersion) > 0) {
-          // Create a statement which simply indicates that the configured cassandra cassandraRequirement is too old for this test.
+        if (minVersion.compareTo(ccmBridge.getCassandraVersion()) > 0) {
+          // Create a statement which simply indicates that the configured cassandra
+          // cassandraRequirement is too old for this test.
           return new Statement() {
 
             @Override
-            public void evaluate() throws Throwable {
+            public void evaluate() {
               throw new AssumptionViolatedException(
                   ""Test requires C* ""
                       + minVersion
                       + "" but ""
-                      + cassandraVersion
+                      + ccmBridge.getCassandraVersion()
                       + "" is configured.  Description: ""
                       + cassandraRequirement.description());
             }","[{'comment': 'This is repeated 4 times, maybe we should extract a method that creates the Statement.\r\n  ', 'commenter': 'olim7t'}, {'comment': 'Good idea, this code is a big repetitive.', 'commenter': 'tolbertam'}]"
919,test-infra/src/main/java/com/datastax/oss/driver/api/testinfra/cluster/ClusterRuleBuilder.java,"@@ -15,12 +15,13 @@
  */
 package com.datastax.oss.driver.api.testinfra.cluster;
 
+import com.datastax.oss.driver.api.core.cql.CqlSession;
 import com.datastax.oss.driver.api.core.metadata.NodeStateListener;
 import com.datastax.oss.driver.api.testinfra.CassandraResourceRule;
 import com.datastax.oss.driver.api.testinfra.ccm.CcmRule;
 import com.datastax.oss.driver.api.testinfra.simulacron.SimulacronRule;
 
-public class ClusterRuleBuilder {
+public class ClusterRuleBuilder<T extends CqlSession> {","[{'comment': 'If we plan to extend this, we should introduce a ""self"" type to use as the return of the *with* methods.\r\n  ', 'commenter': 'olim7t'}]"
919,core/src/main/java/com/datastax/oss/driver/internal/core/metadata/token/ReplicationStrategy.java,"@@ -34,6 +34,8 @@ static ReplicationStrategy newInstance(Map<String, String> replicationConfig, St
         return new SimpleReplicationStrategy(replicationConfig);
       case ""org.apache.cassandra.locator.NetworkTopologyStrategy"":
         return new NetworkTopologyReplicationStrategy(replicationConfig, logPrefix);
+      case ""org.apache.cassandra.locator.EverywhereStrategy"":
+        return new EverywhereStrategy();","[{'comment': 'All things considered, I think it would be cleaner to introduce an overridable factory. I can do it in a separate pull request if you want.', 'commenter': 'olim7t'}, {'comment': ':+1: to doing that in a separate PR,  just did this here to get things working in the near term.  Will back it out of my final commits after I finish testing everything.', 'commenter': 'tolbertam'}]"
919,test-infra/src/main/java/com/datastax/oss/driver/api/testinfra/cluster/ClusterUtils.java,"@@ -55,25 +57,57 @@
 public class ClusterUtils {
   private static final AtomicInteger keyspaceId = new AtomicInteger();
 
+  private static final String CLUSTER_BUILDER_CLASS =
+      System.getProperty(
+          ""cluster.builder"",
+          ""com.datastax.oss.driver.api.testinfra.cluster.DefaultClusterBuilderInstantiator"");
+
+  @SuppressWarnings(""unchecked"")
+  public static <T extends CqlSession> ClusterBuilder<?, ? extends Cluster<T>> baseBuilder() {
+    try {
+      Class<?> clazz = Class.forName(CLUSTER_BUILDER_CLASS);
+      Method m = clazz.getMethod(""builder"");
+      return (ClusterBuilder<?, ? extends Cluster<T>>) m.invoke(null);
+    } catch (Exception e) {
+      e.printStackTrace();
+      // TODO: Logger","[{'comment': 'Is it just a matter of declaring an SLF4J logger?', 'commenter': 'olim7t'}, {'comment': ""Err woops, yes, it's pretty simple just forgot to do it, will fix."", 'commenter': 'tolbertam'}]"
919,test-infra/src/main/java/com/datastax/oss/driver/api/testinfra/cluster/ClusterUtils.java,"@@ -53,27 +59,62 @@
  * ClusterRule} provides a simpler alternative.
  */
 public class ClusterUtils {
+  private static final Logger LOG = LoggerFactory.getLogger(ClusterUtils.class);
   private static final AtomicInteger keyspaceId = new AtomicInteger();
 
+  private static final String CLUSTER_BUILDER_CLASS =
+      System.getProperty(
+          ""cluster.builder"",
+          ""com.datastax.oss.driver.api.testinfra.cluster.DefaultClusterBuilderInstantiator"");
+
+  @SuppressWarnings(""unchecked"")
+  public static <T extends CqlSession> ClusterBuilder<?, ? extends Cluster<T>> baseBuilder() {
+    try {
+      Class<?> clazz = Class.forName(CLUSTER_BUILDER_CLASS);
+      Method m = clazz.getMethod(""builder"");
+      return (ClusterBuilder<?, ? extends Cluster<T>>) m.invoke(null);
+    } catch (Exception e) {
+      LOG.warn(
+          ""Could not construct ClusterBuilder from {}, using default implementation."",
+          CLUSTER_BUILDER_CLASS,
+          e);
+      return (ClusterBuilder<?, ? extends Cluster<T>>) Cluster.builder();
+    }
+  }
+
+  public static String getConfigPath() {
+    try {
+      Class<?> clazz = Class.forName(CLUSTER_BUILDER_CLASS);
+      Method m = clazz.getMethod(""configPath"");
+      return (String) m.invoke(null);
+    } catch (Exception e) {
+      e.printStackTrace();
+      // TODO: Logger","[{'comment': ""There's one more here ;-)"", 'commenter': 'olim7t'}, {'comment': ':man_facepalming: , good catch, will fix.', 'commenter': 'tolbertam'}]"
932,core/src/main/java/com/datastax/oss/driver/api/core/session/Session.java,"@@ -16,25 +16,153 @@
 package com.datastax.oss.driver.api.core.session;
 
 import com.datastax.oss.driver.api.core.AsyncAutoCloseable;
-import com.datastax.oss.driver.api.core.Cluster;
 import com.datastax.oss.driver.api.core.CqlIdentifier;
-import com.datastax.oss.driver.api.core.cql.CqlSession;
+import com.datastax.oss.driver.api.core.config.CoreDriverOption;
+import com.datastax.oss.driver.api.core.context.DriverContext;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.metadata.Metadata;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import com.datastax.oss.driver.api.core.metadata.NodeState;
+import com.datastax.oss.driver.api.core.metadata.NodeStateListener;
+import com.datastax.oss.driver.api.core.metadata.schema.SchemaChangeListener;
 import com.datastax.oss.driver.api.core.type.reflect.GenericType;
+import com.datastax.oss.driver.internal.core.util.concurrent.BlockingOperation;
+import com.datastax.oss.driver.internal.core.util.concurrent.CompletableFutures;
+import java.util.ResourceBundle;
+import java.util.concurrent.CompletionStage;
 
 /**
  * A nexus to send requests to a Cassandra cluster.
  *
  * <p>This is a high-level abstraction capable of handling arbitrary request and result types. For
- * CQL statements, {@link CqlSession} provides convenience methods with more familiar signatures.
+ * CQL statements, {@link CqlSession} provides convenience methods with more familiar signatures (by
+ * default, all the instances returned by the driver also implement {@link CqlSession}).
  *
  * <p>The driver's request execution logic is pluggable (see {@code RequestProcessor} in the
  * internal API). This is intended for future extensions, for example a reactive API for CQL
  * statements, or graph requests in the Datastax Enterprise driver. Hence the generic {@link
  * #execute(Request, GenericType)} method in this interface, that makes no assumptions about the
  * request or result type.
+ *
+ * @see CqlSession#builder()
  */
 public interface Session extends AsyncAutoCloseable {
 
+  /**
+   * The current version of the driver.
+   *
+   * <p>This is intended for products that wrap or extend the driver, as a way to check
+   * compatibility if end-users override the driver version in their application.
+   */
+  static String getDriverVersion() {
+    // Note: getBundle caches its result
+    return ResourceBundle.getBundle(""com.datastax.oss.driver.Driver"").getString(""driver.version"");
+  }
+
+  /**
+   * The unique name identifying this client.
+   *
+   * @see CoreDriverOption#SESSION_NAME
+   */
+  String getName();
+
+  /**
+   * Returns a snapshot of the Cassandra cluster's topology and schema metadata.
+   *
+   * <p>In order to provide atomic updates, this method returns an immutable object: the node list,
+   * token map, and schema contained in a given instance will always be consistent with each other
+   * (but note that {@link Node} itself is not immutable: some of its properties will be updated
+   * dynamically, in particular {@link Node#getState()}).
+   *
+   * <p>As a consequence of the above, you should call this method each time you need a fresh view
+   * of the metadata. <b>Do not</b> call it once and store the result, because it is a frozen
+   * snapshot that will become stale over time.
+   *
+   * <p>If a metadata refresh triggers events (such as node added/removed, or schema events), then
+   * the new version of the metadata is guaranteed to be visible by the time you receive these
+   * events.
+   */
+  Metadata getMetadata();
+
+  /** Whether schema metadata is currently enabled. */
+  boolean isSchemaMetadataEnabled();
+
+  /**
+   * Enable or disable schema metadata programmatically.
+   *
+   * <p>Use this method to override the value defined in the driver's configuration; one typical use
+   * case is to temporarily disable schema metadata while the client issues a sequence of DDL
+   * statements.
+   *
+   * <p>If calling this method re-enables the metadata (that is, {@link #isSchemaMetadataEnabled()}
+   * was false before, and becomes true as a result of the call), a refresh is also triggered.
+   *
+   * @param newValue a boolean value to enable or disable schema metadata programmatically, or
+   *     {@code null} to use the driver's configuration.
+   * @see CoreDriverOption#METADATA_SCHEMA_ENABLED
+   * @return if this call triggered a refresh, a future that will complete when that refresh is
+   *     complete. Otherwise, a completed future with the current metadata.
+   */
+  CompletionStage<Metadata> setSchemaMetadataEnabled(Boolean newValue);
+
+  /**
+   * Force an immediate refresh of the schema metadata, even if it is currently disabled (either in
+   * the configuration or via {@link #setSchemaMetadataEnabled(Boolean)}).
+   *
+   * <p>The new metadata is returned in the resulting future (and will also be reflected by {@link
+   * #getMetadata()} when that future completes).
+   */
+  CompletionStage<Metadata> refreshSchemaAsync();
+
+  /**
+   * Convenience method to call {@link #refreshSchemaAsync()} and block for the result.
+   *
+   * <p>This must not be called on a driver thread.
+   */
+  default Metadata refreshSchema() {
+    BlockingOperation.checkNotDriverThread();
+    return CompletableFutures.getUninterruptibly(refreshSchemaAsync());
+  }
+
+  /**
+   * Checks if all nodes in the cluster agree on a common schema version.
+   *
+   * <p>Due to the distributed nature of Cassandra, schema changes made on one node might not be
+   * immediately visible to others. Under certain circumstances, the driver waits until all nodes
+   * agree on a common schema version (namely: before a schema refresh, and before completing a
+   * successful schema-altering query). To do so, it queries system tables to find out the schema
+   * version of all nodes that are currently {@link NodeState#UP UP}. If all the versions match, the
+   * check succeeds, otherwise it is retried periodically, until a given timeout (specified in the
+   * configuration).
+   *
+   * <p>A schema agreement failure is not fatal, but it might produce unexpected results (for
+   * example, getting an ""unconfigured table"" error for a table that you created right before, just
+   * because the two queries went to different coordinators).
+   *
+   * <p>Note that schema agreement never succeeds in a mixed-version cluster (it would be
+   * challenging because the way the schema version is computed varies across server versions); the
+   * assumption is that schema updates are unlikely to happen during a rolling upgrade anyway.
+   *
+   * @return a future that completes with {@code true} if the nodes agree, or {@code false} if the
+   *     timeout fired.
+   * @see CoreDriverOption#CONTROL_CONNECTION_AGREEMENT_INTERVAL
+   * @see CoreDriverOption#CONTROL_CONNECTION_AGREEMENT_TIMEOUT
+   */
+  CompletionStage<Boolean> checkSchemaAgreementAsync();
+
+  /**
+   * Convenience method to call {@link #checkSchemaAgreementAsync()} and block for the result.
+   *
+   * <p>This must not be called on a driver thread.
+   */
+  default boolean checkSchemaAgreement() {
+    BlockingOperation.checkNotDriverThread();
+    return CompletableFutures.getUninterruptibly(checkSchemaAgreementAsync());
+  }
+
+  /** Returns a context that provides access to all the policies used by this driver instance. */
+  DriverContext getContext();
+
   /**
    * The keyspace that this session is currently connected to.
    *","[{'comment': 'The javadocs for this method contain a few broken links to former `Cluster` class.', 'commenter': 'adutra'}]"
932,core/src/main/java/com/datastax/oss/driver/api/core/cql/CqlSessionBuilder.java,"@@ -13,16 +13,16 @@
  * See the License for the specific language governing permissions and","[{'comment': 'So, `CqlSession` now resides in `com.datastax.oss.driver.api.core` but `CqlSessionBuilder` is still in `com.datastax.oss.driver.api.core.cql`. Is this intentional?', 'commenter': 'adutra'}, {'comment': ""`CqlSession` used to be in the `cql` package, but since it's now the main entry point to the driver, I thought it would be more appropriate for it to be in the root package. I should at least add an implementation comment to explain that.\r\nAs for `CqlSessionBuilder`, it's true that it seems a bit weird to keep it in another package, I don't have strong feelings either way."", 'commenter': 'olim7t'}]"
934,manual/ssl/README.md,"@@ -153,7 +153,7 @@ add it to your dependencies.
 
 There are known runtime incompatibilities between newer versions of
 netty-tcnative and the version of netty that the driver uses.  For best
-results, use version 2.0.1.Final.
+results, use version 4.0.47.Final.","[{'comment': '2.0.1 was correct. This artifact does not follow the global Netty versioning scheme.', 'commenter': 'adutra'}]"
937,core/pom.xml,"@@ -75,6 +77,15 @@
       <groupId>org.slf4j</groupId>
       <artifactId>slf4j-api</artifactId>
     </dependency>
+    <dependency>
+      <groupId>io.dropwizard.metrics</groupId>
+      <artifactId>metrics-core</artifactId>
+    </dependency>
+    <dependency>","[{'comment': 'A few remarks about JMX:\r\n\r\n1) `metrics-jmx` does not seem used at all, not even in tests. Is this intentional? \r\n2) More broadly speaking, are you leaning towards the idea of not providing JMX support inside the driver and leave that for users?\r\n3) You seem to be creating hierarchical metric names such as `s0.nodes.127_0_0_1_9042.retries`. If you provide out-of-the-box JMX support, the nice thing is that you could customize your MBeans by creating hierarchical beans that reflect your internal hierarchy of metrics.', 'commenter': 'adutra'}, {'comment': ""1. True, I added it to write my manual test class, but it's not used elsewhere and might not even be used in future tests. Removing it.\r\n2. Yes. I don't see any need to have it out of the box because a) Adding a JMX reporter is trivial (two lines of code) and b) it's not recommended in production so users are likely to use something else.\r\n3. I don't understand, as far as I can tell the JMX reporter does already create a hierarchy based on the dot separators in the metric names. When I look up the MBeans in jvisualvm, `retries` is a child of `127_0_0_1_9042`, etc."", 'commenter': 'olim7t'}, {'comment': ""> as far as I can tell the JMX reporter does already create a hierarchy based on the dot separators in the metric names\r\n\r\nWas this available since metrics 3? I didn't know. OK then."", 'commenter': 'adutra'}, {'comment': 'I double-checked and actually you\'re right, I stand corrected.\r\n\r\nBut there is a way to do it from outside the driver: build the `JmxReporter` with an `ObjectNameFactory` that turns every dot-separated component into a subdirectory:\r\n\r\n```java\r\nObjectNameFactory objectNameFactory = (type, domain, name) -> {\r\n  StringBuilder objectName = new StringBuilder(domain).append(\':\');\r\n  List<String> nameParts = Splitter.on(\'.\').splitToList(name);\r\n  int i = 0;\r\n  for (String namePart : nameParts) {\r\n    boolean isLast = (i == nameParts.size() - 1);\r\n    String key =\r\n        isLast ? ""name"" : Strings.padStart(Integer.toString(i), 2, \'0\');\r\n    objectName.append(key).append(\'=\').append(namePart);\r\n    if (!isLast) {\r\n      objectName.append(\',\');\r\n    }\r\n    i += 1;\r\n  }\r\n  try {\r\n    return new ObjectName(objectName.toString());\r\n  } catch (MalformedObjectNameException e) {\r\n    throw new RuntimeException(e);\r\n  }\r\n};\r\n\r\nJmxReporter reporter =\r\n    JmxReporter.forRegistry(session.getMetricRegistry())\r\n        .inDomain(""com.datastax.oss.driver"")\r\n        .createsObjectNamesWith(objectNameFactory)\r\n        .build()\r\n```', 'commenter': 'olim7t'}, {'comment': ""Yes that's basically how we configured the JMX reporter in DSBulk. ðŸ‘ "", 'commenter': 'adutra'}]"
937,core/src/main/java/com/datastax/oss/driver/api/core/metrics/CoreNodeMetric.java,"@@ -0,0 +1,46 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core.metrics;
+
+import com.codahale.metrics.Gauge;
+import com.codahale.metrics.Meter;
+
+public enum CoreNodeMetric implements NodeMetric {
+
+  /**
+   * The number of connections open to this node for regular requests (exposed as a {@link Gauge
+   * Gauge&lt;Integer&gt;}.
+   *
+   * <p>This does not include the control connection (which uses at most one extra connection to a
+   * random node in the cluster).
+   */
+  pooled_connection_count,","[{'comment': ""Can't we stick to standards and use upper-case enum constant names?"", 'commenter': 'adutra'}, {'comment': ""I deliberately broke the convention because metric-land seems to favor lower case (from what I saw by looking at a few monitoring tools), and I find it nice to have a direct correspondence between the metric name and constant name.\r\n\r\nIf we want to respect the convention at all cost I could add a `getMetricName` that returns the lower-case version, but then it's a bit of a pity to have to explain that sometimes it's upper-case and sometimes lower."", 'commenter': 'olim7t'}]"
937,core/src/main/java/com/datastax/oss/driver/internal/core/metadata/DefaultNode.java,"@@ -50,8 +53,11 @@
 
   volatile NodeDistance distance;
 
-  public DefaultNode(InetSocketAddress connectAddress) {
+  public DefaultNode(InetSocketAddress connectAddress, InternalDriverContext context) {
     this.connectAddress = connectAddress;
+    // We leak a reference to a partially constructed object (this), but in practice this won't be a
+    // problem because the node updater only needs the connect address to initialize.
+    this.metricUpdater = context.metricUpdaterFactory().newNodeUpdater(this);","[{'comment': 'it seems a little worse than that, I see that the constructor of `DefaultNodeUpdater` also tries to do the following:\r\n```\r\nChannelPool pool = context.poolManager().getPools().get(node);\r\n```\r\n', 'commenter': 'adutra'}, {'comment': ""That's actually done in a lambda that isn't invoked until you read the corresponding gauge (and anyway the hashcode of `DefaultNode` also happens to only use the connect address)."", 'commenter': 'olim7t'}]"
937,core/src/main/java/com/datastax/oss/driver/internal/core/metrics/DefaultNodeMetricUpdater.java,"@@ -0,0 +1,86 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.metrics;
+
+import com.codahale.metrics.Gauge;
+import com.codahale.metrics.MetricRegistry;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import com.datastax.oss.driver.api.core.metrics.CoreNodeMetric;
+import com.datastax.oss.driver.api.core.metrics.NodeMetric;
+import com.datastax.oss.driver.internal.core.context.InternalDriverContext;
+import com.datastax.oss.driver.internal.core.pool.ChannelPool;
+import java.net.Inet4Address;
+import java.net.Inet6Address;
+import java.net.InetAddress;
+import java.net.InetSocketAddress;
+import java.util.Set;
+
+public class DefaultNodeMetricUpdater implements NodeMetricUpdater {
+
+  private final String metricNamePrefix;
+  private final MetricRegistry metricRegistry;
+  private final Set<NodeMetric> enabledMetrics;
+
+  public DefaultNodeMetricUpdater(
+      Node node, Set<NodeMetric> enabledMetrics, InternalDriverContext context) {
+    this.metricNamePrefix = buildPrefix(context.sessionName(), node.getConnectAddress());
+    this.enabledMetrics = enabledMetrics;
+    this.metricRegistry = context.metricRegistry();
+
+    if (enabledMetrics.contains(CoreNodeMetric.pooled_connection_count)) {
+      metricRegistry.register(
+          metricNamePrefix + CoreNodeMetric.pooled_connection_count.name(),
+          (Gauge<Integer>)
+              () -> {
+                ChannelPool pool = context.poolManager().getPools().get(node);
+                return (pool == null) ? 0 : pool.size();
+              });
+    }
+    if (enabledMetrics.contains(CoreNodeMetric.available_stream_count)) {
+      metricRegistry.register(
+          metricNamePrefix + CoreNodeMetric.available_stream_count,
+          (Gauge<Integer>)
+              () -> {
+                ChannelPool pool = context.poolManager().getPools().get(node);
+                return (pool == null) ? 0 : pool.getAvailableIds();
+              });
+    }
+  }
+
+  @Override
+  public void markMeter(NodeMetric metric) {
+    if (enabledMetrics.contains(metric)) {
+      metricRegistry.meter(metricNamePrefix + metric.name()).mark();","[{'comment': ""Hmm what if the metric in question is not a `Meter`? Would you create a new method here? I guess what i'm getting at is:  maybe `NodeMetric` should somehow convey the metric type, and not only the metric name, so that we can update the right metric without risking a mistake here? It could maybe even update itself, stg like:\r\n```\r\nNodeMetric metric = ...\r\nmetric.update(context); // get the registry and the session name from here and update the relevant metric\r\n```\r\n... in which case, the logic from `NodeMetricUpdater` could be merged into `NodeMetric`."", 'commenter': 'adutra'}, {'comment': ""Yes, there will be a method for each metric type: `markMeter`,  `updateTimer`, `incrementCounter`, etc.\r\nThe caller is expected to know what type of metric each enum constant corresponds to. A mistake is very unlikely because it's all driver code, and the same person will add a metric and add the code to update it. And if a mistake does happen the error should be pretty obvious.\r\n\r\nThe problem with something like `NodeMetric#update` is that `NodeMetric` couldn't be an enum anymore (because it would need a reference to the `Meter`, which is created at runtime, and also because there could be multiple instances for separate sessions). Then the list of metric names wouldn't be known statically, we'd probably need a `NodeMetricRegistry` in order to validate the configuration, etc. The (very low) risk of a mistake does not justify all that complexity IMO."", 'commenter': 'olim7t'}, {'comment': 'Sounds reasonable. ðŸ‘ ', 'commenter': 'adutra'}]"
937,core/src/test/resources/logback-test.xml,"@@ -25,4 +25,5 @@
         <appender-ref ref=""STDOUT""/>
     </root>
     <logger name=""com.datastax.oss.driver"" level= ""${driverLevel:-WARN}""/>
+    <logger name=""Tmp"" level= ""DEBUG""/>","[{'comment': 'What is this logger for?', 'commenter': 'adutra'}, {'comment': ""Hehe, that's an accidental commit, fixing."", 'commenter': 'olim7t'}]"
937,pom.xml,"@@ -43,6 +43,7 @@
     <format.validateOnly>true</format.validateOnly>
     <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
     <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>
+    <metrics.version>4.0.0</metrics.version>","[{'comment': '4.0.2 is the latest version.', 'commenter': 'adutra'}, {'comment': ':+1: Fixed', 'commenter': 'olim7t'}]"
937,core/src/main/java/com/datastax/oss/driver/api/core/config/CoreDriverOption.java,"@@ -108,6 +108,9 @@
   TIMESTAMP_GENERATOR_DRIFT_WARNING_INTERVAL(
       ""request.timestamp-generator.drift-warning.interval"", false),
 
+  METRICS_SESSION_ENABLED(""metrics.session.enabled"", false),","[{'comment': 'ðŸ’¯ for separating session and node level metrics!  ', 'commenter': 'tolbertam'}]"
937,core/src/main/java/com/datastax/oss/driver/api/core/metrics/NodeMetric.java,"@@ -0,0 +1,46 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core.metrics;
+
+import com.datastax.oss.driver.api.core.session.Session;
+import java.net.InetAddress;
+
+/**","[{'comment': 'Excellent docs ðŸ‘ ', 'commenter': 'tolbertam'}]"
937,core/src/main/resources/reference.conf,"@@ -550,6 +550,21 @@ datastax-java-driver {
     token-map.enabled = true
   }
 
+  metrics {","[{'comment': ""I think it's cool that you can enable/disable specific metrics.  If you have a metrics db (graphite, influx, etc.) that a lot of clients are feeding into, it's pretty easy to overwhelm it, so to be able to be selective about what you record is nice."", 'commenter': 'tolbertam'}]"
937,core/src/main/java/com/datastax/oss/driver/internal/core/metrics/HdrReservoir.java,"@@ -0,0 +1,240 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.metrics;
+
+import com.codahale.metrics.Reservoir;
+import com.codahale.metrics.Snapshot;
+import java.io.OutputStream;
+import java.io.OutputStreamWriter;
+import java.io.PrintWriter;
+import java.nio.charset.StandardCharsets;
+import java.time.Duration;
+import java.util.concurrent.locks.ReadWriteLock;
+import java.util.concurrent.locks.ReentrantReadWriteLock;
+import org.HdrHistogram.Histogram;
+import org.HdrHistogram.HistogramIterationValue;
+import org.HdrHistogram.Recorder;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * A reservoir implementation backed by the HdrHistogram library.
+ *
+ * <p>It uses a {@link Recorder} to capture snapshots at a configurable interval: calls to {@link
+ * #update(long)} are recorded in a ""live"" histogram, while {@link #getSnapshot()} is based on a
+ * ""cached"", read-only histogram. Each time the cached histogram becomes older than the interval,
+ * the two histograms are switched (therefore statistics won't be available during the first
+ * interval after initialization, since we don't have a cached histogram yet).
+ *
+ * @see <a href=""http://hdrhistogram.github.io/HdrHistogram/"">HdrHistogram</a>
+ */
+public class HdrReservoir implements Reservoir {
+
+  private static final Logger LOG = LoggerFactory.getLogger(HdrReservoir.class);
+
+  private final String logPrefix;
+  private final Recorder recorder;
+  private final long refreshIntervalNanos;
+
+  // The lock only orchestrates `getSnapshot()` calls; `update()` is fed directly to the recorder,
+  // which is lock-free. Read operations are comparatively rare, so locking is not a bottleneck.
+  private final ReadWriteLock cacheLock = new ReentrantReadWriteLock();
+  private Histogram cachedHistogram; // Guarded by cacheLock
+  private long cachedHistogramTimestampNanos; // Guarded by cacheLock
+  private Snapshot cachedSnapshot; // Guarded by cacheLock
+
+  public HdrReservoir(
+      Duration highestTrackableLatency,
+      int numberOfSignificantValueDigits,
+      Duration refreshInterval,
+      String logPrefix) {
+    this.logPrefix = logPrefix;
+    this.recorder = new Recorder(highestTrackableLatency.toNanos(), numberOfSignificantValueDigits);
+    this.refreshIntervalNanos = refreshInterval.toNanos();
+    this.cachedHistogramTimestampNanos = System.nanoTime();
+    this.cachedSnapshot = EMPTY_SNAPSHOT;
+  }
+
+  @Override
+  public void update(long value) {
+    try {
+      recorder.recordValue(value);
+    } catch (ArrayIndexOutOfBoundsException e) {
+      LOG.warn(""[{}] Recorded value ({}) is out of bounds, discarding"", logPrefix, value);
+    }
+  }
+
+  /**
+   * <em>Not implemented</em>.
+   *
+   * <p>This reservoir implementation is intended for use with a {@link
+   * com.codahale.metrics.Histogram}, which doesn't use this method.
+   */
+  @Override
+  public int size() {
+    throw new UnsupportedOperationException(""HdrReservoir does not implement size()"");
+  }
+
+  @Override
+  public Snapshot getSnapshot() {
+    long now = System.nanoTime();
+
+    cacheLock.readLock().lock();
+    try {
+      if (now < cachedHistogramTimestampNanos + refreshIntervalNanos) {","[{'comment': 'This can overflow so I think the recommended approach is to use `(now - cachedHistogramTimestampNanos) > refreshIntervalNanos`', 'commenter': 'adutra'}]"
937,core/src/main/java/com/datastax/oss/driver/internal/core/metrics/HdrReservoir.java,"@@ -0,0 +1,240 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.metrics;
+
+import com.codahale.metrics.Reservoir;
+import com.codahale.metrics.Snapshot;
+import java.io.OutputStream;
+import java.io.OutputStreamWriter;
+import java.io.PrintWriter;
+import java.nio.charset.StandardCharsets;
+import java.time.Duration;
+import java.util.concurrent.locks.ReadWriteLock;
+import java.util.concurrent.locks.ReentrantReadWriteLock;
+import org.HdrHistogram.Histogram;
+import org.HdrHistogram.HistogramIterationValue;
+import org.HdrHistogram.Recorder;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * A reservoir implementation backed by the HdrHistogram library.
+ *
+ * <p>It uses a {@link Recorder} to capture snapshots at a configurable interval: calls to {@link
+ * #update(long)} are recorded in a ""live"" histogram, while {@link #getSnapshot()} is based on a
+ * ""cached"", read-only histogram. Each time the cached histogram becomes older than the interval,
+ * the two histograms are switched (therefore statistics won't be available during the first
+ * interval after initialization, since we don't have a cached histogram yet).
+ *
+ * @see <a href=""http://hdrhistogram.github.io/HdrHistogram/"">HdrHistogram</a>
+ */
+public class HdrReservoir implements Reservoir {
+
+  private static final Logger LOG = LoggerFactory.getLogger(HdrReservoir.class);
+
+  private final String logPrefix;
+  private final Recorder recorder;
+  private final long refreshIntervalNanos;
+
+  // The lock only orchestrates `getSnapshot()` calls; `update()` is fed directly to the recorder,
+  // which is lock-free. Read operations are comparatively rare, so locking is not a bottleneck.
+  private final ReadWriteLock cacheLock = new ReentrantReadWriteLock();
+  private Histogram cachedHistogram; // Guarded by cacheLock
+  private long cachedHistogramTimestampNanos; // Guarded by cacheLock
+  private Snapshot cachedSnapshot; // Guarded by cacheLock
+
+  public HdrReservoir(
+      Duration highestTrackableLatency,
+      int numberOfSignificantValueDigits,
+      Duration refreshInterval,
+      String logPrefix) {
+    this.logPrefix = logPrefix;
+    this.recorder = new Recorder(highestTrackableLatency.toNanos(), numberOfSignificantValueDigits);
+    this.refreshIntervalNanos = refreshInterval.toNanos();
+    this.cachedHistogramTimestampNanos = System.nanoTime();
+    this.cachedSnapshot = EMPTY_SNAPSHOT;
+  }
+
+  @Override
+  public void update(long value) {
+    try {
+      recorder.recordValue(value);
+    } catch (ArrayIndexOutOfBoundsException e) {
+      LOG.warn(""[{}] Recorded value ({}) is out of bounds, discarding"", logPrefix, value);
+    }
+  }
+
+  /**
+   * <em>Not implemented</em>.
+   *
+   * <p>This reservoir implementation is intended for use with a {@link
+   * com.codahale.metrics.Histogram}, which doesn't use this method.
+   */
+  @Override
+  public int size() {
+    throw new UnsupportedOperationException(""HdrReservoir does not implement size()"");
+  }
+
+  @Override
+  public Snapshot getSnapshot() {
+    long now = System.nanoTime();
+
+    cacheLock.readLock().lock();
+    try {
+      if (now < cachedHistogramTimestampNanos + refreshIntervalNanos) {
+        return cachedSnapshot;
+      }
+    } finally {
+      cacheLock.readLock().unlock();
+    }
+
+    cacheLock.writeLock().lock();
+    try {
+      // Might have raced with another writer => re-check the timestamp
+      if (now >= cachedHistogramTimestampNanos + refreshIntervalNanos) {","[{'comment': 'Same here', 'commenter': 'adutra'}]"
937,core/src/main/resources/reference.conf,"@@ -553,16 +553,48 @@ datastax-java-driver {
   metrics {
     # The session-level metrics that are enabled. See CoreSessionMetric in the codebase for a
     # description of each.
-    session.enabled = [
-      cql_requests,
-    ]
+    session {
+      enabled = [
+        cql_requests,
+      ]
+
+      # Extra configuration (for the metrics that need it)
+      cql_requests {
+        # The largest latency that we expect to record (should be consistent with request.timeout).
+        #
+        # This is used to scale internal data structures. If a higher recording is encountered at
+        # runtime, it is discarded and a warning is logged.
+        highest_latency = 2 seconds","[{'comment': ""Shouldn't we default then to `${request.timeout}`?"", 'commenter': 'adutra'}, {'comment': ""ðŸ‘ \r\nI wonder if there is a risk of observing a higher value because of some processing overhead though. With significant_digits = 3 it would have to be a few milliseconds more, so I think we're safe. At worst it will come up as a warning and we can advise users to configure it a bit higher."", 'commenter': 'olim7t'}]"
937,core/src/main/java/com/datastax/oss/driver/internal/core/metrics/HdrReservoir.java,"@@ -0,0 +1,241 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.metrics;
+
+import com.codahale.metrics.Reservoir;
+import com.codahale.metrics.Snapshot;
+import java.io.OutputStream;
+import java.io.OutputStreamWriter;
+import java.io.PrintWriter;
+import java.nio.charset.StandardCharsets;
+import java.time.Duration;
+import java.util.concurrent.locks.ReadWriteLock;
+import java.util.concurrent.locks.ReentrantReadWriteLock;
+import org.HdrHistogram.Histogram;
+import org.HdrHistogram.HistogramIterationValue;
+import org.HdrHistogram.Recorder;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * A reservoir implementation backed by the HdrHistogram library.
+ *
+ * <p>It uses a {@link Recorder} to capture snapshots at a configurable interval: calls to {@link
+ * #update(long)} are recorded in a ""live"" histogram, while {@link #getSnapshot()} is based on a
+ * ""cached"", read-only histogram. Each time the cached histogram becomes older than the interval,
+ * the two histograms are switched (therefore statistics won't be available during the first
+ * interval after initialization, since we don't have a cached histogram yet).
+ *
+ * @see <a href=""http://hdrhistogram.github.io/HdrHistogram/"">HdrHistogram</a>
+ */
+public class HdrReservoir implements Reservoir {
+
+  private static final Logger LOG = LoggerFactory.getLogger(HdrReservoir.class);
+
+  private final String logPrefix;
+  private final Recorder recorder;
+  private final long refreshIntervalNanos;
+
+  // The lock only orchestrates `getSnapshot()` calls; `update()` is fed directly to the recorder,
+  // which is lock-free. `getSnapshot()` calls are comparatively rare, so locking is not a
+  // bottleneck.
+  private final ReadWriteLock cacheLock = new ReentrantReadWriteLock();
+  private Histogram cachedHistogram; // Guarded by cacheLock
+  private long cachedHistogramTimestampNanos; // Guarded by cacheLock
+  private Snapshot cachedSnapshot; // Guarded by cacheLock
+
+  public HdrReservoir(
+      Duration highestTrackableLatency,
+      int numberOfSignificantValueDigits,
+      Duration refreshInterval,
+      String logPrefix) {
+    this.logPrefix = logPrefix;
+    this.recorder = new Recorder(highestTrackableLatency.toNanos(), numberOfSignificantValueDigits);
+    this.refreshIntervalNanos = refreshInterval.toNanos();
+    this.cachedHistogramTimestampNanos = System.nanoTime();
+    this.cachedSnapshot = EMPTY_SNAPSHOT;
+  }
+
+  @Override
+  public void update(long value) {
+    try {
+      recorder.recordValue(value);
+    } catch (ArrayIndexOutOfBoundsException e) {
+      LOG.warn(""[{}] Recorded value ({}) is out of bounds, discarding"", logPrefix, value);
+    }
+  }
+
+  /**
+   * <em>Not implemented</em>.
+   *
+   * <p>This reservoir implementation is intended for use with a {@link
+   * com.codahale.metrics.Histogram}, which doesn't use this method.
+   */
+  @Override
+  public int size() {
+    throw new UnsupportedOperationException(""HdrReservoir does not implement size()"");
+  }
+
+  @Override
+  public Snapshot getSnapshot() {","[{'comment': ""Since live and cached are only switched out on call to `getSnapshot` is there a risk that there can be more data in live and the cached is extra stale if `getSnapshot` is called on intervals > `refreshInterval`?  Since the refreshInterval is configurable I think it's fine, since we can just tell users to set it to the same duration they are using in their Reporters."", 'commenter': 'tolbertam'}]"
937,core/src/main/java/com/datastax/oss/driver/internal/core/cql/CqlRequestHandlerBase.java,"@@ -304,6 +270,12 @@ private void setFinalResult(
           Conversions.toResultSet(resultMessage, executionInfo, session, context);
       if (result.complete(resultSet)) {
         cancelScheduledTasks();
+        session
+            .getMetricUpdater()
+            .updateTimer(
+                CoreSessionMetric.cql_requests,
+                System.nanoTime() - startTimeNanos,","[{'comment': ""Is the reason `nanoTime` is used instead of `Timer.Context` because we can't be sure that a `Timer` is actually in use as metrics can be disabled?"", 'commenter': 'tolbertam'}, {'comment': 'Yes, it is to avoid the extra object allocation, especially if the metric is disabled.', 'commenter': 'olim7t'}]"
937,core/src/main/java/com/datastax/oss/driver/internal/core/metrics/DefaultNodeMetricUpdater.java,"@@ -0,0 +1,126 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.metrics;
+
+import com.codahale.metrics.Gauge;
+import com.codahale.metrics.Timer;
+import com.datastax.oss.driver.api.core.config.CoreDriverOption;
+import com.datastax.oss.driver.api.core.config.DriverConfigProfile;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import com.datastax.oss.driver.api.core.metrics.CoreNodeMetric;
+import com.datastax.oss.driver.api.core.metrics.NodeMetric;
+import com.datastax.oss.driver.internal.core.context.InternalDriverContext;
+import com.datastax.oss.driver.internal.core.pool.ChannelPool;
+import java.net.Inet4Address;
+import java.net.Inet6Address;
+import java.net.InetAddress;
+import java.net.InetSocketAddress;
+import java.time.Duration;
+import java.util.Set;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class DefaultNodeMetricUpdater extends MetricUpdaterBase<NodeMetric>
+    implements NodeMetricUpdater {
+
+  private static final Logger LOG = LoggerFactory.getLogger(DefaultNodeMetricUpdater.class);
+
+  private final String metricNamePrefix;
+
+  public DefaultNodeMetricUpdater(","[{'comment': ""I noticed that a lot of metrics were not showing up, it looks like its because they aren't being registered here.  I.E. at the node level I just see `pooled_connection_count`, `available_stream_count` and `cql_messages`, but there's a lot more in `CoreNodeMetric`."", 'commenter': 'tolbertam'}]"
937,core/src/main/java/com/datastax/oss/driver/internal/core/metrics/DefaultSessionMetricUpdater.java,"@@ -0,0 +1,81 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.metrics;
+
+import com.codahale.metrics.Timer;
+import com.datastax.oss.driver.api.core.config.CoreDriverOption;
+import com.datastax.oss.driver.api.core.config.DriverConfigProfile;
+import com.datastax.oss.driver.api.core.metrics.CoreSessionMetric;
+import com.datastax.oss.driver.api.core.metrics.SessionMetric;
+import com.datastax.oss.driver.internal.core.context.InternalDriverContext;
+import java.time.Duration;
+import java.util.Set;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class DefaultSessionMetricUpdater extends MetricUpdaterBase<SessionMetric>
+    implements SessionMetricUpdater {
+
+  private static final Logger LOG = LoggerFactory.getLogger(DefaultSessionMetricUpdater.class);
+
+  private final String metricNamePrefix;
+
+  public DefaultSessionMetricUpdater(
+      Set<SessionMetric> enabledMetrics, InternalDriverContext context) {","[{'comment': 'Missing registration of `cql_client_timeouts`', 'commenter': 'tolbertam'}]"
937,core/src/main/resources/reference.conf,"@@ -550,6 +566,183 @@ datastax-java-driver {
     token-map.enabled = true
   }
 
+  metrics {
+    # The session-level metrics (all disabled by default).
+    session {
+      enabled = [
+        # The throughput and latency percentiles of CQL requests (exposed as a Timer).
+        #
+        # This corresponds to the overall duration of the session.execute() call, including any
+        # retry.
+        // cql_requests,
+
+        # The number of CQL requests that timed out -- that is, the session.execute() call failed
+        # with a DriverTimeoutException (exposed as a Counter).
+        // cql_client_timeouts,
+      ]
+
+      # Extra configuration (for the metrics that need it)
+      cql_requests {
+        # The largest latency that we expect to record.
+        #
+        # This should be slightly higher than request.timeout (in theory, readings can't be higher
+        # than the timeout, but there might be a small overhead due to internal scheduling).
+        #
+        # This is used to scale internal data structures. If a higher recording is encountered at
+        # runtime, it is discarded and a warning is logged.
+        highest_latency = 3 seconds
+
+        # The number of significant decimal digits to which internal structures will maintain value
+        # resolution and separation (for example, 3 means that recordings up to 1 second will be
+        # recorded with a resolution of 1 millisecond or better).
+        #
+        # This must be between 0 and 5. If the value is out of range, it defaults to 3 and a warning
+        # is logged.
+        significant_digits = 3
+
+        # The interval at which percentile data is refreshed.
+        #
+        # The driver records latency data in a ""live"" histogram, and serves results from a cached
+        # snapshot. Each time the snapshot gets older than the interval, the two are switched. Note
+        # that this switch happens upon fetching the metrics, so if you never fetch the recording
+        # interval might grow higher (that shouldn't be an issue in a production environment because
+        # you would typically have a metrics reporter that exports to a monitoring tool at a regular
+        # interval).
+        #
+        # In practice, this means that if you set this to 5 minutes, you're looking at data from a
+        # 5-minute interval in the past, that is at most 5 minutes old. If you fetch the metrics at
+        # a faster pace, you will observe the same data for 5 minutes until the interval expires.
+        #
+        # Note that this does not apply to the total count and rates (those are updated in real
+        # time).
+        refresh_interval = 5 minutes
+      }
+    }
+    # The node-level metrics (all disabled by default).
+    node {
+      enabled = [
+        # The number of connections open to this node for regular requests (exposed as a
+        # Gauge<Integer>).
+        #
+        # This does not include the control connection (which uses at most one extra connection to a
+        # random node in the cluster).
+        // pooled_connection_count,
+
+        # The number of <em>stream ids</em> available on the connections to this node (exposed as a
+        # Gauge<Integer>).
+        #
+        # Stream ids are used to multiplex requests on each connection, so this is an indication of
+        # how many more requests the node could handle concurrently before becoming saturated (note
+        # that this is a driver-side only consideration, there might be other limitations on the
+        # server that prevent reaching that theoretical limit).
+        // available_stream_count,
+
+        # The throughput and latency percentiles of individual CQL messages sent to this node as
+        # part of an overall request (exposed as a Timer).
+        #
+        # Note that this does not necessarily correspond to the overall duration of the
+        # session.execute() call, since the driver might query multiple nodes because of retries and
+        # speculative executions. Therefore a single ""request"" (as seen from a client of the driver)
+        # can be composed of more than one of the ""messages"" measured by this metric.
+        #
+        # Therefore this metric is intended as an insight into the performance of this particular
+        # node. For statistics on overall request completion, use the session-level cql_requests.
+        // cql_messages,
+
+        # The number of times the driver failed to write a request to this node (exposed as a
+        # Counter).
+        #
+        # In those case we know the request didn't even reach the coordinator, so they are retried
+        # on the next node automatically (without going through the retry policy).
+        // errors.write_errors,
+
+        # The number of times a request was aborted before the driver even received a response from
+        # this node (exposed as a Counter).
+        #
+        # See the API docs of RetryPolicy.onRequestAborted() for a description of the cases when
+        # this can happen.
+        // errors.aborted_requests,
+
+        # The number of times this node replied with a WRITE_TIMEOUT error (exposed as a Counter).
+        #
+        # Whether this error is rethrown directly to the client, rethrown or ignored is determined
+        # by the RetryPolicy.
+        // errors.write_timeouts,
+
+        # The number of times this node replied with a READ_TIMEOUT error (exposed as a Counter).
+        #
+        # Whether this error is rethrown directly to the client, rethrown or ignored is determined
+        # by the RetryPolicy.
+        // errors.read_timeouts,
+
+        # The number of times this node replied with an UNAVAILABLE error (exposed as a Counter).
+        #
+        # Whether this error is rethrown directly to the client, rethrown or ignored is determined
+        # by the RetryPolicy.
+        // errors.unavailables,
+
+        # The number of times this node replied with an error that doesn't fall under other errors.*
+        # metrics (exposed as a Counter).
+        // errors.others,
+
+        # The total number of errors on this node that caused the RetryPolicy to trigger a retry
+        # (exposed as a Counter).
+        #
+        # This is a sum of all the other retries.* metrics.
+        // retries.total,
+
+        # The number of errors on this node that caused the RetryPolicy to trigger a retry, broken
+        # down by error type (exposed as Counters).
+        // retries.aborted,
+        // retries.read_timeout,
+        // retries.write_timeout,
+        // retries.unavailable,
+        // retries.other,
+
+        # The total number of errors on this node that were ignored by the RetryPolicy (exposed as a
+        # Counter).
+        #
+        # This is a sum of all the other ignores.* metrics.
+        // ignores.total,
+
+        # The number of errors on this node that were ignored by the RetryPolicy, broken down by
+        # error type (exposed as Counters).
+        // ignores.aborted,
+        // ignores.read_timeout,
+        // ignores.write_timeout,
+        // ignores.unavailable,
+        // ignores.other,
+
+        # The number of speculative executions triggered by a slow response from this node (exposed
+        # as a Counter).
+        // speculative_executions,
+
+        # The number of errors encountered while trying to establish a connection to this node
+        # (exposed as a Counter).
+        #
+        # Connection errors are not a fatal issue for the driver, failed connections will be retried
+        # periodically according to the reconnection policy. You can choose whether or not to log
+        # those errors at WARN level with the connection.warn-on-init-error option.
+        #
+        # Authentication errors are not included in this counter, they are tracked separately in
+        # authentication_errors.
+        // connection_errors,","[{'comment': 'Should `connection_errors` and `authentication_errors` be under `errors.`?', 'commenter': 'tolbertam'}, {'comment': 'They relate to connections while the others are more specifically for requests, so maybe I should have two sub-categories.', 'commenter': 'olim7t'}]"
939,core/src/main/java/com/datastax/oss/driver/internal/core/data/DefaultUdtValue.java,"@@ -91,6 +92,39 @@ public ProtocolVersion protocolVersion() {
     return type.getAttachmentPoint().protocolVersion();
   }
 
+  @Override","[{'comment': 'It might be better to use IntelliJ generated `equals` and `hashCode` methods which are more straight forward and depends on `ByteBuffer.equals()`:\r\n\r\n```java\r\n  @Override\r\n  public boolean equals(Object o) {\r\n    if (this == o) return true;\r\n    if (o == null || getClass() != o.getClass()) return false;\r\n    DefaultUdtValue that = (DefaultUdtValue) o;\r\n    return Objects.equals(type, that.type) && Arrays.equals(values, that.values);\r\n  }\r\n\r\n  @Override\r\n  public int hashCode() {\r\n\r\n    int result = Objects.hash(type);\r\n    result = 31 * result + Arrays.hashCode(values);\r\n    return result;\r\n  }\r\n```\r\n', 'commenter': 'tolbertam'}, {'comment': ""ðŸ‘  for doing an instance equality check first (`this == o`).\r\n\r\nAbout comparing the byte buffers directly, there is a comment in the 3.x implementation that explains why we might want to avoid that (it might be a good idea to duplicate it here):\r\n\r\n> Deserializing each value is slightly inefficient, but comparing\r\n> the bytes could in theory be wrong (for varint for instance, 2 values\r\n> can have different binary representation but be the same value due to\r\n> leading zeros). So we don't take any risk.\r\n\r\nFinally, rather than comparing `getClass()`, we should do an `instanceof` check, but against the interface, not the implementation. That way if someone writes their own `UdtValue` implementation, ours compares to it nicely. See `DefaultUserDefinedType.equals` for example."", 'commenter': 'olim7t'}]"
939,core/src/main/java/com/datastax/oss/driver/internal/core/data/DefaultUdtValue.java,"@@ -91,6 +92,39 @@ public ProtocolVersion protocolVersion() {
     return type.getAttachmentPoint().protocolVersion();
   }
 
+  @Override
+  public boolean equals(Object o) {
+    if (!(o instanceof DefaultUdtValue)) return false;
+
+    DefaultUdtValue that = (DefaultUdtValue) o;
+    if (!type.equals(that.type)) return false;
+
+    if (size() != that.size()) return false;
+    if (size() > 0) {
+      for (int i = 0; i < size(); i++) {
+        if (this.getBytesUnsafe(i) == null || that.getBytesUnsafe(i) == null) {
+          if (this.getBytesUnsafe(i) != that.getBytesUnsafe(i)) {
+            return false;
+          }
+        } else if (!getBytesUnsafe(i).equals(that.getBytesUnsafe(i))) return false;
+      }
+    }
+    return true;
+  }
+
+  @Override
+  public int hashCode() {","[{'comment': ""I think this is the same as not implementing `hashCode` isn't it?"", 'commenter': 'tolbertam'}]"
939,core/src/test/java/com/datastax/oss/driver/internal/core/type/codec/UdtCodecTest.java,"@@ -128,6 +128,15 @@ public void should_decode_udt() {
     Mockito.verify(textCodec).decode(Bytes.fromHexString(""0x61""), ProtocolVersion.DEFAULT);
   }
 
+  @Test
+  public void should_evaluate_eqaulity() {","[{'comment': 'nit: eqaulity => equality', 'commenter': 'olim7t'}]"
939,core/src/main/java/com/datastax/oss/driver/internal/core/data/DefaultUdtValue.java,"@@ -91,6 +92,39 @@ public ProtocolVersion protocolVersion() {
     return type.getAttachmentPoint().protocolVersion();
   }
 
+  @Override
+  public boolean equals(Object o) {
+    if (!(o instanceof DefaultUdtValue)) return false;","[{'comment': 'nit: our style guide requires braces for if statements, even when there is only one statement. I used to omit braces too, but in hindsight I find it slightly less readable.', 'commenter': 'olim7t'}]"
940,core/src/main/java/com/datastax/oss/driver/internal/core/data/DefaultTupleValue.java,"@@ -93,6 +95,44 @@ private void readObject(ObjectInputStream stream) throws InvalidObjectException
     throw new InvalidObjectException(""Proxy required"");
   }
 
+  @Override
+  public boolean equals(Object o) {
+    if (this == o) return true;","[{'comment': ""Nit: can you add braces, even if there is only one statement? That's part of the Google style guide that we adhere to."", 'commenter': 'olim7t'}]"
940,core/src/main/java/com/datastax/oss/driver/internal/core/data/DefaultTupleValue.java,"@@ -93,6 +95,44 @@ private void readObject(ObjectInputStream stream) throws InvalidObjectException
     throw new InvalidObjectException(""Proxy required"");
   }
 
+  @Override
+  public boolean equals(Object o) {
+    if (this == o) return true;
+
+    if (o == null || !(o instanceof TupleValue)) return false;","[{'comment': ""The `instanceof` check fails if o is null, so you don't need the explicit null check."", 'commenter': 'olim7t'}]"
940,core/src/main/java/com/datastax/oss/driver/internal/core/data/DefaultTupleValue.java,"@@ -93,6 +95,44 @@ private void readObject(ObjectInputStream stream) throws InvalidObjectException
     throw new InvalidObjectException(""Proxy required"");
   }
 
+  @Override
+  public boolean equals(Object o) {
+    if (this == o) return true;
+
+    if (o == null || !(o instanceof TupleValue)) return false;
+    DefaultTupleValue that = (DefaultTupleValue) o;
+
+    if (this.protocolVersion() != that.protocolVersion()) return false;","[{'comment': ""In practice all instances of `ProtocolVersion` are enum values so this works, but since in theory there could be other implementations, this should use `equals()`.\r\n\r\nUnrelated: I've noticed that `hashCode` doesn't use the protocol version, but that's not necessarily an issue: it doesn't break the rule that equal objects must have the same hash code, and in practice different protocol versions will be rare, so I'm ok with it."", 'commenter': 'olim7t'}]"
940,core/src/main/java/com/datastax/oss/driver/internal/core/data/DefaultTupleValue.java,"@@ -93,6 +95,44 @@ private void readObject(ObjectInputStream stream) throws InvalidObjectException
     throw new InvalidObjectException(""Proxy required"");
   }
 
+  @Override
+  public boolean equals(Object o) {
+    if (this == o) return true;
+
+    if (o == null || !(o instanceof TupleValue)) return false;
+    DefaultTupleValue that = (DefaultTupleValue) o;
+
+    if (this.protocolVersion() != that.protocolVersion()) return false;
+
+    for (int i = 0; i < values.length; i++) {
+      DataType innerThisType = type.getComponentTypes().get(i);
+      DataType innerThatType = type.getComponentTypes().get(i);
+      if (!innerThisType.equals(innerThatType)) {
+        return false;
+      }
+      Object thisValue =
+          this.codecRegistry()
+              .codecFor(innerThatType)
+              .decode(this.values[i], this.protocolVersion());
+      Object thatValue =
+          that.codecRegistry()
+              .codecFor(innerThatType)
+              .decode(that.values[i], that.protocolVersion());
+      if (!((thisValue == thatValue) || (thisValue != null && thisValue.equals(thatValue)))) {","[{'comment': 'Java 8 has a method that handles the null check, so this can be simplified as `if (!Objects.equals(thisValue, thatValue))`.', 'commenter': 'olim7t'}]"
940,core/src/main/java/com/datastax/oss/driver/internal/core/data/DefaultTupleValue.java,"@@ -93,6 +95,44 @@ private void readObject(ObjectInputStream stream) throws InvalidObjectException
     throw new InvalidObjectException(""Proxy required"");
   }
 
+  @Override
+  public boolean equals(Object o) {
+    if (this == o) return true;
+
+    if (o == null || !(o instanceof TupleValue)) return false;
+    DefaultTupleValue that = (DefaultTupleValue) o;
+
+    if (this.protocolVersion() != that.protocolVersion()) return false;
+
+    for (int i = 0; i < values.length; i++) {
+      DataType innerThisType = type.getComponentTypes().get(i);
+      DataType innerThatType = type.getComponentTypes().get(i);
+      if (!innerThisType.equals(innerThatType)) {
+        return false;
+      }
+      Object thisValue =
+          this.codecRegistry()
+              .codecFor(innerThatType)
+              .decode(this.values[i], this.protocolVersion());","[{'comment': 'I wonder if we should handle potential errors here: this can happen if the tuple value contains custom types but became detached from the session, so the codec lookup will fail. Maybe we should log a warning and fall back to byte array comparisons in that specific case. Or is it better to simply let the method throw?', 'commenter': 'olim7t'}]"
940,core/src/main/java/com/datastax/oss/driver/internal/core/data/DefaultTupleValue.java,"@@ -93,6 +95,44 @@ private void readObject(ObjectInputStream stream) throws InvalidObjectException
     throw new InvalidObjectException(""Proxy required"");
   }
 
+  @Override
+  public boolean equals(Object o) {
+    if (this == o) return true;
+
+    if (o == null || !(o instanceof TupleValue)) return false;
+    DefaultTupleValue that = (DefaultTupleValue) o;
+
+    if (this.protocolVersion() != that.protocolVersion()) return false;
+
+    for (int i = 0; i < values.length; i++) {
+      DataType innerThisType = type.getComponentTypes().get(i);
+      DataType innerThatType = type.getComponentTypes().get(i);
+      if (!innerThisType.equals(innerThatType)) {
+        return false;
+      }
+      Object thisValue =
+          this.codecRegistry()
+              .codecFor(innerThatType)
+              .decode(this.values[i], this.protocolVersion());
+      Object thatValue =
+          that.codecRegistry()
+              .codecFor(innerThatType)
+              .decode(that.values[i], that.protocolVersion());
+      if (!((thisValue == thatValue) || (thisValue != null && thisValue.equals(thatValue)))) {
+        return false;
+      }
+    }
+    return true;
+  }
+
+  @Override
+  public int hashCode() {
+
+    int result = Objects.hash(type);
+    result = 31 * result + Arrays.hashCode(values);
+    return result;","[{'comment': 'If we decode in `equals()`, we must also decode here, otherwise it violates the rule that equal objects must have the same hash codes. For example two equal `varint` values that have different binary representations will be considered equal, but produce different hash codes.', 'commenter': 'olim7t'}]"
940,core/src/main/java/com/datastax/oss/driver/internal/core/data/DefaultTupleValue.java,"@@ -93,6 +98,78 @@ private void readObject(ObjectInputStream stream) throws InvalidObjectException
     throw new InvalidObjectException(""Proxy required"");
   }
 
+  @Override
+  public boolean equals(Object o) {
+    if (this == o) {
+      return true;
+    }
+
+    if (!(o instanceof TupleValue)) {
+      return false;
+    }
+    DefaultTupleValue that = (DefaultTupleValue) o;
+
+    if (!this.protocolVersion().equals(that.protocolVersion())) {
+      return false;
+    }
+
+    for (int i = 0; i < values.length; i++) {
+      DataType innerThisType = type.getComponentTypes().get(i);
+      DataType innerThatType = that.type.getComponentTypes().get(i);
+      if (!innerThisType.equals(innerThatType)) {
+        return false;
+      }
+      TypeCodec thatCodec = this.codecRegistry().codecFor(innerThatType);
+      TypeCodec thisCodec = that.codecRegistry().codecFor(innerThatType);
+      // If we can't find a needed codec, fallback on byte comparison
+      if (thatCodec == null || thisCodec == null) {
+        LOG.warn(
+            ""No codec found for tuple type ""
+                + innerThisType.toString()
+                + "" falling back to byte comparison"");
+        return Arrays.equals(values, that.values);
+      }
+      Object thisValue =
+          this.codecRegistry()
+              .codecFor(innerThisType)
+              .decode(this.values[i], this.protocolVersion());
+      Object thatValue =
+          that.codecRegistry()
+              .codecFor(innerThatType)
+              .decode(that.values[i], that.protocolVersion());
+      if (!Objects.equals(thisValue, thatValue)) {
+        return false;
+      }
+    }
+    return true;
+  }
+
+  @Override
+  public int hashCode() {
+
+    int result = Objects.hash(type);","[{'comment': 'Since only one object is getting hashed here, this can be replaced by `type.hashCode()`.\r\nSame at line 161.', 'commenter': 'olim7t'}]"
940,core/src/main/java/com/datastax/oss/driver/internal/core/data/DefaultTupleValue.java,"@@ -93,6 +98,78 @@ private void readObject(ObjectInputStream stream) throws InvalidObjectException
     throw new InvalidObjectException(""Proxy required"");
   }
 
+  @Override
+  public boolean equals(Object o) {
+    if (this == o) {
+      return true;
+    }
+
+    if (!(o instanceof TupleValue)) {
+      return false;
+    }
+    DefaultTupleValue that = (DefaultTupleValue) o;
+
+    if (!this.protocolVersion().equals(that.protocolVersion())) {
+      return false;
+    }
+
+    for (int i = 0; i < values.length; i++) {
+      DataType innerThisType = type.getComponentTypes().get(i);
+      DataType innerThatType = that.type.getComponentTypes().get(i);
+      if (!innerThisType.equals(innerThatType)) {
+        return false;
+      }
+      TypeCodec thatCodec = this.codecRegistry().codecFor(innerThatType);
+      TypeCodec thisCodec = that.codecRegistry().codecFor(innerThatType);
+      // If we can't find a needed codec, fallback on byte comparison
+      if (thatCodec == null || thisCodec == null) {
+        LOG.warn(
+            ""No codec found for tuple type ""
+                + innerThisType.toString()
+                + "" falling back to byte comparison"");
+        return Arrays.equals(values, that.values);
+      }
+      Object thisValue =
+          this.codecRegistry()
+              .codecFor(innerThisType)
+              .decode(this.values[i], this.protocolVersion());
+      Object thatValue =
+          that.codecRegistry()
+              .codecFor(innerThatType)
+              .decode(that.values[i], that.protocolVersion());
+      if (!Objects.equals(thisValue, thatValue)) {
+        return false;
+      }
+    }
+    return true;
+  }
+
+  @Override
+  public int hashCode() {
+
+    int result = Objects.hash(type);
+
+    for (int i = 0; i < values.length; i++) {
+      DataType innerThisType = type.getComponentTypes().get(i);
+      TypeCodec thisCodec = this.codecRegistry().codecFor(innerThisType);
+      if (thisCodec == null) {
+        LOG.warn(
+            ""No codec found for tuple type ""
+                + innerThisType.toString()
+                + "" falling bytes for hashcode"");","[{'comment': 'Nit: I think this should say ""falling *back to* bytes"".', 'commenter': 'olim7t'}]"
940,core/src/main/java/com/datastax/oss/driver/internal/core/data/DefaultTupleValue.java,"@@ -93,6 +98,78 @@ private void readObject(ObjectInputStream stream) throws InvalidObjectException
     throw new InvalidObjectException(""Proxy required"");
   }
 
+  @Override
+  public boolean equals(Object o) {
+    if (this == o) {
+      return true;
+    }
+
+    if (!(o instanceof TupleValue)) {
+      return false;
+    }
+    DefaultTupleValue that = (DefaultTupleValue) o;","[{'comment': 'Oops, I had missed this: it should cast to `TupleValue` here, and use the interface methods to access the fields. Otherwise it might throw if the other object is a custom implementation.', 'commenter': 'olim7t'}]"
955,faq/README.md,"@@ -258,10 +258,32 @@ If properly used, the following log message will be logged at INFO on startup:
 
 > Found Netty's native epoll transport in the classpath, but NIO was forced through the FORCE_NIO system property.
 
+### Why am I encountering `NoSuchMethodFoundException`, `NoClassDefFoundError`, or `VerifyError`s and how do I avoid them?
+
+Some monitoring and agent-based tools such as [DynaTrace] offer solutions that instrument the driver to observe
+and record useful metrics such as request rates, latencies and more.  It is possible that the tool
+you are using is not compatible with the version of the java driver you are using.  In this case, check to
+see if a newer version of that tool is available that works with this version of the driver.  If no such
+version is available, you may want to reach out to the maintainer of that tool to request that they provide
+an update with compatibility to the driver version you are using.
+
+Another possibility could simply be that another library you are using depends on a different version of the
+driver.  In this case, observe the stacktrace of the exception to see which library is attempting to use
+the driver.  To identify compatible versions, check that libraries' dependency on the driver to understand","[{'comment': 'nit: ""check that library\'s dependency""', 'commenter': 'adutra'}]"
955,faq/README.md,"@@ -258,10 +258,32 @@ If properly used, the following log message will be logged at INFO on startup:
 
 > Found Netty's native epoll transport in the classpath, but NIO was forced through the FORCE_NIO system property.
 
+### Why am I encountering `NoSuchMethodFoundException`, `NoClassDefFoundError`, or `VerifyError`s and how do I avoid them?
+
+Some monitoring and agent-based tools such as [DynaTrace] offer solutions that instrument the driver to observe
+and record useful metrics such as request rates, latencies and more.  It is possible that the tool
+you are using is not compatible with the version of the java driver you are using.  In this case, check to
+see if a newer version of that tool is available that works with this version of the driver.  If no such
+version is available, you may want to reach out to the maintainer of that tool to request that they provide
+an update with compatibility to the driver version you are using.
+
+Another possibility could simply be that another library you are using depends on a different version of the
+driver.  In this case, observe the stacktrace of the exception to see which library is attempting to use
+the driver.  To identify compatible versions, check that libraries' dependency on the driver to understand
+what version is compatible.
+
+Finally, it could be that an older or newer version of a library that the driver depends on, such as Netty
+or Guava, may be present in your application's classpath.  If you are using maven or another dependency
+management tool, the tool should offer a command, such as `mvn dependency:tree` to identify the dependecies
+in your project to help you understand the dependent versions across the various libraries you use in your
+project.  You may also want to evaluate your classpath to see if there are multiple jars present for a library,
+but with different versions, which could cause compatibility issues.","[{'comment': 'I think that here you could also mention the possibility of using the shaded jar, if the library causing problems is Netty.', 'commenter': 'adutra'}]"
955,faq/README.md,"@@ -258,10 +258,44 @@ If properly used, the following log message will be logged at INFO on startup:
 
 > Found Netty's native epoll transport in the classpath, but NIO was forced through the FORCE_NIO system property.
 
+### Why am I encountering `NoSuchMethodFoundException`, `NoClassDefFoundError`, or `VerifyError`s and how do I avoid them?
+
+Incompatibilities between the java driver and other libraries may cause these exceptions to surface in your
+application at runtime.  
+
+It could be that an older or newer version of a library that the driver depends on, such as Netty
+or Guava, may be present in your application's classpath.  If you are using maven or another dependency
+management tool, the tool should offer a command, such as `mvn dependency:tree` to identify the dependecies
+in your project to help you understand the dependent versions across the various libraries you use in your
+project.  You may also want to evaluate your classpath to see if there are multiple jars present for a library,
+but with different versions, which could cause compatibility issues.  In addition, consider evaluating
+using the logback logging framework, which provides the capability to include [packaging data] for classes
+in stack traces.
+
+For Netty in particular, the driver offers an alternative artifact that shades its netty dependency,","[{'comment': 'Nit: ""netty"" appears twice without capital N.', 'commenter': 'adutra'}, {'comment': 'Good catch, fixed!', 'commenter': 'tolbertam'}]"
955,faq/README.md,"@@ -258,10 +258,44 @@ If properly used, the following log message will be logged at INFO on startup:
 
 > Found Netty's native epoll transport in the classpath, but NIO was forced through the FORCE_NIO system property.
 
+### Why am I encountering `NoSuchMethodFoundException`, `NoClassDefFoundError`, or `VerifyError`s and how do I avoid them?
+
+Incompatibilities between the java driver and other libraries may cause these exceptions to surface in your
+application at runtime.  
+
+It could be that an older or newer version of a library that the driver depends on, such as Netty
+or Guava, may be present in your application's classpath.  If you are using maven or another dependency
+management tool, the tool should offer a command, such as `mvn dependency:tree` to identify the dependecies","[{'comment': 'typo: dependecies => dependencies', 'commenter': 'olim7t'}]"
955,faq/README.md,"@@ -258,10 +258,44 @@ If properly used, the following log message will be logged at INFO on startup:
 
 > Found Netty's native epoll transport in the classpath, but NIO was forced through the FORCE_NIO system property.
 
+### Why am I encountering `NoSuchMethodFoundException`, `NoClassDefFoundError`, or `VerifyError`s and how do I avoid them?
+
+Incompatibilities between the java driver and other libraries may cause these exceptions to surface in your
+application at runtime.  
+
+It could be that an older or newer version of a library that the driver depends on, such as Netty
+or Guava, may be present in your application's classpath.  If you are using maven or another dependency","[{'comment': 'nit: capitalize Maven?\r\n(same for jar=>JAR, Logback, Netty elsewhere in the paragraph)', 'commenter': 'olim7t'}]"
955,faq/README.md,"@@ -258,10 +258,44 @@ If properly used, the following log message will be logged at INFO on startup:
 
 > Found Netty's native epoll transport in the classpath, but NIO was forced through the FORCE_NIO system property.
 
+### Why am I encountering `NoSuchMethodFoundException`, `NoClassDefFoundError`, or `VerifyError`s and how do I avoid them?
+
+Incompatibilities between the java driver and other libraries may cause these exceptions to surface in your
+application at runtime.  
+
+It could be that an older or newer version of a library that the driver depends on, such as Netty
+or Guava, may be present in your application's classpath.  If you are using maven or another dependency
+management tool, the tool should offer a command, such as `mvn dependency:tree` to identify the dependecies
+in your project to help you understand the dependent versions across the various libraries you use in your
+project.  You may also want to evaluate your classpath to see if there are multiple jars present for a library,
+but with different versions, which could cause compatibility issues.  In addition, consider evaluating
+using the logback logging framework, which provides the capability to include [packaging data] for classes
+in stack traces.
+
+For Netty in particular, the driver offers an alternative artifact that shades its Netty dependency,
+allowing you to use newer or older versions of Netty in your application without impacting the driver.
+See [Using the shaded JAR] for more details.
+
+Another possibility could be that another library you are using depends on a different version of the
+driver.  In this case, observe the stacktrace of the exception to see which library is attempting to use
+the driver.  To identify compatible versions, check that library's dependency on the driver to understand
+what version is compatible.
+
+Finally, some monitoring and agent-based tools such as [DynaTrace] offer solutions that instrument the driver to
+observe and record useful metrics such as request rates, latencies and more.  It is possible that the tool
+you are using is not compatible with the version of the java driver you are using.  In this case, check to
+see if a newer version of that tool is available that works with this version of the driver.  If no such
+version is available, you may want to reach out to the maintainer of that tool to request that they provide
+an update with compatibility to the driver version you are using.","[{'comment': 'nit: a bit of repetition of ""you are using"" in this paragraph', 'commenter': 'olim7t'}]"
956,driver-core/src/test/java/com/datastax/driver/core/policies/TokenAwarePolicyTest.java,"@@ -255,58 +307,60 @@ public void should_use_other_nodes_when_replicas_having_token_are_down() {
                     .setRoutingKey(routingKey)
                     .setKeyspace(""keyspace"");
 
+            QueryTracker queryTracker = new QueryTracker();
             queryTracker.query(session, 10, statement);
 
-            // then: The node that is the primary for that key's hash is chosen.
-            queryTracker.assertQueried(sCluster, 1, 1, 0);
+            // then: primary replica is 4, secondary is 1; since the child policy returns [1,2,3,4], the","[{'comment': 'The figures changed because the child policy changed.', 'commenter': 'adutra'}]"
956,driver-core/src/main/java/com/datastax/driver/core/policies/TokenAwarePolicy.java,"@@ -23,31 +23,70 @@
 import java.util.*;
 
 /**
- * A wrapper load balancing policy that add token awareness to a child policy.
+ * A wrapper load balancing policy that adds token awareness to a child policy.
  * <p/>
  * This policy encapsulates another policy. The resulting policy works in
  * the following way:
  * <ul>
  * <li>the {@code distance} method is inherited from the child policy.</li>
- * <li>the iterator return by the {@code newQueryPlan} method will first
- * return the {@code LOCAL} replicas for the query (based on {@link Statement#getRoutingKey})
- * <i>if possible</i> (i.e. if the query {@code getRoutingKey} method
- * doesn't return {@code null} and if {@link Metadata#getReplicas}
- * returns a non empty set of replicas for that partition key). If no
- * local replica can be either found or successfully contacted, the rest
- * of the query plan will fallback to one of the child policy.</li>
+ * <li>the iterator returned by the {@code newQueryPlan} method will first
+ * return the {@link HostDistance#LOCAL LOCAL} replicas for the query
+ * <em>if possible</em> (i.e. if the query's
+ * {@link Statement#getRoutingKey(ProtocolVersion, CodecRegistry) routing key}
+ * is not {@code null} and if the
+ * {@link Metadata#getReplicas(String, ByteBuffer) set of replicas}
+ * for that partition key is not empty). If no local replica can be either found
+ * or successfully contacted, the rest of the query plan will fallback
+ * to the child policy's one.</li>
  * </ul>
+ * The exact order in which local replicas are returned is dictated by the
+ * {@link ReplicaOrdering strategy} provided at instantiation.
  * <p/>
- * Do note that only replica for which the child policy {@code distance}
- * method returns {@code HostDistance.LOCAL} will be considered having
+ * Do note that only replicas for which the child policy's
+ * {@link LoadBalancingPolicy#distance(Host) distance}
+ * method returns {@link HostDistance#LOCAL LOCAL} will be considered having
  * priority. For example, if you wrap {@link DCAwareRoundRobinPolicy} with this
  * token aware policy, replicas from remote data centers may only be
- * returned after all the host of the local data center.
+ * returned after all the hosts of the local data center.
  */
 public class TokenAwarePolicy implements ChainableLoadBalancingPolicy {
 
+    /**
+     * Strategies for replica ordering.
+     */
+    public enum ReplicaOrdering {
+
+        /**
+         * Order replicas by token ring topology, i.e. always return the ""primary"" replica first,
+         * then the second, etc., according to the placement of replicas around the token ring.
+         * <p/>
+         * This strategy is the only one guaranteed to order replicas in a deterministic and
+         * constant way. This increases the effectiveness of server-side row caching (especially
+         * at consistency level ONE), but may create hotspots, since the primary replica is always","[{'comment': 'nit: Would change `but may create hotspots` to `but is more heavily impacted by hotspots`.', 'commenter': 'tolbertam'}]"
956,driver-core/src/main/java/com/datastax/driver/core/policies/TokenAwarePolicy.java,"@@ -56,28 +95,36 @@
      * Creates a new {@code TokenAware} policy.
      *
      * @param childPolicy     the load balancing policy to wrap with token awareness.
-     * @param shuffleReplicas whether to shuffle the replicas returned by {@code getRoutingKey}.
-     *                        Note that setting this parameter to {@code true} might decrease the
-     *                        effectiveness of caching (especially at consistency level ONE), since
-     *                        the same row will be retrieved from any replica (instead of only the
-     *                        ""primary"" replica without shuffling).
-     *                        On the other hand, shuffling will better distribute writes, and can
-     *                        alleviate hotspots caused by ""fat"" partitions.
+     * @param replicaOrdering the strategy to use to order replicas.
      */
-    public TokenAwarePolicy(LoadBalancingPolicy childPolicy, boolean shuffleReplicas) {
+    public TokenAwarePolicy(LoadBalancingPolicy childPolicy, ReplicaOrdering replicaOrdering) {
         this.childPolicy = childPolicy;
-        this.shuffleReplicas = shuffleReplicas;
+        this.replicaOrdering = replicaOrdering;
     }
 
     /**
-     * Creates a new {@code TokenAware} policy with shuffling of replicas.
+     * Creates a new {@code TokenAware} policy.
      *
-     * @param childPolicy the load balancing policy to wrap with token
-     *                    awareness.
-     * @see #TokenAwarePolicy(LoadBalancingPolicy, boolean)
+     * @param childPolicy     the load balancing policy to wrap with token awareness.
+     * @param shuffleReplicas whether or not to shuffle the replicas.
+     *                        If {@code true}, then the {@link ReplicaOrdering#RANDOM RANDOM} strategy will be used,
+     *                        otherwise the {@link ReplicaOrdering#TOPOLOGICAL TOPOLOGICAL} one will be used.
+     * @deprecated Use {@link #TokenAwarePolicy(LoadBalancingPolicy, ReplicaOrdering)} instead.
+     * This constructor will be removed in the next major release.
+     */
+    @SuppressWarnings(""DeprecatedIsStillUsed"")
+    @Deprecated
+    public TokenAwarePolicy(LoadBalancingPolicy childPolicy, boolean shuffleReplicas) {
+        this(childPolicy, shuffleReplicas ? ReplicaOrdering.RANDOM : ReplicaOrdering.TOPOLOGICAL);
+    }
+
+    /**
+     * Creates a new {@code TokenAware} policy with {@link ReplicaOrdering#TOPOLOGICAL TOPOLOGICAL} replica ordering.
+     *
+     * @param childPolicy the load balancing policy to wrap with token awareness.
      */
     public TokenAwarePolicy(LoadBalancingPolicy childPolicy) {
-        this(childPolicy, true);
+        this(childPolicy, ReplicaOrdering.TOPOLOGICAL);","[{'comment': ""For consistency with the existing (3.4.0) behavior, shouldn't this be `ReplicaOrdering.RANDOM`?   Previously we shuffled by default, this would change it so it no longer does."", 'commenter': 'tolbertam'}]"
956,driver-core/src/main/java/com/datastax/driver/core/policies/TokenAwarePolicy.java,"@@ -23,31 +23,70 @@
 import java.util.*;
 
 /**
- * A wrapper load balancing policy that add token awareness to a child policy.
+ * A wrapper load balancing policy that adds token awareness to a child policy.
  * <p/>
  * This policy encapsulates another policy. The resulting policy works in
  * the following way:
  * <ul>
  * <li>the {@code distance} method is inherited from the child policy.</li>
- * <li>the iterator return by the {@code newQueryPlan} method will first
- * return the {@code LOCAL} replicas for the query (based on {@link Statement#getRoutingKey})
- * <i>if possible</i> (i.e. if the query {@code getRoutingKey} method
- * doesn't return {@code null} and if {@link Metadata#getReplicas}
- * returns a non empty set of replicas for that partition key). If no
- * local replica can be either found or successfully contacted, the rest
- * of the query plan will fallback to one of the child policy.</li>
+ * <li>the iterator returned by the {@code newQueryPlan} method will first
+ * return the {@link HostDistance#LOCAL LOCAL} replicas for the query
+ * <em>if possible</em> (i.e. if the query's
+ * {@link Statement#getRoutingKey(ProtocolVersion, CodecRegistry) routing key}
+ * is not {@code null} and if the
+ * {@link Metadata#getReplicas(String, ByteBuffer) set of replicas}
+ * for that partition key is not empty). If no local replica can be either found
+ * or successfully contacted, the rest of the query plan will fallback
+ * to the child policy's one.</li>
  * </ul>
+ * The exact order in which local replicas are returned is dictated by the
+ * {@link ReplicaOrdering strategy} provided at instantiation.
  * <p/>
- * Do note that only replica for which the child policy {@code distance}
- * method returns {@code HostDistance.LOCAL} will be considered having
+ * Do note that only replicas for which the child policy's
+ * {@link LoadBalancingPolicy#distance(Host) distance}
+ * method returns {@link HostDistance#LOCAL LOCAL} will be considered having
  * priority. For example, if you wrap {@link DCAwareRoundRobinPolicy} with this
  * token aware policy, replicas from remote data centers may only be
- * returned after all the host of the local data center.
+ * returned after all the hosts of the local data center.
  */
 public class TokenAwarePolicy implements ChainableLoadBalancingPolicy {
 
+    /**
+     * Strategies for replica ordering.
+     */
+    public enum ReplicaOrdering {","[{'comment': 'Big ðŸ‘ on using the enum instead of having 2 booleans (shuffle, and whether to regard child policy).', 'commenter': 'tolbertam'}]"
956,driver-core/src/main/java/com/datastax/driver/core/policies/TokenAwarePolicy.java,"@@ -94,26 +141,23 @@ public void init(Cluster cluster, Collection<Host> hosts) {
     }
 
     /**
-     * Return the HostDistance for the provided host.
-     *
-     * @param host the host of which to return the distance of.
-     * @return the HostDistance to {@code host} as returned by the wrapped policy.
+     * {@inheritDoc}
+     * <p/>
+     * This implementation always returns distances as reported by the wrapped policy.
      */
     @Override
     public HostDistance distance(Host host) {
         return childPolicy.distance(host);
     }
 
     /**
-     * Returns the hosts to use for a new query.
+     * {@inheritDoc}
      * <p/>
-     * The returned plan will first return replicas (whose {@code HostDistance}
-     * for the child policy is {@code LOCAL}) for the query if it can determine
-     * them (i.e. mainly if {@code statement.getRoutingKey()} is not {@code null}).
-     * Following what it will return the plan of the child policy.
-     *
-     * @param statement the query for which to build the plan.
-     * @return the new query plan.
+     * The returned plan will first return local replicas for the query (i.e.
+     * replicas whose {@link HostDistance distance} according to the child policy is {@code LOCAL}),
+     * if it can determine them (i.e. mainly if the statement's
+     * {@link Statement#getRoutingKey(ProtocolVersion, CodecRegistry)} routing key}
+     * is not {@code null}); following what it will return the rest of the child policy's original query plan.","[{'comment': 'It would also be good to add that the replicas will be ordered by the configured `ReplicaOrdering`.', 'commenter': 'tolbertam'}]"
956,driver-core/src/test/java/com/datastax/driver/core/policies/TokenAwarePolicyTest.java,"@@ -58,20 +125,14 @@ public void setUp() {
      * @test_category load_balancing:token_aware
      */
     @Test(groups = ""short"", dataProvider = ""shuffleProvider"")
-    public void should_shuffle_replicas_based_on_configuration(Boolean shuffleReplicas) {
+    public void should_order_replicas_based_on_configuration(TokenAwarePolicy.ReplicaOrdering ordering) {","[{'comment': 'Nice ðŸ‘ ', 'commenter': 'tolbertam'}]"
959,pom.xml,"@@ -45,25 +45,25 @@
     <properties>
         <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
         <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>
-        <cassandra.version>3.11.1</cassandra.version>
+        <cassandra.version>3.11.2</cassandra.version>
         <java.version>1.6</java.version>
         <log4j.version>1.2.17</log4j.version>
         <slf4j.version>1.7.25</slf4j.version>
         <slf4j-log4j12.version>1.7.25</slf4j-log4j12.version>
         <guava.version>19.0</guava.version>
-        <netty.version>4.0.47.Final</netty.version>
+        <netty.version>4.0.56.Final</netty.version>
         <metrics.version>3.2.2</metrics.version>
-        <snappy.version>1.1.2.6</snappy.version>
+        <snappy.version>1.1.7.1</snappy.version>
         <lz4.version>1.3.0</lz4.version>","[{'comment': 'LZ4 is now available under new GAV coordinates:\r\n\r\n```\r\n<dependency>\r\n    <groupId>org.lz4</groupId>\r\n    <artifactId>lz4-java</artifactId>\r\n    <version>1.4.1</version>\r\n</dependency>\r\n```\r\nThe GAV changed but not the classes, and the bytecode level is still Java 6. I was even able to run the OSGi test with LZ4 so I think we could upgrade the dependency.', 'commenter': 'adutra'}, {'comment': 'Good catch, they must not have published the new coordinates to the existing artifacts.  Will update.', 'commenter': 'tolbertam'}]"
959,pom.xml,"@@ -45,25 +45,25 @@
     <properties>
         <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
         <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>
-        <cassandra.version>3.11.1</cassandra.version>
+        <cassandra.version>3.11.2</cassandra.version>
         <java.version>1.6</java.version>
         <log4j.version>1.2.17</log4j.version>
         <slf4j.version>1.7.25</slf4j.version>
         <slf4j-log4j12.version>1.7.25</slf4j-log4j12.version>
         <guava.version>19.0</guava.version>
-        <netty.version>4.0.47.Final</netty.version>
+        <netty.version>4.0.56.Final</netty.version>","[{'comment': ""I'm seeing an error with SSL tests on both Mac and Windows:\r\n```\r\nCaused by: java.lang.IllegalArgumentException: Failed to load any of the given libraries: [netty_tcnative_windows_x86_64, netty_tcnative_x86_64, netty_tcnative]\r\n\tat io.netty.util.internal.NativeLibraryLoader.loadFirstAvailable(NativeLibraryLoader.java:93)\r\n\t... 41 more\r\n\tSuppressed: java.lang.UnsatisfiedLinkError: could not load a native library: netty_tcnative_windows_x86_64\r\n\t\tat io.netty.util.internal.NativeLibraryLoader.load(NativeLibraryLoader.java:205)\r\n\t\tat io.netty.util.internal.NativeLibraryLoader.loadFirstAvailable(NativeLibraryLoader.java:85)\r\n\t\t... 43 more\r\n\tCaused by: java.io.FileNotFoundException: META-INF/native/netty_tcnative_windows_x86_64.dll\r\n\t\tat io.netty.util.internal.NativeLibraryLoader.load(NativeLibraryLoader.java:161)\r\n```\r\nUpgrading netty-tcnative to 2.0.7.Final solves the problem."", 'commenter': 'adutra'}, {'comment': ""D'oh yes, good catch!  I forgot to upgrade tcnative.  Will take care of that and also update the relevant docs as well."", 'commenter': 'tolbertam'}]"
964,core/src/main/java/com/datastax/oss/driver/api/core/session/SessionBuilder.java,"@@ -200,6 +199,17 @@ protected DriverContext buildContext(
     return new DefaultDriverContext(configLoader, typeCodecs);
   }
 
+  /**
+   * This <b>must</b> return an instance of {@code DefaultSession} (it's not expressed directly in
+   * the signature to avoid leaking that type through the protected API).
+   */
+  protected Session buildDefaultSession(","[{'comment': 'I saw this warning in the comments of `buildContext` and had to do the same for this method. I think it is a code smell that indicates that we should rather extract an interface from this class and keep the implementation details in an internal class.', 'commenter': 'adutra'}]"
964,core/src/main/java/com/datastax/oss/driver/internal/core/session/DefaultSession.java,"@@ -74,14 +79,8 @@
 public class DefaultSession implements CqlSession {
 
   private static final Logger LOG = LoggerFactory.getLogger(DefaultSession.class);
-
-  public static CompletionStage<CqlSession> init(","[{'comment': ""This factory method wasn't really bringing any value, this class is internal so the constructor can be public."", 'commenter': 'adutra'}]"
964,core/src/main/java/com/datastax/oss/driver/internal/core/session/DefaultSession.java,"@@ -107,11 +106,50 @@ private DefaultSession(
     this.metricUpdater = context.metricUpdaterFactory().newSessionUpdater();
   }
 
-  private CompletionStage<CqlSession> init(CqlIdentifier keyspace) {
+  public CompletionStage<CqlSession> init(CqlIdentifier keyspace) {
+    DriverConfigProfile defaultConfig = context.config().getDefaultProfile();
+    if (defaultConfig.isDefined(DefaultDriverOption.STARTUP_CUSTOM_BANNER)) {
+      printCustomBanner(defaultConfig.getString(DefaultDriverOption.STARTUP_CUSTOM_BANNER));
+    }
+    if (defaultConfig.getBoolean(DefaultDriverOption.STARTUP_PRINT_BASIC_INFO)) {
+      printBasicStartupInfo();
+    }
     RunOrSchedule.on(adminExecutor, () -> singleThreaded.init(keyspace));
     return singleThreaded.initFuture;
   }
 
+  protected void printCustomBanner(String banner) {
+    STARTUP_LOG.info(banner);
+  }
+
+  protected void printBasicStartupInfo() {
+    try {
+      DriverInfo driverInfo = getDriverInfo();
+      int processId = -1;
+      if (Native.isGetProcessIdAvailable()) {
+        processId = Native.getProcessId();
+      }
+      long start = System.nanoTime();
+      STARTUP_LOG.info(""[{}] {} initializing (PID: {})"", logPrefix, driverInfo, processId);
+      singleThreaded.initFuture.thenRun(
+          () -> {
+            Duration elapsed =
+                Duration.ofMillis(TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start));
+            String elapsedStr =","[{'comment': ""Just a pretty-print for durations. It is a common enough problem that I think we should have an utility method somewhere to print durations in good ol' English. Currently our log messages print durations in ISO-8601 format, which is not very user-friendly â€“ but that's a different issue."", 'commenter': 'adutra'}, {'comment': 'I wrote `NanoTime.format` to that effect for token map logs.', 'commenter': 'olim7t'}]"
964,core/src/main/resources/reference.conf,"@@ -102,6 +102,32 @@ datastax-java-driver {
   # and metrics.
   // session-name = my_session
 
+  # A set of customizable actions that are performed when the driver starts up, i.e.,
+  # each time a new session is initialized.
+  # Actions in this section that involve printing log messages will always use the special logger
+  # 'com.datastax.oss.driver.api.core.Startup' at level INFO.
+  startup {
+
+    # Whether or not to print basic information about the initialization process,
+    # such as the driver name, its version number, and the process PID, if available.
+    print-basic-info = true
+
+    # An optional custom banner to be printed at startup. This can be useful to differentiate
+    # between two applications using the driver.
+    #
+    # Tip 1: this option works best with multi-line strings, as in the example below.
+    #
+    # Tip 2: you can include system properties, environment variables and/or references to any
+    # other configuration option, as in the example below. Beware that any reference must point
+    # to an existing value, otherwise an error will be raised.
+    //custom-banner = """"""","[{'comment': 'This really shows all the potential of TypeSafe Config imo.', 'commenter': 'adutra'}, {'comment': 'ðŸ‘ Nice, especially considering that in a typical deployment this can be changed without recompiling.', 'commenter': 'olim7t'}]"
964,core/src/main/java/com/datastax/oss/driver/internal/core/DefaultDriverInfo.java,"@@ -0,0 +1,83 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core;
+
+import com.datastax.oss.driver.api.core.DriverInfo;
+import com.datastax.oss.driver.api.core.Version;
+import java.io.IOException;
+import java.io.InputStreamReader;
+import java.io.UncheckedIOException;
+import java.net.URL;
+import java.nio.charset.StandardCharsets;
+import java.util.Properties;
+
+public class DefaultDriverInfo implements DriverInfo {
+
+  public static DefaultDriverInfo buildFromResource(URL resource) {
+    // The resource is assumed to be a properties file, but
+    // encoded in UTF-8, not ISO-8859-1 as required by the Java specs,
+    // since our build tool (Maven) produces UTF-8-encoded resources.
+    try (InputStreamReader reader =
+        new InputStreamReader(resource.openStream(), StandardCharsets.UTF_8)) {
+      Properties props = new Properties();
+      props.load(reader);
+      String name = props.getProperty(""driver.name"");
+      String groupId = props.getProperty(""driver.groupId"");
+      String artifactId = props.getProperty(""driver.artifactId"");
+      String version = props.getProperty(""driver.version"");
+      return new DefaultDriverInfo(name, groupId, artifactId, Version.parse(version));
+    } catch (IOException e) {
+      throw new UncheckedIOException(e);","[{'comment': ""Nice, didn't know this exception."", 'commenter': 'olim7t'}]"
964,core/src/main/java/com/datastax/oss/driver/api/core/Version.java,"@@ -23,24 +23,41 @@
 import java.util.regex.Pattern;
 
 /**
- * The version of a Cassandra release.
+ * A structured version number.
  *
- * <p>It is in the form X.Y.Z, with optional pre-release labels and build metadata.
+ * <p>Versions are expected to follow the general guidelines of<a
+ * href=""https://semver.org/"">semantic versioning</a>, but with a few relaxed rules.
+ *
+ * <p>The general form a version number is expected to be {@code X.Y[.Z]}, where X stands for the
+ * version's major number, Y for the minor number, and Z for an (optional) patch number.
+ *
+ * <p>It can also contain a few optional extensions: a 4th ""DSE patch"" number (specific to DSE
+ * releases); one or many pre-release labels; and one optional build label. See {@link
+ * #parse(String)} for a few examples.
+ *
+ * <p>This class can be used to parse versions of Apache CassandraÂ®, DataStax Enterprise (DSE), or
+ * the DataStax Java driver.
  *
  * <p>Version numbers compare the usual way, the major number (X) is compared first, then the minor
  * one (Y) and then the patch level one (Z). Lastly, versions with pre-release sorts before the
  * versions that don't have one, and labels are sorted alphabetically if necessary. Build metadata
- * are ignored for sorting versions.
+ * are ignored when sorting versions.
  */
-public class CassandraVersion implements Comparable<CassandraVersion> {
+public class Version implements Comparable<Version> {","[{'comment': ""Not a huge deal, but it would have been nice to isolate the name change in a dedicated commit. It's both trivial and the most impacting in terms of changed files, so it would make the other changes much easier to review.\r\n\r\nJust a thing to keep in mind for future refactorings like this."", 'commenter': 'olim7t'}]"
964,core/src/main/java/com/datastax/oss/driver/api/core/Version.java,"@@ -23,24 +23,41 @@
 import java.util.regex.Pattern;
 
 /**
- * The version of a Cassandra release.
+ * A structured version number.
  *
- * <p>It is in the form X.Y.Z, with optional pre-release labels and build metadata.
+ * <p>Versions are expected to follow the general guidelines of<a
+ * href=""https://semver.org/"">semantic versioning</a>, but with a few relaxed rules.
+ *
+ * <p>The general form a version number is expected to be {@code X.Y[.Z]}, where X stands for the
+ * version's major number, Y for the minor number, and Z for an (optional) patch number.
+ *
+ * <p>It can also contain a few optional extensions: a 4th ""DSE patch"" number (specific to DSE
+ * releases); one or many pre-release labels; and one optional build label. See {@link
+ * #parse(String)} for a few examples.
+ *
+ * <p>This class can be used to parse versions of Apache CassandraÂ®, DataStax Enterprise (DSE), or
+ * the DataStax Java driver.
  *
  * <p>Version numbers compare the usual way, the major number (X) is compared first, then the minor
  * one (Y) and then the patch level one (Z). Lastly, versions with pre-release sorts before the
  * versions that don't have one, and labels are sorted alphabetically if necessary. Build metadata
- * are ignored for sorting versions.
+ * are ignored when sorting versions.
  */
-public class CassandraVersion implements Comparable<CassandraVersion> {
+public class Version implements Comparable<Version> {
 
   private static final String VERSION_REGEXP =
-      ""(\\d+)\\.(\\d+)(\\.\\d+)?(\\.\\d+)?([~\\-]\\w[.\\w]*(?:\\-\\w[.\\w]*)*)?(\\+[.\\w]+)?"";
+      ""(\\d+)\\.(\\d+)(\\.\\d+)?(\\.\\d+)?([~\\-]\\w[.\\w]*(?:-\\w[.\\w]*)*)?(\\+[.\\w]+)?"";","[{'comment': 'I think I just blew a few neurons trying to parse that change ðŸ˜‰ \r\n\r\nIf I\'m following correctly, the dash separating pre-release labels &mdash; as in ""rc1-SNAPSHOT"" &mdash; does not need to be escaped. Is it just unnecessary or was it failing?', 'commenter': 'olim7t'}, {'comment': 'Just unnecessary (according to IntelliJ).', 'commenter': 'adutra'}]"
964,core/src/main/java/com/datastax/oss/driver/api/core/Version.java,"@@ -23,24 +23,41 @@
 import java.util.regex.Pattern;
 
 /**
- * The version of a Cassandra release.
+ * A structured version number.
  *
- * <p>It is in the form X.Y.Z, with optional pre-release labels and build metadata.
+ * <p>Versions are expected to follow the general guidelines of<a
+ * href=""https://semver.org/"">semantic versioning</a>, but with a few relaxed rules.
+ *
+ * <p>The general form a version number is expected to be {@code X.Y[.Z]}, where X stands for the
+ * version's major number, Y for the minor number, and Z for an (optional) patch number.
+ *
+ * <p>It can also contain a few optional extensions: a 4th ""DSE patch"" number (specific to DSE
+ * releases); one or many pre-release labels; and one optional build label. See {@link
+ * #parse(String)} for a few examples.
+ *
+ * <p>This class can be used to parse versions of Apache CassandraÂ®, DataStax Enterprise (DSE), or
+ * the DataStax Java driver.
  *
  * <p>Version numbers compare the usual way, the major number (X) is compared first, then the minor
  * one (Y) and then the patch level one (Z). Lastly, versions with pre-release sorts before the
  * versions that don't have one, and labels are sorted alphabetically if necessary. Build metadata
- * are ignored for sorting versions.
+ * are ignored when sorting versions.
  */
-public class CassandraVersion implements Comparable<CassandraVersion> {
+public class Version implements Comparable<Version> {
 
   private static final String VERSION_REGEXP =
-      ""(\\d+)\\.(\\d+)(\\.\\d+)?(\\.\\d+)?([~\\-]\\w[.\\w]*(?:\\-\\w[.\\w]*)*)?(\\+[.\\w]+)?"";
+      ""(\\d+)\\.(\\d+)(\\.\\d+)?(\\.\\d+)?([~\\-]\\w[.\\w]*(?:-\\w[.\\w]*)*)?(\\+[.\\w]+)?"";
+
   private static final Pattern pattern = Pattern.compile(VERSION_REGEXP);
 
-  public static final CassandraVersion V2_1_0 = parse(""2.1.0"");
-  public static final CassandraVersion V2_2_0 = parse(""2.2.0"");
-  public static final CassandraVersion V3_0_0 = parse(""3.0.0"");
+  /** Apache Cassandra's version 2.1.0. */
+  public static final Version CASSANDRA_2_1_0 = parse(""2.1.0"");
+
+  /** Apache Cassandra's version 2.2.0. */
+  public static final Version CASSANDRA_2_2_0 = parse(""2.2.0"");
+
+  /** Apache Cassandra's version 3.0.0. */
+  public static final Version CASSANDRA_3_0_0 = parse(""3.0.0"");","[{'comment': 'Why use ""Cassandra"" in the name if the version is now generic? This could apply to any tool having a 3.0.0 version.', 'commenter': 'olim7t'}, {'comment': 'Yes but the constant names would look weird: why have `_2_1_0` and `_2_2_0` but not `_2_3_0`, `_2_4_0`, etc.?\r\nI think a more elegant approach would be to declare these constants in a dedicated class, will try that.', 'commenter': 'adutra'}]"
964,core/src/main/resources/reference.conf,"@@ -102,6 +102,32 @@ datastax-java-driver {
   # and metrics.
   // session-name = my_session
 
+  # A set of customizable actions that are performed when the driver starts up, i.e.,
+  # each time a new session is initialized.
+  # Actions in this section that involve printing log messages will always use the special logger
+  # 'com.datastax.oss.driver.api.core.Startup' at level INFO.
+  startup {
+
+    # Whether or not to print basic information about the initialization process,
+    # such as the driver name, its version number, and the process PID, if available.
+    print-basic-info = true","[{'comment': 'Why not combine the two options? We could have a single `startup-log` that defaults to the basic info, and then you can customize it, or comment it out to disable the log.\r\n\r\nArtifact coordinates can\'t use regular interpolation, but we could come up with a custom one and use a simple search and replace:\r\n```\r\nstartup-log = ""Starting #{driver.groupId}:#{driver.artifactId}:#{driver.version}""\r\n```', 'commenter': 'olim7t'}, {'comment': '1. I like the idea of combining everything into one banner.\r\n2. I don\'t like mixing two interpolation levels, I think users will get confused.\r\n\r\nI did try today to come up with a solution entirely handled internally by TypeSafe Config, and using the new `ConfigResolver` API. It looked very promising, e.g. the following could have been the default banner:\r\n\r\n```\r\nbanner = ${oss-driver.name} (${oss-driver.groupId}"":""${oss-driver.artifactId}) version ${oss-driver.version} (PID ${pid})\r\n```\r\n\r\nBut unfortunately it seems that there is a bug (or feature) in this new API, see https://github.com/lightbend/config/issues/553.\r\n\r\nIn the meanwhile, I\'m having second thoughts about these banners. An even better solution would be to implement a listener interface for `Session` lifecycle events (`onSessionInitializing`, `onSessionInitialized`, `onSessionClosing`, etc.), and provide by default an implementation that logs basic driver info + time elapsed when the session is fully initialized.\r\n\r\nUnless anyone disagrees, I will implement the idea of a listener.', 'commenter': 'adutra'}, {'comment': 'It turns out the bug is a feature :) so I suggest that we abandon the idea of a custom banner in favor of a listener.', 'commenter': 'adutra'}]"
964,core/src/main/java/com/datastax/oss/driver/api/core/DriverInfo.java,"@@ -0,0 +1,32 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core;
+
+/** Exposes information about the driver. */
+public interface DriverInfo {","[{'comment': '""Info"" is a bit generic. What this really is is the artifact coordinates.', 'commenter': 'olim7t'}, {'comment': 'As you pointed out below, we might want to enrich this interface with more info than just the GAV coordinates, which is why I chose the word ""Info"".', 'commenter': 'adutra'}]"
964,core/src/main/java/com/datastax/oss/driver/api/core/DriverInfo.java,"@@ -0,0 +1,32 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core;
+
+/** Exposes information about the driver. */
+public interface DriverInfo {
+
+  /** @return the driver full name. */
+  String getName();","[{'comment': ""Not sure if this is really needed, it doesn't bring any additional info than the artifactId.\r\n\r\nOn the other hand, it would be nice to know if the driver is shaded. We can't do it through `Driver.properties` because it's generated at compile time, but we could do a runtime check on one of the Netty classes."", 'commenter': 'olim7t'}]"
964,core/src/main/java/com/datastax/oss/driver/api/core/session/Session.java,"@@ -51,15 +52,14 @@
 public interface Session extends AsyncAutoCloseable {
 
   /**
-   * The current version of the core driver (in other words, the version of the {@code
-   * com.datastax.oss:java-driver-core} artifact).
+   * Returns information about the driver, such as its current version number.
    *
    * <p>This is intended for products that wrap or extend the driver, as a way to check
    * compatibility if end-users override the driver version in their application.
    */
-  static String getCoreDriverVersion() {
-    // Note: getBundle caches its result
-    return ResourceBundle.getBundle(""com.datastax.oss.driver.Driver"").getString(""driver.version"");
+  default DriverInfo getDriverInfo() {
+    return DefaultDriverInfo.buildFromResource(
+        getClass().getResource(""/com/datastax/oss/driver/Driver.properties""));","[{'comment': 'ðŸ‘Ž on making this an instance method.\r\nThere is some value in being able to test the driver version before creating a session. I could imagine some 3rd-party tool needing that.', 'commenter': 'olim7t'}, {'comment': 'Fair enough, will revert to static. But in this case I think a more explicit method name is better, e.g. `getOssDriverInfo()`.', 'commenter': 'adutra'}]"
964,core/src/main/java/com/datastax/oss/driver/internal/core/config/typesafe/TypesafeDriverConfigProfile.java,"@@ -181,7 +181,7 @@ void register(Derived derivedProfile) {
   }
 
   /**
-   * A profile that was copied from another profile programatically using {@code withXxx} methods.
+   * A profile that was copied from another profile programmatically using {@code withXxx} methods.","[{'comment': ""ðŸ‘ , but it feels a bit weird in this PR since this class is not impacted by any other change (don't remove it, but let's fix unrelated typos in separate commits in the future)."", 'commenter': 'olim7t'}]"
964,core/src/main/java/com/datastax/oss/driver/internal/core/session/DefaultSession.java,"@@ -74,14 +79,8 @@
 public class DefaultSession implements CqlSession {
 
   private static final Logger LOG = LoggerFactory.getLogger(DefaultSession.class);
-
-  public static CompletionStage<CqlSession> init(
-      InternalDriverContext context,
-      Set<InetSocketAddress> contactPoints,
-      CqlIdentifier keyspace,
-      Set<NodeStateListener> nodeStateListeners) {
-    return new DefaultSession(context, contactPoints, nodeStateListeners).init(keyspace);
-  }
+  private static final Logger STARTUP_LOG =
+      LoggerFactory.getLogger(""com.datastax.oss.driver.api.core.Startup"");","[{'comment': ""Not sure if we need a dedicated log, the message can already be disabled through the configuration if it's unwanted."", 'commenter': 'olim7t'}, {'comment': 'My idea was that users might want to attach special appenders to this logger and/or switch off its additivity to give these messages a special treatment (think monitoring tools that are only interested in knowing how many sessions were successfully created, and when).', 'commenter': 'adutra'}]"
964,core/src/main/java/com/datastax/oss/driver/internal/core/session/DefaultSession.java,"@@ -107,11 +106,50 @@ private DefaultSession(
     this.metricUpdater = context.metricUpdaterFactory().newSessionUpdater();
   }
 
-  private CompletionStage<CqlSession> init(CqlIdentifier keyspace) {
+  public CompletionStage<CqlSession> init(CqlIdentifier keyspace) {
+    DriverConfigProfile defaultConfig = context.config().getDefaultProfile();
+    if (defaultConfig.isDefined(DefaultDriverOption.STARTUP_CUSTOM_BANNER)) {
+      printCustomBanner(defaultConfig.getString(DefaultDriverOption.STARTUP_CUSTOM_BANNER));
+    }
+    if (defaultConfig.getBoolean(DefaultDriverOption.STARTUP_PRINT_BASIC_INFO)) {
+      printBasicStartupInfo();
+    }
     RunOrSchedule.on(adminExecutor, () -> singleThreaded.init(keyspace));
     return singleThreaded.initFuture;
   }
 
+  protected void printCustomBanner(String banner) {
+    STARTUP_LOG.info(banner);
+  }
+
+  protected void printBasicStartupInfo() {
+    try {
+      DriverInfo driverInfo = getDriverInfo();
+      int processId = -1;
+      if (Native.isGetProcessIdAvailable()) {
+        processId = Native.getProcessId();
+      }
+      long start = System.nanoTime();
+      STARTUP_LOG.info(""[{}] {} initializing (PID: {})"", logPrefix, driverInfo, processId);
+      singleThreaded.initFuture.thenRun(
+          () -> {
+            Duration elapsed =
+                Duration.ofMillis(TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start));
+            String elapsedStr =
+                elapsed.toString().substring(2).replaceAll(""(\\d[HMS])(?!$)"", ""$1 "").toLowerCase();
+            int size = getPools().size();
+            STARTUP_LOG.info(
+                ""[{}] Session successfully initialized in {}, connected to {} node{}"",
+                logPrefix,
+                elapsedStr,
+                size,
+                size > 1 ? ""s"" : """");","[{'comment': 'I would prefer to have a single message (so the second one after the session is initialized).\r\n\r\nInitialization time sounds nice to have, not sure if the PID will be that useful but why not. If we go with my suggestion on the single configuration option, these would have to be added to the list of custom interpolation variables.', 'commenter': 'olim7t'}, {'comment': 'Ok to everything, but will try to go forward with my idea of a Session lifecycle listener interface.', 'commenter': 'adutra'}]"
967,core/src/main/java/com/datastax/oss/driver/api/core/config/DriverConfigProfile.java,"@@ -42,6 +43,8 @@
 
   int getInt(DriverOption option);
 
+  Map<String, String> getMap(DriverOption option);","[{'comment': ""1. Could you call it `getStringMap` instead?\r\n2. For completeness, we should have a `withStringMap` as well. It can't reuse `with(DriverOption, Object)` like the others because it's not a primitive type, but I think it should be pretty easy to adapt (basically, we need to convert the other way Map => Config).\r\n3. nit: could you move the `getStringMap/withStringMap` pair under `withStringList`, and use the same order in the implementation class?"", 'commenter': 'olim7t'}]"
967,core/src/main/java/com/datastax/oss/driver/internal/core/config/typesafe/TypesafeDriverConfigProfile.java,"@@ -72,6 +76,19 @@ public Duration getDuration(DriverOption option) {
     return getCached(option.getPath(), getEffectiveOptions()::getDuration);
   }
 
+  @Override
+  public Map<String, String> getMap(DriverOption option) {
+    Config subConfig = getCached(option.getPath(), getEffectiveOptions()::getConfig);
+    Map<String, String> map = new HashMap<>();
+    Set<Map.Entry<String, ConfigValue>> entrySet = subConfig.entrySet();
+    for (Map.Entry<String, ConfigValue> entry : entrySet) {","[{'comment': 'nit: maybe inline `entrySet`?', 'commenter': 'olim7t'}]"
967,core/src/main/java/com/datastax/oss/driver/internal/core/config/typesafe/TypesafeDriverConfigProfile.java,"@@ -72,6 +76,19 @@ public Duration getDuration(DriverOption option) {
     return getCached(option.getPath(), getEffectiveOptions()::getDuration);
   }
 
+  @Override
+  public Map<String, String> getMap(DriverOption option) {
+    Config subConfig = getCached(option.getPath(), getEffectiveOptions()::getConfig);
+    Map<String, String> map = new HashMap<>();
+    Set<Map.Entry<String, ConfigValue>> entrySet = subConfig.entrySet();
+    for (Map.Entry<String, ConfigValue> entry : entrySet) {
+      if (entry.getValue().valueType().equals(ConfigValueType.STRING)) {
+        map.put(entry.getKey(), (String) entry.getValue().unwrapped());","[{'comment': 'ðŸ‘ I agree that ignoring non-string fields is probably the best thing to do.', 'commenter': 'olim7t'}]"
967,core/src/test/java/com/datastax/oss/driver/internal/core/config/typesafe/TypeSafeDriverConfigTest.java,"@@ -94,6 +95,20 @@ public void should_create_derived_profile_overriding_option() {
     assertThat(derived.getInt(MockOptions.REQUIRED_INT)).isEqualTo(43);
   }
 
+  @Test
+  public void should_be_able_to_fecth_maph() {","[{'comment': 'spelling: `fetch_map`?', 'commenter': 'olim7t'}]"
967,core/src/main/java/com/datastax/oss/driver/internal/core/channel/ProtocolInitHandler.java,"@@ -292,8 +292,14 @@ void onResponse(Message response) {
         } else {
           failOnUnexpected(response);
         }
-      } catch (AuthenticationException e) {
-        fail(e);
+      } catch (Exception e) {
+        if (e instanceof AuthenticationException) {
+          fail(e);
+        } else {
+          fail(
+              new AuthenticationException(
+                  channel.remoteAddress(), ""Unexpected exception in encoutered in response "", e));
+        }","[{'comment': 'I think two separate catch blocks would be more idiomatic:\r\n```java\r\n} catch (AuthenticationException e) {\r\n  fail(e);\r\n} catch (Throwable t) {\r\n  fail(\r\n      new AuthenticationException(\r\n          channel.remoteAddress(), ""Unexpected exception in encoutered in response "", t));\r\n}\r\n```\r\n(also, using Throwable for the generic one)', 'commenter': 'olim7t'}]"
967,core/src/main/java/com/datastax/oss/driver/internal/core/config/typesafe/TypesafeDriverConfigProfile.java,"@@ -72,6 +76,19 @@ public Duration getDuration(DriverOption option) {
     return getCached(option.getPath(), getEffectiveOptions()::getDuration);
   }
 
+  @Override
+  public Map<String, String> getMap(DriverOption option) {
+    Config subConfig = getCached(option.getPath(), getEffectiveOptions()::getConfig);
+    Map<String, String> map = new HashMap<>();","[{'comment': ""Maybe I'm being paranoid, but I think it would be safer to return an immutable map. See `DefaultConsistencyLevel.mapByCode` for an example of how to build it."", 'commenter': 'olim7t'}]"
967,core/src/test/java/com/datastax/oss/driver/internal/core/config/typesafe/TypeSafeDriverConfigTest.java,"@@ -96,19 +97,33 @@ public void should_create_derived_profile_overriding_option() {
   }
 
   @Test
-  public void should_be_able_to_fecth_maph() {
+  public void should_be_able_to_fecth_string_map() {
     TypeSafeDriverConfig config =
         parse(
             ""required_int = 42 \n auth_provider { auth_thing_one= one \n auth_thing_two = two \n auth_thing_three = three}"");
     DriverConfigProfile base = config.getDefaultProfile();
-    // DriverConfigProfile derived = base.withInt(MockOptions.OPTIONAL_AUTH, 43);
-    Map<String, String> map = base.getMap(MockOptions.OPTIONAL_AUTH);
+    base.getStringMap(MockOptions.OPTIONAL_AUTH);
+    Map<String, String> map = base.getStringMap(MockOptions.OPTIONAL_AUTH);
     assertThat(map.entrySet().size()).isEqualTo(3);
     assertThat(map.get(""auth_thing_one"")).isEqualTo(""one"");
     assertThat(map.get(""auth_thing_two"")).isEqualTo(""two"");
     assertThat(map.get(""auth_thing_three"")).isEqualTo(""three"");
   }
 
+  @Test
+  public void should_be_able_to_fetch_with_string_map() {
+
+    TypeSafeDriverConfig config = parse(""required_int = 42"");
+    Map<String, String> authThingMap = new HashMap<>();
+    authThingMap.put(""auth_thing_one"", ""one"");
+    authThingMap.put(""auth_thing_two"", ""two"");
+    authThingMap.put(""auth_thing_three"", ""three"");
+    DriverConfigProfile base = config.getDefaultProfile();
+    DriverConfigProfile mapBase = base.withStringMap(MockOptions.OPTIONAL_AUTH, authThingMap);
+    Map<String, String> fetchedMap = mapBase.getStringMap(MockOptions.OPTIONAL_AUTH);
+    assertThat(fetchedMap).isEqualTo(authThingMap);","[{'comment': ':+1:', 'commenter': 'olim7t'}]"
967,core/src/main/java/com/datastax/oss/driver/internal/core/config/typesafe/TypesafeDriverConfigProfile.java,"@@ -109,11 +96,38 @@ public DriverConfigProfile withString(DriverOption option, String value) {
     return getCached(option.getPath(), getEffectiveOptions()::getStringList);
   }
 
+  @Override
+  public Map<String, String> getStringMap(DriverOption option) {
+    Config subConfig = getCached(option.getPath(), getEffectiveOptions()::getConfig);
+    ImmutableMap.Builder<String, String> builder = ImmutableMap.builder();
+    for (Map.Entry<String, ConfigValue> entry : subConfig.entrySet()) {
+      if (entry.getValue().valueType().equals(ConfigValueType.STRING)) {
+        builder.put(entry.getKey(), (String) entry.getValue().unwrapped());
+      }
+    }
+    return builder.build();
+  }
+
   @Override
   public DriverConfigProfile withStringList(DriverOption option, List<String> value) {
     return with(option, value);
   }
 
+  @Override
+  public DriverConfigProfile withStringMap(DriverOption option, Map<String, String> map) {
+    Base base = getBaseProfile();
+    // Add the new option to any already derived options
+    Config newAdded = getAddedOptions();
+    for (String key : map.keySet()) {
+      newAdded =
+          newAdded.withValue(
+              option.getPath() + ""."" + key, ConfigValueFactory.fromAnyRef(map.get(key)));","[{'comment': 'Nice, even simpler than I thought.', 'commenter': 'olim7t'}]"
967,core/src/main/java/com/datastax/oss/driver/internal/core/channel/ProtocolInitHandler.java,"@@ -294,6 +294,8 @@ void onResponse(Message response) {
         }
       } catch (AuthenticationException e) {
         fail(e);
+      } catch (Throwable t) {
+        fail(""Unexpected exception at step "" + step, t);","[{'comment': ""I changed this at the last minute because it could in theory be something else than an authentication exception. I've checked that it's properly surfaced to the thread that creates the session."", 'commenter': 'olim7t'}]"
967,core/src/main/java/com/datastax/oss/driver/internal/core/config/typesafe/TypesafeDriverConfig.java,"@@ -33,9 +33,9 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-public class TypeSafeDriverConfig implements DriverConfig {
+public class TypesafeDriverConfig implements DriverConfig {","[{'comment': 'I noticed that I had been mixing different capitalizations, fixing in a separate commit.', 'commenter': 'olim7t'}]"
978,driver-core/src/main/java/com/datastax/driver/core/RegularStatement.java,"@@ -189,6 +192,52 @@ public boolean hasValues() {
         return hasValues(CodecRegistry.DEFAULT_INSTANCE);
     }
 
+    @Override
+    public int requestSizeInBytes(ProtocolVersion protocolVersion, CodecRegistry codecRegistry) {
+        int size = Header.lengthFor(protocolVersion);
+        try {","[{'comment': 'I think the query string is missing:\r\n```java\r\nsize += CBUtil.sizeOfLongString(getQueryString(codecRegistry));\r\n```', 'commenter': 'olim7t'}, {'comment': 'It is included in the line below (199).', 'commenter': 'adutra'}]"
978,driver-core/src/main/java/com/datastax/driver/core/RegularStatement.java,"@@ -189,6 +192,52 @@ public boolean hasValues() {
         return hasValues(CodecRegistry.DEFAULT_INSTANCE);
     }
 
+    @Override
+    public int requestSizeInBytes(ProtocolVersion protocolVersion, CodecRegistry codecRegistry) {
+        int size = Header.lengthFor(protocolVersion);
+        try {
+            switch (protocolVersion) {
+                case V1:
+                    size += CBUtil.sizeOfConsistencyLevel(getConsistencyLevel());
+                    break;
+                case V2:
+                case V3:
+                case V4:
+                case V5:
+                    size += CBUtil.sizeOfConsistencyLevel(getConsistencyLevel());
+                    size += QueryFlag.serializedSize(protocolVersion);
+                    if (hasValues()) {
+                        if (usesNamedValues()) {
+                            size += CBUtil.sizeOfNamedValueList(getNamedValues(protocolVersion, codecRegistry));
+                        } else {
+                            size += CBUtil.sizeOfValueList(Arrays.asList(getValues(protocolVersion, codecRegistry)));
+                        }
+                    }
+                    if (getFetchSize() > 0) {
+                        size += 4;","[{'comment': ""`fetchSize` and `serialConsistencyLevel` also depend on the session-wide defaults. This is annoying because we don't have a reference to the session here, and it would be overkill to inject it as a parameter.\r\n\r\nI think it's fine if the method is a few bytes off (we should document that), and I'd rather overestimate than underestimate (currently the method underestimates if the session defaults are not the protocol defaults). So I am in favor of always counting those two parameters."", 'commenter': 'olim7t'}, {'comment': 'In this case the rule also applies to `defaultTimestamp` in V3+.', 'commenter': 'adutra'}, {'comment': ""Indeed. They only apply per-message, not inside of the statements in a batch, so we'll overestimate at most 12 bytes."", 'commenter': 'olim7t'}]"
978,driver-core/src/main/java/com/datastax/driver/core/Statement.java,"@@ -584,6 +584,17 @@ public Statement setOutgoingPayload(Map<String, ByteBuffer> payload) {
         return this;
     }
 
+    /**
+     * Returns the number of bytes required to encode this statement.
+     * <p/>
+     * If the size cannot be accurately calculated, this method returns -1.
+     *
+     * @return the number of bytes required to encode this statement.
+     */
+    public int requestSizeInBytes(ProtocolVersion protocolVersion, CodecRegistry codecRegistry) {","[{'comment': 'ðŸ‘ for not naming that `get`, to make it clear that it is recomputed every time. We should also mention it in the javadoc.', 'commenter': 'olim7t'}, {'comment': 'Mmm actually I read ""request"" as a verb, but you probably meant ""the request"".', 'commenter': 'olim7t'}]"
978,driver-core/src/main/java/com/datastax/driver/core/RegularStatement.java,"@@ -189,6 +192,52 @@ public boolean hasValues() {
         return hasValues(CodecRegistry.DEFAULT_INSTANCE);
     }
 
+    @Override
+    public int requestSizeInBytes(ProtocolVersion protocolVersion, CodecRegistry codecRegistry) {
+        int size = Header.lengthFor(protocolVersion);
+        try {
+            switch (protocolVersion) {
+                case V1:
+                    size += CBUtil.sizeOfConsistencyLevel(getConsistencyLevel());
+                    break;
+                case V2:
+                case V3:
+                case V4:
+                case V5:
+                    size += CBUtil.sizeOfConsistencyLevel(getConsistencyLevel());
+                    size += QueryFlag.serializedSize(protocolVersion);
+                    if (hasValues()) {
+                        if (usesNamedValues()) {
+                            size += CBUtil.sizeOfNamedValueList(getNamedValues(protocolVersion, codecRegistry));
+                        } else {
+                            size += CBUtil.sizeOfValueList(Arrays.asList(getValues(protocolVersion, codecRegistry)));
+                        }
+                    }
+                    if (getFetchSize() > 0) {
+                        size += 4;
+                    }
+                    if (getPagingState() != null) {
+                        size += CBUtil.sizeOfValue(getPagingState());
+                    }
+                    if (getSerialConsistencyLevel() != null) {
+                        size += CBUtil.sizeOfConsistencyLevel(getSerialConsistencyLevel());
+                    }
+                    if (getDefaultTimestamp() > Long.MIN_VALUE) {
+                        size += 8;","[{'comment': 'Nit: the default timestamp is only sent for v3 and above', 'commenter': 'olim7t'}]"
978,driver-core/src/main/java/com/datastax/driver/core/Connection.java,"@@ -1451,6 +1457,11 @@ protected void initChannel(SocketChannel channel) throws Exception {
 
             // pipeline.addLast(""debug"", new LoggingHandler(LogLevel.INFO));
 
+            if (metrics != null) {
+                pipeline.addLast(""inboundTrafficMeter"", new InboundTrafficMeter(metrics.getBytesReceived()));
+                pipeline.addLast(""outboundTrafficMeter"", new OutboundTrafficMeter(metrics.getBytesSent()));
+            }
+","[{'comment': ""Open question: should we include SSL traffic in those metrics?\r\n\r\nOn the one hand, we're interested about the impact of our data model and queries on the traffic, so SSL is a bit off topic. On the other, I can see how some people would expect this to be the overall traffic."", 'commenter': 'olim7t'}, {'comment': ""According to [this SO question](https://stackoverflow.com/questions/548029/how-much-overhead-does-ssl-impose) the overhead in bytes sent/received of SSL vs non SSL is basically confined to the sole handshake phase (of course there is a lot of CPU overhead, but that's irrelevant here). Since these meters are never reset, I'd say that this overhead would tend to zero over time, so it doesn't really matter much."", 'commenter': 'adutra'}, {'comment': 'ok', 'commenter': 'olim7t'}]"
978,driver-core/src/main/java/com/datastax/driver/core/InboundTrafficMeter.java,"@@ -0,0 +1,41 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.driver.core;
+
+import com.codahale.metrics.Meter;
+import io.netty.buffer.ByteBuf;
+import io.netty.channel.ChannelHandler.Sharable;
+import io.netty.channel.ChannelHandlerContext;
+import io.netty.channel.ChannelInboundHandlerAdapter;
+
+@Sharable
+class InboundTrafficMeter extends ChannelInboundHandlerAdapter {
+
+    private final Meter meter;
+
+    InboundTrafficMeter(Meter meter) {
+        this.meter = meter;
+    }
+
+    @Override
+    public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception {
+        if (msg instanceof ByteBuf) {
+            meter.mark(((ByteBuf) msg).readableBytes());
+        }
+        super.channelRead(ctx, msg);
+    }","[{'comment': ""Nice, straightforward way to implement this ðŸ’¯ \r\n\r\nI was wondering if we could use a single `ChannelDuplexHandler`, but the two methods don't share any state so it doesn't really make any difference."", 'commenter': 'olim7t'}]"
978,driver-core/src/test/java/com/datastax/driver/core/StatementSizeTest.java,"@@ -0,0 +1,200 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.driver.core;
+
+
+import com.datastax.driver.core.ColumnDefinitions.Definition;
+import com.datastax.driver.core.utils.Bytes;
+import com.google.common.base.Charsets;
+import com.google.common.collect.ImmutableMap;
+import org.mockito.Mock;
+import org.mockito.Mockito;
+import org.mockito.MockitoAnnotations;
+import org.testng.annotations.BeforeMethod;
+import org.testng.annotations.Test;
+
+import java.nio.ByteBuffer;
+
+import static com.datastax.driver.core.Assertions.assertThat;
+
+public class StatementSizeTest {
+
+    private static final byte[] MOCK_PAGING_STATE = Bytes.getArray(Bytes.fromHexString(""0xdeadbeef""));
+    private static final ByteBuffer MOCK_PAYLOAD_VALUE1 = Bytes.fromHexString(""0xabcd"");
+    private static final ByteBuffer MOCK_PAYLOAD_VALUE2 = Bytes.fromHexString(""0xef"");
+    private static final ImmutableMap<String, ByteBuffer> MOCK_PAYLOAD = ImmutableMap.of(""key1"", MOCK_PAYLOAD_VALUE1, ""key2"", MOCK_PAYLOAD_VALUE2);
+    private static final byte[] PREPARED_ID = Bytes.getArray(Bytes.fromHexString(""0xaaaa""));
+    private static final byte[] RESULT_METADATA_ID = Bytes.getArray(Bytes.fromHexString(""0xbbbb""));
+
+    @Mock
+    private PreparedStatement preparedStatement;
+
+    @BeforeMethod
+    public void setup() {
+        MockitoAnnotations.initMocks(this);
+
+        PreparedId preparedId = new PreparedId(
+                new PreparedId.PreparedMetadata(MD5Digest.wrap(PREPARED_ID), null),
+                new PreparedId.PreparedMetadata(MD5Digest.wrap(RESULT_METADATA_ID), null),
+                new int[0], ProtocolVersion.V5);
+        Mockito.when(preparedStatement.getPreparedId()).thenReturn(preparedId);
+
+        ColumnDefinitions columnDefinitions = new ColumnDefinitions(new Definition[]{
+                new Definition(""ks"", ""table"", ""c1"", DataType.cint()),
+                new Definition(""ks"", ""table"", ""c2"", DataType.text())
+        }, CodecRegistry.DEFAULT_INSTANCE);
+        Mockito.when(preparedStatement.getVariables()).thenReturn(columnDefinitions);
+        Mockito.when(preparedStatement.getIncomingPayload()).thenReturn(null);
+        Mockito.when(preparedStatement.getOutgoingPayload()).thenReturn(null);
+        Mockito.when(preparedStatement.getCodecRegistry()).thenReturn(CodecRegistry.DEFAULT_INSTANCE);
+    }
+
+    @Test(groups = ""unit"")
+    public void should_measure_size_of_simple_statement() {
+        String queryString = ""SELECT release_version FROM system.local WHERE key = ?"";
+        SimpleStatement statement = new SimpleStatement(queryString);
+        int expectedSize = 9 // header
+                + (4 + queryString.getBytes(Charsets.UTF_8).length) // query string
+                + 2 // consistency level
+                + 4; // flags
+        assertThat(v5SizeOf(statement)).isEqualTo(expectedSize);
+
+        SimpleStatement statementWithAnonymousValue = new SimpleStatement(statement.getQueryString(), ""local"");
+        assertThat(v5SizeOf(statementWithAnonymousValue))
+                .isEqualTo(expectedSize
+                        + 2 // size of number of values
+                        + (4 + ""local"".getBytes(Charsets.UTF_8).length) // value
+                );
+
+        SimpleStatement statementWithNamedValue = new SimpleStatement(statement.getQueryString(), ImmutableMap.<String, Object>of(""key"", ""local""));
+        assertThat(v5SizeOf(statementWithNamedValue))
+                .isEqualTo(expectedSize
+                        + 2 // size of number of values
+                        + (2 + ""key"".getBytes(Charsets.UTF_8).length) // key
+                        + (4 + ""local"".getBytes(Charsets.UTF_8).length) // value
+                );
+
+        statement.setFetchSize(10);
+        expectedSize += 4;
+        assertThat(v5SizeOf(statement)).isEqualTo(expectedSize);","[{'comment': 'To adapt if we decide to overestimate for session-wide defaults.', 'commenter': 'olim7t'}]"
982,driver-core/src/main/java/com/datastax/driver/core/Requests.java,"@@ -591,7 +591,12 @@ public void encode(Prepare msg, ByteBuf dest, ProtocolVersion version) {
 
             @Override
             public int encodedSize(Prepare msg, ProtocolVersion version) {
-                return CBUtil.sizeOfLongString(msg.query);
+                int size = CBUtil.sizeOfLongString(msg.query);
+
+                if (version.compareTo(ProtocolVersion.V5) >= 0) {
+                    size += 4; //flags
+                }","[{'comment': 'It would be better to use `PREPARED_METADATA_CHANGES.isSupportedBy(version)` for this test.', 'commenter': 'olim7t'}, {'comment': 'was following the logic in `encode` which checks the protocol version, should we change that to?', 'commenter': 'tolbertam'}, {'comment': 'Mmm actually the flag does not relate directly to this feature, but instead to per-query keyspaces, which are not supported at all in the driver at this point. Keep the version test, my bad.', 'commenter': 'olim7t'}]"
983,integration-tests/src/test/java/com/datastax/oss/driver/api/core/metrics/MetricsIT.java,"@@ -40,37 +39,30 @@ public void should_expose_metrics() {
         session.execute(""SELECT release_version FROM system.local"");
       }
 
-      // Metric names are prefixed with the session id, which depends on the number of other tests
-      // run before, so do a linear search to find the metric we're interested in.
-      Timer requestsTimer = null;
-      for (Map.Entry<String, Timer> entry : session.getMetricRegistry().getTimers().entrySet()) {
-        if (entry.getKey().endsWith(DefaultSessionMetric.CQL_REQUESTS.getPath())) {
-          requestsTimer = entry.getValue();
-        }
-      }
-      assertThat(requestsTimer).isNotNull();
+      assertThat(session.getMetrics())
+          .hasValueSatisfying(
+              metrics -> {
+                Timer requestsTimer =
+                    metrics.getSessionMetric(DefaultSessionMetric.CQL_REQUESTS, Timer.class);","[{'comment': 'This is indeed much nicer than the previous code.', 'commenter': 'olim7t'}]"
983,core/src/main/java/com/datastax/oss/driver/api/core/metrics/Metrics.java,"@@ -0,0 +1,79 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core.metrics;
+
+import com.codahale.metrics.Metric;
+import com.codahale.metrics.MetricRegistry;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import com.datastax.oss.driver.api.core.type.reflect.GenericType;
+
+/**
+ * A wrapper around a {@link MetricRegistry} to expose the driver's metrics.
+ *
+ * <p>This type exists mainly to avoid a hard dependency to Dropwizard Metrics (that is, the JAR can
+ * be completely removed from the classpath if metrics are disabled). It also provides convenience
+ * methods to access individual metrics programatically.
+ */
+public interface Metrics {
+
+  /**
+   * Returns the underlying Dropwizard registry.
+   *
+   * <p>Typically, this can be used to configure a reporter.
+   *
+   * @see <a href=""http://metrics.dropwizard.io/4.0.0/manual/core.html#reporters"">Reporters
+   *     (Dropwizard Metrics manual)</a>
+   */
+  MetricRegistry getRegistry();","[{'comment': ""This references Dropwizard classes. This means that if someone were to integrate another metrics framework, they would have to make `session.getMetrics` return `Optional.empty()`,  and find another way to expose that framework's equivalent of the registry.\r\nI don't want to go further than that, because any other solution would have to manifest at a higher level in the API (e.g. `CqlSession<DropwizardMetrics>`, `CqlSession<MicrometerMetrics>`, etc). Ultimately I think switching metrics framework is a very rare edge case, so I'd rather keep the API simpler for the 99.9% regular ones."", 'commenter': 'olim7t'}, {'comment': ""I think that in the general case, if you were plugging in a metrics framework, at some level that integration would provide access to the equivalent of a registry, so I think what you are doing here is fine.  Most users will be content with dropwizard metrics (I'd guess) and direct access to the registry here seems good."", 'commenter': 'tolbertam'}, {'comment': ""There might be a more generic solution.\r\n\r\nThis interface could stay agnostic of Dropwizard metrics:\r\n\r\n1. Remove the `getRegistry()` method (it is not used by the driver anyway)\r\n2. instead of `<T extends Metric>`, use simply `<T>`\r\n\r\nGranted, people would have to cast to `DefaultMetrics` (maybe renamed to `DropwizardMetrics`?) to access the registry, but that looks cleaner to me: if someone wants to change for another metrics framework, it's just a matter of implementing another `MetricsFactory` (unless I'm missing something)."", 'commenter': 'adutra'}, {'comment': '> people would have to cast to DefaultMetrics\r\n\r\nThat\'s what I meant by ""would have to manifest at a higher level in the API"". Requiring a cast for a default feature is really bad IMHO. I\'d rather not punish the 99.9%, and instead cause a minor inconvenience to the 0.1% that might want to change the metrics framework. Frankly the fact that this is even possible is a cherry on top of the cake already.', 'commenter': 'olim7t'}, {'comment': 'I slightly disagree. A cast is still better imo, and I still like the idea that there is no ""default"" metric library in the driver, it just happens that we ship with one impl based on Dropwizard. But that\'s a detail and I don\'t want to spend more time on it, I\'m fine if we leave things as they are.', 'commenter': 'adutra'}]"
983,core/src/main/java/com/datastax/oss/driver/internal/core/metrics/DropwizardMetricUpdater.java,"@@ -15,62 +15,69 @@
  */
 package com.datastax.oss.driver.internal.core.metrics;
 
+import com.codahale.metrics.Metric;
 import com.codahale.metrics.MetricRegistry;
 import com.codahale.metrics.Timer;
 import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
 import com.datastax.oss.driver.api.core.config.DriverConfigProfile;
+import com.datastax.oss.driver.api.core.type.reflect.GenericType;
 import java.time.Duration;
 import java.util.Set;
 import java.util.concurrent.TimeUnit;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-public abstract class MetricUpdaterBase<MetricT> implements MetricUpdater<MetricT> {
+public abstract class DropwizardMetricUpdater<MetricT> implements MetricUpdater<MetricT> {
 
-  private static final Logger LOG = LoggerFactory.getLogger(MetricUpdaterBase.class);
+  private static final Logger LOG = LoggerFactory.getLogger(DropwizardMetricUpdater.class);
 
   protected final Set<MetricT> enabledMetrics;
-  protected final MetricRegistry metricRegistry;
+  protected final MetricRegistry registry;
 
-  protected MetricUpdaterBase(Set<MetricT> enabledMetrics, MetricRegistry metricRegistry) {
+  protected DropwizardMetricUpdater(Set<MetricT> enabledMetrics, MetricRegistry registry) {
     this.enabledMetrics = enabledMetrics;
-    this.metricRegistry = metricRegistry;
+    this.registry = registry;
   }
 
   protected abstract String buildFullName(MetricT metric);
 
   @Override
   public void incrementCounter(MetricT metric, long amount) {
     if (enabledMetrics.contains(metric)) {
-      metricRegistry.counter(buildFullName(metric)).inc(amount);
+      registry.counter(buildFullName(metric)).inc(amount);
     }
   }
 
   @Override
   public void updateHistogram(MetricT metric, long value) {
     if (enabledMetrics.contains(metric)) {
-      metricRegistry.histogram(buildFullName(metric)).update(value);
+      registry.histogram(buildFullName(metric)).update(value);
     }
   }
 
   @Override
   public void markMeter(MetricT metric, long amount) {
     if (enabledMetrics.contains(metric)) {
-      metricRegistry.meter(buildFullName(metric)).mark(amount);
+      registry.meter(buildFullName(metric)).mark(amount);
     }
   }
 
   @Override
   public void updateTimer(MetricT metric, long duration, TimeUnit unit) {
     if (enabledMetrics.contains(metric)) {
-      metricRegistry.timer(buildFullName(metric)).update(duration, unit);
+      registry.timer(buildFullName(metric)).update(duration, unit);
     }
   }
 
+  @SuppressWarnings(""unchecked"")
+  public <T extends Metric> T getMetric(MetricT metric, GenericType<T> expectedType) {","[{'comment': ""`expectedType` is unused, shouldn't we at least check that the returned metric is an instance of it?"", 'commenter': 'adutra'}, {'comment': ""Guava's type token doesn't have any method to do that. Understandably so, because it could only compare to the erased type of the object.\r\nWhich leads me to believe that `expectedType` is redundant here; I put it thinking of the `Class<T>`-based type token pattern, but really it doesn't help at all in this case, since we're going to suppress a warning anyway.\r\n"", 'commenter': 'olim7t'}]"
983,core/src/main/java/com/datastax/oss/driver/internal/core/metrics/MetricsFactory.java,"@@ -16,8 +16,12 @@
 package com.datastax.oss.driver.internal.core.metrics;
 
 import com.datastax.oss.driver.api.core.metadata.Node;
+import com.datastax.oss.driver.api.core.metrics.Metrics;
+import java.util.Optional;
 
-public interface MetricUpdaterFactory {
+public interface MetricsFactory {","[{'comment': 'Slightly better imo:\r\n\r\n```\r\npublic interface MetricsFactory<T extends Metrics> {\r\n\r\n  Optional<? extends T> getMetrics();\r\n```', 'commenter': 'adutra'}, {'comment': ""Based on my reply to your other comment, I think we should avoid specialized `Metrics` subinterfaces, so this wouldn't apply."", 'commenter': 'olim7t'}, {'comment': ""Sorry, I don't get it. I'm just saying that `Optional<? extends Metrics> getMetrics()` is more correct than `Optional<Metrics> getMetrics()`."", 'commenter': 'adutra'}, {'comment': 'The only goal of this method is to be exposed as `Session.getMetrics()`. In what scenario would `? extends Metric` be more useful for a client application?\r\n', 'commenter': 'olim7t'}, {'comment': 'If someone extends `Session` and `Metrics`, then if this method has the return type `Optional<? extends Metrics>`  it would be possible to use covariance and override it with a more specialized `Optional<MySpecialMetrics>`. That is not possible if the original return type is `Optional<Metrics>`. We had a similar problem with `AsyncResultSet.fetchNextPage()` recently.', 'commenter': 'adutra'}, {'comment': 'OK that makes sense, I will change it.', 'commenter': 'olim7t'}]"
983,core/src/main/java/com/datastax/oss/driver/api/core/metrics/Metrics.java,"@@ -42,38 +41,22 @@
   /**
    * Retrieves a session-level metric from the registry.
    *
-   * @param expectedType the type of the metric (refer to the comments in the default {@code
-   *     reference.conf} included in the driver's codebase or JAR file). If it is not parameterized,
-   *     you can use {@link #getSessionMetric(SessionMetric, Class)} as a shortcut.
+   * <p>To determine the type of each metric, refer to the comments in the default {@code
+   * reference.conf} (included in the driver's codebase and JAR file).
+   *
    * @return the metric, or {@code null} if it is disabled.
    * @throws ClassCastException if the type does not match.
    */
-  <T extends Metric> T getSessionMetric(SessionMetric metric, GenericType<T> expectedType);
-
-  /**
-   * Shortcut for {@link #getSessionMetric(SessionMetric, GenericType) getSessionMetric(metric,
-   * GenericType.of(expectedClass))}.
-   */
-  default <T extends Metric> T getSessionMetric(SessionMetric metric, Class<T> expectedClass) {
-    return getSessionMetric(metric, GenericType.of(expectedClass));
-  }
+  <T extends Metric> T getSessionMetric(SessionMetric metric);
 
   /**
    * Retrieves a node-level metric for a given node from the registry.
    *
-   * @param expectedType the type of the metric (refer to the comments in the default {@code
-   *     reference.conf} included in the driver's codebase or JAR file). If it is not parameterized,
-   *     you can use {@link #getSessionMetric(SessionMetric, Class)} as a shortcut.
+   * <p>To determine the type of each metric, refer to the comments in the default {@code
+   * reference.conf} (included in the driver's codebase and JAR file).
+   *
    * @return the metric, or {@code null} if it is disabled.
    * @throws ClassCastException if the type does not match.
    */
-  <T extends Metric> T getNodeMetric(Node node, NodeMetric metric, GenericType<T> expectedType);
-
-  /**
-   * Shortcut for {@link #getNodeMetric(Node, NodeMetric, GenericType) getNodeMetric(node, metric,
-   * GenericType.of(expectedClass))}.
-   */
-  default <T extends Metric> T getNodeMetric(Node node, NodeMetric metric, Class<T> expectedClass) {
-    return getNodeMetric(node, metric, GenericType.of(expectedClass));
-  }
+  <T extends Metric> T getNodeMetric(Node node, NodeMetric metric);","[{'comment': ""That's worse imo, this `T` is not bounded by anything. Granted we can only check against the erased type here so `GenericType` is not an option, but why not check against a `Class<T>`? To me the ideal signature would be:\r\n\r\n```\r\n<T extends Metric> T getNodeMetric(Node node, NodeMetric metric, Class<T> expectedType);\r\n```\r\n\r\nAnd the implementation would enforce that the returned object is an instance of `expectedType`."", 'commenter': 'adutra'}, {'comment': ""Mmm, but then it doesn't work for generic metrics:\r\n\r\n```java\r\n// unchecked assignment\r\nGauge<Integer> nodeMetric = getNodeMetric(node, DefaultNodeMetric.OPEN_CONNECTIONS, Gauge.class);\r\n```\r\n\r\nForcing the client to suppress a warning is not better than not enforcing T with a type token argument. Actually I see it as two sides of the same problem: that Java's type system is not powerful enough to enforce generic types at runtime. There's nothing we can do about it, so I'd rather go with the solution that doesn't generate a warning.\r\n(to put it another way, we're moving the suppression of the warning into our code)"", 'commenter': 'olim7t'}, {'comment': ""Indeed,  my suggestion wouldn't work well will Gauges. Sounds good."", 'commenter': 'adutra'}]"
983,core/src/main/java/com/datastax/oss/driver/api/core/metrics/Metrics.java,"@@ -42,38 +41,22 @@
   /**
    * Retrieves a session-level metric from the registry.
    *
-   * @param expectedType the type of the metric (refer to the comments in the default {@code
-   *     reference.conf} included in the driver's codebase or JAR file). If it is not parameterized,
-   *     you can use {@link #getSessionMetric(SessionMetric, Class)} as a shortcut.
+   * <p>To determine the type of each metric, refer to the comments in the default {@code
+   * reference.conf} (included in the driver's codebase and JAR file).
+   *
    * @return the metric, or {@code null} if it is disabled.
    * @throws ClassCastException if the type does not match.
    */
-  <T extends Metric> T getSessionMetric(SessionMetric metric, GenericType<T> expectedType);
-
-  /**
-   * Shortcut for {@link #getSessionMetric(SessionMetric, GenericType) getSessionMetric(metric,
-   * GenericType.of(expectedClass))}.
-   */
-  default <T extends Metric> T getSessionMetric(SessionMetric metric, Class<T> expectedClass) {
-    return getSessionMetric(metric, GenericType.of(expectedClass));
-  }
+  <T extends Metric> T getSessionMetric(SessionMetric metric);
 
   /**
    * Retrieves a node-level metric for a given node from the registry.
    *
-   * @param expectedType the type of the metric (refer to the comments in the default {@code
-   *     reference.conf} included in the driver's codebase or JAR file). If it is not parameterized,
-   *     you can use {@link #getSessionMetric(SessionMetric, Class)} as a shortcut.
+   * <p>To determine the type of each metric, refer to the comments in the default {@code
+   * reference.conf} (included in the driver's codebase and JAR file).
+   *
    * @return the metric, or {@code null} if it is disabled.
    * @throws ClassCastException if the type does not match.","[{'comment': 'With the current signature the `ClassCastException` will not be thrown from within the method, but in client code (when assigning to a variable of the wrong type), so technically this `@throws` clause should be removed.', 'commenter': 'adutra'}]"
985,driver-core/pom.xml,"@@ -54,11 +54,6 @@
             <artifactId>slf4j-api</artifactId>
         </dependency>
 
-        <dependency>","[{'comment': 'When a project uses some dependency directly, it is usually recommended that it declares that dependency explicitly instead of relying on transitive dependencies. I would prefer that we keep the dependency declared here.', 'commenter': 'adutra'}, {'comment': ""I considered that, but I was sort of put off by how jnr-posix upgraded minor version dependency of jnr-ffi in a patch release.  Since we now have a test to verify the functionality of jnr-posix retrieving pids, we could add the direct dependency back and feel relatively safe, so i'll do that."", 'commenter': 'tolbertam'}]"
985,driver-core/src/main/java/com/datastax/driver/core/utils/UUIDs.java,"@@ -153,7 +153,12 @@ private static String getProcessPiece() {
         if (pid == null && Native.isGetpidAvailable()) {
             try {
                 pid = Native.processId();
-                LOGGER.info(""PID obtained through native call to getpid(): {}"", pid);
+                if (pid == 0) {","[{'comment': 'ðŸ‘ ', 'commenter': 'adutra'}]"
985,pom.xml,"@@ -63,7 +63,6 @@
         <joda.version>2.9.9</joda.version>
         <jsr353-api.version>1.0</jsr353-api.version>
         <jsr353-ri.version>1.0.4</jsr353-ri.version>
-        <jnr-ffi.version>2.0.9</jnr-ffi.version>","[{'comment': 'I would suggest that we declare both dependencies for the reasons outlined above, with maybe a comment here to remind us that jnr-posix and jnr-ffi must have compatible versions.', 'commenter': 'adutra'}]"
992,core/src/main/java/com/datastax/oss/driver/internal/core/type/codec/registry/CachingCodecRegistry.java,"@@ -367,8 +341,7 @@ protected CachingCodecRegistry(String logPrefix, TypeCodec<?>... userCodecs) {
     return builder.build();
   }
 
-  // We call this after validating the types, so we know the cast will never fail.
-  private static <T, U> TypeCodec<T> safeCast(TypeCodec<U> codec) {
+  private static <T, U> TypeCodec<T> uncheckedCast(TypeCodec<U> codec) {","[{'comment': 'This is far from ""safe"", i.e. the following compiles and runs just fine:\r\n\r\n```\r\n  TypeCodec<Number> codec = unsafeCast(TypeCodecs.TEXT);\r\n  System.out.println(codec);\r\n  // prints com.datastax.oss.driver.internal.core.type.codec.StringCodec@ea30797\r\n``` \r\n\r\nThe only benefit of having this method imo si to reduce the scope of the `@SuppressWarnings` annotation to the minimum, but the name is misleading.', 'commenter': 'adutra'}]"
992,core/src/main/java/com/datastax/oss/driver/internal/core/type/codec/registry/CachingCodecRegistry.java,"@@ -302,9 +288,27 @@ protected CachingCodecRegistry(String logPrefix, TypeCodec<?>... userCodecs) {
     throw new CodecNotFoundException(null, javaType);
   }
 
+  // This is only used for the recursion from createCovariantCodec(GenericType)
+  protected TypeCodec<?> covariantCodecFor(GenericType<?> javaType) {
+    LOG.trace(""[{}] Looking up codec for Java type {}"", logPrefix, javaType);
+    for (TypeCodec<?> primitiveCodec : primitiveCodecs) {
+      if (primitiveCodec.getJavaType().isSupertypeOf(javaType)) {
+        LOG.trace(""[{}] Found matching primitive codec {}"", logPrefix, primitiveCodec);
+        return uncheckedCast(primitiveCodec);
+      }
+    }
+    for (TypeCodec<?> userCodec : userCodecs) {
+      if (userCodec.getJavaType().isSupertypeOf(javaType)) {
+        LOG.trace(""[{}] Found matching user codec {}"", logPrefix, userCodec);
+        return uncheckedCast(userCodec);
+      }
+    }
+    return uncheckedCast(getCachedCodec(null, javaType));
+  }
+","[{'comment': ""Please don't move stuff around for no reason, this is really hard to review. There are no structural changes to this method, yet I had to do a line-by-line comparison."", 'commenter': 'olim7t'}, {'comment': ""Hmm don't you think it's easier to follow now that this method is declared below the method where it is invoked from?"", 'commenter': 'adutra'}, {'comment': 'Not necessarily, one might prefer to keep all the ""codecFor"" methods together because they do the same thing (this was probably my train of thought at the time).\r\n\r\nBut that\'s not the point, it\'s about minimising changes to streamline reviews.', 'commenter': 'olim7t'}, {'comment': ""Fair enough, my apologies. And besides, I need this method exposed publicly so my argument in favor of moving it doesn't hold anymore."", 'commenter': 'adutra'}]"
992,core/src/main/java/com/datastax/oss/driver/api/core/type/codec/registry/CodecRegistry.java,"@@ -35,11 +36,39 @@
  * {@link Row#get(int, Class)}, {@link TupleValue#set(int, Object, Class)}, etc.)
  */
 public interface CodecRegistry {
+
+  // roughly sorted by popularity
+  TypeCodec<?>[] DEFAULT_PRIMITIVE_CODECS =
+      new TypeCodec<?>[] {
+        // Must be declared before AsciiCodec so it gets chosen when CQL type not available
+        TypeCodecs.TEXT,
+        // Must be declared before TimeUUIDCodec so it gets chosen when CQL type not available
+        TypeCodecs.UUID,
+        TypeCodecs.TIMEUUID,
+        TypeCodecs.TIMESTAMP,
+        TypeCodecs.INT,
+        TypeCodecs.BIGINT,
+        TypeCodecs.BLOB,
+        TypeCodecs.DOUBLE,
+        TypeCodecs.FLOAT,
+        TypeCodecs.DECIMAL,
+        TypeCodecs.VARINT,
+        TypeCodecs.INET,
+        TypeCodecs.BOOLEAN,
+        TypeCodecs.SMALLINT,
+        TypeCodecs.TINYINT,
+        TypeCodecs.DATE,
+        TypeCodecs.TIME,
+        TypeCodecs.DURATION,
+        TypeCodecs.COUNTER,
+        TypeCodecs.ASCII
+      };
+","[{'comment': ""ðŸ‘ , but I would move it to `DefaultCodecRegistry`: this is an implementation detail and I don't see a strong case for making it part of the public API."", 'commenter': 'olim7t'}, {'comment': ""Believe me or not, It doesn't work. If `DEFAULT_PRIMITIVE_CODECS` is declared in `DefaultCodecRegistry` or `CachingCodecRegistry`,  then when you instantiate `DefaultCodecRegistry` from `CodecRegistry.DEFAULT`, if the class is not loaded yet the constructor will be invoked _before_ the static initializers, and as a consequence, `DEFAULT_PRIMITIVE_CODECS` will be null. That's why I moved `DEFAULT_PRIMITIVE_CODECS` to `CodecRegistry`. There must be some circularity going on. The only alternative would be to declare `DEFAULT_PRIMITIVE_CODECS` in a separate class."", 'commenter': 'adutra'}]"
992,core/src/main/java/com/datastax/oss/driver/api/core/type/codec/registry/CodecRegistry.java,"@@ -70,6 +70,26 @@
    */
   <T> TypeCodec<T> codecFor(DataType cqlType);
 
+  /**
+   * Returns a codec to convert the given Java type to the CQL type deemed most appropriate to
+   * represent it.
+   *
+   * <p>The definition of ""most appropriate"" is unspecified, and left to the appreciation of the
+   * registry implementor.
+   */
+  <T> TypeCodec<T> codecFor(GenericType<T> javaType);","[{'comment': 'I still have doubts about this, in particular with the fact that the implementation _happens to be_ covariant. Is this an implementation detail, or should we make it part of the contract? Or should it actually be invariant, and require a dedicated implementation instead of reusing what we had previously?\r\n\r\nIt doesn\'t help that we have no identified use case for this method. You mentioned people in the community requested it, do you remember where that conversation happened?\r\n\r\nI just don\'t want this to become a ""it sounded like a good idea at the time"" moment ðŸ˜‰ ', 'commenter': 'olim7t'}, {'comment': ""The discussion has been captured in [JAVA-1015](https://datastax-oss.atlassian.net/browse/JAVA-1015).\r\n\r\nThe original pull request had an invariant implementation (i.e. it used `codec.accepts(javaType)`, not `codec.getJavaType().isSuperTypeOf(javaType)`).\r\n\r\nIndeed I'd rather keep it invariant. Suppose that it's covariant, that we have `B extends A` and there is a codec for `A`. The following would happen:\r\n\r\n```\r\nBoundStatement s = ...\r\ns.set(0, new B(), codecFor(B.class)); // ok, will return TypeCodec<A> which can encode B instances\r\nB b = s.get(0, codecFor(B.class)); // TypeCodec<A> will decode B to an A instance -> ClassCastException\r\n```\r\n\r\nBut that's going to be far from trivial. \r\n\r\nBecause there is something new in 4.x: previously lookups by value wouldn't be cached, but now they are. \r\n\r\nWhat was your motivation to change that?\r\n\r\nThe practical consequence is that lookups by java type now _have_ to be covariant, whether or not the initial entry point was a lookup by value or by java type:\r\n\r\n1. `codecFor(Object)` or `codecFor(GenericType<?>)` may both call\r\n2. `getCachedCodec(DataType, GenericType<?>)` which may call\r\n3. `createCodec(DataType, GenericType<?>)` which may call (since `cqlType` is null)\r\n4. `createCovariantCodec(GenericType<?> javaType)`, which may call\r\n5. `codecFor(GenericType<T> javaType)` (previously named `covariantCodecFor(GenericType<T> javaType)`)\r\n\r\n... which must be covariant because the entry point above might have been `codecFor(Object)`, which requires covariance.\r\n\r\nI only see two ways to fix this issue:\r\n\r\n1. Do not cache lookups by value, use covariant lookups only for these lookups.\r\n2. Keep caching lookups by value, but store them in a separate cache that creates covariant codecs.\r\n\r\nI will provide a solution using approach 1 above, as `codecFor(Object)` is only used in `SimpleStatement`s and I don't think that performance is paramount. "", 'commenter': 'adutra'}]"
992,core/src/main/java/com/datastax/oss/driver/api/core/type/codec/TypeCodec.java,"@@ -86,7 +86,7 @@ default boolean accepts(Class<?> javaClass) {
    */
   default boolean accepts(Object value) {
     Preconditions.checkNotNull(value);
-    return getJavaType().__getToken().isSupertypeOf(TypeToken.of(value.getClass()));
+    return getJavaType().isSupertypeOf(GenericType.of(value.getClass()));","[{'comment': 'ðŸ‘ for isolating that in separate commits, that makes it easy to squash with the ""Enhance GenericType..."" one.', 'commenter': 'olim7t'}]"
992,core/src/main/java/com/datastax/oss/driver/api/core/type/codec/registry/CodecRegistry.java,"@@ -65,26 +77,40 @@
    * Returns a codec to convert the given CQL type to the Java type deemed most appropriate to
    * represent it.
    *
-   * <p>The definition of ""most appropriate"" is unspecified, and left to the appreciation of the
-   * registry implementor.
+   * <p>This is used internally by the driver, in cases where the Java type is not explicitly
+   * provided, for example {@link GettableByIndex#getObject(int) row.getObject(0)} (CQL type known
+   * from the row metadata, Java type unspecified).
+   *
+   * <p>The definition of ""most appropriate"" is left to the appreciation of the registry
+   * implementor.
+   *
+   * @throws CodecNotFoundException if there is no such codec.
    */
   <T> TypeCodec<T> codecFor(DataType cqlType);
 
   /**
    * Returns a codec to convert the given Java type to the CQL type deemed most appropriate to
    * represent it.
    *
-   * <p>The definition of ""most appropriate"" is unspecified, and left to the appreciation of the
-   * registry implementor.
+   * <p>The driver does not use this method. It is provided as a convenience for third-party usage,
+   * for example if you were to generate a schema based on a set of Java classes.
+   *
+   * <p>The driver's default registry implementation is <em>invariant</em> with regard to the Java
+   * type: for example, if {@code B extends A} and an {@code A<=>int} codec is registered, {@code
+   * codecFor(DataTypes.INT, B.class)} <b>will not</b> find that codec. This is because this method
+   * we don't know whether this method will be used for encoding, decoding, or both.","[{'comment': 'Small nit here: ""this is because this method we don\'t know...""', 'commenter': 'adutra'}]"
992,core/src/main/java/com/datastax/oss/driver/api/core/type/codec/registry/CodecRegistry.java,"@@ -94,8 +120,18 @@
    * Returns a codec to convert the given Java object to the CQL type deemed most appropriate to
    * represent it.
    *
-   * <p>The definition of ""most appropriate"" is unspecified, and left to the appreciation of the
-   * registry implementor.
+   * <p>This is used internally by the driver, in cases where the CQL type is unknown, for example
+   * for example for {@link SimpleStatement#setPositionalValues(List) simple statement variables}","[{'comment': 'Small nit here, ""for example"" is repeated twice.', 'commenter': 'adutra'}]"
992,core/src/main/java/com/datastax/oss/driver/internal/core/type/codec/registry/CachingCodecRegistry.java,"@@ -64,122 +64,146 @@
   //   traversal is cheap).
 
   protected final String logPrefix;
+  private final TypeCodec<?>[] primitiveCodecs;
   private final TypeCodec<?>[] userCodecs;
+  private final IntMap<TypeCodec> primitiveCodecsByCode;
 
-  protected CachingCodecRegistry(String logPrefix, TypeCodec<?>... userCodecs) {
+  protected CachingCodecRegistry(
+      String logPrefix, TypeCodec<?>[] primitiveCodecs, TypeCodec<?>[] userCodecs) {
     this.logPrefix = logPrefix;
+    this.primitiveCodecs = primitiveCodecs;
     this.userCodecs = userCodecs;
+    this.primitiveCodecsByCode = sortByProtocolCode(primitiveCodecs);
   }
 
   /**
    * Gets a complex codec from the cache.
    *
    * <p>If the codec does not exist in the cache, this method must generate it with {@link
-   * #createCodec(DataType, GenericType)} (and most likely put it in the cache too for future
-   * calls).
+   * #createCodec(DataType, GenericType, boolean)} (and most likely put it in the cache too for
+   * future calls).
    */
-  protected abstract TypeCodec<?> getCachedCodec(DataType cqlType, GenericType<?> javaType);
+  protected abstract TypeCodec<?> getCachedCodec(
+      DataType cqlType, GenericType<?> javaType, boolean isJavaCovariant);
 
   @Override
   public <T> TypeCodec<T> codecFor(DataType cqlType, GenericType<T> javaType) {
     LOG.trace(""[{}] Looking up codec for {} <-> {}"", logPrefix, cqlType, javaType);
-    TypeCodec<?> primitiveCodec = PRIMITIVE_CODECS_BY_CODE.get(cqlType.getProtocolCode());
+    TypeCodec<?> primitiveCodec = primitiveCodecsByCode.get(cqlType.getProtocolCode());
     if (primitiveCodec != null && primitiveCodec.accepts(javaType)) {
       LOG.trace(""[{}] Found matching primitive codec {}"", logPrefix, primitiveCodec);
-      return safeCast(primitiveCodec);
+      return uncheckedCast(primitiveCodec);
     }
     for (TypeCodec<?> userCodec : userCodecs) {
       if (userCodec.accepts(cqlType) && userCodec.accepts(javaType)) {
         LOG.trace(""[{}] Found matching user codec {}"", logPrefix, userCodec);
-        return safeCast(userCodec);
+        return uncheckedCast(userCodec);
       }
     }
-    return safeCast(getCachedCodec(cqlType, javaType));
+    return uncheckedCast(getCachedCodec(cqlType, javaType, false));
   }
 
   @Override
   public <T> TypeCodec<T> codecFor(DataType cqlType, Class<T> javaType) {
     LOG.trace(""[{}] Looking up codec for {} <-> {}"", logPrefix, cqlType, javaType);
-    TypeCodec<?> primitiveCodec = PRIMITIVE_CODECS_BY_CODE.get(cqlType.getProtocolCode());
+    TypeCodec<?> primitiveCodec = primitiveCodecsByCode.get(cqlType.getProtocolCode());
     if (primitiveCodec != null && primitiveCodec.getJavaType().__getToken().getType() == javaType) {
       LOG.trace(""[{}] Found matching primitive codec {}"", logPrefix, primitiveCodec);
-      return safeCast(primitiveCodec);
+      return uncheckedCast(primitiveCodec);
     }
     for (TypeCodec<?> userCodec : userCodecs) {
       if (userCodec.accepts(cqlType) && userCodec.accepts(javaType)) {
         LOG.trace(""[{}] Found matching user codec {}"", logPrefix, userCodec);
-        return safeCast(userCodec);
+        return uncheckedCast(userCodec);
       }
     }
-    return safeCast(getCachedCodec(cqlType, GenericType.of(javaType)));
+    return uncheckedCast(getCachedCodec(cqlType, GenericType.of(javaType), false));
   }
 
   @Override
   public <T> TypeCodec<T> codecFor(DataType cqlType) {
     LOG.trace(""[{}] Looking up codec for CQL type {}"", logPrefix, cqlType);
-    TypeCodec<?> primitiveCodec = PRIMITIVE_CODECS_BY_CODE.get(cqlType.getProtocolCode());
+    TypeCodec<?> primitiveCodec = primitiveCodecsByCode.get(cqlType.getProtocolCode());
     if (primitiveCodec != null) {
       LOG.trace(""[{}] Found matching primitive codec {}"", logPrefix, primitiveCodec);
-      return safeCast(primitiveCodec);
+      return uncheckedCast(primitiveCodec);
     }
     for (TypeCodec<?> userCodec : userCodecs) {
       if (userCodec.accepts(cqlType)) {
         LOG.trace(""[{}] Found matching user codec {}"", logPrefix, userCodec);
-        return safeCast(userCodec);
+        return uncheckedCast(userCodec);
       }
     }
-    return safeCast(getCachedCodec(cqlType, null));
+    return uncheckedCast(getCachedCodec(cqlType, null, false));
   }
 
   @Override
   public <T> TypeCodec<T> codecFor(T value) {
     Preconditions.checkNotNull(value);
     LOG.trace(""[{}] Looking up codec for object {}"", logPrefix, value);
 
-    for (TypeCodec<?> primitiveCodec : PRIMITIVE_CODECS) {
+    for (TypeCodec<?> primitiveCodec : primitiveCodecs) {
       if (primitiveCodec.accepts(value)) {
         LOG.trace(""[{}] Found matching primitive codec {}"", logPrefix, primitiveCodec);
-        return safeCast(primitiveCodec);
+        return uncheckedCast(primitiveCodec);
       }
     }
     for (TypeCodec<?> userCodec : userCodecs) {
       if (userCodec.accepts(value)) {
         LOG.trace(""[{}] Found matching user codec {}"", logPrefix, userCodec);
-        return safeCast(userCodec);
+        return uncheckedCast(userCodec);
       }
     }
 
     if (value instanceof TupleValue) {
-      return safeCast(codecFor(((TupleValue) value).getType(), TupleValue.class));
+      return uncheckedCast(codecFor(((TupleValue) value).getType(), TupleValue.class));
     } else if (value instanceof UdtValue) {
-      return safeCast(codecFor(((UdtValue) value).getType(), UdtValue.class));
+      return uncheckedCast(codecFor(((UdtValue) value).getType(), UdtValue.class));
     }
 
     GenericType<?> javaType = inspectType(value);
     LOG.trace(""[{}] Continuing based on inferred type {}"", logPrefix, javaType);
-    return safeCast(getCachedCodec(null, javaType));
+    return uncheckedCast(getCachedCodec(null, javaType, true));
+  }
+
+  @Override
+  public <T> TypeCodec<T> codecFor(GenericType<T> javaType) {
+    LOG.trace(""[{}] Looking up codec for Java type {}"", logPrefix, javaType);
+    for (TypeCodec<?> primitiveCodec : primitiveCodecs) {
+      if (primitiveCodec.accepts(javaType)) {
+        LOG.trace(""[{}] Found matching primitive codec {}"", logPrefix, primitiveCodec);
+        return uncheckedCast(primitiveCodec);
+      }
+    }
+    for (TypeCodec<?> userCodec : userCodecs) {
+      if (userCodec.accepts(javaType)) {
+        LOG.trace(""[{}] Found matching user codec {}"", logPrefix, userCodec);
+        return uncheckedCast(userCodec);
+      }
+    }
+    return uncheckedCast(getCachedCodec(null, javaType, false));
   }
 
   // Not exposed publicly, this is only used for the recursion from
   // createCovariantCodec(GenericType)
   private TypeCodec<?> covariantCodecFor(GenericType<?> javaType) {","[{'comment': 'This has to be protected, subclasses may need to call this method directly if they override `createCovariantCodec`.', 'commenter': 'adutra'}]"
1000,core/src/main/java/com/datastax/oss/driver/internal/core/metadata/NodeInfo.java,"@@ -44,29 +43,27 @@
    * which the request was made, otherwise the new node will be ignored.
    */
   InetSocketAddress getConnectAddress();
-
   /**
    * The node's broadcast address. That is, the address that other nodes use to communicate with
    * that node.
    *
    * <p>This is only used by the default topology monitor, so if you are writing a custom one and
    * don't need this information, you can leave it empty.
    */
-  Optional<InetAddress> getBroadcastAddress();
+  Optional<InetSocketAddress> getBroadcastAddress();
 
   /**
    * The node's listen address. That is, the address that the Cassandra process binds to.
    *
    * <p>This is currently not used anywhere in the driver. If you write a custom topology monitor
    * and don't need this information, you can leave it empty.
    */
-  Optional<InetAddress> getListenAddress();
-
+  Optional<InetSocketAddress> getListenAddress();
   /**
-   * The data center that this node belongs to, according to the Cassandra snitch.
+   * The node's listen port. That is, the port that the Cassandra process binds to.","[{'comment': 'Accidental comment change?  This was made to `getDatacenter`', 'commenter': 'tolbertam'}]"
1000,core/src/main/java/com/datastax/oss/driver/api/core/config/DefaultDriverOption.java,"@@ -79,6 +79,8 @@
   REQUEST_LOGGER_STACK_TRACES(""request.tracker.logs.show-stack-traces"", false),
 
   CONTROL_CONNECTION_TIMEOUT(""connection.control-connection.timeout"", true),
+  CONTROL_CONNECTION_ALLOW_PORT_DISCOVERY(
+      ""connection.control-connection.allow-port-discovery"", false),","[{'comment': 'Why make this configurable?\r\nThe only use I can think of for this option is ""the cluster has CASSANDRA-7544, but I still want to force the same port for all nodes"". But why would we want to do that? If some nodes actually use different ports, the driver will fail to connect to them. If they all use the same port, I assume that it\'s present in peers_v2 anyway, so we don\'t need to handle this as a special case.', 'commenter': 'olim7t'}, {'comment': ""I wondered the same, there is more info on that on JAVA-1388 (btw why is there 2 tickets?):\r\n```\r\nBy default the client won't using the discovered port numbers. When building the client you have to specify allowing\r\nport discovery. This nit is necessary for transitioning from a cluster where discovered port numbers are for the \r\nnon-SSL port. Only the native port that supports both SSL and non-SSL traffic is propagated. In the future people\r\nwill only use one port for both SSL and non-SSL, but during the transition the client needs to now if it should trust\r\nthe discovered port number or use the one that is supplied in the client configuration.\r\n```\r\nAlthough that explanation is a little unclear to me"", 'commenter': 'newkek'}, {'comment': 're separate tickets: it makes it easier to track progress across the two codebases. Otherwise we mark the JIRA ticket as ""fixed"" when it\'s merged to driver 3 but not yet to driver 4, or vice-versa.', 'commenter': 'olim7t'}, {'comment': 'Agree although I thought both tickets were targeted to driver 4 but maybe I was confused because on JAVA-1338 it says ""Affects Version 4.0.0""', 'commenter': 'newkek'}]"
1000,core/src/main/java/com/datastax/oss/driver/internal/core/metadata/DefaultTopologyMonitor.java,"@@ -188,13 +230,19 @@ public DefaultTopologyMonitor(InternalDriverContext context) {
     return query(channel, queryString, Collections.emptyMap());
   }
 
+  private String fetchPeersTable() {","[{'comment': 'Nit: I find the name a bit misleading, to me ""fetch"" implies that we retrieve it for somewhere (like a database query).', 'commenter': 'olim7t'}]"
1000,core/src/main/java/com/datastax/oss/driver/internal/core/metadata/DefaultTopologyMonitor.java,"@@ -215,9 +263,17 @@ private NodeInfo asNodeInfo(AdminRow row) {
     if (broadcastAddress == null) {
       broadcastAddress = row.getInetAddress(""peer""); // in system.peers
     }
-    builder.withBroadcastAddress(broadcastAddress);
-
-    builder.withListenAddress(row.getInetAddress(""listen_address""));
+    int broadcastPort = 0;
+    if (row.contains(""peer_port"")) {
+      broadcastPort = row.getInteger(""peer_port"");
+    }","[{'comment': 'So we use 0 as a special value for ""port unknown"". This bothers me a bit because 0 already has a different meaning in the JDK: ""A port number of zero will let the system pick up an ephemeral port in a bind operation"".\r\nOn the other hand:\r\n* the alternative would be to keep the two values separate and use `Optional`, which sounds a bit overkill\r\n* this will only happen for `node.getBroadcastAddress()` and `node.getListenAddress()`, which are never used by a typical enterprise application (and even an advanced tool will never bind to them)\r\n\r\nSo overall I\'m fine keeping it this way, but let\'s document it in `Node`\'s javadoc.', 'commenter': 'olim7t'}, {'comment': "":+1:.  The 3.x PR creates a new type [`InetAddressOptPort`](https://github.com/aweisberg/java-driver/blob/648fb67fd14ef0b263b90d50a912c724576a924a/driver-core/src/main/java/com/datastax/driver/core/utils/InetAddressOptPort.java).  Greg and I talked about it and agreed it would be better to just assign the port to 0 in the case it can't be discerned instead of creating a new type.  I agree that we should document these methods and indicate the port can be 0 if it can't be determiend."", 'commenter': 'tolbertam'}]"
1000,core/src/main/java/com/datastax/oss/driver/internal/core/metadata/DefaultTopologyMonitor.java,"@@ -252,4 +305,19 @@ private void savePort(DriverChannel channel) {
       port = ((InetSocketAddress) channel.remoteAddress()).getPort();
     }
   }
+
+  private InetSocketAddress getNativeAddressFromPeers(AdminRow row) {
+    InetAddress native_address = row.getInetAddress(""native_address"");
+    if (native_address == null) {","[{'comment': 'Nit: could you use camel case for variable names?', 'commenter': 'olim7t'}, {'comment': 'Sorry man I this is what I get when I constantly switch from python to java.', 'commenter': 'GregBestland'}]"
1000,core/src/main/java/com/datastax/oss/driver/internal/core/metadata/DefaultTopologyMonitor.java,"@@ -133,10 +156,29 @@ public DefaultTopologyMonitor(InternalDriverContext context) {
     savePort(channel);
 
     CompletionStage<AdminResult> localQuery = query(channel, ""SELECT * FROM system.local"");
-    CompletionStage<AdminResult> peersQuery = query(channel, ""SELECT * FROM system.peers"");
+    CompletionStage<AdminResult> peersV2Query = query(channel, ""SELECT * FROM system.peers_v2"");
+    CompletableFuture<AdminResult> peersQ = new CompletableFuture<>();","[{'comment': 'Nit: could you name this `Query` like the other two variables?', 'commenter': 'olim7t'}, {'comment': ""It saddens me that we have to use an intermediary future for that, but indeed I don't see any other way. We would need a mix of `thenCompose` and `exceptionally` (in other words, chain the second async call only when an exception happened), there is no such method in the API."", 'commenter': 'olim7t'}]"
1000,core/src/main/java/com/datastax/oss/driver/internal/core/metadata/DefaultTopologyMonitor.java,"@@ -133,10 +156,29 @@ public DefaultTopologyMonitor(InternalDriverContext context) {
     savePort(channel);
 
     CompletionStage<AdminResult> localQuery = query(channel, ""SELECT * FROM system.local"");
-    CompletionStage<AdminResult> peersQuery = query(channel, ""SELECT * FROM system.peers"");
+    CompletionStage<AdminResult> peersV2Query = query(channel, ""SELECT * FROM system.peers_v2"");
+    CompletableFuture<AdminResult> peersQ = new CompletableFuture<>();
+
+    peersV2Query.whenComplete(
+        (r, t) -> {
+          if (t != null) {
+            // The query to system.peers_v2 failed, whe should not attempt this query in the future.
+            // Since this method should be run as part of the control connection initialization, we
+            // shouldn't
+            // need this fallback anywhere else.
+            this.isSchemaV2 = false;
+            query(channel, ""SELECT * FROM system.peers"")
+                .whenComplete(
+                    (r2, t2) -> {
+                      peersQ.complete(r2);
+                    });","[{'comment': 'I wrote a utility method for that, you can replace lines 170-174 by:\r\n```java\r\nCompletableFutures.completeFrom(query(channel, ""SELECT * FROM system.peers""), peersQ);\r\n```\r\nAs a bonus, it will also propagate the error if the query fails.', 'commenter': 'olim7t'}]"
1000,core/src/main/java/com/datastax/oss/driver/internal/core/metadata/DefaultTopologyMonitor.java,"@@ -133,10 +156,29 @@ public DefaultTopologyMonitor(InternalDriverContext context) {
     savePort(channel);
 
     CompletionStage<AdminResult> localQuery = query(channel, ""SELECT * FROM system.local"");
-    CompletionStage<AdminResult> peersQuery = query(channel, ""SELECT * FROM system.peers"");
+    CompletionStage<AdminResult> peersV2Query = query(channel, ""SELECT * FROM system.peers_v2"");
+    CompletableFuture<AdminResult> peersQ = new CompletableFuture<>();
+
+    peersV2Query.whenComplete(
+        (r, t) -> {
+          if (t != null) {
+            // The query to system.peers_v2 failed, whe should not attempt this query in the future.
+            // Since this method should be run as part of the control connection initialization, we
+            // shouldn't
+            // need this fallback anywhere else.
+            this.isSchemaV2 = false;
+            query(channel, ""SELECT * FROM system.peers"")
+                .whenComplete(
+                    (r2, t2) -> {
+                      peersQ.complete(r2);
+                    });
+          } else {
+            peersQ.complete(r);
+          }
+        });","[{'comment': ""Could you chain `.exceptionally(UncaughtExceptions::log)` here?\r\nIt's very unlikely that an exception would be thrown from your `whenComplete` callback, but if that happens it would be silently ignored with the current code (because we discard the resulting future). With the log we at least detect it."", 'commenter': 'olim7t'}]"
1000,core/src/main/java/com/datastax/oss/driver/internal/core/metadata/DefaultTopologyMonitor.java,"@@ -133,10 +156,29 @@ public DefaultTopologyMonitor(InternalDriverContext context) {
     savePort(channel);
 
     CompletionStage<AdminResult> localQuery = query(channel, ""SELECT * FROM system.local"");
-    CompletionStage<AdminResult> peersQuery = query(channel, ""SELECT * FROM system.peers"");
+    CompletionStage<AdminResult> peersV2Query = query(channel, ""SELECT * FROM system.peers_v2"");
+    CompletableFuture<AdminResult> peersQ = new CompletableFuture<>();
+
+    peersV2Query.whenComplete(
+        (r, t) -> {
+          if (t != null) {
+            // The query to system.peers_v2 failed, whe should not attempt this query in the future.
+            // Since this method should be run as part of the control connection initialization, we
+            // shouldn't
+            // need this fallback anywhere else.
+            this.isSchemaV2 = false;","[{'comment': ""There is an edge case where it would be done after connection initialization: if we're in a mixed 3.x/4.0 cluster and the first contact point was a 4.0 node, `isSchemaV2` is set when we first connect.\r\nIf the control connection later drops and reconnects to a 3.x node, it will reach this line and flip the flag back to false. But the driver recovers correctly: the first thing we do on a control connection reconnect is a `refreshNodeList`, so there's no risk of running a `refreshNode` with the flag still set to true. And if we're in a mixed cluster, all nodes should still be using the same ports, so it doesn't make any difference whether we use `peers` or `peers_v2` after the reconnection.\r\n\r\nSo I don't think we need to change anything to handle that case."", 'commenter': 'olim7t'}, {'comment': ""agree, also in a mixed version cluster, the user shouldn't be setting up multiple nodes on the same interface, so I agree that it's ok to drop back to the `peers` table permanently."", 'commenter': 'tolbertam'}]"
1000,core/src/main/java/com/datastax/oss/driver/internal/core/metadata/NodeInfo.java,"@@ -44,29 +43,27 @@
    * which the request was made, otherwise the new node will be ignored.
    */
   InetSocketAddress getConnectAddress();
-","[{'comment': 'Nit: probably unintended I guess', 'commenter': 'olim7t'}]"
1000,core/src/main/java/com/datastax/oss/driver/internal/core/metadata/NodeInfo.java,"@@ -44,29 +43,27 @@
    * which the request was made, otherwise the new node will be ignored.
    */
   InetSocketAddress getConnectAddress();
-
   /**
    * The node's broadcast address. That is, the address that other nodes use to communicate with
    * that node.
    *
    * <p>This is only used by the default topology monitor, so if you are writing a custom one and
    * don't need this information, you can leave it empty.
    */
-  Optional<InetAddress> getBroadcastAddress();
+  Optional<InetSocketAddress> getBroadcastAddress();
 
   /**
    * The node's listen address. That is, the address that the Cassandra process binds to.
    *
    * <p>This is currently not used anywhere in the driver. If you write a custom topology monitor
    * and don't need this information, you can leave it empty.
    */
-  Optional<InetAddress> getListenAddress();
-
+  Optional<InetSocketAddress> getListenAddress();
   /**
-   * The data center that this node belongs to, according to the Cassandra snitch.
+   * The node's listen port. That is, the port that the Cassandra process binds to.
    *
-   * <p>This is used by some {@link LoadBalancingPolicy} implementations to compute the {@link
-   * NodeDistance}.
+   * <p>This is currently not used anywhere in the driver. If you write a custom topology monitor
+   * and don't need this information, you can leave it empty.","[{'comment': 'Mmm I disagree, `getDatacenter()` is still used by the default LBP. Not exactly for distance, but non-local nodes are ignored based on this information.', 'commenter': 'olim7t'}]"
1005,core/src/main/java/com/datastax/oss/driver/internal/core/cql/Conversions.java,"@@ -441,4 +450,77 @@ public static CoordinatorException toThrowable(
         return new ProtocolError(node, ""Unknown error code: "" + errorMessage.code);
     }
   }
+
+  /** Returns a common size for all kinds of Request implementations. */","[{'comment': ""I didn't find better class than this to expose these utility methods.."", 'commenter': 'newkek'}]"
1005,core/src/main/java/com/datastax/oss/driver/api/core/cql/Statement.java,"@@ -165,6 +166,20 @@ default T setRoutingKey(ByteBuffer... newRoutingKeyComponents) {
    */
   T setPagingState(ByteBuffer newPagingState);
 
+  /**
+   * Calculates the approximate size in bytes that the statement will have when encoded.
+   *
+   * <p>The size might be over-estimated by a few bytes due to global options that may be defined on
+   * a {@link Session} but not explicitly set on the statement itself.
+   *
+   * <p>The result of this method is not cached, calling it will cause some encoding to be done in
+   * order to determine some of the statement's attributes sizes. Therefore, use this method
+   * sparingly in order to avoid unnecessary computation.
+   *
+   * @return the approximate number of bytes this statement will take when encoded.
+   */
+  int computeSizeInBytes(DriverContext context);","[{'comment': 'The 3.x implementation used the name `requestSizeInBytes` for this method, where we use `computeSizeInBytes`  here.  I think I like the new name better (since `compute` seems more explicit), but just wanted to point out they were different.', 'commenter': 'tolbertam'}, {'comment': ""I was thinking of ways to avoid the `DriverContext` argument (assume the latest protocol version, overestimate more things, etc), in order to avoid the need for an active session to estimate sizes.\r\nBut in the end I think this is futile: if you're working with bound statements (which should be the most common), you need an active session anyway."", 'commenter': 'olim7t'}]"
1005,core/src/main/java/com/datastax/oss/driver/api/core/cql/Statement.java,"@@ -18,6 +18,7 @@
 import com.datastax.oss.driver.api.core.CqlIdentifier;
 import com.datastax.oss.driver.api.core.CqlSession;
 import com.datastax.oss.driver.api.core.config.DriverConfigProfile;
+import com.datastax.oss.driver.api.core.context.DriverContext;","[{'comment': ""The JAVA-708 PR (#978) also introduced new metrics for measuring the number of bytes/received sent (via Inbound/OutboundTrafficMeter channel handlers),  we should make sure to add metrics for that, but it doesn't need to be done in this PR.  I created [JAVA-1829](https://datastax-oss.atlassian.net/browse/JAVA-1829) for that."", 'commenter': 'tolbertam'}, {'comment': ""I'll make an attempt at implementing that (looks easy) and will open up a separate PR."", 'commenter': 'tolbertam'}]"
1005,core/src/main/java/com/datastax/oss/driver/internal/core/ProtocolFeature.java,"@@ -31,5 +31,19 @@
    * @see <a href=""https://issues.apache.org/jira/browse/CASSANDRA-10145"">CASSANDRA-10145</a>
    */
   PER_REQUEST_KEYSPACE,
+
+  /**
+   * The ability to add a custom set of key/value pairs associated to each request.
+   *
+   * @see <a href=""https://issues.apache.org/jira/browse/CASSANDRA-8553"">CASSANDRA-8553</a>
+   */
+  CUSTOM_PAYLOAD,
+
+  /**
+   * The ability at the protocol level to detect when a prepared statement has had its columns altered.","[{'comment': ""This line throws off the formatter as it's too long I think."", 'commenter': 'tolbertam'}, {'comment': ""Right, made this change last minute and didn't re-run `mvn install` thanks "", 'commenter': 'newkek'}]"
1005,core/src/main/java/com/datastax/oss/driver/internal/core/cql/Conversions.java,"@@ -441,4 +450,77 @@ public static CoordinatorException toThrowable(
         return new ProtocolError(node, ""Unknown error code: "" + errorMessage.code);
     }
   }
+
+  /** Returns a common size for all kinds of Request implementations. */
+  static int minimumRequestSize(Request statement, DriverContext context) {
+    Preconditions.checkArgument(
+        context instanceof InternalDriverContext,
+        ""DriverContext provided cannot be used to calculate statement's size"");
+
+    /* Header and payload are common inside a Frame at the protocol level */
+
+    // Frame header has a fixed size of 9 for protocol version > V3, which includes Frame flags size
+    int size = headerEncodedSize();
+
+    if (!statement.getCustomPayload().isEmpty()
+        && ((InternalDriverContext) context)
+            .protocolVersionRegistry()
+            .supports(context.protocolVersion(), ProtocolFeature.CUSTOM_PAYLOAD)) {
+      size += PrimitiveSizes.sizeOfBytesMap(statement.getCustomPayload());
+    }
+
+    /* These are options in the protocol inside a frame that are common to all Statements */
+
+    size += queryFlagsSize(context.protocolVersion().getCode());
+
+    size += PrimitiveSizes.SHORT; // size of consistency level
+    size += PrimitiveSizes.SHORT; // size of serial consistency level
+    size += PrimitiveSizes.LONG; // size of default timestamp","[{'comment': ""It's possible if using a server timestamp generator and timestamp is not explicitly set that we won't be sending a timestamp with the request.   I noticed that the 3.x driver also assumes timestamp is always present.  Do you think we should evaluate the statement and timestamp generator from `DriverContext` to determine if the timestamp will be provided?  On the other hand, using server side timestamps is less common these days so maybe not worth it."", 'commenter': 'tolbertam'}, {'comment': ""Yes initially there wasn't a DriverContext in the methods parameters, only protocol version and codec registry, then I realized it wasn't good without the DriverContext / ProtocolVersionRegistry so I added it. Didn't think it was worth it to make that check but I guess since DriverContext is there now why not"", 'commenter': 'newkek'}]"
1005,core/src/main/java/com/datastax/oss/driver/internal/core/cql/Conversions.java,"@@ -441,4 +450,77 @@ public static CoordinatorException toThrowable(
         return new ProtocolError(node, ""Unknown error code: "" + errorMessage.code);
     }
   }
+
+  /** Returns a common size for all kinds of Request implementations. */
+  static int minimumRequestSize(Request statement, DriverContext context) {
+    Preconditions.checkArgument(
+        context instanceof InternalDriverContext,
+        ""DriverContext provided cannot be used to calculate statement's size"");
+
+    /* Header and payload are common inside a Frame at the protocol level */
+
+    // Frame header has a fixed size of 9 for protocol version > V3, which includes Frame flags size
+    int size = headerEncodedSize();
+
+    if (!statement.getCustomPayload().isEmpty()
+        && ((InternalDriverContext) context)
+            .protocolVersionRegistry()
+            .supports(context.protocolVersion(), ProtocolFeature.CUSTOM_PAYLOAD)) {
+      size += PrimitiveSizes.sizeOfBytesMap(statement.getCustomPayload());
+    }
+
+    /* These are options in the protocol inside a frame that are common to all Statements */
+
+    size += queryFlagsSize(context.protocolVersion().getCode());
+
+    size += PrimitiveSizes.SHORT; // size of consistency level
+    size += PrimitiveSizes.SHORT; // size of serial consistency level
+    size += PrimitiveSizes.LONG; // size of default timestamp
+
+    return size;
+  }
+
+  /**
+   * Returns the size in bytes of a simple statement's values, depending on whether the values are
+   * named or positional.
+   */
+  static int sizeOfSimpleStatementValues(
+      SimpleStatement simpleStatement,
+      ProtocolVersion protocolVersion,
+      CodecRegistry codecRegistry) {
+    int size = 0;
+    if (!simpleStatement.getPositionalValues().isEmpty()) {
+      int totalValuesSize =
+          Values.sizeOfPositionalValues(
+              simpleStatement
+                  .getPositionalValues()
+                  .stream()","[{'comment': 'In general we are trying to avoid using streams if possible (reasoning outlined [here](https://github.com/datastax/java-driver/blob/4.x/CONTRIBUTING.md#no-stream-api)).', 'commenter': 'tolbertam'}, {'comment': 'Hm ok. Will modify to follow the guidelines.', 'commenter': 'newkek'}]"
1005,core/src/main/java/com/datastax/oss/driver/internal/core/cql/Conversions.java,"@@ -489,32 +488,27 @@ static int sizeOfSimpleStatementValues(
       ProtocolVersion protocolVersion,
       CodecRegistry codecRegistry) {
     int size = 0;
+
     if (!simpleStatement.getPositionalValues().isEmpty()) {
-      int totalValuesSize =
-          Values.sizeOfPositionalValues(
-              simpleStatement
-                  .getPositionalValues()
-                  .stream()
-                  // need to convert Object into ByteBuffer
-                  .map(val -> Conversions.encode(val, codecRegistry, protocolVersion))
-                  .collect(Collectors.toList()));
-      size += totalValuesSize;
+
+      List<ByteBuffer> positionalValues =","[{'comment': ""Instead of building up a new List (or Map in the named value case), you could create a running count in place, using `Values.sizeOfValue` for each value and then adding `PrimtiveSizes.SHORT` at the end for the positional value case . That would prevent need for building a new collection, but on the other hand it requires this code to have knowledge about how value collections are sized, so maybe it's better for this to be the done the way you've currently done it as i don't think this code needs to be particularly performant since its not on the hot path (user has to call `computeSizeInBytes`)"", 'commenter': 'tolbertam'}, {'comment': ""Indeed the new map/list instances are not necessary and I could have done it manually, I'd prefer however to calculate the size of values by re-using the utility function that's already there to do it so that we don't duplicate that logic in multiple places if possible."", 'commenter': 'newkek'}]"
1005,core/src/main/java/com/datastax/oss/driver/internal/core/cql/DefaultBatchStatement.java,"@@ -467,4 +473,68 @@ public BatchStatement setTimestamp(long newTimestamp) {
         newTimestamp,
         pagingState);
   }
+
+  @Override
+  public int computeSizeInBytes(DriverContext context) {","[{'comment': ""I think this could be defined as a default method on `BatchStatement` instead. That way potential 3rd-party implementations won't need to reimplement it. Private helpers can be moved to an internal utility class (either `Conversions` or create a new one and move the methods you had added to `Conversions` there as well).\r\n\r\nSame for other statement subtypes."", 'commenter': 'olim7t'}]"
1005,core/src/main/java/com/datastax/oss/driver/internal/core/ProtocolFeature.java,"@@ -31,5 +31,20 @@
    * @see <a href=""https://issues.apache.org/jira/browse/CASSANDRA-10145"">CASSANDRA-10145</a>
    */
   PER_REQUEST_KEYSPACE,
+
+  /**
+   * The ability to add a custom set of key/value pairs associated to each request.
+   *
+   * @see <a href=""https://issues.apache.org/jira/browse/CASSANDRA-8553"">CASSANDRA-8553</a>
+   */
+  CUSTOM_PAYLOAD,
+
+  /**
+   * The ability at the protocol level to detect when a prepared statement has had its columns
+   * altered.
+   *
+   * @see <a href=""https://issues.apache.org/jira/browse/CASSANDRA-10786"">CASSANDRA-10786</a>
+   */
+  PREPARED_RESULT_METADATA_ID,","[{'comment': 'With legacy protocols, `PreparedStatement.getResultMetadataId()` returns null, so you can check that and avoid adding a new `ProtocolFeature`.', 'commenter': 'olim7t'}]"
1005,core/src/main/java/com/datastax/oss/driver/internal/core/cql/Conversions.java,"@@ -15,11 +15,15 @@
  */
 package com.datastax.oss.driver.internal.core.cql;
 
+import static com.datastax.oss.protocol.internal.FrameCodec.headerEncodedSize;
+import static com.datastax.oss.protocol.internal.request.query.QueryOptions.queryFlagsSize;","[{'comment': 'Nit: could you avoid static imports? They make the code harder to follow.\r\nThere are some exceptions for test files (see the contributing guidelines).', 'commenter': 'olim7t'}]"
1005,core/src/main/java/com/datastax/oss/driver/internal/core/cql/Conversions.java,"@@ -441,4 +450,71 @@ public static CoordinatorException toThrowable(
         return new ProtocolError(node, ""Unknown error code: "" + errorMessage.code);
     }
   }
+
+  /** Returns a common size for all kinds of Request implementations. */
+  static int minimumRequestSize(Request statement, DriverContext context) {
+    Preconditions.checkArgument(
+        context instanceof InternalDriverContext,
+        ""DriverContext provided cannot be used to calculate statement's size"");
+
+    /* Header and payload are common inside a Frame at the protocol level */","[{'comment': ""Nit: could you avoid block-style implementation comments? They are annoying because they don't nest, e.g. if I try to enclose the whole method in a block-style comment, this one closes it and I get a compile error."", 'commenter': 'olim7t'}]"
1012,changelog/README.md,"@@ -29,6 +29,7 @@
 - [improvement] JAVA-1772: Revisit multi-response callbacks
 - [new feature] JAVA-1537: Add remaining socket options
 - [bug] JAVA-1756: Propagate custom payload when preparing a statement
+- [new feature] JAVA-1832: Add EC2MultiRegionAddressTranslator","[{'comment': 'Nit: we put new entries first in the 4.x changelog', 'commenter': 'olim7t'}, {'comment': ""d'oh, will fix"", 'commenter': 'tolbertam'}]"
1012,core/src/main/java/com/datastax/oss/driver/internal/core/addresstranslation/EC2MultiRegionAddressTranslator.java,"@@ -0,0 +1,151 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.addresstranslation;
+
+import com.datastax.oss.driver.api.core.addresstranslation.AddressTranslator;
+import com.datastax.oss.driver.api.core.context.DriverContext;
+import com.datastax.oss.driver.shaded.guava.common.annotations.VisibleForTesting;
+import java.net.InetAddress;
+import java.net.InetSocketAddress;
+import java.util.Enumeration;
+import java.util.Hashtable;
+import javax.naming.Context;
+import javax.naming.NamingEnumeration;
+import javax.naming.NamingException;
+import javax.naming.directory.Attribute;
+import javax.naming.directory.Attributes;
+import javax.naming.directory.DirContext;
+import javax.naming.directory.InitialDirContext;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * {@link AddressTranslator} implementation for a multi-region EC2 deployment <b>where clients are
+ * also deployed in EC2</b>.
+ *
+ * <p>Its distinctive feature is that it translates addresses according to the location of the
+ * Cassandra host:
+ *
+ * <ul>
+ *   <li>addresses in different EC2 regions (than the client) are unchanged;
+ *   <li>addresses in the same EC2 region are <b>translated to private IPs</b>.
+ * </ul>
+ *
+ * This optimizes network costs, because Amazon charges more for communication over public IPs.
+ *
+ * <p>Implementation note: this class performs a reverse DNS lookup of the origin address, to find
+ * the domain name of the target instance. Then it performs a forward DNS lookup of the domain name;
+ * the EC2 DNS does the private/public switch automatically based on location.
+ */
+public class EC2MultiRegionAddressTranslator implements AddressTranslator {
+
+  private static final Logger logger =
+      LoggerFactory.getLogger(EC2MultiRegionAddressTranslator.class);
+
+  private final DirContext ctx;
+
+  public EC2MultiRegionAddressTranslator(@SuppressWarnings(""unused"") DriverContext context) {
+    @SuppressWarnings(""JdkObsolete"")
+    Hashtable<Object, Object> env = new Hashtable<>();
+    env.put(Context.INITIAL_CONTEXT_FACTORY, ""com.sun.jndi.dns.DnsContextFactory"");
+    try {
+      ctx = new InitialDirContext(env);
+    } catch (NamingException e) {
+      throw new RuntimeException(""Could not create translator"", e);
+    }
+  }
+
+  @VisibleForTesting
+  EC2MultiRegionAddressTranslator(DirContext ctx) {
+    this.ctx = ctx;
+  }
+
+  @Override
+  public InetSocketAddress translate(InetSocketAddress socketAddress) {
+    InetAddress address = socketAddress.getAddress();
+    try {
+      // InetAddress#getHostName() is supposed to perform a reverse DNS lookup, but for some reason
+      // it doesn't work within the same EC2 region (it returns the IP address itself).
+      // We use an alternate implementation:
+      String domainName = lookupPtrRecord(reverse(address));
+      if (domainName == null) {
+        logger.warn(""Found no domain name for {}, returning it as-is"", address);","[{'comment': 'Could you add a log prefix? You can use `context.sessionName()` like other session-wide policies.', 'commenter': 'olim7t'}]"
1012,manual/core/address_resolution/README.md,"@@ -62,7 +62,7 @@ translation. Write a class that implements [AddressTranslator] with the followin
 
 ```java
 public class MyAddressTranslator implements AddressTranslator {
-  
+","[{'comment': 'ðŸ‘ ', 'commenter': 'olim7t'}]"
1012,core/src/main/java/com/datastax/oss/driver/internal/core/addresstranslation/EC2MultiRegionAddressTranslator.java,"@@ -0,0 +1,151 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.addresstranslation;
+
+import com.datastax.oss.driver.api.core.addresstranslation.AddressTranslator;
+import com.datastax.oss.driver.api.core.context.DriverContext;
+import com.datastax.oss.driver.shaded.guava.common.annotations.VisibleForTesting;
+import java.net.InetAddress;
+import java.net.InetSocketAddress;
+import java.util.Enumeration;
+import java.util.Hashtable;
+import javax.naming.Context;
+import javax.naming.NamingEnumeration;
+import javax.naming.NamingException;
+import javax.naming.directory.Attribute;
+import javax.naming.directory.Attributes;
+import javax.naming.directory.DirContext;
+import javax.naming.directory.InitialDirContext;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * {@link AddressTranslator} implementation for a multi-region EC2 deployment <b>where clients are
+ * also deployed in EC2</b>.
+ *
+ * <p>Its distinctive feature is that it translates addresses according to the location of the
+ * Cassandra host:
+ *
+ * <ul>
+ *   <li>addresses in different EC2 regions (than the client) are unchanged;
+ *   <li>addresses in the same EC2 region are <b>translated to private IPs</b>.
+ * </ul>
+ *
+ * This optimizes network costs, because Amazon charges more for communication over public IPs.
+ *
+ * <p>Implementation note: this class performs a reverse DNS lookup of the origin address, to find
+ * the domain name of the target instance. Then it performs a forward DNS lookup of the domain name;
+ * the EC2 DNS does the private/public switch automatically based on location.
+ */
+public class EC2MultiRegionAddressTranslator implements AddressTranslator {","[{'comment': 'Nit: could you use ""Ec2"" to conform to the 4.x naming rules?', 'commenter': 'olim7t'}]"
1016,clirr-ignores.xml,"@@ -44,6 +44,12 @@
         <justification>This class is only present if the project was compiled with JDK 8+</justification>
     </difference>
 
+    <difference>","[{'comment': ""This seems a bit odd, you shouldn't see Clirr warnings when you add new classes. I don't think you need these exceptions."", 'commenter': 'adutra'}, {'comment': 'I got rid of those changes.', 'commenter': 'jubobs'}]"
1016,driver-extras/src/main/java/com/datastax/driver/extras/codecs/jdk8/LocalDateTimeCodec.java,"@@ -0,0 +1,105 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.driver.extras.codecs.jdk8;
+
+import com.datastax.driver.core.*;
+import com.datastax.driver.core.exceptions.InvalidTypeException;
+
+import java.nio.ByteBuffer;
+
+import static com.datastax.driver.core.ParseUtils.isLongLiteral;
+import static com.datastax.driver.core.ParseUtils.quote;
+
+/**
+ * {@link TypeCodec} that maps
+ * {@link java.time.LocalDateTime} to CQL {@code timestamp},
+ * allowing the setting and retrieval of {@code timestamp}
+ * columns as {@link java.time.LocalDateTime} instances.
+ * <p>
+ * <strong>IMPORTANT</strong>
+ * <p>
+ * 1) The default timestamp formatter used by this codec produces CQL literals
+ * that may include milliseconds.
+ * <strong>This literal format is incompatible with Cassandra < 2.0.9.</strong>
+ * <p>
+ * 2) Even if the ISO-8601 standard accepts timestamps with nanosecond precision,
+ * Cassandra timestamps have millisecond precision; therefore, any sub-millisecond
+ * value set on a {@link java.time.LocalDateTime} will be lost when persisted to Cassandra.
+ *
+ * @see <a href=""https://cassandra.apache.org/doc/cql3/CQL-2.2.html#usingtimestamps"">'Working with timestamps' section of CQL specification</a>
+ */
+@IgnoreJDK6Requirement
+@SuppressWarnings(""Since15"")
+public class LocalDateTimeCodec extends TypeCodec<java.time.LocalDateTime> {
+
+    private static final java.time.format.DateTimeFormatter FORMATTER = java.time.format.DateTimeFormatter.ISO_LOCAL_DATE_TIME;
+
+    public static final LocalDateTimeCodec instance = new LocalDateTimeCodec();
+
+    private LocalDateTimeCodec() {","[{'comment': ""The code is nice, but very similar to `InstantCodec` â€“ why don't you delegate to it instead?"", 'commenter': 'adutra'}, {'comment': 'Does your [earlier comment](https://github.com/datastax/java-driver/pull/1016#discussion_r192770854) make this one obsolete?', 'commenter': 'jubobs'}, {'comment': 'Yes. Sorry for the noise.', 'commenter': 'adutra'}]"
1016,driver-extras/src/main/java/com/datastax/driver/extras/codecs/jdk8/LocalDateTimeCodec.java,"@@ -0,0 +1,105 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.driver.extras.codecs.jdk8;
+
+import com.datastax.driver.core.*;
+import com.datastax.driver.core.exceptions.InvalidTypeException;
+
+import java.nio.ByteBuffer;
+
+import static com.datastax.driver.core.ParseUtils.isLongLiteral;
+import static com.datastax.driver.core.ParseUtils.quote;
+
+/**
+ * {@link TypeCodec} that maps
+ * {@link java.time.LocalDateTime} to CQL {@code timestamp},
+ * allowing the setting and retrieval of {@code timestamp}
+ * columns as {@link java.time.LocalDateTime} instances.
+ * <p>
+ * <strong>IMPORTANT</strong>
+ * <p>
+ * 1) The default timestamp formatter used by this codec produces CQL literals
+ * that may include milliseconds.
+ * <strong>This literal format is incompatible with Cassandra < 2.0.9.</strong>
+ * <p>
+ * 2) Even if the ISO-8601 standard accepts timestamps with nanosecond precision,
+ * Cassandra timestamps have millisecond precision; therefore, any sub-millisecond
+ * value set on a {@link java.time.LocalDateTime} will be lost when persisted to Cassandra.
+ *
+ * @see <a href=""https://cassandra.apache.org/doc/cql3/CQL-2.2.html#usingtimestamps"">'Working with timestamps' section of CQL specification</a>
+ */
+@IgnoreJDK6Requirement
+@SuppressWarnings(""Since15"")
+public class LocalDateTimeCodec extends TypeCodec<java.time.LocalDateTime> {
+
+    private static final java.time.format.DateTimeFormatter FORMATTER = java.time.format.DateTimeFormatter.ISO_LOCAL_DATE_TIME;
+
+    public static final LocalDateTimeCodec instance = new LocalDateTimeCodec();
+
+    private LocalDateTimeCodec() {
+        super(DataType.timestamp(), java.time.LocalDateTime.class);
+    }
+
+    @Override
+    public ByteBuffer serialize(java.time.LocalDateTime value, ProtocolVersion protocolVersion) {
+        if (value == null) {
+            return null;
+        }
+        long millis = value.atZone(java.time.ZoneOffset.UTC).toInstant().toEpochMilli();
+        return bigint().serializeNoBoxing(millis, protocolVersion);
+    }
+
+    @Override
+    public java.time.LocalDateTime deserialize(ByteBuffer bytes, ProtocolVersion protocolVersion) {
+        if (bytes == null || bytes.remaining() == 0) {
+            return null;
+        }
+        long millis = bigint().deserializeNoBoxing(bytes, protocolVersion);
+        return java.time.LocalDateTime.ofInstant(java.time.Instant.ofEpochMilli(millis), java.time.ZoneOffset.UTC);
+    }
+
+    @Override
+    public String format(java.time.LocalDateTime value) {","[{'comment': 'Parse() and format() must produce valid CQL literals. A local date-time is not a valid CQL literal. What you need here is to produce output identical to `InstantCodec`, using UTC. Which is another good reason to re-use `InstantCodec` :)', 'commenter': 'adutra'}, {'comment': 'After giving this some more thought, I think you can leave it that way. Local date-times in ISO-8601 _are_ valid CQL timestamp literals after all, the only drawback is if they were interpreted by C* as full timestamps it would use its own default time zone, and not necessarily UTC.', 'commenter': 'adutra'}]"
1016,driver-extras/src/main/java/com/datastax/driver/extras/codecs/jdk8/ZoneIdCodec.java,"@@ -0,0 +1,83 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.driver.extras.codecs.jdk8;
+
+import com.datastax.driver.core.DataType;
+import com.datastax.driver.core.ProtocolVersion;
+import com.datastax.driver.core.TypeCodec;
+import com.datastax.driver.core.exceptions.InvalidTypeException;
+
+import java.nio.ByteBuffer;
+
+import static com.datastax.driver.core.ParseUtils.*;
+
+/**
+ * {@link TypeCodec} that maps
+ * {@link java.time.ZoneId} to CQL {@code varchar}.
+ */
+@IgnoreJDK6Requirement
+@SuppressWarnings(""Since15"")
+public class ZoneIdCodec extends TypeCodec<java.time.ZoneId> {
+
+    public static final ZoneIdCodec instance = new ZoneIdCodec();
+
+    private ZoneIdCodec() {
+        super(DataType.varchar(), java.time.ZoneId.class);
+    }
+
+    @Override
+    public ByteBuffer serialize(java.time.ZoneId value, ProtocolVersion protocolVersion) {
+        if (value == null) {
+            return null;
+        }
+        return varchar().serialize(value.toString(), protocolVersion);
+    }
+
+    @Override
+    public java.time.ZoneId deserialize(ByteBuffer bytes, ProtocolVersion protocolVersion) {
+        if (bytes == null || bytes.remaining() == 0) {
+            return null;
+        }
+        return java.time.ZoneId.of(varchar().deserialize(bytes, protocolVersion));
+    }
+
+    @Override
+    public String format(java.time.ZoneId value) {","[{'comment': 'Again, here you could leverage `VarcharCodec` to do the actual parse/format just as you did for serialize/deserialize.', 'commenter': 'adutra'}, {'comment': 'Done.', 'commenter': 'jubobs'}]"
1016,driver-extras/src/main/java/com/datastax/driver/extras/codecs/jdk8/LocalDateTimeCodec.java,"@@ -0,0 +1,105 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.driver.extras.codecs.jdk8;
+
+import com.datastax.driver.core.*;
+import com.datastax.driver.core.exceptions.InvalidTypeException;
+
+import java.nio.ByteBuffer;
+
+import static com.datastax.driver.core.ParseUtils.isLongLiteral;
+import static com.datastax.driver.core.ParseUtils.quote;
+
+/**
+ * {@link TypeCodec} that maps
+ * {@link java.time.LocalDateTime} to CQL {@code timestamp},
+ * allowing the setting and retrieval of {@code timestamp}
+ * columns as {@link java.time.LocalDateTime} instances.
+ * <p>
+ * <strong>IMPORTANT</strong>
+ * <p>
+ * 1) The default timestamp formatter used by this codec produces CQL literals
+ * that may include milliseconds.
+ * <strong>This literal format is incompatible with Cassandra < 2.0.9.</strong>
+ * <p>
+ * 2) Even if the ISO-8601 standard accepts timestamps with nanosecond precision,
+ * Cassandra timestamps have millisecond precision; therefore, any sub-millisecond
+ * value set on a {@link java.time.LocalDateTime} will be lost when persisted to Cassandra.
+ *
+ * @see <a href=""https://cassandra.apache.org/doc/cql3/CQL-2.2.html#usingtimestamps"">'Working with timestamps' section of CQL specification</a>
+ */
+@IgnoreJDK6Requirement
+@SuppressWarnings(""Since15"")
+public class LocalDateTimeCodec extends TypeCodec<java.time.LocalDateTime> {
+
+    private static final java.time.format.DateTimeFormatter FORMATTER = java.time.format.DateTimeFormatter.ISO_LOCAL_DATE_TIME;
+
+    public static final LocalDateTimeCodec instance = new LocalDateTimeCodec();
+
+    private LocalDateTimeCodec() {
+        super(DataType.timestamp(), java.time.LocalDateTime.class);
+    }
+
+    @Override
+    public ByteBuffer serialize(java.time.LocalDateTime value, ProtocolVersion protocolVersion) {
+        if (value == null) {
+            return null;
+        }
+        long millis = value.atZone(java.time.ZoneOffset.UTC).toInstant().toEpochMilli();","[{'comment': ""Would it make sense to make the `ZoneOffset` a final field, and expose a constructor that allows to change it? That way you could build a codec instance that matches your server's timezone."", 'commenter': 'olim7t'}, {'comment': 'Since `LocalDateTime` is meant to be a description of time, and not an actual zoned instant of time, I think it makes sense to make it completely devoid of time zone information and use UTC internally like it currently does.', 'commenter': 'tolbertam'}]"
1022,core/src/main/java/com/datastax/oss/driver/api/core/data/AccessibleByName.java,"@@ -56,4 +56,18 @@
    * @throws IndexOutOfBoundsException if the index is invalid.
    */
   DataType getType(String name);
+
+  /**
+   * Try to return the first index where a given identifier appears.
+   *
+   * @see AccessibleByName#firstIndexOf
+   * @throws IllegalArgumentException if name was not found
+   */
+  default int tryFirstIndexOf(String name) {
+    final int indexOf = firstIndexOf(name);
+    if (indexOf == -1) {
+      throw new IllegalArgumentException(name + "" is not a column defined in this metadata"");
+    }
+    return indexOf;
+  }","[{'comment': 'This looks good to me, but since `firstIndexOf` is not called from anywhere else anymore (actually it is from `DefaultRow.getType`, but we would benefit from the improved message there as well), we might as well modify the contract of `firstIndexOf` directly,  instead of adding a separate method: specify in the javadoc that an `IllegalArgumentException` should be thrown, and change the 4 existing implementations. This duplicates the check in each implementation, but we can also give an even better message: ""is not a [column|field|variable] in this [row|UDT|bound statement]"".', 'commenter': 'olim7t'}, {'comment': 'Also, the same should be done for `AccessibleById`.', 'commenter': 'olim7t'}]"
1023,driver-core/src/main/java/com/datastax/driver/core/HostConnectionPool.java,"@@ -115,7 +115,7 @@ public void run() {
 
         int toCreate = coreSize;
 
-        if (reusedConnection != null && reusedConnection.setOwner(this)) {
+        if (reusedConnection != null && reusedConnection.setOwner(this) && toCreate > 0) {","[{'comment': ""I think we should change this slighty to:\r\n\r\n> if (reusedConnection != null && toCreate > 0 && reusedConnection.setOwner(this)) {\r\n\r\nThis way we don't invoke `setOwner` on the connection to this pool unless `toCreate` is greater than 0.  As we don't want to assign ownership unless we will actually do it because then the connection will not get closed in `Cluster.onUp` where we check if the connection has an owner and if not close it."", 'commenter': 'tolbertam'}]"
1023,driver-core/src/main/java/com/datastax/driver/core/HostConnectionPool.java,"@@ -115,7 +115,7 @@ public void run() {
 
         int toCreate = coreSize;
 
-        if (reusedConnection != null && reusedConnection.setOwner(this)) {
+        if (reusedConnection != null && toCreate > 0 && reusedConnection.setOwner(this)) {","[{'comment': ""I was wondering if the connection was getting closed if we don't enter this condition; it's handled in the `finally` block in `Cluster.Manager.onUp` so ðŸ‘ ."", 'commenter': 'olim7t'}]"
1028,integration-tests/src/test/java/com/datastax/oss/driver/api/core/tracker/RequestLoggerIT.java,"@@ -189,4 +215,63 @@ public void should_not_log_when_disabled() throws InterruptedException {
     TimeUnit.MILLISECONDS.sleep(500);
     Mockito.verify(appender, never()).doAppend(any(LoggingEvent.class));
   }
+
+  @Test
+  public void should_log_failed_nodes_on_succesfull_request() {","[{'comment': 'nit: successfull -> successful', 'commenter': 'tolbertam'}]"
1028,core/src/main/java/com/datastax/oss/driver/api/core/tracker/RequestTracker.java,"@@ -48,4 +48,32 @@ void onError(
       long latencyNanos,
       DriverConfigProfile configProfile,
       Node node);
+
+  /**
+   * Invoked each time a request fails at the node level. Similar to onError but at a per node
+   * level.
+   *
+   * @param latencyNanos the overall execution time (from the {@link Session#execute(Request,
+   *     GenericType) session.execute} call until the error is propagated to the client).
+   * @param configProfile the configuration profile that this request was executed with.
+   * @param node the node that returned the error response, or {@code null} if the error occurred
+   */
+  void onNodeError(
+      Request request,
+      Throwable error,
+      long latencyNanos,
+      DriverConfigProfile configProfile,
+      Node node);
+
+  /**
+   * Invoked each time a request succeeds at the node level. Similar to on Success but at per Node","[{'comment': '`on Success` -> `onSuccess` and since referring to another method it would be useful to use `{@link #onSuccess(...)}` so you can click to it in javadoc.  (same for `onError` above).', 'commenter': 'tolbertam'}]"
1028,core/src/main/java/com/datastax/oss/driver/api/core/tracker/RequestTracker.java,"@@ -48,4 +48,32 @@ void onError(
       long latencyNanos,
       DriverConfigProfile configProfile,
       Node node);
+
+  /**
+   * Invoked each time a request fails at the node level. Similar to onError but at a per node
+   * level.
+   *
+   * @param latencyNanos the overall execution time (from the {@link Session#execute(Request,
+   *     GenericType) session.execute} call until the error is propagated to the client).
+   * @param configProfile the configuration profile that this request was executed with.
+   * @param node the node that returned the error response, or {@code null} if the error occurred","[{'comment': ""This part was copied over and I think it may technically be incorrect for this new method:\r\n\r\n> node the node that returned the error response, or `{@code null}` if the error occurred\r\n\r\nSince it's invoked for a specific node, won't it always be non-null?"", 'commenter': 'tolbertam'}]"
1028,core/src/main/java/com/datastax/oss/driver/internal/core/cql/CqlRequestHandlerBase.java,"@@ -339,9 +347,13 @@ public void onThrottleFailure(RequestThrottlingException error) {
   }
 
   private void setFinalError(Throwable error, Node node) {
+
     if (result.completeExceptionally(error)) {
       cancelScheduledTasks();
       long latencyNanos = System.nanoTime() - startTimeNanos;
+      if (node != null) {
+        context.requestTracker().onNodeError(statement, error, latencyNanos, configProfile, node);","[{'comment': ""Couldn't this also use `trackNodeError`?"", 'commenter': 'tolbertam'}, {'comment': 'I see, this is explained by eac3282b, nevermind :)', 'commenter': 'tolbertam'}]"
1028,core/src/main/java/com/datastax/oss/driver/api/core/tracker/RequestTracker.java,"@@ -48,4 +48,32 @@ void onError(
       long latencyNanos,
       DriverConfigProfile configProfile,
       Node node);
+
+  /**
+   * Invoked each time a request fails at the node level. Similar to onError but at a per node
+   * level.
+   *
+   * @param latencyNanos the overall execution time (from the {@link Session#execute(Request,
+   *     GenericType) session.execute} call until the error is propagated to the client).","[{'comment': ""I completely missed this on my first review: the latency passed to node-level methods should be measured from the time we sent the request to that node, not from the beginning of the whole request cycle. Right now it includes all tries on previous nodes.\r\n\r\nThere is already a `NodeResponseCallback.start` field for metrics, you can just use that instead of `startTimeNanos` (the name `start` is not great by the way, maybe we could rename it to `nodeStartTimeNanos` for clarity).\r\n\r\nThat also means we won't be able to handle the final node in `setFinalError`, so instead just add a call to `trackNodeError` everywhere `setFinalError` is called in the `NodeResponseCallback`."", 'commenter': 'olim7t'}]"
1031,manual/statements/built/README.md,"@@ -1,5 +1,268 @@
 ## Built statements
 
-*Coming soon... In the meantime, see the javadoc for [QueryBuilder].*
+Built statements are generated via [QueryBuilder]'s Fluent API.  Use of Fluent API allows","[{'comment': 'Suggestion: ""Use of Fluent API allows for easier build of complex queries, as opposed to the use of hardcoded query strings.""', 'commenter': 'adutra'}]"
1031,manual/statements/built/README.md,"@@ -1,5 +1,268 @@
 ## Built statements
 
-*Coming soon... In the meantime, see the javadoc for [QueryBuilder].*
+Built statements are generated via [QueryBuilder]'s Fluent API.  Use of Fluent API allows
+easier build of the complex queries, comparing with hardcoding the CQL statements into the
+source code.
 
-[QueryBuilder]: http://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html
\ No newline at end of file
+Note: The provider builders perform very little validation of the built query. There is","[{'comment': 'Typo: ""The provided builders"".', 'commenter': 'adutra'}]"
1031,manual/statements/built/README.md,"@@ -1,5 +1,268 @@
 ## Built statements
 
-*Coming soon... In the meantime, see the javadoc for [QueryBuilder].*
+Built statements are generated via [QueryBuilder]'s Fluent API.  Use of Fluent API allows
+easier build of the complex queries, comparing with hardcoding the CQL statements into the
+source code.
 
-[QueryBuilder]: http://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html
\ No newline at end of file
+Note: The provider builders perform very little validation of the built query. There is
+thus no guarantee that a built query is valid, and it is definitively possible to create
+invalid queries.
+
+Queries built with `QueryBuilder` are execute the same way as other queries - via","[{'comment': 'Typo: ""are executed""', 'commenter': 'adutra'}]"
1031,manual/statements/built/README.md,"@@ -1,5 +1,268 @@
 ## Built statements
 
-*Coming soon... In the meantime, see the javadoc for [QueryBuilder].*
+Built statements are generated via [QueryBuilder]'s Fluent API.  Use of Fluent API allows
+easier build of the complex queries, comparing with hardcoding the CQL statements into the
+source code.
 
-[QueryBuilder]: http://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html
\ No newline at end of file
+Note: The provider builders perform very little validation of the built query. There is
+thus no guarantee that a built query is valid, and it is definitively possible to create
+invalid queries.
+
+Queries built with `QueryBuilder` are execute the same way as other queries - via
+`execute` or `executeAsync`.  When query is built with putting all data into it, then it","[{'comment': 'Nit: "" When a query is built with inlined values, then it doesn\'t differ much from a statement specified as a string."" ', 'commenter': 'adutra'}]"
1031,manual/statements/built/README.md,"@@ -1,5 +1,268 @@
 ## Built statements
 
-*Coming soon... In the meantime, see the javadoc for [QueryBuilder].*
+Built statements are generated via [QueryBuilder]'s Fluent API.  Use of Fluent API allows
+easier build of the complex queries, comparing with hardcoding the CQL statements into the
+source code.
 
-[QueryBuilder]: http://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html
\ No newline at end of file
+Note: The provider builders perform very little validation of the built query. There is
+thus no guarantee that a built query is valid, and it is definitively possible to create
+invalid queries.
+
+Queries built with `QueryBuilder` are execute the same way as other queries - via
+`execute` or `executeAsync`.  When query is built with putting all data into it, then it
+doesn't differ much from statement specified as string.  But it's also possible to build
+the query with bind markers inside it, and then convert it into [prepared statement](../prepared/).","[{'comment': 'Nit: ""into a prepared statement"".', 'commenter': 'adutra'}]"
1031,manual/statements/built/README.md,"@@ -1,5 +1,268 @@
 ## Built statements
 
-*Coming soon... In the meantime, see the javadoc for [QueryBuilder].*
+Built statements are generated via [QueryBuilder]'s Fluent API.  Use of Fluent API allows
+easier build of the complex queries, comparing with hardcoding the CQL statements into the
+source code.
 
-[QueryBuilder]: http://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html
\ No newline at end of file
+Note: The provider builders perform very little validation of the built query. There is
+thus no guarantee that a built query is valid, and it is definitively possible to create
+invalid queries.
+
+Queries built with `QueryBuilder` are execute the same way as other queries - via
+`execute` or `executeAsync`.  When query is built with putting all data into it, then it
+doesn't differ much from statement specified as string.  But it's also possible to build
+the query with bind markers inside it, and then convert it into [prepared statement](../prepared/).
+
+### Basics
+
+Generation of `BuiltStatement` is easy - just start with calling of one of the","[{'comment': 'Typo: ""just start by calling one of the...""', 'commenter': 'adutra'}, {'comment': 'Nit: The English typography rules mandate the usage of an em-dash for incises, and without spaces: "" ... is easyâ€”just start...""', 'commenter': 'adutra'}]"
1031,manual/statements/built/README.md,"@@ -1,5 +1,268 @@
 ## Built statements
 
-*Coming soon... In the meantime, see the javadoc for [QueryBuilder].*
+Built statements are generated via [QueryBuilder]'s Fluent API.  Use of Fluent API allows
+easier build of the complex queries, comparing with hardcoding the CQL statements into the
+source code.
 
-[QueryBuilder]: http://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html
\ No newline at end of file
+Note: The provider builders perform very little validation of the built query. There is
+thus no guarantee that a built query is valid, and it is definitively possible to create
+invalid queries.
+
+Queries built with `QueryBuilder` are execute the same way as other queries - via
+`execute` or `executeAsync`.  When query is built with putting all data into it, then it
+doesn't differ much from statement specified as string.  But it's also possible to build
+the query with bind markers inside it, and then convert it into [prepared statement](../prepared/).
+
+### Basics
+
+Generation of `BuiltStatement` is easy - just start with calling of one of the
+[QueryBuilder]'s methods of that represent the CQL ""verb"": `select`, `update`, `delete`,
+`insertInto`, or `truncate`, providing required parameters, and then call ""verb""-specific
+functions to form a complete CQL statement (like, `where`, `from`, etc.).  The command's","[{'comment': 'Suggestion; ""The statement\'s target table can be specified as a simple table name (if a default keyspace has been set when creating the `Session` object), as a combination of keyspace name and table name, or as a [TableMetadata] object.""  ', 'commenter': 'adutra'}]"
1031,manual/statements/built/README.md,"@@ -1,5 +1,268 @@
 ## Built statements
 
-*Coming soon... In the meantime, see the javadoc for [QueryBuilder].*
+Built statements are generated via [QueryBuilder]'s Fluent API.  Use of Fluent API allows
+easier build of the complex queries, comparing with hardcoding the CQL statements into the
+source code.
 
-[QueryBuilder]: http://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html
\ No newline at end of file
+Note: The provider builders perform very little validation of the built query. There is
+thus no guarantee that a built query is valid, and it is definitively possible to create
+invalid queries.
+
+Queries built with `QueryBuilder` are execute the same way as other queries - via
+`execute` or `executeAsync`.  When query is built with putting all data into it, then it
+doesn't differ much from statement specified as string.  But it's also possible to build
+the query with bind markers inside it, and then convert it into [prepared statement](../prepared/).
+
+### Basics
+
+Generation of `BuiltStatement` is easy - just start with calling of one of the
+[QueryBuilder]'s methods of that represent the CQL ""verb"": `select`, `update`, `delete`,
+`insertInto`, or `truncate`, providing required parameters, and then call ""verb""-specific
+functions to form a complete CQL statement (like, `where`, `from`, etc.).  The command's
+target could be specified as table name (if you set default keyspace when creating a
+`Session` object), keyspace & table name, or as [TableMetadata].
+
+Note: The `QueryBuilder` doesn't provide support for full set of CQL.  For generation of","[{'comment': 'Suggestion: ""...for the full set of CQL statements"".', 'commenter': 'adutra'}, {'comment': 'Suggestion: ""For DDL operations...""', 'commenter': 'adutra'}]"
1031,manual/statements/built/README.md,"@@ -1,5 +1,268 @@
 ## Built statements
 
-*Coming soon... In the meantime, see the javadoc for [QueryBuilder].*
+Built statements are generated via [QueryBuilder]'s Fluent API.  Use of Fluent API allows
+easier build of the complex queries, comparing with hardcoding the CQL statements into the
+source code.
 
-[QueryBuilder]: http://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html
\ No newline at end of file
+Note: The provider builders perform very little validation of the built query. There is
+thus no guarantee that a built query is valid, and it is definitively possible to create
+invalid queries.
+
+Queries built with `QueryBuilder` are execute the same way as other queries - via
+`execute` or `executeAsync`.  When query is built with putting all data into it, then it
+doesn't differ much from statement specified as string.  But it's also possible to build
+the query with bind markers inside it, and then convert it into [prepared statement](../prepared/).
+
+### Basics
+
+Generation of `BuiltStatement` is easy - just start with calling of one of the
+[QueryBuilder]'s methods of that represent the CQL ""verb"": `select`, `update`, `delete`,
+`insertInto`, or `truncate`, providing required parameters, and then call ""verb""-specific
+functions to form a complete CQL statement (like, `where`, `from`, etc.).  The command's
+target could be specified as table name (if you set default keyspace when creating a
+`Session` object), keyspace & table name, or as [TableMetadata].
+
+Note: The `QueryBuilder` doesn't provide support for full set of CQL.  For generation of
+DDL operations (`CREATE TABLE`, etc.) you can use the [SchemaBuilder].  To perform other
+operations you still need to use [simple statements](../simple/).
+
+### Selecting data
+
+Selection of data is quite simple - at minimum you need to provide a list of columns to
+select, and the specify from which table to select these columns (and conditions as","[{'comment': 'Typo + suggestion: ""...and then specify which table to select these columns from (you can also optionally specify a condition, as described in the next section):""', 'commenter': 'adutra'}]"
1031,manual/statements/built/README.md,"@@ -1,5 +1,268 @@
 ## Built statements
 
-*Coming soon... In the meantime, see the javadoc for [QueryBuilder].*
+Built statements are generated via [QueryBuilder]'s Fluent API.  Use of Fluent API allows
+easier build of the complex queries, comparing with hardcoding the CQL statements into the
+source code.
 
-[QueryBuilder]: http://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html
\ No newline at end of file
+Note: The provider builders perform very little validation of the built query. There is
+thus no guarantee that a built query is valid, and it is definitively possible to create
+invalid queries.
+
+Queries built with `QueryBuilder` are execute the same way as other queries - via
+`execute` or `executeAsync`.  When query is built with putting all data into it, then it
+doesn't differ much from statement specified as string.  But it's also possible to build
+the query with bind markers inside it, and then convert it into [prepared statement](../prepared/).
+
+### Basics
+
+Generation of `BuiltStatement` is easy - just start with calling of one of the
+[QueryBuilder]'s methods of that represent the CQL ""verb"": `select`, `update`, `delete`,
+`insertInto`, or `truncate`, providing required parameters, and then call ""verb""-specific
+functions to form a complete CQL statement (like, `where`, `from`, etc.).  The command's
+target could be specified as table name (if you set default keyspace when creating a
+`Session` object), keyspace & table name, or as [TableMetadata].
+
+Note: The `QueryBuilder` doesn't provide support for full set of CQL.  For generation of
+DDL operations (`CREATE TABLE`, etc.) you can use the [SchemaBuilder].  To perform other
+operations you still need to use [simple statements](../simple/).
+
+### Selecting data
+
+Selection of data is quite simple - at minimum you need to provide a list of columns to
+select, and the specify from which table to select these columns (and conditions as
+described in the next section):
+
+```java
+BuiltStatement selectAll1 = QueryBuilder.select(""id"", ""t"").from(""test"", ""test"");
+ResultSet rs = session.execute(selectAll1);
+for (Row row: rs) {
+   System.out.println(row);
+}
+```
+
+Note: The call `select(""column1"", ""column2"")` is really a shortcut for a chain of calls
+`select().column(""column1"").column(""column2"")`.
+
+Please note that you can't pass the `*` as column name to select all columns - if you do
+this, you'll get an exception about unknown column. To select all columns you either need to use
+`select` in combination with `all` function, or simply don't specify a list of columns:
+
+```java
+BuiltStatement selectAll2 = QueryBuilder.select().all().from(""test"", ""test"");
+BuiltStatement selectAll3 = QueryBuilder.select().from(""test"", ""test"");
+```
+
+Besides selection of the specific columns there is also a possibility of calling of","[{'comment': 'Suggestion: ""...of selecting arbitrary functions...""', 'commenter': 'adutra'}]"
1031,manual/statements/built/README.md,"@@ -1,5 +1,268 @@
 ## Built statements
 
-*Coming soon... In the meantime, see the javadoc for [QueryBuilder].*
+Built statements are generated via [QueryBuilder]'s Fluent API.  Use of Fluent API allows
+easier build of the complex queries, comparing with hardcoding the CQL statements into the
+source code.
 
-[QueryBuilder]: http://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html
\ No newline at end of file
+Note: The provider builders perform very little validation of the built query. There is
+thus no guarantee that a built query is valid, and it is definitively possible to create
+invalid queries.
+
+Queries built with `QueryBuilder` are execute the same way as other queries - via
+`execute` or `executeAsync`.  When query is built with putting all data into it, then it
+doesn't differ much from statement specified as string.  But it's also possible to build
+the query with bind markers inside it, and then convert it into [prepared statement](../prepared/).
+
+### Basics
+
+Generation of `BuiltStatement` is easy - just start with calling of one of the
+[QueryBuilder]'s methods of that represent the CQL ""verb"": `select`, `update`, `delete`,
+`insertInto`, or `truncate`, providing required parameters, and then call ""verb""-specific
+functions to form a complete CQL statement (like, `where`, `from`, etc.).  The command's
+target could be specified as table name (if you set default keyspace when creating a
+`Session` object), keyspace & table name, or as [TableMetadata].
+
+Note: The `QueryBuilder` doesn't provide support for full set of CQL.  For generation of
+DDL operations (`CREATE TABLE`, etc.) you can use the [SchemaBuilder].  To perform other
+operations you still need to use [simple statements](../simple/).
+
+### Selecting data
+
+Selection of data is quite simple - at minimum you need to provide a list of columns to
+select, and the specify from which table to select these columns (and conditions as
+described in the next section):
+
+```java
+BuiltStatement selectAll1 = QueryBuilder.select(""id"", ""t"").from(""test"", ""test"");
+ResultSet rs = session.execute(selectAll1);
+for (Row row: rs) {
+   System.out.println(row);
+}
+```
+
+Note: The call `select(""column1"", ""column2"")` is really a shortcut for a chain of calls
+`select().column(""column1"").column(""column2"")`.
+
+Please note that you can't pass the `*` as column name to select all columns - if you do
+this, you'll get an exception about unknown column. To select all columns you either need to use
+`select` in combination with `all` function, or simply don't specify a list of columns:
+
+```java
+BuiltStatement selectAll2 = QueryBuilder.select().all().from(""test"", ""test"");
+BuiltStatement selectAll3 = QueryBuilder.select().from(""test"", ""test"");
+```
+
+Besides selection of the specific columns there is also a possibility of calling of
+arbitrary function by using the `fcall` (this is just example, don't do this on real
+data):
+
+```java
+BuiltStatement sum = QueryBuilder.select().fcall(""sum"", column(""id"")).as(""sum_id"")
+        .from(""test"", ""test"");
+```
+
+Note: When using functions, Cassandra will generate column names for you, but you can
+provide explicit name by using `as` right after given expression.","[{'comment': 'Suggestion: ""Cassandra will generate aliases for you, but you can provide explicit aliases by using `as` right after a given selector.""', 'commenter': 'adutra'}]"
1031,manual/statements/built/README.md,"@@ -1,5 +1,268 @@
 ## Built statements
 
-*Coming soon... In the meantime, see the javadoc for [QueryBuilder].*
+Built statements are generated via [QueryBuilder]'s Fluent API.  Use of Fluent API allows
+easier build of the complex queries, comparing with hardcoding the CQL statements into the
+source code.
 
-[QueryBuilder]: http://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html
\ No newline at end of file
+Note: The provider builders perform very little validation of the built query. There is
+thus no guarantee that a built query is valid, and it is definitively possible to create
+invalid queries.
+
+Queries built with `QueryBuilder` are execute the same way as other queries - via
+`execute` or `executeAsync`.  When query is built with putting all data into it, then it
+doesn't differ much from statement specified as string.  But it's also possible to build
+the query with bind markers inside it, and then convert it into [prepared statement](../prepared/).
+
+### Basics
+
+Generation of `BuiltStatement` is easy - just start with calling of one of the
+[QueryBuilder]'s methods of that represent the CQL ""verb"": `select`, `update`, `delete`,
+`insertInto`, or `truncate`, providing required parameters, and then call ""verb""-specific
+functions to form a complete CQL statement (like, `where`, `from`, etc.).  The command's
+target could be specified as table name (if you set default keyspace when creating a
+`Session` object), keyspace & table name, or as [TableMetadata].","[{'comment': 'Nit: avoid using the ampersand sign for ""and"".', 'commenter': 'adutra'}]"
1031,manual/statements/built/README.md,"@@ -1,5 +1,268 @@
 ## Built statements
 
-*Coming soon... In the meantime, see the javadoc for [QueryBuilder].*
+Built statements are generated via [QueryBuilder]'s Fluent API.  Use of Fluent API allows
+easier build of the complex queries, comparing with hardcoding the CQL statements into the
+source code.
 
-[QueryBuilder]: http://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html
\ No newline at end of file
+Note: The provider builders perform very little validation of the built query. There is
+thus no guarantee that a built query is valid, and it is definitively possible to create
+invalid queries.
+
+Queries built with `QueryBuilder` are execute the same way as other queries - via
+`execute` or `executeAsync`.  When query is built with putting all data into it, then it
+doesn't differ much from statement specified as string.  But it's also possible to build
+the query with bind markers inside it, and then convert it into [prepared statement](../prepared/).
+
+### Basics
+
+Generation of `BuiltStatement` is easy - just start with calling of one of the
+[QueryBuilder]'s methods of that represent the CQL ""verb"": `select`, `update`, `delete`,
+`insertInto`, or `truncate`, providing required parameters, and then call ""verb""-specific
+functions to form a complete CQL statement (like, `where`, `from`, etc.).  The command's
+target could be specified as table name (if you set default keyspace when creating a
+`Session` object), keyspace & table name, or as [TableMetadata].
+
+Note: The `QueryBuilder` doesn't provide support for full set of CQL.  For generation of
+DDL operations (`CREATE TABLE`, etc.) you can use the [SchemaBuilder].  To perform other
+operations you still need to use [simple statements](../simple/).
+
+### Selecting data
+
+Selection of data is quite simple - at minimum you need to provide a list of columns to
+select, and the specify from which table to select these columns (and conditions as
+described in the next section):
+
+```java
+BuiltStatement selectAll1 = QueryBuilder.select(""id"", ""t"").from(""test"", ""test"");
+ResultSet rs = session.execute(selectAll1);
+for (Row row: rs) {
+   System.out.println(row);
+}
+```
+
+Note: The call `select(""column1"", ""column2"")` is really a shortcut for a chain of calls
+`select().column(""column1"").column(""column2"")`.
+
+Please note that you can't pass the `*` as column name to select all columns - if you do
+this, you'll get an exception about unknown column. To select all columns you either need to use
+`select` in combination with `all` function, or simply don't specify a list of columns:
+
+```java
+BuiltStatement selectAll2 = QueryBuilder.select().all().from(""test"", ""test"");
+BuiltStatement selectAll3 = QueryBuilder.select().from(""test"", ""test"");
+```
+
+Besides selection of the specific columns there is also a possibility of calling of
+arbitrary function by using the `fcall` (this is just example, don't do this on real
+data):
+
+```java
+BuiltStatement sum = QueryBuilder.select().fcall(""sum"", column(""id"")).as(""sum_id"")
+        .from(""test"", ""test"");
+```
+
+Note: When using functions, Cassandra will generate column names for you, but you can
+provide explicit name by using `as` right after given expression.
+
+For often used functions, there are shortcuts, such as, `countAll`, `ttl`, `writeTime`,
+`uuid`, `now`, `toJson`, etc.:
+
+```
+BuiltStatement count = QueryBuilder.select().countAll()
+        .from(""test"", ""test"");
+
+BuiltStatement ttlAndWriteTime = QueryBuilder.select().column(""id"").column(""t"")
+                .ttl(""t"").as(""id_ttl"").writeTime(""t"")
+                .from(""test"", ""test"");
+```
+
+You can also cast the value of the given column to another type by using the `cast`
+function,
+[specifying](https://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html#cast-java.lang.Object-com.datastax.driver.core.DataType-)
+the column for which it should be performed & to what type it should be casted.
+
+#### Specifying conditions
+
+Selection of data rarely happen on the whole table - in most cases, people are interested
+for particular rows, located in one or several partitions.  Conditions are specified by","[{'comment': 'Typoe: ""interested in""', 'commenter': 'adutra'}]"
1031,manual/statements/built/README.md,"@@ -1,5 +1,268 @@
 ## Built statements
 
-*Coming soon... In the meantime, see the javadoc for [QueryBuilder].*
+Built statements are generated via [QueryBuilder]'s Fluent API.  Use of Fluent API allows
+easier build of the complex queries, comparing with hardcoding the CQL statements into the
+source code.
 
-[QueryBuilder]: http://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html
\ No newline at end of file
+Note: The provider builders perform very little validation of the built query. There is
+thus no guarantee that a built query is valid, and it is definitively possible to create
+invalid queries.
+
+Queries built with `QueryBuilder` are execute the same way as other queries - via
+`execute` or `executeAsync`.  When query is built with putting all data into it, then it
+doesn't differ much from statement specified as string.  But it's also possible to build
+the query with bind markers inside it, and then convert it into [prepared statement](../prepared/).
+
+### Basics
+
+Generation of `BuiltStatement` is easy - just start with calling of one of the
+[QueryBuilder]'s methods of that represent the CQL ""verb"": `select`, `update`, `delete`,
+`insertInto`, or `truncate`, providing required parameters, and then call ""verb""-specific
+functions to form a complete CQL statement (like, `where`, `from`, etc.).  The command's
+target could be specified as table name (if you set default keyspace when creating a
+`Session` object), keyspace & table name, or as [TableMetadata].
+
+Note: The `QueryBuilder` doesn't provide support for full set of CQL.  For generation of
+DDL operations (`CREATE TABLE`, etc.) you can use the [SchemaBuilder].  To perform other
+operations you still need to use [simple statements](../simple/).
+
+### Selecting data
+
+Selection of data is quite simple - at minimum you need to provide a list of columns to
+select, and the specify from which table to select these columns (and conditions as
+described in the next section):
+
+```java
+BuiltStatement selectAll1 = QueryBuilder.select(""id"", ""t"").from(""test"", ""test"");
+ResultSet rs = session.execute(selectAll1);
+for (Row row: rs) {
+   System.out.println(row);
+}
+```
+
+Note: The call `select(""column1"", ""column2"")` is really a shortcut for a chain of calls
+`select().column(""column1"").column(""column2"")`.
+
+Please note that you can't pass the `*` as column name to select all columns - if you do
+this, you'll get an exception about unknown column. To select all columns you either need to use
+`select` in combination with `all` function, or simply don't specify a list of columns:
+
+```java
+BuiltStatement selectAll2 = QueryBuilder.select().all().from(""test"", ""test"");
+BuiltStatement selectAll3 = QueryBuilder.select().from(""test"", ""test"");
+```
+
+Besides selection of the specific columns there is also a possibility of calling of
+arbitrary function by using the `fcall` (this is just example, don't do this on real
+data):
+
+```java
+BuiltStatement sum = QueryBuilder.select().fcall(""sum"", column(""id"")).as(""sum_id"")
+        .from(""test"", ""test"");
+```
+
+Note: When using functions, Cassandra will generate column names for you, but you can
+provide explicit name by using `as` right after given expression.
+
+For often used functions, there are shortcuts, such as, `countAll`, `ttl`, `writeTime`,
+`uuid`, `now`, `toJson`, etc.:
+
+```
+BuiltStatement count = QueryBuilder.select().countAll()
+        .from(""test"", ""test"");
+
+BuiltStatement ttlAndWriteTime = QueryBuilder.select().column(""id"").column(""t"")
+                .ttl(""t"").as(""id_ttl"").writeTime(""t"")
+                .from(""test"", ""test"");
+```
+
+You can also cast the value of the given column to another type by using the `cast`
+function,
+[specifying](https://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html#cast-java.lang.Object-com.datastax.driver.core.DataType-)
+the column for which it should be performed & to what type it should be casted.
+
+#### Specifying conditions
+
+Selection of data rarely happen on the whole table - in most cases, people are interested
+for particular rows, located in one or several partitions.  Conditions are specified by
+using the `where` call, like this:
+
+```java
+BuiltStatement selectOne = QueryBuilder.select().from(""test"", ""test"")
+        .where(QueryBuilder.eq(""id"", 1));
+```
+
+The `where` function accept the `Clause` object that is generated by calling","[{'comment': 'Typo: ""The `where` function accepts a `Clause` object...""', 'commenter': 'adutra'}]"
1031,manual/statements/built/README.md,"@@ -1,5 +1,268 @@
 ## Built statements
 
-*Coming soon... In the meantime, see the javadoc for [QueryBuilder].*
+Built statements are generated via [QueryBuilder]'s Fluent API.  Use of Fluent API allows
+easier build of the complex queries, comparing with hardcoding the CQL statements into the
+source code.
 
-[QueryBuilder]: http://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html
\ No newline at end of file
+Note: The provider builders perform very little validation of the built query. There is
+thus no guarantee that a built query is valid, and it is definitively possible to create
+invalid queries.
+
+Queries built with `QueryBuilder` are execute the same way as other queries - via
+`execute` or `executeAsync`.  When query is built with putting all data into it, then it
+doesn't differ much from statement specified as string.  But it's also possible to build
+the query with bind markers inside it, and then convert it into [prepared statement](../prepared/).
+
+### Basics
+
+Generation of `BuiltStatement` is easy - just start with calling of one of the
+[QueryBuilder]'s methods of that represent the CQL ""verb"": `select`, `update`, `delete`,
+`insertInto`, or `truncate`, providing required parameters, and then call ""verb""-specific
+functions to form a complete CQL statement (like, `where`, `from`, etc.).  The command's
+target could be specified as table name (if you set default keyspace when creating a
+`Session` object), keyspace & table name, or as [TableMetadata].
+
+Note: The `QueryBuilder` doesn't provide support for full set of CQL.  For generation of
+DDL operations (`CREATE TABLE`, etc.) you can use the [SchemaBuilder].  To perform other
+operations you still need to use [simple statements](../simple/).
+
+### Selecting data
+
+Selection of data is quite simple - at minimum you need to provide a list of columns to
+select, and the specify from which table to select these columns (and conditions as
+described in the next section):
+
+```java
+BuiltStatement selectAll1 = QueryBuilder.select(""id"", ""t"").from(""test"", ""test"");
+ResultSet rs = session.execute(selectAll1);
+for (Row row: rs) {
+   System.out.println(row);
+}
+```
+
+Note: The call `select(""column1"", ""column2"")` is really a shortcut for a chain of calls
+`select().column(""column1"").column(""column2"")`.
+
+Please note that you can't pass the `*` as column name to select all columns - if you do
+this, you'll get an exception about unknown column. To select all columns you either need to use
+`select` in combination with `all` function, or simply don't specify a list of columns:
+
+```java
+BuiltStatement selectAll2 = QueryBuilder.select().all().from(""test"", ""test"");
+BuiltStatement selectAll3 = QueryBuilder.select().from(""test"", ""test"");
+```
+
+Besides selection of the specific columns there is also a possibility of calling of
+arbitrary function by using the `fcall` (this is just example, don't do this on real
+data):
+
+```java
+BuiltStatement sum = QueryBuilder.select().fcall(""sum"", column(""id"")).as(""sum_id"")
+        .from(""test"", ""test"");
+```
+
+Note: When using functions, Cassandra will generate column names for you, but you can
+provide explicit name by using `as` right after given expression.
+
+For often used functions, there are shortcuts, such as, `countAll`, `ttl`, `writeTime`,
+`uuid`, `now`, `toJson`, etc.:
+
+```
+BuiltStatement count = QueryBuilder.select().countAll()
+        .from(""test"", ""test"");
+
+BuiltStatement ttlAndWriteTime = QueryBuilder.select().column(""id"").column(""t"")
+                .ttl(""t"").as(""id_ttl"").writeTime(""t"")
+                .from(""test"", ""test"");
+```
+
+You can also cast the value of the given column to another type by using the `cast`
+function,
+[specifying](https://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html#cast-java.lang.Object-com.datastax.driver.core.DataType-)
+the column for which it should be performed & to what type it should be casted.
+
+#### Specifying conditions
+
+Selection of data rarely happen on the whole table - in most cases, people are interested
+for particular rows, located in one or several partitions.  Conditions are specified by
+using the `where` call, like this:
+
+```java
+BuiltStatement selectOne = QueryBuilder.select().from(""test"", ""test"")
+        .where(QueryBuilder.eq(""id"", 1));
+```
+
+The `where` function accept the `Clause` object that is generated by calling
+`QueryBuilder`'s
+[functions](https://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html#eq-java.lang.Iterable-java.lang.Iterable-),
+such as, `eq`, `ne`, `lt`, `in`, `contains`, `notNull`, etc.  In most cases, these
+functions receive 2 arguments - the name of the column, and value to compare, but there","[{'comment': 'Nit: ""...and the value to compare"".', 'commenter': 'adutra'}]"
1031,manual/statements/built/README.md,"@@ -1,5 +1,268 @@
 ## Built statements
 
-*Coming soon... In the meantime, see the javadoc for [QueryBuilder].*
+Built statements are generated via [QueryBuilder]'s Fluent API.  Use of Fluent API allows
+easier build of the complex queries, comparing with hardcoding the CQL statements into the
+source code.
 
-[QueryBuilder]: http://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html
\ No newline at end of file
+Note: The provider builders perform very little validation of the built query. There is
+thus no guarantee that a built query is valid, and it is definitively possible to create
+invalid queries.
+
+Queries built with `QueryBuilder` are execute the same way as other queries - via
+`execute` or `executeAsync`.  When query is built with putting all data into it, then it
+doesn't differ much from statement specified as string.  But it's also possible to build
+the query with bind markers inside it, and then convert it into [prepared statement](../prepared/).
+
+### Basics
+
+Generation of `BuiltStatement` is easy - just start with calling of one of the
+[QueryBuilder]'s methods of that represent the CQL ""verb"": `select`, `update`, `delete`,
+`insertInto`, or `truncate`, providing required parameters, and then call ""verb""-specific
+functions to form a complete CQL statement (like, `where`, `from`, etc.).  The command's
+target could be specified as table name (if you set default keyspace when creating a
+`Session` object), keyspace & table name, or as [TableMetadata].
+
+Note: The `QueryBuilder` doesn't provide support for full set of CQL.  For generation of
+DDL operations (`CREATE TABLE`, etc.) you can use the [SchemaBuilder].  To perform other
+operations you still need to use [simple statements](../simple/).
+
+### Selecting data
+
+Selection of data is quite simple - at minimum you need to provide a list of columns to
+select, and the specify from which table to select these columns (and conditions as
+described in the next section):
+
+```java
+BuiltStatement selectAll1 = QueryBuilder.select(""id"", ""t"").from(""test"", ""test"");
+ResultSet rs = session.execute(selectAll1);
+for (Row row: rs) {
+   System.out.println(row);
+}
+```
+
+Note: The call `select(""column1"", ""column2"")` is really a shortcut for a chain of calls
+`select().column(""column1"").column(""column2"")`.
+
+Please note that you can't pass the `*` as column name to select all columns - if you do
+this, you'll get an exception about unknown column. To select all columns you either need to use
+`select` in combination with `all` function, or simply don't specify a list of columns:
+
+```java
+BuiltStatement selectAll2 = QueryBuilder.select().all().from(""test"", ""test"");
+BuiltStatement selectAll3 = QueryBuilder.select().from(""test"", ""test"");
+```
+
+Besides selection of the specific columns there is also a possibility of calling of
+arbitrary function by using the `fcall` (this is just example, don't do this on real
+data):
+
+```java
+BuiltStatement sum = QueryBuilder.select().fcall(""sum"", column(""id"")).as(""sum_id"")
+        .from(""test"", ""test"");
+```
+
+Note: When using functions, Cassandra will generate column names for you, but you can
+provide explicit name by using `as` right after given expression.
+
+For often used functions, there are shortcuts, such as, `countAll`, `ttl`, `writeTime`,
+`uuid`, `now`, `toJson`, etc.:
+
+```
+BuiltStatement count = QueryBuilder.select().countAll()
+        .from(""test"", ""test"");
+
+BuiltStatement ttlAndWriteTime = QueryBuilder.select().column(""id"").column(""t"")
+                .ttl(""t"").as(""id_ttl"").writeTime(""t"")
+                .from(""test"", ""test"");
+```
+
+You can also cast the value of the given column to another type by using the `cast`
+function,
+[specifying](https://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html#cast-java.lang.Object-com.datastax.driver.core.DataType-)
+the column for which it should be performed & to what type it should be casted.
+
+#### Specifying conditions
+
+Selection of data rarely happen on the whole table - in most cases, people are interested
+for particular rows, located in one or several partitions.  Conditions are specified by
+using the `where` call, like this:
+
+```java
+BuiltStatement selectOne = QueryBuilder.select().from(""test"", ""test"")
+        .where(QueryBuilder.eq(""id"", 1));
+```
+
+The `where` function accept the `Clause` object that is generated by calling
+`QueryBuilder`'s
+[functions](https://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html#eq-java.lang.Iterable-java.lang.Iterable-),
+such as, `eq`, `ne`, `lt`, `in`, `contains`, `notNull`, etc.  In most cases, these
+functions receive 2 arguments - the name of the column, and value to compare, but there
+are also variants that receive 2 iterables for columns & values separately.
+
+Note: as query are becoming more complex, repeating `QueryBuilder` at all places will make","[{'comment': 'Suggestion: ""as queries become more and more complex""', 'commenter': 'adutra'}]"
1031,manual/statements/built/README.md,"@@ -1,5 +1,268 @@
 ## Built statements
 
-*Coming soon... In the meantime, see the javadoc for [QueryBuilder].*
+Built statements are generated via [QueryBuilder]'s Fluent API.  Use of Fluent API allows
+easier build of the complex queries, comparing with hardcoding the CQL statements into the
+source code.
 
-[QueryBuilder]: http://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html
\ No newline at end of file
+Note: The provider builders perform very little validation of the built query. There is
+thus no guarantee that a built query is valid, and it is definitively possible to create
+invalid queries.
+
+Queries built with `QueryBuilder` are execute the same way as other queries - via
+`execute` or `executeAsync`.  When query is built with putting all data into it, then it
+doesn't differ much from statement specified as string.  But it's also possible to build
+the query with bind markers inside it, and then convert it into [prepared statement](../prepared/).
+
+### Basics
+
+Generation of `BuiltStatement` is easy - just start with calling of one of the
+[QueryBuilder]'s methods of that represent the CQL ""verb"": `select`, `update`, `delete`,
+`insertInto`, or `truncate`, providing required parameters, and then call ""verb""-specific
+functions to form a complete CQL statement (like, `where`, `from`, etc.).  The command's
+target could be specified as table name (if you set default keyspace when creating a
+`Session` object), keyspace & table name, or as [TableMetadata].
+
+Note: The `QueryBuilder` doesn't provide support for full set of CQL.  For generation of
+DDL operations (`CREATE TABLE`, etc.) you can use the [SchemaBuilder].  To perform other
+operations you still need to use [simple statements](../simple/).
+
+### Selecting data
+
+Selection of data is quite simple - at minimum you need to provide a list of columns to
+select, and the specify from which table to select these columns (and conditions as
+described in the next section):
+
+```java
+BuiltStatement selectAll1 = QueryBuilder.select(""id"", ""t"").from(""test"", ""test"");
+ResultSet rs = session.execute(selectAll1);
+for (Row row: rs) {
+   System.out.println(row);
+}
+```
+
+Note: The call `select(""column1"", ""column2"")` is really a shortcut for a chain of calls
+`select().column(""column1"").column(""column2"")`.
+
+Please note that you can't pass the `*` as column name to select all columns - if you do
+this, you'll get an exception about unknown column. To select all columns you either need to use
+`select` in combination with `all` function, or simply don't specify a list of columns:
+
+```java
+BuiltStatement selectAll2 = QueryBuilder.select().all().from(""test"", ""test"");
+BuiltStatement selectAll3 = QueryBuilder.select().from(""test"", ""test"");
+```
+
+Besides selection of the specific columns there is also a possibility of calling of
+arbitrary function by using the `fcall` (this is just example, don't do this on real
+data):
+
+```java
+BuiltStatement sum = QueryBuilder.select().fcall(""sum"", column(""id"")).as(""sum_id"")
+        .from(""test"", ""test"");
+```
+
+Note: When using functions, Cassandra will generate column names for you, but you can
+provide explicit name by using `as` right after given expression.
+
+For often used functions, there are shortcuts, such as, `countAll`, `ttl`, `writeTime`,
+`uuid`, `now`, `toJson`, etc.:
+
+```
+BuiltStatement count = QueryBuilder.select().countAll()
+        .from(""test"", ""test"");
+
+BuiltStatement ttlAndWriteTime = QueryBuilder.select().column(""id"").column(""t"")
+                .ttl(""t"").as(""id_ttl"").writeTime(""t"")
+                .from(""test"", ""test"");
+```
+
+You can also cast the value of the given column to another type by using the `cast`
+function,
+[specifying](https://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html#cast-java.lang.Object-com.datastax.driver.core.DataType-)
+the column for which it should be performed & to what type it should be casted.
+
+#### Specifying conditions
+
+Selection of data rarely happen on the whole table - in most cases, people are interested
+for particular rows, located in one or several partitions.  Conditions are specified by
+using the `where` call, like this:
+
+```java
+BuiltStatement selectOne = QueryBuilder.select().from(""test"", ""test"")
+        .where(QueryBuilder.eq(""id"", 1));
+```
+
+The `where` function accept the `Clause` object that is generated by calling
+`QueryBuilder`'s
+[functions](https://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html#eq-java.lang.Iterable-java.lang.Iterable-),
+such as, `eq`, `ne`, `lt`, `in`, `contains`, `notNull`, etc.  In most cases, these
+functions receive 2 arguments - the name of the column, and value to compare, but there
+are also variants that receive 2 iterables for columns & values separately.
+
+Note: as query are becoming more complex, repeating `QueryBuilder` at all places will make
+code less readable.  In this case you can import all (or only required) static functions
+for code simplification (this is rewritten example from above):
+
+```java
+import static com.datastax.driver.core.querybuilder.QueryBuilder.*;
+
+//...
+
+BuiltStatement selectOne = QueryBuilder.select().from(""test"", ""test"")","[{'comment': 'You could even simplify to: `BuiltStatement selectOne = select().from(""test"", ""test"").where(eq(""id"", 1));`', 'commenter': 'adutra'}]"
1031,manual/statements/built/README.md,"@@ -1,5 +1,268 @@
 ## Built statements
 
-*Coming soon... In the meantime, see the javadoc for [QueryBuilder].*
+Built statements are generated via [QueryBuilder]'s Fluent API.  Use of Fluent API allows
+easier build of the complex queries, comparing with hardcoding the CQL statements into the
+source code.
 
-[QueryBuilder]: http://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html
\ No newline at end of file
+Note: The provider builders perform very little validation of the built query. There is
+thus no guarantee that a built query is valid, and it is definitively possible to create
+invalid queries.
+
+Queries built with `QueryBuilder` are execute the same way as other queries - via
+`execute` or `executeAsync`.  When query is built with putting all data into it, then it
+doesn't differ much from statement specified as string.  But it's also possible to build
+the query with bind markers inside it, and then convert it into [prepared statement](../prepared/).
+
+### Basics
+
+Generation of `BuiltStatement` is easy - just start with calling of one of the
+[QueryBuilder]'s methods of that represent the CQL ""verb"": `select`, `update`, `delete`,
+`insertInto`, or `truncate`, providing required parameters, and then call ""verb""-specific
+functions to form a complete CQL statement (like, `where`, `from`, etc.).  The command's
+target could be specified as table name (if you set default keyspace when creating a
+`Session` object), keyspace & table name, or as [TableMetadata].
+
+Note: The `QueryBuilder` doesn't provide support for full set of CQL.  For generation of
+DDL operations (`CREATE TABLE`, etc.) you can use the [SchemaBuilder].  To perform other
+operations you still need to use [simple statements](../simple/).
+
+### Selecting data
+
+Selection of data is quite simple - at minimum you need to provide a list of columns to
+select, and the specify from which table to select these columns (and conditions as
+described in the next section):
+
+```java
+BuiltStatement selectAll1 = QueryBuilder.select(""id"", ""t"").from(""test"", ""test"");
+ResultSet rs = session.execute(selectAll1);
+for (Row row: rs) {
+   System.out.println(row);
+}
+```
+
+Note: The call `select(""column1"", ""column2"")` is really a shortcut for a chain of calls
+`select().column(""column1"").column(""column2"")`.
+
+Please note that you can't pass the `*` as column name to select all columns - if you do
+this, you'll get an exception about unknown column. To select all columns you either need to use
+`select` in combination with `all` function, or simply don't specify a list of columns:
+
+```java
+BuiltStatement selectAll2 = QueryBuilder.select().all().from(""test"", ""test"");
+BuiltStatement selectAll3 = QueryBuilder.select().from(""test"", ""test"");
+```
+
+Besides selection of the specific columns there is also a possibility of calling of
+arbitrary function by using the `fcall` (this is just example, don't do this on real
+data):
+
+```java
+BuiltStatement sum = QueryBuilder.select().fcall(""sum"", column(""id"")).as(""sum_id"")
+        .from(""test"", ""test"");
+```
+
+Note: When using functions, Cassandra will generate column names for you, but you can
+provide explicit name by using `as` right after given expression.
+
+For often used functions, there are shortcuts, such as, `countAll`, `ttl`, `writeTime`,
+`uuid`, `now`, `toJson`, etc.:
+
+```
+BuiltStatement count = QueryBuilder.select().countAll()
+        .from(""test"", ""test"");
+
+BuiltStatement ttlAndWriteTime = QueryBuilder.select().column(""id"").column(""t"")
+                .ttl(""t"").as(""id_ttl"").writeTime(""t"")
+                .from(""test"", ""test"");
+```
+
+You can also cast the value of the given column to another type by using the `cast`
+function,
+[specifying](https://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html#cast-java.lang.Object-com.datastax.driver.core.DataType-)
+the column for which it should be performed & to what type it should be casted.
+
+#### Specifying conditions
+
+Selection of data rarely happen on the whole table - in most cases, people are interested
+for particular rows, located in one or several partitions.  Conditions are specified by
+using the `where` call, like this:
+
+```java
+BuiltStatement selectOne = QueryBuilder.select().from(""test"", ""test"")
+        .where(QueryBuilder.eq(""id"", 1));
+```
+
+The `where` function accept the `Clause` object that is generated by calling
+`QueryBuilder`'s
+[functions](https://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html#eq-java.lang.Iterable-java.lang.Iterable-),
+such as, `eq`, `ne`, `lt`, `in`, `contains`, `notNull`, etc.  In most cases, these
+functions receive 2 arguments - the name of the column, and value to compare, but there
+are also variants that receive 2 iterables for columns & values separately.
+
+Note: as query are becoming more complex, repeating `QueryBuilder` at all places will make
+code less readable.  In this case you can import all (or only required) static functions
+for code simplification (this is rewritten example from above):
+
+```java
+import static com.datastax.driver.core.querybuilder.QueryBuilder.*;
+
+//...
+
+BuiltStatement selectOne = QueryBuilder.select().from(""test"", ""test"")
+        .where(eq(""id"", 1));
+```
+
+In case if you need to specify complex condition you can put additional clauses into chain","[{'comment': 'Typo: ""...specify complex conditions, you can chain additional clauses together with the `and` operator, that accepts...""', 'commenter': 'adutra'}]"
1031,manual/statements/built/README.md,"@@ -1,5 +1,268 @@
 ## Built statements
 
-*Coming soon... In the meantime, see the javadoc for [QueryBuilder].*
+Built statements are generated via [QueryBuilder]'s Fluent API.  Use of Fluent API allows
+easier build of the complex queries, comparing with hardcoding the CQL statements into the
+source code.
 
-[QueryBuilder]: http://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html
\ No newline at end of file
+Note: The provider builders perform very little validation of the built query. There is
+thus no guarantee that a built query is valid, and it is definitively possible to create
+invalid queries.
+
+Queries built with `QueryBuilder` are execute the same way as other queries - via
+`execute` or `executeAsync`.  When query is built with putting all data into it, then it
+doesn't differ much from statement specified as string.  But it's also possible to build
+the query with bind markers inside it, and then convert it into [prepared statement](../prepared/).
+
+### Basics
+
+Generation of `BuiltStatement` is easy - just start with calling of one of the
+[QueryBuilder]'s methods of that represent the CQL ""verb"": `select`, `update`, `delete`,
+`insertInto`, or `truncate`, providing required parameters, and then call ""verb""-specific
+functions to form a complete CQL statement (like, `where`, `from`, etc.).  The command's
+target could be specified as table name (if you set default keyspace when creating a
+`Session` object), keyspace & table name, or as [TableMetadata].
+
+Note: The `QueryBuilder` doesn't provide support for full set of CQL.  For generation of
+DDL operations (`CREATE TABLE`, etc.) you can use the [SchemaBuilder].  To perform other
+operations you still need to use [simple statements](../simple/).
+
+### Selecting data
+
+Selection of data is quite simple - at minimum you need to provide a list of columns to
+select, and the specify from which table to select these columns (and conditions as
+described in the next section):
+
+```java
+BuiltStatement selectAll1 = QueryBuilder.select(""id"", ""t"").from(""test"", ""test"");
+ResultSet rs = session.execute(selectAll1);
+for (Row row: rs) {
+   System.out.println(row);
+}
+```
+
+Note: The call `select(""column1"", ""column2"")` is really a shortcut for a chain of calls
+`select().column(""column1"").column(""column2"")`.
+
+Please note that you can't pass the `*` as column name to select all columns - if you do
+this, you'll get an exception about unknown column. To select all columns you either need to use
+`select` in combination with `all` function, or simply don't specify a list of columns:
+
+```java
+BuiltStatement selectAll2 = QueryBuilder.select().all().from(""test"", ""test"");
+BuiltStatement selectAll3 = QueryBuilder.select().from(""test"", ""test"");
+```
+
+Besides selection of the specific columns there is also a possibility of calling of
+arbitrary function by using the `fcall` (this is just example, don't do this on real
+data):
+
+```java
+BuiltStatement sum = QueryBuilder.select().fcall(""sum"", column(""id"")).as(""sum_id"")
+        .from(""test"", ""test"");
+```
+
+Note: When using functions, Cassandra will generate column names for you, but you can
+provide explicit name by using `as` right after given expression.
+
+For often used functions, there are shortcuts, such as, `countAll`, `ttl`, `writeTime`,
+`uuid`, `now`, `toJson`, etc.:
+
+```
+BuiltStatement count = QueryBuilder.select().countAll()
+        .from(""test"", ""test"");
+
+BuiltStatement ttlAndWriteTime = QueryBuilder.select().column(""id"").column(""t"")
+                .ttl(""t"").as(""id_ttl"").writeTime(""t"")
+                .from(""test"", ""test"");
+```
+
+You can also cast the value of the given column to another type by using the `cast`
+function,
+[specifying](https://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html#cast-java.lang.Object-com.datastax.driver.core.DataType-)
+the column for which it should be performed & to what type it should be casted.
+
+#### Specifying conditions
+
+Selection of data rarely happen on the whole table - in most cases, people are interested
+for particular rows, located in one or several partitions.  Conditions are specified by
+using the `where` call, like this:
+
+```java
+BuiltStatement selectOne = QueryBuilder.select().from(""test"", ""test"")
+        .where(QueryBuilder.eq(""id"", 1));
+```
+
+The `where` function accept the `Clause` object that is generated by calling
+`QueryBuilder`'s
+[functions](https://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html#eq-java.lang.Iterable-java.lang.Iterable-),
+such as, `eq`, `ne`, `lt`, `in`, `contains`, `notNull`, etc.  In most cases, these
+functions receive 2 arguments - the name of the column, and value to compare, but there
+are also variants that receive 2 iterables for columns & values separately.
+
+Note: as query are becoming more complex, repeating `QueryBuilder` at all places will make
+code less readable.  In this case you can import all (or only required) static functions
+for code simplification (this is rewritten example from above):
+
+```java
+import static com.datastax.driver.core.querybuilder.QueryBuilder.*;
+
+//...
+
+BuiltStatement selectOne = QueryBuilder.select().from(""test"", ""test"")
+        .where(eq(""id"", 1));
+```
+
+In case if you need to specify complex condition you can put additional clauses into chain
+of the `and` calls that accepts the same clauses as `where`:
+
+```java
+BuiltStatement select = QueryBuilder.select().from(""test"", ""test"")
+        .where(eq(""id"", ""1"")).and(eq(""txt"", ""test""));
+```
+
+#### Other selection options
+
+For `SELECT` statements you can also specify a lot of different options: 
+ - `allowFiltering` generates corresponding `ALLOW FILTERING` part of query (***only use if you know what you're doing!***);","[{'comment': 'Suggestion: ""generates a corresponding `ALLOW FILTERING` clause""', 'commenter': 'adutra'}]"
1031,manual/statements/built/README.md,"@@ -1,5 +1,268 @@
 ## Built statements
 
-*Coming soon... In the meantime, see the javadoc for [QueryBuilder].*
+Built statements are generated via [QueryBuilder]'s Fluent API.  Use of Fluent API allows
+easier build of the complex queries, comparing with hardcoding the CQL statements into the
+source code.
 
-[QueryBuilder]: http://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html
\ No newline at end of file
+Note: The provider builders perform very little validation of the built query. There is
+thus no guarantee that a built query is valid, and it is definitively possible to create
+invalid queries.
+
+Queries built with `QueryBuilder` are execute the same way as other queries - via
+`execute` or `executeAsync`.  When query is built with putting all data into it, then it
+doesn't differ much from statement specified as string.  But it's also possible to build
+the query with bind markers inside it, and then convert it into [prepared statement](../prepared/).
+
+### Basics
+
+Generation of `BuiltStatement` is easy - just start with calling of one of the
+[QueryBuilder]'s methods of that represent the CQL ""verb"": `select`, `update`, `delete`,
+`insertInto`, or `truncate`, providing required parameters, and then call ""verb""-specific
+functions to form a complete CQL statement (like, `where`, `from`, etc.).  The command's
+target could be specified as table name (if you set default keyspace when creating a
+`Session` object), keyspace & table name, or as [TableMetadata].
+
+Note: The `QueryBuilder` doesn't provide support for full set of CQL.  For generation of
+DDL operations (`CREATE TABLE`, etc.) you can use the [SchemaBuilder].  To perform other
+operations you still need to use [simple statements](../simple/).
+
+### Selecting data
+
+Selection of data is quite simple - at minimum you need to provide a list of columns to
+select, and the specify from which table to select these columns (and conditions as
+described in the next section):
+
+```java
+BuiltStatement selectAll1 = QueryBuilder.select(""id"", ""t"").from(""test"", ""test"");
+ResultSet rs = session.execute(selectAll1);
+for (Row row: rs) {
+   System.out.println(row);
+}
+```
+
+Note: The call `select(""column1"", ""column2"")` is really a shortcut for a chain of calls
+`select().column(""column1"").column(""column2"")`.
+
+Please note that you can't pass the `*` as column name to select all columns - if you do
+this, you'll get an exception about unknown column. To select all columns you either need to use
+`select` in combination with `all` function, or simply don't specify a list of columns:
+
+```java
+BuiltStatement selectAll2 = QueryBuilder.select().all().from(""test"", ""test"");
+BuiltStatement selectAll3 = QueryBuilder.select().from(""test"", ""test"");
+```
+
+Besides selection of the specific columns there is also a possibility of calling of
+arbitrary function by using the `fcall` (this is just example, don't do this on real
+data):
+
+```java
+BuiltStatement sum = QueryBuilder.select().fcall(""sum"", column(""id"")).as(""sum_id"")
+        .from(""test"", ""test"");
+```
+
+Note: When using functions, Cassandra will generate column names for you, but you can
+provide explicit name by using `as` right after given expression.
+
+For often used functions, there are shortcuts, such as, `countAll`, `ttl`, `writeTime`,
+`uuid`, `now`, `toJson`, etc.:
+
+```
+BuiltStatement count = QueryBuilder.select().countAll()
+        .from(""test"", ""test"");
+
+BuiltStatement ttlAndWriteTime = QueryBuilder.select().column(""id"").column(""t"")
+                .ttl(""t"").as(""id_ttl"").writeTime(""t"")
+                .from(""test"", ""test"");
+```
+
+You can also cast the value of the given column to another type by using the `cast`
+function,
+[specifying](https://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html#cast-java.lang.Object-com.datastax.driver.core.DataType-)
+the column for which it should be performed & to what type it should be casted.
+
+#### Specifying conditions
+
+Selection of data rarely happen on the whole table - in most cases, people are interested
+for particular rows, located in one or several partitions.  Conditions are specified by
+using the `where` call, like this:
+
+```java
+BuiltStatement selectOne = QueryBuilder.select().from(""test"", ""test"")
+        .where(QueryBuilder.eq(""id"", 1));
+```
+
+The `where` function accept the `Clause` object that is generated by calling
+`QueryBuilder`'s
+[functions](https://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html#eq-java.lang.Iterable-java.lang.Iterable-),
+such as, `eq`, `ne`, `lt`, `in`, `contains`, `notNull`, etc.  In most cases, these
+functions receive 2 arguments - the name of the column, and value to compare, but there
+are also variants that receive 2 iterables for columns & values separately.
+
+Note: as query are becoming more complex, repeating `QueryBuilder` at all places will make
+code less readable.  In this case you can import all (or only required) static functions
+for code simplification (this is rewritten example from above):
+
+```java
+import static com.datastax.driver.core.querybuilder.QueryBuilder.*;
+
+//...
+
+BuiltStatement selectOne = QueryBuilder.select().from(""test"", ""test"")
+        .where(eq(""id"", 1));
+```
+
+In case if you need to specify complex condition you can put additional clauses into chain
+of the `and` calls that accepts the same clauses as `where`:
+
+```java
+BuiltStatement select = QueryBuilder.select().from(""test"", ""test"")
+        .where(eq(""id"", ""1"")).and(eq(""txt"", ""test""));
+```
+
+#### Other selection options
+
+For `SELECT` statements you can also specify a lot of different options: 
+ - `allowFiltering` generates corresponding `ALLOW FILTERING` part of query (***only use if you know what you're doing!***);
+ - `limit` & `perPartitionLimit` allows to specify amount of data to fetch;","[{'comment': 'Nit: ""the amount of data""', 'commenter': 'adutra'}]"
1031,manual/statements/built/README.md,"@@ -1,5 +1,268 @@
 ## Built statements
 
-*Coming soon... In the meantime, see the javadoc for [QueryBuilder].*
+Built statements are generated via [QueryBuilder]'s Fluent API.  Use of Fluent API allows
+easier build of the complex queries, comparing with hardcoding the CQL statements into the
+source code.
 
-[QueryBuilder]: http://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html
\ No newline at end of file
+Note: The provider builders perform very little validation of the built query. There is
+thus no guarantee that a built query is valid, and it is definitively possible to create
+invalid queries.
+
+Queries built with `QueryBuilder` are execute the same way as other queries - via
+`execute` or `executeAsync`.  When query is built with putting all data into it, then it
+doesn't differ much from statement specified as string.  But it's also possible to build
+the query with bind markers inside it, and then convert it into [prepared statement](../prepared/).
+
+### Basics
+
+Generation of `BuiltStatement` is easy - just start with calling of one of the
+[QueryBuilder]'s methods of that represent the CQL ""verb"": `select`, `update`, `delete`,
+`insertInto`, or `truncate`, providing required parameters, and then call ""verb""-specific
+functions to form a complete CQL statement (like, `where`, `from`, etc.).  The command's
+target could be specified as table name (if you set default keyspace when creating a
+`Session` object), keyspace & table name, or as [TableMetadata].
+
+Note: The `QueryBuilder` doesn't provide support for full set of CQL.  For generation of
+DDL operations (`CREATE TABLE`, etc.) you can use the [SchemaBuilder].  To perform other
+operations you still need to use [simple statements](../simple/).
+
+### Selecting data
+
+Selection of data is quite simple - at minimum you need to provide a list of columns to
+select, and the specify from which table to select these columns (and conditions as
+described in the next section):
+
+```java
+BuiltStatement selectAll1 = QueryBuilder.select(""id"", ""t"").from(""test"", ""test"");
+ResultSet rs = session.execute(selectAll1);
+for (Row row: rs) {
+   System.out.println(row);
+}
+```
+
+Note: The call `select(""column1"", ""column2"")` is really a shortcut for a chain of calls
+`select().column(""column1"").column(""column2"")`.
+
+Please note that you can't pass the `*` as column name to select all columns - if you do
+this, you'll get an exception about unknown column. To select all columns you either need to use
+`select` in combination with `all` function, or simply don't specify a list of columns:
+
+```java
+BuiltStatement selectAll2 = QueryBuilder.select().all().from(""test"", ""test"");
+BuiltStatement selectAll3 = QueryBuilder.select().from(""test"", ""test"");
+```
+
+Besides selection of the specific columns there is also a possibility of calling of
+arbitrary function by using the `fcall` (this is just example, don't do this on real
+data):
+
+```java
+BuiltStatement sum = QueryBuilder.select().fcall(""sum"", column(""id"")).as(""sum_id"")
+        .from(""test"", ""test"");
+```
+
+Note: When using functions, Cassandra will generate column names for you, but you can
+provide explicit name by using `as` right after given expression.
+
+For often used functions, there are shortcuts, such as, `countAll`, `ttl`, `writeTime`,
+`uuid`, `now`, `toJson`, etc.:
+
+```
+BuiltStatement count = QueryBuilder.select().countAll()
+        .from(""test"", ""test"");
+
+BuiltStatement ttlAndWriteTime = QueryBuilder.select().column(""id"").column(""t"")
+                .ttl(""t"").as(""id_ttl"").writeTime(""t"")
+                .from(""test"", ""test"");
+```
+
+You can also cast the value of the given column to another type by using the `cast`
+function,
+[specifying](https://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html#cast-java.lang.Object-com.datastax.driver.core.DataType-)
+the column for which it should be performed & to what type it should be casted.
+
+#### Specifying conditions
+
+Selection of data rarely happen on the whole table - in most cases, people are interested
+for particular rows, located in one or several partitions.  Conditions are specified by
+using the `where` call, like this:
+
+```java
+BuiltStatement selectOne = QueryBuilder.select().from(""test"", ""test"")
+        .where(QueryBuilder.eq(""id"", 1));
+```
+
+The `where` function accept the `Clause` object that is generated by calling
+`QueryBuilder`'s
+[functions](https://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html#eq-java.lang.Iterable-java.lang.Iterable-),
+such as, `eq`, `ne`, `lt`, `in`, `contains`, `notNull`, etc.  In most cases, these
+functions receive 2 arguments - the name of the column, and value to compare, but there
+are also variants that receive 2 iterables for columns & values separately.
+
+Note: as query are becoming more complex, repeating `QueryBuilder` at all places will make
+code less readable.  In this case you can import all (or only required) static functions
+for code simplification (this is rewritten example from above):
+
+```java
+import static com.datastax.driver.core.querybuilder.QueryBuilder.*;
+
+//...
+
+BuiltStatement selectOne = QueryBuilder.select().from(""test"", ""test"")
+        .where(eq(""id"", 1));
+```
+
+In case if you need to specify complex condition you can put additional clauses into chain
+of the `and` calls that accepts the same clauses as `where`:
+
+```java
+BuiltStatement select = QueryBuilder.select().from(""test"", ""test"")
+        .where(eq(""id"", ""1"")).and(eq(""txt"", ""test""));
+```
+
+#### Other selection options
+
+For `SELECT` statements you can also specify a lot of different options: 
+ - `allowFiltering` generates corresponding `ALLOW FILTERING` part of query (***only use if you know what you're doing!***);
+ - `limit` & `perPartitionLimit` allows to specify amount of data to fetch;
+ - `groupBy` performs group of data;
+ - `orderBy` allows to specify sorting direction for specified clustering columns;
+ 
+This very ""artificial"" example shows the use for some of them:
+
+```java
+BuiltStatement selectOne = QueryBuilder.select().from(""test"")
+        .where(QueryBuilder.eq(""id"", 1)).limit(1).allowFiltering()
+        .perPartitionLimit(1).orderBy(desc(""id""));
+```
+
+### Inserting data
+
+Insertion of data is straightforward - you're specifying the target table in call to","[{'comment': 'Nit: ""you specify the target table in the call to...""', 'commenter': 'adutra'}]"
1031,manual/statements/built/README.md,"@@ -1,5 +1,268 @@
 ## Built statements
 
-*Coming soon... In the meantime, see the javadoc for [QueryBuilder].*
+Built statements are generated via [QueryBuilder]'s Fluent API.  Use of Fluent API allows
+easier build of the complex queries, comparing with hardcoding the CQL statements into the
+source code.
 
-[QueryBuilder]: http://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html
\ No newline at end of file
+Note: The provider builders perform very little validation of the built query. There is
+thus no guarantee that a built query is valid, and it is definitively possible to create
+invalid queries.
+
+Queries built with `QueryBuilder` are execute the same way as other queries - via
+`execute` or `executeAsync`.  When query is built with putting all data into it, then it
+doesn't differ much from statement specified as string.  But it's also possible to build
+the query with bind markers inside it, and then convert it into [prepared statement](../prepared/).
+
+### Basics
+
+Generation of `BuiltStatement` is easy - just start with calling of one of the
+[QueryBuilder]'s methods of that represent the CQL ""verb"": `select`, `update`, `delete`,
+`insertInto`, or `truncate`, providing required parameters, and then call ""verb""-specific
+functions to form a complete CQL statement (like, `where`, `from`, etc.).  The command's
+target could be specified as table name (if you set default keyspace when creating a
+`Session` object), keyspace & table name, or as [TableMetadata].
+
+Note: The `QueryBuilder` doesn't provide support for full set of CQL.  For generation of
+DDL operations (`CREATE TABLE`, etc.) you can use the [SchemaBuilder].  To perform other
+operations you still need to use [simple statements](../simple/).
+
+### Selecting data
+
+Selection of data is quite simple - at minimum you need to provide a list of columns to
+select, and the specify from which table to select these columns (and conditions as
+described in the next section):
+
+```java
+BuiltStatement selectAll1 = QueryBuilder.select(""id"", ""t"").from(""test"", ""test"");
+ResultSet rs = session.execute(selectAll1);
+for (Row row: rs) {
+   System.out.println(row);
+}
+```
+
+Note: The call `select(""column1"", ""column2"")` is really a shortcut for a chain of calls
+`select().column(""column1"").column(""column2"")`.
+
+Please note that you can't pass the `*` as column name to select all columns - if you do
+this, you'll get an exception about unknown column. To select all columns you either need to use
+`select` in combination with `all` function, or simply don't specify a list of columns:
+
+```java
+BuiltStatement selectAll2 = QueryBuilder.select().all().from(""test"", ""test"");
+BuiltStatement selectAll3 = QueryBuilder.select().from(""test"", ""test"");
+```
+
+Besides selection of the specific columns there is also a possibility of calling of
+arbitrary function by using the `fcall` (this is just example, don't do this on real
+data):
+
+```java
+BuiltStatement sum = QueryBuilder.select().fcall(""sum"", column(""id"")).as(""sum_id"")
+        .from(""test"", ""test"");
+```
+
+Note: When using functions, Cassandra will generate column names for you, but you can
+provide explicit name by using `as` right after given expression.
+
+For often used functions, there are shortcuts, such as, `countAll`, `ttl`, `writeTime`,
+`uuid`, `now`, `toJson`, etc.:
+
+```
+BuiltStatement count = QueryBuilder.select().countAll()
+        .from(""test"", ""test"");
+
+BuiltStatement ttlAndWriteTime = QueryBuilder.select().column(""id"").column(""t"")
+                .ttl(""t"").as(""id_ttl"").writeTime(""t"")
+                .from(""test"", ""test"");
+```
+
+You can also cast the value of the given column to another type by using the `cast`
+function,
+[specifying](https://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html#cast-java.lang.Object-com.datastax.driver.core.DataType-)
+the column for which it should be performed & to what type it should be casted.
+
+#### Specifying conditions
+
+Selection of data rarely happen on the whole table - in most cases, people are interested
+for particular rows, located in one or several partitions.  Conditions are specified by
+using the `where` call, like this:
+
+```java
+BuiltStatement selectOne = QueryBuilder.select().from(""test"", ""test"")
+        .where(QueryBuilder.eq(""id"", 1));
+```
+
+The `where` function accept the `Clause` object that is generated by calling
+`QueryBuilder`'s
+[functions](https://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html#eq-java.lang.Iterable-java.lang.Iterable-),
+such as, `eq`, `ne`, `lt`, `in`, `contains`, `notNull`, etc.  In most cases, these
+functions receive 2 arguments - the name of the column, and value to compare, but there
+are also variants that receive 2 iterables for columns & values separately.
+
+Note: as query are becoming more complex, repeating `QueryBuilder` at all places will make
+code less readable.  In this case you can import all (or only required) static functions
+for code simplification (this is rewritten example from above):
+
+```java
+import static com.datastax.driver.core.querybuilder.QueryBuilder.*;
+
+//...
+
+BuiltStatement selectOne = QueryBuilder.select().from(""test"", ""test"")
+        .where(eq(""id"", 1));
+```
+
+In case if you need to specify complex condition you can put additional clauses into chain
+of the `and` calls that accepts the same clauses as `where`:
+
+```java
+BuiltStatement select = QueryBuilder.select().from(""test"", ""test"")
+        .where(eq(""id"", ""1"")).and(eq(""txt"", ""test""));
+```
+
+#### Other selection options
+
+For `SELECT` statements you can also specify a lot of different options: 
+ - `allowFiltering` generates corresponding `ALLOW FILTERING` part of query (***only use if you know what you're doing!***);
+ - `limit` & `perPartitionLimit` allows to specify amount of data to fetch;
+ - `groupBy` performs group of data;
+ - `orderBy` allows to specify sorting direction for specified clustering columns;
+ 
+This very ""artificial"" example shows the use for some of them:
+
+```java
+BuiltStatement selectOne = QueryBuilder.select().from(""test"")
+        .where(QueryBuilder.eq(""id"", 1)).limit(1).allowFiltering()
+        .perPartitionLimit(1).orderBy(desc(""id""));
+```
+
+### Inserting data
+
+Insertion of data is straightforward - you're specifying the target table in call to
+`insertInto`, and then provide values to insert either by chaining the several calls to","[{'comment': 'Suggestion: "" either by chaining several calls to the `value` function,  or by using the `values` function and passing lists or arrays of column names and their corresponding values"".', 'commenter': 'adutra'}]"
1031,manual/statements/built/README.md,"@@ -1,5 +1,268 @@
 ## Built statements
 
-*Coming soon... In the meantime, see the javadoc for [QueryBuilder].*
+Built statements are generated via [QueryBuilder]'s Fluent API.  Use of Fluent API allows
+easier build of the complex queries, comparing with hardcoding the CQL statements into the
+source code.
 
-[QueryBuilder]: http://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html
\ No newline at end of file
+Note: The provider builders perform very little validation of the built query. There is
+thus no guarantee that a built query is valid, and it is definitively possible to create
+invalid queries.
+
+Queries built with `QueryBuilder` are execute the same way as other queries - via
+`execute` or `executeAsync`.  When query is built with putting all data into it, then it
+doesn't differ much from statement specified as string.  But it's also possible to build
+the query with bind markers inside it, and then convert it into [prepared statement](../prepared/).
+
+### Basics
+
+Generation of `BuiltStatement` is easy - just start with calling of one of the
+[QueryBuilder]'s methods of that represent the CQL ""verb"": `select`, `update`, `delete`,
+`insertInto`, or `truncate`, providing required parameters, and then call ""verb""-specific
+functions to form a complete CQL statement (like, `where`, `from`, etc.).  The command's
+target could be specified as table name (if you set default keyspace when creating a
+`Session` object), keyspace & table name, or as [TableMetadata].
+
+Note: The `QueryBuilder` doesn't provide support for full set of CQL.  For generation of
+DDL operations (`CREATE TABLE`, etc.) you can use the [SchemaBuilder].  To perform other
+operations you still need to use [simple statements](../simple/).
+
+### Selecting data
+
+Selection of data is quite simple - at minimum you need to provide a list of columns to
+select, and the specify from which table to select these columns (and conditions as
+described in the next section):
+
+```java
+BuiltStatement selectAll1 = QueryBuilder.select(""id"", ""t"").from(""test"", ""test"");
+ResultSet rs = session.execute(selectAll1);
+for (Row row: rs) {
+   System.out.println(row);
+}
+```
+
+Note: The call `select(""column1"", ""column2"")` is really a shortcut for a chain of calls
+`select().column(""column1"").column(""column2"")`.
+
+Please note that you can't pass the `*` as column name to select all columns - if you do
+this, you'll get an exception about unknown column. To select all columns you either need to use
+`select` in combination with `all` function, or simply don't specify a list of columns:
+
+```java
+BuiltStatement selectAll2 = QueryBuilder.select().all().from(""test"", ""test"");
+BuiltStatement selectAll3 = QueryBuilder.select().from(""test"", ""test"");
+```
+
+Besides selection of the specific columns there is also a possibility of calling of
+arbitrary function by using the `fcall` (this is just example, don't do this on real
+data):
+
+```java
+BuiltStatement sum = QueryBuilder.select().fcall(""sum"", column(""id"")).as(""sum_id"")
+        .from(""test"", ""test"");
+```
+
+Note: When using functions, Cassandra will generate column names for you, but you can
+provide explicit name by using `as` right after given expression.
+
+For often used functions, there are shortcuts, such as, `countAll`, `ttl`, `writeTime`,
+`uuid`, `now`, `toJson`, etc.:
+
+```
+BuiltStatement count = QueryBuilder.select().countAll()
+        .from(""test"", ""test"");
+
+BuiltStatement ttlAndWriteTime = QueryBuilder.select().column(""id"").column(""t"")
+                .ttl(""t"").as(""id_ttl"").writeTime(""t"")
+                .from(""test"", ""test"");
+```
+
+You can also cast the value of the given column to another type by using the `cast`
+function,
+[specifying](https://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html#cast-java.lang.Object-com.datastax.driver.core.DataType-)
+the column for which it should be performed & to what type it should be casted.
+
+#### Specifying conditions
+
+Selection of data rarely happen on the whole table - in most cases, people are interested
+for particular rows, located in one or several partitions.  Conditions are specified by
+using the `where` call, like this:
+
+```java
+BuiltStatement selectOne = QueryBuilder.select().from(""test"", ""test"")
+        .where(QueryBuilder.eq(""id"", 1));
+```
+
+The `where` function accept the `Clause` object that is generated by calling
+`QueryBuilder`'s
+[functions](https://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html#eq-java.lang.Iterable-java.lang.Iterable-),
+such as, `eq`, `ne`, `lt`, `in`, `contains`, `notNull`, etc.  In most cases, these
+functions receive 2 arguments - the name of the column, and value to compare, but there
+are also variants that receive 2 iterables for columns & values separately.
+
+Note: as query are becoming more complex, repeating `QueryBuilder` at all places will make
+code less readable.  In this case you can import all (or only required) static functions
+for code simplification (this is rewritten example from above):
+
+```java
+import static com.datastax.driver.core.querybuilder.QueryBuilder.*;
+
+//...
+
+BuiltStatement selectOne = QueryBuilder.select().from(""test"", ""test"")
+        .where(eq(""id"", 1));
+```
+
+In case if you need to specify complex condition you can put additional clauses into chain
+of the `and` calls that accepts the same clauses as `where`:
+
+```java
+BuiltStatement select = QueryBuilder.select().from(""test"", ""test"")
+        .where(eq(""id"", ""1"")).and(eq(""txt"", ""test""));
+```
+
+#### Other selection options
+
+For `SELECT` statements you can also specify a lot of different options: 
+ - `allowFiltering` generates corresponding `ALLOW FILTERING` part of query (***only use if you know what you're doing!***);
+ - `limit` & `perPartitionLimit` allows to specify amount of data to fetch;
+ - `groupBy` performs group of data;
+ - `orderBy` allows to specify sorting direction for specified clustering columns;
+ 
+This very ""artificial"" example shows the use for some of them:
+
+```java
+BuiltStatement selectOne = QueryBuilder.select().from(""test"")
+        .where(QueryBuilder.eq(""id"", 1)).limit(1).allowFiltering()
+        .perPartitionLimit(1).orderBy(desc(""id""));
+```
+
+### Inserting data
+
+Insertion of data is straightforward - you're specifying the target table in call to
+`insertInto`, and then provide values to insert either by chaining the several calls to
+`value` function, or by using the `values` function passing the lists or arrays of column
+names & corresponding values. Following 2 examples are equivalent:","[{'comment': 'Nit: ""The following 2 examples...""', 'commenter': 'adutra'}]"
1031,manual/statements/built/README.md,"@@ -1,5 +1,268 @@
 ## Built statements
 
-*Coming soon... In the meantime, see the javadoc for [QueryBuilder].*
+Built statements are generated via [QueryBuilder]'s Fluent API.  Use of Fluent API allows
+easier build of the complex queries, comparing with hardcoding the CQL statements into the
+source code.
 
-[QueryBuilder]: http://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html
\ No newline at end of file
+Note: The provider builders perform very little validation of the built query. There is
+thus no guarantee that a built query is valid, and it is definitively possible to create
+invalid queries.
+
+Queries built with `QueryBuilder` are execute the same way as other queries - via
+`execute` or `executeAsync`.  When query is built with putting all data into it, then it
+doesn't differ much from statement specified as string.  But it's also possible to build
+the query with bind markers inside it, and then convert it into [prepared statement](../prepared/).
+
+### Basics
+
+Generation of `BuiltStatement` is easy - just start with calling of one of the
+[QueryBuilder]'s methods of that represent the CQL ""verb"": `select`, `update`, `delete`,
+`insertInto`, or `truncate`, providing required parameters, and then call ""verb""-specific
+functions to form a complete CQL statement (like, `where`, `from`, etc.).  The command's
+target could be specified as table name (if you set default keyspace when creating a
+`Session` object), keyspace & table name, or as [TableMetadata].
+
+Note: The `QueryBuilder` doesn't provide support for full set of CQL.  For generation of
+DDL operations (`CREATE TABLE`, etc.) you can use the [SchemaBuilder].  To perform other
+operations you still need to use [simple statements](../simple/).
+
+### Selecting data
+
+Selection of data is quite simple - at minimum you need to provide a list of columns to
+select, and the specify from which table to select these columns (and conditions as
+described in the next section):
+
+```java
+BuiltStatement selectAll1 = QueryBuilder.select(""id"", ""t"").from(""test"", ""test"");
+ResultSet rs = session.execute(selectAll1);
+for (Row row: rs) {
+   System.out.println(row);
+}
+```
+
+Note: The call `select(""column1"", ""column2"")` is really a shortcut for a chain of calls
+`select().column(""column1"").column(""column2"")`.
+
+Please note that you can't pass the `*` as column name to select all columns - if you do
+this, you'll get an exception about unknown column. To select all columns you either need to use
+`select` in combination with `all` function, or simply don't specify a list of columns:
+
+```java
+BuiltStatement selectAll2 = QueryBuilder.select().all().from(""test"", ""test"");
+BuiltStatement selectAll3 = QueryBuilder.select().from(""test"", ""test"");
+```
+
+Besides selection of the specific columns there is also a possibility of calling of
+arbitrary function by using the `fcall` (this is just example, don't do this on real
+data):
+
+```java
+BuiltStatement sum = QueryBuilder.select().fcall(""sum"", column(""id"")).as(""sum_id"")
+        .from(""test"", ""test"");
+```
+
+Note: When using functions, Cassandra will generate column names for you, but you can
+provide explicit name by using `as` right after given expression.
+
+For often used functions, there are shortcuts, such as, `countAll`, `ttl`, `writeTime`,
+`uuid`, `now`, `toJson`, etc.:
+
+```
+BuiltStatement count = QueryBuilder.select().countAll()
+        .from(""test"", ""test"");
+
+BuiltStatement ttlAndWriteTime = QueryBuilder.select().column(""id"").column(""t"")
+                .ttl(""t"").as(""id_ttl"").writeTime(""t"")
+                .from(""test"", ""test"");
+```
+
+You can also cast the value of the given column to another type by using the `cast`
+function,
+[specifying](https://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html#cast-java.lang.Object-com.datastax.driver.core.DataType-)
+the column for which it should be performed & to what type it should be casted.
+
+#### Specifying conditions
+
+Selection of data rarely happen on the whole table - in most cases, people are interested
+for particular rows, located in one or several partitions.  Conditions are specified by
+using the `where` call, like this:
+
+```java
+BuiltStatement selectOne = QueryBuilder.select().from(""test"", ""test"")
+        .where(QueryBuilder.eq(""id"", 1));
+```
+
+The `where` function accept the `Clause` object that is generated by calling
+`QueryBuilder`'s
+[functions](https://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html#eq-java.lang.Iterable-java.lang.Iterable-),
+such as, `eq`, `ne`, `lt`, `in`, `contains`, `notNull`, etc.  In most cases, these
+functions receive 2 arguments - the name of the column, and value to compare, but there
+are also variants that receive 2 iterables for columns & values separately.
+
+Note: as query are becoming more complex, repeating `QueryBuilder` at all places will make
+code less readable.  In this case you can import all (or only required) static functions
+for code simplification (this is rewritten example from above):
+
+```java
+import static com.datastax.driver.core.querybuilder.QueryBuilder.*;
+
+//...
+
+BuiltStatement selectOne = QueryBuilder.select().from(""test"", ""test"")
+        .where(eq(""id"", 1));
+```
+
+In case if you need to specify complex condition you can put additional clauses into chain
+of the `and` calls that accepts the same clauses as `where`:
+
+```java
+BuiltStatement select = QueryBuilder.select().from(""test"", ""test"")
+        .where(eq(""id"", ""1"")).and(eq(""txt"", ""test""));
+```
+
+#### Other selection options
+
+For `SELECT` statements you can also specify a lot of different options: 
+ - `allowFiltering` generates corresponding `ALLOW FILTERING` part of query (***only use if you know what you're doing!***);
+ - `limit` & `perPartitionLimit` allows to specify amount of data to fetch;
+ - `groupBy` performs group of data;
+ - `orderBy` allows to specify sorting direction for specified clustering columns;
+ 
+This very ""artificial"" example shows the use for some of them:
+
+```java
+BuiltStatement selectOne = QueryBuilder.select().from(""test"")
+        .where(QueryBuilder.eq(""id"", 1)).limit(1).allowFiltering()
+        .perPartitionLimit(1).orderBy(desc(""id""));
+```
+
+### Inserting data
+
+Insertion of data is straightforward - you're specifying the target table in call to
+`insertInto`, and then provide values to insert either by chaining the several calls to
+`value` function, or by using the `values` function passing the lists or arrays of column
+names & corresponding values. Following 2 examples are equivalent:
+
+```java
+QueryBuilder.insertInto(""test"").value(""id"", 4).value(""t"", ""test 4"");
+QueryBuilder.insertInto(""test"").values(Arrays.asList(""id"", ""t""), Arrays.asList(4, ""test 4""));
+```
+
+You can also insert JSON-formatted data by calling the `json` function & passing the data:
+
+```java
+QueryBuilder.insertInto(""test"").json(""{\""id\"":4, \""t\"":\""test 4\""}"");
+```
+
+`QueryBuilder` also allows to generate the statement that will use lightweight","[{'comment': 'Suggestion: ""...to generate statements that use lightweight transactions""', 'commenter': 'adutra'}]"
1031,manual/statements/built/README.md,"@@ -1,5 +1,268 @@
 ## Built statements
 
-*Coming soon... In the meantime, see the javadoc for [QueryBuilder].*
+Built statements are generated via [QueryBuilder]'s Fluent API.  Use of Fluent API allows
+easier build of the complex queries, comparing with hardcoding the CQL statements into the
+source code.
 
-[QueryBuilder]: http://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html
\ No newline at end of file
+Note: The provider builders perform very little validation of the built query. There is
+thus no guarantee that a built query is valid, and it is definitively possible to create
+invalid queries.
+
+Queries built with `QueryBuilder` are execute the same way as other queries - via
+`execute` or `executeAsync`.  When query is built with putting all data into it, then it
+doesn't differ much from statement specified as string.  But it's also possible to build
+the query with bind markers inside it, and then convert it into [prepared statement](../prepared/).
+
+### Basics
+
+Generation of `BuiltStatement` is easy - just start with calling of one of the
+[QueryBuilder]'s methods of that represent the CQL ""verb"": `select`, `update`, `delete`,
+`insertInto`, or `truncate`, providing required parameters, and then call ""verb""-specific
+functions to form a complete CQL statement (like, `where`, `from`, etc.).  The command's
+target could be specified as table name (if you set default keyspace when creating a
+`Session` object), keyspace & table name, or as [TableMetadata].
+
+Note: The `QueryBuilder` doesn't provide support for full set of CQL.  For generation of
+DDL operations (`CREATE TABLE`, etc.) you can use the [SchemaBuilder].  To perform other
+operations you still need to use [simple statements](../simple/).
+
+### Selecting data
+
+Selection of data is quite simple - at minimum you need to provide a list of columns to
+select, and the specify from which table to select these columns (and conditions as
+described in the next section):
+
+```java
+BuiltStatement selectAll1 = QueryBuilder.select(""id"", ""t"").from(""test"", ""test"");
+ResultSet rs = session.execute(selectAll1);
+for (Row row: rs) {
+   System.out.println(row);
+}
+```
+
+Note: The call `select(""column1"", ""column2"")` is really a shortcut for a chain of calls
+`select().column(""column1"").column(""column2"")`.
+
+Please note that you can't pass the `*` as column name to select all columns - if you do
+this, you'll get an exception about unknown column. To select all columns you either need to use
+`select` in combination with `all` function, or simply don't specify a list of columns:
+
+```java
+BuiltStatement selectAll2 = QueryBuilder.select().all().from(""test"", ""test"");
+BuiltStatement selectAll3 = QueryBuilder.select().from(""test"", ""test"");
+```
+
+Besides selection of the specific columns there is also a possibility of calling of
+arbitrary function by using the `fcall` (this is just example, don't do this on real
+data):
+
+```java
+BuiltStatement sum = QueryBuilder.select().fcall(""sum"", column(""id"")).as(""sum_id"")
+        .from(""test"", ""test"");
+```
+
+Note: When using functions, Cassandra will generate column names for you, but you can
+provide explicit name by using `as` right after given expression.
+
+For often used functions, there are shortcuts, such as, `countAll`, `ttl`, `writeTime`,
+`uuid`, `now`, `toJson`, etc.:
+
+```
+BuiltStatement count = QueryBuilder.select().countAll()
+        .from(""test"", ""test"");
+
+BuiltStatement ttlAndWriteTime = QueryBuilder.select().column(""id"").column(""t"")
+                .ttl(""t"").as(""id_ttl"").writeTime(""t"")
+                .from(""test"", ""test"");
+```
+
+You can also cast the value of the given column to another type by using the `cast`
+function,
+[specifying](https://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html#cast-java.lang.Object-com.datastax.driver.core.DataType-)
+the column for which it should be performed & to what type it should be casted.
+
+#### Specifying conditions
+
+Selection of data rarely happen on the whole table - in most cases, people are interested
+for particular rows, located in one or several partitions.  Conditions are specified by
+using the `where` call, like this:
+
+```java
+BuiltStatement selectOne = QueryBuilder.select().from(""test"", ""test"")
+        .where(QueryBuilder.eq(""id"", 1));
+```
+
+The `where` function accept the `Clause` object that is generated by calling
+`QueryBuilder`'s
+[functions](https://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html#eq-java.lang.Iterable-java.lang.Iterable-),
+such as, `eq`, `ne`, `lt`, `in`, `contains`, `notNull`, etc.  In most cases, these
+functions receive 2 arguments - the name of the column, and value to compare, but there
+are also variants that receive 2 iterables for columns & values separately.
+
+Note: as query are becoming more complex, repeating `QueryBuilder` at all places will make
+code less readable.  In this case you can import all (or only required) static functions
+for code simplification (this is rewritten example from above):
+
+```java
+import static com.datastax.driver.core.querybuilder.QueryBuilder.*;
+
+//...
+
+BuiltStatement selectOne = QueryBuilder.select().from(""test"", ""test"")
+        .where(eq(""id"", 1));
+```
+
+In case if you need to specify complex condition you can put additional clauses into chain
+of the `and` calls that accepts the same clauses as `where`:
+
+```java
+BuiltStatement select = QueryBuilder.select().from(""test"", ""test"")
+        .where(eq(""id"", ""1"")).and(eq(""txt"", ""test""));
+```
+
+#### Other selection options
+
+For `SELECT` statements you can also specify a lot of different options: 
+ - `allowFiltering` generates corresponding `ALLOW FILTERING` part of query (***only use if you know what you're doing!***);
+ - `limit` & `perPartitionLimit` allows to specify amount of data to fetch;
+ - `groupBy` performs group of data;
+ - `orderBy` allows to specify sorting direction for specified clustering columns;
+ 
+This very ""artificial"" example shows the use for some of them:
+
+```java
+BuiltStatement selectOne = QueryBuilder.select().from(""test"")
+        .where(QueryBuilder.eq(""id"", 1)).limit(1).allowFiltering()
+        .perPartitionLimit(1).orderBy(desc(""id""));
+```
+
+### Inserting data
+
+Insertion of data is straightforward - you're specifying the target table in call to
+`insertInto`, and then provide values to insert either by chaining the several calls to
+`value` function, or by using the `values` function passing the lists or arrays of column
+names & corresponding values. Following 2 examples are equivalent:
+
+```java
+QueryBuilder.insertInto(""test"").value(""id"", 4).value(""t"", ""test 4"");
+QueryBuilder.insertInto(""test"").values(Arrays.asList(""id"", ""t""), Arrays.asList(4, ""test 4""));
+```
+
+You can also insert JSON-formatted data by calling the `json` function & passing the data:
+
+```java
+QueryBuilder.insertInto(""test"").json(""{\""id\"":4, \""t\"":\""test 4\""}"");
+```
+
+`QueryBuilder` also allows to generate the statement that will use lightweight
+transactions (LWT) to check that inserted data doesn't exist yet.  You just need to add
+the call to `ifNotExists` to the statement:
+
+```java
+QueryBuilder.insertInto(""test"").value(""id"", 4).ifNotExists();
+```
+
+It also possible to specify additional metadata for inserted data, such as, TTL or","[{'comment': 'Typo: ""such as TTL or timestamp"".', 'commenter': 'adutra'}]"
1031,manual/statements/built/README.md,"@@ -1,5 +1,268 @@
 ## Built statements
 
-*Coming soon... In the meantime, see the javadoc for [QueryBuilder].*
+Built statements are generated via [QueryBuilder]'s Fluent API.  Use of Fluent API allows
+easier build of the complex queries, comparing with hardcoding the CQL statements into the
+source code.
 
-[QueryBuilder]: http://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html
\ No newline at end of file
+Note: The provider builders perform very little validation of the built query. There is
+thus no guarantee that a built query is valid, and it is definitively possible to create
+invalid queries.
+
+Queries built with `QueryBuilder` are execute the same way as other queries - via
+`execute` or `executeAsync`.  When query is built with putting all data into it, then it
+doesn't differ much from statement specified as string.  But it's also possible to build
+the query with bind markers inside it, and then convert it into [prepared statement](../prepared/).
+
+### Basics
+
+Generation of `BuiltStatement` is easy - just start with calling of one of the
+[QueryBuilder]'s methods of that represent the CQL ""verb"": `select`, `update`, `delete`,
+`insertInto`, or `truncate`, providing required parameters, and then call ""verb""-specific
+functions to form a complete CQL statement (like, `where`, `from`, etc.).  The command's
+target could be specified as table name (if you set default keyspace when creating a
+`Session` object), keyspace & table name, or as [TableMetadata].
+
+Note: The `QueryBuilder` doesn't provide support for full set of CQL.  For generation of
+DDL operations (`CREATE TABLE`, etc.) you can use the [SchemaBuilder].  To perform other
+operations you still need to use [simple statements](../simple/).
+
+### Selecting data
+
+Selection of data is quite simple - at minimum you need to provide a list of columns to
+select, and the specify from which table to select these columns (and conditions as
+described in the next section):
+
+```java
+BuiltStatement selectAll1 = QueryBuilder.select(""id"", ""t"").from(""test"", ""test"");
+ResultSet rs = session.execute(selectAll1);
+for (Row row: rs) {
+   System.out.println(row);
+}
+```
+
+Note: The call `select(""column1"", ""column2"")` is really a shortcut for a chain of calls
+`select().column(""column1"").column(""column2"")`.
+
+Please note that you can't pass the `*` as column name to select all columns - if you do
+this, you'll get an exception about unknown column. To select all columns you either need to use
+`select` in combination with `all` function, or simply don't specify a list of columns:
+
+```java
+BuiltStatement selectAll2 = QueryBuilder.select().all().from(""test"", ""test"");
+BuiltStatement selectAll3 = QueryBuilder.select().from(""test"", ""test"");
+```
+
+Besides selection of the specific columns there is also a possibility of calling of
+arbitrary function by using the `fcall` (this is just example, don't do this on real
+data):
+
+```java
+BuiltStatement sum = QueryBuilder.select().fcall(""sum"", column(""id"")).as(""sum_id"")
+        .from(""test"", ""test"");
+```
+
+Note: When using functions, Cassandra will generate column names for you, but you can
+provide explicit name by using `as` right after given expression.
+
+For often used functions, there are shortcuts, such as, `countAll`, `ttl`, `writeTime`,
+`uuid`, `now`, `toJson`, etc.:
+
+```
+BuiltStatement count = QueryBuilder.select().countAll()
+        .from(""test"", ""test"");
+
+BuiltStatement ttlAndWriteTime = QueryBuilder.select().column(""id"").column(""t"")
+                .ttl(""t"").as(""id_ttl"").writeTime(""t"")
+                .from(""test"", ""test"");
+```
+
+You can also cast the value of the given column to another type by using the `cast`
+function,
+[specifying](https://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html#cast-java.lang.Object-com.datastax.driver.core.DataType-)
+the column for which it should be performed & to what type it should be casted.
+
+#### Specifying conditions
+
+Selection of data rarely happen on the whole table - in most cases, people are interested
+for particular rows, located in one or several partitions.  Conditions are specified by
+using the `where` call, like this:
+
+```java
+BuiltStatement selectOne = QueryBuilder.select().from(""test"", ""test"")
+        .where(QueryBuilder.eq(""id"", 1));
+```
+
+The `where` function accept the `Clause` object that is generated by calling
+`QueryBuilder`'s
+[functions](https://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html#eq-java.lang.Iterable-java.lang.Iterable-),
+such as, `eq`, `ne`, `lt`, `in`, `contains`, `notNull`, etc.  In most cases, these
+functions receive 2 arguments - the name of the column, and value to compare, but there
+are also variants that receive 2 iterables for columns & values separately.
+
+Note: as query are becoming more complex, repeating `QueryBuilder` at all places will make
+code less readable.  In this case you can import all (or only required) static functions
+for code simplification (this is rewritten example from above):
+
+```java
+import static com.datastax.driver.core.querybuilder.QueryBuilder.*;
+
+//...
+
+BuiltStatement selectOne = QueryBuilder.select().from(""test"", ""test"")
+        .where(eq(""id"", 1));
+```
+
+In case if you need to specify complex condition you can put additional clauses into chain
+of the `and` calls that accepts the same clauses as `where`:
+
+```java
+BuiltStatement select = QueryBuilder.select().from(""test"", ""test"")
+        .where(eq(""id"", ""1"")).and(eq(""txt"", ""test""));
+```
+
+#### Other selection options
+
+For `SELECT` statements you can also specify a lot of different options: 
+ - `allowFiltering` generates corresponding `ALLOW FILTERING` part of query (***only use if you know what you're doing!***);
+ - `limit` & `perPartitionLimit` allows to specify amount of data to fetch;
+ - `groupBy` performs group of data;
+ - `orderBy` allows to specify sorting direction for specified clustering columns;
+ 
+This very ""artificial"" example shows the use for some of them:
+
+```java
+BuiltStatement selectOne = QueryBuilder.select().from(""test"")
+        .where(QueryBuilder.eq(""id"", 1)).limit(1).allowFiltering()
+        .perPartitionLimit(1).orderBy(desc(""id""));
+```
+
+### Inserting data
+
+Insertion of data is straightforward - you're specifying the target table in call to
+`insertInto`, and then provide values to insert either by chaining the several calls to
+`value` function, or by using the `values` function passing the lists or arrays of column
+names & corresponding values. Following 2 examples are equivalent:
+
+```java
+QueryBuilder.insertInto(""test"").value(""id"", 4).value(""t"", ""test 4"");
+QueryBuilder.insertInto(""test"").values(Arrays.asList(""id"", ""t""), Arrays.asList(4, ""test 4""));
+```
+
+You can also insert JSON-formatted data by calling the `json` function & passing the data:
+
+```java
+QueryBuilder.insertInto(""test"").json(""{\""id\"":4, \""t\"":\""test 4\""}"");
+```
+
+`QueryBuilder` also allows to generate the statement that will use lightweight
+transactions (LWT) to check that inserted data doesn't exist yet.  You just need to add
+the call to `ifNotExists` to the statement:
+
+```java
+QueryBuilder.insertInto(""test"").value(""id"", 4).ifNotExists();
+```
+
+It also possible to specify additional metadata for inserted data, such as, TTL or
+timestamp.  This is achieved by using the `using` function and providing the `Using`
+object generated either by `ttl`, or `timestamp` functions of the `QueryBuilder` class. In
+case if you want to specify both of them, you need to wrap other into the call to `and`","[{'comment': 'Suggestion: ""If you want to specify both, you need to chain them together with the `and` operator"".', 'commenter': 'adutra'}]"
1031,manual/statements/built/README.md,"@@ -1,5 +1,268 @@
 ## Built statements
 
-*Coming soon... In the meantime, see the javadoc for [QueryBuilder].*
+Built statements are generated via [QueryBuilder]'s Fluent API.  Use of Fluent API allows
+easier build of the complex queries, comparing with hardcoding the CQL statements into the
+source code.
 
-[QueryBuilder]: http://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html
\ No newline at end of file
+Note: The provider builders perform very little validation of the built query. There is
+thus no guarantee that a built query is valid, and it is definitively possible to create
+invalid queries.
+
+Queries built with `QueryBuilder` are execute the same way as other queries - via
+`execute` or `executeAsync`.  When query is built with putting all data into it, then it
+doesn't differ much from statement specified as string.  But it's also possible to build
+the query with bind markers inside it, and then convert it into [prepared statement](../prepared/).
+
+### Basics
+
+Generation of `BuiltStatement` is easy - just start with calling of one of the
+[QueryBuilder]'s methods of that represent the CQL ""verb"": `select`, `update`, `delete`,
+`insertInto`, or `truncate`, providing required parameters, and then call ""verb""-specific
+functions to form a complete CQL statement (like, `where`, `from`, etc.).  The command's
+target could be specified as table name (if you set default keyspace when creating a
+`Session` object), keyspace & table name, or as [TableMetadata].
+
+Note: The `QueryBuilder` doesn't provide support for full set of CQL.  For generation of
+DDL operations (`CREATE TABLE`, etc.) you can use the [SchemaBuilder].  To perform other
+operations you still need to use [simple statements](../simple/).
+
+### Selecting data
+
+Selection of data is quite simple - at minimum you need to provide a list of columns to
+select, and the specify from which table to select these columns (and conditions as
+described in the next section):
+
+```java
+BuiltStatement selectAll1 = QueryBuilder.select(""id"", ""t"").from(""test"", ""test"");
+ResultSet rs = session.execute(selectAll1);
+for (Row row: rs) {
+   System.out.println(row);
+}
+```
+
+Note: The call `select(""column1"", ""column2"")` is really a shortcut for a chain of calls
+`select().column(""column1"").column(""column2"")`.
+
+Please note that you can't pass the `*` as column name to select all columns - if you do
+this, you'll get an exception about unknown column. To select all columns you either need to use
+`select` in combination with `all` function, or simply don't specify a list of columns:
+
+```java
+BuiltStatement selectAll2 = QueryBuilder.select().all().from(""test"", ""test"");
+BuiltStatement selectAll3 = QueryBuilder.select().from(""test"", ""test"");
+```
+
+Besides selection of the specific columns there is also a possibility of calling of
+arbitrary function by using the `fcall` (this is just example, don't do this on real
+data):
+
+```java
+BuiltStatement sum = QueryBuilder.select().fcall(""sum"", column(""id"")).as(""sum_id"")
+        .from(""test"", ""test"");
+```
+
+Note: When using functions, Cassandra will generate column names for you, but you can
+provide explicit name by using `as` right after given expression.
+
+For often used functions, there are shortcuts, such as, `countAll`, `ttl`, `writeTime`,
+`uuid`, `now`, `toJson`, etc.:
+
+```
+BuiltStatement count = QueryBuilder.select().countAll()
+        .from(""test"", ""test"");
+
+BuiltStatement ttlAndWriteTime = QueryBuilder.select().column(""id"").column(""t"")
+                .ttl(""t"").as(""id_ttl"").writeTime(""t"")
+                .from(""test"", ""test"");
+```
+
+You can also cast the value of the given column to another type by using the `cast`
+function,
+[specifying](https://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html#cast-java.lang.Object-com.datastax.driver.core.DataType-)
+the column for which it should be performed & to what type it should be casted.
+
+#### Specifying conditions
+
+Selection of data rarely happen on the whole table - in most cases, people are interested
+for particular rows, located in one or several partitions.  Conditions are specified by
+using the `where` call, like this:
+
+```java
+BuiltStatement selectOne = QueryBuilder.select().from(""test"", ""test"")
+        .where(QueryBuilder.eq(""id"", 1));
+```
+
+The `where` function accept the `Clause` object that is generated by calling
+`QueryBuilder`'s
+[functions](https://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html#eq-java.lang.Iterable-java.lang.Iterable-),
+such as, `eq`, `ne`, `lt`, `in`, `contains`, `notNull`, etc.  In most cases, these
+functions receive 2 arguments - the name of the column, and value to compare, but there
+are also variants that receive 2 iterables for columns & values separately.
+
+Note: as query are becoming more complex, repeating `QueryBuilder` at all places will make
+code less readable.  In this case you can import all (or only required) static functions
+for code simplification (this is rewritten example from above):
+
+```java
+import static com.datastax.driver.core.querybuilder.QueryBuilder.*;
+
+//...
+
+BuiltStatement selectOne = QueryBuilder.select().from(""test"", ""test"")
+        .where(eq(""id"", 1));
+```
+
+In case if you need to specify complex condition you can put additional clauses into chain
+of the `and` calls that accepts the same clauses as `where`:
+
+```java
+BuiltStatement select = QueryBuilder.select().from(""test"", ""test"")
+        .where(eq(""id"", ""1"")).and(eq(""txt"", ""test""));
+```
+
+#### Other selection options
+
+For `SELECT` statements you can also specify a lot of different options: 
+ - `allowFiltering` generates corresponding `ALLOW FILTERING` part of query (***only use if you know what you're doing!***);
+ - `limit` & `perPartitionLimit` allows to specify amount of data to fetch;
+ - `groupBy` performs group of data;
+ - `orderBy` allows to specify sorting direction for specified clustering columns;
+ 
+This very ""artificial"" example shows the use for some of them:
+
+```java
+BuiltStatement selectOne = QueryBuilder.select().from(""test"")
+        .where(QueryBuilder.eq(""id"", 1)).limit(1).allowFiltering()
+        .perPartitionLimit(1).orderBy(desc(""id""));
+```
+
+### Inserting data
+
+Insertion of data is straightforward - you're specifying the target table in call to
+`insertInto`, and then provide values to insert either by chaining the several calls to
+`value` function, or by using the `values` function passing the lists or arrays of column
+names & corresponding values. Following 2 examples are equivalent:
+
+```java
+QueryBuilder.insertInto(""test"").value(""id"", 4).value(""t"", ""test 4"");
+QueryBuilder.insertInto(""test"").values(Arrays.asList(""id"", ""t""), Arrays.asList(4, ""test 4""));
+```
+
+You can also insert JSON-formatted data by calling the `json` function & passing the data:
+
+```java
+QueryBuilder.insertInto(""test"").json(""{\""id\"":4, \""t\"":\""test 4\""}"");
+```
+
+`QueryBuilder` also allows to generate the statement that will use lightweight
+transactions (LWT) to check that inserted data doesn't exist yet.  You just need to add
+the call to `ifNotExists` to the statement:
+
+```java
+QueryBuilder.insertInto(""test"").value(""id"", 4).ifNotExists();
+```
+
+It also possible to specify additional metadata for inserted data, such as, TTL or
+timestamp.  This is achieved by using the `using` function and providing the `Using`
+object generated either by `ttl`, or `timestamp` functions of the `QueryBuilder` class. In
+case if you want to specify both of them, you need to wrap other into the call to `and`
+function:
+
+```java
+QueryBuilder.insertInto(""test"").value(""id"", 4).using(ttl(10)).and(timestamp(1000));
+```
+
+Besides this, for newer versions of Cassandra it's possible to specify additional
+parameters, such as `DEFAULT UNSET` & `DEFAULT NULL` in the `INSERT INTO ... JSON`
+statements, by using `defaultUnset` & `defaultNull` correspondingly.
+
+### Update statements
+
+Updating the data also relatively straightforward - you specify the data to update,","[{'comment': 'Typo: ""Updating data is also relatively straightforward"".', 'commenter': 'adutra'}]"
1031,manual/statements/built/README.md,"@@ -1,5 +1,268 @@
 ## Built statements
 
-*Coming soon... In the meantime, see the javadoc for [QueryBuilder].*
+Built statements are generated via [QueryBuilder]'s Fluent API.  Use of Fluent API allows
+easier build of the complex queries, comparing with hardcoding the CQL statements into the
+source code.
 
-[QueryBuilder]: http://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html
\ No newline at end of file
+Note: The provider builders perform very little validation of the built query. There is
+thus no guarantee that a built query is valid, and it is definitively possible to create
+invalid queries.
+
+Queries built with `QueryBuilder` are execute the same way as other queries - via
+`execute` or `executeAsync`.  When query is built with putting all data into it, then it
+doesn't differ much from statement specified as string.  But it's also possible to build
+the query with bind markers inside it, and then convert it into [prepared statement](../prepared/).
+
+### Basics
+
+Generation of `BuiltStatement` is easy - just start with calling of one of the
+[QueryBuilder]'s methods of that represent the CQL ""verb"": `select`, `update`, `delete`,
+`insertInto`, or `truncate`, providing required parameters, and then call ""verb""-specific
+functions to form a complete CQL statement (like, `where`, `from`, etc.).  The command's
+target could be specified as table name (if you set default keyspace when creating a
+`Session` object), keyspace & table name, or as [TableMetadata].
+
+Note: The `QueryBuilder` doesn't provide support for full set of CQL.  For generation of
+DDL operations (`CREATE TABLE`, etc.) you can use the [SchemaBuilder].  To perform other
+operations you still need to use [simple statements](../simple/).
+
+### Selecting data
+
+Selection of data is quite simple - at minimum you need to provide a list of columns to
+select, and the specify from which table to select these columns (and conditions as
+described in the next section):
+
+```java
+BuiltStatement selectAll1 = QueryBuilder.select(""id"", ""t"").from(""test"", ""test"");
+ResultSet rs = session.execute(selectAll1);
+for (Row row: rs) {
+   System.out.println(row);
+}
+```
+
+Note: The call `select(""column1"", ""column2"")` is really a shortcut for a chain of calls
+`select().column(""column1"").column(""column2"")`.
+
+Please note that you can't pass the `*` as column name to select all columns - if you do
+this, you'll get an exception about unknown column. To select all columns you either need to use
+`select` in combination with `all` function, or simply don't specify a list of columns:
+
+```java
+BuiltStatement selectAll2 = QueryBuilder.select().all().from(""test"", ""test"");
+BuiltStatement selectAll3 = QueryBuilder.select().from(""test"", ""test"");
+```
+
+Besides selection of the specific columns there is also a possibility of calling of
+arbitrary function by using the `fcall` (this is just example, don't do this on real
+data):
+
+```java
+BuiltStatement sum = QueryBuilder.select().fcall(""sum"", column(""id"")).as(""sum_id"")
+        .from(""test"", ""test"");
+```
+
+Note: When using functions, Cassandra will generate column names for you, but you can
+provide explicit name by using `as` right after given expression.
+
+For often used functions, there are shortcuts, such as, `countAll`, `ttl`, `writeTime`,
+`uuid`, `now`, `toJson`, etc.:
+
+```
+BuiltStatement count = QueryBuilder.select().countAll()
+        .from(""test"", ""test"");
+
+BuiltStatement ttlAndWriteTime = QueryBuilder.select().column(""id"").column(""t"")
+                .ttl(""t"").as(""id_ttl"").writeTime(""t"")
+                .from(""test"", ""test"");
+```
+
+You can also cast the value of the given column to another type by using the `cast`
+function,
+[specifying](https://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html#cast-java.lang.Object-com.datastax.driver.core.DataType-)
+the column for which it should be performed & to what type it should be casted.
+
+#### Specifying conditions
+
+Selection of data rarely happen on the whole table - in most cases, people are interested
+for particular rows, located in one or several partitions.  Conditions are specified by
+using the `where` call, like this:
+
+```java
+BuiltStatement selectOne = QueryBuilder.select().from(""test"", ""test"")
+        .where(QueryBuilder.eq(""id"", 1));
+```
+
+The `where` function accept the `Clause` object that is generated by calling
+`QueryBuilder`'s
+[functions](https://docs.datastax.com/en/drivers/java/3.5/com/datastax/driver/core/querybuilder/QueryBuilder.html#eq-java.lang.Iterable-java.lang.Iterable-),
+such as, `eq`, `ne`, `lt`, `in`, `contains`, `notNull`, etc.  In most cases, these
+functions receive 2 arguments - the name of the column, and value to compare, but there
+are also variants that receive 2 iterables for columns & values separately.
+
+Note: as query are becoming more complex, repeating `QueryBuilder` at all places will make
+code less readable.  In this case you can import all (or only required) static functions
+for code simplification (this is rewritten example from above):
+
+```java
+import static com.datastax.driver.core.querybuilder.QueryBuilder.*;
+
+//...
+
+BuiltStatement selectOne = QueryBuilder.select().from(""test"", ""test"")
+        .where(eq(""id"", 1));
+```
+
+In case if you need to specify complex condition you can put additional clauses into chain
+of the `and` calls that accepts the same clauses as `where`:
+
+```java
+BuiltStatement select = QueryBuilder.select().from(""test"", ""test"")
+        .where(eq(""id"", ""1"")).and(eq(""txt"", ""test""));
+```
+
+#### Other selection options
+
+For `SELECT` statements you can also specify a lot of different options: 
+ - `allowFiltering` generates corresponding `ALLOW FILTERING` part of query (***only use if you know what you're doing!***);
+ - `limit` & `perPartitionLimit` allows to specify amount of data to fetch;
+ - `groupBy` performs group of data;
+ - `orderBy` allows to specify sorting direction for specified clustering columns;
+ 
+This very ""artificial"" example shows the use for some of them:
+
+```java
+BuiltStatement selectOne = QueryBuilder.select().from(""test"")
+        .where(QueryBuilder.eq(""id"", 1)).limit(1).allowFiltering()
+        .perPartitionLimit(1).orderBy(desc(""id""));
+```
+
+### Inserting data
+
+Insertion of data is straightforward - you're specifying the target table in call to
+`insertInto`, and then provide values to insert either by chaining the several calls to
+`value` function, or by using the `values` function passing the lists or arrays of column
+names & corresponding values. Following 2 examples are equivalent:
+
+```java
+QueryBuilder.insertInto(""test"").value(""id"", 4).value(""t"", ""test 4"");
+QueryBuilder.insertInto(""test"").values(Arrays.asList(""id"", ""t""), Arrays.asList(4, ""test 4""));
+```
+
+You can also insert JSON-formatted data by calling the `json` function & passing the data:
+
+```java
+QueryBuilder.insertInto(""test"").json(""{\""id\"":4, \""t\"":\""test 4\""}"");
+```
+
+`QueryBuilder` also allows to generate the statement that will use lightweight
+transactions (LWT) to check that inserted data doesn't exist yet.  You just need to add
+the call to `ifNotExists` to the statement:
+
+```java
+QueryBuilder.insertInto(""test"").value(""id"", 4).ifNotExists();
+```
+
+It also possible to specify additional metadata for inserted data, such as, TTL or
+timestamp.  This is achieved by using the `using` function and providing the `Using`
+object generated either by `ttl`, or `timestamp` functions of the `QueryBuilder` class. In
+case if you want to specify both of them, you need to wrap other into the call to `and`
+function:
+
+```java
+QueryBuilder.insertInto(""test"").value(""id"", 4).using(ttl(10)).and(timestamp(1000));
+```
+
+Besides this, for newer versions of Cassandra it's possible to specify additional
+parameters, such as `DEFAULT UNSET` & `DEFAULT NULL` in the `INSERT INTO ... JSON`
+statements, by using `defaultUnset` & `defaultNull` correspondingly.
+
+### Update statements
+
+Updating the data also relatively straightforward - you specify the data to update,
+condition, and additional options if necessary:","[{'comment': 'Nit: ""a condition"".', 'commenter': 'adutra'}]"
1032,integration-tests/src/test/java/com/datastax/oss/driver/api/core/specex/SpeculativeExecutionIT.java,"@@ -327,14 +329,19 @@ public void should_not_speculatively_execute_when_defined_in_profile() {
   private CqlSession buildSession(int maxSpeculativeExecutions, long speculativeDelayMs) {
     return SessionUtils.newSession(
         simulacron,
-        String.format(""basic.request.timeout = %d milliseconds"", SPECULATIVE_DELAY * 10),
-        ""basic.request.default-idempotence = true"",
-        ""basic.load-balancing-policy.class = com.datastax.oss.driver.api.testinfra.loadbalancing.SortingLoadBalancingPolicy"",
-        ""advanced.speculative-execution-policy.class = ConstantSpeculativeExecutionPolicy"",
-        String.format(
-            ""advanced.speculative-execution-policy.max-executions = %d"", maxSpeculativeExecutions),
-        String.format(
-            ""advanced.speculative-execution-policy.delay = %d milliseconds"", speculativeDelayMs));
+        ProgrammaticDriverConfigLoader.builder()","[{'comment': 'Example of using this', 'commenter': 'tolbertam'}]"
1032,integration-tests/src/test/java/com/datastax/oss/driver/api/core/specex/SpeculativeExecutionIT.java,"@@ -343,54 +350,67 @@ private CqlSession buildSessionWithProfile(
       int profile1MaxSpeculativeExecutions,
       long profile1SpeculativeDelayMs) {
 
-    List<String> config = Lists.newArrayList();
-    config.add(String.format(""basic.request.timeout = %d milliseconds"", SPECULATIVE_DELAY * 10));
-    config.add(""basic.request.default-idempotence = true"");
-    config.add(
-        ""basic.load-balancing-policy.class = com.datastax.oss.driver.api.testinfra.loadbalancing.SortingLoadBalancingPolicy"");
+    ProgrammaticDriverConfigLoader.Builder builder =","[{'comment': 'Example of using this with profiles.', 'commenter': 'tolbertam'}, {'comment': 'I see no profile ref here or below. What am I missing?', 'commenter': 'stamhankar999'}]"
1032,core/src/main/java/com/datastax/oss/driver/api/core/config/ProgrammaticDriverConfigLoader.java,"@@ -0,0 +1,217 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core.config;
+
+import com.datastax.oss.driver.internal.core.config.typesafe.DefaultDriverConfigLoader;
+import com.typesafe.config.Config;
+import com.typesafe.config.ConfigFactory;
+import com.typesafe.config.ConfigValueFactory;
+import java.time.Duration;
+import java.util.List;
+import java.util.Map;
+import java.util.function.Supplier;
+
+public class ProgrammaticDriverConfigLoader {
+
+  private ProgrammaticDriverConfigLoader() {}
+
+  public static Builder builder() {
+    return new Builder();
+  }
+
+  public static class Builder {
+
+    private Config config = ConfigFactory.empty();
+
+    private Supplier<Config> fallbackSupplier = DefaultDriverConfigLoader.DEFAULT_CONFIG_SUPPLIER;
+
+    private Builder() {}
+
+    public Builder withBoolean(DriverOption option, boolean value) {
+      return with(option, value);
+    }
+
+    public Builder withBoolean(String profileName, DriverOption option, boolean value) {","[{'comment': 'One idea I had for profiles which would be feasible would be to have a way for user a to create a `DriverConfigProfile` and then have a `withProfile(DriverConfigProfile profile)` method in here.  That would reduce the duplicated methods.   The one downside of that is that you could not use these`DriverConfigProfile` objects when working with the driver, but we could probably find a way to throw an early runtime exception if someone tries that.    What do you think of this idea?', 'commenter': 'tolbertam'}, {'comment': 'Mmm not a fan...\r\nHow about a ""sub-builder"":\r\n```java\r\nwithProfile(\r\n    ""slow"",\r\n    profileBuilder()\r\n        .withDuration(DefaultDriverOption.REQUEST_TIMEOUT, Duration.ofSeconds(30))\r\n        .build()\r\n)\r\n```\r\nIt could reuse the same internal interface that I\'m suggesting below.', 'commenter': 'olim7t'}, {'comment': 'I like it :+1: will pursue this.', 'commenter': 'tolbertam'}]"
1032,integration-tests/src/test/java/com/datastax/oss/driver/api/core/specex/SpeculativeExecutionIT.java,"@@ -343,54 +350,67 @@ private CqlSession buildSessionWithProfile(
       int profile1MaxSpeculativeExecutions,
       long profile1SpeculativeDelayMs) {
 
-    List<String> config = Lists.newArrayList();
-    config.add(String.format(""basic.request.timeout = %d milliseconds"", SPECULATIVE_DELAY * 10));
-    config.add(""basic.request.default-idempotence = true"");
-    config.add(
-        ""basic.load-balancing-policy.class = com.datastax.oss.driver.api.testinfra.loadbalancing.SortingLoadBalancingPolicy"");
+    ProgrammaticDriverConfigLoader.Builder builder =
+        ProgrammaticDriverConfigLoader.builder()
+            .withDuration(
+                DefaultDriverOption.REQUEST_TIMEOUT, Duration.ofMillis(SPECULATIVE_DELAY * 10))
+            .withBoolean(DefaultDriverOption.REQUEST_DEFAULT_IDEMPOTENCE, true);
 
     if (defaultMaxSpeculativeExecutions != -1 || defaultSpeculativeDelayMs != -1) {
-      config.add(
-          ""advanced.speculative-execution-policy.class = ConstantSpeculativeExecutionPolicy"");
+      builder =
+          builder.withClass(
+              DefaultDriverOption.SPECULATIVE_EXECUTION_POLICY_CLASS,
+              ConstantSpeculativeExecutionPolicy.class);
       if (defaultMaxSpeculativeExecutions != -1) {
-        config.add(
-            String.format(
-                ""advanced.speculative-execution-policy.max-executions = %d"",
-                defaultMaxSpeculativeExecutions));
+        builder =
+            builder.withInt(
+                DefaultDriverOption.SPECULATIVE_EXECUTION_MAX, defaultMaxSpeculativeExecutions);
       }
       if (defaultSpeculativeDelayMs != -1) {
-        config.add(
-            String.format(
-                ""advanced.speculative-execution-policy.delay = %d milliseconds"",
-                defaultSpeculativeDelayMs));
+        builder =
+            builder.withDuration(
+                DefaultDriverOption.SPECULATIVE_EXECUTION_DELAY,
+                Duration.ofMillis(defaultSpeculativeDelayMs));
       }
     } else {
-      config.add(""advanced.speculative-execution-policy.class = NoSpeculativeExecutionPolicy"");
+      builder =
+          builder.withClass(
+              DefaultDriverOption.SPECULATIVE_EXECUTION_POLICY_CLASS,
+              NoSpeculativeExecutionPolicy.class);
     }
 
     if (profile1MaxSpeculativeExecutions != -1 || profile1SpeculativeDelayMs != -1) {
-      config.add(
-          ""profiles.profile1.advanced.speculative-execution-policy.class = ConstantSpeculativeExecutionPolicy"");
+      builder =
+          builder.withClass(
+              ""profile1"",","[{'comment': 'This one uses a profile; is this what you were referring to?', 'commenter': 'stamhankar999'}, {'comment': 'Yes, sorry if it was confusing.', 'commenter': 'tolbertam'}]"
1032,core/src/main/java/com/datastax/oss/driver/api/core/config/ProgrammaticDriverConfigLoader.java,"@@ -0,0 +1,217 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core.config;
+
+import com.datastax.oss.driver.internal.core.config.typesafe.DefaultDriverConfigLoader;
+import com.typesafe.config.Config;
+import com.typesafe.config.ConfigFactory;
+import com.typesafe.config.ConfigValueFactory;
+import java.time.Duration;
+import java.util.List;
+import java.util.Map;
+import java.util.function.Supplier;
+
+public class ProgrammaticDriverConfigLoader {
+
+  private ProgrammaticDriverConfigLoader() {}
+
+  public static Builder builder() {
+    return new Builder();
+  }
+
+  public static class Builder {
+
+    private Config config = ConfigFactory.empty();
+
+    private Supplier<Config> fallbackSupplier = DefaultDriverConfigLoader.DEFAULT_CONFIG_SUPPLIER;
+
+    private Builder() {}
+
+    public Builder withBoolean(DriverOption option, boolean value) {
+      return with(option, value);
+    }
+
+    public Builder withBoolean(String profileName, DriverOption option, boolean value) {
+      return with(profileName, option, value);
+    }
+
+    public Builder withBooleanList(DriverOption option, List<Boolean> value) {
+      return with(option, value);
+    }
+
+    public Builder withBooleanList(String profileName, DriverOption option, List<Boolean> value) {
+      return with(profileName, option, value);
+    }
+
+    public Builder withInt(DriverOption option, int value) {
+      return with(option, value);
+    }
+
+    public Builder withInt(String profileName, DriverOption option, int value) {
+      return with(profileName, option, value);
+    }
+
+    public Builder withIntList(DriverOption option, List<Integer> value) {
+      return with(option, value);
+    }
+
+    public Builder withIntList(String profileName, DriverOption option, List<Integer> value) {
+      return with(profileName, option, value);
+    }
+
+    public Builder withLong(DriverOption option, long value) {
+      return with(option, value);
+    }
+
+    public Builder withLong(String profileName, DriverOption option, long value) {
+      return with(profileName, option, value);
+    }
+
+    public Builder withLongList(DriverOption option, List<Long> value) {
+      return with(option, value);
+    }
+
+    public Builder withLongList(String profileName, DriverOption option, List<Long> value) {
+      return with(profileName, option, value);
+    }
+
+    public Builder withDouble(DriverOption option, double value) {
+      return with(option, value);
+    }
+
+    public Builder withDouble(String profileName, DriverOption option, double value) {
+      return with(profileName, option, value);
+    }
+
+    public Builder withDoubleList(DriverOption option, List<Double> value) {
+      return with(option, value);
+    }
+
+    public Builder withDoubleList(String profileName, DriverOption option, List<Double> value) {
+      return with(profileName, option, value);
+    }
+
+    public Builder withString(DriverOption option, String value) {
+      return with(option, value);
+    }
+
+    public Builder withString(String profileName, DriverOption option, String value) {
+      return with(profileName, option, value);
+    }
+
+    public Builder withStringList(DriverOption option, List<String> value) {
+      return with(option, value);
+    }
+
+    public Builder withStringList(String profileName, DriverOption option, List<String> value) {
+      return with(option, value);
+    }
+
+    public Builder withStringMap(DriverOption option, Map<String, String> value) {
+      for (String key : value.keySet()) {
+        config =
+            config.withValue(
+                option.getPath() + '.' + key, ConfigValueFactory.fromAnyRef(value.get(key)));
+      }
+      return this;
+    }
+
+    public Builder withStringMap(
+        String profileName, DriverOption option, Map<String, String> value) {
+      for (String key : value.keySet()) {
+        String path = ""profiles."" + profileName + '.' + option.getPath() + '.' + key;
+        config = config.withValue(path, ConfigValueFactory.fromAnyRef(value.get(key)));
+      }
+      return this;
+    }
+
+    public Builder withBytes(DriverOption option, long value) {
+      return with(option, value);
+    }
+
+    public Builder withBytes(String profileName, DriverOption option, long value) {
+      return with(profileName, option, value);
+    }
+
+    public Builder withBytesList(DriverOption option, List<Long> value) {
+      return with(option, value);
+    }
+
+    public Builder withBytesList(String profileName, DriverOption option, List<Long> value) {
+      return with(profileName, option, value);
+    }
+
+    public Builder withDuration(DriverOption option, Duration value) {
+      return with(option, value);
+    }
+
+    public Builder withDuration(String profileName, DriverOption option, Duration value) {
+      return with(profileName, option, value);
+    }
+
+    public Builder withDurationList(DriverOption option, List<Duration> value) {
+      return with(option, value);
+    }
+
+    public Builder withDurationList(String profileName, DriverOption option, List<Duration> value) {
+      return with(profileName, option, value);
+    }
+
+    public Builder withClass(DriverOption option, Class<?> value) {
+      return with(option, value.getName());
+    }
+
+    public Builder withClass(String profileName, DriverOption option, Class<?> value) {
+      return with(profileName, option, value.getName());
+    }
+
+    /** Unsets an option. */
+    public Builder without(DriverOption option) {
+      return with(option, null);
+    }
+
+    public Builder without(String profileName, DriverOption option) {
+      return with(profileName, option, null);
+    }
+
+    public Builder withFallback(Supplier<Config> configSupplier) {","[{'comment': ""This leaks Typesafe Config through the public API (parameter of a public method in an `api.*` package).\r\n\r\nMore generally, `ProgrammaticDriverConfigLoader` only works with Typesafe config, so I think we should either:\r\n\r\na. extract an interface that has the `withXxx` methods and works with any `DriverConfigLoader`, but that's kind of hard given how generic the latter is.\r\nb. make programmatic configuration an internal-only thing.\r\n\r\nI'm leaning towards b, having the ability to change options programmatically is an implementation-dependent thing."", 'commenter': 'olim7t'}, {'comment': ""Hrm, i'll think some more about this.  Agree that we shouldn't leak typesafe config.  In addition I still need to think a little bit more about how this will work with the dse driver.  I think preferably this could be part of api and not internal, but maybe that is the right thing to do if there isn't a good way to allow overriding the backing config in some way."", 'commenter': 'tolbertam'}]"
1032,core/src/main/java/com/datastax/oss/driver/api/core/config/ProgrammaticDriverConfigLoader.java,"@@ -0,0 +1,217 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core.config;
+
+import com.datastax.oss.driver.internal.core.config.typesafe.DefaultDriverConfigLoader;
+import com.typesafe.config.Config;
+import com.typesafe.config.ConfigFactory;
+import com.typesafe.config.ConfigValueFactory;
+import java.time.Duration;
+import java.util.List;
+import java.util.Map;
+import java.util.function.Supplier;
+
+public class ProgrammaticDriverConfigLoader {
+
+  private ProgrammaticDriverConfigLoader() {}
+
+  public static Builder builder() {
+    return new Builder();
+  }
+
+  public static class Builder {
+
+    private Config config = ConfigFactory.empty();
+
+    private Supplier<Config> fallbackSupplier = DefaultDriverConfigLoader.DEFAULT_CONFIG_SUPPLIER;
+
+    private Builder() {}
+
+    public Builder withBoolean(DriverOption option, boolean value) {
+      return with(option, value);
+    }
+
+    public Builder withBoolean(String profileName, DriverOption option, boolean value) {
+      return with(profileName, option, value);
+    }
+
+    public Builder withBooleanList(DriverOption option, List<Boolean> value) {
+      return with(option, value);","[{'comment': ""This is the same as in `TypesafeDriverConfigProfile`. You could extract an internal interface with default methods and only have to implement `with()`. As a bonus, this would help validate that we're not missing any method here.\r\n(provided that the loader becomes an internal class as suggested in my other comment)"", 'commenter': 'olim7t'}]"
1032,core/src/main/java/com/datastax/oss/driver/api/core/config/ProgrammaticDriverConfigLoader.java,"@@ -0,0 +1,152 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core.config;
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.DefaultConsistencyLevel;
+import com.datastax.oss.driver.internal.core.config.typesafe.DefaultDriverConfigLoader;
+import com.datastax.oss.driver.shaded.guava.common.collect.ImmutableMap;
+import com.datastax.oss.driver.shaded.guava.common.collect.ImmutableSortedSet;
+import com.typesafe.config.Config;
+import com.typesafe.config.ConfigFactory;
+import com.typesafe.config.ConfigValueFactory;
+import java.time.Duration;
+import java.util.AbstractMap;
+import java.util.Comparator;
+import java.util.Map;
+import java.util.SortedSet;
+
+/**
+ * Provides a mechanism for constructing a {@link DriverConfigLoader} programmatically as an
+ * alternative, or in addition to, the default configuration mechanism which is to load
+ * configuration from configuration files using typesafe config.
+ *
+ * <p>The built {@link DriverConfigLoader} provided by {@link Builder#build()} can be passed to
+ * {@link
+ * com.datastax.oss.driver.api.core.session.SessionBuilder#withConfigLoader(DriverConfigLoader)}.
+ *
+ * <p>Configuration provided to {@link Builder}'s {@code withXXX} methods will override the values
+ * provided in the base configuration.
+ */
+public final class ProgrammaticDriverConfigLoader {
+
+  private ProgrammaticDriverConfigLoader() {}
+
+  /** @return a new {@link Builder} to provide programmatic configuration. */
+  public static Builder builder() {
+    return new Builder();
+  }
+
+  /**
+   * @return a new {@link ProfileBuilder} to provide programmatic configuration at a profile level.
+   * @see Builder#withProfile(String, Profile)
+   */
+  public static ProfileBuilder profileBuilder() {
+    return new ProfileBuilder();
+  }
+
+  public static final class Builder implements ProgrammaticBuilder<Builder> {
+
+    private ImmutableMap.Builder<String, Object> values = ImmutableMap.builder();
+
+    private Builder() {}
+
+    /**
+     * @return Constructs a config loader based on the configuration values provided to this builder
+     *     and falls back to the default configuration mechanism of the driver.
+     */
+    public DriverConfigLoader build() {
+      ProgrammaticDriverConfigLoader.Builder configBuilder =","[{'comment': 'woops, accidental commit, will remove this.', 'commenter': 'tolbertam'}]"
1032,test-infra/src/main/java/com/datastax/oss/driver/api/testinfra/session/DefaultSessionBuilderInstantiator.java,"@@ -16,13 +16,18 @@
 package com.datastax.oss.driver.api.testinfra.session;
 
 import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.config.DriverConfigLoaderBuilder;
 import com.datastax.oss.driver.api.core.session.SessionBuilder;
 
 public class DefaultSessionBuilderInstantiator {
   public static SessionBuilder<?, ?> builder() {
     return CqlSession.builder();
   }
 
+  public static DriverConfigLoaderBuilder configLoaderBuilder() {","[{'comment': 'Changing our tests to use this instead of `TestConfigLoader`, we can get rid of `configPath()`, and then this class starts to look at a lot like `CqlSession`.  In fact, we can get rid of `DefaultSessionBuilderInstantiator` completely and just point to `CqlSession` instead.', 'commenter': 'tolbertam'}]"
1032,core/src/main/java/com/datastax/oss/driver/api/core/CqlSession.java,"@@ -36,6 +37,18 @@ static CqlSessionBuilder builder() {
     return new CqlSessionBuilder();
   }
 
+  /**
+   * Returns a builder to create a new {@link DriverConfigLoader} that uses the default config
+   * loader as a basis, but allows providing additional configuration programmatically.
+   *
+   * <p>The built config loader may then be passed to {@link
+   * com.datastax.oss.driver.api.core.session.SessionBuilder#withConfigLoader(DriverConfigLoader)}.
+   */
+  @NonNull
+  static CqlDriverConfigLoaderBuilder configLoaderBuilder() {","[{'comment': ""I think providing `configLoaderBuilder` at the `CqlSession` level is a good happy medium.  It makes it easy enough to pass in programmatic configuration, but not so obvious that it's something that users will want to by default.  So I think this should continue to encourage use of configuration files, with providing programmatic config a fallback for where that isn't viable."", 'commenter': 'tolbertam'}]"
1032,test-infra/src/main/java/com/datastax/oss/driver/api/testinfra/session/SessionUtils.java,"@@ -110,14 +125,26 @@ public static String getConfigPath() {
     return newSession(cassandraResource, keyspace, null, null, null, options);
   }
 
-  @SuppressWarnings({""unchecked"", ""TypeParameterUnusedInFormals""})
+  @SuppressWarnings(""TypeParameterUnusedInFormals"")
   public static <SessionT extends Session> SessionT newSession(
+      CassandraResourceRule cassandraResourceRule, DriverConfigLoaderBuilder loader) {","[{'comment': 'This should take `DriverConfigLoader` instead of `DriverConfigLoaderBuilder`, will update.', 'commenter': 'tolbertam'}, {'comment': 'Actually, I think this will need to take a builder, since we need to do further customization in some places', 'commenter': 'tolbertam'}, {'comment': 'nevermind, found a way around it.', 'commenter': 'tolbertam'}]"
1032,core/src/main/java/com/datastax/oss/driver/api/core/config/DriverConfigLoaderBuilder.java,"@@ -0,0 +1,116 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core.config;
+
+import com.datastax.oss.driver.shaded.guava.common.collect.ImmutableSortedSet;
+import com.datastax.oss.protocol.internal.util.collection.NullAllowingImmutableMap;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import edu.umd.cs.findbugs.annotations.Nullable;
+import java.util.AbstractMap;
+import java.util.Comparator;
+import java.util.Map;
+import java.util.SortedSet;
+import net.jcip.annotations.NotThreadSafe;
+
+/**
+ * Provides a mechanism for constructing a {@link DriverConfigLoader} programmatically.
+ *
+ * <p>The built {@link DriverConfigLoader} provided by {@link #build()} can be passed to {@link
+ * com.datastax.oss.driver.api.core.session.SessionBuilder#withConfigLoader(DriverConfigLoader)}.
+ */
+@NotThreadSafe
+public abstract class DriverConfigLoaderBuilder<SelfT extends DriverConfigLoaderBuilder<SelfT>>
+    implements ProgrammaticConfigBuilder<SelfT> {
+
+  @SuppressWarnings(""unchecked"")
+  protected final SelfT self = (SelfT) this;
+
+  /**
+   * @return a new {@link ProfileBuilder} to provide programmatic configuration at a profile level.
+   * @see #withProfile(String, Profile)
+   */
+  @NonNull
+  public static ProfileBuilder profileBuilder() {
+    return new ProfileBuilder();
+  }
+
+  private NullAllowingImmutableMap.Builder<String, Object> values =
+      NullAllowingImmutableMap.builder();
+
+  /**
+   * @return constructed {@link DriverConfigLoader} using the configuration passed into this
+   *     builder.
+   */
+  @NonNull
+  public abstract DriverConfigLoader build();
+
+  /** @return All configured entries as a map. */
+  @NonNull
+  public SortedSet<Map.Entry<String, Object>> entrySet() {
+    ImmutableSortedSet.Builder<Map.Entry<String, Object>> builder =
+        ImmutableSortedSet.orderedBy(Comparator.comparing(Map.Entry::getKey));
+    for (Map.Entry<String, Object> entry : values.build().entrySet()) {
+      builder.add(new AbstractMap.SimpleEntry<>(entry.getKey(), entry.getValue()));
+    }
+    return builder.build();
+  }
+
+  @NonNull
+  @Override
+  public SelfT with(@NonNull String path, @Nullable Object value) {
+    values.put(path, value);
+    return self;
+  }
+
+  /** Adds configuration for a profile constructed using {@link #profileBuilder()} by name. */
+  @NonNull
+  public SelfT withProfile(String profileName, Profile profile) {
+    String prefix = ""profiles."" + profileName + ""."";","[{'comment': 'This is kind of leaky: it assumes every config implementation will prepend profiles in the path. Which is probably likely, but technically nothing in the `DriverConfig` API mandates it.', 'commenter': 'olim7t'}]"
1032,core/src/main/java/com/datastax/oss/driver/api/core/CqlDriverConfigLoaderBuilder.java,"@@ -0,0 +1,54 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core;
+
+import com.datastax.oss.driver.api.core.config.DriverConfigLoader;
+import com.datastax.oss.driver.api.core.config.DriverConfigLoaderBuilder;
+import com.datastax.oss.driver.internal.core.config.typesafe.DefaultDriverConfigLoader;
+import com.typesafe.config.Config;
+import com.typesafe.config.ConfigFactory;
+import com.typesafe.config.ConfigValueFactory;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import java.util.Map;
+import net.jcip.annotations.NotThreadSafe;
+
+/**
+ * Provides a mechanism for constructing a {@link DriverConfigLoader} programmatically that uses
+ * {@link com.datastax.oss.driver.api.core.CqlSession}'s default config loader with the values of
+ * {@code withXXX(...)} methods overriding the configuration defined in configuration files.
+ *
+ * <p>The built {@link DriverConfigLoader} provided by {@link #build()} can be passed to {@link
+ * com.datastax.oss.driver.api.core.session.SessionBuilder#withConfigLoader(DriverConfigLoader)}.
+ */
+@NotThreadSafe
+public class CqlDriverConfigLoaderBuilder
+    extends DriverConfigLoaderBuilder<CqlDriverConfigLoaderBuilder> {","[{'comment': 'This should not be called ""Cql"". It could be used with any kind of session so ""Default"" or ""Typesafe"" would be better. Also it should be in an internal package.', 'commenter': 'olim7t'}]"
1036,core/src/main/java/com/datastax/oss/driver/api/core/CqlSession.java,"@@ -38,6 +38,9 @@ static CqlSessionBuilder builder() {
   /**
    * Executes a CQL statement synchronously (the calling thread blocks until the result becomes
    * available).
+   *
+   * @param statement The statement to execute; never {@code null}.","[{'comment': ""If we annotate the method as `@Nonnull`, is it necessary to document it?  Doesn't the annotation show up in the javadocs?"", 'commenter': 'tolbertam'}, {'comment': 'Yes it does show up. This is a leftover from a previous state where I was duplicating the information everywhere in the docs. I will remove, thanks for pointing that out.', 'commenter': 'adutra'}]"
1036,core/src/main/java/com/datastax/oss/driver/api/core/CqlSession.java,"@@ -47,6 +50,9 @@ default ResultSet execute(@Nonnull Statement<?> statement) {
   /**
    * Executes a CQL statement synchronously (the calling thread blocks until the result becomes
    * available).
+   *
+   * @param query The query to execute; never {@code null}.
+   * @return The {@linkplain ResultSet results} returned by the server; never {@code null}.","[{'comment': 'I think we tend to omit `@param` / `@return` definitions if it is clear/obvious what the values mean as a way to reduce documentation noise.', 'commenter': 'tolbertam'}]"
1036,core/src/main/java/com/datastax/oss/driver/api/core/data/GettableByIndex.java,"@@ -160,9 +170,12 @@ default Object getObject(int i) {
   default boolean getBoolean(int i) {
     DataType cqlType = getType(i);
     TypeCodec<Boolean> codec = codecRegistry().codecFor(cqlType, Boolean.class);
-    return (codec instanceof PrimitiveBooleanCodec)
-        ? ((PrimitiveBooleanCodec) codec).decodePrimitive(getBytesUnsafe(i), protocolVersion())
-        : get(i, codec);
+    if (codec instanceof PrimitiveBooleanCodec) {","[{'comment': ""why change this?  It doesn't seem to change the behavior (nulls get changed to false in boolean conversion)"", 'commenter': 'tolbertam'}, {'comment': 'There was a risk of NPE in the previous version:\r\n```\r\n    return (codec instanceof PrimitiveByteCodec)\r\n        ? ((PrimitiveByteCodec) codec).decodePrimitive(getBytesUnsafe(i), protocolVersion())\r\n        : get(i, codec);\r\n```\r\nThe unboxing of `get(i, codec)` will throw NPE if that method returns `null` (which can happen depending on which codec has been selected).', 'commenter': 'adutra'}]"
1036,core/src/main/java/com/datastax/oss/driver/api/core/data/GettableByIndex.java,"@@ -427,10 +469,13 @@ default Token getToken(int i) {
    * <p>This method is provided for convenience when the element type is a non-generic type. For
    * more complex list types, use {@link #get(int, GenericType)}.
    *
+   * @return the {@code i}th value as a Java map; may be empty but never {@code null}.
    * @throws IndexOutOfBoundsException if the index is invalid.
    */
+  @Nonnull
   default <T> List<T> getList(int i, Class<T> elementsClass) {
-    return get(i, GenericType.listOf(elementsClass));
+    List<T> value = get(i, GenericType.listOf(elementsClass));
+    return value == null ? new ArrayList<>(0) : value;","[{'comment': ""Hrmm, is this a hard requirement that it returns an empty list?  Shouldn't it be dependent on the registered codec?  The list codec will return nulls as empty collections, but I see that the 3.x code defers to the codec and doesn't make any assumptions otherwise:\r\n\r\nhttps://github.com/datastax/java-driver/blob/3.x/driver-core/src/main/java/com/datastax/driver/core/GettableByIndexData.java#L306-L309"", 'commenter': 'tolbertam'}, {'comment': ""We discussed yesterday about collections and I was under the impression that the general policy is to never return null, but rather empty collections for List, Set and Map. This change is required to comply with the `@Nonnull` annotations. But I don't have strong opinions here, changing the contract to `@Nullable` is fine too. Also, I'm returning regular java.util collections rather than immutable structures as I saw that the built-in codecs do the same (which I find a bit surprising btw)."", 'commenter': 'adutra'}, {'comment': ""When we talked of empty collections, I was thinking more of session-level methods, such as `Metadata.getKeyspaces()`: return an empty collection even when schema metadata is disabled, instead of wrapping the collection in an `Optional`.\r\n\r\nFor row-level getters I think we should keep the current behavior of depending on the registered codec.\r\n\r\nI kept the returned collections mutable because I felt it would trip users too much if you can't manipulate the result directly, and the risk of thread safety issues when you're dealing with a simple column value is pretty minimal."", 'commenter': 'olim7t'}]"
1036,core/src/main/java/com/datastax/oss/driver/api/core/AllNodesFailedException.java,"@@ -75,6 +76,7 @@ private static String buildMessage(Map<Node, Throwable> errors) {
     return errors;
   }
 
+  @Nonnull","[{'comment': 'Out of curiosity: how did you determine where to add the annotations? Is there any tooling, or did you just go over the whole codebase?\r\nWe want to keep using them consistently in new code, it would be nice to have a way to check automatically (ideally as part of the build).', 'commenter': 'olim7t'}, {'comment': ""Thee is no automatic tool to generate or suggest where to place annotations to my knowledge. IntelliJ can however suggest to propagate annotations from interfaces to implementations, but otherwise I've been crawling the entire codebase. "", 'commenter': 'adutra'}]"
1036,core/src/main/java/com/datastax/oss/driver/api/core/CqlIdentifier.java,"@@ -66,7 +67,7 @@
   // One exception is named getters, where we keep raw strings with the 3.x rules.
 
   /** Creates an identifier from its {@link CqlIdentifier CQL form}. */
-  public static CqlIdentifier fromCql(String cql) {
+  public static @Nonnull CqlIdentifier fromCql(@Nonnull String cql) {","[{'comment': 'I know we\'ve not normalized that yet, but I\'m not a big fan of ""sandwiching"" the annotation between the modifiers and the return type. I find the following more readable, maybe because I\'m more used to it:\r\n```java\r\n  @Nonnull\r\n  public static CqlIdentifier fromCql(@Nonnull String cql) {\r\n```\r\nAny other opinions?', 'commenter': 'olim7t'}, {'comment': 'I agree with you actually, this line somewhat was annotated differently from all the others.', 'commenter': 'adutra'}]"
1036,core/src/main/java/com/datastax/oss/driver/api/core/metadata/schema/IndexMetadata.java,"@@ -19,25 +19,33 @@
 import com.datastax.oss.driver.internal.core.metadata.schema.ScriptBuilder;
 import com.datastax.oss.driver.shaded.guava.common.collect.Maps;
 import java.util.Map;
+import java.util.Optional;
+import javax.annotation.Nonnull;
 
 /** A secondary index in the schema metadata. */
 public interface IndexMetadata extends Describable {
 
+  @Nonnull
   CqlIdentifier getKeyspace();
 
+  @Nonnull
   CqlIdentifier getTable();
 
+  @Nonnull
   CqlIdentifier getName();
 
+  @Nonnull
   IndexKind getKind();
 
+  @Nonnull
   String getTarget();
 
   /**
    * If this index is custom, the name of the server-side implementation. Otherwise, {@code null}.
    */
-  default String getClassName() {
-    return getOptions().get(""class_name"");
+  @Nonnull
+  default Optional<String> getClassName() {","[{'comment': 'The javadoc should be updated too, it still says null.', 'commenter': 'olim7t'}]"
1036,core/src/main/java/com/datastax/oss/driver/internal/core/metadata/schema/DefaultAggregateMetadata.java,"@@ -114,7 +128,7 @@ public boolean equals(Object other) {
       AggregateMetadata that = (AggregateMetadata) other;
       return Objects.equals(this.keyspace, that.getKeyspace())
           && Objects.equals(this.signature, that.getSignature())
-          && Objects.equals(this.finalFuncSignature, that.getFinalFuncSignature())
+          && Objects.equals(this.finalFuncSignature, that.getFinalFuncSignature().orElse(null))","[{'comment': ""Good catch. Should be the same for `initCond` on the next line.\r\nIt's a bit unnerving that this is not a compile-time error :(\r\n\r\nEDIT -- I checked with a structural search: `Objects.equals($o1$, $o2$)` with a constraint on `o2`'s expression type `java\\.util\\.Optional`. This is the only occurrence."", 'commenter': 'olim7t'}, {'comment': 'On my side IntelliJ warns me that an equals comparison is being performed on incompatible types in such situations.', 'commenter': 'adutra'}, {'comment': 'I guess the problem with `initCond` is that `initCond` is an `Object` so I had no warnings from IntelliJ.', 'commenter': 'adutra'}]"
1036,core/src/main/java/com/datastax/oss/driver/internal/core/metrics/DropwizardMetricsFactory.java,"@@ -60,19 +61,19 @@ public DropwizardMetricsFactory(InternalDriverContext context) {
       LOG.debug(""[{}] All metrics are disabled, Session.getMetrics will be empty"", logPrefix);
       this.registry = null;
       this.sessionUpdater = NoopSessionMetricUpdater.INSTANCE;
-      this.metrics = Optional.empty();
+      this.metrics = null;
     } else {
       this.registry = new MetricRegistry();
       DropwizardSessionMetricUpdater dropwizardSessionUpdater =
           new DropwizardSessionMetricUpdater(enabledSessionMetrics, registry, context);
       this.sessionUpdater = dropwizardSessionUpdater;
-      this.metrics = Optional.of(new DefaultMetrics(registry, dropwizardSessionUpdater));
+      this.metrics = new DefaultMetrics(registry, dropwizardSessionUpdater);
     }
   }
 
   @Override
   public Optional<Metrics> getMetrics() {
-    return metrics;
+    return Optional.ofNullable(metrics);","[{'comment': ""Why not, I don't care either way since this method should not be called very often.\r\nBut for my understanding: any rationale for wrapping at the last minute rather of storing the optional as a field?"", 'commenter': 'olim7t'}, {'comment': 'Yes, it is not advised to store `Optional` instances as instance fields, mainly because `Optional` is not serializable.', 'commenter': 'adutra'}, {'comment': 'I found a few other occurrences of `Optional` as instance fields. I will fix them in a separate commit, but if you prefer to keep things the way they are, we can just drop that commit later.', 'commenter': 'adutra'}, {'comment': 'I think if the field is final, the enclosing type is not meant to be serializable, and it is on the hot path, then we should store the optional as a field. Otherwise, I\'m fine recreating it each time.\r\n(so this particular example falls into the ""rewrap on every call"" category)', 'commenter': 'olim7t'}]"
1036,core/src/test/java/com/datastax/oss/driver/internal/core/metadata/schema/parsing/AggregateParserTest.java,"@@ -76,12 +77,13 @@ public void should_parse_modern_table() {
         .containsExactly(DataTypes.INT, DataTypes.INT);
     assertThat(aggregate.getStateType()).isEqualTo(DataTypes.INT);
 
-    FunctionSignature finalFuncSignature = aggregate.getFinalFuncSignature();
-    assertThat(finalFuncSignature.getName().asInternal()).isEqualTo(""to_string"");
-    assertThat(finalFuncSignature.getParameterTypes()).containsExactly(DataTypes.INT);
+    Optional<FunctionSignature> finalFuncSignature = aggregate.getFinalFuncSignature();
+    assertThat(finalFuncSignature).isPresent();
+    assertThat(finalFuncSignature.get().getName().asInternal()).isEqualTo(""to_string"");
+    assertThat(finalFuncSignature.get().getParameterTypes()).containsExactly(DataTypes.INT);","[{'comment': 'You can rewrite this in a slightly prettier way with `assertThat(finalFuncSignature).hasValueSatisfying(signature -> { ... })`.', 'commenter': 'olim7t'}]"
1036,core/src/main/java/com/datastax/oss/driver/api/core/cql/ExecutionInfo.java,"@@ -34,9 +36,11 @@
 public interface ExecutionInfo {
 
   /** The statement that was executed. */
+  @NonNull
   Statement<?> getStatement();
 
   /** The node that was used as a coordinator to successfully complete the query. */
+  @NonNull
   Node getCoordinator();","[{'comment': 'This can actually be null under certain circumstances. My bad because I should have documented it in the first place, I will push a commit to address it.', 'commenter': 'olim7t'}, {'comment': ""Nevermind, that's only after my changes in #1038 "", 'commenter': 'olim7t'}]"
1036,core/src/main/java/com/datastax/oss/driver/api/core/cql/Statement.java,"@@ -66,54 +68,72 @@
    * <p>All the driver's built-in implementations are immutable, and return a new instance from this
    * method. However custom implementations may choose to be mutable and return the same instance.
    */
-  T setConfigProfileName(String newConfigProfileName);
+  @NonNull
+  T setConfigProfileName(@NonNull String newConfigProfileName);","[{'comment': '`newConfigProfileName` is nullable, it could be used to remove an existing name. Same for `setConfigProfile`.', 'commenter': 'olim7t'}]"
1036,core/src/main/java/com/datastax/oss/driver/api/core/cql/StatementBuilder.java,"@@ -86,24 +91,28 @@ public T withRoutingKeyspace(CqlIdentifier routingKeyspace) {
    * Shortcut for {@link #withRoutingKeyspace(CqlIdentifier)
    * withRoutingKeyspace(CqlIdentifier.fromCql(routingKeyspaceName))}.
    */
-  public T withRoutingKeyspace(String routingKeyspaceName) {
+  @NonNull
+  public T withRoutingKeyspace(@NonNull String routingKeyspaceName) {","[{'comment': 'Most arguments are nullable if you initialized the builder by copying another statement, and need to clear some of the attributes.', 'commenter': 'olim7t'}]"
1036,core/src/main/java/com/datastax/oss/driver/api/core/cql/TraceEvent.java,"@@ -15,18 +15,21 @@
  */
 package com.datastax.oss.driver.api.core.cql;
 
+import edu.umd.cs.findbugs.annotations.Nullable;
 import java.net.InetAddress;
 
 /** An event in a {@link QueryTrace}. */
 public interface TraceEvent {
 
   /** Which activity this event corresponds to. */
+  @Nullable
   String getActivity();","[{'comment': ""This is read directly from Cassandra tables. In theory it shouldn't be null, but the data could be corrupted (it's possible to alter it manually), so I agree with marking it nullable."", 'commenter': 'olim7t'}]"
1036,core/src/main/java/com/datastax/oss/driver/api/core/loadbalancing/LoadBalancingPolicy.java,"@@ -64,7 +65,8 @@ void init(
    * @return the list of coordinators to try. <b>This must be a concurrent queue</b>; {@link
    *     java.util.concurrent.ConcurrentLinkedQueue} is a good choice.
    */
-  Queue<Node> newQueryPlan(Request request, Session session);
+  @NonNull
+  Queue<Node> newQueryPlan(@NonNull Request request, @NonNull Session session);","[{'comment': 'Both arguments nullable per the javadoc.', 'commenter': 'olim7t'}]"
1036,core/src/main/java/com/datastax/oss/driver/api/core/metadata/Node.java,"@@ -57,12 +60,16 @@
    * <p>This may not be know at all times. In particular, current Cassandra versions (3.10) only
    * store it in {@code system.local}, so this will be known only for the control node.
    */
+  @NonNull
   Optional<InetSocketAddress> getListenAddress();
 
+  @NonNull
   String getDatacenter();
 
+  @NonNull
   String getRack();
 
+  @NonNull
   Version getCassandraVersion();","[{'comment': ""Datacenter, rack and version should be non-null in a healthy deployment, but the driver would still work if the data was corrupted. I'm going to mark them as nullable and add explanatory javadocs.\r\n\r\nedit -- same for host id and schema version"", 'commenter': 'olim7t'}]"
1036,core/src/main/java/com/datastax/oss/driver/api/core/metadata/schema/FunctionSignature.java,"@@ -30,38 +32,45 @@
  */
 @Immutable
 public class FunctionSignature {
-  private final CqlIdentifier name;
-  private final List<DataType> parameterTypes;
+  @NonNull private final CqlIdentifier name;
+  @NonNull private final List<DataType> parameterTypes;
 
-  public FunctionSignature(CqlIdentifier name, Iterable<DataType> parameterTypes) {
+  public FunctionSignature(
+      @NonNull CqlIdentifier name, @NonNull Iterable<DataType> parameterTypes) {
     this.name = name;
     this.parameterTypes = ImmutableList.copyOf(parameterTypes);
   }
 
-  public FunctionSignature(CqlIdentifier name, DataType... parameterTypes) {
-    this(name, ImmutableList.<DataType>builder().add(parameterTypes).build());
+  public FunctionSignature(@NonNull CqlIdentifier name, @Nullable DataType... parameterTypes) {
+    this(
+        name,
+        parameterTypes == null
+            ? ImmutableList.of()
+            : ImmutableList.<DataType>builder().add(parameterTypes).build());","[{'comment': ""Small glitch here but this is more an issue in the initial implementation: if the constructor is called with no varargs, `parameterTypes` is empty, not null. I think it's impossible to get null, I'll fix the method and mark it as non nullable."", 'commenter': 'olim7t'}, {'comment': '> I think it\'s impossible to get null\r\n\r\nMmm actually it is possible ðŸ™ :\r\n```java\r\npublic static void test(Object... args) {\r\n  System.out.println(args == null ? ""null"" : args.length);\r\n}\r\ntest((Object) null); // prints ""1"" (non-null array containing 1 null element)\r\ntest(null); // prints ""null"" (null array)\r\ntest((Object[]) null); // prints ""null"" (null array)\r\n```\r\n\r\n[This answer](https://stackoverflow.com/a/32327819/3775443) suggests a way to differentiate the two. Sounds a bit overkill but I\'ll check all vararg parameters in our codebase.', 'commenter': 'olim7t'}]"
1040,core/src/main/java/com/datastax/oss/driver/internal/core/tracker/RequestLogger.java,"@@ -155,4 +168,21 @@ protected void logError(
       LOG.error(""{} [{}]"", builder.toString(), error.toString());
     }
   }
+","[{'comment': 'My only question here is should these be moved to a general utility for re-use?', 'commenter': 'GregBestland'}, {'comment': 'Maybe this is too clever but we could generalize this to one method on `DriverConfigProfile` using a function variable for the method to call, i.e.:\r\n\r\n```java\r\npublic interface DriverConfigProfile {\r\n  // ...\r\n  default <T> Optional<T> get(\r\n      DriverOption option, BiFunction<DriverConfigProfile, DriverOption, T> getter) {\r\n    return isDefined(option) ? Optional.of(getter.apply(this, option)) : Optional.empty();\r\n  }\r\n  // ...\r\n}\r\n```\r\n\r\nAnd to use it:\r\n\r\n```java\r\nboolean successEnabled =\r\n    configProfile\r\n        .get(\r\n            DefaultDriverOption.REQUEST_LOGGER_SUCCESS_ENABLED, DriverConfigProfile::getBoolean)\r\n        .orElse(false);\r\n```\r\n\r\n', 'commenter': 'tolbertam'}, {'comment': ""I don't like how this creates an `Optional` for every call. This is on the hot path because these options are re-read for each request.\r\nMaybe we could have `DriverConfigOption.getBoolean(DriverOption option, boolean default)`, but then it makes sense to have it for every type. It can be done as default methods as you suggest, so why not."", 'commenter': 'olim7t'}, {'comment': 'To remove optional, tt could be redefined as:\r\n\r\n```java\r\ndefault <T> T get(\r\n      DriverOption option, BiFunction<DriverConfigProfile, DriverOption, T> getter, T defaultValue) {\r\n  return isDefined(option) ? getter.apply(this, option) : defaultvalue;\r\n}\r\n```\r\n\r\nBut that is more of an effort to reduce adding another method for every possible return type.  Really wish java supported default arguments :rage:', 'commenter': 'tolbertam'}]"
1040,core/src/main/java/com/datastax/oss/driver/api/core/config/DefaultDriverOptionUtil.java,"@@ -0,0 +1,227 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.datastax.oss.driver.api.core.config;
+
+import java.time.Duration;
+import java.util.List;
+import java.util.Map;
+
+public class DefaultDriverOptionUtil {
+  /**
+   * Fetch a boolean from the {@link DriverConfigProfile} if it exists, otherwise return provided
+   * default.
+   *
+   * @param configProfile to fetch from.
+   * @param option to fetch.
+   * @param defaultValue fallback value if the option doesn't exist.
+   * @return the specified boolean option, or the default if it's not defined.
+   */
+  public static boolean getConfigBooleanIfDefined(
+      DriverConfigProfile configProfile, DefaultDriverOption option, boolean defaultValue) {","[{'comment': 'My suggestion was to add those directly to `DriverConfigProfile`, as default methods: `getBoolean(DriverOption option, boolean default)`, etc.', 'commenter': 'olim7t'}]"
1043,core/src/main/java/com/datastax/oss/driver/api/core/type/reflect/GenericType.java,"@@ -226,6 +226,24 @@ public final boolean isPrimitive() {
     return where(freeVariable, GenericType.of(actualType));
   }
 
+  /**
+   * Returns the raw type of {@code T}. Formally speaking, if {@code T} is returned by {@link
+   * java.lang.reflect.Method#getGenericReturnType}, the raw type is what's returned by {@link
+   * java.lang.reflect.Method#getReturnType} of the same method object. Specifically:
+   *
+   * <ul>
+   *   <li>If {@code T} is a {@code Class} itself, {@code T} itself is returned.
+   *   <li>If {@code T} is a parameterized type, the raw type of the parameterized type is returned.
+   *   <li>If {@code T} is an array type , the returned type is the corresponding array class. For
+   *       example: {@code List<Integer>[] => List[]}.
+   *   <li>If {@code T} is a type variable or a wildcard type, the raw type of the first upper bound
+   *       is returned. For example: {@code <X extends Foo> => Foo}.
+   * </ul>
+   */
+  public Class<?> getRawType() {","[{'comment': ""Shouldn't this be `<? super T>`?"", 'commenter': 'stamhankar999'}, {'comment': ""you are right, good catch!  I'll fix it."", 'commenter': 'tolbertam'}]"
1043,core/src/main/java/com/datastax/oss/driver/api/core/type/reflect/GenericType.java,"@@ -251,6 +252,7 @@ public final boolean isPrimitive() {
    * Returns the array component type if this type represents an array ({@code int[]}, {@code T[]},
    * {@code <? extends Map<String, Integer>[]>} etc.), or else {@code null} is returned.
    */
+  @Nullable
   @SuppressWarnings(""unchecked"")
   public final GenericType<?> getComponentType() {
     return new GenericType(token.getComponentType());","[{'comment': ""This produces a warning because `new GenericType(...)` is never null. Which is actually the symptom of a bug: `token.getComponentType()` can be null, this must be handled.\r\nI'm fixing it directly."", 'commenter': 'olim7t'}, {'comment': ""Ah right, I didn't even consider that, good catch!"", 'commenter': 'tolbertam'}]"
1044,core/src/main/java/com/datastax/oss/driver/internal/core/data/ValuesHelper.java,"@@ -47,7 +47,9 @@
     for (int i = 0; i < values.length; i++) {
       Object value = values[i];
       ByteBuffer encodedValue;
-      if (value instanceof Token) {
+      if (value == null) {
+        encodedValue = null;
+      } else if (value instanceof Token) {","[{'comment': 'This works because all of our built-in codecs encode `null` as a CQL `null`.\r\n\r\nBut in theory someone could plug their own codec registry, with default codecs that handle null differently. So out of principle I would prefer letting the codec handle it. As suggested on the JIRA ticket, changing line 64 to:\r\n```java\r\nTypeCodec<Object> codec =\r\n    (value == null)\r\n        ? codecRegistry.codecFor(fieldTypes.get(i))\r\n        : codecRegistry.codecFor(fieldTypes.get(i), value);\r\nencodedValue = codec.encode(value, protocolVersion);\r\n```', 'commenter': 'olim7t'}, {'comment': 'I think `encodePreparedValues` has the same issue, this would be apparent in `PreparedStatement.bind(Object...)`.', 'commenter': 'olim7t'}, {'comment': 'Ahh, this totally makes sense. Will make the appropriate changes and resubmit for review.', 'commenter': 'stamhankar999'}]"
1049,driver-core/src/main/java/com/datastax/driver/core/AbstractTableMetadata.java,"@@ -280,6 +280,7 @@ protected StringBuilder appendOptions(StringBuilder sb, boolean formatted) {
         if (cassandraVersion.getMajor() > 3 || (cassandraVersion.getMajor() == 3 && cassandraVersion.getMinor() >= 8)) {
             and(sb, formatted).append(""cdc = "").append(options.isCDC());
         }
+		and(sb, formatted).append(""memtable_flush_period_in_ms = "").append(options.getMemtableFlushPeriodInMs());","[{'comment': 'This option has been introduced in Cassandra 2.0 by CASSANDRA-4237.\r\n\r\nSo i would suggest that you only include it when the version is 2.0 or higher:\r\n\r\n```\r\n        if (cassandraVersion.getMajor() > 1) {\r\n            and(sb, formatted).append(""memtable_flush_period_in_ms = "").append(options.getMemtableFlushPeriodInMs());\r\n        }\r\n```', 'commenter': 'adutra'}, {'comment': 'Thanks for the suggestion. Added suggested changes.', 'commenter': 'hkbharath'}]"
1050,driver-core/src/main/java/com/datastax/driver/core/SchemaParser.java,"@@ -735,4 +761,78 @@ private boolean references(DataType dataType, String typeName) {
             return false;
         }
     }
+
+    private static class V4SchemaParser extends V3SchemaParser {
+
+        private static final String SELECT_KEYSPACES = ""SELECT * FROM system_schema.keyspaces"";
+        private static final String SELECT_TABLES = ""SELECT * FROM system_schema.tables"";
+        private static final String SELECT_COLUMNS = ""SELECT * FROM system_schema.columns"";
+        private static final String SELECT_USERTYPES = ""SELECT * FROM system_schema.types"";
+        private static final String SELECT_FUNCTIONS = ""SELECT * FROM system_schema.functions"";
+        private static final String SELECT_AGGREGATES = ""SELECT * FROM system_schema.aggregates"";
+        private static final String SELECT_INDEXES = ""SELECT * FROM system_schema.indexes"";
+        private static final String SELECT_VIEWS = ""SELECT * FROM system_schema.views"";
+        private static final String SELECT_VIRTUAL_KEYSPACES = ""SELECT * FROM system_virtual_schema.keyspaces"";
+        private static final String SELECT_VIRTUAL_TABLES = ""SELECT * FROM system_virtual_schema.tables"";
+        private static final String SELECT_VIRTUAL_COLUMNS = ""SELECT * FROM system_virtual_schema.columns"";
+
+
+        private static final String TABLE_NAME = ""table_name"";
+
+        @Override
+        SystemRows fetchSystemRows(Cluster cluster, SchemaElement targetType, String targetKeyspace, String targetName, List<String> targetSignature, Connection connection, VersionNumber cassandraVersion)
+                throws ConnectionException, BusyConnectionException, ExecutionException, InterruptedException {
+
+            boolean isSchemaOrKeyspace = (targetType == null || targetType == KEYSPACE);
+
+            ResultSetFuture ksFuture = null,
+                    udtFuture = null,
+                    cfFuture = null,
+                    colsFuture = null,
+                    functionsFuture = null,
+                    aggregatesFuture = null,
+                    indexesFuture = null,
+                    viewsFuture = null,
+                    virtualKeyspacesFuture = null,
+                    virtualTableFuture = null,
+                    virtualColumnsFuture = null;
+
+            ProtocolVersion protocolVersion = cluster.getConfiguration().getProtocolOptions().getProtocolVersion();
+
+            if (isSchemaOrKeyspace)
+                ksFuture = queryAsync(SELECT_KEYSPACES + whereClause(targetType, targetKeyspace, targetName, targetSignature), connection, protocolVersion);
+            virtualKeyspacesFuture = queryAsync(SELECT_VIRTUAL_KEYSPACES + whereClause(targetType, targetKeyspace, targetName, targetSignature), connection, protocolVersion);","[{'comment': ""This should be executed conditionnally, we don't need to run this query if the target type is e.g. `FUNCTION`.\r\n\r\nI don't think we can get targeted schema notifications -- `targetType == KEYSPACE or TABLE` -- for virtual elements (but we need to double-check that, I haven't looked at the server code). If that is the case, then all virtual metadata (including tables and columns) only needs to be queried on a full schema refresh, we would include them in the `isSchemaOrKeyspace` block above."", 'commenter': 'olim7t'}]"
1050,driver-core/src/main/java/com/datastax/driver/core/KeyspaceMetadata.java,"@@ -51,6 +52,15 @@
         this.durableWrites = durableWrites;
         this.replication = replication;
         this.strategy = ReplicationStrategy.create(replication);
+        this.isVirtual=false;
+    }
+    @VisibleForTesting
+    KeyspaceMetadata(String name, boolean durableWrites, Map<String, String> replication, boolean isVirtual) {","[{'comment': 'Nit: rename `isVirtual` to `virtual`', 'commenter': 'tolbertam'}]"
1050,driver-core/src/main/java/com/datastax/driver/core/KeyspaceMetadata.java,"@@ -51,6 +52,15 @@
         this.durableWrites = durableWrites;
         this.replication = replication;
         this.strategy = ReplicationStrategy.create(replication);
+        this.isVirtual=false;
+    }
+    @VisibleForTesting
+    KeyspaceMetadata(String name, boolean durableWrites, Map<String, String> replication, boolean isVirtual) {
+        this.name = name;
+        this.durableWrites = durableWrites;
+        this.replication = replication;
+        this.strategy = ReplicationStrategy.create(replication);
+        this.isVirtual=isVirtual;","[{'comment': 'Nit: space between equal sign', 'commenter': 'tolbertam'}]"
1050,driver-core/src/main/java/com/datastax/driver/core/KeyspaceMetadata.java,"@@ -51,6 +52,15 @@
         this.durableWrites = durableWrites;
         this.replication = replication;
         this.strategy = ReplicationStrategy.create(replication);
+        this.isVirtual=false;","[{'comment': 'Would be preferable for this constructor should reuse the new one (i.e. `this(name, durableWrites, replication, false)`).\r\n\r\nShould also mark this constructor as deprecated as it looks like existing uses of it were updated to use the new one.', 'commenter': 'tolbertam'}]"
1050,driver-core/src/main/java/com/datastax/driver/core/KeyspaceMetadata.java,"@@ -61,13 +71,17 @@ static KeyspaceMetadata build(Row row, VersionNumber cassandraVersion) {
             replicationOptions = new HashMap<String, String>();
             replicationOptions.put(""class"", row.getString(STRATEGY_CLASS));
             replicationOptions.putAll(SimpleJSONParser.parseStringMap(row.getString(STRATEGY_OPTIONS)));
-            return new KeyspaceMetadata(name, durableWrites, replicationOptions);
+            return new KeyspaceMetadata(name, durableWrites, replicationOptions, false);
         } else {
             String name = row.getString(KS_NAME);
             boolean durableWrites = row.getBool(DURABLE_WRITES);
-            return new KeyspaceMetadata(name, durableWrites, row.getMap(REPLICATION, String.class, String.class));
+            return new KeyspaceMetadata(name, durableWrites, row.getMap(REPLICATION, String.class, String.class), false);
         }
     }
+    static KeyspaceMetadata buildVirtual(Row row, VersionNumber cassandraVersion) {","[{'comment': 'Nit: Add space between line 80 and 81', 'commenter': 'tolbertam'}]"
1050,driver-core/src/main/java/com/datastax/driver/core/SchemaParser.java,"@@ -146,6 +148,17 @@ void refresh(Cluster cluster,
             }
             keyspaces.put(keyspace.getName(), keyspace);
         }
+        if(rows.virtualKeyspaces!=null) {","[{'comment': 'nit: space between `!=`', 'commenter': 'tolbertam'}]"
1050,driver-core/src/main/java/com/datastax/driver/core/SchemaParser.java,"@@ -735,4 +761,78 @@ private boolean references(DataType dataType, String typeName) {
             return false;
         }
     }
+
+    private static class V4SchemaParser extends V3SchemaParser {
+
+        private static final String SELECT_KEYSPACES = ""SELECT * FROM system_schema.keyspaces"";
+        private static final String SELECT_TABLES = ""SELECT * FROM system_schema.tables"";
+        private static final String SELECT_COLUMNS = ""SELECT * FROM system_schema.columns"";
+        private static final String SELECT_USERTYPES = ""SELECT * FROM system_schema.types"";
+        private static final String SELECT_FUNCTIONS = ""SELECT * FROM system_schema.functions"";
+        private static final String SELECT_AGGREGATES = ""SELECT * FROM system_schema.aggregates"";
+        private static final String SELECT_INDEXES = ""SELECT * FROM system_schema.indexes"";
+        private static final String SELECT_VIEWS = ""SELECT * FROM system_schema.views"";
+        private static final String SELECT_VIRTUAL_KEYSPACES = ""SELECT * FROM system_virtual_schema.keyspaces"";
+        private static final String SELECT_VIRTUAL_TABLES = ""SELECT * FROM system_virtual_schema.tables"";
+        private static final String SELECT_VIRTUAL_COLUMNS = ""SELECT * FROM system_virtual_schema.columns"";
+
+
+        private static final String TABLE_NAME = ""table_name"";
+
+        @Override
+        SystemRows fetchSystemRows(Cluster cluster, SchemaElement targetType, String targetKeyspace, String targetName, List<String> targetSignature, Connection connection, VersionNumber cassandraVersion)
+                throws ConnectionException, BusyConnectionException, ExecutionException, InterruptedException {
+
+            boolean isSchemaOrKeyspace = (targetType == null || targetType == KEYSPACE);
+
+            ResultSetFuture ksFuture = null,","[{'comment': ""It would be nice to not duplicate a lot of this code from V3Parser, but refactoring it so it could be reused would probably be as much code, so i'm fine with this as is."", 'commenter': 'tolbertam'}, {'comment': 'That was my thought as well.\r\n', 'commenter': 'GregBestland'}]"
1050,driver-core/src/main/java/com/datastax/driver/core/SchemaParser.java,"@@ -491,9 +504,13 @@ private static ResultSet get(ResultSetFuture future) throws InterruptedException
         final Map<String, List<Row>> aggregates;
         final Map<String, List<Row>> views;
         final Map<String, Map<String, List<Row>>> indexes;
+        final ResultSet virtualKeyspaces;
+        final Map<String, List<Row>> virtualTables;
+        final Map<String, Map<String, Map<String, ColumnMetadata.Raw>>> virtualcolumns;","[{'comment': 'Nit: `virtualcolumns` -> `virtualColumns`', 'commenter': 'tolbertam'}]"
1050,driver-core/src/main/java/com/datastax/driver/core/TableMetadata.java,"@@ -73,6 +73,10 @@ private TableMetadata(KeyspaceMetadata keyspace,
     static TableMetadata build(KeyspaceMetadata ksm, Row row, Map<String, ColumnMetadata.Raw> rawCols, List<Row> indexRows, String nameColumn, VersionNumber cassandraVersion, Cluster cluster) {
 
         String name = row.getString(nameColumn);
+        if(ksm.isVirtual()){
+            return  new TableMetadata(ksm, name, new UUID(0L, 0L), Collections.<ColumnMetadata>emptyList(), Collections.<ColumnMetadata>emptyList(),","[{'comment': 'This is currently not passing in columns, so it looks like all virtual tables have no columns.', 'commenter': 'tolbertam'}]"
1050,driver-core/src/main/java/com/datastax/driver/core/SchemaParser.java,"@@ -735,4 +761,78 @@ private boolean references(DataType dataType, String typeName) {
             return false;
         }
     }
+
+    private static class V4SchemaParser extends V3SchemaParser {
+
+        private static final String SELECT_KEYSPACES = ""SELECT * FROM system_schema.keyspaces"";
+        private static final String SELECT_TABLES = ""SELECT * FROM system_schema.tables"";
+        private static final String SELECT_COLUMNS = ""SELECT * FROM system_schema.columns"";
+        private static final String SELECT_USERTYPES = ""SELECT * FROM system_schema.types"";
+        private static final String SELECT_FUNCTIONS = ""SELECT * FROM system_schema.functions"";
+        private static final String SELECT_AGGREGATES = ""SELECT * FROM system_schema.aggregates"";
+        private static final String SELECT_INDEXES = ""SELECT * FROM system_schema.indexes"";
+        private static final String SELECT_VIEWS = ""SELECT * FROM system_schema.views"";","[{'comment': 'You could make these protected in `V3SchemaParser` instead of redefining them.', 'commenter': 'olim7t'}]"
1050,driver-core/src/main/java/com/datastax/driver/core/TableMetadata.java,"@@ -416,6 +423,9 @@ public String exportAsString() {
 ","[{'comment': 'Maybe we should add a `TableMetadata.isVirtual()`, even though it will just be a trivial shortcut to `getKeyspace().isVirtual()`.', 'commenter': 'olim7t'}]"
1050,driver-core/src/main/java/com/datastax/driver/core/TableMetadata.java,"@@ -73,6 +73,10 @@ private TableMetadata(KeyspaceMetadata keyspace,
     static TableMetadata build(KeyspaceMetadata ksm, Row row, Map<String, ColumnMetadata.Raw> rawCols, List<Row> indexRows, String nameColumn, VersionNumber cassandraVersion, Cluster cluster) {
 
         String name = row.getString(nameColumn);
+        if(ksm.isVirtual()){
+            return  new TableMetadata(ksm, name, new UUID(0L, 0L), Collections.<ColumnMetadata>emptyList(), Collections.<ColumnMetadata>emptyList(),
+                    Collections.<String, ColumnMetadata>emptyMap(), Collections.<String, IndexMetadata>emptyMap(), null, Collections.<ClusteringOrder>emptyList(), cassandraVersion);
+        }","[{'comment': ""So a virtual table's metadata never has columns? Shouldn't we expose the contents of `system_virtual_schema.columns` here?"", 'commenter': 'olim7t'}]"
1050,driver-core/src/test/java/com/datastax/driver/core/VirtualTableMetadataTest.java,"@@ -0,0 +1,59 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.driver.core;
+
+import com.datastax.driver.core.utils.CassandraVersion;
+import org.testng.annotations.Test;
+
+import java.util.UUID;
+
+import static com.datastax.driver.core.Assertions.assertThat;
+
+@CassandraVersion(""4.0.0"")
+@CCMConfig()
+public class VirtualTableMetadataTest extends CCMTestsSupport {
+
+    @Test(groups = ""short"")
+    public void should_parse_virtual_metadata() {","[{'comment': 'Looks good, just a few suggestions:\r\n\r\n1. Inconsistent spacing between operators in some cases (i.e. no space between `=`, `>=` in some places.\r\n2. Would be good to add comments inline for each type validated to indicate why their values are that way, i.e.:\r\n\r\n    ``` \r\n    // Keyspace name should be set, marked as virtual, and have a clients table.\r\n    // All other values should be defaulted since they are not defined in schema tables.\r\n    ```', 'commenter': 'tolbertam'}]"
1050,driver-core/src/main/java/com/datastax/driver/core/AbstractTableMetadata.java,"@@ -199,6 +199,15 @@ public TableOptionsMetadata getOptions() {
         return options;
     }
 
+    /**
+     * Returns whether or not this keyspace is a virtual keyspace
+     * @return {@code true} if virtual keyspace","[{'comment': 'Looks like `default )` was accidentally put here.', 'commenter': 'tolbertam'}]"
1050,driver-core/src/main/java/com/datastax/driver/core/KeyspaceMetadata.java,"@@ -47,10 +48,15 @@
 
     @VisibleForTesting
     KeyspaceMetadata(String name, boolean durableWrites, Map<String, String> replication) {
+        this(name,durableWrites,replication,false);","[{'comment': 'nit: spaces between commas.', 'commenter': 'tolbertam'}]"
1050,driver-core/src/main/java/com/datastax/driver/core/KeyspaceMetadata.java,"@@ -47,10 +48,15 @@
 
     @VisibleForTesting
     KeyspaceMetadata(String name, boolean durableWrites, Map<String, String> replication) {","[{'comment': 'Should mark this constructor as deprecated.', 'commenter': 'tolbertam'}]"
1050,driver-core/src/main/java/com/datastax/driver/core/SchemaParser.java,"@@ -491,9 +504,13 @@ private static ResultSet get(ResultSetFuture future) throws InterruptedException
         final Map<String, List<Row>> aggregates;
         final Map<String, List<Row>> views;
         final Map<String, Map<String, List<Row>>> indexes;
+        final ResultSet virtualKeyspaces;
+        final Map<String, List<Row>> virtualTables;
+        final Map<String, Map<String, Map<String, ColumnMetadata.Raw>>> virtualColumns;
 
         public SystemRows(ResultSet keyspaces, Map<String, List<Row>> tables, Map<String, Map<String, Map<String, ColumnMetadata.Raw>>> columns, Map<String, List<Row>> udts, Map<String, List<Row>> functions,
-                          Map<String, List<Row>> aggregates, Map<String, List<Row>> views, Map<String, Map<String, List<Row>>> indexes) {
+                          Map<String, List<Row>> aggregates, Map<String, List<Row>> views, Map<String, Map<String, List<Row>>> indexes, ResultSet virtualKeyspaces, Map<String, List<Row>> virtualTables,
+                          Map<String, Map<String, Map<String, ColumnMetadata.Raw>>> virtualcolumns) {","[{'comment': 'nit: I see `virtualcolumns` was renamed `virtualColumns` for the class member, but the constructor parameter is still `virtualcolumns`.', 'commenter': 'tolbertam'}]"
1050,driver-core/src/main/java/com/datastax/driver/core/SchemaParser.java,"@@ -571,7 +591,10 @@ else if (targetType == AGGREGATE)
                     groupByKeyspace(get(aggregatesFuture)),
                     // No views nor separate indexes table in Cassandra 2:
                     Collections.<String, List<Row>>emptyMap(),
-                    Collections.<String, Map<String, List<Row>>>emptyMap());
+                    Collections.<String, Map<String, List<Row>>>emptyMap(),
+                   null,","[{'comment': 'nit: should be indented another space to match the other arguments.', 'commenter': 'tolbertam'}]"
1050,driver-core/src/main/java/com/datastax/driver/core/SchemaParser.java,"@@ -735,4 +761,70 @@ private boolean references(DataType dataType, String typeName) {
             return false;
         }
     }
+
+    private static class V4SchemaParser extends V3SchemaParser {
+
+        private static final String SELECT_VIRTUAL_KEYSPACES = ""SELECT * FROM system_virtual_schema.keyspaces"";
+        private static final String SELECT_VIRTUAL_TABLES = ""SELECT * FROM system_virtual_schema.tables"";
+        private static final String SELECT_VIRTUAL_COLUMNS = ""SELECT * FROM system_virtual_schema.columns"";
+
+
+        private static final String TABLE_NAME = ""table_name"";
+
+        @Override
+        SystemRows fetchSystemRows(Cluster cluster, SchemaElement targetType, String targetKeyspace, String targetName, List<String> targetSignature, Connection connection, VersionNumber cassandraVersion)
+                throws ConnectionException, BusyConnectionException, ExecutionException, InterruptedException {
+
+            boolean isSchemaOrKeyspace = (targetType == null || targetType == KEYSPACE);
+
+            ResultSetFuture ksFuture = null,
+                    udtFuture = null,
+                    cfFuture = null,
+                    colsFuture = null,
+                    functionsFuture = null,
+                    aggregatesFuture = null,
+                    indexesFuture = null,
+                    viewsFuture = null,
+                    virtualKeyspacesFuture = null,
+                    virtualTableFuture = null,
+                    virtualColumnsFuture = null;
+
+            ProtocolVersion protocolVersion = cluster.getConfiguration().getProtocolOptions().getProtocolVersion();
+
+            if (isSchemaOrKeyspace)
+                ksFuture = queryAsync(SELECT_KEYSPACES + whereClause(targetType, targetKeyspace, targetName, targetSignature), connection, protocolVersion);
+                virtualKeyspacesFuture = queryAsync(SELECT_VIRTUAL_KEYSPACES + whereClause(targetType, targetKeyspace, targetName, targetSignature), connection, protocolVersion);
+                virtualColumnsFuture = queryAsync(SELECT_VIRTUAL_COLUMNS + whereClause(targetType, targetKeyspace, targetName, targetSignature), connection, protocolVersion);
+                virtualTableFuture = queryAsync(SELECT_VIRTUAL_TABLES + whereClause(targetType, targetKeyspace, targetName, targetSignature), connection, protocolVersion);","[{'comment': 'You need curly braces around this block, currently only the first statement is executed conditionally.', 'commenter': 'olim7t'}, {'comment': 'Good catch', 'commenter': 'GregBestland'}]"
1050,driver-core/src/main/java/com/datastax/driver/core/AbstractTableMetadata.java,"@@ -199,6 +199,14 @@ public TableOptionsMetadata getOptions() {
         return options;
     }
 
+    /**
+     * Returns whether or not this keyspace is a virtual keyspace
+     * @return {@code true} if virtual keyspace, {@code false} otherwise.
+     */
+    public boolean isVirtual(){","[{'comment': 'Nit: the javadoc should say table,  not keyspace.', 'commenter': 'olim7t'}]"
1050,driver-core/src/test/java/com/datastax/driver/core/VirtualTableMetadataTest.java,"@@ -0,0 +1,69 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.driver.core;
+
+import com.datastax.driver.core.utils.CassandraVersion;
+import org.testng.annotations.Test;
+
+import java.util.UUID;
+
+import static com.datastax.driver.core.Assertions.assertThat;
+
+@CassandraVersion(""4.0.0"")
+@CCMConfig()
+public class VirtualTableMetadataTest extends CCMTestsSupport {
+
+    @Test(groups = ""short"")
+    public void should_parse_virtual_metadata() {
+        KeyspaceMetadata km=session().getCluster().getMetadata().getKeyspace(""system_views"");
+        // Keyspace name should be set, marked as virtual, and have a clients table.
+        // All other values should be defaulted since they are not defined in the virtual schema tables.
+        assertThat(km.getTables().size() >= 2);
+        assertThat(km.isVirtual()).isTrue();
+        assertThat(km.getTable(""clients"")).isNotNull();","[{'comment': 'Nit: this is duplicated at line 48, I would get rid of this line.', 'commenter': 'olim7t'}]"
1050,driver-core/src/main/java/com/datastax/driver/core/AbstractTableMetadata.java,"@@ -245,6 +253,9 @@ public String asCQLQuery() {
 
     protected StringBuilder appendOptions(StringBuilder sb, boolean formatted) {
         // Options
+        if(options==null){","[{'comment': 'In several places the formatting rules are not respected, please run a global format on this PR before merging, applying the rules for driver 3.x [here](https://github.com/datastax/java-driver/blob/3.x/CONTRIBUTING.md#editor-configuration).', 'commenter': 'adutra'}]"
1050,driver-core/src/main/java/com/datastax/driver/core/TableMetadata.java,"@@ -417,15 +434,20 @@ public String exportAsString() {
     @Override
     protected String asCQLQuery(boolean formatted) {
         StringBuilder sb = new StringBuilder();
-        sb.append(""CREATE TABLE "").append(Metadata.quoteIfNecessary(keyspace.getName())).append('.').append(Metadata.quoteIfNecessary(name)).append("" ("");
+        if(isVirtual()){
+            sb.append(""/* VIRTUAL "");","[{'comment': 'I know this is extremely unlikely but we could theoretically produce invalid CQL here if the table contains weird column names as in ` create table t1 (""pk */"" int primary key)`; this could also happen if some table options contain such forbidden sequences, for example in the `comments` field.\r\nOne simple idea to fix this would be to use single-line comments (`--` or `//`) and force `formatted` to false.\r\nEDIT: nevermind, with virtual tables this should never happen. OK to ignore this comment.', 'commenter': 'adutra'}]"
1054,core/src/main/java/com/datastax/oss/driver/api/core/metadata/schema/TableMetadata.java,"@@ -32,6 +32,10 @@
   @NonNull
   @Override
   default String describe(boolean pretty) {
+    // this is a virtual table, do not return a describe string
+    if (getPrimaryKey().size() == 0) {
+      return """";
+    }","[{'comment': ""We should have an `isVirtual()` method on `TableMetadata` as well.\r\n\r\nAnd, even though the PK size is probably a fairly reliable indicator, it would be cleaner to store the information explicitly, as a `virtual` field that gets passed to `DefaultTableMetadata`'s constructor. It will be easy to set since you already have a separate method to build virtual tables in `TableParser`."", 'commenter': 'olim7t'}]"
1054,core/src/main/java/com/datastax/oss/driver/internal/core/metadata/schema/parsing/TableParser.java,"@@ -226,6 +226,50 @@ TableMetadata parseTable(
         indexesBuilder.build());
   }
 
+  TableMetadata parseVirtualTable(
+      AdminRow tableRow, CqlIdentifier keyspaceId, Map<CqlIdentifier, UserDefinedType> userTypes) {
+
+    CqlIdentifier tableId =
+        CqlIdentifier.fromInternal(
+            tableRow.getString(rows.isCassandraV3 ? ""table_name"" : ""columnfamily_name""));","[{'comment': 'Nit: virtual tables are 4.0+ so the column will always be `table_name`.', 'commenter': 'olim7t'}]"
1054,core/src/main/java/com/datastax/oss/driver/internal/core/metadata/schema/parsing/TableParser.java,"@@ -226,6 +226,50 @@ TableMetadata parseTable(
         indexesBuilder.build());
   }
 
+  TableMetadata parseVirtualTable(
+      AdminRow tableRow, CqlIdentifier keyspaceId, Map<CqlIdentifier, UserDefinedType> userTypes) {
+
+    CqlIdentifier tableId =
+        CqlIdentifier.fromInternal(
+            tableRow.getString(rows.isCassandraV3 ? ""table_name"" : ""columnfamily_name""));
+
+    List<RawColumn> rawColumns =
+        RawColumn.toRawColumns(
+            rows.virtualColumns.getOrDefault(keyspaceId, ImmutableMultimap.of()).get(tableId),
+            keyspaceId,
+            userTypes);
+    if (rawColumns.isEmpty()) {
+      LOG.warn(
+          ""[{}] Processing TABLE refresh for {}.{} but found no matching rows, skipping"",
+          logPrefix,
+          keyspaceId,
+          tableId);
+      return null;
+    }
+
+    Collections.sort(rawColumns);
+    ImmutableMap.Builder<CqlIdentifier, ColumnMetadata> allColumnsBuilder = ImmutableMap.builder();
+
+    for (RawColumn raw : rawColumns) {
+      DataType dataType = dataTypeParser.parse(keyspaceId, raw.dataType, userTypes, context);
+      ColumnMetadata column =
+          new DefaultColumnMetadata(
+              keyspaceId, tableId, raw.name, dataType, raw.kind == RawColumn.Kind.STATIC);
+      allColumnsBuilder.put(column.getName(), column);
+    }
+
+    return new DefaultTableMetadata(
+        keyspaceId,
+        tableId,
+        new UUID(0L, 0L),","[{'comment': 'See my top-level comment on the PR, maybe it would be better to expose this as an `Optional`.', 'commenter': 'olim7t'}]"
1054,core/src/main/java/com/datastax/oss/driver/internal/core/metadata/schema/parsing/TableParser.java,"@@ -226,6 +226,50 @@ TableMetadata parseTable(
         indexesBuilder.build());
   }
 
+  TableMetadata parseVirtualTable(","[{'comment': ""ðŸ‘ for exposing this as a separate method, that leads to a bit of duplication but it's easier to follow."", 'commenter': 'olim7t'}]"
1054,core/src/main/java/com/datastax/oss/driver/api/core/metadata/schema/KeyspaceMetadata.java,"@@ -34,6 +34,9 @@
   /** Whether durable writes are set on this keyspace. */
   boolean isDurableWrites();
 
+  /** Whether durable writes are set on this keyspace. */","[{'comment': 'Wrong comment.', 'commenter': 'adutra'}]"
1054,core/src/main/java/com/datastax/oss/driver/internal/core/metadata/schema/parsing/SchemaParser.java,"@@ -73,15 +74,19 @@ public SchemaParser(SchemaRows rows, InternalDriverContext context) {
   public SchemaRefresh parse() {
     ImmutableMap.Builder<CqlIdentifier, KeyspaceMetadata> keyspacesBuilder = ImmutableMap.builder();
     for (AdminRow row : rows.keyspaces) {
-      KeyspaceMetadata keyspace = parseKeyspace(row);
+      KeyspaceMetadata keyspace = parseKeyspace(row, false);
+      keyspacesBuilder.put(keyspace.getName(), keyspace);
+    }
+    for (AdminRow row : rows.virtualKeyspaces) {
+      KeyspaceMetadata keyspace = parseKeyspace(row, true);
       keyspacesBuilder.put(keyspace.getName(), keyspace);
     }
     SchemaRefresh refresh = new SchemaRefresh(keyspacesBuilder.build());
     LOG.debug(""[{}] Schema parsing took {}"", logPrefix, NanoTime.formatTimeSince(startTimeNs));
     return refresh;
   }
 
-  protected KeyspaceMetadata parseKeyspace(AdminRow keyspaceRow) {
+  protected KeyspaceMetadata parseKeyspace(AdminRow keyspaceRow, boolean isVirtual) {","[{'comment': 'Nit: the convention would be to name this boolean simply `virtual`.', 'commenter': 'adutra'}]"
1054,core/src/main/java/com/datastax/oss/driver/internal/core/metadata/schema/DefaultTableMetadata.java,"@@ -75,15 +75,23 @@ public CqlIdentifier getName() {
 
   @NonNull
   @Override
-  public UUID getId() {
-    return id;
+  public Optional<UUID> getId() {
+    if (id == null) {","[{'comment': 'This can be simplified to `Optional.ofNullable(id)`.', 'commenter': 'adutra'}, {'comment': ""Also, you'll want to mark the `id` field as `Nullable`, currently this generates a warning."", 'commenter': 'olim7t'}]"
1054,core/src/main/java/com/datastax/oss/driver/internal/core/metadata/schema/DefaultViewMetadata.java,"@@ -81,8 +81,8 @@ public CqlIdentifier getName() {
 
   @NonNull
   @Override
-  public UUID getId() {
-    return id;
+  public Optional<UUID> getId() {
+    return Optional.of(id);","[{'comment': ""If `id` can be null than this should rather be `Optional.ofNullable(id)`. (But I'm not sure if views can be virtual, can they?)"", 'commenter': 'adutra'}, {'comment': ""I don't believe that views can be virtual at this time, just keyspaces and tables"", 'commenter': 'GregBestland'}]"
1060,core-shaded/pom.xml,"@@ -28,97 +28,197 @@
   </parent>
 
   <artifactId>java-driver-core-shaded</artifactId>
-  <packaging>bundle</packaging>
 
-  <name>DataStax Java driver for Apache Cassandra(R) - core with netty shaded</name>
+  <name>DataStax Java driver for Apache Cassandra(R) - core with shaded deps</name>","[{'comment': ""I'm choosing a more generic name in case we add more shaded dependencies later."", 'commenter': 'adutra'}]"
1060,core-shaded/pom.xml,"@@ -28,97 +28,197 @@
   </parent>
 
   <artifactId>java-driver-core-shaded</artifactId>
-  <packaging>bundle</packaging>
 
-  <name>DataStax Java driver for Apache Cassandra(R) - core with netty shaded</name>
+  <name>DataStax Java driver for Apache Cassandra(R) - core with shaded deps</name>
 
   <dependencies>
     <dependency>
       <groupId>com.datastax.oss</groupId>
       <artifactId>java-driver-core</artifactId>
       <version>${project.version}</version>
     </dependency>
+    <!--
+    Repeat optional dependencies of the driver here because the shade plugin","[{'comment': 'This is a bit unfortunate but the shade plugin ""forgets"" to promote optional dependencies.', 'commenter': 'adutra'}]"
1060,core-shaded/pom.xml,"@@ -28,97 +28,197 @@
   </parent>
 
   <artifactId>java-driver-core-shaded</artifactId>
-  <packaging>bundle</packaging>
 
-  <name>DataStax Java driver for Apache Cassandra(R) - core with netty shaded</name>
+  <name>DataStax Java driver for Apache Cassandra(R) - core with shaded deps</name>
 
   <dependencies>
     <dependency>
       <groupId>com.datastax.oss</groupId>
       <artifactId>java-driver-core</artifactId>
       <version>${project.version}</version>
     </dependency>
+    <!--
+    Repeat optional dependencies of the driver here because the shade plugin
+    does not automatically promote them to top level dependencies.
+    -->
+    <dependency>
+      <groupId>org.xerial.snappy</groupId>
+      <artifactId>snappy-java</artifactId>
+      <optional>true</optional>
+    </dependency>
+    <dependency>
+      <groupId>org.lz4</groupId>
+      <artifactId>lz4-java</artifactId>
+      <optional>true</optional>
+    </dependency>
   </dependencies>
 
+  <!--
+  Generation of the shaded driver-core bundle during package phase:
+  1) shade plugin shades the driver and creates a shaded jar + source jar
+  2) dependency plugin unpacks the shaded jar to target/classes (and removes unwanted content)
+  3) bundle plugin analyzes shaded classes and generates the bundle manifest
+  4) assembly plugin re-creates the shaded jar by packing target/classes + manifest + shaded pom
+  -->
+
   <build>
     <plugins>
-      <!-- extract shaded manifest from core jar -->
+      <plugin>
+        <artifactId>maven-shade-plugin</artifactId>
+        <executions>
+          <execution>
+            <phase>package</phase>
+            <goals>
+              <goal>shade</goal>
+            </goals>
+            <configuration>
+              <createSourcesJar>true</createSourcesJar>
+              <shadeSourcesContent>true</shadeSourcesContent>
+              <artifactSet>
+                <includes>
+                  <!--
+                  Include:
+                  - The core driver itself; it is not relocated but needs to be included.
+                  - All the dependencies we want to shade & relocate: currently, all the Netty
+                  artifacts.
+                  -->
+                  <include>com.datastax.oss:java-driver-core</include>
+                  <include>io.netty:*</include>
+                </includes>
+              </artifactSet>
+              <relocations>
+                <relocation>
+                  <pattern>io.netty</pattern>
+                  <shadedPattern>com.datastax.oss.driver.shaded.netty</shadedPattern>
+                </relocation>
+              </relocations>
+              <!--
+              Promote all other transitive dependencies of java-driver-core other than Netty
+              to direct dependencies in the shaded pom.
+              -->
+              <promoteTransitiveDependencies>true</promoteTransitiveDependencies>
+            </configuration>
+          </execution>
+        </executions>
+      </plugin>
       <plugin>
         <artifactId>maven-dependency-plugin</artifactId>
         <executions>
           <execution>
-            <id>unpack-manifest</id>
-            <phase>prepare-package</phase>
+            <phase>package</phase>
             <goals>
               <goal>unpack</goal>
             </goals>
             <configuration>
               <artifactItems>
                 <artifactItem>
                   <groupId>com.datastax.oss</groupId>
-                  <artifactId>java-driver-core</artifactId>
+                  <artifactId>java-driver-core-shaded</artifactId>
                   <version>${project.version}</version>
-                  <includes>META-INF-shaded/MANIFEST.MF</includes>
-                  <outputDirectory>${project.build.directory}</outputDirectory>
+                  <type>jar</type>
+                  <outputDirectory>${project.build.outputDirectory}</outputDirectory>
                 </artifactItem>
               </artifactItems>
+              <!--
+              Exclude leftovers from the shading phase (this could also be done with a
+              resource transformer by the shade plugin itself, but this way is more flexible).
+              Note: Netty ships with an embedded JCTools, that's why we need to exclude that too.
+              -->
+              <excludes>
+                META-INF/maven/com.datastax.oss/java-driver-core/**,
+                META-INF/maven/io.netty/**,
+                META-INF/maven/org.jctools/**
+              </excludes>
             </configuration>
           </execution>
         </executions>
       </plugin>
       <plugin>
-        <artifactId>maven-shade-plugin</artifactId>
+        <groupId>org.apache.felix</groupId>
+        <artifactId>maven-bundle-plugin</artifactId>
+        <extensions>true</extensions>
         <executions>
           <execution>
             <phase>package</phase>
             <goals>
-              <goal>shade</goal>
+              <goal>manifest</goal>
             </goals>
             <configuration>
-              <artifactSet>
-                <includes>
-                  <include>com.datastax.oss:java-driver-core</include>
-                  <include>io.netty:*</include>
-                </includes>
-              </artifactSet>
-              <relocations>
-                <relocation>
-                  <pattern>io.netty</pattern>
-                  <shadedPattern>com.datastax.oss.driver.shaded.netty</shadedPattern>
-                </relocation>
-              </relocations>
-              <transformers>
-                <transformer implementation=""org.apache.maven.plugins.shade.resource.DontIncludeResourceTransformer"">
-                  <resources>
-                    <resource>META-INF/MANIFEST.MF</resource>
-                    <resource>META-INF-shaded/MANIFEST.MF</resource>
-                    <resource>META-INF/maven/com.datastax.oss/java-driver-core/pom.xml</resource>
-                    <resource>META-INF/maven/com.datastax.oss/java-driver-core/pom.properties</resource>
-                    <resource>META-INF/maven/io.netty/netty-handler/pom.properties</resource>
-                    <resource>META-INF/maven/io.netty/netty-handler/pom.xml</resource>
-                    <resource>META-INF/maven/io.netty/netty-buffer/pom.properties</resource>
-                    <resource>META-INF/maven/io.netty/netty-buffer/pom.xml</resource>
-                    <resource>META-INF/maven/io.netty/netty-common/pom.properties</resource>
-                    <resource>META-INF/maven/io.netty/netty-common/pom.xml</resource>
-                    <resource>META-INF/maven/io.netty/netty-transport/pom.properties</resource>
-                    <resource>META-INF/maven/io.netty/netty-transport/pom.xml</resource>
-                    <resource>META-INF/maven/io.netty/netty-resolver/pom.properties</resource>
-                    <resource>META-INF/maven/io.netty/netty-resolver/pom.xml</resource>
-                    <resource>META-INF/maven/io.netty/netty-codec/pom.properties</resource>
-                    <resource>META-INF/maven/io.netty/netty-codec/pom.xml</resource>
-                    <!-- netty's shading of jctools does not remove its pom files -->
-                    <resource>META-INF/maven/org.jctools/jctools-core/pom.properties</resource>
-                    <resource>META-INF/maven/org.jctools/jctools-core/pom.xml</resource>
-                  </resources>
-                </transformer>
-                <!-- Pick up the alternate manifest that was extracted from the core module -->
-                <transformer implementation=""org.apache.maven.plugins.shade.resource.IncludeResourceTransformer"">
-                  <resource>META-INF/MANIFEST.MF</resource>
-                  <file>${project.build.directory}/META-INF-shaded/MANIFEST.MF</file>
-                </transformer>
-              </transformers>
-              <!-- Keep the dependencies of driver-core -->
-              <promoteTransitiveDependencies>true</promoteTransitiveDependencies>
+              <instructions>
+                <Bundle-SymbolicName>com.datastax.oss.driver.core</Bundle-SymbolicName>
+                <DynamicImport-Package>*</DynamicImport-Package>
+                <!--
+                Don't include because they aren't OSGi bundles, and the driver bundle can
+                live without them:
+                 - jnr
+                 - net.jcip.annotations
+                 - edu.umd.cs.findbugs.annotations
+                Don't include because they are imported by shaded Netty classes
+                but not used by the driver bundle:
+                 - com.google.protobuf","[{'comment': 'Because Netty has been shaded, BND now detects a few package imports of Netty dependencies that would otherwise appear only inside Netty bundles, not here. These are mostly compression and logging libraries. None of them is used by the driver so I just removed them.', 'commenter': 'adutra'}]"
1060,core-shaded/pom.xml,"@@ -28,97 +28,197 @@
   </parent>
 
   <artifactId>java-driver-core-shaded</artifactId>
-  <packaging>bundle</packaging>
 
-  <name>DataStax Java driver for Apache Cassandra(R) - core with netty shaded</name>
+  <name>DataStax Java driver for Apache Cassandra(R) - core with shaded deps</name>
 
   <dependencies>
     <dependency>
       <groupId>com.datastax.oss</groupId>
       <artifactId>java-driver-core</artifactId>
       <version>${project.version}</version>
     </dependency>
+    <!--
+    Repeat optional dependencies of the driver here because the shade plugin
+    does not automatically promote them to top level dependencies.
+    -->
+    <dependency>
+      <groupId>org.xerial.snappy</groupId>
+      <artifactId>snappy-java</artifactId>
+      <optional>true</optional>
+    </dependency>
+    <dependency>
+      <groupId>org.lz4</groupId>
+      <artifactId>lz4-java</artifactId>
+      <optional>true</optional>
+    </dependency>
   </dependencies>
 
+  <!--
+  Generation of the shaded driver-core bundle during package phase:
+  1) shade plugin shades the driver and creates a shaded jar + source jar
+  2) dependency plugin unpacks the shaded jar to target/classes (and removes unwanted content)
+  3) bundle plugin analyzes shaded classes and generates the bundle manifest
+  4) assembly plugin re-creates the shaded jar by packing target/classes + manifest + shaded pom
+  -->
+
   <build>
     <plugins>
-      <!-- extract shaded manifest from core jar -->
+      <plugin>
+        <artifactId>maven-shade-plugin</artifactId>
+        <executions>
+          <execution>
+            <phase>package</phase>
+            <goals>
+              <goal>shade</goal>
+            </goals>
+            <configuration>
+              <createSourcesJar>true</createSourcesJar>
+              <shadeSourcesContent>true</shadeSourcesContent>
+              <artifactSet>
+                <includes>
+                  <!--
+                  Include:
+                  - The core driver itself; it is not relocated but needs to be included.
+                  - All the dependencies we want to shade & relocate: currently, all the Netty
+                  artifacts.
+                  -->
+                  <include>com.datastax.oss:java-driver-core</include>
+                  <include>io.netty:*</include>
+                </includes>
+              </artifactSet>
+              <relocations>
+                <relocation>
+                  <pattern>io.netty</pattern>
+                  <shadedPattern>com.datastax.oss.driver.shaded.netty</shadedPattern>
+                </relocation>
+              </relocations>
+              <!--
+              Promote all other transitive dependencies of java-driver-core other than Netty
+              to direct dependencies in the shaded pom.
+              -->
+              <promoteTransitiveDependencies>true</promoteTransitiveDependencies>
+            </configuration>
+          </execution>
+        </executions>
+      </plugin>
       <plugin>
         <artifactId>maven-dependency-plugin</artifactId>
         <executions>
           <execution>
-            <id>unpack-manifest</id>
-            <phase>prepare-package</phase>
+            <phase>package</phase>
             <goals>
               <goal>unpack</goal>
             </goals>
             <configuration>
               <artifactItems>
                 <artifactItem>
                   <groupId>com.datastax.oss</groupId>
-                  <artifactId>java-driver-core</artifactId>
+                  <artifactId>java-driver-core-shaded</artifactId>
                   <version>${project.version}</version>
-                  <includes>META-INF-shaded/MANIFEST.MF</includes>
-                  <outputDirectory>${project.build.directory}</outputDirectory>
+                  <type>jar</type>
+                  <outputDirectory>${project.build.outputDirectory}</outputDirectory>
                 </artifactItem>
               </artifactItems>
+              <!--
+              Exclude leftovers from the shading phase (this could also be done with a
+              resource transformer by the shade plugin itself, but this way is more flexible).
+              Note: Netty ships with an embedded JCTools, that's why we need to exclude that too.
+              -->
+              <excludes>
+                META-INF/maven/com.datastax.oss/java-driver-core/**,
+                META-INF/maven/io.netty/**,
+                META-INF/maven/org.jctools/**
+              </excludes>
             </configuration>
           </execution>
         </executions>
       </plugin>
       <plugin>
-        <artifactId>maven-shade-plugin</artifactId>
+        <groupId>org.apache.felix</groupId>
+        <artifactId>maven-bundle-plugin</artifactId>
+        <extensions>true</extensions>
         <executions>
           <execution>
             <phase>package</phase>
             <goals>
-              <goal>shade</goal>
+              <goal>manifest</goal>
             </goals>
             <configuration>
-              <artifactSet>
-                <includes>
-                  <include>com.datastax.oss:java-driver-core</include>
-                  <include>io.netty:*</include>
-                </includes>
-              </artifactSet>
-              <relocations>
-                <relocation>
-                  <pattern>io.netty</pattern>
-                  <shadedPattern>com.datastax.oss.driver.shaded.netty</shadedPattern>
-                </relocation>
-              </relocations>
-              <transformers>
-                <transformer implementation=""org.apache.maven.plugins.shade.resource.DontIncludeResourceTransformer"">
-                  <resources>
-                    <resource>META-INF/MANIFEST.MF</resource>
-                    <resource>META-INF-shaded/MANIFEST.MF</resource>
-                    <resource>META-INF/maven/com.datastax.oss/java-driver-core/pom.xml</resource>
-                    <resource>META-INF/maven/com.datastax.oss/java-driver-core/pom.properties</resource>
-                    <resource>META-INF/maven/io.netty/netty-handler/pom.properties</resource>
-                    <resource>META-INF/maven/io.netty/netty-handler/pom.xml</resource>
-                    <resource>META-INF/maven/io.netty/netty-buffer/pom.properties</resource>
-                    <resource>META-INF/maven/io.netty/netty-buffer/pom.xml</resource>
-                    <resource>META-INF/maven/io.netty/netty-common/pom.properties</resource>
-                    <resource>META-INF/maven/io.netty/netty-common/pom.xml</resource>
-                    <resource>META-INF/maven/io.netty/netty-transport/pom.properties</resource>
-                    <resource>META-INF/maven/io.netty/netty-transport/pom.xml</resource>
-                    <resource>META-INF/maven/io.netty/netty-resolver/pom.properties</resource>
-                    <resource>META-INF/maven/io.netty/netty-resolver/pom.xml</resource>
-                    <resource>META-INF/maven/io.netty/netty-codec/pom.properties</resource>
-                    <resource>META-INF/maven/io.netty/netty-codec/pom.xml</resource>
-                    <!-- netty's shading of jctools does not remove its pom files -->
-                    <resource>META-INF/maven/org.jctools/jctools-core/pom.properties</resource>
-                    <resource>META-INF/maven/org.jctools/jctools-core/pom.xml</resource>
-                  </resources>
-                </transformer>
-                <!-- Pick up the alternate manifest that was extracted from the core module -->
-                <transformer implementation=""org.apache.maven.plugins.shade.resource.IncludeResourceTransformer"">
-                  <resource>META-INF/MANIFEST.MF</resource>
-                  <file>${project.build.directory}/META-INF-shaded/MANIFEST.MF</file>
-                </transformer>
-              </transformers>
-              <!-- Keep the dependencies of driver-core -->
-              <promoteTransitiveDependencies>true</promoteTransitiveDependencies>
+              <instructions>
+                <Bundle-SymbolicName>com.datastax.oss.driver.core</Bundle-SymbolicName>
+                <DynamicImport-Package>*</DynamicImport-Package>
+                <!--
+                Don't include because they aren't OSGi bundles, and the driver bundle can
+                live without them:
+                 - jnr
+                 - net.jcip.annotations
+                 - edu.umd.cs.findbugs.annotations
+                Don't include because they are imported by shaded Netty classes
+                but not used by the driver bundle:
+                 - com.google.protobuf
+                 - com.jcraft.jzlib
+                 - com.ning.compress
+                 - lzma.sdk
+                 - net.jpountz.xxhash
+                 - org.bouncycastle
+                 - org.conscrypt
+                 - org.apache.commons.logging
+                 - org.apache.log4j
+                 - org.apache.logging.log4j
+                 - org.eclipse.jetty
+                 - org.jboss.marshalling
+                 - sun.misc
+                 - sun.security
+                -->
+                <Import-Package>
+                  !com.datastax.oss.driver.shaded.*,
+                  !jnr.*,
+                  !net.jcip.annotations.*,
+                  !edu.umd.cs.findbugs.annotations.*,
+                  !com.google.protobuf.*,
+                  !com.jcraft.jzlib.*,
+                  !com.ning.compress.*,
+                  !lzma.sdk.*,
+                  !net.jpountz.xxhash.*,
+                  !org.bouncycastle.*,
+                  !org.conscrypt.*,
+                  !org.apache.commons.logging.*,
+                  !org.apache.log4j.*,
+                  !org.apache.logging.log4j.*,
+                  !org.eclipse.jetty.*,
+                  !org.jboss.marshalling.*,
+                  !sun.misc.*,
+                  !sun.security.*,
+                  *
+                </Import-Package>
+                <Export-Package>
+                  com.datastax.oss.driver.*.core.*,","[{'comment': ""Note that we need to export not only API classes, but also internal driver classes and shaded classes (one reason being that the DSE driver needs them). I think it's a bit unfortunate to expose so many packages, but we don't have the choice."", 'commenter': 'adutra'}]"
1060,core/pom.xml,"@@ -27,6 +27,7 @@
   </parent>
 
   <artifactId>java-driver-core</artifactId>
+  <packaging>bundle</packaging>","[{'comment': 'We can now use packaging bundle here, since there is no more shading involved.', 'commenter': 'adutra'}]"
1060,core/pom.xml,"@@ -170,117 +166,38 @@
           </properties>
         </configuration>
       </plugin>
-      <!--
-      We avoid packaging bundle because it does not play nicely with the shade plugin, see
-      https://stackoverflow.com/questions/31262032/maven-shade-plugin-and-custom-packaging-type
-      -->
       <plugin>
         <groupId>org.apache.felix</groupId>
         <artifactId>maven-bundle-plugin</artifactId>
-        <configuration>
-          <instructions>
-            <Bundle-SymbolicName>com.datastax.oss.driver.core</Bundle-SymbolicName>
-          </instructions>
-          <!--
-          Prevent customized manifest entries from the project's maven-jar-plugin configuration from being read, see
-          http://apache-felix.18485.x6.nabble.com/how-lt-manifestLocation-gt-is-used-in-maven-bundle-plugin-td4835566.html
-          -->
-          <archive>
-            <forced>true</forced>
-          </archive>
-        </configuration>
+        <extensions>true</extensions>
         <executions>
           <execution>
-            <id>bundle-manifest</id>
-            <phase>process-classes</phase>
             <goals>
-              <goal>manifest</goal>
+              <goal>bundle</goal>
             </goals>
             <configuration>
-              <manifestLocation>${project.build.outputDirectory}/META-INF</manifestLocation>
               <instructions>
+                <Bundle-SymbolicName>com.datastax.oss.driver.core</Bundle-SymbolicName>
                 <!-- Allow importing code from other packages (so reflection-based loading of policies works) -->
                 <DynamicImport-Package>*</DynamicImport-Package>
-                <!-- Don't include JNR packages since some JNR modules are not OSGi bundles,
-                     jcip because its not an OSGi bundle, jctools because its shaded.
-                     Import sun.misc as jctools does this. -->
+                <!--
+                Don't include because they aren't OSGi bundles, and the driver can live without them:
+                 - jnr
+                 - net.jcip.annotations
+                 - edu.umd.cs.findbugs.annotations
+                -->
                 <Import-Package>
-                  !net.jcip.annotations,!jnr.*,!org.jctools.*,sun.misc;resolution:=optional,*
+                  !net.jcip.annotations.*,
+                  !edu.umd.cs.findbugs.annotations.*,
+                  !jnr.*,
+                  *
                 </Import-Package>
-                <!-- Explicitly declare shaded jctools packages since this isn't covered by shade plugin -->
                 <Export-Package>
-                  com.datastax.oss.driver.*.core.*,
-                  com.datastax.oss.driver.shaded.jctools.util;version=""${project.version}"";uses:=""sun.misc"",
-                  com.datastax.oss.driver.shaded.jctools.queues;version=""${project.version}"";uses:=""com.datastax.oss.driver.shaded.jctools.queues.spec"",
-                  com.datastax.oss.driver.shaded.jctools.queues.spec;version=""${project.version}"",
-                  com.datastax.oss.driver.shaded.jctools.queues.atomic;version=""${project.version}"";uses:=""com.datastax.oss.driver.shaded.jctools.queues,com.datastax.oss.driver.shaded.jctools.queues.spec"",
-                  com.datastax.oss.driver.shaded.jctools.maps;version=""${project.version}""
+                  com.datastax.oss.driver.*.core.*","[{'comment': 'Same here, we need to export internal classes along with API classes.', 'commenter': 'adutra'}]"
1060,integration-tests/src/test/java/com/datastax/oss/driver/osgi/BundleOptions.java,"@@ -38,6 +38,7 @@ public static CompositeOption baseOptions() {
             mavenBundle(""org.slf4j"", ""slf4j-api"", getVersion(""slf4j.version"")),
             mavenBundle(""org.hdrhistogram"", ""HdrHistogram"", getVersion(""hdrhistogram.version"")),
             mavenBundle(""com.typesafe"", ""config"", getVersion(""config.version"")),
+            mavenBundle(""org.jctools"", ""jctools-core"", getVersion(""jctools.version"")),","[{'comment': 'JCTools being an OSGi bundle, we need to add the bundle to the provision mix.', 'commenter': 'adutra'}]"
1060,test-infra/pom.xml,"@@ -66,9 +66,15 @@
         <configuration>
           <instructions>
             <Bundle-SymbolicName>com.datastax.oss.driver.testinfra</Bundle-SymbolicName>
+            <!-- allow SessionUtils to instantiate sessions by reflection -->
+            <DynamicImport-Package>*</DynamicImport-Package>","[{'comment': 'That is required if we want `SessionUtils` to instantiate DSE sessions.', 'commenter': 'adutra'}]"
1060,test-infra/pom.xml,"@@ -66,9 +66,15 @@
         <configuration>
           <instructions>
             <Bundle-SymbolicName>com.datastax.oss.driver.testinfra</Bundle-SymbolicName>
+            <!-- allow SessionUtils to instantiate sessions by reflection -->
+            <DynamicImport-Package>*</DynamicImport-Package>
             <!-- TODO: Only needed to use TestConfigLoader in integration-tests, remove this when we have","[{'comment': ""I confess I didn't fully understand this TODO so I left it intact."", 'commenter': 'adutra'}, {'comment': ""It's not needed any more now that #1032 is merged and `TestConfigLoader` has been deleted, so this comment should be removed."", 'commenter': 'tolbertam'}]"
1060,integration-tests/src/test/java/com/datastax/oss/driver/osgi/OsgiBaseIT.java,"@@ -78,7 +78,7 @@ public DriverConfigLoader configLoader() {
   @SuppressWarnings(""unchecked"")
   public void should_connect_and_query() {
     SessionBuilder<CqlSessionBuilder, CqlSession> builder =
-        SessionUtils.baseBuilder()
+        CqlSession.builder()","[{'comment': 'I prefer not to use our test infra inside this test to stay close to what users would actually do in their OSGi applications. We never know if by using the test infra we could introduce a biais (e.g. classes being loaded by the wrong classloader, etc.).', 'commenter': 'adutra'}, {'comment': 'I think this is fine, we will probably want to have a separate test set up anyways if we are using different session types as different jars will likely be required.', 'commenter': 'tolbertam'}, {'comment': 'Exactly. These tests have been failing consistently when ran against the DSE driver because, as you guessed, they rely on specific jars placed in specific folders on the filesystem. We will have to mirror these tests in the DSE driver test suite anyway.', 'commenter': 'adutra'}]"
1060,integration-tests/src/test/java/com/datastax/oss/driver/osgi/OsgiCustomLoadBalancingPolicyIT.java,"@@ -27,9 +33,10 @@
  * DynamicImport-Package: *</code>.
  */
 public class OsgiCustomLoadBalancingPolicyIT extends OsgiBaseIT {
-  @Override
-  public Option[] additionalOptions() {
-    return new Option[0];
+
+  @Configuration
+  public Option[] config() {","[{'comment': 'I know that these things are a matter of taste, but I personally like it better when each class declares its own config, it makes it easier for me to understand what bundles are going to be provisioned. Hope everybody will agree here.', 'commenter': 'adutra'}]"
1060,core-shaded/pom.xml,"@@ -28,97 +28,201 @@
   </parent>
 
   <artifactId>java-driver-core-shaded</artifactId>
-  <packaging>bundle</packaging>
 
-  <name>DataStax Java driver for Apache Cassandra(R) - core with netty shaded</name>
+  <name>DataStax Java driver for Apache Cassandra(R) - core with shaded deps</name>
 
   <dependencies>
     <dependency>
       <groupId>com.datastax.oss</groupId>
       <artifactId>java-driver-core</artifactId>
       <version>${project.version}</version>
     </dependency>
+    <!--
+    Repeat optional dependencies of the driver here because the shade plugin
+    does not automatically promote them to top level dependencies.
+    -->
+    <dependency>
+      <groupId>org.xerial.snappy</groupId>
+      <artifactId>snappy-java</artifactId>
+      <optional>true</optional>
+    </dependency>
+    <dependency>
+      <groupId>org.lz4</groupId>
+      <artifactId>lz4-java</artifactId>
+      <optional>true</optional>
+    </dependency>
   </dependencies>
 
+  <!--
+  Generation of the shaded driver-core bundle during package phase:
+  1) shade plugin shades the driver and creates a shaded jar + source jar
+  2) dependency plugin unpacks the shaded jar to target/classes (and removes unwanted content)
+  3) bundle plugin analyzes shaded classes and generates the bundle manifest
+  4) assembly plugin re-creates the shaded jar by packing target/classes + manifest + shaded pom
+  -->
+
   <build>
     <plugins>
-      <!-- extract shaded manifest from core jar -->
+      <plugin>
+        <artifactId>maven-shade-plugin</artifactId>
+        <executions>
+          <execution>
+            <id>shade-core-dependencies</id>
+            <phase>package</phase>
+            <goals>
+              <goal>shade</goal>
+            </goals>
+            <configuration>
+              <createSourcesJar>true</createSourcesJar>
+              <shadeSourcesContent>true</shadeSourcesContent>
+              <artifactSet>
+                <includes>
+                  <!--
+                  Include:
+                  - The core driver itself; it is not relocated but needs to be included.
+                  - All the dependencies we want to shade & relocate: currently, all the Netty
+                  artifacts.
+                  -->
+                  <include>com.datastax.oss:java-driver-core</include>
+                  <include>io.netty:*</include>
+                </includes>
+              </artifactSet>
+              <relocations>
+                <relocation>
+                  <pattern>io.netty</pattern>
+                  <shadedPattern>com.datastax.oss.driver.shaded.netty</shadedPattern>
+                </relocation>
+              </relocations>
+              <!--
+              Promote all other transitive dependencies of java-driver-core other than Netty
+              to direct dependencies in the shaded pom.
+              -->
+              <promoteTransitiveDependencies>true</promoteTransitiveDependencies>
+            </configuration>
+          </execution>
+        </executions>
+      </plugin>
       <plugin>
         <artifactId>maven-dependency-plugin</artifactId>
         <executions>
           <execution>
-            <id>unpack-manifest</id>
-            <phase>prepare-package</phase>
+            <id>unpack-shaded-classes</id>
+            <phase>package</phase>
             <goals>
               <goal>unpack</goal>
             </goals>
             <configuration>
               <artifactItems>
                 <artifactItem>
                   <groupId>com.datastax.oss</groupId>
-                  <artifactId>java-driver-core</artifactId>
+                  <artifactId>java-driver-core-shaded</artifactId>
                   <version>${project.version}</version>
-                  <includes>META-INF-shaded/MANIFEST.MF</includes>
-                  <outputDirectory>${project.build.directory}</outputDirectory>
+                  <type>jar</type>
+                  <outputDirectory>${project.build.outputDirectory}</outputDirectory>
                 </artifactItem>
               </artifactItems>
+              <!--
+              Exclude leftovers from the shading phase (this could also be done with a
+              resource transformer by the shade plugin itself, but this way is more flexible).
+              Note: Netty ships with an embedded JCTools, that's why we need to exclude that too.
+              -->
+              <excludes>
+                META-INF/maven/com.datastax.oss/java-driver-core/**,
+                META-INF/maven/io.netty/**,
+                META-INF/maven/org.jctools/**
+              </excludes>
             </configuration>
           </execution>
         </executions>
       </plugin>
       <plugin>
-        <artifactId>maven-shade-plugin</artifactId>
+        <groupId>org.apache.felix</groupId>
+        <artifactId>maven-bundle-plugin</artifactId>
+        <extensions>true</extensions>
         <executions>
           <execution>
+            <id>generate-shaded-manifest</id>
             <phase>package</phase>
             <goals>
-              <goal>shade</goal>
+              <goal>manifest</goal>
             </goals>
             <configuration>
-              <artifactSet>
-                <includes>
-                  <include>com.datastax.oss:java-driver-core</include>
-                  <include>io.netty:*</include>
-                </includes>
-              </artifactSet>
-              <relocations>
-                <relocation>
-                  <pattern>io.netty</pattern>
-                  <shadedPattern>com.datastax.oss.driver.shaded.netty</shadedPattern>
-                </relocation>
-              </relocations>
-              <transformers>
-                <transformer implementation=""org.apache.maven.plugins.shade.resource.DontIncludeResourceTransformer"">
-                  <resources>
-                    <resource>META-INF/MANIFEST.MF</resource>
-                    <resource>META-INF-shaded/MANIFEST.MF</resource>
-                    <resource>META-INF/maven/com.datastax.oss/java-driver-core/pom.xml</resource>
-                    <resource>META-INF/maven/com.datastax.oss/java-driver-core/pom.properties</resource>
-                    <resource>META-INF/maven/io.netty/netty-handler/pom.properties</resource>
-                    <resource>META-INF/maven/io.netty/netty-handler/pom.xml</resource>
-                    <resource>META-INF/maven/io.netty/netty-buffer/pom.properties</resource>
-                    <resource>META-INF/maven/io.netty/netty-buffer/pom.xml</resource>
-                    <resource>META-INF/maven/io.netty/netty-common/pom.properties</resource>
-                    <resource>META-INF/maven/io.netty/netty-common/pom.xml</resource>
-                    <resource>META-INF/maven/io.netty/netty-transport/pom.properties</resource>
-                    <resource>META-INF/maven/io.netty/netty-transport/pom.xml</resource>
-                    <resource>META-INF/maven/io.netty/netty-resolver/pom.properties</resource>
-                    <resource>META-INF/maven/io.netty/netty-resolver/pom.xml</resource>
-                    <resource>META-INF/maven/io.netty/netty-codec/pom.properties</resource>
-                    <resource>META-INF/maven/io.netty/netty-codec/pom.xml</resource>
-                    <!-- netty's shading of jctools does not remove its pom files -->
-                    <resource>META-INF/maven/org.jctools/jctools-core/pom.properties</resource>
-                    <resource>META-INF/maven/org.jctools/jctools-core/pom.xml</resource>
-                  </resources>
-                </transformer>
-                <!-- Pick up the alternate manifest that was extracted from the core module -->
-                <transformer implementation=""org.apache.maven.plugins.shade.resource.IncludeResourceTransformer"">
-                  <resource>META-INF/MANIFEST.MF</resource>
-                  <file>${project.build.directory}/META-INF-shaded/MANIFEST.MF</file>
-                </transformer>
-              </transformers>
-              <!-- Keep the dependencies of driver-core -->
-              <promoteTransitiveDependencies>true</promoteTransitiveDependencies>
+              <instructions>
+                <Bundle-SymbolicName>com.datastax.oss.driver.core</Bundle-SymbolicName>
+                <DynamicImport-Package>*</DynamicImport-Package>
+                <!--
+                Don't include because they aren't OSGi bundles, and the driver bundle can
+                live without them:
+                 - jnr
+                 - net.jcip.annotations
+                 - edu.umd.cs.findbugs.annotations
+                Don't include because they are imported by shaded Netty classes
+                but not used by the driver bundle:
+                 - com.google.protobuf
+                 - com.jcraft.jzlib
+                 - com.ning.compress
+                 - lzma.sdk
+                 - net.jpountz.xxhash
+                 - org.bouncycastle
+                 - org.conscrypt
+                 - org.apache.commons.logging
+                 - org.apache.log4j
+                 - org.apache.logging.log4j
+                 - org.eclipse.jetty
+                 - org.jboss.marshalling
+                 - sun.misc
+                 - sun.security
+                -->
+                <Import-Package>
+                  !com.datastax.oss.driver.shaded.*,
+                  !jnr.*,
+                  !net.jcip.annotations.*,
+                  !edu.umd.cs.findbugs.annotations.*,
+                  !com.google.protobuf.*,
+                  !com.jcraft.jzlib.*,
+                  !com.ning.compress.*,
+                  !lzma.sdk.*,
+                  !net.jpountz.xxhash.*,
+                  !org.bouncycastle.*,
+                  !org.conscrypt.*,
+                  !org.apache.commons.logging.*,
+                  !org.apache.log4j.*,
+                  !org.apache.logging.log4j.*,
+                  !org.eclipse.jetty.*,
+                  !org.jboss.marshalling.*,
+                  !sun.misc.*,
+                  !sun.security.*,
+                  *
+                </Import-Package>
+                <Export-Package>
+                  com.datastax.oss.driver.*.core.*,
+                  com.datastax.oss.driver.shaded.*
+                </Export-Package>
+              </instructions>
+              <rebuildBundle>true</rebuildBundle>","[{'comment': ""I don't really understand what magic this option does but for the DSE driver it creates a much cleaner execution, so just to be on the safe side I'm also turning this on here for the OSS driver (although even without it the plugin executes without warnings here)."", 'commenter': 'adutra'}]"
1060,core/pom.xml,"@@ -170,117 +166,38 @@
           </properties>
         </configuration>
       </plugin>
-      <!--
-      We avoid packaging bundle because it does not play nicely with the shade plugin, see
-      https://stackoverflow.com/questions/31262032/maven-shade-plugin-and-custom-packaging-type
-      -->
       <plugin>
         <groupId>org.apache.felix</groupId>
         <artifactId>maven-bundle-plugin</artifactId>
-        <configuration>
-          <instructions>
-            <Bundle-SymbolicName>com.datastax.oss.driver.core</Bundle-SymbolicName>
-          </instructions>
-          <!--
-          Prevent customized manifest entries from the project's maven-jar-plugin configuration from being read, see
-          http://apache-felix.18485.x6.nabble.com/how-lt-manifestLocation-gt-is-used-in-maven-bundle-plugin-td4835566.html
-          -->
-          <archive>
-            <forced>true</forced>
-          </archive>
-        </configuration>
+        <extensions>true</extensions>
         <executions>
           <execution>
-            <id>bundle-manifest</id>
-            <phase>process-classes</phase>
             <goals>
-              <goal>manifest</goal>
+              <goal>bundle</goal>
             </goals>
             <configuration>
-              <manifestLocation>${project.build.outputDirectory}/META-INF</manifestLocation>
               <instructions>
+                <Bundle-SymbolicName>com.datastax.oss.driver.core</Bundle-SymbolicName>
                 <!-- Allow importing code from other packages (so reflection-based loading of policies works) -->
                 <DynamicImport-Package>*</DynamicImport-Package>
-                <!-- Don't include JNR packages since some JNR modules are not OSGi bundles,
-                     jcip because its not an OSGi bundle, jctools because its shaded.
-                     Import sun.misc as jctools does this. -->
+                <!--
+                Don't include because they aren't OSGi bundles, and the driver can live without them:
+                 - jnr
+                 - net.jcip.annotations
+                 - edu.umd.cs.findbugs.annotations","[{'comment': 'Nit: listing the dependencies in the comment is redundant, this is literally a copy-paste of the 3 lines of code below.', 'commenter': 'olim7t'}]"
1060,core-shaded/pom.xml,"@@ -28,97 +28,201 @@
   </parent>
 
   <artifactId>java-driver-core-shaded</artifactId>
-  <packaging>bundle</packaging>
 
-  <name>DataStax Java driver for Apache Cassandra(R) - core with netty shaded</name>
+  <name>DataStax Java driver for Apache Cassandra(R) - core with shaded deps</name>
 
   <dependencies>
     <dependency>
       <groupId>com.datastax.oss</groupId>
       <artifactId>java-driver-core</artifactId>
       <version>${project.version}</version>
     </dependency>
+    <!--
+    Repeat optional dependencies of the driver here because the shade plugin
+    does not automatically promote them to top level dependencies.
+    -->
+    <dependency>
+      <groupId>org.xerial.snappy</groupId>
+      <artifactId>snappy-java</artifactId>
+      <optional>true</optional>
+    </dependency>
+    <dependency>
+      <groupId>org.lz4</groupId>
+      <artifactId>lz4-java</artifactId>
+      <optional>true</optional>
+    </dependency>
   </dependencies>
 
+  <!--
+  Generation of the shaded driver-core bundle during package phase:
+  1) shade plugin shades the driver and creates a shaded jar + source jar
+  2) dependency plugin unpacks the shaded jar to target/classes (and removes unwanted content)
+  3) bundle plugin analyzes shaded classes and generates the bundle manifest
+  4) assembly plugin re-creates the shaded jar by packing target/classes + manifest + shaded pom
+  -->
+
   <build>
     <plugins>
-      <!-- extract shaded manifest from core jar -->
+      <plugin>
+        <artifactId>maven-shade-plugin</artifactId>
+        <executions>
+          <execution>
+            <id>shade-core-dependencies</id>
+            <phase>package</phase>
+            <goals>
+              <goal>shade</goal>
+            </goals>
+            <configuration>
+              <createSourcesJar>true</createSourcesJar>
+              <shadeSourcesContent>true</shadeSourcesContent>
+              <artifactSet>
+                <includes>
+                  <!--
+                  Include:
+                  - The core driver itself; it is not relocated but needs to be included.
+                  - All the dependencies we want to shade & relocate: currently, all the Netty
+                  artifacts.
+                  -->
+                  <include>com.datastax.oss:java-driver-core</include>
+                  <include>io.netty:*</include>
+                </includes>
+              </artifactSet>
+              <relocations>
+                <relocation>
+                  <pattern>io.netty</pattern>
+                  <shadedPattern>com.datastax.oss.driver.shaded.netty</shadedPattern>
+                </relocation>
+              </relocations>
+              <!--
+              Promote all other transitive dependencies of java-driver-core other than Netty
+              to direct dependencies in the shaded pom.
+              -->
+              <promoteTransitiveDependencies>true</promoteTransitiveDependencies>
+            </configuration>
+          </execution>
+        </executions>
+      </plugin>
       <plugin>
         <artifactId>maven-dependency-plugin</artifactId>
         <executions>
           <execution>
-            <id>unpack-manifest</id>
-            <phase>prepare-package</phase>
+            <id>unpack-shaded-classes</id>
+            <phase>package</phase>
             <goals>
               <goal>unpack</goal>
             </goals>
             <configuration>
               <artifactItems>
                 <artifactItem>
                   <groupId>com.datastax.oss</groupId>
-                  <artifactId>java-driver-core</artifactId>
+                  <artifactId>java-driver-core-shaded</artifactId>
                   <version>${project.version}</version>
-                  <includes>META-INF-shaded/MANIFEST.MF</includes>
-                  <outputDirectory>${project.build.directory}</outputDirectory>
+                  <type>jar</type>
+                  <outputDirectory>${project.build.outputDirectory}</outputDirectory>
                 </artifactItem>
               </artifactItems>
+              <!--
+              Exclude leftovers from the shading phase (this could also be done with a
+              resource transformer by the shade plugin itself, but this way is more flexible).
+              Note: Netty ships with an embedded JCTools, that's why we need to exclude that too.
+              -->
+              <excludes>
+                META-INF/maven/com.datastax.oss/java-driver-core/**,
+                META-INF/maven/io.netty/**,
+                META-INF/maven/org.jctools/**
+              </excludes>
             </configuration>
           </execution>
         </executions>
       </plugin>
       <plugin>
-        <artifactId>maven-shade-plugin</artifactId>
+        <groupId>org.apache.felix</groupId>
+        <artifactId>maven-bundle-plugin</artifactId>
+        <extensions>true</extensions>
         <executions>
           <execution>
+            <id>generate-shaded-manifest</id>
             <phase>package</phase>
             <goals>
-              <goal>shade</goal>
+              <goal>manifest</goal>
             </goals>
             <configuration>
-              <artifactSet>
-                <includes>
-                  <include>com.datastax.oss:java-driver-core</include>
-                  <include>io.netty:*</include>
-                </includes>
-              </artifactSet>
-              <relocations>
-                <relocation>
-                  <pattern>io.netty</pattern>
-                  <shadedPattern>com.datastax.oss.driver.shaded.netty</shadedPattern>
-                </relocation>
-              </relocations>
-              <transformers>
-                <transformer implementation=""org.apache.maven.plugins.shade.resource.DontIncludeResourceTransformer"">
-                  <resources>
-                    <resource>META-INF/MANIFEST.MF</resource>
-                    <resource>META-INF-shaded/MANIFEST.MF</resource>
-                    <resource>META-INF/maven/com.datastax.oss/java-driver-core/pom.xml</resource>
-                    <resource>META-INF/maven/com.datastax.oss/java-driver-core/pom.properties</resource>
-                    <resource>META-INF/maven/io.netty/netty-handler/pom.properties</resource>
-                    <resource>META-INF/maven/io.netty/netty-handler/pom.xml</resource>
-                    <resource>META-INF/maven/io.netty/netty-buffer/pom.properties</resource>
-                    <resource>META-INF/maven/io.netty/netty-buffer/pom.xml</resource>
-                    <resource>META-INF/maven/io.netty/netty-common/pom.properties</resource>
-                    <resource>META-INF/maven/io.netty/netty-common/pom.xml</resource>
-                    <resource>META-INF/maven/io.netty/netty-transport/pom.properties</resource>
-                    <resource>META-INF/maven/io.netty/netty-transport/pom.xml</resource>
-                    <resource>META-INF/maven/io.netty/netty-resolver/pom.properties</resource>
-                    <resource>META-INF/maven/io.netty/netty-resolver/pom.xml</resource>
-                    <resource>META-INF/maven/io.netty/netty-codec/pom.properties</resource>
-                    <resource>META-INF/maven/io.netty/netty-codec/pom.xml</resource>
-                    <!-- netty's shading of jctools does not remove its pom files -->
-                    <resource>META-INF/maven/org.jctools/jctools-core/pom.properties</resource>
-                    <resource>META-INF/maven/org.jctools/jctools-core/pom.xml</resource>
-                  </resources>
-                </transformer>
-                <!-- Pick up the alternate manifest that was extracted from the core module -->
-                <transformer implementation=""org.apache.maven.plugins.shade.resource.IncludeResourceTransformer"">
-                  <resource>META-INF/MANIFEST.MF</resource>
-                  <file>${project.build.directory}/META-INF-shaded/MANIFEST.MF</file>
-                </transformer>
-              </transformers>
-              <!-- Keep the dependencies of driver-core -->
-              <promoteTransitiveDependencies>true</promoteTransitiveDependencies>
+              <instructions>
+                <Bundle-SymbolicName>com.datastax.oss.driver.core</Bundle-SymbolicName>
+                <DynamicImport-Package>*</DynamicImport-Package>
+                <!--
+                Don't include because they aren't OSGi bundles, and the driver bundle can
+                live without them:
+                 - jnr
+                 - net.jcip.annotations
+                 - edu.umd.cs.findbugs.annotations
+                Don't include because they are imported by shaded Netty classes
+                but not used by the driver bundle:
+                 - com.google.protobuf
+                 - com.jcraft.jzlib
+                 - com.ning.compress
+                 - lzma.sdk
+                 - net.jpountz.xxhash
+                 - org.bouncycastle
+                 - org.conscrypt
+                 - org.apache.commons.logging
+                 - org.apache.log4j
+                 - org.apache.logging.log4j
+                 - org.eclipse.jetty
+                 - org.jboss.marshalling
+                 - sun.misc
+                 - sun.security
+                -->","[{'comment': 'Nit: same remark as the other POM, the comment paraphrases the code. You can simply have a ""Don\'t include because..."" comment at the beginning of each block in the code.', 'commenter': 'olim7t'}]"
1060,core-shaded/pom.xml,"@@ -28,97 +28,201 @@
   </parent>
 
   <artifactId>java-driver-core-shaded</artifactId>
-  <packaging>bundle</packaging>
 
-  <name>DataStax Java driver for Apache Cassandra(R) - core with netty shaded</name>
+  <name>DataStax Java driver for Apache Cassandra(R) - core with shaded deps</name>
 
   <dependencies>
     <dependency>
       <groupId>com.datastax.oss</groupId>
       <artifactId>java-driver-core</artifactId>
       <version>${project.version}</version>
     </dependency>
+    <!--
+    Repeat optional dependencies of the driver here because the shade plugin
+    does not automatically promote them to top level dependencies.
+    -->
+    <dependency>
+      <groupId>org.xerial.snappy</groupId>
+      <artifactId>snappy-java</artifactId>
+      <optional>true</optional>
+    </dependency>
+    <dependency>
+      <groupId>org.lz4</groupId>
+      <artifactId>lz4-java</artifactId>
+      <optional>true</optional>
+    </dependency>
   </dependencies>
 
+  <!--
+  Generation of the shaded driver-core bundle during package phase:
+  1) shade plugin shades the driver and creates a shaded jar + source jar
+  2) dependency plugin unpacks the shaded jar to target/classes (and removes unwanted content)
+  3) bundle plugin analyzes shaded classes and generates the bundle manifest
+  4) assembly plugin re-creates the shaded jar by packing target/classes + manifest + shaded pom
+  -->
+
   <build>
     <plugins>
-      <!-- extract shaded manifest from core jar -->
+      <plugin>
+        <artifactId>maven-shade-plugin</artifactId>
+        <executions>
+          <execution>
+            <id>shade-core-dependencies</id>
+            <phase>package</phase>
+            <goals>
+              <goal>shade</goal>
+            </goals>
+            <configuration>
+              <createSourcesJar>true</createSourcesJar>
+              <shadeSourcesContent>true</shadeSourcesContent>
+              <artifactSet>
+                <includes>
+                  <!--
+                  Include:
+                  - The core driver itself; it is not relocated but needs to be included.
+                  - All the dependencies we want to shade & relocate: currently, all the Netty
+                  artifacts.
+                  -->
+                  <include>com.datastax.oss:java-driver-core</include>
+                  <include>io.netty:*</include>
+                </includes>
+              </artifactSet>
+              <relocations>
+                <relocation>
+                  <pattern>io.netty</pattern>
+                  <shadedPattern>com.datastax.oss.driver.shaded.netty</shadedPattern>
+                </relocation>
+              </relocations>
+              <!--
+              Promote all other transitive dependencies of java-driver-core other than Netty
+              to direct dependencies in the shaded pom.
+              -->
+              <promoteTransitiveDependencies>true</promoteTransitiveDependencies>","[{'comment': 'Unfortunately this recurses beyond the first level of transitivity. If you look at `dependency-reduced-pom.xml`, it explicitly lists `org.ow2.asm.*` (pulled via `jnr-ffi`), `jnr-constants` (via `jnr-posix`), etc.\r\n\r\nThis is annoying if the end user wishes to exclude those dependencies manually. We need to find a way to avoid that.', 'commenter': 'olim7t'}]"
1060,core-shaded/pom.xml,"@@ -28,97 +28,243 @@
   </parent>
 
   <artifactId>java-driver-core-shaded</artifactId>
-  <packaging>bundle</packaging>
 
-  <name>DataStax Java driver for Apache Cassandra(R) - core with netty shaded</name>
+  <name>DataStax Java driver for Apache Cassandra(R) - core with shaded deps</name>
 
   <dependencies>
+    <!--
+    Declare a dependency to the core driver itself so that all its classes get included;
+    this dependency will be removed from the final pom by the shade plugin.
+    -->
     <dependency>
       <groupId>com.datastax.oss</groupId>
       <artifactId>java-driver-core</artifactId>
       <version>${project.version}</version>
     </dependency>
+    <!--
+    Repeat all dependencies of the core driver *except* the ones that are going to be shaded,
+    so that they get included in the final pom (we don't use the ""promoteTransitiveDependencies""
+    option of the shade plugin because it promotes all dependencies, even nested ones, to top level).
+    -->
+    <dependency>
+      <groupId>com.datastax.oss</groupId>
+      <artifactId>native-protocol</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>com.datastax.oss</groupId>
+      <artifactId>java-driver-shaded-guava</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>com.typesafe</groupId>
+      <artifactId>config</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>com.github.jnr</groupId>
+      <artifactId>jnr-ffi</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>com.github.jnr</groupId>
+      <artifactId>jnr-posix</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.xerial.snappy</groupId>
+      <artifactId>snappy-java</artifactId>
+      <optional>true</optional>
+    </dependency>
+    <dependency>
+      <groupId>org.lz4</groupId>
+      <artifactId>lz4-java</artifactId>
+      <optional>true</optional>
+    </dependency>
+    <dependency>
+      <groupId>org.slf4j</groupId>
+      <artifactId>slf4j-api</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>io.dropwizard.metrics</groupId>
+      <artifactId>metrics-core</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.hdrhistogram</groupId>
+      <artifactId>HdrHistogram</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>com.github.stephenc.jcip</groupId>
+      <artifactId>jcip-annotations</artifactId>
+      <optional>true</optional>
+    </dependency>
+    <dependency>
+      <groupId>com.github.spotbugs</groupId>
+      <artifactId>spotbugs-annotations</artifactId>
+      <optional>true</optional>
+    </dependency>
   </dependencies>
 
+  <!--
+  Generation of the shaded driver-core bundle during package phase:
+  1) shade plugin shades the driver and creates a shaded jar + source jar
+  2) dependency plugin unpacks the shaded jar to target/classes (and removes unwanted content)
+  3) bundle plugin analyzes shaded classes and generates the bundle manifest
+  4) assembly plugin re-creates the shaded jar by packing target/classes + manifest + shaded pom
+  -->
+
   <build>
     <plugins>
-      <!-- extract shaded manifest from core jar -->
+      <plugin>
+        <artifactId>maven-shade-plugin</artifactId>
+        <executions>
+          <execution>
+            <id>shade-core-dependencies</id>
+            <phase>package</phase>
+            <goals>
+              <goal>shade</goal>
+            </goals>
+            <configuration>
+              <createSourcesJar>true</createSourcesJar>
+              <shadeSourcesContent>true</shadeSourcesContent>
+              <artifactSet>
+                <includes>
+                  <!--
+                  Include:
+                  - The core driver itself; it is not relocated but needs to be included.
+                  - All the dependencies we want to shade & relocate: currently
+                    - all the Netty artifacts;
+                    - JCTools.
+                  -->
+                  <include>com.datastax.oss:java-driver-core</include>
+                  <include>io.netty:*</include>
+                  <include>org.jctools:*</include>
+                </includes>
+              </artifactSet>
+              <relocations>
+                <relocation>
+                  <pattern>io.netty</pattern>
+                  <shadedPattern>com.datastax.oss.driver.shaded.netty</shadedPattern>
+                </relocation>
+                <relocation>
+                  <pattern>org.jctools</pattern>
+                  <shadedPattern>com.datastax.oss.driver.shaded.jctools</shadedPattern>
+                </relocation>
+              </relocations>
+            </configuration>
+          </execution>
+        </executions>
+      </plugin>
       <plugin>
         <artifactId>maven-dependency-plugin</artifactId>
         <executions>
           <execution>
-            <id>unpack-manifest</id>
-            <phase>prepare-package</phase>
+            <id>unpack-shaded-classes</id>
+            <phase>package</phase>
             <goals>
               <goal>unpack</goal>
             </goals>
             <configuration>
               <artifactItems>
                 <artifactItem>
                   <groupId>com.datastax.oss</groupId>
-                  <artifactId>java-driver-core</artifactId>
+                  <artifactId>java-driver-core-shaded</artifactId>
                   <version>${project.version}</version>
-                  <includes>META-INF-shaded/MANIFEST.MF</includes>
-                  <outputDirectory>${project.build.directory}</outputDirectory>
+                  <type>jar</type>
+                  <outputDirectory>${project.build.outputDirectory}</outputDirectory>
                 </artifactItem>
               </artifactItems>
+              <!--
+              Exclude leftovers from the shading phase (this could also be done with a
+              resource transformer by the shade plugin itself, but this way is more flexible).
+              -->
+              <excludes>
+                META-INF/maven/com.datastax.oss/java-driver-core/**,
+                META-INF/maven/io.netty/**,
+                META-INF/maven/org.jctools/**
+              </excludes>
             </configuration>
           </execution>
         </executions>
       </plugin>
       <plugin>
-        <artifactId>maven-shade-plugin</artifactId>
+        <groupId>org.apache.felix</groupId>
+        <artifactId>maven-bundle-plugin</artifactId>
+        <extensions>true</extensions>
         <executions>
           <execution>
+            <id>generate-shaded-manifest</id>
             <phase>package</phase>
             <goals>
-              <goal>shade</goal>
+              <goal>manifest</goal>
             </goals>
             <configuration>
-              <artifactSet>
-                <includes>
-                  <include>com.datastax.oss:java-driver-core</include>
-                  <include>io.netty:*</include>
-                </includes>
-              </artifactSet>
-              <relocations>
-                <relocation>
-                  <pattern>io.netty</pattern>
-                  <shadedPattern>com.datastax.oss.driver.shaded.netty</shadedPattern>
-                </relocation>
-              </relocations>
-              <transformers>
-                <transformer implementation=""org.apache.maven.plugins.shade.resource.DontIncludeResourceTransformer"">
-                  <resources>
-                    <resource>META-INF/MANIFEST.MF</resource>
-                    <resource>META-INF-shaded/MANIFEST.MF</resource>
-                    <resource>META-INF/maven/com.datastax.oss/java-driver-core/pom.xml</resource>
-                    <resource>META-INF/maven/com.datastax.oss/java-driver-core/pom.properties</resource>
-                    <resource>META-INF/maven/io.netty/netty-handler/pom.properties</resource>
-                    <resource>META-INF/maven/io.netty/netty-handler/pom.xml</resource>
-                    <resource>META-INF/maven/io.netty/netty-buffer/pom.properties</resource>
-                    <resource>META-INF/maven/io.netty/netty-buffer/pom.xml</resource>
-                    <resource>META-INF/maven/io.netty/netty-common/pom.properties</resource>
-                    <resource>META-INF/maven/io.netty/netty-common/pom.xml</resource>
-                    <resource>META-INF/maven/io.netty/netty-transport/pom.properties</resource>
-                    <resource>META-INF/maven/io.netty/netty-transport/pom.xml</resource>
-                    <resource>META-INF/maven/io.netty/netty-resolver/pom.properties</resource>
-                    <resource>META-INF/maven/io.netty/netty-resolver/pom.xml</resource>
-                    <resource>META-INF/maven/io.netty/netty-codec/pom.properties</resource>
-                    <resource>META-INF/maven/io.netty/netty-codec/pom.xml</resource>
-                    <!-- netty's shading of jctools does not remove its pom files -->
-                    <resource>META-INF/maven/org.jctools/jctools-core/pom.properties</resource>
-                    <resource>META-INF/maven/org.jctools/jctools-core/pom.xml</resource>
-                  </resources>
-                </transformer>
-                <!-- Pick up the alternate manifest that was extracted from the core module -->
-                <transformer implementation=""org.apache.maven.plugins.shade.resource.IncludeResourceTransformer"">
-                  <resource>META-INF/MANIFEST.MF</resource>
-                  <file>${project.build.directory}/META-INF-shaded/MANIFEST.MF</file>
-                </transformer>
-              </transformers>
-              <!-- Keep the dependencies of driver-core -->
-              <promoteTransitiveDependencies>true</promoteTransitiveDependencies>
+              <instructions>
+                <Bundle-SymbolicName>com.datastax.oss.driver.core</Bundle-SymbolicName>
+                <!--
+                Allow importing code from other packages
+                (so reflection-based loading of policies works)
+                -->
+                <DynamicImport-Package>*</DynamicImport-Package>
+                <!--
+                Don't import the packages below because either:
+                1) they were shaded (except Guava which resides in a separate bundle); or
+                2) they aren't OSGi bundles, and the driver bundle can live without them; or
+                2) they are imported by shaded classes, but not used by the driver bundle.
+                -->
+                <Import-Package>
+                  !com.datastax.oss.driver.shaded.netty.*,
+                  !com.datastax.oss.driver.shaded.jctools.*,
+                  !jnr.*,
+                  !net.jcip.annotations.*,
+                  !edu.umd.cs.findbugs.annotations.*,
+                  !com.google.protobuf.*,
+                  !com.jcraft.jzlib.*,
+                  !com.ning.compress.*,
+                  !lzma.sdk.*,
+                  !net.jpountz.xxhash.*,
+                  !org.bouncycastle.*,
+                  !org.conscrypt.*,
+                  !org.apache.commons.logging.*,
+                  !org.apache.log4j.*,
+                  !org.apache.logging.log4j.*,
+                  !org.eclipse.jetty.*,
+                  !org.jboss.marshalling.*,
+                  !sun.misc.*,
+                  !sun.security.*,
+                  *
+                </Import-Package>
+                <!--
+                Export:
+                1) The driver's packages (API and internal);
+                2) All shaded packages, except Guava which resides in a separate bundle.
+                -->
+                <Export-Package>
+                  com.datastax.oss.driver.api.core.*,
+                  com.datastax.oss.driver.internal.core.*,
+                  com.datastax.oss.driver.shaded.netty.*,
+                  com.datastax.oss.driver.shaded.jctools.*","[{'comment': ""With today's changes I realized that the previous pattern (`com.datastax.oss.driver.shaded.*`) was too broad; as a consequence, we were _not importing_ but were instead _exporting_ shaded Guava classes. It's a miracle that it worked this way, but now it's fixed (the proper strategy is the opposite: import shaded Guava classes, but not export them since they live in a separate bundle)."", 'commenter': 'adutra'}]"
1060,core-shaded/pom.xml,"@@ -28,97 +28,243 @@
   </parent>
 
   <artifactId>java-driver-core-shaded</artifactId>
-  <packaging>bundle</packaging>
 
-  <name>DataStax Java driver for Apache Cassandra(R) - core with netty shaded</name>
+  <name>DataStax Java driver for Apache Cassandra(R) - core with shaded deps</name>
 
   <dependencies>
+    <!--
+    Declare a dependency to the core driver itself so that all its classes get included;
+    this dependency will be removed from the final pom by the shade plugin.
+    -->
     <dependency>
       <groupId>com.datastax.oss</groupId>
       <artifactId>java-driver-core</artifactId>
       <version>${project.version}</version>
     </dependency>
+    <!--","[{'comment': 'The only way I can think of to mimic the `promoteTransitiveDependencies` feature is to re-declare all relevant dependencies here. A bit verbose, but at least it works as expected.', 'commenter': 'adutra'}]"
1064,pom.xml,"@@ -87,6 +87,7 @@
         <test.groups>unit</test.groups>
         <test.osgi.skip>true</test.osgi.skip>
         <javadoc.opts />
+        <format.validateOnly>true</format.validateOnly>","[{'comment': ""It seems that this system property is not honored anymore (or never was?). \r\nThis is also a problem affecting the new driver (4.x).\r\nAnd besides I honestly find it very annoying that by default the build only checks the files, but doesn't format them. Couldn't we switch to the `format` goal instead?"", 'commenter': 'adutra'}, {'comment': ""You are right, this option is no longer valid since the maven build uses `check`.  To format, you have to run `mvn fmt:format` explicitly.  I'll remove the property and also make sure we have that documented."", 'commenter': 'tolbertam'}, {'comment': ""Yes, the property is obsolete, it was used by a previous version of the plugin that did everything with a single goal (see coveo/fmt-maven-plugin#14), I forgot to remove it when I upgraded.\r\n\r\n> And besides I honestly find it very annoying that by default the build only checks the files, but doesn't format them. Couldn't we switch to the format goal instead?\r\n\r\nBig ðŸ‘Ž on this (as already discussed).\r\nIf we switch to the format goal, the CI build will pass even if unformatted files are accidentally committed. This makes the check useless.\r\nFailing the build if local changes are not ready to be committed is a good thing. Much like a compile error or a broken unit test."", 'commenter': 'olim7t'}, {'comment': ""Ok, so I see that if someone committed a file before running a build (that performs the formatting), they could commit a file that requires formatting, and we'd want the CI build to fail. \r\n\r\nHowever, when building locally, I'd want the code to be formatted as part of that build.\r\n\r\nCan we use environment variables to alter this behavior locally vs. in CI?"", 'commenter': 'stamhankar999'}, {'comment': ""I prefer the behavior as it is, if the content of the source code is changing, I want that to be an explicit action, much like how the license formatting works.  On the other hand if we could do something that can enable that for those who don't mind it formatting we could add something.\r\n\r\nOne possibility is that we could define a profile that does formatting instead of checking, lets call it `format`.  Those who want automatic formatting instead of checking all they would to do is add this to their `/.m2/settings.xml` so the profile is implicitly activated:\r\n\r\n```xml\r\n  <activeProfiles>\r\n    <activeProfile>format</activeProfile>\r\n  </activeProfiles>\r\n```\r\n\r\nAlternatively, you could add `-Pformat` to your maven build.\r\n\r\nThoughts on that?\r\n "", 'commenter': 'tolbertam'}, {'comment': 'patch that does this: https://gist.github.com/tolbertam/5f6d2334bd582a166a95aba5e02da8c9', 'commenter': 'tolbertam'}, {'comment': 'ðŸ‘ I think this is a great solution. TIL some new Maven magic.', 'commenter': 'stamhankar999'}, {'comment': ""Holding off on contributing that for now as need to discuss the idea further and don't want to hold this PR up.  Created [JAVA-1930](https://datastax-oss.atlassian.net/browse/JAVA-1930) for that."", 'commenter': 'tolbertam'}]"
1065,driver-core/src/main/java/com/datastax/driver/core/ControlConnection.java,"@@ -621,9 +649,61 @@ private static void updateLocationInfo(
     if (!isInitialConnection) cluster.loadBalancingPolicy().onAdd(host);
   }
 
-  private static void refreshNodeListAndTokenMap(
-      Connection connection,
-      Cluster.Manager cluster,
+  /**
+   * Resolves peering information by doing the following:
+   *
+   * <ol>
+   *   <li>if <code>isPeersV2</code> is true, query the <code>system.peers_v2</code> table,
+   *       otherwise query <code>system.peers</code>.
+   *   <li>if <code>system.peers_v2</code> query fails, set <code>isPeersV2</code> to false and call
+   *       selectPeersFuture again.
+   * </ol>
+   *
+   * @param connection connection to send request on.
+   * @return result of peers query.
+   */
+  private ListenableFuture<ResultSet> selectPeersFuture(final Connection connection) {
+    if (isPeersV2) {","[{'comment': ""For < C* 4.x, maybe we should just not even try the peers_v2 table.  We could only allow this for protocol V5+.  I think that's an ok limitation, and we can add some kind of way to bypass the V5 requirement for our Scassandra tests."", 'commenter': 'tolbertam'}, {'comment': ""Thinking about this more. I think for 3.x, this should be more of an opt-in thing via a builder option.  For a minor version making an extra query on initialization seems kinda iffy, and I think it's ok to require use of this feature to be explicit, since the 3.x driver is more driven around ip addresses and global setting of port (via `withPort`), so we should probably make a separate option so users can opt-in to this capability."", 'commenter': 'tolbertam'}, {'comment': ""We discussed this more and decided that it's ok to try `peers_v2` table first."", 'commenter': 'tolbertam'}]"
1065,driver-core/src/main/java/com/datastax/driver/core/ControlConnection.java,"@@ -621,9 +641,27 @@ private static void updateLocationInfo(
     if (!isInitialConnection) cluster.loadBalancingPolicy().onAdd(host);
   }
 
-  private static void refreshNodeListAndTokenMap(
-      Connection connection,
-      Cluster.Manager cluster,
+  /**
+   * Submits query to the appropriate system.peers table based on whether or not {@link
+   * ProtocolOptions#isAllowHostPortDiscovery()} is enabled.
+   *
+   * @param connection connection to send request on.
+   * @return result of peers query.
+   */
+  private ListenableFuture<ResultSet> selectPeersFuture(final Connection connection) {
+    String query =","[{'comment': 'This is very explicit now, the `peers_v2` is only ever queried if `allowHostPortDiscovery` is used, and if the query fails, cluster initialization fails.', 'commenter': 'tolbertam'}, {'comment': 'One thing to think about here is whether or not we should fall back to the old system peers query if we get an invalid query request back. I think in that specific case it might be sane to fall back on old peers query.', 'commenter': 'GregBestland'}, {'comment': ""I've wavered back and forth on this, my current line of thinking is if the user opts in using  `allowHostPortDiscovery`, they are intending to depend on the presence of the peers_v2 table to tell them what ports to use.  In absence of that table, I think failing is the right thing to do.  Whereas with java driver 4.x, we always check this table first, so in that case it makes sense to downgrade to the peers table."", 'commenter': 'tolbertam'}]"
1065,driver-core/src/main/java/com/datastax/driver/core/Cluster.java,"@@ -724,6 +727,23 @@ public Builder withPort(int port) {
       return this;
     }
 
+    /**
+     * Enables host port discovery using the system.peers_v2 table added in Cassandra 4.0 (via
+     * CASSANDRA-7544). This enables running multiple Cassandra","[{'comment': 'Seems like the last sentence is truncated.', 'commenter': 'adutra'}, {'comment': 'woops, might have accidentally triggered a vim hotkey :laughing: will fix.', 'commenter': 'tolbertam'}]"
1065,driver-core/src/main/java/com/datastax/driver/core/Cluster.java,"@@ -724,6 +727,23 @@ public Builder withPort(int port) {
       return this;
     }
 
+    /**
+     * Enables host port discovery using the system.peers_v2 table added in Cassandra 4.0 (via
+     * CASSANDRA-7544). This enables running multiple Cassandra
+     *
+     * <p>Use of this option only works for clusters running version Cassandra 4.0 or newer. Using
+     * it with an older version of Cassandra will likely cause initialization to fail.
+     *
+     * <p>When using this option configuration provided via {@link #withPort(int)} is unused as
+     * ports are resolved from Cassandra.
+     *
+     * @return this builder.
+     */
+    public Builder allowHostPortDiscovery() {","[{'comment': ""Why expose this? Can't we simply detect the presence of the new peers_v2 table? Or is it because if the cluster has some nodes with peers_v2 and others without, that might be a too complex situation to handle?"", 'commenter': 'adutra'}, {'comment': ""I explain my rationale for this here: https://github.com/datastax/java-driver/pull/1065#discussion_r206263173.  I originally tried peers_v2 first, then peers if the table doesn't exist (like 4.x does).   But thinking about it more I wasn't sure if adding another query in a minor version was the right thing to do, so I thought maybe making this an Opt-in feature on 3.x would be the right way to go."", 'commenter': 'tolbertam'}, {'comment': ""Doesn't sound that bad to add that extra query, since it's done only once at the beginning, and if that fails then we always go directly to the legacy table."", 'commenter': 'olim7t'}, {'comment': 'So are we doing the fallback to `system.peers` at all anymore? The PR description says so...', 'commenter': 'olim7t'}, {'comment': ""I was erring on the side of caution by making this an 'opt-in' feature.  If you don't see that as too risky, I will revert it back to my initial implementation which mirrors the approach we took with 4.x.    With regards to not falling back to `system.peers`, my thought process was that if you explicitly opted in using `allowHostPortDiscovery()`, it didn't seem the backing off to peers was the best idea, since you are expressing that you want to utilize the capability which is not being provided by the server.  If i revert to not making this opt-in that point is moot though.\r\n"", 'commenter': 'tolbertam'}]"
1065,driver-core/src/main/java/com/datastax/driver/core/ControlConnection.java,"@@ -571,20 +585,26 @@ private static void updateInfo(
     // After CASSANDRA-9603 (2.0.17, 2.1.8, 2.2.0 rc2) local row contains one more column:
     // - listen_address
 
-    InetAddress broadcastAddress = null;
+    InetSocketAddress broadcastAddress = null;
     if (row.getColumnDefinitions().contains(""peer"")) { // system.peers
-      broadcastAddress = row.getInet(""peer"");
+      int broadcastPort =","[{'comment': ""Instead of zero, shouldn't we use the default port number (9042)?"", 'commenter': 'adutra'}, {'comment': ""This is for the broadcast/listen address which is typically 7000.  I felt that since we can't know for sure what it's configured to, using 0 would be better."", 'commenter': 'tolbertam'}, {'comment': 'Oh sorry, of course. ðŸ‘ ', 'commenter': 'adutra'}]"
1065,driver-core/src/main/java/com/datastax/driver/core/Host.java,"@@ -194,12 +194,26 @@ public InetSocketAddress getSocketAddress() {
   }
 
   /**
-   * Returns the node broadcast address (that is, the IP by which it should be contacted by other
-   * peers in the cluster), if known.
+   * @return the node broadcast address, if known. Otherwise {@code null}.
+   * @see #getBroadcastSocketAddress()
+   * @see <a
+   *     href=""https://docs.datastax.com/en/cassandra/2.1/cassandra/configuration/configCassandra_yaml_r.html"">The
+   *     cassandra.yaml configuration file</a>
+   */
+  public InetAddress getBroadcastAddress() {","[{'comment': 'This method is now used only in tests. Are you keeping it for binary compatibility? In this case we might want to mark it `@Deprecated` just as you did for `getListenAddress()`.', 'commenter': 'adutra'}, {'comment': 'I originally marked these methods deprecated, but then I backed off when I realized for most users the `InetSocketAddress` version will return port 0.', 'commenter': 'tolbertam'}, {'comment': 'Makes sense but then for consistency maybe we should _not_ mark `getListenAddress()` as deprecated?', 'commenter': 'adutra'}, {'comment': ""D'oh!  Didn't realize that, I'll make them consistent, thanks."", 'commenter': 'tolbertam'}, {'comment': 'We should at least explain that it duplicates information already present in the other method.', 'commenter': 'olim7t'}]"
1065,driver-core/src/main/java/com/datastax/driver/core/Host.java,"@@ -212,13 +226,29 @@ public InetSocketAddress getSocketAddress() {
    *     href=""https://docs.datastax.com/en/cassandra/2.1/cassandra/configuration/configCassandra_yaml_r.html"">The
    *     cassandra.yaml configuration file</a>
    */
-  public InetAddress getBroadcastAddress() {
+  public InetSocketAddress getBroadcastSocketAddress() {
     return broadcastAddress;
   }
 
   /**
-   * Returns the node listen address (that is, the IP the node uses to contact other peers in the
-   * cluster), if known.
+   * @return the node listen address, if known. Otherwise {@code null}.
+   * @deprecated","[{'comment': 'You might want to explain why it is deprecated (""Use #getListenSocketAddress() instead"").', 'commenter': 'adutra'}]"
1065,driver-core/src/main/java/com/datastax/driver/core/ControlConnection.java,"@@ -434,81 +441,94 @@ void refreshNodeListAndTokenMap() {
     }
   }
 
-  private static InetSocketAddress rpcAddressForPeerHost(
+  private static InetSocketAddress nativeAddressForPeerHost(
       Row peersRow, InetSocketAddress connectedHost, Cluster.Manager cluster) {
-
-    // after CASSANDRA-9436, system.peers contains the following inet columns:
-    // - peer: this is actually broadcast_address
-    // - rpc_address: the address we are looking for (this corresponds to broadcast_rpc_address in
-    // the peer's cassandra yaml file;
-    //                if this setting if unset, it defaults to the value for rpc_address or
-    // rpc_interface)
-    // - preferred_ip: used by Ec2MultiRegionSnitch and GossipingPropertyFileSnitch, possibly
-    // others; contents unclear
-
-    InetAddress broadcastAddress = peersRow.getInet(""peer"");
-    InetAddress rpcAddress = peersRow.getInet(""rpc_address"");
-
-    if (broadcastAddress == null) {
-      return null;
-    } else if (broadcastAddress.equals(connectedHost.getAddress())
-        || (rpcAddress != null && rpcAddress.equals(connectedHost.getAddress()))) {
-      // Some DSE versions were inserting a line for the local node in peers (with mostly null
-      // values). This has been fixed, but if we
-      // detect that's the case, ignore it as it's not really a big deal.
-      logger.debug(
-          ""System.peers on node {} has a line for itself. This is not normal but is a known problem of some DSE version. Ignoring the entry."",
-          connectedHost);
-      return null;
-    } else if (rpcAddress == null) {
-      return null;
-    } else if (rpcAddress.equals(bindAllAddress)) {
-      logger.warn(
-          ""Found host with 0.0.0.0 as rpc_address, using broadcast_address ({}) to contact it instead. If this is incorrect you should avoid the use of 0.0.0.0 server side."",
-          broadcastAddress);
-      rpcAddress = broadcastAddress;
+    // if native_address is present, this comes from the peers_v2 table.
+    if (peersRow.getColumnDefinitions().contains(""native_address"")) {
+      InetAddress nativeAddress = peersRow.getInet(""native_address"");
+      int nativePort = peersRow.getInt(""native_port"");
+      return cluster.translateAddress(new InetSocketAddress(nativeAddress, nativePort));
+    } else {
+      // after CASSANDRA-9436, system.peers contains the following inet columns:
+      // - peer: this is actually broadcast_address
+      // - rpc_address: the address we are looking for (this corresponds to broadcast_rpc_address in
+      // the peer's cassandra yaml file;
+      //                if this setting if unset, it defaults to the value for rpc_address or
+      // rpc_interface)
+      // - preferred_ip: used by Ec2MultiRegionSnitch and GossipingPropertyFileSnitch, possibly
+      // others; contents unclear
+      InetAddress broadcastAddress = peersRow.getInet(""peer"");
+      InetAddress rpcAddress = peersRow.getInet(""rpc_address"");
+
+      if (broadcastAddress == null) {
+        return null;
+      } else if (broadcastAddress.equals(connectedHost.getAddress())
+          || (rpcAddress != null && rpcAddress.equals(connectedHost.getAddress()))) {
+        // Some DSE versions were inserting a line for the local node in peers (with mostly null
+        // values). This has been fixed, but if we
+        // detect that's the case, ignore it as it's not really a big deal.
+        logger.debug(
+            ""System.peers on node {} has a line for itself. This is not normal but is a known problem of some DSE version. Ignoring the entry."",
+            connectedHost);
+        return null;
+      } else if (rpcAddress == null) {
+        return null;
+      } else if (rpcAddress.equals(bindAllAddress)) {
+        logger.warn(
+            ""Found host with 0.0.0.0 as rpc_address, using broadcast_address ({}) to contact it instead. If this is incorrect you should avoid the use of 0.0.0.0 server side."",
+            broadcastAddress);
+        rpcAddress = broadcastAddress;
+      }
+      return cluster.translateAddress(rpcAddress);
     }
-    return cluster.translateAddress(rpcAddress);
   }
 
   private Row fetchNodeInfo(Host host, Connection c)
       throws ConnectionException, BusyConnectionException, ExecutionException,
           InterruptedException {
     boolean isConnectedHost = c.address.equals(host.getSocketAddress());
-    if (isConnectedHost || host.getBroadcastAddress() != null) {
+    if (isConnectedHost || host.getBroadcastSocketAddress() != null) {
+      String query;
+      if (isConnectedHost) {
+        query = SELECT_LOCAL;
+      } else {
+        InetSocketAddress broadcastAddress = host.getBroadcastSocketAddress();
+        query =
+            isPeersV2
+                ? SELECT_PEERS_V2
+                    + "" WHERE peer='""
+                    + broadcastAddress.getAddress().getHostAddress()
+                    + ""' AND peer_port=""
+                    + broadcastAddress.getPort()
+                : SELECT_PEERS
+                    + "" WHERE peer='""
+                    + broadcastAddress.getAddress().getHostAddress()
+                    + ""'"";
+      }
       DefaultResultSetFuture future =
-          isConnectedHost
-              ? new DefaultResultSetFuture(
-                  null, cluster.protocolVersion(), new Requests.Query(SELECT_LOCAL))
-              : new DefaultResultSetFuture(
-                  null,
-                  cluster.protocolVersion(),
-                  new Requests.Query(
-                      SELECT_PEERS
-                          + "" WHERE peer='""
-                          + host.getBroadcastAddress().getHostAddress()
-                          + '\''));
+          new DefaultResultSetFuture(null, cluster.protocolVersion(), new Requests.Query(query));
       c.write(future);
       Row row = future.get().one();
       if (row != null) {
         return row;
       } else {
+        InetSocketAddress address = host.getBroadcastSocketAddress();
+        // Don't include full address if port is 0.
+        String addressToUse =
+            address.getPort() != 0 ? address.toString() : address.getAddress().toString();
         logger.debug(
             ""Could not find peer with broadcast address {}, ""
                 + ""falling back to a full system.peers scan to fetch info for {} ""
                 + ""(this can happen if the broadcast address changed)"",
-            host.getBroadcastAddress(),
+            address,
             host);","[{'comment': ""I'm a bit lost here: why do we read `broadcastSocketAddress` again?\r\n`addressToUse` is never used.\r\n`address` is only used in the log message."", 'commenter': 'olim7t'}, {'comment': ""i think this is an artifact of some refactoring, i'll fix this."", 'commenter': 'tolbertam'}]"
1065,driver-core/src/main/java/com/datastax/driver/core/ControlConnection.java,"@@ -594,20 +614,26 @@ private static void updateInfo(
     // After CASSANDRA-9603 (2.0.17, 2.1.8, 2.2.0 rc2) local row contains one more column:
     // - listen_address","[{'comment': 'Maybe update this kind of comments to explain the new rows in `peers_v2` (or remove them / have a single comment detailing the table formats in different versions?).', 'commenter': 'olim7t'}]"
1065,driver-core/src/main/java/com/datastax/driver/core/Host.java,"@@ -44,12 +44,12 @@
   // We use that internally because
   // that's the 'peer' in the 'System.peers' table and avoids querying the full peers table in
   // ControlConnection.refreshNodeInfo.
-  private volatile InetAddress broadcastAddress;
+  private volatile InetSocketAddress broadcastAddress;","[{'comment': 'Could we rename this and the corresponding setter with `Socket`?\r\nRight now we have:\r\n* getBroadcastAddress returning an InetAddress\r\n* setBroadcastAddress taking an InetSocketAddress', 'commenter': 'olim7t'}, {'comment': ""I'll rename the setters to `*SocketAddress` for consistency."", 'commenter': 'tolbertam'}]"
1065,driver-core/src/main/java/com/datastax/driver/core/ControlConnection.java,"@@ -434,81 +443,94 @@ void refreshNodeListAndTokenMap() {
     }
   }
 
-  private static InetSocketAddress rpcAddressForPeerHost(
+  private static InetSocketAddress nativeAddressForPeerHost(
       Row peersRow, InetSocketAddress connectedHost, Cluster.Manager cluster) {
-
-    // after CASSANDRA-9436, system.peers contains the following inet columns:
-    // - peer: this is actually broadcast_address
-    // - rpc_address: the address we are looking for (this corresponds to broadcast_rpc_address in
-    // the peer's cassandra yaml file;
-    //                if this setting if unset, it defaults to the value for rpc_address or
-    // rpc_interface)
-    // - preferred_ip: used by Ec2MultiRegionSnitch and GossipingPropertyFileSnitch, possibly
-    // others; contents unclear
-
-    InetAddress broadcastAddress = peersRow.getInet(""peer"");
-    InetAddress rpcAddress = peersRow.getInet(""rpc_address"");
-
-    if (broadcastAddress == null) {
-      return null;
-    } else if (broadcastAddress.equals(connectedHost.getAddress())
-        || (rpcAddress != null && rpcAddress.equals(connectedHost.getAddress()))) {
-      // Some DSE versions were inserting a line for the local node in peers (with mostly null
-      // values). This has been fixed, but if we
-      // detect that's the case, ignore it as it's not really a big deal.
-      logger.debug(
-          ""System.peers on node {} has a line for itself. This is not normal but is a known problem of some DSE version. Ignoring the entry."",
-          connectedHost);
-      return null;
-    } else if (rpcAddress == null) {
-      return null;
-    } else if (rpcAddress.equals(bindAllAddress)) {
-      logger.warn(
-          ""Found host with 0.0.0.0 as rpc_address, using broadcast_address ({}) to contact it instead. If this is incorrect you should avoid the use of 0.0.0.0 server side."",
-          broadcastAddress);
-      rpcAddress = broadcastAddress;
+    // if native_address is present, this comes from the peers_v2 table.
+    if (peersRow.getColumnDefinitions().contains(""native_address"")) {
+      InetAddress nativeAddress = peersRow.getInet(""native_address"");
+      int nativePort = peersRow.getInt(""native_port"");
+      return cluster.translateAddress(new InetSocketAddress(nativeAddress, nativePort));
+    } else {
+      // after CASSANDRA-9436, system.peers contains the following inet columns:
+      // - peer: this is actually broadcast_address
+      // - rpc_address: the address we are looking for (this corresponds to broadcast_rpc_address in
+      // the peer's cassandra yaml file;
+      //                if this setting if unset, it defaults to the value for rpc_address or
+      // rpc_interface)
+      // - preferred_ip: used by Ec2MultiRegionSnitch and GossipingPropertyFileSnitch, possibly
+      // others; contents unclear
+      InetAddress broadcastAddress = peersRow.getInet(""peer"");
+      InetAddress rpcAddress = peersRow.getInet(""rpc_address"");
+
+      if (broadcastAddress == null) {
+        return null;
+      } else if (broadcastAddress.equals(connectedHost.getAddress())
+          || (rpcAddress != null && rpcAddress.equals(connectedHost.getAddress()))) {
+        // Some DSE versions were inserting a line for the local node in peers (with mostly null
+        // values). This has been fixed, but if we
+        // detect that's the case, ignore it as it's not really a big deal.
+        logger.debug(
+            ""System.peers on node {} has a line for itself. This is not normal but is a known problem of some DSE version. Ignoring the entry."",
+            connectedHost);
+        return null;
+      } else if (rpcAddress == null) {
+        return null;
+      } else if (rpcAddress.equals(bindAllAddress)) {
+        logger.warn(
+            ""Found host with 0.0.0.0 as rpc_address, using broadcast_address ({}) to contact it instead. If this is incorrect you should avoid the use of 0.0.0.0 server side."",
+            broadcastAddress);
+        rpcAddress = broadcastAddress;
+      }
+      return cluster.translateAddress(rpcAddress);
     }
-    return cluster.translateAddress(rpcAddress);
   }
 
   private Row fetchNodeInfo(Host host, Connection c)
       throws ConnectionException, BusyConnectionException, ExecutionException,
           InterruptedException {
     boolean isConnectedHost = c.address.equals(host.getSocketAddress());
-    if (isConnectedHost || host.getBroadcastAddress() != null) {
+    if (isConnectedHost || host.getBroadcastSocketAddress() != null) {
+      String query;
+      if (isConnectedHost) {
+        query = SELECT_LOCAL;
+      } else {
+        InetSocketAddress broadcastAddress = host.getBroadcastSocketAddress();
+        query =
+            isPeersV2
+                ? SELECT_PEERS_V2
+                    + "" WHERE peer='""
+                    + broadcastAddress.getAddress().getHostAddress()
+                    + ""' AND peer_port=""
+                    + broadcastAddress.getPort()
+                : SELECT_PEERS
+                    + "" WHERE peer='""
+                    + broadcastAddress.getAddress().getHostAddress()
+                    + ""'"";","[{'comment': ""We don't handle a missing `peers_v2` here, but that's fine: we do a `refreshNodeListAndTokenMap` every time the control connection (re)connects, and it happens before the new connection is published in `connectionRef`, so we know it will never race with this method.\r\nSo `isPeersV2` will always be set correctly at this point. Even in a mixed 3.x/4.0 cluster, if we initially connected to a 4.0 node, `isPeersV2` will downgrade correctly if we reconnect to a legacy node."", 'commenter': 'olim7t'}]"
1067,driver-core/pom.xml,"@@ -228,7 +228,7 @@
                             <manifestLocation>${project.build.outputDirectory}/META-INF</manifestLocation>
                             <instructions>
                                 <!-- JNR does not provide OSGi bundles, so exclude it; the driver can live without it -->
-                                <Import-Package><![CDATA[com.google.common.*;version=""[16.0.1,22)"",!jnr.*,io.netty.channel.epoll,*]]></Import-Package>
+                                <Import-Package><![CDATA[com.google.common.*;version=""16.0.1"",!jnr.*,io.netty.channel.epoll,*]]></Import-Package>","[{'comment': ""Why did you force the guava version to 16.0.1? Shouldn't this be `[16.0.1,27)`? (Same happens in driver-extras and driver-mapping)."", 'commenter': 'adutra'}, {'comment': 'It\'s not clear, but when you set a specific version it implies ""this version or later"", from [section 3.2.6 of the OSGi core specification](https://osgi.org/specification/osgi.core/7.0.0/framework.module.html#i3189032):\r\n\r\n> If a version range is specified as a single version, it must be interpreted as the range [version,âˆž). The default for a non-specified version range is 0, which maps to [0.0.0,âˆž). ', 'commenter': 'tolbertam'}, {'comment': 'Oh TIL :) Thank you for enlightening me. Agreed that it is much more flexible to specify a single version then.', 'commenter': 'adutra'}]"
1071,driver-core/src/main/java/com/datastax/driver/core/policies/WhiteListPolicy.java,"@@ -61,4 +74,51 @@ public boolean apply(Host host) {
       }
     };
   }
+
+  /**
+   * Creates a new policy with the given host names.
+   *
+   * <p>See {@link #fromHosts(LoadBalancingPolicy, String...)} for more details.
+   */
+  public static WhiteListPolicy fromHosts(","[{'comment': ""I had considered adding a `WhiteListPolicy(LoadBalancingPolicy childPolicy, Collection<String> hostnames)` but that doesn't work because of type erasure.  Having a static method seems reasonable in any case."", 'commenter': 'tolbertam'}, {'comment': ""I like the static factory method but wouldn't `ofHosts` be more meaningful than `fromHosts`?"", 'commenter': 'adutra'}]"
1071,driver-core/src/main/java/com/datastax/driver/core/policies/WhiteListPolicy.java,"@@ -61,4 +74,51 @@ public boolean apply(Host host) {
       }
     };
   }
+
+  /**
+   * Creates a new policy with the given host names.
+   *
+   * <p>See {@link #fromHosts(LoadBalancingPolicy, String...)} for more details.
+   */
+  public static WhiteListPolicy fromHosts(
+      LoadBalancingPolicy childPolicy, Collection<String> hostnames) {
+    return fromHosts(childPolicy, hostnames.toArray(new String[0]));
+  }
+
+  /**
+   * Creates a new policy that wraps the provided child policy but only ""allows"" hosts having
+   * addresses that match those from the resolved input host names.
+   *
+   * <p>Note that all host names must be resolvable; if <em>any</em> of them cannot be resolved,
+   * this method will fail.
+   *
+   * @param childPolicy the wrapped policy.
+   * @param hostnames list of host names to resolve white listed addresses from.","[{'comment': 'Maybe add a note here that the individual names cannot be null?', 'commenter': 'adutra'}]"
1071,driver-core/src/main/java/com/datastax/driver/core/policies/WhiteListPolicy.java,"@@ -61,4 +74,51 @@ public boolean apply(Host host) {
       }
     };
   }
+
+  /**
+   * Creates a new policy with the given host names.
+   *
+   * <p>See {@link #fromHosts(LoadBalancingPolicy, String...)} for more details.
+   */
+  public static WhiteListPolicy fromHosts(
+      LoadBalancingPolicy childPolicy, Collection<String> hostnames) {","[{'comment': 'How about `Iterable<String>` instead of `Collection<String>`? (Then you could simply implement the logic here and make the overloaded method below call this one with `Arrays.asList(hostnames)`.)', 'commenter': 'adutra'}, {'comment': 'I used `Collection` for consistency with `Cluster.Builder.addContactPoints`, but I agree that using `Iterable` is better because it is less specific.', 'commenter': 'tolbertam'}]"
1071,driver-core/src/main/java/com/datastax/driver/core/policies/WhiteListPolicy.java,"@@ -61,4 +74,51 @@ public boolean apply(Host host) {
       }
     };
   }
+
+  /**
+   * Creates a new policy with the given host names.
+   *
+   * <p>See {@link #fromHosts(LoadBalancingPolicy, String...)} for more details.
+   */
+  public static WhiteListPolicy fromHosts(
+      LoadBalancingPolicy childPolicy, Collection<String> hostnames) {
+    return fromHosts(childPolicy, hostnames.toArray(new String[0]));
+  }
+
+  /**
+   * Creates a new policy that wraps the provided child policy but only ""allows"" hosts having
+   * addresses that match those from the resolved input host names.
+   *
+   * <p>Note that all host names must be resolvable; if <em>any</em> of them cannot be resolved,
+   * this method will fail.
+   *
+   * @param childPolicy the wrapped policy.
+   * @param hostnames list of host names to resolve white listed addresses from.
+   * @throws IllegalArgumentException if any of the given {@code hostnames} could not be resolved.
+   * @throws NullPointerException If null was provided for a hostname.
+   * @throws SecurityException if a security manager is present and permission to resolve the host
+   *     name is denied.
+   */
+  public static WhiteListPolicy fromHosts(LoadBalancingPolicy childPolicy, String... hostnames) {
+    final Set<InetAddress> validAddresses = new HashSet<InetAddress>();","[{'comment': ""Shouldn't we use a concurrent/immutable structure here, e.g. `ImmutableSet`? It will be accessed concurrently from within the predicate created below so I think it would be safer to make it thread-safe and immutable."", 'commenter': 'adutra'}, {'comment': 'Makes sense to do that, especially to be consistent with the other predicate implementation.', 'commenter': 'tolbertam'}]"
1073,driver-core/src/main/java/com/datastax/driver/core/Requests.java,"@@ -31,6 +31,9 @@ private Requests() {}
   static class Startup extends Message.Request {
     private static final String CQL_VERSION_OPTION = ""CQL_VERSION"";
     private static final String CQL_VERSION = ""3.0.0"";
+    private static final String DRIVER_VERSION_OPTION = ""DRIVER_VERSION"";
+    private static final String DRIVER_NAME_OPTION = ""DRIVER_NAME"";
+    private static final String DRIVER_NAME = ""DataStax Java Driver for Apache Cassandra"";","[{'comment': 'Should we add `Apache Cassandra (R)` or am I being paranoid?', 'commenter': 'adutra'}, {'comment': 'Follow up: We just chatted about this and decided to just use ""DataStax Java Driver"" for now.  Including ""Apache Cassandra"" is probably over descriptive, since this information will be surfaced in nodetool and virtual table output, so it doesn\'t really add anything.', 'commenter': 'tolbertam'}]"
1074,driver-core/src/main/java/com/datastax/driver/core/Cluster.java,"@@ -1448,119 +1448,125 @@ private Manager(
     // on it so synchronized is good enough.
     synchronized void init() {
       checkNotClosed(this);
-      if (isInit) return;
+      if (isInit) {
+        if (initException == null) {
+          return;
+        }
+        throw new IllegalStateException(","[{'comment': ""We should only end up here if initialization was attempted, and did not complete. This method is synchronized so we don't need to worry about a race condition."", 'commenter': 'GregBestland'}]"
1074,driver-core/src/main/java/com/datastax/driver/core/Cluster.java,"@@ -1722,9 +1729,15 @@ private CloseFuture close() {
         logger.debug(""Shutting down"");
 
         // stop debouncers
-        nodeListRefreshRequestDebouncer.stop();
-        nodeRefreshRequestDebouncer.stop();
-        schemaRefreshRequestDebouncer.stop();
+        if (nodeListRefreshRequestDebouncer != null) {","[{'comment': 'Additional null checks to allow for a partially initialized cluster to still be closed', 'commenter': 'GregBestland'}, {'comment': ""It looks overly paranoid, but I think it's worth it :)"", 'commenter': 'tolbertam'}]"
1074,driver-core/src/test/java/com/datastax/driver/core/ClusterInitTest.java,"@@ -278,6 +278,69 @@ public void should_not_abort_init_if_host_does_not_support_protocol_version() {
       scassandraCluster.stop();
     }
   }
+  /**","[{'comment': ""I didn't add a test for 1211 because is validated implicitly by now catching all runtime execptions in cluster init, and closing the cluster in various states of failed initialization in our other tests."", 'commenter': 'GregBestland'}]"
1074,driver-core/src/main/java/com/datastax/driver/core/Cluster.java,"@@ -1448,119 +1448,125 @@ private Manager(
     // on it so synchronized is good enough.
     synchronized void init() {
       checkNotClosed(this);
-      if (isInit) return;
+      if (isInit) {
+        if (initException == null) {","[{'comment': 'It would be better to move this logic to `checkNotClosed`, as that is used in a number of places and offers the opportunity to notify the user that there was an error during initialization (which is more specific than cluster was closed)', 'commenter': 'tolbertam'}]"
1074,driver-core/src/main/java/com/datastax/driver/core/Cluster.java,"@@ -1448,119 +1448,125 @@ private Manager(
     // on it so synchronized is good enough.
     synchronized void init() {
       checkNotClosed(this);
-      if (isInit) return;
+      if (isInit) {
+        if (initException == null) {
+          return;
+        }
+        throw new IllegalStateException(
+            ""Error during cluster initialization, please close and retry"");","[{'comment': ""Should make sure to include the cause (i.e. `throw new IllegalStateException(..., initException)`.\r\n\r\nAlso the message should be clarified because the user shouldn't need to call `close()`, we should (and it looks like we already) do that for them.  It should instead say something like `Error encountered during cluster initialization, please create a new Cluster instance`."", 'commenter': 'tolbertam'}, {'comment': 'I would even add: ""_This cluster has been closed due to an_ error encountered during its initialization, please create a new Cluster instance."".', 'commenter': 'adutra'}, {'comment': "":+1:, That's even better, makes it clear that it was already closed and errored out, not that the current activity caused it to error out."", 'commenter': 'tolbertam'}]"
1074,driver-core/src/main/java/com/datastax/driver/core/SessionManager.java,"@@ -692,6 +695,9 @@ else if (fetchSize != Integer.MAX_VALUE)
    * and handle host failover.
    */
   void execute(final RequestHandler.Callback callback, final Statement statement) {
+    if (cluster.isClosed()) {","[{'comment': 'It\'s also possible that the Session itself is closed, so we should check that too and surface a specific error in that case.\r\n\r\nThe error message could be something like ""Could not send request, <X> is closed."" where X is either \'cluster\' or \'session\'.', 'commenter': 'tolbertam'}]"
1074,driver-core/src/main/java/com/datastax/driver/core/SessionManager.java,"@@ -137,6 +137,9 @@ public String getLoggedKeyspace() {
 
   @Override
   public ResultSetFuture executeAsync(final Statement statement) {
+    if (cluster.isClosed()) {","[{'comment': ""I think this isn't needed, since this ultimately calls the `execute` method you also have logic for further below.  It's better to just rely on that since it will fail the future instead of the calling thread which is nicer for users since then they don't need to handle exceptions in two different places."", 'commenter': 'tolbertam'}, {'comment': ""Hmm it might be required, `isInit` remains true even after the cluster is closed, and in this case we don't want the flow to reach the if block below."", 'commenter': 'adutra'}, {'comment': 'I added this specifically for isInit case. ', 'commenter': 'GregBestland'}, {'comment': ""If it's still required, we should change it to fail the future, not throw an Exception."", 'commenter': 'tolbertam'}]"
1074,driver-core/src/test/java/com/datastax/driver/core/ClusterInitTest.java,"@@ -278,6 +278,69 @@ public void should_not_abort_init_if_host_does_not_support_protocol_version() {
       scassandraCluster.stop();
     }
   }
+  /**
+   * Ensures that if an error occurs doing initialization that subsequent attempts to use the
+   * cluster result in an appropriate error.
+   *
+   * @jira_ticket JAVA-1220
+   * @test_category host:state
+   */
+  @Test(groups = ""short"")
+  public void should_detect_cluster_init_failure() {
+    Cluster cluster =
+        Cluster.builder()
+            .addContactPointsWithPorts(new InetSocketAddress(""127.0.0.1"", 65534))
+            .withNettyOptions(nonQuietClusterCloseOptions)
+            .build();
+    try {
+      cluster.connect();
+      fail(""Should not have been able to connect."");
+    } catch (NoHostAvailableException e) {
+      try {
+        cluster.connect();
+        fail(""Should error when connect is called."");
+      } catch (IllegalStateException e1) {
+        assertThat(
+            e1.getMessage().equals(""Error during cluster initialization, please close and retry""));
+      }
+    } finally {
+      cluster.close();
+    }
+  }
+  /**
+   * Ensures that if a cluster is closed, subsequent attempts to the use the session will throw a
+   * useful error.
+   *
+   * @jira_ticket JAVA-1929
+   * @test_category host:state
+   */
+  @Test(groups = ""short"")
+  public void session_should_detect_cluster_close() {
+    ScassandraCluster scassandraCluster =
+        ScassandraCluster.builder().withIpPrefix(TestUtils.IP_PREFIX).build();
+    Cluster cluster =
+        Cluster.builder()
+            .addContactPoints(scassandraCluster.address(1).getAddress())
+            .withPort(scassandraCluster.getBinaryPort())
+            .withNettyOptions(nonQuietClusterCloseOptions)
+            .build();
+
+    try {
+      scassandraCluster.init();
+      Session session = cluster.connect();
+      cluster.close();
+      try {
+        session.execute(""SELECTS * FROM system.peers"");
+        fail(
+            ""This error when session.execute is called on session associated with closed cluster."");","[{'comment': 'This is sort of awkwardly phrased, should change to something like:\r\n\r\n""Should have failed if session.execute called when cluster was closed.""', 'commenter': 'tolbertam'}]"
1074,driver-core/src/test/java/com/datastax/driver/core/ClusterInitTest.java,"@@ -278,6 +278,69 @@ public void should_not_abort_init_if_host_does_not_support_protocol_version() {
       scassandraCluster.stop();
     }
   }
+  /**
+   * Ensures that if an error occurs doing initialization that subsequent attempts to use the
+   * cluster result in an appropriate error.
+   *
+   * @jira_ticket JAVA-1220
+   * @test_category host:state
+   */
+  @Test(groups = ""short"")
+  public void should_detect_cluster_init_failure() {","[{'comment': 'This passes even with 3.x in its current state, we should also have a more specific test that would have previously failed.', 'commenter': 'tolbertam'}, {'comment': 'This test will produce the failure that is detailed in JAVA-1220:\r\n\r\n```java\r\n  @Test(groups = ""short"")\r\n  public void should_detect_cluster_init_failure_caused_by_bad_credentials() {\r\n    Cluster cluster =\r\n        Cluster.builder()\r\n            .addContactPoints(""127.0.0.1"")\r\n            .withCredentials(""bad"", ""pass"")\r\n            .withNettyOptions(nonQuietClusterCloseOptions)\r\n            .build();\r\n\r\n    try {\r\n      cluster.connect();\r\n      fail(""Should not have been able to connect."");\r\n    } catch (RuntimeException e) {\r\n      try {\r\n        cluster.connect();\r\n        fail(""Should throw IllegalStateException when attempting to connect again."");\r\n      } catch (IllegalStateException e1) {\r\n        assertThat(\r\n            e1.getMessage().equals(""Error during cluster initialization, please close and retry""));\r\n      }\r\n    } finally {\r\n      cluster.close();\r\n    }\r\n  }\r\n```\r\n\r\nproduces:\r\n\r\n```\r\njava.lang.IllegalArgumentException: initialArraySize cannot be negative but was: -2147483648\r\n\tat com.google.common.collect.CollectPreconditions.checkNonnegative(CollectPreconditions.java:39)\r\n\tat com.google.common.collect.Lists.newArrayListWithCapacity(Lists.java:186)\r\n\tat com.datastax.driver.core.HostConnectionPool.initAsync(HostConnectionPool.java:135)\r\n\tat com.datastax.driver.core.SessionManager.replacePool(SessionManager.java:362)\r\n\tat com.datastax.driver.core.SessionManager.maybeAddPool(SessionManager.java:397)\r\n\tat com.datastax.driver.core.SessionManager.createPools(SessionManager.java:129)\r\n\tat com.datastax.driver.core.SessionManager.initAsync(SessionManager.java:96)\r\n\tat com.datastax.driver.core.Cluster.connectAsync(Cluster.java:378)\r\n\tat com.datastax.driver.core.Cluster.connectAsync(Cluster.java:355)\r\n\tat com.datastax.driver.core.Cluster.connect(Cluster.java:305)\r\n\tat com.datastax.driver.core.ClusterInitTest.should_detect_cluster_init_failure(ClusterInitTest.java:303)\r\n```\r\n\r\nThis would need to be updated to fire up a CCM cluster with authentication (see `AuthenticationTest` for reference).  To keep things simple you could just add this test to `AuthenticationTest`', 'commenter': 'tolbertam'}, {'comment': ""So this actually hits the exact same code path as my test does, just in a different way. I'm going to keep mine as it relies on simulacron rather than auth, and therefore will have a shorter run time. The test above produces the same dump now that I've fixed it."", 'commenter': 'GregBestland'}, {'comment': 'I would still suggest adding my proposed test to `AuthenticationTest` since it produces the root issue of JAVA-1220.   It reuses the running ccm cluster for that test, so it adds minimal time to the overall test suite execution.', 'commenter': 'tolbertam'}]"
1074,driver-core/src/main/java/com/datastax/driver/core/Cluster.java,"@@ -1387,7 +1387,7 @@ private static String generateClusterName() {
     final String clusterName;
     private boolean isInit;
     private volatile boolean isFullyInit;
-
+    private Exception initException;","[{'comment': ""When I read this I thought that a `volatile` keyword would be required for this field, but it turns out that it is already guarded by `this` thanks to the `synchronized` block. \r\n_However_ : out of curiosity I checked usages of `isInit`, and IMO it should be volatile since it is accessed without holding the object's lock in `com.datastax.driver.core.Cluster.Manager.close()`. Wdyt?"", 'commenter': 'adutra'}]"
1074,driver-core/src/test/java/com/datastax/driver/core/AuthenticationTest.java,"@@ -76,12 +76,7 @@ public void should_fail_to_connect_with_wrong_credentials() throws InterruptedEx
                 .withPort(ccm().getBinaryPort())
                 .withCredentials(""bogus"", ""bogus"")
                 .build());
-    try {
-      cluster.connect();
-    } finally {
-      assertThat(cluster.getMetrics().getErrorMetrics().getAuthenticationErrors().getCount())
-          .isEqualTo(1);
-    }
+    cluster.connect();
   }
 ","[{'comment': 'We now close the cluster on initialization errors. This means the cluster will be closed, when we call getMetrics, so this check is no longer applicable.', 'commenter': 'GregBestland'}]"
1074,driver-core/src/main/java/com/datastax/driver/core/SessionManager.java,"@@ -692,6 +699,12 @@ else if (fetchSize != Integer.MAX_VALUE)
    * and handle host failover.
    */
   void execute(final RequestHandler.Callback callback, final Statement statement) {
+    if (this.isClosed()) {
+      throw new IllegalStateException(""Could not send request, session is closed"");
+    }
+    if (cluster.isClosed()) {
+      throw new IllegalStateException(""Could not send request, cluster is closed"");
+    }","[{'comment': ""Closing a cluster also closes all of its sessions automatically, so it would be caught by the first test, we don't need to re-check here."", 'commenter': 'olim7t'}]"
1074,driver-core/src/main/java/com/datastax/driver/core/SessionManager.java,"@@ -137,6 +137,13 @@ public String getLoggedKeyspace() {
 
   @Override
   public ResultSetFuture executeAsync(final Statement statement) {
+    if (this.isClosed()) {
+      throw new IllegalStateException(""Could not send request, session is closed"");
+    }
+
+    if (cluster.isClosed()) {
+      throw new IllegalStateException(""Could not send request, cluster is closed"");
+    }","[{'comment': 'If `!isInit`, we take the second branch below and call `execute()`, which does this again.', 'commenter': 'olim7t'}, {'comment': 'That was my intuition too, but there is some debate about that (see [discussion here](https://github.com/datastax/java-driver/pull/1074#discussion_r208751693)).  I need to test it out but i think you are right.', 'commenter': 'tolbertam'}, {'comment': ""I see, the logic at line 151 addresses `RequestHandler(...).sendRequest()` directly, if it called `execute(future, statement)` instead, we wouldn't need this.   I think we should change that."", 'commenter': 'tolbertam'}]"
1074,driver-core/src/main/java/com/datastax/driver/core/SessionManager.java,"@@ -692,6 +699,12 @@ else if (fetchSize != Integer.MAX_VALUE)
    * and handle host failover.
    */
   void execute(final RequestHandler.Callback callback, final Statement statement) {
+    if (this.isClosed()) {
+      throw new IllegalStateException(""Could not send request, session is closed"");","[{'comment': 'Just a reminder that this should not throw the exception, but rather fail the future (the callback in this case I suppose)', 'commenter': 'tolbertam'}]"
1074,driver-core/src/main/java/com/datastax/driver/core/Cluster.java,"@@ -1720,6 +1722,17 @@ boolean isClosed() {
       return closeFuture.get() != null;
     }
 
+    boolean errorDuringInit() {
+      if (isInit && initException != null) {
+        return true;
+      }
+      return false;","[{'comment': 'This can be simplified as `return (isInit && initException != null);`', 'commenter': 'olim7t'}]"
1074,driver-core/src/main/java/com/datastax/driver/core/SessionManager.java,"@@ -692,6 +692,10 @@ else if (fetchSize != Integer.MAX_VALUE)
    * and handle host failover.
    */
   void execute(final RequestHandler.Callback callback, final Statement statement) {
+    if (this.isClosed()) {
+      callback.onException(
+          null, new IllegalStateException(""Could not send request, session is closed""), 0, 0);
+    }","[{'comment': ""OK now the `isInit == true` branch is not covered in `executeAsync` because it runs the handler directly instead of delegating to this method.\r\n\r\nHow about we move the check to the beginning of the `RequestHandler` constructor? That way it will be done in a single place, and we don't have to worry about the different execution paths in the various session classes (which quite frankly are a mess at this point ðŸ˜’ )."", 'commenter': 'olim7t'}, {'comment': ""Yeah it looks like the change to call execute instead of new'ing up the `RequestHandler` is missing.   Agree, lets just move it to the constructor"", 'commenter': 'tolbertam'}, {'comment': ""I ended up implementing Andy's initial solution. Moving the check into the constructor caused other issues, namely how we would inform the caller of the constructor that there was an issue, and abort.\r\n\r\nEverything should be routed through the execute method now, which should provide a single point to check. I also removed the superfluous check on the parent cluster."", 'commenter': 'GregBestland'}]"
1078,core/src/main/java/com/datastax/oss/driver/internal/core/util/ArrayUtils.java,"@@ -50,27 +52,44 @@
   /**
    * Shuffles the first n elements of the array in-place.
    *
+   * @param elements the array to shuffle.
+   * @param n the number of elements to shuffle; must be {@code <= elements.length}.
    * @see <a
    *     href=""https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle#The_modern_algorithm"">Modern
    *     Fisher-Yates shuffle</a>
    */
-  public static <T> void shuffleHead(T[] elements, int n) {
+  public static <T> void shuffleHead(@NonNull T[] elements, int n) {
+    shuffleHead(elements, n, ThreadLocalRandom.current()::nextInt);","[{'comment': ""There's probably a small overhead to translate the method reference on each call. I'm not sure if it's significant or not, but we can avoid it altogether by passing the `ThreadLocalRandom` itself instead of the method.\r\nThen you can mock it in the test:\r\n```java\r\n    ThreadLocalRandom random = Mockito.mock(ThreadLocalRandom.class);\r\n    Mockito.when(random.nextInt(Mockito.anyInt())).thenAnswer((invocation) -> {\r\n      int i = invocation.getArgument(0);\r\n      return i - 2;\r\n    });\r\n```"", 'commenter': 'olim7t'}]"
1079,test-infra/src/main/java/com/datastax/oss/driver/api/testinfra/ccm/CcmBridge.java,"@@ -327,6 +327,10 @@ private static File createTempStore(String storePath) {
     } catch (IOException e) {
       logger.warn(""Failure to write keystore, SSL-enabled servers may fail to start."", e);
     }
+    // ensure the temp store files are deleted when the JVM exits
+    if (f != null) {","[{'comment': ""This could be in a finally block for the preceding try, although I don't expect runtime exceptions to come out of that block, so this is fine as is too."", 'commenter': 'tolbertam'}, {'comment': 'While at it, I suggest rewriting the whole method:\r\n\r\n```\r\n  private static File createTempStore(String storePath) {\r\n    File f = null;\r\n    try (OutputStream os = new FileOutputStream(f = File.createTempFile(""server"", "".store""))) {\r\n      f.deleteOnExit();\r\n      Resources.copy(CcmBridge.class.getResource(storePath), os);\r\n    } catch (IOException e) {\r\n      logger.warn(""Failure to write keystore, SSL-enabled servers may fail to start."", e);\r\n    }\r\n    return f;\r\n  }\r\n```\r\n`Resources` is actually `com.datastax.oss.driver.shaded.guava.common.io.Resources`.', 'commenter': 'adutra'}]"
1079,test-infra/src/main/java/com/datastax/oss/driver/api/testinfra/ccm/CcmBridge.java,"@@ -354,6 +358,10 @@ private Builder() {
       }
       // disable auto_snapshot by default to reduce disk usage when destroying schema.
       withCassandraConfiguration(""auto_snapshot"", ""false"");
+      // mark the ccm temp directories for deletion when the JVM exits
+      if (this.configDirectory != null) {
+        this.configDirectory.toFile().deleteOnExit();","[{'comment': 'Are we sure this directory will be empty? If not, I suggest doing the following instead:\r\n\r\n```\r\nRuntime.getRuntime()\r\n    .addShutdownHook(\r\n        new Thread(\r\n            () -> {\r\n              try (Stream<Path> walk = Files.walk(configDirectory)) {\r\n                walk.sorted(Comparator.reverseOrder())\r\n                    .map(Path::toFile)\r\n                    .forEach(File::delete);\r\n              } catch (IOException e) {\r\n                logger.error(""Could not walk "" + configDirectory, e);\r\n              }\r\n            }));\r\n```', 'commenter': 'adutra'}, {'comment': ""I'm pretty sure the directory will be empty. It seems that when the tests finish, they execute a `ccm remove` that cleans up everything in the directory, just not the directory itself. I did try to setup a shutdown hook to clean up the directory, but it seems to cause issues where the ccm process hangs at the completion of the test."", 'commenter': 'emerkle826'}]"
1079,test-infra/src/main/java/com/datastax/oss/driver/api/testinfra/ccm/CcmBridge.java,"@@ -314,18 +314,15 @@ public void close() {
    */
   private static File createTempStore(String storePath) {
     File f = null;
-    try (InputStream trustStoreIs = CcmBridge.class.getResourceAsStream(storePath)) {
-      f = File.createTempFile(""server"", "".store"");
-      logger.debug(""Created store file {} for {}."", f, storePath);
-      try (OutputStream trustStoreOs = new FileOutputStream(f)) {
-        byte[] buffer = new byte[1024];
-        int len;
-        while ((len = trustStoreIs.read(buffer)) != -1) {
-          trustStoreOs.write(buffer, 0, len);
-        }
-      }
+    try (OutputStream os = new FileOutputStream(f = File.createTempFile(""server"", "".store""))) {
+      Resources.copy(CcmBridge.class.getResource(storePath), os);
     } catch (IOException e) {
       logger.warn(""Failure to write keystore, SSL-enabled servers may fail to start."", e);
+    } finally {","[{'comment': 'Imo this could be simplified to just `f.deleteOnExit();` placed right after the try block start, since `f` cannot be null at that point. But this way is OK too.', 'commenter': 'adutra'}, {'comment': ""I think it is possible that `f` is null, but only if `createTempFile` throws an IOException or a SecurityException above. It is also possible the file is created, but an exception is thrown creating the OutputStream or doing the `Resources.copy`. Putting it in the finally and ensuring it's not null seemed to be the safest bet."", 'commenter': 'emerkle826'}, {'comment': ""Actually, now that I look at it again, the file is either created or not during `createTempFile`. If that throws an Exception, the file isn't created and doesn't need `deleteOnExit()` called. If it is created, a FileOutputStream is created on top. That either throws a FileNotFoundException or a SecurityException if it fails. It would be very difficult to throw either of those if creating the temp file didn't already fail so...\r\nTL;DR; going to do it the way you suggest. :smiley: "", 'commenter': 'emerkle826'}]"
1082,driver-core/src/main/java/com/datastax/driver/core/Statement.java,"@@ -224,6 +224,9 @@ public abstract ByteBuffer getRoutingKey(
    * method is thus only useful in case you want to punctually override the default policy for this
    * request.
    *
+   * <p>Note that unlike other configuration, when this statement is prepared {@link
+   * BoundStatement}s created off of {@link PreparedStatement} do not inherit this configuration.
+   *","[{'comment': 'It looks like the retry policy is actually propagated: https://github.com/datastax/java-driver/blob/3.x/driver-core/src/main/java/com/datastax/driver/core/AbstractSession.java#L131', 'commenter': 'olim7t'}, {'comment': 'Ah, I missed that somehow, good catch!', 'commenter': 'tolbertam'}]"
1082,driver-core/src/main/java/com/datastax/driver/core/PreparedStatement.java,"@@ -125,7 +125,7 @@
    * Sets a default consistency level for all bound statements created from this prepared statement.
    *
    * <p>If no consistency level is set through this method, the bound statement created from this
-   * object will use the default consistency level (ONE).
+   * object will use the default consistency level (LOCAL_ONE).","[{'comment': ""Not related to this PR, but it was wrong. We'll keep the commit separate."", 'commenter': 'olim7t'}, {'comment': 'Good catch', 'commenter': 'tolbertam'}]"
1082,driver-core/src/main/java/com/datastax/driver/core/Session.java,"@@ -249,9 +245,14 @@
   /**
    * Prepares the provided query.
    *
-   * <p>This method behaves like {@link #prepare(String)}, but note that the resulting {@code
-   * PreparedStatement} will inherit the query properties set on {@code statement}. Concretely, this
-   * means that in the following code:
+   * <p>This method behaves like {@link #prepare(String)}, but the resulting {@code","[{'comment': 'Nice :+1: ', 'commenter': 'tolbertam'}]"
1082,driver-core/src/main/java/com/datastax/driver/core/Session.java,"@@ -15,11 +15,7 @@
  */
 package com.datastax.driver.core;
 
-import com.datastax.driver.core.exceptions.AuthenticationException;
-import com.datastax.driver.core.exceptions.NoHostAvailableException;
-import com.datastax.driver.core.exceptions.QueryExecutionException;
-import com.datastax.driver.core.exceptions.QueryValidationException;
-import com.datastax.driver.core.exceptions.UnsupportedFeatureException;
+import com.datastax.driver.core.exceptions.*;","[{'comment': ""Small nit, google formatter doesn't catch this, but we should not be using wildcard imports."", 'commenter': 'tolbertam'}]"
1083,core/src/main/java/com/datastax/oss/driver/internal/core/type/codec/TimestampCodec.java,"@@ -26,44 +24,164 @@
 import com.datastax.oss.driver.internal.core.util.Strings;
 import edu.umd.cs.findbugs.annotations.NonNull;
 import edu.umd.cs.findbugs.annotations.Nullable;
+import io.netty.util.concurrent.FastThreadLocal;
 import java.nio.ByteBuffer;
+import java.text.ParsePosition;
+import java.text.SimpleDateFormat;
 import java.time.Instant;
-import java.time.ZoneOffset;
-import java.time.format.DateTimeFormatter;
-import java.time.format.DateTimeParseException;
-import java.time.temporal.ChronoField;
+import java.util.Date;
+import java.util.TimeZone;
 import net.jcip.annotations.ThreadSafe;
 
+/**
+ * A codec that handles Apache Cassandra(R)'s timestamp type and maps it to Java's {@link Instant}.
+ *
+ * <p>Implementation notes:
+ *
+ * <ol>
+ *   <li>Because {@code Instant} uses a precision of nanoseconds, whereas the timestamp type uses a
+ *       precision of milliseconds, truncation will happen for any excess precision information as
+ *       though the amount in nanoseconds was subject to integer division by one million.
+ *   <li>For compatibility reasons, this codec uses the legacy {@link SimpleDateFormat} API
+ *       internally when parsing and formatting, and converts from {@link Instant} to {@link Date}
+ *       and vice versa. Specially when parsing, this may yield different results as compared to
+ *       what the newer Java Time API parsers would have produced for the same input.
+ *   <li>Also, {@code Instant} can store points on the time-line further in the future and further
+ *       in the past than {@code Date}. This codec will throw an exception when attempting to parse
+ *       or format an {@code Instant} falling in this category.
+ * </ol>
+ *
+ * <h3>Accepted date-time formats</h3>
+ *
+ * The following patterns are considered valid CQL timestamp literal formats and are thus all
+ * recognized when parsing:
+ *
+ * <ol>
+ *   <li>{@code yyyy-MM-dd'T'HH:mm}
+ *   <li>{@code yyyy-MM-dd'T'HH:mm:ss}
+ *   <li>{@code yyyy-MM-dd'T'HH:mm:ss.SSS}
+ *   <li>{@code yyyy-MM-dd'T'HH:mmX}
+ *   <li>{@code yyyy-MM-dd'T'HH:mmXX}
+ *   <li>{@code yyyy-MM-dd'T'HH:mmXXX}
+ *   <li>{@code yyyy-MM-dd'T'HH:mm:ssX}
+ *   <li>{@code yyyy-MM-dd'T'HH:mm:ssXX}
+ *   <li>{@code yyyy-MM-dd'T'HH:mm:ssXXX}
+ *   <li>{@code yyyy-MM-dd'T'HH:mm:ss.SSSX}
+ *   <li>{@code yyyy-MM-dd'T'HH:mm:ss.SSSXX}
+ *   <li>{@code yyyy-MM-dd'T'HH:mm:ss.SSSXXX}
+ *   <li>{@code yyyy-MM-dd'T'HH:mm z}
+ *   <li>{@code yyyy-MM-dd'T'HH:mm:ss z}
+ *   <li>{@code yyyy-MM-dd'T'HH:mm:ss.SSS z}
+ *   <li>{@code yyyy-MM-dd HH:mm}
+ *   <li>{@code yyyy-MM-dd HH:mm:ss}
+ *   <li>{@code yyyy-MM-dd HH:mm:ss.SSS}
+ *   <li>{@code yyyy-MM-dd HH:mmX}
+ *   <li>{@code yyyy-MM-dd HH:mmXX}
+ *   <li>{@code yyyy-MM-dd HH:mmXXX}
+ *   <li>{@code yyyy-MM-dd HH:mm:ssX}
+ *   <li>{@code yyyy-MM-dd HH:mm:ssXX}
+ *   <li>{@code yyyy-MM-dd HH:mm:ssXXX}
+ *   <li>{@code yyyy-MM-dd HH:mm:ss.SSSX}
+ *   <li>{@code yyyy-MM-dd HH:mm:ss.SSSXX}
+ *   <li>{@code yyyy-MM-dd HH:mm:ss.SSSXXX}
+ *   <li>{@code yyyy-MM-dd HH:mm z}
+ *   <li>{@code yyyy-MM-dd HH:mm:ss z}
+ *   <li>{@code yyyy-MM-dd HH:mm:ss.SSS z}
+ *   <li>{@code yyyy-MM-dd}
+ *   <li>{@code yyyy-MM-ddX}
+ *   <li>{@code yyyy-MM-ddXX}
+ *   <li>{@code yyyy-MM-ddXXX}
+ *   <li>{@code yyyy-MM-dd z}
+ * </ol>
+ *
+ * When parsing, timestamp literals that do not include any time zone information will be
+ * interpreted using the system's {@linkplain TimeZone#getDefault() default time zone}. This is
+ * intended to mimic Apache Cassandra(R)'s own parsing behavior.","[{'comment': ""Maybe we should mention the server-side class for easy reference, `TimestampSerializer` if I'm not mistaken."", 'commenter': 'olim7t'}]"
1083,core/src/main/java/com/datastax/oss/driver/internal/core/type/codec/TimestampCodec.java,"@@ -26,44 +24,164 @@
 import com.datastax.oss.driver.internal.core.util.Strings;
 import edu.umd.cs.findbugs.annotations.NonNull;
 import edu.umd.cs.findbugs.annotations.Nullable;
+import io.netty.util.concurrent.FastThreadLocal;
 import java.nio.ByteBuffer;
+import java.text.ParsePosition;
+import java.text.SimpleDateFormat;
 import java.time.Instant;
-import java.time.ZoneOffset;
-import java.time.format.DateTimeFormatter;
-import java.time.format.DateTimeParseException;
-import java.time.temporal.ChronoField;
+import java.util.Date;
+import java.util.TimeZone;
 import net.jcip.annotations.ThreadSafe;
 
+/**
+ * A codec that handles Apache Cassandra(R)'s timestamp type and maps it to Java's {@link Instant}.
+ *
+ * <p>Implementation notes:
+ *
+ * <ol>
+ *   <li>Because {@code Instant} uses a precision of nanoseconds, whereas the timestamp type uses a
+ *       precision of milliseconds, truncation will happen for any excess precision information as
+ *       though the amount in nanoseconds was subject to integer division by one million.
+ *   <li>For compatibility reasons, this codec uses the legacy {@link SimpleDateFormat} API
+ *       internally when parsing and formatting, and converts from {@link Instant} to {@link Date}
+ *       and vice versa. Specially when parsing, this may yield different results as compared to
+ *       what the newer Java Time API parsers would have produced for the same input.
+ *   <li>Also, {@code Instant} can store points on the time-line further in the future and further
+ *       in the past than {@code Date}. This codec will throw an exception when attempting to parse
+ *       or format an {@code Instant} falling in this category.
+ * </ol>
+ *
+ * <h3>Accepted date-time formats</h3>
+ *
+ * The following patterns are considered valid CQL timestamp literal formats and are thus all
+ * recognized when parsing:
+ *
+ * <ol>
+ *   <li>{@code yyyy-MM-dd'T'HH:mm}
+ *   <li>{@code yyyy-MM-dd'T'HH:mm:ss}
+ *   <li>{@code yyyy-MM-dd'T'HH:mm:ss.SSS}
+ *   <li>{@code yyyy-MM-dd'T'HH:mmX}
+ *   <li>{@code yyyy-MM-dd'T'HH:mmXX}
+ *   <li>{@code yyyy-MM-dd'T'HH:mmXXX}
+ *   <li>{@code yyyy-MM-dd'T'HH:mm:ssX}
+ *   <li>{@code yyyy-MM-dd'T'HH:mm:ssXX}
+ *   <li>{@code yyyy-MM-dd'T'HH:mm:ssXXX}
+ *   <li>{@code yyyy-MM-dd'T'HH:mm:ss.SSSX}
+ *   <li>{@code yyyy-MM-dd'T'HH:mm:ss.SSSXX}
+ *   <li>{@code yyyy-MM-dd'T'HH:mm:ss.SSSXXX}
+ *   <li>{@code yyyy-MM-dd'T'HH:mm z}
+ *   <li>{@code yyyy-MM-dd'T'HH:mm:ss z}
+ *   <li>{@code yyyy-MM-dd'T'HH:mm:ss.SSS z}
+ *   <li>{@code yyyy-MM-dd HH:mm}
+ *   <li>{@code yyyy-MM-dd HH:mm:ss}
+ *   <li>{@code yyyy-MM-dd HH:mm:ss.SSS}
+ *   <li>{@code yyyy-MM-dd HH:mmX}
+ *   <li>{@code yyyy-MM-dd HH:mmXX}
+ *   <li>{@code yyyy-MM-dd HH:mmXXX}
+ *   <li>{@code yyyy-MM-dd HH:mm:ssX}
+ *   <li>{@code yyyy-MM-dd HH:mm:ssXX}
+ *   <li>{@code yyyy-MM-dd HH:mm:ssXXX}
+ *   <li>{@code yyyy-MM-dd HH:mm:ss.SSSX}
+ *   <li>{@code yyyy-MM-dd HH:mm:ss.SSSXX}
+ *   <li>{@code yyyy-MM-dd HH:mm:ss.SSSXXX}
+ *   <li>{@code yyyy-MM-dd HH:mm z}
+ *   <li>{@code yyyy-MM-dd HH:mm:ss z}
+ *   <li>{@code yyyy-MM-dd HH:mm:ss.SSS z}
+ *   <li>{@code yyyy-MM-dd}
+ *   <li>{@code yyyy-MM-ddX}
+ *   <li>{@code yyyy-MM-ddXX}
+ *   <li>{@code yyyy-MM-ddXXX}
+ *   <li>{@code yyyy-MM-dd z}
+ * </ol>
+ *
+ * When parsing, timestamp literals that do not include any time zone information will be
+ * interpreted using the system's {@linkplain TimeZone#getDefault() default time zone}. This is
+ * intended to mimic Apache Cassandra(R)'s own parsing behavior.
+ *
+ * <p>When formatting, the pattern used is always {@code yyyy-MM-dd'T'HH:mm:ss.SSSXXX} and the time
+ * zone is always UTC.","[{'comment': 'Why not use the default time zone for formatting as well? In a way, that would be consistent with CQLSH.', 'commenter': 'olim7t'}, {'comment': ""It's really a matter of taste, as long as the output is unambiguous. OK for the default time zone."", 'commenter': 'adutra'}]"
1083,core/src/main/java/com/datastax/oss/driver/internal/core/type/codec/TimestampCodec.java,"@@ -26,44 +24,164 @@
 import com.datastax.oss.driver.internal.core.util.Strings;
 import edu.umd.cs.findbugs.annotations.NonNull;
 import edu.umd.cs.findbugs.annotations.Nullable;
+import io.netty.util.concurrent.FastThreadLocal;
 import java.nio.ByteBuffer;
+import java.text.ParsePosition;
+import java.text.SimpleDateFormat;
 import java.time.Instant;
-import java.time.ZoneOffset;
-import java.time.format.DateTimeFormatter;
-import java.time.format.DateTimeParseException;
-import java.time.temporal.ChronoField;
+import java.util.Date;
+import java.util.TimeZone;
 import net.jcip.annotations.ThreadSafe;
 
+/**
+ * A codec that handles Apache Cassandra(R)'s timestamp type and maps it to Java's {@link Instant}.
+ *
+ * <p>Implementation notes:
+ *
+ * <ol>
+ *   <li>Because {@code Instant} uses a precision of nanoseconds, whereas the timestamp type uses a
+ *       precision of milliseconds, truncation will happen for any excess precision information as
+ *       though the amount in nanoseconds was subject to integer division by one million.
+ *   <li>For compatibility reasons, this codec uses the legacy {@link SimpleDateFormat} API
+ *       internally when parsing and formatting, and converts from {@link Instant} to {@link Date}
+ *       and vice versa. Specially when parsing, this may yield different results as compared to
+ *       what the newer Java Time API parsers would have produced for the same input.
+ *   <li>Also, {@code Instant} can store points on the time-line further in the future and further
+ *       in the past than {@code Date}. This codec will throw an exception when attempting to parse
+ *       or format an {@code Instant} falling in this category.
+ * </ol>
+ *
+ * <h3>Accepted date-time formats</h3>
+ *
+ * The following patterns are considered valid CQL timestamp literal formats and are thus all
+ * recognized when parsing:
+ *
+ * <ol>
+ *   <li>{@code yyyy-MM-dd'T'HH:mm}
+ *   <li>{@code yyyy-MM-dd'T'HH:mm:ss}
+ *   <li>{@code yyyy-MM-dd'T'HH:mm:ss.SSS}
+ *   <li>{@code yyyy-MM-dd'T'HH:mmX}
+ *   <li>{@code yyyy-MM-dd'T'HH:mmXX}
+ *   <li>{@code yyyy-MM-dd'T'HH:mmXXX}
+ *   <li>{@code yyyy-MM-dd'T'HH:mm:ssX}
+ *   <li>{@code yyyy-MM-dd'T'HH:mm:ssXX}
+ *   <li>{@code yyyy-MM-dd'T'HH:mm:ssXXX}
+ *   <li>{@code yyyy-MM-dd'T'HH:mm:ss.SSSX}
+ *   <li>{@code yyyy-MM-dd'T'HH:mm:ss.SSSXX}
+ *   <li>{@code yyyy-MM-dd'T'HH:mm:ss.SSSXXX}
+ *   <li>{@code yyyy-MM-dd'T'HH:mm z}
+ *   <li>{@code yyyy-MM-dd'T'HH:mm:ss z}
+ *   <li>{@code yyyy-MM-dd'T'HH:mm:ss.SSS z}
+ *   <li>{@code yyyy-MM-dd HH:mm}
+ *   <li>{@code yyyy-MM-dd HH:mm:ss}
+ *   <li>{@code yyyy-MM-dd HH:mm:ss.SSS}
+ *   <li>{@code yyyy-MM-dd HH:mmX}
+ *   <li>{@code yyyy-MM-dd HH:mmXX}
+ *   <li>{@code yyyy-MM-dd HH:mmXXX}
+ *   <li>{@code yyyy-MM-dd HH:mm:ssX}
+ *   <li>{@code yyyy-MM-dd HH:mm:ssXX}
+ *   <li>{@code yyyy-MM-dd HH:mm:ssXXX}
+ *   <li>{@code yyyy-MM-dd HH:mm:ss.SSSX}
+ *   <li>{@code yyyy-MM-dd HH:mm:ss.SSSXX}
+ *   <li>{@code yyyy-MM-dd HH:mm:ss.SSSXXX}
+ *   <li>{@code yyyy-MM-dd HH:mm z}
+ *   <li>{@code yyyy-MM-dd HH:mm:ss z}
+ *   <li>{@code yyyy-MM-dd HH:mm:ss.SSS z}
+ *   <li>{@code yyyy-MM-dd}
+ *   <li>{@code yyyy-MM-ddX}
+ *   <li>{@code yyyy-MM-ddXX}
+ *   <li>{@code yyyy-MM-ddXXX}
+ *   <li>{@code yyyy-MM-dd z}
+ * </ol>
+ *
+ * When parsing, timestamp literals that do not include any time zone information will be
+ * interpreted using the system's {@linkplain TimeZone#getDefault() default time zone}. This is
+ * intended to mimic Apache Cassandra(R)'s own parsing behavior.
+ *
+ * <p>When formatting, the pattern used is always {@code yyyy-MM-dd'T'HH:mm:ss.SSSXXX} and the time
+ * zone is always UTC.
+ */
 @ThreadSafe
 public class TimestampCodec implements TypeCodec<Instant> {
 
-  /** A {@link DateTimeFormatter} that parses (most) of the ISO formats accepted in CQL. */
-  private static final DateTimeFormatter PARSER =
-      new java.time.format.DateTimeFormatterBuilder()
-          .parseCaseSensitive()
-          .parseStrict()
-          .append(DateTimeFormatter.ISO_LOCAL_DATE)
-          .optionalStart()
-          .appendLiteral('T')
-          .appendValue(ChronoField.HOUR_OF_DAY, 2)
-          .appendLiteral(':')
-          .appendValue(ChronoField.MINUTE_OF_HOUR, 2)
-          .optionalEnd()
-          .optionalStart()
-          .appendLiteral(':')
-          .appendValue(ChronoField.SECOND_OF_MINUTE, 2)
-          .optionalEnd()
-          .optionalStart()
-          .appendFraction(ChronoField.NANO_OF_SECOND, 0, 9, true)
-          .optionalEnd()
-          .optionalStart()
-          .appendZoneId()
-          .optionalEnd()
-          .toFormatter()
-          .withZone(ZoneOffset.UTC);
+  /**
+   * Patterns accepted by Apache Cassandra(R) 3.0 and higher when parsing CQL literals.
+   *
+   * <p>Note that Cassandra's TimestampSerializer declares many more patterns but some of them are
+   * equivalent when parsing.
+   */
+  private static final String[] DATE_STRING_PATTERNS =
+      new String[] {
+        // 1) date-time patterns separated by 'T'
+        // (declared first because none of the others are ISO compliant, but some of these are)
+        // 1.a) without time zone
+        ""yyyy-MM-dd'T'HH:mm"",
+        ""yyyy-MM-dd'T'HH:mm:ss"",
+        ""yyyy-MM-dd'T'HH:mm:ss.SSS"",
+        // 1.b) with ISO-8601 time zone
+        ""yyyy-MM-dd'T'HH:mmX"",
+        ""yyyy-MM-dd'T'HH:mmXX"",
+        ""yyyy-MM-dd'T'HH:mmXXX"",
+        ""yyyy-MM-dd'T'HH:mm:ssX"",
+        ""yyyy-MM-dd'T'HH:mm:ssXX"",
+        ""yyyy-MM-dd'T'HH:mm:ssXXX"",
+        ""yyyy-MM-dd'T'HH:mm:ss.SSSX"",
+        ""yyyy-MM-dd'T'HH:mm:ss.SSSXX"",
+        ""yyyy-MM-dd'T'HH:mm:ss.SSSXXX"",
+        // 1.c) with generic time zone
+        ""yyyy-MM-dd'T'HH:mm z"",
+        ""yyyy-MM-dd'T'HH:mm:ss z"",
+        ""yyyy-MM-dd'T'HH:mm:ss.SSS z"",
+        // 2) date-time patterns separated by whitespace
+        // 2.a) without time zone
+        ""yyyy-MM-dd HH:mm"",
+        ""yyyy-MM-dd HH:mm:ss"",
+        ""yyyy-MM-dd HH:mm:ss.SSS"",
+        // 2.b) with ISO-8601 time zone
+        ""yyyy-MM-dd HH:mmX"",
+        ""yyyy-MM-dd HH:mmXX"",
+        ""yyyy-MM-dd HH:mmXXX"",
+        ""yyyy-MM-dd HH:mm:ssX"",
+        ""yyyy-MM-dd HH:mm:ssXX"",
+        ""yyyy-MM-dd HH:mm:ssXXX"",
+        ""yyyy-MM-dd HH:mm:ss.SSSX"",
+        ""yyyy-MM-dd HH:mm:ss.SSSXX"",
+        ""yyyy-MM-dd HH:mm:ss.SSSXXX"",
+        // 2.c) with generic time zone
+        ""yyyy-MM-dd HH:mm z"",
+        ""yyyy-MM-dd HH:mm:ss z"",
+        ""yyyy-MM-dd HH:mm:ss.SSS z"",
+        // 3) date patterns without time
+        // 3.a) without time zone
+        ""yyyy-MM-dd"",
+        // 3.b) with ISO-8601 time zone
+        ""yyyy-MM-ddX"",
+        ""yyyy-MM-ddXX"",
+        ""yyyy-MM-ddXXX"",
+        // 3.c) with generic time zone
+        ""yyyy-MM-dd z""
+      };
+
+  private static final FastThreadLocal<SimpleDateFormat> PARSER =
+      new FastThreadLocal<SimpleDateFormat>() {
+        @Override
+        protected SimpleDateFormat initialValue() {
+          SimpleDateFormat parser = new SimpleDateFormat();
+          parser.setLenient(false);
+          parser.setTimeZone(TimeZone.getDefault());","[{'comment': 'How about making the time zone a constructor argument, with a default constructor that sets it to the default one? This is of no consequence to us and makes life easier if someone wants a codec that uses a different one.', 'commenter': 'olim7t'}, {'comment': 'Good idea, will do.', 'commenter': 'adutra'}]"
1083,core/src/test/java/com/datastax/oss/driver/internal/core/type/codec/TimestampCodecTest.java,"@@ -15,16 +15,33 @@
  */
 package com.datastax.oss.driver.internal.core.type.codec;
 
+import static java.time.ZoneOffset.UTC;
+import static java.time.ZoneOffset.ofHours;
 import static org.assertj.core.api.Assertions.assertThat;
+import static org.assertj.core.api.Assertions.assertThatThrownBy;
 
 import com.datastax.oss.driver.api.core.type.codec.TypeCodecs;
+import com.datastax.oss.driver.shaded.guava.common.collect.Lists;
+import com.tngtech.java.junit.dataprovider.DataProvider;
+import com.tngtech.java.junit.dataprovider.DataProviderRunner;
+import com.tngtech.java.junit.dataprovider.UseDataProvider;
 import java.time.Instant;
+import java.time.LocalDate;
+import java.time.LocalDateTime;
+import java.time.ZoneId;
+import java.time.ZoneOffset;
+import java.time.ZonedDateTime;
+import java.util.TimeZone;
 import org.junit.Test;
+import org.junit.runner.RunWith;
 
+@RunWith(DataProviderRunner.class)
 public class TimestampCodecTest extends CodecTestBase<Instant> {
 
   public TimestampCodecTest() {
-    this.codec = TypeCodecs.TIMESTAMP;
+    // force a given timezone for reproducible results in should_format
+    TimeZone.setDefault(TimeZone.getTimeZone(UTC));","[{'comment': 'I had to add that to make the tests platform-independent.', 'commenter': 'olim7t'}, {'comment': 'Mmm actually I spoke too fast, it still fails on the command line...', 'commenter': 'olim7t'}]"
1086,changelog/README.md,"@@ -9,6 +9,7 @@
 - [improvement] JAVA-1925: Rename context getters
 - [improvement] JAVA-1544: Check API compatibility with Revapi
 - [new feature] JAVA-1900: Add support for virtual tables
+- [new feature] JAVA-1917: Add ability to set node on statement","[{'comment': ""Nit: could you add the ticket at the top of the changelog?\r\n\r\nI changed this in 4.x because it feels more natural to put more recent tickets first, since versions are ordered that way. It's also how it's done in the Cassandra project.\r\n\r\nMaybe we should adopt this convention for the next 3.x versions too for consistency."", 'commenter': 'olim7t'}]"
1086,core/src/main/java/com/datastax/oss/driver/api/core/cql/Statement.java,"@@ -93,6 +94,12 @@
   @NonNull
   T setRoutingKeyspace(@Nullable CqlIdentifier newRoutingKeyspace);
 
+  @NonNull
+  T setNode(@Nullable Node node);
+
+  @Nullable
+  Node getNode();","[{'comment': 'This is a pretty generic feature and it might apply to other types of requests than CQL, so I think `Request` would be a better place for this method (`setNode` can stay here, since `Request` only defines its properties, not how they are set in subtypes).\r\nThe only other implementation is `PrepareRequest`, it should never be targeted so `getNode` will return null.\r\n\r\nCould you also add javadocs? I think [the ones from the 3.x PR](https://github.com/datastax/java-driver/blob/3.x/driver-core/src/main/java/com/datastax/driver/core/Statement.java#L625-L638) can almost be copied as-is (the part about not propagating to prepared statements would be better in the javadocs of `CqlSession#prepare(SimpleStatement)`, which already detail all other fields). ', 'commenter': 'olim7t'}]"
1086,core/src/main/java/com/datastax/oss/driver/api/core/cql/BatchStatementBuilder.java,"@@ -146,6 +146,7 @@ public BatchStatement build() {
         pageSize,
         consistencyLevel,
         serialConsistencyLevel,
-        timeout);
+        timeout,
+        null);","[{'comment': 'The last argument should be `node` (inherited from the parent builder) instead of `null`.', 'commenter': 'olim7t'}]"
1086,core/src/main/java/com/datastax/oss/driver/api/core/cql/StatementBuilder.java,"@@ -199,6 +202,11 @@ public T withTimeout(@Nullable Duration timeout) {
     return self;
   }
 
+  public T withNode(@Nullable Node node) {","[{'comment': ""Could you link to the setter's javadocs like other methods?"", 'commenter': 'olim7t'}]"
1086,core/src/main/java/com/datastax/oss/driver/internal/core/cql/CqlRequestHandlerBase.java,"@@ -153,10 +154,16 @@ protected CqlRequestHandlerBase(
               ? config.getDefaultProfile()
               : config.getProfile(profileName);
     }
-    this.queryPlan =
-        context
-            .getLoadBalancingPolicyWrapper()
-            .newQueryPlan(statement, executionProfile.getName(), session);
+    if (this.statement.getNode() != null) {
+      this.queryPlan = new ConcurrentLinkedQueue<Node>();
+      this.queryPlan.add(this.statement.getNode());
+","[{'comment': 'Since JAVA-1883 there is a dedicated queue implementation for query plans, we can also use it here:\r\n```java\r\nthis.queryPlan = new QueryPlan(this.statement.getNode());\r\n```\r\n', 'commenter': 'olim7t'}]"
1086,core/src/main/java/com/datastax/oss/driver/internal/core/cql/DefaultPreparedStatement.java,"@@ -103,7 +103,6 @@ public DefaultPreparedStatement(
     this.consistencyLevelForBoundStatements = consistencyLevelForBoundStatements;
     this.serialConsistencyLevelForBoundStatements = serialConsistencyLevelForBoundStatements;
     this.areBoundStatementsTracing = areBoundStatementsTracing;
-
     this.codecRegistry = codecRegistry;
     this.protocolVersion = protocolVersion;","[{'comment': 'Nit: could you avoid modifying unrelated lines? This adds noise when you do a git blame on the file later (admittedly not for deleted lines, but still).', 'commenter': 'olim7t'}]"
1086,integration-tests/src/test/java/com/datastax/oss/driver/api/core/loadbalancing/NodeTargetingTestIT.java,"@@ -0,0 +1,115 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.datastax.oss.driver.api.core.loadbalancing;
+
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.unavailable;
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.when;
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.assertj.core.api.Fail.fail;
+
+import com.datastax.oss.driver.api.core.AllNodesFailedException;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.api.core.cql.Statement;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import com.datastax.oss.driver.api.core.servererrors.UnavailableException;
+import com.datastax.oss.driver.api.testinfra.session.SessionRule;
+import com.datastax.oss.driver.api.testinfra.simulacron.SimulacronRule;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import com.datastax.oss.simulacron.common.cluster.ClusterSpec;
+import com.datastax.oss.simulacron.common.codec.ConsistencyLevel;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.List;
+import org.junit.Before;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+@Category(ParallelizableTests.class)
+public class NodeTargetingTestIT {","[{'comment': 'Nit: no need to repeat ""Test"" before ""IT"".', 'commenter': 'olim7t'}]"
1086,integration-tests/src/test/java/com/datastax/oss/driver/api/core/loadbalancing/NodeTargetingTestIT.java,"@@ -0,0 +1,115 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.datastax.oss.driver.api.core.loadbalancing;
+
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.unavailable;
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.when;
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.assertj.core.api.Fail.fail;
+
+import com.datastax.oss.driver.api.core.AllNodesFailedException;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.api.core.cql.Statement;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import com.datastax.oss.driver.api.core.servererrors.UnavailableException;
+import com.datastax.oss.driver.api.testinfra.session.SessionRule;
+import com.datastax.oss.driver.api.testinfra.simulacron.SimulacronRule;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import com.datastax.oss.simulacron.common.cluster.ClusterSpec;
+import com.datastax.oss.simulacron.common.codec.ConsistencyLevel;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.List;
+import org.junit.Before;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+@Category(ParallelizableTests.class)
+public class NodeTargetingTestIT {
+
+  @Rule public SimulacronRule simulacron = new SimulacronRule(ClusterSpec.builder().withNodes(5));
+
+  @Rule public SessionRule<CqlSession> sessionRule = SessionRule.builder(simulacron).build();
+
+  @Before
+  public void clear() {
+    simulacron.cluster().clearLogs();
+    simulacron.cluster().clearPrimes(true);
+  }
+
+  @Test
+  public void should_use_host_on_statement() {
+    Collection<Node> nodeCol = sessionRule.session().getMetadata().getNodes().values();
+    List<Node> nodes = new ArrayList(nodeCol);","[{'comment': '`new ArrayList<>(nodeCol)` to avoid the warning (same in the two other methods).', 'commenter': 'olim7t'}]"
1086,integration-tests/src/test/java/com/datastax/oss/driver/api/core/loadbalancing/NodeTargetingTestIT.java,"@@ -0,0 +1,115 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.datastax.oss.driver.api.core.loadbalancing;
+
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.unavailable;
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.when;
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.assertj.core.api.Fail.fail;
+
+import com.datastax.oss.driver.api.core.AllNodesFailedException;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.api.core.cql.Statement;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import com.datastax.oss.driver.api.core.servererrors.UnavailableException;
+import com.datastax.oss.driver.api.testinfra.session.SessionRule;
+import com.datastax.oss.driver.api.testinfra.simulacron.SimulacronRule;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import com.datastax.oss.simulacron.common.cluster.ClusterSpec;
+import com.datastax.oss.simulacron.common.codec.ConsistencyLevel;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.List;
+import org.junit.Before;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+@Category(ParallelizableTests.class)
+public class NodeTargetingTestIT {
+
+  @Rule public SimulacronRule simulacron = new SimulacronRule(ClusterSpec.builder().withNodes(5));
+
+  @Rule public SessionRule<CqlSession> sessionRule = SessionRule.builder(simulacron).build();
+
+  @Before
+  public void clear() {
+    simulacron.cluster().clearLogs();
+    simulacron.cluster().clearPrimes(true);
+  }
+
+  @Test
+  public void should_use_host_on_statement() {
+    Collection<Node> nodeCol = sessionRule.session().getMetadata().getNodes().values();
+    List<Node> nodes = new ArrayList(nodeCol);
+    for (int i = 0; i < 10; i++) {
+      int hostIndex = i % 4 + 1;
+      Node node = nodes.get(hostIndex);
+
+      // given a statement with host explicitly set.
+      Statement statement = SimpleStatement.newInstance(""select * system.local"").setNode(node);
+
+      // when statement is executed
+      ResultSet result = sessionRule.session().execute(statement);
+
+      Node coordinator = result.getExecutionInfo().getCoordinator();","[{'comment': 'unused variable', 'commenter': 'olim7t'}]"
1086,integration-tests/src/test/java/com/datastax/oss/driver/api/core/loadbalancing/NodeTargetingTestIT.java,"@@ -0,0 +1,115 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.datastax.oss.driver.api.core.loadbalancing;
+
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.unavailable;
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.when;
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.assertj.core.api.Fail.fail;
+
+import com.datastax.oss.driver.api.core.AllNodesFailedException;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.api.core.cql.Statement;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import com.datastax.oss.driver.api.core.servererrors.UnavailableException;
+import com.datastax.oss.driver.api.testinfra.session.SessionRule;
+import com.datastax.oss.driver.api.testinfra.simulacron.SimulacronRule;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import com.datastax.oss.simulacron.common.cluster.ClusterSpec;
+import com.datastax.oss.simulacron.common.codec.ConsistencyLevel;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.List;
+import org.junit.Before;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+@Category(ParallelizableTests.class)
+public class NodeTargetingTestIT {
+
+  @Rule public SimulacronRule simulacron = new SimulacronRule(ClusterSpec.builder().withNodes(5));
+
+  @Rule public SessionRule<CqlSession> sessionRule = SessionRule.builder(simulacron).build();
+
+  @Before
+  public void clear() {
+    simulacron.cluster().clearLogs();
+    simulacron.cluster().clearPrimes(true);
+  }
+
+  @Test
+  public void should_use_host_on_statement() {","[{'comment': 'Nit: 4.x uses the terminology ""node"" exclusively (I did that to avoid any ambiguity with `InetSocketAddress.getHostName`, with CASSANDRA-7544 there can now be multiple nodes on the same host).', 'commenter': 'olim7t'}]"
1086,integration-tests/src/test/java/com/datastax/oss/driver/api/core/loadbalancing/NodeTargetingTestIT.java,"@@ -0,0 +1,115 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.datastax.oss.driver.api.core.loadbalancing;
+
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.unavailable;
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.when;
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.assertj.core.api.Fail.fail;
+
+import com.datastax.oss.driver.api.core.AllNodesFailedException;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.api.core.cql.Statement;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import com.datastax.oss.driver.api.core.servererrors.UnavailableException;
+import com.datastax.oss.driver.api.testinfra.session.SessionRule;
+import com.datastax.oss.driver.api.testinfra.simulacron.SimulacronRule;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import com.datastax.oss.simulacron.common.cluster.ClusterSpec;
+import com.datastax.oss.simulacron.common.codec.ConsistencyLevel;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.List;
+import org.junit.Before;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+@Category(ParallelizableTests.class)
+public class NodeTargetingTestIT {
+
+  @Rule public SimulacronRule simulacron = new SimulacronRule(ClusterSpec.builder().withNodes(5));
+
+  @Rule public SessionRule<CqlSession> sessionRule = SessionRule.builder(simulacron).build();
+
+  @Before
+  public void clear() {
+    simulacron.cluster().clearLogs();
+    simulacron.cluster().clearPrimes(true);
+  }
+
+  @Test
+  public void should_use_host_on_statement() {
+    Collection<Node> nodeCol = sessionRule.session().getMetadata().getNodes().values();
+    List<Node> nodes = new ArrayList(nodeCol);
+    for (int i = 0; i < 10; i++) {
+      int hostIndex = i % 4 + 1;
+      Node node = nodes.get(hostIndex);
+
+      // given a statement with host explicitly set.
+      Statement statement = SimpleStatement.newInstance(""select * system.local"").setNode(node);
+
+      // when statement is executed
+      ResultSet result = sessionRule.session().execute(statement);
+
+      Node coordinator = result.getExecutionInfo().getCoordinator();
+      // then the query should have been sent to the configured host.
+      assertThat(result.getExecutionInfo().getCoordinator()).isEqualTo(node);
+    }
+  }
+
+  @Test
+  public void should_fail_if_host_fails_query() {
+    String query = ""mock"";
+    Collection<Node> nodeCol = sessionRule.session().getMetadata().getNodes().values();
+    List<Node> nodes = new ArrayList(nodeCol);
+    simulacron.cluster().node(3).prime(when(query).then(unavailable(ConsistencyLevel.ALL, 1, 0)));
+
+    // given a statement with a host configured to fail the given query.
+    Node node1 = nodes.get(3);
+    Statement statement = SimpleStatement.newInstance(query).setNode(node1);
+    // when statement is executed an error should be raised.
+    try {
+      ResultSet result = sessionRule.session().execute(statement);","[{'comment': 'No need to store the result in a variable here.', 'commenter': 'olim7t'}]"
1086,integration-tests/src/test/java/com/datastax/oss/driver/api/core/loadbalancing/NodeTargetingTestIT.java,"@@ -0,0 +1,115 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.datastax.oss.driver.api.core.loadbalancing;
+
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.unavailable;
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.when;
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.assertj.core.api.Fail.fail;
+
+import com.datastax.oss.driver.api.core.AllNodesFailedException;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.api.core.cql.Statement;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import com.datastax.oss.driver.api.core.servererrors.UnavailableException;
+import com.datastax.oss.driver.api.testinfra.session.SessionRule;
+import com.datastax.oss.driver.api.testinfra.simulacron.SimulacronRule;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import com.datastax.oss.simulacron.common.cluster.ClusterSpec;
+import com.datastax.oss.simulacron.common.codec.ConsistencyLevel;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.List;
+import org.junit.Before;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+@Category(ParallelizableTests.class)
+public class NodeTargetingTestIT {
+
+  @Rule public SimulacronRule simulacron = new SimulacronRule(ClusterSpec.builder().withNodes(5));
+
+  @Rule public SessionRule<CqlSession> sessionRule = SessionRule.builder(simulacron).build();
+
+  @Before
+  public void clear() {
+    simulacron.cluster().clearLogs();
+    simulacron.cluster().clearPrimes(true);
+  }
+
+  @Test
+  public void should_use_host_on_statement() {
+    Collection<Node> nodeCol = sessionRule.session().getMetadata().getNodes().values();
+    List<Node> nodes = new ArrayList(nodeCol);
+    for (int i = 0; i < 10; i++) {
+      int hostIndex = i % 4 + 1;
+      Node node = nodes.get(hostIndex);
+
+      // given a statement with host explicitly set.
+      Statement statement = SimpleStatement.newInstance(""select * system.local"").setNode(node);
+
+      // when statement is executed
+      ResultSet result = sessionRule.session().execute(statement);
+
+      Node coordinator = result.getExecutionInfo().getCoordinator();
+      // then the query should have been sent to the configured host.
+      assertThat(result.getExecutionInfo().getCoordinator()).isEqualTo(node);
+    }
+  }
+
+  @Test
+  public void should_fail_if_host_fails_query() {
+    String query = ""mock"";
+    Collection<Node> nodeCol = sessionRule.session().getMetadata().getNodes().values();
+    List<Node> nodes = new ArrayList(nodeCol);
+    simulacron.cluster().node(3).prime(when(query).then(unavailable(ConsistencyLevel.ALL, 1, 0)));
+
+    // given a statement with a host configured to fail the given query.
+    Node node1 = nodes.get(3);
+    Statement statement = SimpleStatement.newInstance(query).setNode(node1);
+    // when statement is executed an error should be raised.
+    try {
+      ResultSet result = sessionRule.session().execute(statement);
+      fail(""Should have thrown NoNodeAvailableException"");
+    } catch (AllNodesFailedException e) {
+      assertThat(e.getErrors().size()).isEqualTo(1);
+      assertThat(e.getErrors().get(node1)).isInstanceOf(UnavailableException.class);
+    }
+  }
+
+  @Test
+  public void should_fail_if_host_is_not_connected() {
+    // given a statement with host explicitly set that for which we have no active pool.
+    simulacron.cluster().node(4).close();
+    ;","[{'comment': 'Unnecessary semicolumn here and at line 113.', 'commenter': 'olim7t'}]"
1086,integration-tests/src/test/java/com/datastax/oss/driver/api/core/loadbalancing/NodeTargetingTestIT.java,"@@ -0,0 +1,115 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.datastax.oss.driver.api.core.loadbalancing;
+
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.unavailable;
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.when;
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.assertj.core.api.Fail.fail;
+
+import com.datastax.oss.driver.api.core.AllNodesFailedException;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.api.core.cql.Statement;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import com.datastax.oss.driver.api.core.servererrors.UnavailableException;
+import com.datastax.oss.driver.api.testinfra.session.SessionRule;
+import com.datastax.oss.driver.api.testinfra.simulacron.SimulacronRule;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import com.datastax.oss.simulacron.common.cluster.ClusterSpec;
+import com.datastax.oss.simulacron.common.codec.ConsistencyLevel;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.List;
+import org.junit.Before;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+@Category(ParallelizableTests.class)
+public class NodeTargetingTestIT {
+
+  @Rule public SimulacronRule simulacron = new SimulacronRule(ClusterSpec.builder().withNodes(5));
+
+  @Rule public SessionRule<CqlSession> sessionRule = SessionRule.builder(simulacron).build();
+
+  @Before
+  public void clear() {
+    simulacron.cluster().clearLogs();
+    simulacron.cluster().clearPrimes(true);
+  }
+
+  @Test
+  public void should_use_host_on_statement() {
+    Collection<Node> nodeCol = sessionRule.session().getMetadata().getNodes().values();
+    List<Node> nodes = new ArrayList(nodeCol);
+    for (int i = 0; i < 10; i++) {
+      int hostIndex = i % 4 + 1;
+      Node node = nodes.get(hostIndex);
+
+      // given a statement with host explicitly set.
+      Statement statement = SimpleStatement.newInstance(""select * system.local"").setNode(node);
+
+      // when statement is executed
+      ResultSet result = sessionRule.session().execute(statement);
+
+      Node coordinator = result.getExecutionInfo().getCoordinator();
+      // then the query should have been sent to the configured host.
+      assertThat(result.getExecutionInfo().getCoordinator()).isEqualTo(node);
+    }
+  }
+
+  @Test
+  public void should_fail_if_host_fails_query() {
+    String query = ""mock"";
+    Collection<Node> nodeCol = sessionRule.session().getMetadata().getNodes().values();
+    List<Node> nodes = new ArrayList(nodeCol);
+    simulacron.cluster().node(3).prime(when(query).then(unavailable(ConsistencyLevel.ALL, 1, 0)));
+
+    // given a statement with a host configured to fail the given query.
+    Node node1 = nodes.get(3);
+    Statement statement = SimpleStatement.newInstance(query).setNode(node1);
+    // when statement is executed an error should be raised.
+    try {
+      ResultSet result = sessionRule.session().execute(statement);
+      fail(""Should have thrown NoNodeAvailableException"");
+    } catch (AllNodesFailedException e) {
+      assertThat(e.getErrors().size()).isEqualTo(1);
+      assertThat(e.getErrors().get(node1)).isInstanceOf(UnavailableException.class);
+    }
+  }
+
+  @Test
+  public void should_fail_if_host_is_not_connected() {
+    // given a statement with host explicitly set that for which we have no active pool.
+    simulacron.cluster().node(4).close();
+    ;
+    Collection<Node> nodeCol = sessionRule.session().getMetadata().getNodes().values();
+    List<Node> nodes = new ArrayList(nodeCol);
+    Node node4 = nodes.get(4);
+    Statement statement = SimpleStatement.newInstance(""select * system.local"").setNode(node4);
+    try {
+      // when statement is executed
+      sessionRule.session().execute(statement);
+      fail(""Query should have failed"");
+    } catch (AllNodesFailedException e) {","[{'comment': 'More precisely, it\'s a `NoNodeAvailableException` (a subclass).\r\n\r\nWhich IMO is also more clear for the end user: ""no node was available"" and you targeted a node => the targeted node was not available.', 'commenter': 'olim7t'}]"
1087,core/src/main/java/com/datastax/oss/driver/api/core/session/SessionBuilder.java,"@@ -340,10 +342,17 @@ protected DriverContext buildContext(
         schemaChangeListener,
         requestTracker,
         nodeFilters,
-        classLoader);
+        classLoader,
+        startupOptions);
   }
 
   private static <T> T buildIfNull(T value, Supplier<T> builder) {
     return (value == null) ? builder.get() : value;
   }
+
+  @NonNull
+  public SelfT withStartupOptions(@Nullable Map<String, String> options) {
+    this.startupOptions = options;
+    return self;
+  }","[{'comment': ""Maybe we should call this `with[Additional|Extra|Custom]StartupOptions` to emphasize the fact that CQL version and compression are not required (in fact for compression it is probably even recommended to not mess with it manually, and instead let the driver inspect the configured compressor).\r\n\r\nThis method also needs javadocs to explain all this.\r\n\r\nNit: could we also move it after `withClassLoader`? I know that ultimately method order does not matter, but it's convenient to have all the boilerplate `withXxx` together, and then the more interesting methods."", 'commenter': 'olim7t'}]"
1087,core/src/main/java/com/datastax/oss/driver/internal/core/context/DefaultDriverContext.java,"@@ -228,6 +231,20 @@ public DefaultDriverContext(
             ""requestTracker"", () -> buildRequestTracker(requestTrackerFromBuilder), cycleDetector);
     this.nodeFiltersFromBuilder = nodeFilters;
     this.classLoader = classLoader;
+    this.startupOptions = buildStartupOptions(extraStartupOptions);
+  }
+
+  /**
+   * Builds a map of options to use when sending a Startup message. The options argument passed in
+   * will append to, or overwrite, the Internal default options sent by the driver.
+   */
+  protected Map<String, String> buildStartupOptions(Map<String, String> options) {
+    NullAllowingImmutableMap.Builder<String, String> builder = NullAllowingImmutableMap.builder();
+    builder.putAll(new InternalStartupOptions(this).getOptions());
+    if (options != null) {
+      builder.putAll(options);","[{'comment': ""`NAIM.builder()` does not support duplicate entries. Won't that be a problem if the client passes CQL_VERSION or COMPRESSION in their map?\r\nWe should either prevent that, or check the map to allow the user to override those fields (although the use case is dubious)."", 'commenter': 'olim7t'}]"
1087,core/src/main/java/com/datastax/oss/driver/internal/core/context/InternalStartupOptions.java,"@@ -0,0 +1,70 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.context;
+
+import com.datastax.oss.driver.api.core.MavenCoordinates;
+import com.datastax.oss.driver.internal.core.DefaultMavenCoordinates;
+import com.datastax.oss.protocol.internal.request.Startup;
+import com.datastax.oss.protocol.internal.util.collection.NullAllowingImmutableMap;
+import java.util.Map;
+import net.jcip.annotations.Immutable;
+
+@Immutable
+public class InternalStartupOptions {","[{'comment': 'ðŸ‘ to use a separate utility class, this makes tests easier. But:\r\n\r\n1. I find the name a bit misleading, because in the rest of the code ""startup options"" refers to a `Map<String, String>`.  This shows in `DefaultDriverContext`, where we have a `buildStartupOptions` method that constructs an `InternalStartupOptions`, but eventually returns a map.\r\n2. currently the caller still has to merge the additional options manually, why not make this part of the utility class as well?\r\n\r\nI\'m thinking of turning it into a builder:\r\n```java\r\nMap<String, String> options =\r\n    new StartupOptionsBuilder(context)\r\n        .withAdditionalOptions(additionalOptions)\r\n        .build();\r\n```\r\n`build()` would contain the logic of the current `buildOptions()`, while also avoiding duplicates if they were already provided in `additionalOptions`.', 'commenter': 'olim7t'}]"
1087,core/src/main/java/com/datastax/oss/driver/internal/core/context/DefaultDriverContext.java,"@@ -228,6 +231,20 @@ public DefaultDriverContext(
             ""requestTracker"", () -> buildRequestTracker(requestTrackerFromBuilder), cycleDetector);
     this.nodeFiltersFromBuilder = nodeFilters;
     this.classLoader = classLoader;
+    this.startupOptions = buildStartupOptions(extraStartupOptions);
+  }
+
+  /**
+   * Builds a map of options to use when sending a Startup message. The options argument passed in
+   * will append to, or overwrite, the Internal default options sent by the driver.
+   */
+  protected Map<String, String> buildStartupOptions(Map<String, String> options) {","[{'comment': 'ðŸ‘ to make this protected.', 'commenter': 'olim7t'}]"
1087,core/src/test/java/com/datastax/oss/driver/internal/core/channel/ProtocolInitHandlerTest.java,"@@ -136,34 +129,6 @@ public void should_initialize() {
     assertThat(connectFuture).isSuccess();
   }
 
-  @Test
-  public void should_initialize_with_compression() {
-    Mockito.when(compressor.algorithm()).thenReturn(""lz4"");
-    channel
-        .pipeline()
-        .addLast(
-            ""init"",
-            new ProtocolInitHandler(
-                internalDriverContext,
-                DefaultProtocolVersion.V4,
-                null,
-                DriverChannelOptions.DEFAULT,
-                heartbeatHandler));
-
-    ChannelFuture connectFuture = channel.connect(new InetSocketAddress(""localhost"", 9042));
-
-    Frame requestFrame = readOutboundFrame();
-    assertThat(requestFrame.message).isInstanceOf(Startup.class);
-    Startup startup = (Startup) requestFrame.message;
-
-    // STARTUP message should request compression
-    assertThat(startup.options).containsEntry(""COMPRESSION"", ""lz4"");
-
-    writeInboundFrame(buildInboundFrame(requestFrame, new Ready()));
-    writeInboundFrame(readOutboundFrame(), TestResponses.clusterNameResponse(""someClusterName""));
-    assertThat(connectFuture).isSuccess();
-  }
-","[{'comment': ""ðŸ‘ that makes sense, the only thing this was testing was the options map, and since it's now built outside of the handler, covering it here wouldn't be very useful."", 'commenter': 'olim7t'}]"
1087,core/src/test/java/com/datastax/oss/driver/internal/core/context/DefaultDriverContextTest.java,"@@ -0,0 +1,119 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.context;
+
+import static org.assertj.core.api.Assertions.assertThat;
+
+import com.datastax.oss.driver.api.core.MavenCoordinates;
+import com.datastax.oss.driver.api.core.Version;
+import com.datastax.oss.driver.api.core.config.DriverConfig;
+import com.datastax.oss.driver.api.core.config.DriverConfigLoader;
+import com.datastax.oss.driver.api.core.config.DriverExecutionProfile;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import com.datastax.oss.driver.api.core.metadata.NodeStateListener;
+import com.datastax.oss.driver.api.core.metadata.schema.SchemaChangeListener;
+import com.datastax.oss.driver.api.core.tracker.RequestTracker;
+import com.datastax.oss.driver.api.core.type.codec.TypeCodec;
+import com.datastax.oss.driver.internal.core.DefaultMavenCoordinates;
+import com.datastax.oss.driver.shaded.guava.common.collect.Lists;
+import com.datastax.oss.driver.shaded.guava.common.collect.Maps;
+import com.datastax.oss.protocol.internal.request.Startup;
+import com.datastax.oss.protocol.internal.util.collection.NullAllowingImmutableMap;
+import java.util.List;
+import java.util.Map;
+import java.util.function.Predicate;
+import org.junit.Before;
+import org.junit.Test;
+import org.mockito.Mock;
+import org.mockito.Mockito;
+import org.mockito.MockitoAnnotations;
+
+public class DefaultDriverContextTest {","[{'comment': 'This should be called `InternalStartupOptionsTest` (or the corresponding test name if we rename that class).', 'commenter': 'olim7t'}]"
1087,core/src/test/java/com/datastax/oss/driver/internal/core/context/DefaultDriverContextTest.java,"@@ -0,0 +1,119 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.context;
+
+import static org.assertj.core.api.Assertions.assertThat;
+
+import com.datastax.oss.driver.api.core.MavenCoordinates;
+import com.datastax.oss.driver.api.core.Version;
+import com.datastax.oss.driver.api.core.config.DriverConfig;
+import com.datastax.oss.driver.api.core.config.DriverConfigLoader;
+import com.datastax.oss.driver.api.core.config.DriverExecutionProfile;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import com.datastax.oss.driver.api.core.metadata.NodeStateListener;
+import com.datastax.oss.driver.api.core.metadata.schema.SchemaChangeListener;
+import com.datastax.oss.driver.api.core.tracker.RequestTracker;
+import com.datastax.oss.driver.api.core.type.codec.TypeCodec;
+import com.datastax.oss.driver.internal.core.DefaultMavenCoordinates;
+import com.datastax.oss.driver.shaded.guava.common.collect.Lists;
+import com.datastax.oss.driver.shaded.guava.common.collect.Maps;
+import com.datastax.oss.protocol.internal.request.Startup;
+import com.datastax.oss.protocol.internal.util.collection.NullAllowingImmutableMap;
+import java.util.List;
+import java.util.Map;
+import java.util.function.Predicate;
+import org.junit.Before;
+import org.junit.Test;
+import org.mockito.Mock;
+import org.mockito.Mockito;
+import org.mockito.MockitoAnnotations;
+
+public class DefaultDriverContextTest {
+
+  private final MavenCoordinates driverProperties =
+      DefaultMavenCoordinates.buildFromResource(
+          DefaultDriverContextTest.class.getResource(""/com/datastax/oss/driver/Driver.properties""));
+
+  private DefaultDriverContext defaultDriverContext;
+
+  // Mocks for instantiating the default driver context
+  @Mock private DriverConfigLoader configLoader;
+  List<TypeCodec<?>> typeCodecs = Lists.newArrayList();
+  @Mock private NodeStateListener nodeStateListener;
+  @Mock private SchemaChangeListener schemaChangeListener;
+  @Mock private RequestTracker requestTracker;
+  Map<String, Predicate<Node>> nodeFilters = Maps.newHashMap();
+  @Mock private ClassLoader classLoader;
+  @Mock private DriverConfig driverConfig;
+  @Mock private DriverExecutionProfile defaultProfile;
+
+  @Before
+  public void before() {
+    MockitoAnnotations.initMocks(this);
+    Mockito.when(configLoader.getInitialConfig()).thenReturn(driverConfig);
+    Mockito.when(driverConfig.getDefaultProfile()).thenReturn(defaultProfile);
+    defaultDriverContext =
+        new DefaultDriverContext(
+            configLoader,
+            typeCodecs,
+            nodeStateListener,
+            schemaChangeListener,
+            requestTracker,
+            nodeFilters,
+            classLoader,
+            null);
+  }
+
+  private void assertDefaultStartupOptions(Startup startup) {
+    assertThat(startup.getOptions()).containsEntry(Startup.CQL_VERSION_KEY, ""3.0.0"");
+    assertThat(startup.getOptions())
+        .containsEntry(InternalStartupOptions.DRIVER_NAME_KEY, driverProperties.getName());
+    assertThat(startup.getOptions()).containsKey(InternalStartupOptions.DRIVER_VERSION_KEY);
+    Version version =
+        Version.parse(startup.getOptions().get(InternalStartupOptions.DRIVER_VERSION_KEY));
+    // ensure it's a 4.x version, even if a pre-release
+    assertThat(version).isEqualByComparingTo(driverProperties.getVersion());
+  }
+
+  @Test
+  public void test_default_startup_options() {","[{'comment': 'Nit: could you use the `should_` convention for consistency with the rest of the codebase? For example:\r\n`should_build_minimal_startup_options`\r\n`should_build_startup_options_with_compression`\r\n`should_build_startup_options_with_custom_options`\r\n\r\nAnd I would also cover the cases where the custom options contain one of the default ones or the compression.\r\n', 'commenter': 'olim7t'}, {'comment': 'Yes, I forgot about this from my last PR. I need to break my old habit of test method names.', 'commenter': 'emerkle826'}]"
1087,core/src/main/java/com/datastax/oss/driver/internal/core/context/DefaultDriverContext.java,"@@ -731,4 +744,10 @@ public CodecRegistry getCodecRegistry() {
   public ProtocolVersion getProtocolVersion() {
     return getChannelFactory().getProtocolVersion();
   }
+
+  @NonNull
+  @Override
+  public Map<String, String> getStartupOptions() {
+    return startupOptionsBuilderRef.get().build();","[{'comment': 'The options are now rebuilt on every call. Did you want to leave the door open in case we ever need an option that would change for every new connection?', 'commenter': 'olim7t'}, {'comment': ""No, that wasn't the intent. I imagine the options won't change over the course of normal operation."", 'commenter': 'emerkle826'}]"
1087,core/src/main/java/com/datastax/oss/driver/internal/core/context/StartupOptionsBuilder.java,"@@ -0,0 +1,93 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.context;
+
+import com.datastax.oss.driver.api.core.MavenCoordinates;
+import com.datastax.oss.driver.internal.core.DefaultMavenCoordinates;
+import com.datastax.oss.protocol.internal.request.Startup;
+import com.datastax.oss.protocol.internal.util.collection.NullAllowingImmutableMap;
+import java.util.Map;
+import net.jcip.annotations.Immutable;
+
+@Immutable
+public class StartupOptionsBuilder {
+
+  public static final String DRIVER_NAME_KEY = ""DRIVER_NAME"";
+  public static final String DRIVER_VERSION_KEY = ""DRIVER_VERSION"";
+
+  private static final MavenCoordinates MAVEN_COORDINATES =
+      DefaultMavenCoordinates.buildFromResource(
+          StartupOptionsBuilder.class.getResource(""/com/datastax/oss/driver/Driver.properties""));
+
+  private final InternalDriverContext context;
+
+  public StartupOptionsBuilder(InternalDriverContext context) {
+    this.context = context;
+  }
+
+  /**
+   * Builds a map of options to send in a Startup message.
+   *
+   * <p>The default set of options are built here and include {@link
+   * com.datastax.oss.protocol.internal.request.Startup#COMPRESSION_KEY} (if the context passed in
+   * has a compressor/algorithm set), and the driver's {@link #DRIVER_NAME_KEY} and {@link
+   * #DRIVER_VERSION_KEY}. The {@link com.datastax.oss.protocol.internal.request.Startup}
+   * constructor will add {@link
+   * com.datastax.oss.protocol.internal.request.Startup#CQL_VERSION_KEY}.
+   *
+   * <p>Additional options can be set via {@link #withAdditionalOptions(java.util.Map)}.","[{'comment': 'This is now outdated.\r\nwithAdditionalOptions is referenced in two other comments.', 'commenter': 'olim7t'}, {'comment': 'Good catch!', 'commenter': 'emerkle826'}]"
1090,core/src/main/resources/com/datastax/oss/driver/Driver.properties,"@@ -20,4 +20,5 @@
 driver.groupId=${project.groupId}
 driver.artifactId=${project.artifactId}
 driver.version=${project.version}
-driver.name=DataStax Java driver for Apache Cassandra(R)
+# Use the parent POM for the name
+driver.name=${project.parent.name}","[{'comment': 'The reason why I removed the property and replaced it with a plain text is that this property does not get resolved. If you unpack the final jar and check the file contents you will see the following:\r\n\r\n```\r\ndriver.groupId=com.datastax.oss\r\ndriver.artifactId=java-driver-core\r\ndriver.version=4.0.0-beta2-SNAPSHOT\r\n# Use the parent POM for the name\r\ndriver.name=${project.parent.name}\r\n```\r\n\r\nActually none of the properties declared in the parent project get correctly resolved here. It seems that this is caused by the special packaging type `bundle` as if I switch to `jar`, the property gets resolved just fine.\r\n\r\nWe _could_ simply switch to packaging `jar` but I think that the best thing to do would be to customize the driver name and store that name in a property in the current project. Since `project.name` is already taken and not really suitable, we could just define some other property like `driver.name`:\r\n\r\n```\r\n<properties>\r\n  <driver.name>DataStax Java driver for Apache Cassandra(R)</driver.name>\r\n</properties>\r\n```\r\n\r\nAs a bonus, we could then customize the driver name for the shaded driver by simply changing the property to the following in the shaded pom:\r\n\r\n```\r\n<properties>\r\n  <driver.name>DataStax Java driver for Apache Cassandra(R) with shaded dependencies</driver.name>\r\n</properties>\r\n```\r\n\r\nBut for this to work the following is also required:\r\n\r\n1. Create another `Driver.properties` file under `core-shaded/src/main/resources/com/datastax/oss/driver/Driver.properties`, with the same contents;\r\n2. Place the following under the `build` section of the shaded pom:\r\n```\r\n<resources>\r\n  <resource>\r\n    <directory>src/main/resources</directory>\r\n    <filtering>true</filtering>\r\n  </resource>\r\n</resources>\r\n```\r\n3. Exclude the `Driver.properties` file from the non-shaded module by adding the following to the configuration of the dependency plugin in the shaded pom:\r\n```\r\n<excludes>\r\n  META-INF/maven/com.datastax.oss/java-driver-core/**,\r\n  META-INF/maven/io.netty/**,\r\n  com/datastax/oss/driver/Driver.properties\r\n</excludes>\r\n```\r\n', 'commenter': 'adutra'}, {'comment': ""Thanks for the details @adutra. I did see that you changed the property to a fixed value, but wasn't sure why. This explains it nicely."", 'commenter': 'emerkle826'}]"
1100,CONTRIBUTING.md,"@@ -198,22 +198,29 @@ Add them for all new code, with the exception of:
 Make sure you import the types from `net.jcip`, there are homonyms in the classpath.
 
 
+### Nullability annotations
+
+We use the [Spotbugs annotations](https://spotbugs.github.io) to document nullability of parameters,
+method return types and class members.
+
+Please annotate any new class or interface with the appropriate annotations. Make sure you import 
+the types from `edu.umd.cs.findbugs.annotations`, there are homonyms in the classpath.
+
+
 ## Coding style -- test code
 
 Static imports are permitted in a couple of places:
-* AssertJ's `assertThat` / `fail`.
-* Some Mockito methods, provided that you're already using a non-statically imported method at the
-  beginning of the line. For example:
+* AssertJ methods whose names start with `assertThat` or `fail`.","[{'comment': 'Adding this to also allow static import of `assertThatThrownBy`. There is also `catchThrowable` which is very convenient, not sure if we should include it or not.', 'commenter': 'adutra'}, {'comment': 'At this point we might as well say ""all AssertJ methods"". They\'re pretty recognizable since you know you\'re looking at test code.', 'commenter': 'olim7t'}]"
1100,CONTRIBUTING.md,"@@ -198,22 +198,29 @@ Add them for all new code, with the exception of:
 Make sure you import the types from `net.jcip`, there are homonyms in the classpath.
 
 
+### Nullability annotations
+
+We use the [Spotbugs annotations](https://spotbugs.github.io) to document nullability of parameters,
+method return types and class members.
+
+Please annotate any new class or interface with the appropriate annotations. Make sure you import 
+the types from `edu.umd.cs.findbugs.annotations`, there are homonyms in the classpath.","[{'comment': 'ðŸ‘ , it is unfortunately very easy to make that mistake.\r\n\r\nMaybe we should also name the annotations explicitly, there is also a `@NotNull` in the classpath:\r\n```\r\nPlease annotate any new class or interface with the appropriate annotations: `@NonNull`, `@Nullable`.\r\n```', 'commenter': 'olim7t'}]"
1100,CONTRIBUTING.md,"@@ -198,22 +198,29 @@ Add them for all new code, with the exception of:
 Make sure you import the types from `net.jcip`, there are homonyms in the classpath.
 
 
+### Nullability annotations
+
+We use the [Spotbugs annotations](https://spotbugs.github.io) to document nullability of parameters,
+method return types and class members.
+
+Please annotate any new class or interface with the appropriate annotations. Make sure you import 
+the types from `edu.umd.cs.findbugs.annotations`, there are homonyms in the classpath.
+
+
 ## Coding style -- test code
 
 Static imports are permitted in a couple of places:
-* AssertJ's `assertThat` / `fail`.
-* Some Mockito methods, provided that you're already using a non-statically imported method at the
-  beginning of the line. For example:
+* AssertJ methods whose names start with `assertThat` or `fail`.
+* Mockito methods, e.g.:
   ```java
-  // any and eq are statically imported, it's pretty clear that they at least relate to Mockito
-  Mockito.verify(intCodec).decodePrimitive(any(ByteBuffer.class), eq(ProtocolVersion.DEFAULT));
+  verify(intCodec).decodePrimitive(any(ByteBuffer.class), eq(ProtocolVersion.DEFAULT));","[{'comment': ""Sounds good to me. Again Mockito is pretty easy to recognize from the context once you're familiar with it, and we can take a bit more liberty in test code.\r\n\r\nBut then for consistency we should do a pass on existing code and static import it everywhere. I'm not sure if there's a smart way to do that in IDEA."", 'commenter': 'olim7t'}, {'comment': ""> But then for consistency we should do a pass on existing code and static import it everywhere. I'm not sure if there's a smart way to do that in IDEA.\r\n\r\nYes I was planning to do this in a separate commit once we agree on the guidelines. It will probably be a mix of automatic search-replace and manual editing."", 'commenter': 'adutra'}, {'comment': 'It seems like IntelliJ has a way to achieve this, see [here](https://stackoverflow.com/questions/53614944/replace-import-of-constant-by-static-import-in-intellij-idea).', 'commenter': 'adutra'}]"
1102,integration-tests/src/test/java/com/datastax/oss/driver/api/core/ConnectIT.java,"@@ -98,6 +100,34 @@ public void should_wait_for_contact_points_if_reconnection_enabled() throws Exce
     session.close();
   }
 
+  @Test
+  public void should_cleanup_on_lbp_init_failure() throws Exception {
+    try {
+      DriverConfigLoader loader =
+          SessionUtils.configLoaderBuilder()
+              .without(DefaultDriverOption.LOAD_BALANCING_LOCAL_DATACENTER)
+              .build();
+      CqlSession.builder()
+          .addContactPoints(simulacronRule.getContactPoints())
+          .withConfigLoader(loader)
+          .build();
+      fail(""Should have thrown a DriverException for no DC with explicit contact point"");
+    } catch (DriverException e) {
+    }
+    int size = 1;
+    long maxTime = System.currentTimeMillis() + 1000;
+    // One second should be plenty of time for connections to close server side
+    while (size > 0 && System.currentTimeMillis() < maxTime) {","[{'comment': 'This is a very tight loop, you should at least sleep a little bit at each iteration. Or better yet, replace with:\r\n\r\n``` \r\n    checkThat(() -> simulacronRule.cluster().getConnections().getConnections().isEmpty())\r\n        .before(1, SECONDS)\r\n        .becomesTrue();\r\n``` ', 'commenter': 'adutra'}]"
1102,integration-tests/src/test/java/com/datastax/oss/driver/api/core/ConnectIT.java,"@@ -98,6 +100,34 @@ public void should_wait_for_contact_points_if_reconnection_enabled() throws Exce
     session.close();
   }
 
+  @Test
+  public void should_cleanup_on_lbp_init_failure() throws Exception {
+    try {
+      DriverConfigLoader loader =
+          SessionUtils.configLoaderBuilder()
+              .without(DefaultDriverOption.LOAD_BALANCING_LOCAL_DATACENTER)
+              .build();
+      CqlSession.builder()
+          .addContactPoints(simulacronRule.getContactPoints())
+          .withConfigLoader(loader)
+          .build();
+      fail(""Should have thrown a DriverException for no DC with explicit contact point"");
+    } catch (DriverException e) {
+    }
+    int size = 1;
+    long maxTime = System.currentTimeMillis() + 1000;
+    // One second should be plenty of time for connections to close server side
+    while (size > 0 && System.currentTimeMillis() < maxTime) {
+      ClusterConnectionReport report = simulacronRule.cluster().getConnections();
+      size = report.getConnections().size();
+    }
+    assertThat(size).isEqualTo(0);
+  }
+
+  private static String provideNullString() {","[{'comment': 'Unused.', 'commenter': 'adutra'}]"
1102,integration-tests/src/test/java/com/datastax/oss/driver/api/core/ConnectIT.java,"@@ -98,6 +100,34 @@ public void should_wait_for_contact_points_if_reconnection_enabled() throws Exce
     session.close();
   }
 
+  @Test
+  public void should_cleanup_on_lbp_init_failure() throws Exception {
+    try {
+      DriverConfigLoader loader =
+          SessionUtils.configLoaderBuilder()
+              .without(DefaultDriverOption.LOAD_BALANCING_LOCAL_DATACENTER)
+              .build();
+      CqlSession.builder()
+          .addContactPoints(simulacronRule.getContactPoints())
+          .withConfigLoader(loader)
+          .build();
+      fail(""Should have thrown a DriverException for no DC with explicit contact point"");
+    } catch (DriverException e) {","[{'comment': 'Can you rename the exception to `ignored` to make IntelliJ happy? :)', 'commenter': 'adutra'}]"
1102,integration-tests/src/test/java/com/datastax/oss/driver/api/core/ConnectIT.java,"@@ -98,6 +100,34 @@ public void should_wait_for_contact_points_if_reconnection_enabled() throws Exce
     session.close();
   }
 
+  @Test
+  public void should_cleanup_on_lbp_init_failure() throws Exception {","[{'comment': 'Please add a little note in the javadocs of this method to link this test to JAVA-1948 for future reference.', 'commenter': 'adutra'}]"
1115,core/src/main/java/com/datastax/oss/driver/internal/core/cql/MultiPageResultSet.java,"@@ -95,6 +107,10 @@ private void maybeMoveToNextPage() {
       }
     }
 
+    private boolean isFullyFetched() {
+      return currentPage.hasMorePages();","[{'comment': 'Should be `!currentPage.hasMorePages()`.', 'commenter': 'adutra'}]"
1118,core/src/main/java/com/datastax/oss/driver/internal/core/DefaultConsistencyLevelRegistry.java,"@@ -18,26 +18,36 @@
 import com.datastax.oss.driver.api.core.ConsistencyLevel;
 import com.datastax.oss.driver.api.core.DefaultConsistencyLevel;
 import com.datastax.oss.driver.shaded.guava.common.collect.ImmutableList;
+import com.datastax.oss.driver.shaded.guava.common.collect.ImmutableMap;
 import net.jcip.annotations.ThreadSafe;
 
 @ThreadSafe
 public class DefaultConsistencyLevelRegistry implements ConsistencyLevelRegistry {
 
-  private static final ImmutableList<ConsistencyLevel> values =
+  private static final ImmutableList<ConsistencyLevel> VALUES =
       ImmutableList.<ConsistencyLevel>builder().add(DefaultConsistencyLevel.values()).build();
+  private static final ImmutableMap<String, Integer> NAME_TO_CODE;
+
+  static {
+    ImmutableMap.Builder<String, Integer> nameToCodeBuilder = ImmutableMap.builder();
+    for (DefaultConsistencyLevel consistencyLevel : DefaultConsistencyLevel.values()) {
+      nameToCodeBuilder.put(consistencyLevel.name(), consistencyLevel.getProtocolCode());
+    }
+    NAME_TO_CODE = nameToCodeBuilder.build();
+  }
 
   @Override
   public ConsistencyLevel fromCode(int code) {
     return DefaultConsistencyLevel.fromCode(code);
   }
 
   @Override
-  public ConsistencyLevel fromName(String name) {
-    return DefaultConsistencyLevel.valueOf(name);
+  public int toCode(String name) {
+    return NAME_TO_CODE.get(name);","[{'comment': ""This is the most significant change in this PR.\r\nFor the record I've tried immutable map, hash map and an explicit switch statement in the method, no discernable difference between the 3."", 'commenter': 'olim7t'}, {'comment': ""There is now a slight dissymmetry between methods `fromCode` and `toCode` and I don't see why you had to introduce it; couldn't `toCode` simply keep returning the `ConsistencyLevel` instance itself (just as `fromCode` does)? (In which case, the old name `fromName` would be a better name for it)."", 'commenter': 'adutra'}, {'comment': 'The method is only called from `Conversions`, and (String => int) is what we really need there. Previously it would convert to the enum and immediately call `getProtocolCode()` on it, which is an unnecessary extra step (String => ConsistencyLevel => int).\r\n\r\nAs for the names I see what you mean, this is confusing so maybe we should try to be more explicit:\r\n```java\r\n  ConsistencyLevel codeToLevel(int code);\r\n  int nameTocode(String name);\r\n```', 'commenter': 'olim7t'}, {'comment': 'I see, we are really trying to squeeze up to the last drop of performance here. OK then for the method names suggested in your last comment above.', 'commenter': 'adutra'}]"
1123,manual/core/time_types/README.md,"@@ -0,0 +1,113 @@
+## Time types","[{'comment': 'Wouldn\'t ""Temporal types"" sound more appropriate?', 'commenter': 'adutra'}]"
1123,manual/core/time_types/README.md,"@@ -0,0 +1,113 @@
+## Time types
+
+This page provides more details about the various CQL time types, and the Java types they are mapped
+to in the driver.
+
+### Date and time
+
+CQL types `date` and `time` map directly to `java.time.LocalDate` and `java.time.LocalTime`.
+
+These are simple, time-zone-free representations of date-only (`yyyy-mm-dd`) and time-only
+(`HH:MM:SS\[.fff]`) types.
+
+### Timestamp
+
+CQL type `timestamp` is the date-and-time representation, stored as a number of milliseconds since
+the epoch (01/01/1970 UTC).
+ 
+ 
+#### No time zone
+
+`timestamp` does **not** store a time zone. This is not always obvious because clients generally do
+use one for display. For instance, the following CQLSH snippet is from a machine in Pacific time: 
+
+```
+cqlsh> CREATE TABLE test(t timestamp PRIMARY KEY);
+cqlsh> INSERT INTO test (t) VALUES (dateof(now()));
+cqlsh> SELECT * FROM test;
+
+ t
+---------------------------------
+ 2018-11-07 08:50:52.433000-0800
+```
+
+It looks like the timestamp has a zone (`-0800`), but it is actually the client's. If you force
+CQLSH to a different zone and observe the same data, it will be displayed differently:
+
+```
+$ TZ=UTC cqlsh
+cqlsh> SELECT * FROM test;
+
+ t
+---------------------------------
+ 2018-11-07 16:50:52.433000+0000
+```
+
+Internally, Cassandra only stores the raw number of milliseconds. You can observe that with a cast:  ","[{'comment': 'Congrats, this explanatory section is extremely helpful imo.', 'commenter': 'adutra'}]"
1123,manual/core/time_types/README.md,"@@ -0,0 +1,113 @@
+## Time types
+
+This page provides more details about the various CQL time types, and the Java types they are mapped
+to in the driver.
+
+### Date and time
+
+CQL types `date` and `time` map directly to `java.time.LocalDate` and `java.time.LocalTime`.
+
+These are simple, time-zone-free representations of date-only (`yyyy-mm-dd`) and time-only
+(`HH:MM:SS\[.fff]`) types.
+
+### Timestamp
+
+CQL type `timestamp` is the date-and-time representation, stored as a number of milliseconds since
+the epoch (01/01/1970 UTC).
+ 
+ 
+#### No time zone
+
+`timestamp` does **not** store a time zone. This is not always obvious because clients generally do
+use one for display. For instance, the following CQLSH snippet is from a machine in Pacific time: 
+
+```
+cqlsh> CREATE TABLE test(t timestamp PRIMARY KEY);
+cqlsh> INSERT INTO test (t) VALUES (dateof(now()));
+cqlsh> SELECT * FROM test;
+
+ t
+---------------------------------
+ 2018-11-07 08:50:52.433000-0800
+```
+
+It looks like the timestamp has a zone (`-0800`), but it is actually the client's. If you force
+CQLSH to a different zone and observe the same data, it will be displayed differently:
+
+```
+$ TZ=UTC cqlsh
+cqlsh> SELECT * FROM test;
+
+ t
+---------------------------------
+ 2018-11-07 16:50:52.433000+0000
+```
+
+Internally, Cassandra only stores the raw number of milliseconds. You can observe that with a cast:  
+
+```
+cqlsh> SELECT cast(t as bigint) FROM test;
+
+ cast(t as bigint)
+-------------------
+     1541609452433
+```
+
+#### Java equivalent
+
+By default, the driver maps `timestamp` to `java.time.Instant`. This Java type is the closest to the
+internal representation; in particular, it does not have a time zone. On the downside, this means
+you can't directly extract calendar fields (year, month, etc.). You need to call `atZone` to perform
+the conversion: 
+
+```java
+Row row = session.execute(""SELECT t FROM test"").one();
+Instant instant = row.getInstant(""t"");
+ZonedDateTime dateTime = instant.atZone(ZoneId.of(""America/Los_Angeles""));
+System.out.println(dateTime.getYear());
+```
+
+Conversely, you can convert a `ZonedDateTime` back to an `Instant` with `toInstant`.
+
+If you want to automate those `atZone`/`toInstant` conversions, the driver comes with an optional
+`ZonedDateTime` codec, that must be registered explicitly with the session:
+
+```java
+CqlSession session = CqlSession.builder()
+    .addTypeCodecs(TypeCodecs.ZONED_TIMESTAMP_UTC)
+    .build();
+
+Row row = session.execute(""SELECT t FROM test"").one();
+ZonedDateTime dateTime = row.get(""t"", GenericType.ZONED_DATE_TIME);
+``` 
+
+There are various constants and methods to obtain a codec instance for a particular zone:
+
+* [TypeCodecs.ZONED_TIMESTAMP_SYSTEM]\: system default;
+* [TypeCodecs.ZONED_TIMESTAMP_UTC]\: UTC;
+* [TypeCodecs.zonedTimestampAt()]\: user-provided.
+
+Which zone you choose is application-dependent. The driver doesn't map to `ZonedDateTime` by default
+because it would have to make a arbitrary choice; we want you to think about time zones explicitly","[{'comment': 'Small typo: ""...to make _an_ arbitrary choice"".', 'commenter': 'adutra'}]"
1123,manual/core/time_types/README.md,"@@ -0,0 +1,113 @@
+## Time types
+
+This page provides more details about the various CQL time types, and the Java types they are mapped
+to in the driver.
+
+### Date and time
+
+CQL types `date` and `time` map directly to `java.time.LocalDate` and `java.time.LocalTime`.
+
+These are simple, time-zone-free representations of date-only (`yyyy-mm-dd`) and time-only
+(`HH:MM:SS\[.fff]`) types.
+
+### Timestamp
+
+CQL type `timestamp` is the date-and-time representation, stored as a number of milliseconds since
+the epoch (01/01/1970 UTC).
+ 
+ 
+#### No time zone
+
+`timestamp` does **not** store a time zone. This is not always obvious because clients generally do
+use one for display. For instance, the following CQLSH snippet is from a machine in Pacific time: 
+
+```
+cqlsh> CREATE TABLE test(t timestamp PRIMARY KEY);
+cqlsh> INSERT INTO test (t) VALUES (dateof(now()));
+cqlsh> SELECT * FROM test;
+
+ t
+---------------------------------
+ 2018-11-07 08:50:52.433000-0800
+```
+
+It looks like the timestamp has a zone (`-0800`), but it is actually the client's. If you force
+CQLSH to a different zone and observe the same data, it will be displayed differently:
+
+```
+$ TZ=UTC cqlsh
+cqlsh> SELECT * FROM test;
+
+ t
+---------------------------------
+ 2018-11-07 16:50:52.433000+0000
+```
+
+Internally, Cassandra only stores the raw number of milliseconds. You can observe that with a cast:  
+
+```
+cqlsh> SELECT cast(t as bigint) FROM test;
+
+ cast(t as bigint)
+-------------------
+     1541609452433
+```
+
+#### Java equivalent
+
+By default, the driver maps `timestamp` to `java.time.Instant`. This Java type is the closest to the
+internal representation; in particular, it does not have a time zone. On the downside, this means
+you can't directly extract calendar fields (year, month, etc.). You need to call `atZone` to perform
+the conversion: 
+
+```java
+Row row = session.execute(""SELECT t FROM test"").one();
+Instant instant = row.getInstant(""t"");
+ZonedDateTime dateTime = instant.atZone(ZoneId.of(""America/Los_Angeles""));
+System.out.println(dateTime.getYear());
+```
+
+Conversely, you can convert a `ZonedDateTime` back to an `Instant` with `toInstant`.
+
+If you want to automate those `atZone`/`toInstant` conversions, the driver comes with an optional
+`ZonedDateTime` codec, that must be registered explicitly with the session:
+
+```java
+CqlSession session = CqlSession.builder()
+    .addTypeCodecs(TypeCodecs.ZONED_TIMESTAMP_UTC)
+    .build();
+
+Row row = session.execute(""SELECT t FROM test"").one();
+ZonedDateTime dateTime = row.get(""t"", GenericType.ZONED_DATE_TIME);
+``` 
+
+There are various constants and methods to obtain a codec instance for a particular zone:
+
+* [TypeCodecs.ZONED_TIMESTAMP_SYSTEM]\: system default;
+* [TypeCodecs.ZONED_TIMESTAMP_UTC]\: UTC;
+* [TypeCodecs.zonedTimestampAt()]\: user-provided.
+
+Which zone you choose is application-dependent. The driver doesn't map to `ZonedDateTime` by default
+because it would have to make a arbitrary choice; we want you to think about time zones explicitly
+for predictable results.
+
+#### Millisecond-only precision
+
+As already stated, `timestamp` is stored as a number of milliseconds. If you try to write an
+`Instant` or `ZonedDateTime` with higher precision through the driver, the sub-millisecond part will
+be truncated.","[{'comment': ""Maybe add an example of such truncation here? I'm not sure this is clear for all users."", 'commenter': 'adutra'}]"
1123,manual/core/time_types/README.md,"@@ -0,0 +1,113 @@
+## Time types
+
+This page provides more details about the various CQL time types, and the Java types they are mapped
+to in the driver.
+
+### Date and time
+
+CQL types `date` and `time` map directly to `java.time.LocalDate` and `java.time.LocalTime`.
+
+These are simple, time-zone-free representations of date-only (`yyyy-mm-dd`) and time-only
+(`HH:MM:SS\[.fff]`) types.
+
+### Timestamp
+
+CQL type `timestamp` is the date-and-time representation, stored as a number of milliseconds since
+the epoch (01/01/1970 UTC).
+ 
+ 
+#### No time zone
+
+`timestamp` does **not** store a time zone. This is not always obvious because clients generally do
+use one for display. For instance, the following CQLSH snippet is from a machine in Pacific time: 
+
+```
+cqlsh> CREATE TABLE test(t timestamp PRIMARY KEY);
+cqlsh> INSERT INTO test (t) VALUES (dateof(now()));
+cqlsh> SELECT * FROM test;
+
+ t
+---------------------------------
+ 2018-11-07 08:50:52.433000-0800
+```
+
+It looks like the timestamp has a zone (`-0800`), but it is actually the client's. If you force
+CQLSH to a different zone and observe the same data, it will be displayed differently:
+
+```
+$ TZ=UTC cqlsh
+cqlsh> SELECT * FROM test;
+
+ t
+---------------------------------
+ 2018-11-07 16:50:52.433000+0000
+```
+
+Internally, Cassandra only stores the raw number of milliseconds. You can observe that with a cast:  
+
+```
+cqlsh> SELECT cast(t as bigint) FROM test;
+
+ cast(t as bigint)
+-------------------
+     1541609452433
+```
+
+#### Java equivalent
+
+By default, the driver maps `timestamp` to `java.time.Instant`. This Java type is the closest to the
+internal representation; in particular, it does not have a time zone. On the downside, this means
+you can't directly extract calendar fields (year, month, etc.). You need to call `atZone` to perform
+the conversion: 
+
+```java
+Row row = session.execute(""SELECT t FROM test"").one();
+Instant instant = row.getInstant(""t"");
+ZonedDateTime dateTime = instant.atZone(ZoneId.of(""America/Los_Angeles""));
+System.out.println(dateTime.getYear());
+```
+
+Conversely, you can convert a `ZonedDateTime` back to an `Instant` with `toInstant`.
+
+If you want to automate those `atZone`/`toInstant` conversions, the driver comes with an optional
+`ZonedDateTime` codec, that must be registered explicitly with the session:
+
+```java
+CqlSession session = CqlSession.builder()
+    .addTypeCodecs(TypeCodecs.ZONED_TIMESTAMP_UTC)
+    .build();
+
+Row row = session.execute(""SELECT t FROM test"").one();
+ZonedDateTime dateTime = row.get(""t"", GenericType.ZONED_DATE_TIME);
+``` 
+
+There are various constants and methods to obtain a codec instance for a particular zone:
+
+* [TypeCodecs.ZONED_TIMESTAMP_SYSTEM]\: system default;
+* [TypeCodecs.ZONED_TIMESTAMP_UTC]\: UTC;
+* [TypeCodecs.zonedTimestampAt()]\: user-provided.
+
+Which zone you choose is application-dependent. The driver doesn't map to `ZonedDateTime` by default
+because it would have to make a arbitrary choice; we want you to think about time zones explicitly
+for predictable results.
+
+#### Millisecond-only precision
+
+As already stated, `timestamp` is stored as a number of milliseconds. If you try to write an
+`Instant` or `ZonedDateTime` with higher precision through the driver, the sub-millisecond part will
+be truncated.
+
+### Duration
+
+CQL type `duration` represents a period in months, days and nanoseconds. The driver maps it to a
+custom type: [CqlDuration].
+
+We deliberately avoided `java.time.Duration`, because it represents an absolute time-based amount,
+regardless of the calendar. In contrast, a `CqlDuration` of ""2 months"" represents a different amount","[{'comment': 'Suggestion: ""We deliberately avoided `java.time.Period`, because it does not contain a nanoseconds part as `CqlDuration` does; and we also avoided `java.time.Duration`, because it represents an absolute time-based amount, regardless of the calendar, whereas `CqlDuration` manipulates conceptual days and months instead. Thus a `CqlDuration` of ""2 months"" represents a different amount of time depending on the date to which it is applied  (because months have a different number of days, and because daylight savings rules might also apply, etc).', 'commenter': 'adutra'}]"
1123,manual/core/README.md,"@@ -179,35 +179,34 @@ See [AccessibleByName] for an explanation of the conversion rules.
 
 ##### CQL to Java type mapping
 
-<table border=""1"" style=""text-align:center; width:100%;margin-bottom:1em;"">
-    <tr> <td><b>CQL3 data type</b></td> <td><b>Getter name</b></td> <td><b>Java type</b></td> </tr>
-    <tr> <td>ascii</td> <td>getString</td> <td>java.lang.String</td> </tr>
-    <tr> <td>bigint</td> <td>getLong</td> <td>long</td> </tr>
-    <tr> <td>blob</td> <td>getBytes</td> <td>java.nio.ByteBuffer</td> </tr>
-    <tr> <td>boolean</td> <td>getBoolean</td> <td>boolean</td> </tr>
-    <tr> <td>counter</td> <td>getLong</td> <td>long</td> </tr>
-    <tr> <td>date</td> <td>getLocalDate</td> <td>java.time.LocalDate</td> </tr>
-    <tr> <td>decimal</td> <td>getBigDecimal</td> <td>java.math.BigDecimal</td> </tr>
-    <tr> <td>double</td> <td>getDouble</td> <td>double</td> </tr>
-    <tr> <td>duration</td> <td>getCqlDuration</td> <td>CqlDuration</td> </tr>
-    <tr> <td>float</td> <td>getFloat</td> <td>float</td> </tr>
-    <tr> <td>inet</td> <td>getInetAddress</td> <td>java.net.InetAddress</td> </tr>
-    <tr> <td>int</td> <td>getInt</td> <td>int</td> </tr>
-    <tr> <td>list</td> <td>getList</td> <td>java.util.List<T></td> </tr>
-    <tr> <td>map</td> <td>getMap</td> <td>java.util.Map<K, V></td> </tr>
-    <tr> <td>set</td> <td>getSet</td> <td>java.util.Set<T></td> </tr>
-    <tr> <td>smallint</td> <td>getShort</td> <td>short</td> </tr>
-    <tr> <td>text</td> <td>getString</td> <td>java.lang.String</td> </tr>
-    <tr> <td>time</td> <td>getLocalTime</td> <td>java.time.LocalTime</td> </tr>
-    <tr> <td>timestamp</td> <td>getInstant</td> <td>java.time.Instant</td> </tr>
-    <tr> <td>timeuuid</td> <td>getUuid</td> <td>java.util.UUID</td> </tr>
-    <tr> <td>tinyint</td> <td>getByte</td> <td>byte</td> </tr>
-    <tr> <td>tuple</td> <td>getTupleValue</td> <td>TupleValue</td> </tr>
-    <tr> <td>user-defined types</td> <td>getUDTValue</td> <td>UDTValue</td> </tr>
-    <tr> <td>uuid</td> <td>getUuid</td> <td>java.util.UUID</td> </tr>
-    <tr> <td>varchar</td> <td>getString</td> <td>java.lang.String</td> </tr>
-    <tr> <td>varint</td> <td>getVarint</td> <td>java.math.BigInteger</td> </tr>
-</table>
+| CQL3 data type      | Getter name    | Java type            | See also                    |
+|---------------------|----------------|----------------------|-----------------------------|
+| ascii               | getString      | java.lang.String     |                             |
+| bigint              | getLong        | long                 |                             |
+| blob                | getBytes       | java.nio.ByteBuffer  |                             |
+| boolean             | getBoolean     | boolean              |                             |
+| counter             | getLong        | long                 |                             |
+| date                | getLocalDate   | java.time.LocalDate  | [Time types](time_types/)   |
+| decimal             | getBigDecimal  | java.math.BigDecimal |                             |
+| double              | getDouble      | double               |                             |
+| duration            | getCqlDuration | CqlDuration          | [Time types](time_types/)   |","[{'comment': 'Should we mention the FQCN of driver-specific types? Or would it blow up the table layout?', 'commenter': 'adutra'}, {'comment': 'I can link them to the javadoc.', 'commenter': 'olim7t'}]"
1123,manual/core/temporal_types/README.md,"@@ -0,0 +1,141 @@
+## Temporal types
+
+This page provides more details about the various CQL time types, and the Java types they are mapped
+to in the driver.
+
+### Date and time
+
+CQL types `date` and `time` map directly to `java.time.LocalDate` and `java.time.LocalTime`.
+
+These are simple, time-zone-free representations of date-only (`yyyy-mm-dd`) and time-only
+(`HH:MM:SS\[.fff]`) types.
+
+### Timestamp
+
+CQL type `timestamp` is the date-and-time representation, stored as a number of milliseconds since
+the epoch (01/01/1970 UTC).
+ 
+ 
+#### No time zone
+
+`timestamp` does **not** store a time zone. This is not always obvious because clients generally do
+use one for display. For instance, the following CQLSH snippet is from a machine in Pacific time: 
+
+```
+cqlsh> CREATE TABLE test(t timestamp PRIMARY KEY);
+cqlsh> INSERT INTO test (t) VALUES (dateof(now()));
+cqlsh> SELECT * FROM test;
+
+ t
+---------------------------------
+ 2018-11-07 08:50:52.433000-0800
+```
+
+It looks like the timestamp has a zone (`-0800`), but it is actually the client's. If you force
+CQLSH to a different zone and observe the same data, it will be displayed differently:
+
+```
+$ TZ=UTC cqlsh
+cqlsh> SELECT * FROM test;
+
+ t
+---------------------------------
+ 2018-11-07 16:50:52.433000+0000
+```
+
+Internally, Cassandra only stores the raw number of milliseconds. You can observe that with a cast:  
+
+```
+cqlsh> SELECT cast(t as bigint) FROM test;
+
+ cast(t as bigint)
+-------------------
+     1541609452433
+```
+
+#### Java equivalent
+
+By default, the driver maps `timestamp` to `java.time.Instant`. This Java type is the closest to the
+internal representation; in particular, it does not have a time zone. On the downside, this means
+you can't directly extract calendar fields (year, month, etc.). You need to call `atZone` to perform
+the conversion: 
+
+```java
+Row row = session.execute(""SELECT t FROM test"").one();
+Instant instant = row.getInstant(""t"");
+ZonedDateTime dateTime = instant.atZone(ZoneId.of(""America/Los_Angeles""));
+System.out.println(dateTime.getYear());
+```
+
+Conversely, you can convert a `ZonedDateTime` back to an `Instant` with `toInstant`.
+
+If you want to automate those `atZone`/`toInstant` conversions, the driver comes with an optional
+`ZonedDateTime` codec, that must be registered explicitly with the session:
+
+```java
+CqlSession session = CqlSession.builder()
+    .addTypeCodecs(TypeCodecs.ZONED_TIMESTAMP_UTC)
+    .build();
+
+Row row = session.execute(""SELECT t FROM test"").one();
+ZonedDateTime dateTime = row.get(""t"", GenericType.ZONED_DATE_TIME);
+``` 
+
+There are various constants and methods to obtain a codec instance for a particular zone:
+
+* [TypeCodecs.ZONED_TIMESTAMP_SYSTEM]\: system default;
+* [TypeCodecs.ZONED_TIMESTAMP_UTC]\: UTC;
+* [TypeCodecs.zonedTimestampAt()]\: user-provided.
+
+Which zone you choose is application-dependent. The driver doesn't map to `ZonedDateTime` by default
+because it would have to make an arbitrary choice; we want you to think about time zones explicitly
+before you decide to use that type.
+
+#### Millisecond-only precision
+
+As already stated, `timestamp` is stored as a number of milliseconds. If you try to write an
+`Instant` or `ZonedDateTime` with higher precision through the driver, the sub-millisecond part will
+be truncated:
+
+```java
+CqlSession session =
+    CqlSession.builder()
+        .addTypeCodecs(TypeCodecs.ZONED_TIMESTAMP_UTC)","[{'comment': ""TBH the example could be simplified by using `Instant`, but since you just introduced the `ZonedDateTime` codec, I'm fine."", 'commenter': 'adutra'}]"
1124,manual/core/detachable_types/README.md,"@@ -87,6 +87,35 @@ TupleValue tupleValue = tupleType.newValue().setString(0, ""foo"");
 When you pass a detached type to the session (for example by executing a request with a tuple value
 based on a detached tuple type), it will automatically be reattached.
 
+### Sharing data across sessions
+
+If you're reading data from one session and writing it into another, you should take a few extra
+precautions:
+
+* if you use custom codecs, they should obviously be registered with both sessions;
+
+* if the protocol version is different, you should avoid sharing UDT and tuple types; keep a
+  separate set of definitions for each session, and copy the values field by field:
+  
+    ```java
+    UserDefinedType userType2 =","[{'comment': 'I think the example would be a bit more clear if you introduce the declaration of `userType2` later below:\r\n```suggestion\r\n    Row row = session1.execute(""SELECT QUERY..."").one();\r\n    UdtValue user1 = row.getUdtValue(""user"");\r\n    \r\n     // Don\'t pass user1 to session2: create a new copy from userType2 instead\r\n    UserDefinedType userType2 =\r\n        session2.getMetadata().getKeyspace(""ks"").flatMap(ks -> ks.getUserDefinedType(""user"")).get();\r\n    UdtValue user2 = userType2.newValue();\r\n    user2.setString(""first_name"", user1.getString(""first_name""));\r\n    user2.setString(""last_name"", user1.getString(""last_name""));\r\n```', 'commenter': 'adutra'}, {'comment': 'ðŸ‘ makes sense', 'commenter': 'olim7t'}]"
1126,integration-tests/src/test/java/com/datastax/oss/driver/api/core/metadata/CaseSensitiveUdtIT.java,"@@ -0,0 +1,91 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core.metadata;
+
+import static org.assertj.core.api.Assertions.assertThat;
+
+import com.datastax.oss.driver.api.core.CqlIdentifier;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
+import com.datastax.oss.driver.api.core.metadata.schema.KeyspaceMetadata;
+import com.datastax.oss.driver.api.core.type.UserDefinedType;
+import com.datastax.oss.driver.api.testinfra.CassandraRequirement;
+import com.datastax.oss.driver.api.testinfra.ccm.CcmRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionUtils;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import java.time.Duration;
+import org.junit.BeforeClass;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.junit.rules.RuleChain;
+import org.junit.rules.TestRule;
+
+/**
+ * Checks that case-sensitive UDT names are properly handled in schema metadata.
+ *
+ * <p>In Cassandra >= 2.2, whenever a UDT is referenced in a system table (e.g. {@code
+ * system_schema.columns.type}, it uses the CQL form. This is in contrast to the UDT definition
+ * itself ({@code system_schema.types.type_name}), which uses the internal form.
+ *
+ * @see <a href=""https://datastax-oss.atlassian.net/browse/JAVA-2028"">JAVA-2028</a>
+ */
+@Category(ParallelizableTests.class)
+@CassandraRequirement(min = ""2.2"", description = ""Fixed bug doesn't occur with earlier versions"")
+public class CaseSensitiveUdtIT {
+
+  private static CcmRule ccmRule = CcmRule.getInstance();
+
+  private static SessionRule<CqlSession> sessionRule =
+      SessionRule.builder(ccmRule)
+          .withConfigLoader(
+              SessionUtils.configLoaderBuilder()
+                  .withDuration(DefaultDriverOption.REQUEST_TIMEOUT, Duration.ofSeconds(30))
+                  .build())
+          .build();
+
+  @ClassRule public static TestRule chain = RuleChain.outerRule(ccmRule).around(sessionRule);
+
+  @BeforeClass
+  public static void createSchema() {
+    CqlSession session = sessionRule.session();
+    session.execute(""CREATE TYPE \""Address\""(street text)"");
+    session.execute(""CREATE TABLE user(id uuid PRIMARY KEY, address \""Address\"")"");","[{'comment': ""I'm going to expand the test to cover the other cases I mentioned on the ticket (functions, aggregates, etc.)."", 'commenter': 'olim7t'}]"
1126,integration-tests/src/test/java/com/datastax/oss/driver/api/core/metadata/CaseSensitiveUdtIT.java,"@@ -0,0 +1,129 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core.metadata;
+
+import static org.assertj.core.api.Assertions.assertThat;
+
+import com.datastax.oss.driver.api.core.CqlIdentifier;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
+import com.datastax.oss.driver.api.core.metadata.schema.KeyspaceMetadata;
+import com.datastax.oss.driver.api.core.type.UserDefinedType;
+import com.datastax.oss.driver.api.testinfra.CassandraRequirement;
+import com.datastax.oss.driver.api.testinfra.ccm.CcmRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionUtils;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import java.time.Duration;
+import org.junit.BeforeClass;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.junit.rules.RuleChain;
+import org.junit.rules.TestRule;
+
+/**
+ * Checks that case-sensitive UDT names are properly handled in schema metadata.
+ *
+ * <p>In Cassandra >= 2.2, whenever a UDT is referenced in a system table (e.g. {@code
+ * system_schema.columns.type}, it uses the CQL form. This is in contrast to the UDT definition
+ * itself ({@code system_schema.types.type_name}), which uses the internal form.
+ *
+ * @see <a href=""https://datastax-oss.atlassian.net/browse/JAVA-2028"">JAVA-2028</a>
+ */
+@Category(ParallelizableTests.class)
+@CassandraRequirement(min = ""2.2"", description = ""Fixed bug doesn't occur with earlier versions"")","[{'comment': 'But shouldn\'t we still run this test against earlier versions of C* having UDTs (2.1) ? Otherwise how can we know if the changes introduced by this PR aren\'t introducing any regression in C* 2.1? Granted, functions and aggregates are not concerned in 2.1, but we should at least validate that `keyspace          .getUserDefinedType(CqlIdentifier.fromInternal(""Address""))` works as expected.', 'commenter': 'adutra'}, {'comment': ""True, it's not the same code but from a blackbox perspective we can still run the test against former versions with a few modifications."", 'commenter': 'olim7t'}]"
1130,core/src/main/java/com/datastax/oss/driver/internal/core/context/DefaultNettyOptions.java,"@@ -74,6 +77,24 @@ public DefaultNettyOptions(InternalDriverContext context) {
             .setNameFormat(context.getSessionName() + ""-admin-%d"")
             .build();
     this.adminEventLoopGroup = new DefaultEventLoopGroup(adminGroupSize, adminThreadFactory);
+    // setup the Timer
+    ThreadFactory timerThreadFactory =
+        new ThreadFactoryBuilder().setThreadFactory(safeFactory).build();
+    timer =
+        new HashedWheelTimer(
+            timerThreadFactory,
+            config.getDuration(DefaultDriverOption.NETTY_TIMER_TICK_DURATION).toNanos(),
+            TimeUnit.NANOSECONDS,
+            config.getInt(DefaultDriverOption.NETTY_TIMER_TICKS_PER_WHEEL));
+    // In case things shutdown without calling close(), make sure the timer resources are released.
+    Runtime.getRuntime()","[{'comment': ""I don't like having done this. However, some of the integration tests generate a bunch of warnings like this:\r\n```\r\nERROR io.netty.util.ResourceLeakDetector - LEAK: HashedWheelTimer.release() was not called before it's garbage-collected. See http://netty.io/wiki/reference-counted-objects.html for more information.\r\n```\r\nI am calling `timer.stop` in the `onClose()` method below, but that does not seem to get called in certain ITs. After investigating the test and finding one that caused the warning, it seems that it happens when you attempt to create a new session with a misconfigured DriverConfigLoader. Doing so will:\r\n1) create a DriverContext that has a lazy reference to an instance of NettyOptions, then\r\n1) dereference the DriverContext's NettyOptions, which\r\n1) calls the constructor for DefaultNettyOptions, which\r\n1) creates the Timer instance (line above this one)\r\n\r\nHowever since the test expects the session creation to fail, there is nothing left to invoke `onClose()`, so the Timer's resources are never released, as the warning indicates. The only way I was able to eliminate the warning was to add this shutdown hook.\r\n\r\n"", 'commenter': 'emerkle826'}, {'comment': ""Do you remember which test(s) created the warning?\r\n\r\nIf `onClose` is not called, the event loops aren't shut down either, that's also a resource leak. We need to make sure that `onClose` is always called if the options were ever initialized."", 'commenter': 'olim7t'}, {'comment': ""`DriverConfigValidationIT` was one that caused the warning. That test tries to create a session with a config loader that has an non-existent class specified for a variety of Driver options. If I comment out most of the calls to `should_fail_to_init_with_invalid_policy`, the warning doesn't appear. Seems like a race condition of sorts.\r\n\r\nI didn't see any warnings about the event loops, but maybe they aren't tracked the same? I'll try to investigate what's going on some more."", 'commenter': 'emerkle826'}, {'comment': ""This looks like a bug in `DefaultSession`: `NettyOptions.onClose` should be invoked if init fails. Not clear yet why the unclosed event loops don't prevent the VM from exiting, I'm looking into it."", 'commenter': 'olim7t'}]"
1130,core/src/main/java/com/datastax/oss/driver/internal/core/context/DefaultNettyOptions.java,"@@ -74,6 +77,24 @@ public DefaultNettyOptions(InternalDriverContext context) {
             .setNameFormat(context.getSessionName() + ""-admin-%d"")
             .build();
     this.adminEventLoopGroup = new DefaultEventLoopGroup(adminGroupSize, adminThreadFactory);
+    // setup the Timer
+    ThreadFactory timerThreadFactory =
+        new ThreadFactoryBuilder().setThreadFactory(safeFactory).build();","[{'comment': 'Could you use a name format like the two other factories? This is pretty convenient when debugging stuff (thread dumps, etc.)\r\n```suggestion\r\n        new ThreadFactoryBuilder().setThreadFactory(safeFactory).setNameFormat(context.getSessionName() + ""-timer-%d"").build();\r\n```', 'commenter': 'olim7t'}]"
1130,core/src/main/java/com/datastax/oss/driver/internal/core/cql/CqlPrepareHandlerBase.java,"@@ -144,13 +144,13 @@ protected CqlPrepareHandlerBase(
     }
     this.message =
         new Prepare(request.getQuery(), (keyspace == null) ? null : keyspace.asInternal());
-    this.scheduler = context.getNettyOptions().ioEventLoopGroup().next();
+    this.timer = context.getNettyOptions().getTimer();","[{'comment': 'Global ðŸ‘ to timeout management in handlers.', 'commenter': 'olim7t'}]"
1130,core/src/main/resources/reference.conf,"@@ -1369,6 +1369,30 @@ datastax-java-driver {
 
       shutdown {quiet-period = 2, timeout = 15, unit = SECONDS}
     }
+    # The timer used for scheduling request timeouts and speculative executions
+    timer {
+      # The timer tick duration
+      # This is the how frequent the timer should wake-up to check for timed out tasks.
+      # Lower resolution (i.e. longer durations) will leave more CPU cycles for running I/O
+      # operations at the cost of precision of exactly when a request timeout will expire or a
+      # speculative execution will run. Higher resolution (i.e. shorter durations) will result in
+      # more precise request timeouts and speculative execution scheduling, but at the cost of CPU
+      # cycles taken from I/O operations, which could lead to lower overall I/O throughput.
+      #
+      # Required: yes
+      # Modifiable at runtime: no
+      # Overridable in a profile: no
+      tick-duration = 100 milliseconds
+
+      # Number of ticks in a Timer wheel. The underlying implementation uses Netty's
+      # HashedWheelTimer, which uses a hashes to arrange the timeouts. This effectively controls the
+      # size of the timer wheel.","[{'comment': 'ðŸ‘ great docs.\r\n\r\nNit: I think there is a word missing in ""a hashes to arrange the timeouts"".', 'commenter': 'olim7t'}, {'comment': 'Good catch, I think I originally meant it to say ""a hash to arrange the timeouts"" and then decided ""hashes to arrange the timeouts"" and forgot to remove the ""a"". ', 'commenter': 'emerkle826'}]"
1130,core/src/test/java/com/datastax/oss/driver/internal/core/cql/CqlRequestHandlerSpeculativeExecutionTest.java,"@@ -250,16 +239,14 @@ public void should_fail_if_no_more_nodes_and_initial_execution_is_last(
       node1Behavior.setWriteSuccess();
       // do not simulate a response from node1 yet
 
-      harness.nextScheduledTask(); // Discard the timeout task
+      harness.nextScheduledTimeout(); // Discard the timeout task
 
       // Run the next scheduled task to start the speculative execution. node2 will reply with a
       // BOOTSTRAPPING error, causing a RETRY_NEXT; but the query plan is now empty so the
       // speculative execution stops.
-      ScheduledTaskCapturingEventLoop.CapturedTask<?> firstExecutionTask =
-          harness.nextScheduledTask();
-      assertThat(firstExecutionTask.getInitialDelay(TimeUnit.MILLISECONDS))
-          .isEqualTo(firstExecutionDelay);
-      firstExecutionTask.run();
+      // next sceduled timeout should be the first speculative execution. Get it and run it.","[{'comment': 'Nit: sceduled => scheduled\r\nThis is repeated in other places since you copy-pasted this block.', 'commenter': 'olim7t'}]"
1130,core/src/test/java/com/datastax/oss/driver/internal/core/cql/CqlRequestHandlerTest.java,"@@ -103,32 +100,21 @@ public void should_fail_if_no_node_available() {
   }
 
   @Test
-  public void should_time_out_if_first_node_takes_too_long_to_respond() {
+  public void should_time_out_if_first_node_takes_too_long_to_respond() throws Exception {
     RequestHandlerTestHarness.Builder harnessBuilder = RequestHandlerTestHarness.builder();
     PoolBehavior node1Behavior = harnessBuilder.customBehavior(node1);
     node1Behavior.setWriteSuccess();
 
     try (RequestHandlerTestHarness harness = harnessBuilder.build()) {
 
-      CompletionStage<AsyncResultSet> resultSetFuture =
+      CqlRequestAsyncHandler requestHandler =
           new CqlRequestAsyncHandler(
-                  UNDEFINED_IDEMPOTENCE_STATEMENT,
-                  harness.getSession(),
-                  harness.getContext(),
-                  ""test"")
-              .handle();
+              UNDEFINED_IDEMPOTENCE_STATEMENT, harness.getSession(), harness.getContext(), ""test"");
+      CompletionStage<AsyncResultSet> resultSetFuture = requestHandler.handle();","[{'comment': 'Any particular reason to extract the `requestHandler` variable in this test?', 'commenter': 'olim7t'}, {'comment': ""Not any more. At one point in the implementation, I needed it for something, but it seems I don't anymore. I'll revert that bit."", 'commenter': 'emerkle826'}]"
1130,core/src/test/java/com/datastax/oss/driver/internal/core/util/concurrent/ScheduledTimeoutCapturingTimer.java,"@@ -0,0 +1,126 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.datastax.oss.driver.internal.core.util.concurrent;
+
+import static org.assertj.core.api.Assertions.fail;
+
+import io.netty.util.Timeout;
+import io.netty.util.Timer;
+import io.netty.util.TimerTask;
+import java.util.Collections;
+import java.util.HashSet;
+import java.util.Set;
+import java.util.concurrent.ArrayBlockingQueue;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+/**
+ * Implementation of Netty's {@link io.netty.util.Timer Timer} interface to capture scheduled {@link
+ * io.netty.util.Timeout Timeout} instead of running them, events so they can be run manually in
+ * tests.
+ */
+public class ScheduledTimeoutCapturingTimer implements Timer {","[{'comment': 'Nit: I think we don\'t really need ""Scheduled"" in the name here (in the other class it refers to the fact that it captures tasks submitted via the `schedule` method). Maybe even `CapturingTimer` would be enough...\r\n\r\nBut it\'s more a matter of personal taste really, feel free to use what\'s most clear to you.', 'commenter': 'olim7t'}]"
1130,core/src/test/java/com/datastax/oss/driver/internal/core/util/concurrent/ScheduledTimeoutCapturingTimer.java,"@@ -0,0 +1,126 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.datastax.oss.driver.internal.core.util.concurrent;
+
+import static org.assertj.core.api.Assertions.fail;
+
+import io.netty.util.Timeout;
+import io.netty.util.Timer;
+import io.netty.util.TimerTask;
+import java.util.Collections;
+import java.util.HashSet;
+import java.util.Set;
+import java.util.concurrent.ArrayBlockingQueue;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+/**
+ * Implementation of Netty's {@link io.netty.util.Timer Timer} interface to capture scheduled {@link
+ * io.netty.util.Timeout Timeout} instead of running them, events so they can be run manually in
+ * tests.
+ */
+public class ScheduledTimeoutCapturingTimer implements Timer {
+
+  private final ArrayBlockingQueue<Timeout> timeoutQueue = new ArrayBlockingQueue<>(16);
+
+  @Override
+  public Timeout newTimeout(TimerTask task, long delay, TimeUnit unit) {
+    // delay and unit are not needed as the Timeout's TimerTask will be run manually
+    Timeout timeout = new _Timeout(task, this);
+    // add the timeout to the queue
+    timeoutQueue.add(timeout);
+    return timeout;
+  }
+
+  /**
+   * Retrieves the next scheduled Timeout. In tests, this will usually be a request timeout or a
+   * speculative execution. Tests will need be able to predict the ordering as it is not easy to
+   * tell from the returned Timeout itself.
+   */
+  public Timeout getNextTimeout() {
+    try {
+      Timeout t = timeoutQueue.poll(100, TimeUnit.MILLISECONDS);
+      return t;
+    } catch (InterruptedException ie) {
+      fail(""Unexpected interruption"", ie);
+      throw new AssertionError();
+    }
+  }
+
+  @Override
+  public Set<Timeout> stop() {
+    if (timeoutQueue.isEmpty()) {
+      return Collections.EMPTY_SET;
+    }
+    Set<Timeout> timeoutsRemaining = new HashSet<>(timeoutQueue.size());
+    for (Timeout t : timeoutQueue) {
+      if (t != null) {
+        t.cancel();
+        timeoutsRemaining.add(t);
+      }
+    }
+    return timeoutsRemaining;
+  }
+
+  /**
+   * Implementation of Netty's {@link io.netty.util.Timeout Timeout} interface. It is just a simple
+   * class that keeps track of the {@link io.netty.util.TimerTask TimerTask} and the {@link
+   * io.netty.util.Timer Timer} implementation that should only be used in tests. The intended use
+   * is to call the {@link io.netty.util.TimerTask#run(io.netty.util.Timeout) run()} method on the
+   * TimerTask when you want to execute the task (so you don't have to depend on a real timer).
+   *
+   * <p>Example:
+   *
+   * <p>// get the timeout Timeout t = retrieveTimeout(); // run the timeout task t.task().run(t);
+   */
+  private static class _Timeout implements Timeout {","[{'comment': ""I know it's an internal class, but not a big fan of the underscore in the name. Could we use something more descriptive, like maybe `CapturedTimeout` or `MockTimeout`?"", 'commenter': 'olim7t'}]"
1130,core/src/test/java/com/datastax/oss/driver/internal/core/util/concurrent/ScheduledTimeoutCapturingTimer.java,"@@ -0,0 +1,126 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.datastax.oss.driver.internal.core.util.concurrent;
+
+import static org.assertj.core.api.Assertions.fail;
+
+import io.netty.util.Timeout;
+import io.netty.util.Timer;
+import io.netty.util.TimerTask;
+import java.util.Collections;
+import java.util.HashSet;
+import java.util.Set;
+import java.util.concurrent.ArrayBlockingQueue;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+/**
+ * Implementation of Netty's {@link io.netty.util.Timer Timer} interface to capture scheduled {@link
+ * io.netty.util.Timeout Timeout} instead of running them, events so they can be run manually in
+ * tests.
+ */
+public class ScheduledTimeoutCapturingTimer implements Timer {
+
+  private final ArrayBlockingQueue<Timeout> timeoutQueue = new ArrayBlockingQueue<>(16);
+
+  @Override
+  public Timeout newTimeout(TimerTask task, long delay, TimeUnit unit) {
+    // delay and unit are not needed as the Timeout's TimerTask will be run manually
+    Timeout timeout = new _Timeout(task, this);
+    // add the timeout to the queue
+    timeoutQueue.add(timeout);
+    return timeout;","[{'comment': 'Overall ðŸ‘ to the implementation of this class.', 'commenter': 'olim7t'}]"
1130,core/src/test/java/com/datastax/oss/driver/internal/core/util/concurrent/ScheduledTimeoutCapturingTimer.java,"@@ -0,0 +1,126 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.datastax.oss.driver.internal.core.util.concurrent;
+
+import static org.assertj.core.api.Assertions.fail;
+
+import io.netty.util.Timeout;
+import io.netty.util.Timer;
+import io.netty.util.TimerTask;
+import java.util.Collections;
+import java.util.HashSet;
+import java.util.Set;
+import java.util.concurrent.ArrayBlockingQueue;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+/**
+ * Implementation of Netty's {@link io.netty.util.Timer Timer} interface to capture scheduled {@link
+ * io.netty.util.Timeout Timeout} instead of running them, events so they can be run manually in
+ * tests.
+ */
+public class ScheduledTimeoutCapturingTimer implements Timer {
+
+  private final ArrayBlockingQueue<Timeout> timeoutQueue = new ArrayBlockingQueue<>(16);
+
+  @Override
+  public Timeout newTimeout(TimerTask task, long delay, TimeUnit unit) {
+    // delay and unit are not needed as the Timeout's TimerTask will be run manually
+    Timeout timeout = new _Timeout(task, this);
+    // add the timeout to the queue
+    timeoutQueue.add(timeout);
+    return timeout;
+  }
+
+  /**
+   * Retrieves the next scheduled Timeout. In tests, this will usually be a request timeout or a
+   * speculative execution. Tests will need be able to predict the ordering as it is not easy to
+   * tell from the returned Timeout itself.
+   */
+  public Timeout getNextTimeout() {
+    try {
+      Timeout t = timeoutQueue.poll(100, TimeUnit.MILLISECONDS);
+      return t;
+    } catch (InterruptedException ie) {
+      fail(""Unexpected interruption"", ie);
+      throw new AssertionError();
+    }
+  }
+
+  @Override
+  public Set<Timeout> stop() {
+    if (timeoutQueue.isEmpty()) {
+      return Collections.EMPTY_SET;
+    }
+    Set<Timeout> timeoutsRemaining = new HashSet<>(timeoutQueue.size());
+    for (Timeout t : timeoutQueue) {
+      if (t != null) {
+        t.cancel();
+        timeoutsRemaining.add(t);
+      }
+    }
+    return timeoutsRemaining;
+  }
+
+  /**
+   * Implementation of Netty's {@link io.netty.util.Timeout Timeout} interface. It is just a simple
+   * class that keeps track of the {@link io.netty.util.TimerTask TimerTask} and the {@link
+   * io.netty.util.Timer Timer} implementation that should only be used in tests. The intended use
+   * is to call the {@link io.netty.util.TimerTask#run(io.netty.util.Timeout) run()} method on the
+   * TimerTask when you want to execute the task (so you don't have to depend on a real timer).
+   *
+   * <p>Example:
+   *
+   * <p>// get the timeout Timeout t = retrieveTimeout(); // run the timeout task t.task().run(t);
+   */
+  private static class _Timeout implements Timeout {
+
+    private final TimerTask task;
+    private final ScheduledTimeoutCapturingTimer timer;
+    private final AtomicBoolean canceled = new AtomicBoolean(false);","[{'comment': 'Nit: cancelled', 'commenter': 'olim7t'}]"
1130,core/src/test/java/com/datastax/oss/driver/internal/core/util/concurrent/ScheduledTimeoutCapturingTimer.java,"@@ -0,0 +1,126 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.datastax.oss.driver.internal.core.util.concurrent;
+
+import static org.assertj.core.api.Assertions.fail;
+
+import io.netty.util.Timeout;
+import io.netty.util.Timer;
+import io.netty.util.TimerTask;
+import java.util.Collections;
+import java.util.HashSet;
+import java.util.Set;
+import java.util.concurrent.ArrayBlockingQueue;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+/**
+ * Implementation of Netty's {@link io.netty.util.Timer Timer} interface to capture scheduled {@link
+ * io.netty.util.Timeout Timeout} instead of running them, events so they can be run manually in
+ * tests.
+ */
+public class ScheduledTimeoutCapturingTimer implements Timer {
+
+  private final ArrayBlockingQueue<Timeout> timeoutQueue = new ArrayBlockingQueue<>(16);
+
+  @Override
+  public Timeout newTimeout(TimerTask task, long delay, TimeUnit unit) {
+    // delay and unit are not needed as the Timeout's TimerTask will be run manually
+    Timeout timeout = new _Timeout(task, this);
+    // add the timeout to the queue
+    timeoutQueue.add(timeout);
+    return timeout;
+  }
+
+  /**
+   * Retrieves the next scheduled Timeout. In tests, this will usually be a request timeout or a
+   * speculative execution. Tests will need be able to predict the ordering as it is not easy to
+   * tell from the returned Timeout itself.
+   */
+  public Timeout getNextTimeout() {
+    try {
+      Timeout t = timeoutQueue.poll(100, TimeUnit.MILLISECONDS);
+      return t;
+    } catch (InterruptedException ie) {
+      fail(""Unexpected interruption"", ie);
+      throw new AssertionError();
+    }
+  }
+
+  @Override
+  public Set<Timeout> stop() {
+    if (timeoutQueue.isEmpty()) {
+      return Collections.EMPTY_SET;
+    }
+    Set<Timeout> timeoutsRemaining = new HashSet<>(timeoutQueue.size());
+    for (Timeout t : timeoutQueue) {
+      if (t != null) {
+        t.cancel();
+        timeoutsRemaining.add(t);
+      }
+    }
+    return timeoutsRemaining;
+  }
+
+  /**
+   * Implementation of Netty's {@link io.netty.util.Timeout Timeout} interface. It is just a simple
+   * class that keeps track of the {@link io.netty.util.TimerTask TimerTask} and the {@link
+   * io.netty.util.Timer Timer} implementation that should only be used in tests. The intended use
+   * is to call the {@link io.netty.util.TimerTask#run(io.netty.util.Timeout) run()} method on the
+   * TimerTask when you want to execute the task (so you don't have to depend on a real timer).
+   *
+   * <p>Example:
+   *
+   * <p>// get the timeout Timeout t = retrieveTimeout(); // run the timeout task t.task().run(t);","[{'comment': 'Use this for code snippets in javadocs:\r\n```\r\n<pre>{@code\r\n    your code here\r\n}</pre>\r\n```', 'commenter': 'olim7t'}]"
1130,core/src/test/java/com/datastax/oss/driver/internal/core/cql/CqlRequestHandlerSpeculativeExecutionTest.java,"@@ -101,34 +101,28 @@ public void should_schedule_speculative_executions(
       node1Behavior.verifyWrite();
       node1Behavior.setWriteSuccess();
 
-      harness.nextScheduledTask(); // Discard the timeout task
+      harness.nextScheduledTimeout(); // Discard the timeout task
 
-      ScheduledTaskCapturingEventLoop.CapturedTask<?> firstExecutionTask =
-          harness.nextScheduledTask();
-      assertThat(firstExecutionTask.getInitialDelay(TimeUnit.MILLISECONDS))
-          .isEqualTo(firstExecutionDelay);
+      Timeout speculativeExecution1 = harness.nextScheduledTimeout();
       Mockito.verifyNoMoreInteractions(nodeMetricUpdater1);
-      firstExecutionTask.run();
+      speculativeExecution1.task().run(speculativeExecution1);","[{'comment': 'This looks a bit weird but that comes from the Netty API, nothing we can do about it in this PR.', 'commenter': 'olim7t'}, {'comment': ""For the tasks we schedule on the timer, the Timeout argument to the `run()` call in the tests doesn't matter (it could be `null`). But that is only because the TimerTasks we schedule in the Handlers simply don't reference the Timeout in the runnable block of code. If that runnable block of code ever does, then passing `null` here could potentially throw an NPE, so I opted to pass in a known non-null reference.\r\nBut yes, it was (and still looks) weird."", 'commenter': 'emerkle826'}]"
1130,core/src/main/java/com/datastax/oss/driver/internal/core/cql/CqlPrepareHandlerBase.java,"@@ -171,25 +171,25 @@ public void onThrottleReady(boolean wasDelayed) {
     sendRequest(null, 0);
   }
 
-  private ScheduledFuture<?> scheduleTimeout(Duration timeout) {
-    if (timeout.toNanos() > 0) {
-      return scheduler.schedule(
-          () -> {
-            setFinalError(new DriverTimeoutException(""Query timed out after "" + timeout));
+  private Timeout scheduleTimeout(Duration timeoutDuration) {
+    if (timeoutDuration.toNanos() > 0) {
+      return this.timer.newTimeout(","[{'comment': 'can we extract common logic for scheduling timeout for CqlRequestHandlerBase and CqlPrepareHandlerBase?\r\nWe could create a separate class that is responsible only for timeouts and reuse it for both classes', 'commenter': 'tomekl007'}, {'comment': ""Code reuse in request handlers is a dilemma: they have a lot of things in common, but also a lot of differences. For example both do timeouts but CqlPrepareHandlerBase doesn't have speculative executions, CqlRequestHandlerBase doesn't have to deal with repreparing on other nodes, etc.\r\nIf we start to extract some common logic I'm worried it's going to become an ugly abstraction with conditionals everywhere. That also raises the question of exposing that logic for 3rd-party handler implementations, which could make its design even more complicated.\r\n\r\nSo granted there is a bit of code duplication with the current design, but on the other hand each handler is its own self-contained implementation, that is easy to understand and modify."", 'commenter': 'olim7t'}, {'comment': 'Thanks for a detailed explanation', 'commenter': 'tomekl007'}]"
1130,core/src/main/java/com/datastax/oss/driver/internal/core/session/DefaultSession.java,"@@ -94,14 +94,30 @@
 
   private DefaultSession(InternalDriverContext context, Set<InetSocketAddress> contactPoints) {
     LOG.debug(""Creating new session {}"", context.getSessionName());
-    this.adminExecutor = context.getNettyOptions().adminEventExecutorGroup().next();
-    this.context = context;
-    this.singleThreaded = new SingleThreaded(context, contactPoints);
-    this.metadataManager = context.getMetadataManager();
-    this.processorRegistry = context.getRequestProcessorRegistry();
-    this.poolManager = context.getPoolManager();
     this.logPrefix = context.getSessionName();
-    this.metricUpdater = context.getMetricsFactory().getSessionUpdater();
+    this.adminExecutor = context.getNettyOptions().adminEventExecutorGroup().next();","[{'comment': 'this method call cannot throw an exception? I see that we enclosed in a `try` everything besides that line', 'commenter': 'tomekl007'}, {'comment': ""I suppose it could throw an exception, but it would only be in the DefaultNettyOptions constructor. If the constructor doesn't throw an exception, `adminEventExecutor()` simply returns the EventLoopGroup instance created in the constructor, and `next()` just returns the next executor in the group.\r\n\r\nIf an exception is thrown in the constructor, it will fall out of this `DefaultSession` constructor and bubble up the stack, just as would any Exception that would be thrown from any of the methods moved into the try block.\r\nThe reason this line is not in the try block is that if it throws an Exception, there won't be a NettyOptions instance created on the Context (it's lazily created on the first call to `getNettyOptions()` on the context). If there is no NettyOptions instance, then there isn't a need to call `onClose()` to free resources.\r\n\r\nThe idea here is that if any of these methods (including this line) throws an Exception, it should propagate up. However, if a NettyOptions is successfully created, but something else fails, we need to free the resources created in the NettyOptions before the Exception propagates."", 'commenter': 'emerkle826'}, {'comment': 'Thanks for a detailed explanation', 'commenter': 'tomekl007'}, {'comment': ""ðŸ‘ \r\nIn theory the `DefaultNettyOptions` constructor could throw after it created the timer, but we wouldn't have a way to stop the timer from here anyway.\r\nAnd given that it's the last thing done in that constructor, there's little chance that this could happen."", 'commenter': 'olim7t'}]"
1132,manual/object_mapper/using/README.md,"@@ -275,6 +275,10 @@ executed:
       <td><code>ListenableFuture&lt;Result&lt;T&gt;&gt;</code></td>
       <td><code>T</code> must be a mapped class.<br/>Asynchronous execution, returns a list of mapped objects.</td>
     </tr>
+    <tr>
+      <td><code>Statement</code></td>
+      <td>Instance of <code>Statement</code> that could be executed via <code>Session</code> object.</td>","[{'comment': 'Maybe you could stress the fact that in this case, 1) the query is not executed when the method returns, and 2) the returned statement is a bound statement.', 'commenter': 'adutra'}, {'comment': 'Sure, I really first wrote that it returns `BoundStatement`, and then decided to go to more generic `Statement` :-)\r\n\r\ndone', 'commenter': 'alexott'}]"
1140,java-driver-archetype/README.md,"@@ -0,0 +1,147 @@
+
+# Cassandra Java Driver Archetype
+
+This is a Maven Archetype project that can be used to bootstrap a simple Maven project featuring the
+Java Drivers for Cassandra. To build _this_ project, simple run a Maven install:
+
+```
+mvn clean install
+```
+
+This will install the archetype locally so you can then use it to create a bootstrap project.
+
+## Working with this Maven Archetype project
+If you are looking to create a bootstrapped project, you can skip down to
+[Creating a Bootstrap project](#creating-a-bootstrap-project)
+
+### Archetype Metadata
+The [archetype-metadata.xml][1] conforms to the [Maven Archetype Descriptor Model][2]. It defines
+properties that must be set to generate a bootstrapped project as well as fileset rules that control","[{'comment': 'I think that typo: `fileset rules`', 'commenter': 'tomekl007'}]"
1140,java-driver-archetype/src/main/resources/archetype-resources/src/main/java/Main.java,"@@ -0,0 +1,22 @@
+package ${package};
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.CqlSessionBuilder;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import java.net.InetSocketAddress;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class Main {
+  private static final Logger LOG = LoggerFactory.getLogger(Main.class);
+
+  public static void main(String[] args) {
+    CqlSessionBuilder builder = CqlSession.builder();
+    // Set the host and port of the Cassandra server here
+    builder.addContactPoint(InetSocketAddress.createUnresolved(""${cassandra-host}"", ${cassandra-port}));","[{'comment': ""We don't want to give in this archetype a simple configuration mechanism? If we will then we could use an info when user interactively type host and port and load those settings here "", 'commenter': 'tomekl007'}, {'comment': ""It will default to `127.0.0.1` and `9042` here, if you don't explicitly set these two properties. The defaults are in the `archetype-metadata.xml` required properties section. So, when you run `archetype:generate`, if you do not specify these properties (i.e. `-Dcassandra-host=someHost`), it uses the defaults.\r\n\r\nYou have the chance to change them as there is a confirmation prompt that shows the values to be used. If you don't like what is set, you enter the interactive mode where you can type in what you want. I tried to cover it in the README.md, in the `Interactive confirmation` section. Let me know if I should expand on that a bit more."", 'commenter': 'emerkle826'}, {'comment': ""I'm not sure if we want to use `createUnresolved` here. Using unresolved addresses is an advanced feature for specific use cases, and it requires additional configuration to work properly (see JAVA-1978).\r\n\r\nFor a basic prototype we don't know if it will be needed so it is more of a distraction, I would either stick to the (String, int)  constructor, or maybe inject the address in `application.conf`."", 'commenter': 'olim7t'}, {'comment': 'Ok, so I think that this comment `Set the host and port of the Cassandra server here` is a bit misleading. If I will be a user, I would think that we should set them manually but maybe it is only my impression ', 'commenter': 'tomekl007'}]"
1140,java-driver-archetype/src/main/resources/archetype-resources/pom.xml,"@@ -0,0 +1,155 @@
+<!--
+
+    Copyright DataStax, Inc.
+
+    Licensed under the Apache License, Version 2.0 (the ""License"");
+    you may not use this file except in compliance with the License.
+    You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+    Unless required by applicable law or agreed to in writing, software
+    distributed under the License is distributed on an ""AS IS"" BASIS,
+    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+    See the License for the specific language governing permissions and
+    limitations under the License.
+
+-->
+<project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
+         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"">
+  <modelVersion>4.0.0</modelVersion>
+  <groupId>${groupId}</groupId>
+  <artifactId>${artifactId}</artifactId>
+  <version>${version}</version>
+  <properties>
+    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
+    <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>
+    <junit.version>4.12</junit.version>
+    <slf4j.version>1.7.25</slf4j.version>
+    <logback.version>1.2.3</logback.version>
+    <log4j.version>1.2.17</log4j.version>
+    <slf.log4j.version>1.7.25</slf.log4j.version>
+    <java.driver.version>4.0.0-beta3-SNAPSHOT</java.driver.version>
+    <maven.compiler.plugin.version>3.8.0</maven.compiler.plugin.version>
+  </properties>
+  <dependencyManagement>
+    <dependencies>
+      <dependency>
+        <groupId>junit</groupId>
+        <artifactId>junit</artifactId>
+        <version>${junit.version}</version>
+        <scope>test</scope>
+      </dependency>
+      <dependency>
+        <groupId>org.slf4j</groupId>
+        <artifactId>slf4j-api</artifactId>
+        <version>${slf4j.version}</version>
+      </dependency>
+      <dependency>
+        <groupId>ch.qos.logback</groupId>
+        <artifactId>logback-classic</artifactId>
+        <version>${logback.version}</version>
+      </dependency>
+      <dependency>
+        <groupId>log4j</groupId>
+        <artifactId>log4j</artifactId>
+        <version>${log4j.version}</version>
+      </dependency>
+      <dependency>
+        <groupId>org.slf4j</groupId>
+        <artifactId>slf4j-log4j12</artifactId>
+        <version>${slf.log4j.version}</version>
+      </dependency>
+      <dependency>
+        <groupId>com.datastax.oss</groupId>
+        <artifactId>java-driver-core</artifactId>
+        <version>${java.driver.version}</version>
+      </dependency>
+    </dependencies>
+  </dependencyManagement>
+  <dependencies>
+    <dependency>
+      <groupId>junit</groupId>
+      <artifactId>junit</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.slf4j</groupId>
+      <artifactId>slf4j-api</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>com.datastax.oss</groupId>
+      <artifactId>java-driver-core</artifactId>
+    </dependency>
+  </dependencies>
+  <profiles>
+    <profile>
+      <id>default-logging</id>
+      <activation>
+        <property>
+          <name>!logging</name>
+        </property>
+      </activation>
+      <dependencies>
+        <dependency>
+          <groupId>ch.qos.logback</groupId>
+          <artifactId>logback-classic</artifactId>
+        </dependency>
+      </dependencies>
+    </profile>
+    <profile>
+      <id>logback-classic</id>
+      <activation>
+        <property>
+          <name>logging</name>
+          <value>logback-classic</value>
+        </property>
+      </activation>
+      <dependencies>
+        <dependency>
+          <groupId>ch.qos.logback</groupId>
+          <artifactId>logback-classic</artifactId>
+        </dependency>
+      </dependencies>
+    </profile>
+    <profile>
+      <id>log4j</id>
+      <activation>
+        <property>
+          <name>logging</name>
+          <value>log4j</value>
+        </property>
+      </activation>
+      <dependencies>
+        <dependency>
+          <groupId>log4j</groupId>
+          <artifactId>log4j</artifactId>
+        </dependency>
+        <dependency>
+          <groupId>org.slf4j</groupId>
+          <artifactId>slf4j-log4j12</artifactId>
+        </dependency>
+      </dependencies>
+    </profile>
+  </profiles>
+  <build>
+    <pluginManagement>
+      <plugins>
+        <plugin>
+          <artifactId>maven-compiler-plugin</artifactId>
+          <version>${maven.compiler.plugin.version}</version>
+        </plugin>
+      </plugins>
+    </pluginManagement>
+    <plugins>
+      <plugin>
+        <artifactId>maven-compiler-plugin</artifactId>
+        <configuration>
+          <source>1.8</source>","[{'comment': 'maybe we could also parametrize source 1.8 via generator?', 'commenter': 'tomekl007'}, {'comment': ""I guess we could, however the reason this is here is that without it, it defaults to 1.5 (I think). But the code actually need to be at least 1.7 in order to compile due to language features that didn't exist before 1.7. It's also hard-coded in the compiler config for the Java driver project itself. I don't think we want to let them change it to something lower accidentally and then have to explain that it needs to be at least 1.7."", 'commenter': 'emerkle826'}, {'comment': ""I'm in favor of keeping it hard-coded, since it is more of a general Maven concept rather than something directly related to driver usage."", 'commenter': 'olim7t'}, {'comment': 'Ok, so maybe we can leave that info in the documentation that it uses 1.8 on default?', 'commenter': 'tomekl007'}]"
1140,java-driver-archetype/src/main/resources/META-INF/maven/archetype-metadata.xml,"@@ -0,0 +1,86 @@
+<!--
+
+    Copyright DataStax, Inc.
+
+    Licensed under the Apache License, Version 2.0 (the ""License"");
+    you may not use this file except in compliance with the License.
+    You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+    Unless required by applicable law or agreed to in writing, software
+    distributed under the License is distributed on an ""AS IS"" BASIS,
+    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+    See the License for the specific language governing permissions and
+    limitations under the License.
+
+-->
+<archetype-descriptor
+  xmlns=""http://maven.apache.org/plugins/maven-archetype-plugin/archetype-descriptor/1.0.0""
+  xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
+  xsi:schemaLocation=""http://maven.apache.org/plugins/maven-archetype-plugin/archetype-descriptor/1.0.0 http://maven.apache.org/xsd/archetype-descriptor-1.0.0.xsd""
+  name=""datastax-java-driver-archetype""
+  partial=""false"">
+  <requiredProperties>
+    <!--
+    Sets the logging level of the archetype project. Can be overridden when generating the project.
+    -->
+    <requiredProperty key=""logging-level"">
+      <defaultValue>DEBUG</defaultValue>
+      <validationRegex>TRACE|DEBUG|INFO|WARN|ERROR</validationRegex>
+    </requiredProperty>
+    <requiredProperty key=""cassandra-host"">
+      <defaultValue>127.0.0.1</defaultValue>
+    </requiredProperty>
+    <requiredProperty key=""cassandra-port"">
+      <defaultValue>9042</defaultValue>
+    </requiredProperty>
+    <requiredProperty key=""datacenter"">
+      <defaultValue>datacenter1</defaultValue>
+    </requiredProperty>
+  </requiredProperties>
+
+  <fileSets>
+    <!-- 
+    setting ""filtered"" to ""true"" means the selected files will be used as Velocity templates.
+    setting ""packaged"" to ""true"" means the selected files will be generated/copied in a
+    directory structure that is prepended by the package property.
+    -->
+    <fileSet filtered=""true"" packaged=""true"" encoding=""UTF-8"" >
+      <directory>src/main/java</directory>
+      <includes>
+        <include>**/*.java</include>","[{'comment': 'If we support only java we should write it in the documentation of archetype. I can imagine that some person will want to use it in different lang (scale, kotlin) but if we will document that it will be explicit that user needs to add that support ourselves.', 'commenter': 'tomekl007'}]"
1140,java-driver-archetype/README.md,"@@ -0,0 +1,147 @@
+
+# Cassandra Java Driver Archetype
+
+This is a Maven Archetype project that can be used to bootstrap a simple Maven project featuring the
+Java Drivers for Cassandra. To build _this_ project, simple run a Maven install:
+
+```
+mvn clean install
+```
+
+This will install the archetype locally so you can then use it to create a bootstrap project.
+
+## Working with this Maven Archetype project
+If you are looking to create a bootstrapped project, you can skip down to
+[Creating a Bootstrap project](#creating-a-bootstrap-project)
+
+### Archetype Metadata
+The [archetype-metadata.xml][1] conforms to the [Maven Archetype Descriptor Model][2]. It defines
+properties that must be set to generate a bootstrapped project as well as fileset rules that control
+how source files are generated. The file must live in `src/main/resources/META-INF/maven` of _this_
+project.
+
+### Archetype sources
+All of the source and resource files to be generated into the bootstrapped project need to be
+located in `src/main/resources/archetype-resources` directory of _this_ project. A template of the
+generated `pom.xml` is located in this directory, as well as a basic README.md. Any other files that
+should be copied into the root of the generated project should be placed here. Adding files here
+will require updates to [archetype-metadata.xml][1] to ensure they are explicitly listed in the
+fileset that is copied during project generation.
+
+Additionally in `archetype-resources` is the typical Maven project directory structure:
+
+```
+archetype-resources/src/main/java
+archetype-resources/src/main/rescource
+archetype-resources/src/test/java
+archetype-resources/src/test/resources
+```
+
+The only difference is that the Java source and test class files are not in packaged subdirectories,
+they are flattened into the `java` directories. This is because the `archetype:generate` goal will
+copy those sources, substituting any Velocity properties with values provided, into the desired
+package structure, based on the value provided for the `package` property. See
+[Archetype Properties](#archetype-properties) for more on the `package` property.
+
+## Creating a Bootstrap project
+
+Generating the bootstrap project requires some properties to be set. Some properties have default
+values and do not need to be specified. All properties can be provided on the command-line used to
+generate the project, and all can be modified interactively after executing the generate command.
+
+### Archetype Properties
+The following properties are used during the bootstrap project generation. They can be specified
+when executing the generate command, or in interactive mode during bootstrap project generation:
+
+| Property | Default | Example | Description |
+| --- | --- | --- | ---| 
+| groupId | &lt;no default&gt; | com.mycompany.group | Sets the Maven `groupId` value in the pom.xml of the bootstrapped project |
+| artifactId | &lt;no default&gt; | myArtifact | Sets the Maven 'artifactId' value in the pom.xml of the bootstrapped project |
+| version | 1.0-SNAPSHOT | 1.2.3 | Sets the Maven `version` value in the pom.xml of the bootstrapped project |
+| package | &lt;groupId value&gt; | com.mycompany.poc | Controls the base package for all generated source code |
+| logging-level | DEBUG | WARN | Controls the logging level of the generated project (values must be one of &lt;TRACE&vert;DEBUG&vert;INFO&vert;WARN&vert;ERROR&gt; |
+| cassandra-host | 127.0.0.1 | 10.10.1.16 | The host or IP address of the Cassandra instance to which to connect |
+| cassandra-port | 9042 | 9042 | The port to use for the Cassandra connection |
+| datacenter | datacenter1 | datacenter1 | The name of the local datacenter |","[{'comment': ""Nit: could you align the columns?\r\nI'm generally against aligning stuff in source code, but Markdown is designed to be readable as source code as well, so it's kind of an exception."", 'commenter': 'olim7t'}]"
1140,java-driver-archetype/README.md,"@@ -0,0 +1,147 @@
+
+# Cassandra Java Driver Archetype
+
+This is a Maven Archetype project that can be used to bootstrap a simple Maven project featuring the
+Java Drivers for Cassandra. To build _this_ project, simple run a Maven install:
+
+```
+mvn clean install
+```
+
+This will install the archetype locally so you can then use it to create a bootstrap project.
+
+## Working with this Maven Archetype project
+If you are looking to create a bootstrapped project, you can skip down to
+[Creating a Bootstrap project](#creating-a-bootstrap-project)
+
+### Archetype Metadata
+The [archetype-metadata.xml][1] conforms to the [Maven Archetype Descriptor Model][2]. It defines
+properties that must be set to generate a bootstrapped project as well as fileset rules that control
+how source files are generated. The file must live in `src/main/resources/META-INF/maven` of _this_
+project.
+
+### Archetype sources
+All of the source and resource files to be generated into the bootstrapped project need to be
+located in `src/main/resources/archetype-resources` directory of _this_ project. A template of the
+generated `pom.xml` is located in this directory, as well as a basic README.md. Any other files that
+should be copied into the root of the generated project should be placed here. Adding files here
+will require updates to [archetype-metadata.xml][1] to ensure they are explicitly listed in the
+fileset that is copied during project generation.
+
+Additionally in `archetype-resources` is the typical Maven project directory structure:
+
+```
+archetype-resources/src/main/java
+archetype-resources/src/main/rescource
+archetype-resources/src/test/java
+archetype-resources/src/test/resources
+```
+
+The only difference is that the Java source and test class files are not in packaged subdirectories,
+they are flattened into the `java` directories. This is because the `archetype:generate` goal will
+copy those sources, substituting any Velocity properties with values provided, into the desired
+package structure, based on the value provided for the `package` property. See
+[Archetype Properties](#archetype-properties) for more on the `package` property.
+
+## Creating a Bootstrap project","[{'comment': 'I think we should eventually move this section to the manual.', 'commenter': 'olim7t'}]"
1140,java-driver-archetype/README.md,"@@ -0,0 +1,147 @@
+
+# Cassandra Java Driver Archetype
+
+This is a Maven Archetype project that can be used to bootstrap a simple Maven project featuring the
+Java Drivers for Cassandra. To build _this_ project, simple run a Maven install:
+
+```
+mvn clean install
+```
+
+This will install the archetype locally so you can then use it to create a bootstrap project.
+
+## Working with this Maven Archetype project
+If you are looking to create a bootstrapped project, you can skip down to
+[Creating a Bootstrap project](#creating-a-bootstrap-project)
+
+### Archetype Metadata
+The [archetype-metadata.xml][1] conforms to the [Maven Archetype Descriptor Model][2]. It defines
+properties that must be set to generate a bootstrapped project as well as fileset rules that control
+how source files are generated. The file must live in `src/main/resources/META-INF/maven` of _this_
+project.
+
+### Archetype sources
+All of the source and resource files to be generated into the bootstrapped project need to be
+located in `src/main/resources/archetype-resources` directory of _this_ project. A template of the
+generated `pom.xml` is located in this directory, as well as a basic README.md. Any other files that
+should be copied into the root of the generated project should be placed here. Adding files here
+will require updates to [archetype-metadata.xml][1] to ensure they are explicitly listed in the
+fileset that is copied during project generation.
+
+Additionally in `archetype-resources` is the typical Maven project directory structure:
+
+```
+archetype-resources/src/main/java
+archetype-resources/src/main/rescource","[{'comment': 'Small typo: rescource => resource', 'commenter': 'olim7t'}]"
1140,java-driver-archetype/src/main/resources/archetype-resources/src/main/resources/log4j.properties,"@@ -0,0 +1,14 @@
+# Set root logger level to DEBUG and its only appender to STDOUT.
+log4j.rootLogger=WARN, STDOUT
+
+# STDOUT is set to be a ConsoleAppender.
+log4j.appender.STDOUT=org.apache.log4j.ConsoleAppender
+
+# STDOUT uses PatternLayout.
+log4j.appender.STDOUT.layout=org.apache.log4j.PatternLayout
+log4j.appender.STDOUT.layout.ConversionPattern=%d{HH:mm:ss.SSS} [%t] %-5p %.36c - %m%n
+
+# Adjust the Cassandra Driver logging level here if you want more(DEBUG) or less(WARN) logs
+log4j.logger.com.datastax.oss.driver=INFO
+# Set the log level for all logs in the archetype project to DEBUG
+log4j.logger.${package}=${logging-level}","[{'comment': 'You can remove ""to DEBUG"" at the end of the comment, since this is now parameterized. Same in the log4j file.', 'commenter': 'olim7t'}]"
1140,java-driver-archetype/src/main/resources/archetype-resources/src/main/resources/log4j.properties,"@@ -0,0 +1,14 @@
+# Set root logger level to DEBUG and its only appender to STDOUT.
+log4j.rootLogger=WARN, STDOUT
+
+# STDOUT is set to be a ConsoleAppender.
+log4j.appender.STDOUT=org.apache.log4j.ConsoleAppender
+
+# STDOUT uses PatternLayout.
+log4j.appender.STDOUT.layout=org.apache.log4j.PatternLayout
+log4j.appender.STDOUT.layout.ConversionPattern=%d{HH:mm:ss.SSS} [%t] %-5p %.36c - %m%n
+
+# Adjust the Cassandra Driver logging level here if you want more(DEBUG) or less(WARN) logs
+log4j.logger.com.datastax.oss.driver=INFO","[{'comment': ""Since the app level is parameterized, should we make the driver level a parameter as well?\r\n\r\nOr maybe neither of them, editing the level in the files is not that complicated so I'm not sure if this needs to be handled by the archetype."", 'commenter': 'olim7t'}]"
1140,java-driver-archetype/src/main/resources/archetype-resources/pom.xml,"@@ -0,0 +1,155 @@
+<!--
+
+    Copyright DataStax, Inc.
+
+    Licensed under the Apache License, Version 2.0 (the ""License"");
+    you may not use this file except in compliance with the License.
+    You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+    Unless required by applicable law or agreed to in writing, software
+    distributed under the License is distributed on an ""AS IS"" BASIS,
+    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+    See the License for the specific language governing permissions and
+    limitations under the License.
+
+-->
+<project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
+         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"">
+  <modelVersion>4.0.0</modelVersion>
+  <groupId>${groupId}</groupId>
+  <artifactId>${artifactId}</artifactId>
+  <version>${version}</version>
+  <properties>
+    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
+    <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>
+    <junit.version>4.12</junit.version>
+    <slf4j.version>1.7.25</slf4j.version>
+    <logback.version>1.2.3</logback.version>
+    <log4j.version>1.2.17</log4j.version>
+    <slf.log4j.version>1.7.25</slf.log4j.version>
+    <java.driver.version>4.0.0-beta3-SNAPSHOT</java.driver.version>
+    <maven.compiler.plugin.version>3.8.0</maven.compiler.plugin.version>
+  </properties>
+  <dependencyManagement>
+    <dependencies>
+      <dependency>
+        <groupId>junit</groupId>
+        <artifactId>junit</artifactId>
+        <version>${junit.version}</version>
+        <scope>test</scope>
+      </dependency>
+      <dependency>
+        <groupId>org.slf4j</groupId>
+        <artifactId>slf4j-api</artifactId>
+        <version>${slf4j.version}</version>
+      </dependency>
+      <dependency>
+        <groupId>ch.qos.logback</groupId>
+        <artifactId>logback-classic</artifactId>
+        <version>${logback.version}</version>
+      </dependency>
+      <dependency>
+        <groupId>log4j</groupId>
+        <artifactId>log4j</artifactId>
+        <version>${log4j.version}</version>
+      </dependency>","[{'comment': 'Is there any way we could make the logging implementation a parameter of the archetype?\r\nAnd not reference log4j at all in the POM if logback was selected, or vice-versa.', 'commenter': 'olim7t'}]"
1140,java-driver-archetype/src/main/resources/archetype-resources/pom.xml,"@@ -0,0 +1,155 @@
+<!--
+
+    Copyright DataStax, Inc.
+
+    Licensed under the Apache License, Version 2.0 (the ""License"");
+    you may not use this file except in compliance with the License.
+    You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+    Unless required by applicable law or agreed to in writing, software
+    distributed under the License is distributed on an ""AS IS"" BASIS,
+    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+    See the License for the specific language governing permissions and
+    limitations under the License.
+
+-->
+<project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
+         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"">
+  <modelVersion>4.0.0</modelVersion>
+  <groupId>${groupId}</groupId>
+  <artifactId>${artifactId}</artifactId>
+  <version>${version}</version>
+  <properties>
+    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
+    <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>
+    <junit.version>4.12</junit.version>
+    <slf4j.version>1.7.25</slf4j.version>
+    <logback.version>1.2.3</logback.version>
+    <log4j.version>1.2.17</log4j.version>
+    <slf.log4j.version>1.7.25</slf.log4j.version>
+    <java.driver.version>4.0.0-beta3-SNAPSHOT</java.driver.version>","[{'comment': ""The driver version should definitely be configurable through the archetype.\r\n\r\nIf it's not provided, we'll pick the latest 4.x (according to semver, it will always be backward compatible with whatever code we put here). This can be expressed with a range: ```[4.0.0-beta1, 5)```"", 'commenter': 'olim7t'}, {'comment': ""I missed this in my fixups. I'll address it in the next one I push."", 'commenter': 'emerkle826'}, {'comment': ""OK, `latest` works too, at least for now.\r\nEventually it won't when we start releasing 5.x artifacts, but we can update the archetype to handle that."", 'commenter': 'olim7t'}, {'comment': 'I can open-end it so there is no upper bound. (i.e. `[4.0.0-beta1,]`\r\nOr, do you want the _default_ value to be that itself, as opposed to `latest` having a special meaning?', 'commenter': 'emerkle826'}]"
1140,java-driver-archetype/src/main/resources/archetype-resources/README.md,"@@ -0,0 +1,60 @@
+# Cassandra Java Driver Archetype Project
+
+If you are reading this, you've successfully created the Cassandra Java Driver archetype project.
+
+## Running the project
+
+### Ensure Cassandra is running
+
+When generating this project, values for `cassandra-host` and `cassandra-port` were provided. The
+default values are `127.0.0.1` (localhost) and `9042` respectively. Ensure that Cassandra is up and","[{'comment': 'You can filter this file too, and remind the exact values that were provided.', 'commenter': 'olim7t'}]"
1140,java-driver-archetype/src/main/resources/archetype-resources/README.md,"@@ -0,0 +1,60 @@
+# Cassandra Java Driver Archetype Project
+
+If you are reading this, you've successfully created the Cassandra Java Driver archetype project.
+
+## Running the project
+
+### Ensure Cassandra is running
+
+When generating this project, values for `cassandra-host` and `cassandra-port` were provided. The
+default values are `127.0.0.1` (localhost) and `9042` respectively. Ensure that Cassandra is up and
+running on that host and port. If the Cassandra host/port to which you want to connect has changed,
+or was not set correctly when the project was generated, you will have to edit the `Main.java`
+source file and alter this line accordingly:
+
+```
+builder.addContactPoint(InetSocketAddress.createUnresolved(""127.0.0.1"", 9042));
+```
+
+### Compiling and running the example
+
+Once you have a local Cassandra running (or have changed `Main.java` to point to some other instance
+that is running), you can build and run this project by executing the following:
+
+```
+mvn compile exec:java -Dexec.mainClass=com.mycompany.group.Main
+```
+
+where `com.mycompany.group` is the value you used when generating the project from the archetype. If
+you didn't override the default, the package should be the same as the `groupId`.","[{'comment': 'Same here, you can use filtering to show the command with the package exactly as it was provided.', 'commenter': 'olim7t'}]"
1140,java-driver-archetype/src/main/resources/archetype-resources/README.md,"@@ -0,0 +1,61 @@
+# Cassandra Java Driver Archetype Project
+
+If you are reading this, you've successfully created the Cassandra Java Driver archetype project.
+
+## Running the project
+
+### Ensure Cassandra is running
+
+Cassandra is configured at ${cassandra-host}:${cassandra-port}. Ensure that Cassandra is up and
+running on that host and port. If the Cassandra host/port to which you want to connect has changed,
+or was not set correctly when the project was generated, you will have to edit the `Main.java`
+source file and alter this line accordingly:
+
+```
+builder.addContactPoint(InetSocketAddress.createUnresolved(""${cassandra-host}"", ${cassandra-port}));","[{'comment': ""This needs to be updated since we 're not using `createUnresolved` anymore."", 'commenter': 'olim7t'}]"
1140,java-driver-archetype/src/main/resources/archetype-resources/README.md,"@@ -0,0 +1,61 @@
+# Cassandra Java Driver Archetype Project
+
+If you are reading this, you've successfully created the Cassandra Java Driver archetype project.
+
+## Running the project
+
+### Ensure Cassandra is running
+
+Cassandra is configured at ${cassandra-host}:${cassandra-port}. Ensure that Cassandra is up and
+running on that host and port. If the Cassandra host/port to which you want to connect has changed,
+or was not set correctly when the project was generated, you will have to edit the `Main.java`
+source file and alter this line accordingly:
+
+```
+builder.addContactPoint(InetSocketAddress.createUnresolved(""${cassandra-host}"", ${cassandra-port}));
+```
+
+### Compiling and running the example
+
+Once you have a local Cassandra running (or have changed `Main.java` to point to some other instance
+that is running), you can build this project by executing the following:
+
+```
+mvn clean install
+```
+
+Then run the CQL demo with:
+
+```
+mvn exec:java -Dexec.mainClass=${package}.Main -pl ${artifactId}-cql","[{'comment': 'This fails if `${artifactId}-core` is not installed first.\r\nBut I think we should go back to a single module in the generated project, see my other comment.', 'commenter': 'olim7t'}, {'comment': ""Yes, it will. That's why I put the `mvn clean install` part right above. But it's moot if we go back to generating a single module."", 'commenter': 'emerkle826'}]"
1140,java-driver-archetype/src/main/resources/archetype-resources/__rootArtifactId__-cql/src/main/java/Main.java,"@@ -0,0 +1,138 @@
+package ${package};
+
+import com.datastax.oss.driver.api.core.CqlIdentifier;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.CqlSessionBuilder;
+import com.datastax.oss.driver.api.core.cql.ColumnDefinitions;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import com.datastax.oss.driver.api.core.cql.Row;
+import com.datastax.oss.driver.api.core.metadata.Metadata;
+import com.datastax.oss.driver.api.core.metadata.schema.KeyspaceMetadata;
+import java.io.IOException;
+import java.io.InputStreamReader;
+import java.io.LineNumberReader;
+import java.io.PrintStream;
+import java.net.InetSocketAddress;
+import java.util.Iterator;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class Main {","[{'comment': 'Nice, this strikes a good balance between doing more than a simple query without being too complicated.', 'commenter': 'olim7t'}]"
1140,java-driver-archetype/src/main/resources/archetype-resources/__rootArtifactId__-cql/src/main/java/Main.java,"@@ -0,0 +1,138 @@
+package ${package};
+
+import com.datastax.oss.driver.api.core.CqlIdentifier;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.CqlSessionBuilder;
+import com.datastax.oss.driver.api.core.cql.ColumnDefinitions;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import com.datastax.oss.driver.api.core.cql.Row;
+import com.datastax.oss.driver.api.core.metadata.Metadata;
+import com.datastax.oss.driver.api.core.metadata.schema.KeyspaceMetadata;
+import java.io.IOException;
+import java.io.InputStreamReader;
+import java.io.LineNumberReader;
+import java.io.PrintStream;
+import java.net.InetSocketAddress;
+import java.util.Iterator;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class Main {
+
+  private static final Logger LOG = LoggerFactory.getLogger(Main.class);
+  private static final String WELCOME =
+      ""This is a CQL demo. "" +
+      ""See https://docs.datastax.com/en/cql/3.3/cql/cql_reference/cqlCommandsTOC.html "" +
+      ""for more info on CQL commands. Type 'EXIT' to quit."";
+
+  private final PrintStream output;
+
+  private Main() {
+    this.output = System.out;
+  }
+
+  private void promptForQuery(String msg) {
+    if (msg != null) {
+      output.println();
+      output.println(msg);
+    }
+    output.println();
+    output.print(""cql-demo> "");
+  }
+
+  private void printKeyspaceTables(KeyspaceMetadata ksMetadata) {
+    System.out.println();
+    final String ksOutput = ""Keyspace "" + ksMetadata.getName().asInternal();
+    System.out.println(ksOutput);
+    for (int i = 0; i < ksOutput.length(); ++i) {
+      System.out.print(""-"");
+    }
+    System.out.println();
+    for (CqlIdentifier cqlId : ksMetadata.getTables().keySet()) {
+      System.out.println(cqlId.asInternal());
+    }
+  }
+
+  private void handleDescribe(CqlSession session, String query) {
+    // strip the describe command off the query
+    String describeTarget = query.substring(""describe"".length() + 1);
+    // get the metadata
+    Metadata metadata = session.getMetadata();
+    if (describeTarget.startsWith(""keyspaces"") || describeTarget.startsWith(""KEYSPACES"")) {
+      System.out.println();
+      for (CqlIdentifier cqlId : metadata.getKeyspaces().keySet()) {
+        System.out.print(cqlId.asInternal() + ""  "");
+      }
+      System.out.println();
+    } else if (describeTarget.startsWith(""tables"") || describeTarget.startsWith(""TABLES"")) {
+      // dump all tables from the current keyspace or all keyspaces if no current keyspace
+      if (session.getKeyspace().isPresent()) {
+        // get the tables from the current keyspace
+        printKeyspaceTables(metadata.getKeyspace(session.getKeyspace().get()).get());
+      } else {
+        for (KeyspaceMetadata ksMetadata : metadata.getKeyspaces().values()) {
+          printKeyspaceTables(ksMetadata);
+        }
+      }
+    } else {
+      System.out.println(""\nDescribe target not implemented: '"" + describeTarget + ""'"");
+    }
+  }
+
+  private void executeQuery(CqlSession session, String query) {
+    try {
+      ResultSet rs = session.execute(query);
+      Iterator<Row> iterator = rs.iterator();
+      while (iterator.hasNext()) {
+        Row row = iterator.next();
+        ColumnDefinitions cd = row.getColumnDefinitions();
+        for (int i = 0; i < cd.size(); ++i) {
+          System.out.println(cd.get(i).getName() + "":  "" + row.getObject(i).toString());
+        }
+      }
+    } catch (Exception ex) {
+      // something went wrong with the query, just dump the stacktrace to the output
+      ex.printStackTrace(output);
+    }
+  }
+  /**
+   * Basic cqlsh-like prompt. It loops until the user types ""EXIT"", executing queries and dumping
+   * the response to the console (System.out).
+   */
+  private void cqlshLite(CqlSession session, LineNumberReader input) throws IOException {
+    // provide a prompt
+    promptForQuery(WELCOME);
+    // get the query
+    String query = input.readLine();
+    // execute the query
+    while (!""exit"".equalsIgnoreCase(query)) {
+      if (query.startsWith(""describe"") || query.startsWith(""DESCRIBE"")) {
+        handleDescribe(session, query);
+      } else {","[{'comment': 'Nit: could we add a ""help"" command that prints a quick reminder of available commands?', 'commenter': 'olim7t'}]"
1140,java-driver-archetype/src/main/resources/archetype-resources/__rootArtifactId__-cql/src/main/java/Main.java,"@@ -0,0 +1,138 @@
+package ${package};
+
+import com.datastax.oss.driver.api.core.CqlIdentifier;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.CqlSessionBuilder;
+import com.datastax.oss.driver.api.core.cql.ColumnDefinitions;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import com.datastax.oss.driver.api.core.cql.Row;
+import com.datastax.oss.driver.api.core.metadata.Metadata;
+import com.datastax.oss.driver.api.core.metadata.schema.KeyspaceMetadata;
+import java.io.IOException;
+import java.io.InputStreamReader;
+import java.io.LineNumberReader;
+import java.io.PrintStream;
+import java.net.InetSocketAddress;
+import java.util.Iterator;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class Main {
+
+  private static final Logger LOG = LoggerFactory.getLogger(Main.class);
+  private static final String WELCOME =
+      ""This is a CQL demo. "" +
+      ""See https://docs.datastax.com/en/cql/3.3/cql/cql_reference/cqlCommandsTOC.html "" +
+      ""for more info on CQL commands. Type 'EXIT' to quit."";
+
+  private final PrintStream output;
+
+  private Main() {
+    this.output = System.out;
+  }
+
+  private void promptForQuery(String msg) {
+    if (msg != null) {
+      output.println();
+      output.println(msg);
+    }
+    output.println();
+    output.print(""cql-demo> "");
+  }
+
+  private void printKeyspaceTables(KeyspaceMetadata ksMetadata) {
+    System.out.println();
+    final String ksOutput = ""Keyspace "" + ksMetadata.getName().asInternal();
+    System.out.println(ksOutput);
+    for (int i = 0; i < ksOutput.length(); ++i) {
+      System.out.print(""-"");
+    }
+    System.out.println();
+    for (CqlIdentifier cqlId : ksMetadata.getTables().keySet()) {
+      System.out.println(cqlId.asInternal());
+    }
+  }
+
+  private void handleDescribe(CqlSession session, String query) {
+    // strip the describe command off the query
+    String describeTarget = query.substring(""describe"".length() + 1);
+    // get the metadata
+    Metadata metadata = session.getMetadata();
+    if (describeTarget.startsWith(""keyspaces"") || describeTarget.startsWith(""KEYSPACES"")) {
+      System.out.println();
+      for (CqlIdentifier cqlId : metadata.getKeyspaces().keySet()) {
+        System.out.print(cqlId.asInternal() + ""  "");
+      }
+      System.out.println();
+    } else if (describeTarget.startsWith(""tables"") || describeTarget.startsWith(""TABLES"")) {
+      // dump all tables from the current keyspace or all keyspaces if no current keyspace
+      if (session.getKeyspace().isPresent()) {
+        // get the tables from the current keyspace
+        printKeyspaceTables(metadata.getKeyspace(session.getKeyspace().get()).get());
+      } else {
+        for (KeyspaceMetadata ksMetadata : metadata.getKeyspaces().values()) {
+          printKeyspaceTables(ksMetadata);
+        }
+      }
+    } else {
+      System.out.println(""\nDescribe target not implemented: '"" + describeTarget + ""'"");
+    }
+  }
+
+  private void executeQuery(CqlSession session, String query) {
+    try {
+      ResultSet rs = session.execute(query);
+      Iterator<Row> iterator = rs.iterator();
+      while (iterator.hasNext()) {
+        Row row = iterator.next();","[{'comment': 'This would look a bit cleaner with a for-each loop:\r\n```java\r\nfor (Row row: rs) {\r\n  ...\r\n}\r\n```', 'commenter': 'olim7t'}]"
1140,java-driver-archetype/src/main/resources/archetype-resources/__rootArtifactId__-web/pom.xml,"@@ -0,0 +1,63 @@
+
+
+<!--
+
+    Copyright DataStax, Inc.
+
+    Licensed under the Apache License, Version 2.0 (the ""License"");
+    you may not use this file except in compliance with the License.
+    You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+    Unless required by applicable law or agreed to in writing, software
+    distributed under the License is distributed on an ""AS IS"" BASIS,
+    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+    See the License for the specific language governing permissions and
+    limitations under the License.
+
+-->
+<project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
+         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"">
+  <modelVersion>4.0.0</modelVersion>
+  <parent>
+    <groupId>${groupId}</groupId>
+    <artifactId>${rootArtifactId}</artifactId>
+    <version>${version}</version>
+  </parent>
+  <artifactId>${artifactId}</artifactId>","[{'comment': 'When I mentioned multi-module I was thinking more of the archetype project itself. So a multi-module project that deploys multiple archetypes, instead of an archetype that creates a multi-module project.\r\n\r\nI think generated projects should remain single-module for simplicity, there will be a bit of duplication across archetypes for configuration files and stuff, but that does not really matter.', 'commenter': 'olim7t'}, {'comment': 'Ah, I totally misunderstood the original comment. I can flip it back.', 'commenter': 'emerkle826'}]"
1152,upgrade_guide/README.md,"@@ -113,6 +113,15 @@ programming][4.x async programming] and [paging][4.x paging].
 [4.x async programming]: http://docs.datastax.com/en/developer/java-driver/4.0/manual/core/async/
 [4.x paging]: http://docs.datastax.com/en/developer/java-driver/4.0/manual/core/paging/
 
+#### CQL to Java type mappings
+
+Since the driver now has access to Java 8 types, some of the [CQL to Java type
+mappings] have changed, particularly when it comes to [temporal types] such
+as `date` and `timestamp`.","[{'comment': 'I think it would be even more helpful if you mentioned what changed exactly: `getDate` replaced with `getInstant` and `getLocalDate` that now returns `java.time.LocalDate`.', 'commenter': 'adutra'}, {'comment': ""Sure, I can add that.  I think there are a few changes beyond that.  I'll add a listing of those similar to what was done for the 3.0.0 section of the upgrade guide."", 'commenter': 'tolbertam'}, {'comment': 'Done', 'commenter': 'tolbertam'}]"
1153,manual/object_mapper/creating/README.md,"@@ -363,6 +363,26 @@ private Map<String, List<Address>> frozenValueMap;
 [frozenkey]:http://docs.datastax.com/en/drivers/java/3.6/com/datastax/driver/mapping/annotations/FrozenKey.html
 [frozenvalue]:http://docs.datastax.com/en/drivers/java/3.6/com/datastax/driver/mapping/annotations/FrozenValue.html
 
+#### Prefer Frozen Collections
+
+If `Mapper.save` is used to create and update entities, it is recommended to
+use frozen collections over non-frozen collections.
+
+Frozen collections in Cassandra are serialized as a single cell value where
+non-frozen collections serialize each individual element in a collection as a
+cell.
+
+Since `Mapper.save` provides the entire collection for a entity field value on","[{'comment': 'Nit: ""for _an_ entity field"".', 'commenter': 'adutra'}]"
1153,manual/object_mapper/creating/README.md,"@@ -363,6 +363,26 @@ private Map<String, List<Address>> frozenValueMap;
 [frozenkey]:http://docs.datastax.com/en/drivers/java/3.6/com/datastax/driver/mapping/annotations/FrozenKey.html
 [frozenvalue]:http://docs.datastax.com/en/drivers/java/3.6/com/datastax/driver/mapping/annotations/FrozenValue.html
 
+#### Prefer Frozen Collections
+
+If `Mapper.save` is used to create and update entities, it is recommended to
+use frozen collections over non-frozen collections.
+
+Frozen collections in Cassandra are serialized as a single cell value where
+non-frozen collections serialize each individual element in a collection as a
+cell.
+
+Since `Mapper.save` provides the entire collection for a entity field value on
+each invocation, it is more efficient to use frozen collections as the entire
+collection is serialized as one cell.
+
+Also, when using non-frozen collections, on INSERT cassandra must","[{'comment': 'cassandra -> Cassandra', 'commenter': 'adutra'}]"
1160,core/src/main/java/com/datastax/oss/driver/internal/core/control/ControlConnection.java,"@@ -273,12 +275,28 @@ private void init(boolean listenToClusterEvents, boolean reconnectOnFailure) {
               firstConnectionAttemptFuture.complete(null);
             },
             error -> {
-              if (reconnectOnFailure && !closeWasCalled) {
+              boolean authFailure = true;","[{'comment': 'we are assuming upfront that error is an `authFailure`? \r\nI think that `if (error instanceof AllNodesFailedException) {}` is `false` it does not mean that it is an `authFailure`, or is it?', 'commenter': 'tomekl007'}]"
1160,core/src/main/java/com/datastax/oss/driver/internal/core/control/ControlConnection.java,"@@ -273,12 +275,28 @@ private void init(boolean listenToClusterEvents, boolean reconnectOnFailure) {
               firstConnectionAttemptFuture.complete(null);
             },
             error -> {
-              if (reconnectOnFailure && !closeWasCalled) {
+              boolean authFailure = true;
+              if (error instanceof AllNodesFailedException) {
+                Collection<Throwable> errors =
+                    ((AllNodesFailedException) error).getErrors().values();
+                for (Throwable nodeError : errors) {
+                  if (!(nodeError instanceof AuthenticationException)) {
+                    authFailure = false;","[{'comment': 'You could insert a `break` here. Actually, even better: you could extract this logic into a separate method.', 'commenter': 'adutra'}]"
1160,core/src/main/java/com/datastax/oss/driver/internal/core/control/ControlConnection.java,"@@ -273,12 +275,28 @@ private void init(boolean listenToClusterEvents, boolean reconnectOnFailure) {
               firstConnectionAttemptFuture.complete(null);
             },
             error -> {
-              if (reconnectOnFailure && !closeWasCalled) {
+              boolean authFailure = true;
+              if (error instanceof AllNodesFailedException) {","[{'comment': ""Shouldn't we try to generify a bit this logic? Other errors are probably as fatal as authentication errors, e.g. wrong SSL certificates. I suggest creating a method called `isFatalError(Throwable)` and checking all possible cases inside it."", 'commenter': 'adutra'}]"
1162,core/src/main/java/com/datastax/oss/driver/api/core/connection/ReconnectionPolicy.java,"@@ -49,9 +51,21 @@
   @NonNull
   ReconnectionSchedule newNodeSchedule(@NonNull Node node);
 
-  /** Creates a new schedule for the control connection. */
+  /**
+   * Creates a new schedule for the control connection.
+   *
+   * @param isInitialConnection whether this schedule is generated for the driver's initial attempt
+   *     to connect to the cluster.
+   *     <ul>","[{'comment': 'I think we should avoid long texts like this one inside javadoc tags. With the default CSS style, tag texts are rendered with a fixed-size font and all the formatting is lost:\r\n\r\n![image](https://user-images.githubusercontent.com/463876/50488421-9dbdc380-09e9-11e9-963b-b712ba2c7480.png)\r\n', 'commenter': 'adutra'}, {'comment': ""I hadn't noticed that. I'll move those explanations to the main description then."", 'commenter': 'olim7t'}]"
1162,core/src/main/java/com/datastax/oss/driver/internal/core/control/ControlConnection.java,"@@ -294,6 +293,15 @@ private void init(boolean listenToClusterEvents, boolean reconnectOnFailure) {
       }
     }
 
+    private ReconnectionSchedule newSchedule() {
+      boolean isInitialConnection =
+          reconnectOnInit
+              // This is always set after the initial attempt, regardless of its outcome.
+              // If the first attempt failed, we create the schedule before setting it.
+              && !firstConnectionAttemptFuture.isDone();","[{'comment': ""This is overly complex to follow, couldn't you create a dedicated boolean variable to store that information (initial/not initial) instead?\r\n\r\nBesides, are you sure that `firstConnectionAttemptFuture.isDone()` is always false for the initial attempt? From what I understand, in case the initial attempt fails, the thread executing the initial sequence will first call `reconnection.start()` line 276 then complete `firstConnectionAttemptFuture` exceptionally line 289; but since the reconnection process is async, the future may or may not be completed before `newSchedule` is called."", 'commenter': 'adutra'}, {'comment': ""I don't think concurrency is an issue, `Reconnection` uses the same admin thread and `Reconnection.start()` creates the schedule synchronously.\r\n\r\nBut you are right that the code is too convoluted. What makes it complicated is that the control connection can be initialized in two ways:\r\n\r\n1. by default, it's the component that determines if the initial connection to the cluster succeeds.\r\n2. But you could also have a custom `TopologyMonitor` that does that a different way, and only use the control connection for schema metadata and server events. In that case the control connection is initialized late, and `newControlConnectionSchedule()` must never be called with a `true` argument.\r\n\r\nI think I can make the code more clear if there are separate init methods for those two cases. I'll give it a try."", 'commenter': 'olim7t'}, {'comment': ""> I don't think concurrency is an issue, Reconnection uses the same admin thread and Reconnection.start() creates the schedule synchronously.\r\n\r\nHmm my bad, `newSchedule()` is invoked synchronously while `reconnect()` is invoked asynchronously, so the future will never be done when `newSchedule()` is executed.\r\n\r\n> I think I can make the code more clear if there are separate init methods for those two cases. I'll give it a try.\r\n\r\nðŸ’¯ "", 'commenter': 'adutra'}]"
1172,core/src/main/java/com/datastax/oss/driver/internal/core/pool/ChannelPool.java,"@@ -259,7 +259,8 @@ private SingleThreaded(
               () -> reconnectionPolicy.newNodeSchedule(node),
               this::addMissingChannels,
               () -> eventBus.fire(ChannelEvent.reconnectionStarted(node)),
-              () -> eventBus.fire(ChannelEvent.reconnectionStopped(node)));
+              () -> eventBus.fire(ChannelEvent.reconnectionStopped(node)),
+              error -> {});
       this.configListenerKey =","[{'comment': ""This may need to be changed... the onFail here is not doing anything. I'm not 100% clear on what needs to happen here to successfuly close the channel."", 'commenter': 'GregBestland'}, {'comment': 'I think the right thing to do is to force the node down. If the policy decides to abort it means we have really given up on this node. You can see in `ChannelPool.onAllConnected()` how it already does that for invalid keyspace, instead of doing it there we should rethrow it to the policy (see my other comment).\r\n\r\nWhen `onAllConnected` throws, there might be multiple errors, so I think we should use a wrapper:\r\n```java\r\nclass PoolReconnectionException extends DriverException {\r\n  /** The individual error for each connection that was attempted. */\r\n  List<Throwable> getConnectionErrors();\r\n\r\n  /** If the reconnection attempted to complete a pool that already had active channels, their count. */\r\n  int getActiveChannels();\r\n}\r\n```\r\n', 'commenter': 'olim7t'}]"
1172,core/src/main/java/com/datastax/oss/driver/internal/core/util/concurrent/Reconnection.java,"@@ -70,21 +73,24 @@ public Reconnection(
       Supplier<ReconnectionSchedule> scheduleSupplier,
       Callable<CompletionStage<Boolean>> reconnectionTask,
       Runnable onStart,
-      Runnable onStop) {
+      Runnable onStop,
+      Consumer<Throwable> onFail) {","[{'comment': 'I think `onAbort` might be a better name for this, and it would be worth adding a javadoc comment to explain that this is what happens when the policy has decided to stop reconnecting.', 'commenter': 'olim7t'}]"
1172,core/src/main/java/com/datastax/oss/driver/internal/core/util/concurrent/Reconnection.java,"@@ -181,28 +187,37 @@ private void reallyStop() {
     reconnectionSchedule = null;
   }
 
-  private void scheduleNextAttempt() {
+  private void scheduleNextAttempt(Throwable throwable) {
     assert executor.inEventLoop();
     state = State.SCHEDULED;
     if (reconnectionSchedule == null) { // happens if reconnectNow() while we were stopped
       reconnectionSchedule = scheduleSupplier.get();
     }
-    Duration nextInterval = reconnectionSchedule.nextDelay();
-    LOG.debug(""[{}] Scheduling next reconnection in {}"", logPrefix, nextInterval);
-    nextAttempt = executor.schedule(reconnectionTask, nextInterval.toNanos(), TimeUnit.NANOSECONDS);
-    nextAttempt.addListener(
-        (Future<CompletionStage<Boolean>> f) -> {
-          if (f.isSuccess()) {
-            onNextAttemptStarted(f.getNow());
-          } else if (!f.isCancelled()) {
-            Loggers.warnWithException(
-                LOG,
-                ""[{}] Uncaught error while starting reconnection attempt"",
-                logPrefix,
-                f.cause());
-            scheduleNextAttempt();
-          }
-        });
+    Optional<Duration> nextInterval =
+        reconnectionSchedule.nextDelay(Optional.ofNullable(throwable));
+    if (nextInterval.isPresent()) {
+
+      LOG.debug(""[{}] Scheduling next reconnection in {}"", logPrefix, nextInterval);
+      nextAttempt =
+          executor.schedule(reconnectionTask, nextInterval.get().toNanos(), TimeUnit.NANOSECONDS);
+      nextAttempt.addListener(
+          (Future<CompletionStage<Boolean>> f) -> {
+            if (f.isSuccess()) {
+              onNextAttemptStarted(f.getNow());
+            } else if (!f.isCancelled()) {
+              Loggers.warnWithException(
+                  LOG,
+                  ""[{}] Uncaught error while starting reconnection attempt"",
+                  logPrefix,
+                  f.cause());
+              scheduleNextAttempt(throwable);
+            }
+          });
+    } else {
+      Loggers.warnWithException(
+          LOG, ""Reconnection attempt aborted, at discretion of reconnection policy"", logPrefix);","[{'comment': ""This should be a debug log. If we're aborting because the policy decided to, everything is working as expected, so we don't need to alert ops.\r\nIf people want a message at a different level, they can issue it from their policy."", 'commenter': 'olim7t'}]"
1172,core/src/main/java/com/datastax/oss/driver/internal/core/util/concurrent/Reconnection.java,"@@ -229,7 +244,7 @@ private void onNextAttemptCompleted(Boolean success, Throwable error) {
         reallyStop();
       } else {
         assert state == State.ATTEMPT_IN_PROGRESS;
-        scheduleNextAttempt();
+        scheduleNextAttempt(error);","[{'comment': 'This assumes that `reconnectionTask` will rethrow errors, which is not the case currently because the previous convention was that tasks would handle errors on their side an only return a success/failure boolean. So only unexpected errors will be passed here; see for example how `ChannelPool.onAllConnected()` swallows authentication errors, we want them to be reported to the policy now.\r\n\r\nI think we need to change `reconnectionTask` to `Callable<CompletionStage<Void>>`: a successful stage means stop reconnecting, and failed stage means pass the error to the policy (which is already done here, we just need to remove the warning about unexpected errors).\r\nThen `ChannelPool.onAllConnected()` can simply rethrow errors (see my other comment about throwing a wrapper), and `ControlConnection.reconnect` will do:\r\n```java\r\nconnect (\r\n  ...\r\n  error -> result.completeExceptionally(error));\r\n```', 'commenter': 'olim7t'}, {'comment': ""I got most of the way through this... honestly I think this is the wrong way to solve this problem. Not the reconnectionTask in general, but the entire idea of delegating when to retry to the underlying policy.\r\n\r\nMuch of the validation of when to retry and when not to is getting pushed entirely down into the retry policies nextDelay()... this leaves quite a bit discretion in the retry policy, and also complicates the implementation of the retry policy as they need to know the intricacies of each scenario. I don't think it's realistic for a end user to be able to extend retry policy in this manner outside of our team, and I think I'm likely to introduce a bunch of bugs in the process. It also invites duplication of code.. since the retry policies scheduler is a private inner class, which make inheritance a little tricky.\r\n\r\nI talked to a member of the evangelist team. I think they would be satisfied simply by having the log level increased in the cases of authentication failure. This would be far easier way to solve their problem. I could make this configurable, and only set it on the DSOD Connector\r\n\r\n\r\n"", 'commenter': 'GregBestland'}, {'comment': ""This does seem to be a bit cumbersome to have to propagate the connection Exceptions down to the ReconnectionPolicy in order to determine if we should abort the reconnection attempt(s). And I agree that end users trying to provide a custom implementation for the policy will have a difficult time if they aren't aware of all the reasons a connection attempt would fail and how they would manifest themselves in the code/Exceptions.\r\nHowever, if all we do is increase the log level, the only thing that does is make it a little more obvious (if you are looking at the logs) that the client can't connect because it can't authenticate, correct? It will still keep retrying, and the log will be the only indication that it's not hung."", 'commenter': 'emerkle826'}]"
1172,core/src/test/java/com/datastax/oss/driver/internal/core/control/ControlConnectionTest.java,"@@ -525,7 +532,7 @@ public void should_close_channel_if_closed_during_reconnection() {
     channel1.close();
     waitForPendingAdminTasks();
     Mockito.verify(eventBus).fire(ChannelEvent.channelClosed(node1));
-    Mockito.verify(reconnectionSchedule).nextDelay();
+    Mockito.verify(reconnectionSchedule).nextDelay(Optional.empty());","[{'comment': 'Maybe we can name that `Optional.empty()` to state the intent of the test? \r\nlike `static Optional<T> NO_DELAY`', 'commenter': 'tomekl007'}]"
1172,core/src/test/java/com/datastax/oss/driver/internal/core/util/concurrent/ReconnectionTest.java,"@@ -295,7 +321,8 @@ public void should_stop_while_attempt_in_progress(boolean outcome) {
   @Test
   public void should_restart_after_stopped_while_attempt_in_progress() {
     // Given
-    Mockito.when(reconnectionSchedule.nextDelay()).thenReturn(Duration.ofNanos(1));
+    Mockito.when(reconnectionSchedule.nextDelay(Optional.empty()))","[{'comment': 'I think that we are missing test case when `nextDelay()` is getting not `Optional.empty` as an argument', 'commenter': 'tomekl007'}, {'comment': ""~There is one place where it's not an empty Optional: https://github.com/datastax/java-driver/pull/1172/files#diff-513c8e00357a995493fdfebc12eed677R65\r\nBut I think that is the only one.~ My mistake, that uses empty as well"", 'commenter': 'emerkle826'}]"
1172,core/revapi.json,"@@ -700,7 +700,49 @@
         ""newArchive"": ""com.datastax.oss:java-driver-core:jar:4.0.0-rc1-SNAPSHOT"",
         ""elementKind"": ""method"",
         ""justification"": ""JAVA-2077: Allow reconnection policy to detect first connection attempt""
-      }
+      },
+      {
+        ""code"": ""java.method.numberOfParametersChanged"",","[{'comment': 'do we need to make it not backward compatible change? We may introduce `newDelay()` method with parameter while leaving old parameterless version. I am curious what are our guidelines - when we can introduce non-compatible change?', 'commenter': 'tomekl007'}, {'comment': ""We're still pre-GA so we can afford breaking changes, and in that case I think it really makes sense so we don't need to complicate the API for the sake of backward compatibility."", 'commenter': 'olim7t'}]"
1172,core/src/main/java/com/datastax/oss/driver/internal/core/pool/PoolReconnectionException.java,"@@ -0,0 +1,50 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.pool;
+
+import com.datastax.oss.driver.api.core.DriverException;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import java.util.List;
+
+public class PoolReconnectionException extends DriverException {
+  /** The individual error for each connection that was attempted. */
+  List<Throwable> connectionErrors;
+
+  /**
+   * If the reconnection attempted to complete a pool that already had active channels, their count.
+   */
+  int getActiveChannels;
+
+  public PoolReconnectionException(List<Throwable> errors, int activeChannels) {
+    super(null, null, errors.get(0), true);
+    this.connectionErrors = errors;
+    this.getActiveChannels = activeChannels;
+  }
+
+  @NonNull
+  @Override
+  public DriverException copy() {
+    return new PoolReconnectionException(getConnectionErrors(), getActiveChannels);
+  }
+
+  public int getGetActiveChannels() {
+    return getActiveChannels;","[{'comment': ""where we plan to use that information? I don't see usage of that method anywhere"", 'commenter': 'tomekl007'}]"
1172,core/src/main/java/com/datastax/oss/driver/internal/core/connection/ExponentialReconnectionPolicy.java,"@@ -143,6 +147,21 @@ public void close() {
     @NonNull
     @Override
     public Optional<Duration> nextDelay(Optional<Throwable> throwable) {
+      if (throwable.isPresent() && throwable.get() instanceof PoolReconnectionException) {","[{'comment': ""Logic from here and from `ConstantReconnectionPolicy.java` is the same. Can't we extract it to separate class and create `static` method for example: `checkIfThrowableIsFatal`?"", 'commenter': 'tomekl007'}]"
1173,core/src/main/java/com/datastax/oss/driver/api/core/metadata/TokenMap.java,"@@ -144,4 +144,8 @@
   default Set<Node> getReplicas(@NonNull String keyspaceName, @NonNull TokenRange range) {
     return getReplicas(CqlIdentifier.fromCql(keyspaceName), range);
   }
+
+  /** The name of the partitioner class in use, as reported by the Cassandra nodes. */
+  @NonNull
+  String getPartitionerName();","[{'comment': 'Maybe instead of using `String`, we could introduce type like `PartitionerName` to make it more strong typed?', 'commenter': 'tomekl007'}, {'comment': ""I don't think we need strong type safety because this value is only exposed for reading, the user never passes it into the API.\r\nBesides, the partitioner is pluggable(1), so there's no pre-defined set of values.\r\n\r\n(1) at least in theory: you would have to write the server-side component and then plug-in your own `TokenFactoryRegistry` in the driver, that's a lot of work but feasible nevertheless."", 'commenter': 'olim7t'}]"
1173,core/src/main/java/com/datastax/oss/driver/internal/core/metadata/token/ByteOrderedTokenFactory.java,"@@ -26,6 +26,16 @@
 public class ByteOrderedTokenFactory implements TokenFactory {
 
   public static final ByteOrderedToken MIN_TOKEN = new ByteOrderedToken(ByteBuffer.allocate(0));
+  private final String partitionerName;
+
+  public ByteOrderedTokenFactory(String partitionerName) {
+    this.partitionerName = partitionerName;
+  }
+
+  @Override","[{'comment': 'Missing `@NonNull` annotation.', 'commenter': 'adutra'}, {'comment': ""`TokenFactory` doesn't have any nullability annotations yet. Did we ever come up with a rule for internal stuff? I can add it in a separate commit."", 'commenter': 'olim7t'}, {'comment': ""We don't, but you added one to `RandomTokenFactory`, so I figured you would like to add the same annotations to other implementations. (Tip: IntelliJ usually suggests to push down nullability annotations from the interface to all implementations)."", 'commenter': 'adutra'}]"
1173,core/src/main/java/com/datastax/oss/driver/internal/core/metadata/token/Murmur3TokenFactory.java,"@@ -26,6 +27,22 @@
 
   public static final Murmur3Token MIN_TOKEN = new Murmur3Token(Long.MIN_VALUE);
   public static final Murmur3Token MAX_TOKEN = new Murmur3Token(Long.MAX_VALUE);
+  private final String partitionerName;
+
+  public Murmur3TokenFactory(String partitionerName) {
+    this.partitionerName = partitionerName;
+  }
+
+  // Only used from unit tests, where repeating the name everywhere is not really relevant
+  @VisibleForTesting
+  public Murmur3TokenFactory() {
+    this(""org.apache.cassandra.dht.Murmur3Partitioner"");
+  }
+
+  @Override
+  public String getPartitionerName() {","[{'comment': 'Missing `@NonNull` annotation.', 'commenter': 'adutra'}]"
1173,core/src/main/java/com/datastax/oss/driver/internal/core/metadata/token/DefaultTokenFactoryRegistry.java,"@@ -35,13 +35,13 @@ public DefaultTokenFactoryRegistry(InternalDriverContext context) {
   public TokenFactory tokenFactoryFor(String partitioner) {
     if (partitioner.endsWith(""Murmur3Partitioner"")) {","[{'comment': ""Is it really realistic to imagine that a different Murmur3 partitioner could exist other than `org.apache.cassandra.dht.Murmur3Partitioner`? And even so, shouldn't we error out in that case instead of assume that it's the same partitioner? I wonder if we shouldn't simply store the FQCN of each partitioner as a public constant field in each `TokenFactory` implementation and then reference them here using `equals` instead of `endsWith`."", 'commenter': 'adutra'}, {'comment': ""Agreed, I copied that blindly from the 3.x codebase, but I don't think it's justified. It might be a remnant from the server side code where the package name can be omitted in the YAML file, but I don't think that ever propagates to system tables, we should always get the fully-qualified name."", 'commenter': 'olim7t'}]"
1173,integration-tests/src/test/java/com/datastax/oss/driver/api/core/metadata/TokenITBase.java,"@@ -339,4 +343,16 @@ public void should_create_token_from_partition_key() {
     assertThat(tokenMap.newToken(TypeCodecs.INT.encodePrimitive(1, protocolVersion)))
         .isEqualTo(expected);
   }
+
+  private TokenMap getTokenMap() {
+    Optional<? extends TokenMap> maybeTokenMap = session().getMetadata().getTokenMap();
+    if (maybeTokenMap.isPresent()) {","[{'comment': 'Could be simplified to:\r\n\r\n```\r\nassertThat(maybeTokenMap).isPresent().hasValueSatisfying(tokenMap -> {\r\n  assertThat(tokenMap.getPartitionerName()).isEqualTo(expectedPartitionerName);\r\n});\r\n```', 'commenter': 'adutra'}, {'comment': '""simplified""? ðŸ˜‰ \r\nMore seriously, I had also considered:\r\n```java\r\n    return session()\r\n        .getMetadata()\r\n        .getTokenMap()\r\n        .map(\r\n            tokenMap -> {\r\n              assertThat(tokenMap.getPartitionerName()).isEqualTo(expectedPartitionerName);\r\n              return tokenMap;\r\n            })\r\n        .orElseThrow(() -> new AssertionError(""Expected token map to be present""));\r\n```\r\nwhich has a bit more of a ""functional"" flavor but I still find it about as ugly as the imperative version.\r\nI\'m often disappointed by `Optional` code, without algebraic types or pattern matching you can\'t fully take advantage of it.', 'commenter': 'olim7t'}, {'comment': ""Well, 4 lines instead of 8 is a simplification to me ;-) but it doesn't matter really, the current version is fine enough."", 'commenter': 'adutra'}]"
1176,core/src/main/java/com/datastax/oss/driver/internal/core/control/ControlConnection.java,"@@ -501,6 +515,20 @@ private void forceClose() {
     }
   }
 
+  private boolean isAuthFailure(Throwable error) {
+    boolean authFailure = true;
+    if (error instanceof AllNodesFailedException) {
+      Collection<Throwable> errors = ((AllNodesFailedException) error).getErrors().values();","[{'comment': 'if `getErrors()` will return empty connection we will treat it as the `AuthFailure`. Is it ok for us? Is it possible?', 'commenter': 'tomekl007'}, {'comment': ""That SHOULD never happen, I'll add a check regardless."", 'commenter': 'GregBestland'}]"
1176,core/src/main/java/com/datastax/oss/driver/internal/core/control/ControlConnection.java,"@@ -273,6 +275,13 @@ private void init(boolean listenToClusterEvents, boolean reconnectOnFailure) {
               firstConnectionAttemptFuture.complete(null);
             },
             error -> {
+              if (isAuthFailure(error)) {
+                Loggers.warnWithException(
+                    LOG,
+                    ""[{}] Authentication errors encountered on all contact points. Please check your authentication configuration."",
+                    logPrefix,
+                    error);
+              }","[{'comment': ""The errors have already been logged individually in `connect`, do we need another log here?\r\nFor example run `PlainTextAuthProviderIT.should_not_connect_without_credentials()`, there are two warning logs.\r\n\r\nIf we decide to keep this one, we should at least not include the exception, the stack trace doesn't bring any useful information."", 'commenter': 'olim7t'}, {'comment': ""I'd like to keep this to be explicit to the user that something about their configuration is probably wrong. I have no problem removing the exception and stack trace."", 'commenter': 'GregBestland'}, {'comment': 'Fair enough, there could be auth errors on _some_ nodes but not all, the cumulative warning helps distinguish that case.', 'commenter': 'olim7t'}]"
1176,core/src/main/java/com/datastax/oss/driver/internal/core/control/ControlConnection.java,"@@ -332,11 +341,16 @@ private void connect(
                       if (closeWasCalled || initFuture.isCancelled()) {
                         onSuccess.run(); // abort, we don't really care about the result
                       } else {
-                        LOG.debug(
-                            ""[{}] Error connecting to {}, trying next node"",
-                            logPrefix,
-                            node,
-                            error);
+                        if (error instanceof AuthenticationException) {
+                          Loggers.warnWithException(
+                              LOG, ""[{}] Authentication error"", logPrefix, error);
+                        } else {
+                          LOG.debug(
+                              ""[{}] Error connecting to {}, trying next node"",
+                              logPrefix,
+                              node,
+                              error);","[{'comment': 'Could you also honor `DefaultDriverOption.CONNECTION_WARN_INIT_ERROR` here?\r\nSee `ChannelPool.onAllConnected()`.', 'commenter': 'olim7t'}, {'comment': 'I think that should address your issues. If Feel very strongly about the cumulative logging warning I can remove it, I just want to be explicit about their configuration being incorrect.', 'commenter': 'GregBestland'}]"
1177,core/src/main/java/com/datastax/oss/driver/api/core/PagingIterable.java,"@@ -0,0 +1,153 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core;
+
+import com.datastax.oss.driver.api.core.cql.ExecutionInfo;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import com.datastax.oss.driver.internal.core.PagingIterableWrapper;
+import com.datastax.oss.driver.shaded.guava.common.collect.Iterables;
+import com.datastax.oss.driver.shaded.guava.common.collect.Lists;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import edu.umd.cs.findbugs.annotations.Nullable;
+import java.util.Collections;
+import java.util.Iterator;
+import java.util.List;
+import java.util.function.Function;
+
+/**
+ * An iterable of elements which are fetched synchronously by the driver, possibly in multiple
+ * requests.
+ *
+ * <p>It uses asynchronous calls internally, but blocks on the results in order to provide a
+ * synchronous API to its clients. If the query is paged, only the first page will be fetched
+ * initially, and iteration will trigger background fetches of the next pages when necessary.
+ *
+ * <p>Note that this object can only be iterated once: elements are ""consumed"" as they are read,
+ * subsequent calls to {@code iterator()} will return the same iterator instance.
+ *
+ * <p>Implementations of this type are <b>not thread-safe</b>. They can only be iterated by the
+ * thread that invoked {@code session.execute}.
+ *
+ * <p>This is a generalization of {@link ResultSet}, replacing rows by an arbitrary element type.
+ */
+public interface PagingIterable<ElementT> extends Iterable<ElementT> {","[{'comment': ""The sync version never returns itself, so we don't need a self-type."", 'commenter': 'olim7t'}]"
1177,core/src/main/java/com/datastax/oss/driver/api/core/AsyncPagingIterable.java,"@@ -0,0 +1,102 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core;
+
+import com.datastax.oss.driver.api.core.cql.AsyncResultSet;
+import com.datastax.oss.driver.api.core.cql.ExecutionInfo;
+import com.datastax.oss.driver.internal.core.AsyncPagingIterableWrapper;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import edu.umd.cs.findbugs.annotations.Nullable;
+import java.util.Iterator;
+import java.util.concurrent.CompletionStage;
+import java.util.function.Function;
+
+/**
+ * An iterable of elements which are fetched asynchronously by the driver, possibly in multiple
+ * requests.
+ */
+public interface AsyncPagingIterable<ElementT> {
+
+  /** Returns {@linkplain ExecutionInfo information about the execution} of this page of results. */
+  @NonNull
+  ExecutionInfo getExecutionInfo();
+
+  /** How many rows are left before the current page is exhausted. */
+  int remaining();
+
+  /**
+   * The elements in the current page. To keep iterating beyond that, use {@link #hasMorePages()}
+   * and {@link #fetchNextPage()}.
+   *
+   * <p>Note that this method always returns the same object, and that that object can only be
+   * iterated once: elements are ""consumed"" as they are read.
+   */
+  @NonNull
+  Iterable<ElementT> currentPage();
+
+  /**
+   * Returns the next element, or {@code null} if the results are exhausted.
+   *
+   * <p>This is convenient for queries that are known to return exactly one element, for example
+   * count queries.
+   */
+  @Nullable
+  default ElementT one() {
+    Iterator<ElementT> iterator = currentPage().iterator();
+    return iterator.hasNext() ? iterator.next() : null;
+  }
+
+  /**
+   * Whether there are more pages of results. If so, call {@link #fetchNextPage()} to fetch the next
+   * one asynchronously.
+   */
+  boolean hasMorePages();
+
+  /**
+   * Fetch the next page of results asynchronously.
+   *
+   * @throws IllegalStateException if there are no more pages. Use {@link #hasMorePages()} to check
+   *     if you can call this method.
+   */
+  @NonNull
+  CompletionStage<? extends AsyncPagingIterable<ElementT>> fetchNextPage()
+      throws IllegalStateException;
+
+  /**
+   * If the query that produced this result was a conditional update, indicate whether it was
+   * successfully applied.
+   *
+   * <p>In theory this method should not be exposed on {@link AsyncPagingIterable} since it is
+   * specific to CQL; in practice, most {@code AsyncPagingIterable} instances will be created by
+   * wrapping an {@link AsyncResultSet}, so it is convenient to make this information available
+   * without having to unwrap.","[{'comment': 'This is up for debate. But I think it makes sense to map an `INSERT ... IF NOT EXISTS`, and get back `wasApplied() == false` and the existing entity if it failed.', 'commenter': 'olim7t'}, {'comment': '1. I agree to have the method promoted here, but in this case why not push the logic even further and expose here everything that the native protocol provides us with, including `getColumnDefinitions()`? Granted, if you are mapping rows to entities you might want to actually forget about such metadata completely once the mapping is done; but other usages of this interface might be happy if they can inspect them. We never know.\r\n2. This paragraph leaks a bit too many implementation details imo; besides ""specific to CQL"" is vague: everything we do is specific to CQL in some way :) I would suggest that we don\'t go into the details of explaining why this method is here and not in `AsyncResultSet` anymore and remove this paragraph completely. ', 'commenter': 'adutra'}, {'comment': '1. Makes sense, I will pull it up.\r\n2. OK. The one comment that is specific to `ResultSet` is the paragraph about it being equivalent to `this.iterator().next().getBoolean(""[applied]"")`, I will keep it on that type.', 'commenter': 'olim7t'}]"
1177,core/src/main/java/com/datastax/oss/driver/api/core/AsyncPagingIterable.java,"@@ -0,0 +1,102 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core;
+
+import com.datastax.oss.driver.api.core.cql.AsyncResultSet;
+import com.datastax.oss.driver.api.core.cql.ExecutionInfo;
+import com.datastax.oss.driver.internal.core.AsyncPagingIterableWrapper;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import edu.umd.cs.findbugs.annotations.Nullable;
+import java.util.Iterator;
+import java.util.concurrent.CompletionStage;
+import java.util.function.Function;
+
+/**
+ * An iterable of elements which are fetched asynchronously by the driver, possibly in multiple
+ * requests.
+ */
+public interface AsyncPagingIterable<ElementT> {","[{'comment': ""There *could* be a self type here, because of `fetchNextPage()`. I tried to go down that rabbit hole, but it wasn't pretty; it propagates to the result of `map`, but it can't be materialized on the client side:\r\n```java\r\nPagingIterable<?, Integer> iterable = resultSet.map(row -> row.getInt(0));\r\n```\r\nAlso, the implementation of `AsyncPagingIterableWrapper` was horrendous.\r\n\r\nSo I chose to go the simpler route and override the method with a covariant type in `AsyncResultSet`."", 'commenter': 'olim7t'}, {'comment': ""I wonder if a self type for `fetchNextPage` wouldn't even be conceptually wrong, because the contract of this method does not mandate that implementors should return a future of themselves (iow, some implementation A might return a future that once completed yields an object of type B).\r\n\r\nThat said, I would like to I understand what was so horrible regarding the implementation of `AsyncPagingIterableWrapper`; [I gave it a try myself](https://github.com/datastax/java-driver/commit/270a2473b27dc191e1fcdcd0b23daa7e1f581b52) and found it actually pretty manageable."", 'commenter': 'adutra'}, {'comment': ""I ended up having to parameterize it with 4 types: the source and target iterables, and the source and target element types, and there were still unchecked casts.\r\nIndeed your implementation looks simpler. I noticed your `fetchNextPage` doesn't return a covariant future, not sure if this has an impact or not. Maybe I just over-complicated things.\r\n\r\nBut since we agree that a self-type is not desirable (the `?` in the caller code is the worst aspect IMO), I won't dig into it further."", 'commenter': 'olim7t'}]"
1177,core/src/main/java/com/datastax/oss/driver/api/core/AsyncPagingIterable.java,"@@ -0,0 +1,102 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core;
+
+import com.datastax.oss.driver.api.core.cql.AsyncResultSet;
+import com.datastax.oss.driver.api.core.cql.ExecutionInfo;
+import com.datastax.oss.driver.internal.core.AsyncPagingIterableWrapper;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import edu.umd.cs.findbugs.annotations.Nullable;
+import java.util.Iterator;
+import java.util.concurrent.CompletionStage;
+import java.util.function.Function;
+
+/**
+ * An iterable of elements which are fetched asynchronously by the driver, possibly in multiple
+ * requests.
+ */
+public interface AsyncPagingIterable<ElementT> {
+
+  /** Returns {@linkplain ExecutionInfo information about the execution} of this page of results. */
+  @NonNull
+  ExecutionInfo getExecutionInfo();
+
+  /** How many rows are left before the current page is exhausted. */
+  int remaining();
+
+  /**
+   * The elements in the current page. To keep iterating beyond that, use {@link #hasMorePages()}
+   * and {@link #fetchNextPage()}.
+   *
+   * <p>Note that this method always returns the same object, and that that object can only be
+   * iterated once: elements are ""consumed"" as they are read.
+   */
+  @NonNull
+  Iterable<ElementT> currentPage();
+
+  /**
+   * Returns the next element, or {@code null} if the results are exhausted.
+   *
+   * <p>This is convenient for queries that are known to return exactly one element, for example
+   * count queries.
+   */
+  @Nullable
+  default ElementT one() {
+    Iterator<ElementT> iterator = currentPage().iterator();
+    return iterator.hasNext() ? iterator.next() : null;
+  }
+
+  /**
+   * Whether there are more pages of results. If so, call {@link #fetchNextPage()} to fetch the next
+   * one asynchronously.
+   */
+  boolean hasMorePages();
+
+  /**
+   * Fetch the next page of results asynchronously.
+   *
+   * @throws IllegalStateException if there are no more pages. Use {@link #hasMorePages()} to check
+   *     if you can call this method.
+   */
+  @NonNull
+  CompletionStage<? extends AsyncPagingIterable<ElementT>> fetchNextPage()
+      throws IllegalStateException;
+
+  /**
+   * If the query that produced this result was a conditional update, indicate whether it was
+   * successfully applied.
+   *
+   * <p>In theory this method should not be exposed on {@link AsyncPagingIterable} since it is
+   * specific to CQL; in practice, most {@code AsyncPagingIterable} instances will be created by
+   * wrapping an {@link AsyncResultSet}, so it is convenient to make this information available
+   * without having to unwrap.
+   *
+   * @see AsyncResultSet#wasApplied()
+   */
+  boolean wasApplied();
+
+  /**
+   * Creates a new instance by transforming each element of this iterable with the provided
+   * function.
+   *
+   * <p>Note that both instances share the same underlying data: consuming elements from the
+   * transformed iterable will also consume them from this object, and vice-versa.
+   */
+  default <TargetT> AsyncPagingIterable<TargetT> map(
+      Function<? super ElementT, ? extends TargetT> elementMapper) {
+    return new AsyncPagingIterableWrapper<>(this, elementMapper);
+  }","[{'comment': ""I really like to have this here. It's the most natural thing that users might want to do with a result set, so it makes sense to have a shortcut for it. Also it will be super convenient in the object mapper.\r\n\r\nAs I said in JIRA, I think we should stay minimal and not try to implement other functional operators. They don't necessarily work well with the concept of page boundaries."", 'commenter': 'olim7t'}, {'comment': ""I'm not sure actually. First, it's already trivial to obtain a Java 8 stream from `currentPage()` and call `map()` on it; the same is valid for reactive streams, so on-the-fly mapping use cases are imo already well covered. \r\nFor users wanting to transform a result set A into a result set B in one single call, other solutions exist: a) write a custom wrapper (similar to `AsyncPagingIterableWrapper `) or b) register a custom request processor. So in the end I'm not sure that this method buys us much."", 'commenter': 'adutra'}, {'comment': ""> it's already trivial to obtain a Java 8 stream from `currentPage()` and call `map()` on it\r\n\r\nIf you work with async result sets, you still have to write the logic to rewrap every subsequent page, which incurs a bit of boilerplate.\r\nIf you use synchronous ones, you can indeed map the iterator, but you lose the paging methods (`isFullyFetched`, `getAvailableWithoutFetching`).\r\n\r\nWhat's nice with `map()` is that you get all of that for free.\r\n\r\n> write a custom wrapper (similar to `AsyncPagingIterableWrapper`)\r\n\r\nYes, that's pretty easy to write. But we're going to need it for the mapper anyway, so why not make it available?\r\n\r\n> register a custom request processor\r\n\r\nThat's the nuclear option ðŸ˜‰ "", 'commenter': 'olim7t'}, {'comment': '> but you lose the paging methods (isFullyFetched, getAvailableWithoutFetching)\r\n\r\nHmm OK this finally convinced me of the usefulness of the `map` method.', 'commenter': 'adutra'}]"
1177,core/src/main/java/com/datastax/oss/driver/api/core/PagingIterable.java,"@@ -0,0 +1,161 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core;
+
+import com.datastax.oss.driver.api.core.cql.ColumnDefinitions;
+import com.datastax.oss.driver.api.core.cql.ExecutionInfo;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import com.datastax.oss.driver.internal.core.PagingIterableWrapper;
+import com.datastax.oss.driver.shaded.guava.common.collect.Iterables;
+import com.datastax.oss.driver.shaded.guava.common.collect.Lists;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import edu.umd.cs.findbugs.annotations.Nullable;
+import java.util.Collections;
+import java.util.Iterator;
+import java.util.List;
+import java.util.function.Function;
+
+/**
+ * An iterable of elements which are fetched synchronously by the driver, possibly in multiple
+ * requests.
+ *
+ * <p>It uses asynchronous calls internally, but blocks on the results in order to provide a
+ * synchronous API to its clients. If the query is paged, only the first page will be fetched
+ * initially, and iteration will trigger background fetches of the next pages when necessary.
+ *
+ * <p>Note that this object can only be iterated once: elements are ""consumed"" as they are read,
+ * subsequent calls to {@code iterator()} will return the same iterator instance.
+ *
+ * <p>Implementations of this type are <b>not thread-safe</b>. They can only be iterated by the
+ * thread that invoked {@code session.execute}.
+ *
+ * <p>This is a generalization of {@link ResultSet}, replacing rows by an arbitrary element type.
+ */
+public interface PagingIterable<ElementT> extends Iterable<ElementT> {
+
+  /** Metadata about the columns returned by the CQL request that was used to build this result. */
+  @NonNull
+  ColumnDefinitions getColumnDefinitions();
+
+  /**
+   * The execution information for the last query performed for this iterable.
+   *
+   * <p>This is a shortcut for:
+   *
+   * <pre>
+   * getExecutionInfos().get(getExecutionInfos().size() - 1)
+   * </pre>
+   *
+   * @see #getExecutionInfos()
+   */
+  @NonNull
+  default ExecutionInfo getExecutionInfo() {
+    List<ExecutionInfo> infos = getExecutionInfos();
+    return infos.get(infos.size() - 1);
+  }
+
+  /**
+   * The execution information for all the queries that have been performed so far to assemble this
+   * iterable.
+   *
+   * <p>This will have multiple elements if the query is paged, since the driver performs blocking
+   * background queries to fetch additional pages transparently as the result set is being iterated.
+   */
+  @NonNull
+  List<ExecutionInfo> getExecutionInfos();
+
+  /**
+   * Returns the next element, or {@code null} if the iterable is exhausted.
+   *
+   * <p>This is convenient for queries that are known to return exactly one row, for example count
+   * queries.
+   */
+  @Nullable
+  default ElementT one() {
+    Iterator<ElementT> iterator = iterator();
+    return iterator.hasNext() ? iterator.next() : null;
+  }
+
+  /**
+   * Returns all the remaining elements as a list; <b>not recommended for queries that return a
+   * large number of elements</b>.
+   *
+   * <p>Contrary to {@link #iterator()} or successive calls to {@link #one()}, this method forces
+   * fetching the <b>full contents</b> at once; in particular, this means that a large number of
+   * background queries might have to be run, and that all the data will be held in memory locally.
+   * Therefore it is crucial to only call this method for queries that are known to return a
+   * reasonable number of results.
+   */","[{'comment': 'Would it make sense to to checks in the actual implementation to prevent blowing out the heap... or is this just an instance of user better know what they are doing?', 'commenter': 'GregBestland'}, {'comment': ""We can't predict in advance the number of rows, or even the size each row will take in memory, so it's more the latter IMHO.\r\nThis method is already present in 3.x so users should be familiar with the pitfalls."", 'commenter': 'olim7t'}]"
1178,driver-core/src/main/java/com/datastax/driver/core/RequestHandler.java,"@@ -224,6 +225,16 @@ private void setFinalResult(
                 response.getCustomPayload());
       }
       callback.onSet(connection, response, info, statement, System.nanoTime() - startTime);
+      // if the response from the server has warnings, they'll be set on the ExecutionInfo. Log them
+      // here, if enabled.
+      if (logger.isWarnEnabled()
+          && response.warnings != null
+          && !response.warnings.isEmpty()
+          && Boolean.getBoolean(RequestHandler.LOG_REQUEST_WARNINGS_PROPERTY)) {","[{'comment': 'What will happen if that property `RequestHandler.LOG_REQUEST_WARNINGS_PROPERTY))` will be not set?', 'commenter': 'tomekl007'}, {'comment': ""From the Javadocs:\r\n> If there is no property with the specified name, or if the specified name is empty or null, then false is returned.\r\n\r\nSo there shouldn't be any NullPointerException issues, and the current behavior is the default/unset behavior."", 'commenter': 'emerkle826'}]"
1178,driver-core/src/main/java/com/datastax/driver/core/RequestHandler.java,"@@ -64,6 +64,7 @@
 
   private static final boolean HOST_METRICS_ENABLED =
       Boolean.getBoolean(""com.datastax.driver.HOST_METRICS_ENABLED"");
+  static final String LOG_REQUEST_WARNINGS_PROPERTY = ""advanced.request.log-warnings"";","[{'comment': ""In driver 3 we've been following a different naming convention for this kind of system property, see the previous line for example.\r\nAlso, as much as I dislike the idea of logging those warnings, I think we should do it by default, because the assumption is that users will completely miss them otherwise.\r\n\r\nSo I would invert the property and call it `com.datastax.driver.DISABLE_QUERY_WARNING_LOGS`."", 'commenter': 'olim7t'}]"
1178,driver-core/src/main/java/com/datastax/driver/core/RequestHandler.java,"@@ -224,6 +225,16 @@ private void setFinalResult(
                 response.getCustomPayload());
       }
       callback.onSet(connection, response, info, statement, System.nanoTime() - startTime);
+      // if the response from the server has warnings, they'll be set on the ExecutionInfo. Log them
+      // here, if enabled.
+      if (logger.isWarnEnabled()
+          && response.warnings != null
+          && !response.warnings.isEmpty()
+          && Boolean.getBoolean(RequestHandler.LOG_REQUEST_WARNINGS_PROPERTY)) {
+        for (String warning : response.warnings) {
+          logger.warn(warning);","[{'comment': 'Should we provide a bit more context? Something like ""The server generated a warning for a query: {}""... Maybe also include the query string, or a substring if it\'s too long?', 'commenter': 'olim7t'}]"
1178,driver-core/src/test/java/com/datastax/driver/core/WarningsTest.java,"@@ -50,4 +54,52 @@ public void should_expose_warnings_on_execution_info() {
     List<String> warnings = rs.getExecutionInfo().getWarnings();
     assertThat(warnings).hasSize(1);
   }
+
+  @Test(groups = ""short"")
+  public void should_execute_query_with_server_side_warnings() {
+    // Set the system property to enable logging of server side warnings
+    final String originalLoggingFlag =
+        System.getProperty(RequestHandler.LOG_REQUEST_WARNINGS_PROPERTY, ""false"");
+    System.setProperty(RequestHandler.LOG_REQUEST_WARNINGS_PROPERTY, ""true"");","[{'comment': 'Normally tests that manipulate system properties are executed in isolation, but in this case the impact on potentially concurrent tests is insignificant, so I agree with your approach.', 'commenter': 'olim7t'}]"
1178,driver-core/src/test/java/com/datastax/driver/core/WarningsTest.java,"@@ -50,4 +54,52 @@ public void should_expose_warnings_on_execution_info() {
     List<String> warnings = rs.getExecutionInfo().getWarnings();
     assertThat(warnings).hasSize(1);
   }
+
+  @Test(groups = ""short"")
+  public void should_execute_query_with_server_side_warnings() {
+    // Set the system property to enable logging of server side warnings
+    final String originalLoggingFlag =
+        System.getProperty(RequestHandler.LOG_REQUEST_WARNINGS_PROPERTY, ""false"");
+    System.setProperty(RequestHandler.LOG_REQUEST_WARNINGS_PROPERTY, ""true"");
+    assertThat(Boolean.getBoolean(RequestHandler.LOG_REQUEST_WARNINGS_PROPERTY)).isTrue();
+    // create a TestAppender and add it
+    TestAppender logAppender = new TestAppender();
+    Logger.getRootLogger().addAppender(logAppender);
+    // Given a query that will produce server side warnings that will be embedded in the
+    // ExecutionInfo
+    SimpleStatement statement = new SimpleStatement(""SELECT count(*) FROM foo"");
+
+    ResultSet rs = session().execute(statement);
+    ExecutionInfo ei = rs.getExecutionInfo();
+    // When
+    Row row = rs.one();
+
+    // Then
+    assertThat(row).isNotNull();
+    List<String> warnings = ei.getWarnings();
+
+    assertThat(warnings).isNotEmpty();
+    assertThat(warnings.size()).isEqualTo(1);
+    assertThat(warnings.get(0)).isEqualTo(""Aggregation query used without partition key"");
+    // assert the log was generated
+    assertThat(logAppender).isNotNull();
+    assertThat(logAppender.logs).isNotNull();
+    assertThat(logAppender.logs).isNotEmpty();
+    assertThat(logAppender.logs.get(0).getMessage())
+        .isEqualTo(""Aggregation query used without partition key"");
+    // remove the log appender
+    Logger.getRootLogger().removeAppender(logAppender);
+    // reset the logging flag
+    System.setProperty(RequestHandler.LOG_REQUEST_WARNINGS_PROPERTY, originalLoggingFlag);
+  }
+
+  private class TestAppender extends WriterAppender {","[{'comment': 'We already have a `MemoryAppender` class that works this way, and handles a couple of corner cases we ran into over time. I think you could use it here.', 'commenter': 'olim7t'}]"
1178,driver-core/src/test/java/com/datastax/driver/core/WarningsTest.java,"@@ -50,4 +54,52 @@ public void should_expose_warnings_on_execution_info() {
     List<String> warnings = rs.getExecutionInfo().getWarnings();
     assertThat(warnings).hasSize(1);
   }
+
+  @Test(groups = ""short"")
+  public void should_execute_query_with_server_side_warnings() {","[{'comment': 'Good test ðŸ‘ ', 'commenter': 'olim7t'}]"
1178,manual/logging/README.md,"@@ -192,6 +192,17 @@ that can significantly boost latencies when writing log messages.
 without stopping the application. This usually involves JMX and is available for [Logback](http://logback.qos.ch/manual/jmxConfig.html);
 Log4J provides a `configureAndWatch()` method but it is not recommended to use it inside J2EE containers (see [FAQ](https://logging.apache.org/log4j/1.2/faq.html#a3.6)).
 
+### Server Side Warnings","[{'comment': 'ðŸ’¯ for updating the manual', 'commenter': 'olim7t'}]"
1178,manual/logging/README.md,"@@ -192,6 +192,17 @@ that can significantly boost latencies when writing log messages.
 without stopping the application. This usually involves JMX and is available for [Logback](http://logback.qos.ch/manual/jmxConfig.html);
 Log4J provides a `configureAndWatch()` method but it is not recommended to use it inside J2EE containers (see [FAQ](https://logging.apache.org/log4j/1.2/faq.html#a3.6)).
 
+### Server Side Warnings
+
+When using the driver to execute queries, it is possible that the server will generate a warning. Normally,
+these warnings are contained in the `ResultSet`'s `ExecutionInfo` attribute, but are not logged by the driver","[{'comment': 'Nit: this sounds like sometimes the warnings are not in `ExecutionInfo`. It would be better to say ""These warnings are contained ... but are normally not logged...""\r\n\r\nThough this paragraph will change anyway if you follow my other suggestion to invert the boolean.', 'commenter': 'olim7t'}]"
1178,changelog/README.md,"@@ -2,6 +2,7 @@
 
 ### 3.7.0 (In progress)
 
+- [improvement] JAVA-1950: Log server side warnings returned from a query.","[{'comment': 'Nit: for legacy drivers, changelog entries should be placed after the existing ones, not before.', 'commenter': 'adutra'}]"
1178,driver-core/src/main/java/com/datastax/driver/core/RequestHandler.java,"@@ -224,6 +226,11 @@ private void setFinalResult(
                 response.getCustomPayload());
       }
       callback.onSet(connection, response, info, statement, System.nanoTime() - startTime);
+      // if the response from the server has warnings, they'll be set on the ExecutionInfo. Log them
+      // here, unless they've been disabled.
+      if (logger.isWarnEnabled() && response.warnings != null && !response.warnings.isEmpty()) {","[{'comment': ""This is a bit unrelated, but why aren't the warnings set here on the `ExecutionInfo` object? This seems like the most logical place to do it."", 'commenter': 'adutra'}, {'comment': 'The reason I am not using the ExectionInfo instance here is that the warnings may not have been set on it at this point. It seems that ExecutionInfo instance is updated with the warnings during processing of the callback. If I want to use the ExecutionInfo directly here, I believe I would have to wait for the callback Future to complete.', 'commenter': 'emerkle826'}]"
1178,driver-core/src/main/java/com/datastax/driver/core/RequestHandler.java,"@@ -234,6 +241,23 @@ private void setFinalResult(
     }
   }
 
+  private void logServerWarnings(List<String> warnings) {
+    // log the warnings if they have NOT been disabled (double negative)
+    if (!Boolean.getBoolean(RequestHandler.DISABLE_QUERY_WARNING_LOGS)) {","[{'comment': ""Since you already guarded the call to this method by a few conditional checks above, I'd suggest that you add this one to the `if` block above too."", 'commenter': 'adutra'}, {'comment': 'I think I had it that way at some point. Good call.', 'commenter': 'emerkle826'}]"
1178,driver-core/src/main/java/com/datastax/driver/core/RequestHandler.java,"@@ -64,6 +64,8 @@
 
   private static final boolean HOST_METRICS_ENABLED =
       Boolean.getBoolean(""com.datastax.driver.HOST_METRICS_ENABLED"");
+  static final String DISABLE_QUERY_WARNING_LOGS = ""com.datastax.driver.DISABLE_QUERY_WARNING_LOGS"";
+  private static final int MAX_QUERY_LOG_LENGTH = 50;","[{'comment': '50 seems a bit short imo. Besides, we already have `com.datastax.driver.core.QueryLogger#DEFAULT_MAX_QUERY_STRING_LENGTH`, maybe we should simply reuse that.', 'commenter': 'adutra'}, {'comment': 'Suggestion: maybe you could reuse some of the `QueryLogger` functionality and call `QueryLogger.statementAsString()` to format the statement.', 'commenter': 'adutra'}]"
1178,driver-core/src/main/java/com/datastax/driver/core/RequestHandler.java,"@@ -234,6 +241,23 @@ private void setFinalResult(
     }
   }
 
+  private void logServerWarnings(List<String> warnings) {
+    // log the warnings if they have NOT been disabled (double negative)
+    if (!Boolean.getBoolean(RequestHandler.DISABLE_QUERY_WARNING_LOGS)) {
+      // comma separated list of warnings
+      StringBuilder warningString = new StringBuilder();","[{'comment': ""Use Guava's `Joiner` instead."", 'commenter': 'adutra'}, {'comment': 'I decided to log each warning in the list separately, so there is no need for joining anymore.', 'commenter': 'emerkle826'}]"
1178,driver-core/src/main/java/com/datastax/driver/core/RequestHandler.java,"@@ -234,6 +241,23 @@ private void setFinalResult(
     }
   }
 
+  private void logServerWarnings(List<String> warnings) {
+    // log the warnings if they have NOT been disabled (double negative)
+    if (!Boolean.getBoolean(RequestHandler.DISABLE_QUERY_WARNING_LOGS)) {
+      // comma separated list of warnings
+      StringBuilder warningString = new StringBuilder();
+      for (String warning : warnings) {
+        warningString.append(warning).append("", "");
+      }
+      // truncate the statement query to the MAX_QUERY_LOG_LENGTH, if necessary
+      final String queryString = statement.toString();
+      logger.warn(","[{'comment': ""I wonder if we shouldn't print one message per warning; this way logs can be more easily grep'ed or post-processed."", 'commenter': 'adutra'}]"
1178,driver-core/src/test/java/com/datastax/driver/core/WarningsTest.java,"@@ -49,5 +69,80 @@ public void should_expose_warnings_on_execution_info() {
 
     List<String> warnings = rs.getExecutionInfo().getWarnings();
     assertThat(warnings).hasSize(1);
+    // also assert that by default, the warning is logged and truncated to MAX_QUERY_LOG_LENGTH
+    String log = logAppender.getNext();
+    assertThat(log).isNotNull();
+    assertThat(log).isNotEmpty();
+    assertThat(log)
+        .isEqualTo(
+            ""Query '""
+                + ""BEGIN UNLOGGED BATCH\nINSERT INTO foo (k, v) VALUES""
+                + ""' generated server side warning(s): ""
+                + ""Batch for [ks_1.foo] is of size 5152, exceeding specified threshold of 5120 by 32.""
+                + Layout.LINE_SEP);
+  }
+
+  @Test(groups = ""short"")
+  public void should_execute_query_and_log_server_side_warnings() {
+    // Assert that logging of server-side query warnings is NOT disabled
+    assertThat(Boolean.getBoolean(RequestHandler.DISABLE_QUERY_WARNING_LOGS)).isFalse();
+
+    // Given a query that will produce server side warnings that will be embedded in the
+    // ExecutionInfo
+    final String query = ""SELECT count(*) FROM foo"";
+    SimpleStatement statement = new SimpleStatement(query);
+    // When the query is executed
+    ResultSet rs = session().execute(statement);
+    // Then the result has 1 Row
+    Row row = rs.one();
+    assertThat(row).isNotNull();
+    // And there is a server side warning captured in the ResultSet's ExecutionInfo
+    ExecutionInfo ei = rs.getExecutionInfo();
+    List<String> warnings = ei.getWarnings();
+    assertThat(warnings).isNotEmpty();
+    assertThat(warnings.size()).isEqualTo(1);
+    assertThat(warnings.get(0)).isEqualTo(""Aggregation query used without partition key"");
+    // And the driver logged the server side warning
+    String log = logAppender.getNext();
+    assertThat(log).isNotNull();
+    assertThat(log).isNotEmpty();
+    assertThat(log)
+        .isEqualTo(
+            ""Query '""
+                + query
+                + ""' generated server side warning(s): Aggregation query used without partition key""
+                + Layout.LINE_SEP);
+  }
+
+  @Test(groups = ""short"")
+  public void should_execute_query_and_not_log_server_side_warnings() {
+    // Get the system property value for disabling logging server side warnings
+    final String disabledLogFlag =
+        System.getProperty(RequestHandler.DISABLE_QUERY_WARNING_LOGS, ""false"");
+    // assert that logs are NOT disabled
+    assertThat(disabledLogFlag).isEqualTo(""false"");
+    // Disable the logs
+    System.setProperty(RequestHandler.DISABLE_QUERY_WARNING_LOGS, ""true"");","[{'comment': 'Usually when system properties need to be modified, we use the special test group `isolated`.', 'commenter': 'adutra'}]"
1178,driver-core/src/test/java/com/datastax/driver/core/WarningsTest.java,"@@ -49,5 +69,80 @@ public void should_expose_warnings_on_execution_info() {
 
     List<String> warnings = rs.getExecutionInfo().getWarnings();
     assertThat(warnings).hasSize(1);
+    // also assert that by default, the warning is logged and truncated to MAX_QUERY_LOG_LENGTH
+    String log = logAppender.getNext();
+    assertThat(log).isNotNull();
+    assertThat(log).isNotEmpty();
+    assertThat(log)
+        .isEqualTo(
+            ""Query '""
+                + ""BEGIN UNLOGGED BATCH\nINSERT INTO foo (k, v) VALUES""
+                + ""' generated server side warning(s): ""
+                + ""Batch for [ks_1.foo] is of size 5152, exceeding specified threshold of 5120 by 32.""
+                + Layout.LINE_SEP);
+  }
+
+  @Test(groups = ""short"")
+  public void should_execute_query_and_log_server_side_warnings() {
+    // Assert that logging of server-side query warnings is NOT disabled
+    assertThat(Boolean.getBoolean(RequestHandler.DISABLE_QUERY_WARNING_LOGS)).isFalse();
+
+    // Given a query that will produce server side warnings that will be embedded in the
+    // ExecutionInfo
+    final String query = ""SELECT count(*) FROM foo"";
+    SimpleStatement statement = new SimpleStatement(query);
+    // When the query is executed
+    ResultSet rs = session().execute(statement);
+    // Then the result has 1 Row
+    Row row = rs.one();
+    assertThat(row).isNotNull();
+    // And there is a server side warning captured in the ResultSet's ExecutionInfo
+    ExecutionInfo ei = rs.getExecutionInfo();
+    List<String> warnings = ei.getWarnings();
+    assertThat(warnings).isNotEmpty();
+    assertThat(warnings.size()).isEqualTo(1);
+    assertThat(warnings.get(0)).isEqualTo(""Aggregation query used without partition key"");
+    // And the driver logged the server side warning
+    String log = logAppender.getNext();
+    assertThat(log).isNotNull();
+    assertThat(log).isNotEmpty();
+    assertThat(log)
+        .isEqualTo(
+            ""Query '""
+                + query
+                + ""' generated server side warning(s): Aggregation query used without partition key""
+                + Layout.LINE_SEP);
+  }
+
+  @Test(groups = ""short"")
+  public void should_execute_query_and_not_log_server_side_warnings() {
+    // Get the system property value for disabling logging server side warnings
+    final String disabledLogFlag =
+        System.getProperty(RequestHandler.DISABLE_QUERY_WARNING_LOGS, ""false"");
+    // assert that logs are NOT disabled
+    assertThat(disabledLogFlag).isEqualTo(""false"");
+    // Disable the logs
+    System.setProperty(RequestHandler.DISABLE_QUERY_WARNING_LOGS, ""true"");
+
+    // Given a query that will produce server side warnings that will be embedded in the
+    // ExecutionInfo
+    SimpleStatement statement = new SimpleStatement(""SELECT count(*) FROM foo"");
+    // When the query is executed
+    ResultSet rs = session().execute(statement);
+    // Then the result has 1 Row
+    Row row = rs.one();
+    assertThat(row).isNotNull();
+    // And there is a server side warning captured in the ResultSet's ExecutionInfo
+    ExecutionInfo ei = rs.getExecutionInfo();
+    List<String> warnings = ei.getWarnings();
+    assertThat(warnings).isNotEmpty();
+    assertThat(warnings.size()).isEqualTo(1);
+    assertThat(warnings.get(0)).isEqualTo(""Aggregation query used without partition key"");
+    // And the driver di NOT log the server side warning
+    String log = logAppender.getNext();
+    assertThat(log).isNullOrEmpty();
+
+    // reset the logging flag
+    System.setProperty(RequestHandler.DISABLE_QUERY_WARNING_LOGS, disabledLogFlag);","[{'comment': 'For extra safety I suggest that you place this instruction in a `finally` block.', 'commenter': 'adutra'}]"
1178,manual/logging/README.md,"@@ -192,6 +192,47 @@ that can significantly boost latencies when writing log messages.
 without stopping the application. This usually involves JMX and is available for [Logback](http://logback.qos.ch/manual/jmxConfig.html);
 Log4J provides a `configureAndWatch()` method but it is not recommended to use it inside J2EE containers (see [FAQ](https://logging.apache.org/log4j/1.2/faq.html#a3.6)).
 
+### Server Side Warnings
+
+When using the driver to execute queries, it is possible that the server will generate warnings and
+return them along with the results. Consider the following query:
+
+```sql
+SELECT count(*) FROM cycling.cyclist_name;
+```
+
+Executing this query would generate a warning in Cassandra:
+
+```
+Aggregation query used without partition key
+```
+
+These
+[query warnings](http://docs.datastax.com/en/drivers/java/3.6/com/datastax/driver/core/ExecutionInfo.html#getWarnings--)
+are available programmatically from the
+[ExecutionInfo](https://docs.datastax.com/en/drivers/java/3.6/com/datastax/driver/core/ExecutionInfo.html)
+via
+[ResultSet](https://docs.datastax.com/en/drivers/java/3.6/com/datastax/driver/core/ResultSet.html)'s
+[getExecutionInfo()](https://docs.datastax.com/en/drivers/java/3.6/com/datastax/driver/core/PagingIterable.html#getExecutionInfo--)
+method. They are also logged by the driver:
+
+```
+WARN  com.datastax.driver.core.RequestHandler - Query 'SELECT count(*) FROM cycling.cyclist_name' generated server side warning(s): Aggregation query used without partition key
+```
+
+Sometimes, it is not desirable for the driver to log server-side warnings. In such cases, logging
+these warnings can be disabled in the driver by setting the system property `com.datastax.driver.DISABLE_QUERY_WARNING_LOGS`
+to ""true"". This can be done at application startup (`-Dcom.datastax.driver.DISABLE_QUERY_WARNING_LOGS=true`)
+or it can be toggled programmatically in application code:
+
+```java
+// disable driver logging of server-side warnings
+System.getProperty(""com.datastax.driver.DISABLE_QUERY_WARNING_LOGS"", ""true"");
+....
+// enable driver logging of server-side warnings
+System.getProperty(""com.datastax.driver.DISABLE_QUERY_WARNING_LOGS"", ""false"");
+```","[{'comment': 'Very nice docs ðŸ‘ ', 'commenter': 'adutra'}]"
1178,manual/logging/README.md,"@@ -192,6 +192,47 @@ that can significantly boost latencies when writing log messages.
 without stopping the application. This usually involves JMX and is available for [Logback](http://logback.qos.ch/manual/jmxConfig.html);
 Log4J provides a `configureAndWatch()` method but it is not recommended to use it inside J2EE containers (see [FAQ](https://logging.apache.org/log4j/1.2/faq.html#a3.6)).
 
+### Server Side Warnings
+
+When using the driver to execute queries, it is possible that the server will generate warnings and
+return them along with the results. Consider the following query:
+
+```sql
+SELECT count(*) FROM cycling.cyclist_name;
+```
+
+Executing this query would generate a warning in Cassandra:
+
+```
+Aggregation query used without partition key
+```
+
+These
+[query warnings](http://docs.datastax.com/en/drivers/java/3.6/com/datastax/driver/core/ExecutionInfo.html#getWarnings--)
+are available programmatically from the
+[ExecutionInfo](https://docs.datastax.com/en/drivers/java/3.6/com/datastax/driver/core/ExecutionInfo.html)
+via
+[ResultSet](https://docs.datastax.com/en/drivers/java/3.6/com/datastax/driver/core/ResultSet.html)'s
+[getExecutionInfo()](https://docs.datastax.com/en/drivers/java/3.6/com/datastax/driver/core/PagingIterable.html#getExecutionInfo--)
+method. They are also logged by the driver:
+
+```
+WARN  com.datastax.driver.core.RequestHandler - Query 'SELECT count(*) FROM cycling.cyclist_name' generated server side warning(s): Aggregation query used without partition key
+```
+
+Sometimes, it is not desirable for the driver to log server-side warnings. In such cases, logging
+these warnings can be disabled in the driver by setting the system property `com.datastax.driver.DISABLE_QUERY_WARNING_LOGS`
+to ""true"". This can be done at application startup (`-Dcom.datastax.driver.DISABLE_QUERY_WARNING_LOGS=true`)
+or it can be toggled programmatically in application code:
+
+```java
+// disable driver logging of server-side warnings
+System.getProperty(""com.datastax.driver.DISABLE_QUERY_WARNING_LOGS"", ""true"");","[{'comment': 'This should rather be `System.setProperty`.', 'commenter': 'adutra'}]"
1178,driver-core/src/main/java/com/datastax/driver/core/RequestHandler.java,"@@ -234,6 +243,15 @@ private void setFinalResult(
     }
   }
 
+  private void logServerWarnings(List<String> warnings) {
+    // truncate the statement query to the DEFAULT_MAX_QUERY_STRING_LENGTH, if necessary
+    final String queryString = QueryLogger.builder().build().statementAsString(statement);","[{'comment': 'Maybe we could store `QueryLogger.builder().build()` as a constant somewhere?', 'commenter': 'adutra'}]"
1178,driver-core/src/main/java/com/datastax/driver/core/RequestHandler.java,"@@ -86,6 +87,7 @@
 
   private final AtomicBoolean isDone = new AtomicBoolean();
   private final AtomicInteger executionIndex = new AtomicInteger();
+  private final QueryLogger queryLogger = QueryLogger.builder().build();","[{'comment': 'This change affects the hot path, so I would rather not create an instance of `QueryLogger` for every request; can you please turn this field into a static one?', 'commenter': 'adutra'}]"
1184,core/src/main/resources/reference.conf,"@@ -759,6 +759,16 @@ datastax-java-driver {
       # Overridable in a profile: yes
       consistency = ONE
     }
+
+    # Whether logging of server warnings generated during query execution should be disabled by the
+    # driver. All server generated warnings will be available programmatically via the ExecutionInfo
+    # object on the executed statement's ResultSet. If set to ""true"", this will prevent the driver
+    # from logging these warnings.
+    #
+    # Required: yes
+    # Modifiable at runtime: yes, the new value will be used for traces fetched after the change.","[{'comment': 'Nit: ""for traces fetched"" -> ""for query warnings received""', 'commenter': 'adutra'}, {'comment': 'whoops, copy-paste error on my part.', 'commenter': 'emerkle826'}]"
1184,integration-tests/src/test/java/com/datastax/oss/driver/api/core/cql/ExecutionInfoWarningsIT.java,"@@ -0,0 +1,224 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.datastax.oss.driver.api.core.cql;
+
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.mockito.Mockito.after;
+
+import ch.qos.logback.classic.Level;
+import ch.qos.logback.classic.Logger;
+import ch.qos.logback.classic.spi.ILoggingEvent;
+import ch.qos.logback.core.Appender;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
+import com.datastax.oss.driver.api.testinfra.ccm.CcmRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionUtils;
+import com.datastax.oss.driver.api.testinfra.simulacron.SimulacronRule;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import com.datastax.oss.driver.internal.core.config.typesafe.DefaultDriverConfigLoaderBuilder;
+import com.datastax.oss.driver.internal.core.cql.CqlRequestHandler;
+import com.datastax.oss.simulacron.common.cluster.ClusterSpec;
+import com.google.common.base.Strings;
+import java.util.List;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.BeforeClass;
+import org.junit.ClassRule;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.junit.rules.ExpectedException;
+import org.junit.rules.RuleChain;
+import org.junit.rules.TestName;
+import org.junit.rules.TestRule;
+import org.mockito.ArgumentCaptor;
+import org.mockito.Mockito;
+import org.slf4j.LoggerFactory;
+
+@Category(ParallelizableTests.class)
+public class ExecutionInfoWarningsIT {
+
+  private static final CcmRule CCM = CcmRule.getInstance();
+
+  private static final SimulacronRule SIMULACRON =
+      new SimulacronRule(ClusterSpec.builder().withNodes(1));
+
+  private static final SessionRule<CqlSession> SESSION_RULE =
+      SessionRule.builder(CCM)
+          .withConfigLoader(
+              SessionUtils.configLoaderBuilder()
+                  .withInt(DefaultDriverOption.REQUEST_PAGE_SIZE, 20)
+                  .withInt(DefaultDriverOption.REQUEST_LOGGER_MAX_QUERY_LENGTH, 500)
+                  .withProfile(
+                      ""log-disabled"",
+                      DefaultDriverConfigLoaderBuilder.profileBuilder()
+                          .withString(DefaultDriverOption.REQUEST_DISABLE_LOG_WARNINGS, ""true"")
+                          .build())
+                  .build())
+          .build();
+
+  private static final SessionRule<CqlSession> SIMULACRON_SESSION_RULE =
+      SessionRule.builder(SIMULACRON).build();
+
+  @ClassRule public static TestRule CCM_CHAIN = RuleChain.outerRule(CCM).around(SESSION_RULE);","[{'comment': ""Nit: this isn't final so `ccmChain` would be more appropriate."", 'commenter': 'adutra'}, {'comment': ""Side note: it should probably be final, and therefore named like a constant. But that's not hugely important, and existing tests don't follow a consistent convention. I've created [JAVA-2153](https://datastax-oss.atlassian.net/browse/JAVA-2153) to address it later."", 'commenter': 'olim7t'}]"
1184,integration-tests/src/test/java/com/datastax/oss/driver/api/core/cql/ExecutionInfoWarningsIT.java,"@@ -0,0 +1,224 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.datastax.oss.driver.api.core.cql;
+
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.mockito.Mockito.after;
+
+import ch.qos.logback.classic.Level;
+import ch.qos.logback.classic.Logger;
+import ch.qos.logback.classic.spi.ILoggingEvent;
+import ch.qos.logback.core.Appender;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
+import com.datastax.oss.driver.api.testinfra.ccm.CcmRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionUtils;
+import com.datastax.oss.driver.api.testinfra.simulacron.SimulacronRule;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import com.datastax.oss.driver.internal.core.config.typesafe.DefaultDriverConfigLoaderBuilder;
+import com.datastax.oss.driver.internal.core.cql.CqlRequestHandler;
+import com.datastax.oss.simulacron.common.cluster.ClusterSpec;
+import com.google.common.base.Strings;
+import java.util.List;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.BeforeClass;
+import org.junit.ClassRule;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.junit.rules.ExpectedException;
+import org.junit.rules.RuleChain;
+import org.junit.rules.TestName;
+import org.junit.rules.TestRule;
+import org.mockito.ArgumentCaptor;
+import org.mockito.Mockito;
+import org.slf4j.LoggerFactory;
+
+@Category(ParallelizableTests.class)
+public class ExecutionInfoWarningsIT {
+
+  private static final CcmRule CCM = CcmRule.getInstance();
+
+  private static final SimulacronRule SIMULACRON =
+      new SimulacronRule(ClusterSpec.builder().withNodes(1));
+
+  private static final SessionRule<CqlSession> SESSION_RULE =
+      SessionRule.builder(CCM)
+          .withConfigLoader(
+              SessionUtils.configLoaderBuilder()
+                  .withInt(DefaultDriverOption.REQUEST_PAGE_SIZE, 20)
+                  .withInt(DefaultDriverOption.REQUEST_LOGGER_MAX_QUERY_LENGTH, 500)
+                  .withProfile(
+                      ""log-disabled"",
+                      DefaultDriverConfigLoaderBuilder.profileBuilder()
+                          .withString(DefaultDriverOption.REQUEST_DISABLE_LOG_WARNINGS, ""true"")
+                          .build())
+                  .build())
+          .build();
+
+  private static final SessionRule<CqlSession> SIMULACRON_SESSION_RULE =
+      SessionRule.builder(SIMULACRON).build();
+
+  @ClassRule public static TestRule CCM_CHAIN = RuleChain.outerRule(CCM).around(SESSION_RULE);
+
+  @ClassRule
+  public static TestRule simulacronChain =","[{'comment': ""Do we really need Simulacron in this test class? It doesn't seem used in the tests below."", 'commenter': 'adutra'}, {'comment': ""We don't. This is from copying too much from another test and not pruning back. Thanks for the catch!"", 'commenter': 'emerkle826'}]"
1184,integration-tests/src/test/java/com/datastax/oss/driver/api/core/cql/ExecutionInfoWarningsIT.java,"@@ -0,0 +1,224 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.datastax.oss.driver.api.core.cql;
+
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.mockito.Mockito.after;
+
+import ch.qos.logback.classic.Level;
+import ch.qos.logback.classic.Logger;
+import ch.qos.logback.classic.spi.ILoggingEvent;
+import ch.qos.logback.core.Appender;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
+import com.datastax.oss.driver.api.testinfra.ccm.CcmRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionUtils;
+import com.datastax.oss.driver.api.testinfra.simulacron.SimulacronRule;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import com.datastax.oss.driver.internal.core.config.typesafe.DefaultDriverConfigLoaderBuilder;
+import com.datastax.oss.driver.internal.core.cql.CqlRequestHandler;
+import com.datastax.oss.simulacron.common.cluster.ClusterSpec;
+import com.google.common.base.Strings;
+import java.util.List;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.BeforeClass;
+import org.junit.ClassRule;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.junit.rules.ExpectedException;
+import org.junit.rules.RuleChain;
+import org.junit.rules.TestName;
+import org.junit.rules.TestRule;
+import org.mockito.ArgumentCaptor;
+import org.mockito.Mockito;
+import org.slf4j.LoggerFactory;
+
+@Category(ParallelizableTests.class)
+public class ExecutionInfoWarningsIT {
+
+  private static final CcmRule CCM = CcmRule.getInstance();
+
+  private static final SimulacronRule SIMULACRON =
+      new SimulacronRule(ClusterSpec.builder().withNodes(1));
+
+  private static final SessionRule<CqlSession> SESSION_RULE =
+      SessionRule.builder(CCM)
+          .withConfigLoader(
+              SessionUtils.configLoaderBuilder()
+                  .withInt(DefaultDriverOption.REQUEST_PAGE_SIZE, 20)
+                  .withInt(DefaultDriverOption.REQUEST_LOGGER_MAX_QUERY_LENGTH, 500)
+                  .withProfile(
+                      ""log-disabled"",
+                      DefaultDriverConfigLoaderBuilder.profileBuilder()
+                          .withString(DefaultDriverOption.REQUEST_DISABLE_LOG_WARNINGS, ""true"")
+                          .build())
+                  .build())
+          .build();
+
+  private static final SessionRule<CqlSession> SIMULACRON_SESSION_RULE =
+      SessionRule.builder(SIMULACRON).build();
+
+  @ClassRule public static TestRule CCM_CHAIN = RuleChain.outerRule(CCM).around(SESSION_RULE);
+
+  @ClassRule
+  public static TestRule simulacronChain =
+      RuleChain.outerRule(SIMULACRON).around(SIMULACRON_SESSION_RULE);
+
+  @Rule public TestName name = new TestName();
+
+  @Rule public ExpectedException thrown = ExpectedException.none();
+
+  private static final String KEY = ""test"";
+
+  @SuppressWarnings(""unchecked"")
+  private final Appender<ILoggingEvent> appender =
+      (Appender<ILoggingEvent>) Mockito.mock(Appender.class);
+
+  private Logger logger;
+  private Level originalLoggerLevel;
+  private final ArgumentCaptor<ILoggingEvent> loggingEventCaptor =
+      ArgumentCaptor.forClass(ILoggingEvent.class);
+
+  @BeforeClass
+  public static void setupSchema() {
+    // table with simple primary key, single cell.
+    SESSION_RULE
+        .session()
+        .execute(
+            SimpleStatement.builder(""CREATE TABLE IF NOT EXISTS test (k int primary key, v text)"")
+                .withExecutionProfile(SESSION_RULE.slowProfile())
+                .build());
+    for (int i = 0; i < 100; i++) {
+      SESSION_RULE
+          .session()
+          .execute(
+              SimpleStatement.builder(""INSERT INTO test (k, v) VALUES (?, ?)"")
+                  .addPositionalValues(KEY, i)
+                  .build());
+    }
+  }
+
+  @Before
+  public void setupLogger() {
+    SIMULACRON.cluster().clearLogs();
+    SIMULACRON.cluster().clearPrimes(true);
+    // setup the log appender
+    logger = (Logger) LoggerFactory.getLogger(CqlRequestHandler.class);
+    originalLoggerLevel = logger.getLevel();
+    logger.setLevel(Level.WARN);
+    logger.addAppender(appender);
+  }
+
+  @After
+  public void cleanupLogger() {
+    logger.setLevel(originalLoggerLevel);
+    logger.detachAppender(appender);
+  }
+
+  @Test
+  public void should_execute_query_and_log_server_side_warnings() {
+    final String query = ""SELECT count(*) FROM test;"";
+    Statement<?> st = SimpleStatement.builder(String.format(query)).build();
+    ResultSet result = SESSION_RULE.session().execute(st);
+
+    ExecutionInfo executionInfo = result.getExecutionInfo();
+    assertThat(executionInfo).isNotNull();
+    List<String> warnings = executionInfo.getWarnings();
+    assertThat(warnings).isNotNull();
+    assertThat(warnings).isNotEmpty();
+    String warning = warnings.get(0);
+    assertThat(warning).isNotNull();
+    assertThat(warning).isEqualTo(""Aggregation query used without partition key"");
+    // verify the log was generated
+    Mockito.verify(appender, after(500).times(1)).doAppend(loggingEventCaptor.capture());
+    assertThat(loggingEventCaptor.getValue().getMessage()).isNotNull();
+    String logMessage = loggingEventCaptor.getValue().getFormattedMessage();
+    assertThat(logMessage)
+        .startsWith(
+            ""Query '[0 values] ""
+                + query
+                + ""' generated server side warning(s): Aggregation query used without partition key"");
+  }
+
+  @Test
+  public void should_execute_query_and_not_log_server_side_warnings() {
+    final String query = ""SELECT count(*) FROM test;"";
+    Statement<?> st =
+        SimpleStatement.builder(String.format(query))
+            .withExecutionProfileName(""log-disabled"")
+            .build();
+    ResultSet result = SESSION_RULE.session().execute(st);
+
+    ExecutionInfo executionInfo = result.getExecutionInfo();
+    assertThat(executionInfo).isNotNull();
+    List<String> warnings = executionInfo.getWarnings();
+    assertThat(warnings).isNotNull();
+    assertThat(warnings).isNotEmpty();
+    String warning = warnings.get(0);
+    assertThat(warning).isNotNull();
+    assertThat(warning).isEqualTo(""Aggregation query used without partition key"");
+    // verify the log was NOT generated
+    Mockito.verify(appender, after(500).times(0)).doAppend(loggingEventCaptor.capture());
+  }
+
+  @Test
+  public void should_expose_warnings_on_execution_info() {
+    // the default batch size warn threshold is 5 * 1024 bytes, but after CASSANDRA-10876 there must
+    // be
+    // multiple mutations in a batch to trigger this warning so the batch includes 2 different
+    // inserts.
+    final String query =
+        String.format(
+            ""BEGIN UNLOGGED BATCH\n""
+                + ""INSERT INTO test (k, v) VALUES (1, '%s')\n""
+                + ""INSERT INTO test (k, v) VALUES (2, '%s')\n""
+                + ""APPLY BATCH"",
+            Strings.repeat(""1"", 2 * 1024), Strings.repeat(""1"", 3 * 1024));
+    Statement<?> st = SimpleStatement.builder(String.format(query)).build();
+    ResultSet result = SESSION_RULE.session().execute(st);
+    ExecutionInfo executionInfo = result.getExecutionInfo();
+    assertThat(executionInfo).isNotNull();
+    List<String> warnings = executionInfo.getWarnings();
+    assertThat(warnings).isNotNull();
+    assertThat(warnings).isNotEmpty();
+    // verify the log was generated
+    Mockito.verify(appender, after(500).times(1)).doAppend(loggingEventCaptor.capture());","[{'comment': 'The contribution guidelines changed, you should now static-import all Mockito methods.', 'commenter': 'adutra'}]"
1184,integration-tests/src/test/java/com/datastax/oss/driver/api/core/cql/ExecutionInfoWarningsIT.java,"@@ -0,0 +1,224 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.datastax.oss.driver.api.core.cql;
+
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.mockito.Mockito.after;
+
+import ch.qos.logback.classic.Level;
+import ch.qos.logback.classic.Logger;
+import ch.qos.logback.classic.spi.ILoggingEvent;
+import ch.qos.logback.core.Appender;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
+import com.datastax.oss.driver.api.testinfra.ccm.CcmRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionUtils;
+import com.datastax.oss.driver.api.testinfra.simulacron.SimulacronRule;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import com.datastax.oss.driver.internal.core.config.typesafe.DefaultDriverConfigLoaderBuilder;
+import com.datastax.oss.driver.internal.core.cql.CqlRequestHandler;
+import com.datastax.oss.simulacron.common.cluster.ClusterSpec;
+import com.google.common.base.Strings;
+import java.util.List;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.BeforeClass;
+import org.junit.ClassRule;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.junit.rules.ExpectedException;
+import org.junit.rules.RuleChain;
+import org.junit.rules.TestName;
+import org.junit.rules.TestRule;
+import org.mockito.ArgumentCaptor;
+import org.mockito.Mockito;
+import org.slf4j.LoggerFactory;
+
+@Category(ParallelizableTests.class)
+public class ExecutionInfoWarningsIT {
+
+  private static final CcmRule CCM = CcmRule.getInstance();
+
+  private static final SimulacronRule SIMULACRON =
+      new SimulacronRule(ClusterSpec.builder().withNodes(1));
+
+  private static final SessionRule<CqlSession> SESSION_RULE =
+      SessionRule.builder(CCM)
+          .withConfigLoader(
+              SessionUtils.configLoaderBuilder()
+                  .withInt(DefaultDriverOption.REQUEST_PAGE_SIZE, 20)
+                  .withInt(DefaultDriverOption.REQUEST_LOGGER_MAX_QUERY_LENGTH, 500)
+                  .withProfile(
+                      ""log-disabled"",
+                      DefaultDriverConfigLoaderBuilder.profileBuilder()
+                          .withString(DefaultDriverOption.REQUEST_DISABLE_LOG_WARNINGS, ""true"")
+                          .build())
+                  .build())
+          .build();
+
+  private static final SessionRule<CqlSession> SIMULACRON_SESSION_RULE =
+      SessionRule.builder(SIMULACRON).build();
+
+  @ClassRule public static TestRule CCM_CHAIN = RuleChain.outerRule(CCM).around(SESSION_RULE);
+
+  @ClassRule
+  public static TestRule simulacronChain =
+      RuleChain.outerRule(SIMULACRON).around(SIMULACRON_SESSION_RULE);
+
+  @Rule public TestName name = new TestName();
+
+  @Rule public ExpectedException thrown = ExpectedException.none();
+
+  private static final String KEY = ""test"";
+
+  @SuppressWarnings(""unchecked"")
+  private final Appender<ILoggingEvent> appender =
+      (Appender<ILoggingEvent>) Mockito.mock(Appender.class);
+
+  private Logger logger;
+  private Level originalLoggerLevel;
+  private final ArgumentCaptor<ILoggingEvent> loggingEventCaptor =
+      ArgumentCaptor.forClass(ILoggingEvent.class);
+
+  @BeforeClass
+  public static void setupSchema() {
+    // table with simple primary key, single cell.
+    SESSION_RULE
+        .session()
+        .execute(
+            SimpleStatement.builder(""CREATE TABLE IF NOT EXISTS test (k int primary key, v text)"")
+                .withExecutionProfile(SESSION_RULE.slowProfile())
+                .build());
+    for (int i = 0; i < 100; i++) {
+      SESSION_RULE
+          .session()
+          .execute(
+              SimpleStatement.builder(""INSERT INTO test (k, v) VALUES (?, ?)"")
+                  .addPositionalValues(KEY, i)
+                  .build());
+    }
+  }
+
+  @Before
+  public void setupLogger() {
+    SIMULACRON.cluster().clearLogs();
+    SIMULACRON.cluster().clearPrimes(true);
+    // setup the log appender
+    logger = (Logger) LoggerFactory.getLogger(CqlRequestHandler.class);
+    originalLoggerLevel = logger.getLevel();
+    logger.setLevel(Level.WARN);
+    logger.addAppender(appender);
+  }
+
+  @After
+  public void cleanupLogger() {
+    logger.setLevel(originalLoggerLevel);
+    logger.detachAppender(appender);
+  }
+
+  @Test
+  public void should_execute_query_and_log_server_side_warnings() {
+    final String query = ""SELECT count(*) FROM test;"";
+    Statement<?> st = SimpleStatement.builder(String.format(query)).build();
+    ResultSet result = SESSION_RULE.session().execute(st);
+
+    ExecutionInfo executionInfo = result.getExecutionInfo();
+    assertThat(executionInfo).isNotNull();
+    List<String> warnings = executionInfo.getWarnings();
+    assertThat(warnings).isNotNull();
+    assertThat(warnings).isNotEmpty();
+    String warning = warnings.get(0);
+    assertThat(warning).isNotNull();
+    assertThat(warning).isEqualTo(""Aggregation query used without partition key"");
+    // verify the log was generated
+    Mockito.verify(appender, after(500).times(1)).doAppend(loggingEventCaptor.capture());
+    assertThat(loggingEventCaptor.getValue().getMessage()).isNotNull();
+    String logMessage = loggingEventCaptor.getValue().getFormattedMessage();
+    assertThat(logMessage)
+        .startsWith(
+            ""Query '[0 values] ""
+                + query
+                + ""' generated server side warning(s): Aggregation query used without partition key"");
+  }
+
+  @Test
+  public void should_execute_query_and_not_log_server_side_warnings() {
+    final String query = ""SELECT count(*) FROM test;"";
+    Statement<?> st =
+        SimpleStatement.builder(String.format(query))
+            .withExecutionProfileName(""log-disabled"")
+            .build();
+    ResultSet result = SESSION_RULE.session().execute(st);
+
+    ExecutionInfo executionInfo = result.getExecutionInfo();
+    assertThat(executionInfo).isNotNull();
+    List<String> warnings = executionInfo.getWarnings();
+    assertThat(warnings).isNotNull();
+    assertThat(warnings).isNotEmpty();
+    String warning = warnings.get(0);
+    assertThat(warning).isNotNull();
+    assertThat(warning).isEqualTo(""Aggregation query used without partition key"");
+    // verify the log was NOT generated
+    Mockito.verify(appender, after(500).times(0)).doAppend(loggingEventCaptor.capture());
+  }
+
+  @Test
+  public void should_expose_warnings_on_execution_info() {
+    // the default batch size warn threshold is 5 * 1024 bytes, but after CASSANDRA-10876 there must
+    // be
+    // multiple mutations in a batch to trigger this warning so the batch includes 2 different","[{'comment': 'Nit: line wrappings could be improved here.', 'commenter': 'adutra'}]"
1184,integration-tests/src/test/java/com/datastax/oss/driver/api/core/cql/ExecutionInfoWarningsIT.java,"@@ -0,0 +1,224 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.datastax.oss.driver.api.core.cql;
+
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.mockito.Mockito.after;
+
+import ch.qos.logback.classic.Level;
+import ch.qos.logback.classic.Logger;
+import ch.qos.logback.classic.spi.ILoggingEvent;
+import ch.qos.logback.core.Appender;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
+import com.datastax.oss.driver.api.testinfra.ccm.CcmRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionUtils;
+import com.datastax.oss.driver.api.testinfra.simulacron.SimulacronRule;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import com.datastax.oss.driver.internal.core.config.typesafe.DefaultDriverConfigLoaderBuilder;
+import com.datastax.oss.driver.internal.core.cql.CqlRequestHandler;
+import com.datastax.oss.simulacron.common.cluster.ClusterSpec;
+import com.google.common.base.Strings;
+import java.util.List;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.BeforeClass;
+import org.junit.ClassRule;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.junit.rules.ExpectedException;
+import org.junit.rules.RuleChain;
+import org.junit.rules.TestName;
+import org.junit.rules.TestRule;
+import org.mockito.ArgumentCaptor;
+import org.mockito.Mockito;
+import org.slf4j.LoggerFactory;
+
+@Category(ParallelizableTests.class)
+public class ExecutionInfoWarningsIT {
+
+  private static final CcmRule CCM = CcmRule.getInstance();
+
+  private static final SimulacronRule SIMULACRON =
+      new SimulacronRule(ClusterSpec.builder().withNodes(1));
+
+  private static final SessionRule<CqlSession> SESSION_RULE =
+      SessionRule.builder(CCM)
+          .withConfigLoader(
+              SessionUtils.configLoaderBuilder()
+                  .withInt(DefaultDriverOption.REQUEST_PAGE_SIZE, 20)
+                  .withInt(DefaultDriverOption.REQUEST_LOGGER_MAX_QUERY_LENGTH, 500)
+                  .withProfile(
+                      ""log-disabled"",
+                      DefaultDriverConfigLoaderBuilder.profileBuilder()
+                          .withString(DefaultDriverOption.REQUEST_DISABLE_LOG_WARNINGS, ""true"")
+                          .build())
+                  .build())
+          .build();
+
+  private static final SessionRule<CqlSession> SIMULACRON_SESSION_RULE =
+      SessionRule.builder(SIMULACRON).build();
+
+  @ClassRule public static TestRule CCM_CHAIN = RuleChain.outerRule(CCM).around(SESSION_RULE);
+
+  @ClassRule
+  public static TestRule simulacronChain =
+      RuleChain.outerRule(SIMULACRON).around(SIMULACRON_SESSION_RULE);
+
+  @Rule public TestName name = new TestName();
+
+  @Rule public ExpectedException thrown = ExpectedException.none();
+
+  private static final String KEY = ""test"";
+
+  @SuppressWarnings(""unchecked"")
+  private final Appender<ILoggingEvent> appender =
+      (Appender<ILoggingEvent>) Mockito.mock(Appender.class);
+
+  private Logger logger;
+  private Level originalLoggerLevel;
+  private final ArgumentCaptor<ILoggingEvent> loggingEventCaptor =
+      ArgumentCaptor.forClass(ILoggingEvent.class);
+
+  @BeforeClass
+  public static void setupSchema() {
+    // table with simple primary key, single cell.
+    SESSION_RULE
+        .session()
+        .execute(
+            SimpleStatement.builder(""CREATE TABLE IF NOT EXISTS test (k int primary key, v text)"")
+                .withExecutionProfile(SESSION_RULE.slowProfile())
+                .build());
+    for (int i = 0; i < 100; i++) {
+      SESSION_RULE
+          .session()
+          .execute(
+              SimpleStatement.builder(""INSERT INTO test (k, v) VALUES (?, ?)"")
+                  .addPositionalValues(KEY, i)
+                  .build());
+    }
+  }
+
+  @Before
+  public void setupLogger() {
+    SIMULACRON.cluster().clearLogs();
+    SIMULACRON.cluster().clearPrimes(true);
+    // setup the log appender
+    logger = (Logger) LoggerFactory.getLogger(CqlRequestHandler.class);
+    originalLoggerLevel = logger.getLevel();
+    logger.setLevel(Level.WARN);
+    logger.addAppender(appender);
+  }
+
+  @After
+  public void cleanupLogger() {
+    logger.setLevel(originalLoggerLevel);
+    logger.detachAppender(appender);
+  }
+
+  @Test
+  public void should_execute_query_and_log_server_side_warnings() {
+    final String query = ""SELECT count(*) FROM test;"";
+    Statement<?> st = SimpleStatement.builder(String.format(query)).build();
+    ResultSet result = SESSION_RULE.session().execute(st);
+
+    ExecutionInfo executionInfo = result.getExecutionInfo();
+    assertThat(executionInfo).isNotNull();
+    List<String> warnings = executionInfo.getWarnings();
+    assertThat(warnings).isNotNull();
+    assertThat(warnings).isNotEmpty();
+    String warning = warnings.get(0);
+    assertThat(warning).isNotNull();
+    assertThat(warning).isEqualTo(""Aggregation query used without partition key"");
+    // verify the log was generated
+    Mockito.verify(appender, after(500).times(1)).doAppend(loggingEventCaptor.capture());
+    assertThat(loggingEventCaptor.getValue().getMessage()).isNotNull();
+    String logMessage = loggingEventCaptor.getValue().getFormattedMessage();
+    assertThat(logMessage)
+        .startsWith(
+            ""Query '[0 values] ""
+                + query
+                + ""' generated server side warning(s): Aggregation query used without partition key"");
+  }
+
+  @Test
+  public void should_execute_query_and_not_log_server_side_warnings() {
+    final String query = ""SELECT count(*) FROM test;"";
+    Statement<?> st =
+        SimpleStatement.builder(String.format(query))
+            .withExecutionProfileName(""log-disabled"")
+            .build();
+    ResultSet result = SESSION_RULE.session().execute(st);
+
+    ExecutionInfo executionInfo = result.getExecutionInfo();
+    assertThat(executionInfo).isNotNull();
+    List<String> warnings = executionInfo.getWarnings();
+    assertThat(warnings).isNotNull();
+    assertThat(warnings).isNotEmpty();
+    String warning = warnings.get(0);
+    assertThat(warning).isNotNull();
+    assertThat(warning).isEqualTo(""Aggregation query used without partition key"");
+    // verify the log was NOT generated
+    Mockito.verify(appender, after(500).times(0)).doAppend(loggingEventCaptor.capture());
+  }
+
+  @Test
+  public void should_expose_warnings_on_execution_info() {
+    // the default batch size warn threshold is 5 * 1024 bytes, but after CASSANDRA-10876 there must
+    // be
+    // multiple mutations in a batch to trigger this warning so the batch includes 2 different
+    // inserts.
+    final String query =
+        String.format(
+            ""BEGIN UNLOGGED BATCH\n""
+                + ""INSERT INTO test (k, v) VALUES (1, '%s')\n""
+                + ""INSERT INTO test (k, v) VALUES (2, '%s')\n""
+                + ""APPLY BATCH"",
+            Strings.repeat(""1"", 2 * 1024), Strings.repeat(""1"", 3 * 1024));
+    Statement<?> st = SimpleStatement.builder(String.format(query)).build();
+    ResultSet result = SESSION_RULE.session().execute(st);
+    ExecutionInfo executionInfo = result.getExecutionInfo();
+    assertThat(executionInfo).isNotNull();
+    List<String> warnings = executionInfo.getWarnings();
+    assertThat(warnings).isNotNull();","[{'comment': '`isNotNull` is not required if `isNotEmpty` is present.', 'commenter': 'adutra'}]"
1184,integration-tests/src/test/java/com/datastax/oss/driver/api/core/cql/ExecutionInfoWarningsIT.java,"@@ -0,0 +1,224 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.datastax.oss.driver.api.core.cql;
+
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.mockito.Mockito.after;
+
+import ch.qos.logback.classic.Level;
+import ch.qos.logback.classic.Logger;
+import ch.qos.logback.classic.spi.ILoggingEvent;
+import ch.qos.logback.core.Appender;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
+import com.datastax.oss.driver.api.testinfra.ccm.CcmRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionUtils;
+import com.datastax.oss.driver.api.testinfra.simulacron.SimulacronRule;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import com.datastax.oss.driver.internal.core.config.typesafe.DefaultDriverConfigLoaderBuilder;
+import com.datastax.oss.driver.internal.core.cql.CqlRequestHandler;
+import com.datastax.oss.simulacron.common.cluster.ClusterSpec;
+import com.google.common.base.Strings;
+import java.util.List;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.BeforeClass;
+import org.junit.ClassRule;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.junit.rules.ExpectedException;
+import org.junit.rules.RuleChain;
+import org.junit.rules.TestName;
+import org.junit.rules.TestRule;
+import org.mockito.ArgumentCaptor;
+import org.mockito.Mockito;
+import org.slf4j.LoggerFactory;
+
+@Category(ParallelizableTests.class)
+public class ExecutionInfoWarningsIT {
+
+  private static final CcmRule CCM = CcmRule.getInstance();
+
+  private static final SimulacronRule SIMULACRON =
+      new SimulacronRule(ClusterSpec.builder().withNodes(1));
+
+  private static final SessionRule<CqlSession> SESSION_RULE =
+      SessionRule.builder(CCM)
+          .withConfigLoader(
+              SessionUtils.configLoaderBuilder()
+                  .withInt(DefaultDriverOption.REQUEST_PAGE_SIZE, 20)
+                  .withInt(DefaultDriverOption.REQUEST_LOGGER_MAX_QUERY_LENGTH, 500)
+                  .withProfile(
+                      ""log-disabled"",
+                      DefaultDriverConfigLoaderBuilder.profileBuilder()
+                          .withString(DefaultDriverOption.REQUEST_DISABLE_LOG_WARNINGS, ""true"")
+                          .build())
+                  .build())
+          .build();
+
+  private static final SessionRule<CqlSession> SIMULACRON_SESSION_RULE =
+      SessionRule.builder(SIMULACRON).build();
+
+  @ClassRule public static TestRule CCM_CHAIN = RuleChain.outerRule(CCM).around(SESSION_RULE);
+
+  @ClassRule
+  public static TestRule simulacronChain =
+      RuleChain.outerRule(SIMULACRON).around(SIMULACRON_SESSION_RULE);
+
+  @Rule public TestName name = new TestName();
+
+  @Rule public ExpectedException thrown = ExpectedException.none();
+
+  private static final String KEY = ""test"";
+
+  @SuppressWarnings(""unchecked"")
+  private final Appender<ILoggingEvent> appender =
+      (Appender<ILoggingEvent>) Mockito.mock(Appender.class);
+
+  private Logger logger;
+  private Level originalLoggerLevel;
+  private final ArgumentCaptor<ILoggingEvent> loggingEventCaptor =
+      ArgumentCaptor.forClass(ILoggingEvent.class);
+
+  @BeforeClass
+  public static void setupSchema() {
+    // table with simple primary key, single cell.
+    SESSION_RULE
+        .session()
+        .execute(
+            SimpleStatement.builder(""CREATE TABLE IF NOT EXISTS test (k int primary key, v text)"")
+                .withExecutionProfile(SESSION_RULE.slowProfile())
+                .build());
+    for (int i = 0; i < 100; i++) {
+      SESSION_RULE
+          .session()
+          .execute(
+              SimpleStatement.builder(""INSERT INTO test (k, v) VALUES (?, ?)"")
+                  .addPositionalValues(KEY, i)
+                  .build());
+    }
+  }
+
+  @Before
+  public void setupLogger() {
+    SIMULACRON.cluster().clearLogs();
+    SIMULACRON.cluster().clearPrimes(true);
+    // setup the log appender
+    logger = (Logger) LoggerFactory.getLogger(CqlRequestHandler.class);
+    originalLoggerLevel = logger.getLevel();
+    logger.setLevel(Level.WARN);
+    logger.addAppender(appender);
+  }
+
+  @After
+  public void cleanupLogger() {
+    logger.setLevel(originalLoggerLevel);
+    logger.detachAppender(appender);
+  }
+
+  @Test
+  public void should_execute_query_and_log_server_side_warnings() {
+    final String query = ""SELECT count(*) FROM test;"";
+    Statement<?> st = SimpleStatement.builder(String.format(query)).build();
+    ResultSet result = SESSION_RULE.session().execute(st);
+
+    ExecutionInfo executionInfo = result.getExecutionInfo();
+    assertThat(executionInfo).isNotNull();
+    List<String> warnings = executionInfo.getWarnings();
+    assertThat(warnings).isNotNull();
+    assertThat(warnings).isNotEmpty();
+    String warning = warnings.get(0);
+    assertThat(warning).isNotNull();
+    assertThat(warning).isEqualTo(""Aggregation query used without partition key"");
+    // verify the log was generated
+    Mockito.verify(appender, after(500).times(1)).doAppend(loggingEventCaptor.capture());
+    assertThat(loggingEventCaptor.getValue().getMessage()).isNotNull();
+    String logMessage = loggingEventCaptor.getValue().getFormattedMessage();
+    assertThat(logMessage)
+        .startsWith(
+            ""Query '[0 values] ""
+                + query
+                + ""' generated server side warning(s): Aggregation query used without partition key"");
+  }
+
+  @Test
+  public void should_execute_query_and_not_log_server_side_warnings() {
+    final String query = ""SELECT count(*) FROM test;"";
+    Statement<?> st =
+        SimpleStatement.builder(String.format(query))
+            .withExecutionProfileName(""log-disabled"")
+            .build();
+    ResultSet result = SESSION_RULE.session().execute(st);
+
+    ExecutionInfo executionInfo = result.getExecutionInfo();
+    assertThat(executionInfo).isNotNull();
+    List<String> warnings = executionInfo.getWarnings();
+    assertThat(warnings).isNotNull();
+    assertThat(warnings).isNotEmpty();
+    String warning = warnings.get(0);
+    assertThat(warning).isNotNull();
+    assertThat(warning).isEqualTo(""Aggregation query used without partition key"");
+    // verify the log was NOT generated
+    Mockito.verify(appender, after(500).times(0)).doAppend(loggingEventCaptor.capture());
+  }
+
+  @Test
+  public void should_expose_warnings_on_execution_info() {
+    // the default batch size warn threshold is 5 * 1024 bytes, but after CASSANDRA-10876 there must
+    // be
+    // multiple mutations in a batch to trigger this warning so the batch includes 2 different
+    // inserts.
+    final String query =
+        String.format(
+            ""BEGIN UNLOGGED BATCH\n""
+                + ""INSERT INTO test (k, v) VALUES (1, '%s')\n""
+                + ""INSERT INTO test (k, v) VALUES (2, '%s')\n""
+                + ""APPLY BATCH"",
+            Strings.repeat(""1"", 2 * 1024), Strings.repeat(""1"", 3 * 1024));
+    Statement<?> st = SimpleStatement.builder(String.format(query)).build();
+    ResultSet result = SESSION_RULE.session().execute(st);
+    ExecutionInfo executionInfo = result.getExecutionInfo();
+    assertThat(executionInfo).isNotNull();
+    List<String> warnings = executionInfo.getWarnings();
+    assertThat(warnings).isNotNull();
+    assertThat(warnings).isNotEmpty();
+    // verify the log was generated
+    Mockito.verify(appender, after(500).times(1)).doAppend(loggingEventCaptor.capture());
+    assertThat(loggingEventCaptor.getValue().getMessage()).isNotNull();
+    String logMessage = loggingEventCaptor.getValue().getFormattedMessage();
+    assertThat(logMessage)
+        .startsWith(""Query '"")
+        // query will only be logged up to DefaultDriverOption.REQUEST_LOGGER_MAX_QUERY_LENGTH
+        // characters
+        .contains(
+            query.substring(
+                0,
+                SESSION_RULE","[{'comment': ""Maybe we car replace this with a constant `500` since we now that that's the value we set above?"", 'commenter': 'adutra'}]"
1184,core/src/main/java/com/datastax/oss/driver/internal/core/cql/CqlRequestHandler.java,"@@ -188,6 +190,7 @@ protected CqlRequestHandler(
 
     this.throttler = context.getRequestThrottler();
     this.throttler.register(this);
+    this.requestLogFormatter = new RequestLogFormatter(context);","[{'comment': ""@adutra You commented in #1178 that the QueryLogger should be made static to avoid object instantiation on the hot path. I believe this RequestLogFormatter is on a similar hot path here. However, I'm not sure if there is an easy way to avoid it as the RequestLogFormatter needs an instance of DriverContext, which is only set when this request handler is instantiated. Ideas?"", 'commenter': 'emerkle826'}, {'comment': ""You are absolutely right. I see that `RequestLogFormatter` only needs the context to access the codec registry and the protocol version. So I see two options: \r\n\r\n1. Do nothing, maybe it's not that bad to instantiate that component at every request; or\r\n2. Create another constructor in `RequestLogFormatter` that takes a codec registry and a protocol version, and use default values for both (this could break if the user is using custom types).\r\n\r\nIn the end, I think I'd go with option 1."", 'commenter': 'adutra'}, {'comment': 'Maybe `RequestLogFormatter` should belong to the `DriverContext`?\r\nThis way when the `DriverContext` is created we will create `RequestLogFormatter` only once and we will not need to instantiate it in the `CqlRequestHandler` constructor. Then we can call `context.getRequestLogFormatter` and assign it to this `CqlRequestHandler` - similarly to `this.requestTracker = context.getRequestTracker();`', 'commenter': 'tomekl007'}, {'comment': '+1 for exposing an instance of the log formatter in `InternalDriverContext`.\r\n\r\nThis is a simple class so the context can build it directly, no need to have the class name in the configuration. See how `getNettyOptions()` is implemented in `DefaultDriverContext`, it should be exactly the same.\r\n', 'commenter': 'olim7t'}]"
1184,core/src/main/java/com/datastax/oss/driver/internal/core/cql/CqlRequestHandler.java,"@@ -356,11 +359,34 @@ private void setFinalResult(
               TimeUnit.NANOSECONDS);
         }
       }
+      // log the warnings if they have NOT been disabled
+      if (LOG.isWarnEnabled()
+          && !executionInfo.getWarnings().isEmpty()
+          && !executionProfile.getBoolean(DefaultDriverOption.REQUEST_DISABLE_LOG_WARNINGS)) {
+        logServerWarnings(executionInfo.getWarnings());
+      }
     } catch (Throwable error) {
       setFinalError(error, callback.node, -1);
     }
   }
 
+  private void logServerWarnings(List<String> warnings) {
+    // use the RequestLogFormatter to format the query
+    StringBuilder statementString = new StringBuilder();
+    requestLogFormatter.appendRequest(
+        statement,
+        executionProfile.getInt(DefaultDriverOption.REQUEST_LOGGER_MAX_QUERY_LENGTH, 500),","[{'comment': 'those default values `500, false, 0, 0` are used also in other places, right?\r\n(for example in `RequestLogger` `executionProfile.getInt(DefaultDriverOption.REQUEST_LOGGER_MAX_QUERY_LENGTH, 500);`)\r\nSo maybe we should refactor those Defaults as constants somewhere and use those constants in all places where that default is needed (to avoid duplication)?', 'commenter': 'tomekl007'}]"
1184,core/src/main/java/com/datastax/oss/driver/internal/core/cql/CqlRequestHandler.java,"@@ -356,11 +359,34 @@ private void setFinalResult(
               TimeUnit.NANOSECONDS);
         }
       }
+      // log the warnings if they have NOT been disabled
+      if (LOG.isWarnEnabled()","[{'comment': 'is there an option for this `LOG.isWarnEnabled()` to be changed dynamically?\r\nIf not then I think that we can evaluate those conditions only once and assign to a field (to avoid re-evaluation for every request)', 'commenter': 'tomekl007'}, {'comment': ""It is possible to change the log level of a given logger in client code (I don't think any of the driver code currently does this). I'm not sure how often, if ever, it is done in production though."", 'commenter': 'emerkle826'}, {'comment': ""We should assume that log levels can change at runtime. I don't know how common it is in practice, but it definitely makes sense to have a way to adjust them to debug integration/production issues (I would build that into a client application)."", 'commenter': 'olim7t'}]"
1184,integration-tests/src/test/java/com/datastax/oss/driver/api/core/cql/ExecutionInfoWarningsIT.java,"@@ -0,0 +1,191 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.datastax.oss.driver.api.core.cql;
+
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.mockito.Mockito.after;
+import static org.mockito.Mockito.verify;
+
+import ch.qos.logback.classic.Level;
+import ch.qos.logback.classic.Logger;
+import ch.qos.logback.classic.spi.ILoggingEvent;
+import ch.qos.logback.core.Appender;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
+import com.datastax.oss.driver.api.testinfra.ccm.CcmRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionUtils;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import com.datastax.oss.driver.internal.core.config.typesafe.DefaultDriverConfigLoaderBuilder;
+import com.datastax.oss.driver.internal.core.cql.CqlRequestHandler;
+import com.google.common.base.Strings;
+import java.util.List;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.BeforeClass;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.junit.rules.RuleChain;
+import org.junit.rules.TestRule;
+import org.mockito.ArgumentCaptor;
+import org.mockito.Mockito;
+import org.slf4j.LoggerFactory;
+
+@Category(ParallelizableTests.class)
+public class ExecutionInfoWarningsIT {
+
+  private static final int MAX_QUERY_LENGTH = 500;","[{'comment': 'another usage of default that could be moved and accessed here', 'commenter': 'tomekl007'}]"
1184,integration-tests/src/test/java/com/datastax/oss/driver/api/core/cql/ExecutionInfoWarningsIT.java,"@@ -0,0 +1,191 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.datastax.oss.driver.api.core.cql;
+
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.mockito.Mockito.after;
+import static org.mockito.Mockito.verify;
+
+import ch.qos.logback.classic.Level;
+import ch.qos.logback.classic.Logger;
+import ch.qos.logback.classic.spi.ILoggingEvent;
+import ch.qos.logback.core.Appender;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
+import com.datastax.oss.driver.api.testinfra.ccm.CcmRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionUtils;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import com.datastax.oss.driver.internal.core.config.typesafe.DefaultDriverConfigLoaderBuilder;
+import com.datastax.oss.driver.internal.core.cql.CqlRequestHandler;
+import com.google.common.base.Strings;
+import java.util.List;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.BeforeClass;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.junit.rules.RuleChain;
+import org.junit.rules.TestRule;
+import org.mockito.ArgumentCaptor;
+import org.mockito.Mockito;
+import org.slf4j.LoggerFactory;
+
+@Category(ParallelizableTests.class)
+public class ExecutionInfoWarningsIT {
+
+  private static final int MAX_QUERY_LENGTH = 500;
+  private static final CcmRule CCM = CcmRule.getInstance();
+  private static final SessionRule<CqlSession> SESSION_RULE =","[{'comment': 'which version of DSE is picked for that test if we are not specifying it explicitly?', 'commenter': 'tomekl007'}, {'comment': ""Tests don't pick a version, they only define version requirements (`@CassandraRequirement`, `@DseRequirement`). We pick a version when we run the test suite, with the `ccm.version` system property; if a test has requirements that don't match this version, it doesn't run.\r\n\r\nIf there is a `CassandraRequirement` and we run with DSE, we pick compare using the closest matching Cassandra version. See `CcmBridge.getCassandraVersion()`. You can also take a look at `BaseCcmRule.apply()` for more details.\r\n\r\nIn this case the test has no requirements, so it will run with whichever DSE version you start it with."", 'commenter': 'olim7t'}]"
1184,integration-tests/src/test/java/com/datastax/oss/driver/api/core/cql/ExecutionInfoWarningsIT.java,"@@ -0,0 +1,191 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.datastax.oss.driver.api.core.cql;
+
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.mockito.Mockito.after;
+import static org.mockito.Mockito.verify;
+
+import ch.qos.logback.classic.Level;
+import ch.qos.logback.classic.Logger;
+import ch.qos.logback.classic.spi.ILoggingEvent;
+import ch.qos.logback.core.Appender;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
+import com.datastax.oss.driver.api.testinfra.ccm.CcmRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionUtils;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import com.datastax.oss.driver.internal.core.config.typesafe.DefaultDriverConfigLoaderBuilder;
+import com.datastax.oss.driver.internal.core.cql.CqlRequestHandler;
+import com.google.common.base.Strings;
+import java.util.List;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.BeforeClass;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.junit.rules.RuleChain;
+import org.junit.rules.TestRule;
+import org.mockito.ArgumentCaptor;
+import org.mockito.Mockito;
+import org.slf4j.LoggerFactory;
+
+@Category(ParallelizableTests.class)
+public class ExecutionInfoWarningsIT {
+
+  private static final int MAX_QUERY_LENGTH = 500;
+  private static final CcmRule CCM = CcmRule.getInstance();
+  private static final SessionRule<CqlSession> SESSION_RULE =
+      SessionRule.builder(CCM)
+          .withConfigLoader(
+              SessionUtils.configLoaderBuilder()
+                  .withInt(DefaultDriverOption.REQUEST_PAGE_SIZE, 20)
+                  .withInt(DefaultDriverOption.REQUEST_LOGGER_MAX_QUERY_LENGTH, MAX_QUERY_LENGTH)
+                  .withProfile(
+                      ""log-disabled"",
+                      DefaultDriverConfigLoaderBuilder.profileBuilder()
+                          .withString(DefaultDriverOption.REQUEST_DISABLE_LOG_WARNINGS, ""true"")
+                          .build())
+                  .build())
+          .build();
+  private static final String KEY = ""test"";
+
+  @ClassRule public static TestRule ccmChain = RuleChain.outerRule(CCM).around(SESSION_RULE);
+
+  private final Appender<ILoggingEvent> appender =
+      (Appender<ILoggingEvent>) Mockito.mock(Appender.class);
+  private final ArgumentCaptor<ILoggingEvent> loggingEventCaptor =
+      ArgumentCaptor.forClass(ILoggingEvent.class);
+  private Logger logger;
+  private Level originalLoggerLevel;
+
+  @BeforeClass
+  public static void setupSchema() {
+    // table with simple primary key, single cell.
+    SESSION_RULE
+        .session()
+        .execute(
+            SimpleStatement.builder(""CREATE TABLE IF NOT EXISTS test (k int primary key, v text)"")
+                .withExecutionProfile(SESSION_RULE.slowProfile())
+                .build());
+    for (int i = 0; i < 100; i++) {
+      SESSION_RULE
+          .session()
+          .execute(
+              SimpleStatement.builder(""INSERT INTO test (k, v) VALUES (?, ?)"")
+                  .addPositionalValues(KEY, i)
+                  .build());
+    }
+  }
+
+  @Before
+  public void setupLogger() {
+    // setup the log appender
+    logger = (Logger) LoggerFactory.getLogger(CqlRequestHandler.class);
+    originalLoggerLevel = logger.getLevel();
+    logger.setLevel(Level.WARN);
+    logger.addAppender(appender);
+  }
+
+  @After
+  public void cleanupLogger() {
+    logger.setLevel(originalLoggerLevel);","[{'comment': ""Will this `@Before` and `@After` work properly together with `ParallelizableTests`? Isn't there a possiblity that this will work in a non-deterministic way? "", 'commenter': 'tomekl007'}, {'comment': 'Right, if the default level is `ERROR` and other tests also manipulate this particular level dynamically, there could be race conditions. The test should be serial (no `@Category` annotation, build the `CcmRule` with `CustomCcmRule.builder().build()`).\r\n\r\nNow that you mention it, this could be an issue in the 1.x PR as well.', 'commenter': 'olim7t'}]"
1184,core/src/main/resources/reference.conf,"@@ -759,6 +759,16 @@ datastax-java-driver {
       # Overridable in a profile: yes
       consistency = ONE
     }
+
+    # Whether logging of server warnings generated during query execution should be disabled by the
+    # driver. All server generated warnings will be available programmatically via the ExecutionInfo
+    # object on the executed statement's ResultSet. If set to ""true"", this will prevent the driver
+    # from logging these warnings.
+    #
+    # Required: yes
+    # Modifiable at runtime: yes, the new value will be used for query warnings received after the change.
+    # Overridable in a profile: yes
+    disable-log-warnings = false","[{'comment': 'Do we want to expose that setting in a double negation way? (we are setting that it should NOT be disabled)\r\nMaybe it would be better to have `enable-log-warning = true`?', 'commenter': 'tomekl007'}, {'comment': ""Good point. I'll change this in an update."", 'commenter': 'emerkle826'}]"
1184,core/src/main/java/com/datastax/oss/driver/api/core/config/DefaultDriverOption.java,"@@ -171,6 +171,8 @@
 
   NETTY_TIMER_TICK_DURATION(""advanced.netty.timer.tick-duration""),
   NETTY_TIMER_TICKS_PER_WHEEL(""advanced.netty.timer.ticks-per-wheel""),
+
+  REQUEST_DISABLE_LOG_WARNINGS(""advanced.request.disable-log-warnings""),","[{'comment': 'ðŸ‘ for adding it last, otherwise it would break backward compatibility.', 'commenter': 'olim7t'}]"
1184,core/src/main/java/com/datastax/oss/driver/internal/core/cql/CqlRequestHandler.java,"@@ -356,11 +359,34 @@ private void setFinalResult(
               TimeUnit.NANOSECONDS);
         }
       }
+      // log the warnings if they have NOT been disabled
+      if (LOG.isWarnEnabled()
+          && !executionInfo.getWarnings().isEmpty()","[{'comment': ""Nit -- this might be premature optimization but it won't hurt: `!executionInfo.getWarnings().isEmpty()` is a simpler operation than `LOG.isWarnEnabled()` (if you look at how logback implements the latter). It's also probably the case for `executionProfile.getBoolean`. Could you flip the evaluation order?\r\n```java\r\nif (!executionInfo.getWarnings().isEmpty()\r\n    && !executionProfile.getBoolean(DefaultDriverOption.REQUEST_DISABLE_LOG_WARNINGS)\r\n    && LOG.isWarnEnabled()) {\r\n```"", 'commenter': 'olim7t'}]"
1184,core/src/main/java/com/datastax/oss/driver/internal/core/cql/CqlRequestHandler.java,"@@ -356,11 +359,34 @@ private void setFinalResult(
               TimeUnit.NANOSECONDS);
         }
       }
+      // log the warnings if they have NOT been disabled
+      if (LOG.isWarnEnabled()
+          && !executionInfo.getWarnings().isEmpty()
+          && !executionProfile.getBoolean(DefaultDriverOption.REQUEST_DISABLE_LOG_WARNINGS)) {
+        logServerWarnings(executionInfo.getWarnings());
+      }","[{'comment': ""Note: the `if (result.complete(resultSet)) { ... }` block above is in case another speculative execution beat us to completing the result.\r\nWe log the warnings outside of that block, which means we'll do so even if we raced. I think I'm OK with it but I thought I'd just mention it in passing."", 'commenter': 'olim7t'}]"
1184,core/src/main/java/com/datastax/oss/driver/internal/core/cql/CqlRequestHandler.java,"@@ -356,11 +359,34 @@ private void setFinalResult(
               TimeUnit.NANOSECONDS);
         }
       }
+      // log the warnings if they have NOT been disabled
+      if (LOG.isWarnEnabled()
+          && !executionInfo.getWarnings().isEmpty()
+          && !executionProfile.getBoolean(DefaultDriverOption.REQUEST_DISABLE_LOG_WARNINGS)) {
+        logServerWarnings(executionInfo.getWarnings());
+      }
     } catch (Throwable error) {
       setFinalError(error, callback.node, -1);
     }
   }
 
+  private void logServerWarnings(List<String> warnings) {
+    // use the RequestLogFormatter to format the query
+    StringBuilder statementString = new StringBuilder();
+    requestLogFormatter.appendRequest(
+        statement,
+        executionProfile.getInt(DefaultDriverOption.REQUEST_LOGGER_MAX_QUERY_LENGTH, 500),
+        executionProfile.getBoolean(DefaultDriverOption.REQUEST_LOGGER_VALUES, false),
+        executionProfile.getInt(DefaultDriverOption.REQUEST_LOGGER_MAX_VALUES, 0),
+        executionProfile.getInt(DefaultDriverOption.REQUEST_LOGGER_MAX_VALUE_LENGTH, 0),
+        statementString);
+    // log each warning separately
+    warnings.forEach(
+        (warning) -> {
+          LOG.warn(""Query '{}' generated server side warning(s): {}"", statementString, warning);
+        });","[{'comment': 'Nit: unnecessary curly braces for this lambda.', 'commenter': 'olim7t'}]"
1184,core/src/main/resources/reference.conf,"@@ -759,6 +759,16 @@ datastax-java-driver {
       # Overridable in a profile: yes
       consistency = ONE
     }
+
+    # Whether logging of server warnings generated during query execution should be disabled by the
+    # driver. All server generated warnings will be available programmatically via the ExecutionInfo
+    # object on the executed statement's ResultSet. If set to ""true"", this will prevent the driver
+    # from logging these warnings.","[{'comment': 'We should also mention that it will reuse the formatting options in `advanced.request-tracker`.', 'commenter': 'olim7t'}]"
1185,core/src/main/java/com/datastax/oss/driver/internal/core/adminrequest/AdminRequestHandler.java,"@@ -60,7 +65,17 @@ public static AdminRequestHandler query(
         new Query(
             query,
             buildQueryOptions(pageSize, serialize(parameters, channel.protocolVersion()), null));
-    String debugString = ""query '"" + query + ""'"";
+    return createQuery(channel, message, parameters, timeout, logPrefix);
+  }
+
+  private static AdminRequestHandler createQuery(","[{'comment': 'Nit: could be renamed to `createAdminRequestHandler` â€“ `createQuery` is a bit misleading.', 'commenter': 'adutra'}]"
1185,core/src/main/java/com/datastax/oss/driver/internal/core/adminrequest/AdminRequestHandler.java,"@@ -148,7 +163,9 @@ public void onResponse(Frame responseFrame) {
       ByteBuffer pagingState = rows.getMetadata().pagingState;
       AdminRequestHandler nextHandler = (pagingState == null) ? null : this.copy(pagingState);
       setFinalResult(new AdminResult(rows, nextHandler, channel.protocolVersion()));
-    } else if (message instanceof Prepared) {
+    } else if (message instanceof Prepared","[{'comment': 'I think we can broaden the scope of this check to `message instanceof Result` â€“ `Rows` being just a special case of `Result` that was already handled in the previous `if` block.', 'commenter': 'adutra'}, {'comment': 'Good idea - taking into account the fact `Void` and `Prepared` is extending `Result`.\r\nBut the question is: do we want to treat `SchemaChange` and `SetKeyspace` as successes here as well? (both are extending `Result`). I think that we can, wdyt?\r\n', 'commenter': 'tomekl007'}, {'comment': 'The case has never presented itself until now (only `Rows` and `Prepared` were handled so far), so I assume that we can safely consider these as successes as well.', 'commenter': 'adutra'}, {'comment': 'ok, done', 'commenter': 'tomekl007'}]"
1186,core/src/main/java/com/datastax/oss/driver/internal/core/context/LifecycleListener.java,"@@ -0,0 +1,51 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.context;
+
+import com.datastax.oss.driver.api.core.session.SessionBuilder;
+
+/**
+ * A component that gets notified of certain events in the session's lifecycle.
+ *
+ * <p>This is intended for third-party extensions, no built-in components implement this.
+ */
+public interface LifecycleListener extends AutoCloseable {
+
+  /**
+   * Invoked when the session is ready to process user requests.
+   *
+   * <p>This corresponds to the moment when the {@link SessionBuilder#build()} returns, or the
+   * future returned by {@link SessionBuilder#buildAsync()} completes. If the session initialization
+   * fails, this method will not get called.
+   *
+   * <p>This method is invoked on a driver thread, it should complete relatively quickly and not
+   * block.
+   */
+  void onSessionReady();
+
+  /**
+   * Invoked when the session shuts down.
+   *
+   * <p>Note that this method gets called even if the shutdown results from a failed initialization.
+   * In that case, implementations should be ready to handle a call to this method even though
+   * {@link #onSessionReady()} hasn't been invoked.
+   *
+   * <p>This method is invoked on a driver thread, it should complete relatively quickly and not
+   * block.
+   */
+  @Override
+  void close() throws Exception;","[{'comment': 'Is this meant to be where implementors shut down their own listeners, or as a notification hook that gets invoked when the session closes? For clarity I think we should dissociate both and add a method `onSessionClosed` to capture the second use case. Also, we might want to pass the `Throwable` that caused the session initialization to fail.', 'commenter': 'adutra'}, {'comment': 'It gets called when the session closes.\r\nImplementors should perform any necessary cleanup. A typical example would be to cancel a scheduled task that they had scheduled on a driver admin event loop.', 'commenter': 'olim7t'}, {'comment': ""I don't understand how `onSessionClosed` would be a different use case, both methods would be called when the session closes.\r\nAs for passing the `Throwable` that caused an init failure, I don't see a strong case for it. This method mostly exists so that we don't leave anything running on the admin event loops, I think it's unlikely that a listener would do something differently depending on the error.\r\nAnd if we find the need to add it later, it will be relatively easy to have a default implementation of `close(Throwable)` that calls `close()`."", 'commenter': 'olim7t'}, {'comment': 'It\'s a matter of semantics: `onSessionClosed` means: ""I am being notified that a given session has been closed"" whereas the contract of `close` (as defined in `AutoCloseable`) is ""I am being told to close myself"". Here, it sounds to me as if we are distorting the original purpose of `close`.\r\n\r\nMore generally speaking, a typical listener API would expose methods such as `onEventXSuccessful(Foo receiver)`, `onEventYFailed(Bar receiver, Throwable error)`. It would also expose methods to register/unregister the listener against a given Session. None of this is present in this interface, which is why I was a bit thrown off initially.\r\n\r\nBut OK to keep things simple for now; it\'s an internal API after all so we are free to change it later.\r\n\r\n', 'commenter': 'adutra'}]"
1186,core/src/main/java/com/datastax/oss/driver/internal/core/session/DefaultSession.java,"@@ -393,6 +394,7 @@ private void afterInitialSchemaRefresh(CqlIdentifier keyspace) {
                   if (error != null) {
                     initFuture.completeExceptionally(error);
                   } else {
+                    notifyLifecycleListeners();","[{'comment': ""Shouldn't the call come after `initFuture.complete(DefaultSession.this);`?"", 'commenter': 'adutra'}, {'comment': ""It's really up to us to decide and I don't think it makes such a difference either way.\r\n\r\nBut given how I worded it in the javadocs, you're right, I should probably do it after."", 'commenter': 'olim7t'}]"
1186,core/src/main/java/com/datastax/oss/driver/internal/core/context/LifecycleListener.java,"@@ -0,0 +1,51 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.context;
+
+import com.datastax.oss.driver.api.core.session.SessionBuilder;
+
+/**
+ * A component that gets notified of certain events in the session's lifecycle.
+ *
+ * <p>This is intended for third-party extensions, no built-in components implement this.
+ */
+public interface LifecycleListener extends AutoCloseable {
+
+  /**
+   * Invoked when the session is ready to process user requests.
+   *
+   * <p>This corresponds to the moment when the {@link SessionBuilder#build()} returns, or the
+   * future returned by {@link SessionBuilder#buildAsync()} completes. If the session initialization
+   * fails, this method will not get called.
+   *
+   * <p>This method is invoked on a driver thread, it should complete relatively quickly and not
+   * block.
+   */
+  void onSessionReady();","[{'comment': 'We will need to get the unique session identifier as a parameter here (and for `close()` as well)\r\nIf the listener will get the notification for every new session it may want to create a resource for that specific session. Then to be able to identify that resource in the `close()` callback we would need to have session id.', 'commenter': 'tomekl007'}, {'comment': ""We've discussed this elsewhere already, but just to document the answer: a given context is associated to exactly one session, so that won't be an issue."", 'commenter': 'olim7t'}]"
1189,driver-core/src/main/java/com/datastax/driver/core/querybuilder/Select.java,"@@ -857,5 +870,14 @@ public Select from(TableMetadata table) {
       previousSelection = null;
       return super.from(table);
     }
+
+    @Override
+    public Select from(MaterializedViewMetadata view) {
+      if (previousSelection != null) {
+        addName(previousSelection);
+      }
+      previousSelection = null;","[{'comment': 'Why we are setting it back explicitly to `null`? To prevent memory leak?', 'commenter': 'tomekl007'}, {'comment': 'That\'s a trick to handle aliased selections:\r\n```java\r\nselect().column(""column1"").as(""alias1"")\r\n```\r\nInternally all selectors are added to the `columnNames` list. But if a column is aliased, it must be wrapped into an `Alias` before (see implementation of `as()`).\r\n`column()` doesn\'t know yet if its argument will be aliased or not, because that only comes in the `as()` called that is chained after. So instead it stores its argument temporarily in `previousSelection`.\r\nIf the next method is `as()`, it will wrap `previousSelection` and add it.\r\nIf it\'s any other method that can be chained after `column`, it adds it directly. `from()` is one of these methods.\r\n\r\n`previousSelection` must be reset so that another method doesn\'t add it again later.', 'commenter': 'olim7t'}]"
1189,driver-core/src/test/java/com/datastax/driver/core/querybuilder/QueryBuilderExecutionTest.java,"@@ -924,4 +927,45 @@ public void should_support_group_by() throws Exception {
                         .groupBy(""b"")))
         .containsExactly(row(1, 1, 12));
   }
+
+  /**
+   * Validates that {@link QueryBuilder} can construct a SELECT query for a materialized view.
+   *
+   * @test_category queries:builder
+   * @jira_ticket JAVA-2123
+   * @since 3.7.0
+   */
+  @CassandraVersion(
+      value = ""3.0"",
+      description = ""Support for materialized views was added to C* 3.0"")
+  @Test(groups = ""short"")
+  public void should_select_from_materialized_view() {
+
+    String table = TestUtils.generateIdentifier(""table"");
+    final String mv = TestUtils.generateIdentifier(""mv"");
+
+    execute(
+        String.format(""CREATE TABLE %s (pk int, cc int, v int, PRIMARY KEY (pk, cc))"", table),
+        String.format(""INSERT INTO %s (pk, cc, v) VALUES (0,0,0)"", table),
+        String.format(""INSERT INTO %s (pk, cc, v) VALUES (0,1,1)"", table),
+        String.format(""INSERT INTO %s (pk, cc, v) VALUES (0,2,2)"", table),
+        String.format(
+            ""CREATE MATERIALIZED VIEW %s AS SELECT cc FROM %s WHERE cc IS NOT NULL PRIMARY KEY (pk, cc)"",
+            mv, table));
+
+    // Wait until the MV is fully constructed
+    ConditionChecker.check()
+        .that(
+            new Callable<Boolean>() {
+              @Override
+              public Boolean call() {
+                return session().execute(""SELECT * FROM "" + mv).all().size() == 3;
+              }
+            })
+        .before(1, MINUTES)
+        .becomesTrue();
+
+    assertThat(session().execute(select().column(""cc"").as(""mycc"").from(mv)))
+        .containsExactly(row(0), row(1), row(2));","[{'comment': 'this `row(n)` syntax is validating only the fact that row exists? or also the content of a row?', 'commenter': 'tomekl007'}, {'comment': 'It also asserts the contents, see `ResultSetAssert`.', 'commenter': 'adutra'}]"
1189,driver-core/src/test/java/com/datastax/driver/core/querybuilder/QueryBuilderExecutionTest.java,"@@ -924,4 +927,45 @@ public void should_support_group_by() throws Exception {
                         .groupBy(""b"")))
         .containsExactly(row(1, 1, 12));
   }
+
+  /**
+   * Validates that {@link QueryBuilder} can construct a SELECT query for a materialized view.
+   *
+   * @test_category queries:builder
+   * @jira_ticket JAVA-2123
+   * @since 3.7.0
+   */
+  @CassandraVersion(
+      value = ""3.0"",
+      description = ""Support for materialized views was added to C* 3.0"")
+  @Test(groups = ""short"")
+  public void should_select_from_materialized_view() {
+
+    String table = TestUtils.generateIdentifier(""table"");
+    final String mv = TestUtils.generateIdentifier(""mv"");
+
+    execute(
+        String.format(""CREATE TABLE %s (pk int, cc int, v int, PRIMARY KEY (pk, cc))"", table),
+        String.format(""INSERT INTO %s (pk, cc, v) VALUES (0,0,0)"", table),
+        String.format(""INSERT INTO %s (pk, cc, v) VALUES (0,1,1)"", table),
+        String.format(""INSERT INTO %s (pk, cc, v) VALUES (0,2,2)"", table),
+        String.format(
+            ""CREATE MATERIALIZED VIEW %s AS SELECT cc FROM %s WHERE cc IS NOT NULL PRIMARY KEY (pk, cc)"",
+            mv, table));
+
+    // Wait until the MV is fully constructed
+    ConditionChecker.check()
+        .that(
+            new Callable<Boolean>() {
+              @Override
+              public Boolean call() {
+                return session().execute(""SELECT * FROM "" + mv).all().size() == 3;
+              }
+            })
+        .before(1, MINUTES)
+        .becomesTrue();
+
+    assertThat(session().execute(select().column(""cc"").as(""mycc"").from(mv)))","[{'comment': 'I see that we added a method `from(MaterializedViewMetadata view)` that takes `MaterializedViewMetadata` type as an argument. Here we are passing `mv` that is of a `String` type. Where the conversion from `String` to `MaterializedViewMetadata` is happening?', 'commenter': 'tomekl007'}, {'comment': ""There's no conversion, the implementation can work either with a plain string or a metadata object.\r\n\r\nThat's probably an oversight. If we want to cover the functionality that's been added, we should indeed extract a `MaterializedViewMetadata` from the cluster metadata and pass that to the query builder."", 'commenter': 'olim7t'}, {'comment': 'Good catch thanks.', 'commenter': 'adutra'}]"
1192,integration-tests/src/test/java/com/datastax/oss/driver/mapper/GetEntityIt.java,"@@ -0,0 +1,18 @@
+/*","[{'comment': 'IT is not where I want it yet. It will be flushed out when I return', 'commenter': 'GregBestland'}, {'comment': 'There is a problem with the compilation of this branch. \r\nThe `SetEntityIT` fails because there is no class `com.datastax.oss.driver.mapper.model.inventory.InventoryMapperBuilder;`\r\nhttps://travis-ci.com/datastax/java-driver/jobs/180213823\r\n\r\nI cannot compile project in intelliJ because I got got:\r\n`Error:java: Bad service configuration file, or exception thrown while constructing Processor object: javax.annotation.processing.Processor: Provider com.datastax.oss.driver.internal.mapper.processor.MapperProcessor could not be instantiated: java.lang.NoClassDefFoundError: com/datastax/oss/driver/internal/mapper/processor/ProcessorContext`\r\n\r\nOther question: how to start the annotation processor from `mvn`?', 'commenter': 'tomekl007'}]"
1192,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoGetEntityMethodGenerator.java,"@@ -0,0 +1,145 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.mapper.processor.dao;
+
+import com.datastax.oss.driver.api.core.AsyncPagingIterable;
+import com.datastax.oss.driver.api.core.PagingIterable;
+import com.datastax.oss.driver.api.core.cql.AsyncResultSet;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import com.datastax.oss.driver.api.mapper.annotations.Entity;
+import com.datastax.oss.driver.api.mapper.annotations.GetEntity;
+import com.datastax.oss.driver.internal.mapper.processor.MethodGenerator;
+import com.datastax.oss.driver.internal.mapper.processor.ProcessorContext;
+import com.datastax.oss.driver.internal.mapper.processor.SkipGenerationException;
+import com.squareup.javapoet.ClassName;
+import com.squareup.javapoet.MethodSpec;
+import java.util.List;
+import javax.lang.model.element.Element;
+import javax.lang.model.element.ElementKind;
+import javax.lang.model.element.ExecutableElement;
+import javax.lang.model.element.Modifier;
+import javax.lang.model.element.TypeElement;
+import javax.lang.model.element.VariableElement;
+import javax.lang.model.type.DeclaredType;
+import javax.lang.model.type.TypeKind;
+import javax.lang.model.type.TypeMirror;
+
+public class DaoGetEntityMethodGenerator implements MethodGenerator {
+
+  private final ExecutableElement methodElement;
+  private final DaoImplementationGenerator daoImplementationGenerator;
+  private final String targetParameterName;
+  private final TypeElement entityElement;
+  private boolean pagingReturnType;
+
+  public DaoGetEntityMethodGenerator(
+      ExecutableElement methodElement,
+      DaoImplementationGenerator daoImplementationGenerator,
+      ProcessorContext context) {
+    this.methodElement = methodElement;
+    this.daoImplementationGenerator = daoImplementationGenerator;
+    // Methods should only have one parameter
+    if (methodElement.getParameters().size() != 1) {
+      context
+          .getMessager()","[{'comment': 'typo here: `getMessanger()`', 'commenter': 'tomekl007'}, {'comment': ""This naming is inherited from javax.annotation.processing's Messager interface. The naming is wonky, but it's consistent."", 'commenter': 'GregBestland'}]"
1192,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoGetEntityMethodGenerator.java,"@@ -0,0 +1,145 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.mapper.processor.dao;
+
+import com.datastax.oss.driver.api.core.AsyncPagingIterable;
+import com.datastax.oss.driver.api.core.PagingIterable;
+import com.datastax.oss.driver.api.core.cql.AsyncResultSet;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import com.datastax.oss.driver.api.mapper.annotations.Entity;
+import com.datastax.oss.driver.api.mapper.annotations.GetEntity;
+import com.datastax.oss.driver.internal.mapper.processor.MethodGenerator;
+import com.datastax.oss.driver.internal.mapper.processor.ProcessorContext;
+import com.datastax.oss.driver.internal.mapper.processor.SkipGenerationException;
+import com.squareup.javapoet.ClassName;
+import com.squareup.javapoet.MethodSpec;
+import java.util.List;
+import javax.lang.model.element.Element;
+import javax.lang.model.element.ElementKind;
+import javax.lang.model.element.ExecutableElement;
+import javax.lang.model.element.Modifier;
+import javax.lang.model.element.TypeElement;
+import javax.lang.model.element.VariableElement;
+import javax.lang.model.type.DeclaredType;
+import javax.lang.model.type.TypeKind;
+import javax.lang.model.type.TypeMirror;
+
+public class DaoGetEntityMethodGenerator implements MethodGenerator {
+
+  private final ExecutableElement methodElement;
+  private final DaoImplementationGenerator daoImplementationGenerator;
+  private final String targetParameterName;
+  private final TypeElement entityElement;
+  private boolean pagingReturnType;
+
+  public DaoGetEntityMethodGenerator(
+      ExecutableElement methodElement,
+      DaoImplementationGenerator daoImplementationGenerator,
+      ProcessorContext context) {
+    this.methodElement = methodElement;
+    this.daoImplementationGenerator = daoImplementationGenerator;
+    // Methods should only have one parameter
+    if (methodElement.getParameters().size() != 1) {","[{'comment': ""If I understand correctly, all code in a constructor is a validation, right?\r\nIsn't there a risk that we will finish with the partially constructed object if the constructor invocation fails in let say half-of the-process. Maybe MethodGenerator should have `pre-validate()` method that is called from an external component before calling the constructor of the implementation?"", 'commenter': 'tomekl007'}, {'comment': ""Yes, the constructor code is all validation. If validation fails, the constructor throws `SkipGenerationException` and the parent class will discard this instance.\r\n\r\nWe could add a `validate()` method, make computed fields like `entityElement` non-final, and move the code around, but that wouldn't make a big difference."", 'commenter': 'olim7t'}]"
1192,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoGetEntityMethodGenerator.java,"@@ -0,0 +1,145 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.mapper.processor.dao;
+
+import com.datastax.oss.driver.api.core.AsyncPagingIterable;
+import com.datastax.oss.driver.api.core.PagingIterable;
+import com.datastax.oss.driver.api.core.cql.AsyncResultSet;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import com.datastax.oss.driver.api.mapper.annotations.Entity;
+import com.datastax.oss.driver.api.mapper.annotations.GetEntity;
+import com.datastax.oss.driver.internal.mapper.processor.MethodGenerator;
+import com.datastax.oss.driver.internal.mapper.processor.ProcessorContext;
+import com.datastax.oss.driver.internal.mapper.processor.SkipGenerationException;
+import com.squareup.javapoet.ClassName;
+import com.squareup.javapoet.MethodSpec;
+import java.util.List;
+import javax.lang.model.element.Element;
+import javax.lang.model.element.ElementKind;
+import javax.lang.model.element.ExecutableElement;
+import javax.lang.model.element.Modifier;
+import javax.lang.model.element.TypeElement;
+import javax.lang.model.element.VariableElement;
+import javax.lang.model.type.DeclaredType;
+import javax.lang.model.type.TypeKind;
+import javax.lang.model.type.TypeMirror;
+
+public class DaoGetEntityMethodGenerator implements MethodGenerator {
+
+  private final ExecutableElement methodElement;
+  private final DaoImplementationGenerator daoImplementationGenerator;
+  private final String targetParameterName;
+  private final TypeElement entityElement;
+  private boolean pagingReturnType;
+
+  public DaoGetEntityMethodGenerator(
+      ExecutableElement methodElement,
+      DaoImplementationGenerator daoImplementationGenerator,
+      ProcessorContext context) {
+    this.methodElement = methodElement;
+    this.daoImplementationGenerator = daoImplementationGenerator;
+    // Methods should only have one parameter
+    if (methodElement.getParameters().size() != 1) {
+      context
+          .getMessager()
+          .error(
+              methodElement, ""%s method must have one parameter"", GetEntity.class.getSimpleName());
+      throw new SkipGenerationException();
+    }
+    // Parameter name as a string
+    String tmpParameterString = null;
+    // The return type as an element
+    TypeElement returnTypeElement = null;
+    VariableElement parameterElement = methodElement.getParameters().get(0);
+    TypeMirror parameterType = parameterElement.asType();
+    // Check the parameter type make sure it matches an expected type
+    if (context.getClassUtils().implementsGettableByName(parameterType)
+        || context.getClassUtils().isSame(parameterType, ResultSet.class)
+        || context.getClassUtils().isSame(parameterType, AsyncResultSet.class)) {
+      tmpParameterString = parameterElement.getSimpleName().toString();
+    }
+    // Check return type. Make sure it matches the expected type
+    TypeMirror returnType = methodElement.getReturnType();
+    if (returnType.getKind() == TypeKind.DECLARED) {
+      Element element = ((DeclaredType) returnType).asElement();
+      // Simple case return type is an entity type
+      if (element.getKind() == ElementKind.CLASS) {
+        if (element.getAnnotation(Entity.class) != null) {
+          returnTypeElement = ((TypeElement) ((DeclaredType) returnType).asElement());","[{'comment': ""don't we need checks for instance of before that?\r\nIs this `(returnType.getKind() == TypeKind.DECLARED) ` imply that we can cast to DeclaredType, right?\r\nWhat about cast to TypeElement? This check takes care of it`(if (element.getKind() == ElementKind.CLASS))`?"", 'commenter': 'tomekl007'}, {'comment': ""If a type mirror is of kind `DECLARED`, you can cast it to `DeclaredType` and get to the corresponding element (the body of the type that is declared).\r\nIf an element is of kind `CLASS`, you can cast it to `TypeElement`.\r\n\r\nWhat's a bit confusing here is that there is some duplication, line 81 can be simplified as `returnTypeElement = (TypeElement) element;`"", 'commenter': 'olim7t'}]"
1192,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoGetEntityMethodGenerator.java,"@@ -0,0 +1,145 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.mapper.processor.dao;
+
+import com.datastax.oss.driver.api.core.AsyncPagingIterable;
+import com.datastax.oss.driver.api.core.PagingIterable;
+import com.datastax.oss.driver.api.core.cql.AsyncResultSet;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import com.datastax.oss.driver.api.mapper.annotations.Entity;
+import com.datastax.oss.driver.api.mapper.annotations.GetEntity;
+import com.datastax.oss.driver.internal.mapper.processor.MethodGenerator;
+import com.datastax.oss.driver.internal.mapper.processor.ProcessorContext;
+import com.datastax.oss.driver.internal.mapper.processor.SkipGenerationException;
+import com.squareup.javapoet.ClassName;
+import com.squareup.javapoet.MethodSpec;
+import java.util.List;
+import javax.lang.model.element.Element;
+import javax.lang.model.element.ElementKind;
+import javax.lang.model.element.ExecutableElement;
+import javax.lang.model.element.Modifier;
+import javax.lang.model.element.TypeElement;
+import javax.lang.model.element.VariableElement;
+import javax.lang.model.type.DeclaredType;
+import javax.lang.model.type.TypeKind;
+import javax.lang.model.type.TypeMirror;
+
+public class DaoGetEntityMethodGenerator implements MethodGenerator {
+
+  private final ExecutableElement methodElement;
+  private final DaoImplementationGenerator daoImplementationGenerator;
+  private final String targetParameterName;
+  private final TypeElement entityElement;
+  private boolean pagingReturnType;
+
+  public DaoGetEntityMethodGenerator(
+      ExecutableElement methodElement,
+      DaoImplementationGenerator daoImplementationGenerator,
+      ProcessorContext context) {
+    this.methodElement = methodElement;
+    this.daoImplementationGenerator = daoImplementationGenerator;
+    // Methods should only have one parameter
+    if (methodElement.getParameters().size() != 1) {
+      context
+          .getMessager()
+          .error(
+              methodElement, ""%s method must have one parameter"", GetEntity.class.getSimpleName());
+      throw new SkipGenerationException();
+    }
+    // Parameter name as a string
+    String tmpParameterString = null;
+    // The return type as an element
+    TypeElement returnTypeElement = null;
+    VariableElement parameterElement = methodElement.getParameters().get(0);
+    TypeMirror parameterType = parameterElement.asType();
+    // Check the parameter type make sure it matches an expected type
+    if (context.getClassUtils().implementsGettableByName(parameterType)
+        || context.getClassUtils().isSame(parameterType, ResultSet.class)
+        || context.getClassUtils().isSame(parameterType, AsyncResultSet.class)) {
+      tmpParameterString = parameterElement.getSimpleName().toString();
+    }
+    // Check return type. Make sure it matches the expected type
+    TypeMirror returnType = methodElement.getReturnType();
+    if (returnType.getKind() == TypeKind.DECLARED) {
+      Element element = ((DeclaredType) returnType).asElement();
+      // Simple case return type is an entity type
+      if (element.getKind() == ElementKind.CLASS) {
+        if (element.getAnnotation(Entity.class) != null) {
+          returnTypeElement = ((TypeElement) ((DeclaredType) returnType).asElement());
+        }
+        // Return type is a an iterable type, check nested generic as well
+      } else if (element.getKind() == ElementKind.INTERFACE) {","[{'comment': ""can't we be more specific in that check?\r\nIs there a way to check at this level that `element.getKind()` is also implementing `AsyncPagingIterable` or `PagingIterable`?"", 'commenter': 'tomekl007'}, {'comment': ""That's done below, but I agree that it would be a bit more clear to do all the checks on the main type first, and only then inspect the type parameters. I'll change it."", 'commenter': 'olim7t'}]"
1192,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoGetEntityMethodGenerator.java,"@@ -0,0 +1,145 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.mapper.processor.dao;
+
+import com.datastax.oss.driver.api.core.AsyncPagingIterable;
+import com.datastax.oss.driver.api.core.PagingIterable;
+import com.datastax.oss.driver.api.core.cql.AsyncResultSet;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import com.datastax.oss.driver.api.mapper.annotations.Entity;
+import com.datastax.oss.driver.api.mapper.annotations.GetEntity;
+import com.datastax.oss.driver.internal.mapper.processor.MethodGenerator;
+import com.datastax.oss.driver.internal.mapper.processor.ProcessorContext;
+import com.datastax.oss.driver.internal.mapper.processor.SkipGenerationException;
+import com.squareup.javapoet.ClassName;
+import com.squareup.javapoet.MethodSpec;
+import java.util.List;
+import javax.lang.model.element.Element;
+import javax.lang.model.element.ElementKind;
+import javax.lang.model.element.ExecutableElement;
+import javax.lang.model.element.Modifier;
+import javax.lang.model.element.TypeElement;
+import javax.lang.model.element.VariableElement;
+import javax.lang.model.type.DeclaredType;
+import javax.lang.model.type.TypeKind;
+import javax.lang.model.type.TypeMirror;
+
+public class DaoGetEntityMethodGenerator implements MethodGenerator {
+
+  private final ExecutableElement methodElement;
+  private final DaoImplementationGenerator daoImplementationGenerator;
+  private final String targetParameterName;
+  private final TypeElement entityElement;
+  private boolean pagingReturnType;
+
+  public DaoGetEntityMethodGenerator(
+      ExecutableElement methodElement,
+      DaoImplementationGenerator daoImplementationGenerator,
+      ProcessorContext context) {
+    this.methodElement = methodElement;
+    this.daoImplementationGenerator = daoImplementationGenerator;
+    // Methods should only have one parameter
+    if (methodElement.getParameters().size() != 1) {
+      context
+          .getMessager()
+          .error(
+              methodElement, ""%s method must have one parameter"", GetEntity.class.getSimpleName());
+      throw new SkipGenerationException();
+    }
+    // Parameter name as a string
+    String tmpParameterString = null;
+    // The return type as an element
+    TypeElement returnTypeElement = null;
+    VariableElement parameterElement = methodElement.getParameters().get(0);
+    TypeMirror parameterType = parameterElement.asType();
+    // Check the parameter type make sure it matches an expected type
+    if (context.getClassUtils().implementsGettableByName(parameterType)
+        || context.getClassUtils().isSame(parameterType, ResultSet.class)
+        || context.getClassUtils().isSame(parameterType, AsyncResultSet.class)) {
+      tmpParameterString = parameterElement.getSimpleName().toString();
+    }
+    // Check return type. Make sure it matches the expected type
+    TypeMirror returnType = methodElement.getReturnType();
+    if (returnType.getKind() == TypeKind.DECLARED) {
+      Element element = ((DeclaredType) returnType).asElement();
+      // Simple case return type is an entity type
+      if (element.getKind() == ElementKind.CLASS) {
+        if (element.getAnnotation(Entity.class) != null) {
+          returnTypeElement = ((TypeElement) ((DeclaredType) returnType).asElement());
+        }
+        // Return type is a an iterable type, check nested generic as well
+      } else if (element.getKind() == ElementKind.INTERFACE) {
+        List<? extends TypeMirror> generics = ((DeclaredType) returnType).getTypeArguments();
+        if (generics.size() == 1) {
+          TypeMirror generic = generics.get(0);
+          Element genericElement = ((DeclaredType) generic).asElement();
+          if (genericElement.getAnnotation(Entity.class) != null) {
+            if (element.getSimpleName().toString().equals(PagingIterable.class.getSimpleName())","[{'comment': ""isn't it better to check equals on classes instead of strings?\r\nLike `element.getClass.equals(PagingIterable.class)`?"", 'commenter': 'tomekl007'}, {'comment': '`element.getClass()` will give you the actual class of the model object (a subtype of `javax.lang.model.element.Element`), not the class that it represents.\r\n\r\nHowever another problem here is that this would match any `PagingIterable` class, regardless of the package.\r\n\r\nThere is a method `context.getClassUtils().isSame(Element, Class<?>)` for this kind of check.', 'commenter': 'olim7t'}]"
1192,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoGetEntityMethodGenerator.java,"@@ -0,0 +1,145 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.mapper.processor.dao;
+
+import com.datastax.oss.driver.api.core.AsyncPagingIterable;
+import com.datastax.oss.driver.api.core.PagingIterable;
+import com.datastax.oss.driver.api.core.cql.AsyncResultSet;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import com.datastax.oss.driver.api.mapper.annotations.Entity;
+import com.datastax.oss.driver.api.mapper.annotations.GetEntity;
+import com.datastax.oss.driver.internal.mapper.processor.MethodGenerator;
+import com.datastax.oss.driver.internal.mapper.processor.ProcessorContext;
+import com.datastax.oss.driver.internal.mapper.processor.SkipGenerationException;
+import com.squareup.javapoet.ClassName;
+import com.squareup.javapoet.MethodSpec;
+import java.util.List;
+import javax.lang.model.element.Element;
+import javax.lang.model.element.ElementKind;
+import javax.lang.model.element.ExecutableElement;
+import javax.lang.model.element.Modifier;
+import javax.lang.model.element.TypeElement;
+import javax.lang.model.element.VariableElement;
+import javax.lang.model.type.DeclaredType;
+import javax.lang.model.type.TypeKind;
+import javax.lang.model.type.TypeMirror;
+
+public class DaoGetEntityMethodGenerator implements MethodGenerator {
+
+  private final ExecutableElement methodElement;
+  private final DaoImplementationGenerator daoImplementationGenerator;
+  private final String targetParameterName;
+  private final TypeElement entityElement;
+  private boolean pagingReturnType;
+
+  public DaoGetEntityMethodGenerator(
+      ExecutableElement methodElement,
+      DaoImplementationGenerator daoImplementationGenerator,
+      ProcessorContext context) {
+    this.methodElement = methodElement;
+    this.daoImplementationGenerator = daoImplementationGenerator;
+    // Methods should only have one parameter
+    if (methodElement.getParameters().size() != 1) {
+      context
+          .getMessager()
+          .error(
+              methodElement, ""%s method must have one parameter"", GetEntity.class.getSimpleName());
+      throw new SkipGenerationException();
+    }
+    // Parameter name as a string
+    String tmpParameterString = null;
+    // The return type as an element
+    TypeElement returnTypeElement = null;
+    VariableElement parameterElement = methodElement.getParameters().get(0);
+    TypeMirror parameterType = parameterElement.asType();
+    // Check the parameter type make sure it matches an expected type
+    if (context.getClassUtils().implementsGettableByName(parameterType)
+        || context.getClassUtils().isSame(parameterType, ResultSet.class)
+        || context.getClassUtils().isSame(parameterType, AsyncResultSet.class)) {
+      tmpParameterString = parameterElement.getSimpleName().toString();
+    }
+    // Check return type. Make sure it matches the expected type
+    TypeMirror returnType = methodElement.getReturnType();
+    if (returnType.getKind() == TypeKind.DECLARED) {
+      Element element = ((DeclaredType) returnType).asElement();
+      // Simple case return type is an entity type
+      if (element.getKind() == ElementKind.CLASS) {
+        if (element.getAnnotation(Entity.class) != null) {
+          returnTypeElement = ((TypeElement) ((DeclaredType) returnType).asElement());
+        }
+        // Return type is a an iterable type, check nested generic as well
+      } else if (element.getKind() == ElementKind.INTERFACE) {
+        List<? extends TypeMirror> generics = ((DeclaredType) returnType).getTypeArguments();
+        if (generics.size() == 1) {
+          TypeMirror generic = generics.get(0);
+          Element genericElement = ((DeclaredType) generic).asElement();
+          if (genericElement.getAnnotation(Entity.class) != null) {
+            if (element.getSimpleName().toString().equals(PagingIterable.class.getSimpleName())
+                || element
+                    .getSimpleName()
+                    .toString()
+                    .equals(AsyncPagingIterable.class.getSimpleName())) {
+              returnTypeElement = ((TypeElement) ((DeclaredType) generic).asElement());
+              pagingReturnType = true;
+            }
+          }
+        }
+      }
+    }
+    if (returnTypeElement == null) {
+      context
+          .getMessager()
+          .error(
+              methodElement,
+              ""Invalid return type specified. Expected annotated entity, PagingIterable, or AsyncPagingIterable "");","[{'comment': 'maybe in a log error, we should specify fully qualified class names?\r\nInstead of `com.datastax.driver.core.PagingIterable` to give more explicit info about expected type.\r\nAlso, we could assign it to constants ', 'commenter': 'tomekl007'}, {'comment': 'It\'s nice to be precise, but on the other hand we want to keep messages reasonably short and it should be pretty obvious that those are driver classes. I\'ll try with the full path to see what the compiler messages look like.\r\n\r\nAs for constants, one already exists, the class itself:\r\n```java\r\nerror(methodElement, ""blah blah %s"", PagingIterable.class.getSimpleName())\r\n```\r\nThe nice thing is that it survives refactorings.\r\n\r\nI did that for annotations already, but not for other driver classes, I\'ll revisit my code.', 'commenter': 'olim7t'}, {'comment': 'Nit: this error can be targeted more precisely to `parameterElement`.', 'commenter': 'olim7t'}]"
1192,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoGetEntityMethodGenerator.java,"@@ -0,0 +1,145 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.mapper.processor.dao;
+
+import com.datastax.oss.driver.api.core.AsyncPagingIterable;
+import com.datastax.oss.driver.api.core.PagingIterable;
+import com.datastax.oss.driver.api.core.cql.AsyncResultSet;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import com.datastax.oss.driver.api.mapper.annotations.Entity;
+import com.datastax.oss.driver.api.mapper.annotations.GetEntity;
+import com.datastax.oss.driver.internal.mapper.processor.MethodGenerator;
+import com.datastax.oss.driver.internal.mapper.processor.ProcessorContext;
+import com.datastax.oss.driver.internal.mapper.processor.SkipGenerationException;
+import com.squareup.javapoet.ClassName;
+import com.squareup.javapoet.MethodSpec;
+import java.util.List;
+import javax.lang.model.element.Element;
+import javax.lang.model.element.ElementKind;
+import javax.lang.model.element.ExecutableElement;
+import javax.lang.model.element.Modifier;
+import javax.lang.model.element.TypeElement;
+import javax.lang.model.element.VariableElement;
+import javax.lang.model.type.DeclaredType;
+import javax.lang.model.type.TypeKind;
+import javax.lang.model.type.TypeMirror;
+
+public class DaoGetEntityMethodGenerator implements MethodGenerator {
+
+  private final ExecutableElement methodElement;
+  private final DaoImplementationGenerator daoImplementationGenerator;
+  private final String targetParameterName;
+  private final TypeElement entityElement;
+  private boolean pagingReturnType;
+
+  public DaoGetEntityMethodGenerator(
+      ExecutableElement methodElement,
+      DaoImplementationGenerator daoImplementationGenerator,
+      ProcessorContext context) {
+    this.methodElement = methodElement;
+    this.daoImplementationGenerator = daoImplementationGenerator;
+    // Methods should only have one parameter
+    if (methodElement.getParameters().size() != 1) {
+      context
+          .getMessager()
+          .error(
+              methodElement, ""%s method must have one parameter"", GetEntity.class.getSimpleName());
+      throw new SkipGenerationException();
+    }
+    // Parameter name as a string
+    String tmpParameterString = null;
+    // The return type as an element
+    TypeElement returnTypeElement = null;
+    VariableElement parameterElement = methodElement.getParameters().get(0);
+    TypeMirror parameterType = parameterElement.asType();
+    // Check the parameter type make sure it matches an expected type
+    if (context.getClassUtils().implementsGettableByName(parameterType)","[{'comment': 'I think that maybe we can create 3 different paths of execution instead of handling everything in one.\r\nSo we could have 3 ifs:\r\n```\r\nif(context.getClassUtils().implementsGettableByName(parameterType)){\r\n   handleGettableByName()\r\n}\r\nelse if(context.getClassUtils().isSame(parameterType, ResultSet.class)){\r\nhandleResultSet()\r\n}\r\nelse if(context.getClassUtils().isSame(parameterType, AsyncResultSet.class)){\r\nhandleAsyncResultSet()\r\n}\r\n```\r\nThen in every of the `handle*` method we can focus on validaing only one possbile path instead of doing everything in multiple ifs that needs to take care of every of those 3 combinations, wdyt?', 'commenter': 'tomekl007'}, {'comment': 'This was my initial approach however what you will find is the validation that needs to be done is for the most part divorced from the parameter type, and that the code we create in each handel method would be just as convulted and complex as what we have now.', 'commenter': 'GregBestland'}]"
1192,mapper-processor/pom.xml,"@@ -94,6 +94,13 @@
           </execution>
         </executions>
       </plugin>
+      <plugin>
+        <groupId>org.revapi</groupId>
+        <artifactId>revapi-maven-plugin</artifactId>
+        <configuration>
+          <skip>true</skip>","[{'comment': ""I have problems with revapi configuration that doesn't allow me to compile the project (I've added two skip rules for `mapper-runtime` and `mapper-processor`)"", 'commenter': 'tomekl007'}, {'comment': ""The plugin fails because I forgot to add `revapi.json` files.\r\n\r\n> I've added two skip rules\r\n\r\nðŸ‘  for `mapper-processor` (it's purely internal). For `mapper-runtime` there are public types, such as the annotations, so we need to keep the plugin enabled and add a configuration file. I'll do this on `java2078`."", 'commenter': 'olim7t'}]"
1192,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/entity/EntityHelperGetMethodGenerator.java,"@@ -18,37 +18,98 @@
 import com.datastax.oss.driver.api.core.data.GettableByName;
 import com.datastax.oss.driver.internal.mapper.processor.PartialClassGenerator;
 import com.datastax.oss.driver.internal.mapper.processor.ProcessorContext;
+import com.datastax.oss.driver.shaded.guava.common.collect.ImmutableMap;
 import com.squareup.javapoet.ClassName;
 import com.squareup.javapoet.MethodSpec;
 import com.squareup.javapoet.ParameterSpec;
+import com.squareup.javapoet.TypeName;
 import com.squareup.javapoet.TypeSpec;
+import java.util.Map;
 import javax.lang.model.element.Modifier;
+import javax.lang.model.element.TypeElement;
 
 public class EntityHelperGetMethodGenerator implements PartialClassGenerator {
 
   private final EntityDefinition entityDefinition;
 
+  private static final Map<TypeName, String> PRIMITIVE_ACCESSORS =
+      ImmutableMap.<TypeName, String>builder()
+          .put(TypeName.BOOLEAN, ""Boolean"")
+          .put(TypeName.BYTE, ""Byte"")
+          .put(TypeName.DOUBLE, ""Double"")
+          .put(TypeName.FLOAT, ""Float"")
+          .put(TypeName.INT, ""Int"")
+          .put(TypeName.LONG, ""Long"")
+          .build();
+
+  private final EntityHelperGenerator enclosingClass;
+
   public EntityHelperGetMethodGenerator(
       EntityDefinition entityDefinition,
       EntityHelperGenerator enclosingClass,
       ProcessorContext context) {
     this.entityDefinition = entityDefinition;
+    this.enclosingClass = enclosingClass;
   }
 
   @Override
   public void addMembers(TypeSpec.Builder classBuilder) {
 
-    MethodSpec.Builder extractBuilder =
+    MethodSpec.Builder getBuilder =
         MethodSpec.methodBuilder(""get"")
             .addAnnotation(Override.class)
             .addModifiers(Modifier.PUBLIC)
             .addParameter(
                 ParameterSpec.builder(ClassName.get(GettableByName.class), ""source"").build())
-            .returns(entityDefinition.getClassName())
-            // TODO generate proper implementation (JAVA-2112)
-            .addStatement(""throw new $T($S)"", UnsupportedOperationException.class, ""TODO"");
+            .returns(entityDefinition.getClassName());
+
+    TypeName returnType = entityDefinition.getClassName();
+    String returnName = ""returnValue"";
+    getBuilder.addStatement(""$T $L = new $T()"", returnType, returnName, returnType);
+
+    int udtIndex = 0;
+    for (PropertyDefinition property : entityDefinition.getProperties()) {
+      TypeName type = property.getType();
+      String cqlName = property.getCqlName();
+      String setterName = property.getSetterName();
+      TypeElement childEntityElement = property.getEntityElement();
+      getBuilder.addCode(""\n"");","[{'comment': ""It looks a bit suspicious. \r\nIsn't there a method in the getBuilder that encapsulate that new line invocation (something like `beginControlFlow` similar to `endControlFlow`)? If not maybe we want to use `System.lineSeparator()` to make that system agnostic"", 'commenter': 'tomekl007'}, {'comment': ""I don't see a specific method in JavaPoet (`CodeBlock` is the class that all those methods delegate to).\r\nNote that JavaPoet uses `\\n` for all line breaks internally."", 'commenter': 'olim7t'}]"
1192,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/entity/EntityHelperGetMethodGenerator.java,"@@ -18,37 +18,98 @@
 import com.datastax.oss.driver.api.core.data.GettableByName;
 import com.datastax.oss.driver.internal.mapper.processor.PartialClassGenerator;
 import com.datastax.oss.driver.internal.mapper.processor.ProcessorContext;
+import com.datastax.oss.driver.shaded.guava.common.collect.ImmutableMap;
 import com.squareup.javapoet.ClassName;
 import com.squareup.javapoet.MethodSpec;
 import com.squareup.javapoet.ParameterSpec;
+import com.squareup.javapoet.TypeName;
 import com.squareup.javapoet.TypeSpec;
+import java.util.Map;
 import javax.lang.model.element.Modifier;
+import javax.lang.model.element.TypeElement;
 
 public class EntityHelperGetMethodGenerator implements PartialClassGenerator {
 
   private final EntityDefinition entityDefinition;
 
+  private static final Map<TypeName, String> PRIMITIVE_ACCESSORS =
+      ImmutableMap.<TypeName, String>builder()
+          .put(TypeName.BOOLEAN, ""Boolean"")
+          .put(TypeName.BYTE, ""Byte"")
+          .put(TypeName.DOUBLE, ""Double"")
+          .put(TypeName.FLOAT, ""Float"")
+          .put(TypeName.INT, ""Int"")
+          .put(TypeName.LONG, ""Long"")
+          .build();","[{'comment': ""This is duplicated from `EntityHelperSetMethodGenerator` but I'm leaving it here for now, it will all get extracted to a common class in JAVA-2115."", 'commenter': 'olim7t'}]"
1192,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/entity/EntityHelperGetMethodGenerator.java,"@@ -18,37 +18,98 @@
 import com.datastax.oss.driver.api.core.data.GettableByName;
 import com.datastax.oss.driver.internal.mapper.processor.PartialClassGenerator;
 import com.datastax.oss.driver.internal.mapper.processor.ProcessorContext;
+import com.datastax.oss.driver.shaded.guava.common.collect.ImmutableMap;
 import com.squareup.javapoet.ClassName;
 import com.squareup.javapoet.MethodSpec;
 import com.squareup.javapoet.ParameterSpec;
+import com.squareup.javapoet.TypeName;
 import com.squareup.javapoet.TypeSpec;
+import java.util.Map;
 import javax.lang.model.element.Modifier;
+import javax.lang.model.element.TypeElement;
 
 public class EntityHelperGetMethodGenerator implements PartialClassGenerator {
 
   private final EntityDefinition entityDefinition;
 
+  private static final Map<TypeName, String> PRIMITIVE_ACCESSORS =
+      ImmutableMap.<TypeName, String>builder()
+          .put(TypeName.BOOLEAN, ""Boolean"")
+          .put(TypeName.BYTE, ""Byte"")
+          .put(TypeName.DOUBLE, ""Double"")
+          .put(TypeName.FLOAT, ""Float"")
+          .put(TypeName.INT, ""Int"")
+          .put(TypeName.LONG, ""Long"")
+          .build();
+
+  private final EntityHelperGenerator enclosingClass;
+
   public EntityHelperGetMethodGenerator(
       EntityDefinition entityDefinition,
       EntityHelperGenerator enclosingClass,
       ProcessorContext context) {
     this.entityDefinition = entityDefinition;
+    this.enclosingClass = enclosingClass;
   }
 
   @Override
   public void addMembers(TypeSpec.Builder classBuilder) {
 
-    MethodSpec.Builder extractBuilder =
+    MethodSpec.Builder getBuilder =
         MethodSpec.methodBuilder(""get"")
             .addAnnotation(Override.class)
             .addModifiers(Modifier.PUBLIC)
             .addParameter(
                 ParameterSpec.builder(ClassName.get(GettableByName.class), ""source"").build())
-            .returns(entityDefinition.getClassName())
-            // TODO generate proper implementation (JAVA-2112)
-            .addStatement(""throw new $T($S)"", UnsupportedOperationException.class, ""TODO"");
+            .returns(entityDefinition.getClassName());
+
+    TypeName returnType = entityDefinition.getClassName();
+    String returnName = ""returnValue"";
+    getBuilder.addStatement(""$T $L = new $T()"", returnType, returnName, returnType);","[{'comment': 'Nit: you can number placeholders to avoid repeating the values:\r\n```suggestion\r\n    getBuilder.addStatement(""$1T $2L = new $1T()"", returnType, returnName);\r\n```', 'commenter': 'olim7t'}]"
1192,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/entity/EntityHelperGetMethodGenerator.java,"@@ -18,37 +18,98 @@
 import com.datastax.oss.driver.api.core.data.GettableByName;
 import com.datastax.oss.driver.internal.mapper.processor.PartialClassGenerator;
 import com.datastax.oss.driver.internal.mapper.processor.ProcessorContext;
+import com.datastax.oss.driver.shaded.guava.common.collect.ImmutableMap;
 import com.squareup.javapoet.ClassName;
 import com.squareup.javapoet.MethodSpec;
 import com.squareup.javapoet.ParameterSpec;
+import com.squareup.javapoet.TypeName;
 import com.squareup.javapoet.TypeSpec;
+import java.util.Map;
 import javax.lang.model.element.Modifier;
+import javax.lang.model.element.TypeElement;
 
 public class EntityHelperGetMethodGenerator implements PartialClassGenerator {
 
   private final EntityDefinition entityDefinition;
 
+  private static final Map<TypeName, String> PRIMITIVE_ACCESSORS =
+      ImmutableMap.<TypeName, String>builder()
+          .put(TypeName.BOOLEAN, ""Boolean"")
+          .put(TypeName.BYTE, ""Byte"")
+          .put(TypeName.DOUBLE, ""Double"")
+          .put(TypeName.FLOAT, ""Float"")
+          .put(TypeName.INT, ""Int"")
+          .put(TypeName.LONG, ""Long"")
+          .build();
+
+  private final EntityHelperGenerator enclosingClass;
+
   public EntityHelperGetMethodGenerator(
       EntityDefinition entityDefinition,
       EntityHelperGenerator enclosingClass,
       ProcessorContext context) {
     this.entityDefinition = entityDefinition;
+    this.enclosingClass = enclosingClass;
   }
 
   @Override
   public void addMembers(TypeSpec.Builder classBuilder) {
 
-    MethodSpec.Builder extractBuilder =
+    MethodSpec.Builder getBuilder =
         MethodSpec.methodBuilder(""get"")
             .addAnnotation(Override.class)
             .addModifiers(Modifier.PUBLIC)
             .addParameter(
                 ParameterSpec.builder(ClassName.get(GettableByName.class), ""source"").build())
-            .returns(entityDefinition.getClassName())
-            // TODO generate proper implementation (JAVA-2112)
-            .addStatement(""throw new $T($S)"", UnsupportedOperationException.class, ""TODO"");
+            .returns(entityDefinition.getClassName());
+
+    TypeName returnType = entityDefinition.getClassName();
+    String returnName = ""returnValue"";
+    getBuilder.addStatement(""$T $L = new $T()"", returnType, returnName, returnType);
+
+    int udtIndex = 0;
+    for (PropertyDefinition property : entityDefinition.getProperties()) {
+      TypeName type = property.getType();
+      String cqlName = property.getCqlName();
+      String setterName = property.getSetterName();
+      TypeElement childEntityElement = property.getEntityElement();
+      getBuilder.addCode(""\n"");
+      if (childEntityElement != null) {
+        udtIndex += 1;
+        // Populate udtInformation
+        String udtValueName = ""udtValue"" + udtIndex;
+        String valueName = ""value"" + udtIndex;
+        // Extract UdtValue to pass it on to underlying helper method
+        getBuilder.addStatement(""UdtValue $L = source.getUdtValue($S)"", udtValueName, cqlName);","[{'comment': 'Types should be injected with `$T` even if they are known statically. Otherwise the generated code might not have the import (in this case I think it works because the set method already imports it).\r\n```suggestion\r\n        getBuilder.addStatement(""$T $L = source.getUdtValue($S)"", UdtValue.class, udtValueName, cqlName);\r\n```\r\nAlso this more refactoring-friendly (although `UdtValue` is unlikely to get renamed at this stage).', 'commenter': 'olim7t'}]"
1192,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoGetEntityMethodGenerator.java,"@@ -0,0 +1,145 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.mapper.processor.dao;
+
+import com.datastax.oss.driver.api.core.AsyncPagingIterable;
+import com.datastax.oss.driver.api.core.PagingIterable;
+import com.datastax.oss.driver.api.core.cql.AsyncResultSet;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import com.datastax.oss.driver.api.mapper.annotations.Entity;
+import com.datastax.oss.driver.api.mapper.annotations.GetEntity;
+import com.datastax.oss.driver.internal.mapper.processor.MethodGenerator;
+import com.datastax.oss.driver.internal.mapper.processor.ProcessorContext;
+import com.datastax.oss.driver.internal.mapper.processor.SkipGenerationException;
+import com.squareup.javapoet.ClassName;
+import com.squareup.javapoet.MethodSpec;
+import java.util.List;
+import javax.lang.model.element.Element;
+import javax.lang.model.element.ElementKind;
+import javax.lang.model.element.ExecutableElement;
+import javax.lang.model.element.Modifier;
+import javax.lang.model.element.TypeElement;
+import javax.lang.model.element.VariableElement;
+import javax.lang.model.type.DeclaredType;
+import javax.lang.model.type.TypeKind;
+import javax.lang.model.type.TypeMirror;
+
+public class DaoGetEntityMethodGenerator implements MethodGenerator {
+
+  private final ExecutableElement methodElement;
+  private final DaoImplementationGenerator daoImplementationGenerator;
+  private final String targetParameterName;
+  private final TypeElement entityElement;
+  private boolean pagingReturnType;
+
+  public DaoGetEntityMethodGenerator(
+      ExecutableElement methodElement,
+      DaoImplementationGenerator daoImplementationGenerator,
+      ProcessorContext context) {
+    this.methodElement = methodElement;
+    this.daoImplementationGenerator = daoImplementationGenerator;
+    // Methods should only have one parameter
+    if (methodElement.getParameters().size() != 1) {
+      context
+          .getMessager()
+          .error(
+              methodElement, ""%s method must have one parameter"", GetEntity.class.getSimpleName());
+      throw new SkipGenerationException();
+    }
+    // Parameter name as a string
+    String tmpParameterString = null;
+    // The return type as an element
+    TypeElement returnTypeElement = null;
+    VariableElement parameterElement = methodElement.getParameters().get(0);
+    TypeMirror parameterType = parameterElement.asType();
+    // Check the parameter type make sure it matches an expected type
+    if (context.getClassUtils().implementsGettableByName(parameterType)
+        || context.getClassUtils().isSame(parameterType, ResultSet.class)
+        || context.getClassUtils().isSame(parameterType, AsyncResultSet.class)) {
+      tmpParameterString = parameterElement.getSimpleName().toString();
+    }","[{'comment': ""That's the only place where we set this variable, so we can handle the error case directly here, and we don't need a temporary variable."", 'commenter': 'olim7t'}]"
1192,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoGetEntityMethodGenerator.java,"@@ -0,0 +1,145 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.mapper.processor.dao;
+
+import com.datastax.oss.driver.api.core.AsyncPagingIterable;
+import com.datastax.oss.driver.api.core.PagingIterable;
+import com.datastax.oss.driver.api.core.cql.AsyncResultSet;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import com.datastax.oss.driver.api.mapper.annotations.Entity;
+import com.datastax.oss.driver.api.mapper.annotations.GetEntity;
+import com.datastax.oss.driver.internal.mapper.processor.MethodGenerator;
+import com.datastax.oss.driver.internal.mapper.processor.ProcessorContext;
+import com.datastax.oss.driver.internal.mapper.processor.SkipGenerationException;
+import com.squareup.javapoet.ClassName;
+import com.squareup.javapoet.MethodSpec;
+import java.util.List;
+import javax.lang.model.element.Element;
+import javax.lang.model.element.ElementKind;
+import javax.lang.model.element.ExecutableElement;
+import javax.lang.model.element.Modifier;
+import javax.lang.model.element.TypeElement;
+import javax.lang.model.element.VariableElement;
+import javax.lang.model.type.DeclaredType;
+import javax.lang.model.type.TypeKind;
+import javax.lang.model.type.TypeMirror;
+
+public class DaoGetEntityMethodGenerator implements MethodGenerator {
+
+  private final ExecutableElement methodElement;
+  private final DaoImplementationGenerator daoImplementationGenerator;
+  private final String targetParameterName;
+  private final TypeElement entityElement;
+  private boolean pagingReturnType;
+
+  public DaoGetEntityMethodGenerator(
+      ExecutableElement methodElement,
+      DaoImplementationGenerator daoImplementationGenerator,
+      ProcessorContext context) {
+    this.methodElement = methodElement;
+    this.daoImplementationGenerator = daoImplementationGenerator;
+    // Methods should only have one parameter
+    if (methodElement.getParameters().size() != 1) {
+      context
+          .getMessager()
+          .error(
+              methodElement, ""%s method must have one parameter"", GetEntity.class.getSimpleName());
+      throw new SkipGenerationException();
+    }
+    // Parameter name as a string
+    String tmpParameterString = null;
+    // The return type as an element
+    TypeElement returnTypeElement = null;
+    VariableElement parameterElement = methodElement.getParameters().get(0);
+    TypeMirror parameterType = parameterElement.asType();
+    // Check the parameter type make sure it matches an expected type
+    if (context.getClassUtils().implementsGettableByName(parameterType)
+        || context.getClassUtils().isSame(parameterType, ResultSet.class)
+        || context.getClassUtils().isSame(parameterType, AsyncResultSet.class)) {
+      tmpParameterString = parameterElement.getSimpleName().toString();
+    }
+    // Check return type. Make sure it matches the expected type
+    TypeMirror returnType = methodElement.getReturnType();
+    if (returnType.getKind() == TypeKind.DECLARED) {
+      Element element = ((DeclaredType) returnType).asElement();
+      // Simple case return type is an entity type
+      if (element.getKind() == ElementKind.CLASS) {
+        if (element.getAnnotation(Entity.class) != null) {
+          returnTypeElement = ((TypeElement) ((DeclaredType) returnType).asElement());
+        }
+        // Return type is a an iterable type, check nested generic as well
+      } else if (element.getKind() == ElementKind.INTERFACE) {
+        List<? extends TypeMirror> generics = ((DeclaredType) returnType).getTypeArguments();
+        if (generics.size() == 1) {
+          TypeMirror generic = generics.get(0);
+          Element genericElement = ((DeclaredType) generic).asElement();
+          if (genericElement.getAnnotation(Entity.class) != null) {
+            if (element.getSimpleName().toString().equals(PagingIterable.class.getSimpleName())
+                || element
+                    .getSimpleName()
+                    .toString()
+                    .equals(AsyncPagingIterable.class.getSimpleName())) {
+              returnTypeElement = ((TypeElement) ((DeclaredType) generic).asElement());
+              pagingReturnType = true;
+            }
+          }
+        }
+      }
+    }
+    if (returnTypeElement == null) {
+      context
+          .getMessager()
+          .error(
+              methodElement,
+              ""Invalid return type specified. Expected annotated entity, PagingIterable, or AsyncPagingIterable "");
+      throw new SkipGenerationException();
+    }
+    if (tmpParameterString == null) {
+      context
+          .getMessager()
+          .error(
+              methodElement,
+              ""Could not match parameter, expected a GettableByName, ResultSet or AsyncResultSet"");
+      throw new SkipGenerationException();
+    }
+    this.entityElement = returnTypeElement;
+    this.targetParameterName = tmpParameterString;
+  }
+
+  @Override
+  public MethodSpec.Builder generate() {
+    String helperFieldName = daoImplementationGenerator.addEntityHelperField(entityElement);
+
+    MethodSpec.Builder overridingMethodBuilder =
+        MethodSpec.methodBuilder(methodElement.getSimpleName().toString())
+            .addAnnotation(Override.class)
+            .addModifiers(Modifier.PUBLIC)
+            .returns(ClassName.get(methodElement.getReturnType()));
+    for (VariableElement parameterElement : methodElement.getParameters()) {
+      overridingMethodBuilder.addParameter(
+          ClassName.get(parameterElement.asType()), parameterElement.getSimpleName().toString());
+    }
+    if (!pagingReturnType) {
+
+      overridingMethodBuilder.addStatement(
+          ""return $L.get($L)"", helperFieldName, targetParameterName);
+    } else {
+      overridingMethodBuilder.addStatement(
+          ""return $L.map($L::get)"", targetParameterName, helperFieldName);
+    }","[{'comment': 'I think it would be nice to handle one additional case: converting a single-row result set to an entity instance.\r\n```java\r\nProduct get(ResultSet|AsyncResultSet rs);\r\n```', 'commenter': 'olim7t'}]"
1192,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoGetEntityMethodGenerator.java,"@@ -0,0 +1,145 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.mapper.processor.dao;
+
+import com.datastax.oss.driver.api.core.AsyncPagingIterable;
+import com.datastax.oss.driver.api.core.PagingIterable;
+import com.datastax.oss.driver.api.core.cql.AsyncResultSet;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import com.datastax.oss.driver.api.mapper.annotations.Entity;
+import com.datastax.oss.driver.api.mapper.annotations.GetEntity;
+import com.datastax.oss.driver.internal.mapper.processor.MethodGenerator;
+import com.datastax.oss.driver.internal.mapper.processor.ProcessorContext;
+import com.datastax.oss.driver.internal.mapper.processor.SkipGenerationException;
+import com.squareup.javapoet.ClassName;
+import com.squareup.javapoet.MethodSpec;
+import java.util.List;
+import javax.lang.model.element.Element;
+import javax.lang.model.element.ElementKind;
+import javax.lang.model.element.ExecutableElement;
+import javax.lang.model.element.Modifier;
+import javax.lang.model.element.TypeElement;
+import javax.lang.model.element.VariableElement;
+import javax.lang.model.type.DeclaredType;
+import javax.lang.model.type.TypeKind;
+import javax.lang.model.type.TypeMirror;
+
+public class DaoGetEntityMethodGenerator implements MethodGenerator {
+
+  private final ExecutableElement methodElement;
+  private final DaoImplementationGenerator daoImplementationGenerator;
+  private final String targetParameterName;
+  private final TypeElement entityElement;
+  private boolean pagingReturnType;
+
+  public DaoGetEntityMethodGenerator(
+      ExecutableElement methodElement,
+      DaoImplementationGenerator daoImplementationGenerator,
+      ProcessorContext context) {
+    this.methodElement = methodElement;
+    this.daoImplementationGenerator = daoImplementationGenerator;
+    // Methods should only have one parameter
+    if (methodElement.getParameters().size() != 1) {
+      context
+          .getMessager()
+          .error(
+              methodElement, ""%s method must have one parameter"", GetEntity.class.getSimpleName());
+      throw new SkipGenerationException();
+    }
+    // Parameter name as a string
+    String tmpParameterString = null;
+    // The return type as an element
+    TypeElement returnTypeElement = null;
+    VariableElement parameterElement = methodElement.getParameters().get(0);
+    TypeMirror parameterType = parameterElement.asType();
+    // Check the parameter type make sure it matches an expected type
+    if (context.getClassUtils().implementsGettableByName(parameterType)
+        || context.getClassUtils().isSame(parameterType, ResultSet.class)
+        || context.getClassUtils().isSame(parameterType, AsyncResultSet.class)) {
+      tmpParameterString = parameterElement.getSimpleName().toString();
+    }
+    // Check return type. Make sure it matches the expected type","[{'comment': 'Additionally, we should check that the return type matches the argument, because not all combinations are valid:\r\n```java\r\nGettableByName => Entity\r\nResultSet => PagingIterable or Entity\r\nAsyncResultSet => AsyncPagingIterable or Entity\r\n```', 'commenter': 'olim7t'}]"
1196,integration-tests/src/test/java/com/datastax/oss/driver/api/core/metadata/TableOptionsIT.java,"@@ -0,0 +1,93 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core.metadata;
+
+import static org.assertj.core.api.Assertions.assertThat;
+
+import com.datastax.oss.driver.api.core.CqlIdentifier;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import com.datastax.oss.driver.api.core.metadata.schema.KeyspaceMetadata;
+import com.datastax.oss.driver.api.core.metadata.schema.TableMetadata;
+import com.datastax.oss.driver.api.testinfra.CassandraRequirement;
+import com.datastax.oss.driver.api.testinfra.ccm.CcmRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionUtils;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import java.time.Duration;
+import java.util.Map;
+import java.util.Optional;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.junit.rules.RuleChain;
+import org.junit.rules.TestRule;
+
+@Category(ParallelizableTests.class)
+public class TableOptionsIT {
+
+  // public static CustomCcmRule ccmRule = CustomCcmRule.builder().build();","[{'comment': 'Forgot to remove this after testing with it. ', 'commenter': 'emerkle826'}]"
1196,changelog/README.md,"@@ -6,6 +6,7 @@
 
 ### 4.0.0-rc1
 
+- [improvement] JAVA-2090: Add support for additional_write_policy and read_repair table options","[{'comment': 'Just noticed that the changelog entry is misplaced.', 'commenter': 'adutra'}]"
1196,core/src/main/java/com/datastax/oss/driver/internal/core/metadata/schema/parsing/RelationParser.java,"@@ -138,5 +138,7 @@ public static void appendOptions(Map<CqlIdentifier, Object> options, ScriptBuild
           .put(""min_index_interval"", TypeCodecs.INT)
           .put(""read_repair_chance"", TypeCodecs.DOUBLE)
           .put(""speculative_retry"", TypeCodecs.TEXT)
+          .put(""read_repair"", TypeCodecs.TEXT)
+          .put(""additional_write_policy"", TypeCodecs.TEXT)","[{'comment': ""Nit: I moved the lines to respect alphabetical order. That way the output of `TableMetadata.describe()` will looks the same as CQLSH's `desc`."", 'commenter': 'olim7t'}]"
1196,integration-tests/src/test/java/com/datastax/oss/driver/api/core/metadata/TableOptionsIT.java,"@@ -0,0 +1,92 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core.metadata;
+
+import static org.assertj.core.api.Assertions.assertThat;
+
+import com.datastax.oss.driver.api.core.CqlIdentifier;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import com.datastax.oss.driver.api.core.metadata.schema.KeyspaceMetadata;
+import com.datastax.oss.driver.api.core.metadata.schema.TableMetadata;
+import com.datastax.oss.driver.api.testinfra.CassandraRequirement;
+import com.datastax.oss.driver.api.testinfra.ccm.CcmRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionUtils;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import java.time.Duration;
+import java.util.Map;
+import java.util.Optional;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.junit.rules.RuleChain;
+import org.junit.rules.TestRule;
+
+@Category(ParallelizableTests.class)
+public class TableOptionsIT {
+
+  public static CcmRule ccmRule = CcmRule.getInstance();
+  // disable debouncer to speed up test.
+  private static SessionRule<CqlSession> sessionRule =
+      SessionRule.builder(ccmRule)
+          .withKeyspace(false)
+          .withConfigLoader(
+              SessionUtils.configLoaderBuilder()
+                  .withDuration(DefaultDriverOption.REQUEST_TIMEOUT, Duration.ofSeconds(30))
+                  .withDuration(DefaultDriverOption.METADATA_SCHEMA_WINDOW, Duration.ofSeconds(0))
+                  .build())
+          .build();
+
+  @Rule public TestRule chain = RuleChain.outerRule(ccmRule).around(sessionRule);
+
+  @Test
+  @CassandraRequirement(min = ""4.0"", description = ""required to ensure table options are available"")
+  public void ensure_table_options() {
+    CqlIdentifier keyspace = SessionUtils.uniqueKeyspaceId();
+    CqlIdentifier readRepairKey = CqlIdentifier.fromCql(""read_repair"");
+    CqlIdentifier additionalWritePolicyKey = CqlIdentifier.fromCql(""additional_write_policy"");
+    CqlSession session = sessionRule.session();
+    String createKeyspace =
+        String.format(
+            ""CREATE KEYSPACE %s WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1}"",
+            keyspace);
+    // create keyspace
+    session.execute(createKeyspace);
+    String useKeyspace = String.format(""USE %s"", keyspace.asCql(false));
+    // connect session to this keyspace.
+    session.execute(useKeyspace);
+
+    Optional<? extends KeyspaceMetadata> originalKsMeta =
+        session.getMetadata().getKeyspace(keyspace);
+
+    // A simple table with read_repair and additional_write_policy options
+    ResultSet rs =
+        session.execute(
+            ""CREATE TABLE foo(k int, a text, PRIMARY KEY(k)) WITH read_repair='NONE' AND additional_write_policy='40p'"");
+    assertThat(rs.wasApplied()).isTrue();","[{'comment': '`wasApplied()` is for lightweight transactions only, it will always be true for a create statement.', 'commenter': 'olim7t'}]"
1196,integration-tests/src/test/java/com/datastax/oss/driver/api/core/metadata/TableOptionsIT.java,"@@ -0,0 +1,92 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core.metadata;
+
+import static org.assertj.core.api.Assertions.assertThat;
+
+import com.datastax.oss.driver.api.core.CqlIdentifier;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import com.datastax.oss.driver.api.core.metadata.schema.KeyspaceMetadata;
+import com.datastax.oss.driver.api.core.metadata.schema.TableMetadata;
+import com.datastax.oss.driver.api.testinfra.CassandraRequirement;
+import com.datastax.oss.driver.api.testinfra.ccm.CcmRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionUtils;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import java.time.Duration;
+import java.util.Map;
+import java.util.Optional;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.junit.rules.RuleChain;
+import org.junit.rules.TestRule;
+
+@Category(ParallelizableTests.class)
+public class TableOptionsIT {
+
+  public static CcmRule ccmRule = CcmRule.getInstance();
+  // disable debouncer to speed up test.
+  private static SessionRule<CqlSession> sessionRule =
+      SessionRule.builder(ccmRule)
+          .withKeyspace(false)","[{'comment': ""Why not let the rule create a keyspace? That way you don't need to do it in your test method."", 'commenter': 'olim7t'}]"
1199,core/src/main/java/com/datastax/oss/driver/internal/core/os/Native.java,"@@ -75,6 +77,32 @@ public static int getProcessId() {
     return PosixLoader.POSIX.getpid();
   }
 
+  /**
+   * Returns {@code true} if JNR {@link Platform} class is loaded, and {@code false} otherwise.
+   *
+   * @return {@code true} if JNR {@link Platform} class is loaded.
+   */
+  public static boolean isPlatformAvailable() {
+    try {
+      return PlatformLoader.PLATFORM != null;
+    } catch (NoClassDefFoundError e) {
+      return false;
+    }
+  }
+
+  /**
+   * Returns the current processor architecture the JVM is running on, as reported by {@link
+   * Platform#getCPU()}.
+   *
+   * @return the current processor architecture.
+   * @throws UnsupportedOperationException if JNR Platform library is not loaded.
+   */
+  public static String getCPU() {
+    if (!isPlatformAvailable())
+      throw new UnsupportedOperationException(""JNR Platform class not loaded"");","[{'comment': ""I see that similar methods in this class use `IllegalStateException`, shouldn't we do the same here?"", 'commenter': 'adutra'}, {'comment': 'done', 'commenter': 'tomekl007'}]"
1199,core/src/main/java/com/datastax/oss/driver/internal/core/os/Native.java,"@@ -150,4 +178,27 @@ private Timeval(Runtime runtime) {
       GET_PID_AVAILABLE = getPidAvailable;
     }
   }
+
+  private static class PlatformLoader {
+
+    private static final Platform PLATFORM;
+
+    static {
+      Platform platform;
+      try {
+        Class<?> platformClass = Class.forName(""jnr.ffi.Platform"");
+        Method getNativePlatform = platformClass.getMethod(""getNativePlatform"");
+        platform = (Platform) getNativePlatform.invoke(null);
+      } catch (Throwable t) {
+        platform = null;
+        if (LOG.isDebugEnabled())","[{'comment': 'I see that contrary to 3.x, in 4.x we only log at DEBUG level, again I think we should harmonize the code.', 'commenter': 'adutra'}, {'comment': 'done', 'commenter': 'tomekl007'}]"
1199,core/src/main/java/com/datastax/oss/driver/internal/core/os/Native.java,"@@ -75,6 +77,34 @@ public static int getProcessId() {
     return PosixLoader.POSIX.getpid();
   }
 
+  /**
+   * Returns {@code true} if JNR {@link Platform} class is loaded, and {@code false} otherwise.
+   *
+   * @return {@code true} if JNR {@link Platform} class is loaded.
+   */
+  public static boolean isPlatformAvailable() {
+    try {
+      return PlatformLoader.PLATFORM != null;
+    } catch (NoClassDefFoundError e) {
+      return false;
+    }
+  }
+
+  /**
+   * Returns the current processor architecture the JVM is running on, as reported by {@link
+   * Platform#getCPU()}.
+   *
+   * @return the current processor architecture.
+   * @throws UnsupportedOperationException if JNR Platform library is not loaded.
+   */
+  public static String getCPU() {
+    if (!isPlatformAvailable())
+      throw new IllegalStateException(
+          ""JNR Platform class not loaded. ""
+              + ""Check isGetProcessIdAvailable() before calling this method."");","[{'comment': 'Should be: ""Check isPlatformAvailable()"".', 'commenter': 'adutra'}, {'comment': 'done, sorry for that copy/pasting', 'commenter': 'tomekl007'}]"
1199,core/src/main/java/com/datastax/oss/driver/internal/core/os/Native.java,"@@ -75,6 +77,34 @@ public static int getProcessId() {
     return PosixLoader.POSIX.getpid();
   }
 
+  /**
+   * Returns {@code true} if JNR {@link Platform} class is loaded, and {@code false} otherwise.
+   *
+   * @return {@code true} if JNR {@link Platform} class is loaded.
+   */
+  public static boolean isPlatformAvailable() {
+    try {
+      return PlatformLoader.PLATFORM != null;
+    } catch (NoClassDefFoundError e) {
+      return false;
+    }
+  }
+
+  /**
+   * Returns the current processor architecture the JVM is running on, as reported by {@link
+   * Platform#getCPU()}.
+   *
+   * @return the current processor architecture.
+   * @throws UnsupportedOperationException if JNR Platform library is not loaded.","[{'comment': 'Should be `IllegalStateException`', 'commenter': 'adutra'}]"
1201,integration-tests/src/test/java/com/datastax/oss/driver/api/core/cql/ExecutionInfoWarningsIT.java,"@@ -169,24 +165,36 @@ public void should_expose_warnings_on_execution_info() {
                 + ""APPLY BATCH"",
             Strings.repeat(""1"", 2 * 1024), Strings.repeat(""1"", 3 * 1024));
     Statement<?> st = SimpleStatement.builder(String.format(query)).build();
-    ResultSet result = SESSION_RULE.session().execute(st);
+    ResultSet result = sessionRule.session().execute(st);
     ExecutionInfo executionInfo = result.getExecutionInfo();
     assertThat(executionInfo).isNotNull();
     List<String> warnings = executionInfo.getWarnings();
     assertThat(warnings).isNotEmpty();
     // verify the log was generated
-    verify(appender, after(500).times(1)).doAppend(loggingEventCaptor.capture());
+    verify(appender, after(500).atLeast(1)).doAppend(loggingEventCaptor.capture());","[{'comment': 'what is a reason why it works in at least once way?', 'commenter': 'tomekl007'}, {'comment': 'I think it is explained in the comment above: ""the number of warnings and the specific wording varies with different Cassandra versions"".', 'commenter': 'adutra'}, {'comment': 'Yes, that is correct. For Cassandra 2.2 and 3.0, this query generates 2 warnings, both of which are logged. In 3.11, only one warning is generated.', 'commenter': 'emerkle826'}]"
1201,integration-tests/src/test/java/com/datastax/oss/driver/api/core/cql/ExecutionInfoWarningsIT.java,"@@ -169,24 +165,36 @@ public void should_expose_warnings_on_execution_info() {
                 + ""APPLY BATCH"",
             Strings.repeat(""1"", 2 * 1024), Strings.repeat(""1"", 3 * 1024));
     Statement<?> st = SimpleStatement.builder(String.format(query)).build();
-    ResultSet result = SESSION_RULE.session().execute(st);
+    ResultSet result = sessionRule.session().execute(st);
     ExecutionInfo executionInfo = result.getExecutionInfo();
     assertThat(executionInfo).isNotNull();
     List<String> warnings = executionInfo.getWarnings();
     assertThat(warnings).isNotEmpty();
     // verify the log was generated
-    verify(appender, after(500).times(1)).doAppend(loggingEventCaptor.capture());
+    verify(appender, after(500).atLeast(1)).doAppend(loggingEventCaptor.capture());
     assertThat(loggingEventCaptor.getValue().getMessage()).isNotNull();
-    String logMessage = loggingEventCaptor.getValue().getFormattedMessage();
-    assertThat(logMessage)
-        .startsWith(""Query '"")
-        // query will only be logged up to MAX_QUERY_LENGTH
-        // characters
-        .contains(query.substring(0, RequestLogger.DEFAULT_REQUEST_LOGGER_MAX_QUERY_LENGTH))
-        .contains(""' generated server side warning(s): "")
-        .contains(
-            String.format(
-                ""Batch for [%s.test] is of size 5152, exceeding specified threshold of 5120 by 32."",
-                SESSION_RULE.keyspace().asCql(true)));
+    boolean testPassed = false;
+    for (ILoggingEvent log : loggingEventCaptor.getAllValues()) {
+      // some versions of Cassandra return multiple warnings for this query. We're only verifying
+      // the common pieces of the warnings.
+      String logMessage = log.getFormattedMessage();
+      // all logs should start with the driver log prefix
+      assertThat(logMessage)
+          .startsWith(""Query '"")
+          // query will only be logged up to MAX_QUERY_LENGTH characters
+          .contains(query.substring(0, RequestLogger.DEFAULT_REQUEST_LOGGER_MAX_QUERY_LENGTH))
+          .contains(""' generated server side warning(s): "");
+      // we're now only looking for:
+      // ""Batch ... for [KS.test] is of size .... exceeding specified threshold""
+      if (logMessage.contains(""Batch"")) {","[{'comment': 'Why not create an assertion for that? E.g.:\r\n\r\n```\r\nassertThat(logMessage)\r\n    .contains(""Batch"")\r\n    .contains(String.format(""for [%s.test] is of size"", sessionRule.keyspace().asCql(true)))\r\n    .contains(""exceeding specified threshold"");\r\n```\r\nBesides, you are affecting true to `testPassed` inside a loop, which means that as long as the variable is set to true once, the whole test will pass, which imo is not what we want. Iow I think it would be better to get rid of the `testPassed`  variable.', 'commenter': 'adutra'}, {'comment': 'The reason for the loop and the flag is related to the multiple logs issue for certain versions of Cassandra. For versions 2.2 and 3.0, when 2 logs are generated, one of the logs does not contain any of the above assertions. And I\'m not sure I am guaranteed the same order in the log capture. So I loop through the logs here, and when I come across one that has ""Batch"", I assert the rest of the message, set a flag and break out of the loop on line 195 below.\r\n\r\nIn the case where it\'s an older version of Cassandra, and I get two logs, this skips over the other log that doesn\'t appear in 3.11, either because it it doesn\'t contain ""Batch"", or the loop was broken out of on the log that did.', 'commenter': 'emerkle826'}, {'comment': 'I see. Would the following work?\r\n\r\n```\r\nList<String> logMessages =\r\n    loggingEventCaptor.getAllValues().stream()\r\n        .map(ILoggingEvent::getFormattedMessage)\r\n        .collect(Collectors.toList());\r\nassertThat(logMessages)\r\n    .hasOnlyOneElementSatisfying(\r\n        logMessage -> {\r\n          assertThat(logMessage)\r\n              .contains(""Batch"")\r\n              .contains(\r\n                  String.format(""for [%s.test] is of size"", sessionRule.keyspace().asCql(true)))\r\n              .contains(""exceeding specified threshold"");\r\n        });\r\n```', 'commenter': 'adutra'}, {'comment': 'I can change it up to do this.', 'commenter': 'emerkle826'}, {'comment': ""So I tried implementing this, but it still fails as `hasOnlyOneElementSatisfying` will fail the assertion if the List of logMessages has more than one element. I thought that method would verify that the list only contained one element that passes the assertion, but it in fact asserts that there is only one element AND that it passes the provided assertion.\r\nI'm not seeing an easy way to assert that, no matter how many logs are in the Captor, there is only one that passes this specific set of assertions, without looping through the logs."", 'commenter': 'emerkle826'}]"
1204,driver-core/src/main/java/com/datastax/driver/core/Metadata.java,"@@ -241,11 +241,15 @@ public static String quoteIfNecessary(String id) {
   /**
    * We don't need to escape an identifier if it matches non-quoted CQL3 ids ([a-z][a-z0-9_]*), and
    * if it's not a CQL reserved keyword.
+   *
+   * <p>When 'Migrating from compact storage' after DROP COMPACT STORAGE on the table, it can have a","[{'comment': 'I would mention the jira here for future reference.', 'commenter': 'adutra'}]"
1204,driver-core/src/main/java/com/datastax/driver/core/Metadata.java,"@@ -241,11 +241,15 @@ public static String quoteIfNecessary(String id) {
   /**
    * We don't need to escape an identifier if it matches non-quoted CQL3 ids ([a-z][a-z0-9_]*), and
    * if it's not a CQL reserved keyword.
+   *
+   * <p>When 'Migrating from compact storage' after DROP COMPACT STORAGE on the table, it can have a
+   * column with an empty name. For that case, we need to escape empty column name.
    */
   private static boolean needsQuote(String s) {
     // this method should only be called for C*-provided identifiers,
     // so we expect it to be non-null and non-empty.","[{'comment': ""The comment needs to be changed, it's not true anymore."", 'commenter': 'adutra'}, {'comment': 'Nit: we can remove the comment here about it being expected to be ""non-empty"", as you have it covered in the javadoc comment above (and it\'s no longer true)', 'commenter': 'emerkle826'}, {'comment': 'adutra beat me to it!', 'commenter': 'emerkle826'}]"
1204,driver-core/src/main/java/com/datastax/driver/core/Metadata.java,"@@ -241,11 +241,16 @@ public static String quoteIfNecessary(String id) {
   /**
    * We don't need to escape an identifier if it matches non-quoted CQL3 ids ([a-z][a-z0-9_]*), and
    * if it's not a CQL reserved keyword.
+   *
+   * <p>When 'Migrating from compact storage' after DROP COMPACT STORAGE on the table, it can have a
+   * column with an empty name. (See @jira_ticket JAVA-2174 for the reference) For that case, we","[{'comment': ""Sorry to be nit-picky but please remove the `@jira_ticket` tag, this is an internal tag that is only recognized in tests by some of our CI tools (I don't even remember which one now). In any case this isn't a valid javadoc tag so it shouldn't appear in production code."", 'commenter': 'adutra'}, {'comment': 'thx, I was also not sure that It can be in production code. Changed', 'commenter': 'tomekl007'}]"
1204,changelog/README.md,"@@ -18,6 +18,7 @@
 - [improvement] JAVA-1950: Log server side warnings returned from a query.
 - [improvement] JAVA-2123: Allow to use QueryBuilder for building queries against Materialized Views.
 - [bug] JAVA-2082: Avoid race condition during cluster close and schema refresh.
+- [bug] JAVA-2174: quote empty identifier","[{'comment': 'Also the correct changelog entry is `Metadata.needsQuote should accept empty strings.` (with a final period).', 'commenter': 'adutra'}]"
1206,changelog/README.md,"@@ -7,6 +7,7 @@
 - [improvement] JAVA-2158: Allow BuildableQuery to build statement with values
 - [improvement] JAVA-2150: Improve query builder error message on unsupported literal type
 - [documentation] JAVA-2149: Improve Term javadocs in the query builder
+- [bug] Java 2178: QueryBuilder: Alias after function column is not included in a query","[{'comment': 'In driver 4 you should add new entries at the top of the list, not the bottom.', 'commenter': 'adutra'}, {'comment': 'ok, thx', 'commenter': 'tomekl007'}]"
1207,query-builder/src/main/java/com/datastax/oss/driver/api/querybuilder/insert/InsertInto.java,"@@ -31,4 +37,29 @@
   /** Makes this statement an INSERT JSON with a bind marker, as in {@code INSERT JSON ?}. */
   @NonNull
   JsonInsert json(@NonNull BindMarker bindMarker);
+
+  /**
+   * Makes this statement an INSERT JSON with a custom type mapping, as in {@code INSERT JSON ?}.","[{'comment': 'The description is not accurate: `as in {@code INSERT JSON ?}` does not apply here.', 'commenter': 'adutra'}, {'comment': 'It is true. The json insert when using `json(String)` is:\r\n```\r\nINSERT INTO json_jackson_row JSON \'{ ""id"": 2, ""name"": ""Alice"", ""age"": 3 }\'\r\n```\r\nWhen using variants with codec:\r\n```\r\nINSERT INTO json_jackson_row JSON \'{""id"":2,""name"":""Alice"",""age"":3}\'\r\n```\r\nSo in both cases the `INSERT JSON` is used\r\n\r\n', 'commenter': 'tomekl007'}, {'comment': 'But there is no bind marker here, so why are the javadocs mentioning `{@code INSERT JSON ?}`? (Note the question mark.)', 'commenter': 'adutra'}, {'comment': ""Oh, right. Sorry for missing that. I've updated the doc."", 'commenter': 'tomekl007'}]"
1207,query-builder/src/main/java/com/datastax/oss/driver/api/querybuilder/insert/InsertInto.java,"@@ -31,4 +37,29 @@
   /** Makes this statement an INSERT JSON with a bind marker, as in {@code INSERT JSON ?}. */
   @NonNull
   JsonInsert json(@NonNull BindMarker bindMarker);
+
+  /**
+   * Makes this statement an INSERT JSON with a custom type mapping, as in {@code INSERT JSON ?}.
+   *
+   * <p>This is an alternative to {@link #json(String)} for custom type mappings. The provided
+   * registry should contain a codec that can format the value. Typically, this will be your
+   * session's registry, which is accessible via {@code session.getContext().getCodecRegistry()}.
+   *
+   * @throws CodecNotFoundException if {@code codecRegistry} does not contain any codec that can
+   *     handle {@code value}.
+   * @see DriverContext#getCodecRegistry()
+   * @see DefaultInsert#json(Object, CodecRegistry)
+   */
+  @NonNull
+  JsonInsert json(@Nullable Object value, @NonNull CodecRegistry codecRegistry);
+
+  /**
+   * Makes this statement an INSERT JSON with a custom type mapping, as in {@code INSERT JSON ?}.","[{'comment': 'Same thing here.', 'commenter': 'adutra'}]"
1207,query-builder/src/main/java/com/datastax/oss/driver/api/querybuilder/insert/InsertInto.java,"@@ -31,4 +37,29 @@
   /** Makes this statement an INSERT JSON with a bind marker, as in {@code INSERT JSON ?}. */
   @NonNull
   JsonInsert json(@NonNull BindMarker bindMarker);
+
+  /**
+   * Makes this statement an INSERT JSON with a custom type mapping, as in {@code INSERT JSON ?}.
+   *
+   * <p>This is an alternative to {@link #json(String)} for custom type mappings. The provided
+   * registry should contain a codec that can format the value. Typically, this will be your
+   * session's registry, which is accessible via {@code session.getContext().getCodecRegistry()}.
+   *
+   * @throws CodecNotFoundException if {@code codecRegistry} does not contain any codec that can
+   *     handle {@code value}.
+   * @see DriverContext#getCodecRegistry()
+   * @see DefaultInsert#json(Object, CodecRegistry)
+   */
+  @NonNull
+  JsonInsert json(@Nullable Object value, @NonNull CodecRegistry codecRegistry);
+
+  /**
+   * Makes this statement an INSERT JSON with a custom type mapping, as in {@code INSERT JSON ?}.
+   * The value will be turned into a string with {@link TypeCodec#format(Object)}, and inlined in
+   * the query.
+   *
+   * @see DefaultInsert#json(T, TypeCodec)
+   */
+  @NonNull
+  <T> JsonInsert json(@Nullable T value, @Nullable TypeCodec<T> codec);","[{'comment': ""I don't think `codec` can be null."", 'commenter': 'adutra'}, {'comment': 'yes, you right. Changed', 'commenter': 'tomekl007'}, {'comment': 'After checking again the `codec` can be null because it proxy that into the `literal`. And `literal` is taking `codec` that is `@Nullable`:\r\n```\r\nliteral(@Nullable T value, @Nullable TypeCodec<T> codec)\r\n```\r\nThis is a case when the `value` that is passed to the `json` is `null`. Then the logic is:\r\n```\r\nvalue, (value == null) ? null : codecRegistry.codecFor(value)\r\n```', 'commenter': 'tomekl007'}]"
1207,query-builder/src/main/java/com/datastax/oss/driver/api/querybuilder/insert/InsertInto.java,"@@ -15,8 +15,15 @@
  */
 package com.datastax.oss.driver.api.querybuilder.insert;
 
+import com.datastax.oss.driver.api.core.context.DriverContext;
+import com.datastax.oss.driver.api.core.type.codec.CodecNotFoundException;
+import com.datastax.oss.driver.api.core.type.codec.TypeCodec;
+import com.datastax.oss.driver.api.core.type.codec.registry.CodecRegistry;
 import com.datastax.oss.driver.api.querybuilder.BindMarker;
+import com.datastax.oss.driver.internal.querybuilder.insert.DefaultInsert;
+import com.sun.istack.internal.NotNull;","[{'comment': 'Wrong annotation, the correct one is `NonNull` (next line).\r\nI make that mistake all the time too ðŸ˜¢ ', 'commenter': 'olim7t'}, {'comment': ""API Plumber could check that. It doesn't support annotations yet, but I've created an issue to add it: datastax/api-plumber-doclet#1"", 'commenter': 'olim7t'}]"
1207,query-builder/src/main/java/com/datastax/oss/driver/api/querybuilder/insert/InsertInto.java,"@@ -31,4 +38,29 @@
   /** Makes this statement an INSERT JSON with a bind marker, as in {@code INSERT JSON ?}. */
   @NonNull
   JsonInsert json(@NonNull BindMarker bindMarker);
+
+  /**
+   * Makes this statement an INSERT JSON with a custom type mapping, as in {@code INSERT JSON ?}.
+   *
+   * <p>This is an alternative to {@link #json(String)} for custom type mappings. The provided
+   * registry should contain a codec that can format the value. Typically, this will be your
+   * session's registry, which is accessible via {@code session.getContext().getCodecRegistry()}.
+   *
+   * @throws CodecNotFoundException if {@code codecRegistry} does not contain any codec that can
+   *     handle {@code value}.
+   * @see DriverContext#getCodecRegistry()
+   * @see DefaultInsert#json(Object, CodecRegistry)
+   */
+  @NonNull
+  JsonInsert json(@Nullable Object value, @NonNull CodecRegistry codecRegistry);","[{'comment': 'I think this method could have a `default` implementation that calls the `TypeCodec` variant.', 'commenter': 'olim7t'}]"
1207,query-builder/src/main/java/com/datastax/oss/driver/internal/querybuilder/insert/DefaultInsert.java,"@@ -95,6 +98,32 @@ public JsonInsert json(@NonNull BindMarker json) {
         keyspace, table, json, missingJsonBehavior, ImmutableMap.of(), timestamp, ifNotExists);
   }
 
+  @NonNull
+  @Override
+  public JsonInsert json(@Nullable Object value, @NonNull CodecRegistry codecRegistry) {
+    return new DefaultInsert(
+        keyspace,
+        table,
+        QueryBuilder.literal(value, (value == null) ? null : codecRegistry.codecFor(value)),","[{'comment': 'We should catch `CodecNotFoundException` and rethrow with a more context-specific message, like we did in [JAVA-2150](https://datastax-oss.atlassian.net/browse/JAVA-2150). See `QueryBuilder.literal(Object, CodecRegistry)`.', 'commenter': 'olim7t'}, {'comment': 'Good idea. But I see one problem here:\r\nhttps://github.com/datastax/java-driver/pull/1194/files#diff-ec647efb880ba814afddcf79eaa95570R402\r\n`@Nullable Object value` can be null but next, we are doing assert `value != null`.\r\nThen there is a possiblity in this line:\r\nhttps://github.com/datastax/java-driver/pull/1194/files#diff-ec647efb880ba814afddcf79eaa95570R412\r\nthat it will cause NPE, isn\'t it?\r\nShouldn\'t the logic be:\r\n```\r\n  throw new IllegalArgumentException(\r\n          String.format(\r\n              ""Could not inline literal of type %s. ""\r\n                  + ""This happens because the driver doesn\'t know how to map it to a CQL type. ""\r\n                  + ""Try passing a TypeCodec or CodecRegistry to literal()."",\r\n              (value == null) ? null : value.getClass().getName()),\r\n          e);`\r\n```\r\nI will make it llike this in the `json()` `DefaultInsert`. Let me know wdyt. I can update logic in a `QueryBuilder` accordingly', 'commenter': 'tomekl007'}, {'comment': ""`null` is a valid literal in some CQL queries, but `INSERT ... JSON null` is invalid. So we can mark the value as `@NonNull`.\r\n(the only case against it was if a codec encoded `null` into something else than the CQL literal `null`, but that's too contrived to be realistic)"", 'commenter': 'olim7t'}]"
1207,query-builder/revapi.json,"@@ -17,6 +17,28 @@
       }
     },
     ""ignore"": [
+      {
+        ""code"": ""java.method.addedToInterface"",
+        ""new"": ""method <T> com.datastax.oss.driver.api.querybuilder.insert.JsonInsert com.datastax.oss.driver.api.querybuilder.insert.InsertInto::json(T, com.datastax.oss.driver.api.core.type.codec.TypeCodec<T>)"",
+        ""package"": ""com.datastax.oss.driver.api.querybuilder.insert"",
+        ""classQualifiedName"": ""com.datastax.oss.driver.api.querybuilder.insert.InsertInto"",
+        ""classSimpleName"": ""InsertInto"",
+        ""methodName"": ""json"",
+        ""newArchive"": ""com.datastax.oss:java-driver-query-builder:jar:4.0.0-rc2-SNAPSHOT"",
+        ""elementKind"": ""method"",
+        ""justification"": ""add missing Inert json() method""","[{'comment': 'Typo: Inert=>Insert\r\nAlso, could you just copy over the changelog/commit message?', 'commenter': 'olim7t'}, {'comment': 'Good idea, done', 'commenter': 'tomekl007'}]"
1207,changelog/README.md,"@@ -4,6 +4,7 @@
 
 ### 4.0.0 (in progress)
 
+- [improvement] JAVA-2182: QueryBuilder is missing json() method that takes object","[{'comment': ""Could you describe what the commit _does_, instead of what's missing/not working before the commit? For example:\r\n> Add insertInto().json() variant that takes an object in QueryBuilder\r\n\r\nI know I'm being picky, and here it makes almost no difference, but for more complex changes I find that this approach produces better messages."", 'commenter': 'olim7t'}]"
1207,integration-tests/src/test/java/com/datastax/oss/driver/api/querybuilder/JsonInsertIT.java,"@@ -0,0 +1,253 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.querybuilder;
+
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.bindMarker;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.insertInto;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.selectFrom;
+import static com.datastax.oss.driver.assertions.Assertions.assertThat;
+import static org.assertj.core.api.Assertions.assertThatThrownBy;
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
+import com.datastax.oss.driver.api.core.cql.PreparedStatement;
+import com.datastax.oss.driver.api.core.cql.Row;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.api.core.cql.Statement;
+import com.datastax.oss.driver.api.core.type.codec.CodecNotFoundException;
+import com.datastax.oss.driver.api.testinfra.ccm.CcmRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionUtils;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import java.time.Duration;
+import java.util.List;
+import java.util.Objects;
+import org.assertj.core.api.Assertions;
+import org.junit.After;
+import org.junit.BeforeClass;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.junit.rules.RuleChain;
+import org.junit.rules.TestRule;
+
+@Category(ParallelizableTests.class)
+public class JsonInsertIT {
+  private static final CcmRule ccmRule = CcmRule.getInstance();
+  private static final JacksonJsonCodec<User> JACKSON_JSON_CODEC =
+      new JacksonJsonCodec<>(User.class);
+
+  private static SessionRule<CqlSession> sessionRule =
+      SessionRule.builder(ccmRule)
+          .withConfigLoader(
+              SessionUtils.configLoaderBuilder()
+                  .withDuration(DefaultDriverOption.REQUEST_TIMEOUT, Duration.ofSeconds(30))","[{'comment': 'Note that you can also use `sessionRule.slowProfile()` on a per-statement basis:\r\n```java\r\n    sessionRule\r\n        .session()\r\n        .execute(\r\n            SimpleStatement.builder(\r\n                    ""CREATE TABLE json_jackson_row(id int PRIMARY KEY, name text, age int)"")\r\n                .withExecutionProfile(sessionRule.slowProfile())\r\n                .build());\r\n```\r\nSo you don\'t necessarily need to override it for the whole session.\r\n\r\nBut this approach works too, both are valid.', 'commenter': 'olim7t'}, {'comment': ""Thx for the suggestion, I've seen that approach in a code base. Here when we are reusing the same session multiple times I think that solution with doing it once is better because needs less boilerplate code in every test case"", 'commenter': 'tomekl007'}]"
1207,integration-tests/src/test/java/com/datastax/oss/driver/api/querybuilder/JsonInsertIT.java,"@@ -0,0 +1,253 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.querybuilder;
+
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.bindMarker;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.insertInto;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.selectFrom;
+import static com.datastax.oss.driver.assertions.Assertions.assertThat;
+import static org.assertj.core.api.Assertions.assertThatThrownBy;
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
+import com.datastax.oss.driver.api.core.cql.PreparedStatement;
+import com.datastax.oss.driver.api.core.cql.Row;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.api.core.cql.Statement;
+import com.datastax.oss.driver.api.core.type.codec.CodecNotFoundException;
+import com.datastax.oss.driver.api.testinfra.ccm.CcmRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionUtils;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import java.time.Duration;
+import java.util.List;
+import java.util.Objects;
+import org.assertj.core.api.Assertions;
+import org.junit.After;
+import org.junit.BeforeClass;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.junit.rules.RuleChain;
+import org.junit.rules.TestRule;
+
+@Category(ParallelizableTests.class)
+public class JsonInsertIT {
+  private static final CcmRule ccmRule = CcmRule.getInstance();
+  private static final JacksonJsonCodec<User> JACKSON_JSON_CODEC =
+      new JacksonJsonCodec<>(User.class);
+
+  private static SessionRule<CqlSession> sessionRule =
+      SessionRule.builder(ccmRule)
+          .withConfigLoader(
+              SessionUtils.configLoaderBuilder()
+                  .withDuration(DefaultDriverOption.REQUEST_TIMEOUT, Duration.ofSeconds(30))
+                  .build())
+          .build();
+
+  @ClassRule public static TestRule chain = RuleChain.outerRule(ccmRule).around(sessionRule);
+
+  @BeforeClass
+  public static void setup() {
+    sessionRule
+        .session()
+        .execute(""CREATE TABLE json_jackson_row(id int PRIMARY KEY, name text, age int)"");
+  }
+
+  @After
+  public void clearTable() {
+    sessionRule.session().execute(""TRUNCATE TABLE json_jackson_row"");
+  }
+
+  @Test
+  public void should_insert_string_as_json_using_simple_statement() {
+    // given a simple statement
+    try (CqlSession session = sessionWithCustomCodec()) {
+      String jsonUser = ""{ \""id\"": 2, \""name\"": \""Alice\"", \""age\"": 3 }"";
+      Statement stmt = insertInto(""json_jackson_row"").json(jsonUser).build();
+
+      // when
+      session.execute(stmt);
+
+      // then
+      String jsonUserResult =
+          session
+              .execute(selectFrom(""json_jackson_row"").json().all().build())
+              .all()
+              .get(0)
+              .getString(0);
+
+      assertThat(jsonUserResult).contains(""\""id\"": 2"");
+      assertThat(jsonUserResult).contains("" \""name\"": \""Alice\"""");
+      assertThat(jsonUserResult).contains(""\""age\"": 3"");
+    }
+  }
+
+  @Test
+  public void should_insert_json_using_prepare_statement() {
+    // givenÂ prepare statement
+    try (CqlSession session = sessionWithCustomCodec()) {
+      User user = new User(2, ""bob"", 35);
+      PreparedStatement pst =
+          session.prepare(insertInto(""json_jackson_row"").json(bindMarker(""user"")).build());
+
+      // when
+      session.execute(pst.bind().set(""user"", user, User.class));
+
+      // then
+      List<Row> rows = session.execute(selectFrom(""json_jackson_row"").json().all().build()).all();
+      Assertions.assertThat(rows.get(0).get(0, User.class)).isEqualTo(user);","[{'comment': 'Nit: remove `Assertions.`, the method is statically imported.\r\n(this happens in a few other places)', 'commenter': 'olim7t'}]"
1207,query-builder/src/main/java/com/datastax/oss/driver/internal/querybuilder/insert/DefaultInsert.java,"@@ -29,6 +31,7 @@
 import com.datastax.oss.driver.internal.querybuilder.ImmutableCollections;
 import com.datastax.oss.driver.shaded.guava.common.base.Preconditions;
 import com.datastax.oss.driver.shaded.guava.common.collect.ImmutableMap;
+import com.sun.istack.internal.NotNull;","[{'comment': 'Wrong annotation here too.', 'commenter': 'olim7t'}]"
1207,integration-tests/src/test/java/com/datastax/oss/driver/api/querybuilder/JsonInsertIT.java,"@@ -0,0 +1,253 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.querybuilder;
+
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.bindMarker;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.insertInto;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.selectFrom;
+import static com.datastax.oss.driver.assertions.Assertions.assertThat;
+import static org.assertj.core.api.Assertions.assertThatThrownBy;
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
+import com.datastax.oss.driver.api.core.cql.PreparedStatement;
+import com.datastax.oss.driver.api.core.cql.Row;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.api.core.cql.Statement;
+import com.datastax.oss.driver.api.core.type.codec.CodecNotFoundException;
+import com.datastax.oss.driver.api.testinfra.ccm.CcmRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionUtils;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import java.time.Duration;
+import java.util.List;
+import java.util.Objects;
+import org.assertj.core.api.Assertions;
+import org.junit.After;
+import org.junit.BeforeClass;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.junit.rules.RuleChain;
+import org.junit.rules.TestRule;
+
+@Category(ParallelizableTests.class)
+public class JsonInsertIT {","[{'comment': 'Great coverage ðŸ‘ \r\n\r\nI wonder if we need an integration test for this: the query builder only does text generation, so any part that requires a session here is not something that was impacted by this ticket. You could do all those checks in a unit test (building a `DefaultCodecRegistry` with your codec).\r\n\r\nIntegration tests take longer to run, so I think we should favor unit tests when we can.', 'commenter': 'olim7t'}, {'comment': ""I've was examining that approach to test it similarly to `JsonInsertTest` but I think that unit test for `json()` will not give us as much assurance as of the `IT`.\r\nA unit test is only testing the generated SQL. For json with codec variant, we still need to create some class (like `User`) and codec that is able to process it. \r\nIf we are doing this the next step is to just write an integration test. When I was implementing that functionality it was very useful to have the `e-2-e` test for that. Also, we have no integration test for `JSON` so I think it will be beneficial to have one. In the `JsonInsertIT` I am using `ParallelizableTests` so it shares the same ccm with other tests - it is reasonably fast (on my mac ~1s for all tests. With already started ccm by other tests it will be faster). \r\nSo I would like to leave this test as the `IT`, wdyt?\r\n"", 'commenter': 'tomekl007'}, {'comment': 'OK, works for me.', 'commenter': 'olim7t'}]"
1207,query-builder/revapi.json,"@@ -17,6 +17,28 @@
       }
     },
     ""ignore"": [
+      {
+        ""code"": ""java.method.addedToInterface"",
+        ""new"": ""method <T> com.datastax.oss.driver.api.querybuilder.insert.JsonInsert com.datastax.oss.driver.api.querybuilder.insert.InsertInto::json(T, com.datastax.oss.driver.api.core.type.codec.TypeCodec<T>)"",
+        ""package"": ""com.datastax.oss.driver.api.querybuilder.insert"",
+        ""classQualifiedName"": ""com.datastax.oss.driver.api.querybuilder.insert.InsertInto"",
+        ""classSimpleName"": ""InsertInto"",
+        ""methodName"": ""json"",
+        ""newArchive"": ""com.datastax.oss:java-driver-query-builder:jar:4.0.0-rc2-SNAPSHOT"",
+        ""elementKind"": ""method"",
+        ""justification"": ""add missing Inert json() method""
+      },
+      {
+        ""code"": ""java.method.addedToInterface"",
+        ""new"": ""method com.datastax.oss.driver.api.querybuilder.insert.JsonInsert com.datastax.oss.driver.api.querybuilder.insert.InsertInto::json(java.lang.Object, com.datastax.oss.driver.api.core.type.codec.registry.CodecRegistry)"",
+        ""package"": ""com.datastax.oss.driver.api.querybuilder.insert"",
+        ""classQualifiedName"": ""com.datastax.oss.driver.api.querybuilder.insert.InsertInto"",
+        ""classSimpleName"": ""InsertInto"",
+        ""methodName"": ""json"",
+        ""newArchive"": ""com.datastax.oss:java-driver-query-builder:jar:4.0.0-rc2-SNAPSHOT"",
+        ""elementKind"": ""method"",
+        ""justification"": ""add missing Inert json() method""
+      }","[{'comment': 'Revapi includes a lot of information in its suggestions, but in practice not every field is necessary. For this error code, the minimal set is `code, new, newArchive, justification`, all the others can be deduced.\r\nFor the sake of keeping this file short and easy to read, I think we should not include them.', 'commenter': 'olim7t'}, {'comment': ""It turns out that the Revapi plugin has an option to include the minimal necessary fields in suggestions. I've enabled it in 6194b1d9b."", 'commenter': 'olim7t'}]"
1207,integration-tests/src/test/java/com/datastax/oss/driver/api/querybuilder/JacksonJsonCodec.java,"@@ -0,0 +1,124 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.querybuilder;
+
+import com.datastax.oss.driver.api.core.ProtocolVersion;
+import com.datastax.oss.driver.api.core.type.DataType;
+import com.datastax.oss.driver.api.core.type.DataTypes;
+import com.datastax.oss.driver.api.core.type.codec.TypeCodec;
+import com.datastax.oss.driver.api.core.type.reflect.GenericType;
+import com.datastax.oss.driver.internal.core.util.Strings;
+import com.datastax.oss.protocol.internal.util.Bytes;
+import com.fasterxml.jackson.core.JsonProcessingException;
+import com.fasterxml.jackson.databind.JavaType;
+import com.fasterxml.jackson.databind.ObjectMapper;
+import com.fasterxml.jackson.databind.type.TypeFactory;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import edu.umd.cs.findbugs.annotations.Nullable;
+import java.io.IOException;
+import java.nio.ByteBuffer;
+
+public class JacksonJsonCodec<T> implements TypeCodec<T> {
+
+  private final ObjectMapper objectMapper;
+  private final GenericType<T> javaType;
+
+  JacksonJsonCodec(Class<T> javaClass) {
+    this(javaClass, new ObjectMapper());
+  }
+
+  private JacksonJsonCodec(Class<T> javaClass, ObjectMapper objectMapper) {
+    this.javaType = GenericType.of(javaClass);
+    this.objectMapper = objectMapper;
+  }
+
+  @NonNull
+  @Override
+  public GenericType<T> getJavaType() {
+    return javaType;
+  }
+
+  @NonNull
+  @Override
+  public DataType getCqlType() {
+    return DataTypes.TEXT;
+  }
+
+  @Nullable
+  @Override
+  public ByteBuffer encode(@Nullable T value, @NonNull ProtocolVersion protocolVersion) {
+    if (value == null) return null;
+    try {
+      return ByteBuffer.wrap(objectMapper.writeValueAsBytes(value));
+    } catch (JsonProcessingException e) {
+      throw new InvalidTypeException(e.getMessage(), e);","[{'comment': ""As I said in #1208 : In driver 4.0 we don't have any specific exception for codecs except `CodecNotFoundException`. When a codec cannot serialize or deserialize something, we use `IllegalArgumentException`. I don't think you need to introduce a special exception in these examples."", 'commenter': 'adutra'}]"
1207,integration-tests/src/test/java/com/datastax/oss/driver/api/querybuilder/JacksonJsonCodec.java,"@@ -98,27 +98,16 @@ public String format(T value) {
   public T parse(String value) {
     if (value == null || value.isEmpty() || value.equalsIgnoreCase(""NULL"")) return null;","[{'comment': 'In driver 4 the contribution guidelines mandate that if/for/while blocks must be enclosed with braces. Same thing for the line below.', 'commenter': 'adutra'}]"
1208,driver-examples/src/main/java/com/datastax/driver/examples/json/jsr/Jsr353JsonRow.java,"@@ -0,0 +1,148 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.driver.examples.json.jsr;
+
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.bindMarker;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.insertInto;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.literal;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.selectFrom;
+
+import com.datastax.driver.examples.json.PlainTextJson;
+import com.datastax.driver.examples.json.codecs.Jsr353JsonCodec;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.CqlSessionBuilder;
+import com.datastax.oss.driver.api.core.cql.PreparedStatement;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import com.datastax.oss.driver.api.core.cql.Row;
+import com.datastax.oss.driver.api.core.cql.Statement;
+import com.datastax.oss.driver.api.core.type.codec.TypeCodec;
+import javax.json.Json;
+import javax.json.JsonObject;
+import javax.json.JsonStructure;
+
+/**
+ * Illustrates how to map an entire table row to a Java object using the <a
+ * href=""https://jcp.org/en/jsr/detail?id=353"">Java API for JSON processing</a>, and leveraging the
+ * {@code SELECT JSON} and {@code INSERT JSON} syntaxes introduced in Cassandra 2.2.
+ *
+ * <p>This example makes usage of a custom {@link TypeCodec codec}, {@link Jsr353JsonCodec}, which
+ * is declared in the driver-extras module. If you plan to follow this example, make sure to include
+ * the following Maven dependencies in your project:
+ *
+ * <pre>{@code
+ * <dependency>
+ *     <groupId>javax.json</groupId>
+ *     <artifactId>javax.json-api</artifactId>
+ *     <version>${jsr353-api.version}</version>
+ * </dependency>
+ *
+ * <dependency>
+ *     <groupId>org.glassfish</groupId>
+ *     <artifactId>javax.json</artifactId>
+ *     <version>${jsr353-ri.version}</version>
+ *     <scope>runtime</scope>
+ * </dependency>
+ * }</pre>
+ *
+ * This example also uses the {@link com.datastax.oss.driver.api.querybuilder.QueryBuilder
+ * QueryBuilder}; for examples using the ""core"" API, see {@link PlainTextJson} (they are easily
+ * translatable to the queries in this class).
+ *
+ * <p>Preconditions: - a Cassandra 2.2+ cluster is running and accessible through the contacts
+ * points identified by basic.contact-points (see application.conf);
+ *
+ * <p>Side effects: - creates a new keyspace ""examples"" in the cluster. If a keyspace with this name
+ * already exists, it will be reused; - creates a table ""examples.json_jsr353_row"". If it already
+ * exists, it will be reused; - inserts data in the table.
+ *
+ * @see <a href=""http://www.datastax.com/dev/blog/whats-new-in-cassandra-2-2-json-support"">Whatâ€™s
+ *     New in Cassandra 2.2: JSON Support</a>
+ */
+public class Jsr353JsonRow {
+
+  // A codec to convert JSON payloads into JsonObject instances;
+  // this codec is declared in the driver-extras module
+  private static final Jsr353JsonCodec USER_CODEC = new Jsr353JsonCodec();
+
+  public static void main(String[] args) {
+    try (CqlSession session = new CqlSessionBuilder().addTypeCodecs(USER_CODEC).build()) {
+      createSchema(session);
+      insertJsonRow(session);
+      selectJsonRow(session);
+    }
+  }
+
+  private static void createSchema(CqlSession session) {
+    session.execute(
+        ""CREATE KEYSPACE IF NOT EXISTS examples ""
+            + ""WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1}"");
+    session.execute(
+        ""CREATE TABLE IF NOT EXISTS examples.json_jsr353_row(""
+            + ""id int PRIMARY KEY, name text, age int)"");
+  }
+
+  // Mapping a User instance to a table row using INSERT JSON
+  private static void insertJsonRow(CqlSession session) {
+
+    JsonObject alice =
+        Json.createObjectBuilder().add(""id"", 1).add(""name"", ""alice"").add(""age"", 30).build();
+
+    JsonObject bob =
+        Json.createObjectBuilder().add(""id"", 2).add(""name"", ""bob"").add(""age"", 35).build();
+
+    // Build and execute a simple statement
+    Statement stmt = insertInto(""examples"", ""json_jsr353_row"").json("""").build();
+    // .json(alice); todo implement","[{'comment': 'blocked on https://github.com/datastax/java-driver/pull/1207', 'commenter': 'tomekl007'}]"
1208,driver-examples/src/main/java/com/datastax/driver/examples/json/jackson/JacksonJsonRow.java,"@@ -0,0 +1,160 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.driver.examples.json.jackson;
+
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.bindMarker;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.insertInto;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.literal;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.selectFrom;
+
+import com.datastax.driver.examples.json.PlainTextJson;
+import com.datastax.driver.examples.json.codecs.JacksonJsonCodec;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.CqlSessionBuilder;
+import com.datastax.oss.driver.api.core.cql.PreparedStatement;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import com.datastax.oss.driver.api.core.cql.Row;
+import com.datastax.oss.driver.api.core.cql.Statement;
+import com.datastax.oss.driver.api.core.type.codec.TypeCodec;
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonProperty;
+
+/**
+ * Illustrates how to map an entire table row to a Java object using the <a
+ * href=""http://wiki.fasterxml.com/JacksonHome"">Jackson</a> library, and leveraging the {@code
+ * SELECT JSON} and {@code INSERT JSON} syntaxes introduced in Cassandra 2.2.
+ *
+ * <p>This example makes usage of a custom {@link TypeCodec codec}, {@link JacksonJsonCodec}. If you
+ * plan to follow this example, make sure to include the following Maven dependencies in your
+ * project:
+ *
+ * <p><dependency> <groupId>com.fasterxml.jackson.core</groupId>
+ * <artifactId>jackson-databind</artifactId> <version>${jackson.version}</version> </dependency> }
+ * </pre>
+ *
+ * This example also uses the {@link com.datastax.oss.driver.api.querybuilder.QueryBuilder
+ * QueryBuilder}; for examples using the ""core"" API, see {@link PlainTextJson} (they are easily
+ * translatable to the queries in this class).
+ *
+ * <p>Preconditions: - a Cassandra 2.2+ cluster is running and accessible through the contacts
+ * points identified by basic.contact-points (see application.conf);
+ *
+ * <p>Side effects: - creates a new keyspace ""examples"" in the cluster. If a keyspace with this name
+ * already exists, it will be reused; - creates a table ""examples.json_jackson_row"". If it already
+ * exists, it will be reused; - inserts data in the table.
+ *
+ * @see <a href=""http://www.datastax.com/dev/blog/whats-new-in-cassandra-2-2-json-support"">Whatâ€™s
+ *     New in Cassandra 2.2: JSON Support</a>
+ */
+public class JacksonJsonRow {
+  // A codec to convert JSON payloads into User instances;
+  // this codec is declared in the driver-extras module
+  private static final TypeCodec<User> USER_CODEC = new JacksonJsonCodec<>(User.class);
+
+  public static void main(String[] args) {
+    try (CqlSession session = new CqlSessionBuilder().addTypeCodecs(USER_CODEC).build()) {
+      createSchema(session);
+      insertJsonRow(session);
+      selectJsonRow(session);
+    }
+  }
+
+  private static void createSchema(CqlSession session) {
+    session.execute(
+        ""CREATE KEYSPACE IF NOT EXISTS examples ""
+            + ""WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1}"");
+    session.execute(
+        ""CREATE TABLE IF NOT EXISTS examples.json_jackson_row(""
+            + ""id int PRIMARY KEY, name text, age int)"");
+  }
+
+  // Mapping a User instance to a table row using INSERT JSON
+  private static void insertJsonRow(CqlSession session) {
+    // Build and execute a simple statement
+    Statement stmt =
+        insertInto(""examples"", ""json_jackson_row"")
+            .json("""")
+            //            .json(new User(1, ""alice"", 30))
+            .build(); // todo implement","[{'comment': 'blocked on https://github.com/datastax/java-driver/pull/1207', 'commenter': 'tomekl007'}]"
1208,pom.xml,"@@ -38,6 +38,7 @@
     <module>test-infra</module>
     <module>integration-tests</module>
     <module>distribution</module>
+    <module>driver-examples</module>","[{'comment': 'In driver 4.x we do not prefix submodule directories with `driver-`, so you should rename this directory to just `examples`.', 'commenter': 'adutra'}]"
1208,driver-examples/src/main/java/com/datastax/driver/examples/json/jackson/JacksonJsonColumn.java,"@@ -0,0 +1,162 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.driver.examples.json.jackson;
+
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.bindMarker;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.insertInto;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.literal;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.selectFrom;
+
+import com.datastax.driver.examples.json.PlainTextJson;
+import com.datastax.driver.examples.json.codecs.JacksonJsonCodec;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.CqlSessionBuilder;
+import com.datastax.oss.driver.api.core.cql.PreparedStatement;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import com.datastax.oss.driver.api.core.cql.Row;
+import com.datastax.oss.driver.api.core.cql.Statement;
+import com.datastax.oss.driver.api.core.type.codec.TypeCodec;
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonProperty;
+
+/**
+ * Illustrates how to map a single table column of type {@code VARCHAR}, containing JSON payloads,
+ * into a Java object using the <a href=""http://wiki.fasterxml.com/JacksonHome"">Jackson</a> library.
+ *
+ * <p>This example makes usage of a custom {@link TypeCodec codec}, {@link JacksonJsonCodec}, which
+ * is implemented in the driver-examples module. If you plan to follow this example, make sure to","[{'comment': 'Rename to `java-driver-examples`.', 'commenter': 'adutra'}]"
1208,driver-examples/pom.xml,"@@ -0,0 +1,127 @@
+<!--
+
+    Copyright DataStax, Inc.
+
+    Licensed under the Apache License, Version 2.0 (the ""License"");
+    you may not use this file except in compliance with the License.
+    You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+    Unless required by applicable law or agreed to in writing, software
+    distributed under the License is distributed on an ""AS IS"" BASIS,
+    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+    See the License for the specific language governing permissions and
+    limitations under the License.
+
+-->
+<project xmlns=""http://maven.apache.org/POM/4.0.0""
+         xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
+         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
+    <modelVersion>4.0.0</modelVersion>
+
+    <parent>
+        <artifactId>java-driver-parent</artifactId>
+        <groupId>com.datastax.oss</groupId>
+        <version>4.0.0-rc2-SNAPSHOT</version>
+    </parent>
+
+    <artifactId>driver-examples</artifactId>","[{'comment': 'Rename to `java-driver-examples`.', 'commenter': 'adutra'}]"
1208,driver-examples/pom.xml,"@@ -0,0 +1,127 @@
+<!--
+
+    Copyright DataStax, Inc.
+
+    Licensed under the Apache License, Version 2.0 (the ""License"");
+    you may not use this file except in compliance with the License.
+    You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+    Unless required by applicable law or agreed to in writing, software
+    distributed under the License is distributed on an ""AS IS"" BASIS,
+    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+    See the License for the specific language governing permissions and
+    limitations under the License.
+
+-->
+<project xmlns=""http://maven.apache.org/POM/4.0.0""
+         xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
+         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
+    <modelVersion>4.0.0</modelVersion>
+
+    <parent>
+        <artifactId>java-driver-parent</artifactId>
+        <groupId>com.datastax.oss</groupId>
+        <version>4.0.0-rc2-SNAPSHOT</version>
+    </parent>
+
+    <artifactId>driver-examples</artifactId>
+    <name>DataStax Java Driver for Apache Cassandra - Examples</name>","[{'comment': 'Rename to `DataStax Java driver for Apache Cassandra(R) - examples`.', 'commenter': 'adutra'}]"
1208,driver-examples/pom.xml,"@@ -0,0 +1,127 @@
+<!--
+
+    Copyright DataStax, Inc.
+
+    Licensed under the Apache License, Version 2.0 (the ""License"");
+    you may not use this file except in compliance with the License.
+    You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+    Unless required by applicable law or agreed to in writing, software
+    distributed under the License is distributed on an ""AS IS"" BASIS,
+    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+    See the License for the specific language governing permissions and
+    limitations under the License.
+
+-->
+<project xmlns=""http://maven.apache.org/POM/4.0.0""
+         xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
+         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
+    <modelVersion>4.0.0</modelVersion>
+
+    <parent>
+        <artifactId>java-driver-parent</artifactId>
+        <groupId>com.datastax.oss</groupId>
+        <version>4.0.0-rc2-SNAPSHOT</version>
+    </parent>
+
+    <artifactId>driver-examples</artifactId>
+    <name>DataStax Java Driver for Apache Cassandra - Examples</name>
+    <description>A collection of examples to demonstrate DataStax Java Driver for Apache Cassandra.</description>","[{'comment': 'Please add `(R)` after `Cassandra`.', 'commenter': 'adutra'}]"
1208,pom.xml,"@@ -192,6 +198,59 @@
         <artifactId>org.apache.felix.framework</artifactId>
         <version>6.0.0</version>
       </dependency>
+      <dependency>
+        <groupId>org.glassfish</groupId>
+        <artifactId>javax.json</artifactId>
+        <version>${jsr353-ri.version}</version>
+      </dependency>
+","[{'comment': 'Nit: remove spaces between dependencies declarations to align with other dependency declarations already present here.', 'commenter': 'adutra'}]"
1208,driver-examples/README.md,"@@ -0,0 +1,9 @@
+# DataStax Java Driver for Apache Cassandra - Examples","[{'comment': 'Please add `(R)` after `Cassandra`.', 'commenter': 'adutra'}]"
1208,driver-examples/README.md,"@@ -0,0 +1,9 @@
+# DataStax Java Driver for Apache Cassandra - Examples
+
+This module contains examples of how to use the DataStax Java driver for
+Apache Cassandra.","[{'comment': 'Same here.', 'commenter': 'adutra'}]"
1208,driver-examples/src/main/java/com/datastax/driver/examples/basic/CreateAndPopulateKeyspace.java,"@@ -0,0 +1,142 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.driver.examples.basic;","[{'comment': 'The base package should be `com.datastax.oss.driver.examples`.', 'commenter': 'adutra'}]"
1208,driver-examples/src/main/java/com/datastax/driver/examples/basic/CreateAndPopulateKeyspace.java,"@@ -0,0 +1,142 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.driver.examples.basic;
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.CqlSessionBuilder;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import com.datastax.oss.driver.api.core.cql.Row;
+
+/**
+ * Creates a keyspace and tables, and loads some data into them.
+ *
+ * <p>Preconditions: - a Cassandra session is running and accessible through the contacts points
+ * identified by basic.contact-points (see application.conf)","[{'comment': 'Nit: missing a final period. Same for other example classes.', 'commenter': 'adutra'}]"
1208,driver-examples/pom.xml,"@@ -0,0 +1,127 @@
+<!--
+
+    Copyright DataStax, Inc.
+
+    Licensed under the Apache License, Version 2.0 (the ""License"");
+    you may not use this file except in compliance with the License.
+    You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+    Unless required by applicable law or agreed to in writing, software
+    distributed under the License is distributed on an ""AS IS"" BASIS,
+    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+    See the License for the specific language governing permissions and
+    limitations under the License.
+
+-->
+<project xmlns=""http://maven.apache.org/POM/4.0.0""
+         xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
+         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
+    <modelVersion>4.0.0</modelVersion>
+
+    <parent>
+        <artifactId>java-driver-parent</artifactId>
+        <groupId>com.datastax.oss</groupId>
+        <version>4.0.0-rc2-SNAPSHOT</version>
+    </parent>
+
+    <artifactId>driver-examples</artifactId>
+    <name>DataStax Java Driver for Apache Cassandra - Examples</name>
+    <description>A collection of examples to demonstrate DataStax Java Driver for Apache Cassandra.</description>
+
+
+    <dependencies>
+
+        <!-- driver dependencies -->
+
+        <dependency>
+            <groupId>${project.groupId}</groupId>
+            <artifactId>java-driver-core</artifactId>
+            <version>${project.version}</version>
+        </dependency>
+        <dependency>
+            <groupId>${project.groupId}</groupId>
+            <artifactId>java-driver-query-builder</artifactId>
+            <version>${project.version}</version>
+        </dependency>
+
+        <!--JSR-353 (Java JSON API)-->
+
+        <dependency>
+            <groupId>javax.json</groupId>
+            <artifactId>javax.json-api</artifactId>
+            <optional>true</optional>
+        </dependency>
+
+        <dependency>
+            <groupId>org.glassfish</groupId>
+            <artifactId>javax.json</artifactId>
+            <optional>true</optional>
+            <scope>runtime</scope>
+        </dependency>
+
+        <!--JAX-RS-->
+
+        <dependency>
+            <groupId>javax.ws.rs</groupId>
+            <artifactId>javax.ws.rs-api</artifactId>
+            <optional>true</optional>
+        </dependency>
+
+        <!--Jersey-->
+
+        <dependency>
+            <groupId>org.glassfish.jersey.core</groupId>
+            <artifactId>jersey-server</artifactId>
+            <optional>true</optional>
+        </dependency>
+
+        <dependency>
+            <groupId>org.glassfish.jersey.media</groupId>
+            <artifactId>jersey-media-json-jackson</artifactId>
+            <optional>true</optional>
+        </dependency>
+
+        <dependency>
+            <groupId>org.glassfish.jersey.containers</groupId>
+            <artifactId>jersey-container-jdk-http</artifactId>
+            <optional>true</optional>
+        </dependency>
+
+        <!--CDI frameworks (HK2)-->
+
+        <dependency>
+            <groupId>org.glassfish.hk2</groupId>
+            <artifactId>hk2-api</artifactId>
+            <optional>true</optional>
+        </dependency>
+
+        <!-- Standard annotations -->
+
+        <dependency>
+            <groupId>javax.inject</groupId>
+            <artifactId>javax.inject</artifactId>
+            <optional>true</optional>
+        </dependency>
+
+        <dependency>
+            <groupId>javax.annotation</groupId>
+            <artifactId>javax.annotation-api</artifactId>
+            <optional>true</optional>
+        </dependency>
+    </dependencies>","[{'comment': 'We are missing the logback dependency + a `logback.xml` resource.', 'commenter': 'adutra'}]"
1208,driver-examples/src/main/java/com/datastax/driver/examples/basic/CreateAndPopulateKeyspace.java,"@@ -0,0 +1,142 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.driver.examples.basic;
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.CqlSessionBuilder;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import com.datastax.oss.driver.api.core.cql.Row;
+
+/**
+ * Creates a keyspace and tables, and loads some data into them.
+ *
+ * <p>Preconditions: - a Cassandra session is running and accessible through the contacts points
+ * identified by basic.contact-points (see application.conf)
+ *
+ * <p>Side effects: - creates a new keyspace ""simplex"" in the session. If a keyspace with this name
+ * already exists, it will be reused; - creates two tables ""simplex.songs"" and ""simplex.playlists"".
+ * If they exist already, they will be reused; - inserts a row in each table.
+ *
+ * @see <a href=""http://datastax.github.io/java-driver/manual/"">Java driver online manual</a>
+ */
+public class CreateAndPopulateKeyspace {
+
+  public static void main(String[] args) {
+
+    CreateAndPopulateKeyspace client = new CreateAndPopulateKeyspace();
+
+    try {
+      client.connect();
+      client.createSchema();
+      client.loadData();
+      client.querySchema();
+
+    } catch (Exception ex) {
+      ex.printStackTrace();
+    } finally {
+      client.close();
+    }
+  }
+
+  private CqlSession session;
+
+  /** Initiates a connection to the session specified by the application.conf. */
+  public void connect() {
+
+    session = new CqlSessionBuilder().build();
+
+    System.out.printf(""Connected session: %s%n"", session.getName());
+  }
+
+  /** Creates the schema (keyspace) and tables for this example. */
+  public void createSchema() {
+
+    session.execute(","[{'comment': 'This fails with a cryptic `NoNodeAvailableException` if the datacenter is not accurate. IMO we need to figure out a more user-friendly error message.', 'commenter': 'adutra'}]"
1208,driver-examples/src/main/java/com/datastax/driver/examples/concurrent/LimitConcurrencyRequestThrottler.java,"@@ -0,0 +1,103 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.driver.examples.concurrent;
+
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.bindMarker;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.insertInto;
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.CqlSessionBuilder;
+import com.datastax.oss.driver.api.core.cql.PreparedStatement;
+import com.datastax.oss.driver.internal.core.session.throttling.ConcurrencyLimitingRequestThrottler;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.UUID;
+import java.util.concurrent.CompletableFuture;
+import java.util.concurrent.ExecutionException;
+
+/**
+ * Creates a keyspace and tables, and loads data using Async API into them.
+ *
+ * <p>This example makes usage of a {@link CqlSession#executeAsync(String)} method, which is
+ * responsible for executing requests in a non-blocking way. It uses {@link
+ * ConcurrencyLimitingRequestThrottler} to limit number of concurrent requests to 32. It uses
+ * advanced.throttler configuration to limit async concurrency (max-concurrent-requests = 32) The
+ * max-queue-size is set to 10000 to buffer TOTAL_NUMBER_OF_INSERTS in a queue in a case of initial
+ * delay. (see application.conf)
+ *
+ * <p>Preconditions: - a Cassandra session is running and accessible through the contacts points
+ * identified by basic.contact-points (see application.conf)
+ *
+ * <p>Side effects: - creates a new keyspace ""examples"" in the session. If a keyspace with this name
+ * already exists, it will be reused; - creates a table ""examples.tbl_sample_kv"". If it exist
+ * already, it will be reused; - inserts a TOTAL_NUMBER_OF_INSERTS of rows into the table.
+ *
+ * @see <a href=""http://datastax.github.io/java-driver/manual/"">Java driver online manual</a>
+ */
+public class LimitConcurrencyRequestThrottler {
+  private static final int TOTAL_NUMBER_OF_INSERTS = 10_000;
+
+  public static void main(String[] args) throws InterruptedException, ExecutionException {
+
+    try (CqlSession session = new CqlSessionBuilder().build()) {
+      createSchema(session);
+      insertConcurrent(session);
+    }
+  }
+
+  private static void insertConcurrent(CqlSession session)
+      throws InterruptedException, ExecutionException {
+    PreparedStatement pst =
+        session.prepare(
+            insertInto(""examples"", ""tbl_sample_kv"")
+                .value(""id"", bindMarker(""id""))
+                .value(""value"", bindMarker(""value""))
+                .build());
+    // Create list of pending CompletableFutures. We will add every operation returned from
+    // executeAsync
+    // Next, we will wait for completion of all TOTAL_NUMBER_OF_INSERTS
+    List<CompletableFuture> pending = new ArrayList<>();","[{'comment': 'Some IDEs will complain that you are using the raw type `CompletableFuture` instead of `CompletabeFuture<?>`.', 'commenter': 'adutra'}]"
1208,driver-examples/src/main/java/com/datastax/driver/examples/concurrent/LimitConcurrencyRequestThrottler.java,"@@ -0,0 +1,103 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.driver.examples.concurrent;
+
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.bindMarker;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.insertInto;
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.CqlSessionBuilder;
+import com.datastax.oss.driver.api.core.cql.PreparedStatement;
+import com.datastax.oss.driver.internal.core.session.throttling.ConcurrencyLimitingRequestThrottler;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.UUID;
+import java.util.concurrent.CompletableFuture;
+import java.util.concurrent.ExecutionException;
+
+/**
+ * Creates a keyspace and tables, and loads data using Async API into them.
+ *
+ * <p>This example makes usage of a {@link CqlSession#executeAsync(String)} method, which is
+ * responsible for executing requests in a non-blocking way. It uses {@link
+ * ConcurrencyLimitingRequestThrottler} to limit number of concurrent requests to 32. It uses
+ * advanced.throttler configuration to limit async concurrency (max-concurrent-requests = 32) The
+ * max-queue-size is set to 10000 to buffer TOTAL_NUMBER_OF_INSERTS in a queue in a case of initial
+ * delay. (see application.conf)
+ *
+ * <p>Preconditions: - a Cassandra session is running and accessible through the contacts points
+ * identified by basic.contact-points (see application.conf)
+ *
+ * <p>Side effects: - creates a new keyspace ""examples"" in the session. If a keyspace with this name
+ * already exists, it will be reused; - creates a table ""examples.tbl_sample_kv"". If it exist
+ * already, it will be reused; - inserts a TOTAL_NUMBER_OF_INSERTS of rows into the table.
+ *
+ * @see <a href=""http://datastax.github.io/java-driver/manual/"">Java driver online manual</a>
+ */
+public class LimitConcurrencyRequestThrottler {
+  private static final int TOTAL_NUMBER_OF_INSERTS = 10_000;
+
+  public static void main(String[] args) throws InterruptedException, ExecutionException {
+
+    try (CqlSession session = new CqlSessionBuilder().build()) {
+      createSchema(session);
+      insertConcurrent(session);
+    }
+  }
+
+  private static void insertConcurrent(CqlSession session)
+      throws InterruptedException, ExecutionException {
+    PreparedStatement pst =
+        session.prepare(
+            insertInto(""examples"", ""tbl_sample_kv"")
+                .value(""id"", bindMarker(""id""))
+                .value(""value"", bindMarker(""value""))
+                .build());
+    // Create list of pending CompletableFutures. We will add every operation returned from
+    // executeAsync
+    // Next, we will wait for completion of all TOTAL_NUMBER_OF_INSERTS
+    List<CompletableFuture> pending = new ArrayList<>();
+
+    // For every i we will insert a record to db
+    for (int i = 0; i < TOTAL_NUMBER_OF_INSERTS; i++) {","[{'comment': ""Isn't this going to blow up the server? We should introduce a mechanism to limit the number of in-flight requests, otherwise all the 10000 requests will be submitted together."", 'commenter': 'adutra'}]"
1208,driver-examples/src/main/java/com/datastax/driver/examples/concurrent/LimitConcurrencyRequestThrottler.java,"@@ -0,0 +1,103 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.driver.examples.concurrent;
+
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.bindMarker;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.insertInto;
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.CqlSessionBuilder;
+import com.datastax.oss.driver.api.core.cql.PreparedStatement;
+import com.datastax.oss.driver.internal.core.session.throttling.ConcurrencyLimitingRequestThrottler;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.UUID;
+import java.util.concurrent.CompletableFuture;
+import java.util.concurrent.ExecutionException;
+
+/**
+ * Creates a keyspace and tables, and loads data using Async API into them.
+ *
+ * <p>This example makes usage of a {@link CqlSession#executeAsync(String)} method, which is
+ * responsible for executing requests in a non-blocking way. It uses {@link
+ * ConcurrencyLimitingRequestThrottler} to limit number of concurrent requests to 32. It uses
+ * advanced.throttler configuration to limit async concurrency (max-concurrent-requests = 32) The
+ * max-queue-size is set to 10000 to buffer TOTAL_NUMBER_OF_INSERTS in a queue in a case of initial
+ * delay. (see application.conf)
+ *
+ * <p>Preconditions: - a Cassandra session is running and accessible through the contacts points
+ * identified by basic.contact-points (see application.conf)
+ *
+ * <p>Side effects: - creates a new keyspace ""examples"" in the session. If a keyspace with this name
+ * already exists, it will be reused; - creates a table ""examples.tbl_sample_kv"". If it exist
+ * already, it will be reused; - inserts a TOTAL_NUMBER_OF_INSERTS of rows into the table.
+ *
+ * @see <a href=""http://datastax.github.io/java-driver/manual/"">Java driver online manual</a>
+ */
+public class LimitConcurrencyRequestThrottler {
+  private static final int TOTAL_NUMBER_OF_INSERTS = 10_000;
+
+  public static void main(String[] args) throws InterruptedException, ExecutionException {
+
+    try (CqlSession session = new CqlSessionBuilder().build()) {
+      createSchema(session);
+      insertConcurrent(session);
+    }
+  }
+
+  private static void insertConcurrent(CqlSession session)
+      throws InterruptedException, ExecutionException {
+    PreparedStatement pst =
+        session.prepare(
+            insertInto(""examples"", ""tbl_sample_kv"")
+                .value(""id"", bindMarker(""id""))
+                .value(""value"", bindMarker(""value""))
+                .build());
+    // Create list of pending CompletableFutures. We will add every operation returned from
+    // executeAsync
+    // Next, we will wait for completion of all TOTAL_NUMBER_OF_INSERTS
+    List<CompletableFuture> pending = new ArrayList<>();
+
+    // For every i we will insert a record to db
+    for (int i = 0; i < TOTAL_NUMBER_OF_INSERTS; i++) {
+      pending.add(
+          session
+              .executeAsync(
+                  pst.bind()
+                      .set(""id"", UUID.randomUUID(), UUID.class)
+                      .set(""value"", String.format(""Value for: %s"", i), String.class))
+              // Transform CompletionState toCompletableFuture to be able to wait for execution of
+              // all using CompletableFuture.allOf
+              .toCompletableFuture());
+    }
+
+    // Wait for completion of all TOTAL_NUMBER_OF_INSERTS pending requests
+    CompletableFuture.allOf(pending.toArray(new CompletableFuture[pending.size()])).get();","[{'comment': 'It is usually recommended to use `new CompletableFuture[0]`.', 'commenter': 'adutra'}]"
1208,driver-examples/src/main/java/com/datastax/driver/examples/concurrent/LimitConcurrencyRequestThrottler.java,"@@ -0,0 +1,103 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.driver.examples.concurrent;
+
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.bindMarker;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.insertInto;
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.CqlSessionBuilder;
+import com.datastax.oss.driver.api.core.cql.PreparedStatement;
+import com.datastax.oss.driver.internal.core.session.throttling.ConcurrencyLimitingRequestThrottler;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.UUID;
+import java.util.concurrent.CompletableFuture;
+import java.util.concurrent.ExecutionException;
+
+/**
+ * Creates a keyspace and tables, and loads data using Async API into them.
+ *
+ * <p>This example makes usage of a {@link CqlSession#executeAsync(String)} method, which is
+ * responsible for executing requests in a non-blocking way. It uses {@link
+ * ConcurrencyLimitingRequestThrottler} to limit number of concurrent requests to 32. It uses
+ * advanced.throttler configuration to limit async concurrency (max-concurrent-requests = 32) The
+ * max-queue-size is set to 10000 to buffer TOTAL_NUMBER_OF_INSERTS in a queue in a case of initial
+ * delay. (see application.conf)
+ *
+ * <p>Preconditions: - a Cassandra session is running and accessible through the contacts points
+ * identified by basic.contact-points (see application.conf)
+ *
+ * <p>Side effects: - creates a new keyspace ""examples"" in the session. If a keyspace with this name
+ * already exists, it will be reused; - creates a table ""examples.tbl_sample_kv"". If it exist
+ * already, it will be reused; - inserts a TOTAL_NUMBER_OF_INSERTS of rows into the table.
+ *
+ * @see <a href=""http://datastax.github.io/java-driver/manual/"">Java driver online manual</a>
+ */
+public class LimitConcurrencyRequestThrottler {
+  private static final int TOTAL_NUMBER_OF_INSERTS = 10_000;
+
+  public static void main(String[] args) throws InterruptedException, ExecutionException {
+
+    try (CqlSession session = new CqlSessionBuilder().build()) {
+      createSchema(session);
+      insertConcurrent(session);
+    }
+  }
+
+  private static void insertConcurrent(CqlSession session)
+      throws InterruptedException, ExecutionException {
+    PreparedStatement pst =
+        session.prepare(
+            insertInto(""examples"", ""tbl_sample_kv"")
+                .value(""id"", bindMarker(""id""))
+                .value(""value"", bindMarker(""value""))
+                .build());
+    // Create list of pending CompletableFutures. We will add every operation returned from
+    // executeAsync
+    // Next, we will wait for completion of all TOTAL_NUMBER_OF_INSERTS
+    List<CompletableFuture> pending = new ArrayList<>();
+
+    // For every i we will insert a record to db
+    for (int i = 0; i < TOTAL_NUMBER_OF_INSERTS; i++) {
+      pending.add(
+          session
+              .executeAsync(
+                  pst.bind()
+                      .set(""id"", UUID.randomUUID(), UUID.class)
+                      .set(""value"", String.format(""Value for: %s"", i), String.class))
+              // Transform CompletionState toCompletableFuture to be able to wait for execution of
+              // all using CompletableFuture.allOf
+              .toCompletableFuture());
+    }
+
+    // Wait for completion of all TOTAL_NUMBER_OF_INSERTS pending requests
+    CompletableFuture.allOf(pending.toArray(new CompletableFuture[pending.size()])).get();
+
+    System.out.println(
+        String.format(
+            ""Finished executing %s queries with a concurrency level of 32."", pending.size()));","[{'comment': ""I don't see anything limiting the concurrency to 32."", 'commenter': 'adutra'}]"
1208,driver-examples/src/main/java/com/datastax/driver/examples/datatypes/Blobs.java,"@@ -0,0 +1,255 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.driver.examples.datatypes;
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.CqlSessionBuilder;
+import com.datastax.oss.driver.api.core.cql.BoundStatement;
+import com.datastax.oss.driver.api.core.cql.PreparedStatement;
+import com.datastax.oss.driver.api.core.cql.Row;
+import com.datastax.oss.protocol.internal.util.Bytes;
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.nio.channels.FileChannel;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Objects;
+
+/**
+ * Inserts and retrieves values in BLOB columns.
+ *
+ * <p>By default, the Java driver maps this type to {@link java.nio.ByteBuffer}. The ByteBuffer API
+ * is a bit tricky to use at times, so we will show common pitfalls as well. We strongly recommend
+ * that you read the {@link java.nio.Buffer} and {@link ByteBuffer} API docs and become familiar
+ * with the capacity, limit and position properties. <a
+ * href=""http://tutorials.jenkov.com/java-nio/buffers.html"">This tutorial</a> might also help.
+ *
+ * <p>Preconditions: - a Cassandra cluster is running and accessible through the contacts points
+ * identified by basic.contact-points (see application.conf) - FILE references an existing file.
+ *
+ * <p>Side effects: - creates a new keyspace ""examples"" in the cluster. If a keyspace with this name
+ * already exists, it will be reused; - creates a table ""examples.blobs"". If it already exists, it
+ * will be reused; - inserts data in the table.
+ */
+public class Blobs {
+
+  private static File FILE = new File(Blobs.class.getResource(""/cassandra_logo.png"").getFile());
+
+  public static void main(String[] args) throws IOException {
+
+    try (CqlSession session = new CqlSessionBuilder().build()) {
+
+      createSchema(session);
+      allocateAndInsert(session);
+      retrieveSimpleColumn(session);
+      retrieveMapColumn(session);
+      insertConcurrent(session);
+      insertFromAndRetrieveToFile(session);
+    }
+  }
+
+  private static void createSchema(CqlSession session) {
+    session.execute(
+        ""CREATE KEYSPACE IF NOT EXISTS examples ""
+            + ""WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1}"");
+    session.execute(
+        ""CREATE TABLE IF NOT EXISTS examples.blobs(k int PRIMARY KEY, b blob, m map<text, blob>)"");
+  }
+
+  private static void allocateAndInsert(CqlSession session) {
+    // One way to get a byte buffer is to allocate it and fill it yourself:
+    ByteBuffer buffer = ByteBuffer.allocate(16);
+    while (buffer.hasRemaining()) buffer.put((byte) 0xFF);","[{'comment': 'Nit: add braces around while loop.', 'commenter': 'adutra'}]"
1208,driver-examples/src/main/java/com/datastax/driver/examples/datatypes/Blobs.java,"@@ -0,0 +1,255 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.driver.examples.datatypes;
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.CqlSessionBuilder;
+import com.datastax.oss.driver.api.core.cql.BoundStatement;
+import com.datastax.oss.driver.api.core.cql.PreparedStatement;
+import com.datastax.oss.driver.api.core.cql.Row;
+import com.datastax.oss.protocol.internal.util.Bytes;
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.nio.channels.FileChannel;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Objects;
+
+/**
+ * Inserts and retrieves values in BLOB columns.
+ *
+ * <p>By default, the Java driver maps this type to {@link java.nio.ByteBuffer}. The ByteBuffer API
+ * is a bit tricky to use at times, so we will show common pitfalls as well. We strongly recommend
+ * that you read the {@link java.nio.Buffer} and {@link ByteBuffer} API docs and become familiar
+ * with the capacity, limit and position properties. <a
+ * href=""http://tutorials.jenkov.com/java-nio/buffers.html"">This tutorial</a> might also help.
+ *
+ * <p>Preconditions: - a Cassandra cluster is running and accessible through the contacts points
+ * identified by basic.contact-points (see application.conf) - FILE references an existing file.
+ *
+ * <p>Side effects: - creates a new keyspace ""examples"" in the cluster. If a keyspace with this name
+ * already exists, it will be reused; - creates a table ""examples.blobs"". If it already exists, it
+ * will be reused; - inserts data in the table.
+ */
+public class Blobs {
+
+  private static File FILE = new File(Blobs.class.getResource(""/cassandra_logo.png"").getFile());
+
+  public static void main(String[] args) throws IOException {
+
+    try (CqlSession session = new CqlSessionBuilder().build()) {
+
+      createSchema(session);
+      allocateAndInsert(session);
+      retrieveSimpleColumn(session);
+      retrieveMapColumn(session);
+      insertConcurrent(session);
+      insertFromAndRetrieveToFile(session);
+    }
+  }
+
+  private static void createSchema(CqlSession session) {
+    session.execute(
+        ""CREATE KEYSPACE IF NOT EXISTS examples ""
+            + ""WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1}"");
+    session.execute(
+        ""CREATE TABLE IF NOT EXISTS examples.blobs(k int PRIMARY KEY, b blob, m map<text, blob>)"");
+  }
+
+  private static void allocateAndInsert(CqlSession session) {
+    // One way to get a byte buffer is to allocate it and fill it yourself:
+    ByteBuffer buffer = ByteBuffer.allocate(16);
+    while (buffer.hasRemaining()) buffer.put((byte) 0xFF);
+
+    // Don't forget to flip! The driver expects a buffer that is ready for reading. That is, it will
+    // consider all
+    // the data between buffer.position() and buffer.limit().","[{'comment': 'The line wrapping is a bit weird here.', 'commenter': 'adutra'}]"
1208,driver-examples/src/main/java/com/datastax/driver/examples/datatypes/Blobs.java,"@@ -0,0 +1,255 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.driver.examples.datatypes;
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.CqlSessionBuilder;
+import com.datastax.oss.driver.api.core.cql.BoundStatement;
+import com.datastax.oss.driver.api.core.cql.PreparedStatement;
+import com.datastax.oss.driver.api.core.cql.Row;
+import com.datastax.oss.protocol.internal.util.Bytes;
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.nio.channels.FileChannel;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Objects;
+
+/**
+ * Inserts and retrieves values in BLOB columns.
+ *
+ * <p>By default, the Java driver maps this type to {@link java.nio.ByteBuffer}. The ByteBuffer API
+ * is a bit tricky to use at times, so we will show common pitfalls as well. We strongly recommend
+ * that you read the {@link java.nio.Buffer} and {@link ByteBuffer} API docs and become familiar
+ * with the capacity, limit and position properties. <a
+ * href=""http://tutorials.jenkov.com/java-nio/buffers.html"">This tutorial</a> might also help.
+ *
+ * <p>Preconditions: - a Cassandra cluster is running and accessible through the contacts points
+ * identified by basic.contact-points (see application.conf) - FILE references an existing file.
+ *
+ * <p>Side effects: - creates a new keyspace ""examples"" in the cluster. If a keyspace with this name
+ * already exists, it will be reused; - creates a table ""examples.blobs"". If it already exists, it
+ * will be reused; - inserts data in the table.
+ */
+public class Blobs {
+
+  private static File FILE = new File(Blobs.class.getResource(""/cassandra_logo.png"").getFile());
+
+  public static void main(String[] args) throws IOException {
+
+    try (CqlSession session = new CqlSessionBuilder().build()) {
+
+      createSchema(session);
+      allocateAndInsert(session);
+      retrieveSimpleColumn(session);
+      retrieveMapColumn(session);
+      insertConcurrent(session);
+      insertFromAndRetrieveToFile(session);
+    }
+  }
+
+  private static void createSchema(CqlSession session) {
+    session.execute(
+        ""CREATE KEYSPACE IF NOT EXISTS examples ""
+            + ""WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1}"");
+    session.execute(
+        ""CREATE TABLE IF NOT EXISTS examples.blobs(k int PRIMARY KEY, b blob, m map<text, blob>)"");
+  }
+
+  private static void allocateAndInsert(CqlSession session) {
+    // One way to get a byte buffer is to allocate it and fill it yourself:
+    ByteBuffer buffer = ByteBuffer.allocate(16);
+    while (buffer.hasRemaining()) buffer.put((byte) 0xFF);
+
+    // Don't forget to flip! The driver expects a buffer that is ready for reading. That is, it will
+    // consider all
+    // the data between buffer.position() and buffer.limit().
+    // Right now we are positioned at the end because we just finished writing, so if we passed the
+    // buffer as-is it
+    // would appear to be empty:
+    assert buffer.limit() - buffer.position() == 0;
+
+    buffer.flip();
+    // Now position is back to the beginning, so the driver will see all 16 bytes.
+    assert buffer.limit() - buffer.position() == 16;
+
+    Map<String, ByteBuffer> map = new HashMap<String, ByteBuffer>();
+    map.put(""test"", buffer);
+
+    PreparedStatement prepared =
+        session.prepare(""INSERT INTO examples.blobs (k, b, m) VALUES (1, ?, ?)"");
+
+    session.execute(prepared.bind(buffer, map));
+  }
+
+  private static void retrieveSimpleColumn(CqlSession session) {
+    Row row = session.execute(""SELECT b, m FROM examples.blobs WHERE k = 1"").one();
+
+    assert row != null;
+    ByteBuffer buffer = row.getByteBuffer(""b"");
+
+    // The driver always returns buffers that are ready for reading.
+    assert buffer != null;
+    assert buffer.limit() - buffer.position() == 16;
+
+    // One way to read from the buffer is to use absolute getters. Do NOT start reading at index 0,
+    // as the buffer
+    // might start at a different position (we'll see an example of that later).
+    for (int i = buffer.position(); i < buffer.limit(); i++) {
+      byte b = buffer.get(i);
+      assert b == (byte) 0xFF;
+    }
+
+    // Another way is to use relative getters.
+    while (buffer.hasRemaining()) {
+      byte b = buffer.get();
+      assert b == (byte) 0xFF;
+    }
+    // Note that relative getters change the position, so when we're done reading we're at the end
+    // again.
+    assert buffer.position() == buffer.limit();
+
+    // Reset the position for the next operation.
+    buffer.flip();
+
+    // Yet another way is to convert the buffer to a byte array. Do NOT use buffer.array(), because
+    // it returns the
+    // buffer's *backing array*, which is not the same thing as its contents:
+    // - not all byte buffers have backing arrays
+    // - even then, the backing array might be larger than the buffer's contents
+    //
+    // The driver provides a utility method that handles those details for you:
+    byte[] array = Bytes.getArray(buffer);
+    assert array.length == 16;
+    for (byte b : array) {
+      assert b == (byte) 0xFF;
+    }
+  }
+
+  private static void retrieveMapColumn(CqlSession session) {
+    Row row = session.execute(""SELECT b, m FROM examples.blobs WHERE k = 1"").one();
+
+    // The map columns illustrates the pitfalls with position() and array().
+    assert row != null;
+    Map<String, ByteBuffer> m = row.getMap(""m"", String.class, ByteBuffer.class);
+    assert m != null;
+    ByteBuffer buffer = m.get(""test"");
+
+    // We did get back a buffer that contains 16 bytes as expected.
+    assert buffer.limit() - buffer.position() == 16;
+    // However, it is not positioned at 0. And you can also see that its backing array contains more
+    // than 16 bytes.
+    // What happens is that the buffer is a ""view"" of the last 16 of a 32-byte array.
+    // This is an implementation detail and you shouldn't have to worry about it if you process the
+    // buffer correctly
+    // (don't iterate from 0, use Bytes.getArray()).
+    assert buffer.position() == 16;
+    assert buffer.array().length == 32;
+  }
+
+  private static void insertConcurrent(CqlSession session) {
+    PreparedStatement preparedStatement =
+        session.prepare(""INSERT INTO examples.blobs (k, b) VALUES (1, :b)"");
+
+    // This is another convenient utility provided by the driver. It's useful for tests.
+    ByteBuffer buffer = Bytes.fromHexString(""0xffffff"");
+
+    // When you pass a byte buffer to a bound statement, it creates a shallow copy internally with
+    // the
+    // buffer.duplicate() method.
+    BoundStatement boundStatement = preparedStatement.bind();
+    boundStatement.setByteBuffer(""b"", buffer);
+
+    // This means you can now move in the original buffer, without affecting the insertion if it
+    // happens later.
+    buffer.position(buffer.limit());
+
+    session.execute(boundStatement);
+    Row row = session.execute(""SELECT b FROM examples.blobs WHERE k = 1"").one();
+    assert row != null;
+    assert Objects.equals(Bytes.toHexString(row.getByteBuffer(""b"")), ""0xffffff"");
+
+    buffer.flip();
+
+    // HOWEVER duplicate() only performs a shallow copy. The two buffers still share the same
+    // contents. So if you
+    // modify the contents of the original buffer, this will affect another execution of the bound
+    // statement.
+    buffer.put(0, (byte) 0xaa);
+    session.execute(boundStatement);
+    row = session.execute(""SELECT b FROM examples.blobs WHERE k = 1"").one();
+    assert row != null;
+    assert Objects.equals(Bytes.toHexString(row.getByteBuffer(""b"")), ""0xaaffff"");
+
+    // This will also happen if you use the async API, e.g. create the bound statement, call
+    // executeAsync() on it
+    // and reuse the buffer immediately.
+
+    // If you reuse buffers concurrently and want to avoid those issues, perform a deep copy of the
+    // buffer before
+    // passing it to the bound statement.
+    int startPosition = buffer.position();
+    ByteBuffer buffer2 = ByteBuffer.allocate(buffer.limit() - startPosition);
+    buffer2.put(buffer);
+    buffer.position(startPosition);
+    buffer2.flip();
+    boundStatement.setByteBuffer(""b"", buffer2);
+    session.execute(boundStatement);
+
+    // Note: unlike BoundStatement, SimpleStatement does not duplicate its arguments, so even the
+    // position will be
+    // affected if you change it before executing the statement. Again, resort to deep copies if
+    // required.
+  }
+
+  private static void insertFromAndRetrieveToFile(CqlSession session) throws IOException {","[{'comment': 'I think what you mean here is `retrieveFromFileAndInsertInto`. ""Retrieve to file"" sounds awkward.', 'commenter': 'adutra'}]"
1208,driver-examples/src/main/java/com/datastax/driver/examples/json/codecs/JacksonJsonCodec.java,"@@ -0,0 +1,155 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.driver.examples.json.codecs;
+
+import com.datastax.driver.examples.json.exceptions.InvalidTypeException;
+import com.datastax.oss.driver.api.core.ProtocolVersion;
+import com.datastax.oss.driver.api.core.type.DataType;
+import com.datastax.oss.driver.api.core.type.DataTypes;
+import com.datastax.oss.driver.api.core.type.codec.TypeCodec;
+import com.datastax.oss.driver.api.core.type.reflect.GenericType;
+import com.datastax.oss.driver.internal.core.util.Strings;
+import com.datastax.oss.protocol.internal.util.Bytes;
+import com.fasterxml.jackson.core.JsonProcessingException;
+import com.fasterxml.jackson.databind.JavaType;
+import com.fasterxml.jackson.databind.ObjectMapper;
+import com.fasterxml.jackson.databind.type.TypeFactory;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import edu.umd.cs.findbugs.annotations.Nullable;
+import java.io.IOException;
+import java.nio.ByteBuffer;
+
+/**
+ * A JSON codec that uses the <a href=""http://wiki.fasterxml.com/JacksonHome"">Jackson</a> library to
+ * perform serialization and deserialization of JSON objects.
+ *
+ * <p>This codec maps a single Java object to a single JSON structure at a time; mapping of arrays
+ * or collections to root-level JSON arrays is not supported, but such a codec can be easily crafted
+ * after this one.
+ *
+ * <p>Note that this codec requires the presence of Jackson library at runtime. If you use Maven,
+ * this can be done by declaring the following dependency in your project:
+ *
+ * <p>
+ *
+ * <pre>{@code
+ * <dependency>
+ *   <groupId>com.fasterxml.jackson.core</groupId>
+ *   <artifactId>jackson-databind</artifactId>
+ *   <version>2.6.3</version>","[{'comment': 'This is a rather old version, please use the latest one available.', 'commenter': 'adutra'}]"
1208,driver-examples/src/main/java/com/datastax/driver/examples/json/codecs/JacksonJsonCodec.java,"@@ -0,0 +1,155 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.driver.examples.json.codecs;
+
+import com.datastax.driver.examples.json.exceptions.InvalidTypeException;
+import com.datastax.oss.driver.api.core.ProtocolVersion;
+import com.datastax.oss.driver.api.core.type.DataType;
+import com.datastax.oss.driver.api.core.type.DataTypes;
+import com.datastax.oss.driver.api.core.type.codec.TypeCodec;
+import com.datastax.oss.driver.api.core.type.reflect.GenericType;
+import com.datastax.oss.driver.internal.core.util.Strings;
+import com.datastax.oss.protocol.internal.util.Bytes;
+import com.fasterxml.jackson.core.JsonProcessingException;
+import com.fasterxml.jackson.databind.JavaType;
+import com.fasterxml.jackson.databind.ObjectMapper;
+import com.fasterxml.jackson.databind.type.TypeFactory;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import edu.umd.cs.findbugs.annotations.Nullable;
+import java.io.IOException;
+import java.nio.ByteBuffer;
+
+/**
+ * A JSON codec that uses the <a href=""http://wiki.fasterxml.com/JacksonHome"">Jackson</a> library to
+ * perform serialization and deserialization of JSON objects.
+ *
+ * <p>This codec maps a single Java object to a single JSON structure at a time; mapping of arrays
+ * or collections to root-level JSON arrays is not supported, but such a codec can be easily crafted
+ * after this one.
+ *
+ * <p>Note that this codec requires the presence of Jackson library at runtime. If you use Maven,
+ * this can be done by declaring the following dependency in your project:
+ *
+ * <p>
+ *
+ * <pre>{@code
+ * <dependency>
+ *   <groupId>com.fasterxml.jackson.core</groupId>
+ *   <artifactId>jackson-databind</artifactId>
+ *   <version>2.6.3</version>
+ * </dependency>
+ * }</pre>
+ */
+public class JacksonJsonCodec<T> implements TypeCodec<T> {
+
+  private final ObjectMapper objectMapper;
+  private final GenericType<T> javaType;
+
+  /**
+   * Creates a new instance for the provided {@code javaClass}, using a default, newly-allocated
+   * {@link ObjectMapper}.
+   *
+   * @param javaClass the Java class this codec maps to.
+   */
+  public JacksonJsonCodec(Class<T> javaClass) {
+    this(javaClass, new ObjectMapper());
+  }
+
+  /**
+   * Creates a new instance for the provided {@code javaClass}, and using the provided {@link
+   * ObjectMapper}.
+   *
+   * @param javaClass the Java class this codec maps to.
+   */
+  public JacksonJsonCodec(Class<T> javaClass, ObjectMapper objectMapper) {
+    this.javaType = GenericType.of(javaClass);
+    this.objectMapper = objectMapper;
+  }
+
+  @NonNull
+  @Override
+  public GenericType<T> getJavaType() {
+    return javaType;
+  }
+
+  @NonNull
+  @Override
+  public DataType getCqlType() {
+    return DataTypes.TEXT;
+  }
+
+  @Nullable
+  @Override
+  public ByteBuffer encode(@Nullable T value, @NonNull ProtocolVersion protocolVersion) {
+    if (value == null) return null;
+    try {
+      return ByteBuffer.wrap(objectMapper.writeValueAsBytes(value));
+    } catch (JsonProcessingException e) {
+      throw new InvalidTypeException(e.getMessage(), e);","[{'comment': ""In driver 4.0 we don't have any specific exception for codecs except `CodecNotFoundException`. When a codec cannot serialize or deserialize something, we use `IllegalArgumentException`. I don't think you need to introduce a special exception in these examples."", 'commenter': 'adutra'}]"
1208,driver-examples/src/main/java/com/datastax/driver/examples/json/codecs/JacksonJsonCodec.java,"@@ -0,0 +1,155 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.driver.examples.json.codecs;
+
+import com.datastax.driver.examples.json.exceptions.InvalidTypeException;
+import com.datastax.oss.driver.api.core.ProtocolVersion;
+import com.datastax.oss.driver.api.core.type.DataType;
+import com.datastax.oss.driver.api.core.type.DataTypes;
+import com.datastax.oss.driver.api.core.type.codec.TypeCodec;
+import com.datastax.oss.driver.api.core.type.reflect.GenericType;
+import com.datastax.oss.driver.internal.core.util.Strings;
+import com.datastax.oss.protocol.internal.util.Bytes;
+import com.fasterxml.jackson.core.JsonProcessingException;
+import com.fasterxml.jackson.databind.JavaType;
+import com.fasterxml.jackson.databind.ObjectMapper;
+import com.fasterxml.jackson.databind.type.TypeFactory;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import edu.umd.cs.findbugs.annotations.Nullable;
+import java.io.IOException;
+import java.nio.ByteBuffer;
+
+/**
+ * A JSON codec that uses the <a href=""http://wiki.fasterxml.com/JacksonHome"">Jackson</a> library to
+ * perform serialization and deserialization of JSON objects.
+ *
+ * <p>This codec maps a single Java object to a single JSON structure at a time; mapping of arrays
+ * or collections to root-level JSON arrays is not supported, but such a codec can be easily crafted
+ * after this one.
+ *
+ * <p>Note that this codec requires the presence of Jackson library at runtime. If you use Maven,
+ * this can be done by declaring the following dependency in your project:
+ *
+ * <p>
+ *
+ * <pre>{@code
+ * <dependency>
+ *   <groupId>com.fasterxml.jackson.core</groupId>
+ *   <artifactId>jackson-databind</artifactId>
+ *   <version>2.6.3</version>
+ * </dependency>
+ * }</pre>
+ */
+public class JacksonJsonCodec<T> implements TypeCodec<T> {
+
+  private final ObjectMapper objectMapper;
+  private final GenericType<T> javaType;
+
+  /**
+   * Creates a new instance for the provided {@code javaClass}, using a default, newly-allocated
+   * {@link ObjectMapper}.
+   *
+   * @param javaClass the Java class this codec maps to.
+   */
+  public JacksonJsonCodec(Class<T> javaClass) {
+    this(javaClass, new ObjectMapper());
+  }
+
+  /**
+   * Creates a new instance for the provided {@code javaClass}, and using the provided {@link
+   * ObjectMapper}.
+   *
+   * @param javaClass the Java class this codec maps to.
+   */
+  public JacksonJsonCodec(Class<T> javaClass, ObjectMapper objectMapper) {
+    this.javaType = GenericType.of(javaClass);
+    this.objectMapper = objectMapper;
+  }
+
+  @NonNull
+  @Override
+  public GenericType<T> getJavaType() {
+    return javaType;
+  }
+
+  @NonNull
+  @Override
+  public DataType getCqlType() {
+    return DataTypes.TEXT;
+  }
+
+  @Nullable
+  @Override
+  public ByteBuffer encode(@Nullable T value, @NonNull ProtocolVersion protocolVersion) {
+    if (value == null) return null;
+    try {
+      return ByteBuffer.wrap(objectMapper.writeValueAsBytes(value));
+    } catch (JsonProcessingException e) {
+      throw new InvalidTypeException(e.getMessage(), e);
+    }
+  }
+
+  @Nullable
+  @Override
+  public T decode(@Nullable ByteBuffer bytes, @NonNull ProtocolVersion protocolVersion) {
+    if (bytes == null) return null;
+    try {
+      return objectMapper.readValue(Bytes.getArray(bytes), toJacksonJavaType());
+    } catch (IOException e) {
+      throw new InvalidTypeException(e.getMessage(), e);
+    }
+  }
+
+  @NonNull
+  @Override
+  public String format(T value) {
+    if (value == null) return ""NULL"";
+    String json;
+    try {
+      json = objectMapper.writeValueAsString(value);
+    } catch (IOException e) {
+      throw new InvalidTypeException(e.getMessage(), e);
+    }
+    return Strings.quote(json);
+  }
+
+  @Nullable
+  @Override
+  @SuppressWarnings(""unchecked"")
+  public T parse(String value) {
+    if (value == null || value.isEmpty() || value.equalsIgnoreCase(""NULL"")) return null;
+    if (!Strings.isQuoted(value))
+      throw new InvalidTypeException(""JSON strings must be enclosed by single quotes"");
+    String json = Strings.unquote(value);
+    try {
+      return (T) objectMapper.readValue(json, toJacksonJavaType());
+    } catch (IOException e) {
+      throw new InvalidTypeException(e.getMessage(), e);
+    }
+  }
+
+  /**
+   * This method acts as a bridge between Guava's {@link","[{'comment': 'Change to ""This method acts as a bridge between the driver\'s {@link com.datastax.oss.driver.api.core.type.reflect.GenericType GenericType} API and Jackson\'s {@link JavaType} API.', 'commenter': 'adutra'}]"
1208,driver-examples/src/main/java/com/datastax/driver/examples/json/codecs/Jsr353JsonCodec.java,"@@ -0,0 +1,193 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.driver.examples.json.codecs;
+
+import com.datastax.driver.examples.json.exceptions.InvalidTypeException;
+import com.datastax.oss.driver.api.core.ProtocolVersion;
+import com.datastax.oss.driver.api.core.type.DataType;
+import com.datastax.oss.driver.api.core.type.DataTypes;
+import com.datastax.oss.driver.api.core.type.codec.TypeCodec;
+import com.datastax.oss.driver.api.core.type.reflect.GenericType;
+import com.datastax.oss.driver.internal.core.util.Strings;
+import com.datastax.oss.protocol.internal.util.Bytes;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import edu.umd.cs.findbugs.annotations.Nullable;
+import java.io.ByteArrayInputStream;
+import java.io.ByteArrayOutputStream;
+import java.io.IOException;
+import java.io.StringReader;
+import java.io.StringWriter;
+import java.nio.ByteBuffer;
+import java.util.Map;
+import javax.json.Json;
+import javax.json.JsonArray;
+import javax.json.JsonException;
+import javax.json.JsonObject;
+import javax.json.JsonReader;
+import javax.json.JsonReaderFactory;
+import javax.json.JsonStructure;
+import javax.json.JsonWriter;
+import javax.json.JsonWriterFactory;
+
+/**
+ * A JSON codec that uses the <a href=""https://jcp.org/en/jsr/detail?id=353"">Java API for JSON
+ * processing</a> to perform serialization and deserialization of JSON structures.
+ *
+ * <p>More specifically, this codec maps an arbitrary {@link JsonStructure} to a CQL {@code varchar}
+ * column.
+ *
+ * <p>This codec handles the Java type {@link JsonStructure}. It is therefore required that values
+ * are set and retrieved using that exact Java type; users should manually downcast to either {@link
+ * JsonObject} or {@link JsonArray}, as in the example below:
+ *
+ * <pre>{@code
+ * // setting values
+ * JsonObject myObject = ...
+ * PreparedStatement ps = ...
+ * // set values using JsonStructure as target Java type
+ * BoundStatement bs = ps.bind().set(1, myObject, JsonStructure.class);
+ *
+ * // retrieving values
+ * Row row = session.execute(bs).one();
+ * // use JsonStructure as target Java type to retrieve values
+ * JsonStructure json = row.get(0, JsonStructure.class);
+ * if (json instanceof JsonObject) {
+ *     myObject = (JsonObject) json;
+ *     ...
+ * }
+ * }</pre>
+ *
+ * <p>Note that at runtime, this codec requires the presence of both JSR-353 API and a
+ * JSR-353-compatible runtime library, such as <a
+ * href=""https://jsonp.java.net/download.html"">JSR-353's reference implementation</a>. If you use
+ * Maven, this can be done by declaring the following dependencies in your project:
+ *
+ * <pre>{@code
+ * <dependency>
+ *   <groupId>javax.json</groupId>
+ *   <artifactId>javax.json-api</artifactId>
+ *   <version>1.0</version>
+ * </dependency>
+ *
+ * <dependency>
+ *   <groupId>org.glassfish</groupId>
+ *   <artifactId>javax.json</artifactId>
+ *   <version>1.0.4</version>","[{'comment': 'Last version is 1.1.4.', 'commenter': 'adutra'}]"
1208,driver-examples/src/main/java/com/datastax/driver/examples/json/codecs/Jsr353JsonCodec.java,"@@ -0,0 +1,193 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.driver.examples.json.codecs;
+
+import com.datastax.driver.examples.json.exceptions.InvalidTypeException;
+import com.datastax.oss.driver.api.core.ProtocolVersion;
+import com.datastax.oss.driver.api.core.type.DataType;
+import com.datastax.oss.driver.api.core.type.DataTypes;
+import com.datastax.oss.driver.api.core.type.codec.TypeCodec;
+import com.datastax.oss.driver.api.core.type.reflect.GenericType;
+import com.datastax.oss.driver.internal.core.util.Strings;
+import com.datastax.oss.protocol.internal.util.Bytes;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import edu.umd.cs.findbugs.annotations.Nullable;
+import java.io.ByteArrayInputStream;
+import java.io.ByteArrayOutputStream;
+import java.io.IOException;
+import java.io.StringReader;
+import java.io.StringWriter;
+import java.nio.ByteBuffer;
+import java.util.Map;
+import javax.json.Json;
+import javax.json.JsonArray;
+import javax.json.JsonException;
+import javax.json.JsonObject;
+import javax.json.JsonReader;
+import javax.json.JsonReaderFactory;
+import javax.json.JsonStructure;
+import javax.json.JsonWriter;
+import javax.json.JsonWriterFactory;
+
+/**
+ * A JSON codec that uses the <a href=""https://jcp.org/en/jsr/detail?id=353"">Java API for JSON
+ * processing</a> to perform serialization and deserialization of JSON structures.
+ *
+ * <p>More specifically, this codec maps an arbitrary {@link JsonStructure} to a CQL {@code varchar}
+ * column.
+ *
+ * <p>This codec handles the Java type {@link JsonStructure}. It is therefore required that values
+ * are set and retrieved using that exact Java type; users should manually downcast to either {@link
+ * JsonObject} or {@link JsonArray}, as in the example below:
+ *
+ * <pre>{@code
+ * // setting values
+ * JsonObject myObject = ...
+ * PreparedStatement ps = ...
+ * // set values using JsonStructure as target Java type
+ * BoundStatement bs = ps.bind().set(1, myObject, JsonStructure.class);
+ *
+ * // retrieving values
+ * Row row = session.execute(bs).one();
+ * // use JsonStructure as target Java type to retrieve values
+ * JsonStructure json = row.get(0, JsonStructure.class);
+ * if (json instanceof JsonObject) {
+ *     myObject = (JsonObject) json;
+ *     ...
+ * }
+ * }</pre>
+ *
+ * <p>Note that at runtime, this codec requires the presence of both JSR-353 API and a
+ * JSR-353-compatible runtime library, such as <a
+ * href=""https://jsonp.java.net/download.html"">JSR-353's reference implementation</a>. If you use
+ * Maven, this can be done by declaring the following dependencies in your project:
+ *
+ * <pre>{@code
+ * <dependency>
+ *   <groupId>javax.json</groupId>
+ *   <artifactId>javax.json-api</artifactId>
+ *   <version>1.0</version>
+ * </dependency>
+ *
+ * <dependency>
+ *   <groupId>org.glassfish</groupId>
+ *   <artifactId>javax.json</artifactId>
+ *   <version>1.0.4</version>
+ * </dependency>
+ * }</pre>
+ */
+public class Jsr353JsonCodec implements TypeCodec<JsonStructure> {
+
+  private final JsonReaderFactory readerFactory;
+
+  private final JsonWriterFactory writerFactory;
+
+  /** Creates a new instance using a default configuration. */
+  public Jsr353JsonCodec() {
+    this(null);
+  }
+
+  /**
+   * Creates a new instance using the provided configuration.
+   *
+   * @param config A map of provider-specific configuration properties. May be empty or {@code
+   *     null}.
+   */
+  public Jsr353JsonCodec(Map<String, ?> config) {
+    readerFactory = Json.createReaderFactory(config);
+    writerFactory = Json.createWriterFactory(config);
+  }
+
+  @NonNull
+  @Override
+  public GenericType<JsonStructure> getJavaType() {
+    return GenericType.of(JsonStructure.class);
+  }
+
+  @NonNull
+  @Override
+  public DataType getCqlType() {
+    return DataTypes.TEXT;
+  }
+
+  @Nullable
+  @Override
+  public ByteBuffer encode(
+      @Nullable JsonStructure value, @NonNull ProtocolVersion protocolVersion) {
+    if (value == null) return null;","[{'comment': 'In driver 4 we always add braces to if/for/while blocks.', 'commenter': 'adutra'}]"
1208,driver-examples/src/main/java/com/datastax/driver/examples/json/jackson/JacksonJsonColumn.java,"@@ -0,0 +1,162 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.driver.examples.json.jackson;
+
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.bindMarker;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.insertInto;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.literal;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.selectFrom;
+
+import com.datastax.driver.examples.json.PlainTextJson;
+import com.datastax.driver.examples.json.codecs.JacksonJsonCodec;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.CqlSessionBuilder;
+import com.datastax.oss.driver.api.core.cql.PreparedStatement;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import com.datastax.oss.driver.api.core.cql.Row;
+import com.datastax.oss.driver.api.core.cql.Statement;
+import com.datastax.oss.driver.api.core.type.codec.TypeCodec;
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonProperty;
+
+/**
+ * Illustrates how to map a single table column of type {@code VARCHAR}, containing JSON payloads,
+ * into a Java object using the <a href=""http://wiki.fasterxml.com/JacksonHome"">Jackson</a> library.
+ *
+ * <p>This example makes usage of a custom {@link TypeCodec codec}, {@link JacksonJsonCodec}, which
+ * is implemented in the driver-examples module. If you plan to follow this example, make sure to
+ * include the following Maven dependencies in your project:
+ *
+ * <pre>{@code
+ * <dependency>
+ *     <groupId>com.fasterxml.jackson.core</groupId>
+ *     <artifactId>jackson-databind</artifactId>
+ *     <version>${jackson.version}</version>","[{'comment': ""I don't think this will get resolved by Maven, I suggest that you use a raw version number, if possible the latest version available."", 'commenter': 'adutra'}]"
1208,driver-examples/src/main/java/com/datastax/driver/examples/json/jackson/JacksonJsonFunction.java,"@@ -0,0 +1,216 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.driver.examples.json.jackson;
+
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.bindMarker;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.function;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.insertInto;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.literal;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.selectFrom;
+
+import com.datastax.driver.examples.json.PlainTextJson;
+import com.datastax.driver.examples.json.codecs.JacksonJsonCodec;
+import com.datastax.oss.driver.api.core.CqlIdentifier;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.CqlSessionBuilder;
+import com.datastax.oss.driver.api.core.cql.PreparedStatement;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import com.datastax.oss.driver.api.core.cql.Row;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.api.core.cql.Statement;
+import com.datastax.oss.driver.api.core.type.codec.TypeCodec;
+import com.datastax.oss.driver.api.querybuilder.select.Selector;
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.databind.JsonNode;
+import com.fasterxml.jackson.databind.node.JsonNodeFactory;
+import com.fasterxml.jackson.databind.node.ObjectNode;
+
+/**
+ * Illustrates how to map a single table column of an arbitrary type to a Java object using the <a
+ * href=""http://wiki.fasterxml.com/JacksonHome"">Jackson</a> library, and leveraging the {@code
+ * toJson()} and {@code fromJson()} functions introduced in Cassandra 2.2.
+ *
+ * <p>This example makes usage of a custom {@link TypeCodec codec}, {@link JacksonJsonCodec}. If you
+ * plan to follow this example, make sure to include the following Maven dependencies in your
+ * project:
+ *
+ * <p><dependency> <groupId>com.fasterxml.jackson.core</groupId>
+ * <artifactId>jackson-databind</artifactId> <version>${jackson.version}</version> </dependency> }","[{'comment': '1. Formatting is deformed\r\n2. Replace `${jackson.version}` with an actual version.', 'commenter': 'adutra'}]"
1208,driver-examples/src/main/java/com/datastax/driver/examples/json/jackson/JacksonJsonFunction.java,"@@ -0,0 +1,216 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.driver.examples.json.jackson;
+
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.bindMarker;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.function;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.insertInto;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.literal;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.selectFrom;
+
+import com.datastax.driver.examples.json.PlainTextJson;
+import com.datastax.driver.examples.json.codecs.JacksonJsonCodec;
+import com.datastax.oss.driver.api.core.CqlIdentifier;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.CqlSessionBuilder;
+import com.datastax.oss.driver.api.core.cql.PreparedStatement;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import com.datastax.oss.driver.api.core.cql.Row;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.api.core.cql.Statement;
+import com.datastax.oss.driver.api.core.type.codec.TypeCodec;
+import com.datastax.oss.driver.api.querybuilder.select.Selector;
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.databind.JsonNode;
+import com.fasterxml.jackson.databind.node.JsonNodeFactory;
+import com.fasterxml.jackson.databind.node.ObjectNode;
+
+/**
+ * Illustrates how to map a single table column of an arbitrary type to a Java object using the <a
+ * href=""http://wiki.fasterxml.com/JacksonHome"">Jackson</a> library, and leveraging the {@code
+ * toJson()} and {@code fromJson()} functions introduced in Cassandra 2.2.
+ *
+ * <p>This example makes usage of a custom {@link TypeCodec codec}, {@link JacksonJsonCodec}. If you
+ * plan to follow this example, make sure to include the following Maven dependencies in your
+ * project:
+ *
+ * <p><dependency> <groupId>com.fasterxml.jackson.core</groupId>
+ * <artifactId>jackson-databind</artifactId> <version>${jackson.version}</version> </dependency> }
+ * </pre>
+ *
+ * This example also uses the {@link com.datastax.oss.driver.api.querybuilder.QueryBuilder
+ * QueryBuilder}; for examples using the ""core"" API, see {@link PlainTextJson} (they are easily
+ * translatable to the queries in this class).
+ *
+ * <p>Preconditions: - a Cassandra cluster is running and accessible through the contacts points
+ * identified by basic.contact-points (see application.conf);
+ *
+ * <p>Side effects: - creates a new keyspace ""examples"" in the cluster. If a keyspace with this name
+ * already exists, it will be reused; - creates a user-defined type (UDT)
+ * ""examples.json_jackson_function_user"". If it already exists, it will be reused; - creates a table
+ * ""examples.json_jackson_function"". If it already exists, it will be reused; - inserts data in the
+ * table.
+ *
+ * @see <a href=""http://www.datastax.com/dev/blog/whats-new-in-cassandra-2-2-json-support"">Whatâ€™s
+ *     New in Cassandra 2.2: JSON Support</a>
+ */
+public class JacksonJsonFunction {
+
+  // A codec to convert JSON payloads into User instances;
+  private static final TypeCodec<User> USER_CODEC = new JacksonJsonCodec<>(User.class);
+  // A codec to convert generic JSON payloads into JsonNode instances
+","[{'comment': 'Spurious empty line between comment and field declaration.', 'commenter': 'adutra'}]"
1208,driver-examples/src/main/java/com/datastax/driver/examples/json/jackson/JacksonJsonRow.java,"@@ -0,0 +1,160 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.driver.examples.json.jackson;
+
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.bindMarker;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.insertInto;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.literal;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.selectFrom;
+
+import com.datastax.driver.examples.json.PlainTextJson;
+import com.datastax.driver.examples.json.codecs.JacksonJsonCodec;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.CqlSessionBuilder;
+import com.datastax.oss.driver.api.core.cql.PreparedStatement;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import com.datastax.oss.driver.api.core.cql.Row;
+import com.datastax.oss.driver.api.core.cql.Statement;
+import com.datastax.oss.driver.api.core.type.codec.TypeCodec;
+import com.fasterxml.jackson.annotation.JsonCreator;
+import com.fasterxml.jackson.annotation.JsonProperty;
+
+/**
+ * Illustrates how to map an entire table row to a Java object using the <a
+ * href=""http://wiki.fasterxml.com/JacksonHome"">Jackson</a> library, and leveraging the {@code
+ * SELECT JSON} and {@code INSERT JSON} syntaxes introduced in Cassandra 2.2.
+ *
+ * <p>This example makes usage of a custom {@link TypeCodec codec}, {@link JacksonJsonCodec}. If you
+ * plan to follow this example, make sure to include the following Maven dependencies in your
+ * project:
+ *
+ * <p><dependency> <groupId>com.fasterxml.jackson.core</groupId>","[{'comment': 'Same here, formatting issue + version issue.', 'commenter': 'adutra'}]"
1215,query-builder/src/main/java/com/datastax/oss/driver/api/querybuilder/insert/InsertInto.java,"@@ -40,15 +38,6 @@
   /**
    * Makes this statement an INSERT JSON with a custom type mapping. The provided {@code Object
    * value} will be mapped to a JSON string.
-   *
-   * <p>This is an alternative to {@link #json(String)} for custom type mappings. The provided
-   * registry should contain a codec that can format the value. Typically, this will be your
-   * session's registry, which is accessible via {@code session.getContext().getCodecRegistry()}.
-   *
-   * @throws CodecNotFoundException if {@code codecRegistry} does not contain any codec that can
-   *     handle {@code value}.
-   * @see DriverContext#getCodecRegistry()
-   * @see DefaultInsert#json(Object, CodecRegistry)","[{'comment': 'Why remove so much? I think you could remove only the offending line: `@see DefaultInsert#json(Object, CodecRegistry)`', 'commenter': 'adutra'}, {'comment': 'Ok, I thought that all `@see` and `code` are problematic. Reverted', 'commenter': 'tomekl007'}]"
1220,core/src/main/java/com/datastax/oss/driver/api/core/config/DriverConfigLoader.java,"@@ -16,14 +16,123 @@
 package com.datastax.oss.driver.api.core.config;
 
 import com.datastax.oss.driver.api.core.context.DriverContext;
+import com.datastax.oss.driver.api.core.session.SessionBuilder;
+import com.datastax.oss.driver.internal.core.config.typesafe.DefaultDriverConfigLoader;
+import com.typesafe.config.Config;
+import com.typesafe.config.ConfigFactory;
 import edu.umd.cs.findbugs.annotations.NonNull;
+import java.io.File;
+import java.net.URL;
 import java.util.concurrent.CompletionStage;
 
 /**
  * Manages the initialization, and optionally the periodic reloading, of the driver configuration.
+ *
+ * @see SessionBuilder#withConfigLoader(DriverConfigLoader)
  */
 public interface DriverConfigLoader extends AutoCloseable {
 
+  /**
+   * Builds an instance using the driver's default implementation (based on Typesafe config), except
+   * that application-specific options are loaded from a classpath resource with a custom name.
+   *
+   * <p>More precisely, configuration properties are loaded and merged from the following
+   * (first-listed are higher priority):
+   *
+   * <ul>
+   *   <li>system properties
+   *   <li>{@code <resourceBaseName>.conf} (all resources on classpath with this name)
+   *   <li>{@code <resourceBaseName>.json} (all resources on classpath with this name)
+   *   <li>{@code <resourceBaseName>.properties} (all resources on classpath with this name)
+   *   <li>{@code reference.conf} (all resources on classpath with this name). In particular, this","[{'comment': ""What will happen if the user will create a file `reference.conf` and put it on it's classpath?"", 'commenter': 'tomekl007'}, {'comment': ""The two files are merged, the order of precedence is undefined.\r\n\r\nThere is no reason for a client application to do that. One valid case where it might happen is if another dependency is using Typesafe config, but in that case the options won't collide thanks to the `datastax-java-driver` prefix."", 'commenter': 'olim7t'}]"
1220,core/src/main/java/com/datastax/oss/driver/api/core/config/DriverConfigLoader.java,"@@ -16,14 +16,123 @@
 package com.datastax.oss.driver.api.core.config;
 
 import com.datastax.oss.driver.api.core.context.DriverContext;
+import com.datastax.oss.driver.api.core.session.SessionBuilder;
+import com.datastax.oss.driver.internal.core.config.typesafe.DefaultDriverConfigLoader;
+import com.typesafe.config.Config;
+import com.typesafe.config.ConfigFactory;
 import edu.umd.cs.findbugs.annotations.NonNull;
+import java.io.File;
+import java.net.URL;
 import java.util.concurrent.CompletionStage;
 
 /**
  * Manages the initialization, and optionally the periodic reloading, of the driver configuration.
+ *
+ * @see SessionBuilder#withConfigLoader(DriverConfigLoader)
  */
 public interface DriverConfigLoader extends AutoCloseable {
 
+  /**
+   * Builds an instance using the driver's default implementation (based on Typesafe config), except
+   * that application-specific options are loaded from a classpath resource with a custom name.
+   *
+   * <p>More precisely, configuration properties are loaded and merged from the following
+   * (first-listed are higher priority):
+   *
+   * <ul>
+   *   <li>system properties
+   *   <li>{@code <resourceBaseName>.conf} (all resources on classpath with this name)
+   *   <li>{@code <resourceBaseName>.json} (all resources on classpath with this name)
+   *   <li>{@code <resourceBaseName>.properties} (all resources on classpath with this name)
+   *   <li>{@code reference.conf} (all resources on classpath with this name). In particular, this
+   *       will load the {@code reference.conf} included in the core driver JAR, that defines
+   *       default options for all mandatory options.
+   * </ul>
+   *
+   * The resulting configuration is expected to contain a {@code datastax-java-driver} section.
+   *
+   * <p>The returned loader will honor the reload interval defined by the option {@code
+   * basic.config-reload-interval}.
+   */
+  static DriverConfigLoader fromClasspath(String resourceBaseName) {
+    return new DefaultDriverConfigLoader(
+        () -> {
+          ConfigFactory.invalidateCaches();","[{'comment': 'Why we need that step?', 'commenter': 'tomekl007'}, {'comment': 'For periodic config reloading. Otherwise Typesafe Config caches the values and does not detect changes to the file.', 'commenter': 'olim7t'}]"
1220,core/src/main/java/com/datastax/oss/driver/api/core/config/DriverConfigLoader.java,"@@ -16,14 +16,123 @@
 package com.datastax.oss.driver.api.core.config;
 
 import com.datastax.oss.driver.api.core.context.DriverContext;
+import com.datastax.oss.driver.api.core.session.SessionBuilder;
+import com.datastax.oss.driver.internal.core.config.typesafe.DefaultDriverConfigLoader;
+import com.typesafe.config.Config;
+import com.typesafe.config.ConfigFactory;
 import edu.umd.cs.findbugs.annotations.NonNull;
+import java.io.File;
+import java.net.URL;
 import java.util.concurrent.CompletionStage;
 
 /**
  * Manages the initialization, and optionally the periodic reloading, of the driver configuration.
+ *
+ * @see SessionBuilder#withConfigLoader(DriverConfigLoader)
  */
 public interface DriverConfigLoader extends AutoCloseable {
 
+  /**
+   * Builds an instance using the driver's default implementation (based on Typesafe config), except
+   * that application-specific options are loaded from a classpath resource with a custom name.
+   *
+   * <p>More precisely, configuration properties are loaded and merged from the following
+   * (first-listed are higher priority):
+   *
+   * <ul>
+   *   <li>system properties
+   *   <li>{@code <resourceBaseName>.conf} (all resources on classpath with this name)
+   *   <li>{@code <resourceBaseName>.json} (all resources on classpath with this name)
+   *   <li>{@code <resourceBaseName>.properties} (all resources on classpath with this name)
+   *   <li>{@code reference.conf} (all resources on classpath with this name). In particular, this
+   *       will load the {@code reference.conf} included in the core driver JAR, that defines
+   *       default options for all mandatory options.
+   * </ul>
+   *
+   * The resulting configuration is expected to contain a {@code datastax-java-driver} section.
+   *
+   * <p>The returned loader will honor the reload interval defined by the option {@code
+   * basic.config-reload-interval}.
+   */
+  static DriverConfigLoader fromClasspath(String resourceBaseName) {
+    return new DefaultDriverConfigLoader(
+        () -> {
+          ConfigFactory.invalidateCaches();
+          Config config =
+              ConfigFactory.defaultOverrides()
+                  .withFallback(ConfigFactory.parseResourcesAnySyntax(resourceBaseName))
+                  .withFallback(ConfigFactory.defaultReference())
+                  .resolve();
+          return config.getConfig(""datastax-java-driver"");
+        });
+  }
+
+  /**
+   * Builds an instance using the driver's default implementation (based on Typesafe config), except
+   * that application-specific options are loaded from the given file.
+   *
+   * <p>More precisely, configuration properties are loaded and merged from the following
+   * (first-listed are higher priority):
+   *
+   * <ul>
+   *   <li>system properties
+   *   <li>the contents of {@code file}
+   *   <li>{@code reference.conf} (all resources on classpath with this name). In particular, this
+   *       will load the {@code reference.conf} included in the core driver JAR, that defines
+   *       default options for all mandatory options.
+   * </ul>
+   *
+   * The resulting configuration is expected to contain a {@code datastax-java-driver} section.
+   *
+   * <p>The returned loader will honor the reload interval defined by the option {@code
+   * basic.config-reload-interval}.
+   */
+  static DriverConfigLoader fromFile(File file) {
+    return new DefaultDriverConfigLoader(
+        () -> {
+          ConfigFactory.invalidateCaches();
+          Config config =
+              ConfigFactory.defaultOverrides()
+                  .withFallback(ConfigFactory.parseFileAnySyntax(file))
+                  .withFallback(ConfigFactory.defaultReference())
+                  .resolve();
+          return config.getConfig(""datastax-java-driver"");
+        });
+  }
+
+  /**
+   * Builds an instance using the driver's default implementation (based on Typesafe config), except
+   * that application-specific options are loaded from the given URL.
+   *
+   * <p>More precisely, configuration properties are loaded and merged from the following
+   * (first-listed are higher priority):
+   *
+   * <ul>
+   *   <li>system properties
+   *   <li>the contents of {@code url}
+   *   <li>{@code reference.conf} (all resources on classpath with this name). In particular, this
+   *       will load the {@code reference.conf} included in the core driver JAR, that defines
+   *       default options for all mandatory options.
+   * </ul>
+   *
+   * The resulting configuration is expected to contain a {@code datastax-java-driver} section.
+   *
+   * <p>The returned loader will honor the reload interval defined by the option {@code
+   * basic.config-reload-interval}.
+   */
+  static DriverConfigLoader fromUrl(URL url) {","[{'comment': 'Shouldn\'t we have one generic method that is taking, for example, a `File`? Then methods like `fromUrl` or `fromClasspath` could only load a file using url or string and call the generic method that operates on file. That\'s why we will not need to have 3 almost identical methods.\r\n\r\nOr another solution: we could create a generic method that takes a supplier function:\r\n```\r\n static DriverConfigLoader from(Supplier<Config> fallbackSupplier ) {\r\n    return new DefaultDriverConfigLoader(\r\n        () -> {\r\n          ConfigFactory.invalidateCaches();\r\n          Config config =\r\n              ConfigFactory.defaultOverrides()\r\n                  .withFallback(fallabackSupplier)\r\n                  .withFallback(ConfigFactory.defaultReference())\r\n                  .resolve();\r\n          return config.getConfig(""datastax-java-driver"");\r\n        });\r\n  }\r\n```\r\nAnd then call that method for every varian:\r\n`from(() -> ConfigFactory.parseURL(url))`, `from(() -> ConfigFactory.parseFileAnySyntax(file)`, ...', 'commenter': 'tomekl007'}, {'comment': ""> a generic method that takes a supplier function\r\n\r\nThat would work if it's a private helper (we don't want to leak `Config` through the public API). The only thing is that we can't have private methods in an interface, so it would have to live somewhere else. It's just a few lines of code duplicated so I thought it was not worth it."", 'commenter': 'olim7t'}]"
1220,core/src/test/resources/config/customApplication.conf,"@@ -0,0 +1,3 @@
+datastax-java-driver {
+  basic.request.timeout = 5 seconds
+}","[{'comment': 'I think that we should add a property that has exactly the same key in each of those files(but different value) and write a test that validates which value is the final one (and which was overridden)', 'commenter': 'tomekl007'}, {'comment': ""I'm not sure if Typesafe Config formally defines an order of precedence. It's very unlikely that someone would use multiple formats anyway. On that note, maybe I should even split the test into 3 ones using different names, but it's just convenient to do it all in one test."", 'commenter': 'olim7t'}]"
1220,core/src/main/java/com/datastax/oss/driver/api/core/session/SessionBuilder.java,"@@ -79,25 +79,23 @@
    * Sets the configuration loader to use.
    *
    * <p>If you don't call this method, the builder will use the default implementation, based on the
-   * Typesafe config library. More precisely:
+   * Typesafe config library. More precisely, configuration properties are loaded and merged from
+   * the following (first-listed are higher priority):
    *
    * <ul>
-   *   <li>configuration properties are loaded and merged from the following (first-listed are
-   *       higher priority):
-   *       <ul>
-   *         <li>system properties
-   *         <li>{@code application.conf} (all resources on classpath with this name)
-   *         <li>{@code application.json} (all resources on classpath with this name)
-   *         <li>{@code application.properties} (all resources on classpath with this name)
-   *         <li>{@code reference.conf} (all resources on classpath with this name)
-   *       </ul>
-   *   <li>the resulting configuration is expected to contain a {@code datastax-java-driver}
-   *       section.
-   *   <li>that section is validated against the {@link DefaultDriverOption core driver options}.
+   *   <li>system properties
+   *   <li>{@code application.conf} (all resources on classpath with this name)
+   *   <li>{@code application.json} (all resources on classpath with this name)
+   *   <li>{@code application.properties} (all resources on classpath with this name)
+   *   <li>{@code reference.conf} (all resources on classpath with this name). In particular, this
+   *       will load the {@code reference.conf} included in the core driver JAR, that defines
+   *       default options for all mandatory options.
    * </ul>
    *
-   * The core driver JAR includes a {@code reference.conf} file with sensible defaults for all
-   * mandatory options, except the contact points.
+   * The resulting configuration is expected to contain a {@code datastax-java-driver} section.
+   *
+   * <p>The returned loader will honor the reload interval defined by the option {@code","[{'comment': ""I think this paragraph doesn't apply here."", 'commenter': 'adutra'}, {'comment': 'Right, it should say ""the default loader"", it\'s not returned.', 'commenter': 'olim7t'}]"
1220,core/src/test/java/com/datastax/oss/driver/internal/core/config/typesafe/DefaultDriverConfigLoaderTest.java,"@@ -174,4 +178,48 @@ public void should_not_notify_from_manual_reload_if_config_has_not_changed() {
     verify(eventBus, never()).fire(ConfigChangeEvent.INSTANCE);
     assertThatStage(reloaded).isSuccess(changed -> assertThat(changed).isFalse());
   }
+
+  @Test
+  public void should_load_from_other_classpath_resource() {
+    DriverConfigLoader loader = DriverConfigLoader.fromClasspath(""config/customApplication"");
+    DriverConfig config = loader.getInitialConfig();
+    // From customApplication.conf:
+    assertThat(config.getDefaultProfile().getDuration(DefaultDriverOption.REQUEST_TIMEOUT))
+        .isEqualTo(Duration.ofSeconds(5));
+    // From customApplication.json:
+    assertThat(config.getDefaultProfile().getInt(DefaultDriverOption.REQUEST_PAGE_SIZE))
+        .isEqualTo(2000);
+    // From customApplication.properties:
+    assertThat(config.getDefaultProfile().getString(DefaultDriverOption.REQUEST_CONSISTENCY))
+        .isEqualTo(DefaultConsistencyLevel.ONE.name());
+    // From reference.conf:
+    assertThat(config.getDefaultProfile().getString(DefaultDriverOption.REQUEST_SERIAL_CONSISTENCY))
+        .isEqualTo(DefaultConsistencyLevel.SERIAL.name());
+  }
+
+  @Test
+  public void should_load_from_file() {
+    File file = new File(""src/test/resources/config/customApplication.conf"");
+    DriverConfigLoader loader = DriverConfigLoader.fromFile(file);
+    DriverConfig config = loader.getInitialConfig();
+    // From customApplication.json:
+    assertThat(config.getDefaultProfile().getDuration(DefaultDriverOption.REQUEST_TIMEOUT))
+        .isEqualTo(Duration.ofSeconds(5));
+    // From reference.conf:
+    assertThat(config.getDefaultProfile().getString(DefaultDriverOption.REQUEST_SERIAL_CONSISTENCY))
+        .isEqualTo(DefaultConsistencyLevel.SERIAL.name());
+  }
+
+  @Test
+  public void should_load_from_url() throws Exception {
+    URL url = new File(""src/test/resources/config/customApplication.conf"").toURI().toURL();","[{'comment': 'From what I saw in the TypeSafe code this is not really exercising URLs because file URLs are special-cased, but not sure if we really want to got as far as creating other types of URLs for tests. Maybe we could create a special `URLStreamHandler` for a `foo` scheme? Or use WireMock?', 'commenter': 'adutra'}, {'comment': ""> From what I saw in the TypeSafe code this is not really exercising URLs\r\n\r\nGood catch.\r\nYeah I'm not sure if it's worth starting a web container from the test just for this. It's not even our code. I think I'll just do a quick manual test to validate that it works, and then we can trust [Config's own tests](https://github.com/lightbend/config/blob/master/config/src/test/scala/com/typesafe/config/impl/HttpTest.scala)."", 'commenter': 'olim7t'}]"
1220,core/src/test/java/com/datastax/oss/driver/internal/core/config/typesafe/DefaultDriverConfigLoaderTest.java,"@@ -174,4 +178,48 @@ public void should_not_notify_from_manual_reload_if_config_has_not_changed() {
     verify(eventBus, never()).fire(ConfigChangeEvent.INSTANCE);
     assertThatStage(reloaded).isSuccess(changed -> assertThat(changed).isFalse());
   }
+
+  @Test
+  public void should_load_from_other_classpath_resource() {
+    DriverConfigLoader loader = DriverConfigLoader.fromClasspath(""config/customApplication"");
+    DriverConfig config = loader.getInitialConfig();
+    // From customApplication.conf:
+    assertThat(config.getDefaultProfile().getDuration(DefaultDriverOption.REQUEST_TIMEOUT))
+        .isEqualTo(Duration.ofSeconds(5));
+    // From customApplication.json:
+    assertThat(config.getDefaultProfile().getInt(DefaultDriverOption.REQUEST_PAGE_SIZE))
+        .isEqualTo(2000);
+    // From customApplication.properties:
+    assertThat(config.getDefaultProfile().getString(DefaultDriverOption.REQUEST_CONSISTENCY))
+        .isEqualTo(DefaultConsistencyLevel.ONE.name());
+    // From reference.conf:
+    assertThat(config.getDefaultProfile().getString(DefaultDriverOption.REQUEST_SERIAL_CONSISTENCY))
+        .isEqualTo(DefaultConsistencyLevel.SERIAL.name());
+  }
+
+  @Test
+  public void should_load_from_file() {
+    File file = new File(""src/test/resources/config/customApplication.conf"");
+    DriverConfigLoader loader = DriverConfigLoader.fromFile(file);
+    DriverConfig config = loader.getInitialConfig();
+    // From customApplication.json:","[{'comment': 'You probably meant `customApplication.conf`.', 'commenter': 'adutra'}, {'comment': 'ðŸ‘ , I fixed it in a later commit.', 'commenter': 'olim7t'}]"
1220,core/src/test/java/com/datastax/oss/driver/internal/core/config/typesafe/DefaultDriverConfigLoaderTest.java,"@@ -174,4 +178,48 @@ public void should_not_notify_from_manual_reload_if_config_has_not_changed() {
     verify(eventBus, never()).fire(ConfigChangeEvent.INSTANCE);
     assertThatStage(reloaded).isSuccess(changed -> assertThat(changed).isFalse());
   }
+
+  @Test
+  public void should_load_from_other_classpath_resource() {
+    DriverConfigLoader loader = DriverConfigLoader.fromClasspath(""config/customApplication"");
+    DriverConfig config = loader.getInitialConfig();
+    // From customApplication.conf:
+    assertThat(config.getDefaultProfile().getDuration(DefaultDriverOption.REQUEST_TIMEOUT))
+        .isEqualTo(Duration.ofSeconds(5));
+    // From customApplication.json:
+    assertThat(config.getDefaultProfile().getInt(DefaultDriverOption.REQUEST_PAGE_SIZE))
+        .isEqualTo(2000);
+    // From customApplication.properties:
+    assertThat(config.getDefaultProfile().getString(DefaultDriverOption.REQUEST_CONSISTENCY))
+        .isEqualTo(DefaultConsistencyLevel.ONE.name());
+    // From reference.conf:
+    assertThat(config.getDefaultProfile().getString(DefaultDriverOption.REQUEST_SERIAL_CONSISTENCY))
+        .isEqualTo(DefaultConsistencyLevel.SERIAL.name());
+  }
+
+  @Test
+  public void should_load_from_file() {
+    File file = new File(""src/test/resources/config/customApplication.conf"");
+    DriverConfigLoader loader = DriverConfigLoader.fromFile(file);
+    DriverConfig config = loader.getInitialConfig();
+    // From customApplication.json:
+    assertThat(config.getDefaultProfile().getDuration(DefaultDriverOption.REQUEST_TIMEOUT))
+        .isEqualTo(Duration.ofSeconds(5));
+    // From reference.conf:
+    assertThat(config.getDefaultProfile().getString(DefaultDriverOption.REQUEST_SERIAL_CONSISTENCY))
+        .isEqualTo(DefaultConsistencyLevel.SERIAL.name());
+  }
+
+  @Test
+  public void should_load_from_url() throws Exception {
+    URL url = new File(""src/test/resources/config/customApplication.conf"").toURI().toURL();
+    DriverConfigLoader loader = DriverConfigLoader.fromUrl(url);
+    DriverConfig config = loader.getInitialConfig();
+    // From customApplication.json:","[{'comment': 'Same here, You probably meant `customApplication.conf`.', 'commenter': 'adutra'}]"
1220,core/src/main/java/com/datastax/oss/driver/api/core/config/DriverConfigLoader.java,"@@ -133,6 +134,51 @@ static DriverConfigLoader fromUrl(URL url) {
         });
   }
 
+  /**
+   * Starts a builder that allows configuration options to be overridden programmatically.
+   *
+   * <p>Sample usage:
+   *
+   * <pre>{@code
+   * DriverConfigLoader loader =
+   *     DriverConfigLoader.programmaticBuilder()
+   *         .withDuration(DefaultDriverOption.REQUEST_TIMEOUT, Duration.ofSeconds(5))
+   *         .startProfile(""slow"")
+   *         .withDuration(DefaultDriverOption.REQUEST_TIMEOUT, Duration.ofSeconds(30))
+   *         .endProfile()
+   *         .build();
+   * }</pre>
+   *
+   * The resulting loader still uses the driver's default implementation (based on Typesafe config),
+   * except that the programmatic configuration takes precedence. More precisely, configuration
+   * properties are loaded and merged from the following (first-listed are higher priority):
+   *
+   * <ul>
+   *   <li>system properties
+   *   <li>properties that were provided programmatically
+   *   <li>{@code application.conf} (all resources on classpath with this name)
+   *   <li>{@code application.json} (all resources on classpath with this name)
+   *   <li>{@code application.properties} (all resources on classpath with this name)
+   *   <li>{@code reference.conf} (all resources on classpath with this name). In particular, this
+   *       will load the {@code reference.conf} included in the core driver JAR, that defines
+   *       default options for all mandatory options.
+   * </ul>
+   *
+   * Note that {@code application.*} is entirely optional, you may choose to only rely on the
+   * driver's built-in {@code reference.conf} and programmatic overrides.
+   *
+   * <p>The resulting configuration is expected to contain a {@code datastax-java-driver} section.
+   *
+   * <p>The loader will honor the reload interval defined by the option {@code
+   * basic.config-reload-interval}.
+   *
+   * <p>Note that the returned builder is <b>not thread-safe</b>.
+   */
+  @NonNull
+  static ProgrammaticConfigBuilder programmaticBuilder() {","[{'comment': ""1. How can users combine JAVA-2205 and JAVA-2201, e.g. by using the programmatic API _and_ loading from a different classpath resource name? Is the idea to use `new DefaultProgrammaticConfigBuilder(Supplier<Config>, String)`?\r\n2. Why not follow similar patterns in the code base and call this  interface `DriverConfigLoaderBuilder` (or maybe `ProgrammaticDriverConfigLoaderBuilder`)? Similarly, shouldn't this method be called just `builder()`?"", 'commenter': 'adutra'}, {'comment': '1. There\'s only so much we can do without leaking Typesafe config in the public API. I\'ve tried to pick what seemed to be the most generic cases, but yes if you want programmatic _and_ URL-based it\'s not possible to compose, you\'ll have to switch to the internal API (and indeed use that constructor).\r\n2. When we did the internal version there was a debate and we compromised on a crappy name. I want ""programmatic"" in the name (type and method) because that\'s what it is: the thing where you set options from your code instead of loading them from a file. Formally it should be `ProgrammaticDriverConfigLoaderBuilder` indeed, but that\'s a mouthful. So I shortened it to keep the bare minimum: it\'s programmatic, it relates to the config, and it\'s a builder. The fact that it produces a loader is implied by the return type of `build()`. I\'m not hellbent on that, if you think it\'s not clear we can go back to the longer name.', 'commenter': 'olim7t'}]"
1220,core/src/main/java/com/datastax/oss/driver/api/core/config/DriverExecutionProfile.java,"@@ -37,7 +37,7 @@
  *
  * @see DriverConfig
  */
-public interface DriverExecutionProfile {
+public interface DriverExecutionProfile extends OngoingConfigOptions<DriverExecutionProfile> {","[{'comment': 'This bothers me a bit. The methods that were pulled up have different behaviors depending on the implementation class:\r\n\r\n* `ProgrammaticConfigBuilder`: builder pattern applies, each invocation of these methods return the same object.\r\n* `DriverExecutionProfile`: each invocation returns a different object (dynamic profile creation).\r\n\r\nI am actually not sure that it is a good idea to group these methods in a common super-interface.', 'commenter': 'adutra'}, {'comment': ""What I like is that the list of all possible setters lives in a single interface. If we add a method on execution profiles, we won't have to remember to add it on the programmatic builder every time."", 'commenter': 'olim7t'}, {'comment': ""Which means that we are back to the same situation as in `BoundStatementBuilder` vs `BoundStatement`: you will have to annotate all the methods in this interface with `@CheckReturnValue` to warn people but it won't be necessary sometimes to re-assign the returned value. OK, I guess I can live with that."", 'commenter': 'adutra'}]"
1220,core/src/main/java/com/datastax/oss/driver/api/core/config/OngoingConfigOptions.java,"@@ -0,0 +1,71 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core.config;
+
+import edu.umd.cs.findbugs.annotations.NonNull;
+import java.time.Duration;
+import java.util.List;
+import java.util.Map;
+
+public interface OngoingConfigOptions<SelfT extends OngoingConfigOptions<SelfT>> {","[{'comment': '1. Missing javadocs.\r\n2. The name is a bit strange, but I guess you already explored all other options.', 'commenter': 'adutra'}]"
1220,core/src/main/java/com/datastax/oss/driver/internal/core/config/typesafe/DefaultDriverConfigLoader.java,"@@ -127,7 +127,11 @@ public void close() {
    * <p>In the general case, use of this is not recommended, but it may be useful in situations
    * where configuration must be defined at runtime or is derived from some other configuration
    * source.
+   *
+   * @deprecated this feature is now available in the public API. Use {@link
+   *     DriverConfigLoader#programmaticBuilder()} instead.
    */
+  @Deprecated","[{'comment': ""Can't we simply remove it, since it's an internal package?"", 'commenter': 'adutra'}, {'comment': 'Even though we allow ourselves to break the internal API, I think we should avoid it as much as possible.', 'commenter': 'olim7t'}]"
1220,core/src/main/java/com/datastax/oss/driver/internal/core/config/typesafe/DefaultProgrammaticConfigBuilder.java,"@@ -0,0 +1,225 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.config.typesafe;
+
+import com.datastax.oss.driver.api.core.config.DriverConfigLoader;
+import com.datastax.oss.driver.api.core.config.DriverExecutionProfile;
+import com.datastax.oss.driver.api.core.config.DriverOption;
+import com.datastax.oss.driver.api.core.config.ProgrammaticConfigBuilder;
+import com.datastax.oss.driver.shaded.guava.common.base.Preconditions;
+import com.datastax.oss.protocol.internal.util.collection.NullAllowingImmutableMap;
+import com.typesafe.config.Config;
+import com.typesafe.config.ConfigFactory;
+import com.typesafe.config.ConfigValueFactory;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import edu.umd.cs.findbugs.annotations.Nullable;
+import java.time.Duration;
+import java.util.List;
+import java.util.Map;
+import java.util.function.Supplier;
+import net.jcip.annotations.NotThreadSafe;
+
+@NotThreadSafe
+public class DefaultProgrammaticConfigBuilder implements ProgrammaticConfigBuilder {
+
+  public static final Supplier<Config> DEFAULT_FALLBACK_SUPPLIER = ConfigFactory::defaultReference;
+
+  private final NullAllowingImmutableMap.Builder<String, Object> values =
+      NullAllowingImmutableMap.builder();
+  private final Supplier<Config> fallbackSupplier;
+  private final String rootPath;
+
+  private String currentProfileName = DriverExecutionProfile.DEFAULT_NAME;
+
+  /**
+   * @param fallbackSupplier the supplier that will provide fallback configuration for options that
+   *     haven't been specified programmatically.
+   * @param rootPath the root path used in non-programmatic sources (fallback reference.conf and
+   *     system properties).
+   */
+  public DefaultProgrammaticConfigBuilder(Supplier<Config> fallbackSupplier, String rootPath) {
+    this.fallbackSupplier = fallbackSupplier;
+    this.rootPath = rootPath;
+  }
+
+  public DefaultProgrammaticConfigBuilder() {
+    this(DEFAULT_FALLBACK_SUPPLIER, ""datastax-java-driver"");
+  }
+
+  private ProgrammaticConfigBuilder with(@NonNull DriverOption option, @Nullable Object value) {
+    String path = option.getPath();
+    if (!DriverExecutionProfile.DEFAULT_NAME.equals(currentProfileName)) {
+      path = ""profiles."" + currentProfileName + ""."" + path;
+    }
+    if (!rootPath.isEmpty()) {
+      path = rootPath + ""."" + path;
+    }
+    return with(path, value);
+  }
+
+  private ProgrammaticConfigBuilder with(@NonNull String path, @Nullable Object value) {
+    values.put(path, value);
+    return this;
+  }
+
+  @NonNull
+  @Override
+  public ProgrammaticConfigBuilder startProfile(@NonNull String profileName) {
+    Preconditions.checkNotNull(profileName);
+    currentProfileName = profileName;
+    return this;
+  }
+
+  @NonNull
+  @Override
+  public ProgrammaticConfigBuilder endProfile() {
+    currentProfileName = DriverExecutionProfile.DEFAULT_NAME;
+    return this;
+  }
+
+  @NonNull
+  @Override
+  public ProgrammaticConfigBuilder withBoolean(@NonNull DriverOption option, boolean value) {
+    return with(option, value);
+  }
+
+  @NonNull
+  @Override
+  public ProgrammaticConfigBuilder withBooleanList(
+      @NonNull DriverOption option, @NonNull List<Boolean> value) {
+    return with(option, value);
+  }
+
+  @NonNull
+  @Override
+  public ProgrammaticConfigBuilder withInt(@NonNull DriverOption option, int value) {
+    return with(option, value);
+  }
+
+  @NonNull
+  @Override
+  public ProgrammaticConfigBuilder withIntList(
+      @NonNull DriverOption option, @NonNull List<Integer> value) {
+    return with(option, value);
+  }
+
+  @NonNull
+  @Override
+  public ProgrammaticConfigBuilder withLong(@NonNull DriverOption option, long value) {
+    return with(option, value);
+  }
+
+  @NonNull
+  @Override
+  public ProgrammaticConfigBuilder withLongList(
+      @NonNull DriverOption option, @NonNull List<Long> value) {
+    return with(option, value);
+  }
+
+  @NonNull
+  @Override
+  public ProgrammaticConfigBuilder withDouble(@NonNull DriverOption option, double value) {
+    return with(option, value);
+  }
+
+  @NonNull
+  @Override
+  public ProgrammaticConfigBuilder withDoubleList(
+      @NonNull DriverOption option, @NonNull List<Double> value) {
+    return with(option, value);
+  }
+
+  @NonNull
+  @Override
+  public ProgrammaticConfigBuilder withString(@NonNull DriverOption option, @NonNull String value) {
+    return with(option, value);
+  }
+
+  @NonNull
+  @Override
+  public ProgrammaticConfigBuilder withStringList(
+      @NonNull DriverOption option, @NonNull List<String> value) {
+    return with(option, value);
+  }
+
+  @SuppressWarnings(""unchecked"")","[{'comment': 'Unnecessary annotation.', 'commenter': 'adutra'}]"
1220,core/src/main/java/com/datastax/oss/driver/internal/core/config/typesafe/DefaultProgrammaticConfigBuilder.java,"@@ -0,0 +1,225 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.config.typesafe;
+
+import com.datastax.oss.driver.api.core.config.DriverConfigLoader;
+import com.datastax.oss.driver.api.core.config.DriverExecutionProfile;
+import com.datastax.oss.driver.api.core.config.DriverOption;
+import com.datastax.oss.driver.api.core.config.ProgrammaticConfigBuilder;
+import com.datastax.oss.driver.shaded.guava.common.base.Preconditions;
+import com.datastax.oss.protocol.internal.util.collection.NullAllowingImmutableMap;
+import com.typesafe.config.Config;
+import com.typesafe.config.ConfigFactory;
+import com.typesafe.config.ConfigValueFactory;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import edu.umd.cs.findbugs.annotations.Nullable;
+import java.time.Duration;
+import java.util.List;
+import java.util.Map;
+import java.util.function.Supplier;
+import net.jcip.annotations.NotThreadSafe;
+
+@NotThreadSafe
+public class DefaultProgrammaticConfigBuilder implements ProgrammaticConfigBuilder {
+
+  public static final Supplier<Config> DEFAULT_FALLBACK_SUPPLIER = ConfigFactory::defaultReference;
+
+  private final NullAllowingImmutableMap.Builder<String, Object> values =
+      NullAllowingImmutableMap.builder();
+  private final Supplier<Config> fallbackSupplier;
+  private final String rootPath;
+
+  private String currentProfileName = DriverExecutionProfile.DEFAULT_NAME;
+
+  /**
+   * @param fallbackSupplier the supplier that will provide fallback configuration for options that
+   *     haven't been specified programmatically.
+   * @param rootPath the root path used in non-programmatic sources (fallback reference.conf and
+   *     system properties).
+   */
+  public DefaultProgrammaticConfigBuilder(Supplier<Config> fallbackSupplier, String rootPath) {
+    this.fallbackSupplier = fallbackSupplier;
+    this.rootPath = rootPath;
+  }
+
+  public DefaultProgrammaticConfigBuilder() {
+    this(DEFAULT_FALLBACK_SUPPLIER, ""datastax-java-driver"");
+  }
+
+  private ProgrammaticConfigBuilder with(@NonNull DriverOption option, @Nullable Object value) {
+    String path = option.getPath();
+    if (!DriverExecutionProfile.DEFAULT_NAME.equals(currentProfileName)) {
+      path = ""profiles."" + currentProfileName + ""."" + path;
+    }
+    if (!rootPath.isEmpty()) {
+      path = rootPath + ""."" + path;
+    }
+    return with(path, value);
+  }
+
+  private ProgrammaticConfigBuilder with(@NonNull String path, @Nullable Object value) {
+    values.put(path, value);
+    return this;
+  }
+
+  @NonNull
+  @Override
+  public ProgrammaticConfigBuilder startProfile(@NonNull String profileName) {
+    Preconditions.checkNotNull(profileName);","[{'comment': ""1. Nit: couldn't we switch from Guava to regular Java and use `Objects.requireNonNull` instead?\r\n2. My IDE complains about the return value of these two methods not being assigned, so I'd suggest the following idiom instead:\r\n\r\n```\r\ncurrentProfileName = Objects.requireNonNull(profileName);\r\n```"", 'commenter': 'adutra'}]"
1220,core/src/main/java/com/datastax/oss/driver/internal/core/config/typesafe/DefaultProgrammaticConfigBuilder.java,"@@ -0,0 +1,225 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.config.typesafe;
+
+import com.datastax.oss.driver.api.core.config.DriverConfigLoader;
+import com.datastax.oss.driver.api.core.config.DriverExecutionProfile;
+import com.datastax.oss.driver.api.core.config.DriverOption;
+import com.datastax.oss.driver.api.core.config.ProgrammaticConfigBuilder;
+import com.datastax.oss.driver.shaded.guava.common.base.Preconditions;
+import com.datastax.oss.protocol.internal.util.collection.NullAllowingImmutableMap;
+import com.typesafe.config.Config;
+import com.typesafe.config.ConfigFactory;
+import com.typesafe.config.ConfigValueFactory;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import edu.umd.cs.findbugs.annotations.Nullable;
+import java.time.Duration;
+import java.util.List;
+import java.util.Map;
+import java.util.function.Supplier;
+import net.jcip.annotations.NotThreadSafe;
+
+@NotThreadSafe
+public class DefaultProgrammaticConfigBuilder implements ProgrammaticConfigBuilder {
+
+  public static final Supplier<Config> DEFAULT_FALLBACK_SUPPLIER = ConfigFactory::defaultReference;","[{'comment': 'Could be private.', 'commenter': 'adutra'}]"
1220,integration-tests/src/test/java/com/datastax/oss/driver/api/core/config/DriverExecutionProfileIT.java,"@@ -79,11 +78,8 @@ public void should_use_profile_request_timeout() {
     DriverConfigLoader loader =
         SessionUtils.configLoaderBuilder()
             .withDuration(DefaultDriverOption.REQUEST_TIMEOUT, Duration.ofSeconds(2))
-            .withProfile(
-                ""olap"",
-                DefaultDriverConfigLoaderBuilder.profileBuilder()
-                    .withDuration(DefaultDriverOption.REQUEST_TIMEOUT, Duration.ofSeconds(10))
-                    .build())
+            .startProfile(""olap"")","[{'comment': 'Nice ðŸ‘ ', 'commenter': 'adutra'}]"
1220,integration-tests/src/test/java/com/datastax/oss/driver/api/core/ConnectIT.java,"@@ -87,8 +87,9 @@ public void should_wait_for_contact_points_if_reconnection_enabled() throws Exce
     DriverConfigLoader loader =
         SessionUtils.configLoaderBuilder()
             .withBoolean(DefaultDriverOption.RECONNECT_ON_INIT, true)
-            .withClass(","[{'comment': 'Why remove `withClass`?', 'commenter': 'adutra'}]"
1220,integration-tests/src/test/java/com/datastax/oss/driver/api/core/tracker/RequestLoggerIT.java,"@@ -132,9 +120,14 @@
               DefaultDriverOption.REQUEST_LOGGER_MAX_VALUES,
               RequestLogger.DEFAULT_REQUEST_LOGGER_MAX_VALUES)
           .withBoolean(DefaultDriverOption.REQUEST_LOGGER_STACK_TRACES, true)
-          .withProfile(""low-threshold"", lowThresholdProfile)","[{'comment': 'I see that we cannot compose configs from other configs anymore. Not sure if this is worrisome?', 'commenter': 'adutra'}, {'comment': 'Yeah I lazily inlined everything, but I guess I could have instead created static methods that add the profiles to a given builder.', 'commenter': 'olim7t'}]"
1221,query-builder/src/main/java/com/datastax/oss/driver/api/querybuilder/insert/Insert.java,"@@ -44,4 +44,10 @@
    */
   @NonNull
   Insert usingTimestamp(@Nullable BindMarker bindMarker);
+
+  @NonNull","[{'comment': 'Need to add missing javadocs', 'commenter': 'emerkle826'}]"
1221,query-builder/src/main/java/com/datastax/oss/driver/api/querybuilder/update/UpdateStart.java,"@@ -41,4 +41,10 @@
    */
   @NonNull
   UpdateStart usingTimestamp(@NonNull BindMarker bindMarker);
+
+  @NonNull","[{'comment': 'Again, add missing javadocs', 'commenter': 'emerkle826'}]"
1221,query-builder/revapi.json,"@@ -2752,6 +2752,26 @@
         ""new"": ""method SelfT com.datastax.oss.driver.api.querybuilder.relation.OngoingWhereClause<SelfT extends com.datastax.oss.driver.api.querybuilder.relation.OngoingWhereClause<SelfT extends com.datastax.oss.driver.api.querybuilder.relation.OngoingWhereClause<SelfT>>>::whereRaw(java.lang.String) @ com.datastax.oss.driver.api.querybuilder.update.UpdateWithAssignments"",
         ""annotation"": ""@edu.umd.cs.findbugs.annotations.CheckReturnValue"",
         ""justification"": ""JAVA-2161: Annotate mutating methods with @CheckReturnValue""
+      },
+      {
+        ""code"": ""java.method.addedToInterface"",
+        ""new"": ""method com.datastax.oss.driver.api.querybuilder.insert.Insert com.datastax.oss.driver.api.querybuilder.insert.Insert::usingTtl(com.datastax.oss.driver.api.querybuilder.BindMarker)"",
+        ""justification"": ""JAVA-2210: Add ability to set TTL for modification queries""
+      },
+      {
+        ""code"": ""java.method.addedToInterface"",
+        ""new"": ""method com.datastax.oss.driver.api.querybuilder.insert.Insert com.datastax.oss.driver.api.querybuilder.insert.Insert::usingTtl(int)"",
+        ""justification"": ""JAVA-2210: Add ability to set TTL for modification queries""
+      },
+      {
+        ""code"": ""java.method.addedToInterface"",
+        ""new"": ""method com.datastax.oss.driver.api.querybuilder.update.UpdateStart com.datastax.oss.driver.api.querybuilder.update.UpdateStart::usingTtl(com.datastax.oss.driver.api.querybuilder.BindMarker)"",
+        ""justification"": ""JAVA-2210: Add ability to set TTL for modification queries""
+      },
+      {
+        ""code"": ""java.method.addedToInterface"",
+        ""new"": ""method com.datastax.oss.driver.api.querybuilder.update.UpdateStart com.datastax.oss.driver.api.querybuilder.update.UpdateStart::usingTtl(int)"",
+        ""justification"": ""JAVA-2210: Add ability to set TTL for modification queries""","[{'comment': 'ðŸ‘ ', 'commenter': 'olim7t'}]"
1221,manual/query_builder/README.md,"@@ -183,6 +183,6 @@ For a complete tour of the API, browse the child pages in this manual:
   * [Terms](term/)
   * [Idempotence](idempotence/)
   
-[QueryBuilder]:  http://docs.datastax.com/en/drivers/java/4.0/com/datastax/oss/driver/api/query-builder/QueryBuilder.html
-[SchemaBuilder]: http://docs.datastax.com/en/drivers/java/4.0/com/datastax/oss/driver/api/query-builder/SchemaBuilder.html
+[QueryBuilder]:  http://docs.datastax.com/en/drivers/java/4.0/com/datastax/oss/driver/api/querybuilder/QueryBuilder.html
+[SchemaBuilder]: http://docs.datastax.com/en/drivers/java/4.0/com/datastax/oss/driver/api/querybuilder/SchemaBuilder.html","[{'comment': 'Thanks for keeping this in a separate commit.', 'commenter': 'olim7t'}]"
1221,query-builder/src/main/java/com/datastax/oss/driver/internal/querybuilder/insert/DefaultInsert.java,"@@ -192,6 +220,14 @@ public String asCql() {
         builder.append(timestamp);
       }
     }
+    if (ttl != null) {
+      builder.append("" USING TTL "");","[{'comment': 'If the timestamp was also specified, this must be `""  AND TTL ""`.\r\n\r\nSince there are only two possible clauses I don\'t think we need to get too fancy, we can just re-check `timestamp != null` to determine which keyword to use.', 'commenter': 'olim7t'}]"
1221,query-builder/src/main/java/com/datastax/oss/driver/api/querybuilder/insert/Insert.java,"@@ -44,4 +44,10 @@
    */
   @NonNull
   Insert usingTimestamp(@Nullable BindMarker bindMarker);
+
+  @NonNull
+  Insert usingTtl(int ttl);","[{'comment': ""Good use of the naming conventions, we don't want to upper-case TTL here ðŸ‘ "", 'commenter': 'olim7t'}, {'comment': ""1. Please add a `@param` tag to the javadocs and specify that the value must be understood in seconds, I think it's important to note that.\r\n2. I would also rename the parameter to `ttlInSeconds`.\r\n\r\nThat brings the question: should we provide a method overload that takes a `Duration` instead? That could be nice, but then we would have to cope with sub-second durations, etc. Not sure if it's really helpful."", 'commenter': 'adutra'}, {'comment': 'For builders, we are extensively using `Duration` so I am +1 on that. I am not sure if we should still keep `int ttlInSeconds` method if we would provide the method that takes `Duration`', 'commenter': 'tomekl007'}, {'comment': ""I don't know. So far the query builder has stayed pretty close to the raw CQL syntax. If we go down that route, how about time types for the timestamp? I'd rather keep things simple rather than opening another can of worms."", 'commenter': 'olim7t'}, {'comment': ""I'll revert to just an `int` and name it `ttlInSeconds`"", 'commenter': 'emerkle826'}]"
1221,query-builder/src/main/java/com/datastax/oss/driver/api/querybuilder/insert/Insert.java,"@@ -45,9 +45,23 @@
   @NonNull
   Insert usingTimestamp(@Nullable BindMarker bindMarker);
 
+  /**
+   * Adds a USING TTL clause to this statement with a literal value. Setting a value of {@code 0}
+   * will clear a previously set TTL.","[{'comment': ""I find this a bit ambiguous because it's not clear whether this applies to the query builder or Cassandra.\r\n* passing `null` removes the `USING TTL` clause from the generated query\r\n* passing 0 will adds `USING TTL 0` clause to the generated query, which when executed will delete the TTL for the column(s)"", 'commenter': 'olim7t'}, {'comment': ""Good catch, I'll update the docs to reflect it."", 'commenter': 'emerkle826'}]"
1221,manual/query_builder/insert/README.md,"@@ -87,4 +87,20 @@ insertInto(""user"").json(bindMarker()).usingTimestamp(bindMarker())
 
 If you call the method multiple times, the last value will be used.
 
-[QueryBuilder]: http://docs.datastax.com/en/drivers/java/4.0/com/datastax/oss/driver/api/query-builder/QueryBuilder.html
+### Time To Live (TTL)
+
+Similar to Timestamps, you can specify data expiration TTLs with a literal value:","[{'comment': 'I would harmonize the description between inserts and updates around stg along the lines of ""You can generate a `USING TTL` clause that will cause column values to be deleted (marked with a tombstone) after the\r\nspecified time (in seconds) has expired. This can be done with a literal: ..."".', 'commenter': 'adutra'}]"
1221,query-builder/src/main/java/com/datastax/oss/driver/api/querybuilder/insert/Insert.java,"@@ -44,4 +44,24 @@
    */
   @NonNull
   Insert usingTimestamp(@Nullable BindMarker bindMarker);
+
+  /**
+   * Adds a USING TTL clause to this statement with a literal value. Setting a value of {@code 0}
+   * will clear a previously set TTL.
+   *
+   * <p>If this method or {@link #usingTtl(BindMarker) } is called multiple times, the value from
+   * the last invocation is used.
+   */
+  @NonNull
+  Insert usingTtl(int ttl);
+
+  /**
+   * Adds a USING TTL clause to this statement with a bind marker. Setting a value of {@code 0} will
+   * clear a previously set TTL.
+   *
+   * <p>If this method or {@link #usingTtl(int) } is called multiple times, the value from the last
+   * invocation is used.
+   */
+  @NonNull
+  Insert usingTtl(@Nullable BindMarker bindMarker);","[{'comment': 'Again I would include a `@param` tag and specify that the bind marker should be understood in seconds.', 'commenter': 'adutra'}]"
1221,query-builder/src/main/java/com/datastax/oss/driver/internal/querybuilder/update/DefaultUpdate.java,"@@ -154,6 +180,15 @@ public String asCql() {
       }
     }
 
+    if (ttl != null) {
+      builder.append("" USING TTL "");","[{'comment': 'Same here, we need to check if both clauses are present and append an `AND` if required.', 'commenter': 'adutra'}, {'comment': ""Can't we extract that logic somewhere? (superclass)?\r\n"", 'commenter': 'tomekl007'}, {'comment': 'Again, not really worth it, this would ""save"" like 4 lines of code.', 'commenter': 'olim7t'}, {'comment': 'My worry is not duplication but the problem of remembering to add that specific part of the code to every implementation that is doing insert/modification. (to prevent bugs in future similar to what we had in https://datastax-oss.atlassian.net/browse/JAVA-2178)\r\nMaybe `usingTtl` method should be created in the `OngoingAssignment` interface?', 'commenter': 'tomekl007'}]"
1221,query-builder/src/test/java/com/datastax/oss/driver/api/querybuilder/insert/RegularInsertTest.java,"@@ -74,4 +74,10 @@ public void should_generate_if_not_exists_and_timestamp_clauses() {
     assertThat(insertInto(""foo"").value(""a"", bindMarker()).ifNotExists().usingTimestamp(1))
         .hasCql(""INSERT INTO foo (a) VALUES (?) IF NOT EXISTS USING TIMESTAMP 1"");
   }
+
+  @Test
+  public void should_generate_ttl_clause() {
+    assertThat(insertInto(""foo"").value(""a"", bindMarker()).usingTtl(10))","[{'comment': 'I suggest that we add tests for when both `USING TIMESTAMP` and `USING TTL` are present.', 'commenter': 'adutra'}, {'comment': 'And also for `AND TTL`', 'commenter': 'tomekl007'}]"
1221,manual/query_builder/insert/README.md,"@@ -87,4 +87,20 @@ insertInto(""user"").json(bindMarker()).usingTimestamp(bindMarker())
 
 If you call the method multiple times, the last value will be used.
 
-[QueryBuilder]: http://docs.datastax.com/en/drivers/java/4.0/com/datastax/oss/driver/api/query-builder/QueryBuilder.html
+### Time To Live (TTL)
+
+Similar to Timestamps, you can specify data expiration TTLs with a literal value:
+
+```java
+insertInto(""user"").json(bindMarker()).usingTtl(500)
+// INSERT INTO user JSON ? USING TTL 500","[{'comment': 'maybe our example should not use JSON to not complicate that? Only simple bindMarker for field and TTL to focus the attention of the reader on the TTL part of the example?', 'commenter': 'tomekl007'}]"
1221,query-builder/src/main/java/com/datastax/oss/driver/api/querybuilder/update/UpdateStart.java,"@@ -41,4 +41,24 @@
    */
   @NonNull
   UpdateStart usingTimestamp(@NonNull BindMarker bindMarker);
+
+  /**
+   * Adds a USING TTL clause to this statement with a literal value. Setting a value of {@code 0}
+   * will clear a previously set TTL.
+   *
+   * <p>If this method or {@link #usingTtl(BindMarker) } is called multiple times, the value from
+   * the last invocation is used.
+   */
+  @NonNull
+  UpdateStart usingTtl(int ttl);
+
+  /**
+   * Adds a USING TTL clause to this statement with a bind marker. Setting a value of {@code 0} will
+   * clear a previously set TTL.
+   *
+   * <p>If this method or {@link #usingTtl(int) } is called multiple times, the value from the last
+   * invocation is used.
+   */
+  @NonNull
+  UpdateStart usingTtl(@NonNull BindMarker bindMarker);","[{'comment': 'I see that usingTtl methods are repeated in multiple interfaces. Maybe we should extract them to interface `ttlSupport` or smth similar and just implements that in all interfaces that need ttl support? (to not duplicate the code)', 'commenter': 'tomekl007'}, {'comment': 'I would, but both `Insert` and `UpdateStart` inherit from different super interfaces already, and neither of those should have `usingTtl` added to them.', 'commenter': 'emerkle826'}, {'comment': ""Just for two methods that are repeated two times I think it's not worth it. "", 'commenter': 'olim7t'}]"
1221,query-builder/src/main/java/com/datastax/oss/driver/internal/querybuilder/insert/DefaultInsert.java,"@@ -62,6 +63,7 @@ public DefaultInsert(
       @Nullable MissingJsonBehavior missingJsonBehavior,
       @NonNull ImmutableMap<CqlIdentifier, Term> assignments,
       @Nullable Object timestamp,
+      @Nullable Object ttl,","[{'comment': ""Shouldn't it be specific type instead of Object?"", 'commenter': 'tomekl007'}, {'comment': ""In this case, it is possible to set the TTL as a literal, or a BindMarker. They don't share a common Object hierarchy outside of `Object`. Later in the code, where the cql statement is built, the logic checks to see if the TTL value is a bind marker or a literal and builds accordingly."", 'commenter': 'emerkle826'}, {'comment': ""We could introduce yet another wrapper class. But this is private code, local to an internal class. An `instanceof` check is good enough, we don't need to add more clutter."", 'commenter': 'olim7t'}]"
1221,manual/query_builder/update/README.md,"@@ -32,6 +32,35 @@ update(""user"").usingTimestamp(bindMarker());
 
 If you call the method multiple times, the last value will be used.
 
+### Time To Live (TTL)
+
+You can generate a USING TTL clause that will cause column values to be deleted (marked with a
+tombstone) after the specified time (in seconds) has expired. This can be done with a literal:
+
+```java
+update(""user"").usingTtl(60).setColumn(""v"", bindMarker()).whereColumn(""k"").isEqualTo(bindMarker());
+// UPDATE user USING TTL 60 SET v=? WHERE k=?
+```
+
+Or a bind marker:
+
+```java
+update(""user"").usingTtl(bindMarker()).setColumn(""v"", bindMarker()).whereColumn(""k"").isEqualTo(bindMarker());
+// UPDATE user USING TTL ? SET v=? WHERE k=?
+```
+
+You can clear a previously set TTL by setting the value to 0:
+
+```java
+update(""user"").usingTtl(0).setColumn(""v"", bindMarker()).whereColumn(""k"").isEqualTo(bindMarker());
+// UPDATE user USING TTL 0 SET v=? WHERE k=?
+```
+
+Setting the value to 0 will result in removing the TTL from the column in Cassandra when the query","[{'comment': 'I see that you updated this paragraph to distinguish the two situations, but now I think you should update the corresponding paragraph in the `Insert` documentation above.', 'commenter': 'adutra'}]"
1221,query-builder/src/main/java/com/datastax/oss/driver/internal/querybuilder/insert/DefaultInsert.java,"@@ -192,6 +255,14 @@ public String asCql() {
         builder.append(timestamp);
       }
     }
+    if (ttlInSeconds != null) {
+      builder.append((timestamp != null) ? "" AND "" : "" USING "").append(""TTL "");
+      if (ttlInSeconds instanceof BindMarker) {
+        ((BindMarker) ttlInSeconds).appendTo(builder);
+      } else {","[{'comment': 'That `else` is worrying me. What if someone will pass some custom object as the `Object ttlInSeconds`?\r\nWe will set it on the builder and the user will get the error while executing the query, right?\r\nMaybe we should do \r\n```\r\nelse if(ttlInSeconds instanceof Integer){\r\n...\r\n}else{\r\nthrow IllegalArgumentException()\r\n}\r\n```\r\nTo fail fast on not-supported type?', 'commenter': 'tomekl007'}, {'comment': 'I see your point. I modeled this after the code for `timestamp`, as it supports a literal value and a BindMarker as well. For `ttlInSeconds`, I believe the literal can be an `Integer` or a `String` in this case, so long as it is a whole number. It could be a `Long` as well, as long as the value isn\'t larger than what is supported in Cassandra. In fact, it could be any Object that has a `toString()` implementation that returns a whole number between 0 and whatever the max is in Cassandra. \r\n\r\nWe could be a little permissive and only throw the Exception if we can\'t parse an Integer value from the Object:\r\n```\r\ntry {\r\n  Integer ttlValue = Integer.parseInt(ttlInSeconds.toString());\r\n  if (ttlValue.compareTo(0) < 0) {\r\n    throw new IllegalArgumentException(""Provided ttlInSeconds value can not be negative"");\r\n  }\r\n  // add  USING TTL clause\r\n} catch (NumberFormatException nfe) {\r\n  throw new IllegalArgumentException(""Provided ttlInSeconds is not a valid value"");\r\n}\r\n```\r\nI guess there is value in failing to build the Builder just prior to trying to have the Builder build the CQL. That would be the only benefit to failing at this point.', 'commenter': 'emerkle826'}]"
1225,manual/query_builder/truncate/README.md,"@@ -0,0 +1,16 @@
+## INSERT
+
+To create a TRUNCATE query, use one of the `truncate` methods in [QueryBuilder]. There are
+several variants depending on whether your table name is qualified, and whether you use
+case-sensitive identifiers or case-insensitive strings:
+
+```java
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.*;
+
+InsertInto insertCaseInsensitve = truncate(""ks"", ""table"");
+InsertInto insertCaseSensitive = truncate(CqlIdentifier.fromCql(""ks""), CqlIdentifier.fromCql(""table""));
+```
+
+Note that, at this stage, the query is ready to built. After creating a TRUNCATE query it does not take any values.","[{'comment': 'typo:\r\n`the query is ready to built` should be\r\n`the query is ready to build`', 'commenter': 'emerkle826'}]"
1225,changelog/README.md,"@@ -7,6 +7,7 @@
 - [improvement] JAVA-2211: Upgrade Jersey examples to fix security issue sid-3606
 - [bug] JAVA-2193: Fix flaky tests in ExecutionInfoWarningsIT
 - [improvement] JAVA-2197: Skip deployment of examples and integration tests to Maven central
+- [improvement] JAVA-2212: Add truncate to QueryBuilder","[{'comment': 'Nit: please add the entry at the beginning', 'commenter': 'olim7t'}]"
1225,manual/query_builder/truncate/README.md,"@@ -0,0 +1,16 @@
+## INSERT","[{'comment': 'Should be TRUNCATE.', 'commenter': 'olim7t'}]"
1225,manual/query_builder/truncate/README.md,"@@ -0,0 +1,16 @@
+## INSERT
+
+To create a TRUNCATE query, use one of the `truncate` methods in [QueryBuilder]. There are
+several variants depending on whether your table name is qualified, and whether you use
+case-sensitive identifiers or case-insensitive strings:
+
+```java
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.*;
+
+InsertInto insertCaseInsensitve = truncate(""ks"", ""table"");
+InsertInto insertCaseSensitive = truncate(CqlIdentifier.fromCql(""ks""), CqlIdentifier.fromCql(""table""));","[{'comment': 'InsertInto=>Truncate\r\nAlso the variables need to be renamed truncateXxx (and notice that there\'s an ""i"" missing in the first one CaseInsensit**i**ve).\r\n\r\nCould you also add a comment with the generated CQL under each one? See the pages for the other statements for examples of how we\'ve done it so far.', 'commenter': 'olim7t'}]"
1225,query-builder/src/main/java/com/datastax/oss/driver/api/querybuilder/QueryBuilder.java,"@@ -470,4 +472,47 @@ public static UserDefinedType udt(@NonNull CqlIdentifier name) {
   public static UserDefinedType udt(@NonNull String name) {
     return udt(CqlIdentifier.fromCql(name));
   }
+
+  /**
+   * Creates a new {@code TRUNCATE} query.
+   *
+   * @param table the name of the table to truncate.
+   * @return the truncation query.
+   */
+  public static Truncate truncate(@NonNull CqlIdentifier table) {
+    return truncate(null, table);
+  }
+
+  /**
+   * Creates a new {@code TRUNCATE} query.
+   *
+   * @param table the name of the table to truncate.
+   * @return the truncation query.
+   */
+  public static Truncate truncate(@NonNull String table) {
+    return truncate(null, CqlIdentifier.fromCql(table));
+  }
+
+  /**
+   * Creates a new {@code TRUNCATE} query.
+   *
+   * @param keyspace the name of the keyspace to use.
+   * @param table the name of the table to truncate.
+   * @return the truncation query.
+   */
+  public static Truncate truncate(@Nullable CqlIdentifier keyspace, @NonNull CqlIdentifier table) {
+    return new DefaultTruncate(keyspace, table);
+  }
+
+  /**
+   * Creates a new {@code TRUNCATE} query.
+   *
+   * @param keyspace the name of the keyspace to use.
+   * @param table the name of the table to truncate.
+   * @return the truncation query.
+   */
+  public static Truncate truncate(@Nullable String keyspace, @NonNull String table) {
+    return truncate(
+        keyspace == null ? null : CqlIdentifier.fromCql(keyspace), CqlIdentifier.fromCql(table));
+  }","[{'comment': 'ðŸ‘ \r\nCan you add a comment for the string variants to indicate that they use `CqlIdentifier.fromCql` for the conversion? See selectFrom at line 68 for example.', 'commenter': 'olim7t'}]"
1225,query-builder/src/main/java/com/datastax/oss/driver/internal/querybuilder/truncate/DefaultTruncate.java,"@@ -0,0 +1,73 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.querybuilder.truncate;
+
+import com.datastax.oss.driver.api.core.CqlIdentifier;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.api.core.cql.SimpleStatementBuilder;
+import com.datastax.oss.driver.api.querybuilder.truncate.Truncate;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import edu.umd.cs.findbugs.annotations.Nullable;
+import java.util.Map;
+
+public class DefaultTruncate implements Truncate {
+  private final CqlIdentifier keyspace;
+  private final CqlIdentifier table;
+  private static final boolean IS_IDEMPOTENT = true; // TRUNCATE is always IDEMPOTENT
+
+  public DefaultTruncate(@Nullable CqlIdentifier keyspace, @NonNull CqlIdentifier table) {
+    this.keyspace = keyspace;
+    this.table = table;
+  }
+
+  @NonNull
+  @Override
+  public String asCql() {
+    StringBuilder builder = new StringBuilder();
+    builder.append(""TRUNCATE "");
+    if (keyspace != null) {
+      builder.append(keyspace.asCql(true)).append(""."");
+    }
+    builder.append(table.asCql(true));
+    return builder.toString();
+  }","[{'comment': 'We have an internal utility to generate qualified names:\r\n```java\r\n    StringBuilder builder = new StringBuilder(""TRUNCATE "");\r\n    CqlHelper.qualify(keyspace, table, builder);\r\n    return builder.toString();\r\n```', 'commenter': 'olim7t'}]"
1225,query-builder/src/main/java/com/datastax/oss/driver/internal/querybuilder/truncate/DefaultTruncate.java,"@@ -0,0 +1,73 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.querybuilder.truncate;
+
+import com.datastax.oss.driver.api.core.CqlIdentifier;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.api.core.cql.SimpleStatementBuilder;
+import com.datastax.oss.driver.api.querybuilder.truncate.Truncate;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import edu.umd.cs.findbugs.annotations.Nullable;
+import java.util.Map;
+
+public class DefaultTruncate implements Truncate {
+  private final CqlIdentifier keyspace;
+  private final CqlIdentifier table;
+  private static final boolean IS_IDEMPOTENT = true; // TRUNCATE is always IDEMPOTENT
+
+  public DefaultTruncate(@Nullable CqlIdentifier keyspace, @NonNull CqlIdentifier table) {
+    this.keyspace = keyspace;
+    this.table = table;
+  }
+
+  @NonNull
+  @Override
+  public String asCql() {
+    StringBuilder builder = new StringBuilder();
+    builder.append(""TRUNCATE "");
+    if (keyspace != null) {
+      builder.append(keyspace.asCql(true)).append(""."");
+    }
+    builder.append(table.asCql(true));
+    return builder.toString();
+  }
+
+  @NonNull
+  @Override
+  public SimpleStatementBuilder builder() {
+    return SimpleStatement.builder(asCql()).setIdempotence(IS_IDEMPOTENT);","[{'comment': 'Please don\'t use a constant here. `setIdempotence(true)` is perfectly clear, and it\'s not like we might decide to use ""false"" to mean ""idempotent"" at any point in the future.', 'commenter': 'olim7t'}]"
1225,query-builder/src/main/java/com/datastax/oss/driver/internal/querybuilder/truncate/DefaultTruncate.java,"@@ -0,0 +1,73 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.querybuilder.truncate;
+
+import com.datastax.oss.driver.api.core.CqlIdentifier;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.api.core.cql.SimpleStatementBuilder;
+import com.datastax.oss.driver.api.querybuilder.truncate.Truncate;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import edu.umd.cs.findbugs.annotations.Nullable;
+import java.util.Map;
+
+public class DefaultTruncate implements Truncate {
+  private final CqlIdentifier keyspace;
+  private final CqlIdentifier table;
+  private static final boolean IS_IDEMPOTENT = true; // TRUNCATE is always IDEMPOTENT
+
+  public DefaultTruncate(@Nullable CqlIdentifier keyspace, @NonNull CqlIdentifier table) {
+    this.keyspace = keyspace;
+    this.table = table;
+  }
+
+  @NonNull
+  @Override
+  public String asCql() {
+    StringBuilder builder = new StringBuilder();
+    builder.append(""TRUNCATE "");
+    if (keyspace != null) {
+      builder.append(keyspace.asCql(true)).append(""."");
+    }
+    builder.append(table.asCql(true));
+    return builder.toString();
+  }
+
+  @NonNull
+  @Override
+  public SimpleStatementBuilder builder() {
+    return SimpleStatement.builder(asCql()).setIdempotence(IS_IDEMPOTENT);
+  }
+
+  @NonNull
+  @Override
+  public SimpleStatement build(@NonNull Object... values) {
+    throw new UnsupportedOperationException(
+        ""TRUNCATE doesn't take values as parameters. Use build() method instead."");","[{'comment': ""ðŸ‘ \r\nWe don't do any kind of validation on the values for other types of statements, but here it's trivial so why not."", 'commenter': 'olim7t'}]"
1235,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/entity/EntityHelperUpdateMethodGenerator.java,"@@ -0,0 +1,69 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.mapper.processor.entity;
+
+import com.datastax.oss.driver.api.core.CqlIdentifier;
+import com.datastax.oss.driver.api.querybuilder.QueryBuilder;
+import com.datastax.oss.driver.api.querybuilder.update.UpdateStart;
+import com.datastax.oss.driver.api.querybuilder.update.UpdateWithAssignments;
+import com.datastax.oss.driver.internal.mapper.processor.MethodGenerator;
+import com.datastax.oss.driver.internal.mapper.processor.ProcessorContext;
+import com.squareup.javapoet.MethodSpec;
+import java.util.Optional;
+import javax.lang.model.element.Modifier;
+
+public class EntityHelperUpdateMethodGenerator implements MethodGenerator {
+
+  private final EntityDefinition entityDefinition;
+
+  public EntityHelperUpdateMethodGenerator(
+      EntityDefinition entityDefinition,
+      EntityHelperGenerator enclosingClass,
+      ProcessorContext context) {
+    this.entityDefinition = entityDefinition;
+  }
+
+  @Override
+  public Optional<MethodSpec> generate() {
+    MethodSpec.Builder insertBuilder =
+        MethodSpec.methodBuilder(""update"")
+            .addAnnotation(Override.class)
+            .addModifiers(Modifier.PUBLIC)
+            .returns(UpdateWithAssignments.class)
+            .addStatement(""$T keyspaceId = context.getKeyspaceId()"", CqlIdentifier.class)
+            .addStatement(""$T tableId = context.getTableId()"", CqlIdentifier.class)
+            .beginControlFlow(""if (tableId == null)"")
+            .addStatement(""tableId = DEFAULT_TABLE_ID"")
+            .endControlFlow()
+            .addStatement(
+                ""$1T update = (keyspaceId == null)\n""
+                    + ""? $2T.update(tableId)\n""
+                    + "": $2T.update(keyspaceId, tableId)"",
+                UpdateStart.class,
+                QueryBuilder.class)
+            .addCode(""$[return update"");","[{'comment': 'One of the things that are different comparing` @Insert` generator is the moment of adding `USING TIMESTAMP | USING TTL""`. According to documentation https://docs.datastax.com/en/cql/3.3/cql/cql_reference/cqlUpdate.html and in our `Update` builder the `usingTimestamp` and `usingTtl` are available on the `UpdateStart` class.\r\nBecause of that, we should do it on this line. \r\nUnfortunately, we don\'t have annotation values (`customUsingClause`) in this class. Other solution will be to move this block (https://github.com/datastax/java-driver/pull/1235/files#diff-75706beabe9b72dbf270c073d07d201fR59 to line 64)\r\nto \r\n`generatePrepareRequest` method\r\nhttps://github.com/datastax/java-driver/pull/1235/files#diff-25ba2cdaf8c214d0d4a6f05f1c7922acR147.\r\nIn that case, this `generate()` method will return `UpdateStart`, not the `UpdateWithAssignments` (so we will be able to append usingTtl or using TImestamp in the `generatePrepareRequest(`.\r\nBut the problem is that we will need to pass `EntityDefinition entityDefinition` to `generatePrepareRequest` metho as well. It could be done in the `EntityHelperGenerator` method: `methodGenerator.generate(entityDefinition)`\r\nbut it is not present in `DaoImplementationGenerator` nor `MapperImplementationGenerator` classes - so it will need bigger refactor. @olim7t let me know wdyt', 'commenter': 'tomekl007'}, {'comment': ""I see. We have three options:\r\n1. make `EntityHelper.update` not return the statement directly, but take an existing statement and modify it. Then the DAO method would be responsible for starting the statement.\r\n2. modify the query builder to allow `usingTimestamp/usingTtl` at later steps.\r\n3. cast to `DefaultUpdate`, which gives us access to the methods again. (this is safe here because we control the creation of the statement, we know it's the default implementation)\r\n\r\nI would have liked 2, but it breaks the API. So I think we should do 3."", 'commenter': 'olim7t'}, {'comment': ""I was thinking about the 2nd approach but you are right - it breaks the API.\r\n3 seems to be a clever trick to alleviate that problem. I've implemented it in this commit:\r\nhttps://github.com/datastax/java-driver/pull/1235/commits/eb1c29f2338b4341a9759ca9fa9a641ea9936d62"", 'commenter': 'tomekl007'}]"
1235,mapper-processor/src/test/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoImplementationGeneratorTest.java,"@@ -23,6 +23,7 @@
 
 public class DaoImplementationGeneratorTest extends DaoMethodGeneratorTest {
 
+  // todo handle Update where there is no set","[{'comment': 'fails in this test are related to this line:\r\nhttps://github.com/datastax/java-driver/pull/1235/files#diff-75706beabe9b72dbf270c073d07d201fR61\r\nTo set all columns for Update we need to get all non PKs columns: `entityDefinition.getRegularColumns`.\r\nIf there is a case (like in this test) that the Entity for which we are generating update prepared statement has only one column and that column is PK the generated Update statement is incorrect.\r\n(The WHERE clause in Update is mandatory and it cannot contains `PK`). Because of that, the generated update statement will be incorrect. I am not sure how should we handle such a situation: maybe we should not generate update code path if the entity has only PK? But if the user is annotating the method with `@Update` for such entity it will be confusing for end-user: so other solution will be to throw an exception that we cannot generate update for the entity with only one column. Wdyt?\r\n\r\nTo make this test pass I can just add one additional non-PK key to `protected static final TypeSpec ENTITY_SPEC =` in the DaoMethodGeneratorTest but I am not 100% sure why in this test the Mapper tries to auto-generate  \r\n```\r\n@Override\r\n  public UpdateWithAssignments update() {\r\n```', 'commenter': 'tomekl007'}, {'comment': 'Yes, I think we should throw in corner cases like this. I handled a similar case in `EntityHelperDeleteByPrimaryKeyMethodGenerator`, you can see the generated `throw` statement at the beginning of the `generate()` method.', 'commenter': 'olim7t'}, {'comment': 'Error handling implemented in https://github.com/datastax/java-driver/pull/1235/commits/07644279cf79e5e86905da796da7d75c14badb1b.', 'commenter': 'tomekl007'}]"
1235,mapper-runtime/src/main/java/com/datastax/oss/driver/api/mapper/annotations/Update.java,"@@ -0,0 +1,34 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.mapper.annotations;
+
+import java.lang.annotation.ElementType;
+import java.lang.annotation.Retention;
+import java.lang.annotation.RetentionPolicy;
+import java.lang.annotation.Target;
+
+@Target(ElementType.METHOD)
+@Retention(RetentionPolicy.CLASS)
+public @interface Update {
+
+  boolean ifExists() default false;
+
+  String ifCondition() default """";","[{'comment': ""Could you name this `customIfClause()` to be consistent with `@Delete`?\r\n\r\nAlso, `@Delete` requires the IF keyword in the string, this one doesn't, it should be the same everywhere."", 'commenter': 'olim7t'}, {'comment': 'Implemented in https://github.com/datastax/java-driver/pull/1235/commits/87c8ef779b814b68e760348a1d4f13c74ed34d48', 'commenter': 'tomekl007'}]"
1235,integration-tests/src/test/java/com/datastax/oss/driver/mapper/UpdateEntityIT.java,"@@ -0,0 +1,270 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.mapper;
+
+import static org.assertj.core.api.Assertions.assertThat;
+
+import com.datastax.oss.driver.api.core.CqlIdentifier;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.cql.AsyncResultSet;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.api.mapper.annotations.Dao;
+import com.datastax.oss.driver.api.mapper.annotations.DaoFactory;
+import com.datastax.oss.driver.api.mapper.annotations.DaoKeyspace;
+import com.datastax.oss.driver.api.mapper.annotations.Mapper;
+import com.datastax.oss.driver.api.mapper.annotations.Select;
+import com.datastax.oss.driver.api.mapper.annotations.Update;
+import com.datastax.oss.driver.api.testinfra.CassandraRequirement;
+import com.datastax.oss.driver.api.testinfra.ccm.CcmRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionRule;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import com.datastax.oss.driver.internal.core.util.concurrent.CompletableFutures;
+import java.util.UUID;
+import java.util.concurrent.CompletableFuture;
+import org.junit.Before;
+import org.junit.BeforeClass;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.junit.rules.RuleChain;
+import org.junit.rules.TestRule;
+
+@Category(ParallelizableTests.class)
+@CassandraRequirement(min = ""3.4"", description = ""Creates a SASI index"")
+public class UpdateEntityIT extends InventoryITBase {
+
+  private static CcmRule ccm = CcmRule.getInstance();
+
+  private static SessionRule<CqlSession> sessionRule = SessionRule.builder(ccm).build();
+
+  @ClassRule public static TestRule chain = RuleChain.outerRule(ccm).around(sessionRule);
+
+  private static ProductDao dao;
+
+  @BeforeClass
+  public static void setup() {
+    CqlSession session = sessionRule.session();
+
+    for (String query : createStatements()) {
+      session.execute(
+          SimpleStatement.builder(query).setExecutionProfile(sessionRule.slowProfile()).build());
+    }
+
+    InventoryMapper inventoryMapper = new UpdateEntityIT_InventoryMapperBuilder(session).build();
+    dao = inventoryMapper.productDao(sessionRule.keyspace());
+  }
+
+  @Before
+  public void clearProductData() {
+    CqlSession session = sessionRule.session();
+    session.execute(
+        SimpleStatement.builder(""TRUNCATE product"")
+            .setExecutionProfile(sessionRule.slowProfile())
+            .build());
+  }
+
+  @Test
+  public void should_insert_entity() {
+    assertThat(dao.findById(FLAMETHROWER.getId())).isNull();
+
+    dao.update(FLAMETHROWER);
+    assertThat(dao.findById(FLAMETHROWER.getId())).isEqualTo(FLAMETHROWER);
+  }
+
+  @Test
+  public void should_insert_entity_asynchronously() {
+    assertThat(dao.findById(FLAMETHROWER.getId())).isNull();
+
+    CompletableFutures.getUninterruptibly(dao.updateAsync(FLAMETHROWER));
+    assertThat(dao.findById(FLAMETHROWER.getId())).isEqualTo(FLAMETHROWER);
+  }
+
+  //  @Ignore(""Using not work yet"")
+  //  @Test
+  //  public void should_insert_entity_with_custom_clause() {
+  //    assertThat(dao.findById(FLAMETHROWER.getId())).isNull();
+  //
+  //    long timestamp = 1234;
+  //    dao.updateWithBoundTimestamp(FLAMETHROWER, timestamp);
+  //
+  //    CqlSession session = sessionRule.session();
+  //    Row row =
+  //        session
+  //            .execute(
+  //                SimpleStatement.newInstance(
+  //                    ""SELECT WRITETIME(description) FROM product WHERE id = ?"",
+  //                    FLAMETHROWER.getId()))
+  //            .one();
+  //    assert row != null;
+  //    long writeTime = row.getLong(0);
+  //    assertThat(writeTime).isEqualTo(timestamp);
+  //  }
+
+  //  @Ignore(""using not working yet"")
+  //  @Test
+  //  public void should_insert_entity_with_custom_clause_asynchronously() {
+  //    assertThat(dao.findById(FLAMETHROWER.getId())).isNull();
+  //
+  //    long timestamp = 1234;
+  //    CompletableFutures.getUninterruptibly(
+  //        dao.updateAsyncWithBoundTimestamp(FLAMETHROWER, timestamp));
+  //
+  //    CqlSession session = sessionRule.session();
+  //    Row row =
+  //        session
+  //            .execute(
+  //                SimpleStatement.newInstance(
+  //                    ""SELECT WRITETIME(description) FROM product WHERE id = ?"",
+  //                    FLAMETHROWER.getId()))
+  //            .one();
+  //    assert row != null;
+  //    long writeTime = row.getLong(0);
+  //    assertThat(writeTime).isEqualTo(timestamp);
+  //  }
+  //
+  @Test
+  public void should_insert_entity_if_exists() {
+    dao.update(FLAMETHROWER);
+    assertThat(dao.findById(FLAMETHROWER.getId())).isNotNull();
+
+    Product otherProduct =
+        new Product(FLAMETHROWER.getId(), ""Other description"", new Dimensions(1, 1, 1));
+    assertThat(dao.updateIfExists(otherProduct).wasApplied()).isEqualTo(true);
+  }
+
+  @Test
+  public void should_not_insert_entity_if_not_exists() {
+    assertThat(dao.findById(FLAMETHROWER.getId())).isNull();
+
+    Product otherProduct =
+        new Product(FLAMETHROWER.getId(), ""Other description"", new Dimensions(1, 1, 1));
+    assertThat(dao.updateIfExists(otherProduct).wasApplied()).isEqualTo(false);
+  }
+
+  @Test
+  public void should_insert_entity_if_exists_asynchronously() {
+    dao.update(FLAMETHROWER);
+    assertThat(dao.findById(FLAMETHROWER.getId())).isNotNull();
+
+    Product otherProduct =
+        new Product(FLAMETHROWER.getId(), ""Other description"", new Dimensions(1, 1, 1));
+    assertThat(
+            CompletableFutures.getUninterruptibly(dao.updateAsyncIfExists(otherProduct))
+                .wasApplied())
+        .isEqualTo(true);
+  }
+
+  @Test
+  public void should_not_insert_entity_if_not_exists_asynchronously() {
+    assertThat(dao.findById(FLAMETHROWER.getId())).isNull();
+
+    Product otherProduct =
+        new Product(FLAMETHROWER.getId(), ""Other description"", new Dimensions(1, 1, 1));
+    assertThat(
+            CompletableFutures.getUninterruptibly(dao.updateAsyncIfExists(otherProduct))
+                .wasApplied())
+        .isEqualTo(false);
+  }
+
+  @Test
+  public void should_insert_entity_if_condition_is_met() {
+    dao.update(
+        new Product(FLAMETHROWER.getId(), ""Description for length 10"", new Dimensions(10, 1, 1)));
+    assertThat(dao.findById(FLAMETHROWER.getId())).isNotNull();
+
+    Product otherProduct =
+        new Product(FLAMETHROWER.getId(), ""Other description"", new Dimensions(1, 1, 1));
+    assertThat(dao.updateIfLength(otherProduct, 10).wasApplied()).isEqualTo(true);
+  }
+
+  @Test
+  public void should_not_insert_entity_if_condition_is_not_met() {
+    dao.update(
+        new Product(FLAMETHROWER.getId(), ""Description for length 10"", new Dimensions(10, 1, 1)));
+    assertThat(dao.findById(FLAMETHROWER.getId())).isNotNull();
+
+    Product otherProduct =
+        new Product(FLAMETHROWER.getId(), ""Other description"", new Dimensions(1, 1, 1));
+    assertThat(dao.updateIfLength(otherProduct, 20).wasApplied()).isEqualTo(false);
+  }
+
+  @Test
+  public void should_async_insert_entity_if_condition_is_met() {
+    dao.update(
+        new Product(FLAMETHROWER.getId(), ""Description for length 10"", new Dimensions(10, 1, 1)));
+    assertThat(dao.findById(FLAMETHROWER.getId())).isNotNull();
+
+    Product otherProduct =
+        new Product(FLAMETHROWER.getId(), ""Other description"", new Dimensions(1, 1, 1));
+    assertThat(
+            CompletableFutures.getUninterruptibly(dao.updateIfLengthAsync(otherProduct, 10))
+                .wasApplied())
+        .isEqualTo(true);
+  }
+
+  @Test
+  public void should_not_async_insert_entity_if_condition_is_not_met() {
+    dao.update(
+        new Product(FLAMETHROWER.getId(), ""Description for length 10"", new Dimensions(10, 1, 1)));
+    assertThat(dao.findById(FLAMETHROWER.getId())).isNotNull();
+
+    Product otherProduct =
+        new Product(FLAMETHROWER.getId(), ""Other description"", new Dimensions(1, 1, 1));
+    assertThat(
+            CompletableFutures.getUninterruptibly(dao.updateIfLengthAsync(otherProduct, 20))
+                .wasApplied())
+        .isEqualTo(false);
+  }
+
+  @Mapper
+  public interface InventoryMapper {
+    @DaoFactory
+    ProductDao productDao(@DaoKeyspace CqlIdentifier keyspace);
+  }
+
+  @Dao
+  public interface ProductDao {
+
+    @Update(whereClause = ""id = :id"")
+    void update(Product product);","[{'comment': ""We shouldn't have to specify the where clause here, we know it's the PK since the method receives an entity instance, it can be generated automatically. For example, `@Delete` does it that way.\r\n\r\nIn fact  I think we could get rid of `whereClause` completely, the argument will always be an entity (you can do update queries that don't target an entity, but the mapper can really generate anything automatically in that case, so it's as simple to use `@Query` and provide the whole query string). "", 'commenter': 'olim7t'}, {'comment': 'Implemented in: https://github.com/datastax/java-driver/pull/1235/commits/6f2fa9167d05a42d62230fac5f3e16b30f96f75c', 'commenter': 'tomekl007'}, {'comment': ""Don't we want to give our users the ability to specify WHERE IN: \r\n ```UPDATE users\r\n  SET state = 'TX'\r\n  WHERE user_uuid\r\n  IN (88b8fd18-b1ed-4e96-bf79-4280797cba80,\r\n    06a8913c-c0d6-477c-937d-6c1b69a95d43,\r\n    bc108776-7cb5-477f-917d-869c12dfffa8);\r\n```\r\n?"", 'commenter': 'tomekl007'}, {'comment': ""Yeah actually you're right, we should preserve that ability.\r\n\r\nSo if the custom where clause is empty, it's an update by primary key and the mapper generates the where clause.\r\nIf it's not empty, then we concatenate it as-is at the end of our request; the primary key components of the entity are ignored."", 'commenter': 'olim7t'}]"
1235,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoImplementationGenerator.java,"@@ -130,7 +130,7 @@ protected String getFileName() {
                 .getMessager()
                 .error(
                     methodElement,
-                    ""Unrecognized method signature: no implementation will be generated"");
+                    ""[DaoImplementationGenerator] Unrecognized method signature: no implementation will be generated"");","[{'comment': 'I disagree: this will be surfaced to the user as a compiler error. The user has no idea what `DaoImplementationGenerator` is, the mapper processor is a black box to them.', 'commenter': 'olim7t'}, {'comment': 'I was trying to find a way to make it easier to debug and was planning to remove it before PR.\r\nThx for reminder. I agree that it should be black-box\r\n', 'commenter': 'tomekl007'}]"
1235,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoUpdateMethodGenerator.java,"@@ -0,0 +1,189 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.mapper.processor.dao;
+
+import static com.datastax.oss.driver.internal.mapper.processor.dao.ReturnTypeKind.FUTURE_OF_ASYNC_RESULT_SET;
+import static com.datastax.oss.driver.internal.mapper.processor.dao.ReturnTypeKind.FUTURE_OF_VOID;
+import static com.datastax.oss.driver.internal.mapper.processor.dao.ReturnTypeKind.RESULT_SET;
+import static com.datastax.oss.driver.internal.mapper.processor.dao.ReturnTypeKind.VOID;
+
+import com.datastax.oss.driver.api.core.cql.BoundStatement;
+import com.datastax.oss.driver.api.core.cql.BoundStatementBuilder;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.api.mapper.annotations.Update;
+import com.datastax.oss.driver.api.querybuilder.QueryBuilder;
+import com.datastax.oss.driver.internal.core.util.concurrent.CompletableFutures;
+import com.datastax.oss.driver.internal.mapper.processor.ProcessorContext;
+import com.datastax.oss.driver.internal.mapper.processor.util.generation.GeneratedCodePatterns;
+import com.datastax.oss.driver.internal.querybuilder.DefaultRaw;
+import com.squareup.javapoet.ClassName;
+import com.squareup.javapoet.MethodSpec;
+import java.util.EnumSet;
+import java.util.List;
+import java.util.Optional;
+import javax.lang.model.element.ExecutableElement;
+import javax.lang.model.element.TypeElement;
+import javax.lang.model.element.VariableElement;
+
+public class DaoUpdateMethodGenerator extends DaoMethodGenerator {
+
+  private static final EnumSet<ReturnTypeKind> SUPPORTED_RETURN_TYPES =
+      EnumSet.of(VOID, FUTURE_OF_VOID, RESULT_SET, FUTURE_OF_ASYNC_RESULT_SET);","[{'comment': 'I would also add `BOOLEAN` and ` FUTURE_OF_BOOLEAN` for `ifExists`.', 'commenter': 'olim7t'}, {'comment': ""Yes, I was also considering that. Implemented in this commit:\r\nhttps://github.com/datastax/java-driver/pull/1235/commits/95f96621ed26dab04a2bd7f7ce904be9ff9758a3\r\nI've added tests and `BOOLEAN` as a supporter return type. \r\nWhat is not clear to me how it was handled automatically - I thought that I will need to add mapping logic (from results set to BOOLEAN) but it worked out of the box.\r\nCould you tell me where is that mapping logic?"", 'commenter': 'tomekl007'}, {'comment': 'In `ReturnTypeKind.addExecuteStatement`. The generated code relies on `executeXxx` helper methods defined in `DaoBase`.', 'commenter': 'olim7t'}, {'comment': 'OK, I see:\r\n```protected boolean executeAndMapWasAppliedToBoolean(Statement<?> statement) {\r\n    ResultSet rs = execute(statement);\r\n    return rs.wasApplied();\r\n  }\r\n```\r\nthx', 'commenter': 'tomekl007'}]"
1235,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoUpdateMethodGenerator.java,"@@ -0,0 +1,189 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.mapper.processor.dao;
+
+import static com.datastax.oss.driver.internal.mapper.processor.dao.ReturnTypeKind.FUTURE_OF_ASYNC_RESULT_SET;
+import static com.datastax.oss.driver.internal.mapper.processor.dao.ReturnTypeKind.FUTURE_OF_VOID;
+import static com.datastax.oss.driver.internal.mapper.processor.dao.ReturnTypeKind.RESULT_SET;
+import static com.datastax.oss.driver.internal.mapper.processor.dao.ReturnTypeKind.VOID;
+
+import com.datastax.oss.driver.api.core.cql.BoundStatement;
+import com.datastax.oss.driver.api.core.cql.BoundStatementBuilder;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.api.mapper.annotations.Update;
+import com.datastax.oss.driver.api.querybuilder.QueryBuilder;
+import com.datastax.oss.driver.internal.core.util.concurrent.CompletableFutures;
+import com.datastax.oss.driver.internal.mapper.processor.ProcessorContext;
+import com.datastax.oss.driver.internal.mapper.processor.util.generation.GeneratedCodePatterns;
+import com.datastax.oss.driver.internal.querybuilder.DefaultRaw;
+import com.squareup.javapoet.ClassName;
+import com.squareup.javapoet.MethodSpec;
+import java.util.EnumSet;
+import java.util.List;
+import java.util.Optional;
+import javax.lang.model.element.ExecutableElement;
+import javax.lang.model.element.TypeElement;
+import javax.lang.model.element.VariableElement;
+
+public class DaoUpdateMethodGenerator extends DaoMethodGenerator {
+
+  private static final EnumSet<ReturnTypeKind> SUPPORTED_RETURN_TYPES =
+      EnumSet.of(VOID, FUTURE_OF_VOID, RESULT_SET, FUTURE_OF_ASYNC_RESULT_SET);
+
+  public DaoUpdateMethodGenerator(
+      ExecutableElement methodElement,
+      DaoImplementationSharedCode enclosingClass,
+      ProcessorContext context) {
+    super(methodElement, enclosingClass, context);
+  }
+
+  @Override
+  public Optional<MethodSpec> generate() {
+
+    // Validate the parameters:
+    // - the first one must be the entity.
+    // - the others are completely free-form (they'll be used as additional bind variables)
+    if (methodElement.getParameters().isEmpty()) {
+      context
+          .getMessager()
+          .error(
+              methodElement,
+              ""Wrong number of parameters: %s methods must have at least one"",
+              Update.class.getSimpleName());
+      return Optional.empty();
+    }
+    VariableElement firstParameter = methodElement.getParameters().get(0);
+    TypeElement entityElement = asEntityElement(firstParameter);
+    if (entityElement == null) {
+      context
+          .getMessager()
+          .error(
+              methodElement,
+              ""Invalid parameter type: ""
+                  + ""%s methods must take the entity to update as the first parameter"",
+              Update.class.getSimpleName());
+      return Optional.empty();
+    }
+
+    // Validate the return type:
+    ReturnType returnType = parseReturnType(methodElement.getReturnType());
+    if (!SUPPORTED_RETURN_TYPES.contains(returnType.kind)) {
+      context
+          .getMessager()
+          .error(
+              methodElement,
+              ""Invalid return type: %s methods must return either void or the entity class ""
+                  + ""(possibly wrapped in a CompletionStage/CompletableFuture)"",
+              Update.class.getSimpleName());
+      return Optional.empty();
+    }
+    if (returnType.entityElement != null && !returnType.entityElement.equals(entityElement)) {","[{'comment': 'This is not needed here: the types in `SUPPORTED_RETURN_TYPES` never contain an entity.', 'commenter': 'olim7t'}]"
1235,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoUpdateMethodGenerator.java,"@@ -0,0 +1,189 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.mapper.processor.dao;
+
+import static com.datastax.oss.driver.internal.mapper.processor.dao.ReturnTypeKind.FUTURE_OF_ASYNC_RESULT_SET;
+import static com.datastax.oss.driver.internal.mapper.processor.dao.ReturnTypeKind.FUTURE_OF_VOID;
+import static com.datastax.oss.driver.internal.mapper.processor.dao.ReturnTypeKind.RESULT_SET;
+import static com.datastax.oss.driver.internal.mapper.processor.dao.ReturnTypeKind.VOID;
+
+import com.datastax.oss.driver.api.core.cql.BoundStatement;
+import com.datastax.oss.driver.api.core.cql.BoundStatementBuilder;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.api.mapper.annotations.Update;
+import com.datastax.oss.driver.api.querybuilder.QueryBuilder;
+import com.datastax.oss.driver.internal.core.util.concurrent.CompletableFutures;
+import com.datastax.oss.driver.internal.mapper.processor.ProcessorContext;
+import com.datastax.oss.driver.internal.mapper.processor.util.generation.GeneratedCodePatterns;
+import com.datastax.oss.driver.internal.querybuilder.DefaultRaw;
+import com.squareup.javapoet.ClassName;
+import com.squareup.javapoet.MethodSpec;
+import java.util.EnumSet;
+import java.util.List;
+import java.util.Optional;
+import javax.lang.model.element.ExecutableElement;
+import javax.lang.model.element.TypeElement;
+import javax.lang.model.element.VariableElement;
+
+public class DaoUpdateMethodGenerator extends DaoMethodGenerator {
+
+  private static final EnumSet<ReturnTypeKind> SUPPORTED_RETURN_TYPES =
+      EnumSet.of(VOID, FUTURE_OF_VOID, RESULT_SET, FUTURE_OF_ASYNC_RESULT_SET);
+
+  public DaoUpdateMethodGenerator(
+      ExecutableElement methodElement,
+      DaoImplementationSharedCode enclosingClass,
+      ProcessorContext context) {
+    super(methodElement, enclosingClass, context);
+  }
+
+  @Override
+  public Optional<MethodSpec> generate() {
+
+    // Validate the parameters:
+    // - the first one must be the entity.
+    // - the others are completely free-form (they'll be used as additional bind variables)
+    if (methodElement.getParameters().isEmpty()) {
+      context
+          .getMessager()
+          .error(
+              methodElement,
+              ""Wrong number of parameters: %s methods must have at least one"",
+              Update.class.getSimpleName());
+      return Optional.empty();
+    }
+    VariableElement firstParameter = methodElement.getParameters().get(0);
+    TypeElement entityElement = asEntityElement(firstParameter);
+    if (entityElement == null) {
+      context
+          .getMessager()
+          .error(
+              methodElement,
+              ""Invalid parameter type: ""
+                  + ""%s methods must take the entity to update as the first parameter"",
+              Update.class.getSimpleName());
+      return Optional.empty();
+    }
+
+    // Validate the return type:
+    ReturnType returnType = parseReturnType(methodElement.getReturnType());
+    if (!SUPPORTED_RETURN_TYPES.contains(returnType.kind)) {
+      context
+          .getMessager()
+          .error(
+              methodElement,
+              ""Invalid return type: %s methods must return either void or the entity class ""
+                  + ""(possibly wrapped in a CompletionStage/CompletableFuture)"",
+              Update.class.getSimpleName());
+      return Optional.empty();
+    }
+    if (returnType.entityElement != null && !returnType.entityElement.equals(entityElement)) {
+      context
+          .getMessager()
+          .error(
+              methodElement,
+              ""Invalid return type: %s methods must return the same entity as their argument "",
+              Update.class.getSimpleName());
+      return Optional.empty();
+    }
+
+    // Generate the method:
+    String helperFieldName = enclosingClass.addEntityHelperField(ClassName.get(entityElement));
+    String statementName =
+        enclosingClass.addPreparedStatement(
+            methodElement,
+            (methodBuilder, requestName) ->
+                generatePrepareRequest(methodBuilder, requestName, helperFieldName));
+
+    MethodSpec.Builder insertBuilder = GeneratedCodePatterns.override(methodElement);","[{'comment': ""Nit: this should be called `updateBuilder` for consistency. Although I've been wondering if it really needs to be named after the type of method, `methodBuilder` would be good too."", 'commenter': 'olim7t'}]"
1235,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/mapper/MapperImplementationGenerator.java,"@@ -105,7 +105,7 @@ protected String getFileName() {
                 .getMessager()
                 .error(
                     methodElement,
-                    ""Unrecognized method signature: no implementation will be generated"");
+                    ""[MapperImplementationGenerator] Unrecognized method signature: no implementation will be generated"");","[{'comment': 'Same remark as in `DaoImplementationGenerator`.', 'commenter': 'olim7t'}]"
1235,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoUpdateMethodGenerator.java,"@@ -0,0 +1,241 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.mapper.processor.dao;
+
+import static com.datastax.oss.driver.internal.mapper.processor.dao.ReturnTypeKind.FUTURE_OF_ASYNC_RESULT_SET;
+import static com.datastax.oss.driver.internal.mapper.processor.dao.ReturnTypeKind.FUTURE_OF_VOID;
+import static com.datastax.oss.driver.internal.mapper.processor.dao.ReturnTypeKind.RESULT_SET;
+import static com.datastax.oss.driver.internal.mapper.processor.dao.ReturnTypeKind.VOID;
+
+import com.datastax.oss.driver.api.core.cql.BoundStatement;
+import com.datastax.oss.driver.api.core.cql.BoundStatementBuilder;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.api.mapper.annotations.Update;
+import com.datastax.oss.driver.api.querybuilder.QueryBuilder;
+import com.datastax.oss.driver.internal.core.util.concurrent.CompletableFutures;
+import com.datastax.oss.driver.internal.mapper.processor.ProcessorContext;
+import com.datastax.oss.driver.internal.mapper.processor.util.generation.GeneratedCodePatterns;
+import com.datastax.oss.driver.internal.querybuilder.DefaultRaw;
+import com.datastax.oss.driver.internal.querybuilder.update.DefaultUpdate;
+import com.datastax.oss.driver.shaded.guava.common.annotations.VisibleForTesting;
+import com.squareup.javapoet.ClassName;
+import com.squareup.javapoet.MethodSpec;
+import java.util.EnumSet;
+import java.util.List;
+import java.util.Optional;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+import javax.lang.model.element.ExecutableElement;
+import javax.lang.model.element.TypeElement;
+import javax.lang.model.element.VariableElement;
+
+public class DaoUpdateMethodGenerator extends DaoMethodGenerator {
+
+  private static final EnumSet<ReturnTypeKind> SUPPORTED_RETURN_TYPES =
+      EnumSet.of(VOID, FUTURE_OF_VOID, RESULT_SET, FUTURE_OF_ASYNC_RESULT_SET);
+  private static final Pattern USING_TIMESTAMP_VALUE_PATTER =
+      Pattern.compile(""(?i)USING TIMESTAMP (\\d+$)"");
+  private static final Pattern USING_TIMESTAMP_BIND_MARKER_PATTERN =
+      Pattern.compile(""(?i)USING TIMESTAMP :(\\w+$)"");
+  private static final Pattern USING_TTL_VALUE_PATTER = Pattern.compile(""(?i)USING TTL (\\d+)$"");
+  private static final Pattern USING_TTL_BIND_MARKER_PATTERN =
+      Pattern.compile(""(?i)USING TTL :(\\w+)$"");
+  // todo should we support ? bindMarker","[{'comment': ""I am not sure if we should support `USING TTL ?` syntax in the `customUsingClause`. \r\nI think we don't need it since we are binding parameters from the method arguments using named parameters"", 'commenter': 'tomekl007'}, {'comment': 'As we discussed offline, let\'s support named parameters only.\r\n\r\nHowever this makes me realize something else: unlike `Insert`, we can\'t simply append the provided string at the end of the query (because USING is not at the end for UPDATE, which is quite an unfortunate choice by the CQL language designers tbh).\r\n\r\nHence the string parsing you have to do here; it\'s completely justified but I find it a bit awkward. So let\'s revisit the annotation syntax to have the user provide the values directly instead:\r\n```\r\n@Update(ttl = ""..."", timestamp = ""..."")\r\n```\r\nBoth attributes are strings that default to empty.\r\n* if the string is empty we don\'t generate the corresponding clause\r\n* if it starts with `:` it\'s a bind marker name\r\n* otherwise we try to parse it as an int (for ttl) or long (for timestamp). If parsing fails we issue a compiler error.\r\n\r\nI\'m creating a ticket to implement a similar change for `@Insert` ([JAVA-2257](https://datastax-oss.atlassian.net/browse/JAVA-2257)).', 'commenter': 'olim7t'}, {'comment': ""One more thing ðŸ˜ƒ \r\nWith 2257, concatenating won't work anymore for `@Insert.customWhereClause` (because USING comes after). We'll have to use `whereRaw()` (like you initially did on this PR), and therefore it's simpler if we don't require the `WHERE` keyword in the provided string.\r\nSame for `customIfClause` (no `IF` keyword in the string, use `ifRaw` to concatenate).\r\n\r\nAgain I'm creating a ticket to revisit existing annotations ([JAVA-2258](https://datastax-oss.atlassian.net/browse/JAVA-2258)), do it the new way directly in this PR."", 'commenter': 'olim7t'}, {'comment': 'I fixed 2257 on the base branch. You should be able to reuse the two protected methods in `DaoMethodGenerator`.', 'commenter': 'olim7t'}, {'comment': 'I see one potential problem with `ifRaw`  -> I think that there could be a problem with creating the enclosed ifs:\r\nSo the following clause: ""IF a = a AND b =b"" - previously we would just append code. With `ifRaw()` you will need to create two `ifRaw()` statements (I didn\'t test that yet)\r\n\r\nConcerning  `@Update(ttl = ""..."", timestamp = ""..."")` - I agree, I was thinking about that as well. It is more explicit and safe that just string value', 'commenter': 'tomekl007'}, {'comment': '`ifRaw` appends whatever you pass as-is, so if you put the `AND` keyword in your string it should work fine.\r\n```java\r\n@Update(customIfClause = ""a = :a AND b = :b""\r\n```', 'commenter': 'olim7t'}]"
1235,mapper-processor/src/test/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoUpdateMethodGeneratorTest.java,"@@ -0,0 +1,92 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.mapper.processor.dao;
+
+import static com.google.common.truth.Truth.assertThat;
+
+import com.squareup.javapoet.CodeBlock;
+import com.squareup.javapoet.MethodSpec;
+import com.tngtech.java.junit.dataprovider.DataProvider;
+import com.tngtech.java.junit.dataprovider.DataProviderRunner;
+import com.tngtech.java.junit.dataprovider.UseDataProvider;
+import org.junit.Test;
+import org.junit.runner.RunWith;
+
+@RunWith(DataProviderRunner.class)
+public class DaoUpdateMethodGeneratorTest {
+
+  @Test
+  @UseDataProvider(""usingTimestampProvider"")
+  public void should_process_timestamp(String timestamp, String expected) {
+    // given
+    DaoUpdateMethodGenerator daoUpdateMethodGenerator =
+        new DaoUpdateMethodGenerator(null, null, null);
+    MethodSpec.Builder builder = MethodSpec.constructorBuilder();
+
+    // when
+    daoUpdateMethodGenerator.maybeAddTimestamp(timestamp, builder);
+
+    // then
+    assertThat(builder.build().code).isEqualTo(CodeBlock.of(expected));
+  }
+
+  @Test
+  @UseDataProvider(""usingTtlProvider"")
+  public void should_process_ttl(String ttl, String expected) {
+    // given
+    DaoUpdateMethodGenerator daoUpdateMethodGenerator =
+        new DaoUpdateMethodGenerator(null, null, null);
+    MethodSpec.Builder builder = MethodSpec.constructorBuilder();
+
+    // when
+    daoUpdateMethodGenerator.maybeAddTtl(ttl, builder);
+
+    // then
+    assertThat(builder.build().code).isEqualTo(CodeBlock.of(expected));
+  }
+
+  @DataProvider
+  public static Object[][] usingTimestampProvider() {
+    return new Object[][] {
+      {""1"", "".usingTimestamp(1)""},
+      {
+        "":ts"",
+        "".usingTimestamp(com.datastax.oss.driver.api.querybuilder.QueryBuilder.bindMarker(\""ts\""))""","[{'comment': 'Nit: you could create code blocks directly here instead of doing the conversion in the test method.\r\n```suggestion\r\n        CodeBlock.of("".usingTimestamp($T.bindMarker($S))"", QueryBuilder.class, ""ts"")\r\n```\r\nThis is a bit easier to read.', 'commenter': 'olim7t'}]"
1235,mapper-processor/src/test/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoUpdateGeneratorTest.java,"@@ -0,0 +1,85 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.mapper.processor.dao;
+
+import com.datastax.oss.driver.api.mapper.annotations.Update;
+import com.squareup.javapoet.AnnotationSpec;
+import com.squareup.javapoet.MethodSpec;
+import com.squareup.javapoet.ParameterSpec;
+import com.squareup.javapoet.TypeName;
+import com.tngtech.java.junit.dataprovider.DataProvider;
+import com.tngtech.java.junit.dataprovider.DataProviderRunner;
+import com.tngtech.java.junit.dataprovider.UseDataProvider;
+import javax.lang.model.element.Modifier;
+import org.junit.Test;
+import org.junit.runner.RunWith;
+
+@RunWith(DataProviderRunner.class)
+public class DaoUpdateGeneratorTest extends DaoMethodGeneratorTest {","[{'comment': 'Could you merge this with `DaoUpdateMethodGeneratorTest`? I find it confusing to have two classes with almost the same name that test the same target class (also all the other classes that test compiler outputs are called `XxxMethodGeneratorTest`, except this one).', 'commenter': 'olim7t'}]"
1235,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoUpdateMethodGenerator.java,"@@ -0,0 +1,196 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.mapper.processor.dao;
+
+import static com.datastax.oss.driver.internal.mapper.processor.dao.ReturnTypeKind.BOOLEAN;
+import static com.datastax.oss.driver.internal.mapper.processor.dao.ReturnTypeKind.FUTURE_OF_ASYNC_RESULT_SET;
+import static com.datastax.oss.driver.internal.mapper.processor.dao.ReturnTypeKind.FUTURE_OF_BOOLEAN;
+import static com.datastax.oss.driver.internal.mapper.processor.dao.ReturnTypeKind.FUTURE_OF_VOID;
+import static com.datastax.oss.driver.internal.mapper.processor.dao.ReturnTypeKind.RESULT_SET;
+import static com.datastax.oss.driver.internal.mapper.processor.dao.ReturnTypeKind.VOID;
+
+import com.datastax.oss.driver.api.core.cql.BoundStatement;
+import com.datastax.oss.driver.api.core.cql.BoundStatementBuilder;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.api.mapper.annotations.Update;
+import com.datastax.oss.driver.internal.core.util.concurrent.CompletableFutures;
+import com.datastax.oss.driver.internal.mapper.processor.ProcessorContext;
+import com.datastax.oss.driver.internal.mapper.processor.util.generation.GeneratedCodePatterns;
+import com.datastax.oss.driver.internal.querybuilder.update.DefaultUpdate;
+import com.squareup.javapoet.ClassName;
+import com.squareup.javapoet.MethodSpec;
+import java.util.EnumSet;
+import java.util.List;
+import java.util.Optional;
+import javax.lang.model.element.ExecutableElement;
+import javax.lang.model.element.TypeElement;
+import javax.lang.model.element.VariableElement;
+
+public class DaoUpdateMethodGenerator extends DaoMethodGenerator {
+
+  private static final EnumSet<ReturnTypeKind> SUPPORTED_RETURN_TYPES =
+      EnumSet.of(
+          VOID, FUTURE_OF_VOID, RESULT_SET, FUTURE_OF_ASYNC_RESULT_SET, BOOLEAN, FUTURE_OF_BOOLEAN);
+
+  public DaoUpdateMethodGenerator(
+      ExecutableElement methodElement,
+      DaoImplementationSharedCode enclosingClass,
+      ProcessorContext context) {
+    super(methodElement, enclosingClass, context);
+  }
+
+  @Override
+  public Optional<MethodSpec> generate() {
+
+    // Validate the parameters:
+    // - the first one must be the entity.
+    // - the others are completely free-form (they'll be used as additional bind variables)
+    if (methodElement.getParameters().isEmpty()) {
+      context
+          .getMessager()
+          .error(
+              methodElement,
+              ""Wrong number of parameters: %s methods must have at least one"",
+              Update.class.getSimpleName());
+      return Optional.empty();
+    }
+    VariableElement firstParameter = methodElement.getParameters().get(0);
+    TypeElement entityElement = asEntityElement(firstParameter);
+    if (entityElement == null) {
+      context
+          .getMessager()
+          .error(
+              methodElement,
+              ""Invalid parameter type: ""
+                  + ""%s methods must take the entity to update as the first parameter"",
+              Update.class.getSimpleName());
+      return Optional.empty();
+    }
+
+    // Validate the return type:
+    ReturnType returnType = parseReturnType(methodElement.getReturnType());
+    if (!SUPPORTED_RETURN_TYPES.contains(returnType.kind)) {
+      context
+          .getMessager()
+          .error(
+              methodElement,
+              ""Invalid return type: %s methods must return either void, ResultSet or boolean ""
+                  + ""(possibly wrapped in a CompletionStage/CompletableFuture)"",
+              Update.class.getSimpleName());
+      return Optional.empty();
+    }
+
+    // Generate the method:
+    String helperFieldName = enclosingClass.addEntityHelperField(ClassName.get(entityElement));
+    String statementName =
+        enclosingClass.addPreparedStatement(
+            methodElement,
+            (methodBuilder, requestName) ->
+                generatePrepareRequest(methodBuilder, requestName, helperFieldName));
+
+    MethodSpec.Builder methodBuilder = GeneratedCodePatterns.override(methodElement);
+
+    if (returnType.kind.isAsync) {
+      methodBuilder.beginControlFlow(""try"");
+    }
+    methodBuilder.addStatement(
+        ""$T boundStatementBuilder = $L.boundStatementBuilder()"",
+        BoundStatementBuilder.class,
+        statementName);
+
+    List<? extends VariableElement> parameters = methodElement.getParameters();
+    String entityParameterName = parameters.get(0).getSimpleName().toString();
+    methodBuilder.addStatement(
+        ""$L.set($L, boundStatementBuilder)"", helperFieldName, entityParameterName);","[{'comment': 'There is a problem here: if the annotation has a custom WHERE clause, we can\'t set the whole entity at once because there might be no placeholders for PK components.\r\n\r\nYou can observe it by changing the DAO in the integration test like this:\r\n```java\r\n    @Update(customWhereClause = ""id IN (:id1, :id2)"")\r\n    void updateWhereIdIn(Product product, UUID id1, UUID id2);\r\n```\r\n(currently it uses `id IN (:id, :id2)`, which masks the issue)\r\n\r\nSo you need to add a conditional here:\r\n* if `customWhereClause` is empty: OK, do what you are doing right now\r\n* else: you need to bind only the *non-PK* properties of the entity. You can copy what the Delete code does [here](https://github.com/datastax/java-driver/blob/java2078/mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoDeleteMethodGenerator.java#L171), it\'s almost the same except that you need to traverse `getRegularColumns` instead of `getPrimaryKey`.', 'commenter': 'olim7t'}]"
1237,integration-tests/src/test/java/com/datastax/oss/driver/mapper/RunTimeAttributesIT.java,"@@ -0,0 +1,255 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.mapper;
+
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.noRows;
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.query;
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.when;
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.assertj.core.api.Assertions.assertThat;
+
+import com.datastax.oss.driver.api.core.CqlIdentifier;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.DefaultConsistencyLevel;
+import com.datastax.oss.driver.api.mapper.DefaultRunTimeAttributes;
+import com.datastax.oss.driver.api.mapper.DefaultRunTimeAttributesBuilder;
+import com.datastax.oss.driver.api.mapper.RuntimeAttributes;
+import com.datastax.oss.driver.api.mapper.annotations.Dao;
+import com.datastax.oss.driver.api.mapper.annotations.DaoFactory;
+import com.datastax.oss.driver.api.mapper.annotations.DaoKeyspace;
+import com.datastax.oss.driver.api.mapper.annotations.Delete;
+import com.datastax.oss.driver.api.mapper.annotations.Entity;
+import com.datastax.oss.driver.api.mapper.annotations.Insert;
+import com.datastax.oss.driver.api.mapper.annotations.Mapper;
+import com.datastax.oss.driver.api.mapper.annotations.PartitionKey;
+import com.datastax.oss.driver.api.mapper.annotations.Select;
+import com.datastax.oss.driver.api.testinfra.session.SessionRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionUtils;
+import com.datastax.oss.driver.api.testinfra.simulacron.SimulacronRule;
+import com.datastax.oss.protocol.internal.Message;
+import com.datastax.oss.protocol.internal.request.Execute;
+import com.datastax.oss.simulacron.common.cluster.ClusterQueryLogReport;
+import com.datastax.oss.simulacron.common.cluster.ClusterSpec;
+import com.datastax.oss.simulacron.common.cluster.QueryLog;
+import com.google.common.collect.ImmutableMap;
+import com.google.common.collect.Lists;
+import java.nio.ByteBuffer;
+import java.util.Map;
+import java.util.Objects;
+import java.util.UUID;
+import java.util.concurrent.TimeUnit;
+import org.junit.Before;
+import org.junit.ClassRule;
+import org.junit.Test;
+
+public class RunTimeAttributesIT {
+
+  @ClassRule
+  public static SimulacronRule simulacronRule =
+      new SimulacronRule(ClusterSpec.builder().withNodes(1));
+
+  private static SessionRule<CqlSession> sessionRule = SessionRule.builder(simulacronRule).build();
+
+  private static SimpleDao simpleDao;
+
+  private static String PAGING_STATE = ""paging_state"";
+  private static int PAGE_SIZE = 13;
+
+  private static final Simple simple = new Simple(UUID.randomUUID(), ""DATA"");
+
+  @Before
+  public void setup() {
+    simulacronRule.cluster().clearPrimes(true);
+    simulacronRule.cluster().clearLogs();
+  }
+
+  @Test
+  public void should_honor_runtime_attributes_insert() {
+    Map<String, Object> params = ImmutableMap.of(""pk"", simple.getPk(), ""data"", simple.getData());
+    Map<String, String> paramTypes = ImmutableMap.of(""pk"", ""uuid"", ""data"", ""ascii"");
+    simulacronRule
+        .cluster()
+        .prime(
+            when(query(
+                    ""INSERT INTO simple (pk,data) VALUES (:pk,:data)"",
+                    Lists.newArrayList(
+                        com.datastax.oss.simulacron.common.codec.ConsistencyLevel.ONE,
+                        com.datastax.oss.simulacron.common.codec.ConsistencyLevel.ANY),
+                    params,
+                    paramTypes))
+                .then(noRows()));
+    CqlSession session = SessionUtils.newSession(simulacronRule);
+    InventoryMapper inventoryMapper =
+        new RunTimeAttributesIT_InventoryMapperBuilder(session).build();
+    simpleDao = inventoryMapper.simpleDao(sessionRule.keyspace());
+    RuntimeAttributes attributes = buildRunTimeAttributes();
+    simulacronRule.cluster().clearLogs();
+    simpleDao.save(simple, attributes);
+    ClusterQueryLogReport report = simulacronRule.cluster().getLogs();
+    validateQueryOptions(report.getQueryLogs().get(0));
+  }
+
+  @Test
+  public void should_honor_runtime_attributes_delete() {
+    Map<String, Object> params = ImmutableMap.of(""pk"", simple.getPk());
+    Map<String, String> paramTypes = ImmutableMap.of(""pk"", ""uuid"");
+    simulacronRule
+        .cluster()
+        .prime(
+            when(query(
+                    ""DELETE FROM simple WHERE pk=:pk"",
+                    Lists.newArrayList(
+                        com.datastax.oss.simulacron.common.codec.ConsistencyLevel.ONE,
+                        com.datastax.oss.simulacron.common.codec.ConsistencyLevel.ANY),
+                    params,
+                    paramTypes))
+                .then(noRows())
+                .delay(1, TimeUnit.MILLISECONDS));
+    CqlSession session = SessionUtils.newSession(simulacronRule);
+    InventoryMapper inventoryMapper =
+        new RunTimeAttributesIT_InventoryMapperBuilder(session).build();
+    simpleDao = inventoryMapper.simpleDao(sessionRule.keyspace());
+    RuntimeAttributes attributes = buildRunTimeAttributes();
+    simulacronRule.cluster().clearLogs();
+    simpleDao.delete(simple, attributes);
+    ClusterQueryLogReport report = simulacronRule.cluster().getLogs();
+    validateQueryOptions(report.getQueryLogs().get(0));
+  }
+
+  @Test
+  public void should_honor_runtime_attributes_select() {
+    Map<String, Object> params = ImmutableMap.of(""pk"", simple.getPk());
+    Map<String, String> paramTypes = ImmutableMap.of(""pk"", ""uuid"");
+    simulacronRule
+        .cluster()
+        .prime(
+            when(query(
+                    ""SELECT pk,data FROM simple WHERE pk=:pk"",
+                    Lists.newArrayList(
+                        com.datastax.oss.simulacron.common.codec.ConsistencyLevel.ONE,
+                        com.datastax.oss.simulacron.common.codec.ConsistencyLevel.ANY),
+                    params,
+                    paramTypes))
+                .then(noRows())
+                .delay(1, TimeUnit.MILLISECONDS));
+    CqlSession session = SessionUtils.newSession(simulacronRule);
+    InventoryMapper inventoryMapper =
+        new RunTimeAttributesIT_InventoryMapperBuilder(session).build();
+    simpleDao = inventoryMapper.simpleDao(sessionRule.keyspace());
+
+    RuntimeAttributes attributes = buildRunTimeAttributes();
+    simulacronRule.cluster().clearLogs();
+    simpleDao.findByPk(simple.getPk(), attributes);
+    ClusterQueryLogReport report = simulacronRule.cluster().getLogs();
+    validateQueryOptions(report.getQueryLogs().get(0));
+  }
+
+  private RuntimeAttributes buildRunTimeAttributes() {
+    DefaultRunTimeAttributesBuilder builder = DefaultRunTimeAttributes.builder();
+
+    return builder
+        .withConsistencyLevel(DefaultConsistencyLevel.ANY)
+        .withPageSize(PAGE_SIZE)
+        .withSerialConsistencyLevel(DefaultConsistencyLevel.QUORUM)
+        .withPagingState(ByteBuffer.wrap(PAGING_STATE.getBytes(UTF_8)))
+        .build();
+  }
+
+  private void validateQueryOptions(QueryLog log) {
+
+    Message message = log.getFrame().message;
+    assertThat(message).isInstanceOf(Execute.class);
+    Execute queryExecute = (Execute) message;
+    assertThat(queryExecute.options.consistency)
+        .isEqualTo(DefaultConsistencyLevel.ANY.getProtocolCode());
+    assertThat(queryExecute.options.serialConsistency)
+        .isEqualTo(DefaultConsistencyLevel.QUORUM.getProtocolCode());
+    assertThat(queryExecute.options.pageSize).isEqualTo(PAGE_SIZE);
+    String pagingState = UTF_8.decode(queryExecute.options.pagingState).toString();
+    assertThat(pagingState).isEqualTo(PAGING_STATE);
+  }
+
+  @Mapper
+  public interface InventoryMapper {
+    @DaoFactory
+    RunTimeAttributesIT.SimpleDao simpleDao(@DaoKeyspace CqlIdentifier keyspace);
+  }
+
+  @Dao
+  public interface SimpleDao {
+    @Insert
+    void save(Simple simple, RuntimeAttributes attributes);","[{'comment': 'Should we use annotation based mechanism for that as pointed in a ticket?\r\nWhat if someone will want to give that save method Consistency Level `ANY`?\r\nIn the current mechanism, he will need to remember about passing that RuntimeAttributes argument on every call to save even if that will not change (I can hardly imagine an example when someone is using different consistency level for the same save method).\r\nSo should it be:\r\n``` \r\n @Insert(consistencyLevel = ANY)\r\n    void save(Simple simple);\r\n```\r\n?\r\nAlso, I think that we should give our users the ability to set it on the `@Dao` level.\r\n`@Dao(consistencyLevel = ANY)`\r\nwdyt?', 'commenter': 'tomekl007'}, {'comment': 'This was considered and decided against. The ticket has the explanation as to why.', 'commenter': 'GregBestland'}, {'comment': 'Could you give me a link to the discussion?\r\nIn the ticket https://datastax-oss.atlassian.net/browse/JAVA-2139 I see only `Open questions:`\r\n\r\n', 'commenter': 'tomekl007'}, {'comment': ""[JAVA-2138](https://datastax-oss.atlassian.net/browse/JAVA-2138) will introduce method-level annotations.\r\nI hadn't thought of overriding at the DAO level, but why not."", 'commenter': 'olim7t'}]"
1237,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoDeleteMethodGenerator.java,"@@ -164,7 +174,13 @@ public DaoDeleteMethodGenerator(
         ""$T boundStatementBuilder = $L.boundStatementBuilder()"",
         BoundStatementBuilder.class,
         statementName);
-
+    if (runtTimeAttributeParam != null) {
+      if (runtTimeAttributeParam != null) {","[{'comment': 'why two ifs?', 'commenter': 'tomekl007'}, {'comment': ""because I'm a dope. Thanks I'll remove this.\r\n"", 'commenter': 'GregBestland'}, {'comment': 'Still present.', 'commenter': 'olim7t'}]"
1237,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoInsertMethodGenerator.java,"@@ -83,7 +86,12 @@ public DaoInsertMethodGenerator(
               Insert.class.getSimpleName());
       return Optional.empty();
     }
-
+    VariableElement runtTimeAttributeParam = null;","[{'comment': 'I think we should refactor that fragment(from this `#89` line to this line https://github.com/datastax/java-driver/pull/1237/files#diff-2e74e170fe12b9d3b3b34263492d38ddR94) somewhere else and reuse that block of code. \r\nMaybe the method in `DaoBase`?', 'commenter': 'tomekl007'}, {'comment': "":+1:,  at least the detection of the parameter can be extracted to a utility method in `DaoBase`.\r\nThe `sublist` call can stay in each child class because that's more convenient."", 'commenter': 'olim7t'}]"
1237,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoDeleteMethodGenerator.java,"@@ -91,6 +94,13 @@ public DaoDeleteMethodGenerator(
               Delete.class.getSimpleName());
       return Optional.empty();
     }
+    VariableElement runtTimeAttributeParam = null;
+    int lastParamIndex = methodElement.getParameters().size() - 1;
+    VariableElement potRunTimeParam = methodElement.getParameters().get(lastParamIndex);
+    if (potRunTimeParam.asType().toString().equals(RUNTIME_ATTRIBUTES_NAME)) {","[{'comment': 'Use `context.getClassUtils().isSame` instead.', 'commenter': 'olim7t'}]"
1237,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoDeleteMethodGenerator.java,"@@ -164,7 +174,13 @@ public DaoDeleteMethodGenerator(
         ""$T boundStatementBuilder = $L.boundStatementBuilder()"",
         BoundStatementBuilder.class,
         statementName);
-
+    if (runtTimeAttributeParam != null) {
+      if (runtTimeAttributeParam != null) {
+        deleteBuilder.addStatement(
+            ""boundStatementBuilder = DaoBase.populateBoundStatementWithAttributes(boundStatementBuilder,$L)"",","[{'comment': 'Nice, good idea to do that from `DaoBase` :+1:', 'commenter': 'olim7t'}]"
1237,mapper-runtime/src/main/java/com/datastax/oss/driver/api/mapper/RuntimeAttributes.java,"@@ -0,0 +1,78 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.mapper;
+
+import com.datastax.oss.driver.api.core.ConsistencyLevel;
+import com.datastax.oss.driver.api.core.config.DriverExecutionProfile;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import java.nio.ByteBuffer;
+import java.time.Duration;
+import java.util.Map;
+
+public interface RuntimeAttributes {
+
+  boolean isExecutionProfileSet();
+
+  DriverExecutionProfile getExecutionProfile();","[{'comment': 'There should also be a `getExecutionProfileName`.', 'commenter': 'olim7t'}]"
1237,mapper-runtime/src/main/java/com/datastax/oss/driver/api/mapper/RuntimeAttributes.java,"@@ -0,0 +1,78 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.mapper;
+
+import com.datastax.oss.driver.api.core.ConsistencyLevel;
+import com.datastax.oss.driver.api.core.config.DriverExecutionProfile;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import java.nio.ByteBuffer;
+import java.time.Duration;
+import java.util.Map;
+
+public interface RuntimeAttributes {
+
+  boolean isExecutionProfileSet();","[{'comment': ""Can't we use the same semantics as `Statement/Request`?\r\nE.g. `executionProfile == null` means unset, `pageSize <= 0` means use the default, etc."", 'commenter': 'olim7t'}, {'comment': ""I had two reasons I did this.\r\n1. I didn't want to duplicate that logic here. Putting in all the defaults here means that if they ever change we are going to have to change them here as well. I wanted to avoid that. With this solution the defaults can change, and this should continue to function appropriately. The only time the runtime attributes are used is if they the person set them explicitly.\r\n2. Boolean state is more complicated, because we can't really infer anything, so we need a separate value to track whether or not they intended to set anything. "", 'commenter': 'GregBestland'}, {'comment': 'The defaults are very unlikely to change on `Statement` at this point; while technically not a breaking change it would be a serious incompatibility with previous versions. If anything, I find it more intuitive to have something that behaves exactly like the core statement.\r\n\r\nFor boolean values, `Statement` uses `Boolean` for idempotence, with null meaning ""use the config"", and `boolean` for tracing because the default is false.', 'commenter': 'olim7t'}]"
1237,mapper-runtime/src/main/java/com/datastax/oss/driver/api/mapper/RuntimeAttributes.java,"@@ -0,0 +1,78 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.mapper;
+
+import com.datastax.oss.driver.api.core.ConsistencyLevel;
+import com.datastax.oss.driver.api.core.config.DriverExecutionProfile;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import java.nio.ByteBuffer;
+import java.time.Duration;
+import java.util.Map;
+
+public interface RuntimeAttributes {
+
+  boolean isExecutionProfileSet();
+
+  DriverExecutionProfile getExecutionProfile();
+
+  boolean isIdempotentSet();
+
+  boolean isIdempotent();
+
+  boolean isPageSizeSet();
+
+  int getPageSize();
+
+  boolean isConsistencyLevelSet();
+
+  ConsistencyLevel getConstencyLevel();","[{'comment': 'typo: getCons***is***tencyLevel', 'commenter': 'olim7t'}, {'comment': 'English is my second language.', 'commenter': 'GregBestland'}]"
1237,mapper-runtime/src/main/java/com/datastax/oss/driver/api/mapper/RuntimeAttributes.java,"@@ -0,0 +1,78 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.mapper;
+
+import com.datastax.oss.driver.api.core.ConsistencyLevel;
+import com.datastax.oss.driver.api.core.config.DriverExecutionProfile;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import java.nio.ByteBuffer;
+import java.time.Duration;
+import java.util.Map;
+
+public interface RuntimeAttributes {","[{'comment': 'Not sure about the name, I think it would be nice to have ""statement"" somewhere in there.', 'commenter': 'olim7t'}]"
1237,mapper-runtime/src/main/java/com/datastax/oss/driver/api/mapper/DefaultRunTimeAttributes.java,"@@ -0,0 +1,244 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.mapper;
+
+import com.datastax.oss.driver.api.core.ConsistencyLevel;
+import com.datastax.oss.driver.api.core.config.DriverExecutionProfile;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import java.nio.ByteBuffer;
+import java.time.Duration;
+import java.util.Map;
+
+public class DefaultRunTimeAttributes implements RuntimeAttributes {
+
+  private DriverExecutionProfile profile = null;
+
+  private boolean isIdempotent = false;
+
+  private boolean isIdempotentSet = false;
+
+  private int pageSize = -1;
+
+  private ConsistencyLevel consitencyLevel = null;
+
+  private ConsistencyLevel serialConsistencyLevel = null;
+
+  private Duration timeout = null;
+
+  private ByteBuffer routingKey = null;
+
+  private String routingKeyspace = null;
+
+  private boolean tracing = false;
+
+  private boolean tracingSet = false;
+
+  private ByteBuffer pagingState = null;
+
+  private long timeStamp = -1;","[{'comment': 'So far in the driver codebase we don\'t capitalize ""stamp"" (e.g. `Statement.getQueryTimestamp()`).', 'commenter': 'olim7t'}]"
1237,mapper-runtime/src/main/java/com/datastax/oss/driver/api/mapper/DefaultRunTimeAttributes.java,"@@ -0,0 +1,244 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.mapper;
+
+import com.datastax.oss.driver.api.core.ConsistencyLevel;
+import com.datastax.oss.driver.api.core.config.DriverExecutionProfile;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import java.nio.ByteBuffer;
+import java.time.Duration;
+import java.util.Map;
+
+public class DefaultRunTimeAttributes implements RuntimeAttributes {
+
+  private DriverExecutionProfile profile = null;
+
+  private boolean isIdempotent = false;
+
+  private boolean isIdempotentSet = false;
+
+  private int pageSize = -1;
+
+  private ConsistencyLevel consitencyLevel = null;","[{'comment': 'typo: consi***s***tencyLevel', 'commenter': 'olim7t'}]"
1237,mapper-runtime/src/main/java/com/datastax/oss/driver/api/mapper/DefaultRunTimeAttributes.java,"@@ -0,0 +1,244 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.mapper;
+
+import com.datastax.oss.driver.api.core.ConsistencyLevel;
+import com.datastax.oss.driver.api.core.config.DriverExecutionProfile;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import java.nio.ByteBuffer;
+import java.time.Duration;
+import java.util.Map;
+
+public class DefaultRunTimeAttributes implements RuntimeAttributes {","[{'comment': '""Runtime"" should be capitalized consistently (so no capital T here).\r\nSame for the builder.', 'commenter': 'olim7t'}, {'comment': 'To follow the patterns used elsewhere in the driver, I think this class should be immutable.', 'commenter': 'olim7t'}]"
1237,mapper-runtime/src/main/java/com/datastax/oss/driver/api/mapper/DefaultRunTimeAttributes.java,"@@ -0,0 +1,244 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.mapper;","[{'comment': 'Should be in the ""internal"" package.', 'commenter': 'olim7t'}]"
1237,mapper-runtime/src/main/java/com/datastax/oss/driver/api/mapper/RuntimeAttributes.java,"@@ -0,0 +1,78 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.mapper;
+
+import com.datastax.oss.driver.api.core.ConsistencyLevel;
+import com.datastax.oss.driver.api.core.config.DriverExecutionProfile;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import java.nio.ByteBuffer;
+import java.time.Duration;
+import java.util.Map;
+
+public interface RuntimeAttributes {
+
+  boolean isExecutionProfileSet();
+
+  DriverExecutionProfile getExecutionProfile();
+
+  boolean isIdempotentSet();
+
+  boolean isIdempotent();
+
+  boolean isPageSizeSet();
+
+  int getPageSize();
+
+  boolean isConsistencyLevelSet();
+
+  ConsistencyLevel getConstencyLevel();
+
+  boolean isSerialConsistencyLevelSet();
+
+  ConsistencyLevel getSerialConsistencyLevel();
+
+  boolean isTimeoutSet();
+
+  Duration getTimeout();
+
+  boolean isRoutingKeySet();
+
+  ByteBuffer getRoutingKey();
+
+  boolean isRoutingKeyspaceSet();
+
+  String getRoutingKeyspace();
+
+  boolean isTracingSet();
+
+  boolean isTracing();
+
+  boolean isPagingStateSet();
+
+  ByteBuffer getPagingState();
+
+  boolean isTimeStampSet();
+
+  long getTimestamp();
+
+  boolean isNodeSet();
+
+  Node getNode();
+
+  boolean isCustomPayloadSet();
+
+  Map<String, ByteBuffer> getCustomPayload();
+}","[{'comment': 'I would also add a static `build()` method as a shortcut to the builder (similar to what statement interfaces do in the core driver).', 'commenter': 'olim7t'}, {'comment': 'I think you mean add a static builder() method?', 'commenter': 'GregBestland'}, {'comment': 'Yeah sorry I meant builder().', 'commenter': 'olim7t'}]"
1237,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoDeleteMethodGenerator.java,"@@ -91,6 +94,13 @@ public DaoDeleteMethodGenerator(
               Delete.class.getSimpleName());
       return Optional.empty();
     }
+    VariableElement runtTimeAttributeParam = null;
+    int lastParamIndex = methodElement.getParameters().size() - 1;
+    VariableElement potRunTimeParam = methodElement.getParameters().get(lastParamIndex);","[{'comment': 'Is ""pot"" for ""potential""?\r\nYou could also just name this `lastParam`.', 'commenter': 'olim7t'}]"
1237,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoInsertMethodGenerator.java,"@@ -83,7 +86,12 @@ public DaoInsertMethodGenerator(
               Insert.class.getSimpleName());
       return Optional.empty();
     }
-
+    VariableElement runtTimeAttributeParam = null;
+    int lastParamIndex = methodElement.getParameters().size() - 1;
+    VariableElement potRunTimeParam = methodElement.getParameters().get(lastParamIndex);
+    if (potRunTimeParam.asType().toString().equals(RUNTIME_ATTRIBUTES_NAME)) {
+      runtTimeAttributeParam = potRunTimeParam;","[{'comment': ""Any reason why you don't do the sublist call here? That's where it's done in other generators."", 'commenter': 'olim7t'}]"
1237,mapper-runtime/src/main/java/com/datastax/oss/driver/api/mapper/StatementAttributes.java,"@@ -0,0 +1,58 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.mapper;
+
+import com.datastax.oss.driver.api.core.ConsistencyLevel;
+import com.datastax.oss.driver.api.core.config.DriverExecutionProfile;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import java.nio.ByteBuffer;
+import java.time.Duration;
+import java.util.Map;
+
+public interface StatementAttributes {
+
+  DriverExecutionProfile getExecutionProfile();
+
+  String getExecutionProfileName();
+
+  Boolean isIdempotent();
+
+  int getPageSize();
+
+  ConsistencyLevel getConsistencyLevel();
+
+  ConsistencyLevel getSerialConsistencyLevel();
+
+  Duration getTimeout();
+
+  ByteBuffer getRoutingKey();","[{'comment': 'Missing: `Token getRoutingToken()`\r\n(will need to handle it in the builder and `DaoBase` too)', 'commenter': 'olim7t'}]"
1237,mapper-runtime/src/main/java/com/datastax/oss/driver/api/mapper/StatementAttributes.java,"@@ -0,0 +1,58 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.mapper;
+
+import com.datastax.oss.driver.api.core.ConsistencyLevel;
+import com.datastax.oss.driver.api.core.config.DriverExecutionProfile;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import java.nio.ByteBuffer;
+import java.time.Duration;
+import java.util.Map;
+
+public interface StatementAttributes {","[{'comment': 'Could you add javadocs on API types? Here it will be mostly copied from `Request` / `Statement` in the core driver.', 'commenter': 'olim7t'}]"
1238,core/src/main/java/com/datastax/oss/driver/internal/core/type/codec/registry/CachingCodecRegistry.java,"@@ -413,4 +423,14 @@ protected boolean matches(TypeCodec<?> codec, GenericType<?> javaType, boolean i
     TypeCodec<DeclaredT> result = (TypeCodec<DeclaredT>) codec;
     return result;
   }
+
+  // These are mock types that are used as placeholders when we try to find a codec for an empty
+  // Java collection instance. All empty collections are serialized in the same way, so any element
+  // type will do:
+  private static final GenericType<List<Boolean>> JAVA_TYPE_FOR_EMPTY_LISTS =
+      GenericType.listOf(Boolean.class);
+  private static final GenericType<Set<Boolean>> JAVA_TYPE_FOR_EMPTY_SETS =
+      GenericType.setOf(Boolean.class);
+  private static final GenericType<Map<Boolean, Boolean>> JAVA_TYPE_FOR_EMPTY_MAPS =","[{'comment': ""If the type is not important why we don't pass Object?\r\nOr maybe even better we could create marker `class Any` and use it here for a readability "", 'commenter': 'tomekl007'}, {'comment': 'Because the registry will still look for a codec for the element type, and there is no `Object` codec. We pick one of the primitive CQL types.', 'commenter': 'olim7t'}]"
1238,core/src/main/java/com/datastax/oss/driver/internal/core/type/codec/registry/CachingCodecRegistry.java,"@@ -157,6 +157,16 @@ protected CachingCodecRegistry(
     Preconditions.checkNotNull(value);
     LOG.trace(""[{}] Looking up codec for CQL type {} and object {}"", logPrefix, cqlType, value);
 
+    // Special case for empty collections: inspectType won't work because we have nothing to infer","[{'comment': ""Two remarks:\r\n\r\n1) I don't think this is going to solve the problem for all cases. For example, if you change your unit test to this:\r\n\r\n```\r\n    ListType cqlType = DataTypes.listOf(DataTypes.listOf(DataTypes.INT));\r\n    List<List<Integer>> value = Collections.singletonList(Collections.emptyList());\r\n```\r\nThe test will fail because the enclosing collection is not empty.\r\nIMO the correct way to handle this is inside `inspectType` and `getCachedCodec`. You might need to give up on generic codecs for empty collections and actually create a codec that truly accepts the target CQL type.\r\n\r\n2) I also think that this added code snippet is appearing too soon, and in particular it is bypassing all user-registered codecs, but users might have registered special codecs for collections, in which case we should use them if possible."", 'commenter': 'adutra'}, {'comment': ""Good catch.\r\nI like with your solution of inferring the Java type from the CQL type if available, I'm cherry-picking your commits on this PR."", 'commenter': 'olim7t'}]"
1238,core/src/test/java/com/datastax/oss/driver/internal/core/type/codec/registry/CachingCodecRegistryTest.java,"@@ -265,6 +266,26 @@ public void should_create_list_codec_for_cql_type_and_java_value() {
         .lookup(DataTypes.listOf(DataTypes.INT), GenericType.listOf(GenericType.INTEGER), true);
   }
 
+  @Test
+  public void should_create_list_codec_for_cql_type_and_empty_java_value() {
+    ListType cqlType = DataTypes.listOf(DataTypes.listOf(DataTypes.INT));
+    List<List<Integer>> value = Collections.emptyList();","[{'comment': ""Like I said above the tests would be more meaningful if the enclosing collection wasn't empty but the enclosed one was."", 'commenter': 'adutra'}]"
1238,core/src/main/java/com/datastax/oss/driver/internal/core/type/codec/registry/CachingCodecRegistry.java,"@@ -290,6 +291,71 @@ protected boolean matches(TypeCodec<?> codec, GenericType<?> javaType, boolean i
     }
   }
 
+  protected GenericType<?> inferJavaTypeFromCqlType(@NonNull DataType cqlType) {
+    if (cqlType instanceof ListType) {
+      DataType elementType = ((ListType) cqlType).getElementType();
+      return GenericType.listOf(inferJavaTypeFromCqlType(elementType));
+    } else if (cqlType instanceof SetType) {
+      DataType elementType = ((SetType) cqlType).getElementType();
+      return GenericType.setOf(inferJavaTypeFromCqlType(elementType));
+    } else if (cqlType instanceof MapType) {
+      DataType keyType = ((MapType) cqlType).getKeyType();
+      DataType valueType = ((MapType) cqlType).getValueType();
+      return GenericType.mapOf(
+          inferJavaTypeFromCqlType(keyType), inferJavaTypeFromCqlType(valueType));
+    }
+    switch (cqlType.getProtocolCode()) {
+      case ProtocolConstants.DataType.CUSTOM:","[{'comment': 'IDEA reports a few duplicate branches here, we can factor the cases that return the same result.', 'commenter': 'olim7t'}, {'comment': ""Yeah I noticed them too but couldn't decide what is best: to factor together or to leave the case instructions in the order in which they are declared in `ProtocolConstants.DataType`. Your call."", 'commenter': 'adutra'}]"
1238,core/src/main/java/com/datastax/oss/driver/internal/core/type/codec/registry/CachingCodecRegistry.java,"@@ -279,9 +291,74 @@ protected boolean matches(TypeCodec<?> codec, GenericType<?> javaType, boolean i
     }
   }
 
+  protected GenericType<?> inferJavaTypeFromCqlType(@NonNull DataType cqlType) {
+    if (cqlType instanceof ListType) {
+      DataType elementType = ((ListType) cqlType).getElementType();
+      return GenericType.listOf(inferJavaTypeFromCqlType(elementType));","[{'comment': ""don't we want to provide some upper bound for that recursion?\r\nIs it possible that the user will create so many enclosing collections that our code will throw `StackOverflow`?"", 'commenter': 'tomekl007'}, {'comment': 'I think it would blow up server-side first, because you would need to create a collection type with thousands of nested types for that to happen, as in `list<list<list<list<...`.', 'commenter': 'adutra'}]"
1239,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/entity/DefaultEntityFactory.java,"@@ -347,4 +352,17 @@ private CqlNameGenerator buildCqlNameGenerator(TypeElement classElement) {
     }
     return new TypeMirror[0];
   }
+
+  private boolean isTransient(ExecutableElement getMethod, VariableElement field) {
+    Transient getterAnnotation = getMethod.getAnnotation(Transient.class);","[{'comment': ""Of note: we don't check that other annotations, such as `PartitionKey` or `ClusteringKey` are present and emit a warning, is that worth doing?"", 'commenter': 'tolbertam'}, {'comment': ""Yes, if it's not too much work I think it would be worth it."", 'commenter': 'olim7t'}, {'comment': ':+1: will add that', 'commenter': 'tolbertam'}]"
1239,mapper-runtime/src/main/java/com/datastax/oss/driver/api/mapper/annotations/Transient.java,"@@ -0,0 +1,40 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.mapper.annotations;
+
+import java.lang.annotation.ElementType;
+import java.lang.annotation.Retention;
+import java.lang.annotation.RetentionPolicy;
+import java.lang.annotation.Target;
+
+/**
+ * Annotates the field or getter of an {@link Entity} property, to indicate that it will not be
+ * mapped to any column (neither during reads nor writes).
+ *
+ * <p>Example:
+ *
+ * <pre>
+ * &#64;Transient private int notAColumn;
+ * </pre>
+ *
+ * This information is used by the mapper processor to exclude the property from generated queries.
+ *
+ * <p>Please note that {@code Transient} takes precedence over {@link PartitionKey} and {@link
+ * ClusteringColumn} annotations.
+ */
+@Target({ElementType.FIELD, ElementType.METHOD})
+@Retention(RetentionPolicy.CLASS)
+public @interface Transient {}","[{'comment': 'Some things that the 3.x mapper has that this (currently) doesn\'t:\r\n\r\n1. There was a way of specifying a [`PropertyTransienceStrategy`](https://github.com/datastax/java-driver/blob/3.x/driver-mapping/src/main/java/com/datastax/driver/mapping/PropertyTransienceStrategy.java), which allowed one to choose whether properties need to be explicitly annotated or not to be included.  Since we don\'t have a `@Column` annotation, this didn\'t seem appropriate here.\r\n\r\n2. You could provide a global configuration for specifying transient property names, i.e.: [`addTransientPropertyNames`](https://github.com/datastax/java-driver/blob/3.x/driver-mapping/src/main/java/com/datastax/driver/mapping/DefaultPropertyMapper.java#L179-L242).  I think a good solution for this might be to add an entity-scoped annotation named `@TransientProperties` that is akin to jackson-annotations\' [`JsonIgnoreProperties`](https://fasterxml.github.io/jackson-annotations/javadoc/2.6/com/fasterxml/jackson/annotation/JsonIgnoreProperties.html), i.e.:\r\n\r\n```java\r\n@TransientProperties({""a"", ""b"", ""c"", ""d""})\r\n@Entity\r\nclass MyEntity {}\r\n```\r\n\r\nwhat do you think?', 'commenter': 'tolbertam'}, {'comment': '1. I guess the question is: do we need an `@Column` annotation? I\'m not really convinced, mapping all by default sounds pretty good as long as there\'s a proper exclusion mechanism.\r\nAlso this can always be added later as long as it defaults to ""map all"", so no need to rush it.\r\n\r\n2. That sound like a good idea, for those cases where you inherit from a class that you don\'t own, and which has properties that you don\'t want to map.', 'commenter': 'olim7t'}, {'comment': ""1. Agreed that this isn't compelling enough to add a `@Column`.\r\n2. :+1:, I'll add that."", 'commenter': 'tolbertam'}]"
1239,integration-tests/src/test/java/com/datastax/oss/driver/mapper/TransientIT.java,"@@ -0,0 +1,288 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.mapper;
+
+import static com.datastax.oss.driver.assertions.Assertions.assertThat;
+
+import com.datastax.oss.driver.api.core.CqlIdentifier;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.api.mapper.annotations.Dao;
+import com.datastax.oss.driver.api.mapper.annotations.DaoFactory;
+import com.datastax.oss.driver.api.mapper.annotations.DaoKeyspace;
+import com.datastax.oss.driver.api.mapper.annotations.DaoTable;
+import com.datastax.oss.driver.api.mapper.annotations.Entity;
+import com.datastax.oss.driver.api.mapper.annotations.Insert;
+import com.datastax.oss.driver.api.mapper.annotations.Mapper;
+import com.datastax.oss.driver.api.mapper.annotations.PartitionKey;
+import com.datastax.oss.driver.api.mapper.annotations.Select;
+import com.datastax.oss.driver.api.mapper.annotations.Transient;
+import com.datastax.oss.driver.api.testinfra.ccm.CcmRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionRule;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import java.util.concurrent.atomic.AtomicInteger;
+import org.junit.BeforeClass;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.junit.rules.RuleChain;
+import org.junit.rules.TestRule;
+
+@Category(ParallelizableTests.class)
+public class TransientIT {","[{'comment': ""I'm only using `Transient` here because that was what we used in 3.x."", 'commenter': 'tolbertam'}]"
1239,integration-tests/src/test/java/com/datastax/oss/driver/mapper/TransientIT.java,"@@ -0,0 +1,288 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.mapper;
+
+import static com.datastax.oss.driver.assertions.Assertions.assertThat;
+
+import com.datastax.oss.driver.api.core.CqlIdentifier;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.api.mapper.annotations.Dao;
+import com.datastax.oss.driver.api.mapper.annotations.DaoFactory;
+import com.datastax.oss.driver.api.mapper.annotations.DaoKeyspace;
+import com.datastax.oss.driver.api.mapper.annotations.DaoTable;
+import com.datastax.oss.driver.api.mapper.annotations.Entity;
+import com.datastax.oss.driver.api.mapper.annotations.Insert;
+import com.datastax.oss.driver.api.mapper.annotations.Mapper;
+import com.datastax.oss.driver.api.mapper.annotations.PartitionKey;
+import com.datastax.oss.driver.api.mapper.annotations.Select;
+import com.datastax.oss.driver.api.mapper.annotations.Transient;
+import com.datastax.oss.driver.api.testinfra.ccm.CcmRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionRule;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import java.util.concurrent.atomic.AtomicInteger;
+import org.junit.BeforeClass;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.junit.rules.RuleChain;
+import org.junit.rules.TestRule;
+
+@Category(ParallelizableTests.class)
+public class TransientIT {
+
+  private static CcmRule ccm = CcmRule.getInstance();
+
+  private static SessionRule<CqlSession> sessionRule = SessionRule.builder(ccm).build();
+
+  @ClassRule public static TestRule chain = RuleChain.outerRule(ccm).around(sessionRule);
+
+  private static TestMapper mapper;
+
+  private static final AtomicInteger keyProvider = new AtomicInteger(0);
+
+  @BeforeClass
+  public static void setup() {
+    CqlSession session = sessionRule.session();
+
+    session.execute(
+        SimpleStatement.builder(""CREATE TABLE entity(id int primary key, v int)"")
+            .setExecutionProfile(sessionRule.slowProfile())
+            .build());
+
+    mapper = new TransientIT_TestMapperBuilder(session).build();
+  }
+
+  @Test
+  public void should_ignore_field_with_transient_annotated_field() {
+    EntityWithTransientAnnotatedFieldDao dao =
+        mapper.entityWithTransientAnnotatedFieldDao(
+            sessionRule.keyspace(), CqlIdentifier.fromCql(""entity""));","[{'comment': 'Nice use of table parameterization ðŸ‘ ', 'commenter': 'olim7t'}]"
1239,manual/mapper/entities/README.md,"@@ -159,10 +159,21 @@ private int day;
 This information is used by some of the DAO method annotations; for example,
 [@Select](../daos/select/)'s default behavior is to generate a selection by primary key.
 
+#### Transient properties","[{'comment': 'Just realized I need to add doc for `@TransientProperties`, will do', 'commenter': 'tolbertam'}]"
1239,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/entity/DefaultEntityFactory.java,"@@ -347,4 +352,80 @@ private CqlNameGenerator buildCqlNameGenerator(TypeElement classElement) {
     }
     return new TypeMirror[0];
   }
+
+  private boolean isTransient(
+      String propertyName,
+      Set<String> transientProperties,
+      TypeElement classElement,
+      ExecutableElement getMethod,
+      VariableElement field,
+      int partitionKeyIndex,
+      int clusteringColumnIndex) {
+
+    // check if property is present in @TransientProperties
+    Class<?> otherAnnotationClass =
+        partitionKeyIndex >= 0
+            ? PartitionKey.class
+            : clusteringColumnIndex >= 0 ? ClusteringColumn.class : null;","[{'comment': ""We should also check other mapper annotations: `CqlName`, `Computed` and maybe others in the future. I'm not sure if passing a value for each annotation will scale well, so maybe we could just re-read them on the field and getter.\r\n```java\r\nfor (Class<?> annotationClass : ImmutableList.of(PartitionKey.class, ClusteringColumn.class, CqlName.class) {\r\n  if (getMethod.getAnnotation(annotationClass) != null || field != null && field.getAnnotation(annotationClass) != null) {\r\n    ...\r\n  }\r\n}\r\n```\r\nWe could extract that into another method so that it's easier to call multiple times in this method (see my other comment).\r\n```java\r\nvoid checkTransientNotAnnotated(\r\n      ExecutableElement getMethod,\r\n      VariableElement field,\r\n      Element elementToWarnOn)\r\n```\r\nAlso possibly report all offending annotations if there is more than one."", 'commenter': 'olim7t'}, {'comment': 'I like this idea, I think we could take it one step further and use it for more than just transient as I will also face the same issue with `@Computed`.  I realized that the 3.x mapper must have similar logic to prevent multiple conflicting annotations from being used on a property and that is the case: \r\n\r\nhttps://github.com/datastax/java-driver/blob/3.x/driver-mapping/src/main/java/com/datastax/driver/mapping/AnnotationChecks.java#L53-L93\r\n\r\nI think the solution there works but is a bit more complicated than it needs to be.  I""ll try to come up with something.', 'commenter': 'tolbertam'}, {'comment': ""Actually, there are parts of it that I like a lot, such as resolving an annotation in a general way (i.e. pass both the field and the method to resolve the annotation), which will be nice for custom inheritence strategies when we add it.   I'll take a stab at refactoring things in a way to meet my needs, without trying to complicate things too much."", 'commenter': 'tolbertam'}]"
1239,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/entity/DefaultEntityFactory.java,"@@ -347,4 +352,80 @@ private CqlNameGenerator buildCqlNameGenerator(TypeElement classElement) {
     }
     return new TypeMirror[0];
   }
+
+  private boolean isTransient(
+      String propertyName,
+      Set<String> transientProperties,
+      TypeElement classElement,
+      ExecutableElement getMethod,
+      VariableElement field,
+      int partitionKeyIndex,
+      int clusteringColumnIndex) {
+
+    // check if property is present in @TransientProperties
+    Class<?> otherAnnotationClass =
+        partitionKeyIndex >= 0
+            ? PartitionKey.class
+            : clusteringColumnIndex >= 0 ? ClusteringColumn.class : null;
+    if (transientProperties.contains(propertyName)) {
+      if (otherAnnotationClass != null) {
+        context
+            .getMessager()
+            .error(
+                classElement,
+                ""Properties included in @%s can't be annotated with @%s."",
+                TransientProperties.class.getSimpleName(),
+                otherAnnotationClass.getSimpleName());
+      }
+      return true;
+    }
+
+    // check if property is annotated with @Transient
+    Transient getterAnnotation = getMethod.getAnnotation(Transient.class);
+    boolean isTransient;
+    Element transientElement = getMethod;
+    isTransient = getterAnnotation != null;","[{'comment': 'This is really hard to follow. How about treating the cases separately:\r\n```java\r\nif (getMethod.getAnnotation(Transient.class) != null) {\r\n  checkTransientNotAnnotated(field, getMethod, getMethod);\r\n  return true;\r\n} else if (field != null && (field.getAnnotation(Transient.class) != null || field.getModifiers().contains(Modifier.TRANSIENT))) {\r\n  checkTransientNotAnnotated(field, getMethod, field);\r\n  return true;\r\n}\r\nreturn false;\r\n```\r\n\r\nAnd `checkTransientNotAnnotated` would have a more generic message ""Transient properties can\'t be annotated with %s"" to handle both field annotation and modifier.', 'commenter': 'olim7t'}, {'comment': 'Maybe we can go one step further and extract `field != null && (field.getAnnotation(Transient.class) != null || field.getModifiers().contains(Modifier.TRANSIENT)` condition to separete method to name that check (to make it more readable)', 'commenter': 'tomekl007'}]"
1239,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/entity/DefaultEntityFactory.java,"@@ -347,4 +304,126 @@ private CqlNameGenerator buildCqlNameGenerator(TypeElement classElement) {
     }
     return new TypeMirror[0];
   }
+
+  private boolean isTransient(
+      Map<Class<? extends Annotation>, Annotation> annotations,
+      String propertyName,
+      Set<String> transientProperties,
+      ExecutableElement getMethod,
+      VariableElement field) {
+
+    Transient transientAnnotation = (Transient) annotations.get(Transient.class);
+    // check if property name is included in @TransientProperties
+    // -or- if property is annotated with @Transient
+    // -or- if field has transient keyword modifier
+    boolean isTransient =
+        transientProperties.contains(propertyName)
+            || transientAnnotation != null
+            || field != null && field.getModifiers().contains(Modifier.TRANSIENT);
+
+    // if annotations contains an exclusive annotation that isn't transient, raise
+    // an error here.
+    Class<? extends Annotation> exclusiveAnnotation = getExclusiveAnnotation(annotations);
+    if (isTransient && transientAnnotation == null && exclusiveAnnotation != null) {
+      Element element = field != null ? field : getMethod;
+      context
+          .getMessager()
+          .error(
+              element,
+              ""Property that is considered transient cannot be annotated with @%s."",
+              exclusiveAnnotation.getSimpleName());
+    }
+
+    return isTransient;
+  }
+
+  private Set<String> getTransientPropertyNames(TypeElement classElement) {
+    TransientProperties transientProperties = classElement.getAnnotation(TransientProperties.class);
+
+    return transientProperties != null
+        ? Sets.newHashSet(transientProperties.value())
+        : Collections.emptySet();
+  }
+
+  private void reportMultipleAnnotationError(
+      Element element, Class<? extends Annotation> a0, Class<? extends Annotation> a1) {
+    if (a0 == a1) {
+      context
+          .getMessager()
+          .warn(
+              element,
+              ""@%s should be used either on the field or the getter, but not both. ""
+                  + ""The annotation on this field will be ignored."",
+              a0.getSimpleName());
+    } else {
+      context
+          .getMessager()
+          .error(
+              element,
+              ""Properties can't be annotated with both @%s and @%s."",
+              a0.getSimpleName(),
+              a1.getSimpleName());
+    }
+  }
+
+  private Map<Class<? extends Annotation>, Annotation> scanPropertyAnnotations(
+      ExecutableElement getMethod, VariableElement field) {
+    Map<Class<? extends Annotation>, Annotation> annotations = Maps.newHashMap();
+
+    // scan methods first as they should take precedence.
+    scanMethodAnnotations(getMethod, annotations, null);
+    if (field != null) {
+      scanFieldAnnotations(field, annotations, getExclusiveAnnotation(annotations));
+    }
+
+    return ImmutableMap.copyOf(annotations);
+  }
+
+  private Class<? extends Annotation> getExclusiveAnnotation(
+      Map<Class<? extends Annotation>, Annotation> annotations) {
+    for (Class<? extends Annotation> annotationClass : annotations.keySet()) {
+      if (EXCLUSIVE_PROPERTY_ANNOTATIONS.contains(annotationClass)) {
+        return annotationClass;
+      }
+    }
+    return null;
+  }
+
+  private void scanFieldAnnotations(","[{'comment': 'code for `scanFieldAnnotations` and `scanMethodAnnotations` is largely the same, but anticipate that changing with [JAVA-2111](https://datastax-oss.atlassian.net/browse/JAVA-2111)', 'commenter': 'tolbertam'}]"
1239,mapper-processor/src/test/java/com/datastax/oss/driver/internal/mapper/processor/entity/EntityPropertyAnnotationsTest.java,"@@ -105,7 +105,7 @@ public void should_fail_with_expected_error(String expectedError, TypeSpec entit
   public static Object[][] entitiesWithErrors() {
     return new Object[][] {
       {
-        ""Properties can't be annotated with both @PartitionKey and @ClusteringColumn."",
+        ""Properties can't be annotated with both @ClusteringColumn and @PartitionKey."",","[{'comment': 'Was wondering if we had a way to test for compiler errors being surfaced, I should add some more tests as I had been testing manually up to this point, but if it expedites things more I can add tests when i work on [JAVA-2111](https://datastax-oss.atlassian.net/browse/JAVA-2111)', 'commenter': 'tolbertam'}, {'comment': ""See `EntityPropertyAnnotationsTest`.\r\n\r\nThose tests are pretty slow, so try not to add too many (I'm thinking of adding something in the POM to skip them)."", 'commenter': 'olim7t'}, {'comment': 'Mmm forget that, looks like I misunderstood your question.', 'commenter': 'olim7t'}, {'comment': 'No need to expedite this PR, we might as well add all necessary tests now.', 'commenter': 'olim7t'}]"
1241,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/util/generation/GeneratedCodePatterns.java,"@@ -259,6 +292,113 @@ public static void setValue(
     }
   }
 
+  private static void generateParameterizedSet(
+      CodeBlock cqlName,
+      CodeBlock valueExtractor,
+      String targetName,
+      MethodSpec.Builder methodBuilder,
+      TypeName typeName,
+      BindableHandlingSharedCode enclosingClass,
+      CodeBlock nullSavingStrategy) {
+    generateSetWithNullSavingStrategy(
+        cqlName,
+        valueExtractor,
+        targetName,
+        methodBuilder,
+        nullSavingStrategy,
+        CodeBlock.of(
+            ""$1L = $1L.set($2L, $3L, $4L)"",
+            targetName,
+            cqlName,
+            valueExtractor,
+            enclosingClass.addGenericTypeConstant(typeName)));
+  }
+
+  private static void generateSetWithClass(
+      CodeBlock cqlName,
+      CodeBlock valueExtractor,
+      String targetName,
+      MethodSpec.Builder methodBuilder,
+      TypeName typeName,
+      CodeBlock nullSavingStrategy) {
+
+    generateSetWithNullSavingStrategy(
+        cqlName,
+        valueExtractor,
+        targetName,
+        methodBuilder,
+        nullSavingStrategy,
+        CodeBlock.of(
+            ""$1L = $1L.set($2L, $3L, $4T.class)"", targetName, cqlName, valueExtractor, typeName));
+  }
+
+  private static void generatePrimitiveSet(
+      CodeBlock cqlName,
+      CodeBlock valueExtractor,
+      String targetName,
+      MethodSpec.Builder methodBuilder,
+      String primitiveAccessor,
+      CodeBlock nullSavingStrategy) {
+
+    // todo how to handle primitive type getters? (int id == null is not possible)","[{'comment': '@olim7t should we handle primitive types?\r\nPrimitive types cannot have null value so I think null handling strategy does not apply', 'commenter': 'tomekl007'}, {'comment': 'that sounds right to me, no special logic should be needed for primitives :+1:', 'commenter': 'tolbertam'}, {'comment': 'Agreed. If you want the ability to do partial updates on a property, it should be mapped to the boxed type.', 'commenter': 'olim7t'}]"
1241,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/util/generation/GeneratedCodePatterns.java,"@@ -162,7 +168,25 @@ public static void setValue(
       String targetName,
       MethodSpec.Builder methodBuilder,
       BindableHandlingSharedCode enclosingClass) {
+    // invoke with SET_TO_NULL to preserve default behaviour when retrieving an entity
+    setValue(
+        cqlName,
+        type,
+        valueExtractor,
+        targetName,
+        methodBuilder,
+        enclosingClass,
+        CodeBlock.of(""$T.$L"", NullSavingStrategy.class, NullSavingStrategy.SET_TO_NULL));","[{'comment': 'This method is using for retrieval of the entity (`@Select`) and `@Delete` - it does not apply to save so I fall back to default - `NullSavingStrategy.SET_TO_NULL` - it means that it just set nulls on the retrieved entity.\r\nOther solution will be to create `CodeBlock.of(""noOp"")` and add `else if` branch here:\r\nhttps://github.com/datastax/java-driver/pull/1241/files#diff-8992e8a0bae64eda9aaf4ff5012fa95fR378\r\nand handle that case exactly like previously but I don\'t think that we need to add that complexity. Let me know wdyt', 'commenter': 'tomekl007'}, {'comment': ""Its an interesting decision point because it's possible that fields may be initialized with some non-null value in their default constructors.  However, conceptually, I think what you are doing is the right thing here.  If the value is unset in C*, it should be effectively null in the entity itself."", 'commenter': 'tolbertam'}]"
1241,mapper-runtime/src/main/java/com/datastax/oss/driver/api/mapper/annotations/Query.java,"@@ -141,4 +141,6 @@
    * was built with. See the top-level javadocs of this class for more explanations.
    */
   String value();
+
+  // todo do we need NullSavingStrategy to Query? (I think that it only retrieves data)","[{'comment': 'I am not sure why in JIRA ticket there was info that we need to handle `NullSavingStrategy` for `@Query`.\r\nIn `QueryIT` I see that all queries are `SELECT` or `DELETE` so `nullSavingStrategy` has no influence over them.\r\nDo we plan to support `INSERT ..` in `@Query` annotation?', 'commenter': 'tomekl007'}, {'comment': '`@Query` is completely free form, it can be whatever you put in the query string.', 'commenter': 'olim7t'}]"
1241,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/util/generation/GeneratedCodePatterns.java,"@@ -259,6 +292,113 @@ public static void setValue(
     }
   }
 
+  private static void generateParameterizedSet(
+      CodeBlock cqlName,
+      CodeBlock valueExtractor,
+      String targetName,
+      MethodSpec.Builder methodBuilder,
+      TypeName typeName,
+      BindableHandlingSharedCode enclosingClass,
+      CodeBlock nullSavingStrategy) {
+    generateSetWithNullSavingStrategy(
+        cqlName,
+        valueExtractor,
+        targetName,
+        methodBuilder,
+        nullSavingStrategy,
+        CodeBlock.of(
+            ""$1L = $1L.set($2L, $3L, $4L)"",
+            targetName,
+            cqlName,
+            valueExtractor,
+            enclosingClass.addGenericTypeConstant(typeName)));
+  }
+
+  private static void generateSetWithClass(
+      CodeBlock cqlName,
+      CodeBlock valueExtractor,
+      String targetName,
+      MethodSpec.Builder methodBuilder,
+      TypeName typeName,
+      CodeBlock nullSavingStrategy) {
+
+    generateSetWithNullSavingStrategy(
+        cqlName,
+        valueExtractor,
+        targetName,
+        methodBuilder,
+        nullSavingStrategy,
+        CodeBlock.of(
+            ""$1L = $1L.set($2L, $3L, $4T.class)"", targetName, cqlName, valueExtractor, typeName));
+  }
+
+  private static void generatePrimitiveSet(
+      CodeBlock cqlName,
+      CodeBlock valueExtractor,
+      String targetName,
+      MethodSpec.Builder methodBuilder,
+      String primitiveAccessor,
+      CodeBlock nullSavingStrategy) {
+
+    // todo how to handle primitive type getters? (int id == null is not possible)
+    //    generateSetWithNullSavingStrategy(cqlName, valueExtractor, targetName, methodBuilder,
+    // nullSavingStrategy,
+    //        CodeBlock.of(
+    //            ""$1L = $1L.set$2L($3L, $4L)"", targetName, primitiveAccessor, cqlName,
+    // valueExtractor));
+    methodBuilder.addStatement(
+        ""$1L = $1L.set$2L($3L, $4L)"",
+        targetName,
+        primitiveAccessor,
+        cqlName,
+        valueExtractor); // ignore nullSaving strategy for now
+  }
+
+  private static void generateSetWithNullSavingStrategy(
+      CodeBlock cqlName,
+      CodeBlock valueExtractor,
+      String targetName,
+      MethodSpec.Builder methodBuilder,
+      CodeBlock nullSavingStrategy,
+      CodeBlock nonNullStatement) {
+    if (nullSavingStrategy.equals(DO_NOT_SET_CODE_BLOCK)) {
+      methodBuilder.beginControlFlow(""if($1L == null)"", valueExtractor);","[{'comment': ""This essentially generates:\r\n\r\n```java\r\nif (___ == null) {\r\n  // do not set this field\r\n} else {\r\n  __nonNullStatement__\r\n}\r\n```\r\n\r\nCouldn't it just  be rewritten as \r\n\r\n```java\r\nif (___ != null) {\r\n  __nonNullStatement__\r\n}\r\n```"", 'commenter': 'tolbertam'}]"
1241,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/util/generation/GeneratedCodePatterns.java,"@@ -259,6 +292,113 @@ public static void setValue(
     }
   }
 
+  private static void generateParameterizedSet(
+      CodeBlock cqlName,
+      CodeBlock valueExtractor,
+      String targetName,
+      MethodSpec.Builder methodBuilder,
+      TypeName typeName,
+      BindableHandlingSharedCode enclosingClass,
+      CodeBlock nullSavingStrategy) {
+    generateSetWithNullSavingStrategy(
+        cqlName,
+        valueExtractor,
+        targetName,
+        methodBuilder,
+        nullSavingStrategy,
+        CodeBlock.of(
+            ""$1L = $1L.set($2L, $3L, $4L)"",
+            targetName,
+            cqlName,
+            valueExtractor,
+            enclosingClass.addGenericTypeConstant(typeName)));
+  }
+
+  private static void generateSetWithClass(
+      CodeBlock cqlName,
+      CodeBlock valueExtractor,
+      String targetName,
+      MethodSpec.Builder methodBuilder,
+      TypeName typeName,
+      CodeBlock nullSavingStrategy) {
+
+    generateSetWithNullSavingStrategy(
+        cqlName,
+        valueExtractor,
+        targetName,
+        methodBuilder,
+        nullSavingStrategy,
+        CodeBlock.of(
+            ""$1L = $1L.set($2L, $3L, $4T.class)"", targetName, cqlName, valueExtractor, typeName));
+  }
+
+  private static void generatePrimitiveSet(
+      CodeBlock cqlName,
+      CodeBlock valueExtractor,
+      String targetName,
+      MethodSpec.Builder methodBuilder,
+      String primitiveAccessor,
+      CodeBlock nullSavingStrategy) {
+
+    // todo how to handle primitive type getters? (int id == null is not possible)
+    //    generateSetWithNullSavingStrategy(cqlName, valueExtractor, targetName, methodBuilder,
+    // nullSavingStrategy,
+    //        CodeBlock.of(
+    //            ""$1L = $1L.set$2L($3L, $4L)"", targetName, primitiveAccessor, cqlName,
+    // valueExtractor));
+    methodBuilder.addStatement(
+        ""$1L = $1L.set$2L($3L, $4L)"",
+        targetName,
+        primitiveAccessor,
+        cqlName,
+        valueExtractor); // ignore nullSaving strategy for now
+  }
+
+  private static void generateSetWithNullSavingStrategy(
+      CodeBlock cqlName,
+      CodeBlock valueExtractor,
+      String targetName,
+      MethodSpec.Builder methodBuilder,
+      CodeBlock nullSavingStrategy,
+      CodeBlock nonNullStatement) {
+    if (nullSavingStrategy.equals(DO_NOT_SET_CODE_BLOCK)) {
+      methodBuilder.beginControlFlow(""if($1L == null)"", valueExtractor);
+      methodBuilder.addComment(""do not set this field"");
+      methodBuilder.endControlFlow();
+      methodBuilder.beginControlFlow(""else"");
+      methodBuilder.addStatement(nonNullStatement);
+      methodBuilder.endControlFlow();
+    } else if (nullSavingStrategy.equals(SET_TO_NULL_CODE_BLOCK)) {
+      methodBuilder.beginControlFlow(""if($1L == null)"", valueExtractor);
+      methodBuilder.addStatement(""$1L = $1L.setToNull($2L)"", targetName, cqlName);
+      methodBuilder.endControlFlow();
+      methodBuilder.beginControlFlow(""else"");
+      methodBuilder.addStatement(nonNullStatement);
+      methodBuilder.endControlFlow();
+    } else {
+      // the nullSavingStrategy is dynamic - we cannot infer it at compile time
+      methodBuilder.beginControlFlow(""if($1L == null)"", valueExtractor);
+      methodBuilder.beginControlFlow(
+          ""if($1L.equals($2T.$3L))"",
+          nullSavingStrategy,
+          NullSavingStrategy.class,
+          NullSavingStrategy.SET_TO_NULL);
+      methodBuilder.addStatement(""$1L = $1L.setToNull($2L)"", targetName, cqlName);
+      methodBuilder.endControlFlow();
+      methodBuilder.beginControlFlow(","[{'comment': ""Similarly, this else if branch isn't needed if it doesn't doing anything right?"", 'commenter': 'tolbertam'}, {'comment': 'Agreed (referring to Andy\'s comment). Example of generated code (in `InventoryITBase_ProductHelper__MapperGenerated.set`):\r\n```java\r\n    if(entity.getId() == null) {\r\n      if(nullSavingStrategy.equals(NullSavingStrategy.SET_TO_NULL)) {\r\n        target = target.setToNull(""id"");\r\n      }\r\n      else if(nullSavingStrategy.equals(NullSavingStrategy.DO_NOT_SET)) {\r\n        // do not set this field\r\n      }\r\n    }\r\n    else {\r\n      target = target.set(""id"", entity.getId(), UUID.class);\r\n    }\r\n```\r\nCould be simplified as:\r\n```java\r\nif (entity.getId() != null || nullSavingStrategy == NullSavingStrategy.SET_TO_NULL) {\r\n  target = target.set(""id"", entity.getId(), UUID.class);\r\n}\r\n```', 'commenter': 'olim7t'}]"
1241,mapper-runtime/src/main/java/com/datastax/oss/driver/api/mapper/annotations/Insert.java,"@@ -125,4 +126,11 @@
    * the rules of {@link Long#parseLong(String)}), the mapper will issue a compile-time warning.
    */
   String timestamp() default """";
+
+  /**
+   * The nullSavingStrategy to use in the generated INSERT query.
+   *
+   * <p>The default value for @Insert is {@link NullSavingStrategy#SET_TO_NULL}
+   */
+  NullSavingStrategy nullSavingStrategy() default NullSavingStrategy.SET_TO_NULL;","[{'comment': ""I understand that this is effectively needed because protocol V3 does not support leaving values unset, but this being the default was a truly sharp edge for the mapper in 3.x.\r\n\r\nI know that C* 2.1 is still very popular, but I wonder if there's anything we should do to help mitigate our users dumping a ton of tombstones into their tables.\r\n\r\nOne possibility is that we could make `NullSavingStrategy.DO_NOT_SET` the default, and if is protocol V3 is selected, throw a preemptive runtime error earlier with a descriptive error.  I think this is better than adding tombstones unknowingly, and as more users embrace C 3.0+ this becomes a non-issue.   This wasn't done in the 3.x mapper because we added this capability after the mapper was originally released and we didn't want to change default behavior, but now that we're doing a major revision, changing the default is worth considering.\r\n\r\nThoughts?"", 'commenter': 'tolbertam'}, {'comment': ""I agree with having 3.0-friendly defaults, and throwing early for lower versions (probably from the DAO's `initAsync`).\r\n\r\nI think the default should depend on the annotation: for `@Update`, `DO_NOT_SET` feels intuitive for the partial update behavior, but with `@Insert`/`@SetEntity` I think I would expect everything to be saved/bound so `SET_TO_NULL`. `@Query` is less obvious as the query can be anything, I would go with `SET_TO_NULL` too.\r\n\r\n(PS - just to be clear, that's how @tomekl007 did it already)"", 'commenter': 'olim7t'}, {'comment': ""> for `@Update`, DO_NOT_SET feels intuitive for the partial update behavior\r\n\r\nAh, I did not realize `@Update` was implemented in this way.\r\n\r\n> with @Insert/@SetEntity I  think I would expect everything to be saved/bound so SET_TO_NULL\r\n\r\nI don't think we should treat the null setting behavior based on the kind of mutation operation differently as I think in either case we are being opinionated about whether the user intends to apply a tombstone or not when really we can't be sure.\r\n\r\nOne could even argue that setting to null on update makes more sense than insert as creating a tombstone is meaningful in that its a marker that can mean something is  deleted which is important when merging cells and determining if replicas are consistent.  Not creating a tombstone does not have that impact, which is why you could argue that inserts shouldn't create them."", 'commenter': 'tolbertam'}, {'comment': '- Concerning catching exception add adding custom: \r\n don\'t you think that this:\r\n`Unset value at index 0. If you want this value to be null, please set it to null explicitly.`\r\nis self-explanatory? (I\'ve added test to capture that for 2.1:\r\nhttps://github.com/datastax/java-driver/pull/1241/commits/d1042844209bbad29945e2dbcbd7aa7c0d135e11#diff-f21dbbd50b1f380dcadfd0e026c9e219R69)\r\nAlso as @tolbertam pointed out it is generated at the client side so there is no roundtrip involved.\r\n\r\n- Concerning defaults - I agree with @tolbertam that it is better to have the same default for all cases.\r\n\r\n>I don\'t think we should treat the null setting behavior based on the kind of mutation operation differently as I think in either case we are being opinionated about whether the user intends to apply a tombstone or not when really we can\'t be sure.\r\n\r\nSetting it to `DO_NOT_SET` for `@Update` is our guess and may surprise users when for example they will choose to use firstly `@Query(""UPDATE a ...)` and then they will decide to migrate `@Update`. In that case, the null saving strategy will change to `SET_TO_NULL` and they may be surprised by that fact. \r\nSo maybe we should pick one default and use for all operations? (`@Query`, `@SetEntity`, `@Update` and `@Insert`)?\r\nAnother aspect is that I think we should instruct our users to set that nullSavingStrategy parameter explicitly - it will enforce them to make a conscious decision and read about those strategies.', 'commenter': 'tomekl007'}]"
1241,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/util/generation/GeneratedCodePatterns.java,"@@ -259,6 +292,113 @@ public static void setValue(
     }
   }
 
+  private static void generateParameterizedSet(
+      CodeBlock cqlName,
+      CodeBlock valueExtractor,
+      String targetName,
+      MethodSpec.Builder methodBuilder,
+      TypeName typeName,
+      BindableHandlingSharedCode enclosingClass,
+      CodeBlock nullSavingStrategy) {
+    generateSetWithNullSavingStrategy(
+        cqlName,
+        valueExtractor,
+        targetName,
+        methodBuilder,
+        nullSavingStrategy,
+        CodeBlock.of(
+            ""$1L = $1L.set($2L, $3L, $4L)"",
+            targetName,
+            cqlName,
+            valueExtractor,
+            enclosingClass.addGenericTypeConstant(typeName)));
+  }
+
+  private static void generateSetWithClass(
+      CodeBlock cqlName,
+      CodeBlock valueExtractor,
+      String targetName,
+      MethodSpec.Builder methodBuilder,
+      TypeName typeName,
+      CodeBlock nullSavingStrategy) {
+
+    generateSetWithNullSavingStrategy(
+        cqlName,
+        valueExtractor,
+        targetName,
+        methodBuilder,
+        nullSavingStrategy,
+        CodeBlock.of(
+            ""$1L = $1L.set($2L, $3L, $4T.class)"", targetName, cqlName, valueExtractor, typeName));
+  }
+
+  private static void generatePrimitiveSet(
+      CodeBlock cqlName,
+      CodeBlock valueExtractor,
+      String targetName,
+      MethodSpec.Builder methodBuilder,
+      String primitiveAccessor,
+      CodeBlock nullSavingStrategy) {
+
+    // todo how to handle primitive type getters? (int id == null is not possible)
+    //    generateSetWithNullSavingStrategy(cqlName, valueExtractor, targetName, methodBuilder,
+    // nullSavingStrategy,
+    //        CodeBlock.of(
+    //            ""$1L = $1L.set$2L($3L, $4L)"", targetName, primitiveAccessor, cqlName,
+    // valueExtractor));
+    methodBuilder.addStatement(
+        ""$1L = $1L.set$2L($3L, $4L)"",
+        targetName,
+        primitiveAccessor,
+        cqlName,
+        valueExtractor); // ignore nullSaving strategy for now
+  }
+
+  private static void generateSetWithNullSavingStrategy(
+      CodeBlock cqlName,
+      CodeBlock valueExtractor,
+      String targetName,
+      MethodSpec.Builder methodBuilder,
+      CodeBlock nullSavingStrategy,
+      CodeBlock nonNullStatement) {
+    if (nullSavingStrategy.equals(DO_NOT_SET_CODE_BLOCK)) {
+      methodBuilder.beginControlFlow(""if($1L == null)"", valueExtractor);
+      methodBuilder.addComment(""do not set this field"");
+      methodBuilder.endControlFlow();
+      methodBuilder.beginControlFlow(""else"");","[{'comment': 'Another thought: if we leave values unset, this will raise an error:\r\n\r\n```\r\njava.lang.IllegalStateException: Unset value at index 1. If you want this value to be null, please set it to null explicitly.\r\n```\r\n\r\nFortunately, we never send the query to the server, so I think that\'s how we want this to behave.\r\n\r\nBut I think we don\'t have any exposure to this in our testing because all mapper tests that use this option have a `@CassandraRequirement(min=""3.4"", description = ""Creates a SASI index"")` annotation.  We should probably reorg our tests so we can run as much of it on older versions as possible.  I think this would touch a lot of classes, so we should probably do this towards the end of development in this phase.', 'commenter': 'tolbertam'}, {'comment': ""Yes, I think that `java.lang.IllegalStateException: Unset value at index 1. If you want this value to be null, please set it to null explicitly.` is expected in this case.\r\nI am +1 on extending our tests all annotations for Cassandra 2, 3 and 4. \r\nI think we should create a separate ticket to reorg our tests and cover 2 and 4 as well.\r\nDo you want to create such a ticket or should I do it?\r\nI've added an integration test for `nullSavingStraetgy` that is covering Cassandra 2.1 and\r\ncapturing that exception: `java.lang.IllegalStateException: Unset value at index 1. If you want this value to be null, please set it to null explicitly.`\r\nPlease see this commit:\r\nhttps://github.com/datastax/java-driver/pull/1241/commits/d1042844209bbad29945e2dbcbd7aa7c0d135e11"", 'commenter': 'tomekl007'}]"
1241,integration-tests/src/test/java/com/datastax/oss/driver/mapper/InsertIT.java,"@@ -238,6 +267,12 @@ public void should_insert_entity_if_not_exists_returning_optional_asynchronously
     @Insert
     void save(Product product);
 
+    @Insert(nullSavingStrategy = NullSavingStrategy.DO_NOT_SET)","[{'comment': ""In mapper 3.x, this could be configured at the mapper level ([mapper options](https://docs.datastax.com/en/developer/java-driver/3.7/manual/object_mapper/using/#mapper-options)), so all save operations could behave in a consistent way.   I think we'd also want a similar concept in the new mapper as well right?"", 'commenter': 'tolbertam'}, {'comment': 'We can\'t really propagate annotations from the mapper to DAOs. A DAO is processed independently, it doesn\'t know which mapper it ""belongs"" to. In fact, more than one mapper could return a given DAO interface.', 'commenter': 'olim7t'}, {'comment': 'Yes, I was also thinking about it but as Olivier mentioned - it is processed on a different level.', 'commenter': 'tomekl007'}]"
1241,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/entity/EntityHelperSetMethodGenerator.java,"@@ -63,6 +65,7 @@ public EntityHelperSetMethodGenerator(
             .addTypeVariable(settableT)
             .addParameter(ParameterSpec.builder(entityDefinition.getClassName(), ""entity"").build())
             .addParameter(ParameterSpec.builder(settableT, ""target"").build())
+            .addParameter(ParameterSpec.builder(nullSavingStrategyT, ""nullSavingStrategy"").build())","[{'comment': 'You don\'t need to explicitly build a type name, the builder accepts a simple class:\r\n```suggestion\r\n            .addParameter(ParameterSpec.builder(NullSavingStrategy.class, ""nullSavingStrategy"").build())\r\n```\r\n(I only did it for `SettableT` because it\'s more complex)', 'commenter': 'olim7t'}]"
1241,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/util/generation/GeneratedCodePatterns.java,"@@ -259,6 +292,113 @@ public static void setValue(
     }
   }
 
+  private static void generateParameterizedSet(
+      CodeBlock cqlName,
+      CodeBlock valueExtractor,
+      String targetName,
+      MethodSpec.Builder methodBuilder,
+      TypeName typeName,
+      BindableHandlingSharedCode enclosingClass,
+      CodeBlock nullSavingStrategy) {
+    generateSetWithNullSavingStrategy(
+        cqlName,
+        valueExtractor,
+        targetName,
+        methodBuilder,
+        nullSavingStrategy,
+        CodeBlock.of(
+            ""$1L = $1L.set($2L, $3L, $4L)"",
+            targetName,
+            cqlName,
+            valueExtractor,
+            enclosingClass.addGenericTypeConstant(typeName)));
+  }
+
+  private static void generateSetWithClass(
+      CodeBlock cqlName,
+      CodeBlock valueExtractor,
+      String targetName,
+      MethodSpec.Builder methodBuilder,
+      TypeName typeName,
+      CodeBlock nullSavingStrategy) {
+
+    generateSetWithNullSavingStrategy(
+        cqlName,
+        valueExtractor,
+        targetName,
+        methodBuilder,
+        nullSavingStrategy,
+        CodeBlock.of(
+            ""$1L = $1L.set($2L, $3L, $4T.class)"", targetName, cqlName, valueExtractor, typeName));
+  }
+
+  private static void generatePrimitiveSet(
+      CodeBlock cqlName,
+      CodeBlock valueExtractor,
+      String targetName,
+      MethodSpec.Builder methodBuilder,
+      String primitiveAccessor,
+      CodeBlock nullSavingStrategy) {
+
+    // todo how to handle primitive type getters? (int id == null is not possible)
+    //    generateSetWithNullSavingStrategy(cqlName, valueExtractor, targetName, methodBuilder,
+    // nullSavingStrategy,
+    //        CodeBlock.of(
+    //            ""$1L = $1L.set$2L($3L, $4L)"", targetName, primitiveAccessor, cqlName,
+    // valueExtractor));
+    methodBuilder.addStatement(
+        ""$1L = $1L.set$2L($3L, $4L)"",
+        targetName,
+        primitiveAccessor,
+        cqlName,
+        valueExtractor); // ignore nullSaving strategy for now
+  }
+
+  private static void generateSetWithNullSavingStrategy(
+      CodeBlock cqlName,
+      CodeBlock valueExtractor,
+      String targetName,
+      MethodSpec.Builder methodBuilder,
+      CodeBlock nullSavingStrategy,
+      CodeBlock nonNullStatement) {
+    if (nullSavingStrategy.equals(DO_NOT_SET_CODE_BLOCK)) {
+      methodBuilder.beginControlFlow(""if($1L == null)"", valueExtractor);
+      methodBuilder.addComment(""do not set this field"");
+      methodBuilder.endControlFlow();
+      methodBuilder.beginControlFlow(""else"");
+      methodBuilder.addStatement(nonNullStatement);
+      methodBuilder.endControlFlow();
+    } else if (nullSavingStrategy.equals(SET_TO_NULL_CODE_BLOCK)) {
+      methodBuilder.beginControlFlow(""if($1L == null)"", valueExtractor);
+      methodBuilder.addStatement(""$1L = $1L.setToNull($2L)"", targetName, cqlName);
+      methodBuilder.endControlFlow();
+      methodBuilder.beginControlFlow(""else"");
+      methodBuilder.addStatement(nonNullStatement);
+      methodBuilder.endControlFlow();
+    } else {
+      // the nullSavingStrategy is dynamic - we cannot infer it at compile time
+      methodBuilder.beginControlFlow(""if($1L == null)"", valueExtractor);","[{'comment': 'Could you add a space after `if` in the generated code? That\'s how it\'s formatted everywhere else.\r\n```suggestion\r\n      methodBuilder.beginControlFlow(""if ($1L == null)"", valueExtractor);\r\n```', 'commenter': 'olim7t'}]"
1241,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/util/generation/GeneratedCodePatterns.java,"@@ -259,6 +292,113 @@ public static void setValue(
     }
   }
 
+  private static void generateParameterizedSet(
+      CodeBlock cqlName,
+      CodeBlock valueExtractor,
+      String targetName,
+      MethodSpec.Builder methodBuilder,
+      TypeName typeName,
+      BindableHandlingSharedCode enclosingClass,
+      CodeBlock nullSavingStrategy) {
+    generateSetWithNullSavingStrategy(
+        cqlName,
+        valueExtractor,
+        targetName,
+        methodBuilder,
+        nullSavingStrategy,
+        CodeBlock.of(
+            ""$1L = $1L.set($2L, $3L, $4L)"",
+            targetName,
+            cqlName,
+            valueExtractor,
+            enclosingClass.addGenericTypeConstant(typeName)));
+  }
+
+  private static void generateSetWithClass(
+      CodeBlock cqlName,
+      CodeBlock valueExtractor,
+      String targetName,
+      MethodSpec.Builder methodBuilder,
+      TypeName typeName,
+      CodeBlock nullSavingStrategy) {
+
+    generateSetWithNullSavingStrategy(
+        cqlName,
+        valueExtractor,
+        targetName,
+        methodBuilder,
+        nullSavingStrategy,
+        CodeBlock.of(
+            ""$1L = $1L.set($2L, $3L, $4T.class)"", targetName, cqlName, valueExtractor, typeName));
+  }
+
+  private static void generatePrimitiveSet(
+      CodeBlock cqlName,
+      CodeBlock valueExtractor,
+      String targetName,
+      MethodSpec.Builder methodBuilder,
+      String primitiveAccessor,
+      CodeBlock nullSavingStrategy) {
+
+    // todo how to handle primitive type getters? (int id == null is not possible)
+    //    generateSetWithNullSavingStrategy(cqlName, valueExtractor, targetName, methodBuilder,
+    // nullSavingStrategy,
+    //        CodeBlock.of(
+    //            ""$1L = $1L.set$2L($3L, $4L)"", targetName, primitiveAccessor, cqlName,
+    // valueExtractor));
+    methodBuilder.addStatement(
+        ""$1L = $1L.set$2L($3L, $4L)"",
+        targetName,
+        primitiveAccessor,
+        cqlName,
+        valueExtractor); // ignore nullSaving strategy for now
+  }
+
+  private static void generateSetWithNullSavingStrategy(
+      CodeBlock cqlName,
+      CodeBlock valueExtractor,
+      String targetName,
+      MethodSpec.Builder methodBuilder,
+      CodeBlock nullSavingStrategy,
+      CodeBlock nonNullStatement) {
+    if (nullSavingStrategy.equals(DO_NOT_SET_CODE_BLOCK)) {
+      methodBuilder.beginControlFlow(""if($1L == null)"", valueExtractor);
+      methodBuilder.addComment(""do not set this field"");
+      methodBuilder.endControlFlow();
+      methodBuilder.beginControlFlow(""else"");
+      methodBuilder.addStatement(nonNullStatement);
+      methodBuilder.endControlFlow();
+    } else if (nullSavingStrategy.equals(SET_TO_NULL_CODE_BLOCK)) {
+      methodBuilder.beginControlFlow(""if($1L == null)"", valueExtractor);
+      methodBuilder.addStatement(""$1L = $1L.setToNull($2L)"", targetName, cqlName);","[{'comment': 'You don\'t need to call `setToNull` explicitly, all object setters handle null properly. This generates the following code:\r\n```java\r\n    if (searchString == null) {\r\n      boundStatementBuilder = boundStatementBuilder.setToNull(""searchString"");\r\n    }\r\n    else {\r\n      boundStatementBuilder = boundStatementBuilder.set(""searchString"", searchString, String.class);\r\n    }\r\n```\r\nBut it could just be the same as before:\r\n```\r\nboundStatementBuilder = boundStatementBuilder.set(""searchString"", searchString, String.class);\r\n```', 'commenter': 'olim7t'}]"
1241,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/util/generation/GeneratedCodePatterns.java,"@@ -162,7 +168,25 @@ public static void setValue(
       String targetName,
       MethodSpec.Builder methodBuilder,
       BindableHandlingSharedCode enclosingClass) {
+    // invoke with SET_TO_NULL to preserve default behaviour when retrieving an entity
+    setValue(
+        cqlName,
+        type,
+        valueExtractor,
+        targetName,
+        methodBuilder,
+        enclosingClass,
+        CodeBlock.of(""$T.$L"", NullSavingStrategy.class, NullSavingStrategy.SET_TO_NULL));
+  }
 
+  public static void setValue(
+      CodeBlock cqlName,
+      PropertyType type,
+      CodeBlock valueExtractor,
+      String targetName,
+      MethodSpec.Builder methodBuilder,
+      BindableHandlingSharedCode enclosingClass,
+      CodeBlock nullSavingStrategy) {","[{'comment': 'Passing the strategy as a code block makes things hard to follow. Also I don\'t like how we sometimes artificially build a block that contains a known strategy, just for the sake of comparing it to `DO_NOT_SET_CODE_BLOCK` or `SET_TO_NULL_CODE_BLOCK`.\r\n\r\nThere are 3 different cases in our generated code:\r\n1. There is no strategy. E.g. when we bind parameters of custom clauses for `@Select` or `@Delete`. In this case we want to keep generating the exact same code as before this PR.\r\n2. The strategy comes from a local variable (or parameter) in scope. This is the case when we generate helper methods. As I mentioned in another comment, we want to generate the following code:\r\n    ```java\r\n    if (entity.getId() != null || nullSavingStrategy == NullSavingStrategy.SET_TO_NULL) {\r\n      target = target.set(""id"", entity.getId(), UUID.class);\r\n    }\r\n    ```\r\n3. The strategy is known at compile time. This will be the case when we bind the parameters in `@Query`, because we call `setValue` directly from the main method generation, without going through a helper.\r\n\r\nTo simplify things, we can actually merge 3 into 2 by generating a local variable at the beginning, before we bind the arguments:\r\n```java\r\n    // In the main method generator, initialize the variable to what\'s declared on the annotation:\r\n    NullSavingStrategy nullSavingStrategy = NullSavingStrategy.DO_NOT_SET;\r\n    // Then call setValue to generate this:\r\n    if (entity.getId() != null || nullSavingStrategy == NullSavingStrategy.SET_TO_NULL) {\r\n      target = target.set(""id"", entity.getId(), UUID.class);\r\n    }\r\n```\r\nThis looks a bit weird but whatever. If there\'s a performance impact it will be minimal, the VM will probably optimize it at runtime anyway.\r\n\r\nSo with 2 and 3 merged, we are left with two cases to handle:\r\n1. Don\'t use a strategy, generate the same code as before.\r\n2. Use a strategy that comes from a local variable. I would simplify even further and assume that this variable is always named `nullSavingStrategy`.\r\n\r\nWith this, the last parameter of `setValue` can be a simple boolean: `useNullSavingStrategy`; and we only have two cases to handle in the generated code.\r\n\r\nWDYT?', 'commenter': 'olim7t'}, {'comment': 'I think it is a good idea. \r\nI was a bit worried about the performance impact of generating: \r\n```\r\nif (entity.getId() != null || nullSavingStrategy == NullSavingStrategy.SET_TO_NULL) {\r\n    target = target.set(""id"", entity.getId(), UUID.class);\r\n}\r\n```\r\nif we know up front that `nullSavingStrategy == NullSavingStrategy.DO_NOT_SET`.\r\nBut you are right that JIT should optimize that so the impact should be negligible.\r\nI\'ve implemented it in this way and it simplified the generation of the code a lot.\r\n\r\nOne con of it is that we need to rely on the fact that the `NullSavingStrategy  nullSavingStrategy` variable needs to be present on the method builder. But if you are ok with it I am +1 on that. I\'ve added `nullSavingStrategy` to `@Query` annotation as well.', 'commenter': 'tomekl007'}]"
1241,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/util/generation/GeneratedCodePatterns.java,"@@ -222,7 +245,8 @@ public static void setValue(
           .addStatement(""$T $L = $L.newValue()"", UdtValue.class, udtValueName, udtTypeName);
       String childHelper = enclosingClass.addEntityHelperField(entityClass);
       methodBuilder
-          .addStatement(""$L.set($L, $L)"", childHelper, valueName, udtValueName)
+          .addStatement(
+              ""$L.set($L, $L, $L)"", childHelper, valueName, udtValueName, NULL_SAVING_STRATEGY)","[{'comment': ""The null saving strategy we pass to `EntityHelper.set` doesn't really matter here: the driver doesn't have the ability to send partial UDTs, unset values will be serialized to null anyway (see `UdtCodec`). So for clarity's sake I think we should hard-code this to `DO_NOT_SET` in the generated code.\r\n\r\nSame for collections of entities, so you don't need to propagate a strategy to `convertEntitiesIntoUdts`."", 'commenter': 'olim7t'}]"
1241,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/util/generation/GeneratedCodePatterns.java,"@@ -222,7 +245,8 @@ public static void setValue(
           .addStatement(""$T $L = $L.newValue()"", UdtValue.class, udtValueName, udtTypeName);
       String childHelper = enclosingClass.addEntityHelperField(entityClass);
       methodBuilder
-          .addStatement(""$L.set($L, $L)"", childHelper, valueName, udtValueName)
+          .addStatement(
+              ""$L.set($L, $L, $L)"", childHelper, valueName, udtValueName, NULL_SAVING_STRATEGY)
           .addStatement(""$1L = $1L.setUdtValue($2L, $3L)"", targetName, cqlName, udtValueName)
           .endControlFlow();","[{'comment': 'Just realized that we should also handle null for entities / collections of entities (not specific to this PR, I forgot to do it in the original code).\r\n\r\nSo we should generate an else block here:\r\n```java\r\n    if (value != null) {\r\n      UserDefinedType udtType = (UserDefinedType) target.getType(""dimensions"");\r\n      UdtValue udtValue = udtType.newValue();\r\n      dimensionsHelper.set(value, udtValue, nullSavingStrategy);\r\n      target = target.setUdtValue(""dimensions"", udtValue);\r\n    // new code:\r\n    } else if (nullSavingStrategy == NullSavingStrategy.SET_TO_NULL) {\r\n      target = target.setUdtValue(""dimensions"", null);\r\n    }\r\n```\r\n\r\nSame at line 283 for collections of entities.', 'commenter': 'olim7t'}]"
1241,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoQueryMethodGenerator.java,"@@ -157,8 +158,14 @@ public DaoQueryMethodGenerator(
         BoundStatementBuilder.class,
         statementName);
 
+    Query annotation = methodElement.getAnnotation(Query.class);
+    NullSavingStrategy nullSavingStrategy = annotation.nullSavingStrategy();
+
+    queryBuilder.addStatement(
+        ""$1T nullSavingStrategy = $1T.$2L"", NullSavingStrategy.class, nullSavingStrategy);","[{'comment': 'ðŸ‘ ', 'commenter': 'olim7t'}]"
1241,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoSetEntityMethodGenerator.java,"@@ -117,15 +118,20 @@ public DaoSetEntityMethodGenerator(
     // Generate the method:
     String helperFieldName = enclosingClass.addEntityHelperField(ClassName.get(entityElement));
 
+    NullSavingStrategy nullSavingStrategy =
+        methodElement.getAnnotation(SetEntity.class).nullSavingStrategy();
+
     // Forward to the base injector in the helper:
     return Optional.of(
         GeneratedCodePatterns.override(methodElement)
             .addStatement(
-                ""$L$L.set($L, $L)"",
+                ""$1L$2L.set($3L, $4L, $5T.$6L)"",","[{'comment': ""Nit: you don't strictly need to number placeholders if none of them is used more than once (but if you find it easier to read that's fine with me as well)."", 'commenter': 'olim7t'}]"
1241,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoUpdateMethodGenerator.java,"@@ -119,13 +120,22 @@ public DaoUpdateMethodGenerator(
     List<? extends VariableElement> parameters = methodElement.getParameters();
     String entityParameterName = parameters.get(0).getSimpleName().toString();
 
-    String customWhereClause = methodElement.getAnnotation(Update.class).customWhereClause();
+    Update annotation = methodElement.getAnnotation(Update.class);
+    String customWhereClause = annotation.customWhereClause();
+    NullSavingStrategy nullSavingStrategy = annotation.nullSavingStrategy();
+
+    methodBuilder.addStatement(
+        ""$1T nullSavingStrategy = $1T.$2L"", NullSavingStrategy.class, nullSavingStrategy);
     if (customWhereClause.isEmpty()) {
       // We generated an update by primary key (see maybeAddWhereClause), all entity properties are
       // present as placeholders.
       methodBuilder.addStatement(
-          ""$L.set($L, boundStatementBuilder)"", helperFieldName, entityParameterName);
+          ""$1L.set($2L, boundStatementBuilder, $3L)"",","[{'comment': 'Nit: you can hardcode `nullSavingStrategy` here since it\'s constant\r\n```suggestion\r\n          ""$1L.set($2L, boundStatementBuilder, nullSavingStrategy)"",\r\n```', 'commenter': 'olim7t'}]"
1241,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoUpdateMethodGenerator.java,"@@ -119,13 +120,22 @@ public DaoUpdateMethodGenerator(
     List<? extends VariableElement> parameters = methodElement.getParameters();
     String entityParameterName = parameters.get(0).getSimpleName().toString();
 
-    String customWhereClause = methodElement.getAnnotation(Update.class).customWhereClause();
+    Update annotation = methodElement.getAnnotation(Update.class);
+    String customWhereClause = annotation.customWhereClause();
+    NullSavingStrategy nullSavingStrategy = annotation.nullSavingStrategy();
+
+    methodBuilder.addStatement(
+        ""$1T nullSavingStrategy = $1T.$2L"", NullSavingStrategy.class, nullSavingStrategy);","[{'comment': 'Nit: I would only declare the variable in the `else` below, and hard-code it (like for `Insert`) in the first `if` branch.', 'commenter': 'olim7t'}]"
1242,manual/mapper/entities/README.md,"@@ -159,10 +159,43 @@ private int day;
 This information is used by some of the DAO method annotations; for example,
 [@Select](../daos/select/)'s default behavior is to generate a selection by primary key.
 
+#### Computed properties
+
+Annotating an entity property with [@Computed] indicates that when retrieving data with a 
+[@Select]-annotated method that this property should be set to the result of a computation on the
+Cassandra side, typically a function call:
+
+```java
+private int v;
+
+@Computed(""writetime(v)"")
+private int writeTime;
+```
+
+The CQL return type of the formula must match the type of the property, otherwise an exception 
+will be thrown.
+
+[@Computed] does not support case-sensitivity. If the expression contains case-sensitive column 
+or function names, you'll have to escape them:
+
+```java
+@Computed(""\""myFunction\""(\""myColumn\"")"")
+private int f;
+```
+
+Finally, note that [@Computed] fields are only used for [@Select]-annotated dao methods. Both ","[{'comment': 'Is this ok? Mapper 3.x was similar with `@Computed` not working with `@Accessor`, for which we opened [JAVA-832](https://datastax-oss.atlassian.net/browse/JAVA-832).   I\'m not sure there is an easy way to resolve this without changing query contents.\r\n\r\nOne possible way to make that work is to alias the result to the name of the property.  We can then instruct others that when they make their queries to do the same, i.e.:\r\n\r\n```\r\n@Computed(""ttl(x)"")\r\nprivate int xttl;\r\n```\r\n\r\nWould cause our select query to query this as `ttl(x) as xttl` instead of what i\'m currently doing which is `ttl(x) as ""[alias0]""`.', 'commenter': 'tolbertam'}, {'comment': 'Yeah, I think it would make sense to use `getCqlName()` as the alias, and require that people do the  same in their custom queries.', 'commenter': 'olim7t'}, {'comment': 'Awesome, definitely like this solution, its much more pleasant and also solves my other problem in `EntityHelperGetMethodGenerator` and also opens up computed to be used with the other query-based annotations :+1:', 'commenter': 'tolbertam'}, {'comment': ""Took a stab at that in 3767374 and I think it's a much more elegant solution overall.\r\n\r\nI also made it work such that naming strategy / and @CqlName are regarded.\r\n\r\nUpdated tests to verify `@Query` and `@GetEntity` work assuming you make the right query, and updated the documentation to give instruction on how to do that."", 'commenter': 'tolbertam'}]"
1242,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/entity/EntityHelperGetMethodGenerator.java,"@@ -62,24 +62,34 @@ public EntityHelperGetMethodGenerator(
     String returnName = ""returnValue"";
     getBuilder.addStatement(""$1T $2L = new $1T()"", returnType, returnName);
 
-    for (PropertyDefinition property : entityDefinition.getAllColumns()) {
+    for (PropertyDefinition property : entityDefinition.getAllValues()) {
       PropertyType type = property.getType();
       CodeBlock cqlName = property.getCqlName();
+      CodeBlock cqlResultName = property.getCqlResultName();
       String setterName = property.getSetterName();
       getBuilder.addCode(""\n"");
+      // if the result name is not the same as the name, it's likely this is a computed","[{'comment': 'Not really happy with this, namely because throwing exceptions can be costly, but with `GettableByName` there is no way to determine if a definition is present without throwing an exception (`firstIndexOf` will throw an `IllegalArgumentException`).   If this was a `Row`, i could use `getDefinitions`.\r\n\r\nAlternatively if we based alias names on property names (as described [here](https://github.com/tolbertam)) we could just remove this logic and assume the value is present, and if not fail like we would do for any other column.', 'commenter': 'tolbertam'}, {'comment': 'this is no longer an issue as this special logic is not needed.', 'commenter': 'tolbertam'}]"
1242,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/entity/ComputedPropertyDefinition.java,"@@ -0,0 +1,74 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.mapper.processor.entity;
+
+import com.datastax.oss.driver.internal.mapper.processor.util.generation.PropertyType;
+import com.squareup.javapoet.CodeBlock;
+import java.util.Optional;
+
+public class ComputedPropertyDefinition implements PropertyDefinition {
+
+  private final CodeBlock cqlName;
+  private final CodeBlock cqlResultName;
+  private final String getterName;
+  private final String setterName;
+  private final PropertyType type;
+
+  public ComputedPropertyDefinition(
+      String javaName,
+      Optional<String> customCqlName,
+      String formula,
+      String getterName,
+      String setterName,
+      PropertyType type,
+      CqlNameGenerator cqlNameGenerator) {
+    this.cqlResultName =
+        customCqlName
+            .map(n -> CodeBlock.of(""$S"", n))
+            .orElse(cqlNameGenerator.buildCqlName(javaName));
+    // the queried name is ""formula as alias""
+    this.cqlName =
+        CodeBlock.of(""$S + $S +"", formula, "" as "").toBuilder().add(this.cqlResultName).build();","[{'comment': 'Is there a better way to do this?  The generated code looks like:\r\n\r\n```java\r\n    return selectFrom\r\n        .column(""id"")\r\n        .column(""c_id"")\r\n        .column(""v"")\r\n        .raw(""writetime(v)"" + "" as "" +""writetime"")\r\n        .raw(""ttl(v)"" + "" as "" +""myttl"");\r\n```\r\n\r\nWhich works, but would have been nice to do it as one clean string constant.\r\n', 'commenter': 'tolbertam'}, {'comment': 'You can alias raw expressions:\r\n```java\r\n.raw(""writetime(v)"").as(""writetime"")\r\n.raw(""ttl(v)"").as(""myttl"");\r\n```', 'commenter': 'olim7t'}, {'comment': ""Nice, that's definitely better.  It will require some reorienting of `ComputedPropertyDefinition` and how it is used in `EntityHelperSelectStartMethodGenerator`, but I should probably do that anyways."", 'commenter': 'tolbertam'}, {'comment': 'Was a little easier to do that than i originally anticipated (6b24969)', 'commenter': 'tolbertam'}]"
1242,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/entity/DefaultEntityFactory.java,"@@ -81,6 +81,8 @@ public EntityDefinition getDefinition(TypeElement classElement) {
     SortedMap<Integer, PropertyDefinition> partitionKey = new TreeMap<>();
     SortedMap<Integer, PropertyDefinition> clusteringColumns = new TreeMap<>();
     ImmutableList.Builder<PropertyDefinition> regularColumns = ImmutableList.builder();
+    ImmutableList.Builder<PropertyDefinition> computedValues = ImmutableList.builder();
+    int aliasCounter = 0;","[{'comment': 'Nit: unused', 'commenter': 'olim7t'}]"
1242,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/entity/EntityDefinition.java,"@@ -54,4 +56,17 @@
         .addAll(getRegularColumns())
         .build();
   }
+
+  /**
+   * @return the concatenation of {@link #getPartitionKey()}, {@link #getClusteringColumns()},
+   *     {@link #getRegularColumns()}, and {@link #getComputedValues()} in that order.
+   */
+  default List<PropertyDefinition> getAllValues() {
+    return ImmutableList.<PropertyDefinition>builder()
+        .addAll(getPartitionKey())
+        .addAll(getClusteringColumns())
+        .addAll(getRegularColumns())
+        .addAll(getComputedValues())
+        .build();
+  }","[{'comment': 'ðŸ‘ the terminology makes sense.', 'commenter': 'olim7t'}]"
1242,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/entity/ComputedPropertyDefinition.java,"@@ -0,0 +1,72 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.mapper.processor.entity;
+
+import com.datastax.oss.driver.internal.mapper.processor.util.generation.PropertyType;
+import com.squareup.javapoet.CodeBlock;
+import java.util.Optional;
+
+public class ComputedPropertyDefinition implements PropertyDefinition {","[{'comment': 'Do we really need to duplicate the whole class? It feels like adding an `Optional<String> formula` parameter to the original class would be pretty easy.', 'commenter': 'olim7t'}]"
1242,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/entity/PropertyDefinition.java,"@@ -22,6 +22,10 @@
 
   CodeBlock getCqlName();
 
+  default CodeBlock getCqlResultName() {
+    return getCqlName();
+  }
+","[{'comment': 'I get which is which when I read this class, but in the rest of the code my head is exploding ðŸ˜ƒ \r\n\r\nFirst I think we need javadocs:\r\n* `getCqlName`: a Java snippet that produces the corresponding expression in a SELECT statement, for example:\r\n  * `""id""` in `selectFrom.column(""id"")` for a regular column\r\n  * `""writetime(v)""` in `selectFrom.raw(""writetime(v)"")` for a computed value\r\n* `getCqlResultName`: a Java snippet that produces the name of the property in a GettableByName or SettableByName, for example `""id""` in `row.get(""id"")`.\r\n\r\nThen I would also rename them. Proposal:\r\n* `getCqlName` => `getSelector`\r\n* `getCqlResultName` => `getAlias`', 'commenter': 'olim7t'}, {'comment': 'Sounds good, i was taking a path of least resistance initially.  Agree that renaming things makes things easier to understand.', 'commenter': 'tolbertam'}]"
1242,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/entity/EntityHelperGetMethodGenerator.java,"@@ -62,9 +62,10 @@ public EntityHelperGetMethodGenerator(
     String returnName = ""returnValue"";
     getBuilder.addStatement(""$1T $2L = new $1T()"", returnType, returnName);
 
-    for (PropertyDefinition property : entityDefinition.getAllColumns()) {
+    for (PropertyDefinition property : entityDefinition.getAllValues()) {
       PropertyType type = property.getType();
       CodeBlock cqlName = property.getCqlName();","[{'comment': ""`cqlName` shouldn't be referenced at all in this method. There are a couple of uses for UDTs and collections of UDTs below, but I think those should be changed to use `cqlResultName` too."", 'commenter': 'olim7t'}]"
1242,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/entity/EntityHelperGetMethodGenerator.java,"@@ -74,12 +75,12 @@ public EntityHelperGetMethodGenerator(
           // Primitive type: use dedicated getter, since it is optimized to avoid boxing
           //     returnValue.setLength(source.getInt(""length""));
           getBuilder.addStatement(
-              ""returnValue.$L(source.get$L($L))"", setterName, primitiveAccessor, cqlName);
+              ""returnValue.$L(source.get$L($L))"", setterName, primitiveAccessor, cqlResultName);","[{'comment': ""There are other method generators where we should do that change, for example `EntityHelperInsertMethodGenerator`. In practice things work because they happen to deal exclusively with regular columns where `cqlName == cqlResultName`, but strictly speaking they should use `cqlResultName`.\r\n\r\nIf we do the `cqlName => selector` change that I'm suggesting in another comment, it will be more obvious and we can do a quick pass on the code."", 'commenter': 'olim7t'}]"
1242,integration-tests/src/test/java/com/datastax/oss/driver/mapper/ComputedIT.java,"@@ -0,0 +1,306 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.mapper;
+
+import static com.datastax.oss.driver.assertions.Assertions.assertThat;
+
+import com.datastax.oss.driver.api.core.CqlIdentifier;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.cql.BoundStatement;
+import com.datastax.oss.driver.api.core.cql.BoundStatementBuilder;
+import com.datastax.oss.driver.api.core.cql.PreparedStatement;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import com.datastax.oss.driver.api.core.cql.Row;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.api.mapper.annotations.ClusteringColumn;
+import com.datastax.oss.driver.api.mapper.annotations.Computed;
+import com.datastax.oss.driver.api.mapper.annotations.CqlName;
+import com.datastax.oss.driver.api.mapper.annotations.Dao;
+import com.datastax.oss.driver.api.mapper.annotations.DaoFactory;
+import com.datastax.oss.driver.api.mapper.annotations.DaoKeyspace;
+import com.datastax.oss.driver.api.mapper.annotations.Delete;
+import com.datastax.oss.driver.api.mapper.annotations.Entity;
+import com.datastax.oss.driver.api.mapper.annotations.GetEntity;
+import com.datastax.oss.driver.api.mapper.annotations.Insert;
+import com.datastax.oss.driver.api.mapper.annotations.Mapper;
+import com.datastax.oss.driver.api.mapper.annotations.PartitionKey;
+import com.datastax.oss.driver.api.mapper.annotations.Query;
+import com.datastax.oss.driver.api.mapper.annotations.Select;
+import com.datastax.oss.driver.api.mapper.annotations.SetEntity;
+import com.datastax.oss.driver.api.mapper.annotations.Update;
+import com.datastax.oss.driver.api.testinfra.ccm.CcmRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionRule;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import com.datastax.oss.driver.shaded.guava.common.collect.ImmutableList;
+import java.util.concurrent.atomic.AtomicInteger;
+import org.junit.BeforeClass;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.junit.rules.RuleChain;
+import org.junit.rules.TestRule;
+
+@Category(ParallelizableTests.class)
+public class ComputedIT {
+
+  private static CcmRule ccm = CcmRule.getInstance();
+
+  private static SessionRule<CqlSession> sessionRule = SessionRule.builder(ccm).build();
+
+  @ClassRule public static TestRule chain = RuleChain.outerRule(ccm).around(sessionRule);
+
+  private static TestMapper mapper;
+
+  private static AtomicInteger keyProvider = new AtomicInteger();
+
+  @BeforeClass
+  public static void setup() {
+    CqlSession session = sessionRule.session();
+
+    for (String query :
+        ImmutableList.of(
+            ""CREATE TABLE computed_entity(id int, c_id int, v int, primary key (id, c_id))"")) {
+      session.execute(
+          SimpleStatement.builder(query).setExecutionProfile(sessionRule.slowProfile()).build());
+    }
+
+    mapper = new ComputedIT_TestMapperBuilder(session).build();
+  }
+
+  @Test
+  public void should_not_include_computed_values_in_insert() {
+    ComputedDao computedDao = mapper.computedDao(sessionRule.keyspace());
+    int key = keyProvider.incrementAndGet();
+
+    ComputedEntity entity = new ComputedEntity(key, 1, 2);
+    computedDao.save(entity);
+
+    ComputedEntity retrievedValue = computedDao.findById(key, 1);
+    assertThat(retrievedValue.getId()).isEqualTo(key);
+    assertThat(retrievedValue.getcId()).isEqualTo(1);
+    assertThat(retrievedValue.getV()).isEqualTo(2);
+  }
+
+  @Test
+  public void should_return_computed_values_in_select() {
+    ComputedDao computedDao = mapper.computedDao(sessionRule.keyspace());
+    int key = keyProvider.incrementAndGet();
+
+    long time = System.currentTimeMillis() - 1000;
+    ComputedEntity entity = new ComputedEntity(key, 1, 2);
+    computedDao.saveWithTime(entity, 3600, time);
+
+    ComputedEntity retrievedValue = computedDao.findById(key, 1);
+    assertThat(retrievedValue.getId()).isEqualTo(key);
+    assertThat(retrievedValue.getcId()).isEqualTo(1);
+    assertThat(retrievedValue.getV()).isEqualTo(2);
+    assertThat(retrievedValue.getTtl()).isEqualTo(3600);
+    assertThat(retrievedValue.getWritetime()).isEqualTo(time);
+  }
+
+  @Test
+  public void should_not_include_computed_values_in_delete() {
+    // should not be the case since delete operates on primary key..
+    ComputedDao computedDao = mapper.computedDao(sessionRule.keyspace());
+    int key = keyProvider.incrementAndGet();
+
+    ComputedEntity entity = new ComputedEntity(key, 1, 2);
+    computedDao.save(entity);
+
+    // retrieve values so computed values are present.
+    ComputedEntity retrievedValue = computedDao.findById(key, 1);
+
+    computedDao.delete(retrievedValue);
+
+    assertThat(computedDao.findById(key, 1)).isNull();
+  }
+
+  @Test
+  public void should_not_include_computed_values_in_SetEntity() {
+    ComputedDao computedDao = mapper.computedDao(sessionRule.keyspace());
+    int key = keyProvider.incrementAndGet();
+
+    CqlSession session = sessionRule.session();
+    PreparedStatement preparedStatement =
+        session.prepare(""INSERT INTO computed_entity (id, c_id, v) VALUES (?, ?, ?)"");
+    BoundStatementBuilder builder = preparedStatement.boundStatementBuilder();
+    ComputedEntity entity = new ComputedEntity(key, 1, 2);
+    BoundStatement statement = computedDao.set(builder, entity).build();
+    session.execute(statement);
+
+    // retrieve values to ensure was successful.
+    ComputedEntity retrievedValue = computedDao.findById(key, 1);
+
+    assertThat(retrievedValue.getId()).isEqualTo(key);
+    assertThat(retrievedValue.getcId()).isEqualTo(1);
+    assertThat(retrievedValue.getV()).isEqualTo(2);
+  }
+
+  @Test
+  public void should_return_computed_values_in_GetEntity() {
+    ComputedDao computedDao = mapper.computedDao(sessionRule.keyspace());
+    int key = keyProvider.incrementAndGet();
+
+    long time = System.currentTimeMillis() - 1000;
+    ComputedEntity entity = new ComputedEntity(key, 1, 2);
+    computedDao.saveWithTime(entity, 3600, time);
+
+    CqlSession session = sessionRule.session();
+    // query with the computed values included.
+    // since we the mapper expects the result name to match the property name, we used aliasing
+    // here.
+    ResultSet result =
+        session.execute(
+            SimpleStatement.newInstance(
+                ""select id, c_id, v, writetime(v) as writetime, ttl(v) as myttl from ""
+                    + ""computed_entity where ""
+                    + ""id=? and ""
+                    + ""c_id=? limit 1"",
+                key,
+                1));
+    assertThat(result.getAvailableWithoutFetching()).isEqualTo(1);
+
+    ComputedEntity retrievedValue = computedDao.get(result.one());
+    assertThat(retrievedValue.getId()).isEqualTo(key);
+    assertThat(retrievedValue.getcId()).isEqualTo(1);
+    assertThat(retrievedValue.getV()).isEqualTo(2);
+
+    // these should be set
+    assertThat(retrievedValue.getTtl()).isEqualTo(3600);
+    assertThat(retrievedValue.getWritetime()).isEqualTo(time);
+  }
+
+  @Test
+  public void should_return_computed_values_in_query() {
+    ComputedDao computedDao = mapper.computedDao(sessionRule.keyspace());
+    int key = keyProvider.incrementAndGet();
+
+    long time = System.currentTimeMillis() - 1000;
+    ComputedEntity entity = new ComputedEntity(key, 1, 2);
+    computedDao.saveWithTime(entity, 3600, time);
+
+    ComputedEntity retrievedValue = computedDao.findByIdQuery(key, 1);
+    assertThat(retrievedValue.getId()).isEqualTo(key);
+    assertThat(retrievedValue.getcId()).isEqualTo(1);
+    assertThat(retrievedValue.getV()).isEqualTo(2);
+
+    // these should be set
+    assertThat(retrievedValue.getTtl()).isEqualTo(3600);
+    assertThat(retrievedValue.getWritetime()).isEqualTo(time);
+  }","[{'comment': 'I think we are missing test case that does `@Computed(""ttl(v)"")` and `@CqlName(""myttl"")` but cql name is not present in results from the query', 'commenter': 'tomekl007'}, {'comment': 'You mean just to validate that an exception is thrown if the user provided query does not match the expectation of what the result alias should be?', 'commenter': 'tolbertam'}]"
1242,integration-tests/src/test/java/com/datastax/oss/driver/mapper/ComputedIT.java,"@@ -0,0 +1,306 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.mapper;
+
+import static com.datastax.oss.driver.assertions.Assertions.assertThat;
+
+import com.datastax.oss.driver.api.core.CqlIdentifier;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.cql.BoundStatement;
+import com.datastax.oss.driver.api.core.cql.BoundStatementBuilder;
+import com.datastax.oss.driver.api.core.cql.PreparedStatement;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import com.datastax.oss.driver.api.core.cql.Row;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.api.mapper.annotations.ClusteringColumn;
+import com.datastax.oss.driver.api.mapper.annotations.Computed;
+import com.datastax.oss.driver.api.mapper.annotations.CqlName;
+import com.datastax.oss.driver.api.mapper.annotations.Dao;
+import com.datastax.oss.driver.api.mapper.annotations.DaoFactory;
+import com.datastax.oss.driver.api.mapper.annotations.DaoKeyspace;
+import com.datastax.oss.driver.api.mapper.annotations.Delete;
+import com.datastax.oss.driver.api.mapper.annotations.Entity;
+import com.datastax.oss.driver.api.mapper.annotations.GetEntity;
+import com.datastax.oss.driver.api.mapper.annotations.Insert;
+import com.datastax.oss.driver.api.mapper.annotations.Mapper;
+import com.datastax.oss.driver.api.mapper.annotations.PartitionKey;
+import com.datastax.oss.driver.api.mapper.annotations.Query;
+import com.datastax.oss.driver.api.mapper.annotations.Select;
+import com.datastax.oss.driver.api.mapper.annotations.SetEntity;
+import com.datastax.oss.driver.api.mapper.annotations.Update;
+import com.datastax.oss.driver.api.testinfra.ccm.CcmRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionRule;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import com.datastax.oss.driver.shaded.guava.common.collect.ImmutableList;
+import java.util.concurrent.atomic.AtomicInteger;
+import org.junit.BeforeClass;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.junit.rules.RuleChain;
+import org.junit.rules.TestRule;
+
+@Category(ParallelizableTests.class)
+public class ComputedIT {
+
+  private static CcmRule ccm = CcmRule.getInstance();
+
+  private static SessionRule<CqlSession> sessionRule = SessionRule.builder(ccm).build();
+
+  @ClassRule public static TestRule chain = RuleChain.outerRule(ccm).around(sessionRule);
+
+  private static TestMapper mapper;
+
+  private static AtomicInteger keyProvider = new AtomicInteger();
+
+  @BeforeClass
+  public static void setup() {
+    CqlSession session = sessionRule.session();
+
+    for (String query :
+        ImmutableList.of(
+            ""CREATE TABLE computed_entity(id int, c_id int, v int, primary key (id, c_id))"")) {
+      session.execute(
+          SimpleStatement.builder(query).setExecutionProfile(sessionRule.slowProfile()).build());
+    }
+
+    mapper = new ComputedIT_TestMapperBuilder(session).build();
+  }
+
+  @Test
+  public void should_not_include_computed_values_in_insert() {
+    ComputedDao computedDao = mapper.computedDao(sessionRule.keyspace());
+    int key = keyProvider.incrementAndGet();
+
+    ComputedEntity entity = new ComputedEntity(key, 1, 2);
+    computedDao.save(entity);
+
+    ComputedEntity retrievedValue = computedDao.findById(key, 1);
+    assertThat(retrievedValue.getId()).isEqualTo(key);
+    assertThat(retrievedValue.getcId()).isEqualTo(1);
+    assertThat(retrievedValue.getV()).isEqualTo(2);
+  }
+
+  @Test
+  public void should_return_computed_values_in_select() {
+    ComputedDao computedDao = mapper.computedDao(sessionRule.keyspace());
+    int key = keyProvider.incrementAndGet();
+
+    long time = System.currentTimeMillis() - 1000;
+    ComputedEntity entity = new ComputedEntity(key, 1, 2);
+    computedDao.saveWithTime(entity, 3600, time);
+
+    ComputedEntity retrievedValue = computedDao.findById(key, 1);
+    assertThat(retrievedValue.getId()).isEqualTo(key);
+    assertThat(retrievedValue.getcId()).isEqualTo(1);
+    assertThat(retrievedValue.getV()).isEqualTo(2);
+    assertThat(retrievedValue.getTtl()).isEqualTo(3600);
+    assertThat(retrievedValue.getWritetime()).isEqualTo(time);
+  }
+
+  @Test
+  public void should_not_include_computed_values_in_delete() {
+    // should not be the case since delete operates on primary key..
+    ComputedDao computedDao = mapper.computedDao(sessionRule.keyspace());
+    int key = keyProvider.incrementAndGet();
+
+    ComputedEntity entity = new ComputedEntity(key, 1, 2);
+    computedDao.save(entity);
+
+    // retrieve values so computed values are present.
+    ComputedEntity retrievedValue = computedDao.findById(key, 1);
+
+    computedDao.delete(retrievedValue);
+
+    assertThat(computedDao.findById(key, 1)).isNull();
+  }
+
+  @Test
+  public void should_not_include_computed_values_in_SetEntity() {
+    ComputedDao computedDao = mapper.computedDao(sessionRule.keyspace());
+    int key = keyProvider.incrementAndGet();
+
+    CqlSession session = sessionRule.session();
+    PreparedStatement preparedStatement =
+        session.prepare(""INSERT INTO computed_entity (id, c_id, v) VALUES (?, ?, ?)"");
+    BoundStatementBuilder builder = preparedStatement.boundStatementBuilder();
+    ComputedEntity entity = new ComputedEntity(key, 1, 2);
+    BoundStatement statement = computedDao.set(builder, entity).build();
+    session.execute(statement);
+
+    // retrieve values to ensure was successful.
+    ComputedEntity retrievedValue = computedDao.findById(key, 1);
+
+    assertThat(retrievedValue.getId()).isEqualTo(key);
+    assertThat(retrievedValue.getcId()).isEqualTo(1);
+    assertThat(retrievedValue.getV()).isEqualTo(2);
+  }
+
+  @Test
+  public void should_return_computed_values_in_GetEntity() {
+    ComputedDao computedDao = mapper.computedDao(sessionRule.keyspace());
+    int key = keyProvider.incrementAndGet();
+
+    long time = System.currentTimeMillis() - 1000;
+    ComputedEntity entity = new ComputedEntity(key, 1, 2);
+    computedDao.saveWithTime(entity, 3600, time);
+
+    CqlSession session = sessionRule.session();
+    // query with the computed values included.
+    // since we the mapper expects the result name to match the property name, we used aliasing
+    // here.
+    ResultSet result =
+        session.execute(
+            SimpleStatement.newInstance(
+                ""select id, c_id, v, writetime(v) as writetime, ttl(v) as myttl from ""","[{'comment': 'Looking at this definition:\r\n``` @CqlName(""myttl"")\r\n    @Computed(""ttl(v)"")\r\n    private int ttl;\r\n```\r\nDon\'t you think it is a little bit confusing? \r\nIf the select query is returning `mytll` alias maybe it will be enough to do: `@Compued(ttl(myttl))`?', 'commenter': 'tomekl007'}, {'comment': 'i\'m intentionally doing this to test that the `@CqlName` annotation is used over the property name:\r\n\r\n```java\r\n\r\n    @CqlName(""myttl"")\r\n    @Computed(""ttl(v)"")\r\n    private int ttl;\r\n```\r\n\r\nI\'ll update the comment to reflect that.', 'commenter': 'tolbertam'}]"
1242,manual/mapper/entities/README.md,"@@ -159,6 +159,67 @@ private int day;
 This information is used by some of the DAO method annotations; for example,
 [@Select](../daos/select/)'s default behavior is to generate a selection by primary key.
 
+#### Computed properties
+
+Annotating an entity property with [@Computed] indicates that when retrieving data with the mapper
+this property should be set to the result of a computation on the Cassandra side, typically a
+function call:
+
+```java
+private int v;
+
+@Computed(""writetime(v)"")
+private long writetime;
+```
+
+The CQL return type of the formula must match the type of the property, otherwise an exception
+will be thrown.
+
+[@Computed] does not support case-sensitivity. If the expression contains case-sensitive column
+or function names, you'll have to escape them:
+
+```java
+@Computed(""\""myFunction\""(\""myColumn\"")"")
+private int f;
+```
+
+[@Computed] fields are only used for queries, so they will not be considered for [@Update] or","[{'comment': 'To clarify I would add: `[@Computed] fields are only used for [@Select] queries, so they will not be considered for [@Update]...`', 'commenter': 'adutra'}, {'comment': 'It also works for `@GetEntity` and `@Query` as well (see sections below), so I think it makes sense as written, but can adjust it if you prefer.', 'commenter': 'tolbertam'}]"
1242,manual/mapper/entities/README.md,"@@ -159,6 +159,67 @@ private int day;
 This information is used by some of the DAO method annotations; for example,
 [@Select](../daos/select/)'s default behavior is to generate a selection by primary key.
 
+#### Computed properties
+
+Annotating an entity property with [@Computed] indicates that when retrieving data with the mapper
+this property should be set to the result of a computation on the Cassandra side, typically a
+function call:
+
+```java
+private int v;
+
+@Computed(""writetime(v)"")
+private long writetime;
+```
+
+The CQL return type of the formula must match the type of the property, otherwise an exception
+will be thrown.
+
+[@Computed] does not support case-sensitivity. If the expression contains case-sensitive column
+or function names, you'll have to escape them:
+
+```java
+@Computed(""\""myFunction\""(\""myColumn\"")"")
+private int f;
+```
+
+[@Computed] fields are only used for queries, so they will not be considered for [@Update] or
+[@Insert] operations.
+
+Also note that like all other properties, the expected name in a query result for a [@Computed]
+property is based on the property name and the employed [@NamingStrategy](#naming-strategy). You may
+override this behavior using [@CqlName](#user-provided-names).
+
+Mapping computed results to property names is accomplished using [aliases].  If you wish to use
+entities with [@Computed] properties with [@GetEntity] or [@Query]-annotated dao methods, you
+must also do the same:
+
+```java
+@Entity
+class MyEntity {
+  @PartitionKey private int k;
+
+  private int v;
+
+  @Computed(""ttl(v)"")
+  private int myTtl;
+
+  @Computed(""writetime(v)"")","[{'comment': 'Wouldn\'t `@Computed(expression = ""writetime(v)"", alias = ""ts"")` be more explicit?\r\nIf not: wouldn\'t `@Alias` be a better name than `@CqlName`?', 'commenter': 'adutra'}, {'comment': '> `@Computed(expression = ""writetime(v)"", alias = ""ts"")`\r\n\r\nThis introduces a special case though, isn\'t it better if computed values behave like any other property?\r\n\r\n> wouldn\'t `@Alias` be a better name than `@CqlName`?\r\n\r\nðŸ‘ to that. This is in line with the renaming I suggested [here](https://github.com/datastax/java-driver/pull/1242#discussion_r287579567).', 'commenter': 'olim7t'}]"
1242,mapper-runtime/src/main/java/com/datastax/oss/driver/api/mapper/annotations/Computed.java,"@@ -0,0 +1,43 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.mapper.annotations;
+
+/**
+ * Annotates the field or getter of an {@link Entity} property, to indicate that when retrieving
+ * data that the property should be set to the result of computation on the Cassandra side,
+ * typically a function call.
+ *
+ * <p>Example:
+ *
+ * <pre>
+ * private int v;","[{'comment': 'This line seems unnecessary.', 'commenter': 'adutra'}]"
1242,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/entity/ComputedPropertyDefinition.java,"@@ -0,0 +1,72 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.mapper.processor.entity;
+
+import com.datastax.oss.driver.internal.mapper.processor.util.generation.PropertyType;
+import com.squareup.javapoet.CodeBlock;
+import java.util.Optional;
+
+public class ComputedPropertyDefinition implements PropertyDefinition {
+
+  private final CodeBlock cqlName;
+  private final CodeBlock cqlResultName;
+  private final String getterName;
+  private final String setterName;
+  private final PropertyType type;
+
+  public ComputedPropertyDefinition(
+      String javaName,
+      Optional<String> customCqlName,","[{'comment': ""It is not a good practice to pass `Optional` as a method parameter, to avoid unnecessary wrapping/unwrapping (and also because `Optional` is not serializable). I know it's already done in a few other places, but wanted to pointed that out. A better approach is to pass the string annotated with `@Nullable`."", 'commenter': 'adutra'}, {'comment': ""I'll be factoring this code out into DefaultPropertyDefinition, will update the optional parameters in that constructor when I get to it."", 'commenter': 'tolbertam'}, {'comment': ""> It is not a good practice to pass Optional as a method parameter\r\n\r\nI don't 100% agree with that IDE warning. Performance is not paramount in the processor (at least not to the point where wrapping something in an optional will make a difference), and we don't store the optional in a field so serialization is not an issue (we don't serialize this object anyway for that matter).\r\n\r\nPassing an optional here works nicely because the annotation parsing code that is used to fill that parameter already returns one."", 'commenter': 'olim7t'}, {'comment': ""I agree with you, the only real bad thing is to store optionals as fields, but that is not the case here, so I'm fine if we keep them."", 'commenter': 'adutra'}]"
1246,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoMethodGenerator.java,"@@ -214,6 +218,36 @@ protected void maybeAddTimestamp(String timestamp, MethodSpec.Builder methodBuil
     }
   }
 
+  protected boolean nullSavingStrategyExplicitlySet(
+      ExecutableElement methodElement, String annotationCanonicalName) {","[{'comment': 'This is code responsible for finding out if the `nullSavingStrategy` value was explicitly set on the `@Update` annotation using `AnnotationMirror`. It looks a bit complex, @olim7t do you know if there is a way to create `ExecutableElement` using java-poet for the purpose of this test?', 'commenter': 'tomekl007'}, {'comment': 'JavaPoet only generates strings. The only way to create an `ExecutableElement` for a test is either through mocking or with a full compiler execution I think.', 'commenter': 'olim7t'}]"
1246,mapper-runtime/src/main/java/com/datastax/oss/driver/api/mapper/annotations/DefaultNullSavingStrategy.java,"@@ -0,0 +1,46 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.mapper.annotations;
+
+import com.datastax.oss.driver.api.mapper.entity.saving.NullSavingStrategy;
+import java.lang.annotation.ElementType;
+import java.lang.annotation.Retention;
+import java.lang.annotation.RetentionPolicy;
+import java.lang.annotation.Target;
+
+@Target(ElementType.TYPE)
+@Retention(RetentionPolicy.CLASS)
+public @interface DefaultNullSavingStrategy {","[{'comment': 'I think If we choose to add new annotation `DefaultNullSavingStrategy` this annotation should have `nullSavingStrategy` without default. If someone is adding that at the `@Dao` level it means that he must specify `nullSavingStrategy` that will apply to inherited strategy for methods: `@Update`, `@Insert`, `@SetEntity`, `@Query`.', 'commenter': 'tomekl007'}, {'comment': 'I agree with no default.', 'commenter': 'olim7t'}, {'comment': 'What do you think about naming this simply `@NullSavingStrategy`?  Default seems kind of implicit, as to me it is intuitive to expect that if you were to also define `nullSavingStrategy` at the method-annotation level that it would override the interface level annotation.', 'commenter': 'tolbertam'}]"
1246,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoMethodGenerator.java,"@@ -214,6 +218,36 @@ protected void maybeAddTimestamp(String timestamp, MethodSpec.Builder methodBuil
     }
   }
 
+  protected boolean nullSavingStrategyExplicitlySet(
+      ExecutableElement methodElement, String annotationCanonicalName) {
+    return !methodElement
+        .getAnnotationMirrors()
+        .stream()
+        .filter(
+            v -> {
+              // Find only for Update annotation
+              DeclaredType annotationType = v.getAnnotationType();
+              if (annotationType instanceof Type.ClassType) {
+                return ((Type.ClassType) annotationType)
+                    .tsym
+                    .getQualifiedName()
+                    .toString()
+                    .equals(annotationCanonicalName);","[{'comment': 'You can compare the type with a `Class` directly:\r\n```java\r\ncontext.getClassUtils().isSame(getAnnotationType(), Update.class)\r\n```', 'commenter': 'olim7t'}]"
1246,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoMethodGenerator.java,"@@ -214,6 +218,36 @@ protected void maybeAddTimestamp(String timestamp, MethodSpec.Builder methodBuil
     }
   }
 
+  protected boolean nullSavingStrategyExplicitlySet(
+      ExecutableElement methodElement, String annotationCanonicalName) {
+    return !methodElement
+        .getAnnotationMirrors()
+        .stream()
+        .filter(
+            v -> {
+              // Find only for Update annotation
+              DeclaredType annotationType = v.getAnnotationType();
+              if (annotationType instanceof Type.ClassType) {
+                return ((Type.ClassType) annotationType)
+                    .tsym
+                    .getQualifiedName()
+                    .toString()
+                    .equals(annotationCanonicalName);
+              } else {
+                return false;
+              }
+            })
+        .flatMap(v -> v.getElementValues().values().stream())
+        .filter(v -> v instanceof Attribute.Enum)","[{'comment': 'From `getElementValues()` javadocs:\r\n> Only those elements with values explicitly present in the annotation are included, not those that are implicitly assuming their default values.\r\n\r\nSo we just need to iterate `getElementValues().keySet()` and find out if there is an element such that `key.getSimpleName().contentEquals(""nullSavingStrategy"")`.', 'commenter': 'olim7t'}]"
1246,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoMethodGenerator.java,"@@ -214,6 +218,36 @@ protected void maybeAddTimestamp(String timestamp, MethodSpec.Builder methodBuil
     }
   }
 
+  protected boolean nullSavingStrategyExplicitlySet(
+      ExecutableElement methodElement, String annotationCanonicalName) {
+    return !methodElement
+        .getAnnotationMirrors()
+        .stream()","[{'comment': ""I think the Stream API really makes things worse here (and to be honest that's true most of the time -- that's why it's discouraged in the contribution guidelines).\r\n\r\nThere's an imperative version for a similar case [here](https://github.com/datastax/java-driver/blob/java2078/mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoQueryProviderMethodGenerator.java#L84). It's not pretty either, but I find it way more readable."", 'commenter': 'olim7t'}, {'comment': 'I\'ve changed (https://github.com/datastax/java-driver/pull/1246/commits/75a978baf7118914f2907e47e89a46ead8e7561f) it to imperative way but I am not sure if this is more readable that stream versions after simplification:\r\n```\r\n  return methodElement\r\n        .getAnnotationMirrors()\r\n        .stream()\r\n        .filter(v -> classUtils.isSame(v.getAnnotationType(), javaClass))\r\n        .flatMap(v -> v.getElementValues().keySet().stream())\r\n        .anyMatch(v -> v.getSimpleName().contentEquals(""nullSavingStrategy""));\r\n```', 'commenter': 'tomekl007'}]"
1246,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoImplementationGenerator.java,"@@ -320,56 +328,59 @@ private void generateProtocolVersionCheck(MethodSpec.Builder builder) {
             .map(v -> v.methodElement)
             .anyMatch(
                 v ->
-                    updateHasDoNotSet(v)
-                        || insertHasDoNotSet(v)
-                        || setEntityHasDoNotSet(v)
-                        || queryHasDoNotSet(v));
-
-    if (hasDoNotSetStatement) {
-      builder
-          .beginControlFlow(
-              ""if (context.getSession().getContext().getProtocolVersion().getCode() <= $T.Version.V3)"",
-              ProtocolConstants.class)
-          .addStatement(
-              ""throw new IllegalArgumentException($S)"",
-              ""You cannot use NullSavingStrategy.DO_NOT_SET for protocol version V3."")
-          .endControlFlow()
-          .build();
-    }
-  }
-
-  private boolean queryHasDoNotSet(ExecutableElement v) {
-    Query annotation = v.getAnnotation(Query.class);
-    if (annotation != null) {
-      return annotation.nullSavingStrategy() == NullSavingStrategy.DO_NOT_SET;
-    }
-    return false;
-  }
+                    updateHasDoNotSet(v, false)
+                        || insertHasDoNotSet(v, false)
+                        || setEntityHasDoNotSet(v, false)
+                        || queryHasDoNotSet(v, false));
 
-  private boolean setEntityHasDoNotSet(ExecutableElement v) {
-    SetEntity annotation = v.getAnnotation(SetEntity.class);
-    if (annotation != null) {
-      return annotation.nullSavingStrategy() == NullSavingStrategy.DO_NOT_SET;
-    }
-    return false;
-  }
+    boolean hasDoNotSetStatementSetExplicitly =
+        preparedStatements
+            .stream()
+            .map(v -> v.methodElement)
+            .anyMatch(
+                v ->
+                    updateHasDoNotSet(v, true)
+                        || insertHasDoNotSet(v, true)
+                        || setEntityHasDoNotSet(v, true)
+                        || queryHasDoNotSet(v, true));
 
-  private boolean insertHasDoNotSet(ExecutableElement v) {
-    Insert annotation = v.getAnnotation(Insert.class);
-    if (annotation != null) {
-      return annotation.nullSavingStrategy() == NullSavingStrategy.DO_NOT_SET;
-    } else {
-      return false;
+    boolean hadSetToNullStatementSetExplicitlyForAll =
+        preparedStatements
+            .stream()
+            .map(v -> v.methodElement)
+            .filter(NullSavingStrategyValidation::isOperationWithNullSavingStrategy)
+            .allMatch(
+                v ->
+                    updateHasSetToNullExplicitly(v)
+                        || insertHasSetToNullExplicitly(v)
+                        || setEntitySetToNullExplicitly(v)
+                        || queryHasSetToNullExplicitly(v));
+
+    // if DAO level SET_TO_NULL check all underlying annotations for explicit set to DO_NOT_SET
+    // (they may override it)
+    if (hasDaoLevelSetToNullStatement(interfaceElement) && hasDoNotSetStatementSetExplicitly) {
+      addVersionCheckToBuilder(builder);
+      // if DAO level DO_NOT_SET check if all underlying override it explicitly to SET_TO_NULL
+    } else if (hasDaoLevelDoNotSetStatement(interfaceElement)
+        && !hadSetToNullStatementSetExplicitlyForAll) {
+      addVersionCheckToBuilder(builder);
+    } else if (hadDaoLevelUnset(interfaceElement) && hasDoNotSetStatement) {
+      // if DAO level annotation do not present, check method level strategy
+      // (including the default one)
+      addVersionCheckToBuilder(builder);
     }
   }
 
-  private boolean updateHasDoNotSet(ExecutableElement v) {
-    Update annotation = v.getAnnotation(Update.class);
-    if (annotation != null) {
-      return annotation.nullSavingStrategy() == NullSavingStrategy.DO_NOT_SET;
-    } else {
-      return false;
-    }
+  private void addVersionCheckToBuilder(MethodSpec.Builder builder) {
+    builder
+        .beginControlFlow(
+            ""if (context.getSession().getContext().getProtocolVersion().getCode() <= $T.Version.V3)"",
+            ProtocolConstants.class)
+        .addStatement(
+            ""throw new IllegalArgumentException($S)"",
+            ""You cannot use NullSavingStrategy.DO_NOT_SET for protocol version V3."")
+        .endControlFlow()
+        .build();","[{'comment': ""You could move this to a static method on `DaoBase`, so that you only have to generate the method call.\r\n\r\nOn the other hand the code is still relatively simple so it's not a big deal to generate it. Your call."", 'commenter': 'olim7t'}, {'comment': 'Yes, I think that this is a good idea to have a ""normal"" java code where possible (instead of generated). Changed (https://github.com/datastax/java-driver/pull/1246/commits/37aada0af2153961c75d4fb7714b7b42ffc56ee4)', 'commenter': 'tomekl007'}]"
1246,mapper-runtime/src/main/java/com/datastax/oss/driver/api/mapper/annotations/DefaultNullSavingStrategy.java,"@@ -0,0 +1,46 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.mapper.annotations;
+
+import com.datastax.oss.driver.api.mapper.entity.saving.NullSavingStrategy;
+import java.lang.annotation.ElementType;
+import java.lang.annotation.Retention;
+import java.lang.annotation.RetentionPolicy;
+import java.lang.annotation.Target;
+
+@Target(ElementType.TYPE)
+@Retention(RetentionPolicy.CLASS)
+public @interface DefaultNullSavingStrategy {
+  /**
+   * How to handle null entity properties during the insertion.
+   *
+   * <p>If you specify {@code DefaultNullSavingStrategy} at the {@link Dao} level and set one of:
+   * {@link Update#nullSavingStrategy()}, {@link Insert#nullSavingStrategy()}, {@link
+   * SetEntity#nullSavingStrategy()} {@link Query#nullSavingStrategy()} explicitly the method level
+   * annotation value will be taken.
+   *
+   * <p>If you specify {@link DefaultNullSavingStrategy} at the {@link Dao} level and do not set one
+   * of: {@link Update#nullSavingStrategy()}, {@link Insert#nullSavingStrategy()}, {@link
+   * SetEntity#nullSavingStrategy()} {@link Query#nullSavingStrategy()} explicitly the {@code
+   * DefaultNullSavingStrategy#nullSavingStrategy()} will be taken.
+   *
+   * <p>If you do not specify {@code DefaultNullSavingStrategy} at the {@link Dao} level the default
+   * or explicitly user set values from one of: {@link Update#nullSavingStrategy()}, {@link
+   * Insert#nullSavingStrategy()}, {@link SetEntity#nullSavingStrategy()} {@link
+   * Query#nullSavingStrategy()} will be taken
+   */
+  NullSavingStrategy nullSavingStrategy();","[{'comment': ""I think we don't need a dedicated parameter name here, since it's the only one and it's pretty obvious from the name of the annotation. So I would call this method `value()` so that it can be written directly like this:\r\n\r\n```java\r\n@DefaultNullSavingStrategy(SET_TO_NULL)\r\n```"", 'commenter': 'olim7t'}]"
1246,manual/mapper/daos/defaultnullsavingstrategy/README.md,"@@ -0,0 +1,33 @@
+## DefaultNullSavingStrategy
+
+If you don't specify `nullSavingStrategy` parameter on the method level you can suppress 
+defaults by using `@DefaultNullSavingStrategy` annotation.","[{'comment': 'Can you make `@DefaultNullSavingStrategy` a link to the Javadocs?', 'commenter': 'olim7t'}]"
1246,manual/mapper/daos/null_saving/README.md,"@@ -39,6 +39,11 @@ Two strategies are available:
     
 If you don't specify an explicit strategy for a method, the mapper defaults to `SET_TO_NULL`.
 
+## Default Dao level null saving strategy
+
+If you don't specify `nullSavingStrategy` parameter on the method level you can suppress defaults by using
+`@DefaultNullSavingStrategy` annotation. See [@DefaultNullSavingStrategy](../defaultnullsavingstrategy/) for more details.","[{'comment': ""I don't think a separate page is necessary here, I would inline it in this section.\r\nIt fits in nicely since it relates to the main topic."", 'commenter': 'olim7t'}]"
1246,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoImplementationGenerator.java,"@@ -71,11 +69,13 @@
   private final Map<ClassName, String> entityHelperFields = new LinkedHashMap<>();
   private final List<GeneratedPreparedStatement> preparedStatements = new ArrayList<>();
   private final List<GeneratedQueryProvider> queryProviders = new ArrayList<>();
+  private final NullSavingStrategyValidation nullSavingStrategyValidation;
 
   public DaoImplementationGenerator(TypeElement interfaceElement, ProcessorContext context) {
     super(context);
     this.interfaceElement = interfaceElement;
     implementationName = GeneratedNames.daoImplementation(interfaceElement);
+    nullSavingStrategyValidation = new NullSavingStrategyValidation(context);","[{'comment': 'ðŸ‘ Good idea to extract the boilerplate into a separate component', 'commenter': 'olim7t'}]"
1246,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoInsertMethodGenerator.java,"@@ -127,8 +129,21 @@ public DaoInsertMethodGenerator(
 
     List<? extends VariableElement> parameters = methodElement.getParameters();
     String entityParameterName = parameters.get(0).getSimpleName().toString();
-    NullSavingStrategy nullSavingStrategy =
-        methodElement.getAnnotation(Insert.class).nullSavingStrategy();
+
+    Insert annotation = methodElement.getAnnotation(Insert.class);
+    NullSavingStrategy nullSavingStrategy;
+
+    Optional<NullSavingStrategy> defaultNullSavingStrategy = enclosingClass.getNullSavingStrategy();","[{'comment': 'Maybe calling this `daoNullSavingStrategy` would make the code below a bit easier to read?', 'commenter': 'olim7t'}]"
1246,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/NullSavingStrategyValidation.java,"@@ -0,0 +1,246 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.mapper.processor.dao;
+
+import com.datastax.oss.driver.api.mapper.annotations.DefaultNullSavingStrategy;
+import com.datastax.oss.driver.api.mapper.annotations.Insert;
+import com.datastax.oss.driver.api.mapper.annotations.Query;
+import com.datastax.oss.driver.api.mapper.annotations.SetEntity;
+import com.datastax.oss.driver.api.mapper.annotations.Update;
+import com.datastax.oss.driver.api.mapper.entity.saving.NullSavingStrategy;
+import com.datastax.oss.driver.internal.mapper.processor.ProcessorContext;
+import com.datastax.oss.driver.internal.mapper.processor.util.Classes;
+import java.util.List;
+import java.util.Optional;
+import javax.lang.model.element.AnnotationMirror;
+import javax.lang.model.element.Element;
+import javax.lang.model.element.ExecutableElement;
+import javax.lang.model.element.TypeElement;
+
+class NullSavingStrategyValidation {","[{'comment': ""We might want to make this class (and its two package-private methods) public.\r\nWe're not committing to the API because it's in an internal package, and it could come in handy if we extend the generators or create new ones from the DSE mapper."", 'commenter': 'olim7t'}]"
1246,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/NullSavingStrategyValidation.java,"@@ -0,0 +1,246 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.mapper.processor.dao;
+
+import com.datastax.oss.driver.api.mapper.annotations.DefaultNullSavingStrategy;
+import com.datastax.oss.driver.api.mapper.annotations.Insert;
+import com.datastax.oss.driver.api.mapper.annotations.Query;
+import com.datastax.oss.driver.api.mapper.annotations.SetEntity;
+import com.datastax.oss.driver.api.mapper.annotations.Update;
+import com.datastax.oss.driver.api.mapper.entity.saving.NullSavingStrategy;
+import com.datastax.oss.driver.internal.mapper.processor.ProcessorContext;
+import com.datastax.oss.driver.internal.mapper.processor.util.Classes;
+import java.util.List;
+import java.util.Optional;
+import javax.lang.model.element.AnnotationMirror;
+import javax.lang.model.element.Element;
+import javax.lang.model.element.ExecutableElement;
+import javax.lang.model.element.TypeElement;
+
+class NullSavingStrategyValidation {
+  private final Classes classUtils;
+
+  NullSavingStrategyValidation(ProcessorContext context) {
+    classUtils = context.getClassUtils();
+  }
+
+  /**
+   * For every ExecutableElement that has @{@link NullSavingStrategy} property checks if {@link
+   * NullSavingStrategy#DO_NOT_SET} was set. The underlying annotations that have that strategy are:
+   * {@link Update#nullSavingStrategy()}, {@link Insert#nullSavingStrategy()}, {@link
+   * SetEntity#nullSavingStrategy()} {@link Query#nullSavingStrategy()}
+   *
+   * @return true if:
+   *     <p>DAO level has SET_TO_NULL and any of underlying has explicit set to DO_NOT_SET
+   *     <p>DAO level has DO_NOT_SET and not all underlying override it explicitly to SET_TO_NULL
+   *     <p>DAO level annotation do not present and any of method level strategy has DO_NOT_SET
+   *     (including the default one)
+   */
+  boolean hasDoNotSetOnAnyLevel(
+      List<ExecutableElement> methodElements, TypeElement daoLevelInterface) {
+    boolean hasDoNotSetStatement =
+        methodElements
+            .stream()
+            .anyMatch(
+                v ->
+                    updateHasDoNotSet(v, false)
+                        || insertHasDoNotSet(v, false)
+                        || setEntityHasDoNotSet(v, false)
+                        || queryHasDoNotSet(v, false));
+
+    boolean hasDoNotSetStatementSetExplicitly =
+        methodElements
+            .stream()
+            .anyMatch(
+                v ->
+                    updateHasDoNotSet(v, true)
+                        || insertHasDoNotSet(v, true)
+                        || setEntityHasDoNotSet(v, true)
+                        || queryHasDoNotSet(v, true));
+
+    boolean hasSetToNullStatementSetExplicitlyForAll =
+        methodElements
+            .stream()
+            .filter(this::isOperationWithNullSavingStrategy)
+            .allMatch(
+                v ->
+                    updateHasSetToNullExplicitly(v)
+                        || insertHasSetToNullExplicitly(v)
+                        || setEntitySetToNullExplicitly(v)
+                        || queryHasSetToNullExplicitly(v));
+
+    // if DAO level SET_TO_NULL check all underlying annotations for explicit set to DO_NOT_SET
+    // (they may override it)
+    if (hasDaoLevelSetToNullStatement(daoLevelInterface) && hasDoNotSetStatementSetExplicitly) {
+      return true;
+      // if DAO level DO_NOT_SET check if all underlying override it explicitly to SET_TO_NULL
+    } else if (hasDaoLevelDoNotSetStatement(daoLevelInterface)
+        && !hasSetToNullStatementSetExplicitlyForAll) {
+      return true;
+      // if DAO level annotation do not present, check method level strategy
+      // (including the default one)
+    } else {
+      return hasDaoLevelUnset(daoLevelInterface) && hasDoNotSetStatement;
+    }","[{'comment': 'This is a bit hard to follow, but I think it\'s more because of the inherent complexity, I\'m not sure I could do better :)\r\n\r\nMaybe a few naming suggestions (as you see fit):\r\n```\r\nhasDoNotSetStatement => anyMethodHasOrDefaultsToDoNotSet\r\nhasDoNotSetStatementSetExplicitly => anyMethodHasDoNotSetExplicitly\r\nhasSetToNullStatementSetExplicitlyForAll => allMethodsHaveSetToNull (no need to say ""explicitly"" here, SET_TO_NULL is always explicit)\r\n\r\nhasDaoLevelSetToNullStatement => daoHasSetToNull\r\nhasDaoLevelDoNotSetStatement => daoHasDoNotSet\r\nhasDaoLevelUnset => daoIsNotAnnotated\r\n```', 'commenter': 'olim7t'}]"
1246,mapper-runtime/src/main/java/com/datastax/oss/driver/api/mapper/annotations/DefaultNullSavingStrategy.java,"@@ -0,0 +1,46 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.mapper.annotations;
+
+import com.datastax.oss.driver.api.mapper.entity.saving.NullSavingStrategy;
+import java.lang.annotation.ElementType;
+import java.lang.annotation.Retention;
+import java.lang.annotation.RetentionPolicy;
+import java.lang.annotation.Target;
+
+@Target(ElementType.TYPE)
+@Retention(RetentionPolicy.CLASS)
+public @interface DefaultNullSavingStrategy {
+  /**
+   * How to handle null entity properties during the insertion.
+   *
+   * <p>If you specify {@code DefaultNullSavingStrategy} at the {@link Dao} level and set one of:
+   * {@link Update#nullSavingStrategy()}, {@link Insert#nullSavingStrategy()}, {@link
+   * SetEntity#nullSavingStrategy()} {@link Query#nullSavingStrategy()} explicitly the method level
+   * annotation value will be taken.
+   *
+   * <p>If you specify {@link DefaultNullSavingStrategy} at the {@link Dao} level and do not set one
+   * of: {@link Update#nullSavingStrategy()}, {@link Insert#nullSavingStrategy()}, {@link
+   * SetEntity#nullSavingStrategy()} {@link Query#nullSavingStrategy()} explicitly the {@code
+   * DefaultNullSavingStrategy#nullSavingStrategy()} will be taken.
+   *
+   * <p>If you do not specify {@code DefaultNullSavingStrategy} at the {@link Dao} level the default
+   * or explicitly user set values from one of: {@link Update#nullSavingStrategy()}, {@link
+   * Insert#nullSavingStrategy()}, {@link SetEntity#nullSavingStrategy()} {@link
+   * Query#nullSavingStrategy()} will be taken
+   */","[{'comment': ""I would move this as the main javadoc for the annotation. Also a few suggestions to reorganize the explanations:\r\n```java\r\n/**\r\n * Annotates a {@link Dao} interface to define a default {@link NullSavingStrategy}, that will apply\r\n * to all methods that don't explicitly declare one.\r\n *\r\n * <p>For example, given this interface:\r\n *\r\n * <pre>\r\n * &#64;Dao\r\n * &#64;DefaultNullSavingStrategy(SET_TO_NULL)\r\n * public interface ProductDao {\r\n *\r\n *   &#64;Insert\r\n *   void insert(Product product);\r\n *\r\n *   &#64;Update(nullSavingStrategy = DO_NOT_SET)\r\n *   void update(Product product);\r\n * }\r\n * </pre>\r\n *\r\n * <ul>\r\n *   <li>{@code insert(Product)} will use {@link NullSavingStrategy#SET_TO_NULL SET_TO_NULL}\r\n *       (inherited from the DAO's default).\r\n *   <li>{@code update(Product)} will use {@link NullSavingStrategy#DO_NOT_SET DO_NOT_SET}.\r\n * </ul>\r\n *\r\n * If the DAO interface isn't annotated with {@link DefaultNullSavingStrategy}, any method that does\r\n * not declare its own value defaults to {@link NullSavingStrategy#DO_NOT_SET DO_NOT_SET}.\r\n *\r\n * <p>Note that null saving strategies are only relevant for {@link Update}, {@link Insert}, {@link\r\n * Query} and {@link SetEntity} methods.\r\n */\r\n```"", 'commenter': 'olim7t'}]"
1246,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/NullSavingStrategyValidation.java,"@@ -0,0 +1,246 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.mapper.processor.dao;
+
+import com.datastax.oss.driver.api.mapper.annotations.DefaultNullSavingStrategy;
+import com.datastax.oss.driver.api.mapper.annotations.Insert;
+import com.datastax.oss.driver.api.mapper.annotations.Query;
+import com.datastax.oss.driver.api.mapper.annotations.SetEntity;
+import com.datastax.oss.driver.api.mapper.annotations.Update;
+import com.datastax.oss.driver.api.mapper.entity.saving.NullSavingStrategy;
+import com.datastax.oss.driver.internal.mapper.processor.ProcessorContext;
+import com.datastax.oss.driver.internal.mapper.processor.util.Classes;
+import java.util.List;
+import java.util.Optional;
+import javax.lang.model.element.AnnotationMirror;
+import javax.lang.model.element.Element;
+import javax.lang.model.element.ExecutableElement;
+import javax.lang.model.element.TypeElement;
+
+class NullSavingStrategyValidation {
+  private final Classes classUtils;
+
+  NullSavingStrategyValidation(ProcessorContext context) {
+    classUtils = context.getClassUtils();
+  }
+
+  /**
+   * For every ExecutableElement that has @{@link NullSavingStrategy} property checks if {@link
+   * NullSavingStrategy#DO_NOT_SET} was set. The underlying annotations that have that strategy are:
+   * {@link Update#nullSavingStrategy()}, {@link Insert#nullSavingStrategy()}, {@link
+   * SetEntity#nullSavingStrategy()} {@link Query#nullSavingStrategy()}
+   *
+   * @return true if:
+   *     <p>DAO level has SET_TO_NULL and any of underlying has explicit set to DO_NOT_SET
+   *     <p>DAO level has DO_NOT_SET and not all underlying override it explicitly to SET_TO_NULL
+   *     <p>DAO level annotation do not present and any of method level strategy has DO_NOT_SET
+   *     (including the default one)
+   */
+  boolean hasDoNotSetOnAnyLevel(
+      List<ExecutableElement> methodElements, TypeElement daoLevelInterface) {
+    boolean hasDoNotSetStatement =
+        methodElements
+            .stream()
+            .anyMatch(
+                v ->
+                    updateHasDoNotSet(v, false)
+                        || insertHasDoNotSet(v, false)
+                        || setEntityHasDoNotSet(v, false)
+                        || queryHasDoNotSet(v, false));
+
+    boolean hasDoNotSetStatementSetExplicitly =
+        methodElements
+            .stream()
+            .anyMatch(
+                v ->
+                    updateHasDoNotSet(v, true)
+                        || insertHasDoNotSet(v, true)
+                        || setEntityHasDoNotSet(v, true)
+                        || queryHasDoNotSet(v, true));
+
+    boolean hasSetToNullStatementSetExplicitlyForAll =
+        methodElements
+            .stream()
+            .filter(this::isOperationWithNullSavingStrategy)
+            .allMatch(
+                v ->
+                    updateHasSetToNullExplicitly(v)
+                        || insertHasSetToNullExplicitly(v)
+                        || setEntitySetToNullExplicitly(v)
+                        || queryHasSetToNullExplicitly(v));
+
+    // if DAO level SET_TO_NULL check all underlying annotations for explicit set to DO_NOT_SET
+    // (they may override it)
+    if (hasDaoLevelSetToNullStatement(daoLevelInterface) && hasDoNotSetStatementSetExplicitly) {
+      return true;
+      // if DAO level DO_NOT_SET check if all underlying override it explicitly to SET_TO_NULL
+    } else if (hasDaoLevelDoNotSetStatement(daoLevelInterface)
+        && !hasSetToNullStatementSetExplicitlyForAll) {
+      return true;
+      // if DAO level annotation do not present, check method level strategy
+      // (including the default one)
+    } else {
+      return hasDaoLevelUnset(daoLevelInterface) && hasDoNotSetStatement;
+    }
+  }
+
+  private boolean hasDaoLevelDoNotSetStatement(TypeElement interfaceElement) {
+    DefaultNullSavingStrategy annotation =
+        interfaceElement.getAnnotation(DefaultNullSavingStrategy.class);
+    if (annotation != null) {
+      return annotation.nullSavingStrategy() == NullSavingStrategy.DO_NOT_SET;
+    }
+    return false;
+  }
+
+  private boolean hasDaoLevelSetToNullStatement(TypeElement interfaceElement) {
+    DefaultNullSavingStrategy annotation =
+        interfaceElement.getAnnotation(DefaultNullSavingStrategy.class);
+    if (annotation != null) {
+      return annotation.nullSavingStrategy() == NullSavingStrategy.SET_TO_NULL;
+    }
+    return false;
+  }
+
+  private boolean hasDaoLevelUnset(TypeElement interfaceElement) {
+    return interfaceElement.getAnnotation(DefaultNullSavingStrategy.class) == null;
+  }
+
+  private boolean queryHasDoNotSet(ExecutableElement v, boolean explicitSet) {","[{'comment': 'There is a lot of boilerplate here between `queryHasDoNotSet`, `setEntityHasDoNotSet`, `insertHasDoNotSet` and `updateHasDoNotSet` that could probably be factored into a common method, for example you could do something like this:\r\n\r\n```java\r\n  private <A extends Annotation> boolean hasDoNotSet(Class<A> clazz,\r\n                              Function<A, NullSavingStrategy> extractor,\r\n                              ExecutableElement v,\r\n                              boolean explicitSet) {\r\n    A annotation = v.getAnnotation(clazz);\r\n    if (annotation != null) {\r\n      NullSavingStrategy strategy = extractor.apply(annotation);\r\n      if (explicitSet) {\r\n        return strategy == NullSavingStrategy.DO_NOT_SET\r\n          && nullSavingStrategyExplicitlySet(v, clazz);\r\n      } else {\r\n        return strategy == NullSavingStrategy.DO_NOT_SET;\r\n      }\r\n    }\r\n    return false;\r\n  }\r\n\r\n  private boolean queryHasDoNotSet(ExecutableElement v, boolean explicitSet) {\r\n    return hasDoNotSet(Query.class, Query::nullSavingStrategy, v, explicitSet);\r\n  }\r\n```', 'commenter': 'tolbertam'}, {'comment': 'I was also thinking about it - I had an impression that sometimes we are favoring code readability over DRY but If that is your impression I am +1 on this as well.\r\nImplemented in https://github.com/datastax/java-driver/pull/1246/commits/0e99daa6a413387ae8478a01b5fd88c463edbc4f\r\n', 'commenter': 'tomekl007'}]"
1246,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/NullSavingStrategyValidation.java,"@@ -0,0 +1,246 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.mapper.processor.dao;
+
+import com.datastax.oss.driver.api.mapper.annotations.DefaultNullSavingStrategy;
+import com.datastax.oss.driver.api.mapper.annotations.Insert;
+import com.datastax.oss.driver.api.mapper.annotations.Query;
+import com.datastax.oss.driver.api.mapper.annotations.SetEntity;
+import com.datastax.oss.driver.api.mapper.annotations.Update;
+import com.datastax.oss.driver.api.mapper.entity.saving.NullSavingStrategy;
+import com.datastax.oss.driver.internal.mapper.processor.ProcessorContext;
+import com.datastax.oss.driver.internal.mapper.processor.util.Classes;
+import java.util.List;
+import java.util.Optional;
+import javax.lang.model.element.AnnotationMirror;
+import javax.lang.model.element.Element;
+import javax.lang.model.element.ExecutableElement;
+import javax.lang.model.element.TypeElement;
+
+class NullSavingStrategyValidation {
+  private final Classes classUtils;
+
+  NullSavingStrategyValidation(ProcessorContext context) {
+    classUtils = context.getClassUtils();
+  }
+
+  /**
+   * For every ExecutableElement that has @{@link NullSavingStrategy} property checks if {@link
+   * NullSavingStrategy#DO_NOT_SET} was set. The underlying annotations that have that strategy are:
+   * {@link Update#nullSavingStrategy()}, {@link Insert#nullSavingStrategy()}, {@link
+   * SetEntity#nullSavingStrategy()} {@link Query#nullSavingStrategy()}
+   *
+   * @return true if:
+   *     <p>DAO level has SET_TO_NULL and any of underlying has explicit set to DO_NOT_SET
+   *     <p>DAO level has DO_NOT_SET and not all underlying override it explicitly to SET_TO_NULL
+   *     <p>DAO level annotation do not present and any of method level strategy has DO_NOT_SET
+   *     (including the default one)
+   */
+  boolean hasDoNotSetOnAnyLevel(
+      List<ExecutableElement> methodElements, TypeElement daoLevelInterface) {
+    boolean hasDoNotSetStatement =
+        methodElements
+            .stream()
+            .anyMatch(
+                v ->
+                    updateHasDoNotSet(v, false)
+                        || insertHasDoNotSet(v, false)
+                        || setEntityHasDoNotSet(v, false)
+                        || queryHasDoNotSet(v, false));
+
+    boolean hasDoNotSetStatementSetExplicitly =
+        methodElements
+            .stream()
+            .anyMatch(
+                v ->
+                    updateHasDoNotSet(v, true)
+                        || insertHasDoNotSet(v, true)
+                        || setEntityHasDoNotSet(v, true)
+                        || queryHasDoNotSet(v, true));
+
+    boolean hasSetToNullStatementSetExplicitlyForAll =
+        methodElements
+            .stream()
+            .filter(this::isOperationWithNullSavingStrategy)
+            .allMatch(
+                v ->
+                    updateHasSetToNullExplicitly(v)
+                        || insertHasSetToNullExplicitly(v)
+                        || setEntitySetToNullExplicitly(v)
+                        || queryHasSetToNullExplicitly(v));
+
+    // if DAO level SET_TO_NULL check all underlying annotations for explicit set to DO_NOT_SET
+    // (they may override it)
+    if (hasDaoLevelSetToNullStatement(daoLevelInterface) && hasDoNotSetStatementSetExplicitly) {
+      return true;
+      // if DAO level DO_NOT_SET check if all underlying override it explicitly to SET_TO_NULL
+    } else if (hasDaoLevelDoNotSetStatement(daoLevelInterface)
+        && !hasSetToNullStatementSetExplicitlyForAll) {
+      return true;
+      // if DAO level annotation do not present, check method level strategy
+      // (including the default one)
+    } else {
+      return hasDaoLevelUnset(daoLevelInterface) && hasDoNotSetStatement;
+    }
+  }
+
+  private boolean hasDaoLevelDoNotSetStatement(TypeElement interfaceElement) {
+    DefaultNullSavingStrategy annotation =
+        interfaceElement.getAnnotation(DefaultNullSavingStrategy.class);
+    if (annotation != null) {
+      return annotation.nullSavingStrategy() == NullSavingStrategy.DO_NOT_SET;
+    }
+    return false;
+  }
+
+  private boolean hasDaoLevelSetToNullStatement(TypeElement interfaceElement) {
+    DefaultNullSavingStrategy annotation =
+        interfaceElement.getAnnotation(DefaultNullSavingStrategy.class);
+    if (annotation != null) {
+      return annotation.nullSavingStrategy() == NullSavingStrategy.SET_TO_NULL;
+    }
+    return false;
+  }
+
+  private boolean hasDaoLevelUnset(TypeElement interfaceElement) {
+    return interfaceElement.getAnnotation(DefaultNullSavingStrategy.class) == null;
+  }
+
+  private boolean queryHasDoNotSet(ExecutableElement v, boolean explicitSet) {
+    Query annotation = v.getAnnotation(Query.class);
+    if (annotation != null) {
+      if (explicitSet) {
+        return annotation.nullSavingStrategy() == NullSavingStrategy.DO_NOT_SET
+            && nullSavingStrategyExplicitlySet(v, Query.class);
+      } else {
+        return annotation.nullSavingStrategy() == NullSavingStrategy.DO_NOT_SET;
+      }
+    }
+    return false;
+  }
+
+  private boolean setEntityHasDoNotSet(ExecutableElement v, boolean explicitSet) {
+    SetEntity annotation = v.getAnnotation(SetEntity.class);
+    if (annotation != null) {
+      if (explicitSet) {
+        return annotation.nullSavingStrategy() == NullSavingStrategy.DO_NOT_SET
+            && nullSavingStrategyExplicitlySet(v, SetEntity.class);
+      } else {
+        return annotation.nullSavingStrategy() == NullSavingStrategy.DO_NOT_SET;
+      }
+    }
+    return false;
+  }
+
+  private boolean insertHasDoNotSet(ExecutableElement v, boolean explicitSet) {
+    Insert annotation = v.getAnnotation(Insert.class);
+    if (annotation != null) {
+      if (explicitSet) {
+        return annotation.nullSavingStrategy() == NullSavingStrategy.DO_NOT_SET
+            && nullSavingStrategyExplicitlySet(v, Insert.class);
+      } else {
+        return annotation.nullSavingStrategy() == NullSavingStrategy.DO_NOT_SET;
+      }
+    } else {
+      return false;
+    }
+  }
+
+  private boolean updateHasDoNotSet(ExecutableElement v, boolean explicitSet) {
+    Update annotation = v.getAnnotation(Update.class);
+    if (annotation != null) {
+      if (explicitSet) {
+        return annotation.nullSavingStrategy() == NullSavingStrategy.DO_NOT_SET
+            && nullSavingStrategyExplicitlySet(v, Update.class);
+      } else {
+        return annotation.nullSavingStrategy() == NullSavingStrategy.DO_NOT_SET;
+      }
+    } else {
+      return false;
+    }
+  }
+
+  private boolean queryHasSetToNullExplicitly(ExecutableElement v) {","[{'comment': 'same here, these `HasSetToNullExplicitly` methods could be factored into a common method.', 'commenter': 'tolbertam'}]"
1246,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/NullSavingStrategyValidation.java,"@@ -0,0 +1,246 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.mapper.processor.dao;
+
+import com.datastax.oss.driver.api.mapper.annotations.DefaultNullSavingStrategy;
+import com.datastax.oss.driver.api.mapper.annotations.Insert;
+import com.datastax.oss.driver.api.mapper.annotations.Query;
+import com.datastax.oss.driver.api.mapper.annotations.SetEntity;
+import com.datastax.oss.driver.api.mapper.annotations.Update;
+import com.datastax.oss.driver.api.mapper.entity.saving.NullSavingStrategy;
+import com.datastax.oss.driver.internal.mapper.processor.ProcessorContext;
+import com.datastax.oss.driver.internal.mapper.processor.util.Classes;
+import java.util.List;
+import java.util.Optional;
+import javax.lang.model.element.AnnotationMirror;
+import javax.lang.model.element.Element;
+import javax.lang.model.element.ExecutableElement;
+import javax.lang.model.element.TypeElement;
+
+class NullSavingStrategyValidation {
+  private final Classes classUtils;
+
+  NullSavingStrategyValidation(ProcessorContext context) {
+    classUtils = context.getClassUtils();
+  }
+
+  /**
+   * For every ExecutableElement that has @{@link NullSavingStrategy} property checks if {@link
+   * NullSavingStrategy#DO_NOT_SET} was set. The underlying annotations that have that strategy are:
+   * {@link Update#nullSavingStrategy()}, {@link Insert#nullSavingStrategy()}, {@link
+   * SetEntity#nullSavingStrategy()} {@link Query#nullSavingStrategy()}
+   *
+   * @return true if:
+   *     <p>DAO level has SET_TO_NULL and any of underlying has explicit set to DO_NOT_SET
+   *     <p>DAO level has DO_NOT_SET and not all underlying override it explicitly to SET_TO_NULL
+   *     <p>DAO level annotation do not present and any of method level strategy has DO_NOT_SET
+   *     (including the default one)
+   */
+  boolean hasDoNotSetOnAnyLevel(
+      List<ExecutableElement> methodElements, TypeElement daoLevelInterface) {
+    boolean hasDoNotSetStatement =
+        methodElements
+            .stream()
+            .anyMatch(
+                v ->
+                    updateHasDoNotSet(v, false)
+                        || insertHasDoNotSet(v, false)
+                        || setEntityHasDoNotSet(v, false)
+                        || queryHasDoNotSet(v, false));
+
+    boolean hasDoNotSetStatementSetExplicitly =
+        methodElements
+            .stream()
+            .anyMatch(
+                v ->
+                    updateHasDoNotSet(v, true)
+                        || insertHasDoNotSet(v, true)
+                        || setEntityHasDoNotSet(v, true)
+                        || queryHasDoNotSet(v, true));
+
+    boolean hasSetToNullStatementSetExplicitlyForAll =
+        methodElements
+            .stream()
+            .filter(this::isOperationWithNullSavingStrategy)
+            .allMatch(
+                v ->
+                    updateHasSetToNullExplicitly(v)
+                        || insertHasSetToNullExplicitly(v)
+                        || setEntitySetToNullExplicitly(v)
+                        || queryHasSetToNullExplicitly(v));
+
+    // if DAO level SET_TO_NULL check all underlying annotations for explicit set to DO_NOT_SET
+    // (they may override it)
+    if (hasDaoLevelSetToNullStatement(daoLevelInterface) && hasDoNotSetStatementSetExplicitly) {
+      return true;
+      // if DAO level DO_NOT_SET check if all underlying override it explicitly to SET_TO_NULL
+    } else if (hasDaoLevelDoNotSetStatement(daoLevelInterface)
+        && !hasSetToNullStatementSetExplicitlyForAll) {
+      return true;
+      // if DAO level annotation do not present, check method level strategy
+      // (including the default one)
+    } else {
+      return hasDaoLevelUnset(daoLevelInterface) && hasDoNotSetStatement;
+    }
+  }
+
+  private boolean hasDaoLevelDoNotSetStatement(TypeElement interfaceElement) {
+    DefaultNullSavingStrategy annotation =
+        interfaceElement.getAnnotation(DefaultNullSavingStrategy.class);
+    if (annotation != null) {
+      return annotation.nullSavingStrategy() == NullSavingStrategy.DO_NOT_SET;
+    }
+    return false;
+  }
+
+  private boolean hasDaoLevelSetToNullStatement(TypeElement interfaceElement) {
+    DefaultNullSavingStrategy annotation =
+        interfaceElement.getAnnotation(DefaultNullSavingStrategy.class);
+    if (annotation != null) {
+      return annotation.nullSavingStrategy() == NullSavingStrategy.SET_TO_NULL;
+    }
+    return false;
+  }
+
+  private boolean hasDaoLevelUnset(TypeElement interfaceElement) {
+    return interfaceElement.getAnnotation(DefaultNullSavingStrategy.class) == null;
+  }
+
+  private boolean queryHasDoNotSet(ExecutableElement v, boolean explicitSet) {
+    Query annotation = v.getAnnotation(Query.class);
+    if (annotation != null) {
+      if (explicitSet) {
+        return annotation.nullSavingStrategy() == NullSavingStrategy.DO_NOT_SET
+            && nullSavingStrategyExplicitlySet(v, Query.class);
+      } else {
+        return annotation.nullSavingStrategy() == NullSavingStrategy.DO_NOT_SET;
+      }
+    }
+    return false;
+  }
+
+  private boolean setEntityHasDoNotSet(ExecutableElement v, boolean explicitSet) {
+    SetEntity annotation = v.getAnnotation(SetEntity.class);
+    if (annotation != null) {
+      if (explicitSet) {
+        return annotation.nullSavingStrategy() == NullSavingStrategy.DO_NOT_SET
+            && nullSavingStrategyExplicitlySet(v, SetEntity.class);
+      } else {
+        return annotation.nullSavingStrategy() == NullSavingStrategy.DO_NOT_SET;
+      }
+    }
+    return false;
+  }
+
+  private boolean insertHasDoNotSet(ExecutableElement v, boolean explicitSet) {
+    Insert annotation = v.getAnnotation(Insert.class);
+    if (annotation != null) {
+      if (explicitSet) {
+        return annotation.nullSavingStrategy() == NullSavingStrategy.DO_NOT_SET
+            && nullSavingStrategyExplicitlySet(v, Insert.class);
+      } else {
+        return annotation.nullSavingStrategy() == NullSavingStrategy.DO_NOT_SET;
+      }
+    } else {
+      return false;
+    }
+  }
+
+  private boolean updateHasDoNotSet(ExecutableElement v, boolean explicitSet) {
+    Update annotation = v.getAnnotation(Update.class);
+    if (annotation != null) {
+      if (explicitSet) {
+        return annotation.nullSavingStrategy() == NullSavingStrategy.DO_NOT_SET
+            && nullSavingStrategyExplicitlySet(v, Update.class);
+      } else {
+        return annotation.nullSavingStrategy() == NullSavingStrategy.DO_NOT_SET;
+      }
+    } else {
+      return false;
+    }
+  }
+
+  private boolean queryHasSetToNullExplicitly(ExecutableElement v) {
+    Query annotation = v.getAnnotation(Query.class);
+    if (annotation != null) {
+      return annotation.nullSavingStrategy() == NullSavingStrategy.SET_TO_NULL
+          && nullSavingStrategyExplicitlySet(v, Query.class);
+    }
+    return false;
+  }
+
+  private boolean setEntitySetToNullExplicitly(ExecutableElement v) {
+    SetEntity annotation = v.getAnnotation(SetEntity.class);
+    if (annotation != null) {
+      return annotation.nullSavingStrategy() == NullSavingStrategy.SET_TO_NULL
+          && nullSavingStrategyExplicitlySet(v, SetEntity.class);
+    }
+    return false;
+  }
+
+  private boolean insertHasSetToNullExplicitly(ExecutableElement v) {
+    Insert annotation = v.getAnnotation(Insert.class);
+    if (annotation != null) {
+      return annotation.nullSavingStrategy() == NullSavingStrategy.SET_TO_NULL
+          && nullSavingStrategyExplicitlySet(v, Insert.class);
+    } else {
+      return false;
+    }
+  }
+
+  private boolean updateHasSetToNullExplicitly(ExecutableElement v) {
+    Update annotation = v.getAnnotation(Update.class);
+    if (annotation != null) {
+      return annotation.nullSavingStrategy() == NullSavingStrategy.SET_TO_NULL
+          && nullSavingStrategyExplicitlySet(v, Update.class);
+    } else {
+      return false;
+    }
+  }
+
+  private boolean isOperationWithNullSavingStrategy(ExecutableElement v) {
+    return v.getAnnotation(Update.class) != null
+        || v.getAnnotation(Insert.class) != null
+        || v.getAnnotation(SetEntity.class) != null
+        || v.getAnnotation(Query.class) != null;
+  }
+
+  boolean nullSavingStrategyExplicitlySet(Element methodElement, Class<?> javaClass) {","[{'comment': ""If we were to remove `default NullSavingStrategy.DO_NOT_SET` from all annotations so they are null instead couldn't this be more effective as all you would have to do is null check?"", 'commenter': 'tolbertam'}, {'comment': 'Removing the default makes the attribute mandatory on the annotation.', 'commenter': 'olim7t'}, {'comment': ""ah, right.  That didn't occur to me at the time, in any case, could we just set default to null?"", 'commenter': 'tolbertam'}, {'comment': 'If we will set it to `null` we will still need to interpret is somehow for a case where \r\n`@DefaultNullSavingStrategy` on the `@Dao` level is not a set AND the method level `nullSavingStrategy` property for example on `@Update` will not be set. \r\nIn such case that null needs to become `DO_NOT_SET` or `SET_TO_NULL`. I think that it is better to have that default logic embedded within the annotation default:\r\n`NullSavingStrategy nullSavingStrategy() default NullSavingStrategy.DO_NOT_SET;`\r\ninstead of adding another case to handle when the value is null. \r\n', 'commenter': 'tomekl007'}]"
1246,manual/mapper/daos/defaultnullsavingstrategy/README.md,"@@ -0,0 +1,33 @@
+## DefaultNullSavingStrategy
+
+If you don't specify `nullSavingStrategy` parameter on the method level you can suppress ","[{'comment': 'instead of `you can suppress defaults`, I would recommend `you can adjust the default behavior`', 'commenter': 'tolbertam'}]"
1246,manual/mapper/daos/defaultnullsavingstrategy/README.md,"@@ -0,0 +1,33 @@
+## DefaultNullSavingStrategy
+
+If you don't specify `nullSavingStrategy` parameter on the method level you can suppress 
+defaults by using `@DefaultNullSavingStrategy` annotation.
+
+This annotation works at the [@Dao] level:
+```java
+@Dao
+@DefaultNullSavingStrategy(nullSavingStrategy = NullSavingStrategy.SET_TO_NULL)
+public interface DefaultNullStrategyDao {
+  // dao methods
+}
+
+```
+
+Strategies when using `@DefaultNullSavingStrategy` with method level `nullSavingStrategy` on annotations: 
+[@Insert](../insert/), [@Query](../query/), [@SetEntity](../setentity/) or
+[@Update](../update/):
+
+* If you specify `@DefaultNullSavingStrategy` at the [@Dao] level and set `nullSavingStrategy` parameter on one of the 
+  [@Update](../update/), [@Insert](../insert/), [@Query](../query/) or [@SetEntity](../setentity./)
+  explicitly, the method level annotation value will be taken.
+
+* If you specify `@DefaultNullSavingStrategy` at the [@Dao] level and do not set `nullSavingStrategy` parameter on one of the ","[{'comment': ""These next two paragraphs don't add much as I think it should be apparent that if you only use one of the ways to configure null saving, the one that is used is used."", 'commenter': 'tolbertam'}]"
1246,manual/mapper/daos/defaultnullsavingstrategy/README.md,"@@ -0,0 +1,33 @@
+## DefaultNullSavingStrategy
+
+If you don't specify `nullSavingStrategy` parameter on the method level you can suppress 
+defaults by using `@DefaultNullSavingStrategy` annotation.
+
+This annotation works at the [@Dao] level:
+```java
+@Dao
+@DefaultNullSavingStrategy(nullSavingStrategy = NullSavingStrategy.SET_TO_NULL)
+public interface DefaultNullStrategyDao {
+  // dao methods
+}
+
+```
+
+Strategies when using `@DefaultNullSavingStrategy` with method level `nullSavingStrategy` on annotations: ","[{'comment': ""If you agree and remove the two paragraphs below, you don't need this first line either."", 'commenter': 'tolbertam'}]"
1246,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoInsertMethodGenerator.java,"@@ -127,8 +129,21 @@ public DaoInsertMethodGenerator(
 
     List<? extends VariableElement> parameters = methodElement.getParameters();
     String entityParameterName = parameters.get(0).getSimpleName().toString();
-    NullSavingStrategy nullSavingStrategy =
-        methodElement.getAnnotation(Insert.class).nullSavingStrategy();
+
+    Insert annotation = methodElement.getAnnotation(Insert.class);","[{'comment': 'This code is repeated among the next four classes, could probably be refactored similarly to how I describe in `NullSavingStrategyValidation`', 'commenter': 'tolbertam'}]"
1255,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/entity/DefaultEntityDefinition.java,"@@ -44,6 +47,7 @@ public DefaultEntityDefinition(
         customCqlName
             .map(n -> CodeBlock.of(""$S"", n))
             .orElse(cqlNameGenerator.buildCqlName(javaName));
+    this.defaultKeyspace = defaultKeyspace;","[{'comment': 'I am not sure if we want to apply cqlNameGenerator on the `defaultKeyspace`.\r\nIf someone is adding explicit `@Entity(defaultKeyspace =""a"")` I am hesitating about transforming it to other ks name using `cqlNameGenerator` in this case. @tolbertam @olim7t wdyt?', 'commenter': 'tomekl007'}, {'comment': ""You're right, it shouldn't be transformed. The user provides a CQL name directly here, like in `@CqlName`. If it's case-sensitive, they'll have to double quote manually."", 'commenter': 'olim7t'}]"
1255,integration-tests/src/test/java/com/datastax/oss/driver/mapper/DefaultKeyspaceIT.java,"@@ -0,0 +1,336 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.mapper;
+
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.assertj.core.api.Assertions.assertThatThrownBy;
+
+import com.datastax.oss.driver.api.core.CqlIdentifier;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.api.core.servererrors.InvalidQueryException;
+import com.datastax.oss.driver.api.mapper.annotations.Dao;
+import com.datastax.oss.driver.api.mapper.annotations.DaoFactory;
+import com.datastax.oss.driver.api.mapper.annotations.DaoKeyspace;
+import com.datastax.oss.driver.api.mapper.annotations.Entity;
+import com.datastax.oss.driver.api.mapper.annotations.Mapper;
+import com.datastax.oss.driver.api.mapper.annotations.PartitionKey;
+import com.datastax.oss.driver.api.mapper.annotations.Select;
+import com.datastax.oss.driver.api.mapper.annotations.Update;
+import com.datastax.oss.driver.api.testinfra.ccm.CcmRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionRule;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import java.util.Objects;
+import java.util.UUID;
+import org.junit.BeforeClass;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.junit.rules.RuleChain;
+import org.junit.rules.TestRule;
+
+@Category(ParallelizableTests.class)
+public class DefaultKeyspaceIT {
+  private static final String DEFAULT_KEYSPACE = ""default_keyspace"";
+  private static CcmRule ccm = CcmRule.getInstance();
+
+  private static SessionRule<CqlSession> sessionRule = SessionRule.builder(ccm).build();
+
+  private static InventoryMapper mapper;
+  @ClassRule public static TestRule chain = RuleChain.outerRule(ccm).around(sessionRule);
+
+  @BeforeClass
+  public static void setup() {
+    CqlSession session = sessionRule.session();
+    session.execute(
+        SimpleStatement.builder(
+                ""CREATE KEYSPACE IF NOT EXISTS ""
+                    + DEFAULT_KEYSPACE
+                    + "" ""
+                    + ""WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1}"")","[{'comment': 'Nit: you can use `String.format` to inject the keyspace name, it will be a bit more readable.', 'commenter': 'olim7t'}]"
1255,integration-tests/src/test/java/com/datastax/oss/driver/mapper/DefaultKeyspaceIT.java,"@@ -0,0 +1,336 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.mapper;
+
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.assertj.core.api.Assertions.assertThatThrownBy;
+
+import com.datastax.oss.driver.api.core.CqlIdentifier;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.api.core.servererrors.InvalidQueryException;
+import com.datastax.oss.driver.api.mapper.annotations.Dao;
+import com.datastax.oss.driver.api.mapper.annotations.DaoFactory;
+import com.datastax.oss.driver.api.mapper.annotations.DaoKeyspace;
+import com.datastax.oss.driver.api.mapper.annotations.Entity;
+import com.datastax.oss.driver.api.mapper.annotations.Mapper;
+import com.datastax.oss.driver.api.mapper.annotations.PartitionKey;
+import com.datastax.oss.driver.api.mapper.annotations.Select;
+import com.datastax.oss.driver.api.mapper.annotations.Update;
+import com.datastax.oss.driver.api.testinfra.ccm.CcmRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionRule;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import java.util.Objects;
+import java.util.UUID;
+import org.junit.BeforeClass;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.junit.rules.RuleChain;
+import org.junit.rules.TestRule;
+
+@Category(ParallelizableTests.class)
+public class DefaultKeyspaceIT {","[{'comment': 'ðŸ‘ ', 'commenter': 'olim7t'}]"
1255,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/entity/EntityDefinition.java,"@@ -26,6 +27,9 @@
 
   CodeBlock getCqlName();
 
+  @Nullable
+  String getDefaultKeyspace();","[{'comment': ""We should probably generalize the use of nullability annotations in the code. I tend to leave them out for internal code, but really it doesn't hurt to have them either.\r\n\r\nCreated [JAVA-2284](https://datastax-oss.atlassian.net/browse/JAVA-2284)."", 'commenter': 'olim7t'}]"
1255,mapper-processor/src/test/java/com/datastax/oss/driver/internal/mapper/processor/entity/EntityAnnotationTest.java,"@@ -74,4 +75,29 @@ public void should_fail_on_interface() {
             .addAnnotation(Entity.class)
             .build());
   }
+
+  @Test
+  public void should_fail_when_default_keyspace_attribute_has_more_than_one_element() {","[{'comment': ""Nit: compilation doesn't really fail, this should be named `should_warn...`"", 'commenter': 'olim7t'}]"
1255,mapper-runtime/src/main/java/com/datastax/oss/driver/api/mapper/annotations/Entity.java,"@@ -74,4 +74,24 @@
  */
 @Target(ElementType.TYPE)
 @Retention(RetentionPolicy.CLASS)
-public @interface Entity {}
+public @interface Entity {
+  /**
+   * Specifies a default keyspace to use when doing operations on a given {@code @Entity}","[{'comment': ""```suggestion\r\n   * Specifies a default keyspace to use when doing operations on this entity.\r\n```\r\nJavadoc doesn't need to link to itself."", 'commenter': 'olim7t'}]"
1255,mapper-runtime/src/main/java/com/datastax/oss/driver/api/mapper/annotations/Entity.java,"@@ -74,4 +74,24 @@
  */
 @Target(ElementType.TYPE)
 @Retention(RetentionPolicy.CLASS)
-public @interface Entity {}
+public @interface Entity {
+  /**
+   * Specifies a default keyspace to use when doing operations on a given {@code @Entity}
+   *
+   * <p>The defaultKeyspace is ignored if you will create your {@link Dao} using {@link DaoFactory}
+   * with explicitly setting {@link DaoKeyspace}:","[{'comment': 'I would just say ""if you pass a keyspace when building your DAO"" (and modify the example as suggested in my other comment).', 'commenter': 'olim7t'}]"
1255,mapper-runtime/src/main/java/com/datastax/oss/driver/api/mapper/annotations/Entity.java,"@@ -74,4 +74,24 @@
  */
 @Target(ElementType.TYPE)
 @Retention(RetentionPolicy.CLASS)
-public @interface Entity {}
+public @interface Entity {
+  /**
+   * Specifies a default keyspace to use when doing operations on a given {@code @Entity}
+   *
+   * <p>The defaultKeyspace is ignored if you will create your {@link Dao} using {@link DaoFactory}
+   * with explicitly setting {@link DaoKeyspace}:
+   *
+   * <pre>
+   *  &#64;Mapper
+   *  public interface InventoryMapper {
+   *    &#64;DaoFactory
+   *    ProductDao productDao(@DaoKeyspace String keyspace);
+   *  }
+   * </pre>","[{'comment': 'This might be obvious, but just to drive the message home I would make the example more descriptive:\r\n\r\nAdd an outline of the entity definition before the mapper:\r\n```java\r\n@Product(defaultKeyspace = ""inventory"")\r\npublic class Product { ... }\r\n```\r\n\r\nAdd a parameterless factory method to the mapper:\r\n```java\r\n@DaoFactory\r\nProductDao productDao();\r\n```\r\n\r\nAdd sample client code:\r\n```java\r\nProductDao productDao = mapper.productDao();\r\nproductDao.insert(product); // inserts into inventory.product\r\n\r\nProductDao productDaoTest = mapper.productDao(""test"");\r\nproductDaoTest.insert(product); // inserts into test.product\r\n```', 'commenter': 'olim7t'}]"
1255,manual/mapper/entities/README.md,"@@ -251,8 +251,32 @@ i.e.:
 private transient int notAColumn;
 ```
 
+### Setting Default Keyspace
+
+You can specify a default keyspace to use when doing operations on a given [@Entity]:
+
+```java
+@Entity(defaultKeyspace = ""ks_1"")
+public class Product {
+  //....
+}
+```
+
+The `defaultKeyspace` is ignored if you will create your [@Dao] using [@DaoFactory] with explicitly setting [@DaoKeyspace]:
+```java
+@Mapper
+public interface InventoryMapper { 
+  @DaoFactory
+  ProductDao productDao(@DaoKeyspace String keyspace);
+}
+```","[{'comment': 'See my suggestions on the annotation javadocs.', 'commenter': 'olim7t'}]"
1256,examples/src/main/java/com/datastax/oss/driver/examples/concurrent/LimitConcurrencyCustom.java,"@@ -0,0 +1,148 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.examples.concurrent;
+
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.bindMarker;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.insertInto;
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.CqlSessionBuilder;
+import com.datastax.oss.driver.api.core.cql.PreparedStatement;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import java.util.Set;
+import java.util.UUID;
+import java.util.concurrent.CompletableFuture;
+import java.util.concurrent.ConcurrentSkipListSet;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.concurrent.Semaphore;
+import java.util.concurrent.atomic.AtomicInteger;
+
+/**
+ * Creates a keyspace and tables, and loads data using Multi-Threaded approach into them.","[{'comment': '```suggestion\r\n * Creates a keyspace and table, and loads data using a multi-threaded approach.\r\n```', 'commenter': 'tolbertam'}]"
1256,examples/src/main/java/com/datastax/oss/driver/examples/concurrent/LimitConcurrencyCustom.java,"@@ -0,0 +1,148 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.examples.concurrent;
+
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.bindMarker;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.insertInto;
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.CqlSessionBuilder;
+import com.datastax.oss.driver.api.core.cql.PreparedStatement;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import java.util.Set;
+import java.util.UUID;
+import java.util.concurrent.CompletableFuture;
+import java.util.concurrent.ConcurrentSkipListSet;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.concurrent.Semaphore;
+import java.util.concurrent.atomic.AtomicInteger;
+
+/**
+ * Creates a keyspace and tables, and loads data using Multi-Threaded approach into them.
+ *
+ * <p>This example makes usage of a {@link CqlSession#execute(String)} method, which is responsible
+ * for executing requests in a blocking way. It uses {@link ExecutorService} to limit number of
+ * concurrent request to {@code CONCURRENCY_LEVEL}. It leverages {@link CompletableFuture} to
+ * achieve concurrency. It maintains at most {@code IN_FLIGHT_REQUESTS} using {@link Semaphore}.
+ *
+ * <p>Preconditions:
+ *
+ * <ul>
+ *   <li>An Apache Cassandra(R) cluster is running and accessible through the contacts points","[{'comment': '```suggestion\r\n *   <li>An Apache Cassandra(R) cluster is running and accessible through the contact points\r\n```', 'commenter': 'tolbertam'}]"
1256,examples/src/main/java/com/datastax/oss/driver/examples/concurrent/LimitConcurrencyCustom.java,"@@ -0,0 +1,148 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.examples.concurrent;
+
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.bindMarker;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.insertInto;
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.CqlSessionBuilder;
+import com.datastax.oss.driver.api.core.cql.PreparedStatement;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import java.util.Set;
+import java.util.UUID;
+import java.util.concurrent.CompletableFuture;
+import java.util.concurrent.ConcurrentSkipListSet;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.concurrent.Semaphore;
+import java.util.concurrent.atomic.AtomicInteger;
+
+/**
+ * Creates a keyspace and tables, and loads data using Multi-Threaded approach into them.
+ *
+ * <p>This example makes usage of a {@link CqlSession#execute(String)} method, which is responsible
+ * for executing requests in a blocking way. It uses {@link ExecutorService} to limit number of
+ * concurrent request to {@code CONCURRENCY_LEVEL}. It leverages {@link CompletableFuture} to
+ * achieve concurrency. It maintains at most {@code IN_FLIGHT_REQUESTS} using {@link Semaphore}.
+ *
+ * <p>Preconditions:
+ *
+ * <ul>
+ *   <li>An Apache Cassandra(R) cluster is running and accessible through the contacts points
+ *       identified by basic.contact-points (see application.conf).
+ * </ul>
+ *
+ * <p>Side effects:
+ *
+ * <ul>
+ *   <li>creates a new keyspace ""examples"" in the session. If a keyspace with this name already
+ *       exists, it will be reused;
+ *   <li>creates a table ""examples.tbl_sample_kv"". If it exist already, it will be reused;","[{'comment': '```suggestion\r\n *   <li>creates a table ""examples.tbl_sample_kv"". If it exists already, it will be reused;\r\n```', 'commenter': 'tolbertam'}]"
1256,examples/src/main/java/com/datastax/oss/driver/examples/concurrent/LimitConcurrencyCustomAsync.java,"@@ -0,0 +1,191 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.examples.concurrent;
+
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.bindMarker;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.insertInto;
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.CqlSessionBuilder;
+import com.datastax.oss.driver.api.core.cql.AsyncResultSet;
+import com.datastax.oss.driver.api.core.cql.PreparedStatement;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.UUID;
+import java.util.concurrent.CompletableFuture;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.atomic.AtomicInteger;
+import java.util.function.BiConsumer;
+
+/**
+ * Creates a keyspace and tables, and loads data using Async API into them.","[{'comment': '```suggestion\r\n * Creates a keyspace and table, and loads data using an async API.\r\n```', 'commenter': 'tolbertam'}]"
1256,examples/src/main/java/com/datastax/oss/driver/examples/concurrent/LimitConcurrencyCustomAsync.java,"@@ -0,0 +1,191 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.examples.concurrent;
+
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.bindMarker;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.insertInto;
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.CqlSessionBuilder;
+import com.datastax.oss.driver.api.core.cql.AsyncResultSet;
+import com.datastax.oss.driver.api.core.cql.PreparedStatement;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.UUID;
+import java.util.concurrent.CompletableFuture;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.atomic.AtomicInteger;
+import java.util.function.BiConsumer;
+
+/**
+ * Creates a keyspace and tables, and loads data using Async API into them.
+ *
+ * <p>This example makes usage of a {@link CqlSession#executeAsync(String)} method, which is
+ * responsible for executing requests in a non-blocking way. It uses {@link CompletableFuture} to
+ * limit number of concurrent request to {@code CONCURRENCY_LEVEL}.
+ *
+ * <p>Preconditions:
+ *
+ * <ul>
+ *   <li>An Apache Cassandra(R) cluster is running and accessible through the contacts points","[{'comment': '```suggestion\r\n *   <li>An Apache Cassandra(R) cluster is running and accessible through the contact points\r\n```', 'commenter': 'tolbertam'}]"
1256,examples/src/main/java/com/datastax/oss/driver/examples/concurrent/LimitConcurrencyCustomAsync.java,"@@ -0,0 +1,191 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.examples.concurrent;
+
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.bindMarker;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.insertInto;
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.CqlSessionBuilder;
+import com.datastax.oss.driver.api.core.cql.AsyncResultSet;
+import com.datastax.oss.driver.api.core.cql.PreparedStatement;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.UUID;
+import java.util.concurrent.CompletableFuture;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.atomic.AtomicInteger;
+import java.util.function.BiConsumer;
+
+/**
+ * Creates a keyspace and tables, and loads data using Async API into them.
+ *
+ * <p>This example makes usage of a {@link CqlSession#executeAsync(String)} method, which is
+ * responsible for executing requests in a non-blocking way. It uses {@link CompletableFuture} to
+ * limit number of concurrent request to {@code CONCURRENCY_LEVEL}.
+ *
+ * <p>Preconditions:
+ *
+ * <ul>
+ *   <li>An Apache Cassandra(R) cluster is running and accessible through the contacts points
+ *       identified by basic.contact-points (see application.conf).
+ * </ul>
+ *
+ * <p>Side effects:
+ *
+ * <ul>
+ *   <li>creates a new keyspace ""examples"" in the session. If a keyspace with this name already
+ *       exists, it will be reused;
+ *   <li>creates a table ""examples.tbl_sample_kv"". If it exist already, it will be reused;
+ *   <li>inserts a TOTAL_NUMBER_OF_INSERTS of rows into the table.
+ * </ul>
+ *
+ * @see <a href=""http://datastax.github.io/java-driver/manual/"">Java driver online manual</a>
+ */
+public class LimitConcurrencyCustomAsync {
+  private static final int CONCURRENCY_LEVEL = 32;
+  private static final int TOTAL_NUMBER_OF_INSERTS = 10_000;
+  // Used to track number of total inserts
+  private static final AtomicInteger INSERTS_COUNTER = new AtomicInteger();
+
+  public static void main(String[] args) throws InterruptedException, ExecutionException {
+
+    try (CqlSession session = new CqlSessionBuilder().build()) {
+      createSchema(session);
+      insertConcurrent(session);
+    }
+  }
+
+  private static void insertConcurrent(CqlSession session)
+      throws InterruptedException, ExecutionException {
+    PreparedStatement pst =
+        session.prepare(
+            insertInto(""examples"", ""tbl_sample_kv"")
+                .value(""id"", bindMarker(""id""))
+                .value(""value"", bindMarker(""value""))
+                .build());
+
+    // Construct CONCURRENCY_LEVEL number of ranges.
+    // Each range will be executed independently.
+    List<Range> ranges = createRanges(CONCURRENCY_LEVEL, TOTAL_NUMBER_OF_INSERTS);
+
+    // List of pending CONCURRENCY_LEVEL features that we will wait for at the end of the program.
+    List<CompletableFuture<?>> pending = new ArrayList<>();
+
+    // Every range will have dedicated CompletableFuture handling the execution.
+    for (Range range : ranges) {","[{'comment': ""This is an interesting way of controlling concurrency but it suffers a weakness in that if you have a slow response it may delay submitting requests in that range while other ranges may have otherwise completed, so you aren't effectively optimizing your concurrency.  For the intent of the example it's fine though, especially since it mirrors the nodejs implementation.\r\n\r\nAn alternative way to accomplish this would be to submit requests from the same thread, and on completion notify/wake up the thread in some way (could be as simple as enqueuing the next task into a queue) to evaluate the current number of inflight requests and then submit more.  I think we don't need to go that far though."", 'commenter': 'tolbertam'}, {'comment': 'I think we should provide users with something simpler to grasp. How about this (using my own suggestion of creating a queue of statements first):\r\n\r\n```\r\nprivate static void insertConcurrent(CqlSession session) throws InterruptedException {\r\n  PreparedStatement pst =\r\n      session.prepare(\r\n          insertInto(""examples"", ""tbl_sample_kv"")\r\n              .value(""id"", bindMarker(""id""))\r\n              .value(""value"", bindMarker(""value""))\r\n              .build());\r\n\r\n  // Executor service with CONCURRENCY_LEVEL number of threads that states an upper limit\r\n  // on number of request in progress.\r\n  ExecutorService executor = Executors.newFixedThreadPool(CONCURRENCY_LEVEL);\r\n\r\n  // create statements\r\n  Queue<BoundStatement> statements = new ConcurrentLinkedQueue<>();\r\n  for (int i = 0; i < TOTAL_NUMBER_OF_INSERTS; i++) {\r\n    statements.add(pst.bind().setUuid(""id"", UUID.randomUUID()).setInt(""value"", i));\r\n  }\r\n\r\n  // execute statements\r\n  for (int i = 0; i < CONCURRENCY_LEVEL; i++) {\r\n    executor.submit(() -> acquireQueryAndRelease(executor, session, statements));\r\n  }\r\n\r\n  REQUEST_LATCH.await();\r\n\r\n  // Shutdown executor to free resources\r\n  executor.shutdown();\r\n}\r\n\r\nprivate static void acquireQueryAndRelease(\r\n    ExecutorService executor, CqlSession session, Queue<BoundStatement> statements) {\r\n  BoundStatement stmt = statements.poll();\r\n  if (stmt == null) {\r\n    return;\r\n  }\r\n  SEMAPHORE.acquireUninterruptibly();\r\n  session\r\n      .executeAsync(stmt)\r\n      .whenCompleteAsync(\r\n          (result, error) -> {\r\n            REQUEST_LATCH.countDown();\r\n            SEMAPHORE.release();\r\n            if (error != null) {\r\n              error.printStackTrace();\r\n            }\r\n            acquireQueryAndRelease(executor, session, statements);\r\n          },\r\n          executor);\r\n}\r\n```', 'commenter': 'adutra'}, {'comment': ""That's a good solution :+1: and I like that better than my suggestion.  One suggestion, since the generated statements are based on an incrementing value, is there a need to pregenerate them?  You could just use an atomic long and increment and generate the statements as you execute them, and then return in `acquireQueryAndRelease` when the long is above `TOTAL_NUMBER_OF_INSERTS`."", 'commenter': 'tolbertam'}, {'comment': ""I think for the purpose of this example, having multiple sequential async executions is good enough. It's true that it may suffer from a single slow response in each `executeOneAtATime()` call, but with a large enough sample it should average out.\r\n\r\nI would like to avoid generating the statements upfront in a queue as we will be allocating all the memory upfront, making the example unsuitable for unbounded `TOTAL_NUMBER_OF_INSERTS` values."", 'commenter': 'jorgebay'}, {'comment': 'Unbounded inserts are rare. I still believe that 99% of users will have if not the statements, at least the data ready in memory before the insert operation begins. Large datasets will typically be broken into smaller batches of a few 1000 rows if need be. But in any case, nobody generates data to be inserted on-the-fly, unless you are populating the database with random data.\r\nTruly unbounded use cases would imo be typically handled by using the java Stream API, a Producer-Consumer setup with shared queue, or by reactive programming.', 'commenter': 'adutra'}, {'comment': 'I think that the main question is: do we want to have parity between examples in nodejs and java. Currently, the nodejs example is doing it using ranges and existing java example does the same. I like @adutra example because is more java idiomatic but still there is a problem of throttling requests - if we will have unbounded `Queue` there could be an out of memory error.\r\nThis line: \r\n```\r\n  for (int i = 0; i < TOTAL_NUMBER_OF_INSERTS; i++) {\r\n    statements.add(pst.bind().setUuid(""id"", UUID.randomUUID()).setInt(""value"", i));\r\n  }\r\n```\r\nstill is not solving that problem of out of memory on statements.\r\n ', 'commenter': 'tomekl007'}, {'comment': ""1. I think it's better if each driver strives to present idiomatic examples for its language.\r\n2. About the potential out of memory problem: this is just an example, so why not simply add a comment above the for-loop that creates the statements:\r\n\r\n> The code below creates statements in a loop for the sake of simplicity. In real life, you would probably obtain the data to insert from an external resource or system (e.g. a file, or a REST service); care should be taken not to run out of memory if too much data is held in memory."", 'commenter': 'adutra'}, {'comment': '@adutra Looking at your example I think that it is almost the same as existing: `LimitConcurrencyCustom`. Both are using `SEMAPHORE` to limit `IN_FLIGHT` and both are using     \r\n`ExecutorService executor = Executors.newFixedThreadPool(CONCURRENCY_LEVEL);` to limit number of concurrent requests. There is only one difference - in your example, you are using intermediate `Queue`.\r\nMy suggestion is like this:\r\n- I can add a `Queue` to the `LimitConcurrencyCustom` example to make a clearer distinction between consumer and producer - this will be java idiomatic way OR add the 3rd example with a Queue.\r\n- keep `LimitConcurrencyCustomAsync` according to @jorgebay suggestion to also have an example that is showing the throttling of produced queries (and is one to one with `NodeJs` example).\r\nWdyt?\r\n', 'commenter': 'tomekl007'}, {'comment': 'I agree with @tomekl007 about keeping `LimitConcurrencyCustomAsync` in the current state, as it shows a technique that chains futures without relying on any external component and concepts (latch / queue / semaphore). It covers important concepts that are hard to grasp for most users in few lines of code.', 'commenter': 'jorgebay'}, {'comment': ""Ok to keep the code as is. I don't mind, really."", 'commenter': 'adutra'}]"
1256,examples/src/main/java/com/datastax/oss/driver/examples/concurrent/LimitConcurrencyCustom.java,"@@ -0,0 +1,148 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.examples.concurrent;
+
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.bindMarker;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.insertInto;
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.CqlSessionBuilder;
+import com.datastax.oss.driver.api.core.cql.PreparedStatement;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import java.util.Set;
+import java.util.UUID;
+import java.util.concurrent.CompletableFuture;
+import java.util.concurrent.ConcurrentSkipListSet;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.concurrent.Semaphore;
+import java.util.concurrent.atomic.AtomicInteger;
+
+/**
+ * Creates a keyspace and tables, and loads data using Multi-Threaded approach into them.
+ *
+ * <p>This example makes usage of a {@link CqlSession#execute(String)} method, which is responsible
+ * for executing requests in a blocking way. It uses {@link ExecutorService} to limit number of
+ * concurrent request to {@code CONCURRENCY_LEVEL}. It leverages {@link CompletableFuture} to
+ * achieve concurrency. It maintains at most {@code IN_FLIGHT_REQUESTS} using {@link Semaphore}.
+ *
+ * <p>Preconditions:
+ *
+ * <ul>
+ *   <li>An Apache Cassandra(R) cluster is running and accessible through the contacts points
+ *       identified by basic.contact-points (see application.conf).
+ * </ul>
+ *
+ * <p>Side effects:
+ *
+ * <ul>
+ *   <li>creates a new keyspace ""examples"" in the session. If a keyspace with this name already
+ *       exists, it will be reused;
+ *   <li>creates a table ""examples.tbl_sample_kv"". If it exist already, it will be reused;
+ *   <li>inserts a TOTAL_NUMBER_OF_INSERTS of rows into the table.
+ * </ul>
+ *
+ * @see <a href=""https://docs.datastax.com/en/developer/java-driver/4.0"">Java driver online
+ *     manual</a>
+ */
+public class LimitConcurrencyCustom {
+  private static final int CONCURRENCY_LEVEL = 32;
+  private static final int TOTAL_NUMBER_OF_INSERTS = 10_000;
+  private static final int IN_FLIGHT_REQUESTS = 500;
+  // Semaphore for limiting number of in-flight requests.
+  private static final Semaphore SEMAPHORE = new Semaphore(IN_FLIGHT_REQUESTS);
+
+  // Create CountDownLatch that wait for completion of all pending requests
+  private static final CountDownLatch REQUEST_LATCH = new CountDownLatch(TOTAL_NUMBER_OF_INSERTS);
+
+  public static void main(String[] args) throws InterruptedException {
+
+    try (CqlSession session = new CqlSessionBuilder().build()) {
+      createSchema(session);
+      insertConcurrent(session);
+    }
+  }
+
+  private static void insertConcurrent(CqlSession session) throws InterruptedException {
+    PreparedStatement pst =
+        session.prepare(
+            insertInto(""examples"", ""tbl_sample_kv"")
+                .value(""id"", bindMarker(""id""))
+                .value(""value"", bindMarker(""value""))
+                .build());
+
+    // Used to track number of total inserts
+    AtomicInteger insertsCounter = new AtomicInteger();
+    // Used to track threads that were involved in a processing
+    Set<String> threads = new ConcurrentSkipListSet<>();
+
+    // Executor service with CONCURRENCY_LEVEL number of threads that states an upper limit
+    // on number of request in progress.
+    ExecutorService executor = Executors.newFixedThreadPool(CONCURRENCY_LEVEL);
+
+    // For every i we will insert a record to db
+    for (int i = 0; i < TOTAL_NUMBER_OF_INSERTS; i++) {
+      // Before submitting a request, we need to acquire 1 permit.
+      // If there is no permits available it blocks caller thread.
+      SEMAPHORE.acquire();
+      // Copy to final variable for usage in a separate thread
+      final int counter = i;
+
+      // We are running CqlSession.execute in a separate thread pool (executor)
+      CompletableFuture.supplyAsync(
+          () -> {
+            insertsCounter.incrementAndGet();
+            threads.add(Thread.currentThread().getName());
+
+            ResultSet executeRequest;
+            try {
+              executeRequest =
+                  session.execute(
+                      pst.bind().setUuid(""id"", UUID.randomUUID()).setInt(""value"", counter));
+            } finally {
+              // Signal that processing of this request finishes
+              REQUEST_LATCH.countDown();
+              // Once the request is executed, we release 1 permit.
+              // By doing so we allow caller thread to submit another async request.
+              SEMAPHORE.release();
+            }
+
+            return executeRequest;
+          },
+          // Here the separate thread pool is passed as the argument
+          executor);
+    }
+    // Await for execution of TOTAL_NUMBER_OF_INSERTS
+    REQUEST_LATCH.await();
+
+    System.out.println(
+        String.format(
+            ""Finished executing %s queries with a concurrency level of %s."",
+            insertsCounter.get(), threads.size()));
+    // Shutdown executor to free resources
+    executor.shutdown();
+  }
+
+  private static void createSchema(CqlSession session) {
+    session.execute(","[{'comment': 'If you have a multi-node cluster, its possible that the schema queries will timeout after 2s.  Maybe we should add a `schema` profile to application.conf and have all our examples use it.', 'commenter': 'tolbertam'}, {'comment': ""Good idea, I think it will be good to do it in a separate PR.\r\nI've created JIRA ticket to capture that:\r\nhttps://datastax-oss.atlassian.net/browse/JAVA-2318"", 'commenter': 'tomekl007'}]"
1256,examples/src/main/java/com/datastax/oss/driver/examples/concurrent/LimitConcurrencyCustom.java,"@@ -0,0 +1,148 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.examples.concurrent;
+
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.bindMarker;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.insertInto;
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.CqlSessionBuilder;
+import com.datastax.oss.driver.api.core.cql.PreparedStatement;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import java.util.Set;
+import java.util.UUID;
+import java.util.concurrent.CompletableFuture;
+import java.util.concurrent.ConcurrentSkipListSet;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.concurrent.Semaphore;
+import java.util.concurrent.atomic.AtomicInteger;
+
+/**
+ * Creates a keyspace and tables, and loads data using Multi-Threaded approach into them.
+ *
+ * <p>This example makes usage of a {@link CqlSession#execute(String)} method, which is responsible
+ * for executing requests in a blocking way. It uses {@link ExecutorService} to limit number of
+ * concurrent request to {@code CONCURRENCY_LEVEL}. It leverages {@link CompletableFuture} to
+ * achieve concurrency. It maintains at most {@code IN_FLIGHT_REQUESTS} using {@link Semaphore}.
+ *
+ * <p>Preconditions:
+ *
+ * <ul>
+ *   <li>An Apache Cassandra(R) cluster is running and accessible through the contacts points
+ *       identified by basic.contact-points (see application.conf).
+ * </ul>
+ *
+ * <p>Side effects:
+ *
+ * <ul>
+ *   <li>creates a new keyspace ""examples"" in the session. If a keyspace with this name already
+ *       exists, it will be reused;
+ *   <li>creates a table ""examples.tbl_sample_kv"". If it exist already, it will be reused;
+ *   <li>inserts a TOTAL_NUMBER_OF_INSERTS of rows into the table.
+ * </ul>
+ *
+ * @see <a href=""https://docs.datastax.com/en/developer/java-driver/4.0"">Java driver online
+ *     manual</a>
+ */
+public class LimitConcurrencyCustom {
+  private static final int CONCURRENCY_LEVEL = 32;
+  private static final int TOTAL_NUMBER_OF_INSERTS = 10_000;
+  private static final int IN_FLIGHT_REQUESTS = 500;
+  // Semaphore for limiting number of in-flight requests.
+  private static final Semaphore SEMAPHORE = new Semaphore(IN_FLIGHT_REQUESTS);
+
+  // Create CountDownLatch that wait for completion of all pending requests
+  private static final CountDownLatch REQUEST_LATCH = new CountDownLatch(TOTAL_NUMBER_OF_INSERTS);
+
+  public static void main(String[] args) throws InterruptedException {
+
+    try (CqlSession session = new CqlSessionBuilder().build()) {
+      createSchema(session);
+      insertConcurrent(session);
+    }
+  }
+
+  private static void insertConcurrent(CqlSession session) throws InterruptedException {
+    PreparedStatement pst =
+        session.prepare(
+            insertInto(""examples"", ""tbl_sample_kv"")
+                .value(""id"", bindMarker(""id""))
+                .value(""value"", bindMarker(""value""))
+                .build());
+
+    // Used to track number of total inserts
+    AtomicInteger insertsCounter = new AtomicInteger();
+    // Used to track threads that were involved in a processing
+    Set<String> threads = new ConcurrentSkipListSet<>();
+
+    // Executor service with CONCURRENCY_LEVEL number of threads that states an upper limit
+    // on number of request in progress.
+    ExecutorService executor = Executors.newFixedThreadPool(CONCURRENCY_LEVEL);
+
+    // For every i we will insert a record to db
+    for (int i = 0; i < TOTAL_NUMBER_OF_INSERTS; i++) {
+      // Before submitting a request, we need to acquire 1 permit.
+      // If there is no permits available it blocks caller thread.
+      SEMAPHORE.acquire();
+      // Copy to final variable for usage in a separate thread
+      final int counter = i;
+
+      // We are running CqlSession.execute in a separate thread pool (executor)
+      CompletableFuture.supplyAsync(","[{'comment': ""I think it may be easier to follow if you just use `executor.submit`.  Also, there would be no need to return `executeRequest`, since we aren't using the resulting future anyways."", 'commenter': 'tolbertam'}, {'comment': 'Another reason why `supplyAsync` is not the best option is that suppliers are not really meant to throw exceptions, but `session.execute` may throw an exception.', 'commenter': 'adutra'}]"
1256,examples/src/main/java/com/datastax/oss/driver/examples/concurrent/LimitConcurrencyCustom.java,"@@ -0,0 +1,148 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.examples.concurrent;
+
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.bindMarker;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.insertInto;
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.CqlSessionBuilder;
+import com.datastax.oss.driver.api.core.cql.PreparedStatement;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import java.util.Set;
+import java.util.UUID;
+import java.util.concurrent.CompletableFuture;
+import java.util.concurrent.ConcurrentSkipListSet;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.concurrent.Semaphore;
+import java.util.concurrent.atomic.AtomicInteger;
+
+/**
+ * Creates a keyspace and tables, and loads data using Multi-Threaded approach into them.
+ *
+ * <p>This example makes usage of a {@link CqlSession#execute(String)} method, which is responsible
+ * for executing requests in a blocking way. It uses {@link ExecutorService} to limit number of
+ * concurrent request to {@code CONCURRENCY_LEVEL}. It leverages {@link CompletableFuture} to
+ * achieve concurrency. It maintains at most {@code IN_FLIGHT_REQUESTS} using {@link Semaphore}.
+ *
+ * <p>Preconditions:
+ *
+ * <ul>
+ *   <li>An Apache Cassandra(R) cluster is running and accessible through the contacts points
+ *       identified by basic.contact-points (see application.conf).
+ * </ul>
+ *
+ * <p>Side effects:
+ *
+ * <ul>
+ *   <li>creates a new keyspace ""examples"" in the session. If a keyspace with this name already
+ *       exists, it will be reused;
+ *   <li>creates a table ""examples.tbl_sample_kv"". If it exist already, it will be reused;
+ *   <li>inserts a TOTAL_NUMBER_OF_INSERTS of rows into the table.
+ * </ul>
+ *
+ * @see <a href=""https://docs.datastax.com/en/developer/java-driver/4.0"">Java driver online
+ *     manual</a>
+ */
+public class LimitConcurrencyCustom {
+  private static final int CONCURRENCY_LEVEL = 32;
+  private static final int TOTAL_NUMBER_OF_INSERTS = 10_000;
+  private static final int IN_FLIGHT_REQUESTS = 500;
+  // Semaphore for limiting number of in-flight requests.
+  private static final Semaphore SEMAPHORE = new Semaphore(IN_FLIGHT_REQUESTS);
+
+  // Create CountDownLatch that wait for completion of all pending requests
+  private static final CountDownLatch REQUEST_LATCH = new CountDownLatch(TOTAL_NUMBER_OF_INSERTS);
+
+  public static void main(String[] args) throws InterruptedException {
+
+    try (CqlSession session = new CqlSessionBuilder().build()) {
+      createSchema(session);
+      insertConcurrent(session);
+    }
+  }
+
+  private static void insertConcurrent(CqlSession session) throws InterruptedException {
+    PreparedStatement pst =
+        session.prepare(
+            insertInto(""examples"", ""tbl_sample_kv"")
+                .value(""id"", bindMarker(""id""))
+                .value(""value"", bindMarker(""value""))
+                .build());
+
+    // Used to track number of total inserts
+    AtomicInteger insertsCounter = new AtomicInteger();
+    // Used to track threads that were involved in a processing
+    Set<String> threads = new ConcurrentSkipListSet<>();
+
+    // Executor service with CONCURRENCY_LEVEL number of threads that states an upper limit
+    // on number of request in progress.
+    ExecutorService executor = Executors.newFixedThreadPool(CONCURRENCY_LEVEL);
+
+    // For every i we will insert a record to db
+    for (int i = 0; i < TOTAL_NUMBER_OF_INSERTS; i++) {
+      // Before submitting a request, we need to acquire 1 permit.
+      // If there is no permits available it blocks caller thread.
+      SEMAPHORE.acquire();
+      // Copy to final variable for usage in a separate thread
+      final int counter = i;
+
+      // We are running CqlSession.execute in a separate thread pool (executor)
+      CompletableFuture.supplyAsync(
+          () -> {
+            insertsCounter.incrementAndGet();
+            threads.add(Thread.currentThread().getName());
+
+            ResultSet executeRequest;
+            try {
+              executeRequest =
+                  session.execute(
+                      pst.bind().setUuid(""id"", UUID.randomUUID()).setInt(""value"", counter));
+            } finally {
+              // Signal that processing of this request finishes
+              REQUEST_LATCH.countDown();
+              // Once the request is executed, we release 1 permit.
+              // By doing so we allow caller thread to submit another async request.
+              SEMAPHORE.release();
+            }
+
+            return executeRequest;
+          },
+          // Here the separate thread pool is passed as the argument
+          executor);
+    }
+    // Await for execution of TOTAL_NUMBER_OF_INSERTS
+    REQUEST_LATCH.await();
+
+    System.out.println(
+        String.format(
+            ""Finished executing %s queries with a concurrency level of %s."",
+            insertsCounter.get(), threads.size()));","[{'comment': 'Do we really need to store the thread names just to count how many were actually used? Sound a bit anecdotic to me; the size of this set is likely to be equal to `CONCURRENCY_LEVEL` anyway.', 'commenter': 'adutra'}, {'comment': 'Same for `insertsCounter`, it can only be equal to `TOTAL_NUMBER_OF_INSERTS`.', 'commenter': 'adutra'}, {'comment': 'Concerning `threads` I agree (changed).\r\nConcerning `insertCounter` I think it is beneficial to have that information in a log to see an actual number of inserts that succeeded.\r\nFor `LimitConcurrencyRequestThrottler` we have `List<CompletableFuture<?>> pending ` list to get that info. Here to assert that all records were properly inserter we need some additional counter. (I moved the incrementing logic after the execute call)\r\nhttps://github.com/datastax/java-driver/pull/1256/commits/704ac8e626e00df0387d79fee52a70254c72c843\r\nwdyt?', 'commenter': 'tomekl007'}]"
1256,examples/src/main/java/com/datastax/oss/driver/examples/concurrent/LimitConcurrencyCustom.java,"@@ -0,0 +1,148 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.examples.concurrent;
+
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.bindMarker;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.insertInto;
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.CqlSessionBuilder;
+import com.datastax.oss.driver.api.core.cql.PreparedStatement;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import java.util.Set;
+import java.util.UUID;
+import java.util.concurrent.CompletableFuture;
+import java.util.concurrent.ConcurrentSkipListSet;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.concurrent.Semaphore;
+import java.util.concurrent.atomic.AtomicInteger;
+
+/**
+ * Creates a keyspace and tables, and loads data using Multi-Threaded approach into them.
+ *
+ * <p>This example makes usage of a {@link CqlSession#execute(String)} method, which is responsible
+ * for executing requests in a blocking way. It uses {@link ExecutorService} to limit number of
+ * concurrent request to {@code CONCURRENCY_LEVEL}. It leverages {@link CompletableFuture} to
+ * achieve concurrency. It maintains at most {@code IN_FLIGHT_REQUESTS} using {@link Semaphore}.
+ *
+ * <p>Preconditions:
+ *
+ * <ul>
+ *   <li>An Apache Cassandra(R) cluster is running and accessible through the contacts points
+ *       identified by basic.contact-points (see application.conf).
+ * </ul>
+ *
+ * <p>Side effects:
+ *
+ * <ul>
+ *   <li>creates a new keyspace ""examples"" in the session. If a keyspace with this name already
+ *       exists, it will be reused;
+ *   <li>creates a table ""examples.tbl_sample_kv"". If it exist already, it will be reused;
+ *   <li>inserts a TOTAL_NUMBER_OF_INSERTS of rows into the table.
+ * </ul>
+ *
+ * @see <a href=""https://docs.datastax.com/en/developer/java-driver/4.0"">Java driver online
+ *     manual</a>
+ */
+public class LimitConcurrencyCustom {
+  private static final int CONCURRENCY_LEVEL = 32;
+  private static final int TOTAL_NUMBER_OF_INSERTS = 10_000;
+  private static final int IN_FLIGHT_REQUESTS = 500;
+  // Semaphore for limiting number of in-flight requests.
+  private static final Semaphore SEMAPHORE = new Semaphore(IN_FLIGHT_REQUESTS);
+
+  // Create CountDownLatch that wait for completion of all pending requests
+  private static final CountDownLatch REQUEST_LATCH = new CountDownLatch(TOTAL_NUMBER_OF_INSERTS);
+
+  public static void main(String[] args) throws InterruptedException {
+
+    try (CqlSession session = new CqlSessionBuilder().build()) {
+      createSchema(session);
+      insertConcurrent(session);
+    }
+  }
+
+  private static void insertConcurrent(CqlSession session) throws InterruptedException {
+    PreparedStatement pst =
+        session.prepare(
+            insertInto(""examples"", ""tbl_sample_kv"")
+                .value(""id"", bindMarker(""id""))
+                .value(""value"", bindMarker(""value""))
+                .build());
+
+    // Used to track number of total inserts
+    AtomicInteger insertsCounter = new AtomicInteger();
+    // Used to track threads that were involved in a processing
+    Set<String> threads = new ConcurrentSkipListSet<>();
+
+    // Executor service with CONCURRENCY_LEVEL number of threads that states an upper limit
+    // on number of request in progress.
+    ExecutorService executor = Executors.newFixedThreadPool(CONCURRENCY_LEVEL);
+
+    // For every i we will insert a record to db
+    for (int i = 0; i < TOTAL_NUMBER_OF_INSERTS; i++) {
+      // Before submitting a request, we need to acquire 1 permit.
+      // If there is no permits available it blocks caller thread.
+      SEMAPHORE.acquire();
+      // Copy to final variable for usage in a separate thread
+      final int counter = i;
+
+      // We are running CqlSession.execute in a separate thread pool (executor)
+      CompletableFuture.supplyAsync(
+          () -> {
+            insertsCounter.incrementAndGet();
+            threads.add(Thread.currentThread().getName());
+
+            ResultSet executeRequest;
+            try {
+              executeRequest =
+                  session.execute(
+                      pst.bind().setUuid(""id"", UUID.randomUUID()).setInt(""value"", counter));
+            } finally {
+              // Signal that processing of this request finishes
+              REQUEST_LATCH.countDown();
+              // Once the request is executed, we release 1 permit.
+              // By doing so we allow caller thread to submit another async request.
+              SEMAPHORE.release();
+            }
+
+            return executeRequest;
+          },
+          // Here the separate thread pool is passed as the argument
+          executor);
+    }
+    // Await for execution of TOTAL_NUMBER_OF_INSERTS
+    REQUEST_LATCH.await();
+
+    System.out.println(
+        String.format(
+            ""Finished executing %s queries with a concurrency level of %s."",
+            insertsCounter.get(), threads.size()));
+    // Shutdown executor to free resources
+    executor.shutdown();","[{'comment': 'You should call `executor.awaitTermination()` after `shutdown()`.', 'commenter': 'adutra'}]"
1256,examples/src/main/java/com/datastax/oss/driver/examples/concurrent/LimitConcurrencyCustom.java,"@@ -0,0 +1,148 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.examples.concurrent;
+
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.bindMarker;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.insertInto;
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.CqlSessionBuilder;
+import com.datastax.oss.driver.api.core.cql.PreparedStatement;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import java.util.Set;
+import java.util.UUID;
+import java.util.concurrent.CompletableFuture;
+import java.util.concurrent.ConcurrentSkipListSet;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.concurrent.Semaphore;
+import java.util.concurrent.atomic.AtomicInteger;
+
+/**
+ * Creates a keyspace and tables, and loads data using Multi-Threaded approach into them.
+ *
+ * <p>This example makes usage of a {@link CqlSession#execute(String)} method, which is responsible
+ * for executing requests in a blocking way. It uses {@link ExecutorService} to limit number of
+ * concurrent request to {@code CONCURRENCY_LEVEL}. It leverages {@link CompletableFuture} to
+ * achieve concurrency. It maintains at most {@code IN_FLIGHT_REQUESTS} using {@link Semaphore}.
+ *
+ * <p>Preconditions:
+ *
+ * <ul>
+ *   <li>An Apache Cassandra(R) cluster is running and accessible through the contacts points
+ *       identified by basic.contact-points (see application.conf).
+ * </ul>
+ *
+ * <p>Side effects:
+ *
+ * <ul>
+ *   <li>creates a new keyspace ""examples"" in the session. If a keyspace with this name already
+ *       exists, it will be reused;
+ *   <li>creates a table ""examples.tbl_sample_kv"". If it exist already, it will be reused;
+ *   <li>inserts a TOTAL_NUMBER_OF_INSERTS of rows into the table.
+ * </ul>
+ *
+ * @see <a href=""https://docs.datastax.com/en/developer/java-driver/4.0"">Java driver online
+ *     manual</a>
+ */
+public class LimitConcurrencyCustom {
+  private static final int CONCURRENCY_LEVEL = 32;
+  private static final int TOTAL_NUMBER_OF_INSERTS = 10_000;
+  private static final int IN_FLIGHT_REQUESTS = 500;
+  // Semaphore for limiting number of in-flight requests.
+  private static final Semaphore SEMAPHORE = new Semaphore(IN_FLIGHT_REQUESTS);
+
+  // Create CountDownLatch that wait for completion of all pending requests
+  private static final CountDownLatch REQUEST_LATCH = new CountDownLatch(TOTAL_NUMBER_OF_INSERTS);
+
+  public static void main(String[] args) throws InterruptedException {
+
+    try (CqlSession session = new CqlSessionBuilder().build()) {
+      createSchema(session);
+      insertConcurrent(session);
+    }
+  }
+
+  private static void insertConcurrent(CqlSession session) throws InterruptedException {
+    PreparedStatement pst =
+        session.prepare(
+            insertInto(""examples"", ""tbl_sample_kv"")
+                .value(""id"", bindMarker(""id""))
+                .value(""value"", bindMarker(""value""))
+                .build());
+
+    // Used to track number of total inserts
+    AtomicInteger insertsCounter = new AtomicInteger();
+    // Used to track threads that were involved in a processing
+    Set<String> threads = new ConcurrentSkipListSet<>();
+
+    // Executor service with CONCURRENCY_LEVEL number of threads that states an upper limit
+    // on number of request in progress.
+    ExecutorService executor = Executors.newFixedThreadPool(CONCURRENCY_LEVEL);
+
+    // For every i we will insert a record to db
+    for (int i = 0; i < TOTAL_NUMBER_OF_INSERTS; i++) {
+      // Before submitting a request, we need to acquire 1 permit.
+      // If there is no permits available it blocks caller thread.
+      SEMAPHORE.acquire();
+      // Copy to final variable for usage in a separate thread
+      final int counter = i;
+
+      // We are running CqlSession.execute in a separate thread pool (executor)
+      CompletableFuture.supplyAsync(
+          () -> {
+            insertsCounter.incrementAndGet();
+            threads.add(Thread.currentThread().getName());
+
+            ResultSet executeRequest;
+            try {
+              executeRequest =
+                  session.execute(
+                      pst.bind().setUuid(""id"", UUID.randomUUID()).setInt(""value"", counter));
+            } finally {","[{'comment': 'I think you should handle errors here for the sake of example, to show users how to deal with failed queries (a simple `printStackTrace()` is enough).', 'commenter': 'adutra'}]"
1256,examples/src/main/java/com/datastax/oss/driver/examples/concurrent/LimitConcurrencyCustomAsync.java,"@@ -0,0 +1,191 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.examples.concurrent;
+
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.bindMarker;
+import static com.datastax.oss.driver.api.querybuilder.QueryBuilder.insertInto;
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.CqlSessionBuilder;
+import com.datastax.oss.driver.api.core.cql.AsyncResultSet;
+import com.datastax.oss.driver.api.core.cql.PreparedStatement;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.UUID;
+import java.util.concurrent.CompletableFuture;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.atomic.AtomicInteger;
+import java.util.function.BiConsumer;
+
+/**
+ * Creates a keyspace and tables, and loads data using Async API into them.
+ *
+ * <p>This example makes usage of a {@link CqlSession#executeAsync(String)} method, which is
+ * responsible for executing requests in a non-blocking way. It uses {@link CompletableFuture} to
+ * limit number of concurrent request to {@code CONCURRENCY_LEVEL}.
+ *
+ * <p>Preconditions:
+ *
+ * <ul>
+ *   <li>An Apache Cassandra(R) cluster is running and accessible through the contacts points
+ *       identified by basic.contact-points (see application.conf).
+ * </ul>
+ *
+ * <p>Side effects:
+ *
+ * <ul>
+ *   <li>creates a new keyspace ""examples"" in the session. If a keyspace with this name already
+ *       exists, it will be reused;
+ *   <li>creates a table ""examples.tbl_sample_kv"". If it exist already, it will be reused;
+ *   <li>inserts a TOTAL_NUMBER_OF_INSERTS of rows into the table.
+ * </ul>
+ *
+ * @see <a href=""http://datastax.github.io/java-driver/manual/"">Java driver online manual</a>
+ */
+public class LimitConcurrencyCustomAsync {
+  private static final int CONCURRENCY_LEVEL = 32;
+  private static final int TOTAL_NUMBER_OF_INSERTS = 10_000;","[{'comment': 'I find the usage of `TOTAL_NUMBER_OF_INSERTS` a bit artificial. What people will typically use is a list, or queue, or stream of statements to execute. I think that the examples would be more helpful if we demonstrated how to execute statements stored in such structures concurrently.', 'commenter': 'adutra'}]"
1257,mapper-runtime/src/main/java/com/datastax/oss/driver/api/mapper/annotations/Attributes.java,"@@ -0,0 +1,48 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.mapper.annotations;
+
+import com.datastax.oss.driver.api.mapper.entity.saving.NullSavingStrategy;
+import java.lang.annotation.ElementType;
+import java.lang.annotation.Retention;
+import java.lang.annotation.RetentionPolicy;
+import java.lang.annotation.Target;
+
+@Target(ElementType.METHOD)
+@Retention(RetentionPolicy.CLASS)
+public @interface Attributes {
+
+  String ExecutionProfileName() default """";","[{'comment': '```suggestion\r\n  String executionProfileName() default """";\r\n```', 'commenter': 'olim7t'}]"
1257,mapper-runtime/src/main/java/com/datastax/oss/driver/api/mapper/annotations/Attributes.java,"@@ -0,0 +1,48 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.mapper.annotations;
+
+import com.datastax.oss.driver.api.mapper.entity.saving.NullSavingStrategy;
+import java.lang.annotation.ElementType;
+import java.lang.annotation.Retention;
+import java.lang.annotation.RetentionPolicy;
+import java.lang.annotation.Target;
+
+@Target(ElementType.METHOD)
+@Retention(RetentionPolicy.CLASS)
+public @interface Attributes {
+
+  String ExecutionProfileName() default """";
+
+  int pageSize() default Integer.MIN_VALUE;
+
+  boolean idempotence() default false;
+
+  String consistencyLevel() default """";
+
+  String serialConsistencyLevel() default """";
+
+  int timeout() default Integer.MIN_VALUE;
+
+  String routingKeyspace() default """";
+
+  /**
+   * How to handle null query parameters.
+   *
+   * <p>This defaults to {@link NullSavingStrategy#DO_NOT_SET}.
+   */
+  NullSavingStrategy nullSavingStrategy() default NullSavingStrategy.DO_NOT_SET;","[{'comment': 'Looks unused, and probably not needed here.', 'commenter': 'olim7t'}]"
1257,integration-tests/src/test/java/com/datastax/oss/driver/mapper/StatementAttributesIT.java,"@@ -198,11 +243,23 @@ private void validateQueryOptions(QueryLog log) {
     @Insert
     void save(Simple simple, StatementAttributes attributes);
 
+    @Insert
+    @Attributes(consistencyLevel = ""ANY"", serialConsistencyLevel = ""QUORUM"", pageSize = 13)","[{'comment': ""This is probably my OCD speaking, but it bothers me that the annotation does not use the same terminology as the parameter type. Of course it would be ludicrous to name them the same, but it would be nice to have some consistency.\r\nMy suggestion is `@WithStatementAttributes`, but I'm open to other ideas."", 'commenter': 'olim7t'}]"
1257,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoDeleteMethodGenerator.java,"@@ -169,6 +171,17 @@ public DaoDeleteMethodGenerator(
         ""$T boundStatementBuilder = $L.boundStatementBuilder()"",
         BoundStatementBuilder.class,
         statementName);
+    if (attributes != null) {
+      deleteBuilder.addStatement(
+          ""boundStatementBuilder = populateBoundStatementWithAttributes(boundStatementBuilder, $S, $S, $S, $L, $L, $L, $S)"",
+          attributes.ExecutionProfileName(),
+          attributes.consistencyLevel(),
+          attributes.serialConsistencyLevel(),
+          attributes.idempotence(),
+          attributes.pageSize(),
+          attributes.timeout(),
+          attributes.routingKeyspace());
+    }
     if (statementAttributeParam != null) {","[{'comment': ""This allows both the annotation and the parameter.\r\n1. :+1: I think this is the right thing to do. I can see cases where you might want one attribute static and another dynamic.\r\n2. We should cover that in tests.\r\n3. There is going to be a problem with the way default values are handled. Currently the parameter version of `populateBoundStatementWithAttributes` always sets the values, it might overwrite an annotation value with a default that the user didn't set explicitly. Actually I think we might in fact need a way to tell which values were set on `StatementAttributes` (sorry I made you change that in 2139): either `xxxWasSet` methods, or we make every getter return an optional."", 'commenter': 'olim7t'}]"
1257,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoDeleteMethodGenerator.java,"@@ -169,6 +171,17 @@ public DaoDeleteMethodGenerator(
         ""$T boundStatementBuilder = $L.boundStatementBuilder()"",
         BoundStatementBuilder.class,
         statementName);
+    if (attributes != null) {
+      deleteBuilder.addStatement(
+          ""boundStatementBuilder = populateBoundStatementWithAttributes(boundStatementBuilder, $S, $S, $S, $L, $L, $L, $S)"",
+          attributes.ExecutionProfileName(),
+          attributes.consistencyLevel(),
+          attributes.serialConsistencyLevel(),
+          attributes.idempotence(),
+          attributes.pageSize(),
+          attributes.timeout(),
+          attributes.routingKeyspace());
+    }","[{'comment': 'This can be extracted as a shared method in `DaoMethodGenerator`.', 'commenter': 'olim7t'}]"
1257,mapper-runtime/src/main/java/com/datastax/oss/driver/internal/mapper/DaoBase.java,"@@ -143,6 +147,50 @@ public static BoundStatementBuilder populateBoundStatementWithAttributes(
     return builder;
   }
 
+  public BoundStatementBuilder populateBoundStatementWithAttributes(
+      BoundStatementBuilder builder,
+      String profileName,
+      String consistencyLevel,
+      String serialConsistencyLevel,
+      boolean idempotent,
+      int pageSize,
+      long timeout,
+      String keyspace) {
+
+    Duration duration = null;
+    if (timeout != Long.MIN_VALUE) {
+      duration = Duration.ofMillis(timeout);
+    }
+    ConsistencyLevel consistency =
+        getConsistencyLevelFromName(fetchDefaultAttributeStringValue(consistencyLevel));
+    ConsistencyLevel serialConsistency =
+        getConsistencyLevelFromName(fetchDefaultAttributeStringValue(serialConsistencyLevel));
+    builder =
+        builder
+            .setExecutionProfileName(fetchDefaultAttributeStringValue(profileName))
+            .setConsistencyLevel(consistency)
+            .setSerialConsistencyLevel(serialConsistency)
+            .setIdempotence(idempotent)
+            .setPageSize(pageSize)
+            .setTimeout(Duration.ofMillis(timeout))
+            .setRoutingKeyspace(fetchDefaultAttributeStringValue(keyspace));
+
+    return builder;
+  }
+
+  private String fetchDefaultAttributeStringValue(String value) {
+    if (value.equals("""")) {
+      return null;
+    }
+    return value;
+  }
+
+  private ConsistencyLevel getConsistencyLevelFromName(String name) {
+    DefaultDriverContext ddContext = (DefaultDriverContext) (context.getSession().getContext());
+    ConsistencyLevelRegistry registry = ddContext.getConsistencyLevelRegistry();
+    return registry.codeToLevel(registry.nameToCode(name));","[{'comment': 'ðŸ‘ , but cast to `InternalDriverContext` instead.', 'commenter': 'olim7t'}]"
1257,mapper-runtime/src/main/java/com/datastax/oss/driver/api/mapper/annotations/Attributes.java,"@@ -0,0 +1,48 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.mapper.annotations;
+
+import com.datastax.oss.driver.api.mapper.entity.saving.NullSavingStrategy;
+import java.lang.annotation.ElementType;
+import java.lang.annotation.Retention;
+import java.lang.annotation.RetentionPolicy;
+import java.lang.annotation.Target;
+
+@Target(ElementType.METHOD)
+@Retention(RetentionPolicy.CLASS)
+public @interface Attributes {
+
+  String ExecutionProfileName() default """";
+
+  int pageSize() default Integer.MIN_VALUE;
+
+  boolean idempotence() default false;
+
+  String consistencyLevel() default """";
+
+  String serialConsistencyLevel() default """";
+
+  int timeout() default Integer.MIN_VALUE;","[{'comment': 'Maybe we should use a string for this, with the format expected by [Duration.parse()](https://docs.oracle.com/javase/8/docs/api/java/time/Duration.html#parse-java.lang.CharSequence-) (I hesitate calling that ""more user-friendly"", that format is horrific).', 'commenter': 'olim7t'}]"
1257,mapper-runtime/src/main/java/com/datastax/oss/driver/api/mapper/annotations/Attributes.java,"@@ -0,0 +1,48 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.mapper.annotations;
+
+import com.datastax.oss.driver.api.mapper.entity.saving.NullSavingStrategy;
+import java.lang.annotation.ElementType;
+import java.lang.annotation.Retention;
+import java.lang.annotation.RetentionPolicy;
+import java.lang.annotation.Target;
+
+@Target(ElementType.METHOD)
+@Retention(RetentionPolicy.CLASS)
+public @interface Attributes {","[{'comment': ""ðŸ‘ I think this is the right set of attributes to expose here, the others don't really make sense statically."", 'commenter': 'olim7t'}, {'comment': 'On which level the `Attributes` annotation can be specified? Only on the method level?', 'commenter': 'tomekl007'}, {'comment': 'Yes only on the method level.', 'commenter': 'GregBestland'}]"
1257,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoDeleteMethodGenerator.java,"@@ -169,6 +171,17 @@ public DaoDeleteMethodGenerator(
         ""$T boundStatementBuilder = $L.boundStatementBuilder()"",
         BoundStatementBuilder.class,
         statementName);
+    if (attributes != null) {","[{'comment': ""Don't we want to support `Update` as well?\r\nI think that we should create tests in mapper tests per operation (InsertIT, DeleteIt)"", 'commenter': 'tomekl007'}, {'comment': ""I don't think that's the right way to distribute the tests. The attribute, attribute statement tests all leverage a specific configuration and setup in simulacron. Replicating this setup in the other various unit tests would be costly IMHO. "", 'commenter': 'GregBestland'}]"
1257,mapper-runtime/src/main/java/com/datastax/oss/driver/internal/mapper/DaoBase.java,"@@ -143,6 +147,50 @@ public static BoundStatementBuilder populateBoundStatementWithAttributes(
     return builder;
   }
 
+  public BoundStatementBuilder populateBoundStatementWithAttributes(
+      BoundStatementBuilder builder,
+      String profileName,
+      String consistencyLevel,
+      String serialConsistencyLevel,
+      boolean idempotent,
+      int pageSize,
+      long timeout,
+      String keyspace) {
+
+    Duration duration = null;
+    if (timeout != Long.MIN_VALUE) {
+      duration = Duration.ofMillis(timeout);
+    }","[{'comment': ""don't we need a similar check and set default for pageSize and idempotent?\r\nOr those two defaults:\r\n```\r\n\r\n  int pageSize() default Integer.MIN_VALUE;\r\n\r\n  boolean idempotence() default false;\r\n``` \r\nare safe to pass to the `builder` without conversion to builder neutral values? (like for example for `setExecutionProfileName` the `null` is set if annotation didn't set it explicitly)"", 'commenter': 'tomekl007'}, {'comment': 'This has been reworked.\r\n\r\nBut to answer your original question, if not set these are the default values the builder would use anyway.', 'commenter': 'GregBestland'}]"
1257,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoInsertMethodGenerator.java,"@@ -122,6 +124,17 @@ public DaoInsertMethodGenerator(
         BoundStatementBuilder.class,
         statementName);
 
+    if (attributes != null) {
+      insertBuilder.addStatement(
+          ""boundStatementBuilder = populateBoundStatementWithAttributes(boundStatementBuilder, $S, $S, $S, $L, $L, $L, $S)"",
+          attributes.ExecutionProfileName(),
+          attributes.consistencyLevel(),
+          attributes.serialConsistencyLevel(),
+          attributes.idempotence(),
+          attributes.pageSize(),
+          attributes.timeout(),
+          attributes.routingKeyspace());
+    }
     if (statementAttributesParam != null) {","[{'comment': 'should the `statementAttributesParam` programmatic values override those specified by the annotation?\r\nMaybe we should log a warning if both mechanisms are used on the same method?', 'commenter': 'tomekl007'}, {'comment': ""The statementAttributesParam should definitely override the Attributes set at compile time, provided they are being explictly set. As Olivier mentioned above right now we have  aproblem where the defaults of statementAttributesParam will override ones that are explictly set by Attributes, which is the real problem here.  \r\n\r\n\r\nHere is how I think it should work.\r\n1. Attributes are applied, if they aren't specific defaults are used.\r\n2. statementAttributes are applied... their defaults should not override Specific Attributes that are set. \r\n"", 'commenter': 'GregBestland'}]"
1257,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoMethodGenerator.java,"@@ -215,6 +216,21 @@ protected void maybeAddTimestamp(String timestamp, MethodSpec.Builder methodBuil
     }
   }
 
+  protected void populateBuilderWithStatementAttribute(
+      MethodSpec.Builder builder, WithStatementAttributes withStatementAttributes) {","[{'comment': '1. `methodElement` is accessible from this class, so maybe you can extract the annotation here instead of passing it as a parameter.\r\n2. nit: should be populateBuilderWithStatementAttribute**s** to be consistent', 'commenter': 'olim7t'}]"
1257,mapper-runtime/src/main/java/com/datastax/oss/driver/internal/mapper/DaoBase.java,"@@ -114,35 +118,105 @@ protected static SimpleStatement replaceKeyspaceAndTablePlaceholders(
     return SimpleStatement.newInstance(queryString);
   }
 
-  public static BoundStatementBuilder populateBoundStatementWithAttributes(
+  public static BoundStatementBuilder populateBoundStatementWithStatementAttributes(
       BoundStatementBuilder builder, StatementAttributes attributes) {
 
-    builder =
-        builder
-            .setExecutionProfileName(attributes.getExecutionProfileName())
-            .setExecutionProfile(attributes.getExecutionProfile())
-            .setConsistencyLevel(attributes.getConsistencyLevel())
-            .setSerialConsistencyLevel(attributes.getSerialConsistencyLevel())
-            .setIdempotence(attributes.isIdempotent())
-            .setPageSize(attributes.getPageSize())
-            .setRoutingKey(attributes.getRoutingKey())
-            .setRoutingKeyspace(attributes.getRoutingKeyspace())
-            .setPagingState(attributes.getPagingState())
-            .setTimeout(attributes.getTimeout())
-            .setQueryTimestamp(attributes.getQueryTimestamp())
-            .setNode(attributes.getNode())
-            .setRoutingToken(attributes.getRoutingToken());
-    for (String customPayloadKey : attributes.getCustomPayload().keySet()) {
-      builder =
-          builder.addCustomPayload(
-              customPayloadKey, attributes.getCustomPayload().get(customPayloadKey));
+    if (attributes.getExecutionProfileName().isPresent()) {
+      builder = builder.setExecutionProfileName(attributes.getExecutionProfileName().get());
+    }","[{'comment': ""You can simplify this as:\r\n```java\r\nattributes.getExecutionProfileName().ifPresent(builder::setExecutionProfileName);\r\n```\r\nYou just need to make `builder` final (for the custom payload loop you don't need to reassign builder, we know it's mutable)."", 'commenter': 'olim7t'}]"
1257,mapper-runtime/src/main/java/com/datastax/oss/driver/internal/mapper/DaoBase.java,"@@ -114,35 +118,105 @@ protected static SimpleStatement replaceKeyspaceAndTablePlaceholders(
     return SimpleStatement.newInstance(queryString);
   }
 
-  public static BoundStatementBuilder populateBoundStatementWithAttributes(
+  public static BoundStatementBuilder populateBoundStatementWithStatementAttributes(
       BoundStatementBuilder builder, StatementAttributes attributes) {
 
-    builder =
-        builder
-            .setExecutionProfileName(attributes.getExecutionProfileName())
-            .setExecutionProfile(attributes.getExecutionProfile())
-            .setConsistencyLevel(attributes.getConsistencyLevel())
-            .setSerialConsistencyLevel(attributes.getSerialConsistencyLevel())
-            .setIdempotence(attributes.isIdempotent())
-            .setPageSize(attributes.getPageSize())
-            .setRoutingKey(attributes.getRoutingKey())
-            .setRoutingKeyspace(attributes.getRoutingKeyspace())
-            .setPagingState(attributes.getPagingState())
-            .setTimeout(attributes.getTimeout())
-            .setQueryTimestamp(attributes.getQueryTimestamp())
-            .setNode(attributes.getNode())
-            .setRoutingToken(attributes.getRoutingToken());
-    for (String customPayloadKey : attributes.getCustomPayload().keySet()) {
-      builder =
-          builder.addCustomPayload(
-              customPayloadKey, attributes.getCustomPayload().get(customPayloadKey));
+    if (attributes.getExecutionProfileName().isPresent()) {
+      builder = builder.setExecutionProfileName(attributes.getExecutionProfileName().get());
+    }
+    if (attributes.getExecutionProfile().isPresent()) {
+      builder = builder.setExecutionProfile(attributes.getExecutionProfile().get());
+    }
+    if (attributes.getConsistencyLevel().isPresent()) {
+      builder = builder.setConsistencyLevel(attributes.getConsistencyLevel().get());
+    }
+    if (attributes.getSerialConsistencyLevel().isPresent()) {
+      builder = builder.setSerialConsistencyLevel(attributes.getSerialConsistencyLevel().get());
+    }
+    if (attributes.isIdempotent().isPresent()) {
+      builder = builder.setIdempotence(attributes.isIdempotent().get());
+    }
+    if (attributes.getPageSize().isPresent()) {
+      builder = builder.setPageSize(attributes.getPageSize().get());
+    }
+    if (attributes.getRoutingKey().isPresent()) {
+      builder = builder.setRoutingKey(attributes.getRoutingKey().get());
+    }
+    if (attributes.getRoutingKeyspace().isPresent()) {
+      builder = builder.setRoutingKeyspace(attributes.getRoutingKeyspace().get());
+    }
+    if (attributes.getPagingState().isPresent()) {
+      builder = builder.setPagingState(attributes.getPagingState().get());
+    }
+    if (attributes.getTimeout().isPresent()) {
+      builder = builder.setTimeout(attributes.getTimeout().get());
+    }
+    if (attributes.getQueryTimestamp().isPresent()) {
+      builder = builder.setQueryTimestamp(attributes.getQueryTimestamp().get());
+    }
+    if (attributes.getNode().isPresent()) {
+      builder = builder.setNode(attributes.getNode().get());
+    }
+    if (attributes.getRoutingToken().isPresent()) {
+      builder = builder.setRoutingToken(attributes.getRoutingToken().get());
     }
-    if (attributes.isTracing()) {
-      builder = builder.setTracing();
+    if (attributes.isTracing().isPresent() && attributes.isTracing().get()) {
+      builder.setTracing();
     }
+    if (attributes.getCustomPayload().isPresent()) {
+      for (String customPayloadKey : attributes.getCustomPayload().get().keySet()) {
+        builder =
+            builder.addCustomPayload(
+                customPayloadKey, attributes.getCustomPayload().get().get(customPayloadKey));
+      }
+    }
+    return builder;
+  }
+
+  public BoundStatementBuilder populateBoundStatementWithStatementAttributes(
+      BoundStatementBuilder builder,
+      String profileName,
+      String consistencyLevel,
+      String serialConsistencyLevel,
+      boolean idempotent,
+      int pageSize,
+      String timeout,
+      String keyspace) {
+
+    Duration duration = null;
+    if (!timeout.equals("""")) {","[{'comment': 'nit: `timeout.isEmpty()`', 'commenter': 'olim7t'}]"
1257,mapper-runtime/src/main/java/com/datastax/oss/driver/internal/mapper/DefaultStatementAttributes.java,"@@ -80,88 +81,88 @@ public DefaultStatementAttributes(
 
   @Nullable
   @Override
-  public DriverExecutionProfile getExecutionProfile() {
-    return profile;
+  public Optional<DriverExecutionProfile> getExecutionProfile() {
+    return Optional.ofNullable(profile);","[{'comment': ""I know IDEA complains about it, but I think we should have `Optional<...>` fields instead of rewrapping on every call. I can easily see someone storing a `StatementAttributes` in a constant to reduce GC pressure, we should not reallocate every time it's used.\r\n"", 'commenter': 'olim7t'}, {'comment': ""I just do what my IDE tells me. I'll make the change\r\n"", 'commenter': 'GregBestland'}]"
1257,mapper-runtime/src/main/java/com/datastax/oss/driver/api/mapper/StatementAttributes.java,"@@ -49,8 +48,7 @@
    *
    * @see DriverExecutionProfile
    */
-  @Nullable
-  DriverExecutionProfile getExecutionProfile();
+  Optional<DriverExecutionProfile> getExecutionProfile();","[{'comment': 'Can you annotate all those methods as `@NonNull`?', 'commenter': 'olim7t'}]"
1257,integration-tests/src/test/java/com/datastax/oss/driver/mapper/StatementAttributesIT.java,"@@ -78,6 +75,88 @@ public void setup() {
 
   @Test
   public void should_honor_runtime_attributes_insert() {
+    primeInsertQuery();
+    CqlSession session = SessionUtils.newSession(simulacronRule);
+    InventoryMapper inventoryMapper =
+        new StatementAttributesIT_InventoryMapperBuilder(session).build();
+    SimpleDao simpleDao = inventoryMapper.simpleDao(sessionRule.keyspace());
+    StatementAttributes attributes = buildRunTimeAttributes();
+    simulacronRule.cluster().clearLogs();
+    simpleDao.save(simple, attributes);
+    ClusterQueryLogReport report = simulacronRule.cluster().getLogs();
+    validateQueryOptions(report.getQueryLogs().get(0), true);
+  }
+
+  @Test
+  public void should_honor_attributes_insert() {","[{'comment': 'maybe you could change the name of this test to: `should_honor_annotation_attributes_insert` (ant other tests that use: `@WithStatementAttributes`) for better readability?', 'commenter': 'tomekl007'}]"
1257,mapper-runtime/src/main/java/com/datastax/oss/driver/api/mapper/annotations/WithStatementAttributes.java,"@@ -0,0 +1,40 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.mapper.annotations;
+
+import java.lang.annotation.ElementType;
+import java.lang.annotation.Retention;
+import java.lang.annotation.RetentionPolicy;
+import java.lang.annotation.Target;
+
+@Target(ElementType.METHOD)
+@Retention(RetentionPolicy.CLASS)
+public @interface WithStatementAttributes {
+
+  String executionProfileName() default """";
+
+  int pageSize() default Integer.MIN_VALUE;
+
+  boolean idempotence() default false;
+
+  String consistencyLevel() default """";","[{'comment': 'Why we are not using `com.datastax.oss.driver.api.core.ConsistencyLevel` here instead of a `String`?', 'commenter': 'tomekl007'}, {'comment': ""Annotations don't support complex objects as data types. "", 'commenter': 'GregBestland'}]"
1257,integration-tests/src/test/java/com/datastax/oss/driver/mapper/StatementAttributesIT.java,"@@ -198,11 +243,35 @@ private void validateQueryOptions(QueryLog log) {
     @Insert
     void save(Simple simple, StatementAttributes attributes);
 
+    @Insert
+    @WithStatementAttributes(
+      consistencyLevel = ""ANY"",
+      serialConsistencyLevel = ""QUORUM"",
+      pageSize = 13
+    )
+    void save2(Simple simple);
+
     @Delete
     void delete(Simple simple, StatementAttributes attributes);
 
+    @Delete
+    @WithStatementAttributes(
+      consistencyLevel = ""ANY"",
+      serialConsistencyLevel = ""QUORUM"",
+      pageSize = 13
+    )
+    void delete2(Simple simple);
+
     @Select
     Simple findByPk(UUID pk, StatementAttributes attributes);
+
+    @Select
+    @WithStatementAttributes(","[{'comment': 'Are there any edge cases that exclude each other in the `@WithStatementAttributes` configuration?\r\nFor example, do `serialConsistency` can be higher than `consistencyLevel`?\r\nI am asking if we should add some kind of validation or we want to pass those properties to actual statements as they are', 'commenter': 'tomekl007'}, {'comment': 'No, the attributes are independent from each other.', 'commenter': 'olim7t'}]"
1257,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoMethodGenerator.java,"@@ -42,6 +42,8 @@
   protected final ExecutableElement methodElement;
   protected final DaoImplementationSharedCode enclosingClass;
   protected final ProcessorContext context;
+  protected final String FUNCTION_STATEMENT_BUILDER_TYPE =
+      ""java.util.function.Function<com.datastax.oss.driver.api.core.cql.BoundStatementBuilder,com.datastax.oss.driver.api.core.cql.BoundStatementBuilder>"";","[{'comment': ""I do no like this. I tried for a few hours to come up with a better method for type matching than String comparison and came up empty handed. It's relatively trivial to match A TypeMirror object to a Class provided generics are not involved. I was able to figure out how introspect and check the function generics  parameters against a class. The problem was that the TypeMirror object representing the Function parameter would not match against Function.class, and I was not able to construct a TypeMirror object that included the generics. If anyone has a better way to do this I'm all ears.  The current solution is a kludge but it should work."", 'commenter': 'GregBestland'}, {'comment': 'You need to extract the element from the mirror to match the raw type, and then match the type parameters separately. I think this will do it:\r\n```java\r\nVariableElement parameter = ...\r\nTypeMirror mirror = parameter.asType();\r\nif (mirror.getKind() == TypeKind.DECLARED) {\r\n  DeclaredType declaredType = (DeclaredType) mirror;\r\n  if (context.getClassUtils().isSame(declaredType.asElement(), Function.class)\r\n      && context.getClassUtils().isSame(declaredType.getTypeArguments().get(0), BoundStatementBuilder.class)\r\n      && context.getClassUtils().isSame(declaredType.getTypeArguments().get(1), BoundStatementBuilder.class)) {\r\n    // match\r\n  }\r\n}\r\n```', 'commenter': 'olim7t'}, {'comment': ""This worked, it occurred to me to use The DeclaredType to match the TypeArguments of the function, but I didn't understand that isSame would would with the element but not with TypeMirror."", 'commenter': 'GregBestland'}]"
1264,core/src/main/java/com/datastax/oss/driver/api/core/tracker/RequestTracker.java,"@@ -40,28 +40,73 @@
    *     GenericType) session.execute} call until the result is made available to the client).
    * @param executionProfile the execution profile of this request.
    * @param node the node that returned the successful response.
+   * @deprecated override {@link #onSuccess(Request, long, DriverExecutionProfile, Node, String)}
+   *     instead.
    */
+  @Deprecated
   default void onSuccess(
       @NonNull Request request,
       long latencyNanos,
       @NonNull DriverExecutionProfile executionProfile,
       @NonNull Node node) {}
 
+  /**
+   * Invoked each time a request succeeds.
+   *
+   * @param latencyNanos the overall execution time (from the {@link Session#execute(Request,
+   *     GenericType) session.execute} call until the result is made available to the client).
+   * @param executionProfile the execution profile of this request.
+   * @param node the node that returned the successful response.
+   * @param requestPrefix the dedicated log prefix for a request
+   */
+  default void onSuccess(
+      @NonNull Request request,
+      long latencyNanos,
+      @NonNull DriverExecutionProfile executionProfile,
+      @NonNull Node node,
+      @NonNull String requestPrefix) {","[{'comment': 'Could you name this `requestLogPrefix`?', 'commenter': 'olim7t'}]"
1264,core/src/main/java/com/datastax/oss/driver/api/core/tracker/RequestTracker.java,"@@ -40,28 +40,73 @@
    *     GenericType) session.execute} call until the result is made available to the client).
    * @param executionProfile the execution profile of this request.
    * @param node the node that returned the successful response.
+   * @deprecated override {@link #onSuccess(Request, long, DriverExecutionProfile, Node, String)}
+   *     instead.
    */
+  @Deprecated
   default void onSuccess(
       @NonNull Request request,
       long latencyNanos,
       @NonNull DriverExecutionProfile executionProfile,
       @NonNull Node node) {}
 
+  /**
+   * Invoked each time a request succeeds.
+   *
+   * @param latencyNanos the overall execution time (from the {@link Session#execute(Request,
+   *     GenericType) session.execute} call until the result is made available to the client).
+   * @param executionProfile the execution profile of this request.
+   * @param node the node that returned the successful response.
+   * @param requestPrefix the dedicated log prefix for a request","[{'comment': '```suggestion\r\n   * @param requestPrefix the dedicated log prefix for this request\r\n```\r\n(same as in the description for executionProfile)', 'commenter': 'olim7t'}]"
1264,core/src/main/java/com/datastax/oss/driver/api/core/tracker/RequestTracker.java,"@@ -40,28 +40,73 @@
    *     GenericType) session.execute} call until the result is made available to the client).
    * @param executionProfile the execution profile of this request.
    * @param node the node that returned the successful response.
+   * @deprecated override {@link #onSuccess(Request, long, DriverExecutionProfile, Node, String)}
+   *     instead.","[{'comment': 'Can you add at the beginning of the sentence: ""This method only exists for backward compatibility. Override ...""\r\n\r\nAlso, I think we can remove everything except the `@deprecated` tag, to make it more clear that this should not be used.', 'commenter': 'olim7t'}]"
1264,core/src/main/java/com/datastax/oss/driver/api/core/tracker/RequestTracker.java,"@@ -86,10 +155,33 @@ default void onNodeError(
    *     GenericType) session.execute} call until the result is made available to the client).
    * @param executionProfile the execution profile of this request.
    * @param node the node that returned the successful response.
+   * @deprecated override {@link #onNodeSuccess(Request, long, DriverExecutionProfile, Node,
+   *     String)} instead.
    */
+  @Deprecated
   default void onNodeSuccess(
       @NonNull Request request,
       long latencyNanos,
       @NonNull DriverExecutionProfile executionProfile,
       @NonNull Node node) {}
+
+  /**
+   * Invoked each time a request succeeds at the node level. Similar to {@link #onSuccess(Request,
+   * long, DriverExecutionProfile, Node)} but at per Node level.","[{'comment': '```suggestion\r\n   * long, DriverExecutionProfile, Node, String)} but at per Node level.\r\n```', 'commenter': 'olim7t'}]"
1264,core/src/main/java/com/datastax/oss/driver/api/core/tracker/RequestTracker.java,"@@ -70,14 +115,38 @@ default void onError(
    *     GenericType) session.execute} call until the error is propagated to the client).
    * @param executionProfile the execution profile of this request.
    * @param node the node that returned the error response.
+   * @deprecated override {@link #onNodeError(Request, Throwable, long, DriverExecutionProfile,
+   *     Node, String)} instead.
    */
+  @Deprecated
   default void onNodeError(
       @NonNull Request request,
       @NonNull Throwable error,
       long latencyNanos,
       @NonNull DriverExecutionProfile executionProfile,
       @NonNull Node node) {}
 
+  /**
+   * Invoked each time a request fails at the node level. Similar to {@link #onError(Request,
+   * Throwable, long, DriverExecutionProfile, Node)} but at a per node level.","[{'comment': '```suggestion\r\n   * Throwable, long, DriverExecutionProfile, Node, String)} but at a per node level.\r\n```', 'commenter': 'olim7t'}]"
1264,manual/core/request_tracker/README.md,"@@ -77,20 +77,26 @@ datastax-java-driver.advanced.request-tracker {
 All requests are logged under the category
 `com.datastax.oss.driver.internal.core.tracker.RequestLogger`.
 
+Prefix of the log will always contain at least: ","[{'comment': '```suggestion\r\nThe prefix of the log will always contain at least: \r\n```', 'commenter': 'olim7t'}]"
1264,manual/core/request_tracker/README.md,"@@ -77,20 +77,26 @@ datastax-java-driver.advanced.request-tracker {
 All requests are logged under the category
 `com.datastax.oss.driver.internal.core.tracker.RequestLogger`.
 
+Prefix of the log will always contain at least: 
+
+``s0|274426173``
+The `s0` is a session prefix. The `274426173` is a unique hash code calculated per request.","[{'comment': ""To expand a little:\r\n```\r\n`s0` is the session name (see the `basic.session-name` configuration option). `274426173` is a unique\r\nhash code calculated per request, that can be used for correlation with the driver's debug and trace\r\nlogs.\r\n```"", 'commenter': 'olim7t'}]"
1264,manual/core/request_tracker/README.md,"@@ -77,20 +77,26 @@ datastax-java-driver.advanced.request-tracker {
 All requests are logged under the category
 `com.datastax.oss.driver.internal.core.tracker.RequestLogger`.
 
+Prefix of the log will always contain at least: 
+
+``s0|274426173``","[{'comment': 'Formatting is a bit off, the next sentence starts on the same line ([github rendering](https://github.com/datastax/java-driver/tree/java2278/manual/core/request_tracker)).\r\n\r\nIf you want this to appear as a code block, use triple backticks:\r\n\r\n    ```\r\n    s0|274426173\r\n    ```\r\n', 'commenter': 'olim7t'}]"
1267,mapper-runtime/src/main/java/com/datastax/oss/driver/internal/mapper/DaoBase.java,"@@ -270,4 +274,19 @@ protected static void throwIfProtocolVersionV3(MapperContext context) {
               NullSavingStrategy.class.getSimpleName(), NullSavingStrategy.DO_NOT_SET.name()));
     }
   }
+
+  protected static <EntityT> void throwIfKeyspaceMissing(","[{'comment': ""The jira ticket called for an error message like:\r\n\r\n```\r\nMissing keyspace for userDao(). Suggestions: use SessionBuilder.withKeyspace() when creating your session, specify a default keyspace on User with @Entity(defaultKeyspace), or use a DAO factory method with an @DaoKeyspace parameter\r\n```\r\n\r\nI was almost able to match this 1:1 except for providing the dao method name.  Fortunately that shows up in the stack trace.  I think we could probably add some context to resolve it, but wasn't sure how important that was."", 'commenter': 'tolbertam'}, {'comment': 'ðŸ‘ if it appears in the stack trace that should be clear enough.', 'commenter': 'olim7t'}]"
1267,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoImplementationGenerator.java,"@@ -417,6 +417,7 @@ protected ClassName getPrincipalTypeName() {
       String fieldName = entry.getValue();
       // - create an instance
       initAsyncBuilder.addStatement(""$1T $2L = new $1T(context)"", fieldTypeName, fieldName);
+      initAsyncBuilder.addStatement(""throwIfKeyspaceMissing($L, context.getSession())"", fieldName);","[{'comment': ""One thought is that in theory, a user could only specify Dao methods that don't require a keyspace, like `@GetEntity` and `@SetEntity`, we could choose not to fail in this case, but not sure if it's worth worrying about?"", 'commenter': 'tolbertam'}, {'comment': 'We could move the check to each entity helper method, we have the context there so I think we have everything we need. Another advantage of that is that the check would also kick in when you call a helper method from a query provider.\r\n\r\nWe can introduce an `EntityHelperBase` parent class to factor the method.', 'commenter': 'olim7t'}, {'comment': ""yeah this seems easy enough to do.  I had initially went about it this way, but seemed like validating in each of these methods was less simple than validating it on entity helper creation. I'll change it."", 'commenter': 'tolbertam'}, {'comment': 'Adding `EntityHelperBase` also eliminates some boilerplate in the generated code, so that looks like a good change overall.', 'commenter': 'tolbertam'}]"
1267,mapper-runtime/src/main/java/com/datastax/oss/driver/internal/mapper/DaoBase.java,"@@ -285,4 +289,19 @@ protected static void throwIfProtocolVersionV3(MapperContext context) {
               NullSavingStrategy.class.getSimpleName(), NullSavingStrategy.DO_NOT_SET.name()));
     }
   }
+
+  protected static <EntityT> void throwIfKeyspaceMissing(
+      EntityHelper<EntityT> helper, Session session) {
+    if (helper.getKeyspaceId() == null && !session.getKeyspace().isPresent()) {
+      throw new MapperException(
+          String.format(
+              ""Missing keyspace. Suggestions: use SessionBuilder.withKeyspace() ""
+                  + ""when creating your session, specify a default keyspace on %s with @%s""
+                  + ""(defaultKeyspace), or use a @%s method with a @%s parameter"",
+              helper.getEntityClass(),","[{'comment': ""In similar messages (see `replaceKeyspaceAndTablePlaceholders`), I've used the entity's simple name. My reasoning was that it would be pretty obvious from the context which class it is.\r\nI don't mind either way but we should be consistent, if you think it's worth having the package we'll modify the other message."", 'commenter': 'olim7t'}, {'comment': 'Fixed that to use `getSimpleName()` :+1:', 'commenter': 'tolbertam'}]"
1267,mapper-runtime/src/main/java/com/datastax/oss/driver/internal/mapper/entity/EntityHelperBase.java,"@@ -0,0 +1,75 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.mapper.entity;
+
+import com.datastax.oss.driver.api.core.CqlIdentifier;
+import com.datastax.oss.driver.api.mapper.MapperContext;
+import com.datastax.oss.driver.api.mapper.MapperException;
+import com.datastax.oss.driver.api.mapper.annotations.DaoFactory;
+import com.datastax.oss.driver.api.mapper.annotations.DaoKeyspace;
+import com.datastax.oss.driver.api.mapper.annotations.Entity;
+import com.datastax.oss.driver.api.mapper.entity.EntityHelper;
+
+public abstract class EntityHelperBase<EntityT> implements EntityHelper<EntityT> {
+
+  protected final CqlIdentifier keyspaceId;
+
+  protected final CqlIdentifier tableId;
+
+  protected final MapperContext context;
+
+  protected EntityHelperBase(MapperContext context, String tableName) {
+    this(context, null, tableName);
+  }
+
+  protected EntityHelperBase(MapperContext context, String keyspaceName, String tableName) {
+    this.context = context;
+    this.tableId =
+        context.getTableId() != null ? context.getTableId() : CqlIdentifier.fromCql(tableName);
+    if (keyspaceName == null) {
+      this.keyspaceId = context.getKeyspaceId();
+    } else {
+      this.keyspaceId =
+          context.getKeyspaceId() != null
+              ? context.getKeyspaceId()
+              : CqlIdentifier.fromCql(keyspaceName);
+    }
+  }
+
+  @Override
+  public CqlIdentifier getKeyspaceId() {
+    return keyspaceId;
+  }
+
+  @Override
+  public CqlIdentifier getTableId() {","[{'comment': 'Missing `@NonNull` annotation.', 'commenter': 'olim7t'}]"
1267,mapper-runtime/src/main/java/com/datastax/oss/driver/internal/mapper/entity/EntityHelperBase.java,"@@ -0,0 +1,75 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.mapper.entity;
+
+import com.datastax.oss.driver.api.core.CqlIdentifier;
+import com.datastax.oss.driver.api.mapper.MapperContext;
+import com.datastax.oss.driver.api.mapper.MapperException;
+import com.datastax.oss.driver.api.mapper.annotations.DaoFactory;
+import com.datastax.oss.driver.api.mapper.annotations.DaoKeyspace;
+import com.datastax.oss.driver.api.mapper.annotations.Entity;
+import com.datastax.oss.driver.api.mapper.entity.EntityHelper;
+
+public abstract class EntityHelperBase<EntityT> implements EntityHelper<EntityT> {
+
+  protected final CqlIdentifier keyspaceId;
+
+  protected final CqlIdentifier tableId;
+
+  protected final MapperContext context;
+
+  protected EntityHelperBase(MapperContext context, String tableName) {
+    this(context, null, tableName);
+  }
+
+  protected EntityHelperBase(MapperContext context, String keyspaceName, String tableName) {","[{'comment': 'Maybe name those parameters `defaultKeyspaceName` and `defaultTableName`.', 'commenter': 'olim7t'}]"
1267,mapper-runtime/src/main/java/com/datastax/oss/driver/internal/mapper/entity/EntityHelperBase.java,"@@ -0,0 +1,75 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.mapper.entity;
+
+import com.datastax.oss.driver.api.core.CqlIdentifier;
+import com.datastax.oss.driver.api.mapper.MapperContext;
+import com.datastax.oss.driver.api.mapper.MapperException;
+import com.datastax.oss.driver.api.mapper.annotations.DaoFactory;
+import com.datastax.oss.driver.api.mapper.annotations.DaoKeyspace;
+import com.datastax.oss.driver.api.mapper.annotations.Entity;
+import com.datastax.oss.driver.api.mapper.entity.EntityHelper;
+
+public abstract class EntityHelperBase<EntityT> implements EntityHelper<EntityT> {
+
+  protected final CqlIdentifier keyspaceId;
+
+  protected final CqlIdentifier tableId;
+
+  protected final MapperContext context;
+
+  protected EntityHelperBase(MapperContext context, String tableName) {
+    this(context, null, tableName);
+  }
+
+  protected EntityHelperBase(MapperContext context, String keyspaceName, String tableName) {
+    this.context = context;
+    this.tableId =
+        context.getTableId() != null ? context.getTableId() : CqlIdentifier.fromCql(tableName);
+    if (keyspaceName == null) {
+      this.keyspaceId = context.getKeyspaceId();
+    } else {
+      this.keyspaceId =
+          context.getKeyspaceId() != null
+              ? context.getKeyspaceId()
+              : CqlIdentifier.fromCql(keyspaceName);
+    }","[{'comment': ""I'm generally not a big fan of chained ternary operators, but in this case this might be a bit easier to follow:\r\n```java\r\n    this.keyspaceId =\r\n        context.getKeyspaceId() != null\r\n            ? context.getKeyspaceId()\r\n            : (keyspaceName == null ? null : CqlIdentifier.fromCql(keyspaceName));\r\n```"", 'commenter': 'olim7t'}]"
1271,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoMethodGenerator.java,"@@ -198,4 +200,55 @@ protected VariableElement findBoundStatementFunction(ExecutableElement methodEle
     }
     return null;
   }
+
+  protected boolean validateCqlNamesPresent(List<? extends VariableElement> parameters) {
+    boolean valid = true;
+    if (isFromClassFile()) {
+      for (VariableElement parameter : parameters) {
+        CqlName cqlName = parameter.getAnnotation(CqlName.class);
+        if (cqlName == null) {
+          context
+              .getMessager()
+              .error(
+                  parameter,
+                  ""Method %s: parameter %s is declared in a compiled method ""
+                      + ""and refers to a bind marker ""
+                      + ""and thus must be annotated with @CqlName"",
+                  methodElement,
+                  parameter.getSimpleName());
+          valid = false;
+        }
+      }
+    }
+    return valid;
+  }
+
+  protected boolean validateCqlNamesAbsent(List<? extends VariableElement> parameters) {
+    boolean valid = true;
+    for (VariableElement parameter : parameters) {
+      CqlName cqlName = parameter.getAnnotation(CqlName.class);
+      if (cqlName != null) {
+        context
+            .getMessager()
+            .error(
+                parameter,
+                ""Method %s: parameter %s does not refer to a bind marker ""
+                    + ""and must not be annotated with @CqlName"",
+                methodElement,
+                parameter.getSimpleName());","[{'comment': 'This is not really blocking, I think we could just warn that the name will be ignored.', 'commenter': 'olim7t'}]"
1271,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/util/generation/GeneratedCodePatterns.java,"@@ -125,19 +128,31 @@ public static void addFinalFieldAndConstructorArgument(
   }
 
   /**
-   * Treats a list of method parameters as bind variables in a query, assuming the bing markers have
-   * the same names as the parameters.
+   * Treats a list of method parameters as bind variables in a query, assuming that the bind markers
+   * have the same names as the parameters, unless they are annotated with {@link CqlName}.
    *
    * <p>The generated code assumes that a {@code BoundStatementBuilder boundStatementBuilder} local
    * variable already exists.
    */
   public static void bindParameters(
-      List<? extends VariableElement> parameters,
+      @NonNull List<? extends VariableElement> parameters,
       CodeBlock.Builder methodBuilder,
       BindableHandlingSharedCode enclosingClass,
       ProcessorContext context,
       boolean useNullSavingStrategy) {
-    bindParameters(parameters, null, methodBuilder, enclosingClass, context, useNullSavingStrategy);
+    List<CodeBlock> bindMarkerNames = new ArrayList<>();
+    for (VariableElement parameter : parameters) {
+      CqlName cqlName = parameter.getAnnotation(CqlName.class);
+      String parameterName;
+      if (cqlName == null) {
+        parameterName = parameter.getSimpleName().toString();
+      } else {
+        parameterName = cqlName.value();
+      }
+      bindMarkerNames.add(CodeBlock.of(""$S"", parameterName));
+    }
+    bindParameters(
+        parameters, bindMarkerNames, methodBuilder, enclosingClass, context, useNullSavingStrategy);","[{'comment': 'Nice way to handle this ðŸ‘ \r\nI like how this simplifies the other method.', 'commenter': 'olim7t'}]"
1271,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoMethodGenerator.java,"@@ -198,4 +200,55 @@ protected VariableElement findBoundStatementFunction(ExecutableElement methodEle
     }
     return null;
   }
+
+  protected boolean validateCqlNamesPresent(List<? extends VariableElement> parameters) {
+    boolean valid = true;
+    if (isFromClassFile()) {
+      for (VariableElement parameter : parameters) {
+        CqlName cqlName = parameter.getAnnotation(CqlName.class);
+        if (cqlName == null) {
+          context
+              .getMessager()
+              .error(
+                  parameter,
+                  ""Method %s: parameter %s is declared in a compiled method ""
+                      + ""and refers to a bind marker ""
+                      + ""and thus must be annotated with @CqlName"",","[{'comment': ""I think we should also include the name of the child interface, but this can wait for [JAVA-2302](https://datastax-oss.atlassian.net/browse/JAVA-2302) (currently we don't have a reference to `DaoImplementationGenerator.interfaceElement` in this class, and that's one of the things that ticket will address)."", 'commenter': 'olim7t'}]"
1271,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoInsertMethodGenerator.java,"@@ -120,27 +120,33 @@ public DaoInsertMethodGenerator(
 
     populateBuilderWithStatementAttributes(methodBodyBuilder, methodElement);
     populateBuilderWithFunction(methodBodyBuilder, boundStatementFunction);
-    String entityParameterName = parameters.get(0).getSimpleName().toString();
 
-    NullSavingStrategy nullSavingStrategy =
-        nullSavingStrategyValidation.getNullSavingStrategy(
-            Insert.class, Insert::nullSavingStrategy, methodElement, enclosingClass);
+    if (validateCqlNamesAbsent(parameters.subList(0, 1))) {","[{'comment': ""Side note: this doesn't validate the optional `Function<BoundStatementBuilder, BoundStatementBuilder>` argument from [JAVA-2139](https://datastax-oss.atlassian.net/browse/JAVA-2139) because it's already been removed from the list at this point. But that's not a big deal, the annotation will be ignored silently if present."", 'commenter': 'olim7t'}]"
1271,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoDeleteMethodGenerator.java,"@@ -210,12 +210,14 @@ public DaoDeleteMethodGenerator(
                     + ""parameters if they specify a custom IF clause"",
                 Delete.class.getSimpleName());
       }
-      GeneratedCodePatterns.bindParameters(
-          parameters.subList(nextParameterIndex, parameters.size()),
-          methodBodyBuilder,
-          enclosingClass,
-          context,
-          false);
+      List<? extends VariableElement> bindMarkers =
+          parameters.subList(nextParameterIndex, parameters.size());
+      if (validateCqlNamesPresent(bindMarkers)) {","[{'comment': 'We should also validate the case when a full entity is provided (previous branch).', 'commenter': 'olim7t'}]"
1281,core/src/main/java/com/datastax/oss/driver/internal/core/auth/RuntimePlainTextAuthProvider.java,"@@ -0,0 +1,54 @@
+/*
+ * Copyright DataStax, Inc.","[{'comment': ""This Could also be folded into the normal PlainTextAuthProvider if appropriate. That's what I did initially, but with the introduction of the PlainTextAuthProviderBase, it seemed appropriate to have a separate class."", 'commenter': 'GregBestland'}]"
1281,core/revapi.json,"@@ -4756,6 +4756,12 @@
         ""new"": ""method java.util.UUID com.datastax.oss.driver.api.core.metadata.Node::getHostId()"",
         ""annotation"": ""@edu.umd.cs.findbugs.annotations.Nullable"",
         ""justification"": ""Node.getHostId() should have been annotated with @NonNull""
+      },
+      {
+        ""code"": ""java.method.numberOfParametersChanged"",
+        ""old"": ""method com.datastax.oss.driver.api.core.context.DriverContext com.datastax.oss.driver.api.core.session.SessionBuilder<SelfT extends com.datastax.oss.driver.api.core.session.SessionBuilder, SessionT>::buildContext(com.datastax.oss.driver.api.core.config.DriverConfigLoader, java.util.List<com.datastax.oss.driver.api.core.type.codec.TypeCodec<?>>, com.datastax.oss.driver.api.core.metadata.NodeStateListener, com.datastax.oss.driver.api.core.metadata.schema.SchemaChangeListener, com.datastax.oss.driver.api.core.tracker.RequestTracker, java.util.Map<java.lang.String, java.lang.String>, java.util.Map<java.lang.String, java.util.function.Predicate<com.datastax.oss.driver.api.core.metadata.Node>>, java.lang.ClassLoader)"",
+        ""new"": ""method com.datastax.oss.driver.api.core.context.DriverContext com.datastax.oss.driver.api.core.session.SessionBuilder<SelfT extends com.datastax.oss.driver.api.core.session.SessionBuilder, SessionT>::buildContext(com.datastax.oss.driver.api.core.config.DriverConfigLoader, java.util.List<com.datastax.oss.driver.api.core.type.codec.TypeCodec<?>>, com.datastax.oss.driver.api.core.metadata.NodeStateListener, com.datastax.oss.driver.api.core.metadata.schema.SchemaChangeListener, com.datastax.oss.driver.api.core.tracker.RequestTracker, com.datastax.oss.driver.api.core.auth.AuthProvider, java.util.Map<java.lang.String, java.lang.String>, java.util.Map<java.lang.String, java.util.function.Predicate<com.datastax.oss.driver.api.core.metadata.Node>>, java.lang.ClassLoader)"",
+        ""justification"": ""Allow  for AuthProvider to be loaded programmatically""","[{'comment': ""This method breaks every time we add a new programmatic builder option, and I think we're at a point where we should avoid that. I've opened [JAVA-2315](https://datastax-oss.atlassian.net/browse/JAVA-2315) to address this."", 'commenter': 'olim7t'}]"
1281,core/src/main/java/com/datastax/oss/driver/api/core/session/SessionBuilder.java,"@@ -223,6 +225,18 @@ public SelfT withRequestTracker(@Nullable RequestTracker requestTracker) {
     return self;
   }
 
+  /**
+   * Register an auth provider
+   *
+   * <p>If the auth provider is specified programmatically with this method, it overrides the
+   * configuration (that is, the {@code advanced.auth-provider} option will be ignored).
+   */
+  @NonNull
+  public SelfT withAuth(@Nullable AuthProvider authProvider) {","[{'comment': 'Nit: could you name this `withAuthProvider`?', 'commenter': 'olim7t'}, {'comment': ""Maybe we could also add `withPlainTextCredentials(String username, String password)` that wraps the creation and registration of a `RuntimePlainTextAuthProvider`. That way the user doesn't have to deal with an internal class.\r\n\r\nIt would come with the following caveats:\r\n* only works with OSS Cassandra's plain text authenticator\r\n* does not support changing the credentials at runtime\r\n* stores the credentials as strings in memory"", 'commenter': 'olim7t'}]"
1281,core/src/main/java/com/datastax/oss/driver/internal/core/auth/RuntimePlainTextAuthProvider.java,"@@ -0,0 +1,54 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.auth;
+
+import edu.umd.cs.findbugs.annotations.NonNull;
+import net.jcip.annotations.ThreadSafe;
+
+/**
+ * A simple authentication provider that supports SASL authentication using the PLAIN mechanism for
+ * version 3 (or above) of the CQL native protocol.
+ *
+ * <p>To activate this provider, you will need to define it at runtime, and pass it into the
+ * SessionBuilder during session creation.
+ *
+ * <pre>
+ *     SessionBuilder builder =
+ *         SessionUtils.baseBuilder()
+ *             .withAuth(new RuntimePlainTextAuthProvider(""logPrefix"", ""username"", ""password""));
+ * </pre>
+ */
+@ThreadSafe
+public class RuntimePlainTextAuthProvider extends PlainTextAuthProviderBase {
+  private final String username;","[{'comment': ""Let's apply JAVA-2306 here and avoid storing sensitive data as strings. I suggest that you create a `Credentials` instance upfront and store it here instead of these two fields."", 'commenter': 'adutra'}, {'comment': "":+1: that's a good idea."", 'commenter': 'GregBestland'}, {'comment': ""So while this seemed like a good idea, it won't actually work. The credentials are cleared when used, and the auth provider get credentials is invoked a few times during session creation. As a result the credentials end up cleared as part of protocol negotiation. In order to accomplish this we would need a way to encrypt the user name password in memory, and recreate the credentials when needed."", 'commenter': 'GregBestland'}, {'comment': 'Then you could implement a `Credentials.copy()` method as follows:\r\n\r\n```\r\npublic Credentials copy() {\r\n  return new Credentials(username.clone(), password.clone());\r\n}\r\n```\r\nAnd use it as below:\r\n```\r\npublic class RuntimePlainTextAuthProvider extends PlainTextAuthProviderBase {\r\n  private final Credentials credentials;\r\n\r\n  public RuntimePlainTextAuthProvider(String username, String password) {\r\n    this("""", username, password);\r\n  }\r\n\r\n  public RuntimePlainTextAuthProvider(String logPrefix, String username, String password) {\r\n    super(logPrefix);\r\n    credentials = new Credentials(username.toCharArray(), password.toCharArray());\r\n  }\r\n\r\n  @NonNull\r\n  @Override\r\n  protected Credentials getCredentials() {\r\n    return credentials.copy();\r\n  }\r\n}\r\n```', 'commenter': 'adutra'}, {'comment': ""But then we're still holding the credentials in memory, because the original field is never getting cleared.\r\nI don't think there's a way to avoid it when the credentials have been provided programmatically (unless we encrypted them, but that's way off topic). I'll add a few explanations in the javadocs."", 'commenter': 'olim7t'}]"
1285,mapper-runtime/src/main/java/com/datastax/oss/driver/api/mapper/entity/EntityHelper.java,"@@ -188,6 +188,23 @@
   @NonNull
   Select selectByPrimaryKey();
 
+  /**
+   * Similar to {@link #selectByPrimaryKey()} but may operate over a subset of primary key
+   * components (partition key + clustering columns) to fetch an entity or entities.
+   *
+   * <p><code>parameterCount</code> components are listed in the {@code WHERE} clause as bindable
+   * values (the bind markers have the same names as the columns). They are listed in the natural
+   * order, i.e. partition key columns first, followed by clustering columns (in the order defined
+   * by the {@link PartitionKey} and {@link ClusteringColumn} annotations on the entity class).
+   *
+   * <p>If <code>parameterCount</code> of 0 is provided, this is the equivalent of a select query
+   * without a where clause, which queries for all rows in a table.
+   */
+  @NonNull
+  default Select selectByPrimaryKeyParts(int parameterCount) {
+    throw new UnsupportedOperationException(""selectbyPrimaryKeyParts is unimplemented"");","[{'comment': 'Done to avoid breaking changes, although it is a bit awkward.', 'commenter': 'tolbertam'}, {'comment': '> Done to avoid breaking changes, although it is a bit awkward.\r\n\r\nYes, and this only hides the breaking change by turning it into a runtime error. I\'ve been thinking about other possible approaches:\r\n\r\n1. introduce a sub-interface and require a cast (as we\'ve done in other cases). The problem is that this doesn\'t scale well: we add `ExtendedProductHelper` now but what if we need more methods in the next version? `ExtendedExtendedProductHelper`?\r\n2. Accept the breaking change and document it, on the assumption that clients probably don\'t implement this interface. The problem is in the ""probably""; also, it\'s a slippery slope.\r\n3. Don\'t add those methods to `EntityHelper`. Instead, generate the query directly from the method generators. For SELECT we already have `selectStart()`; for DELETE we have to duplicate the whole method, but it\'s not huge either.\r\n\r\n    We won\'t make those queries available to users through the interface (e.g. for custom query providers), but they\'re not that hard to rewrite from scratch anyway.\r\n\r\nMy vote goes to 3.', 'commenter': 'olim7t'}, {'comment': ""> Don't add those methods to EntityHelper\r\n\r\nYou know, it didn't occur to me that generated code used the implementations instead of declaring things as `EntityHelper`.  In this case, there is no reason to add something to the interface, so I agree this seems like a good course of action.\r\n\r\nIt looks like the only place (currently) that `EntityHelper` is used as an interface in other code is in `QueryProvider` and in `DaoBase`, where it's more used for setting/getting fields.\r\n\r\nSince this method is used for query generation, I suppose it's probably fine to not put directly in `EntityHelper`.\r\n\r\nBut I do wonder if we should just consider removing all the query-generation based methods from `EntityHelper` now if we see no utility.  What do you think?"", 'commenter': 'tolbertam'}, {'comment': '> You know, it didn\'t occur to me that generated code used the implementations instead of declaring things as `EntityHelper`.\r\n\r\nIt didn\'t occur to me either ðŸ˜† I was initially thinking of moving the code to the DAO implementation class, but keeping them as ""internal"" methods in the helper class is even better.\r\n\r\n> removing all the query-generation based methods from `EntityHelper`\r\n\r\nAs we discussed privately, those methods can be useful for `@QueryProvider` implementors, so it\'s nice to have them on the interface.', 'commenter': 'olim7t'}]"
1285,changelog/README.md,"@@ -2,8 +2,10 @@
 
 <!-- Note: contrary to 3.x, insert new entries *first* in their section -->
 
+","[{'comment': 'Unnecessary change.', 'commenter': 'adutra'}]"
1285,changelog/README.md,"@@ -2,8 +2,10 @@
 
 <!-- Note: contrary to 3.x, insert new entries *first* in their section -->
 
+
 ### 4.2.0 (in progress)
 
+- [improvement] JAVA-2307: Improve @Select and @Delete by not requiring full primary key","[{'comment': 'Tip: place all words starting with `@` within backticks here and in commit messages, to avoid having Github consider that these are user handles.', 'commenter': 'adutra'}, {'comment': ""Thanks, I'll amend the commit when merging. It looks like github doesn't hijack markdown in the same way, but agree that we shoulds use backticks here as well."", 'commenter': 'tolbertam'}]"
1285,integration-tests/src/test/java/com/datastax/oss/driver/mapper/InventoryITBase.java,"@@ -31,14 +33,31 @@
       new Product(UUID.randomUUID(), ""Flamethrower"", new InventoryITBase.Dimensions(30, 10, 8));
   protected static Product MP3_DOWNLOAD = new Product(UUID.randomUUID(), ""MP3 download"", null);
 
+  protected static ProductSale FLAMETHROWER_SALE_1 =
+      new ProductSale(FLAMETHROWER.getId(), ""2019-06-27"", 1, Uuids.startOf(1561643130), 500.00, 5);","[{'comment': 'Maybe you could have the date in the constant name, to make things more obvious in tests.', 'commenter': 'olim7t'}]"
1285,manual/mapper/daos/delete/README.md,"@@ -36,6 +36,19 @@ The method can operate on:
     In addition, because the entity class can't be inferred from the method signature, it must be
     specified via the annotation's `entityClass` element.
 
+* a subset of the primary key (partition key, or partition key + subset of clustering columns):","[{'comment': ""It's always the first k clustering columns, correct? Like if the PK is `((pk1, pk2), cc1, cc2, cc3)` you can't do `((pk1, pk2), cc3)`. We should say it explicitly in the docs (I'm not sure how best to formulate it)."", 'commenter': 'olim7t'}, {'comment': ""That's right.  Although if `cc1`, `cc2` and `cc3` have the same types technically the code would still generate, but it's not what user wants.  I'll document this."", 'commenter': 'tolbertam'}]"
1285,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoDeleteMethodGenerator.java,"@@ -105,8 +105,10 @@ public DaoDeleteMethodGenerator(
     VariableElement firstParameter = parameters.get(0);
     entityElement = EntityUtils.asEntityElement(firstParameter, typeParameters);
     hasEntityParameter = (entityElement != null);
+    final int keyParameterSize;","[{'comment': ""I think we can find a better name for this. It's the number of PK columns that we want in the generated query."", 'commenter': 'olim7t'}]"
1285,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoDeleteMethodGenerator.java,"@@ -120,25 +122,39 @@ public DaoDeleteMethodGenerator(
         return Optional.empty();
       }
       entityDefinition = context.getEntityFactory().getDefinition(entityElement);
-      List<TypeName> primaryKeyTypes =
-          entityDefinition.getPrimaryKey().stream()
-              .map(d -> d.getType().asTypeName())
-              .collect(Collectors.toList());
-      List<TypeName> parameterTypes =
-          parameters.stream().map(p -> TypeName.get(p.asType())).collect(Collectors.toList());
-      // Note that we only check the types: we allow the names to be different.
-      if (parameterTypes.size() < primaryKeyTypes.size()
-          || !primaryKeyTypes.equals(parameterTypes.subList(0, primaryKeyTypes.size()))) {
-        context
-            .getMessager()
-            .error(
-                methodElement,
-                ""Invalid parameter list: %s methods that do not operate on an entity instance ""
-                    + ""must take the partition key components in the exact order ""
-                    + ""(expected PK of %s: %s)"",
-                Delete.class.getSimpleName(),
-                entityElement.getSimpleName(),
-                primaryKeyTypes);
+      // if a custom if clause is provided, the whole primary key must also be provided.
+      List<? extends VariableElement> keyParameters = parameters;
+      if (!annotation.customIfClause().isEmpty()) {
+        if (keyParameters.size() < entityDefinition.getPrimaryKey().size()) {","[{'comment': ""This gave me pause because the check is not accurate, we don't know how many placeholders the custom IF clause has.\r\nBut it's more of a guard so that we can perform the `subList` call in the other branch; if this check succeeds it's indeed a definitive mistake (even though there could still be something wrong if the check fails, but that will be caught later).\r\nSo ðŸ‘ "", 'commenter': 'olim7t'}, {'comment': ""That's right.  It's not legal to not provide the full primary key and use a custom if clause.\r\n\r\nSo there's possibly the case where enough parameters are provided to fulfill the number of primary key parameters, but some of those parameters were meant for the custom if clause.  In this case it's likely it won't match the primary key definition and you'll get an error which explains that he parameters don't match the primary key, which is technically correct.  If they do somehow match, there's not much we can do at compile time (I think?).  In any case, you'll get an error at run time, which is similar to what would happen if you don't provide enough parameters to match the query bind markers."", 'commenter': 'tolbertam'}]"
1285,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/EntityUtils.java,"@@ -81,4 +88,86 @@ public static TypeElement asEntityElement(
     }
     return typeElement;
   }
+
+  /**
+   * Validates that the given parameters are valid for an {@link EntityDefinition}, meaning that
+   * there are at least enough parameters provided to match the number of partition key columns and
+   * that that parameter types match the primary key types.","[{'comment': '```suggestion\r\n   * that parameter types match the primary key types.\r\n```', 'commenter': 'olim7t'}]"
1285,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoDeleteMethodGenerator.java,"@@ -187,7 +204,7 @@ public DaoDeleteMethodGenerator(
           entityDefinition.getPrimaryKey().stream()
               .map(PropertyDefinition::getCqlName)
               .collect(Collectors.toList());
-      List<? extends VariableElement> bindMarkers = parameters.subList(0, primaryKeyNames.size());
+      List<? extends VariableElement> bindMarkers = parameters.subList(0, keyParameterSize);","[{'comment': 'I think this assertion in `GeneratedCodePatterns.bindParameters` is going to fail with this change:\r\n```java\r\nassert bindMarkerNames.size() == parameters.size();\r\n```\r\nThe cleanest fix is to sublist `primaryKeyNames` as well.', 'commenter': 'olim7t'}, {'comment': 'good catch :+1:', 'commenter': 'tolbertam'}]"
1285,manual/mapper/daos/delete/README.md,"@@ -36,6 +36,28 @@ The method can operate on:
     In addition, because the entity class can't be inferred from the method signature, it must be
     specified via the annotation's `entityClass` element.
 
+* a subset of the primary key.  As in the partition key, or partition key + subset of clustering 
+  columns:
+
+    ```java
+    // given: PRIMARY KEY ((product_id, day), customer_id, ts)
+    // delete all rows in partition
+    @Delete(entityClass = ProductSale.class)
+    void deleteByIdForDay(UUID productId, LocalDate day);
+
+    // delete by partition key and partial clustering key
+    @Delete(entityClass = ProductSale.class)
+    void deleteByIdForCustomer(UUID productId, LocalDate day, UUID customerId);
+   
+    /* Note that the clustering columns in your primary key definition are significant. All
+     * proceeding clustering columns must be provided if any are.","[{'comment': '> proceeding clustering columns\r\n\r\nDid you mean ""preceding""?', 'commenter': 'olim7t'}, {'comment': "":man_facepalming: yep, that's what I meant.  Good catch, i'll fix."", 'commenter': 'tolbertam'}]"
1300,mapper-runtime/src/main/java/com/datastax/oss/driver/internal/mapper/DefaultMapperContext.java,"@@ -107,6 +130,11 @@ public NameConverter getNameConverter(Class<? extends NameConverter> converterCl
     return customState;
   }
 
+  @Override
+  public boolean isSchemaValidationEnabled() {","[{'comment': ""@olim7t  according to your suggestion:\r\n```\r\nenabling schema validation through an annotation is a bit too static. It would probably be more flexible with a runtime parameter:\r\nnew InventoryMapperBuilder(session).setSchemaValidation(true).build()\r\n```\r\nI've moved the check into Builder. I've created a new field `schemaValidationEnabled`  but it obviously breaks the backward compatibility. I can put that schemaValidatEnabled field into `customState` and propagate it there. Let me know wdyt.\r\nAlso, I can set the default to be `true` or `false` if we want to have that behavior"", 'commenter': 'tomekl007'}, {'comment': 'solved by using `customState`', 'commenter': 'tomekl007'}]"
1300,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/entity/EntityHelperSchemaValidationMethodGenerator.java,"@@ -0,0 +1,261 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.mapper.processor.entity;
+
+import static com.datastax.oss.driver.api.mapper.annotations.SchemaHint.*;
+
+import com.datastax.oss.driver.api.core.CqlIdentifier;
+import com.datastax.oss.driver.api.core.metadata.schema.TableMetadata;
+import com.datastax.oss.driver.api.core.type.UserDefinedType;
+import com.datastax.oss.driver.api.mapper.annotations.SchemaHint;
+import com.datastax.oss.driver.internal.core.metadata.schema.DefaultTableMetadata;
+import com.datastax.oss.driver.internal.mapper.processor.MethodGenerator;
+import com.squareup.javapoet.CodeBlock;
+import com.squareup.javapoet.MethodSpec;
+import com.squareup.javapoet.TypeName;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Optional;
+import java.util.stream.Collectors;
+import javax.lang.model.element.Modifier;
+import javax.lang.model.element.TypeElement;
+
+public class EntityHelperSchemaValidationMethodGenerator implements MethodGenerator {
+
+  private final EntityDefinition entityDefinition;
+  private TypeElement entityTypeElement;
+
+  public EntityHelperSchemaValidationMethodGenerator(
+      EntityDefinition entityDefinition, TypeElement entityTypeElement) {
+    this.entityDefinition = entityDefinition;
+    this.entityTypeElement = entityTypeElement;
+  }
+
+  @Override
+  public Optional<MethodSpec> generate() {
+    MethodSpec.Builder methodBuilder =
+        MethodSpec.methodBuilder(""validateEntityFields"")
+            .addAnnotation(Override.class)
+            .addModifiers(Modifier.PUBLIC)
+            .returns(TypeName.VOID);
+
+    // get keyspaceId from context, and if not present fallback to keyspace set on session
+    methodBuilder.addStatement(
+        ""$1T keyspaceId = context.getKeyspaceId() != null ? context.getKeyspaceId() : context.getSession().getKeyspace().orElse(null)"",
+        CqlIdentifier.class);
+
+    // Handle case where keyspaceId = null. In such case we cannot infer and validate schema for
+    // table
+    // or udt
+    methodBuilder.beginControlFlow(
+        ""if (!context.getSession().getMetadata().getKeyspace(keyspaceId).isPresent())"");
+    methodBuilder.addStatement(
+        ""return""); // todo maybe we should do log.warn(""we cannot validate schema because keyspace","[{'comment': 'should we add `warn` for that case or fail-silently?', 'commenter': 'tomekl007'}, {'comment': ""I think we should warn: if `schemaValidationEnabled == true` and the validation can't run, we should let the user know."", 'commenter': 'olim7t'}]"
1300,mapper-runtime/src/main/java/com/datastax/oss/driver/api/mapper/MapperBuilder.java,"@@ -33,13 +33,17 @@
  */
 public abstract class MapperBuilder<MapperT> {
 
+  public static final String SCHEMA_VALIDATION_ENABLED_SETTING =
+      ""datastax.mapper.schemaValidationEnabled"";
   protected final CqlSession session;
   protected CqlIdentifier defaultKeyspaceId;
   protected Map<Object, Object> customState;
 
   protected MapperBuilder(CqlSession session) {
     this.session = session;
     this.customState = new HashMap<>();
+    // schema validation is enabled by default
+    customState.put(SCHEMA_VALIDATION_ENABLED_SETTING, true);","[{'comment': ""I would have used `false` as the default, but now that I think about it I can see pros and cons on both sides:\r\n* enabled by default: errors are reported sooner. But if users don't realize that they can disable it, the (possibly expensive?) checks will run every time a new DAO gets created.\r\n* disabled by default: more efficient by default. But errors are only reported at usage time, and might be less clear (stack trace involving generated mapper classes). Also they will be detected one at a time.\r\n\r\nIn the end I think I agree with enabled by default."", 'commenter': 'olim7t'}, {'comment': 'I agree - I think that the overhead of checking for every `Dao` maybe not so big because it is happening only one time at the compile time. ', 'commenter': 'tomekl007'}]"
1300,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/entity/EntityHelperSchemaValidationMethodGenerator.java,"@@ -0,0 +1,309 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.mapper.processor.entity;
+
+import static com.datastax.oss.driver.api.mapper.annotations.SchemaHint.*;
+
+import com.datastax.oss.driver.api.core.CqlIdentifier;
+import com.datastax.oss.driver.api.core.metadata.schema.TableMetadata;
+import com.datastax.oss.driver.api.core.type.UserDefinedType;
+import com.datastax.oss.driver.api.core.type.reflect.GenericType;
+import com.datastax.oss.driver.api.mapper.annotations.SchemaHint;
+import com.datastax.oss.driver.internal.core.metadata.schema.DefaultTableMetadata;
+import com.datastax.oss.driver.internal.mapper.processor.MethodGenerator;
+import com.datastax.oss.driver.internal.mapper.processor.dao.LoggingGenerator;
+import com.squareup.javapoet.CodeBlock;
+import com.squareup.javapoet.MethodSpec;
+import com.squareup.javapoet.TypeName;
+import java.util.ArrayList;
+import java.util.LinkedHashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Optional;
+import java.util.stream.Collectors;
+import javax.lang.model.element.Modifier;
+import javax.lang.model.element.TypeElement;
+
+public class EntityHelperSchemaValidationMethodGenerator implements MethodGenerator {
+
+  private final EntityDefinition entityDefinition;
+  private TypeElement entityTypeElement;
+  private LoggingGenerator loggingGenerator;
+  private EntityHelperGenerator entityHelperGenerator;
+
+  public EntityHelperSchemaValidationMethodGenerator(
+      EntityDefinition entityDefinition,
+      TypeElement entityTypeElement,
+      LoggingGenerator loggingGenerator,
+      EntityHelperGenerator entityHelperGenerator) {
+    this.entityDefinition = entityDefinition;
+    this.entityTypeElement = entityTypeElement;
+    this.loggingGenerator = loggingGenerator;
+    this.entityHelperGenerator = entityHelperGenerator;
+  }
+
+  @Override
+  public Optional<MethodSpec> generate() {
+    MethodSpec.Builder methodBuilder =
+        MethodSpec.methodBuilder(""validateEntityFields"")
+            .addAnnotation(Override.class)
+            .addModifiers(Modifier.PUBLIC)
+            .returns(TypeName.VOID);
+
+    // get keyspaceId from context, and if not present fallback to keyspace set on session
+    methodBuilder.addStatement(
+        ""$1T keyspaceId = context.getKeyspaceId() != null ? context.getKeyspaceId() : context.getSession().getKeyspace().orElse(null)"",
+        CqlIdentifier.class);
+
+    methodBuilder.addStatement(""String entityClassName = $S"", entityDefinition.getClassName());
+
+    // Handle case where keyspaceId = null.
+    // In such case we cannot infer and validate schema for table or udt
+    methodBuilder.beginControlFlow(
+        ""if (!context.getSession().getMetadata().getKeyspace(keyspaceId).isPresent())"");
+    loggingGenerator.warn(
+        methodBuilder,
+        ""[{}] Unable to validate table: {} for the entity class: {} because keyspace: {} is not present"",
+        CodeBlock.of(""context.getSession().getName()""),
+        CodeBlock.of(""tableId""),
+        CodeBlock.of(""entityClassName""),
+        CodeBlock.of(""keyspaceId""));","[{'comment': 'Nit: `keyspaceId` might be null. In fact, I think this will be the most common reason for this warning: when neither the session nor the DAO was created with a keyspace. So I think we should handle it separately with a more descriptive error message than ""keyspace null is not present"".\r\n\r\n(The other possible reasons are: keyspace name present but wrong, or keyspace name present but metadata is out of date.)', 'commenter': 'olim7t'}, {'comment': ""I've expanded this check to two (https://github.com/datastax/java-driver/pull/1300/commits/0d16fe9a2133385acb4baa71446f5e19cfd1403e):\r\n1. lookup the `keyspaceId` in the `context.getSession().getMetadata().getKeyspaces()` -> if not present, then report that provided keyspace is not pressent in metadata\r\n2. When the `keyspaceId`  is present but returned `keyspace` is of a type `Optional.empty` then report that keyspace is not present."", 'commenter': 'tomekl007'}, {'comment': '> keyspace name present but metadata is out of date\r\naddressed in this commit:  https://github.com/datastax/java-driver/pull/1300/commits/9442728d5e0fa5274c283bf4c1ade0f2ed2c873d\r\nSee my comment: https://github.com/datastax/java-driver/pull/1300#discussion_r358248749\r\nfor more explanation. ', 'commenter': 'tomekl007'}, {'comment': ""Hrmm I don't get it, the two checks are equivalent. From the generated code with the new version:\r\n```java\r\nif(!keyspaceNamePresent(context.getSession().getMetadata().getKeyspaces(), keyspaceId)) {\r\n  ... // warning 1\r\n}\r\nOptional<KeyspaceMetadata> keyspace = context.getSession().getMetadata().getKeyspace(keyspaceId);\r\nif (!keyspace.isPresent()) {\r\n  ... // warning 2\r\n}\r\n```\r\nThe two checks to the same thing: if the key is not present in the map, then getKeyspace returns an empty result.\r\n\r\nMy suggestion was more in the lines of:\r\n```java\r\nCqlIdentifier keyspaceId = context.getKeyspaceId() != null ? context.getKeyspaceId() : context.getSession().getKeyspace().orElse(null);\r\nif (keyspaceId == null) {\r\n  ... // DAO will run unqualified requests on an unqualified session: probably a mapper configuration error, the requests will fail at runtime\r\n}\r\nOptional<KeyspaceMetadata> keyspace = context.getSession().getMetadata().getKeyspace(keyspaceId);\r\nif (!keyspace.isPresent()) {\r\n  ... // what you already have for this case\r\n}\r\n```"", 'commenter': 'olim7t'}, {'comment': 'They are not equivalent. \r\n1. The first check is checking the case that you mentioned - so in the case of `keyspaceId == null` it will log warn and progress to unqualified requests. But besides that, it is also checking if the keyspace (if provided as not null) is present in the metadata, see:\r\n```\r\n if(!keyspaceNamePresent(context.getSession().getMetadata().getKeyspaces(), keyspaceId)) {\r\n}\r\n.....\r\npublic boolean keyspaceNamePresent(\r\n      Map<CqlIdentifier, KeyspaceMetadata> keyspaces, CqlIdentifier keyspaceId) {\r\n    return keyspaces.keySet().contains(keyspaceId);\r\n  }\r\n```\r\n(`null` will be not present so it will catch your first case.\r\nPlease take a look at this test: `SchemaValidationIT#should_log_warning_when_passing_not_existing_keyspace`\r\n2. This second case:\r\n```\r\n Optional<KeyspaceMetadata> keyspace = context.getSession().getMetadata().getKeyspace(keyspaceId);\r\n    if (!keyspace.isPresent()) {\r\n      LOG.warn(""[{}] Unable to validate table: {} for the entity class: {} because keyspace: {} is not present."",\r\n          context.getSession().getName(),\r\n          tableId,\r\n          entityClassName,\r\n          keyspaceId);\r\n      return;\r\n    }\r\n```\r\nis getting the actual `KespaceMetadata` for not null `keyspaceId` (if it was null, then previous case will catch that)\r\n\r\nSo I think that the current solution is equivalent to your suggestion + it is adding the validation of the existence of the keyspace name.', 'commenter': 'tomekl007'}, {'comment': ""These two conditions are equivalent:\r\n```java\r\nsession.getMetadata().getKeyspaces().keySet().contains(keyspaceId)\r\nsession.getMetadata().getKeyspace(keyspaceId).isPresent()\r\n```\r\nExcept if you assume that:\r\n* the map can contain null values (which is in fact impossible because our implementation relies on Guava's `ImmutableMap`, which doesn't allow it)\r\n* or that there is a bug in `getKeyspace()`.\r\n\r\nSo the first check already covers the second check. Try to write a test that produces the second warning.\r\n\r\nWe have two error conditions to cover:\r\n1. the DAO was not built with sufficient information to determine a keyspace id\r\n2. it was, but the keyspace id does not exist\r\n\r\nCurrently, the first check does both 1 and 2, and the second check does 2 (but is unreachable)."", 'commenter': 'olim7t'}, {'comment': ""Ah, you right - sorry for back and forth on this.\r\nI've switch checks (according to your suggestion here: https://github.com/datastax/java-driver/pull/1300#discussion_r364867139) to first check if `keyspaceId == null` and in that case give a detailed message. Otherwise the `keyspaceNamePresent` check."", 'commenter': 'tomekl007'}]"
1300,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/entity/EntityHelperSchemaValidationMethodGenerator.java,"@@ -0,0 +1,309 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.mapper.processor.entity;
+
+import static com.datastax.oss.driver.api.mapper.annotations.SchemaHint.*;
+
+import com.datastax.oss.driver.api.core.CqlIdentifier;
+import com.datastax.oss.driver.api.core.metadata.schema.TableMetadata;
+import com.datastax.oss.driver.api.core.type.UserDefinedType;
+import com.datastax.oss.driver.api.core.type.reflect.GenericType;
+import com.datastax.oss.driver.api.mapper.annotations.SchemaHint;
+import com.datastax.oss.driver.internal.core.metadata.schema.DefaultTableMetadata;
+import com.datastax.oss.driver.internal.mapper.processor.MethodGenerator;
+import com.datastax.oss.driver.internal.mapper.processor.dao.LoggingGenerator;
+import com.squareup.javapoet.CodeBlock;
+import com.squareup.javapoet.MethodSpec;
+import com.squareup.javapoet.TypeName;
+import java.util.ArrayList;
+import java.util.LinkedHashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Optional;
+import java.util.stream.Collectors;
+import javax.lang.model.element.Modifier;
+import javax.lang.model.element.TypeElement;
+
+public class EntityHelperSchemaValidationMethodGenerator implements MethodGenerator {
+
+  private final EntityDefinition entityDefinition;
+  private TypeElement entityTypeElement;
+  private LoggingGenerator loggingGenerator;
+  private EntityHelperGenerator entityHelperGenerator;
+
+  public EntityHelperSchemaValidationMethodGenerator(
+      EntityDefinition entityDefinition,
+      TypeElement entityTypeElement,
+      LoggingGenerator loggingGenerator,
+      EntityHelperGenerator entityHelperGenerator) {
+    this.entityDefinition = entityDefinition;
+    this.entityTypeElement = entityTypeElement;
+    this.loggingGenerator = loggingGenerator;
+    this.entityHelperGenerator = entityHelperGenerator;
+  }
+
+  @Override
+  public Optional<MethodSpec> generate() {
+    MethodSpec.Builder methodBuilder =
+        MethodSpec.methodBuilder(""validateEntityFields"")
+            .addAnnotation(Override.class)
+            .addModifiers(Modifier.PUBLIC)
+            .returns(TypeName.VOID);
+
+    // get keyspaceId from context, and if not present fallback to keyspace set on session
+    methodBuilder.addStatement(
+        ""$1T keyspaceId = context.getKeyspaceId() != null ? context.getKeyspaceId() : context.getSession().getKeyspace().orElse(null)"",
+        CqlIdentifier.class);
+
+    methodBuilder.addStatement(""String entityClassName = $S"", entityDefinition.getClassName());
+
+    // Handle case where keyspaceId = null.
+    // In such case we cannot infer and validate schema for table or udt
+    methodBuilder.beginControlFlow(
+        ""if (!context.getSession().getMetadata().getKeyspace(keyspaceId).isPresent())"");
+    loggingGenerator.warn(
+        methodBuilder,
+        ""[{}] Unable to validate table: {} for the entity class: {} because keyspace: {} is not present"",
+        CodeBlock.of(""context.getSession().getName()""),
+        CodeBlock.of(""tableId""),
+        CodeBlock.of(""entityClassName""),
+        CodeBlock.of(""keyspaceId""));
+    methodBuilder.addStatement(""return"");
+    methodBuilder.endControlFlow();
+
+    // Generates expected names to be present in cql (table or udt)
+    List<CodeBlock> expectedCqlNames =
+        entityDefinition.getAllColumns().stream()
+            .map(PropertyDefinition::getCqlName)
+            .collect(Collectors.toList());
+    methodBuilder.addStatement(
+        ""$1T<$2T> expectedCqlNames = new $3T<>()"",
+        List.class,
+        CqlIdentifier.class,
+        ArrayList.class);
+    for (CodeBlock expectedCqlName : expectedCqlNames) {
+      methodBuilder.addStatement(
+          ""expectedCqlNames.add($1T.fromCql($2L))"", CqlIdentifier.class, expectedCqlName);
+    }
+
+    methodBuilder.addStatement(
+        ""$1T<$2T> tableMetadata = context.getSession().getMetadata().getKeyspace(keyspaceId).flatMap(v -> v.getTable(tableId))"",
+        Optional.class,
+        TableMetadata.class);","[{'comment': 'Nit: could you extract `context.getSession().getMetadata().getKeyspace(keyspaceId)` and reuse it for table and UDT?', 'commenter': 'olim7t'}]"
1300,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/entity/EntityHelperSchemaValidationMethodGenerator.java,"@@ -0,0 +1,309 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.mapper.processor.entity;
+
+import static com.datastax.oss.driver.api.mapper.annotations.SchemaHint.*;
+
+import com.datastax.oss.driver.api.core.CqlIdentifier;
+import com.datastax.oss.driver.api.core.metadata.schema.TableMetadata;
+import com.datastax.oss.driver.api.core.type.UserDefinedType;
+import com.datastax.oss.driver.api.core.type.reflect.GenericType;
+import com.datastax.oss.driver.api.mapper.annotations.SchemaHint;
+import com.datastax.oss.driver.internal.core.metadata.schema.DefaultTableMetadata;
+import com.datastax.oss.driver.internal.mapper.processor.MethodGenerator;
+import com.datastax.oss.driver.internal.mapper.processor.dao.LoggingGenerator;
+import com.squareup.javapoet.CodeBlock;
+import com.squareup.javapoet.MethodSpec;
+import com.squareup.javapoet.TypeName;
+import java.util.ArrayList;
+import java.util.LinkedHashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Optional;
+import java.util.stream.Collectors;
+import javax.lang.model.element.Modifier;
+import javax.lang.model.element.TypeElement;
+
+public class EntityHelperSchemaValidationMethodGenerator implements MethodGenerator {
+
+  private final EntityDefinition entityDefinition;
+  private TypeElement entityTypeElement;
+  private LoggingGenerator loggingGenerator;
+  private EntityHelperGenerator entityHelperGenerator;
+
+  public EntityHelperSchemaValidationMethodGenerator(
+      EntityDefinition entityDefinition,
+      TypeElement entityTypeElement,
+      LoggingGenerator loggingGenerator,
+      EntityHelperGenerator entityHelperGenerator) {
+    this.entityDefinition = entityDefinition;
+    this.entityTypeElement = entityTypeElement;
+    this.loggingGenerator = loggingGenerator;
+    this.entityHelperGenerator = entityHelperGenerator;
+  }
+
+  @Override
+  public Optional<MethodSpec> generate() {
+    MethodSpec.Builder methodBuilder =
+        MethodSpec.methodBuilder(""validateEntityFields"")
+            .addAnnotation(Override.class)
+            .addModifiers(Modifier.PUBLIC)
+            .returns(TypeName.VOID);
+
+    // get keyspaceId from context, and if not present fallback to keyspace set on session
+    methodBuilder.addStatement(
+        ""$1T keyspaceId = context.getKeyspaceId() != null ? context.getKeyspaceId() : context.getSession().getKeyspace().orElse(null)"",
+        CqlIdentifier.class);
+
+    methodBuilder.addStatement(""String entityClassName = $S"", entityDefinition.getClassName());
+
+    // Handle case where keyspaceId = null.
+    // In such case we cannot infer and validate schema for table or udt
+    methodBuilder.beginControlFlow(
+        ""if (!context.getSession().getMetadata().getKeyspace(keyspaceId).isPresent())"");
+    loggingGenerator.warn(
+        methodBuilder,
+        ""[{}] Unable to validate table: {} for the entity class: {} because keyspace: {} is not present"",
+        CodeBlock.of(""context.getSession().getName()""),
+        CodeBlock.of(""tableId""),
+        CodeBlock.of(""entityClassName""),
+        CodeBlock.of(""keyspaceId""));
+    methodBuilder.addStatement(""return"");
+    methodBuilder.endControlFlow();
+
+    // Generates expected names to be present in cql (table or udt)
+    List<CodeBlock> expectedCqlNames =
+        entityDefinition.getAllColumns().stream()
+            .map(PropertyDefinition::getCqlName)
+            .collect(Collectors.toList());
+    methodBuilder.addStatement(
+        ""$1T<$2T> expectedCqlNames = new $3T<>()"",
+        List.class,
+        CqlIdentifier.class,
+        ArrayList.class);
+    for (CodeBlock expectedCqlName : expectedCqlNames) {
+      methodBuilder.addStatement(
+          ""expectedCqlNames.add($1T.fromCql($2L))"", CqlIdentifier.class, expectedCqlName);
+    }
+
+    methodBuilder.addStatement(
+        ""$1T<$2T> tableMetadata = context.getSession().getMetadata().getKeyspace(keyspaceId).flatMap(v -> v.getTable(tableId))"",
+        Optional.class,
+        TableMetadata.class);
+
+    // Generated UserDefineTypes metadata
+    methodBuilder.addStatement(
+        ""$1T<$2T> userDefinedType = context.getSession().getMetadata().getKeyspace(keyspaceId).flatMap(v -> v.getUserDefinedType(tableId))"",
+        Optional.class,
+        UserDefinedType.class);
+
+    generateFindMissingChecks(methodBuilder);
+
+    // Throw if there is not keyspace.table for defined entity
+    CodeBlock missingKeyspaceTableExceptionMessage =
+        CodeBlock.of(
+            ""String.format(\""There is no ks.table: %s.%s for the entity class: %s\"", keyspaceId, tableId, entityClassName)"");
+    methodBuilder.beginControlFlow(""else"");
+    methodBuilder.addStatement(
+        ""throw new $1T($2L)"", IllegalArgumentException.class, missingKeyspaceTableExceptionMessage);
+    methodBuilder.endControlFlow();
+
+    return Optional.of(methodBuilder.build());
+  }
+
+  private void generateFindMissingChecks(MethodSpec.Builder methodBuilder) {
+    Optional<TargetElement> targetElement =
+        Optional.ofNullable(entityTypeElement.getAnnotation(SchemaHint.class))
+            .map(SchemaHint::targetElement);
+
+    // if SchemaHint was not provided explicitly try to match TABLE, then fallback to UDT
+    if (!targetElement.isPresent()) {
+      findMissingColumnsInTable(methodBuilder);
+      findMissingColumnsInUdt(methodBuilder, true);
+    }
+    // if explicitly provided SchemaHint is TABLE, then generate only TABLE check
+    else if (targetElement.get().equals(TargetElement.TABLE)) {
+      findMissingColumnsInTable(methodBuilder);
+    }
+    // if explicitly provided SchemaHint is UDT, then generate only UDT check
+    else if (targetElement.get().equals(TargetElement.UDT)) {
+      findMissingColumnsInUdt(methodBuilder, false);
+    }
+  }
+
+  // Finds out missingTableCqlNames - columns that are present in Entity Mapping but NOT present in
+  // CQL table
+  private void findMissingColumnsInTable(MethodSpec.Builder methodBuilder) {
+    methodBuilder.beginControlFlow(""if (tableMetadata.isPresent())"");
+
+    methodBuilder.addComment(""validation of missing Clustering Columns"");
+    generateMissingClusteringColumnsCheck(methodBuilder);","[{'comment': 'Nit: we could skip this if there are no clustering columns, that would save a few unnecessary lines in the generated code. For example in `UserHelper__MapperGenerated`:\r\n```java\r\n      // validation of missing Clustering Columns\r\n      List<CqlIdentifier> expectedCqlClusteringColumns = new ArrayList<>();\r\n      List<CqlIdentifier> missingTableClusteringColumnNames = findMissingColumns(expectedCqlClusteringColumns, tableMetadata.get().getClusteringColumns().keySet());\r\n      if (!missingTableClusteringColumnNames.isEmpty()) {\r\n        throw new IllegalArgumentException(String.format(""The CQL ks.table: %s.%s has missing Clustering columns: %s that are defined in the entity class: %s"", keyspaceId, tableId, missingTableClusteringColumnNames, entityClassName));\r\n      }\r\n```', 'commenter': 'olim7t'}]"
1300,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/entity/EntityHelperSchemaValidationMethodGenerator.java,"@@ -0,0 +1,309 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.mapper.processor.entity;
+
+import static com.datastax.oss.driver.api.mapper.annotations.SchemaHint.*;
+
+import com.datastax.oss.driver.api.core.CqlIdentifier;
+import com.datastax.oss.driver.api.core.metadata.schema.TableMetadata;
+import com.datastax.oss.driver.api.core.type.UserDefinedType;
+import com.datastax.oss.driver.api.core.type.reflect.GenericType;
+import com.datastax.oss.driver.api.mapper.annotations.SchemaHint;
+import com.datastax.oss.driver.internal.core.metadata.schema.DefaultTableMetadata;
+import com.datastax.oss.driver.internal.mapper.processor.MethodGenerator;
+import com.datastax.oss.driver.internal.mapper.processor.dao.LoggingGenerator;
+import com.squareup.javapoet.CodeBlock;
+import com.squareup.javapoet.MethodSpec;
+import com.squareup.javapoet.TypeName;
+import java.util.ArrayList;
+import java.util.LinkedHashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Optional;
+import java.util.stream.Collectors;
+import javax.lang.model.element.Modifier;
+import javax.lang.model.element.TypeElement;
+
+public class EntityHelperSchemaValidationMethodGenerator implements MethodGenerator {
+
+  private final EntityDefinition entityDefinition;
+  private TypeElement entityTypeElement;
+  private LoggingGenerator loggingGenerator;
+  private EntityHelperGenerator entityHelperGenerator;
+
+  public EntityHelperSchemaValidationMethodGenerator(
+      EntityDefinition entityDefinition,
+      TypeElement entityTypeElement,
+      LoggingGenerator loggingGenerator,
+      EntityHelperGenerator entityHelperGenerator) {
+    this.entityDefinition = entityDefinition;
+    this.entityTypeElement = entityTypeElement;
+    this.loggingGenerator = loggingGenerator;
+    this.entityHelperGenerator = entityHelperGenerator;
+  }
+
+  @Override
+  public Optional<MethodSpec> generate() {
+    MethodSpec.Builder methodBuilder =
+        MethodSpec.methodBuilder(""validateEntityFields"")
+            .addAnnotation(Override.class)
+            .addModifiers(Modifier.PUBLIC)
+            .returns(TypeName.VOID);
+
+    // get keyspaceId from context, and if not present fallback to keyspace set on session
+    methodBuilder.addStatement(
+        ""$1T keyspaceId = context.getKeyspaceId() != null ? context.getKeyspaceId() : context.getSession().getKeyspace().orElse(null)"",
+        CqlIdentifier.class);
+
+    methodBuilder.addStatement(""String entityClassName = $S"", entityDefinition.getClassName());
+
+    // Handle case where keyspaceId = null.
+    // In such case we cannot infer and validate schema for table or udt
+    methodBuilder.beginControlFlow(
+        ""if (!context.getSession().getMetadata().getKeyspace(keyspaceId).isPresent())"");
+    loggingGenerator.warn(
+        methodBuilder,
+        ""[{}] Unable to validate table: {} for the entity class: {} because keyspace: {} is not present"",
+        CodeBlock.of(""context.getSession().getName()""),
+        CodeBlock.of(""tableId""),
+        CodeBlock.of(""entityClassName""),
+        CodeBlock.of(""keyspaceId""));
+    methodBuilder.addStatement(""return"");
+    methodBuilder.endControlFlow();
+
+    // Generates expected names to be present in cql (table or udt)
+    List<CodeBlock> expectedCqlNames =
+        entityDefinition.getAllColumns().stream()
+            .map(PropertyDefinition::getCqlName)
+            .collect(Collectors.toList());
+    methodBuilder.addStatement(
+        ""$1T<$2T> expectedCqlNames = new $3T<>()"",
+        List.class,
+        CqlIdentifier.class,
+        ArrayList.class);
+    for (CodeBlock expectedCqlName : expectedCqlNames) {
+      methodBuilder.addStatement(
+          ""expectedCqlNames.add($1T.fromCql($2L))"", CqlIdentifier.class, expectedCqlName);
+    }
+
+    methodBuilder.addStatement(
+        ""$1T<$2T> tableMetadata = context.getSession().getMetadata().getKeyspace(keyspaceId).flatMap(v -> v.getTable(tableId))"",
+        Optional.class,
+        TableMetadata.class);
+
+    // Generated UserDefineTypes metadata
+    methodBuilder.addStatement(
+        ""$1T<$2T> userDefinedType = context.getSession().getMetadata().getKeyspace(keyspaceId).flatMap(v -> v.getUserDefinedType(tableId))"",
+        Optional.class,
+        UserDefinedType.class);
+
+    generateFindMissingChecks(methodBuilder);
+
+    // Throw if there is not keyspace.table for defined entity
+    CodeBlock missingKeyspaceTableExceptionMessage =
+        CodeBlock.of(
+            ""String.format(\""There is no ks.table: %s.%s for the entity class: %s\"", keyspaceId, tableId, entityClassName)"");
+    methodBuilder.beginControlFlow(""else"");
+    methodBuilder.addStatement(
+        ""throw new $1T($2L)"", IllegalArgumentException.class, missingKeyspaceTableExceptionMessage);
+    methodBuilder.endControlFlow();
+
+    return Optional.of(methodBuilder.build());
+  }
+
+  private void generateFindMissingChecks(MethodSpec.Builder methodBuilder) {
+    Optional<TargetElement> targetElement =
+        Optional.ofNullable(entityTypeElement.getAnnotation(SchemaHint.class))
+            .map(SchemaHint::targetElement);
+
+    // if SchemaHint was not provided explicitly try to match TABLE, then fallback to UDT
+    if (!targetElement.isPresent()) {
+      findMissingColumnsInTable(methodBuilder);
+      findMissingColumnsInUdt(methodBuilder, true);
+    }
+    // if explicitly provided SchemaHint is TABLE, then generate only TABLE check
+    else if (targetElement.get().equals(TargetElement.TABLE)) {
+      findMissingColumnsInTable(methodBuilder);
+    }
+    // if explicitly provided SchemaHint is UDT, then generate only UDT check
+    else if (targetElement.get().equals(TargetElement.UDT)) {
+      findMissingColumnsInUdt(methodBuilder, false);
+    }
+  }
+
+  // Finds out missingTableCqlNames - columns that are present in Entity Mapping but NOT present in
+  // CQL table
+  private void findMissingColumnsInTable(MethodSpec.Builder methodBuilder) {
+    methodBuilder.beginControlFlow(""if (tableMetadata.isPresent())"");
+
+    methodBuilder.addComment(""validation of missing Clustering Columns"");
+    generateMissingClusteringColumnsCheck(methodBuilder);
+
+    methodBuilder.addComment(""validation of missing PKs"");
+    generateMissingPKsCheck(methodBuilder);
+
+    methodBuilder.addComment(""validation of all columns"");
+    generateMissingColumnsCheck(methodBuilder);
+
+    methodBuilder.addComment(""validation of types"");
+    generateColumnsTypeCheck(methodBuilder);
+
+    methodBuilder.endControlFlow();
+  }
+
+  private void generateColumnsTypeCheck(MethodSpec.Builder methodBuilder) {
+    methodBuilder.addStatement(
+        ""$1T<$2T, $3T<?>> expectedTypesPerColumn = new $4T<>()"",
+        Map.class,
+        CqlIdentifier.class,
+        GenericType.class,
+        LinkedHashMap.class);
+
+    Map<CodeBlock, TypeName> expectedTypesPerColumn =
+        entityDefinition.getAllColumns().stream()
+            .collect(
+                Collectors.toMap(PropertyDefinition::getCqlName, v -> v.getType().asRawTypeName()));
+
+    for (Map.Entry<CodeBlock, TypeName> expected : expectedTypesPerColumn.entrySet()) {
+      methodBuilder.addStatement(
+          ""expectedTypesPerColumn.put($1T.fromCql($2L), $3L)"",
+          CqlIdentifier.class,
+          expected.getKey(),
+          entityHelperGenerator.addGenericTypeConstant(expected.getValue().box()));
+    }
+
+    methodBuilder.addStatement(
+        ""$1T<$2T> missingTableTypes = findMissingTypes(expectedTypesPerColumn, (($3T) tableMetadata.get()).getColumns(), context.getSession().getContext().getCodecRegistry())"",
+        List.class,
+        String.class,
+        DefaultTableMetadata.class);
+    methodBuilder.addStatement(
+        ""throwMissingTypesIfNotEmpty(missingTableTypes, keyspaceId, tableId, entityClassName)"");
+  }
+
+  private void generateMissingColumnsCheck(MethodSpec.Builder methodBuilder) {
+
+    methodBuilder.addStatement(
+        ""$1T<$2T> missingTableCqlNames = findMissingColumnsCql(expectedCqlNames, (($3T) tableMetadata.get()).getColumns().keySet())"",
+        List.class,
+        CqlIdentifier.class,
+        DefaultTableMetadata.class);
+
+    // Throw if there are any missingTableCqlNames
+    CodeBlock missingCqlColumnExceptionMessage =
+        CodeBlock.of(
+            ""String.format(\""The CQL ks.table: %s.%s has missing columns: %s that are defined in the entity class: %s\"", ""","[{'comment': '```suggestion\r\n            ""String.format(\\""The CQL table: %s.%s has missing columns: %s that are defined in the entity class: %s\\"", ""\r\n```\r\n?\r\n\r\n\r\nAlso occurs a couple of other times below.', 'commenter': 'olim7t'}, {'comment': 'the `ks.table` is a format of the message - in cases for UDt it is `ks.udt`. \r\nI think we should keep it because it gives the user information about the type of the validated entity.', 'commenter': 'tomekl007'}, {'comment': 'If you just say ""table"" or ""udt"" it\'s enough to indicate the type of entity.\r\n\r\nI see the intent now, but it looks a bit unusual in a plain english message. Also if you just say ""the table inventory.product"", it should be pretty obvious enough that ""inventory"" is the keyspace.\r\n\r\nBut that\'s just a cosmetic detail so I don\'t want to dwell on it too much, I\'ll leave the final decision to you.', 'commenter': 'olim7t'}, {'comment': ""When writing tests and debugging it was useful to me have that `ks.udt` and `table.udt` in logs so I will leave those in the logs if you don't mind"", 'commenter': 'tomekl007'}]"
1300,mapper-runtime/src/main/java/com/datastax/oss/driver/internal/mapper/entity/EntityHelperBase.java,"@@ -75,4 +86,70 @@ protected void throwIfKeyspaceMissing() {
               DaoKeyspace.class.getSimpleName()));
     }
   }
+
+  public List<CqlIdentifier> findMissingColumns(
+      List<CqlIdentifier> entityColumns, Collection<ColumnMetadata> cqlColumns) {
+    return findMissingColumnsCql(
+        entityColumns,
+        cqlColumns.stream().map(ColumnMetadata::getName).collect(Collectors.toList()));
+  }
+
+  public List<CqlIdentifier> findMissingColumnsCql(
+      List<CqlIdentifier> entityColumns, Collection<CqlIdentifier> cqlColumns) {","[{'comment': ""Naming is a bit confusing here, it's not immediately clear how `findMissingColumns` / `findMissingColumnsCql` differ.\r\nThe only reason to extract the latter is because you're passing `tableMetadata.get()).getColumns().keySet()` when validating regular columns. You could pass `tableMetadata.get()).getColumns().values()` instead, and merge the two methods."", 'commenter': 'olim7t'}, {'comment': 'I will still need this method for udt check, see:\r\n```\r\n else if (userDefinedType.isPresent()) {\r\n      List<CqlIdentifier> columns = userDefinedType.get().getFieldNames();\r\n      List<CqlIdentifier> missingTableCqlNames = findMissingColumnsCql(expectedCqlNames, columns);\r\n      if (!missingTableCqlNames.isEmpty()) {\r\n        throw new IllegalArgumentException(String.format(""The CQL ks.udt: %s.%s has missing columns: %s that are defined in the entity class: %s"", keyspaceId, tableId, missingTableCqlNames, entityClassName));\r\n      }\r\n    }\r\n```', 'commenter': 'tomekl007'}, {'comment': 'I see. Maybe rename it to `findMissingIdentifiers` then...', 'commenter': 'olim7t'}]"
1300,mapper-runtime/src/main/java/com/datastax/oss/driver/internal/mapper/entity/EntityHelperBase.java,"@@ -75,4 +86,70 @@ protected void throwIfKeyspaceMissing() {
               DaoKeyspace.class.getSimpleName()));
     }
   }
+
+  public List<CqlIdentifier> findMissingColumns(
+      List<CqlIdentifier> entityColumns, Collection<ColumnMetadata> cqlColumns) {","[{'comment': 'I was going to comment on the fact that the generated code boxes every identifier before invoking this method:\r\n```java\r\n      List<CqlIdentifier> expectedCqlPKs = new ArrayList<>();\r\n      expectedCqlPKs.add(CqlIdentifier.fromCql(""userid"")); // <== LIKE THIS\r\n      List<CqlIdentifier> missingTablePksNames = findMissingColumns(expectedCqlPKs, tableMetadata.get().getPartitionKey());\r\n```\r\nBut that\'s actually the right way to do it. These identifiers may be provided directly by the user with a `CqlName` annotation, so comparing them directly to `cqlColumn.asCql(...)` would not be reliable.\r\n\r\nSo nothing to change here ðŸ‘ ', 'commenter': 'olim7t'}]"
1300,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/entity/EntityHelperSchemaValidationMethodGenerator.java,"@@ -0,0 +1,309 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.mapper.processor.entity;
+
+import static com.datastax.oss.driver.api.mapper.annotations.SchemaHint.*;
+
+import com.datastax.oss.driver.api.core.CqlIdentifier;
+import com.datastax.oss.driver.api.core.metadata.schema.TableMetadata;
+import com.datastax.oss.driver.api.core.type.UserDefinedType;
+import com.datastax.oss.driver.api.core.type.reflect.GenericType;
+import com.datastax.oss.driver.api.mapper.annotations.SchemaHint;
+import com.datastax.oss.driver.internal.core.metadata.schema.DefaultTableMetadata;
+import com.datastax.oss.driver.internal.mapper.processor.MethodGenerator;
+import com.datastax.oss.driver.internal.mapper.processor.dao.LoggingGenerator;
+import com.squareup.javapoet.CodeBlock;
+import com.squareup.javapoet.MethodSpec;
+import com.squareup.javapoet.TypeName;
+import java.util.ArrayList;
+import java.util.LinkedHashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Optional;
+import java.util.stream.Collectors;
+import javax.lang.model.element.Modifier;
+import javax.lang.model.element.TypeElement;
+
+public class EntityHelperSchemaValidationMethodGenerator implements MethodGenerator {
+
+  private final EntityDefinition entityDefinition;
+  private TypeElement entityTypeElement;
+  private LoggingGenerator loggingGenerator;
+  private EntityHelperGenerator entityHelperGenerator;
+
+  public EntityHelperSchemaValidationMethodGenerator(
+      EntityDefinition entityDefinition,
+      TypeElement entityTypeElement,
+      LoggingGenerator loggingGenerator,
+      EntityHelperGenerator entityHelperGenerator) {
+    this.entityDefinition = entityDefinition;
+    this.entityTypeElement = entityTypeElement;
+    this.loggingGenerator = loggingGenerator;
+    this.entityHelperGenerator = entityHelperGenerator;
+  }
+
+  @Override
+  public Optional<MethodSpec> generate() {
+    MethodSpec.Builder methodBuilder =
+        MethodSpec.methodBuilder(""validateEntityFields"")
+            .addAnnotation(Override.class)
+            .addModifiers(Modifier.PUBLIC)
+            .returns(TypeName.VOID);
+
+    // get keyspaceId from context, and if not present fallback to keyspace set on session
+    methodBuilder.addStatement(
+        ""$1T keyspaceId = context.getKeyspaceId() != null ? context.getKeyspaceId() : context.getSession().getKeyspace().orElse(null)"",
+        CqlIdentifier.class);
+
+    methodBuilder.addStatement(""String entityClassName = $S"", entityDefinition.getClassName());
+
+    // Handle case where keyspaceId = null.
+    // In such case we cannot infer and validate schema for table or udt
+    methodBuilder.beginControlFlow(
+        ""if (!context.getSession().getMetadata().getKeyspace(keyspaceId).isPresent())"");
+    loggingGenerator.warn(
+        methodBuilder,
+        ""[{}] Unable to validate table: {} for the entity class: {} because keyspace: {} is not present"",
+        CodeBlock.of(""context.getSession().getName()""),
+        CodeBlock.of(""tableId""),
+        CodeBlock.of(""entityClassName""),
+        CodeBlock.of(""keyspaceId""));
+    methodBuilder.addStatement(""return"");
+    methodBuilder.endControlFlow();
+
+    // Generates expected names to be present in cql (table or udt)
+    List<CodeBlock> expectedCqlNames =
+        entityDefinition.getAllColumns().stream()
+            .map(PropertyDefinition::getCqlName)
+            .collect(Collectors.toList());
+    methodBuilder.addStatement(
+        ""$1T<$2T> expectedCqlNames = new $3T<>()"",
+        List.class,
+        CqlIdentifier.class,
+        ArrayList.class);
+    for (CodeBlock expectedCqlName : expectedCqlNames) {
+      methodBuilder.addStatement(
+          ""expectedCqlNames.add($1T.fromCql($2L))"", CqlIdentifier.class, expectedCqlName);
+    }
+
+    methodBuilder.addStatement(
+        ""$1T<$2T> tableMetadata = context.getSession().getMetadata().getKeyspace(keyspaceId).flatMap(v -> v.getTable(tableId))"",
+        Optional.class,
+        TableMetadata.class);
+
+    // Generated UserDefineTypes metadata
+    methodBuilder.addStatement(
+        ""$1T<$2T> userDefinedType = context.getSession().getMetadata().getKeyspace(keyspaceId).flatMap(v -> v.getUserDefinedType(tableId))"",
+        Optional.class,
+        UserDefinedType.class);
+
+    generateFindMissingChecks(methodBuilder);
+
+    // Throw if there is not keyspace.table for defined entity
+    CodeBlock missingKeyspaceTableExceptionMessage =
+        CodeBlock.of(
+            ""String.format(\""There is no ks.table: %s.%s for the entity class: %s\"", keyspaceId, tableId, entityClassName)"");
+    methodBuilder.beginControlFlow(""else"");
+    methodBuilder.addStatement(
+        ""throw new $1T($2L)"", IllegalArgumentException.class, missingKeyspaceTableExceptionMessage);
+    methodBuilder.endControlFlow();
+
+    return Optional.of(methodBuilder.build());
+  }
+
+  private void generateFindMissingChecks(MethodSpec.Builder methodBuilder) {
+    Optional<TargetElement> targetElement =
+        Optional.ofNullable(entityTypeElement.getAnnotation(SchemaHint.class))
+            .map(SchemaHint::targetElement);
+
+    // if SchemaHint was not provided explicitly try to match TABLE, then fallback to UDT
+    if (!targetElement.isPresent()) {
+      findMissingColumnsInTable(methodBuilder);
+      findMissingColumnsInUdt(methodBuilder, true);
+    }
+    // if explicitly provided SchemaHint is TABLE, then generate only TABLE check
+    else if (targetElement.get().equals(TargetElement.TABLE)) {
+      findMissingColumnsInTable(methodBuilder);
+    }
+    // if explicitly provided SchemaHint is UDT, then generate only UDT check
+    else if (targetElement.get().equals(TargetElement.UDT)) {
+      findMissingColumnsInUdt(methodBuilder, false);
+    }
+  }
+
+  // Finds out missingTableCqlNames - columns that are present in Entity Mapping but NOT present in
+  // CQL table
+  private void findMissingColumnsInTable(MethodSpec.Builder methodBuilder) {
+    methodBuilder.beginControlFlow(""if (tableMetadata.isPresent())"");
+
+    methodBuilder.addComment(""validation of missing Clustering Columns"");
+    generateMissingClusteringColumnsCheck(methodBuilder);
+
+    methodBuilder.addComment(""validation of missing PKs"");
+    generateMissingPKsCheck(methodBuilder);
+
+    methodBuilder.addComment(""validation of all columns"");
+    generateMissingColumnsCheck(methodBuilder);
+
+    methodBuilder.addComment(""validation of types"");
+    generateColumnsTypeCheck(methodBuilder);
+
+    methodBuilder.endControlFlow();
+  }
+
+  private void generateColumnsTypeCheck(MethodSpec.Builder methodBuilder) {
+    methodBuilder.addStatement(
+        ""$1T<$2T, $3T<?>> expectedTypesPerColumn = new $4T<>()"",
+        Map.class,
+        CqlIdentifier.class,
+        GenericType.class,
+        LinkedHashMap.class);
+
+    Map<CodeBlock, TypeName> expectedTypesPerColumn =
+        entityDefinition.getAllColumns().stream()
+            .collect(
+                Collectors.toMap(PropertyDefinition::getCqlName, v -> v.getType().asRawTypeName()));
+
+    for (Map.Entry<CodeBlock, TypeName> expected : expectedTypesPerColumn.entrySet()) {
+      methodBuilder.addStatement(
+          ""expectedTypesPerColumn.put($1T.fromCql($2L), $3L)"",
+          CqlIdentifier.class,
+          expected.getKey(),
+          entityHelperGenerator.addGenericTypeConstant(expected.getValue().box()));
+    }
+
+    methodBuilder.addStatement(
+        ""$1T<$2T> missingTableTypes = findMissingTypes(expectedTypesPerColumn, (($3T) tableMetadata.get()).getColumns(), context.getSession().getContext().getCodecRegistry())"",
+        List.class,
+        String.class,
+        DefaultTableMetadata.class);
+    methodBuilder.addStatement(
+        ""throwMissingTypesIfNotEmpty(missingTableTypes, keyspaceId, tableId, entityClassName)"");
+  }
+
+  private void generateMissingColumnsCheck(MethodSpec.Builder methodBuilder) {
+
+    methodBuilder.addStatement(
+        ""$1T<$2T> missingTableCqlNames = findMissingColumnsCql(expectedCqlNames, (($3T) tableMetadata.get()).getColumns().keySet())"",
+        List.class,
+        CqlIdentifier.class,
+        DefaultTableMetadata.class);
+
+    // Throw if there are any missingTableCqlNames
+    CodeBlock missingCqlColumnExceptionMessage =
+        CodeBlock.of(
+            ""String.format(\""The CQL ks.table: %s.%s has missing columns: %s that are defined in the entity class: %s\"", ""
+                + ""keyspaceId, tableId, missingTableCqlNames, entityClassName)"");
+    methodBuilder.beginControlFlow(""if (!missingTableCqlNames.isEmpty())"");
+    methodBuilder.addStatement(
+        ""throw new $1T($2L)"", IllegalArgumentException.class, missingCqlColumnExceptionMessage);
+    methodBuilder.endControlFlow();
+  }
+
+  private void generateMissingPKsCheck(MethodSpec.Builder methodBuilder) {
+    List<CodeBlock> expectedCqlPKs =
+        entityDefinition.getPartitionKey().stream()
+            .map(PropertyDefinition::getCqlName)
+            .collect(Collectors.toList());
+
+    methodBuilder.addStatement(
+        ""$1T<$2T> expectedCqlPKs = new $3T<>()"", List.class, CqlIdentifier.class, ArrayList.class);
+    for (CodeBlock expectedCqlName : expectedCqlPKs) {
+      methodBuilder.addStatement(
+          ""expectedCqlPKs.add($1T.fromCql($2L))"", CqlIdentifier.class, expectedCqlName);
+    }
+    methodBuilder.addStatement(
+        ""$1T<$2T> missingTablePksNames = findMissingColumns(expectedCqlPKs, tableMetadata.get().getPartitionKey())"",
+        List.class,
+        CqlIdentifier.class);
+
+    // throw if there are any missing PK columns
+    CodeBlock missingCqlColumnExceptionMessage =
+        CodeBlock.of(
+            ""String.format(\""The CQL ks.table: %s.%s has missing Primary Key columns: %s that are defined in the entity class: %s\"", ""
+                + ""keyspaceId, tableId, missingTablePksNames, entityClassName)"");
+    methodBuilder.beginControlFlow(""if (!missingTablePksNames.isEmpty())"");
+    methodBuilder.addStatement(
+        ""throw new $1T($2L)"", IllegalArgumentException.class, missingCqlColumnExceptionMessage);
+    methodBuilder.endControlFlow();
+  }
+
+  private void generateMissingClusteringColumnsCheck(MethodSpec.Builder methodBuilder) {
+    List<CodeBlock> expectedCqlClusteringColumns =
+        entityDefinition.getClusteringColumns().stream()
+            .map(PropertyDefinition::getCqlName)
+            .collect(Collectors.toList());
+
+    methodBuilder.addStatement(
+        ""$1T<$2T> expectedCqlClusteringColumns = new $3T<>()"",
+        List.class,
+        CqlIdentifier.class,
+        ArrayList.class);
+    for (CodeBlock expectedCqlName : expectedCqlClusteringColumns) {
+      methodBuilder.addStatement(
+          ""expectedCqlClusteringColumns.add($1T.fromCql($2L))"",
+          CqlIdentifier.class,
+          expectedCqlName);
+    }
+
+    methodBuilder.addStatement(
+        ""$1T<$2T> missingTableClusteringColumnNames = findMissingColumns(expectedCqlClusteringColumns, tableMetadata.get().getClusteringColumns().keySet())"",
+        List.class,
+        CqlIdentifier.class);
+
+    // throw if there are any missing Clustering Columns columns
+    CodeBlock missingCqlColumnExceptionMessage =
+        CodeBlock.of(
+            ""String.format(\""The CQL ks.table: %s.%s has missing Clustering columns: %s that are defined in the entity class: %s\"", ""
+                + ""keyspaceId, tableId, missingTableClusteringColumnNames, entityClassName)"");
+    methodBuilder.beginControlFlow(""if (!missingTableClusteringColumnNames.isEmpty())"");
+    methodBuilder.addStatement(
+        ""throw new $1T($2L)"", IllegalArgumentException.class, missingCqlColumnExceptionMessage);
+    methodBuilder.endControlFlow();
+  }
+
+  // Finds out missingTableCqlNames - columns that are present in Entity Mapping but NOT present in
+  // UDT table
+  private void findMissingColumnsInUdt(MethodSpec.Builder methodBuilder, boolean generateElse) {
+    if (generateElse) {
+      methodBuilder.beginControlFlow(""else if (userDefinedType.isPresent())"");
+    } else {
+      methodBuilder.beginControlFlow(""if (userDefinedType.isPresent())"");
+    }
+
+    methodBuilder.addStatement(
+        ""$1T<$2T> columns = userDefinedType.get().getFieldNames()"",
+        List.class,
+        CqlIdentifier.class);
+
+    methodBuilder.addStatement(
+        ""$1T<$2T> missingTableCqlNames = findMissingColumnsCql(expectedCqlNames, columns)"",
+        List.class,
+        CqlIdentifier.class);
+
+    // Throw if there are any missingTableCqlNames
+    CodeBlock missingCqlUdtExceptionMessage =
+        CodeBlock.of(
+            ""String.format(\""The CQL ks.udt: %s.%s has missing columns: %s that are defined in the entity class: %s\"", ""
+                + ""keyspaceId, tableId, missingTableCqlNames, entityClassName)"");
+    methodBuilder.beginControlFlow(""if (!missingTableCqlNames.isEmpty())"");
+    methodBuilder.addStatement(
+        ""throw new $1T($2L)"", IllegalArgumentException.class, missingCqlUdtExceptionMessage);
+    methodBuilder.endControlFlow();
+
+    methodBuilder.endControlFlow();","[{'comment': 'We should also check the types for UDTs.', 'commenter': 'olim7t'}]"
1300,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/entity/EntityHelperSchemaValidationMethodGenerator.java,"@@ -0,0 +1,309 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.mapper.processor.entity;
+
+import static com.datastax.oss.driver.api.mapper.annotations.SchemaHint.*;
+
+import com.datastax.oss.driver.api.core.CqlIdentifier;
+import com.datastax.oss.driver.api.core.metadata.schema.TableMetadata;
+import com.datastax.oss.driver.api.core.type.UserDefinedType;
+import com.datastax.oss.driver.api.core.type.reflect.GenericType;
+import com.datastax.oss.driver.api.mapper.annotations.SchemaHint;
+import com.datastax.oss.driver.internal.core.metadata.schema.DefaultTableMetadata;
+import com.datastax.oss.driver.internal.mapper.processor.MethodGenerator;
+import com.datastax.oss.driver.internal.mapper.processor.dao.LoggingGenerator;
+import com.squareup.javapoet.CodeBlock;
+import com.squareup.javapoet.MethodSpec;
+import com.squareup.javapoet.TypeName;
+import java.util.ArrayList;
+import java.util.LinkedHashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Optional;
+import java.util.stream.Collectors;
+import javax.lang.model.element.Modifier;
+import javax.lang.model.element.TypeElement;
+
+public class EntityHelperSchemaValidationMethodGenerator implements MethodGenerator {
+
+  private final EntityDefinition entityDefinition;
+  private TypeElement entityTypeElement;
+  private LoggingGenerator loggingGenerator;
+  private EntityHelperGenerator entityHelperGenerator;
+
+  public EntityHelperSchemaValidationMethodGenerator(
+      EntityDefinition entityDefinition,
+      TypeElement entityTypeElement,
+      LoggingGenerator loggingGenerator,
+      EntityHelperGenerator entityHelperGenerator) {
+    this.entityDefinition = entityDefinition;
+    this.entityTypeElement = entityTypeElement;
+    this.loggingGenerator = loggingGenerator;
+    this.entityHelperGenerator = entityHelperGenerator;
+  }
+
+  @Override
+  public Optional<MethodSpec> generate() {
+    MethodSpec.Builder methodBuilder =
+        MethodSpec.methodBuilder(""validateEntityFields"")
+            .addAnnotation(Override.class)
+            .addModifiers(Modifier.PUBLIC)
+            .returns(TypeName.VOID);
+
+    // get keyspaceId from context, and if not present fallback to keyspace set on session
+    methodBuilder.addStatement(
+        ""$1T keyspaceId = context.getKeyspaceId() != null ? context.getKeyspaceId() : context.getSession().getKeyspace().orElse(null)"",
+        CqlIdentifier.class);
+
+    methodBuilder.addStatement(""String entityClassName = $S"", entityDefinition.getClassName());
+
+    // Handle case where keyspaceId = null.
+    // In such case we cannot infer and validate schema for table or udt
+    methodBuilder.beginControlFlow(
+        ""if (!context.getSession().getMetadata().getKeyspace(keyspaceId).isPresent())"");
+    loggingGenerator.warn(
+        methodBuilder,
+        ""[{}] Unable to validate table: {} for the entity class: {} because keyspace: {} is not present"",
+        CodeBlock.of(""context.getSession().getName()""),
+        CodeBlock.of(""tableId""),
+        CodeBlock.of(""entityClassName""),
+        CodeBlock.of(""keyspaceId""));
+    methodBuilder.addStatement(""return"");
+    methodBuilder.endControlFlow();
+
+    // Generates expected names to be present in cql (table or udt)
+    List<CodeBlock> expectedCqlNames =
+        entityDefinition.getAllColumns().stream()
+            .map(PropertyDefinition::getCqlName)
+            .collect(Collectors.toList());
+    methodBuilder.addStatement(
+        ""$1T<$2T> expectedCqlNames = new $3T<>()"",
+        List.class,
+        CqlIdentifier.class,
+        ArrayList.class);
+    for (CodeBlock expectedCqlName : expectedCqlNames) {
+      methodBuilder.addStatement(
+          ""expectedCqlNames.add($1T.fromCql($2L))"", CqlIdentifier.class, expectedCqlName);
+    }
+
+    methodBuilder.addStatement(
+        ""$1T<$2T> tableMetadata = context.getSession().getMetadata().getKeyspace(keyspaceId).flatMap(v -> v.getTable(tableId))"",
+        Optional.class,
+        TableMetadata.class);
+
+    // Generated UserDefineTypes metadata
+    methodBuilder.addStatement(
+        ""$1T<$2T> userDefinedType = context.getSession().getMetadata().getKeyspace(keyspaceId).flatMap(v -> v.getUserDefinedType(tableId))"",
+        Optional.class,
+        UserDefinedType.class);
+
+    generateFindMissingChecks(methodBuilder);
+
+    // Throw if there is not keyspace.table for defined entity
+    CodeBlock missingKeyspaceTableExceptionMessage =
+        CodeBlock.of(
+            ""String.format(\""There is no ks.table: %s.%s for the entity class: %s\"", keyspaceId, tableId, entityClassName)"");
+    methodBuilder.beginControlFlow(""else"");
+    methodBuilder.addStatement(
+        ""throw new $1T($2L)"", IllegalArgumentException.class, missingKeyspaceTableExceptionMessage);
+    methodBuilder.endControlFlow();
+
+    return Optional.of(methodBuilder.build());
+  }
+
+  private void generateFindMissingChecks(MethodSpec.Builder methodBuilder) {
+    Optional<TargetElement> targetElement =
+        Optional.ofNullable(entityTypeElement.getAnnotation(SchemaHint.class))
+            .map(SchemaHint::targetElement);
+
+    // if SchemaHint was not provided explicitly try to match TABLE, then fallback to UDT
+    if (!targetElement.isPresent()) {
+      findMissingColumnsInTable(methodBuilder);
+      findMissingColumnsInUdt(methodBuilder, true);
+    }
+    // if explicitly provided SchemaHint is TABLE, then generate only TABLE check
+    else if (targetElement.get().equals(TargetElement.TABLE)) {
+      findMissingColumnsInTable(methodBuilder);
+    }
+    // if explicitly provided SchemaHint is UDT, then generate only UDT check
+    else if (targetElement.get().equals(TargetElement.UDT)) {
+      findMissingColumnsInUdt(methodBuilder, false);
+    }
+  }
+
+  // Finds out missingTableCqlNames - columns that are present in Entity Mapping but NOT present in
+  // CQL table
+  private void findMissingColumnsInTable(MethodSpec.Builder methodBuilder) {","[{'comment': 'Nit: this also validates the types now, so the name is not accurate.', 'commenter': 'olim7t'}]"
1300,mapper-runtime/src/main/java/com/datastax/oss/driver/internal/mapper/entity/EntityHelperBase.java,"@@ -75,4 +86,70 @@ protected void throwIfKeyspaceMissing() {
               DaoKeyspace.class.getSimpleName()));
     }
   }
+
+  public List<CqlIdentifier> findMissingColumns(
+      List<CqlIdentifier> entityColumns, Collection<ColumnMetadata> cqlColumns) {
+    return findMissingColumnsCql(
+        entityColumns,
+        cqlColumns.stream().map(ColumnMetadata::getName).collect(Collectors.toList()));
+  }
+
+  public List<CqlIdentifier> findMissingColumnsCql(
+      List<CqlIdentifier> entityColumns, Collection<CqlIdentifier> cqlColumns) {
+    List<CqlIdentifier> missingColumns = new ArrayList<>();
+    for (CqlIdentifier entityCqlIdentifier : entityColumns) {
+      if (!cqlColumns.contains(entityCqlIdentifier)) {
+        missingColumns.add(entityCqlIdentifier);
+      }
+    }
+    return missingColumns;
+  }
+
+  /**
+   * When the new instance of a class annotated with {@link Dao} is created an automatic check for
+   * schema validation is performed. It verifies if all {@link Dao} entity fields are present in CQL
+   * table. If not the {@link IllegalArgumentException} exception with detailed message is thrown.
+   * This check has startup overhead so once your app is stable you may want to disable it. The
+   * schema validation check is enabled by default. It can be disabled using the {@link
+   * MapperBuilder#withSchemaValidationEnabled(boolean)} method.
+   */
+  public abstract void validateEntityFields();
+
+  public static List<String> findMissingTypes(
+      Map<CqlIdentifier, GenericType<?>> entityColumns,
+      Map<CqlIdentifier, ColumnMetadata> cqlColumns,
+      CodecRegistry codecRegistry) {
+    List<String> missingCodecs = new ArrayList<>();
+
+    for (Map.Entry<CqlIdentifier, GenericType<?>> entityEntry : entityColumns.entrySet()) {
+      ColumnMetadata columnMetadata = cqlColumns.get(entityEntry.getKey());
+      if (columnMetadata == null) {
+        // this will not happen because it will be catch by the generateMissingColumnsCheck() method
+        throw new IllegalArgumentException(
+            ""There is no cql column for entity column: "" + entityEntry.getKey());","[{'comment': 'For this kind of impossible case, I think `AssertionError` is the most appropriate type to throw.', 'commenter': 'olim7t'}]"
1300,mapper-runtime/src/main/java/com/datastax/oss/driver/internal/mapper/entity/EntityHelperBase.java,"@@ -75,4 +86,70 @@ protected void throwIfKeyspaceMissing() {
               DaoKeyspace.class.getSimpleName()));
     }
   }
+
+  public List<CqlIdentifier> findMissingColumns(
+      List<CqlIdentifier> entityColumns, Collection<ColumnMetadata> cqlColumns) {
+    return findMissingColumnsCql(
+        entityColumns,
+        cqlColumns.stream().map(ColumnMetadata::getName).collect(Collectors.toList()));
+  }
+
+  public List<CqlIdentifier> findMissingColumnsCql(
+      List<CqlIdentifier> entityColumns, Collection<CqlIdentifier> cqlColumns) {
+    List<CqlIdentifier> missingColumns = new ArrayList<>();
+    for (CqlIdentifier entityCqlIdentifier : entityColumns) {
+      if (!cqlColumns.contains(entityCqlIdentifier)) {
+        missingColumns.add(entityCqlIdentifier);
+      }
+    }
+    return missingColumns;
+  }
+
+  /**
+   * When the new instance of a class annotated with {@link Dao} is created an automatic check for
+   * schema validation is performed. It verifies if all {@link Dao} entity fields are present in CQL
+   * table. If not the {@link IllegalArgumentException} exception with detailed message is thrown.
+   * This check has startup overhead so once your app is stable you may want to disable it. The
+   * schema validation check is enabled by default. It can be disabled using the {@link
+   * MapperBuilder#withSchemaValidationEnabled(boolean)} method.
+   */
+  public abstract void validateEntityFields();
+
+  public static List<String> findMissingTypes(","[{'comment': 'Nit: it\'s not really that the types are ""missing"", more that they don\'t match. Maybe `findTypeMismatches`?', 'commenter': 'olim7t'}]"
1300,mapper-runtime/src/main/java/com/datastax/oss/driver/internal/mapper/entity/EntityHelperBase.java,"@@ -75,4 +86,70 @@ protected void throwIfKeyspaceMissing() {
               DaoKeyspace.class.getSimpleName()));
     }
   }
+
+  public List<CqlIdentifier> findMissingColumns(
+      List<CqlIdentifier> entityColumns, Collection<ColumnMetadata> cqlColumns) {
+    return findMissingColumnsCql(
+        entityColumns,
+        cqlColumns.stream().map(ColumnMetadata::getName).collect(Collectors.toList()));
+  }
+
+  public List<CqlIdentifier> findMissingColumnsCql(
+      List<CqlIdentifier> entityColumns, Collection<CqlIdentifier> cqlColumns) {
+    List<CqlIdentifier> missingColumns = new ArrayList<>();
+    for (CqlIdentifier entityCqlIdentifier : entityColumns) {
+      if (!cqlColumns.contains(entityCqlIdentifier)) {
+        missingColumns.add(entityCqlIdentifier);
+      }
+    }
+    return missingColumns;
+  }
+
+  /**
+   * When the new instance of a class annotated with {@link Dao} is created an automatic check for
+   * schema validation is performed. It verifies if all {@link Dao} entity fields are present in CQL
+   * table. If not the {@link IllegalArgumentException} exception with detailed message is thrown.
+   * This check has startup overhead so once your app is stable you may want to disable it. The
+   * schema validation check is enabled by default. It can be disabled using the {@link
+   * MapperBuilder#withSchemaValidationEnabled(boolean)} method.
+   */
+  public abstract void validateEntityFields();
+
+  public static List<String> findMissingTypes(
+      Map<CqlIdentifier, GenericType<?>> entityColumns,
+      Map<CqlIdentifier, ColumnMetadata> cqlColumns,
+      CodecRegistry codecRegistry) {
+    List<String> missingCodecs = new ArrayList<>();
+
+    for (Map.Entry<CqlIdentifier, GenericType<?>> entityEntry : entityColumns.entrySet()) {
+      ColumnMetadata columnMetadata = cqlColumns.get(entityEntry.getKey());
+      if (columnMetadata == null) {
+        // this will not happen because it will be catch by the generateMissingColumnsCheck() method
+        throw new IllegalArgumentException(
+            ""There is no cql column for entity column: "" + entityEntry.getKey());
+      }
+      try {
+        codecRegistry.codecFor(columnMetadata.getType(), entityEntry.getValue());
+      } catch (CodecNotFoundException exception) {
+        missingCodecs.add(
+            String.format(
+                ""Field: %s, Entity Type: %s, CQL table type: %s"",
+                entityEntry.getKey(), exception.getJavaType(), exception.getCqlType()));
+      }
+    }
+    return missingCodecs;
+  }
+
+  public void throwMissingTypesIfNotEmpty(
+      List<String> missingTypes,
+      CqlIdentifier keyspaceId,
+      CqlIdentifier tableId,
+      String entityClassName) {
+    if (!missingTypes.isEmpty()) {
+      throw new IllegalArgumentException(
+          String.format(
+              ""The CQL ks.table: %s.%s defined in the entity class: %s has wrong types:\n%s"",
+              keyspaceId, tableId, entityClassName, String.join(""\n"", missingTypes)));","[{'comment': '""has wrong types"" => ""declares type mappings that are not supported by the codec registry""', 'commenter': 'olim7t'}]"
1300,changelog/README.md,"@@ -4,6 +4,7 @@
 
 ### 4.4.0 (in progress)
 
+- [new feature] JAVA-2263: Add optional schema validation ","[{'comment': ""Since there's less context here:\r\n```suggestion\r\n- [new feature] JAVA-2263: Add optional schema validation to the mapper\r\n```"", 'commenter': 'olim7t'}]"
1300,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/entity/EntityHelperSchemaValidationMethodGenerator.java,"@@ -109,18 +109,25 @@ public EntityHelperSchemaValidationMethodGenerator(
 
     generateValidationChecks(methodBuilder);
 
-    // Throw if there is not keyspace.table for defined entity
-    CodeBlock missingKeyspaceTableExceptionMessage =
-        CodeBlock.of(
-            ""String.format(\""There is no ks.table: %s.%s for the entity class: %s\"", keyspaceId, tableId, entityClassName)"");
-    methodBuilder.beginControlFlow(""else"");
-    methodBuilder.addStatement(
-        ""throw new $1T($2L)"", IllegalArgumentException.class, missingKeyspaceTableExceptionMessage);
-    methodBuilder.endControlFlow();
+    logMissingMetadata(methodBuilder);
 
     return Optional.of(methodBuilder.build());
   }
 
+  private void logMissingMetadata(MethodSpec.Builder methodBuilder) {
+    methodBuilder.addComment(","[{'comment': ""There were some failures of https://jenkins-drivers.build.dsinternal.org/job/datastax.java-driver.java2263_rebase.default/lastCompletedBuild/testReport/ `DefaultKeyspaceId`, it reported that the table is not present (it fails only on jenkins) I suspect that there is some kind of race condition in metadata and this is related to your comment:\r\n>keyspace name present but metadata is out of date\r\n\r\nI don't know how can we alleviate that problem - maybe we could refresh the schema explicitly?\r\nIf not, I think the viable solution is to `log.warning`:\r\n`warn if there are is keyspace.table for defined entity - it means that table is missing, or schema it out of date.`\r\ninstead of throwing an exception and fail fast if we cannot be sure if the schema was out-dated."", 'commenter': 'tomekl007'}]"
1302,core/src/main/java/com/datastax/oss/driver/api/core/data/ByteUtils.java,"@@ -0,0 +1,89 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core.data;
+
+import com.datastax.oss.protocol.internal.util.Bytes;
+import java.nio.ByteBuffer;
+
+/**
+ * A set of static utility methods to work with byte buffers (associated with CQL type {@code
+ * blob}).
+ */
+public class ByteUtils {
+
+  // Implementation note: this is just a gateway to the internal `Bytes` class in native-protocol.
+  // The difference is that this one is part of the public API.
+
+  /**
+   * Converts a blob to its CQL hex string representation.
+   *
+   * <p>A CQL blob string representation consists of the hexadecimal representation of the blob
+   * bytes prefixed by ""0x"".
+   *
+   * @param bytes the blob/bytes to convert to a string.
+   * @return the CQL string representation of {@code bytes}. If {@code bytes} is {@code null}, this
+   *     method returns {@code null}.
+   */
+  public static String toHexString(ByteBuffer bytes) {
+    return Bytes.toHexString(bytes);
+  }
+
+  /**
+   * Converts a blob to its CQL hex string representation.
+   *
+   * <p>A CQL blob string representation consists of the hexadecimal representation of the blob
+   * bytes prefixed by ""0x"".
+   *
+   * @param byteArray the blob/bytes array to convert to a string.
+   * @return the CQL string representation of {@code bytes}. If {@code bytes} is {@code null}, this
+   *     method returns {@code null}.
+   */
+  public static String toHexString(byte[] byteArray) {
+    return Bytes.toHexString(byteArray);
+  }
+
+  /**
+   * Parses a hex string representing a CQL blob.
+   *
+   * <p>The input should be a valid representation of a CQL blob, i.e. it must start by ""0x""
+   * followed by the hexadecimal representation of the blob bytes.
+   *
+   * @param str the CQL blob string representation to parse.
+   * @return the bytes corresponding to {@code str}. If {@code str} is {@code null}, this method
+   *     returns {@code null}.
+   * @throws IllegalArgumentException if {@code str} is not a valid CQL blob string.
+   */
+  public static ByteBuffer fromHexString(String str) {
+    return Bytes.fromHexString(str);
+  }
+
+  /**
+   * Extracts the content of the provided {@code ByteBuffer} as a byte array.
+   *
+   * <p>This method works with any type of {@code ByteBuffer} (direct and non-direct ones), but when
+   * the buffer is backed by an array, it will try to avoid copy when possible. As a consequence,
+   * changes to the returned byte array may or may not reflect into the initial buffer.
+   *
+   * @param bytes the buffer whose contents to extract.
+   * @return a byte array with the contents of {@code bytes}. That array may be the array backing
+   *     {@code bytes} if this can avoid a copy.
+   */
+  public static byte[] getArray(ByteBuffer bytes) {
+    return Bytes.getArray(bytes);
+  }","[{'comment': ""There is a new method `Bytes.erase` since JAVA-2306, but I'm not sure if it's worth including it here since it's pretty trivial and I doubt it's very useful in terms of manipulating BLOB values."", 'commenter': 'olim7t'}]"
1302,core/src/main/java/com/datastax/oss/driver/api/core/data/ByteUtils.java,"@@ -0,0 +1,89 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core.data;
+
+import com.datastax.oss.protocol.internal.util.Bytes;
+import java.nio.ByteBuffer;
+
+/**
+ * A set of static utility methods to work with byte buffers (associated with CQL type {@code
+ * blob}).
+ */
+public class ByteUtils {","[{'comment': 'I\'m still seeing `Bytes` being used in a lot of places in the java driver:\r\n\r\n```\r\n        java-driver-core  (24 usages found)\r\n            com.datastax.oss.driver.internal.core.cql  (5 usages found)\r\n                Conversions  (3 usages found)\r\n                    toMessage(Statement<?>, DriverExecutionProfile, InternalDriverContext)  (3 usages found)\r\n                        191 Bytes.getArray(id),\r\n                        192 (resultMetadataId == null) ? null : Bytes.getArray(resultMetadataId),\r\n                        222 queriesOrIds.add(Bytes.getArray(boundStatement.getPreparedStatement().getId()));\r\n                CqlRequestHandler.NodeResponseCallback  (1 usage found)\r\n                    processErrorResponse(Error)  (1 usage found)\r\n                        655 Bytes.toHexString(id)));\r\n                DefaultRow.SerializationProxy  (1 usage found)\r\n                    SerializationProxy(DefaultRow)  (1 usage found)\r\n                        154 this.values[i] = (buffer == null) ? null : Bytes.getArray(buffer);\r\n            com.datastax.oss.driver.internal.core.data  (2 usages found)\r\n                DefaultTupleValue.SerializationProxy  (1 usage found)\r\n                    SerializationProxy(DefaultTupleValue)  (1 usage found)\r\n                        187 this.values[i] = (buffer == null) ? null : Bytes.getArray(buffer);\r\n                DefaultUdtValue.SerializationProxy  (1 usage found)\r\n                    SerializationProxy(DefaultUdtValue)  (1 usage found)\r\n                        204 this.values[i] = (buffer == null) ? null : Bytes.getArray(buffer);\r\n            com.datastax.oss.driver.internal.core.metadata.schema.parsing  (2 usages found)\r\n                DataTypeClassNameParser  (1 usage found)\r\n                    parse(CqlIdentifier, String, Map<CqlIdentifier, UserDefinedType>, InternalDriverContext)  (1 usage found)\r\n                        107 Bytes.fromHexString(""0x"" + parser.readOne()), context.getProtocolVersion());\r\n                DataTypeClassNameParser.Parser  (1 usage found)\r\n                    getNameAndTypeParameters()  (1 usage found)\r\n                        269 Bytes.fromHexString(""0x"" + bbHex), DefaultProtocolVersion.DEFAULT);\r\n            com.datastax.oss.driver.internal.core.metadata.token  (6 usages found)\r\n                ByteOrderedToken  (3 usages found)\r\n                    ByteOrderedToken(ByteBuffer)  (1 usage found)\r\n                        33 this.value = ByteBuffer.wrap(Bytes.getArray(value)).asReadOnlyBuffer();\r\n                    compareTo(Token)  (1 usage found)\r\n                        63 .compare(Bytes.getArray(value), Bytes.getArray(((ByteOrderedToken) other).value));\r\n                    toString()  (1 usage found)\r\n                        68 return ""ByteOrderedToken("" + Bytes.toHexString(value) + "")"";\r\n                ByteOrderedTokenFactory  (2 usages found)\r\n                    parse(String)  (1 usage found)\r\n                        51 ByteBuffer value = Bytes.fromHexString(tokenString);\r\n                    format(Token)  (1 usage found)\r\n                        59 return Bytes.toHexString(((ByteOrderedToken) token).getValue());\r\n                ByteOrderedTokenRange  (1 usage found)\r\n                    toBigInteger(ByteBuffer, int)  (1 usage found)\r\n                        108 byte[] bytes = Bytes.getArray(bb);\r\n            com.datastax.oss.driver.internal.core.session  (1 usage found)\r\n                ReprepareOnUp  (1 usage found)\r\n                    gatherPayloadsToReprepare()  (1 usage found)\r\n                        176 Bytes.toHexString(payload.id));\r\n            com.datastax.oss.driver.internal.core.type.codec  (8 usages found)\r\n                BlobCodec  (2 usages found)\r\n                    format(ByteBuffer)  (1 usage found)\r\n                        68 return (value == null) ? ""NULL"" : Bytes.toHexString(value);\r\n                    parse(String)  (1 usage found)\r\n                        76 : Bytes.fromHexString(value);\r\n                CqlDurationCodec  (1 usage found)\r\n                    decode(ByteBuffer, ProtocolVersion)  (1 usage found)\r\n                        90 DataInput in = ByteStreams.newDataInput(Bytes.getArray(bytes));\r\n                CustomCodec  (2 usages found)\r\n                    format(ByteBuffer)  (1 usage found)\r\n                        75 return (value == null) ? ""NULL"" : Bytes.toHexString(value);\r\n                    parse(String)  (1 usage found)\r\n                        83 : Bytes.fromHexString(value);\r\n                InetCodec  (1 usage found)\r\n                    decode(ByteBuffer, ProtocolVersion)  (1 usage found)\r\n                        69 return InetAddress.getByAddress(Bytes.getArray(bytes));\r\n                StringCodec  (1 usage found)\r\n                    decode(ByteBuffer, ProtocolVersion)  (1 usage found)\r\n                        77 return new String(Bytes.getArray(bytes), charset);\r\n                VarIntCodec  (1 usage found)\r\n                    decode(ByteBuffer, ProtocolVersion)  (1 usage found)\r\n                        63 return (bytes == null) || bytes.remaining() == 0 ? null : new BigInteger(Bytes.getArray(bytes));\r\n        java-driver-examples  (4 usages found)\r\n            com.datastax.oss.driver.examples.json.codecs  (2 usages found)\r\n                JacksonJsonCodec  (1 usage found)\r\n                    decode(ByteBuffer, ProtocolVersion)  (1 usage found)\r\n                        111 return objectMapper.readValue(Bytes.getArray(bytes), toJacksonJavaType());\r\n                Jsr353JsonCodec  (1 usage found)\r\n                    decode(ByteBuffer, ProtocolVersion)  (1 usage found)\r\n                        152 try (ByteArrayInputStream bais = new ByteArrayInputStream(Bytes.getArray(bytes))) {\r\n            com.datastax.oss.driver.examples.paging  (2 usages found)\r\n                ForwardPagingRestUi.UserService  (2 usages found)\r\n                    getUserVideos(int, String)  (2 usages found)\r\n                        221 statementBuilder.setPagingState(Bytes.fromHexString(page));\r\n                        225 String nextPage = Bytes.toHexString(rs.getExecutionInfo().getPagingState());\r\n```\r\nThere are also usages in test classes.\r\nThere are also usages in the DSE driver, we will have to keep in mind to change that as well.', 'commenter': 'adutra'}, {'comment': ""I don't think we need to change those. The goal is to give 3rd-party users something they can rely on without importing from an `internal` package. But from our own code, using the internal class is fine."", 'commenter': 'olim7t'}]"
1303,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/TypeMemberMessager.java,"@@ -0,0 +1,97 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.mapper.processor;
+
+import edu.umd.cs.findbugs.annotations.NonNull;
+import javax.lang.model.element.Element;
+import javax.lang.model.element.ExecutableElement;
+import javax.lang.model.element.TypeElement;
+
+/**
+ * Handles warning and error messages for a processed method or field.
+ *
+ * <p>We don't issue those messages on the {@link ExecutableElement} directly, because that can lead
+ * to imprecise compiler messages if the member is inherited from an already compiled parent type.
+ * Consider the following situation:
+ *
+ * <pre>
+ *   interface BaseDao {
+ *     &#64;Select
+ *     void select();
+ *   }
+ *   &#64;Dao
+ *   interface ConcreteDao extends BaseDao {}
+ * </pre>
+ *
+ * If {@code BaseDao} belongs to a JAR dependency, it is already compiled and the warning or error
+ * message can't reference a file or line number, it doesn't even mention {@code ConcreteDao}.
+ *
+ * <p>The goal of this class is to detect those cases, and issue the message on {@code ConcreteDao}
+ * instead.
+ */
+public class TypeMemberMessager {
+
+  private final DecoratedMessager delegate;
+
+  // The element where we'll issue the message
+  private final Element targetElement;
+  // Additional location information that will get prepended to every message
+  private final String locationInfo;
+
+  /**
+   * @param processedType the type that we are currently processing ({@code ConcreteDao} in the
+   *     example above).
+   */
+  public TypeMemberMessager(
+      @NonNull Element member,
+      @NonNull TypeElement processedType,
+      @NonNull ProcessorContext context) {
+
+    this.delegate = context.getMessager();
+
+    // The type that declares the method (BaseDao in the example above). That's the one that might
+    // be already compiled.
+    Element declaringType = member.getEnclosingElement();
+    assert declaringType instanceof TypeElement : ""member must be a field or method"";","[{'comment': ""According to documentation `TypeElement`: `Represents a class or interface program element`.\r\nSo this statement: `member must be a field or method` don't seems to be true"", 'commenter': 'tomekl007'}, {'comment': 'The assertion is not on `member`, but on `member.getEnclosingElement()`. If `member` is a field or method, its enclosing element is a class or interface.', 'commenter': 'olim7t'}]"
1303,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/TypeMemberMessager.java,"@@ -0,0 +1,97 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.mapper.processor;
+
+import edu.umd.cs.findbugs.annotations.NonNull;
+import javax.lang.model.element.Element;
+import javax.lang.model.element.ExecutableElement;
+import javax.lang.model.element.TypeElement;
+
+/**
+ * Handles warning and error messages for a processed method or field.
+ *
+ * <p>We don't issue those messages on the {@link ExecutableElement} directly, because that can lead
+ * to imprecise compiler messages if the member is inherited from an already compiled parent type.
+ * Consider the following situation:
+ *
+ * <pre>
+ *   interface BaseDao {
+ *     &#64;Select
+ *     void select();
+ *   }
+ *   &#64;Dao
+ *   interface ConcreteDao extends BaseDao {}
+ * </pre>
+ *
+ * If {@code BaseDao} belongs to a JAR dependency, it is already compiled and the warning or error
+ * message can't reference a file or line number, it doesn't even mention {@code ConcreteDao}.
+ *
+ * <p>The goal of this class is to detect those cases, and issue the message on {@code ConcreteDao}
+ * instead.
+ */
+public class TypeMemberMessager {
+
+  private final DecoratedMessager delegate;
+
+  // The element where we'll issue the message
+  private final Element targetElement;
+  // Additional location information that will get prepended to every message
+  private final String locationInfo;
+
+  /**
+   * @param processedType the type that we are currently processing ({@code ConcreteDao} in the
+   *     example above).
+   */
+  public TypeMemberMessager(
+      @NonNull Element member,
+      @NonNull TypeElement processedType,
+      @NonNull ProcessorContext context) {
+
+    this.delegate = context.getMessager();
+
+    // The type that declares the method (BaseDao in the example above). That's the one that might
+    // be already compiled.
+    Element declaringType = member.getEnclosingElement();
+    assert declaringType instanceof TypeElement : ""member must be a field or method"";
+
+    if (processedType.equals(declaringType) || isSourceFile((TypeElement) declaringType)) {
+      this.targetElement = member;
+      this.locationInfo = """";
+    } else {
+      this.targetElement = processedType;
+      this.locationInfo =
+          String.format(""[%s inherited from %s] "", member, declaringType.getSimpleName());
+    }","[{'comment': 'what will happen if we will have such inheritance:\r\n```\r\ninterface Parent {\r\n\t@Update u();\r\n}\r\n\r\ninterface BaseDao extends Parent{\r\n@Select\r\n   void select();\r\n}\r\n@Dao\r\ninterface ConcreteDao extends BaseDao {}\r\n```\r\nand:\r\n- only the Parent belongs to Jar dependency\r\n- BaseDao and Parent belongs to Jar dependency\r\n- BaseDao belongs to Jar dependency and Parent does not (I think it is impossible but just asking)\r\nAre we able to catch such a hierarchy and add error info about all classes in a chain?\r\nIf it is already supported, maybe we can add an integration test that covers this?', 'commenter': 'tomekl007'}, {'comment': 'Yes, those cases will be handled correctly (and the last one is impossible indeed).\r\n\r\nCheck out `DefaultEntityFactory` and `DaoImplementationGenerator`. In both cases, we get all the parent types as a flat structure. What matters is the class we are currently processing (`processedClass`), and the type where we are inheriting the member from (`declaringType`). How many types there are in between has no impact on the algorithm used here.', 'commenter': 'olim7t'}]"
1303,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/TypeMemberMessager.java,"@@ -0,0 +1,97 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.mapper.processor;
+
+import edu.umd.cs.findbugs.annotations.NonNull;
+import javax.lang.model.element.Element;
+import javax.lang.model.element.ExecutableElement;
+import javax.lang.model.element.TypeElement;
+
+/**
+ * Handles warning and error messages for a processed method or field.
+ *
+ * <p>We don't issue those messages on the {@link ExecutableElement} directly, because that can lead
+ * to imprecise compiler messages if the member is inherited from an already compiled parent type.
+ * Consider the following situation:
+ *
+ * <pre>
+ *   interface BaseDao {
+ *     &#64;Select
+ *     void select();
+ *   }
+ *   &#64;Dao
+ *   interface ConcreteDao extends BaseDao {}
+ * </pre>
+ *
+ * If {@code BaseDao} belongs to a JAR dependency, it is already compiled and the warning or error
+ * message can't reference a file or line number, it doesn't even mention {@code ConcreteDao}.
+ *
+ * <p>The goal of this class is to detect those cases, and issue the message on {@code ConcreteDao}
+ * instead.
+ */
+public class TypeMemberMessager {
+
+  private final DecoratedMessager delegate;
+
+  // The element where we'll issue the message
+  private final Element targetElement;
+  // Additional location information that will get prepended to every message
+  private final String locationInfo;
+
+  /**
+   * @param processedType the type that we are currently processing ({@code ConcreteDao} in the
+   *     example above).
+   */
+  public TypeMemberMessager(
+      @NonNull Element member,
+      @NonNull TypeElement processedType,
+      @NonNull ProcessorContext context) {
+
+    this.delegate = context.getMessager();
+
+    // The type that declares the method (BaseDao in the example above). That's the one that might
+    // be already compiled.
+    Element declaringType = member.getEnclosingElement();
+    assert declaringType instanceof TypeElement : ""member must be a field or method"";
+
+    if (processedType.equals(declaringType) || isSourceFile((TypeElement) declaringType)) {","[{'comment': 'I\'m wonder if the current design is the best. If I understand correctly, if `processedType.equals(declaringType) || isSourceFile((TypeElement) declaringType)` is true, this class will behave exactly like `DecoratedMessager`. So wouldn\'t be a better design the make this class extend `DecoratedMessager` and add a factory method somewhere that will determine if it should create a ""vanilla"" `DecoratedMessager` or the special `TypeMemberMessager` for cases where the member in question is pre-compiled?\r\nAlso wouldn\'t `PreCompiledMessager` be a better name?', 'commenter': 'adutra'}, {'comment': ""Indeed, a weakness of the current design is that it relies on us using `TypeMemberMessager` instead of the default messager when necessary.\r\n\r\nMaybe I could instead have all methods in `DecoratedMessager` receive `Element targetElement, TypeElement processedClass`, and do the detection every time.\r\n\r\nI'll take a shot at it."", 'commenter': 'olim7t'}, {'comment': 'Done, this looks much cleaner indeed. LMK what you think.', 'commenter': 'olim7t'}]"
1303,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoDeleteMethodGenerator.java,"@@ -70,12 +72,9 @@ public DaoDeleteMethodGenerator(
     Delete annotation = methodElement.getAnnotation(Delete.class);
     assert annotation != null;
     if (annotation.ifExists() && !annotation.customIfClause().isEmpty()) {
-      context
-          .getMessager()
-          .error(
-              methodElement,
-              ""Invalid annotation parameters: %s cannot have both ifExists and customIfClause"",
-              Delete.class.getSimpleName());
+      methodMessager.error(","[{'comment': ""Possibly related to my previous comment: wouldn't it be possible to obtain the messager from the context in all cases, and let the context decide which specific implementation to use?"", 'commenter': 'adutra'}]"
1303,mapper-processor/src/test/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoDeleteMethodGeneratorTest.java,"@@ -145,8 +145,7 @@ public void should_fail_with_expected_error(
   @Test
   public void should_warn_when_non_bind_marker_has_cql_name() {
     should_succeed_with_expected_warning(
-        ""delete(java.util.UUID,java.lang.String): parameter id does not refer ""
-            + ""to a bind marker, @CqlName annotation will be ignored"",
+        ""Parameter id does not refer "" + ""to a bind marker, @CqlName annotation will be ignored"",","[{'comment': 'Nit: strings can be joined.', 'commenter': 'adutra'}]"
1303,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/DecoratedMessager.java,"@@ -28,15 +31,146 @@ public DecoratedMessager(Messager messager) {
     this.messager = messager;
   }
 
-  public void warn(Element element, String template, Object... arguments) {
-    messager.printMessage(Diagnostic.Kind.WARNING, String.format(template, arguments), element);
-  }
-
+  /** Emits a global warning that doesn't target a particular element. */
   public void warn(String template, Object... arguments) {
     messager.printMessage(Diagnostic.Kind.WARNING, String.format(template, arguments));
   }
 
-  public void error(Element element, String template, Object... arguments) {
-    messager.printMessage(Diagnostic.Kind.ERROR, String.format(template, arguments), element);
+  /** Emits a warning for a type. */
+  public void warn(TypeElement typeElement, String template, Object... arguments) {
+    messager.printMessage(Diagnostic.Kind.WARNING, String.format(template, arguments), typeElement);
+  }
+
+  /** Emits an error for a type. */
+  public void error(TypeElement typeElement, String template, Object... arguments) {
+    messager.printMessage(Diagnostic.Kind.ERROR, String.format(template, arguments), typeElement);
+  }
+
+  /**
+   * Emits a warning for a program element that might be inherited from another type.
+   *
+   * @param targetElement the element to target.
+   * @param processedType the type that we were processing when we detected the issue.
+   */
+  public void warn(
+      Element targetElement, TypeElement processedType, String template, Object... arguments) {
+    new ElementMessager(targetElement, processedType)
+        .print(Diagnostic.Kind.WARNING, template, arguments);
+  }
+
+  /**
+   * Emits an error for a program element that might be inherited from another type.
+   *
+   * @param targetElement the element to target.
+   * @param processedType the type that we were processing when we detected the issue.
+   */
+  public void error(
+      Element targetElement, TypeElement processedType, String template, Object... arguments) {
+    new ElementMessager(targetElement, processedType)
+        .print(Diagnostic.Kind.ERROR, template, arguments);
+  }
+
+  /**
+   * Abstracts logic to produce better messages if the target element is inherited from a compiled
+   * type.
+   *
+   * <p>Consider the following situation:
+   *
+   * <pre>
+   *   interface BaseDao {
+   *     &#64;Select
+   *     void select();
+   *   }
+   *   &#64;Dao
+   *   interface ConcreteDao extends BaseDao {}
+   * </pre>
+   *
+   * If {@code BaseDao} belongs to a JAR dependency, it is already compiled and the warning or error
+   * message can't reference a file or line number, it doesn't even mention {@code ConcreteDao}.
+   *
+   * <p>The goal of this class is to detect those cases, and issue the message on {@code
+   * ConcreteDao} instead.
+   */
+  private class ElementMessager {
+
+    private final Element actualTargetElement;
+    // Additional location information that will get prepended to the message
+    private final String locationInfo;
+
+    /**
+     * @param processedType the type that we are currently processing ({@code ConcreteDao} in the
+     *     example above).
+     */
+    ElementMessager(@NonNull Element intendedTargetElement, @NonNull TypeElement processedType) {
+
+      TypeElement declaringType;
+      switch (intendedTargetElement.getKind()) {
+        case CLASS:
+        case INTERFACE:
+          if (processedType.equals(intendedTargetElement)
+              || isSourceFile((TypeElement) intendedTargetElement)) {
+            this.actualTargetElement = intendedTargetElement;
+            this.locationInfo = """";
+          } else {
+            this.actualTargetElement = processedType;
+            this.locationInfo =
+                String.format(""[Ancestor %s]"", intendedTargetElement.getSimpleName());
+          }
+          break;
+        case FIELD:
+        case METHOD:
+        case CONSTRUCTOR:
+          declaringType = (TypeElement) intendedTargetElement.getEnclosingElement();
+          if (processedType.equals(declaringType) || isSourceFile(declaringType)) {
+            this.actualTargetElement = intendedTargetElement;
+            this.locationInfo = """";
+          } else {
+            this.actualTargetElement = processedType;
+            this.locationInfo =
+                String.format(
+                    ""[%s inherited from %s] "",
+                    intendedTargetElement, declaringType.getSimpleName());
+          }
+          break;
+        case PARAMETER:
+          ExecutableElement method =
+              (ExecutableElement) intendedTargetElement.getEnclosingElement();
+          declaringType = (TypeElement) method.getEnclosingElement();
+          if (processedType.equals(declaringType) || isSourceFile(declaringType)) {
+            this.actualTargetElement = intendedTargetElement;
+            this.locationInfo = """";
+          } else {
+            this.actualTargetElement = processedType;
+            this.locationInfo =","[{'comment': ""The design is indeed much better now. However I'm not seeing any specific test exercising this case branch and the case branch above for classes and interfaces."", 'commenter': 'adutra'}, {'comment': 'I implemented those cases because there were likely to happen in the future, but currently there is no message in the processor that covers them.', 'commenter': 'olim7t'}]"
1304,query-builder/pom.xml,"@@ -71,6 +71,7 @@
         <artifactId>maven-bundle-plugin</artifactId>
         <configuration>
           <instructions>
+            <Automatic-Module-Name>com.datastax.oss.driver.querybuilder</Automatic-Module-Name>","[{'comment': 'I think this needs to go to the jar-plugin configuration here as well.', 'commenter': 'olim7t'}]"
1304,core-shaded/pom.xml,"@@ -303,6 +303,7 @@
             </goals>
             <configuration>
               <instructions>
+                <Automatic-Module-Name>com.datastax.oss.driver.core</Automatic-Module-Name>","[{'comment': 'For the shaded core, we have to do this from the bundle plugin, because we generate a separate manifest file and include it manually in the final archive.', 'commenter': 'olim7t'}]"
1306,examples/src/main/java/com/datastax/oss/driver/examples/mapper/killrvideo/user/PasswordHashing.java,"@@ -0,0 +1,37 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.examples.mapper.killrvideo.user;
+
+import at.favre.lib.crypto.bcrypt.BCrypt;
+
+/**
+ * Utility methods to safely store passwords in the database.
+ *
+ * <p>We rely on a third-party implementation of the bcrypt password hash function.
+ *
+ * @see <a href=""https://github.com/patrickfav/bcrypt"">patrickfav/bcrypt</a>
+ */
+public class PasswordHashing {","[{'comment': 'A bit off-topic, but I figured we might as well show the proper way to do this.', 'commenter': 'olim7t'}, {'comment': 'I like it, this is certainly going to be very useful for our users. ðŸ‘ ', 'commenter': 'adutra'}]"
1306,examples/src/main/java/com/datastax/oss/driver/examples/mapper/killrvideo/user/CreateUserQueryProvider.java,"@@ -0,0 +1,103 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.examples.mapper.killrvideo.user;
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.cql.BoundStatementBuilder;
+import com.datastax.oss.driver.api.core.cql.PreparedStatement;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import com.datastax.oss.driver.api.mapper.MapperContext;
+import com.datastax.oss.driver.api.mapper.entity.EntityHelper;
+import com.datastax.oss.driver.api.mapper.entity.saving.NullSavingStrategy;
+import java.time.Instant;
+import java.util.Objects;
+import java.util.Optional;
+import java.util.UUID;
+
+/**
+ * Provides the implementation of {@link UserDao#create}.
+ *
+ * <p>Package-private visibility is sufficient, this will be called only from the generated DAO
+ * implementation.
+ */
+class CreateUserQueryProvider {
+
+  private final CqlSession session;
+  private final EntityHelper<User> userHelper;
+  private final EntityHelper<UserCredentials> credentialsHelper;
+  private final PreparedStatement preparedInsertCredentials;
+  private final PreparedStatement preparedDeleteCredentials;
+  private final PreparedStatement preparedInsertUser;
+
+  CreateUserQueryProvider(
+      MapperContext context,
+      EntityHelper<User> userHelper,
+      EntityHelper<UserCredentials> credentialsHelper) {
+
+    this.session = context.getSession();
+
+    this.userHelper = userHelper;
+    this.credentialsHelper = credentialsHelper;
+
+    this.preparedInsertCredentials =
+        session.prepare(credentialsHelper.insert().ifNotExists().asCql());
+    this.preparedDeleteCredentials =
+        session.prepare(credentialsHelper.deleteByPrimaryKey().asCql());
+    this.preparedInsertUser = session.prepare(userHelper.insert().ifNotExists().asCql());
+  }
+
+  Optional<User> create(String firstname, String lastname, String email, char[] password) {
+    Instant now = Instant.now();
+    while (true) {
+      UUID userid = UUID.randomUUID();
+
+      if (!insertCredentialsIfNotExists(email, password, userid)) {
+        // email already exists
+        return Optional.empty();
+      }
+
+      User user = new User(userid, firstname, lastname, email, now);
+      if (insertUserIfNotExists(user)) {
+        return Optional.of(user);
+      } else {
+        // The only way the insert can fail is a UUID collision. This is highly unlikely, but handle","[{'comment': 'IMO there is no need for this. ""Highly unlikely"", according to Wikipedia, is actually virtually impossible: ""for there to be a one in a billion chance of duplication, 103 trillion version 4 UUIDs must be generated."" And since `UUID.random()` uses `SecureRandom` behind the scenes, I think it is safe to assume that the Java implementation has the same probability of collision.\r\n\r\nI suggest that we either do not instruct users to care about UUID collisions, or that we switch to `timeuuid` and use `Uuids.timeBased()` to create UUIDs.', 'commenter': 'adutra'}, {'comment': ""ðŸ‘\r\nI wasn't sure about this (I didn't do it for videos). I'll remove the loop."", 'commenter': 'olim7t'}]"
1306,examples/src/main/java/com/datastax/oss/driver/examples/mapper/killrvideo/user/CreateUserQueryProvider.java,"@@ -0,0 +1,103 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.examples.mapper.killrvideo.user;
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.cql.BoundStatementBuilder;
+import com.datastax.oss.driver.api.core.cql.PreparedStatement;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import com.datastax.oss.driver.api.mapper.MapperContext;
+import com.datastax.oss.driver.api.mapper.entity.EntityHelper;
+import com.datastax.oss.driver.api.mapper.entity.saving.NullSavingStrategy;
+import java.time.Instant;
+import java.util.Objects;
+import java.util.Optional;
+import java.util.UUID;
+
+/**
+ * Provides the implementation of {@link UserDao#create}.
+ *
+ * <p>Package-private visibility is sufficient, this will be called only from the generated DAO
+ * implementation.
+ */
+class CreateUserQueryProvider {
+
+  private final CqlSession session;
+  private final EntityHelper<User> userHelper;
+  private final EntityHelper<UserCredentials> credentialsHelper;
+  private final PreparedStatement preparedInsertCredentials;
+  private final PreparedStatement preparedDeleteCredentials;
+  private final PreparedStatement preparedInsertUser;
+
+  CreateUserQueryProvider(
+      MapperContext context,
+      EntityHelper<User> userHelper,
+      EntityHelper<UserCredentials> credentialsHelper) {
+
+    this.session = context.getSession();
+
+    this.userHelper = userHelper;
+    this.credentialsHelper = credentialsHelper;
+
+    this.preparedInsertCredentials =
+        session.prepare(credentialsHelper.insert().ifNotExists().asCql());
+    this.preparedDeleteCredentials =
+        session.prepare(credentialsHelper.deleteByPrimaryKey().asCql());
+    this.preparedInsertUser = session.prepare(userHelper.insert().ifNotExists().asCql());
+  }
+
+  Optional<User> create(String firstname, String lastname, String email, char[] password) {","[{'comment': 'Why not use a logged batch to ensure atomicity?', 'commenter': 'adutra'}, {'comment': 'The fact that Optional<User>.empty() is returned when the actual user is present in the db seems a bit odd.\r\nI wonder if there are other options or this is an idiomatic way to handle that?', 'commenter': 'tomekl007'}, {'comment': ""A batch containing conditional updates can operate only within a single partition, we're inserting in two different tables."", 'commenter': 'olim7t'}, {'comment': 'The reason I wanted to return the user is because the method generates the id. But since creating an id is trivial anyway, maybe we can take it as a parameter, and the method will just return a boolean.', 'commenter': 'olim7t'}]"
1306,examples/src/main/java/com/datastax/oss/driver/examples/mapper/KillrVideoMapperExample.java,"@@ -0,0 +1,148 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.examples.mapper;
+
+import com.datastax.oss.driver.api.core.CqlIdentifier;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.PagingIterable;
+import com.datastax.oss.driver.examples.mapper.killrvideo.KillrVideoMapper;
+import com.datastax.oss.driver.examples.mapper.killrvideo.user.User;
+import com.datastax.oss.driver.examples.mapper.killrvideo.user.UserDao;
+import com.datastax.oss.driver.examples.mapper.killrvideo.video.LatestVideo;
+import com.datastax.oss.driver.examples.mapper.killrvideo.video.UserVideo;
+import com.datastax.oss.driver.examples.mapper.killrvideo.video.Video;
+import com.datastax.oss.driver.examples.mapper.killrvideo.video.VideoByTag;
+import com.datastax.oss.driver.examples.mapper.killrvideo.video.VideoDao;
+import java.time.Instant;
+import java.time.ZoneOffset;
+import java.time.format.DateTimeFormatter;
+import java.util.HashSet;
+import java.util.Optional;
+import java.util.Set;
+
+/**
+ * Uses the driver's object mapper to interact with a schema.
+ *
+ * <p>We use the data model of the <a href=""https://killrvideo.github.io/"">KillrVideo</a> sample
+ * application. The mapped entities and DAOs are in the {@link
+ * com.datastax.oss.driver.examples.mapper.killrvideo} package. We only cover a subset of the data
+ * model (ratings, stats, recommendations and comments are not covered).
+ *
+ * <p>Preconditions:
+ *
+ * <ul>
+ *   <li>An Apache Cassandra(R) cluster is running and accessible through the contacts points
+ *       identified by basic.contact-points (see application.conf).
+ *   <li>that cluster has a {@code killrvideo} keyspace with the KillrVideo tables (see {@code
+ *       src/main/resources/killrvideo_schema.cql} in this project).
+ *   <li>the tables are empty.
+ * </ul>
+ *
+ * <p>Side effects:
+ *
+ * <ul>
+ *   <li>inserts data in the tables.
+ * </ul>
+ *
+ * @see <a href=""https://docs.datastax.com/en/developer/java-driver/latest/manual/mapper/"">Java
+ *     Driver Mapper manual</a>
+ */
+public class KillrVideoMapperExample {
+
+  private static final CqlIdentifier KEYSPACE_ID = CqlIdentifier.fromCql(""killrvideo"");
+
+  public static void main(String[] args) {
+    try (CqlSession session = CqlSession.builder().build()) {
+      KillrVideoMapper mapper =","[{'comment': 'For other examples, we are including prepare phase, that is creating keyspace and tables in an automatic way.\r\nFor example:\r\n```\r\n  private static void createSchema(CqlSession session) {\r\n    session.execute(\r\n        ""CREATE KEYSPACE IF NOT EXISTS examples ""\r\n            + ""WITH replication = {\'class\': \'SimpleStrategy\', \'replication_factor\': 1}"");\r\n    session.execute(\r\n        ""CREATE TABLE IF NOT EXISTS examples.forward_paging_rest_ui(""\r\n            + ""userid int, username text, ""\r\n            + ""added timestamp, ""\r\n            + ""videoid int, title text, ""\r\n            + ""PRIMARY KEY (userid, added, videoid)""\r\n            + "") WITH CLUSTERING ORDER BY (added DESC, videoid ASC)"");\r\n  }\r\n```\r\nI think we should do the same here.', 'commenter': 'tomekl007'}, {'comment': ""We can parse the CQL file and execute the statements I guess, I'll look into that."", 'commenter': 'olim7t'}]"
1306,examples/src/main/java/com/datastax/oss/driver/examples/mapper/killrvideo/video/LatestVideo.java,"@@ -0,0 +1,63 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.examples.mapper.killrvideo.video;
+
+import com.datastax.oss.driver.api.mapper.annotations.ClusteringColumn;
+import com.datastax.oss.driver.api.mapper.annotations.CqlName;
+import com.datastax.oss.driver.api.mapper.annotations.Entity;
+import com.datastax.oss.driver.api.mapper.annotations.PartitionKey;
+import java.time.Instant;
+import java.util.UUID;
+
+@Entity
+@CqlName(""latest_videos"")
+public class LatestVideo extends VideoBase {
+  @PartitionKey private String yyyymmdd;","[{'comment': 'This is semantically a CQL `date` type, why model it as a `varchar`?', 'commenter': 'adutra'}, {'comment': ""I'm not sure, I just reused the KillrVideo data model as-is, and it's how they do it. Maybe they wanted to support legacy versions that don't have `date`."", 'commenter': 'olim7t'}]"
1306,examples/src/main/java/com/datastax/oss/driver/examples/mapper/KillrVideoMapperExample.java,"@@ -0,0 +1,148 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.examples.mapper;
+
+import com.datastax.oss.driver.api.core.CqlIdentifier;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.PagingIterable;
+import com.datastax.oss.driver.examples.mapper.killrvideo.KillrVideoMapper;
+import com.datastax.oss.driver.examples.mapper.killrvideo.user.User;
+import com.datastax.oss.driver.examples.mapper.killrvideo.user.UserDao;
+import com.datastax.oss.driver.examples.mapper.killrvideo.video.LatestVideo;
+import com.datastax.oss.driver.examples.mapper.killrvideo.video.UserVideo;
+import com.datastax.oss.driver.examples.mapper.killrvideo.video.Video;
+import com.datastax.oss.driver.examples.mapper.killrvideo.video.VideoByTag;
+import com.datastax.oss.driver.examples.mapper.killrvideo.video.VideoDao;
+import java.time.Instant;
+import java.time.ZoneOffset;
+import java.time.format.DateTimeFormatter;
+import java.util.HashSet;
+import java.util.Optional;
+import java.util.Set;
+
+/**
+ * Uses the driver's object mapper to interact with a schema.
+ *
+ * <p>We use the data model of the <a href=""https://killrvideo.github.io/"">KillrVideo</a> sample
+ * application. The mapped entities and DAOs are in the {@link
+ * com.datastax.oss.driver.examples.mapper.killrvideo} package. We only cover a subset of the data
+ * model (ratings, stats, recommendations and comments are not covered).
+ *
+ * <p>Preconditions:
+ *
+ * <ul>
+ *   <li>An Apache Cassandra(R) cluster is running and accessible through the contacts points
+ *       identified by basic.contact-points (see application.conf).
+ *   <li>that cluster has a {@code killrvideo} keyspace with the KillrVideo tables (see {@code
+ *       src/main/resources/killrvideo_schema.cql} in this project).
+ *   <li>the tables are empty.
+ * </ul>
+ *
+ * <p>Side effects:
+ *
+ * <ul>
+ *   <li>inserts data in the tables.
+ * </ul>
+ *
+ * @see <a href=""https://docs.datastax.com/en/developer/java-driver/latest/manual/mapper/"">Java
+ *     Driver Mapper manual</a>
+ */
+public class KillrVideoMapperExample {
+
+  private static final CqlIdentifier KEYSPACE_ID = CqlIdentifier.fromCql(""killrvideo"");
+
+  public static void main(String[] args) {
+    try (CqlSession session = CqlSession.builder().build()) {
+      KillrVideoMapper mapper =
+          KillrVideoMapper.builder(session).withDefaultKeyspace(KEYSPACE_ID).build();
+
+      // Create a new user
+      UserDao userDao = mapper.userDao();
+
+      Optional<User> maybeUser =
+          userDao.create(""test"", ""user"", ""testuser@example.com"", ""password123"".toCharArray());
+      if (!maybeUser.isPresent()) {
+        throw new AssertionError(""User creation should succeed (expected empty tables)"");
+      }
+      User user = maybeUser.get();
+      System.out.println(""Created "" + user);
+
+      // Creating another user with the same email should fail
+      assert !userDao
+          .create(""test2"", ""user"", ""testuser@example.com"", ""secret123"".toCharArray())
+          .isPresent();
+
+      // Simulate login attempts
+      tryLogin(userDao, ""testuser@example.com"", ""password123"");
+      tryLogin(userDao, ""testuser@example.com"", ""secret123"");
+
+      // Insert a video
+      VideoDao videoDao = mapper.videoDao();
+
+      Video video = new Video();
+      video.setUserid(user.getUserid());
+      video.setName(
+          ""Getting Started with DataStax Apache Cassandra as a Service on DataStax Constellation"");
+      video.setLocation(""https://www.youtube.com/watch?v=68xzKpcZURA"");
+      Set<String> tags = new HashSet<>();
+      tags.add(""apachecassandra"");
+      tags.add(""nosql"");
+      tags.add(""hybridcloud"");
+      video.setTags(tags);
+
+      videoDao.create(video);
+      System.out.printf(""Created video [%s] %s%n"", video.getVideoid(), video.getName());
+
+      // Check that associated denormalized tables have also been updated:
+      PagingIterable<UserVideo> userVideos = videoDao.getByUser(user.getUserid());
+      System.out.printf(""Videos for %s %s:%n"", user.getFirstname(), user.getLastname());
+      for (UserVideo userVideo : userVideos) {
+        System.out.printf(""  [%s] %s%n"", userVideo.getVideoid(), userVideo.getName());
+      }
+
+      String yyyymmdd =
+          DateTimeFormatter.ofPattern(""yyyyMMdd"").withZone(ZoneOffset.UTC).format(Instant.now());","[{'comment': 'That `Instant.now()` is a bit non-intuitive. It is also embedded within the logic of `CreateVideoQueryProvider`.\r\nTo improve design maybe we could refactor it to use a time provider? Instead of hardcoding that in the logic?', 'commenter': 'tomekl007'}, {'comment': ""Yeah I admit it looks a bit weird in the middle of the client code. I'll extract it in a method and add a few explanations."", 'commenter': 'olim7t'}]"
1306,examples/src/main/java/com/datastax/oss/driver/examples/mapper/killrvideo/user/LoginQueryProvider.java,"@@ -0,0 +1,72 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.examples.mapper.killrvideo.user;
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.cql.PreparedStatement;
+import com.datastax.oss.driver.api.core.cql.Row;
+import com.datastax.oss.driver.api.mapper.MapperContext;
+import com.datastax.oss.driver.api.mapper.entity.EntityHelper;
+import java.util.Optional;
+import java.util.UUID;
+
+/**
+ * Provides the implementation of {@link UserDao#login}.
+ *
+ * <p>Package-private visibility is sufficient, this will be called only from the generated DAO
+ * implementation.
+ */
+class LoginQueryProvider {
+
+  private final CqlSession session;
+  private final EntityHelper<User> userHelper;
+  private final PreparedStatement preparedSelectCredentials;
+  private final PreparedStatement preparedSelectUser;
+
+  LoginQueryProvider(
+      MapperContext context,
+      EntityHelper<User> userHelper,
+      EntityHelper<UserCredentials> credentialsHelper) {
+
+    this.session = context.getSession();
+
+    this.userHelper = userHelper;
+
+    this.preparedSelectCredentials =
+        session.prepare(credentialsHelper.selectByPrimaryKey().asCql());
+    this.preparedSelectUser = session.prepare(userHelper.selectByPrimaryKey().asCql());
+  }
+
+  Optional<User> login(String email, char[] password) {
+    return Optional.ofNullable(session.execute(preparedSelectCredentials.bind(email)).one())
+        .flatMap(
+            credentialsRow -> {
+              String hashedPassword = credentialsRow.getString(""password"");
+              if (PasswordHashing.matches(password, hashedPassword)) {
+                UUID userid = credentialsRow.getUuid(""userid"");
+                Row userRow = session.execute(preparedSelectUser.bind(userid)).one();
+                if (userRow == null) {
+                  throw new IllegalStateException(","[{'comment': 'This method works in a functional way but we are throwing an exception from the inside of it.\r\nMaybe we could Encapsulate user in a `Try` construct? We could use https://www.vavr.io/vavr-docs/#_try but I guess this is not worth adding an additional dependency', 'commenter': 'tomekl007'}, {'comment': '> I guess this is not worth adding an additional dependency\r\n\r\nYeah I agree.', 'commenter': 'olim7t'}]"
1306,examples/src/main/java/com/datastax/oss/driver/examples/mapper/killrvideo/user/UserDao.java,"@@ -0,0 +1,58 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.examples.mapper.killrvideo.user;
+
+import com.datastax.oss.driver.api.mapper.annotations.Dao;
+import com.datastax.oss.driver.api.mapper.annotations.QueryProvider;
+import com.datastax.oss.driver.api.mapper.annotations.Select;
+import java.util.Optional;
+import java.util.UUID;
+
+@Dao
+public interface UserDao {
+
+  /** Simple selection by full primary key. */
+  @Select
+  User get(UUID userid);
+
+  /**
+   * Creating a user is more than a single insert: we have to update two different tables, check
+   * that the email is not used already, and handle password encryption.
+   *
+   * <p>We use a query provider to wrap everything into a single method.
+   *
+   * <p>Note that you could opt for a more layered approach: only expose basic operations on the DAO
+   * (insertCredentialsIfNotExists, insertUser...) and add a service layer on top for more complex
+   * logic. Both designs are valid, this is a matter of personal choice.
+   *
+   * @return the newly created user, or {@link Optional#empty()} if the email address is already
+   *     taken.
+   */
+  @QueryProvider(
+      providerClass = CreateUserQueryProvider.class,
+      entityHelpers = {User.class, UserCredentials.class})
+  Optional<User> create(String firstName, String lastName, String email, char[] password);
+
+  /**
+   * Similar to {@link #create}, this encapsulate encryption so use a query provider.","[{'comment': 'Nit: ""this encapsulate**s** encryption so use**s** a query provider.""', 'commenter': 'adutra'}]"
1306,examples/src/main/java/com/datastax/oss/driver/examples/mapper/killrvideo/user/CreateUserQueryProvider.java,"@@ -0,0 +1,103 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.examples.mapper.killrvideo.user;
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.cql.BoundStatementBuilder;
+import com.datastax.oss.driver.api.core.cql.PreparedStatement;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import com.datastax.oss.driver.api.mapper.MapperContext;
+import com.datastax.oss.driver.api.mapper.entity.EntityHelper;
+import com.datastax.oss.driver.api.mapper.entity.saving.NullSavingStrategy;
+import java.time.Instant;
+import java.util.Objects;
+import java.util.Optional;
+import java.util.UUID;
+
+/**
+ * Provides the implementation of {@link UserDao#create}.
+ *
+ * <p>Package-private visibility is sufficient, this will be called only from the generated DAO
+ * implementation.
+ */
+class CreateUserQueryProvider {
+
+  private final CqlSession session;
+  private final EntityHelper<User> userHelper;
+  private final EntityHelper<UserCredentials> credentialsHelper;
+  private final PreparedStatement preparedInsertCredentials;
+  private final PreparedStatement preparedDeleteCredentials;
+  private final PreparedStatement preparedInsertUser;
+
+  CreateUserQueryProvider(
+      MapperContext context,
+      EntityHelper<User> userHelper,
+      EntityHelper<UserCredentials> credentialsHelper) {
+
+    this.session = context.getSession();
+
+    this.userHelper = userHelper;
+    this.credentialsHelper = credentialsHelper;
+
+    this.preparedInsertCredentials =
+        session.prepare(credentialsHelper.insert().ifNotExists().asCql());
+    this.preparedDeleteCredentials =
+        session.prepare(credentialsHelper.deleteByPrimaryKey().asCql());
+    this.preparedInsertUser = session.prepare(userHelper.insert().ifNotExists().asCql());
+  }
+
+  Optional<User> create(String firstname, String lastname, String email, char[] password) {
+    Instant now = Instant.now();","[{'comment': 'This line should be refactored layer higher to not hard-code `now()`', 'commenter': 'tomekl007'}, {'comment': 'In a real app I would agree (in order to improve testability), but since this is just a simple example I think we can take this shortcut.', 'commenter': 'olim7t'}]"
1306,examples/src/main/resources/killrvideo_schema.cql,"@@ -0,0 +1,137 @@
+// User credentials, keyed by email address so we can authenticate
+CREATE TABLE IF NOT EXISTS user_credentials (
+    email text,
+    password text,
+    userid uuid,
+    PRIMARY KEY (email)
+);
+
+// Users keyed by id
+CREATE TABLE IF NOT EXISTS users (
+    userid uuid,
+    firstname text,
+    lastname text,
+    email text,
+    created_date timestamp,
+    PRIMARY KEY (userid)
+);
+
+// Videos by id
+CREATE TABLE IF NOT EXISTS videos (
+    videoid uuid,
+    userid uuid,
+    name text,
+    description text,
+    location text,
+    location_type int,
+    preview_image_location text,
+    tags set<text>,
+    added_date timestamp,
+    PRIMARY KEY (videoid)
+);
+
+// One-to-many from user point of view (lookup table)
+CREATE TABLE IF NOT EXISTS user_videos (
+    userid uuid,
+    added_date timestamp,
+    videoid uuid,
+    name text,
+    preview_image_location text,
+    PRIMARY KEY (userid, added_date, videoid)
+) WITH CLUSTERING ORDER BY (added_date DESC, videoid ASC);
+
+// Track latest videos, grouped by day (if we ever develop a bad hotspot from the daily grouping here, we could mitigate by
+// splitting the row using an arbitrary group number, making the partition key (yyyymmdd, group_number))
+CREATE TABLE IF NOT EXISTS latest_videos (
+    yyyymmdd text,
+    added_date timestamp,
+    videoid uuid,
+    userid uuid,
+    name text,
+    preview_image_location text,
+    PRIMARY KEY (yyyymmdd, added_date, videoid)
+) WITH CLUSTERING ORDER BY (added_date DESC, videoid ASC);
+
+// Video ratings (counter table)
+CREATE TABLE IF NOT EXISTS video_ratings (
+    videoid uuid,
+    rating_counter counter,
+    rating_total counter,
+    PRIMARY KEY (videoid)
+);
+
+// Video ratings by user (to try and mitigate voting multiple times)
+CREATE TABLE IF NOT EXISTS video_ratings_by_user (
+    videoid uuid,
+    userid uuid,
+    rating int,
+    PRIMARY KEY (videoid, userid)
+);
+
+// Records the number of views/playbacks of a video
+CREATE TABLE IF NOT EXISTS video_playback_stats (
+    videoid uuid,
+    views counter,
+    PRIMARY KEY (videoid)
+);
+
+// Recommendations by user (powered by Spark), with the newest videos added to the site always first
+CREATE TABLE IF NOT EXISTS video_recommendations ( ","[{'comment': 'Some of these tables are not mapped in this example, should we remove their DDL declaration?', 'commenter': 'adutra'}, {'comment': 'I see no harm in leaving them, maybe we can expand the example later.', 'commenter': 'olim7t'}]"
1306,examples/src/main/java/com/datastax/oss/driver/examples/mapper/KillrVideoMapperExample.java,"@@ -0,0 +1,148 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.examples.mapper;
+
+import com.datastax.oss.driver.api.core.CqlIdentifier;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.PagingIterable;
+import com.datastax.oss.driver.examples.mapper.killrvideo.KillrVideoMapper;
+import com.datastax.oss.driver.examples.mapper.killrvideo.user.User;
+import com.datastax.oss.driver.examples.mapper.killrvideo.user.UserDao;
+import com.datastax.oss.driver.examples.mapper.killrvideo.video.LatestVideo;
+import com.datastax.oss.driver.examples.mapper.killrvideo.video.UserVideo;
+import com.datastax.oss.driver.examples.mapper.killrvideo.video.Video;
+import com.datastax.oss.driver.examples.mapper.killrvideo.video.VideoByTag;
+import com.datastax.oss.driver.examples.mapper.killrvideo.video.VideoDao;
+import java.time.Instant;
+import java.time.ZoneOffset;
+import java.time.format.DateTimeFormatter;
+import java.util.HashSet;
+import java.util.Optional;
+import java.util.Set;
+
+/**
+ * Uses the driver's object mapper to interact with a schema.
+ *
+ * <p>We use the data model of the <a href=""https://killrvideo.github.io/"">KillrVideo</a> sample
+ * application. The mapped entities and DAOs are in the {@link
+ * com.datastax.oss.driver.examples.mapper.killrvideo} package. We only cover a subset of the data
+ * model (ratings, stats, recommendations and comments are not covered).
+ *
+ * <p>Preconditions:
+ *
+ * <ul>
+ *   <li>An Apache Cassandra(R) cluster is running and accessible through the contacts points
+ *       identified by basic.contact-points (see application.conf).
+ *   <li>that cluster has a {@code killrvideo} keyspace with the KillrVideo tables (see {@code
+ *       src/main/resources/killrvideo_schema.cql} in this project).
+ *   <li>the tables are empty.
+ * </ul>
+ *
+ * <p>Side effects:
+ *
+ * <ul>
+ *   <li>inserts data in the tables.
+ * </ul>
+ *
+ * @see <a href=""https://docs.datastax.com/en/developer/java-driver/latest/manual/mapper/"">Java
+ *     Driver Mapper manual</a>
+ */
+public class KillrVideoMapperExample {
+
+  private static final CqlIdentifier KEYSPACE_ID = CqlIdentifier.fromCql(""killrvideo"");
+
+  public static void main(String[] args) {
+    try (CqlSession session = CqlSession.builder().build()) {
+      KillrVideoMapper mapper =
+          KillrVideoMapper.builder(session).withDefaultKeyspace(KEYSPACE_ID).build();
+
+      // Create a new user
+      UserDao userDao = mapper.userDao();
+
+      Optional<User> maybeUser =
+          userDao.create(""test"", ""user"", ""testuser@example.com"", ""password123"".toCharArray());
+      if (!maybeUser.isPresent()) {
+        throw new AssertionError(""User creation should succeed (expected empty tables)"");
+      }
+      User user = maybeUser.get();
+      System.out.println(""Created "" + user);
+
+      // Creating another user with the same email should fail
+      assert !userDao
+          .create(""test2"", ""user"", ""testuser@example.com"", ""secret123"".toCharArray())
+          .isPresent();
+
+      // Simulate login attempts
+      tryLogin(userDao, ""testuser@example.com"", ""password123"");
+      tryLogin(userDao, ""testuser@example.com"", ""secret123"");
+
+      // Insert a video
+      VideoDao videoDao = mapper.videoDao();
+
+      Video video = new Video();
+      video.setUserid(user.getUserid());
+      video.setName(
+          ""Getting Started with DataStax Apache Cassandra as a Service on DataStax Constellation"");
+      video.setLocation(""https://www.youtube.com/watch?v=68xzKpcZURA"");
+      Set<String> tags = new HashSet<>();
+      tags.add(""apachecassandra"");
+      tags.add(""nosql"");
+      tags.add(""hybridcloud"");
+      video.setTags(tags);
+
+      videoDao.create(video);","[{'comment': 'Why the decision about having inconsistent API between `videoDao.create` and `UserDao.create`?\r\nThe first method is taking the whole Entity object whereas the second one is not taking the user as the argument but single fields of the User entity.', 'commenter': 'tomekl007'}, {'comment': ""Honestly I didn't give it much thought, I'll see if I can improve it."", 'commenter': 'olim7t'}]"
1306,examples/src/main/java/com/datastax/oss/driver/examples/mapper/killrvideo/video/CreateVideoQueryProvider.java,"@@ -0,0 +1,134 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.examples.mapper.killrvideo.video;
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.cql.BatchStatement;
+import com.datastax.oss.driver.api.core.cql.BatchStatementBuilder;
+import com.datastax.oss.driver.api.core.cql.BoundStatement;
+import com.datastax.oss.driver.api.core.cql.BoundStatementBuilder;
+import com.datastax.oss.driver.api.core.cql.DefaultBatchType;
+import com.datastax.oss.driver.api.core.cql.PreparedStatement;
+import com.datastax.oss.driver.api.mapper.MapperContext;
+import com.datastax.oss.driver.api.mapper.entity.EntityHelper;
+import com.datastax.oss.driver.api.mapper.entity.saving.NullSavingStrategy;
+import java.time.Instant;
+import java.time.ZoneOffset;
+import java.time.format.DateTimeFormatter;
+import java.util.UUID;
+
+/**
+ * Provides the implementation of {@link VideoDao#create}.
+ *
+ * <p>Package-private visibility is sufficient, this will be called only from the generated DAO
+ * implementation.
+ */
+class CreateVideoQueryProvider {
+
+  private final CqlSession session;
+  private final EntityHelper<Video> videoHelper;
+  private final EntityHelper<UserVideo> userVideoHelper;
+  private final EntityHelper<LatestVideo> latestVideoHelper;
+  private final EntityHelper<VideoByTag> videoByTagHelper;
+  private final PreparedStatement preparedInsertVideo;
+  private final PreparedStatement preparedInsertUserVideo;
+  private final PreparedStatement preparedInsertLatestVideo;
+  private final PreparedStatement preparedInsertVideoByTag;
+
+  CreateVideoQueryProvider(
+      MapperContext context,
+      EntityHelper<Video> videoHelper,
+      EntityHelper<UserVideo> userVideoHelper,
+      EntityHelper<LatestVideo> latestVideoHelper,
+      EntityHelper<VideoByTag> videoByTagHelper) {
+    this.session = context.getSession();
+    this.videoHelper = videoHelper;
+    this.userVideoHelper = userVideoHelper;
+    this.latestVideoHelper = latestVideoHelper;
+    this.videoByTagHelper = videoByTagHelper;
+
+    this.preparedInsertVideo = prepareInsert(session, videoHelper);
+    this.preparedInsertUserVideo = prepareInsert(session, userVideoHelper);
+    this.preparedInsertLatestVideo = prepareInsert(session, latestVideoHelper);
+    this.preparedInsertVideoByTag = prepareInsert(session, videoByTagHelper);
+  }
+
+  void create(Video video) {","[{'comment': ""In the `CreateUserQueryProvider.create` we have logic that is able to retry in a case of a failure.\r\nHere we are trying only once and if the batch failed the exception is just propagated. In the first case, we are guarding against `UUID` collisions but I think that in this method `video.setVideoid(UUID.randomUUID());` can also lead to a collision. Shouldn't we handle that?"", 'commenter': 'tomekl007'}, {'comment': ""I'm removing UUID collision handling for user creation per Alexandre's other comment."", 'commenter': 'olim7t'}]"
1306,examples/src/main/java/com/datastax/oss/driver/examples/mapper/killrvideo/user/CreateUserQueryProvider.java,"@@ -0,0 +1,89 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.examples.mapper.killrvideo.user;
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.cql.BoundStatementBuilder;
+import com.datastax.oss.driver.api.core.cql.PreparedStatement;
+import com.datastax.oss.driver.api.core.cql.ResultSet;
+import com.datastax.oss.driver.api.mapper.MapperContext;
+import com.datastax.oss.driver.api.mapper.entity.EntityHelper;
+import com.datastax.oss.driver.api.mapper.entity.saving.NullSavingStrategy;
+import java.time.Instant;
+import java.util.Objects;
+import java.util.UUID;
+
+/**
+ * Provides the implementation of {@link UserDao#create}.
+ *
+ * <p>Package-private visibility is sufficient, this will be called only from the generated DAO
+ * implementation.
+ */
+class CreateUserQueryProvider {
+
+  private final CqlSession session;
+  private final EntityHelper<User> userHelper;
+  private final EntityHelper<UserCredentials> credentialsHelper;
+  private final PreparedStatement preparedInsertCredentials;
+  private final PreparedStatement preparedInsertUser;
+
+  CreateUserQueryProvider(
+      MapperContext context,
+      EntityHelper<User> userHelper,
+      EntityHelper<UserCredentials> credentialsHelper) {
+
+    this.session = context.getSession();
+
+    this.userHelper = userHelper;
+    this.credentialsHelper = credentialsHelper;
+
+    this.preparedInsertCredentials =
+        session.prepare(credentialsHelper.insert().ifNotExists().asCql());
+    this.preparedInsertUser = session.prepare(userHelper.insert().asCql());
+  }
+
+  boolean create(User user, char[] password) {
+    Objects.requireNonNull(user.getUserid());
+    Objects.requireNonNull(user.getEmail());
+
+    if (!insertCredentialsIfNotExists(user.getEmail(), password, user.getUserid())) {
+      // email already exists
+      return false;
+    }
+
+    if (user.getCreatedDate() == null) {
+      user.setCreatedDate(Instant.now());
+    }
+    insertUser(user);","[{'comment': ""I now understand why you didn't use a batch here, but what if the user insertion fails, e.g. because of an `UnavailableException`? It would be wiser to delete the corresponding credentials."", 'commenter': 'adutra'}, {'comment': ""I've amended the code to clean up any inserted row if there is an exception."", 'commenter': 'olim7t'}]"
1306,examples/src/main/java/com/datastax/oss/driver/examples/mapper/KillrVideoMapperExample.java,"@@ -0,0 +1,192 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.examples.mapper;
+
+import com.datastax.oss.driver.api.core.CqlIdentifier;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.PagingIterable;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.examples.mapper.killrvideo.KillrVideoMapper;
+import com.datastax.oss.driver.examples.mapper.killrvideo.user.User;
+import com.datastax.oss.driver.examples.mapper.killrvideo.user.UserDao;
+import com.datastax.oss.driver.examples.mapper.killrvideo.video.LatestVideo;
+import com.datastax.oss.driver.examples.mapper.killrvideo.video.UserVideo;
+import com.datastax.oss.driver.examples.mapper.killrvideo.video.Video;
+import com.datastax.oss.driver.examples.mapper.killrvideo.video.VideoByTag;
+import com.datastax.oss.driver.examples.mapper.killrvideo.video.VideoDao;
+import java.nio.charset.StandardCharsets;
+import java.nio.file.Files;
+import java.nio.file.Path;
+import java.nio.file.Paths;
+import java.time.Instant;
+import java.time.ZoneOffset;
+import java.time.format.DateTimeFormatter;
+import java.util.Arrays;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Optional;
+import java.util.Set;
+import java.util.UUID;
+import java.util.stream.Collectors;
+
+/**
+ * Uses the driver's object mapper to interact with a schema.
+ *
+ * <p>We use the data model of the <a href=""https://killrvideo.github.io/"">KillrVideo</a> sample
+ * application. The mapped entities and DAOs are in the {@link
+ * com.datastax.oss.driver.examples.mapper.killrvideo} package. We only cover a subset of the data
+ * model (ratings, stats, recommendations and comments are not covered).
+ *
+ * <p>Preconditions:
+ *
+ * <ul>
+ *   <li>An Apache Cassandra(R) cluster is running and accessible through the contacts points
+ *       identified by basic.contact-points (see application.conf).
+ * </ul>
+ *
+ * <p>Side effects:
+ *
+ * <ul>
+ *   <li>creates a new keyspace ""killrvideo"" in the session. If a keyspace with this name already
+ *       exists, it will be reused;
+ *   <li>creates the tables of the KillrVideo data model, if they don't already exist;
+ *   <li>inserts a new user, or reuse the existing one if the email address is already taken;
+ *   <li>inserts a video for that user.
+ * </ul>
+ *
+ * @see <a href=""https://docs.datastax.com/en/developer/java-driver/latest/manual/mapper/"">Java
+ *     Driver Mapper manual</a>
+ */
+public class KillrVideoMapperExample {
+
+  private static final CqlIdentifier KEYSPACE_ID = CqlIdentifier.fromCql(""killrvideo"");
+
+  public static void main(String[] args) {
+
+    try (CqlSession session = CqlSession.builder().build()) {
+
+      maybeCreateSchema(session);
+
+      KillrVideoMapper mapper =
+          KillrVideoMapper.builder(session).withDefaultKeyspace(KEYSPACE_ID).build();
+
+      // Create a new user
+      UserDao userDao = mapper.userDao();
+
+      User user =
+          new User(UUID.randomUUID(), ""test"", ""user"", ""testuser@example.com"", Instant.now());
+
+      if (userDao.create(user, ""password123"".toCharArray())) {
+        System.out.println(""Created "" + user);
+      } else {
+        user = userDao.getByEmail(""testuser@example.com"");
+        System.out.println(""Reusing existing "" + user);
+      }
+
+      // Creating another user with the same email should fail
+      assert !userDao.create(
+          new User(UUID.randomUUID(), ""test2"", ""user"", ""testuser@example.com"", Instant.now()),
+          ""secret123"".toCharArray());
+
+      // Simulate login attempts
+      tryLogin(userDao, ""testuser@example.com"", ""password123"");
+      tryLogin(userDao, ""testuser@example.com"", ""secret123"");
+
+      // Insert a video
+      VideoDao videoDao = mapper.videoDao();
+
+      Video video = new Video();
+      video.setUserid(user.getUserid());
+      video.setName(
+          ""Getting Started with DataStax Apache Cassandra as a Service on DataStax Constellation"");
+      video.setLocation(""https://www.youtube.com/watch?v=68xzKpcZURA"");
+      Set<String> tags = new HashSet<>();
+      tags.add(""apachecassandra"");
+      tags.add(""nosql"");
+      tags.add(""hybridcloud"");
+      video.setTags(tags);
+
+      videoDao.create(video);
+      System.out.printf(""Created video [%s] %s%n"", video.getVideoid(), video.getName());
+
+      // Check that associated denormalized tables have also been updated:
+      PagingIterable<UserVideo> userVideos = videoDao.getByUser(user.getUserid());
+      System.out.printf(""Videos for %s %s:%n"", user.getFirstname(), user.getLastname());
+      for (UserVideo userVideo : userVideos) {
+        System.out.printf(""  [%s] %s%n"", userVideo.getVideoid(), userVideo.getName());
+      }
+
+      PagingIterable<LatestVideo> latestVideos = videoDao.getLatest(todaysTimestamp());
+      System.out.println(""Latest videos:"");
+      for (LatestVideo latestVideo : latestVideos) {
+        System.out.printf(""  [%s] %s%n"", latestVideo.getVideoid(), latestVideo.getName());
+      }
+
+      PagingIterable<VideoByTag> videosByTag = videoDao.getByTag(""apachecassandra"");
+      System.out.println(""Videos tagged with apachecassandra:"");
+      for (VideoByTag videoByTag : videosByTag) {
+        System.out.printf(""  [%s] %s%n"", videoByTag.getVideoid(), videoByTag.getName());
+      }
+
+      // Update the existing video:
+      Video template = new Video();
+      template.setVideoid(video.getVideoid());
+      template.setName(
+          ""Getting Started with DataStax Apache CassandraÂ® as a Service on DataStax Constellation"");
+      videoDao.update(template);
+      // Reload the whole entity and check the fields
+      video = videoDao.get(video.getVideoid());
+      System.out.printf(""Updated name for video %s: %s%n"", video.getVideoid(), video.getName());
+    } catch (Exception e) {
+      e.printStackTrace();
+    }
+  }
+
+  private static void tryLogin(UserDao userDao, String email, String password) {
+    Optional<User> maybeUser = userDao.login(email, password.toCharArray());
+    System.out.printf(
+        ""Logging in with %s/%s: %s%n"",
+        email, password, maybeUser.isPresent() ? ""Success"" : ""Failure"");
+  }
+
+  public static void maybeCreateSchema(CqlSession session) throws Exception {","[{'comment': 'Can be private', 'commenter': 'adutra'}]"
1306,examples/src/main/java/com/datastax/oss/driver/examples/mapper/KillrVideoMapperExample.java,"@@ -0,0 +1,192 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.examples.mapper;
+
+import com.datastax.oss.driver.api.core.CqlIdentifier;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.PagingIterable;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.examples.mapper.killrvideo.KillrVideoMapper;
+import com.datastax.oss.driver.examples.mapper.killrvideo.user.User;
+import com.datastax.oss.driver.examples.mapper.killrvideo.user.UserDao;
+import com.datastax.oss.driver.examples.mapper.killrvideo.video.LatestVideo;
+import com.datastax.oss.driver.examples.mapper.killrvideo.video.UserVideo;
+import com.datastax.oss.driver.examples.mapper.killrvideo.video.Video;
+import com.datastax.oss.driver.examples.mapper.killrvideo.video.VideoByTag;
+import com.datastax.oss.driver.examples.mapper.killrvideo.video.VideoDao;
+import java.nio.charset.StandardCharsets;
+import java.nio.file.Files;
+import java.nio.file.Path;
+import java.nio.file.Paths;
+import java.time.Instant;
+import java.time.ZoneOffset;
+import java.time.format.DateTimeFormatter;
+import java.util.Arrays;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Optional;
+import java.util.Set;
+import java.util.UUID;
+import java.util.stream.Collectors;
+
+/**
+ * Uses the driver's object mapper to interact with a schema.
+ *
+ * <p>We use the data model of the <a href=""https://killrvideo.github.io/"">KillrVideo</a> sample
+ * application. The mapped entities and DAOs are in the {@link
+ * com.datastax.oss.driver.examples.mapper.killrvideo} package. We only cover a subset of the data
+ * model (ratings, stats, recommendations and comments are not covered).
+ *
+ * <p>Preconditions:
+ *
+ * <ul>
+ *   <li>An Apache Cassandra(R) cluster is running and accessible through the contacts points
+ *       identified by basic.contact-points (see application.conf).
+ * </ul>
+ *
+ * <p>Side effects:
+ *
+ * <ul>
+ *   <li>creates a new keyspace ""killrvideo"" in the session. If a keyspace with this name already
+ *       exists, it will be reused;
+ *   <li>creates the tables of the KillrVideo data model, if they don't already exist;
+ *   <li>inserts a new user, or reuse the existing one if the email address is already taken;
+ *   <li>inserts a video for that user.
+ * </ul>
+ *
+ * @see <a href=""https://docs.datastax.com/en/developer/java-driver/latest/manual/mapper/"">Java
+ *     Driver Mapper manual</a>
+ */
+public class KillrVideoMapperExample {
+
+  private static final CqlIdentifier KEYSPACE_ID = CqlIdentifier.fromCql(""killrvideo"");
+
+  public static void main(String[] args) {
+
+    try (CqlSession session = CqlSession.builder().build()) {
+
+      maybeCreateSchema(session);
+
+      KillrVideoMapper mapper =
+          KillrVideoMapper.builder(session).withDefaultKeyspace(KEYSPACE_ID).build();
+
+      // Create a new user
+      UserDao userDao = mapper.userDao();
+
+      User user =
+          new User(UUID.randomUUID(), ""test"", ""user"", ""testuser@example.com"", Instant.now());
+
+      if (userDao.create(user, ""password123"".toCharArray())) {
+        System.out.println(""Created "" + user);
+      } else {
+        user = userDao.getByEmail(""testuser@example.com"");
+        System.out.println(""Reusing existing "" + user);
+      }
+
+      // Creating another user with the same email should fail
+      assert !userDao.create(
+          new User(UUID.randomUUID(), ""test2"", ""user"", ""testuser@example.com"", Instant.now()),
+          ""secret123"".toCharArray());
+
+      // Simulate login attempts
+      tryLogin(userDao, ""testuser@example.com"", ""password123"");
+      tryLogin(userDao, ""testuser@example.com"", ""secret123"");
+
+      // Insert a video
+      VideoDao videoDao = mapper.videoDao();
+
+      Video video = new Video();
+      video.setUserid(user.getUserid());
+      video.setName(
+          ""Getting Started with DataStax Apache Cassandra as a Service on DataStax Constellation"");
+      video.setLocation(""https://www.youtube.com/watch?v=68xzKpcZURA"");
+      Set<String> tags = new HashSet<>();
+      tags.add(""apachecassandra"");
+      tags.add(""nosql"");
+      tags.add(""hybridcloud"");
+      video.setTags(tags);
+
+      videoDao.create(video);
+      System.out.printf(""Created video [%s] %s%n"", video.getVideoid(), video.getName());
+
+      // Check that associated denormalized tables have also been updated:
+      PagingIterable<UserVideo> userVideos = videoDao.getByUser(user.getUserid());
+      System.out.printf(""Videos for %s %s:%n"", user.getFirstname(), user.getLastname());
+      for (UserVideo userVideo : userVideos) {
+        System.out.printf(""  [%s] %s%n"", userVideo.getVideoid(), userVideo.getName());
+      }
+
+      PagingIterable<LatestVideo> latestVideos = videoDao.getLatest(todaysTimestamp());
+      System.out.println(""Latest videos:"");
+      for (LatestVideo latestVideo : latestVideos) {
+        System.out.printf(""  [%s] %s%n"", latestVideo.getVideoid(), latestVideo.getName());
+      }
+
+      PagingIterable<VideoByTag> videosByTag = videoDao.getByTag(""apachecassandra"");
+      System.out.println(""Videos tagged with apachecassandra:"");
+      for (VideoByTag videoByTag : videosByTag) {
+        System.out.printf(""  [%s] %s%n"", videoByTag.getVideoid(), videoByTag.getName());
+      }
+
+      // Update the existing video:
+      Video template = new Video();
+      template.setVideoid(video.getVideoid());
+      template.setName(
+          ""Getting Started with DataStax Apache CassandraÂ® as a Service on DataStax Constellation"");
+      videoDao.update(template);
+      // Reload the whole entity and check the fields
+      video = videoDao.get(video.getVideoid());
+      System.out.printf(""Updated name for video %s: %s%n"", video.getVideoid(), video.getName());
+    } catch (Exception e) {
+      e.printStackTrace();
+    }
+  }
+
+  private static void tryLogin(UserDao userDao, String email, String password) {
+    Optional<User> maybeUser = userDao.login(email, password.toCharArray());
+    System.out.printf(
+        ""Logging in with %s/%s: %s%n"",
+        email, password, maybeUser.isPresent() ? ""Success"" : ""Failure"");
+  }
+
+  public static void maybeCreateSchema(CqlSession session) throws Exception {
+    session.execute(
+        SimpleStatement.newInstance(
+                ""CREATE KEYSPACE IF NOT EXISTS killrvideo WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1}"")
+            .setExecutionProfileName(""slow""));
+    session.execute(""USE killrvideo"");
+    for (String statement : getStatements(""killrvideo_schema.cql"")) {
+      session.execute(SimpleStatement.newInstance(statement).setExecutionProfileName(""slow""));
+    }
+  }
+
+  private static List<String> getStatements(String fileName) throws Exception {
+    Path path = Paths.get(ClassLoader.getSystemResource(fileName).toURI());
+    String contents = new String(Files.readAllBytes(path), StandardCharsets.UTF_8);
+    return Arrays.stream(contents.split("";""))
+        .map(String::trim)
+        .filter(s -> !s.isEmpty())
+        .collect(Collectors.toList());
+  }
+
+  /**
+   * KillrVideo uses a textual timestamp to partition recent video. Build the timestamp for today to
+   * fetch our latest insertions.
+   */
+  private static String todaysTimestamp() {
+    return DateTimeFormatter.ofPattern(""yyyyMMdd"").withZone(ZoneOffset.UTC).format(Instant.now());","[{'comment': ""This is repeated in `CreateVideoQueryProvider`, can't we factor the two together?"", 'commenter': 'adutra'}]"
1310,core/src/main/java/com/datastax/oss/driver/internal/core/type/codec/MapCodec.java,"@@ -134,18 +134,28 @@ public ByteBuffer encode(
       int size = input.getInt();
       Map<KeyT, ValueT> result = Maps.newLinkedHashMapWithExpectedSize(size);
       for (int i = 0; i < size; i++) {
+        KeyT key;","[{'comment': ""If we are handling Map shouldn't we handle that for Tuple and Udt as well?"", 'commenter': 'tomekl007'}, {'comment': ""`TupleCodec` and `UdtCodec` already handle null values.\r\n\r\nUDT field names don't appear in the encoded representation. And there's no way they can ever be null anyway."", 'commenter': 'olim7t'}]"
1310,core/src/test/java/com/datastax/oss/driver/internal/core/type/codec/MapCodecTest.java,"@@ -118,6 +118,16 @@ public void should_decode_non_empty_map() {
         .containsEntry(""c"", 3);
   }
 
+  @Test
+  public void should_decode_map_with_null_elements() {
+    when(keyCodec.decode(Bytes.fromHexString(""0x10""), ProtocolVersion.DEFAULT)).thenReturn(""a"");
+    when(valueCodec.decode(Bytes.fromHexString(""0x0002""), ProtocolVersion.DEFAULT)).thenReturn(2);
+    assertThat(decode(""0x"" + ""00000002"" + ""0000000110"" + ""FFFFFFFF"" + ""FFFFFFFF"" + ""000000020002""))
+        .containsOnlyKeys(""a"", null)
+        .containsEntry(""a"", null)
+        .containsEntry(null, 2);","[{'comment': ""I think that we are missing a test case for key and value null for the same entry but I don't know if such a test gives much value."", 'commenter': 'tomekl007'}, {'comment': ""It wouldn't increase coverage so I would say no."", 'commenter': 'olim7t'}]"
1310,core/src/main/java/com/datastax/oss/driver/internal/core/type/codec/ListCodec.java,"@@ -115,11 +115,19 @@ public ByteBuffer encode(
       int size = input.getInt();
       List<ElementT> result = new ArrayList<>(size);
       for (int i = 0; i < size; i++) {
+        ElementT element;
         int elementSize = input.getInt();
-        ByteBuffer encodedElement = input.slice();
-        encodedElement.limit(elementSize);
-        input.position(input.position() + elementSize);
-        result.add(elementCodec.decode(encodedElement, protocolVersion));
+        // Allow null elements on the decode path, because Cassandra might return such collections
+        // for some computed values in the future -- e.g. SELECT ttl(some_collection)","[{'comment': ""I'm probably a bit paranoid but should we include the example of `ttl(collection)`? This example is DSE-specific."", 'commenter': 'adutra'}, {'comment': '[CASSANDRA-8877](https://issues.apache.org/jira/browse/CASSANDRA-8877) will likely add that feature.', 'commenter': 'olim7t'}]"
1310,core/src/main/java/com/datastax/oss/driver/internal/core/type/codec/ListCodec.java,"@@ -115,11 +115,19 @@ public ByteBuffer encode(
       int size = input.getInt();
       List<ElementT> result = new ArrayList<>(size);
       for (int i = 0; i < size; i++) {
+        ElementT element;
         int elementSize = input.getInt();
-        ByteBuffer encodedElement = input.slice();
-        encodedElement.limit(elementSize);
-        input.position(input.position() + elementSize);
-        result.add(elementCodec.decode(encodedElement, protocolVersion));
+        // Allow null elements on the decode path, because Cassandra might return such collections
+        // for some computed values in the future -- e.g. SELECT ttl(some_collection)
+        if (elementSize < 0) {
+          element = null;
+        } else {
+          ByteBuffer encodedElement = input.slice();
+          encodedElement.limit(elementSize);
+          input.position(input.position() + elementSize);","[{'comment': 'Nit: it would be a bit easier to understand this code if you invert these two lines:\r\n\r\n```\r\nelement = elementCodec.decode(encodedElement, protocolVersion);\r\ninput.position(input.position() + elementSize);\r\n```\r\n\r\n(I initially thought that updating the position of `input` was required before the element can be decoded.)', 'commenter': 'adutra'}, {'comment': ""ok I'm doing it in a separate commit"", 'commenter': 'olim7t'}]"
1310,core/src/test/java/com/datastax/oss/driver/internal/core/type/codec/ListCodecTest.java,"@@ -93,6 +93,12 @@ public void should_decode_non_empty_list() {
         .containsExactly(1, 2, 3);
   }
 
+  @Test
+  public void should_decode_list_with_null_elements() {
+    when(elementCodec.decode(Bytes.fromHexString(""0x0002""), ProtocolVersion.DEFAULT)).thenReturn(2);
+    assertThat(decode(""0x"" + ""00000002"" + ""FFFFFFFF"" + ""000000020002"")).containsExactly(null, 2);","[{'comment': 'This is very cryptic, how about:\r\n\r\n```\r\nassertThat(\r\n        decode(\r\n            ""0x""\r\n                + ""00000002"" // number of elements\r\n                + ""FFFFFFFF"" // size of element 1 (null)\r\n                + ""00000002"" // size of element 2 (2 bytes)\r\n                + ""0002"" // element 2\r\n            ))\r\n    .containsExactly(null, 2);\r\n``` ', 'commenter': 'adutra'}, {'comment': ""There's a commented example 30 lines above. When you have the whole file open in an IDE it's pretty visible, I think it's fine not to repeat it in every test."", 'commenter': 'olim7t'}, {'comment': 'I agree with Alex on that, I had the same impression that it was very cryptic', 'commenter': 'tomekl007'}, {'comment': 'Well it\'s not exactly the same example, here we have a `""FFFFFFFF""` block that seems strange at first glance, and `""000000020002""` is also strange since it contains both the size of element 2 and element 2 itself.', 'commenter': 'adutra'}]"
1322,manual/core/custom_codecs/README.md,"@@ -87,6 +87,18 @@ CqlSession session = CqlSession.builder()
     .build();
 ```
 
+You may also add codecs to an existing session at runtime:
+
+```java
+// The cast is required for backward compatibility reasons (registry mutability was introduced in
+// 4.3.0). It is safe as long as you didn't hack the driver internals to plug a custom registry
+// implementation.
+MutableCodecRegistry registry =
+    (MutableCodecRegistry) session.getContext().getCodecRegistry();
+
+registry.register(new CqlIntToStringCodec());","[{'comment': 'Canonical way to use the new feature from client code.', 'commenter': 'olim7t'}]"
1322,core/src/main/java/com/datastax/oss/driver/api/core/type/codec/registry/MutableCodecRegistry.java,"@@ -0,0 +1,58 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core.type.codec.registry;
+
+import com.datastax.oss.driver.api.core.type.codec.TypeCodec;
+
+/**
+ * A codec registry that can be extended with new user codecs at runtime.
+ *
+ * <p>This interface only exists to preserve backward compatibility. In practice, the default {@link
+ * CodecRegistry} implementation returned by the driver implements this interface, so it can safely
+ * be cast.
+ *
+ * <p>However {@link CodecRegistry#DEFAULT} is immutable. It implements this interface, but {@link
+ * #register(TypeCodec)} throws an {@link IllegalStateException}.
+ *
+ * @since 4.3.0
+ */
+public interface MutableCodecRegistry extends CodecRegistry {
+
+  /**
+   * Adds the given codec to the registry.
+   *
+   * <p>This method will log a warning and ignore the codec if it collides with one already present
+   * in the registry. Note that this check is not done in a completely thread-safe manner; codecs
+   * should typically be registered at application startup, not in a highly concurrent context (if a
+   * race condition occurs, the worst possible outcome is that no warning gets logged, and the codec
+   * gets ignored silently).
+   */
+  void register(TypeCodec<?> codec);","[{'comment': ""Same methods as in 3.x.\r\nI made them void to emphasize the fact that they mutate the existing registry (since that's the only way you can operate on the instance stored in the context)."", 'commenter': 'olim7t'}]"
1322,core/src/main/java/com/datastax/oss/driver/internal/core/type/codec/registry/CachingCodecRegistry.java,"@@ -68,19 +69,71 @@
 
   protected final String logPrefix;
   private final TypeCodec<?>[] primitiveCodecs;
-  private final TypeCodec<?>[] userCodecs;
+  private final CopyOnWriteArrayList<TypeCodec<?>> userCodecs = new CopyOnWriteArrayList<>();
   private final IntMap<TypeCodec> primitiveCodecsByCode;
 
   protected CachingCodecRegistry(
-      @NonNull String logPrefix,
-      @NonNull TypeCodec<?>[] primitiveCodecs,
-      @NonNull TypeCodec<?>[] userCodecs) {
+      @NonNull String logPrefix, @NonNull TypeCodec<?>[] primitiveCodecs) {
     this.logPrefix = logPrefix;
     this.primitiveCodecs = primitiveCodecs;
-    this.userCodecs = userCodecs;
     this.primitiveCodecsByCode = sortByProtocolCode(primitiveCodecs);
   }
 
+  /**
+   * @deprecated this constructor calls an overridable method ({@link #register(TypeCodec[])}),
+   *     which is a bad practice. The recommended alternative is to use {@link
+   *     #CachingCodecRegistry(String, TypeCodec[])}, then add the codecs with one of the {@link
+   *     #register} methods.
+   */
+  @Deprecated
+  protected CachingCodecRegistry(
+      @NonNull String logPrefix,
+      @NonNull TypeCodec<?>[] primitiveCodecs,
+      @NonNull TypeCodec<?>[] userCodecs) {
+    this(logPrefix, primitiveCodecs);
+    register(userCodecs);
+  }","[{'comment': ""Deprecating this constructor is the best way I've found to address the fact that it calls an overridable method. 3.x didn't have a way to pass user codecs to the constructor so I don't think it's a big deal to drop that."", 'commenter': 'olim7t'}, {'comment': 'It should be possible to avoid the deprecation of this constructor:\r\n\r\n```\r\n protected CachingCodecRegistry(\r\n    @NonNull String logPrefix,\r\n    @NonNull TypeCodec<?>[] primitiveCodecs,\r\n    @NonNull TypeCodec<?>[] userCodecs) {\r\n  this.logPrefix = logPrefix;\r\n  this.primitiveCodecs = primitiveCodecs;\r\n  this.primitiveCodecsByCode = sortByProtocolCode(primitiveCodecs);\r\n  for (TypeCodec<?> userCodec : userCodecs) {\r\n    registerInternal(userCodec);\r\n  }\r\n}\r\n\r\n@Override\r\npublic void register(TypeCodec<?> newCodec) {\r\n  registerInternal(newCodec);\r\n}\r\n\r\nprivate void registerInternal(TypeCodec<?> newCodec) {\r\n  ...\r\n}\r\n```', 'commenter': 'adutra'}, {'comment': 'I thought of that, but then if you have a subclass that needs to do some additional processing when you register a codec, it can only do so in the overridden method, not change how the constructor handles registration.', 'commenter': 'olim7t'}]"
1322,core/src/main/java/com/datastax/oss/driver/internal/core/type/codec/registry/DefaultCodecRegistry.java,"@@ -48,7 +48,24 @@
   private final LoadingCache<CacheKey, TypeCodec<?>> cache;
 
   /**
-   * Creates a new instance, with some amount of control over the cache behavior.
+   * Creates a new instance that accepts user codecs, with the default built-in codecs and the
+   * default cache behavior.
+   */
+  public DefaultCodecRegistry(@NonNull String logPrefix) {
+    this(logPrefix, CodecRegistryConstants.PRIMITIVE_CODECS);
+  }
+
+  /**
+   * Creates a new instance that accepts user codecs, with the given built-in codecs and the default
+   * cache behavior.
+   */
+  public DefaultCodecRegistry(@NonNull String logPrefix, @NonNull TypeCodec<?>[] primitiveCodecs) {
+    this(logPrefix, 0, null, 0, null, primitiveCodecs);
+  }","[{'comment': ""Unlike `CachingCodecRegistry`, I didn't preserve backward compatibility for this class's constructors. It's much less likely that someone extended it.\r\n\r\nOne catch is that the last argument of `DefaultCodecRegistry(String, TypeCodec<?>[])` now corresponds to primitive codecs (vs. user codecs in 4.2.0). But if someone runs into that things will blow out pretty fast, there's no risk that it goes undetected."", 'commenter': 'olim7t'}]"
1322,core/src/main/java/com/datastax/oss/driver/api/core/detach/AttachmentPoint.java,"@@ -39,6 +40,12 @@ public CodecRegistry getCodecRegistry() {
   @NonNull
   ProtocolVersion getProtocolVersion();
 
+  /**
+   * Note that the default registry implementation returned by the driver also implements {@link
+   * MutableCodecRegistry}, which allows you to register new codecs at runtime. You can safely cast
+   * the result of this method (as long as you didn't hack the driver internals to plug a custom","[{'comment': '```\r\nas long as you didn\'t hack the driver internals to plug a custom\r\n   * registry implementation\r\n```\r\nmaybe we can give an example of what exactly we want to prevent? Users may do this without being aware that they are doing this ""hack""', 'commenter': 'tomekl007'}, {'comment': 'Good point, I\'ll be more specific like ""extend the driver context to override buildCodecRegistry"".', 'commenter': 'olim7t'}]"
1322,core/src/main/java/com/datastax/oss/driver/api/core/type/codec/registry/MutableCodecRegistry.java,"@@ -0,0 +1,58 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core.type.codec.registry;
+
+import com.datastax.oss.driver.api.core.type.codec.TypeCodec;
+
+/**
+ * A codec registry that can be extended with new user codecs at runtime.
+ *
+ * <p>This interface only exists to preserve backward compatibility. In practice, the default {@link
+ * CodecRegistry} implementation returned by the driver implements this interface, so it can safely
+ * be cast.
+ *
+ * <p>However {@link CodecRegistry#DEFAULT} is immutable. It implements this interface, but {@link
+ * #register(TypeCodec)} throws an {@link IllegalStateException}.
+ *
+ * @since 4.3.0
+ */
+public interface MutableCodecRegistry extends CodecRegistry {
+
+  /**
+   * Adds the given codec to the registry.
+   *
+   * <p>This method will log a warning and ignore the codec if it collides with one already present
+   * in the registry. Note that this check is not done in a completely thread-safe manner; codecs
+   * should typically be registered at application startup, not in a highly concurrent context (if a","[{'comment': '` not in a highly concurrent context` - I will feel this requirement is a bit vague. What does it mean not highly concurrent? Is it 2 threads, 10? ', 'commenter': 'tomekl007'}, {'comment': ""Shouldn't we simply add a `synchronized` block to this method's implementation? This way there would be no possible race condition and, as you mentioned, this method is not meant to be called on the application's hot path, so it shouldn't be too much of a penalty to have a synchronized section here."", 'commenter': 'adutra'}, {'comment': 'OK, adding synchronization.', 'commenter': 'olim7t'}]"
1322,core/src/main/java/com/datastax/oss/driver/internal/core/type/codec/registry/CachingCodecRegistry.java,"@@ -68,19 +69,71 @@
 
   protected final String logPrefix;
   private final TypeCodec<?>[] primitiveCodecs;
-  private final TypeCodec<?>[] userCodecs;
+  private final CopyOnWriteArrayList<TypeCodec<?>> userCodecs = new CopyOnWriteArrayList<>();
   private final IntMap<TypeCodec> primitiveCodecsByCode;
 
   protected CachingCodecRegistry(
-      @NonNull String logPrefix,
-      @NonNull TypeCodec<?>[] primitiveCodecs,
-      @NonNull TypeCodec<?>[] userCodecs) {
+      @NonNull String logPrefix, @NonNull TypeCodec<?>[] primitiveCodecs) {
     this.logPrefix = logPrefix;
     this.primitiveCodecs = primitiveCodecs;
-    this.userCodecs = userCodecs;
     this.primitiveCodecsByCode = sortByProtocolCode(primitiveCodecs);
   }
 
+  /**
+   * @deprecated this constructor calls an overridable method ({@link #register(TypeCodec[])}),
+   *     which is a bad practice. The recommended alternative is to use {@link
+   *     #CachingCodecRegistry(String, TypeCodec[])}, then add the codecs with one of the {@link
+   *     #register} methods.
+   */
+  @Deprecated
+  protected CachingCodecRegistry(
+      @NonNull String logPrefix,
+      @NonNull TypeCodec<?>[] primitiveCodecs,
+      @NonNull TypeCodec<?>[] userCodecs) {
+    this(logPrefix, primitiveCodecs);
+    register(userCodecs);
+  }
+
+  @Override
+  public void register(TypeCodec<?> newCodec) {
+    for (TypeCodec<?> primitiveCodec : primitiveCodecs) {
+      if (primitiveCodec.accepts(newCodec.getCqlType())","[{'comment': 'I think that this check can be refactored to a method and reused by this line and https://github.com/datastax/java-driver/pull/1322/files#diff-34d4c85a5bfd4f5ccb02c01deb989c99R111', 'commenter': 'tomekl007'}, {'comment': 'ðŸ‘ extracted `collides(TypeCodec, TypeCodec)`', 'commenter': 'olim7t'}]"
1322,core/src/main/java/com/datastax/oss/driver/internal/core/type/codec/registry/CachingCodecRegistry.java,"@@ -68,19 +69,71 @@
 
   protected final String logPrefix;
   private final TypeCodec<?>[] primitiveCodecs;
-  private final TypeCodec<?>[] userCodecs;
+  private final CopyOnWriteArrayList<TypeCodec<?>> userCodecs = new CopyOnWriteArrayList<>();
   private final IntMap<TypeCodec> primitiveCodecsByCode;
 
   protected CachingCodecRegistry(
-      @NonNull String logPrefix,
-      @NonNull TypeCodec<?>[] primitiveCodecs,
-      @NonNull TypeCodec<?>[] userCodecs) {
+      @NonNull String logPrefix, @NonNull TypeCodec<?>[] primitiveCodecs) {
     this.logPrefix = logPrefix;
     this.primitiveCodecs = primitiveCodecs;
-    this.userCodecs = userCodecs;
     this.primitiveCodecsByCode = sortByProtocolCode(primitiveCodecs);
   }
 
+  /**
+   * @deprecated this constructor calls an overridable method ({@link #register(TypeCodec[])}),
+   *     which is a bad practice. The recommended alternative is to use {@link
+   *     #CachingCodecRegistry(String, TypeCodec[])}, then add the codecs with one of the {@link
+   *     #register} methods.
+   */
+  @Deprecated
+  protected CachingCodecRegistry(
+      @NonNull String logPrefix,
+      @NonNull TypeCodec<?>[] primitiveCodecs,
+      @NonNull TypeCodec<?>[] userCodecs) {
+    this(logPrefix, primitiveCodecs);
+    register(userCodecs);
+  }
+
+  @Override
+  public void register(TypeCodec<?> newCodec) {
+    for (TypeCodec<?> primitiveCodec : primitiveCodecs) {
+      if (primitiveCodec.accepts(newCodec.getCqlType())
+          && primitiveCodec.accepts(newCodec.getJavaType())) {
+        LOG.warn(
+            ""[{}] Ignoring codec {} because it collides with built-in primitive codec {}"",
+            logPrefix,
+            newCodec,
+            primitiveCodec);
+        return;
+      }
+    }
+    for (TypeCodec<?> userCodec : userCodecs) {
+      if (userCodec.accepts(newCodec.getCqlType()) && userCodec.accepts(newCodec.getJavaType())) {
+        LOG.warn(
+            ""[{}] Ignoring codec {} because it collides with previously registered codec {}"",
+            logPrefix,
+            newCodec,
+            userCodec);
+        return;
+      }
+    }
+    // Technically this would cover the two previous cases as well, but we want precise messages.
+    try {
+      TypeCodec<?> cachedCodec =
+          getCachedCodec(newCodec.getCqlType(), newCodec.getJavaType(), false);
+      LOG.warn(
+          ""[{}] Ignoring codec {} because it collides with previously generated codec {}"",","[{'comment': 'what is a difference between `previously generated codec` and `previously registered codec` (from the client perspective)?', 'commenter': 'tomekl007'}, {'comment': 'Generated means that the registry created the codec on the fly, e.g. a codec for `List<Foo>` can be automatically generated if the user registers a codec for `Foo`.', 'commenter': 'adutra'}]"
1322,core/src/main/java/com/datastax/oss/driver/internal/core/type/codec/registry/CachingCodecRegistry.java,"@@ -68,19 +69,71 @@
 
   protected final String logPrefix;
   private final TypeCodec<?>[] primitiveCodecs;
-  private final TypeCodec<?>[] userCodecs;
+  private final CopyOnWriteArrayList<TypeCodec<?>> userCodecs = new CopyOnWriteArrayList<>();
   private final IntMap<TypeCodec> primitiveCodecsByCode;
 
   protected CachingCodecRegistry(
-      @NonNull String logPrefix,
-      @NonNull TypeCodec<?>[] primitiveCodecs,
-      @NonNull TypeCodec<?>[] userCodecs) {
+      @NonNull String logPrefix, @NonNull TypeCodec<?>[] primitiveCodecs) {
     this.logPrefix = logPrefix;
     this.primitiveCodecs = primitiveCodecs;
-    this.userCodecs = userCodecs;
     this.primitiveCodecsByCode = sortByProtocolCode(primitiveCodecs);
   }
 
+  /**
+   * @deprecated this constructor calls an overridable method ({@link #register(TypeCodec[])}),
+   *     which is a bad practice. The recommended alternative is to use {@link
+   *     #CachingCodecRegistry(String, TypeCodec[])}, then add the codecs with one of the {@link
+   *     #register} methods.
+   */
+  @Deprecated
+  protected CachingCodecRegistry(
+      @NonNull String logPrefix,
+      @NonNull TypeCodec<?>[] primitiveCodecs,
+      @NonNull TypeCodec<?>[] userCodecs) {
+    this(logPrefix, primitiveCodecs);
+    register(userCodecs);
+  }
+
+  @Override
+  public void register(TypeCodec<?> newCodec) {
+    for (TypeCodec<?> primitiveCodec : primitiveCodecs) {
+      if (primitiveCodec.accepts(newCodec.getCqlType())
+          && primitiveCodec.accepts(newCodec.getJavaType())) {
+        LOG.warn(
+            ""[{}] Ignoring codec {} because it collides with built-in primitive codec {}"",
+            logPrefix,
+            newCodec,
+            primitiveCodec);
+        return;
+      }
+    }
+    for (TypeCodec<?> userCodec : userCodecs) {
+      if (userCodec.accepts(newCodec.getCqlType()) && userCodec.accepts(newCodec.getJavaType())) {
+        LOG.warn(
+            ""[{}] Ignoring codec {} because it collides with previously registered codec {}"",
+            logPrefix,
+            newCodec,
+            userCodec);
+        return;
+      }
+    }
+    // Technically this would cover the two previous cases as well, but we want precise messages.
+    try {
+      TypeCodec<?> cachedCodec =
+          getCachedCodec(newCodec.getCqlType(), newCodec.getJavaType(), false);
+      LOG.warn(
+          ""[{}] Ignoring codec {} because it collides with previously generated codec {}"",
+          logPrefix,
+          newCodec,
+          cachedCodec);
+      return;
+    } catch (CodecNotFoundException ignored) {
+      // Catching the exception is ugly, but it avoids breaking the internal API (e.g. by adding a","[{'comment': 'maybe we should log a warning?', 'commenter': 'tomekl007'}, {'comment': 'It would be overkill imo because we are only testing if such a codec has been generated; the nominal case would be that such a code should not exist, so the exception is actually the expected outcome.', 'commenter': 'adutra'}]"
1322,core/src/main/java/com/datastax/oss/driver/api/core/type/codec/registry/CodecRegistry.java,"@@ -40,13 +40,24 @@
  *
  * <p>They may also provide additional mappings to other Java types (for use with methods such as
  * {@link Row#get(int, Class)}, {@link TupleValue#set(int, Object, Class)}, etc.)
+ *
+ * <p>The default implementation returned by the driver also implements {@link
+ * MutableCodecRegistry}, and we strongly recommend that custom implementations do as well. The two
+ * interfaces are only separate for backward compatibility, because mutability was introduced in
+ * 4.3.0.
  */
 public interface CodecRegistry {
   /**
    * An immutable instance, that only handles built-in driver types (that is, primitive types, and
    * collections, tuples, and user defined types thereof).
    */
-  CodecRegistry DEFAULT = new DefaultCodecRegistry(""default"");
+  CodecRegistry DEFAULT =
+      new DefaultCodecRegistry(""default"") {
+        @Override
+        public void register(TypeCodec<?> newCodec) {
+          throw new IllegalStateException(""CodecRegistry.DEFAULT is immutable"");","[{'comment': '`java.lang.UnsupportedOperationException` would be a better fit here imho.', 'commenter': 'adutra'}, {'comment': ""You're right, I will change that."", 'commenter': 'olim7t'}]"
1322,core/src/main/java/com/datastax/oss/driver/api/core/type/codec/registry/MutableCodecRegistry.java,"@@ -0,0 +1,58 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core.type.codec.registry;
+
+import com.datastax.oss.driver.api.core.type.codec.TypeCodec;
+
+/**
+ * A codec registry that can be extended with new user codecs at runtime.
+ *
+ * <p>This interface only exists to preserve backward compatibility. In practice, the default {@link
+ * CodecRegistry} implementation returned by the driver implements this interface, so it can safely
+ * be cast.
+ *
+ * <p>However {@link CodecRegistry#DEFAULT} is immutable. It implements this interface, but {@link
+ * #register(TypeCodec)} throws an {@link IllegalStateException}.","[{'comment': ""If you change the thrown exception as I suggested don't forget to change this line as well."", 'commenter': 'adutra'}]"
1322,core/src/main/java/com/datastax/oss/driver/internal/core/type/codec/registry/DefaultCodecRegistry.java,"@@ -61,10 +78,9 @@ public DefaultCodecRegistry(
       @Nullable BiFunction<CacheKey, TypeCodec<?>, Integer> cacheWeigher,
       int maximumCacheWeight,
       @Nullable BiConsumer<CacheKey, TypeCodec<?>> cacheRemovalListener,
-      @NonNull TypeCodec<?>[] primitiveCodecs,
-      @NonNull TypeCodec<?>[] userCodecs) {
+      @NonNull TypeCodec<?>[] primitiveCodecs) {","[{'comment': 'You could use varargs here.', 'commenter': 'adutra'}]"
1330,core/src/test/java/com/datastax/oss/driver/api/core/cql/StatementBuilderTest.java,"@@ -0,0 +1,77 @@
+/*","[{'comment': ""A unit test for a property setter on a builder is (arguably) overkill... especially since there was no existing test for other builder functionality.  That said, I had a pseudo-integration test for confirming that everything worked as expected.. so switching that over to a unit test was pretty trivial.\r\n\r\nI'm good with scrapping this if it's judged to be unnecessary, but it shouldn't be... harmful really."", 'commenter': 'absurdfarce'}]"
1330,core/src/main/java/com/datastax/oss/driver/api/core/cql/StatementBuilder.java,"@@ -157,7 +157,13 @@ public SelfT setIdempotence(@Nullable Boolean idempotent) {
   /** @see Statement#setTracing(boolean) */
   @NonNull
   public SelfT setTracing() {
-    this.tracing = true;
+    return setTracing(true);","[{'comment': 'Maybe we should add a short explanation in the javadoc of this method, along the lines of: ""this method exists for historical reasons, it is just a shortcut for setTracing(true)"".', 'commenter': 'olim7t'}, {'comment': 'Agreed, this seems reasonable... will change.', 'commenter': 'absurdfarce'}]"
1330,core/src/test/java/com/datastax/oss/driver/api/core/cql/StatementBuilderTest.java,"@@ -0,0 +1,77 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core.cql;
+
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertTrue;
+
+import org.junit.Test;
+
+public class StatementBuilderTest {
+
+  private static class NullStatementBuilder
+      extends StatementBuilder<NullStatementBuilder, SimpleStatement> {","[{'comment': ""I don't mind creating a class, but why not just use `SimpleStatementBuilder`? The only thing it needs is a query string."", 'commenter': 'olim7t'}, {'comment': ""The functionality in question is defined in the abstract superclass so I wanted a way to get at it as cleanly as possible.  While admittedly quite unlikely there's always the possibility that a subclass could modify the behaviour... so I stayed away from SimpleStatementBuilder."", 'commenter': 'absurdfarce'}]"
1330,core/src/test/java/com/datastax/oss/driver/api/core/cql/StatementBuilderTest.java,"@@ -0,0 +1,77 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core.cql;
+
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertTrue;
+
+import org.junit.Test;
+
+public class StatementBuilderTest {
+
+  private static class NullStatementBuilder
+      extends StatementBuilder<NullStatementBuilder, SimpleStatement> {
+
+    public NullStatementBuilder() {
+      super();
+    }
+
+    public NullStatementBuilder(SimpleStatement template) {
+      super(template);
+    }
+
+    @Override
+    public SimpleStatement build() {
+      return null;
+    }
+  }
+
+  @Test
+  public void should_handle_set_tracing_without_args() {
+
+    NullStatementBuilder builder = new NullStatementBuilder();
+    assertFalse(builder.tracing);","[{'comment': ""Nit: we prefer AssertJ for assertions.\r\n```\r\nimport static com.datastax.oss.driver.Assertions.assertThat;\r\n...\r\nassertThat(builder.tracing).isFalse()\r\n```\r\nNot a huge difference for a simple boolean, but for more complex types like collections it's way more expressive."", 'commenter': 'olim7t'}, {'comment': ""This makes sense.  I saw the use of AssertJ elsewhere but I went to the JUnit ones out of muscle memory.  There's no reason to make a special case for this one test, though, so I'll update to use AssertJ."", 'commenter': 'absurdfarce'}]"
1335,core/src/main/java/com/datastax/oss/driver/api/core/metadata/schema/KeyspaceMetadata.java,"@@ -245,4 +245,6 @@ default String describeWithChildren(boolean pretty) {
 
     return builder.build();
   }
+
+  boolean shallowEquals(Object other);","[{'comment': 'This should be default, in order to avoid a breaking change. You can implement it in terms of the other interface methods.', 'commenter': 'olim7t'}, {'comment': 'good idea, pushed a fix', 'commenter': 'nastra'}]"
1336,core/src/test/java/com/datastax/oss/driver/internal/core/context/MockedDriverContextFactory.java,"@@ -0,0 +1,68 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.context;
+
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.when;
+
+import com.datastax.oss.driver.api.core.config.DriverConfig;
+import com.datastax.oss.driver.api.core.config.DriverConfigLoader;
+import com.datastax.oss.driver.api.core.config.DriverExecutionProfile;
+import com.datastax.oss.driver.api.core.metadata.NodeStateListener;
+import com.datastax.oss.driver.api.core.metadata.schema.SchemaChangeListener;
+import com.datastax.oss.driver.api.core.session.ProgrammaticArguments;
+import com.datastax.oss.driver.api.core.tracker.RequestTracker;
+import com.datastax.oss.driver.api.core.type.codec.TypeCodec;
+import com.datastax.oss.driver.shaded.guava.common.collect.Maps;
+import java.util.Optional;
+
+public class MockedDriverContextFactory {
+
+  public static DefaultDriverContext defaultDriverContext() {
+    return defaultDriverContext(Optional.empty());
+  }
+
+  public static DefaultDriverContext defaultDriverContext(
+      Optional<DriverExecutionProfile> profileOption) {
+
+    /* If the caller provided a profile use that, otherwise make a new one */
+    final DriverExecutionProfile profile = profileOption.orElse(mock(DriverExecutionProfile.class));
+
+    /* Setup machinery to connect the input DriverExecutionProfile to the config loader */
+    final DriverConfig driverConfig = mock(DriverConfig.class);
+    final DriverConfigLoader configLoader = mock(DriverConfigLoader.class);
+    when(configLoader.getInitialConfig()).thenReturn(driverConfig);
+    when(driverConfig.getDefaultProfile()).thenReturn(profile);
+
+    /* Setup remaining mocks */
+    final NodeStateListener nodeStateListener = mock(NodeStateListener.class);
+    final SchemaChangeListener schemaChangeListener = mock(SchemaChangeListener.class);
+    final RequestTracker requestTracker = mock(RequestTracker.class);
+    final ClassLoader classLoader = mock(ClassLoader.class);
+
+    ProgrammaticArguments args =
+        ProgrammaticArguments.builder()
+            .addTypeCodecs(new TypeCodec[0])
+            .withNodeStateListener(nodeStateListener)
+            .withSchemaChangeListener(schemaChangeListener)
+            .withRequestTracker(requestTracker)
+            .withLocalDatacenters(Maps.newHashMap())
+            .withNodeFilters(Maps.newHashMap())
+            .withClassLoader(classLoader)
+            .build();
+    return new DefaultDriverContext(configLoader, args);","[{'comment': ""With the conversion of this test to the new constructor there are only a few integration tests continuing to use the old (deprecated) constructor.  I'm happy to change those tests to use the new constructor (and remove the old constructor entirely) as part of this PR if that's judged to be useful.... I don't have enough context to know if that's a Good Idea :tm: or not."", 'commenter': 'absurdfarce'}, {'comment': ""Yes, we didn't go over the tests when we deprecated the constructor, but it would be a good idea to do it.\r\nI'd just like to keep that as a separate commit though, so maybe it's simpler to open a separate PR."", 'commenter': 'olim7t'}, {'comment': ""Seems reasonable; I'll handle that in a separate PR once this goes in"", 'commenter': 'absurdfarce'}]"
1336,core/src/test/java/com/datastax/oss/driver/internal/core/context/DefaultDriverContextTest.java,"@@ -0,0 +1,87 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.context;
+
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.when;
+
+import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
+import com.datastax.oss.driver.api.core.config.DriverExecutionProfile;
+import com.datastax.oss.driver.internal.core.protocol.Lz4Compressor;
+import com.datastax.oss.driver.internal.core.protocol.SnappyCompressor;
+import com.datastax.oss.protocol.internal.Compressor;
+import com.datastax.oss.protocol.internal.NoopCompressor;
+import io.netty.buffer.ByteBuf;
+import java.util.Optional;
+import org.junit.Test;
+
+public class DefaultDriverContextTest {
+
+  private DefaultDriverContext buildMockedContextWithoutDef() {
+
+    DriverExecutionProfile defaultProfile = mock(DriverExecutionProfile.class);
+    when(defaultProfile.isDefined(DefaultDriverOption.PROTOCOL_COMPRESSION))
+        .thenReturn(Boolean.FALSE);
+    return MockedDriverContextFactory.defaultDriverContext(Optional.of(defaultProfile));
+  }
+
+  private DefaultDriverContext buildMockedContextWithDef(String compression) {
+
+    DriverExecutionProfile defaultProfile = mock(DriverExecutionProfile.class);
+    when(defaultProfile.isDefined(DefaultDriverOption.PROTOCOL_COMPRESSION))
+        .thenReturn(Boolean.TRUE);
+    when(defaultProfile.getString(DefaultDriverOption.PROTOCOL_COMPRESSION))
+        .thenReturn(compression);
+    return MockedDriverContextFactory.defaultDriverContext(Optional.of(defaultProfile));
+  }
+
+  @Test
+  public void should_create_lz4_compressor() {
+
+    DefaultDriverContext ctx = buildMockedContextWithDef(""lz4"");
+    Compressor<ByteBuf> compressor = ctx.getCompressor();
+    assertThat(compressor).isNotNull();
+    assertThat(compressor).isInstanceOf(Lz4Compressor.class);
+  }
+
+  @Test
+  public void should_create_snappy_compressor() {
+
+    DefaultDriverContext ctx = buildMockedContextWithDef(""snappy"");
+    Compressor<ByteBuf> compressor = ctx.getCompressor();
+    assertThat(compressor).isNotNull();
+    assertThat(compressor).isInstanceOf(SnappyCompressor.class);
+  }
+
+  @Test
+  public void should_create_noop_compressor_if_undefined() {
+
+    DefaultDriverContext ctx = buildMockedContextWithoutDef();
+    Compressor<ByteBuf> compressor = ctx.getCompressor();
+    assertThat(compressor).isNotNull();
+    assertThat(compressor).isInstanceOf(NoopCompressor.class);
+  }
+
+  @Test
+  public void should_create_noop_compressor_if_defined_as_none() {
+
+    DefaultDriverContext ctx = buildMockedContextWithDef(""none"");
+    Compressor<ByteBuf> compressor = ctx.getCompressor();
+    assertThat(compressor).isNotNull();
+    assertThat(compressor).isInstanceOf(NoopCompressor.class);
+  }","[{'comment': 'We were indirectly testing the cases above via the StartupOptionsBuilderTest but it seemed worthwhile to explicitly test this as the DefaultDriverContext level as well.  As a nice side effect the additional need for a mocked DefaultDriverContext instance (perhaps given an execution profile) encouraged the move of that logic into a discrete factory impl.', 'commenter': 'absurdfarce'}]"
1336,core/src/main/java/com/datastax/oss/driver/internal/core/context/DefaultDriverContext.java,"@@ -361,20 +361,23 @@ protected EventBus buildEventBus() {
   @SuppressWarnings(""unchecked"")
   protected Compressor<ByteBuf> buildCompressor() {
     DriverExecutionProfile defaultProfile = getConfig().getDefaultProfile();
-    if (defaultProfile.isDefined(DefaultDriverOption.PROTOCOL_COMPRESSION)) {
-      String name = defaultProfile.getString(DefaultDriverOption.PROTOCOL_COMPRESSION);
-      if (name.equalsIgnoreCase(""lz4"")) {
+    if (!defaultProfile.isDefined(DefaultDriverOption.PROTOCOL_COMPRESSION)) {
+      return Compressor.none();
+    }
+
+    String name = defaultProfile.getString(DefaultDriverOption.PROTOCOL_COMPRESSION);
+    switch (name) {
+      case ""lz4"":","[{'comment': ""The options were case-insensitive, let's preserve that behavior.\r\nIf you want to keep the switch statement, you can lower-case `name`."", 'commenter': 'olim7t'}, {'comment': ""Good catch; that was a straight miss on my part. :(  Will fix, and I'll probably add some unit tests around that to validate going forward."", 'commenter': 'absurdfarce'}]"
1336,core/src/test/java/com/datastax/oss/driver/internal/core/context/StartupOptionsBuilderTest.java,"@@ -87,22 +61,43 @@ private void assertDefaultStartupOptions(Startup startup) {
   }
 
   @Test
-  public void should_build_minimal_startup_options() {
-    buildDriverContext();
-    Startup startup = new Startup(defaultDriverContext.getStartupOptions());
+  public void should_build_startup_options_with_no_compression_if_undefined() {
+
+    DefaultDriverContext ctx = MockedDriverContextFactory.defaultDriverContext();
+    Startup startup = new Startup(ctx.getStartupOptions());
     assertThat(startup.options).doesNotContainKey(Startup.COMPRESSION_KEY);
     assertDefaultStartupOptions(startup);
   }
 
   @Test
-  public void should_build_startup_options_with_compression() {
-    when(defaultProfile.isDefined(DefaultDriverOption.PROTOCOL_COMPRESSION))
-        .thenReturn(Boolean.TRUE);
-    when(defaultProfile.getString(DefaultDriverOption.PROTOCOL_COMPRESSION)).thenReturn(""lz4"");
-    buildDriverContext();
-    Startup startup = new Startup(defaultDriverContext.getStartupOptions());
-    // assert the compression option is present
-    assertThat(startup.options).containsEntry(Startup.COMPRESSION_KEY, ""lz4"");
+  public void should_build_startup_options_with_no_compression_if_defined_as_none() {
+
+    DefaultDriverContext ctx = buildMockedContext(""none"");
+    Startup startup = new Startup(ctx.getStartupOptions());
+    assertThat(startup.options).doesNotContainKey(Startup.COMPRESSION_KEY);
     assertDefaultStartupOptions(startup);
   }
+
+  @Test
+  public void should_build_startup_options_with_lz4_compression() {
+
+    doCompressorStartupOptionsTest(""lz4"");
+  }
+
+  @Test
+  public void should_build_startup_options_with_snappy_compression() {
+
+    doCompressorStartupOptionsTest(""snappy"");
+  }
+
+  @Test
+  public void should_build_startup_options_with_invalid_compression() {","[{'comment': 'Nit: maybe this should be called `should_fail_to_build...`', 'commenter': 'olim7t'}, {'comment': ""Agreed... I think that's probably a more descriptive name."", 'commenter': 'absurdfarce'}]"
1336,core/src/test/java/com/datastax/oss/driver/internal/core/context/StartupOptionsBuilderTest.java,"@@ -87,22 +61,43 @@ private void assertDefaultStartupOptions(Startup startup) {
   }
 
   @Test
-  public void should_build_minimal_startup_options() {
-    buildDriverContext();
-    Startup startup = new Startup(defaultDriverContext.getStartupOptions());
+  public void should_build_startup_options_with_no_compression_if_undefined() {
+
+    DefaultDriverContext ctx = MockedDriverContextFactory.defaultDriverContext();
+    Startup startup = new Startup(ctx.getStartupOptions());
     assertThat(startup.options).doesNotContainKey(Startup.COMPRESSION_KEY);
     assertDefaultStartupOptions(startup);
   }
 
   @Test
-  public void should_build_startup_options_with_compression() {
-    when(defaultProfile.isDefined(DefaultDriverOption.PROTOCOL_COMPRESSION))
-        .thenReturn(Boolean.TRUE);
-    when(defaultProfile.getString(DefaultDriverOption.PROTOCOL_COMPRESSION)).thenReturn(""lz4"");
-    buildDriverContext();
-    Startup startup = new Startup(defaultDriverContext.getStartupOptions());
-    // assert the compression option is present
-    assertThat(startup.options).containsEntry(Startup.COMPRESSION_KEY, ""lz4"");
+  public void should_build_startup_options_with_no_compression_if_defined_as_none() {
+
+    DefaultDriverContext ctx = buildMockedContext(""none"");
+    Startup startup = new Startup(ctx.getStartupOptions());
+    assertThat(startup.options).doesNotContainKey(Startup.COMPRESSION_KEY);
     assertDefaultStartupOptions(startup);
   }
+
+  @Test
+  public void should_build_startup_options_with_lz4_compression() {
+
+    doCompressorStartupOptionsTest(""lz4"");
+  }
+
+  @Test
+  public void should_build_startup_options_with_snappy_compression() {
+
+    doCompressorStartupOptionsTest(""snappy"");","[{'comment': 'I think you can replace those one line tests with one parameterized test', 'commenter': 'tomekl007'}, {'comment': ""You're thinking of... the built-in JUnit support for parameterized tests @tomekl007 ?  Yeah, I think that could be made to work... but there are some things I'm not super fond of there:\r\n\r\n* The reliance on test state (via either the constructor usage or injection) isn't super-awesome\r\n* The inability to mix and match parameterized vs. other tests in the same class seems problematic\r\n\r\nSeems like the only way to address that last problem would be to create a distinct test just for these two cases... and that seems like an increase in complexity.\r\n\r\nOr were you thinking of something more along the lines of https://github.com/Pragmatists/JUnitParams?  That addresses both concerns above with a much simpler API.  I could get behind that... is it a big issue to add that as another dep?"", 'commenter': 'absurdfarce'}, {'comment': 'We are already using parameterized tests in this project via: \r\n`com.tngtech.java.junit.dataprovider`\r\n(see for example `DataTypeIT`) - I think we should stick to that library and approach of parameterized tests :) ', 'commenter': 'tomekl007'}, {'comment': 'Nice!  I think that takes care of my concerns mentioned above... will gladly switch to that.  Thanks for the pointer @tomekl007 !', 'commenter': 'absurdfarce'}]"
1336,core/src/main/java/com/datastax/oss/driver/internal/core/context/DefaultDriverContext.java,"@@ -361,20 +361,23 @@ protected EventBus buildEventBus() {
   @SuppressWarnings(""unchecked"")
   protected Compressor<ByteBuf> buildCompressor() {","[{'comment': ""Suggestion: maybe instead of mocking whole `DriverContext` you can just test this method?\r\nIt seems that this method is only using the `DriverExecutionProfile defaultProfile` so you don't need to access anything from `driverContext` besides that. \r\nBut to make it happen you would need to create a second method that is taking `DriverExecutionProfile` as an argument and this method is just delegating to that second method: `buildCompressor(){return buildCompressor(getConfig().getDefaultProfile())}` but I don't know if this is worth the effort."", 'commenter': 'tomekl007'}, {'comment': ""It's an interesting idea @tomekl007 , but I'm reluctant to change the API of DefaultDriverContext to make that happen.  Your point does remind me of something I was wondering, though.\r\n\r\nBecause of the isolation built into the various components of DefaultDriverContext we probably don't _need_ all the mocks created in MockedDriverContextFactory; we can probably just pass in nulls for most of the dependencies and only do the bits that actually setup the plumbing for the config loader.  I just tested that out locally and it _does_ work.  But given that we already have a working impl now _with_ all the mocks it feels like passing in a bunch of nulls via ProgrammaticArguments would be a step backwards.\r\n\r\nAbstracting creation of a mock DefaultDriverContext into the factory leaves open the door for future tests to build upon what's there now to provide different ways of creating these mocks in the future, which seems like something of a goodness.  Hopefully this will enable DefaultDriverContextTest to grow over time as things change in the default context. :crossed_fingers: "", 'commenter': 'absurdfarce'}]"
1336,core/src/main/java/com/datastax/oss/driver/internal/core/context/DefaultDriverContext.java,"@@ -361,20 +361,23 @@ protected EventBus buildEventBus() {
   @SuppressWarnings(""unchecked"")
   protected Compressor<ByteBuf> buildCompressor() {
     DriverExecutionProfile defaultProfile = getConfig().getDefaultProfile();
-    if (defaultProfile.isDefined(DefaultDriverOption.PROTOCOL_COMPRESSION)) {
-      String name = defaultProfile.getString(DefaultDriverOption.PROTOCOL_COMPRESSION);
-      if (name.equalsIgnoreCase(""lz4"")) {
+    if (!defaultProfile.isDefined(DefaultDriverOption.PROTOCOL_COMPRESSION)) {
+      return Compressor.none();
+    }
+
+    String name = defaultProfile.getString(DefaultDriverOption.PROTOCOL_COMPRESSION);
+    switch (name.toLowerCase()) {
+      case ""lz4"":","[{'comment': 'This is feeling more and more like a case for an enum.  There used to be such an enum in 3.x.... did we move away from that for a reason?', 'commenter': 'absurdfarce'}, {'comment': ""Unfortunately we don't have `getEnum()` in `DriverExecutionProfile`."", 'commenter': 'adutra'}, {'comment': 'Sure, but you could just as easily create the enum instance within this method, something like:\r\n\r\n```java\r\n  enum ValidCompressor {\r\n    LZ4,\r\n    SNAPPY,\r\n    NONE\r\n  }\r\n\r\n  @SuppressWarnings(""unchecked"")\r\n  protected Compressor<ByteBuf> buildCompressor() {\r\n    DriverExecutionProfile defaultProfile = getConfig().getDefaultProfile();\r\n    if (!defaultProfile.isDefined(DefaultDriverOption.PROTOCOL_COMPRESSION)) {\r\n      return Compressor.none();\r\n    }\r\n\r\n    String name = defaultProfile.getString(DefaultDriverOption.PROTOCOL_COMPRESSION);\r\n    ValidCompressor compressor = ValidCompressor.valueOf(name.toUpperCase());\r\n    switch (compressor) {\r\n      case LZ4:\r\n        return new Lz4Compressor(this);\r\n      case SNAPPY:\r\n        return new SnappyCompressor(this);\r\n      case NONE:\r\n        return Compressor.none();\r\n      default:\r\n        throw new IllegalArgumentException(\r\n            String.format(\r\n                ""Unsupported compression algorithm \'%s\' (from configuration option %s)"",\r\n                name, DefaultDriverOption.PROTOCOL_COMPRESSION.getPath()));\r\n    }\r\n  }\r\n```\r\n\r\nThis is closer to the ProtocolOptions.Compression enum in 3.x.  It\'s probably _marginally_ cleaner than relying on checks for explicit string values but I don\'t feel _terribly_ strongly about it.', 'commenter': 'absurdfarce'}]"
1336,core/src/test/java/com/datastax/oss/driver/internal/core/context/DefaultDriverContextTest.java,"@@ -0,0 +1,93 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.context;
+
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.when;
+
+import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
+import com.datastax.oss.driver.api.core.config.DriverExecutionProfile;
+import com.datastax.oss.driver.internal.core.protocol.Lz4Compressor;
+import com.datastax.oss.driver.internal.core.protocol.SnappyCompressor;
+import com.datastax.oss.protocol.internal.Compressor;
+import com.datastax.oss.protocol.internal.NoopCompressor;
+import io.netty.buffer.ByteBuf;
+import java.util.Optional;
+import org.junit.Test;
+
+public class DefaultDriverContextTest {
+
+  private DefaultDriverContext buildMockedContext(Optional<String> compressionOption) {
+
+    DriverExecutionProfile defaultProfile = mock(DriverExecutionProfile.class);
+    when(defaultProfile.isDefined(DefaultDriverOption.PROTOCOL_COMPRESSION))
+        .thenReturn(compressionOption.isPresent());
+    compressionOption.ifPresent(
+        compression -> {
+          when(defaultProfile.getString(DefaultDriverOption.PROTOCOL_COMPRESSION))
+              .thenReturn(compression);
+        });
+    return MockedDriverContextFactory.defaultDriverContext(Optional.of(defaultProfile));
+  }
+
+  private void doCreateCompressorTest(Optional<String> configVal, Class<?> expectedClz) {
+
+    DefaultDriverContext ctx = buildMockedContext(configVal);
+    Compressor<ByteBuf> compressor = ctx.getCompressor();
+    assertThat(compressor).isNotNull();
+    assertThat(compressor).isInstanceOf(expectedClz);
+  }
+
+  @Test
+  public void should_create_lz4_compressor() {
+
+    doCreateCompressorTest(Optional.of(""lz4""), Lz4Compressor.class);
+
+    /* Config should be case-insensitive */
+    doCreateCompressorTest(Optional.of(""lZ4""), Lz4Compressor.class);
+    doCreateCompressorTest(Optional.of(""Lz4""), Lz4Compressor.class);
+    doCreateCompressorTest(Optional.of(""LZ4""), Lz4Compressor.class);
+  }
+
+  @Test
+  public void should_create_snappy_compressor() {
+
+    doCreateCompressorTest(Optional.of(""snappy""), SnappyCompressor.class);
+
+    /* Config should be case-insensitive */
+    doCreateCompressorTest(Optional.of(""SNAPPY""), SnappyCompressor.class);
+    doCreateCompressorTest(Optional.of(""sNaPpY""), SnappyCompressor.class);
+    doCreateCompressorTest(Optional.of(""SNapPy""), SnappyCompressor.class);
+  }
+
+  @Test
+  public void should_create_noop_compressor_if_undefined() {
+
+    doCreateCompressorTest(Optional.empty(), NoopCompressor.class);
+  }
+
+  @Test
+  public void should_create_noop_compressor_if_defined_as_none() {
+
+    doCreateCompressorTest(Optional.of(""none""), NoopCompressor.class);
+
+    /* Config should be case-insensitive */
+    doCreateCompressorTest(Optional.of(""NONE""), NoopCompressor.class);
+    doCreateCompressorTest(Optional.of(""NoNe""), NoopCompressor.class);
+    doCreateCompressorTest(Optional.of(""nONe""), NoopCompressor.class);","[{'comment': ""This (and the similar cases above) are a great argument for generative testing... but per the conversation with @tomekl007 elsewhere re: parameterized tests I didn't want to introduce a new dependency just for this case.  I'm happy to look at this more deeply if something like that is desired, though."", 'commenter': 'absurdfarce'}, {'comment': 'We should use `com.tngtech.java.junit.dataprovider`, see my comment:\r\nhttps://github.com/datastax/java-driver/pull/1336#discussion_r329432605', 'commenter': 'tomekl007'}, {'comment': ""It's not exactly equivalent to generative testing... but switching to the dataprovider seems to strike a pretty reasonable middle ground."", 'commenter': 'absurdfarce'}]"
1336,core/src/test/java/com/datastax/oss/driver/internal/core/context/DefaultDriverContextTest.java,"@@ -0,0 +1,85 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.context;
+
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.when;
+
+import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
+import com.datastax.oss.driver.api.core.config.DriverExecutionProfile;
+import com.datastax.oss.driver.internal.core.protocol.Lz4Compressor;
+import com.datastax.oss.driver.internal.core.protocol.SnappyCompressor;
+import com.datastax.oss.protocol.internal.Compressor;
+import com.datastax.oss.protocol.internal.NoopCompressor;
+import com.tngtech.java.junit.dataprovider.DataProvider;
+import com.tngtech.java.junit.dataprovider.DataProviderRunner;
+import io.netty.buffer.ByteBuf;
+import java.util.Optional;
+import org.junit.Test;
+import org.junit.runner.RunWith;
+
+@RunWith(DataProviderRunner.class)
+public class DefaultDriverContextTest {
+
+  private DefaultDriverContext buildMockedContext(Optional<String> compressionOption) {
+
+    DriverExecutionProfile defaultProfile = mock(DriverExecutionProfile.class);
+    when(defaultProfile.isDefined(DefaultDriverOption.PROTOCOL_COMPRESSION))
+        .thenReturn(compressionOption.isPresent());
+    compressionOption.ifPresent(
+        compression -> {
+          when(defaultProfile.getString(DefaultDriverOption.PROTOCOL_COMPRESSION))
+              .thenReturn(compression);
+        });
+    return MockedDriverContextFactory.defaultDriverContext(Optional.of(defaultProfile));
+  }
+
+  private void doCreateCompressorTest(Optional<String> configVal, Class<?> expectedClz) {
+
+    DefaultDriverContext ctx = buildMockedContext(configVal);
+    Compressor<ByteBuf> compressor = ctx.getCompressor();
+    assertThat(compressor).isNotNull();
+    assertThat(compressor).isInstanceOf(expectedClz);
+  }","[{'comment': 'I thought about creating a data provider for both params, something along the lines of the following:\r\n\r\n```java\r\n  @DataProvider\r\n  public static Object[][] createCompressorDP() {\r\n\r\n\t  Object[][] rv = new Object[12][2];\r\n\t  int idx = 0;\r\n\t  for (String name : new String[] { ""lz4"", ""lZ4"", ""Lz4"", ""LZ4""}) {\r\n\t\t  rv[idx++] = new Object[] { name, Lz4Compressor.class };\r\n\t  }\r\n\t  for (String name : new String[] { ""snappy"", ""SNAPPY"", ""sNaPpY"", ""SNapPy""}) {\r\n\t\t  rv[idx++] = new Object[] { name, SnappyCompressor.class };\r\n\t  }\r\n\t  for (String name : new String[] { ""none"", ""NONE"", ""NoNe"", ""nONe""}) {\r\n\t\t  rv[idx++] = new Object[] { name, NoopCompressor.class };\r\n\t  }\r\n\t  return rv;\r\n  }\r\n\r\n  @Test\r\n  @UseDataProvider(""createCompressorDP"")\r\n  public void should_create_compressor(String configVal, Class<?> expectedClz) {\r\n\r\n\t    DefaultDriverContext ctx = buildMockedContext(Optional.of(configVal));\r\n\t    Compressor<ByteBuf> compressor = ctx.getCompressor();\r\n\t    assertThat(compressor).isNotNull();\r\n\t    assertThat(compressor).isInstanceOf(expectedClz);\r\n\t  }\r\n```\r\n\r\nThis made the test output just a bit harder to read, though, so I settled on the middle ground above.', 'commenter': 'absurdfarce'}]"
1336,core/src/test/java/com/datastax/oss/driver/internal/core/context/DefaultDriverContextTest.java,"@@ -0,0 +1,85 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.context;
+
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.when;
+
+import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
+import com.datastax.oss.driver.api.core.config.DriverExecutionProfile;
+import com.datastax.oss.driver.internal.core.protocol.Lz4Compressor;
+import com.datastax.oss.driver.internal.core.protocol.SnappyCompressor;
+import com.datastax.oss.protocol.internal.Compressor;
+import com.datastax.oss.protocol.internal.NoopCompressor;
+import com.tngtech.java.junit.dataprovider.DataProvider;
+import com.tngtech.java.junit.dataprovider.DataProviderRunner;
+import io.netty.buffer.ByteBuf;
+import java.util.Optional;
+import org.junit.Test;
+import org.junit.runner.RunWith;
+
+@RunWith(DataProviderRunner.class)
+public class DefaultDriverContextTest {
+
+  private DefaultDriverContext buildMockedContext(Optional<String> compressionOption) {
+
+    DriverExecutionProfile defaultProfile = mock(DriverExecutionProfile.class);
+    when(defaultProfile.isDefined(DefaultDriverOption.PROTOCOL_COMPRESSION))
+        .thenReturn(compressionOption.isPresent());
+    compressionOption.ifPresent(
+        compression -> {
+          when(defaultProfile.getString(DefaultDriverOption.PROTOCOL_COMPRESSION))
+              .thenReturn(compression);
+        });
+    return MockedDriverContextFactory.defaultDriverContext(Optional.of(defaultProfile));
+  }
+
+  private void doCreateCompressorTest(Optional<String> configVal, Class<?> expectedClz) {
+
+    DefaultDriverContext ctx = buildMockedContext(configVal);
+    Compressor<ByteBuf> compressor = ctx.getCompressor();
+    assertThat(compressor).isNotNull();
+    assertThat(compressor).isInstanceOf(expectedClz);
+  }
+
+  @Test
+  @DataProvider({""lz4"", ""lZ4"", ""Lz4"", ""LZ4""})","[{'comment': 'LGTM, but to make it even more maintanable maybe you could coalesce it to one test?\r\n```\r\n@DataProvider({""lz4"", ""lZ4"", ""Lz4"", ""LZ4"",""snappy"", ""SNAPPY"", ""sNaPpY"", ""SNapPy"",""none"", ""NONE"", ""NoNe"", ""nONe"" })\r\npublic void should_create_compressor()\r\n```', 'commenter': 'tomekl007'}, {'comment': ""Unfortunately you need to match up an input name with an expected compressor class to make the assertions work out... so you'd need a bit more to make it work.  I experimented with something like what you're describing in https://github.com/datastax/java-driver/pull/1336/files#r329775123 but it seemed like splitting things out into discrete test cases kept the output readable while still keeping things maintainable."", 'commenter': 'absurdfarce'}]"
1336,changelog/README.md,"@@ -9,6 +9,7 @@
 - [documentation] JAVA-2416: Update paging section in the manual
 - [improvement] JAVA-2402: Add setTracing(boolean) to StatementBuilder
 - [bug] JAVA-2466: Set idempotence to null in BatchStatement.newInstance
+- [improvement] JAVA-2452: Allow ""none"" as a compression option","[{'comment': ""Nit: changelog entries should be inserted first in their section for new drivers, and last for legacy drivers. I know, we don't make it any easier :-D "", 'commenter': 'adutra'}]"
1336,core/src/test/java/com/datastax/oss/driver/internal/core/context/MockedDriverContextFactory.java,"@@ -0,0 +1,68 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.context;
+
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.when;
+
+import com.datastax.oss.driver.api.core.config.DriverConfig;
+import com.datastax.oss.driver.api.core.config.DriverConfigLoader;
+import com.datastax.oss.driver.api.core.config.DriverExecutionProfile;
+import com.datastax.oss.driver.api.core.metadata.NodeStateListener;
+import com.datastax.oss.driver.api.core.metadata.schema.SchemaChangeListener;
+import com.datastax.oss.driver.api.core.session.ProgrammaticArguments;
+import com.datastax.oss.driver.api.core.tracker.RequestTracker;
+import com.datastax.oss.driver.api.core.type.codec.TypeCodec;
+import com.datastax.oss.driver.shaded.guava.common.collect.Maps;
+import java.util.Optional;
+
+public class MockedDriverContextFactory {
+
+  public static DefaultDriverContext defaultDriverContext() {
+    return defaultDriverContext(Optional.empty());
+  }
+
+  public static DefaultDriverContext defaultDriverContext(
+      Optional<DriverExecutionProfile> profileOption) {
+
+    /* If the caller provided a profile use that, otherwise make a new one */
+    final DriverExecutionProfile profile = profileOption.orElse(mock(DriverExecutionProfile.class));
+
+    /* Setup machinery to connect the input DriverExecutionProfile to the config loader */
+    final DriverConfig driverConfig = mock(DriverConfig.class);
+    final DriverConfigLoader configLoader = mock(DriverConfigLoader.class);
+    when(configLoader.getInitialConfig()).thenReturn(driverConfig);
+    when(driverConfig.getDefaultProfile()).thenReturn(profile);
+
+    /* Setup remaining mocks */
+    final NodeStateListener nodeStateListener = mock(NodeStateListener.class);
+    final SchemaChangeListener schemaChangeListener = mock(SchemaChangeListener.class);
+    final RequestTracker requestTracker = mock(RequestTracker.class);
+    final ClassLoader classLoader = mock(ClassLoader.class);
+
+    ProgrammaticArguments args =
+        ProgrammaticArguments.builder()
+            .addTypeCodecs(new TypeCodec[0])","[{'comment': 'This line can be removed.', 'commenter': 'adutra'}, {'comment': 'This is a holdover from the original impl.  Seems like there might be merit in explicitly providing something here even if the current impl of provides a sane default.  Either way, it seems like the usage here should match what we do for withLocalDatacenters() and withNodeFilters(); seems like we should rely on builder defaults for none or all three, does it not?', 'commenter': 'absurdfarce'}]"
1336,core/src/test/java/com/datastax/oss/driver/internal/core/context/MockedDriverContextFactory.java,"@@ -0,0 +1,68 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.context;
+
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.when;
+
+import com.datastax.oss.driver.api.core.config.DriverConfig;
+import com.datastax.oss.driver.api.core.config.DriverConfigLoader;
+import com.datastax.oss.driver.api.core.config.DriverExecutionProfile;
+import com.datastax.oss.driver.api.core.metadata.NodeStateListener;
+import com.datastax.oss.driver.api.core.metadata.schema.SchemaChangeListener;
+import com.datastax.oss.driver.api.core.session.ProgrammaticArguments;
+import com.datastax.oss.driver.api.core.tracker.RequestTracker;
+import com.datastax.oss.driver.api.core.type.codec.TypeCodec;
+import com.datastax.oss.driver.shaded.guava.common.collect.Maps;
+import java.util.Optional;
+
+public class MockedDriverContextFactory {","[{'comment': 'Could be package-private.', 'commenter': 'adutra'}, {'comment': 'Agreed, will fix', 'commenter': 'absurdfarce'}]"
1336,core/src/test/java/com/datastax/oss/driver/internal/core/context/MockedDriverContextFactory.java,"@@ -0,0 +1,68 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.context;
+
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.when;
+
+import com.datastax.oss.driver.api.core.config.DriverConfig;
+import com.datastax.oss.driver.api.core.config.DriverConfigLoader;
+import com.datastax.oss.driver.api.core.config.DriverExecutionProfile;
+import com.datastax.oss.driver.api.core.metadata.NodeStateListener;
+import com.datastax.oss.driver.api.core.metadata.schema.SchemaChangeListener;
+import com.datastax.oss.driver.api.core.session.ProgrammaticArguments;
+import com.datastax.oss.driver.api.core.tracker.RequestTracker;
+import com.datastax.oss.driver.api.core.type.codec.TypeCodec;
+import com.datastax.oss.driver.shaded.guava.common.collect.Maps;
+import java.util.Optional;
+
+public class MockedDriverContextFactory {
+
+  public static DefaultDriverContext defaultDriverContext() {
+    return defaultDriverContext(Optional.empty());
+  }
+
+  public static DefaultDriverContext defaultDriverContext(
+      Optional<DriverExecutionProfile> profileOption) {
+
+    /* If the caller provided a profile use that, otherwise make a new one */
+    final DriverExecutionProfile profile = profileOption.orElse(mock(DriverExecutionProfile.class));","[{'comment': 'Unnecessary `final` keywords.', 'commenter': 'adutra'}, {'comment': ""I'm inclined to keep them (unless there's a convention not to do so).  Explicitly marking those refs as final seems like a reasonable communication of intent."", 'commenter': 'absurdfarce'}]"
1338,core/src/main/java/com/datastax/oss/driver/api/core/cql/StatementBuilder.java,"@@ -123,6 +124,11 @@ public SelfT setRoutingKey(@Nullable ByteBuffer routingKey) {
     return self;
   }
 
+  @NonNull","[{'comment': 'Other methods in this class have this kind of javadocs:\r\n\r\n```\r\n  /** @see Statement#setRoutingKey(ByteBuffer) */\r\n```\r\n\r\nSo I think we should do the same here.', 'commenter': 'adutra'}, {'comment': ""Agreed; I saw those and simply forgot to make sure they were here.  I'll add those in for sure."", 'commenter': 'absurdfarce'}]"
1338,core/src/test/java/com/datastax/oss/driver/api/core/cql/StatementBuilderTest.java,"@@ -16,61 +16,95 @@
 package com.datastax.oss.driver.api.core.cql;
 
 import static org.assertj.core.api.Assertions.assertThat;
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.when;
 
+import java.nio.ByteBuffer;
+import java.util.Random;
 import org.junit.Test;
 
 public class StatementBuilderTest {
 
-  private static class NullStatementBuilder
-      extends StatementBuilder<NullStatementBuilder, SimpleStatement> {
+  private static class MockSimpleStatementBuilder
+      extends StatementBuilder<MockSimpleStatementBuilder, SimpleStatement> {
 
-    public NullStatementBuilder() {
+    public MockSimpleStatementBuilder() {
       super();
     }
 
-    public NullStatementBuilder(SimpleStatement template) {
+    public MockSimpleStatementBuilder(SimpleStatement template) {
       super(template);
     }
 
     @Override
     public SimpleStatement build() {
-      return null;
+
+      SimpleStatement rv = mock(SimpleStatement.class);
+      when(rv.isTracing()).thenReturn(this.tracing);
+      when(rv.getRoutingKey()).thenReturn(this.routingKey);
+      return rv;
     }
   }
 
   @Test
   public void should_handle_set_tracing_without_args() {
 
-    NullStatementBuilder builder = new NullStatementBuilder();
-    assertThat(builder.tracing).isFalse();
+    MockSimpleStatementBuilder builder = new MockSimpleStatementBuilder();
+    assertThat(builder.build().isTracing()).isFalse();
     builder.setTracing();
-    assertThat(builder.tracing).isTrue();
+    assertThat(builder.build().isTracing()).isTrue();
   }
 
   @Test
   public void should_handle_set_tracing_with_args() {
 
-    NullStatementBuilder builder = new NullStatementBuilder();
-    assertThat(builder.tracing).isFalse();
+    MockSimpleStatementBuilder builder = new MockSimpleStatementBuilder();
+    assertThat(builder.build().isTracing()).isFalse();
     builder.setTracing(true);
-    assertThat(builder.tracing).isTrue();
+    assertThat(builder.build().isTracing()).isTrue();
     builder.setTracing(false);
-    assertThat(builder.tracing).isFalse();
+    assertThat(builder.build().isTracing()).isFalse();
   }
 
   @Test
-  public void should_override_template() {
+  public void should_override_set_tracing_in_template() {
 
     SimpleStatement template = SimpleStatement.builder(""select * from system.peers"").build();
-    NullStatementBuilder builder = new NullStatementBuilder(template);
-    assertThat(builder.tracing).isFalse();
+    MockSimpleStatementBuilder builder = new MockSimpleStatementBuilder(template);
+    assertThat(builder.build().isTracing()).isFalse();
     builder.setTracing(true);
-    assertThat(builder.tracing).isTrue();
+    assertThat(builder.build().isTracing()).isTrue();
 
     template = SimpleStatement.builder(""select * from system.peers"").setTracing().build();
-    builder = new NullStatementBuilder(template);
-    assertThat(builder.tracing).isTrue();
+    builder = new MockSimpleStatementBuilder(template);
+    assertThat(builder.build().isTracing()).isTrue();
     builder.setTracing(false);
-    assertThat(builder.tracing).isFalse();
+    assertThat(builder.build().isTracing()).isFalse();
+  }
+
+  private ByteBuffer randomBuffer(Random random) {
+
+    byte[] arr = new byte[8];
+    random.nextBytes(arr);
+    return ByteBuffer.wrap(arr);
+  }
+
+  @Test
+  public void should_match_set_routing_key_vararg() {
+
+    Random random = new Random();
+    ByteBuffer buff1 = randomBuffer(random);","[{'comment': 'Do we really need random byte buffers? What about just this:\r\n\r\n```\r\nByteBuffer buff1 = Bytes.fromHexString(""0xcafe"");\r\nByteBuffer buff2 = Bytes.fromHexString(""0xbabe"");\r\n```', 'commenter': 'adutra'}, {'comment': ""I don't have a strong opinion either way I guess.  I'm sympathetic to the argument for randomized testing (to potentially catch data-driven oddness).  I'm also sympathetic to the argument for predictable testing to catch regressions.\r\n\r\nIf there's a preference for a fixed value here I can make that change as well."", 'commenter': 'absurdfarce'}, {'comment': ""Yeah I hear you; but I think I have a slight preference for test repeatability. Random tests, unless we are really dealing with randomness, are usually of little value; here for instance, I can't see why the actual byte buffer contents would matter â€“ except, of course, for special values such as `null` or empty."", 'commenter': 'adutra'}, {'comment': ""Okay, I'll shift this over to a fixed string in the next round of changes."", 'commenter': 'absurdfarce'}]"
1338,core/src/test/java/com/datastax/oss/driver/api/core/cql/StatementBuilderTest.java,"@@ -16,61 +16,95 @@
 package com.datastax.oss.driver.api.core.cql;
 
 import static org.assertj.core.api.Assertions.assertThat;
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.when;
 
+import java.nio.ByteBuffer;
+import java.util.Random;
 import org.junit.Test;
 
 public class StatementBuilderTest {
 
-  private static class NullStatementBuilder
-      extends StatementBuilder<NullStatementBuilder, SimpleStatement> {
+  private static class MockSimpleStatementBuilder","[{'comment': 'Not sure we need this inner class? I just replaced all references to it with `SimpleStatementBuilder builder = new SimpleStatementBuilder(""IRRELEVANT"");` (or `builder = new SimpleStatementBuilder(template)`) and the tests passed.', 'commenter': 'adutra'}, {'comment': ""This came up in the PR for JAVA-2402 (which introduced the test in question).  The functionality we're testing here is implemented in StatementBuilder so my intent was to test that class directly.  Although it's not necessarily likely a subclass can always modify that behaviour; better to test as close to the functionality in question as possible.\r\n\r\nOriginal comment should be at https://github.com/datastax/java-driver/pull/1330#discussion_r326422401"", 'commenter': 'absurdfarce'}, {'comment': ""IOW we are trying to test an abstract class, and that's why we had to introduce this concrete class. But we shouldn't be testing anything abstract in the first place, only its concrete implementations. Which are 3 currently:\r\n\r\n1. BatchStatementBuilder\r\n2. BoundStatementBuilder\r\n3. SimpleStatementBuilder\r\n\r\nSo why don't you parameterize each test and make it run against an instance of each of the types above?\r\n"", 'commenter': 'adutra'}, {'comment': ""I'm not sure I agree with the reasoning here.\r\n\r\nThere is a function implemented on the abstract super-class StatementBuilder.  That function should be tested just like any other function (so that we know it does the right thing if a sub-class doesn't override it).  We could test that by using an existing sub-class which doesn't (currently) override that method... but that test then is subject to silent changes in behaviour if that sub-class later overrides the method.\r\n\r\nIn order to avoid that concern, the unit test extends the abstract sub-class directly.\r\n\r\nTesting all three concrete sub-classes _in this test_ seems like a bad idea for similar reasons; this test is intended to validate behaviours in the abstract super-class and the introduction of a new concrete sub-class requires an update to this unrelated test in a way that's not obvious.  Now, if you're arguing that we should have similar tests for each of the three concrete sub-classes _in addition to_ this test I could certainly go along with that.\r\n\r\nI'm not opposed to testing the concrete sub-classes at all.  I just want to be very clear about the scope of each individual test."", 'commenter': 'absurdfarce'}, {'comment': ""OK OK I don't feel strong about it. Creating a dummy child class to test a parent abstract class seems reasonable."", 'commenter': 'adutra'}]"
1341,core/src/test/java/com/datastax/oss/driver/internal/core/metadata/schema/queries/DefaultSchemaQueriesFactoryTest.java,"@@ -0,0 +1,130 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.metadata.schema.queries;
+
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.when;
+
+import com.datastax.oss.driver.When;
+import com.datastax.oss.driver.api.core.Version;
+import com.datastax.oss.driver.api.core.config.DriverConfig;
+import com.datastax.oss.driver.api.core.config.DriverExecutionProfile;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import com.datastax.oss.driver.internal.core.channel.DriverChannel;
+import com.datastax.oss.driver.internal.core.context.InternalDriverContext;
+import com.tngtech.java.junit.dataprovider.DataProvider;
+import com.tngtech.java.junit.dataprovider.DataProviderRunner;
+import com.tngtech.java.junit.dataprovider.UseDataProvider;
+import java.util.concurrent.CompletableFuture;
+import org.junit.Test;
+import org.junit.runner.RunWith;
+
+@RunWith(DataProviderRunner.class)
+public class DefaultSchemaQueriesFactoryTest {
+
+  enum Expected {
+    CASS_21(Cassandra21SchemaQueries.class),
+    CASS_22(Cassandra22SchemaQueries.class),
+    CASS_3(Cassandra3SchemaQueries.class),
+    CASS_4(Cassandra4SchemaQueries.class);
+
+    final Class<? extends SchemaQueries> clz;
+
+    Expected(Class<? extends SchemaQueries> clz) {
+      this.clz = clz;
+    }
+
+    public Class<? extends SchemaQueries> getClz() {
+      return clz;
+    }
+  }
+
+  private static When<Expected> whenCassandra =
+      When.<Expected>builder()
+          .args(""2.1.0"")
+          .expect(Expected.CASS_21)
+          .args(""2.2.0"")
+          .expect(Expected.CASS_22)
+          .args(""2.2.1"")
+          .expect(Expected.CASS_22)
+          /* Not a real version, just documenting behaviour of existing impl */
+          .args(""2.3.0"")
+          .expect(Expected.CASS_22)
+          /* We now return you to real versions */
+          .args(""3.0.0"")
+          .expect(Expected.CASS_3)
+          .args(""3.0.1"")
+          .expect(Expected.CASS_3)
+          .args(""3.1.0"")
+          .expect(Expected.CASS_3)
+          .args(""4.0.0"")
+          .expect(Expected.CASS_4)
+          .args(""4.0.1"")
+          .expect(Expected.CASS_4)
+          .args(""4.1.0"")
+          .expect(Expected.CASS_4)
+          .build();
+
+  private static When<Expected> whenDse =
+      When.<Expected>builder()
+          /* DSE 6.0.0 */
+          .args(""4.0.0.2284"")","[{'comment': 'This is failing when I run it. It returns `Expected.CASS_4`', 'commenter': 'tomekl007'}, {'comment': ""Yeah, this is expected.  That's the root of the underlying ticket; we're returning schema queries for DSE < 6.7.0 that don't actually match up to what that version of DSE has available.  I implemented this test as a mechanism to document and validate the underlying problem... as of this commit I haven't actually implemented the fix yet. :)"", 'commenter': 'absurdfarce'}]"
1341,core/src/test/java/com/datastax/oss/driver/internal/core/metadata/schema/queries/DefaultSchemaQueriesFactoryTest.java,"@@ -0,0 +1,130 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.metadata.schema.queries;
+
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.when;
+
+import com.datastax.oss.driver.When;
+import com.datastax.oss.driver.api.core.Version;
+import com.datastax.oss.driver.api.core.config.DriverConfig;
+import com.datastax.oss.driver.api.core.config.DriverExecutionProfile;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import com.datastax.oss.driver.internal.core.channel.DriverChannel;
+import com.datastax.oss.driver.internal.core.context.InternalDriverContext;
+import com.tngtech.java.junit.dataprovider.DataProvider;
+import com.tngtech.java.junit.dataprovider.DataProviderRunner;
+import com.tngtech.java.junit.dataprovider.UseDataProvider;
+import java.util.concurrent.CompletableFuture;
+import org.junit.Test;
+import org.junit.runner.RunWith;
+
+@RunWith(DataProviderRunner.class)
+public class DefaultSchemaQueriesFactoryTest {
+
+  enum Expected {
+    CASS_21(Cassandra21SchemaQueries.class),
+    CASS_22(Cassandra22SchemaQueries.class),
+    CASS_3(Cassandra3SchemaQueries.class),
+    CASS_4(Cassandra4SchemaQueries.class);
+
+    final Class<? extends SchemaQueries> clz;
+
+    Expected(Class<? extends SchemaQueries> clz) {
+      this.clz = clz;
+    }
+
+    public Class<? extends SchemaQueries> getClz() {
+      return clz;
+    }
+  }
+
+  private static When<Expected> whenCassandra =
+      When.<Expected>builder()
+          .args(""2.1.0"")
+          .expect(Expected.CASS_21)
+          .args(""2.2.0"")
+          .expect(Expected.CASS_22)
+          .args(""2.2.1"")
+          .expect(Expected.CASS_22)
+          /* Not a real version, just documenting behaviour of existing impl */
+          .args(""2.3.0"")
+          .expect(Expected.CASS_22)
+          /* We now return you to real versions */
+          .args(""3.0.0"")
+          .expect(Expected.CASS_3)
+          .args(""3.0.1"")
+          .expect(Expected.CASS_3)
+          .args(""3.1.0"")
+          .expect(Expected.CASS_3)
+          .args(""4.0.0"")
+          .expect(Expected.CASS_4)
+          .args(""4.0.1"")
+          .expect(Expected.CASS_4)
+          .args(""4.1.0"")
+          .expect(Expected.CASS_4)
+          .build();
+
+  private static When<Expected> whenDse =
+      When.<Expected>builder()
+          /* DSE 6.0.0 */
+          .args(""4.0.0.2284"")
+          .expect(Expected.CASS_3)
+          /* DSE 6.0.1 */
+          .args(""4.0.0.2349"")
+          .expect(Expected.CASS_3)
+          /* DSE 6.0.2 moved to DSE version (minus dots) in an extra element */
+          .args(""4.0.0.602"")
+          .expect(Expected.CASS_3)
+          /* DSE 6.7.0 continued with the same idea */
+          .args(""4.0.0.670"")
+          .expect(Expected.CASS_3)
+          /* DSE 6.8.0 does the same */
+          .args(""4.0.0.680"")
+          .expect(Expected.CASS_3)
+          .build();
+
+  @DataProvider(format = ""%m %p[1] => %p[0]"")
+  public static Iterable<Iterable<Object>> expected() {
+
+    return whenCassandra.merge(whenDse);
+  }
+
+  @Test
+  @UseDataProvider(""expected"")
+  public void should_return_correct_schema_queries_impl(Expected expected, String version) {
+
+    final Node mockNode = mock(Node.class);
+    when(mockNode.getCassandraVersion()).thenReturn(Version.parse(version));
+
+    DefaultSchemaQueriesFactory factory = buildFactory();
+    SchemaQueries queries =
+        factory.newInstance(mockNode, mock(DriverChannel.class), mock(CompletableFuture.class));","[{'comment': 'You may want to add `SupressWarninig(""unchecked"")` for this one:\r\n`mock(CompletableFuture.class)`', 'commenter': 'tomekl007'}, {'comment': ""Agreed.  I'd noticed this in Eclipse and hadn't actually put in the fix yet.  Will add in the next round of updates."", 'commenter': 'absurdfarce'}]"
1341,core/src/test/java/com/datastax/oss/driver/When.java,"@@ -0,0 +1,94 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver;
+
+import com.datastax.oss.driver.shaded.guava.common.collect.Collections2;
+import com.datastax.oss.driver.shaded.guava.common.collect.ImmutableList;
+import com.datastax.oss.driver.shaded.guava.common.collect.ImmutableMap;
+import java.util.Iterator;
+import java.util.List;
+
+public class When<T> implements Iterable<Iterable<Object>> {","[{'comment': ""Where it is plugin into logic? I don't see any usage of this new class besides the test"", 'commenter': 'tomekl007'}, {'comment': ""Yeah, it's only a testing thing.  I added it to make the data provider definition more explicitly declarative."", 'commenter': 'absurdfarce'}, {'comment': ""To be honest, I'm not a fan. To me, it doesn't make the provider declarations any easier to read.\r\n\r\nIf the goal is to document what each field in the provided data is (and I agree it's a weakness in the array syntax), then I find that a simple comment at the beginning does a pretty decent job. See example [here](https://github.com/datastax/java-driver/blob/4.x/query-builder/src/test/java/com/datastax/oss/driver/api/querybuilder/BuildableQueryTest.java#L42)."", 'commenter': 'olim7t'}, {'comment': 'I personally don\'t find that example nearly as declarative (or as readable) as what\'s in this PR, but at the same time I don\'t want to introduce an impl that\'s only going to be used in some subset of unit tests.  I\'ll see if I can re-work my test into a middle ground between those ideas (and remove the ""when"" impl) but it does seem to me like there\'s _considerable_ room to improve how we define these providers in the future.', 'commenter': 'absurdfarce'}]"
1341,core/src/main/java/com/datastax/oss/driver/internal/core/metadata/token/DefaultReplicationStrategyFactory.java,"@@ -41,6 +41,8 @@ public ReplicationStrategy newInstance(Map<String, String> replicationConfig) {
         return new SimpleReplicationStrategy(replicationConfig);
       case ""org.apache.cassandra.locator.NetworkTopologyStrategy"":
         return new NetworkTopologyReplicationStrategy(replicationConfig, logPrefix);
+      case ""org.apache.cassandra.locator.EverywhereStrategy"":
+        return new EverywhereReplicationStrategy();","[{'comment': 'A drive-by fix noticed while testing this against various DSE versions.  In addition to inappropriate queries against system_virtual_schema for DSE 6.0.x (the heart of this ticket) I was also seeing exceptions for failures to find a replication strategy impl for EverywhereStrategy.  Brought in the fix from the DSE side to address this failure.', 'commenter': 'absurdfarce'}]"
1341,core/src/main/java/com/datastax/oss/driver/api/core/metadata/NodeProperties.java,"@@ -0,0 +1,29 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core.metadata;
+
+import com.datastax.oss.driver.api.core.Version;
+
+/** The keys for the additional properties stored in {@link Node#getExtras()}. */
+public class NodeProperties {","[{'comment': ""Given that this is kind of temporary (we'll use `DseNodeProperties` after we merge the drivers), I'd rather put this in an internal package."", 'commenter': 'olim7t'}, {'comment': ""Makes sense, I'll move it to an equivalent spot in the internal space"", 'commenter': 'absurdfarce'}]"
1341,core/src/main/java/com/datastax/oss/driver/internal/core/metadata/DefaultTopologyMonitor.java,"@@ -347,18 +349,23 @@ private String getPeerTableName() {
       listenAddress = new InetSocketAddress(listenInetAddress, listenPort);
     }
 
-    return DefaultNodeInfo.builder()
-        .withEndPoint(endPoint)
-        .withBroadcastRpcAddress(broadcastRpcAddress)
-        .withBroadcastAddress(broadcastAddress)
-        .withListenAddress(listenAddress)
-        .withDatacenter(row.getString(""data_center""))
-        .withRack(row.getString(""rack""))
-        .withCassandraVersion(row.getString(""release_version""))
-        .withTokens(row.getSetOfString(""tokens""))
-        .withPartitioner(row.getString(""partitioner""))
-        .withHostId(Objects.requireNonNull(row.getUuid(""host_id"")))
-        .withSchemaVersion(row.getUuid(""schema_version""));
+    DefaultNodeInfo.Builder rv =","[{'comment': ""Nit: we don't have rules for short-lived local variable names, but why `rv`?"", 'commenter': 'olim7t'}, {'comment': ""rv = return value.  I use it quite a bit in cases like this, although my preference would be to avoid the variable declaration all together and just return the result of the expression. That's pretty hard to do with a conditional application such as this, though."", 'commenter': 'absurdfarce'}]"
1341,core/src/main/java/com/datastax/oss/driver/internal/core/metadata/schema/queries/DefaultSchemaQueriesFactory.java,"@@ -81,6 +82,19 @@ protected SchemaQueries newInstance(
     } else if (version.compareTo(Version.V4_0_0) < 0) {
       return new Cassandra3SchemaQueries(channel, refreshFuture, config, logPrefix);
     } else {
+
+      /* A bit of custom logic for DSE 6.0.x.  These versions report a Cassandra version of 4.0.0 but don't
+       * have support for system_virtual_schema tables supported by that version.  To compensate we return
+       * the Cassandra 3 schema queries here for those versions */","[{'comment': ""Nit: could you avoid block-style comments (ie. use `//` style instead)?\r\n\r\nSometimes when you're on a test server and all you have is the command line and vi, it's convenient to use `/* ... */` to comment parts of the code, but that only works if it doesn't already contain such comments."", 'commenter': 'olim7t'}, {'comment': ""Sure, although the block syntax is common even throughout the existing code base.  I personally prefer it (I find the double-slash syntax visually very distracting) but I don't care enough to go to the mat for it."", 'commenter': 'absurdfarce'}]"
1348,integration-tests/src/test/java/com/datastax/oss/driver/core/SerializationIT.java,"@@ -0,0 +1,142 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.core;
+
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.rows;
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.serverError;
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.when;
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.assertj.core.api.Assertions.fail;
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.cql.ColumnDefinition;
+import com.datastax.oss.driver.api.core.cql.Row;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import com.datastax.oss.driver.api.core.servererrors.ServerError;
+import com.datastax.oss.driver.api.core.type.DataTypes;
+import com.datastax.oss.driver.api.testinfra.session.SessionRule;
+import com.datastax.oss.driver.api.testinfra.simulacron.SimulacronRule;
+import com.datastax.oss.simulacron.common.cluster.ClusterSpec;
+import java.io.ByteArrayInputStream;
+import java.io.ByteArrayOutputStream;
+import java.io.ObjectInputStream;
+import java.io.ObjectOutputStream;
+import org.junit.Before;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.rules.RuleChain;
+import org.junit.rules.TestRule;
+
+public class SerializationIT {
+  private static final SimulacronRule SIMULACRON_RULE =
+      new SimulacronRule(ClusterSpec.builder().withNodes(1));
+
+  private static final SessionRule<CqlSession> SESSION_RULE =
+      SessionRule.builder(SIMULACRON_RULE).build();
+
+  @ClassRule
+  public static final TestRule CHAIN = RuleChain.outerRule(SIMULACRON_RULE).around(SESSION_RULE);
+
+  @Before
+  public void clear() {
+    SIMULACRON_RULE.cluster().clearPrimes(true);
+  }
+
+  @Test
+  public void should_serialize_node() {
+    // Given
+    Node node = SESSION_RULE.session().getMetadata().getNodes().values().iterator().next();
+
+    // When
+    Node deserializedNode = serializeAndDeserialize(node);
+
+    // Then
+    // verify a few fields, no need to be exhaustive
+    assertThat(deserializedNode.getHostId()).isEqualTo(node.getHostId());
+    assertThat(deserializedNode.getEndPoint()).isEqualTo(node.getEndPoint());
+    assertThat(deserializedNode.getCassandraVersion()).isEqualTo(node.getCassandraVersion());
+  }
+
+  @Test
+  public void should_serialize_driver_exception() {
+    // Given
+    SIMULACRON_RULE.cluster().prime(when(""mock query"").then(serverError(""mock server error"")));
+    try {
+      SESSION_RULE.session().execute(""mock query"");
+      fail(""Expected a ServerError"");
+    } catch (ServerError error) {
+      assertThat(error.getExecutionInfo()).isNotNull();
+
+      // When
+      ServerError deserializedError = serializeAndDeserialize(error);
+
+      // Then
+      assertThat(deserializedError.getMessage()).isEqualTo(""mock server error"");
+      assertThat(deserializedError.getCoordinator().getEndPoint())
+          .isEqualTo(error.getCoordinator().getEndPoint());
+      assertThat(deserializedError.getExecutionInfo()).isNull(); // transient
+    }
+  }
+
+  @Test
+  public void should_serialize_row() {
+    // Given
+    SIMULACRON_RULE
+        .cluster()
+        .prime(when(""mock query"").then(rows().row(""t"", ""mock data"").columnTypes(""t"", ""varchar"")));
+    Row row = SESSION_RULE.session().execute(""mock query"").one();
+
+    // When
+    row = serializeAndDeserialize(row);
+
+    // Then
+    ColumnDefinition columnDefinition = row.getColumnDefinitions().get(""t"");
+    assertThat(columnDefinition.getType()).isEqualTo(DataTypes.TEXT);
+    assertThat(row.getString(""t"")).isEqualTo(""mock data"");
+  }
+
+  // Serialization methods copied from SerializationHelper in core tests.
+
+  private static <T> byte[] serialize(T t) {","[{'comment': ""It's the 3rd time already that we are copying-pasting this code: I see one `SerializationHelper` in native-protocol, another one in OSS core. Why don't you re-use the one from OSS core? The test-jar is already being produced, it's just a matter of adding a dependency."", 'commenter': 'adutra'}, {'comment': ""I got ahead of myself a little bit on this one: I think when the two drivers get merged, we should stop deploying a `core` test JAR. Maven central is a community resource and we should use as little of it as possible. Publishing a test JAR for every release just to factor a few lines of trivial code is not a good reason IMHO.\r\n\r\nBut right now we have to publish it anyway because it's used in the DSE driver, which reuses more than a few trivial things I think. I'll add the dependency as you suggest, and we can discuss this again in the driver merge PR."", 'commenter': 'olim7t'}]"
1352,manual/cloud/README.md,"@@ -110,7 +110,63 @@ database.
           }
           ```
 
-    c. Save and close the ConnectDatabase.java file.
+    c. **File-based configuration**. An alternative to the programmatic configuration method 
+    detailed above is to include the information required to connect in the driver's configuration 
+    file (`application.conf`). Merge the following options with any other options that you might 
+    want to include in the configuration file:
+    
+    ```hocon
+   basic {
+     # change this to match the target keyspace
+     session-keyspace = keyspace_name","[{'comment': 'does it need to be specified? I think it will be taken from `secure-connect-bundle`', 'commenter': 'tomekl007'}, {'comment': ""We don't parse the keyspace anymore. We still parse the credentials though, even if they are now always absent."", 'commenter': 'adutra'}, {'comment': 'Ok, but even in that case, the session-keyspace is not required, right?\r\nThe client can specify keyspace later. I am not sure if we should mention that in the docs', 'commenter': 'tomekl007'}, {'comment': ""I am not sure about the keyspace actually. It was said at some point that users would have access to just one keyspace. Not sure if this is valid still. But it doesn't hurt to specify a keyspace anyways, so I would leave it that way. @emerkle826 do you know by chance if a keyspace is required for cloud?"", 'commenter': 'adutra'}, {'comment': ""The last I was aware, you don't need to specify a keyspace, but I believe you are correct in that they will only have access to one keyspace (for now at least). From Jorge:\r\n>I think there was consensus for not parsing the keyspace: users might be able to create their own keyspace in future versions of cloud and it will not be provided when switching to token-based auth"", 'commenter': 'emerkle826'}, {'comment': ""In this case let's leave the keyspace example, it seems that at least for the initial Cloud version it will be necessary to set it since users will only have access to that specific keyspace."", 'commenter': 'adutra'}]"
1352,manual/cloud/README.md,"@@ -121,3 +177,4 @@ database.
 [Download the secure connect bundle - GCP]: https://helpdocs.datastax.com/gcp/dscloud/apollo/dscloudObtainingCredentials.html
 [Download the secure connect bundle - AWS]: https://helpdocs.datastax.com/aws/dscloud/apollo/dscloudObtainingCredentials.html
 [Example pom.xml file]: ../core/integration/#minimal-project-structure
+[driver documentation]: https://docs.datastax.com/en/developer/java-driver/4.3/manual/core/configuration/","[{'comment': 'this link can be simplified to:\r\n```\r\n[driver documentation]: ../core/configuration/\r\n```', 'commenter': 'emerkle826'}]"
1354,driver-core/src/main/java/com/datastax/driver/core/Cluster.java,"@@ -2735,9 +2735,19 @@ public void handle(Message.Response response) {
         case TOPOLOGY_CHANGE:
           ProtocolEvent.TopologyChange tpc = (ProtocolEvent.TopologyChange) event;
           Host.statesLogger.debug(""[{}] received event {}"", tpc.node, tpc.change);
-          // Always do a full refresh for topology changes. This is simpler than trying to infer a
-          // new EndPoint from the rpc_address only.
-          submitNodeListRefresh();
+          // Do NOT translate the address, it will be matched against Host.getBroadcastRpcAddress()
+          // to find the target host.
+          switch (tpc.change) {","[{'comment': 'This is the logic pre-cloud API.', 'commenter': 'adutra'}, {'comment': ""If we do this, we should also re-add the branches for `NEW_NODE` and `REMOVED_NODE` in `NodeRefreshRequestDeliveryCallback`, see [this](https://github.com/datastax/java-driver/blob/a815196b81a6180d555b60b08bcbec2e87673fff/driver-core/src/main/java/com/datastax/driver/core/Cluster.java#L2924). Currently `NodeRefreshDebouncerTest` passes because it gets an UP event for an unknown node and turns it into a full list refresh; the addition is silently ignored.\r\n\r\nBut then we'll run into the issue that we can't create a new node for ADDED because we don't have the host id (like in the UP case where you added a comment below). This is why I changed to a full refresh in the first place."", 'commenter': 'olim7t'}]"
1354,driver-core/src/main/java/com/datastax/driver/core/Cluster.java,"@@ -3070,6 +3080,8 @@ public String toString() {
             case UP:
               Host upHost = metadata.getHost(address);
               if (upHost == null) {
+                // We don't have enough information to create a new Host (we are missing it's ID)
+                // so trigger a full node refresh
                 submitNodeListRefresh();","[{'comment': ""This logic changed with the Cloud API, previously we would create a new Host here calling `Metadata.newHost()` but now it's not possible anymore because we don't have the host ID."", 'commenter': 'adutra'}]"
1354,driver-core/src/test/java/com/datastax/driver/core/StateListenerTest.java,"@@ -52,7 +52,7 @@ public void should_receive_events_when_node_states_change() throws InterruptedEx
     ccm().start(1);
     listener.waitForEvent();
 
-    listener.setExpectedEvent(REMOVE);
+    listener.setExpectedEvent(DOWN);","[{'comment': ""The event received is now DOWN because of the changes in the NodeRefreshRequestDeliveryCallback logic. I don't know how to fix it."", 'commenter': 'adutra'}]"
1354,driver-core/src/main/java/com/datastax/driver/core/Cluster.java,"@@ -2735,9 +2735,19 @@ public void handle(Message.Response response) {
         case TOPOLOGY_CHANGE:
           ProtocolEvent.TopologyChange tpc = (ProtocolEvent.TopologyChange) event;
           Host.statesLogger.debug(""[{}] received event {}"", tpc.node, tpc.change);
-          // Always do a full refresh for topology changes. This is simpler than trying to infer a
-          // new EndPoint from the rpc_address only.
-          submitNodeListRefresh();
+          // Do NOT translate the address, it will be matched against Host.getBroadcastRpcAddress()
+          // to find the target host.
+          switch (tpc.change) {
+            case NEW_NODE:
+              submitNodeRefresh(tpc.node, HostEvent.ADDED);
+              break;
+            case REMOVED_NODE:
+              submitNodeRefresh(tpc.node, HostEvent.REMOVED);
+              break;
+            case MOVED_NODE:","[{'comment': ""Side note: I'm not sure where this MOVED_NODE event comes from, it's not referenced anywhere in the protocol spec.\r\nIt's been in the codebase for a long time, maybe it used to be a thing.\r\n\r\nDriver 4 does not have it (nor do I think it should).\r\n\r\nedit - with my change it will get covered by the `default` case in the switch."", 'commenter': 'olim7t'}]"
1354,driver-core/src/main/java/com/datastax/driver/core/Cluster.java,"@@ -3096,6 +3107,10 @@ public String toString() {
                 }
               }
               break;
+            case REMOVED:","[{'comment': ""OK I see you added this case branch that was somehow missing since the first Cloud API. But I also see that a case branch for `ADDED` existed before the Cloud API:\r\n\r\n```\r\ncase ADDED:\r\n              Host newHost = metadata.newHost(address);\r\n              Host previous = metadata.addIfAbsent(newHost);\r\n              if (previous == null) {\r\n                futures.add(schedule(hostAdded(newHost)));\r\n              } else if (!previous.isUp()) {\r\n                futures.add(schedule(hostUp(previous)));\r\n              }\r\n              break;\r\n```\r\n\r\nShouldn't we trigger a `submitNodeListRefresh()` for it? Or is it because `ADDED` is not being triggered anymore? (But in this case, maybe we can delete the `HostEvent.ADDED` enum constant.)"", 'commenter': 'adutra'}, {'comment': ""> is it because ADDED is not being triggered anymore?\r\n\r\nYes, that.\r\n\r\nYou're right, I'll remove the unused constant."", 'commenter': 'olim7t'}]"
1361,manual/mapper/daos/delete/README.md,"@@ -105,6 +105,15 @@ The method can return:
     ResultSet deleteIfDescriptionMatches(UUID productId, String expectedDescription);
     // if the condition fails, the result set will contain columns '[applied]' and 'description'
     ```
+  
+* a [BoundStatement]. This is intended for queries with custom IF clauses; where you will execute
+  this statement later or in a batch.
+  
+    ```java
+    @Delete(entityClass = Product.class, customIfClause = ""description = :expectedDescription"")
+    BoundStatement deleteIfDescriptionMatches(UUID productId, String expectedDescription);
+    // if the condition fails, the result set will contain columns '[applied]' and 'description'
+    ```","[{'comment': ""> This is intended for queries with custom IF clauses\r\n\r\nI don't see why, is this a copy-paste error?\r\n\r\nIf so we should also modify the example to something simpler. A simple `BoundStatement delete(Product product)` should suffice."", 'commenter': 'olim7t'}, {'comment': ""This should also be added to the [annotation javadocs](https://docs.datastax.com/en/drivers/java/4.3/com/datastax/oss/driver/api/mapper/annotations/Delete.html).\r\n\r\nThere's a lot of overlap between the javadocs and the manual, we try to get the information out any way we can :)"", 'commenter': 'olim7t'}, {'comment': ""That makes sense, since the primary goal of this feature it to give you the bound statement to do what you will with it later. It's just a bonus that you can also use conditionals etc too if you choose. With how it's documented now it's slightly confusing in that you could only use this if you where using a conditional."", 'commenter': 'fromanator'}, {'comment': 'Added the docs to the annotation classes too.', 'commenter': 'fromanator'}]"
1361,mapper-processor/src/test/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoDeleteMethodGeneratorTest.java,"@@ -105,7 +105,7 @@ public void should_fail_with_expected_error(
       },
       {
         ""Delete methods must return one of [VOID, FUTURE_OF_VOID, BOOLEAN, FUTURE_OF_BOOLEAN, ""
-            + ""RESULT_SET, FUTURE_OF_ASYNC_RESULT_SET]"",
+            + ""RESULT_SET, BOUND_STATEMENT, FUTURE_OF_ASYNC_RESULT_SET]"",","[{'comment': 'Note to team members: we need to amend DSE mapper tests once this gets merged, some of the tests that expect a specific error message are failing with these changes.', 'commenter': 'adutra'}]"
1361,manual/mapper/daos/delete/README.md,"@@ -105,6 +105,13 @@ The method can return:
     ResultSet deleteIfDescriptionMatches(UUID productId, String expectedDescription);
     // if the condition fails, the result set will contain columns '[applied]' and 'description'
     ```
+  
+* a [BoundStatement]. This is intended for queries where you will execute this statement later or in a batch.","[{'comment': 'Nit: our coding convention uses a column limit of 100 characters, could you add a line break here?', 'commenter': 'olim7t'}]"
1361,manual/mapper/daos/query/README.md,"@@ -54,6 +54,9 @@ The method can return:
 
 * a [ResultSet]. The method will return the raw query result, without any conversion.
 
+* a [BoundStatement]. This is intended for queries where you will execute this statement later","[{'comment': 'Added this because I missed it.', 'commenter': 'fromanator'}]"
1361,mapper-runtime/src/main/java/com/datastax/oss/driver/api/mapper/annotations/Query.java,"@@ -83,6 +84,8 @@
  *   <li>an {@link Optional} of an entity class. The method will extract the first row and convert
  *       it, or return {@code Optional.empty()} if the result set is empty.
  *   <li>a {@link ResultSet}. The method will return the raw query result, without any conversion.
+ *   <li>a {@link BoundStatement}. This is intended for cases where you intend to execute this","[{'comment': 'Also added missing `Query` annotation java docs.', 'commenter': 'fromanator'}]"
1371,manual/core/shaded_jar/README.md,"@@ -3,6 +3,10 @@
 The default driver JAR depends on [Netty](http://netty.io/), which is
 used internally for networking.
 
+The driver is compatible with all Netty versions in the range [4.1.7, 4.2.0),
+that is, it can work with any version equal to or higher than 4.1.7, and","[{'comment': 'Compatibility with Netty < 4.1.7 is much trickier, so I preferred to stop here.', 'commenter': 'adutra'}]"
1371,core/src/main/java/com/datastax/oss/driver/internal/core/channel/ConnectInitHandler.java,"@@ -59,8 +58,10 @@ public void connect(
     realConnectPromise.addListener(future -> onRealConnect(ctx));
 
     // Make the caller's promise wait on the other two:
-    PromiseCombiner combiner = new PromiseCombiner(ImmediateEventExecutor.INSTANCE);
-    combiner.addAll(new Future[] {realConnectPromise, initPromise});
+    // JAVA-2569: preserve binary compatibility with Netty < 4.1.34
+    @SuppressWarnings(""deprecation"")
+    PromiseCombiner combiner = new PromiseCombiner();","[{'comment': 'I understand that `PromiseComibner()` constructor is deprecated, maybe we could extract this as a method and do deprecation inside of it? Then we will not need that duplication(https://github.com/datastax/java-driver/pull/1371/files#diff-08569c129701967a21d9a64ee733151eR179), and if someone will need to construct this class, he could call this method that encapsulates the logic?', 'commenter': 'tomekl007'}, {'comment': ""You mean, create a utility class exposing a static method `PromiseCombiner newPromiseCombiner()`? I think I disagree with you for two reasons: 1) the creation of that utility class requires the same amount of lines of code than the current situation so it doesn't bring concision; and 2) I bet we would forget to use this utility class and keep instantiating `PromiseCombiner` by simply doing `new PromiseCombiner()`..."", 'commenter': 'adutra'}]"
1371,core/src/main/java/com/datastax/oss/driver/internal/core/util/concurrent/PromiseCombiner.java,"@@ -0,0 +1,83 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.util.concurrent;
+
+import edu.umd.cs.findbugs.annotations.NonNull;
+import io.netty.util.concurrent.Future;
+import io.netty.util.concurrent.GenericFutureListener;
+import io.netty.util.concurrent.Promise;
+import java.util.concurrent.atomic.AtomicInteger;
+import java.util.concurrent.atomic.AtomicReference;
+import net.jcip.annotations.ThreadSafe;
+
+/**
+ * A thread-safe version of Netty's {@link io.netty.util.concurrent.PromiseCombiner} that uses
+ * proper synchronization to trigger the completion of the aggregate promise.
+ */
+@ThreadSafe
+public class PromiseCombiner {
+
+  /**
+   * Combines the given futures into the given promise, that is, ties the completion of the latter
+   * to that of the formers.
+   *
+   * @param aggregatePromise The promise that will complete when all parents complete.
+   * @param parents The parent futures.
+   */
+  public static void combine(","[{'comment': '@olim7t I changed your design slightly because it felt odd to instantiate an object without doing anything with it after (that is, all the action was happening in its constructor).', 'commenter': 'adutra'}]"
1371,core/src/main/java/com/datastax/oss/driver/internal/core/util/concurrent/PromiseCombiner.java,"@@ -0,0 +1,83 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.util.concurrent;
+
+import edu.umd.cs.findbugs.annotations.NonNull;
+import io.netty.util.concurrent.Future;
+import io.netty.util.concurrent.GenericFutureListener;
+import io.netty.util.concurrent.Promise;
+import java.util.concurrent.atomic.AtomicInteger;
+import java.util.concurrent.atomic.AtomicReference;
+import net.jcip.annotations.ThreadSafe;
+
+/**
+ * A thread-safe version of Netty's {@link io.netty.util.concurrent.PromiseCombiner} that uses
+ * proper synchronization to trigger the completion of the aggregate promise.
+ */
+@ThreadSafe
+public class PromiseCombiner {
+
+  /**
+   * Combines the given futures into the given promise, that is, ties the completion of the latter
+   * to that of the formers.
+   *
+   * @param aggregatePromise The promise that will complete when all parents complete.
+   * @param parents The parent futures.
+   */
+  public static void combine(
+      @NonNull Promise<Void> aggregatePromise, @NonNull Future<?>... parents) {
+    PromiseCombinerListener listener =
+        new PromiseCombinerListener(aggregatePromise, parents.length);
+    for (Future<?> parent : parents) {
+      parent.addListener(listener);
+    }
+  }
+
+  private static class PromiseCombinerListener implements GenericFutureListener<Future<Object>> {
+
+    private final Promise<Void> aggregatePromise;
+    private final AtomicInteger remainingCount;
+    private final AtomicReference<Throwable> aggregateFailureRef = new AtomicReference<>();
+
+    private PromiseCombinerListener(Promise<Void> aggregatePromise, int numberOfParents) {
+      this.aggregatePromise = aggregatePromise;
+      remainingCount = new AtomicInteger(numberOfParents);
+    }
+
+    @Override
+    public void operationComplete(Future<Object> future) {
+      if (!future.isSuccess()) {
+        aggregateFailureRef.updateAndGet(
+            aggregateFailure -> {
+              if (aggregateFailure == null) {
+                aggregateFailure = future.cause();","[{'comment': 'Not wrapping the cause in an `ExecutionException` because `Promise.get()`  already does so.', 'commenter': 'adutra'}, {'comment': ""There's a possibility that the exception we get here doesn't support suppressed exceptions, but I think we can ignore this corner case. The worst that can happen is that other futures' causes will be ignored when we add them at line 68."", 'commenter': 'olim7t'}]"
1371,core/src/main/java/com/datastax/oss/driver/internal/core/util/concurrent/PromiseCombiner.java,"@@ -0,0 +1,83 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.util.concurrent;
+
+import edu.umd.cs.findbugs.annotations.NonNull;
+import io.netty.util.concurrent.Future;
+import io.netty.util.concurrent.GenericFutureListener;
+import io.netty.util.concurrent.Promise;
+import java.util.concurrent.atomic.AtomicInteger;
+import java.util.concurrent.atomic.AtomicReference;
+import net.jcip.annotations.ThreadSafe;
+
+/**
+ * A thread-safe version of Netty's {@link io.netty.util.concurrent.PromiseCombiner} that uses
+ * proper synchronization to trigger the completion of the aggregate promise.
+ */
+@ThreadSafe
+public class PromiseCombiner {
+
+  /**
+   * Combines the given futures into the given promise, that is, ties the completion of the latter
+   * to that of the formers.
+   *
+   * @param aggregatePromise The promise that will complete when all parents complete.
+   * @param parents The parent futures.
+   */
+  public static void combine(
+      @NonNull Promise<Void> aggregatePromise, @NonNull Future<?>... parents) {
+    PromiseCombinerListener listener =
+        new PromiseCombinerListener(aggregatePromise, parents.length);
+    for (Future<?> parent : parents) {
+      parent.addListener(listener);
+    }
+  }
+
+  private static class PromiseCombinerListener implements GenericFutureListener<Future<Object>> {
+
+    private final Promise<Void> aggregatePromise;
+    private final AtomicInteger remainingCount;
+    private final AtomicReference<Throwable> aggregateFailureRef = new AtomicReference<>();
+
+    private PromiseCombinerListener(Promise<Void> aggregatePromise, int numberOfParents) {
+      this.aggregatePromise = aggregatePromise;
+      remainingCount = new AtomicInteger(numberOfParents);
+    }
+
+    @Override
+    public void operationComplete(Future<Object> future) {
+      if (!future.isSuccess()) {
+        aggregateFailureRef.updateAndGet(
+            aggregateFailure -> {
+              if (aggregateFailure == null) {
+                aggregateFailure = future.cause();
+              } else {
+                aggregateFailure.addSuppressed(future.cause());
+              }
+              return aggregateFailure;
+            });
+      }
+      if (remainingCount.decrementAndGet() == 0) {
+        Throwable aggregateFailure = aggregateFailureRef.get();
+        if (aggregateFailure != null) {
+          aggregatePromise.tryFailure(aggregateFailure);","[{'comment': 'The aggregated promise is now completed in an arbitrary thread. I think this is OK for us since the 2 usages we had so far also mix promises from different event executors. But just wanted to point this out here for reviewers.', 'commenter': 'adutra'}, {'comment': ""Completing a promise is thread-safe so I don't think this matters."", 'commenter': 'olim7t'}]"
1371,core/src/test/java/com/datastax/oss/driver/internal/core/util/concurrent/PromiseCombinerTest.java,"@@ -0,0 +1,62 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.util.concurrent;
+
+import static org.assertj.core.api.Assertions.assertThat;
+
+import io.netty.util.concurrent.EventExecutor;
+import io.netty.util.concurrent.ImmediateEventExecutor;
+import io.netty.util.concurrent.Promise;
+import java.io.IOException;
+import org.junit.Test;
+
+public class PromiseCombinerTest {
+
+  private final EventExecutor executor = ImmediateEventExecutor.INSTANCE;
+
+  @Test
+  public void should_complete_normally_if_all_parents_complete_normally() {
+    // given
+    Promise<Void> promise = executor.newPromise();
+    Promise<Void> parent1 = executor.newPromise();
+    Promise<Void> parent2 = executor.newPromise();
+    // when
+    PromiseCombiner.combine(promise, parent1, parent2);
+    parent1.setSuccess(null);
+    parent2.setSuccess(null);
+    // then
+    assertThat(promise.isSuccess()).isTrue();
+  }
+
+  @Test
+  public void should_complete_exceptionally_if_any_parent_completes_exceptionally() {
+    // given
+    Promise<Void> promise = executor.newPromise();
+    Promise<Void> parent1 = executor.newPromise();
+    Promise<Void> parent2 = executor.newPromise();
+    Promise<Void> parent3 = executor.newPromise();
+    NullPointerException npe = new NullPointerException();
+    IOException ioe = new IOException();
+    // when
+    PromiseCombiner.combine(promise, parent1, parent2, parent3);
+    parent1.setSuccess(null);
+    parent2.setFailure(npe);
+    parent3.setFailure(ioe);","[{'comment': 'Is it possible to do `setFailure(null)`? If so, maybe we should add a test case for it?', 'commenter': 'tomekl007'}, {'comment': ""It's not allowed, you get: `java.lang.NullPointerException: cause`."", 'commenter': 'adutra'}]"
1371,core/src/main/java/com/datastax/oss/driver/internal/core/util/concurrent/PromiseCombiner.java,"@@ -0,0 +1,83 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.util.concurrent;
+
+import edu.umd.cs.findbugs.annotations.NonNull;
+import io.netty.util.concurrent.Future;
+import io.netty.util.concurrent.GenericFutureListener;
+import io.netty.util.concurrent.Promise;
+import java.util.concurrent.atomic.AtomicInteger;
+import java.util.concurrent.atomic.AtomicReference;
+import net.jcip.annotations.ThreadSafe;
+
+/**
+ * A thread-safe version of Netty's {@link io.netty.util.concurrent.PromiseCombiner} that uses
+ * proper synchronization to trigger the completion of the aggregate promise.
+ */
+@ThreadSafe
+public class PromiseCombiner {
+
+  /**
+   * Combines the given futures into the given promise, that is, ties the completion of the latter
+   * to that of the formers.
+   *
+   * @param aggregatePromise The promise that will complete when all parents complete.
+   * @param parents The parent futures.
+   */
+  public static void combine(
+      @NonNull Promise<Void> aggregatePromise, @NonNull Future<?>... parents) {
+    PromiseCombinerListener listener =
+        new PromiseCombinerListener(aggregatePromise, parents.length);
+    for (Future<?> parent : parents) {
+      parent.addListener(listener);
+    }
+  }
+
+  private static class PromiseCombinerListener implements GenericFutureListener<Future<Object>> {
+
+    private final Promise<Void> aggregatePromise;
+    private final AtomicInteger remainingCount;
+    private final AtomicReference<Throwable> aggregateFailureRef = new AtomicReference<>();
+
+    private PromiseCombinerListener(Promise<Void> aggregatePromise, int numberOfParents) {
+      this.aggregatePromise = aggregatePromise;
+      remainingCount = new AtomicInteger(numberOfParents);
+    }
+
+    @Override
+    public void operationComplete(Future<Object> future) {
+      if (!future.isSuccess()) {
+        aggregateFailureRef.updateAndGet(
+            aggregateFailure -> {
+              if (aggregateFailure == null) {
+                aggregateFailure = future.cause();
+              } else {
+                aggregateFailure.addSuppressed(future.cause());","[{'comment': 'is it possible that `future.cause()` will be `null`? or `future`? if so our logic may not work because of this check: https://github.com/datastax/java-driver/pull/1371/files/8ae9ee79a503454657dda8814ccb3b132e6e0db4..3242753311848dd161315230777949c7de750dc3#diff-ab0e22e2e333da4968ffb1f8318c49afR75', 'commenter': 'tomekl007'}, {'comment': '`future` itself cannot be null. `future.cause()` can be null if 1) the future is not completed yet, or if it completed successfully. But here these conditions are not met, since when `operationComplete` is called the future is necessarily complete, and since we tested that `future.isSuccess() == false`, we know that the future failed.', 'commenter': 'adutra'}]"
1373,integration-tests/src/test/java/com/datastax/oss/driver/api/core/cloud/CloudIT.java,"@@ -219,4 +222,23 @@ public void should_error_when_contact_points_and_secure_bundle_used() {
         .hasMessage(
             ""Can't use withCloudSecureConnectBundle and addContactPoint(s). They are mutually exclusive."");
   }
+
+  @Test
+  public void should_error_when_ssl_context_and_secure_bundle_used() {
+    // given
+    try {
+      Path bundle = proxyRule.getProxy().getBundleWithoutCredentialsPath();
+      CqlSessionBuilder builder =
+          CqlSession.builder()
+              .withCloudSecureConnectBundle(bundle)
+              .withAuthCredentials(""cassandra"", ""cassandra"")
+              .withSslContext(SSLContext.getInstance(""SSL""));
+      assertThatThrownBy(() -> builder.build())
+          .isInstanceOf(IllegalStateException.class)
+          .hasMessage(
+              ""Can't use withCloudSecureConnectBundle and addContactPoint(s). They are mutually exclusive."");
+    } catch (NoSuchAlgorithmException e) {","[{'comment': 'Maybe we could pick an existing `SSLContext` https://github.com/datastax/java-driver/pull/1373/files#diff-d001c2e83a104c3b75049085a3425839R235 to not have that `NoSuchAlgorithmException`? It makes this test a little bit less readable.', 'commenter': 'tomekl007'}, {'comment': 'I think the mistake was to catch `NoSuchAlgorithmException`: it would be better to simply have the test method declare to throw it imo.', 'commenter': 'adutra'}]"
1373,core/src/main/java/com/datastax/oss/driver/api/core/session/SessionBuilder.java,"@@ -523,6 +525,10 @@ public SessionT build() {
           throw new IllegalStateException(
               ""Can't use withCloudSecureConnectBundle and addContactPoint(s). They are mutually exclusive."");
         }
+        if (sslConfigured) {","[{'comment': 'I think you should also check if SSL was configured through configuration, that is, if `datastax-java-driver.advanced.ssl-engine-factory.class` is defined.', 'commenter': 'adutra'}]"
1373,core/src/main/java/com/datastax/oss/driver/api/core/session/SessionBuilder.java,"@@ -525,7 +525,9 @@ public SessionT build() {
           throw new IllegalStateException(
               ""Can't use withCloudSecureConnectBundle and addContactPoint(s). They are mutually exclusive."");
         }
-        if (sslConfigured) {
+        String configuredSSLFactory =
+            defaultConfig.getString(DefaultDriverOption.SSL_ENGINE_FACTORY_CLASS, null);
+        if (sslConfigured || configuredSSLFactory != null) {","[{'comment': '`isDefined()` would look slightly better:\r\n\r\n```\r\nboolean sslDefined =\r\n    defaultConfig.isDefined(DefaultDriverOption.SSL_ENGINE_FACTORY_CLASS);\r\nif (sslConfigured || sslDefined) {\r\n```', 'commenter': 'adutra'}]"
1395,changelog/README.md,"@@ -3,7 +3,7 @@
 <!-- Note: contrary to 3.x, insert new entries *first* in their section -->
 
 ### 4.5.0 (in progress)
-
+- [new feature] JAVA-2226: Provide user-friendly programmatic configuration for kerberos","[{'comment': ""```suggestion\r\n- [new feature] JAVA-2625: Provide user-friendly programmatic configuration for kerberos\r\n```\r\nAlso we've left a blank line after the title for other sections so far, let's keep it consistent."", 'commenter': 'olim7t'}]"
1395,core/src/main/java/com/datastax/dse/driver/api/core/auth/DseGssApiAuthProviderBase.java,"@@ -182,6 +183,17 @@ public Builder withLoginConfiguration(@Nullable Configuration loginConfiguration
         this.loginConfiguration = loginConfiguration;
         return this;
       }
+      /**
+       * Sets a login configuration that will be used to create a {@link LoginContext}.
+       *
+       * <p>You MUST call either this method or {@link #withSubject(Subject)}; if both are called,
+       * the subject takes precedence, and the login configuration will be ignored.
+       */
+      @NonNull
+      public Builder withLoginConfiguration(@Nullable Map<String, String> loginConfiguration) {","[{'comment': 'Makes sense.\r\n\r\nMaybe the javadoc could explain how this methods differs from the other one, something like: ""Alternative to {@link #withLoginConfiguration(Configuration)}, that builds the configuration from {@code Krb5LoginModule} with the given options"".', 'commenter': 'olim7t'}]"
1395,core/src/main/java/com/datastax/dse/driver/api/core/auth/DseGssApiAuthProviderBase.java,"@@ -182,6 +183,17 @@ public Builder withLoginConfiguration(@Nullable Configuration loginConfiguration
         this.loginConfiguration = loginConfiguration;
         return this;
       }
+      /**
+       * Sets a login configuration that will be used to create a {@link LoginContext}.
+       *
+       * <p>You MUST call either this method or {@link #withSubject(Subject)}; if both are called,
+       * the subject takes precedence, and the login configuration will be ignored.","[{'comment': 'Nit: there are now 3 different methods that can be called. But I think this will be clear enough, trying to reformulate it with 3 methods makes it more confusing.', 'commenter': 'olim7t'}]"
1395,core/src/main/java/com/datastax/dse/driver/internal/core/auth/ProgrammaticDseGssApiAuthProvider.java,"@@ -0,0 +1,170 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.dse.driver.internal.core.auth;
+
+import com.datastax.dse.driver.api.core.auth.DseGssApiAuthProviderBase;
+import com.datastax.oss.driver.api.core.auth.AuthProvider;
+import com.datastax.oss.driver.api.core.metadata.EndPoint;
+import edu.umd.cs.findbugs.annotations.NonNull;
+
+/**
+ * {@link AuthProvider} that provides GSSAPI authenticator instances for clients to connect to DSE
+ * clusters secured with {@code DseAuthenticator}, in a programmatic way.
+ *
+ * <p>To use this provider the corresponding GssApiOptions must be passed into the provider
+ * directly, for example:
+ *
+ * <pre>
+ *    DseGssApiAuthProviderBase.GssApiOptions.builder();
+ *     Map<String, String> loginConfig =
+ *         ImmutableMap.of(
+ *             ""principal"",
+ *             ""user principal here ex cassandra@DATASTAX.COM"",
+ *             ""useKeyTab"",
+ *             ""true"",
+ *             ""refreshKrb5Config"",
+ *             ""true"",
+ *             ""keyTab"",
+ *             ""Path to keytab file here"");
+ *
+ *     builder.withLoginConfiguration(loginConfig);
+ *
+ *     CqlSession session =
+ *              CqlSession.builder()
+ *             .withAuthProvider(new ProgrammaticDseGssApiAuthProvider(builder.build()))
+ *             .build())","[{'comment': ""* you need to assign the first line to a `builder` variable.\r\n* angled brackets won't render correctly in the generated javadoc. The simplest way to address this is to replace them by `&lt;` and `&gt;`\r\n* could you fix the indentation? Generally I create a temporary class for code examples and copy-paste them."", 'commenter': 'olim7t'}]"
1395,integration-tests/src/test/java/com/datastax/dse/driver/api/core/auth/DseGssApiAuthProviderIT.java,"@@ -100,6 +103,35 @@ public void should_not_authenticate_if_keytab_does_not_map_to_valid_principal()
       verifyException(e);
     }
   }
+  /**
+   * Ensures that a Session can be established to a DSE server secured with Kerberos and that simple
+   * queries can be made using a client configuration that is provided via programatic interface
+   */
+  @Test
+  public void should_authenticate_using_kerberos_with_keytab_programatic() {","[{'comment': 'Nit:\r\n```suggestion\r\n  public void should_authenticate_using_kerberos_with_keytab_programmatically() {\r\n```\r\n?', 'commenter': 'olim7t'}]"
1396,bom/pom.xml,"@@ -0,0 +1,63 @@
+<?xml version=""1.0"" encoding=""UTF-8""?>
+<!--
+
+    Copyright DataStax, Inc.
+
+    Licensed under the Apache License, Version 2.0 (the ""License"");
+    you may not use this file except in compliance with the License.
+    You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+    Unless required by applicable law or agreed to in writing, software
+    distributed under the License is distributed on an ""AS IS"" BASIS,
+    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+    See the License for the specific language governing permissions and
+    limitations under the License.
+
+-->
+<project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"">
+  <modelVersion>4.0.0</modelVersion>
+  <parent>
+    <groupId>com.datastax.oss</groupId>
+    <artifactId>java-driver-parent</artifactId>
+    <version>4.5.0-SNAPSHOT</version>
+  </parent>
+  <artifactId>java-driver-bom</artifactId>
+  <packaging>pom</packaging>
+  <name>DataStax Java driver for Apache Cassandra(R) - Bill Of Materials</name>
+  <dependencyManagement>
+    <dependencies>
+      <dependency>
+        <groupId>${project.groupId}</groupId>
+        <artifactId>java-driver-core</artifactId>
+        <version>${project.version}</version>","[{'comment': ""The original reporter of the jira ticket advised against that:\r\n> A good practice is to NOT use project.version property when declaring a version, but the real version\r\n\r\nBut it works, so unless someone can explain why this would be a bad idea, I'm going with the simplest option.\r\n\r\nI would even have omitted the version altogether, because we already have a `<dependencyManagement>` section in the parent POM. But apparently this one doesn't inherit from it."", 'commenter': 'olim7t'}, {'comment': '@mp911de would you mind taking a look? ^', 'commenter': 'adutra'}, {'comment': ""I'm not exactly sure about the full rationale behind why placeholders should not be used in a BOM declaration. From what I know is that placeholders require resolution during consumption. With the current arrangement, a consumer needs to fetch `java-driver-parent` just to be able to resolve version attributes regardless of whether dependencies are included in the project.\r\n\r\nA typical project using the BOM directly shouldn't be impacted too much as they would depend on the Cassandra driver anyway. For frameworks (such as Spring Boot), they typically come with a wide dependency management arrangement that points to pre-defined versions while a lot of users do not use all declared dependency versions."", 'commenter': 'mp911de'}, {'comment': ""The rationale is that `${project.version}` is a bit misleading in the sense of it is a standard property of the project. Right now Maven resolves that token locally to the project (so it uses the version of the bom) but if you look at `${project.version}` in a project that imports the BOM, it does not refer to the same value at all. \r\n\r\nThat's why things like the flattened maven plugin resolves the version and hardcodes it to make it more explicit. I'd recommend using that plugin btw, especially for a bom."", 'commenter': 'snicoll'}]"
1396,bom/pom.xml,"@@ -0,0 +1,63 @@
+<?xml version=""1.0"" encoding=""UTF-8""?>
+<!--
+
+    Copyright DataStax, Inc.
+
+    Licensed under the Apache License, Version 2.0 (the ""License"");
+    you may not use this file except in compliance with the License.
+    You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+    Unless required by applicable law or agreed to in writing, software
+    distributed under the License is distributed on an ""AS IS"" BASIS,
+    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+    See the License for the specific language governing permissions and
+    limitations under the License.
+
+-->
+<project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"">
+  <modelVersion>4.0.0</modelVersion>
+  <parent>
+    <groupId>com.datastax.oss</groupId>
+    <artifactId>java-driver-parent</artifactId>
+    <version>4.5.0-SNAPSHOT</version>
+  </parent>
+  <artifactId>java-driver-bom</artifactId>
+  <packaging>pom</packaging>
+  <name>DataStax Java driver for Apache Cassandra(R) - Bill Of Materials</name>
+  <dependencyManagement>
+    <dependencies>
+      <dependency>
+        <groupId>${project.groupId}</groupId>
+        <artifactId>java-driver-core</artifactId>
+        <version>${project.version}</version>
+      </dependency>
+      <dependency>
+        <groupId>${project.groupId}</groupId>
+        <artifactId>java-driver-core-shaded</artifactId>
+        <version>${project.version}</version>
+      </dependency>
+      <dependency>
+        <groupId>${project.groupId}</groupId>
+        <artifactId>java-driver-mapper-processor</artifactId>
+        <version>${project.version}</version>
+      </dependency>
+      <dependency>
+        <groupId>${project.groupId}</groupId>
+        <artifactId>java-driver-mapper-runtime</artifactId>
+        <version>${project.version}</version>
+      </dependency>
+      <dependency>
+        <groupId>${project.groupId}</groupId>
+        <artifactId>java-driver-query-builder</artifactId>
+        <version>${project.version}</version>
+      </dependency>
+      <dependency>
+        <groupId>${project.groupId}</groupId>
+        <artifactId>java-driver-test-infra</artifactId>
+        <version>${project.version}</version>
+      </dependency>
+    </dependencies>","[{'comment': ""I don't see the shaded Guava artifact nor native-protocol in this list, isn't the whole purpose to groupe together all driver transitive dependencies so that we make sure users are importing compatible versions of each artifact?"", 'commenter': 'adutra'}, {'comment': "":+1: A BOM helps to resolve version ambiguities as `<dependencyManagement>` affects also versions for transitive dependencies. That being said, you should include all 1st party artifacts that you're shipping. The native protocol and the shaded guava libraries should be included.\r\n\r\nYou can see the effect in the following example: If a project uses two dependencies, that depend each on netty these dependencies use different netty versions, then importing the netty BOM ties the actually used netty version to a single consistent version."", 'commenter': 'mp911de'}, {'comment': ""Anything that's yours and that another bom of yours does not manage should indeed be here. "", 'commenter': 'snicoll'}]"
1396,bom/pom.xml,"@@ -0,0 +1,63 @@
+<?xml version=""1.0"" encoding=""UTF-8""?>
+<!--
+
+    Copyright DataStax, Inc.
+
+    Licensed under the Apache License, Version 2.0 (the ""License"");
+    you may not use this file except in compliance with the License.
+    You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+    Unless required by applicable law or agreed to in writing, software
+    distributed under the License is distributed on an ""AS IS"" BASIS,
+    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+    See the License for the specific language governing permissions and
+    limitations under the License.
+
+-->
+<project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"">
+  <modelVersion>4.0.0</modelVersion>
+  <parent>
+    <groupId>com.datastax.oss</groupId>
+    <artifactId>java-driver-parent</artifactId>","[{'comment': ""you can't really do that. The parent must be as small as possible and, of course, should not include any dependency management at all."", 'commenter': 'snicoll'}]"
1403,core/src/main/java/com/datastax/oss/driver/api/core/metadata/SessionAwareNodeStateListener.java,"@@ -0,0 +1,65 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core.metadata;
+
+import com.datastax.oss.driver.api.core.CqlSessionBuilder;
+import com.datastax.oss.driver.api.core.loadbalancing.NodeDistance;
+import com.datastax.oss.driver.api.core.session.Session;
+import com.datastax.oss.driver.api.core.session.SessionBuilder;
+import edu.umd.cs.findbugs.annotations.NonNull;
+
+/**
+ * A node state listener that gets notified when its owning session is ready to execute requests.
+ *
+ * <p>Note that {@link #onSessionReady(Session)} will not be the first method invoked on this
+ * object; the driver emits node events before that, during the initialization of the session:
+ *
+ * <ul>
+ *   <li>First the driver shuffles the contact points, and tries each one sequentially. For any
+ *       contact point that can't be reached, {@link #onDown(Node)} is invoked; for the one that
+ *       eventually succeeds, {@link #onUp(Node)} is invoked and that node becomes the control node
+ *       (if none succeeds, the session initialization fails and the process stops here).
+ *   <li>The control node's {@code system.peers} table is inspected to discover the remaining nodes
+ *       in the cluster. For any node that wasn't already a contact point, {@link #onAdd(Node)} is
+ *       invoked; for any contact point that doesn't have a corresponding entry in the table, {@link
+ *       #onRemove(Node)} is invoked;
+ *   <li>The load balancing policy computes the nodes' {@link NodeDistance distances}, and, for each","[{'comment': 'Nit: linkplain would look better.', 'commenter': 'adutra'}]"
1403,core/src/main/java/com/datastax/oss/driver/internal/core/session/DefaultSession.java,"@@ -431,7 +433,7 @@ private void afterInitialSchemaRefresh(CqlIdentifier keyspace) {
       }
     }
 
-    private void notifyLifecycleListeners() {
+    private void notifyListeners() {
       for (LifecycleListener lifecycleListener : context.getLifecycleListeners()) {
         try {
           lifecycleListener.onSessionReady();","[{'comment': ""Shouldn't we pass the session here too?"", 'commenter': 'adutra'}, {'comment': 'This has worked without it so far, so I see no reason to change it now. If we reuse `LifecycleListener` in the future, and the new use case requires the session, we can change it then.', 'commenter': 'olim7t'}]"
1403,core/src/main/java/com/datastax/oss/driver/api/core/metadata/SafeInitNodeStateListener.java,"@@ -0,0 +1,188 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core.metadata;
+
+import com.datastax.oss.driver.api.core.session.Session;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Objects;
+import java.util.concurrent.locks.ReadWriteLock;
+import java.util.concurrent.locks.ReentrantReadWriteLock;
+import java.util.function.BiConsumer;
+import net.jcip.annotations.GuardedBy;
+
+/**
+ * A node state listener wrapper that delays (or ignores) init events until after the session is
+ * ready.
+ *
+ * <p>By default, the driver calls node state events, such as {@link #onUp} and {@link #onAdd},
+ * before the session is ready; see {@link NodeStateListener#onSessionReady(Session)} for a detailed
+ * explanation. This can make things complicated if your listener implementation needs the session
+ * to process those events.
+ *
+ * <p>This class wraps another implementation to shield it from those details:
+ *
+ * <pre>
+ * NodeStateListener delegate = ... // your listener implementation
+ *
+ * SafeInitNodeStateListener wrapper =
+ *     new SafeInitNodeStateListener(delegate, true);
+ *
+ * CqlSession session = CqlSession.builder()
+ *     .withNodeStateListener(wrapper)
+ *     .build();
+ * </pre>
+ *
+ * With this setup, {@code delegate.onSessionReady} is guaranteed to be invoked first, before any
+ * other method. The second constructor argument indicates what to do with the method calls that
+ * were ignored before that:
+ *
+ * <ul>
+ *   <li>if {@code true}, they are recorded, and replayed to {@code delegate} immediately after
+ *       {@link #onSessionReady}. They are guaranteed to happen in the original order, and before
+ *       any post-initialization events.
+ *   <li>if {@code false}, they are discarded.
+ * </ul>
+ *
+ * @since 4.6.0
+ */
+public class SafeInitNodeStateListener implements NodeStateListener {
+
+  private final NodeStateListener delegate;
+  private final boolean replayInitEvents;
+
+  // Write lock: recording init events or setting sessionReady
+  // Read lock: reading init events or checking sessionReady
+  private final ReadWriteLock lock = new ReentrantReadWriteLock();
+
+  @GuardedBy(""lock"")
+  private boolean sessionReady;
+
+  @GuardedBy(""lock"")
+  private final List<InitEvent> initEvents = new ArrayList<>();
+
+  /**
+   * Creates a new instance.
+   *
+   * @param delegate the wrapped listener, to which method invocations will be forwarded.
+   * @param replayInitEvents whether to record events during initialization and replay them to the
+   *     child listener once it's created, or just ignore them.
+   */
+  public SafeInitNodeStateListener(@NonNull NodeStateListener delegate, boolean replayInitEvents) {
+    this.delegate = Objects.requireNonNull(delegate);
+    this.replayInitEvents = replayInitEvents;
+  }
+
+  @Override
+  public void onSessionReady(@NonNull Session session) {
+    lock.writeLock().lock();
+    try {
+      sessionReady = true;","[{'comment': ""Should we check `sessionReady` to prevent multiple invocations of the delegate's methods (especially double delivery of init events)? I know the driver won't do this, but application code might."", 'commenter': 'adutra'}, {'comment': 'Hrmm not sure if I follow, the boolean is flipped only once at the end of init, how could it prevent double delivery?', 'commenter': 'olim7t'}, {'comment': 'I was just suggesting the following idiom:\r\n```\r\nlock.writeLock().lock();\r\ntry {\r\n\tif (!sessionReady) {\r\n\t\tsessionReady = true;\r\n\t\tdelegate.onSessionReady(session);\r\n\t\tif (replayInitEvents) {\r\n\t\t\tfor (InitEvent event : initEvents) {\r\n\t\t\t\tevent.invoke(delegate);\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n} finally {\r\n\tlock.writeLock().unlock();\r\n}\r\n```', 'commenter': 'adutra'}, {'comment': ""I see. Yeah I'll add it, as you noted it's not supposed to happen, but it doesn't hurt to have an extra check."", 'commenter': 'olim7t'}]"
1403,core/src/main/java/com/datastax/oss/driver/internal/core/session/SchemaListenerNotifier.java,"@@ -33,6 +34,11 @@
   private final SchemaChangeListener listener;
   private final EventExecutor adminExecutor;
 
+  // It is technically possible that a schema change could happen in the middle of session
+  // initialization. Don't forward events in this case, it would likely do more harm than good if a
+  // listener implementation doesn't expect it.
+  private boolean sessionReady;","[{'comment': ""Unlike node events, I think it's highly unlikely that anyone would be interested in schema changes that happened before the session was ready. So I changed the driver internals to ignore them, instead of introducing a replay mechanism."", 'commenter': 'olim7t'}]"
1410,core/src/main/java/com/datastax/oss/driver/api/core/config/DriverConfigLoader.java,"@@ -58,13 +62,33 @@
    */
   @NonNull
   static DriverConfigLoader fromClasspath(@NonNull String resourceBaseName) {
+    return fromClasspath(resourceBaseName, DRIVER_CLASS_LOADER);
+  }
+
+  /**
+   * Just like {@link #fromClasspath(java.lang.String)} except that the loader will use the provided
+   * {@code ClassLoader}, as opposed to the Driver's {@code ClassLoader}, for the effective
+   * classpath.
+   *
+   * <p>This is intended for use when an application's ClassLoader is different from the Driver's
+   * ClassLoader, and the current Context ClassLoader is not the same CLassLoader that loaded the
+   * application (eg. OSGi bundles). For web deployments not using OSGi, use of this method is
+   * normally not necessary, as the Context ClassLoader in that environment can usually access
+   * application bundled resources.
+   */
+  @NonNull
+  static DriverConfigLoader fromClasspath(
+      @NonNull String resourceBaseName, ClassLoader appClassLoader) {
     return new DefaultDriverConfigLoader(
         () -> {
           ConfigFactory.invalidateCaches();
           Config config =
               ConfigFactory.defaultOverrides()
-                  .withFallback(ConfigFactory.parseResourcesAnySyntax(resourceBaseName))
-                  .withFallback(ConfigFactory.defaultReference())
+                  .withFallback(
+                      ConfigFactory.parseResourcesAnySyntax(
+                          resourceBaseName,
+                          ConfigParseOptions.defaults().setClassLoader(appClassLoader)))","[{'comment': 'For the static methods `fromClasspath`, `fromPath`, `fromFile`, and `fromUrl`, the idea here is to use the App ClassLoader provided for the calls to `ConfigFactory.parseXYZ()` because the ClassLoader for the App will be the one that can access any OSGi bundled resrouces.\r\n\r\nNone of these methods should be needed in a web context (with the app class loader typically being a child of the container/server class loader).', 'commenter': 'emerkle826'}]"
1410,core/src/main/java/com/datastax/oss/driver/api/core/config/DriverConfigLoader.java,"@@ -58,13 +62,33 @@
    */
   @NonNull
   static DriverConfigLoader fromClasspath(@NonNull String resourceBaseName) {
+    return fromClasspath(resourceBaseName, DRIVER_CLASS_LOADER);
+  }
+
+  /**
+   * Just like {@link #fromClasspath(java.lang.String)} except that the loader will use the provided
+   * {@code ClassLoader}, as opposed to the Driver's {@code ClassLoader}, for the effective
+   * classpath.
+   *
+   * <p>This is intended for use when an application's ClassLoader is different from the Driver's
+   * ClassLoader, and the current Context ClassLoader is not the same CLassLoader that loaded the
+   * application (eg. OSGi bundles). For web deployments not using OSGi, use of this method is
+   * normally not necessary, as the Context ClassLoader in that environment can usually access
+   * application bundled resources.
+   */
+  @NonNull
+  static DriverConfigLoader fromClasspath(
+      @NonNull String resourceBaseName, ClassLoader appClassLoader) {
     return new DefaultDriverConfigLoader(
         () -> {
           ConfigFactory.invalidateCaches();
           Config config =
               ConfigFactory.defaultOverrides()
-                  .withFallback(ConfigFactory.parseResourcesAnySyntax(resourceBaseName))
-                  .withFallback(ConfigFactory.defaultReference())
+                  .withFallback(
+                      ConfigFactory.parseResourcesAnySyntax(
+                          resourceBaseName,
+                          ConfigParseOptions.defaults().setClassLoader(appClassLoader)))
+                  .withFallback(ConfigFactory.defaultReference(DRIVER_CLASS_LOADER))","[{'comment': ""Here, we use the Driver's ClassLoader (pulled from `CqlSession.class.getClassLoader()` above) as it should be the one that will have access to the Driver's defaults (i.e. `reference.conf`)"", 'commenter': 'emerkle826'}]"
1410,core/src/main/java/com/datastax/oss/driver/internal/core/config/typesafe/DefaultProgrammaticDriverConfigLoaderBuilder.java,"@@ -37,7 +37,9 @@
     implements ProgrammaticDriverConfigLoaderBuilder {
 
   public static final Supplier<Config> DEFAULT_FALLBACK_SUPPLIER =
-      () -> ConfigFactory.defaultApplication().withFallback(ConfigFactory.defaultReference());
+      () ->
+          ConfigFactory.defaultApplication()
+              .withFallback(ConfigFactory.defaultReference(DriverConfigLoader.DRIVER_CLASS_LOADER));","[{'comment': ""This should fix the programmatic config case where users will provide overrides programmatically for all settings that should override the Driver's defaults, and should correctly be able to access the Driver's defaults by specifying its class loader."", 'commenter': 'emerkle826'}, {'comment': ""Hmm I think this isn't enough. You need to use `defaultApplication(java.lang.ClassLoader)` instead: \r\n\r\n```\r\nConfigFactory.defaultApplication(DriverConfigLoader.DRIVER_CLASS_LOADER)\r\n              .withFallback(ConfigFactory.defaultReference(DriverConfigLoader.DRIVER_CLASS_LOADER))\r\n```"", 'commenter': 'adutra'}, {'comment': 'I did think about doing that, but in a web environment, it may be more limiting. If we do not provide a ClassLoader to TypeSafe\'s `ConfigFactory.defaultApplication()`, it will use the current Thread\'s Context ClassLoader. Please correct me where I\'m wrong:\r\n1) In a normal Java application, the class loaders are all the same as the context class loader.\r\n2) In a web environment, the context class loader should be able to load resources from the app bundle (""WEB-INF/lib"") as well as the web container (""/lib""). If we specify the Driver\'s ClassLoader here, then we could be limiting the ability to find application config if the Driver jar is in the web container\'s system folder (""/lib""), and the app jar is in the app folder (""WEB-INF/lib"").\r\n3) In an OSGi container, neither of these will work for finding application config in the application bundle as the Context ClassLoader isn\'t the ClassLoader for the application, and the Driver\'s ClassLoader is in a different bundle.\r\n\r\nAlso, I don\'t think we really need to look for Application config specifically in the Driver\'s ClassLoader since the Driver doesn\'t ship with an ""application.conf"". It only works if the Driver\'s ClassLoader is the ClassLoader for the app jar (or is a child of the app jar\'s ClassLoader).', 'commenter': 'emerkle826'}, {'comment': 'You are absolutely right ðŸ‘ ', 'commenter': 'adutra'}]"
1410,core/src/main/java/com/datastax/oss/driver/internal/core/config/typesafe/DefaultProgrammaticDriverConfigLoaderBuilder.java,"@@ -46,6 +48,25 @@
 
   private String currentProfileName = DriverExecutionProfile.DEFAULT_NAME;
 
+  /**
+   * Builds a supplier that uses the default driver fallback config, but uses the supplied
+   * ClassLoader (as opposed to the Driver's ClassLoader) for application-specific config overrides.
+   *
+   * <p>This method is primarily intended for cases where programmatic config is desired, but you
+   * also want to provide application defaults that override Driver defaults, and the application is
+   * loaded by a different ClassLoader than the current context of the Driver's ClassLoader. This
+   * can be the situation in an OSGi application bundle that uses programmatic configuration with an
+   * {@code application.conf}.
+   *
+   * @param appClassLoader The application's ClassLoader from which to load application
+   *     configuration defaults.
+   */
+  public static Supplier<Config> getDefaultConfigSupplier(ClassLoader appClassLoader) {
+    return () ->
+        ConfigFactory.defaultApplication(appClassLoader)
+            .withFallback(ConfigFactory.defaultReference(DriverConfigLoader.DRIVER_CLASS_LOADER));
+  }","[{'comment': 'This will behave identically to the `DEFAULT_FALLBACK_SUPPLIER` above, except that it will not rely on the current Context ClassLoader to find application overrides (i.e. `application.conf`), and will instead use the supplied ClassLoader for that.', 'commenter': 'emerkle826'}, {'comment': ""I don't think we should expose such a method, I would instead inline this code in the constructor below."", 'commenter': 'adutra'}, {'comment': ""I wasn't sure if it was a good idea to expose this. I will do as you suggest."", 'commenter': 'emerkle826'}]"
1410,core/src/main/java/com/datastax/oss/driver/internal/core/config/typesafe/DefaultProgrammaticDriverConfigLoaderBuilder.java,"@@ -62,6 +83,18 @@ public DefaultProgrammaticDriverConfigLoaderBuilder() {
     this(DEFAULT_FALLBACK_SUPPLIER, DefaultDriverConfigLoader.DEFAULT_ROOT_PATH);
   }
 
+  /**
+   * An instance of a programmatic builder that uses application specific config defaults from the
+   * specified application CLassLoader to override Driver config defaults for config that is not
+   * supplied programmatically.
+   *
+   * @param appClassLoader The application's ClassLoader from which to load application
+   *     configuration defaults.
+   */
+  public DefaultProgrammaticDriverConfigLoaderBuilder(ClassLoader appClassLoader) {
+    this(getDefaultConfigSupplier(appClassLoader), DefaultDriverConfigLoader.DEFAULT_ROOT_PATH);
+  }","[{'comment': ""There wasn't an easy way to automatically get a hold of an OSGi bundle's ClassLoader from within the Driver's code as the Driver is a separate bundle and doesn't know which bundle's might be using it. For this to work in an OSGi container, an application bundle using the programmatic config loader will have to pass in a reference to its ClassLoader here. This allows us to create a Config supplier that can locate application bundled resources, instead of letting TypeSafe try to use the context ClassLoader, which isn't the same class loader as the OSGi app bundle ClassLoader."", 'commenter': 'emerkle826'}]"
1410,core/src/main/java/com/datastax/oss/driver/api/core/config/DriverConfigLoader.java,"@@ -58,13 +62,33 @@
    */
   @NonNull
   static DriverConfigLoader fromClasspath(@NonNull String resourceBaseName) {
+    return fromClasspath(resourceBaseName, DRIVER_CLASS_LOADER);
+  }
+
+  /**
+   * Just like {@link #fromClasspath(java.lang.String)} except that the loader will use the provided","[{'comment': '1. I would avoid the word ""loader"" since we are talking about a config loader that uses a class loader... to avoid confusion I would rephrase to `... except that application configuration resources will be located using the provided {@link ClassLoader} instead of the driver\'s {@linkplain DriverConfigLoader#DRIVER_CLASS_LOADER default one}`. Just a suggestion, you don\'t have to agree :-)\r\n2. I wonder if the second paragraph is really necessary, given that this is very hard to explain and, arguably, only of concern to a very small set of users.', 'commenter': 'adutra'}]"
1410,core/src/main/java/com/datastax/oss/driver/api/core/config/DriverConfigLoader.java,"@@ -58,13 +62,33 @@
    */
   @NonNull
   static DriverConfigLoader fromClasspath(@NonNull String resourceBaseName) {
+    return fromClasspath(resourceBaseName, DRIVER_CLASS_LOADER);
+  }
+
+  /**
+   * Just like {@link #fromClasspath(java.lang.String)} except that the loader will use the provided
+   * {@code ClassLoader}, as opposed to the Driver's {@code ClassLoader}, for the effective
+   * classpath.
+   *
+   * <p>This is intended for use when an application's ClassLoader is different from the Driver's
+   * ClassLoader, and the current Context ClassLoader is not the same CLassLoader that loaded the
+   * application (eg. OSGi bundles). For web deployments not using OSGi, use of this method is
+   * normally not necessary, as the Context ClassLoader in that environment can usually access
+   * application bundled resources.
+   */
+  @NonNull
+  static DriverConfigLoader fromClasspath(
+      @NonNull String resourceBaseName, ClassLoader appClassLoader) {","[{'comment': 'Missing `@NonNull`. ', 'commenter': 'adutra'}]"
1410,core/src/main/java/com/datastax/oss/driver/api/core/config/DriverConfigLoader.java,"@@ -92,7 +116,22 @@ static DriverConfigLoader fromClasspath(@NonNull String resourceBaseName) {
    */
   @NonNull
   static DriverConfigLoader fromPath(@NonNull Path file) {
-    return fromFile(file.toFile());
+    return fromPath(file, DRIVER_CLASS_LOADER);
+  }
+
+  /**
+   * Just like {@link #fromPath(java.nio.file.Path)} except that the loader will use the provided
+   * {@code ClassLoader}, as opposed to the Driver's {@code ClassLoader}, to find the {@code Path}.
+   *
+   * <p>This is intended for use when an application's ClassLoader is different from the Driver's
+   * ClassLoader, and the current Context ClassLoader is not the same CLassLoader that loaded the
+   * application (eg. OSGi bundles). For web deployments not using OSGi, use of this method is
+   * normally not necessary, as the Context ClassLoader in that environment can usually access
+   * application bundled resources.
+   */
+  @NonNull
+  static DriverConfigLoader fromPath(@NonNull Path file, ClassLoader appClassLoader) {","[{'comment': ""For `Path`, `File` and `URL` you don't need a class loader. The contents are retrieved by other mechanisms that do not depend on the class loader being used, so I think the overloaded methods are not useful."", 'commenter': 'adutra'}]"
1410,core/src/main/java/com/datastax/oss/driver/api/core/config/DriverConfigLoader.java,"@@ -221,6 +294,22 @@ static ProgrammaticDriverConfigLoaderBuilder programmaticBuilder() {
     return new DefaultProgrammaticDriverConfigLoaderBuilder();
   }
 
+  /**
+   * Just like {@link #programmaticBuilder()} except that the loader will use the provided {@code
+   * ClassLoader}, as opposed to the Driver's {@code ClassLoader}, to find application specific
+   * configuration defaults.
+   *
+   * <p>This is intended for use when an application's ClassLoader is different from the Driver's
+   * ClassLoader, and the current Context ClassLoader is not the same CLassLoader that loaded the
+   * application (eg. OSGi bundles). For web deployments not using OSGi, use of this method is
+   * normally not necessary, as the Context ClassLoader in that environment can usually access
+   * application bundled resources.
+   */
+  @NonNull
+  static ProgrammaticDriverConfigLoaderBuilder programmaticBuilder(ClassLoader appClassLoader) {","[{'comment': 'Missing `@NonNull`', 'commenter': 'adutra'}]"
1410,core/src/main/java/com/datastax/oss/driver/internal/core/config/typesafe/DefaultProgrammaticDriverConfigLoaderBuilder.java,"@@ -62,6 +83,18 @@ public DefaultProgrammaticDriverConfigLoaderBuilder() {
     this(DEFAULT_FALLBACK_SUPPLIER, DefaultDriverConfigLoader.DEFAULT_ROOT_PATH);
   }
 
+  /**
+   * An instance of a programmatic builder that uses application specific config defaults from the","[{'comment': ""Again I would reduce the verbosity to avoid confusing 99% of the users that don't need this: \r\n\r\n```\r\nCreates an instance of {@link DefaultProgrammaticDriverConfigLoaderBuilder} that locates application configuration resources using the provided {@link ClassLoader} instead of the driver's {@linkplain DriverConfigLoader#DRIVER_CLASS_LOADER default one}.\r\n```"", 'commenter': 'adutra'}]"
1410,core/src/main/java/com/datastax/oss/driver/api/core/config/DriverConfigLoader.java,"@@ -34,6 +36,8 @@
  */
 public interface DriverConfigLoader extends AutoCloseable {
 
+  static final ClassLoader DRIVER_CLASS_LOADER = CqlSession.class.getClassLoader();","[{'comment': 'Maybe add javadocs here and explain briefly that this is used to load classpath config resources and that most people should not care about it.', 'commenter': 'adutra'}]"
1410,core/src/main/java/com/datastax/oss/driver/internal/core/config/typesafe/DefaultDriverConfigLoader.java,"@@ -55,7 +55,8 @@
   public static final Supplier<Config> DEFAULT_CONFIG_SUPPLIER =
       () -> {
         ConfigFactory.invalidateCaches();
-        return ConfigFactory.load().getConfig(DEFAULT_ROOT_PATH);
+        return ConfigFactory.load(DriverConfigLoader.DRIVER_CLASS_LOADER)","[{'comment': ""Isn't this going to use `DRIVER_CLASS_LOADER` to load both application and reference files? Maybe we need to make this identical to `Default ProgrammaticDriverConfigLoaderBuilder.DEFAULT_FALLBACK_SUPPLIER`."", 'commenter': 'adutra'}]"
1410,core/src/main/java/com/datastax/oss/driver/api/core/config/DriverConfigLoader.java,"@@ -58,13 +62,27 @@
    */
   @NonNull
   static DriverConfigLoader fromClasspath(@NonNull String resourceBaseName) {
+    return fromClasspath(resourceBaseName, DRIVER_CLASS_LOADER);","[{'comment': 'Thinking about your comment [here](https://github.com/datastax/java-driver/pull/1410/files#r391900433): I wonder if this is the best class loader we can pass here and in other similar locations; maybe the context class loader is better because for web deployments it will cover more cases (and for OSGi, as you said, neither class loader would work anyways).', 'commenter': 'adutra'}]"
1410,core/src/main/java/com/datastax/oss/driver/api/core/config/DriverConfigLoader.java,"@@ -58,13 +62,27 @@
    */
   @NonNull
   static DriverConfigLoader fromClasspath(@NonNull String resourceBaseName) {
+    return fromClasspath(resourceBaseName, DRIVER_CLASS_LOADER);
+  }
+
+  /**
+   * Just like {@link #fromClasspath(java.lang.String)} except that application configuration
+   * resources will be located using the provided {@link ClassLoader} instead of the driver's
+   * {@linkplain DriverConfigLoader#DRIVER_CLASS_LOADER default one}.
+   */
+  @NonNull
+  static DriverConfigLoader fromClasspath(
+      @NonNull String resourceBaseName, @NonNull ClassLoader appClassLoader) {
     return new DefaultDriverConfigLoader(
         () -> {
           ConfigFactory.invalidateCaches();","[{'comment': ""This is unrelated but I think we should improve cache invalidation, it's being done more than once in certain cases. I think it's the config loader's responsibility to invalidate the caches, not the config suppliers'."", 'commenter': 'adutra'}, {'comment': 'Do you want me to create a new ticket for this, or address it in this PR?', 'commenter': 'emerkle826'}, {'comment': 'New ticket, because this PR should be released in 4.5.1. BTW, I think you need to change the base branch to 4.5.x, right @olim7t ?', 'commenter': 'adutra'}, {'comment': 'Will do to both.', 'commenter': 'emerkle826'}, {'comment': 'JAVA-2689 created for the cache improvements.', 'commenter': 'emerkle826'}, {'comment': 'Thank you!', 'commenter': 'adutra'}]"
1410,changelog/README.md,"@@ -4,6 +4,7 @@
 
 ### 4.6.0 (in progress)
 
+- [bug] JAVA-2657: DriverConfigLoader.programmaticBuilder() doesn't work in OSGi container","[{'comment': 'We might try to come up with a better name here, suggestion: ""Ability to specify the class loader to use for application-specific classpath resources"".', 'commenter': 'adutra'}]"
1410,manual/osgi/README.md,"@@ -70,6 +70,46 @@ CqlSession session = CqlSession.builder()
     .build();
 ```
 
+### Using a custom `ClassLoader` for Application Bundled configuration
+
+In addition to specifying a `ClassLoader` when constructing a `Session`, you can also specify
+a `ClassLoader` instance on certain `DriverConfigLoader` methods for cases when your OSGi
+application bundle provides overrides to Driver configuration defaults. This is typically done by
+including an `application.conf` file in your application bundle. For example, if you want to use
+[DriverConfigLoader.fromClasspath] to specify a different resource name (other than ""application""):","[{'comment': 'Actually it would be nicer if we started by explaining the general case: using the default config loader with a custom class loader:\r\n\r\n```\r\nCqlSession session = CqlSession.builder()\r\n    .withClassLoader(classLoader)\r\n    .withConfigLoader(new DefaultDriverConfigLoader(classLoader))\r\n    .build();\r\n```', 'commenter': 'adutra'}, {'comment': ""I pushed a small commit that propagates the session builder's CL to the config loader, iif the user doesn't specify their own CL. SO the above snippet can be shortened to:\r\n\r\n```\r\nCqlSession session = CqlSession.builder()\r\n    .withClassLoader(classLoader)\r\n    .build();\r\n```\r\n"", 'commenter': 'adutra'}]"
1410,core/src/main/java/com/datastax/oss/driver/api/core/session/SessionBuilder.java,"@@ -404,8 +413,20 @@ public SelfT withKeyspace(@Nullable String keyspaceName) {
   /**
    * The {@link ClassLoader} to use to reflectively load class names defined in configuration.
    *
-   * <p>If null, the driver attempts to use the same {@link ClassLoader} that loaded the core driver
-   * classes, which is generally the right thing to do.
+   * <p>Unless you define a custom {@link #configLoader}, this class loader will also be used to
+   * locate application-specific configuration resources.
+   *
+   * <p>If you do not provide any custom class loader, the driver will attempt to use the following
+   * ones:
+   *
+   * <ol>
+   *   <li>When reflectively loading class names defined in configuration: same class loader that","[{'comment': ""I think we can improve the defaults, I'm bootstrapping an internal debate on that. Stay tuned."", 'commenter': 'adutra'}]"
1410,core/src/main/java/com/datastax/oss/driver/api/core/session/SessionBuilder.java,"@@ -617,7 +638,13 @@ public SessionT build() {
   @NonNull
   protected final CompletionStage<CqlSession> buildDefaultSessionAsync() {
     try {
-      DriverConfigLoader configLoader = buildIfNull(this.configLoader, this::defaultConfigLoader);
+
+      ProgrammaticArguments programmaticArguments = programmaticArgumentsBuilder.build();
+
+      DriverConfigLoader configLoader =","[{'comment': 'Propagate the CL if no config loader is defined.', 'commenter': 'adutra'}, {'comment': 'This way we avoid having to specify the CL twice as we currently have to:\r\n\r\n```\r\nCqlSession session = CqlSession.builder()\r\n    .withClassLoader(classLoader)\r\n    .withConfigLoader(new DefaultDriverConfigLoader(classLoader))\r\n    .build();\r\n```', 'commenter': 'adutra'}]"
1411,integration-tests/src/test/java/com/datastax/oss/driver/core/cql/QueryTraceIT.java,"@@ -90,4 +91,23 @@ public void should_fetch_trace_when_tracing_enabled() {
     // Don't want to get too deep into event testing because that could change across versions
     assertThat(queryTrace.getEvents()).isNotEmpty();
   }
+
+  @CassandraRequirement(min = ""4.0"", description = ""Check for added port fields"")
+  @Test
+  public void should_fetch_ports() {
+    ExecutionInfo executionInfo =
+        SESSION_RULE
+            .session()
+            .execute(
+                SimpleStatement.builder(""SELECT release_version FROM system.local"")
+                    .setTracing()
+                    .build())
+            .getExecutionInfo();
+
+    assertThat(executionInfo.getTracingId()).isNotNull();
+
+    QueryTrace queryTrace = executionInfo.getQueryTrace();
+    assertThat(queryTrace.getCoordinatorPort()).isEqualTo(7000);
+    assertThat(queryTrace.getEvents().get(0).getSourcePort()).isEqualTo(7000);
+  }","[{'comment': ""I think it would be better to include that in the main test, `should_fetch_trace_when_tracing_enabled`. The condition for ports to be present is:\r\n```java\r\n    boolean expectPorts =\r\n        CCM_RULE.getCassandraVersion().nextStable().compareTo(Version.V4_0_0) >= 0\r\n            && !CCM_RULE.getDseVersion().isPresent();\r\n```\r\nThat way we can also assert that they're set to 0 when not expected."", 'commenter': 'olim7t'}]"
1411,core/src/main/java/com/datastax/oss/driver/api/core/cql/QueryTrace.java,"@@ -43,6 +43,12 @@
   @NonNull
   InetAddress getCoordinator();
 
+  /**
+   * The port of the node that coordinated the query. Prior to C* 4.0 this is not set and will
+   * default to 0.
+   */
+  int getCoordinatorPort();
+","[{'comment': '> I took the approach of adding a separate port int.. another approach would have to used an InetSocketAddress and set the port there. However that would involve me modifying an existing interface method, instead of simply adding to one. Let me know what you think.\r\n\r\nAgreed, but we can have the new method return `InetSocketAddress` if it has a different name. I suggest `getCoordinatorAddress` (for lack of a better idea). We can also make it default to avoid the breaking change:\r\n\r\n```java\r\n  @NonNull\r\n  @Deprecated\r\n  InetAddress getCoordinator();\r\n\r\n  @NonNull\r\n  default InetSocketAddress getCoordinatorAddress() {\r\n    return new InetSocketAddress(getCoordinator(), 0);\r\n  }\r\n```\r\nIt would still be overridden in the default implementation, to return the correct port if available.', 'commenter': 'olim7t'}, {'comment': 'I took your suggestion, and converted the port into an InetSocketAddress, I did this for both the TraceEvent and the QueryTrace. I was able to remove the revapi exception. Let me know if anything else needs to be addressed,  but I think this is buttoned up.', 'commenter': 'GregBestland'}]"
1413,core/src/main/java/com/datastax/oss/driver/api/core/type/codec/TypeCodecs.java,"@@ -78,39 +98,171 @@
   public static final PrimitiveIntCodec INT = new IntCodec();
   public static final PrimitiveLongCodec BIGINT = new BigIntCodec();
   public static final PrimitiveShortCodec SMALLINT = new SmallIntCodec();
+
+  /**
+   * A codec that handles Apache Cassandra(R)'s timestamp type and maps it to Java's {@link
+   * Instant}, using the system's {@linkplain ZoneId#systemDefault() default time zone} as its
+   * source of time zone information. If you need a different time zone, consider other constants in
+   * this class, or call {@link #timestampAt(ZoneId)} instead.
+   *
+   * @see #TIMESTAMP_UTC
+   * @see #timestampAt(ZoneId)
+   */
   public static final TypeCodec<Instant> TIMESTAMP = new TimestampCodec();","[{'comment': ""I'm following the established pattern of proposing 2 constants and 1 method for groups of related timestamp codecs:\r\n\r\n1. Constant for system default zone: `FOO_SYSTEM`\r\n2. Constant for UTC: `FOO_UTC`\r\n3. Factory method for other zones: `fooAt`\r\n\r\nFollowing this logic the constant `TIMESTAMP` should have been named `TIMESTAMP_SYSTEM` but it's too late for that."", 'commenter': 'adutra'}]"
1413,core/src/main/java/com/datastax/oss/driver/internal/core/type/codec/extras/enums/EnumNameCodec.java,"@@ -0,0 +1,58 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.type.codec.extras.enums;
+
+import com.datastax.oss.driver.api.core.type.codec.MappingCodec;
+import com.datastax.oss.driver.api.core.type.codec.TypeCodecs;
+import com.datastax.oss.driver.api.core.type.reflect.GenericType;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import edu.umd.cs.findbugs.annotations.Nullable;
+import java.util.Objects;
+import net.jcip.annotations.Immutable;
+
+/**
+ * A codec that serializes {@link Enum} instances as CQL {@code varchar}s representing their","[{'comment': ""I find it a bit disappointing that javadocs for codecs are not published. They contain very valuable information. Don't you think we should start publishing javadocs for internal classes as well?"", 'commenter': 'adutra'}]"
1413,core/src/main/java/com/datastax/oss/driver/api/core/type/codec/TypeCodecs.java,"@@ -153,6 +335,35 @@
     return new UdtCodec(cqlType);
   }
 
+  /**
+   * Returns a codec that handles Apache Cassandra(R)'s timestamp type and maps it to Java's {@link
+   * Instant}, using the supplied {@link ZoneId} as its source of time zone information.
+   *
+   * @see #TIMESTAMP
+   * @see #TIMESTAMP_UTC
+   */
+  @NonNull
+  public static TypeCodec<Instant> timestampAt(@NonNull ZoneId timeZone) {","[{'comment': 'I think we forgot this method, so I added it now.', 'commenter': 'adutra'}]"
1413,core/src/main/java/com/datastax/oss/driver/api/core/type/codec/TypeCodecs.java,"@@ -163,9 +374,114 @@
    *
    * @see #ZONED_TIMESTAMP_SYSTEM
    * @see #ZONED_TIMESTAMP_UTC
+   * @see #ZONED_TIMESTAMP_PERSISTED
    */
   @NonNull
   public static TypeCodec<ZonedDateTime> zonedTimestampAt(@NonNull ZoneId timeZone) {
     return new ZonedTimestampCodec(timeZone);
   }
+
+  /**
+   * Returns a codec that handles Apache Cassandra(R)'s timestamp type and maps it to Java's {@link
+   * LocalDateTime}, using the supplied {@link ZoneId} as its source of time zone information.
+   *
+   * <p>Note that Apache Cassandra(R)'s timestamp type does not store any time zone; the codecs
+   * created by this method are provided merely as a convenience for users that need to deal with
+   * local date-times in their applications.
+   *
+   * @see #LOCAL_TIMESTAMP_UTC
+   * @see #localTimestampAt(ZoneId)
+   */
+  @NonNull
+  public static TypeCodec<LocalDateTime> localTimestampAt(@NonNull ZoneId timeZone) {
+    return new LocalTimestampCodec(timeZone);
+  }
+
+  /**
+   * Returns a codec mapping CQL lists to Java object arrays. Serialization and deserialization of
+   * elements in the array is delegated to the provided element codec.
+   *
+   * <p>This method is not suitable for Java primitive arrays. Use {@link #BOOLEAN_ARRAY}, {@link
+   * #BYTE_ARRAY}, {@link #SHORT_ARRAY}, {@link #INT_ARRAY}, {@link #LONG_ARRAY}, {@link
+   * #FLOAT_ARRAY} or {@link #DOUBLE_ARRAY} instead.
+   */
+  @NonNull
+  public static <T> TypeCodec<T[]> arrayOf(@NonNull TypeCodec<T> elementCodec) {
+    return new ObjectArrayCodec<>(elementCodec);
+  }
+
+  /**
+   * Returns a codec mapping Java Enums to CQL ints, according to their {@linkplain Enum#ordinal()
+   * ordinals}.
+   *
+   * @see #enumNamesOf(Class)
+   */
+  @NonNull
+  public static <EnumT extends Enum<EnumT>> TypeCodec<EnumT> enumOrdinalsOf(
+      @NonNull Class<EnumT> enumClass) {
+    return new EnumOrdinalCodec<>(enumClass);
+  }
+
+  /**
+   * Returns a codec mapping Java Enums to CQL varchars, according to their programmatic {@linkplain
+   * Enum#name() names}.
+   *
+   * @see #enumOrdinalsOf(Class)
+   */
+  @NonNull
+  public static <EnumT extends Enum<EnumT>> TypeCodec<EnumT> enumNamesOf(
+      @NonNull Class<EnumT> enumClass) {
+    return new EnumNameCodec<>(enumClass);
+  }
+
+  /** Returns a codec wrapping another codec's Java type into {@link Optional} instances. */
+  @NonNull
+  public static <T> TypeCodec<Optional<T>> optionalOf(@NonNull TypeCodec<T> innerCodec) {
+    return new OptionalCodec<>(innerCodec);
+  }
+
+  /**
+   * Returns a codec that maps the given Java type to JSON strings, using a default Jackson JSON
+   * mapper to perform serialization and deserialization of JSON objects.
+   *
+   * @see <a href=""http://wiki.fasterxml.com/JacksonHome"">Jackson JSON Library</a>
+   */
+  @NonNull
+  public static <T> TypeCodec<T> json(@NonNull GenericType<T> javaType) {
+    return new JsonCodec<>(javaType);
+  }
+
+  /**
+   * Returns a codec that maps the given Java class to JSON strings, using a default Jackson JSON
+   * mapper to perform serialization and deserialization of JSON objects.
+   *
+   * @see <a href=""http://wiki.fasterxml.com/JacksonHome"">Jackson JSON Library</a>
+   */
+  @NonNull
+  public static <T> TypeCodec<T> json(@NonNull Class<T> javaType) {
+    return new JsonCodec<>(javaType);
+  }
+
+  /**
+   * Returns a codec that maps the given Java type to JSON strings, using the provided Jackson
+   * {@link JsonMapper} to perform serialization and deserialization of JSON objects.
+   *
+   * @see <a href=""http://wiki.fasterxml.com/JacksonHome"">Jackson JSON Library</a>
+   */
+  @NonNull
+  public static <T> TypeCodec<T> json(
+      @NonNull GenericType<T> javaType, @NonNull JsonMapper jsonMapper) {","[{'comment': 'The exposure of `JsonMapper` causes strange revapi exceptions. We might want to revisit this. Besides, Jackson (non-shaded) is included in the core driver because of the Insights client, which may be removed in the future.', 'commenter': 'adutra'}, {'comment': ""Yeah I'm not a big fan of piggybacking on that dependency, indeed we don't know how long it will be there. There's also a chance thats someone uses an incompatible version.\r\n\r\nHow about moving JSON to the examples?"", 'commenter': 'olim7t'}, {'comment': 'I find it a bit disappointing because moving this to the examples module would require users to copy-paste code. I would like to see something more straightforward. I\'m tempted to re-introduce the ""extras"" module for everything related to Json.', 'commenter': 'adutra'}, {'comment': 'After giving this some more thought, I take back my previous statement. I think it is still better for users if everything is available from driver-core. If we ever remove Insights, we could switch Jackson to an optional dependency.', 'commenter': 'adutra'}]"
1413,core/src/main/java/com/datastax/oss/driver/internal/core/type/codec/extras/OptionalCodec.java,"@@ -0,0 +1,70 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.type.codec.extras;
+
+import com.datastax.oss.driver.api.core.type.codec.MappingCodec;
+import com.datastax.oss.driver.api.core.type.codec.TypeCodec;
+import com.datastax.oss.driver.api.core.type.reflect.GenericType;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import edu.umd.cs.findbugs.annotations.Nullable;
+import java.util.Collection;
+import java.util.Map;
+import java.util.Objects;
+import java.util.Optional;
+import net.jcip.annotations.Immutable;
+
+/**
+ * A codec that wraps other codecs around {@link Optional} instances.
+ *
+ * @param <T> The wrapped Java type.
+ */
+@Immutable
+public class OptionalCodec<T> extends MappingCodec<T, Optional<T>> {
+
+  public OptionalCodec(@NonNull TypeCodec<T> innerCodec) {
+    super(
+        Objects.requireNonNull(innerCodec, ""innerCodec must not be null""),
+        GenericType.optionalOf(innerCodec.getJavaType()));
+  }
+
+  @Override
+  public boolean accepts(@NonNull Object value) {
+    Objects.requireNonNull(value);
+    if (value instanceof Optional) {","[{'comment': 'We agreed not to push the check too deep in `MappingCodec`, but I figured that for Optionals, it was easy enough to do dit so I did.', 'commenter': 'adutra'}]"
1413,core/src/main/java/com/datastax/oss/driver/internal/core/type/codec/extras/array/ByteArrayCodec.java,"@@ -0,0 +1,95 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.type.codec.extras.array;
+
+import com.datastax.oss.driver.api.core.ProtocolVersion;
+import com.datastax.oss.driver.api.core.type.DataTypes;
+import com.datastax.oss.driver.api.core.type.reflect.GenericType;
+import com.datastax.oss.driver.internal.core.type.codec.SimpleBlobCodec;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import java.nio.ByteBuffer;
+import java.util.Objects;
+import net.jcip.annotations.Immutable;
+
+/**
+ * A codec that maps the CQL type {@code list<byte>} to the Java type {@code byte[]}.
+ *
+ * <p>Note that this codec is not suitable for reading CQL blobs as byte arrays; you should use
+ * {@link SimpleBlobCodec} for that.","[{'comment': 'We have 3 different codecs dealing with bytes:\r\n\r\n1. `BlobCodec` : `blob <-> ByteBuffer`\r\n1. `SimpleBlobCodec` : `blob <-> byte[]`\r\n1. `ByteArrayCodec` : `list<tinyint> <-> byte[]`', 'commenter': 'adutra'}]"
1413,core/src/main/java/com/datastax/oss/driver/internal/core/type/codec/extras/json/JsonCodec.java,"@@ -0,0 +1,182 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.type.codec.extras.json;
+
+import com.datastax.oss.driver.api.core.ProtocolVersion;
+import com.datastax.oss.driver.api.core.type.DataType;
+import com.datastax.oss.driver.api.core.type.DataTypes;
+import com.datastax.oss.driver.api.core.type.codec.TypeCodec;
+import com.datastax.oss.driver.api.core.type.reflect.GenericType;
+import com.datastax.oss.driver.internal.core.util.Strings;
+import com.datastax.oss.protocol.internal.util.Bytes;
+import com.fasterxml.jackson.core.JsonProcessingException;
+import com.fasterxml.jackson.databind.JavaType;
+import com.fasterxml.jackson.databind.json.JsonMapper;
+import com.fasterxml.jackson.databind.type.TypeFactory;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import edu.umd.cs.findbugs.annotations.Nullable;
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.Objects;
+
+/**
+ * A JSON codec that maps arbitrary Java objects to JSON strings stored as CQL type {@code text},
+ * using the Jackson library to perform serialization and deserialization of JSON objects.
+ *
+ * <p>Note that this codec requires the presence of Jackson library at runtime. If you use Maven,
+ * this can be done by declaring the following dependency in your project:
+ *
+ * <pre>{@code
+ * <dependency>
+ *   <groupId>com.fasterxml.jackson.core</groupId>
+ *   <artifactId>jackson-databind</artifactId>
+ *   <version>LATEST</version>
+ * </dependency>
+ * }</pre>
+ *
+ * @see <a href=""http://wiki.fasterxml.com/JacksonHome"">Jackson JSON Library</a>
+ * @param <T> The Java type that this codec serializes from and deserializes to, from JSON strings.
+ */
+public class JsonCodec<T> implements TypeCodec<T> {","[{'comment': ""I'm proposing one single `JsonCodec` using Jackson. I think this will satisfy 90% of the users. I therefore removed the `JacksonJsonCodec` classes from the query-builder and examples modules, and made these modules use this one instead.\r\n\r\nThe examples module has another Json codec using JSR-353. I left that codec there because 1) I think there is no real demand for that and 2) it drags a lot of new dependencies."", 'commenter': 'adutra'}]"
1413,core/src/main/java/com/datastax/oss/driver/internal/core/type/codec/extras/time/PersistentZonedTimestampCodec.java,"@@ -0,0 +1,103 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.type.codec.extras.time;
+
+import com.datastax.oss.driver.api.core.data.TupleValue;
+import com.datastax.oss.driver.api.core.type.DataTypes;
+import com.datastax.oss.driver.api.core.type.TupleType;
+import com.datastax.oss.driver.api.core.type.codec.MappingCodec;
+import com.datastax.oss.driver.api.core.type.codec.TypeCodec;
+import com.datastax.oss.driver.api.core.type.codec.TypeCodecs;
+import com.datastax.oss.driver.api.core.type.reflect.GenericType;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import edu.umd.cs.findbugs.annotations.Nullable;
+import java.time.Instant;
+import java.time.ZoneId;
+import java.time.ZonedDateTime;
+import java.util.Objects;
+import net.jcip.annotations.Immutable;
+
+/**
+ * {@link TypeCodec} that maps {@link ZonedDateTime} to CQL {@code tuple<timestamp,varchar>},
+ * providing a pattern for maintaining timezone information in Cassandra.
+ *
+ * <p>Since Cassandra's <code>timestamp</code> type does not store any time zone, by using a <code>
+ * tuple&lt;timestamp,varchar&gt;</code> a timezone can be persisted in the <code>varchar
+ * </code> field of such tuples, and so when the value is deserialized the original timezone is
+ * preserved.
+ *
+ * <p>Note: if you want to retrieve CQL timestamps as {@link ZonedDateTime} instances but don't need
+ * to persist the time zone to the database, you should rather use {@link ZonedTimestampCodec}.
+ */
+@Immutable
+public class PersistentZonedTimestampCodec extends MappingCodec<TupleValue, ZonedDateTime> {","[{'comment': 'This is the ""old"" `ZonedTimestampCodec`, but that name is already taken. We used to have users using this codec before, see [JAVA-1952](https://datastax-oss.atlassian.net/browse/JAVA-1952).', 'commenter': 'adutra'}, {'comment': 'So we have the following codecs now:\r\n\r\n1. `TimestampCodec`: `timestamp <-> Instant` (uses pre-defined zone for parsing only)\r\n1. `TimestampMillisCodec`: `timestamp <-> long`  (uses pre-defined zone for parsing only)\r\n1. `ZonedTimestampCodec`: `timestamp <-> ZonedDateTime`  (uses pre-defined zone for conversion and parsing)\r\n1. `LocalTimestampCodec`: `timestamp <-> LocalDateTime` (uses pre-defined zone for conversion and parsing)\r\n1. `PersistentZonedTimestampCodec`: `tuple<timestamp,string> <-> ZonedDateTime` (zone is persisted in tuple)', 'commenter': 'adutra'}]"
1413,core/src/main/java/com/datastax/oss/driver/internal/core/type/codec/extras/time/PersistentZonedTimestampCodec.java,"@@ -0,0 +1,103 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.type.codec.extras.time;
+
+import com.datastax.oss.driver.api.core.data.TupleValue;
+import com.datastax.oss.driver.api.core.type.DataTypes;
+import com.datastax.oss.driver.api.core.type.TupleType;
+import com.datastax.oss.driver.api.core.type.codec.MappingCodec;
+import com.datastax.oss.driver.api.core.type.codec.TypeCodec;
+import com.datastax.oss.driver.api.core.type.codec.TypeCodecs;
+import com.datastax.oss.driver.api.core.type.reflect.GenericType;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import edu.umd.cs.findbugs.annotations.Nullable;
+import java.time.Instant;
+import java.time.ZoneId;
+import java.time.ZonedDateTime;
+import java.util.Objects;
+import net.jcip.annotations.Immutable;
+
+/**
+ * {@link TypeCodec} that maps {@link ZonedDateTime} to CQL {@code tuple<timestamp,varchar>},
+ * providing a pattern for maintaining timezone information in Cassandra.
+ *
+ * <p>Since Cassandra's <code>timestamp</code> type does not store any time zone, by using a <code>
+ * tuple&lt;timestamp,varchar&gt;</code> a timezone can be persisted in the <code>varchar
+ * </code> field of such tuples, and so when the value is deserialized the original timezone is
+ * preserved.
+ *
+ * <p>Note: if you want to retrieve CQL timestamps as {@link ZonedDateTime} instances but don't need
+ * to persist the time zone to the database, you should rather use {@link ZonedTimestampCodec}.
+ */
+@Immutable
+public class PersistentZonedTimestampCodec extends MappingCodec<TupleValue, ZonedDateTime> {
+
+  private static final TupleType CQL_TYPE = DataTypes.tupleOf(DataTypes.TIMESTAMP, DataTypes.TEXT);
+
+  public PersistentZonedTimestampCodec() {
+    super(TypeCodecs.tupleOf(CQL_TYPE), GenericType.ZONED_DATE_TIME);
+  }
+
+  @Override
+  public boolean accepts(@NonNull Object value) {
+    Objects.requireNonNull(value);
+    return value instanceof ZonedDateTime;
+  }
+
+  @NonNull
+  @Override
+  public TupleType getCqlType() {
+    return CQL_TYPE;
+  }
+
+  @NonNull
+  @Override
+  public String format(@Nullable ZonedDateTime value) {
+    if (value == null) {
+      return ""NULL"";
+    }
+    // Use TIMESTAMP_UTC for a better-looking format
+    return ""(""","[{'comment': ""This allows the usage of `TypeCodecs.TIMESTAMP_UTC` instead of the default `TypeCodecs.TIMESTAMP`, which produces better-looking results such as `('2020-03-19T12:23:56.789Z','+02:00')` â€“ instead of the awkward looking output e.g. if your system TZ is `America/New_York`: `('2020-03-19T08:23:56.789Z [America/New_York]','+02:00')`."", 'commenter': 'adutra'}]"
1413,core/src/main/java/com/datastax/oss/driver/internal/core/type/codec/extras/time/ZonedTimestampCodec.java,"@@ -13,19 +13,17 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package com.datastax.oss.driver.internal.core.type.codec;
+package com.datastax.oss.driver.internal.core.type.codec.extras.time;","[{'comment': 'I moved this codec to `com.datastax.oss.driver.internal.core.type.codec.extras.time` to align with other temporal codecs added in this PR.', 'commenter': 'adutra'}]"
1413,core/src/main/java/com/datastax/oss/driver/internal/core/type/codec/extras/time/ZonedTimestampCodec.java,"@@ -34,17 +32,17 @@
  *
  * <p>Note that Apache Cassandra(R)'s timestamp type does not store any time zone; this codec is
  * provided merely as a convenience for users that need to deal with zoned timestamps in their
- * applications.
+ * applications. If you need to persist the time zone in the database, consider using {@link
+ * PersistentZonedTimestampCodec} instead.
  *
  * <p>This codec shares its logic with {@link TimestampCodec}. See the javadocs of this codec for
  * important remarks about implementation notes and accepted timestamp formats.
  *
  * @see TimestampCodec
  */
 @ThreadSafe
-public class ZonedTimestampCodec implements TypeCodec<ZonedDateTime> {
+public class ZonedTimestampCodec extends MappingCodec<Instant, ZonedDateTime> {","[{'comment': 'Also retrofitted to extend `MappingCodec`.', 'commenter': 'adutra'}]"
1413,examples/src/main/java/com/datastax/oss/driver/examples/json/jsr/Jsr353JsonCodec.java,"@@ -13,7 +13,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package com.datastax.oss.driver.examples.json.codecs;
+package com.datastax.oss.driver.examples.json.jsr;","[{'comment': ""Moved this codec here since he was alone in its old package and there was no reason why we wouldn't include it here along with the examples that use it."", 'commenter': 'adutra'}]"
1413,core/src/main/java/com/datastax/oss/driver/internal/core/type/codec/extras/array/BooleanArrayCodec.java,"@@ -0,0 +1,95 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.type.codec.extras.array;
+
+import com.datastax.oss.driver.api.core.ProtocolVersion;
+import com.datastax.oss.driver.api.core.type.DataTypes;
+import com.datastax.oss.driver.api.core.type.reflect.GenericType;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import java.nio.ByteBuffer;
+import java.util.Objects;
+import net.jcip.annotations.Immutable;
+
+/**
+ * A codec that maps the CQL type {@code list<boolean>} to the Java type {@code boolean[]}.
+ *
+ * <p>Note that this codec is designed for performance and converts CQL lists <em>directly</em> to
+ * {@code boolean[]}, thus avoiding any unnecessary boxing and unboxing of Java primitive {@code
+ * boolean} values; it also instantiates arrays without the need for an intermediary Java {@code
+ * List} object.
+ */
+@Immutable
+public class BooleanArrayCodec extends AbstractPrimitiveArrayCodec<boolean[]> {","[{'comment': ""The legacy driver extras module had codecs for int[], long[], float[] and double[] only. For completeness, I'm also including codecs for boolean[], byte[] and short[]."", 'commenter': 'adutra'}]"
1413,core/revapi.json,"@@ -4991,6 +4991,31 @@
         ""old"": ""class org.apache.tinkerpop.gremlin.process.traversal.util.TraversalExplanation"",
         ""new"": ""class org.apache.tinkerpop.gremlin.process.traversal.util.TraversalExplanation"",
         ""justification"": ""Upgrade to Tinkerpop 3.4.4""
+      },
+      {
+        ""code"": ""java.class.nonPublicPartOfAPI"",
+        ""new"": ""class com.fasterxml.jackson.databind.introspect.AnnotatedConstructor.Serialization"",
+        ""justification"": ""Caused by the exposure of JsonMapper as a parameter in TypeCodecs.json()""
+      },
+      {
+        ""code"": ""java.class.nonPublicPartOfAPI"",
+        ""new"": ""class com.fasterxml.jackson.databind.introspect.AnnotatedField.Serialization"",
+        ""justification"": ""Caused by the exposure of JsonMapper as a parameter in TypeCodecs.json()""
+      },
+      {
+        ""code"": ""java.class.nonPublicPartOfAPI"",
+        ""new"": ""class com.fasterxml.jackson.databind.introspect.AnnotatedMethod.Serialization"",
+        ""justification"": ""Caused by the exposure of JsonMapper as a parameter in TypeCodecs.json()""
+      },
+      {
+        ""code"": ""java.class.nonPublicPartOfAPI"",
+        ""new"": ""class com.fasterxml.jackson.databind.type.TypeParser.MyTokenizer"",
+        ""justification"": ""Caused by the exposure of JsonMapper as a parameter in TypeCodecs.json()""
+      },
+      {
+        ""code"": ""java.class.nonPublicPartOfAPI"",
+        ""new"": ""class com.fasterxml.jackson.databind.util.PrimitiveArrayBuilder.Node<T extends java.lang.Object>"",
+        ""justification"": ""Caused by the exposure of JsonMapper as a parameter in TypeCodecs.json()""","[{'comment': 'ðŸ‘ I verified that other `TypeCodecs` methods and constant can still be referenced even if Jackson is excluded from the classpath.', 'commenter': 'olim7t'}]"
1413,core/src/main/java/com/datastax/oss/driver/api/core/type/codec/TypeCodecs.java,"@@ -78,39 +98,171 @@
   public static final PrimitiveIntCodec INT = new IntCodec();
   public static final PrimitiveLongCodec BIGINT = new BigIntCodec();
   public static final PrimitiveShortCodec SMALLINT = new SmallIntCodec();
+
+  /**
+   * A codec that handles Apache Cassandra(R)'s timestamp type and maps it to Java's {@link
+   * Instant}, using the system's {@linkplain ZoneId#systemDefault() default time zone} as its
+   * source of time zone information. If you need a different time zone, consider other constants in
+   * this class, or call {@link #timestampAt(ZoneId)} instead.
+   *
+   * @see #TIMESTAMP_UTC
+   * @see #timestampAt(ZoneId)
+   */
   public static final TypeCodec<Instant> TIMESTAMP = new TimestampCodec();
 
+  /**
+   * A codec that handles Apache Cassandra(R)'s timestamp type and maps it to Java's {@link
+   * Instant}, using {@link ZoneOffset#UTC} as its source of time zone information. If you need a
+   * different time zone, consider other constants in this class, or call {@link
+   * #timestampAt(ZoneId)} instead.","[{'comment': ""Maybe we should emphasize that the time zone is only used for parsing and formatting. It's a bit surprising at first because neither `Instant` nor `timestamp` are zoned."", 'commenter': 'olim7t'}]"
1413,core/src/main/java/com/datastax/oss/driver/api/core/type/codec/TypeCodecs.java,"@@ -78,39 +98,171 @@
   public static final PrimitiveIntCodec INT = new IntCodec();
   public static final PrimitiveLongCodec BIGINT = new BigIntCodec();
   public static final PrimitiveShortCodec SMALLINT = new SmallIntCodec();
+
+  /**
+   * A codec that handles Apache Cassandra(R)'s timestamp type and maps it to Java's {@link
+   * Instant}, using the system's {@linkplain ZoneId#systemDefault() default time zone} as its
+   * source of time zone information. If you need a different time zone, consider other constants in
+   * this class, or call {@link #timestampAt(ZoneId)} instead.
+   *
+   * @see #TIMESTAMP_UTC
+   * @see #timestampAt(ZoneId)
+   */
   public static final TypeCodec<Instant> TIMESTAMP = new TimestampCodec();
 
+  /**
+   * A codec that handles Apache Cassandra(R)'s timestamp type and maps it to Java's {@link
+   * Instant}, using {@link ZoneOffset#UTC} as its source of time zone information. If you need a
+   * different time zone, consider other constants in this class, or call {@link
+   * #timestampAt(ZoneId)} instead.
+   *
+   * @see #TIMESTAMP
+   * @see #timestampAt(ZoneId)
+   */
+  public static final TypeCodec<Instant> TIMESTAMP_UTC = new TimestampCodec(ZoneOffset.UTC);
+
+  /**
+   * A codec that maps CQL {@code timestamp} to Java {@code long}, representing the number of
+   * milliseconds since the Epoch, using the system's {@linkplain ZoneId#systemDefault() default
+   * time zone} as its source of time zone information. If you need a different time zone, consider
+   * other constants in this class, or call {@link #timestampMillisAt(ZoneId)} instead.
+   *
+   * <p>This codec can serve as a replacement for the driver's built-in {@linkplain #TIMESTAMP
+   * timestamp} codec, when application code prefers to deal with raw milliseconds than with {@link
+   * Instant} instances.
+   *
+   * @see #TIMESTAMP_MILLIS_UTC
+   * @see #timestampMillisAt(ZoneId)
+   */
+  public static final PrimitiveLongCodec TIMESTAMP_MILLIS_SYSTEM = new TimestampMillisCodec();
+
+  /**
+   * A codec that maps CQL {@code timestamp} to Java {@code long}, representing the number of
+   * milliseconds since the Epoch, using {@link ZoneOffset#UTC} as its source of time zone
+   * information. If you need a different time zone, consider other constants in this class, or call
+   * {@link #timestampMillisAt(ZoneId)} instead.
+   *
+   * <p>This codec can serve as a replacement for the driver's built-in {@linkplain #TIMESTAMP
+   * timestamp} codec, when application code prefers to deal with raw milliseconds than with {@link
+   * Instant} instances.
+   *
+   * @see #TIMESTAMP_MILLIS_SYSTEM
+   * @see #timestampMillisAt(ZoneId)
+   */
+  public static final PrimitiveLongCodec TIMESTAMP_MILLIS_UTC =
+      new TimestampMillisCodec(ZoneOffset.UTC);
+
   /**
    * A codec that handles Apache Cassandra(R)'s timestamp type and maps it to Java's {@link
    * ZonedDateTime}, using the system's {@linkplain ZoneId#systemDefault() default time zone} as its
-   * source of time zone information.
+   * source of time zone information. If you need a different time zone, consider using other
+   * constants in this class, or call {@link #zonedTimestampAt(ZoneId)} instead.
    *
    * <p>Note that Apache Cassandra(R)'s timestamp type does not store any time zone; this codec is
    * provided merely as a convenience for users that need to deal with zoned timestamps in their
    * applications.
    *
    * @see #ZONED_TIMESTAMP_UTC
+   * @see #ZONED_TIMESTAMP_PERSISTED
    * @see #zonedTimestampAt(ZoneId)
    */
   public static final TypeCodec<ZonedDateTime> ZONED_TIMESTAMP_SYSTEM = new ZonedTimestampCodec();
 
   /**
    * A codec that handles Apache Cassandra(R)'s timestamp type and maps it to Java's {@link
-   * ZonedDateTime}, using {@link ZoneOffset#UTC} as its source of time zone information.
+   * ZonedDateTime}, using {@link ZoneOffset#UTC} as its source of time zone information. If you
+   * need a different time zone, consider using other constants in this class, or call {@link
+   * #zonedTimestampAt(ZoneId)} instead.
    *
    * <p>Note that Apache Cassandra(R)'s timestamp type does not store any time zone; this codec is
    * provided merely as a convenience for users that need to deal with zoned timestamps in their
    * applications.
    *
    * @see #ZONED_TIMESTAMP_SYSTEM
+   * @see #ZONED_TIMESTAMP_PERSISTED
    * @see #zonedTimestampAt(ZoneId)
    */
   public static final TypeCodec<ZonedDateTime> ZONED_TIMESTAMP_UTC =
       new ZonedTimestampCodec(ZoneOffset.UTC);
 
+  /**
+   * A codec that maps {@link ZonedDateTime} to CQL {@code tuple<timestamp,varchar>}, providing a
+   * pattern for maintaining timezone information in Cassandra.
+   *
+   * <p>Since Cassandra's <code>timestamp</code> type does not store any time zone, by using a
+   * <code>tuple&lt;timestamp,varchar&gt;</code> a timezone can be persisted in the <code>varchar
+   * </code> field of such tuples, and so when the value is deserialized the original timezone is
+   * preserved.
+   *
+   * @see #ZONED_TIMESTAMP_SYSTEM
+   * @see #ZONED_TIMESTAMP_UTC
+   * @see #zonedTimestampAt(ZoneId)
+   */
+  public static final TypeCodec<ZonedDateTime> ZONED_TIMESTAMP_PERSISTED =
+      new PersistentZonedTimestampCodec();
+
+  /**
+   * A codec that handles Apache Cassandra(R)'s timestamp type and maps it to Java's {@link
+   * LocalDateTime}, using the system's {@linkplain ZoneId#systemDefault() default time zone} as its
+   * source of time zone information. If you need a different time zone, consider using other
+   * constants in this class, or call {@link #localTimestampAt(ZoneId)} instead.
+   *
+   * <p>Note that Apache Cassandra(R)'s timestamp type does not store any time zone; this codec is
+   * provided merely as a convenience for users that need to deal with local date-times in their
+   * applications.
+   *
+   * @see #LOCAL_TIMESTAMP_UTC
+   * @see #localTimestampAt(ZoneId)
+   */
+  public static final TypeCodec<LocalDateTime> LOCAL_TIMESTAMP_SYSTEM = new LocalTimestampCodec();
+
+  /**
+   * A codec that handles Apache Cassandra(R)'s timestamp type and maps it to Java's {@link
+   * LocalDateTime}, using {@link ZoneOffset#UTC} as its source of time zone information.If you need
+   * a different time zone, consider using other constants in this class, or call {@link
+   * #localTimestampAt(ZoneId)} instead.
+   *
+   * <p>Note that Apache Cassandra(R)'s timestamp type does not store any time zone; this codec is
+   * provided merely as a convenience for users that need to deal with local date-times in their
+   * applications.
+   *
+   * @see #LOCAL_TIMESTAMP_SYSTEM
+   * @see #localTimestampAt(ZoneId)
+   */
+  public static final TypeCodec<LocalDateTime> LOCAL_TIMESTAMP_UTC =
+      new LocalTimestampCodec(ZoneOffset.UTC);
+
   public static final TypeCodec<LocalDate> DATE = new DateCodec();
   public static final TypeCodec<LocalTime> TIME = new TimeCodec();
+
+  /**
+   * A codec that maps the CQL type {@code blob} to the Java type {@link ByteBuffer}.
+   *
+   * <p>If you are looking for a codec mapping the CQL type {@code blob} to the Java type {@code
+   * byte[]}, you should use {@link #BLOB_SIMPLE} instead.
+   *
+   * <p>If you are looking for a codec mapping the CQL type {@code list<tinyint} to the Java type
+   * {@code byte[]}, you should use {@link #BYTE_ARRAY} instead.
+   *
+   * @see #BLOB_SIMPLE
+   * @see #BYTE_ARRAY
+   */
   public static final TypeCodec<ByteBuffer> BLOB = new BlobCodec();
+
+  /**
+   * A codec that maps the CQL type {@code blob} to the Java type {@code byte[]}.
+   *
+   * <p>If you are looking for a codec mapping the CQL type {@code blob} to the Java type {@link
+   * ByteBuffer}, you should use {@link #BLOB} instead.
+   *
+   * <p>If you are looking for a codec mapping the CQL type {@code list<tinyint} to the Java type
+   * {@code byte[]}, you should use {@link #BYTE_ARRAY} instead.
+   *
+   * @see #BLOB
+   * @see #BYTE_ARRAY
+   */
+  public static final TypeCodec<byte[]> BLOB_SIMPLE = new SimpleBlobCodec();","[{'comment': ""Not a fan of the name, although I'm not sure I have a better proposal. Maybe `BLOB_TO_ARRAY`?"", 'commenter': 'olim7t'}, {'comment': 'Agreed. I think I will also change the names of fields and classes dealing with list <-> arrays.', 'commenter': 'adutra'}]"
1413,core/src/main/java/com/datastax/oss/driver/internal/core/type/codec/extras/array/BooleanArrayCodec.java,"@@ -0,0 +1,95 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.type.codec.extras.array;
+
+import com.datastax.oss.driver.api.core.ProtocolVersion;
+import com.datastax.oss.driver.api.core.type.DataTypes;
+import com.datastax.oss.driver.api.core.type.reflect.GenericType;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import java.nio.ByteBuffer;
+import java.util.Objects;
+import net.jcip.annotations.Immutable;
+
+/**
+ * A codec that maps the CQL type {@code list<boolean>} to the Java type {@code boolean[]}.
+ *
+ * <p>Note that this codec is designed for performance and converts CQL lists <em>directly</em> to
+ * {@code boolean[]}, thus avoiding any unnecessary boxing and unboxing of Java primitive {@code
+ * boolean} values; it also instantiates arrays without the need for an intermediary Java {@code
+ * List} object.","[{'comment': 'Those comments would be more useful on the constants and methods in `TypeCodecs`.\r\n\r\nThe codec implementations themselves are internal, the end user does not usually reference them directly.', 'commenter': 'olim7t'}]"
1413,core/src/main/java/com/datastax/oss/driver/api/core/type/codec/TypeCodecs.java,"@@ -163,9 +374,114 @@
    *
    * @see #ZONED_TIMESTAMP_SYSTEM
    * @see #ZONED_TIMESTAMP_UTC
+   * @see #ZONED_TIMESTAMP_PERSISTED
    */
   @NonNull
   public static TypeCodec<ZonedDateTime> zonedTimestampAt(@NonNull ZoneId timeZone) {
     return new ZonedTimestampCodec(timeZone);
   }
+
+  /**
+   * Returns a codec that handles Apache Cassandra(R)'s timestamp type and maps it to Java's {@link
+   * LocalDateTime}, using the supplied {@link ZoneId} as its source of time zone information.
+   *
+   * <p>Note that Apache Cassandra(R)'s timestamp type does not store any time zone; the codecs
+   * created by this method are provided merely as a convenience for users that need to deal with
+   * local date-times in their applications.
+   *
+   * @see #LOCAL_TIMESTAMP_UTC
+   * @see #localTimestampAt(ZoneId)
+   */
+  @NonNull
+  public static TypeCodec<LocalDateTime> localTimestampAt(@NonNull ZoneId timeZone) {
+    return new LocalTimestampCodec(timeZone);
+  }
+
+  /**
+   * Returns a codec mapping CQL lists to Java object arrays. Serialization and deserialization of
+   * elements in the array is delegated to the provided element codec.
+   *
+   * <p>This method is not suitable for Java primitive arrays. Use {@link #BOOLEAN_ARRAY}, {@link
+   * #BYTE_ARRAY}, {@link #SHORT_ARRAY}, {@link #INT_ARRAY}, {@link #LONG_ARRAY}, {@link
+   * #FLOAT_ARRAY} or {@link #DOUBLE_ARRAY} instead.
+   */
+  @NonNull
+  public static <T> TypeCodec<T[]> arrayOf(@NonNull TypeCodec<T> elementCodec) {
+    return new ObjectArrayCodec<>(elementCodec);
+  }
+
+  /**
+   * Returns a codec mapping Java Enums to CQL ints, according to their {@linkplain Enum#ordinal()
+   * ordinals}.
+   *
+   * @see #enumNamesOf(Class)
+   */
+  @NonNull
+  public static <EnumT extends Enum<EnumT>> TypeCodec<EnumT> enumOrdinalsOf(
+      @NonNull Class<EnumT> enumClass) {
+    return new EnumOrdinalCodec<>(enumClass);
+  }","[{'comment': ""I wonder if we should provide this out of the box. Using ordinals is a terrible idea, let's not encourage it."", 'commenter': 'olim7t'}, {'comment': 'But we already provided this in driver 3.x, do you think we should remove this functionality then?', 'commenter': 'adutra'}, {'comment': ""I'm in favor of leaving it. Ordinals can save space on disk, if the enum never changes I don't see why it's so horrible."", 'commenter': 'adutra'}, {'comment': 'Sorry to rekindle the debate, I was about to concede on the grounds of 3.x compatibility, but this really bothers me:\r\n\r\n> if the enum never changes I don\'t see why it\'s so horrible\r\n\r\nThat\'s a very big if. The enum always changes. It just takes one careless developer inserting a new constant in the middle, or reordering the constants to their liking, and you start inserting different codes, that overlap with the old ones. Your database is trashed, and it\'s not recoverable (beyond restoring a backup, or maybe a script that would track the writetime, but imagine the mess).\r\n\r\nIt\'s far from evident: the enum might come from a different layer of the application, or even a different project, where people are not even aware that it\'s being inserted in a database.\r\n\r\nIt\'s almost impossible to enforce: you can add a comment (""the order of these constants matters""), but you don\'t know if it will be read. Tools like Clirr or Revapi will warn you, but I\'m not sure how widespread their use is.\r\n\r\nSo all in all it\'s a very dangerous feature, I think we are doing our users a disservice by providing it. It was indeed in 3.x, but I think it was a mistake to include it then.\r\nIf people want to save bytes, the better solution is to have an explicit `fromCode` / `toCode` conversion layer, and write your own codec, which should be relatively straightforward with `MappingCodec`.', 'commenter': 'olim7t'}, {'comment': "">  It just takes one careless developer inserting a new constant in the middle, or reordering the constants to their liking\r\n\r\n... or changing an enum constant name. And your database is screwed just the same. In this case we shouldn't provide any codec for enums. And then we would be back to careful users complaining that our driver is hard to use because they need to create their codecs themselves.\r\n\r\nThere is demand for storing enums in the database. Someone in DataStax Community recently was even [asking for an enum type in Cassandra itself](https://community.datastax.com/questions/5844/enum-datatype-for-cassandra-column.html). One of the reasons invoked is storage efficiency, and enum ordinals would be better at that.\r\n\r\nI would like us to provide both codecs, just like in driver 3. We are not responsible for careless developers, we have warnings in the docs explaining the caveats of both approaches, so we're covered imo."", 'commenter': 'adutra'}, {'comment': ""> or changing an enum constant name. And your database is screwed just the same\r\n\r\nNo, it's much more obvious: client code will stop compiling; and even after that, the codec will fail to decode the old name. Also, you would likely pick an entirely new name, not swap existing constants.\r\n\r\nLet's say you store card suits by name, but one of the names is wrong: `enum Suit { HEARTS, DIAMONDS, SPADES, CLOVERS }`. Someone fixes `CLOVERS` => `CLUBS`. Assuming no one picked up on the compile errors, and your tests didn't catch it either, the worst that can happen is that you only find out in production. This will fail hard and fast as soon as the codec tries to decode `CLOVERS`. You revert the upgrade, and you also need an annoying but doable migration of any corrupted writes: find any row with `CLUBS` and replace it with `CLOVERS`.\r\n\r\nOn the other hand, you store suits by ordinal, and have `enum Suit { HEARTS, DIAMONDS, SPADES, CLUBS }`. But someone decides that enum constants should be sorted in alphabetical order: `enum Suit { CLUBS, DIAMONDS, HEARTS, SPADES }`. The code still compiles. Tests won't catch it if they use enum constants for both reads & writes. Production code won't fail fast, it still happily reads and writes data. It's more likely that a human will detect the issue, which might take significantly longer. When all is said and done, you're left to reconcile rows where 0 means hearts, 2 means spades and 3 means clubs, with rows where 0 means clubs, 2 means hearts and 3 means spades. This is an order of magnitude worse.\r\n\r\n> We are not responsible for careless developers\r\n\r\nNo, but we shouldn't give users a ticking timebomb in a nice giftwrap either."", 'commenter': 'olim7t'}, {'comment': ""@csplinter any opinion on this?\r\nI'm not dead-set on not having an enum ordinal codec out of the box, but there are strong arguments against it."", 'commenter': 'olim7t'}]"
1413,core/src/main/java/com/datastax/oss/driver/api/core/type/codec/ExtraTypeCodecs.java,"@@ -0,0 +1,482 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core.type.codec;
+
+import com.datastax.oss.driver.api.core.session.SessionBuilder;
+import com.datastax.oss.driver.api.core.type.codec.registry.MutableCodecRegistry;
+import com.datastax.oss.driver.api.core.type.reflect.GenericType;
+import com.datastax.oss.driver.internal.core.type.codec.SimpleBlobCodec;
+import com.datastax.oss.driver.internal.core.type.codec.TimestampCodec;
+import com.datastax.oss.driver.internal.core.type.codec.extras.OptionalCodec;
+import com.datastax.oss.driver.internal.core.type.codec.extras.array.BooleanListToArrayCodec;
+import com.datastax.oss.driver.internal.core.type.codec.extras.array.ByteListToArrayCodec;
+import com.datastax.oss.driver.internal.core.type.codec.extras.array.DoubleListToArrayCodec;
+import com.datastax.oss.driver.internal.core.type.codec.extras.array.FloatListToArrayCodec;
+import com.datastax.oss.driver.internal.core.type.codec.extras.array.IntListToArrayCodec;
+import com.datastax.oss.driver.internal.core.type.codec.extras.array.LongListToArrayCodec;
+import com.datastax.oss.driver.internal.core.type.codec.extras.array.ObjectListToArrayCodec;
+import com.datastax.oss.driver.internal.core.type.codec.extras.array.ShortListToArrayCodec;
+import com.datastax.oss.driver.internal.core.type.codec.extras.enums.EnumNameCodec;
+import com.datastax.oss.driver.internal.core.type.codec.extras.enums.EnumOrdinalCodec;
+import com.datastax.oss.driver.internal.core.type.codec.extras.json.JsonCodec;
+import com.datastax.oss.driver.internal.core.type.codec.extras.time.LocalTimestampCodec;
+import com.datastax.oss.driver.internal.core.type.codec.extras.time.PersistentZonedTimestampCodec;
+import com.datastax.oss.driver.internal.core.type.codec.extras.time.TimestampMillisCodec;
+import com.datastax.oss.driver.internal.core.type.codec.extras.time.ZonedTimestampCodec;
+import com.fasterxml.jackson.databind.ObjectMapper;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import java.nio.ByteBuffer;
+import java.time.Instant;
+import java.time.LocalDateTime;
+import java.time.ZoneId;
+import java.time.ZoneOffset;
+import java.time.ZonedDateTime;
+import java.util.Optional;
+
+/**
+ * Additional codecs that can be registered to handle different type mappings.
+ *
+ * @see SessionBuilder#addTypeCodecs(TypeCodec[])
+ * @see MutableCodecRegistry#register(TypeCodec)
+ */
+public class ExtraTypeCodecs {
+
+  /**
+   * A codec that maps CQL type {@code timestamp} to Java's {@link Instant}, using the UTC time zone
+   * to parse and format CQL literals.
+   *
+   * <p>This codec uses {@link ZoneOffset#UTC} as its source of time zone information when
+   * formatting values as CQL literals, or parsing CQL literals that do not have any time zone
+   * indication. Note that this only applies to the {@link TypeCodec#format(Object)} and {@link
+   * TypeCodec#parse(String)} methods; regular encoding and decoding, like setting a value on a
+   * bound statement or reading a column from a row, are not affected by the time zone.
+   *
+   * <p>If you need a different time zone, consider other constants in this class, or call {@link
+   * ExtraTypeCodecs#timestampAt(ZoneId)} instead.
+   *
+   * @see TypeCodecs#TIMESTAMP
+   * @see ExtraTypeCodecs#timestampAt(ZoneId)
+   */
+  public static final TypeCodec<Instant> TIMESTAMP_UTC = new TimestampCodec(ZoneOffset.UTC);
+
+  /**
+   * A codec that maps CQL type {@code timestamp} to Java's {@code long}, representing the number of
+   * milliseconds since the Epoch, using the system's default time zone to parse and format CQL
+   * literals.
+   *
+   * <p>This codec uses the system's {@linkplain ZoneId#systemDefault() default time zone} as its
+   * source of time zone information when formatting values as CQL literals, or parsing CQL literals
+   * that do not have any time zone indication. Note that this only applies to the {@link
+   * TypeCodec#format(Object)} and {@link TypeCodec#parse(String)} methods; regular encoding and
+   * decoding, like setting a value on a bound statement or reading a column from a row, are not
+   * affected by the time zone.
+   *
+   * <p>If you need a different time zone, consider other constants in this class, or call {@link
+   * #timestampMillisAt(ZoneId)} instead.
+   *
+   * <p>This codec can serve as a replacement for the driver's built-in {@linkplain
+   * TypeCodecs#TIMESTAMP timestamp} codec, when application code prefers to deal with raw
+   * milliseconds than with {@link Instant} instances.
+   *
+   * @see #TIMESTAMP_MILLIS_UTC
+   * @see #timestampMillisAt(ZoneId)
+   */
+  public static final PrimitiveLongCodec TIMESTAMP_MILLIS_SYSTEM = new TimestampMillisCodec();
+
+  /**
+   * A codec that maps CQL type {@code timestamp} to Java's {@code long}, representing the number of
+   * milliseconds since the Epoch, using the UTC time zone to parse and format CQL literals.
+   *
+   * <p>This codec uses {@link ZoneOffset#UTC} as its source of time zone information when
+   * formatting values as CQL literals, or parsing CQL literals that do not have any time zone
+   * indication. Note that this only applies to the {@link TypeCodec#format(Object)} and {@link
+   * TypeCodec#parse(String)} methods; regular encoding and decoding, like setting a value on a
+   * bound statement or reading a column from a row, are not affected by the time zone.
+   *
+   * <p>If you need a different time zone, consider other constants in this class, or call {@link
+   * #timestampMillisAt(ZoneId)} instead.
+   *
+   * <p>This codec can serve as a replacement for the driver's built-in {@linkplain
+   * TypeCodecs#TIMESTAMP timestamp} codec, when application code prefers to deal with raw
+   * milliseconds than with {@link Instant} instances.
+   *
+   * @see #TIMESTAMP_MILLIS_SYSTEM
+   * @see #timestampMillisAt(ZoneId)
+   */
+  public static final PrimitiveLongCodec TIMESTAMP_MILLIS_UTC =
+      new TimestampMillisCodec(ZoneOffset.UTC);
+
+  /**
+   * A codec that maps CQL type {@code timestamp} to Java's {@link ZonedDateTime}, using the
+   * system's default time zone.
+   *
+   * <p>This codec uses the system's {@linkplain ZoneId#systemDefault() default time zone} as its
+   * source of time zone information when encoding or decoding. If you need a different time zone,
+   * consider using other constants in this class, or call {@link #zonedTimestampAt(ZoneId)}
+   * instead.
+   *
+   * <p>Note that CQL type {@code timestamp} type does not store any time zone; this codec is
+   * provided merely as a convenience for users that need to deal with zoned timestamps in their
+   * applications.
+   *
+   * @see #ZONED_TIMESTAMP_UTC
+   * @see #ZONED_TIMESTAMP_PERSISTED
+   * @see #zonedTimestampAt(ZoneId)
+   */
+  public static final TypeCodec<ZonedDateTime> ZONED_TIMESTAMP_SYSTEM = new ZonedTimestampCodec();
+
+  /**
+   * A codec that maps CQL type {@code timestamp} to Java's {@link ZonedDateTime}, using the UTC
+   * time zone.
+   *
+   * <p>This codec uses {@link ZoneOffset#UTC} as its source of time zone information when encoding
+   * or decoding. If you need a different time zone, consider using other constants in this class,
+   * or call {@link #zonedTimestampAt(ZoneId)} instead.
+   *
+   * <p>Note that CQL type {@code timestamp} type does not store any time zone; this codec is
+   * provided merely as a convenience for users that need to deal with zoned timestamps in their
+   * applications.
+   *
+   * @see #ZONED_TIMESTAMP_SYSTEM
+   * @see #ZONED_TIMESTAMP_PERSISTED
+   * @see #zonedTimestampAt(ZoneId)
+   */
+  public static final TypeCodec<ZonedDateTime> ZONED_TIMESTAMP_UTC =
+      new ZonedTimestampCodec(ZoneOffset.UTC);
+
+  /**
+   * A codec that maps CQL type {@code tuple<timestamp,text>} to Java's {@link ZonedDateTime},
+   * providing a pattern for maintaining timezone information in Cassandra.
+   *
+   * <p>Since CQL type {@code timestamp} does not store any time zone, it is persisted separately in
+   * the {@code text} field of such the tuple, and so when the value is deserialized the original
+   * timezone is preserved.
+   *
+   * @see #ZONED_TIMESTAMP_SYSTEM
+   * @see #ZONED_TIMESTAMP_UTC
+   * @see #zonedTimestampAt(ZoneId)
+   */
+  public static final TypeCodec<ZonedDateTime> ZONED_TIMESTAMP_PERSISTED =
+      new PersistentZonedTimestampCodec();
+
+  /**
+   * A codec that maps CQL type {@code timestamp} to Java's {@link LocalDateTime}, using the
+   * system's default time zone.
+   *
+   * <p>This codec uses the system's {@linkplain ZoneId#systemDefault() default time zone} as its
+   * source of time zone information when encoding or decoding. If you need a different time zone,
+   * consider using other constants in this class, or call {@link #localTimestampAt(ZoneId)}
+   * instead.
+   *
+   * <p>Note that CQL type {@code timestamp} does not store any time zone; this codec is provided
+   * merely as a convenience for users that need to deal with local date-times in their
+   * applications.
+   *
+   * @see #LOCAL_TIMESTAMP_UTC
+   * @see #localTimestampAt(ZoneId)
+   */
+  public static final TypeCodec<LocalDateTime> LOCAL_TIMESTAMP_SYSTEM = new LocalTimestampCodec();
+
+  /**
+   * A codec that maps CQL type {@code timestamp} to Java's {@link LocalDateTime}, using the UTC
+   * time zone.
+   *
+   * <p>This codec uses {@link ZoneOffset#UTC} as its source of time zone information when encoding
+   * or decoding. If you need a different time zone, consider using other constants in this class,
+   * or call {@link #localTimestampAt(ZoneId)} instead.
+   *
+   * <p>Note that CQL type {@code timestamp} does not store any time zone; this codec is provided
+   * merely as a convenience for users that need to deal with local date-times in their
+   * applications.
+   *
+   * @see #LOCAL_TIMESTAMP_SYSTEM
+   * @see #localTimestampAt(ZoneId)
+   */
+  public static final TypeCodec<LocalDateTime> LOCAL_TIMESTAMP_UTC =
+      new LocalTimestampCodec(ZoneOffset.UTC);
+
+  /**
+   * A codec that maps CQL type {@code blob} to Java's {@code byte[]}.
+   *
+   * <p>If you are looking for a codec mapping CQL type {@code blob} to the Java type {@link
+   * ByteBuffer}, you should use {@link TypeCodecs#BLOB} instead.
+   *
+   * <p>If you are looking for a codec mapping CQL type {@code list<tinyint} to the Java type {@code
+   * byte[]}, you should use {@link #BYTE_LIST_TO_ARRAY} instead.
+   *
+   * @see TypeCodecs#BLOB
+   * @see #BYTE_LIST_TO_ARRAY
+   */
+  public static final TypeCodec<byte[]> BLOB_TO_ARRAY = new SimpleBlobCodec();
+
+  /**
+   * A codec that maps CQL type {@code list<boolean>} to Java's {@code boolean[]}.
+   *
+   * <p>Note that this codec is designed for performance and converts CQL lists <em>directly</em> to
+   * {@code boolean[]}, thus avoiding any unnecessary boxing and unboxing of Java primitive {@code
+   * boolean} values; it also instantiates arrays without the need for an intermediary Java {@code
+   * List} object.
+   */
+  public static final TypeCodec<boolean[]> BOOLEAN_ARRAY = new BooleanListToArrayCodec();","[{'comment': ""@olim7t I'm overall ðŸ‘ on the new class but why did you change the name of this field and not the others below?"", 'commenter': 'adutra'}]"
1414,core/src/main/java/com/datastax/oss/driver/internal/core/metadata/token/ReplicationFactor.java,"@@ -0,0 +1,84 @@
+package com.datastax.oss.driver.internal.core.metadata.token;
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import com.datastax.oss.driver.shaded.guava.common.base.Preconditions;
+import com.datastax.oss.driver.shaded.guava.common.base.Splitter;
+import java.util.List;
+import java.util.Objects;
+
+// This class is a subset of server version at org.apache.cassandra.locator.ReplicationFactor
+public class ReplicationFactor {
+  private final int allReplicas;
+  private final int fullReplicas;
+
+  private ReplicationFactor(int allReplicas, int transientReplicas) {","[{'comment': 'nit: `allReplicas` and `totalReplicas` names are used for the same thing, maybe we should normalize it?', 'commenter': 'tomekl007'}]"
1414,core/src/main/java/com/datastax/oss/driver/internal/core/metadata/token/ReplicationFactor.java,"@@ -0,0 +1,84 @@
+package com.datastax.oss.driver.internal.core.metadata.token;
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import com.datastax.oss.driver.shaded.guava.common.base.Preconditions;
+import com.datastax.oss.driver.shaded.guava.common.base.Splitter;
+import java.util.List;
+import java.util.Objects;
+
+// This class is a subset of server version at org.apache.cassandra.locator.ReplicationFactor
+public class ReplicationFactor {
+  private final int allReplicas;
+  private final int fullReplicas;
+
+  private ReplicationFactor(int allReplicas, int transientReplicas) {
+    this.allReplicas = allReplicas;
+    this.fullReplicas = allReplicas - transientReplicas;
+  }
+
+  public static ReplicationFactor withTransient(int totalReplicas, int transientReplicas) {
+    return new ReplicationFactor(totalReplicas, transientReplicas);
+  }
+
+  public static ReplicationFactor fullOnly(int totalReplicas) {
+    return new ReplicationFactor(totalReplicas, 0);
+  }
+
+  public int fullReplicas() {
+    return fullReplicas;
+  }
+
+  public int transientReplicas() {
+    return allReplicas - fullReplicas;
+  }
+
+  public boolean hasTransientReplicas() {
+    return allReplicas != fullReplicas;
+  }
+
+  public static ReplicationFactor fromString(String s) {","[{'comment': 'I think it will be beneficial to create a unit test for this method', 'commenter': 'tomekl007'}]"
1414,core/src/main/java/com/datastax/oss/driver/internal/core/metadata/token/ReplicationFactor.java,"@@ -0,0 +1,84 @@
+package com.datastax.oss.driver.internal.core.metadata.token;
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import com.datastax.oss.driver.shaded.guava.common.base.Preconditions;
+import com.datastax.oss.driver.shaded.guava.common.base.Splitter;
+import java.util.List;
+import java.util.Objects;
+
+// This class is a subset of server version at org.apache.cassandra.locator.ReplicationFactor
+public class ReplicationFactor {
+  private final int allReplicas;
+  private final int fullReplicas;
+
+  private ReplicationFactor(int allReplicas, int transientReplicas) {
+    this.allReplicas = allReplicas;
+    this.fullReplicas = allReplicas - transientReplicas;
+  }
+
+  public static ReplicationFactor withTransient(int totalReplicas, int transientReplicas) {
+    return new ReplicationFactor(totalReplicas, transientReplicas);
+  }
+
+  public static ReplicationFactor fullOnly(int totalReplicas) {
+    return new ReplicationFactor(totalReplicas, 0);
+  }
+
+  public int fullReplicas() {
+    return fullReplicas;
+  }
+
+  public int transientReplicas() {
+    return allReplicas - fullReplicas;
+  }
+
+  public boolean hasTransientReplicas() {
+    return allReplicas != fullReplicas;
+  }
+
+  public static ReplicationFactor fromString(String s) {
+    if (s.contains(""/"")) {
+      List<String> parts = Splitter.on('/').splitToList(s);
+      Preconditions.checkArgument(
+          parts.size() == 2, ""Replication factor format is <replicas> or <replicas>/<transient>"");
+      return new ReplicationFactor(Integer.valueOf(parts.get(0)), Integer.valueOf(parts.get(1)));
+    } else {
+      return new ReplicationFactor(Integer.valueOf(s), 0);
+    }
+  }
+
+  public String toParseableString() {
+    return String.valueOf(allReplicas) + (hasTransientReplicas() ? ""/"" + transientReplicas() : """");","[{'comment': 'same here (unit test)', 'commenter': 'tomekl007'}, {'comment': 'Nit: unnecessary `String.valueOf` call (IDEA warning)', 'commenter': 'olim7t'}]"
1414,core/src/main/java/com/datastax/oss/driver/internal/core/metadata/token/SimpleReplicationStrategy.java,"@@ -63,9 +67,9 @@ private static Token getTokenWrapping(int i, List<Token> ring) {
     return ring.get(i % ring.size());
   }
 
-  private static int extractReplicationFactor(Map<String, String> replicationConfig) {
+  private static ReplicationFactor extractReplicationFactor(Map<String, String> replicationConfig) {
     String factorString = replicationConfig.get(""replication_factor"");
     Preconditions.checkNotNull(factorString, ""Missing replication factor in "" + replicationConfig);
-    return Integer.parseInt(factorString);
+    return ReplicationFactor.fromString(factorString);","[{'comment': 'to sum up this change, the transient replicas are only loaded and parsed, but currently, they are ignored by the `ReplicationStrategy`, right?', 'commenter': 'tomekl007'}, {'comment': ""Yes. They won't be used for token-aware routing, nor surfaced in token metadata. I think that's the right thing to do (see my other comment)."", 'commenter': 'olim7t'}, {'comment': 'That is correct see documentation here for more information on how this is working under the covers.\r\n\r\nhttp://cassandra.apache.org/blog/2018/12/03/introducing-transient-replication.html', 'commenter': 'GregBestland'}]"
1414,core/src/main/java/com/datastax/oss/driver/internal/core/metadata/token/SimpleReplicationStrategy.java,"@@ -30,22 +30,26 @@
 @ThreadSafe
 class SimpleReplicationStrategy implements ReplicationStrategy {
 
-  private final int replicationFactor;
+  private final ReplicationFactor replicationFactor;
 
   SimpleReplicationStrategy(Map<String, String> replicationConfig) {
     this(extractReplicationFactor(replicationConfig));
   }
 
   @VisibleForTesting
   SimpleReplicationStrategy(int replicationFactor) {
+    this(ReplicationFactor.fullOnly(replicationFactor));
+  }","[{'comment': 'Rather than preserving this constructor, we can have unit tests create a replication factor explicitly and call `SimpleReplicationStrategy(ReplicationFactor)`.\r\n\r\nThen maybe we can parameterize them to assert that a non-zero `transientReplicas` produces the same results.', 'commenter': 'olim7t'}]"
1414,core/src/main/java/com/datastax/oss/driver/internal/core/metadata/token/ReplicationFactor.java,"@@ -0,0 +1,84 @@
+package com.datastax.oss.driver.internal.core.metadata.token;
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import com.datastax.oss.driver.shaded.guava.common.base.Preconditions;
+import com.datastax.oss.driver.shaded.guava.common.base.Splitter;
+import java.util.List;
+import java.util.Objects;
+
+// This class is a subset of server version at org.apache.cassandra.locator.ReplicationFactor
+public class ReplicationFactor {
+  private final int allReplicas;
+  private final int fullReplicas;
+
+  private ReplicationFactor(int allReplicas, int transientReplicas) {
+    this.allReplicas = allReplicas;
+    this.fullReplicas = allReplicas - transientReplicas;
+  }
+
+  public static ReplicationFactor withTransient(int totalReplicas, int transientReplicas) {
+    return new ReplicationFactor(totalReplicas, transientReplicas);
+  }
+
+  public static ReplicationFactor fullOnly(int totalReplicas) {
+    return new ReplicationFactor(totalReplicas, 0);
+  }
+
+  public int fullReplicas() {
+    return fullReplicas;
+  }
+
+  public int transientReplicas() {
+    return allReplicas - fullReplicas;
+  }
+
+  public boolean hasTransientReplicas() {
+    return allReplicas != fullReplicas;
+  }
+
+  public static ReplicationFactor fromString(String s) {
+    if (s.contains(""/"")) {
+      List<String> parts = Splitter.on('/').splitToList(s);
+      Preconditions.checkArgument(
+          parts.size() == 2, ""Replication factor format is <replicas> or <replicas>/<transient>"");
+      return new ReplicationFactor(Integer.valueOf(parts.get(0)), Integer.valueOf(parts.get(1)));","[{'comment': 'Nit: use `Integer.parseInt` to avoid the IDEA warnings (redundant boxing)', 'commenter': 'olim7t'}]"
1414,core/src/main/java/com/datastax/oss/driver/internal/core/metadata/token/SimpleReplicationStrategy.java,"@@ -30,22 +29,21 @@
 @ThreadSafe
 class SimpleReplicationStrategy implements ReplicationStrategy {
 
-  private final int replicationFactor;
+  private final ReplicationFactor replicationFactor;
 
   SimpleReplicationStrategy(Map<String, String> replicationConfig) {
     this(extractReplicationFactor(replicationConfig));
   }
 
-  @VisibleForTesting","[{'comment': 'Nit: you can keep this annotation, the constructor is still only exposed for tests', 'commenter': 'olim7t'}]"
1414,core/src/main/java/com/datastax/oss/driver/internal/core/metadata/token/ReplicationFactor.java,"@@ -0,0 +1,84 @@
+package com.datastax.oss.driver.internal.core.metadata.token;
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import com.datastax.oss.driver.shaded.guava.common.base.Preconditions;
+import com.datastax.oss.driver.shaded.guava.common.base.Splitter;
+import java.util.List;
+import java.util.Objects;
+
+// This class is a subset of server version at org.apache.cassandra.locator.ReplicationFactor
+public class ReplicationFactor {
+  private final int allReplicas;
+  private final int fullReplicas;
+
+  private ReplicationFactor(int allReplicas, int transientReplicas) {
+    this.allReplicas = allReplicas;
+    this.fullReplicas = allReplicas - transientReplicas;","[{'comment': ""Why don't you store `transientReplicas` as a field? You are computing `fullReplicas = allReplicas - transientReplicas` here, but in `transientReplicas()` we need the inverse computation: `allReplicas - fullReplicas`."", 'commenter': 'adutra'}]"
1414,core/src/main/java/com/datastax/oss/driver/internal/core/metadata/token/ReplicationFactor.java,"@@ -0,0 +1,84 @@
+package com.datastax.oss.driver.internal.core.metadata.token;
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import com.datastax.oss.driver.shaded.guava.common.base.Preconditions;
+import com.datastax.oss.driver.shaded.guava.common.base.Splitter;
+import java.util.List;
+import java.util.Objects;
+
+// This class is a subset of server version at org.apache.cassandra.locator.ReplicationFactor
+public class ReplicationFactor {
+  private final int allReplicas;
+  private final int fullReplicas;
+
+  private ReplicationFactor(int allReplicas, int transientReplicas) {
+    this.allReplicas = allReplicas;
+    this.fullReplicas = allReplicas - transientReplicas;
+  }
+
+  public static ReplicationFactor withTransient(int allReplicas, int transientReplicas) {
+    return new ReplicationFactor(allReplicas, transientReplicas);
+  }
+
+  public static ReplicationFactor fullOnly(int allReplicas) {
+    return new ReplicationFactor(allReplicas, 0);
+  }
+
+  public int fullReplicas() {
+    return fullReplicas;
+  }
+
+  public int transientReplicas() {
+    return allReplicas - fullReplicas;
+  }
+
+  public boolean hasTransientReplicas() {
+    return allReplicas != fullReplicas;
+  }
+
+  public static ReplicationFactor fromString(String s) {
+    if (s.contains(""/"")) {
+      List<String> parts = Splitter.on('/').splitToList(s);
+      Preconditions.checkArgument(
+          parts.size() == 2, ""Replication factor format is <replicas> or <replicas>/<transient>"");
+      return new ReplicationFactor(Integer.parseInt(parts.get(0)), Integer.parseInt(parts.get(1)));
+    } else {
+      return new ReplicationFactor(Integer.parseInt(s), 0);
+    }
+  }
+
+  public String toParseableString() {
+    return allReplicas + (hasTransientReplicas() ? ""/"" + transientReplicas() : """");
+  }
+
+  @Override
+  public String toString() {
+    return ""rf("" + toParseableString() + ')';
+  }
+
+  @Override
+  public boolean equals(Object o) {
+    if (this == o) return true;","[{'comment': 'missing braces', 'commenter': 'adutra'}]"
1414,core/src/main/java/com/datastax/oss/driver/internal/core/metadata/token/ReplicationFactor.java,"@@ -0,0 +1,84 @@
+package com.datastax.oss.driver.internal.core.metadata.token;
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import com.datastax.oss.driver.shaded.guava.common.base.Preconditions;
+import com.datastax.oss.driver.shaded.guava.common.base.Splitter;
+import java.util.List;
+import java.util.Objects;
+
+// This class is a subset of server version at org.apache.cassandra.locator.ReplicationFactor
+public class ReplicationFactor {
+  private final int allReplicas;
+  private final int fullReplicas;
+
+  private ReplicationFactor(int allReplicas, int transientReplicas) {
+    this.allReplicas = allReplicas;
+    this.fullReplicas = allReplicas - transientReplicas;
+  }
+
+  public static ReplicationFactor withTransient(int allReplicas, int transientReplicas) {","[{'comment': 'Not convinced by the utility of these factory methods vs usage of the raw constructor (+ maybe an overloading constructor that assigns 0 transient replicas).', 'commenter': 'adutra'}]"
1414,core/src/main/java/com/datastax/oss/driver/internal/core/metadata/token/ReplicationFactor.java,"@@ -0,0 +1,84 @@
+package com.datastax.oss.driver.internal.core.metadata.token;
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import com.datastax.oss.driver.shaded.guava.common.base.Preconditions;
+import com.datastax.oss.driver.shaded.guava.common.base.Splitter;
+import java.util.List;
+import java.util.Objects;
+
+// This class is a subset of server version at org.apache.cassandra.locator.ReplicationFactor
+public class ReplicationFactor {
+  private final int allReplicas;
+  private final int fullReplicas;
+
+  private ReplicationFactor(int allReplicas, int transientReplicas) {
+    this.allReplicas = allReplicas;
+    this.fullReplicas = allReplicas - transientReplicas;
+  }
+
+  public static ReplicationFactor withTransient(int allReplicas, int transientReplicas) {
+    return new ReplicationFactor(allReplicas, transientReplicas);
+  }
+
+  public static ReplicationFactor fullOnly(int allReplicas) {
+    return new ReplicationFactor(allReplicas, 0);
+  }
+
+  public int fullReplicas() {
+    return fullReplicas;
+  }
+
+  public int transientReplicas() {
+    return allReplicas - fullReplicas;
+  }
+
+  public boolean hasTransientReplicas() {
+    return allReplicas != fullReplicas;
+  }
+
+  public static ReplicationFactor fromString(String s) {
+    if (s.contains(""/"")) {
+      List<String> parts = Splitter.on('/').splitToList(s);","[{'comment': ""`splitToList` is nice but marked as beta, we might want to use a good ol' \r\n\r\n```\r\nint slash = s.indexOf('/');\r\nString s1 = s.substring(0, slash);\r\nString s2 = s.substring(slash + 1);\r\n```"", 'commenter': 'adutra'}]"
1414,core/src/main/java/com/datastax/oss/driver/internal/core/metadata/token/ReplicationFactor.java,"@@ -0,0 +1,84 @@
+package com.datastax.oss.driver.internal.core.metadata.token;
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import com.datastax.oss.driver.shaded.guava.common.base.Preconditions;
+import java.util.Objects;
+
+// This class is a subset of server version at org.apache.cassandra.locator.ReplicationFactor
+public class ReplicationFactor {
+  private final int allReplicas;
+  private final int fullReplicas;
+  private final int transientReplicas;
+
+  public ReplicationFactor(int allReplicas, int transientReplicas) {
+    this.allReplicas = allReplicas;
+    this.transientReplicas = transientReplicas;
+    this.fullReplicas = allReplicas - transientReplicas;
+  }
+
+  public ReplicationFactor(int allReplicas) {
+    this(allReplicas, 0);
+  }
+
+  public int fullReplicas() {
+    return fullReplicas;
+  }
+
+  public int transientReplicas() {
+    return transientReplicas;
+  }
+
+  public boolean hasTransientReplicas() {
+    return allReplicas != fullReplicas;
+  }
+
+  public static ReplicationFactor fromString(String s) {
+    if (s.contains(""/"")) {
+
+      int slash = s.indexOf('/');
+      String allPart = s.substring(0, slash);
+      String transientPart = s.substring(slash + 1);
+      Preconditions.checkArgument(
+          allPart != null && transientPart != null,","[{'comment': ""The result of a substring cannot be null, you should be checking `allPart.isEmpty()` instead â€“ although the check isn't really necessary imo, the `parseInt` method below would fail if the strings contain anything but valid integers."", 'commenter': 'adutra'}]"
1420,driver-core/src/test/java/com/datastax/driver/core/CCMBridge.java,"@@ -1089,7 +1093,12 @@ public void run() {
     }
 
     private static boolean isThriftSupported(VersionNumber cassandraVersion) {
-      return cassandraVersion.compareTo(VersionNumber.parse(""4.0"")) < 0;
+      // Thrift is removed from some pre-release 4.x versions, make the comparison work for those
+      return cassandraVersion.compareTo(VersionNumber.parse(""4.0-a"")) < 0;","[{'comment': 'See my comment in the other PR, you should be able to use `VersionNumber.nextStable()` for that.', 'commenter': 'adutra'}]"
1420,driver-core/src/test/java/com/datastax/driver/core/TestUtils.java,"@@ -986,7 +986,7 @@ public static Level setLogLevel(String logger, Level newLevel) {
    * @param ccm cluster to check against
    */
   public static void compactStorageSupportCheck(CCMAccess ccm) {
-    if (ccm.getCassandraVersion().compareTo(VersionNumber.parse(""4.0.0"")) >= 0) {
+    if (ccm.getCassandraVersion().compareTo(VersionNumber.parse(""4.0-a"")) >= 0) {","[{'comment': 'again, change this to use `nextStable()`', 'commenter': 'emerkle826'}]"
1420,driver-core/src/main/java/com/datastax/driver/core/AbstractTableMetadata.java,"@@ -258,14 +258,19 @@ protected StringBuilder appendOptions(StringBuilder sb, boolean formatted) {
     sb.append(""WITH "");
     if (options.isCompactStorage()) and(sb.append(""COMPACT STORAGE""), formatted);
     if (!clusteringOrder.isEmpty()) and(appendClusteringOrder(sb), formatted);
-    sb.append(""read_repair_chance = "").append(options.getReadRepairChance());
-    and(sb, formatted)
-        .append(""dclocal_read_repair_chance = "")
-        .append(options.getLocalReadRepairChance());
+    if (cassandraVersion.getMajor() < 4)
+      sb.append(""read_repair_chance = "").append(options.getReadRepairChance());
+    else sb.append(""read_repair = 'BLOCKING'"");
+    if (cassandraVersion.getMajor() < 4)
+      and(sb, formatted)
+          .append(""dclocal_read_repair_chance = "")
+          .append(options.getLocalReadRepairChance());","[{'comment': 'I have created https://datastax-oss.atlassian.net/browse/JAVA-2730 for addressing this.', 'commenter': 'emerkle826'}]"
1420,changelog/README.md,"@@ -21,6 +21,7 @@
 - [documentation] JAVA-2504: Migrate Cloud ""getting started"" page to driver manual.
 - [improvement] JAVA-2516: Enable hostname validation with Cloud
 - [bug] JAVA-2515: NEW_NODE and REMOVED_NODE events should trigger ADDED and REMOVED.
+- [improvement[ JAVA-2730: Add support for CassandraÂ® 4.0 table options","[{'comment': 'typo', 'commenter': 'emerkle826'}]"
1420,driver-core/src/main/java/com/datastax/driver/core/TableOptionsMetadata.java,"@@ -23,9 +23,10 @@
 public class TableOptionsMetadata {
 
   private static final String COMMENT = ""comment"";
-  private static final String READ_REPAIR = ""read_repair_chance"";
-  private static final String DCLOCAL_READ_REPAIR = ""dclocal_read_repair_chance"";
-  private static final String LOCAL_READ_REPAIR = ""local_read_repair_chance"";
+  private static final String READ_REPAIR_CHANCE = ""read_repair_chance"";","[{'comment': 'I renamed a few private variables:\r\n`READ_REPAIR` to `READ_REPAIR_CHANCE`\r\n`DCLOCAL_READ_REPAIR` to `DCLOCAL_READ_REPAIR_CHANCE`\r\n`LOCAL_READ_REPAIR` to `LOCAL_READ_REPAIR_CHANCE`\r\nso that I could introduce `READ_REPAIR`.\r\n\r\nLuckily the public access methods were already `getXYZchance()`', 'commenter': 'emerkle826'}]"
1420,driver-core/src/main/java/com/datastax/driver/core/TableOptionsMetadata.java,"@@ -206,6 +218,19 @@ public String getComment() {
    * @return the read repair chance set for table (in [0.0, 1.0]).
    */
   public double getReadRepairChance() {
+    return readRepairChance;
+  }
+
+  /**
+   * Returns the read_repair option for this table. <b>NOTE:</b> this is a CassandraÂ® 4.0 and newer
+   * option (described here<a","[{'comment': 'Nit: spurious word ""here"" before &lt;a> tag.', 'commenter': 'adutra'}, {'comment': 'Actually, I intended ""here"" to not be part of the link, but either way I\'ve got it twice which was not the intent.', 'commenter': 'emerkle826'}]"
1420,driver-core/src/main/java/com/datastax/driver/core/TableOptionsMetadata.java,"@@ -206,6 +218,19 @@ public String getComment() {
    * @return the read repair chance set for table (in [0.0, 1.0]).
    */
   public double getReadRepairChance() {
+    return readRepairChance;
+  }
+
+  /**
+   * Returns the read_repair option for this table. <b>NOTE:</b> this is a CassandraÂ® 4.0 and newer
+   * option (described here<a
+   * href=""http://cassandra.apache.org/doc/latest/operating/read_repair.html"">here:
+   * http://cassandra.apache.org/doc/latest/operating/read_repair.html</a>). Possible values are","[{'comment': ""The link is included in the link's text, is that on purpose?"", 'commenter': 'adutra'}, {'comment': 'Yes, my intent was to link to the Cassandra documentation, and have the link show the full address.', 'commenter': 'emerkle826'}]"
1420,driver-core/src/test/java/com/datastax/driver/core/CCMBridge.java,"@@ -1089,7 +1097,16 @@ public void run() {
     }
 
     private static boolean isThriftSupported(VersionNumber cassandraVersion) {
-      return cassandraVersion.compareTo(VersionNumber.parse(""4.0"")) < 0;
+      // Thrift is removed from some pre-release 4.x versions, make the comparison work for those
+      return cassandraVersion.nextStable().compareTo(VersionNumber.parse(""4.0"")) < 0;
+    }
+
+    private static boolean isMaterializedViewsSupported(VersionNumber cassandraVersion) {
+      return cassandraVersion.nextStable().compareTo(VersionNumber.parse(""4.0"")) >= 0;","[{'comment': 'MVs are available for C* 3+, but indeed they are disabled by default in C* 4+.\r\n\r\nIf the intent of this method is to detect whether or not we need to include `enable_materialized_views` in C* options, then its name is misleading. I suggest `isMaterializedViewsDisabledByDefault`.\r\n\r\nSee JAVA-2183.\r\n', 'commenter': 'adutra'}, {'comment': 'Good point, this makes more sense.', 'commenter': 'emerkle826'}]"
1420,driver-core/src/test/java/com/datastax/driver/core/MissingRpcAddressTest.java,"@@ -67,10 +67,18 @@ private void deleteNode2RpcAddressFromNode1() throws Exception {
                         Policies.defaultLoadBalancingPolicy(), Lists.newArrayList(firstHost)))
                 .build());
     Session session = cluster.connect();
-    String deleteStmt =
-        String.format(
-            ""DELETE rpc_address FROM system.peers WHERE peer = '%s'"",
-            ccm().addressOfNode(2).getHostName());
+    String deleteStmt;
+    if (ccm().getCassandraVersion().nextStable().compareTo(VersionNumber.parse(""4.0"")) < 0) {
+      deleteStmt =","[{'comment': ""Shouldn't we instead always delete from system.peers, and additionally, also delete from system.peers_v2 if this table exists?"", 'commenter': 'adutra'}, {'comment': ""Yes, I think this is correct. I missed that detail when focusing on fixing this for 4.0 as deleting from peers wasn't enough."", 'commenter': 'emerkle826'}]"
1420,driver-tests/osgi/src/test/java/com/datastax/driver/osgi/BundleOptions.java,"@@ -165,7 +165,16 @@ public static CompositeOption defaultOptions() {
                 mavenBundle(""ch.qos.logback"", ""logback-classic"", getVersion(""logback.version"")),
                 mavenBundle(""ch.qos.logback"", ""logback-core"", getVersion(""logback.version"")),
                 mavenBundle(""io.dropwizard.metrics"", ""metrics-core"", getVersion(""metrics.version"")),
-                mavenBundle(""org.yaml"", ""snakeyaml"", getVersion(""snakeyaml.version"")),
+                mavenBundle(","[{'comment': ""1. So snakeyaml was being included for nothing?\r\n2. Why do we need jackson in all OSGi configs now? Isn't this going to be a problem for the shaded jar?"", 'commenter': 'adutra'}, {'comment': ""I'm not exactly sure about this, but the Cloud API support is where this changed. The commit [here](https://github.com/datastax/java-driver/commit/aa5d6d859a5490e21a5216edf5aef54d9f6cd7c7) introduced Jackson dependencies, but also added snakeyaml to the OSGi BundleOptions.\r\n\r\nI'm guessing snakeyaml was added as an attempt to avoid adding Jackson, and put in the OSGi BundleOptions and subsequently forgotten. If I recall correctly, we tried snakeyaml for the config at first, but it was easier to just use Jackson. So maybe later, Jackson got added, but the OSGi stuff was left alone.\r\n\r\nIt seems the Jackson libraries are only added for testing though. I'm still digging in on the shaded jar to see...."", 'commenter': 'emerkle826'}, {'comment': ""OK but let's not spend an insane amount of time on OSGi tests, given that JAVA-2658 will change all of this radically."", 'commenter': 'adutra'}]"
1422,core/src/main/java/com/datastax/oss/driver/internal/core/os/Native.java,"@@ -15,67 +15,32 @@
  */
 package com.datastax.oss.driver.internal.core.os;
 
-import com.datastax.oss.driver.internal.core.util.Reflection;
-import java.lang.reflect.Method;
-import jnr.ffi.LibraryLoader;
 import jnr.ffi.Platform;
-import jnr.ffi.Pointer;
-import jnr.ffi.Runtime;
-import jnr.ffi.Struct;
-import jnr.ffi.annotations.Out;
-import jnr.ffi.annotations.Transient;
-import jnr.posix.POSIXFactory;
-import jnr.posix.util.DefaultPOSIXHandler;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
 
 /** A gateway to perform system calls. */
 public class Native {
 
-  private static final Logger LOG = LoggerFactory.getLogger(Native.class);
+  private static NativeImpl impl = new JnrNativeImpl();","[{'comment': ""The key to the refactoring of this class: I'm laying the groundwork here for JAVA-2663 as well.  It's fairly easy to see how a GraalNativeImpl would fit in here."", 'commenter': 'absurdfarce'}, {'comment': 'How do you plan to parameterize it and inject different implementations? We need to move it higher in the call chain (here probably constructor injection) and pass it from somewhere? Maybe parameterized `DriverContext`?', 'commenter': 'tomekl007'}, {'comment': 'I\'m envisioning something like the following:\r\n\r\n```java\r\n  private static NativeImpl impl =\r\n          /* Hope is that Graal build-time analysis will detect this case and follow the right path\r\n          * (i.e. exclude JnrNativeImpl from consideration in the build).  Obviously this needs to be\r\n          * tested.\r\n          *\r\n          * Constant values here come from org.graalvm.nativeimage.ImageInfo and are guaranteed not to change\r\n          * in future releases.  Use the string literals here in order to avoid pulling in an import of GraalVM\r\n          * code in this class. */\r\n          System.getProperty(""org.graalvm.nativeimage.imagecode"").equals(""buildtime"") ?\r\n            new GraalNativeImpl() :\r\n              new JnrNativeImpl();\r\n```\r\n\r\nObviously that\'ll need to be tested in a real case before we call it good.  That work will be handled in JAVA-2663.', 'commenter': 'absurdfarce'}, {'comment': ""Suggestion: make `Native` the interface directly, in order to avoid the additional abstraction.\r\n```java\r\npublic interface Native {\r\n  static Native INSTANCE = new JnrNative();\r\n  boolean isCurrentTimeMicrosAvailable();\r\n  ...\r\n}\r\npublic class JnrNative implements Native {\r\n  @Override\r\n  public boolean isCurrentTimeMicrosAvailable() { ... }\r\n  ...\r\n}\r\n```\r\nThis will just require a small change to every call site: `Native.currentTimeMicros()` => `Native.INSTANCE.currentTimeMicros()`. But again this is internal code so we're allowed to do it, and it's cleaner."", 'commenter': 'olim7t'}, {'comment': ""In isolation I'd probably agree with you, but keep in mind we'll be adding a different NativeImpl for Graal in JAVA-2663.  With that in mind I'd argue it's cleaner to maintain Native as a top-level facade and swap the composed class (the NativeImpl implementation) based on Graal/not-Graal."", 'commenter': 'absurdfarce'}, {'comment': ""As far as adding another implementation, this doesn't change anything:\r\n```java\r\npublic class GraalNative implements Native {\r\n  ...\r\n}\r\n```\r\nThe `Native.impl` or `Native.INSTANCE` singleton is held in a static variable in both cases, so it shouldn't be any harder to initialize it based on some environment condition."", 'commenter': 'olim7t'}]"
1422,core/src/main/java/com/datastax/oss/driver/internal/core/os/JnrNativeImpl.java,"@@ -0,0 +1,154 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.os;
+
+import com.datastax.oss.driver.internal.core.util.Reflection;
+import java.lang.reflect.Method;
+import java.util.Optional;
+import java.util.function.Supplier;
+import jnr.ffi.Platform;
+import jnr.posix.POSIX;
+import jnr.posix.POSIXFactory;
+import jnr.posix.Timeval;
+import jnr.posix.util.DefaultPOSIXHandler;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class JnrNativeImpl implements NativeImpl {
+
+  private static final Logger LOG = LoggerFactory.getLogger(JnrNativeImpl.class);
+
+  private final Optional<POSIX> posix;
+  private final Optional<Platform> platform;
+
+  public JnrNativeImpl() {
+
+    this.posix = loadPosix();
+    this.platform = loadPlatform();
+  }
+
+  private Optional<POSIX> loadPosix() {
+
+    try {
+      return Optional.of(POSIXFactory.getPOSIX(new DefaultPOSIXHandler(), true))
+          .flatMap(this::validatePosix);
+    } catch (Throwable t) {
+      LOG.debug(""Error loading POSIX"", t);
+      return Optional.empty();
+    }
+  }
+
+  private Optional<POSIX> validatePosix(POSIX posix) {
+
+    try {
+
+      posix.getpid();
+    } catch (Throwable t) {
+
+      LOG.debug(""Error calling getpid()"", t);
+      return Optional.empty();
+    }
+
+    try {
+
+      Timeval tv = posix.allocateTimeval();
+      int rv = posix.gettimeofday(tv);
+      if (rv != 0) {
+
+        LOG.debug(""Expected getitimeofday() to return zero, observed {}"", rv);
+        return Optional.empty();
+      }
+    } catch (Throwable t) {
+
+      LOG.debug(""Error calling gettimeofday()"", t);
+      return Optional.empty();
+    }
+
+    return Optional.of(posix);
+  }
+
+  private Optional<Platform> loadPlatform() {
+
+    try {
+
+      Class<?> platformClass = Reflection.loadClass(null, ""jnr.ffi.Platform"");
+      if (platformClass != null) {
+        Method getNativePlatform = platformClass.getMethod(""getNativePlatform"");
+        return Optional.of((jnr.ffi.Platform) getNativePlatform.invoke(null));
+      }
+      return Optional.empty();
+    } catch (Throwable t) {
+      LOG.debug(""Error loading jnr.ffi.Platform class, this class will not be available."", t);
+      return Optional.empty();
+    }
+  }
+
+  @Override
+  public boolean gettimeofdayAvailable() {
+    return posix.isPresent();
+  }
+
+  @Override
+  public long gettimeofday() {
+
+    Supplier<IllegalStateException> exceptionSupplier =
+        () -> {
+          throw new IllegalStateException(
+              ""Native call not available. ""
+                  + ""Check isCurrentTimeMicrosAvailable() before calling this method."");
+        };
+
+    Timeval tv = this.posix.map(POSIX::allocateTimeval).orElseThrow(exceptionSupplier);
+    int rv = this.posix.map(p -> p.gettimeofday(tv)).orElseThrow(exceptionSupplier);","[{'comment': ""I'm being conservative here.  It's hard to imagine a case in which we'd be able to allocate a new Timeval but somehow get an exception (as opposed to a non-zero return val) when we go to make the actual syscall... but you never know."", 'commenter': 'absurdfarce'}]"
1422,core/src/main/java/com/datastax/oss/driver/internal/core/os/JnrNativeImpl.java,"@@ -0,0 +1,154 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.os;
+
+import com.datastax.oss.driver.internal.core.util.Reflection;
+import java.lang.reflect.Method;
+import java.util.Optional;
+import java.util.function.Supplier;
+import jnr.ffi.Platform;
+import jnr.posix.POSIX;
+import jnr.posix.POSIXFactory;
+import jnr.posix.Timeval;
+import jnr.posix.util.DefaultPOSIXHandler;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class JnrNativeImpl implements NativeImpl {
+
+  private static final Logger LOG = LoggerFactory.getLogger(JnrNativeImpl.class);
+
+  private final Optional<POSIX> posix;
+  private final Optional<Platform> platform;
+
+  public JnrNativeImpl() {
+
+    this.posix = loadPosix();
+    this.platform = loadPlatform();
+  }
+
+  private Optional<POSIX> loadPosix() {
+
+    try {
+      return Optional.of(POSIXFactory.getPOSIX(new DefaultPOSIXHandler(), true))
+          .flatMap(this::validatePosix);
+    } catch (Throwable t) {
+      LOG.debug(""Error loading POSIX"", t);
+      return Optional.empty();
+    }
+  }
+
+  private Optional<POSIX> validatePosix(POSIX posix) {
+
+    try {
+
+      posix.getpid();
+    } catch (Throwable t) {
+
+      LOG.debug(""Error calling getpid()"", t);
+      return Optional.empty();
+    }
+
+    try {
+
+      Timeval tv = posix.allocateTimeval();
+      int rv = posix.gettimeofday(tv);
+      if (rv != 0) {
+
+        LOG.debug(""Expected getitimeofday() to return zero, observed {}"", rv);
+        return Optional.empty();
+      }
+    } catch (Throwable t) {
+
+      LOG.debug(""Error calling gettimeofday()"", t);
+      return Optional.empty();
+    }
+
+    return Optional.of(posix);
+  }","[{'comment': 'There\'s a _slight_ difference in behaviour from the old impl here.  We\'ve moved both calls into the jnr-posix orbit but don\'t maintain discrete vals for the validation of each discrete call; we just validate the whole thing by confirming that we can call _both_ native functions.  Logically this means that we could find ourselves in a situation where getpid() succeeded but gettimeofday() didn\'t, and in that case we\'d prevent users from subsequent calls to _either_ syscall when (in theory) we could allow them to call getpid() and just throw on gettimeofday() calls.\r\n\r\nNote that gettimeofday() ""failing"" above means throwing an exception (i.e.a failure in the Java infrastructure) not just a non-zero return val (which indicates that the syscall was made and something went wrong within the kernel).\r\n\r\nI kinda like this idea even in the abstract; if we can\'t validate that your POSIX instance can do _everything_ we expect it\'s in an untrustworthy state and we don\'t allow you to do anything.  More practically, it seems _extremely_ unlikely that we\'d get a Java exception with one syscall but not another; the whole point of POSIX is that we expect syscalls with these names and params to be available.  If one is there and another isn\'t we\'ve got some kind of very odd libc we shouldn\'t trust much anyway.', 'commenter': 'absurdfarce'}, {'comment': ""> More practically, it seems extremely unlikely that we'd get a Java exception with one syscall but not another\r\n\r\nðŸ’¯ \r\nI'm fine with the change."", 'commenter': 'olim7t'}]"
1422,core/src/main/java/com/datastax/oss/driver/internal/core/os/Native.java,"@@ -15,67 +15,32 @@
  */
 package com.datastax.oss.driver.internal.core.os;
 
-import com.datastax.oss.driver.internal.core.util.Reflection;
-import java.lang.reflect.Method;
-import jnr.ffi.LibraryLoader;
 import jnr.ffi.Platform;","[{'comment': 'this class cannot have any dependency on `jnr.ffi`, otherwise quarkus native image build will fail. We need to remove this dependency.', 'commenter': 'tomekl007'}, {'comment': ""Platform is a different case from the other jnr-ffi code.  It doesn't actually execute any native calls... it just builds up some state based on sysprops and exposes that.  I think there's a comment or two to this effect at https://github.com/absurdfarce/graal-cassandra-driver.\r\n\r\nRegardless, we probably can expose this information in some other way.  I'll dig into the Platform code a bit to see what it's looking at to populate this data.  If we can pull the info from the same place and just include a default impl in the NativeImpl interface that would be awesome. :)"", 'commenter': 'absurdfarce'}, {'comment': 'I wound up folding the jnr.ffi.Platform code into Native and doing away with our dependence on Platform all together.  As mentioned above our usage of Platform is really just an eval of a sysprop, so now we just do all of that inline.', 'commenter': 'absurdfarce'}, {'comment': ""> Platform is a different case from the other jnr-ffi code. It doesn't actually execute any native calls...\r\n\r\nWow I never realized that. In that case, ðŸ‘ to remove it, it's not worth the extra dependency. We'll also need to update the [integration page](https://docs.datastax.com/en/developer/java-driver/4.5/manual/core/integration/#native-libraries) in the manual.\r\n\r\njnr-ffi is ASL so copying the code is fine. However I think it would be a good idea to reproduce their header in our code, along the lines of what I did in [CountingIterator](https://github.com/datastax/java-driver/blob/4.x/core/src/main/java/com/datastax/oss/driver/internal/core/util/CountingIterator.java#L40-L57)."", 'commenter': 'olim7t'}, {'comment': "":100: agree with the idea of maximizing attribution so I'll add the jnr-ffi license as requested.  I'm also updating our docs to clearly indicate that jnr-posix rather than jnr-ffi is the guiding force behind the underlying native calls.\r\n\r\nNote, however, that we won't wind up removing jnr-ffi in it's entirety as it's required by jnr-posix.  But we can remove our declaration of it and just let jnr-posix pull in whatever it needs.  Apparently this has come up before (JAVA-1797) with Andy landing on a similar idea.\r\n\r\nMy only concern with that approach is making sure we're okay on the shaded JAR; I'll check that before I put all this in."", 'commenter': 'absurdfarce'}]"
1422,core/src/main/java/com/datastax/oss/driver/internal/core/os/JnrNativeImpl.java,"@@ -0,0 +1,154 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.os;
+
+import com.datastax.oss.driver.internal.core.util.Reflection;
+import java.lang.reflect.Method;
+import java.util.Optional;
+import java.util.function.Supplier;
+import jnr.ffi.Platform;
+import jnr.posix.POSIX;
+import jnr.posix.POSIXFactory;
+import jnr.posix.Timeval;
+import jnr.posix.util.DefaultPOSIXHandler;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class JnrNativeImpl implements NativeImpl {
+
+  private static final Logger LOG = LoggerFactory.getLogger(JnrNativeImpl.class);
+
+  private final Optional<POSIX> posix;
+  private final Optional<Platform> platform;
+
+  public JnrNativeImpl() {
+
+    this.posix = loadPosix();
+    this.platform = loadPlatform();
+  }
+
+  private Optional<POSIX> loadPosix() {
+
+    try {
+      return Optional.of(POSIXFactory.getPOSIX(new DefaultPOSIXHandler(), true))
+          .flatMap(this::validatePosix);
+    } catch (Throwable t) {
+      LOG.debug(""Error loading POSIX"", t);
+      return Optional.empty();
+    }
+  }
+
+  private Optional<POSIX> validatePosix(POSIX posix) {
+
+    try {
+
+      posix.getpid();
+    } catch (Throwable t) {
+
+      LOG.debug(""Error calling getpid()"", t);
+      return Optional.empty();
+    }
+
+    try {
+
+      Timeval tv = posix.allocateTimeval();
+      int rv = posix.gettimeofday(tv);
+      if (rv != 0) {
+
+        LOG.debug(""Expected getitimeofday() to return zero, observed {}"", rv);
+        return Optional.empty();
+      }
+    } catch (Throwable t) {
+
+      LOG.debug(""Error calling gettimeofday()"", t);
+      return Optional.empty();
+    }
+
+    return Optional.of(posix);
+  }
+
+  private Optional<Platform> loadPlatform() {
+
+    try {
+
+      Class<?> platformClass = Reflection.loadClass(null, ""jnr.ffi.Platform"");
+      if (platformClass != null) {
+        Method getNativePlatform = platformClass.getMethod(""getNativePlatform"");
+        return Optional.of((jnr.ffi.Platform) getNativePlatform.invoke(null));
+      }
+      return Optional.empty();
+    } catch (Throwable t) {
+      LOG.debug(""Error loading jnr.ffi.Platform class, this class will not be available."", t);
+      return Optional.empty();
+    }
+  }
+
+  @Override
+  public boolean gettimeofdayAvailable() {
+    return posix.isPresent();
+  }
+
+  @Override
+  public long gettimeofday() {
+
+    Supplier<IllegalStateException> exceptionSupplier =
+        () -> {
+          throw new IllegalStateException(
+              ""Native call not available. ""
+                  + ""Check isCurrentTimeMicrosAvailable() before calling this method."");
+        };
+
+    Timeval tv = this.posix.map(POSIX::allocateTimeval).orElseThrow(exceptionSupplier);
+    int rv = this.posix.map(p -> p.gettimeofday(tv)).orElseThrow(exceptionSupplier);
+    if (rv != 0) {
+      throw new IllegalStateException(""Expected 0 return valu from gettimeofday(), observed "" + rv);","[{'comment': 'Small typo:\r\n```suggestion\r\n      throw new IllegalStateException(""Expected 0 return value from gettimeofday(), observed "" + rv);\r\n```', 'commenter': 'olim7t'}]"
1422,core/src/main/java/com/datastax/oss/driver/internal/core/os/JnrNativeImpl.java,"@@ -0,0 +1,154 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.os;
+
+import com.datastax.oss.driver.internal.core.util.Reflection;
+import java.lang.reflect.Method;
+import java.util.Optional;
+import java.util.function.Supplier;
+import jnr.ffi.Platform;
+import jnr.posix.POSIX;
+import jnr.posix.POSIXFactory;
+import jnr.posix.Timeval;
+import jnr.posix.util.DefaultPOSIXHandler;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class JnrNativeImpl implements NativeImpl {
+
+  private static final Logger LOG = LoggerFactory.getLogger(JnrNativeImpl.class);
+
+  private final Optional<POSIX> posix;
+  private final Optional<Platform> platform;
+
+  public JnrNativeImpl() {
+
+    this.posix = loadPosix();
+    this.platform = loadPlatform();
+  }
+
+  private Optional<POSIX> loadPosix() {
+
+    try {
+      return Optional.of(POSIXFactory.getPOSIX(new DefaultPOSIXHandler(), true))
+          .flatMap(this::validatePosix);
+    } catch (Throwable t) {
+      LOG.debug(""Error loading POSIX"", t);
+      return Optional.empty();
+    }
+  }
+
+  private Optional<POSIX> validatePosix(POSIX posix) {
+
+    try {
+
+      posix.getpid();
+    } catch (Throwable t) {
+
+      LOG.debug(""Error calling getpid()"", t);
+      return Optional.empty();
+    }
+
+    try {
+
+      Timeval tv = posix.allocateTimeval();
+      int rv = posix.gettimeofday(tv);
+      if (rv != 0) {
+
+        LOG.debug(""Expected getitimeofday() to return zero, observed {}"", rv);
+        return Optional.empty();
+      }
+    } catch (Throwable t) {
+
+      LOG.debug(""Error calling gettimeofday()"", t);
+      return Optional.empty();
+    }
+
+    return Optional.of(posix);
+  }
+
+  private Optional<Platform> loadPlatform() {
+
+    try {
+
+      Class<?> platformClass = Reflection.loadClass(null, ""jnr.ffi.Platform"");
+      if (platformClass != null) {
+        Method getNativePlatform = platformClass.getMethod(""getNativePlatform"");
+        return Optional.of((jnr.ffi.Platform) getNativePlatform.invoke(null));
+      }
+      return Optional.empty();
+    } catch (Throwable t) {
+      LOG.debug(""Error loading jnr.ffi.Platform class, this class will not be available."", t);
+      return Optional.empty();
+    }
+  }
+
+  @Override
+  public boolean gettimeofdayAvailable() {
+    return posix.isPresent();
+  }
+
+  @Override
+  public long gettimeofday() {
+
+    Supplier<IllegalStateException> exceptionSupplier =
+        () -> {
+          throw new IllegalStateException(
+              ""Native call not available. ""
+                  + ""Check isCurrentTimeMicrosAvailable() before calling this method."");
+        };
+
+    Timeval tv = this.posix.map(POSIX::allocateTimeval).orElseThrow(exceptionSupplier);
+    int rv = this.posix.map(p -> p.gettimeofday(tv)).orElseThrow(exceptionSupplier);
+    if (rv != 0) {
+      throw new IllegalStateException(""Expected 0 return valu from gettimeofday(), observed "" + rv);
+    }
+    return tv.sec() * 1000000 + tv.usec();","[{'comment': 'nit:\r\n```suggestion\r\n    return tv.sec() * 1_000_000 + tv.usec();\r\n```', 'commenter': 'olim7t'}]"
1422,core/src/main/java/com/datastax/oss/driver/internal/core/os/Native.java,"@@ -84,11 +49,7 @@ public static int getProcessId() {
    * @return {@code true} if JNR {@link Platform} class is loaded.
    */
   public static boolean isPlatformAvailable() {","[{'comment': ""Unfortunate naming here, this should be called `isCpuAvailable`.\r\n\r\nWe could take advantage of this ticket to rename it, it's internal code so it won't break the API."", 'commenter': 'olim7t'}, {'comment': ""I agree re: unfortunate naming and I'm happy to change it.\r\n\r\nOh wait, nevermind, that's right... this method actually disappears because we're loading CPU information directly rather than relying on Platform to do it. :man_facepalming: "", 'commenter': 'absurdfarce'}]"
1422,core/src/main/java/com/datastax/oss/driver/internal/core/os/JnrNativeImpl.java,"@@ -0,0 +1,154 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.os;
+
+import com.datastax.oss.driver.internal.core.util.Reflection;
+import java.lang.reflect.Method;
+import java.util.Optional;
+import java.util.function.Supplier;
+import jnr.ffi.Platform;
+import jnr.posix.POSIX;
+import jnr.posix.POSIXFactory;
+import jnr.posix.Timeval;
+import jnr.posix.util.DefaultPOSIXHandler;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class JnrNativeImpl implements NativeImpl {
+
+  private static final Logger LOG = LoggerFactory.getLogger(JnrNativeImpl.class);
+
+  private final Optional<POSIX> posix;
+  private final Optional<Platform> platform;","[{'comment': ""We allow users to [exclude JNR dependencies manually](https://docs.datastax.com/en/developer/java-driver/4.5/manual/core/integration/#native-libraries). If those JARs are missing from the classpath, `Native` should still load correctly, even though all `isXxxAvailable()` methods will return false.\r\n\r\nThis doesn't work with this version, if I exclude JNR I get:\r\n```\r\njava.lang.NoClassDefFoundError: jnr/posix/POSIXHandler\r\n\tat com.datastax.oss.driver.internal.core.os.Native.<clinit>(Native.java:23)\r\n```\r\n\r\nI suggest we simply copy over the code from 4.x, with the `PosixLoader` and `PlatformLoader` inner classes. It's a bit less pretty, but it's tried and tested."", 'commenter': 'olim7t'}, {'comment': 'Ah, that explains the NoClassDefFoundError catch blocks in the old code.  I thought that was handling something else but now it makes considerably more sense.\r\n\r\nThere\'s no need to copy in the old loader code; all we really need here is a loader which can handle exceptions coming off attempts to create instances.  I\'d argue that\'s actually a cleaner way to represent what\'s occurring in this case; the old way (loading everything fine and then having NCDFE checks around the ""isXAvailable"" calls) is far less clear (as evidenced by my confusion above).\r\n\r\nI\'ll put together a quick change to handle this... it shouldn\'t be too bad.', 'commenter': 'absurdfarce'}, {'comment': 'OK, I saw your empty implementation approach, that works for me.', 'commenter': 'olim7t'}]"
1422,core/src/main/java/com/datastax/oss/driver/internal/core/os/Native.java,"@@ -15,191 +15,173 @@
  */
 package com.datastax.oss.driver.internal.core.os;
 
-import com.datastax.oss.driver.internal.core.util.Reflection;
-import java.lang.reflect.Method;
-import jnr.ffi.LibraryLoader;
-import jnr.ffi.Platform;
-import jnr.ffi.Pointer;
-import jnr.ffi.Runtime;
-import jnr.ffi.Struct;
-import jnr.ffi.annotations.Out;
-import jnr.ffi.annotations.Transient;
-import jnr.posix.POSIXFactory;
-import jnr.posix.util.DefaultPOSIXHandler;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
+import java.util.Locale;
 
 /** A gateway to perform system calls. */
 public class Native {
 
-  private static final Logger LOG = LoggerFactory.getLogger(Native.class);
+  /* Copied from equivalent op in jnr.ffi.Platform.  We have to have this here as it has to be defined
+   * before it's (multiple) uses in determineCpu() */
+  private static final Locale LOCALE = java.util.Locale.ENGLISH;
+
+  private static final NativeImpl IMPL = new JnrNativeImpl();
+
+  @SuppressWarnings(""VariableNameSameAsType"")
+  private static final CPU CPU = determineCPU();
 
   /** Whether {@link Native#currentTimeMicros()} is available on this system. */
   public static boolean isCurrentTimeMicrosAvailable() {
-    try {
-      return LibCLoader.GET_TIME_OF_DAY_AVAILABLE;
-    } catch (NoClassDefFoundError e) {
-      return false;
-    }
+    return IMPL.gettimeofdayAvailable();
   }
 
   /**
    * The current time in microseconds, as returned by libc.gettimeofday(); can only be used if
    * {@link #isCurrentTimeMicrosAvailable()} is true.
    */
   public static long currentTimeMicros() {
-    if (!isCurrentTimeMicrosAvailable()) {
-      throw new IllegalStateException(
-          ""Native call not available. ""
-              + ""Check isCurrentTimeMicrosAvailable() before calling this method."");
-    }
-    LibCLoader.Timeval tv = new LibCLoader.Timeval(LibCLoader.LIB_C_RUNTIME);
-    int res = LibCLoader.LIB_C.gettimeofday(tv, null);
-    if (res != 0) {
-      throw new IllegalStateException(""Call to libc.gettimeofday() failed with result "" + res);
-    }
-    return tv.tv_sec.get() * 1000000 + tv.tv_usec.get();
+    return IMPL.gettimeofday();
   }
 
   public static boolean isGetProcessIdAvailable() {
-    try {
-      return PosixLoader.GET_PID_AVAILABLE;
-    } catch (NoClassDefFoundError e) {
-      return false;
-    }
+    return IMPL.getpidAvailable();
   }
 
   public static int getProcessId() {
-    if (!isGetProcessIdAvailable()) {
-      throw new IllegalStateException(
-          ""Native call not available. ""
-              + ""Check isGetProcessIdAvailable() before calling this method."");
-    }
-    return PosixLoader.POSIX.getpid();
+    return IMPL.getpid();
   }
 
   /**
-   * Returns {@code true} if JNR {@link Platform} class is loaded, and {@code false} otherwise.
-   *
-   * @return {@code true} if JNR {@link Platform} class is loaded.
-   */
-  public static boolean isPlatformAvailable() {
-    try {
-      return PlatformLoader.PLATFORM != null;
-    } catch (NoClassDefFoundError e) {
-      return false;
-    }
-  }
-
-  /**
-   * Returns the current processor architecture the JVM is running on, as reported by {@link
-   * Platform#getCPU()}.
+   * Returns the current processor architecture the JVM is running on. This value should match up to
+   * what's returned by jnr-ffi's Platform.getCPU() method.
    *
    * @return the current processor architecture.
-   * @throws IllegalStateException if JNR Platform library is not loaded.
    */
   public static String getCPU() {
-    if (!isPlatformAvailable())
-      throw new IllegalStateException(
-          ""JNR Platform class not loaded. ""
-              + ""Check isPlatformAvailable() before calling this method."");
-    return PlatformLoader.PLATFORM.getCPU().toString();
+    return CPU.toString();
   }
 
-  /**
-   * If jnr-ffi is not in the classpath at runtime, we'll fail to initialize the static fields
-   * below, but we still want {@link Native} to initialize successfully, so use an inner class.
+  /* The remainder of this class is copied from jnr.ffi.Platform in jnr-ffi version 2.1.10.
+   * We copy it manually here in order to avoid introducing an extra dependency merely for the sake of
+   * evaluating some system properties.","[{'comment': ""In isolation this comment looks a bit odd.  We already have JnrNativeImpl which hides the dependency on jnr-posix... why not do something similar with jnr-ffi and avoid copying code?  This suggestion breaks down when we consider adding Graal support in JAVA-2663.  In that case we'll still want to provide this functionality for _both_ impls but ideally we wouldn't mix jnr-ffi and the Graal impl.  With this in mind the cleanest answer was to promote this functionality to the Native level and implement it by copyig code."", 'commenter': 'absurdfarce'}]"
1422,core/src/main/java/com/datastax/oss/driver/internal/core/os/Native.java,"@@ -34,172 +24,172 @@
 
   private static final Logger LOG = LoggerFactory.getLogger(Native.class);
 
+  private static class ImplLoader {
+
+    public NativeImpl load() {
+      try {
+        return new JnrNativeImpl();
+      } catch (Throwable t) {
+        LOG.info(
+            ""Unable to load JNR native implementation.  This could be normal if JNR is excluded from the classpath"",
+            t);
+        return new EmptyNativeImpl();","[{'comment': 'Manual testing confirmed that (a) I get the error message above and (b) other driver functions appear to work fine (without native support) if JNR is excluded.', 'commenter': 'absurdfarce'}]"
1422,core/src/main/java/com/datastax/oss/driver/internal/core/os/Native.java,"@@ -34,172 +24,172 @@
 
   private static final Logger LOG = LoggerFactory.getLogger(Native.class);
 
+  private static class ImplLoader {
+
+    public NativeImpl load() {
+      try {
+        return new JnrNativeImpl();
+      } catch (Throwable t) {
+        LOG.info(
+            ""Unable to load JNR native implementation.  This could be normal if JNR is excluded from the classpath"",
+            t);
+        return new EmptyNativeImpl();
+      }
+    }
+  }
+
+  /* Copied from equivalent op in jnr.ffi.Platform.  We have to have this here as it has to be defined
+   * before it's (multiple) uses in determineCpu() */
+  private static final Locale LOCALE = java.util.Locale.ENGLISH;
+
+  private static final NativeImpl IMPL = new ImplLoader().load();
+
+  @SuppressWarnings(""VariableNameSameAsType"")
+  private static final CPU CPU = determineCPU();
+
   /** Whether {@link Native#currentTimeMicros()} is available on this system. */
   public static boolean isCurrentTimeMicrosAvailable() {
-    try {
-      return LibCLoader.GET_TIME_OF_DAY_AVAILABLE;
-    } catch (NoClassDefFoundError e) {
-      return false;
-    }
+    return IMPL.gettimeofdayAvailable();
   }
 
   /**
    * The current time in microseconds, as returned by libc.gettimeofday(); can only be used if
    * {@link #isCurrentTimeMicrosAvailable()} is true.
    */
   public static long currentTimeMicros() {
-    if (!isCurrentTimeMicrosAvailable()) {
-      throw new IllegalStateException(
-          ""Native call not available. ""
-              + ""Check isCurrentTimeMicrosAvailable() before calling this method."");
-    }
-    LibCLoader.Timeval tv = new LibCLoader.Timeval(LibCLoader.LIB_C_RUNTIME);
-    int res = LibCLoader.LIB_C.gettimeofday(tv, null);
-    if (res != 0) {
-      throw new IllegalStateException(""Call to libc.gettimeofday() failed with result "" + res);
-    }
-    return tv.tv_sec.get() * 1000000 + tv.tv_usec.get();
+    return IMPL.gettimeofday();
   }
 
   public static boolean isGetProcessIdAvailable() {
-    try {
-      return PosixLoader.GET_PID_AVAILABLE;
-    } catch (NoClassDefFoundError e) {
-      return false;
-    }
+    return IMPL.getpidAvailable();
   }
 
   public static int getProcessId() {
-    if (!isGetProcessIdAvailable()) {
-      throw new IllegalStateException(
-          ""Native call not available. ""
-              + ""Check isGetProcessIdAvailable() before calling this method."");
-    }
-    return PosixLoader.POSIX.getpid();
-  }
-
-  /**
-   * Returns {@code true} if JNR {@link Platform} class is loaded, and {@code false} otherwise.
-   *
-   * @return {@code true} if JNR {@link Platform} class is loaded.
-   */
-  public static boolean isPlatformAvailable() {
-    try {
-      return PlatformLoader.PLATFORM != null;
-    } catch (NoClassDefFoundError e) {
-      return false;
-    }
+    return IMPL.getpid();
   }
 
   /**
-   * Returns the current processor architecture the JVM is running on, as reported by {@link
-   * Platform#getCPU()}.
+   * Returns the current processor architecture the JVM is running on. This value should match up to
+   * what's returned by jnr-ffi's Platform.getCPU() method.
    *
    * @return the current processor architecture.
-   * @throws IllegalStateException if JNR Platform library is not loaded.
    */
   public static String getCPU() {
-    if (!isPlatformAvailable())
-      throw new IllegalStateException(
-          ""JNR Platform class not loaded. ""
-              + ""Check isPlatformAvailable() before calling this method."");
-    return PlatformLoader.PLATFORM.getCPU().toString();
+    return CPU.name().toLowerCase(LOCALE);
   }
 
-  /**
-   * If jnr-ffi is not in the classpath at runtime, we'll fail to initialize the static fields
-   * below, but we still want {@link Native} to initialize successfully, so use an inner class.
+  /* The remainder of this class is largely based on jnr.ffi.Platform in jnr-ffi version 2.1.10.
+   * We copy it manually here in order to avoid introducing an extra dependency merely for the sake of
+   * evaluating some system properties.
+   *
+   * jnr-ffi copyright notice follows:
+   *
+   * Copyright (C) 2008-2010 Wayne Meissner
+   *
+   * This file is part of the JNR project.
+   *
+   * Licensed under the Apache License, Version 2.0 (the ""License"");
+   * you may not use this file except in compliance with the License.
+   * You may obtain a copy of the License at
+   *
+   *    http://www.apache.org/licenses/LICENSE-2.0
+   *
+   * Unless required by applicable law or agreed to in writing, software
+   * distributed under the License is distributed on an ""AS IS"" BASIS,
+   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   * See the License for the specific language governing permissions and
+   * limitations under the License.
    */
-  private static class LibCLoader {
+  /** The supported CPU architectures. */
+  private enum CPU {
+    /*
+     * <b>Note</b> The names of the enum values are used in other parts of the
+     * code to determine where to find the native stub library.  Do NOT rename.
+     */
 
-    /** Handles libc calls through JNR (must be public). */
-    public interface LibC {
-      int gettimeofday(@Out @Transient Timeval tv, Pointer unused);
-    }
+    /** 32 bit legacy Intel */
+    I386,
 
-    // See http://man7.org/linux/man-pages/man2/settimeofday.2.html
-    private static class Timeval extends Struct {
-      private final time_t tv_sec = new time_t();
-      private final Unsigned32 tv_usec = new Unsigned32();
+    /** 64 bit AMD (aka EM64T/X64) */
+    X86_64,
 
-      private Timeval(Runtime runtime) {
-        super(runtime);
-      }
-    }
+    /** 32 bit Power PC */
+    PPC,
 
-    private static final LibC LIB_C;
-    private static final Runtime LIB_C_RUNTIME;
-    private static final boolean GET_TIME_OF_DAY_AVAILABLE;
+    /** 64 bit Power PC */
+    PPC64,
 
-    static {
-      LibC libc;
-      Runtime runtime = null;
-      try {
-        libc = LibraryLoader.create(LibC.class).load(""c"");
-        runtime = Runtime.getRuntime(libc);
-      } catch (Throwable t) {
-        libc = null;
-        LOG.debug(""Error loading libc"", t);
-      }
-      LIB_C = libc;
-      LIB_C_RUNTIME = runtime;
-      boolean getTimeOfDayAvailable = false;
-      if (LIB_C_RUNTIME != null) {
-        try {
-          getTimeOfDayAvailable = LIB_C.gettimeofday(new Timeval(LIB_C_RUNTIME), null) == 0;
-        } catch (Throwable t) {
-          LOG.debug(""Error accessing libc.gettimeofday()"", t);
-        }
-      }
-      GET_TIME_OF_DAY_AVAILABLE = getTimeOfDayAvailable;
-    }
-  }
+    /** 64 bit Power PC little endian */
+    PPC64LE,
 
-  /** @see LibCLoader */
-  private static class PosixLoader {
-    @SuppressWarnings(""VariableNameSameAsType"")
-    private static final jnr.posix.POSIX POSIX;
+    /** 32 bit Sun sparc */
+    SPARC,
 
-    private static final boolean GET_PID_AVAILABLE;
+    /** 64 bit Sun sparc */
+    SPARCV9,
 
-    static {
-      jnr.posix.POSIX posix;
-      try {
-        posix = POSIXFactory.getPOSIX(new DefaultPOSIXHandler(), true);
-      } catch (Throwable t) {
-        posix = null;
-        LOG.debug(""Error loading POSIX"", t);
-      }
-      POSIX = posix;
-      boolean getPidAvailable = false;
-      if (POSIX != null) {
-        try {
-          POSIX.getpid();
-          getPidAvailable = true;
-        } catch (Throwable t) {
-          LOG.debug(""Error accessing posix.getpid()"", t);
-        }
-      }
-      GET_PID_AVAILABLE = getPidAvailable;
-    }
-  }
+    /** IBM zSeries S/390 */
+    S390X,
 
-  private static class PlatformLoader {
+    /** 32 bit MIPS (used by nestedvm) */
+    MIPS32,
 
-    private static final Platform PLATFORM;
+    /** 32 bit ARM */
+    ARM,
 
-    static {
-      Platform platform = null;
-      try {
-        Class<?> platformClass = Reflection.loadClass(null, ""jnr.ffi.Platform"");
-        if (platformClass != null) {
-          Method getNativePlatform = platformClass.getMethod(""getNativePlatform"");
-          platform = (Platform) getNativePlatform.invoke(null);
-        }
-      } catch (Throwable t) {
-        LOG.debug(""Error loading jnr.ffi.Platform class, this class will not be available."", t);
+    /** 64 bit ARM */
+    AARCH64,
+
+    /**
+     * Unknown CPU architecture. A best effort will be made to infer architecture specific values
+     * such as address and long size.
+     */
+    UNKNOWN;
+  }
+
+  private static CPU determineCPU() {","[{'comment': ""This is the way you try to make It work with quarkus, right (when JNR is missing)?\r\nShouldn't we do a similar thing for `gettimeofday`?"", 'commenter': 'tomekl007'}]"
1422,core/src/main/java/com/datastax/oss/driver/internal/core/os/Native.java,"@@ -34,172 +24,172 @@
 
   private static final Logger LOG = LoggerFactory.getLogger(Native.class);
 
+  private static class ImplLoader {
+
+    public NativeImpl load() {
+      try {
+        return new JnrNativeImpl();
+      } catch (Throwable t) {
+        LOG.info(
+            ""Unable to load JNR native implementation.  This could be normal if JNR is excluded from the classpath"",
+            t);
+        return new EmptyNativeImpl();
+      }
+    }
+  }
+
+  /* Copied from equivalent op in jnr.ffi.Platform.  We have to have this here as it has to be defined
+   * before it's (multiple) uses in determineCpu() */
+  private static final Locale LOCALE = java.util.Locale.ENGLISH;
+
+  private static final NativeImpl IMPL = new ImplLoader().load();
+
+  @SuppressWarnings(""VariableNameSameAsType"")
+  private static final CPU CPU = determineCPU();
+
   /** Whether {@link Native#currentTimeMicros()} is available on this system. */
   public static boolean isCurrentTimeMicrosAvailable() {
-    try {
-      return LibCLoader.GET_TIME_OF_DAY_AVAILABLE;
-    } catch (NoClassDefFoundError e) {
-      return false;
-    }
+    return IMPL.gettimeofdayAvailable();
   }
 
   /**
    * The current time in microseconds, as returned by libc.gettimeofday(); can only be used if
    * {@link #isCurrentTimeMicrosAvailable()} is true.
    */
   public static long currentTimeMicros() {
-    if (!isCurrentTimeMicrosAvailable()) {
-      throw new IllegalStateException(
-          ""Native call not available. ""
-              + ""Check isCurrentTimeMicrosAvailable() before calling this method."");
-    }
-    LibCLoader.Timeval tv = new LibCLoader.Timeval(LibCLoader.LIB_C_RUNTIME);
-    int res = LibCLoader.LIB_C.gettimeofday(tv, null);
-    if (res != 0) {
-      throw new IllegalStateException(""Call to libc.gettimeofday() failed with result "" + res);
-    }
-    return tv.tv_sec.get() * 1000000 + tv.tv_usec.get();
+    return IMPL.gettimeofday();
   }
 
   public static boolean isGetProcessIdAvailable() {
-    try {
-      return PosixLoader.GET_PID_AVAILABLE;
-    } catch (NoClassDefFoundError e) {
-      return false;
-    }
+    return IMPL.getpidAvailable();
   }
 
   public static int getProcessId() {
-    if (!isGetProcessIdAvailable()) {
-      throw new IllegalStateException(
-          ""Native call not available. ""
-              + ""Check isGetProcessIdAvailable() before calling this method."");
-    }
-    return PosixLoader.POSIX.getpid();
-  }
-
-  /**
-   * Returns {@code true} if JNR {@link Platform} class is loaded, and {@code false} otherwise.
-   *
-   * @return {@code true} if JNR {@link Platform} class is loaded.
-   */
-  public static boolean isPlatformAvailable() {
-    try {
-      return PlatformLoader.PLATFORM != null;
-    } catch (NoClassDefFoundError e) {
-      return false;
-    }
+    return IMPL.getpid();
   }
 
   /**
-   * Returns the current processor architecture the JVM is running on, as reported by {@link
-   * Platform#getCPU()}.
+   * Returns the current processor architecture the JVM is running on. This value should match up to
+   * what's returned by jnr-ffi's Platform.getCPU() method.
    *
    * @return the current processor architecture.
-   * @throws IllegalStateException if JNR Platform library is not loaded.
    */
   public static String getCPU() {
-    if (!isPlatformAvailable())
-      throw new IllegalStateException(
-          ""JNR Platform class not loaded. ""
-              + ""Check isPlatformAvailable() before calling this method."");
-    return PlatformLoader.PLATFORM.getCPU().toString();
+    return CPU.name().toLowerCase(LOCALE);
   }
 
-  /**
-   * If jnr-ffi is not in the classpath at runtime, we'll fail to initialize the static fields
-   * below, but we still want {@link Native} to initialize successfully, so use an inner class.
+  /* The remainder of this class is largely based on jnr.ffi.Platform in jnr-ffi version 2.1.10.
+   * We copy it manually here in order to avoid introducing an extra dependency merely for the sake of
+   * evaluating some system properties.
+   *
+   * jnr-ffi copyright notice follows:
+   *
+   * Copyright (C) 2008-2010 Wayne Meissner
+   *
+   * This file is part of the JNR project.
+   *
+   * Licensed under the Apache License, Version 2.0 (the ""License"");
+   * you may not use this file except in compliance with the License.
+   * You may obtain a copy of the License at
+   *
+   *    http://www.apache.org/licenses/LICENSE-2.0
+   *
+   * Unless required by applicable law or agreed to in writing, software
+   * distributed under the License is distributed on an ""AS IS"" BASIS,
+   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   * See the License for the specific language governing permissions and
+   * limitations under the License.
    */
-  private static class LibCLoader {
+  /** The supported CPU architectures. */
+  private enum CPU {
+    /*
+     * <b>Note</b> The names of the enum values are used in other parts of the
+     * code to determine where to find the native stub library.  Do NOT rename.
+     */
 
-    /** Handles libc calls through JNR (must be public). */
-    public interface LibC {
-      int gettimeofday(@Out @Transient Timeval tv, Pointer unused);
-    }
+    /** 32 bit legacy Intel */
+    I386,
 
-    // See http://man7.org/linux/man-pages/man2/settimeofday.2.html
-    private static class Timeval extends Struct {
-      private final time_t tv_sec = new time_t();
-      private final Unsigned32 tv_usec = new Unsigned32();
+    /** 64 bit AMD (aka EM64T/X64) */
+    X86_64,
 
-      private Timeval(Runtime runtime) {
-        super(runtime);
-      }
-    }
+    /** 32 bit Power PC */
+    PPC,
 
-    private static final LibC LIB_C;
-    private static final Runtime LIB_C_RUNTIME;
-    private static final boolean GET_TIME_OF_DAY_AVAILABLE;
+    /** 64 bit Power PC */
+    PPC64,
 
-    static {
-      LibC libc;
-      Runtime runtime = null;
-      try {
-        libc = LibraryLoader.create(LibC.class).load(""c"");
-        runtime = Runtime.getRuntime(libc);
-      } catch (Throwable t) {
-        libc = null;
-        LOG.debug(""Error loading libc"", t);
-      }
-      LIB_C = libc;
-      LIB_C_RUNTIME = runtime;
-      boolean getTimeOfDayAvailable = false;
-      if (LIB_C_RUNTIME != null) {
-        try {
-          getTimeOfDayAvailable = LIB_C.gettimeofday(new Timeval(LIB_C_RUNTIME), null) == 0;
-        } catch (Throwable t) {
-          LOG.debug(""Error accessing libc.gettimeofday()"", t);
-        }
-      }
-      GET_TIME_OF_DAY_AVAILABLE = getTimeOfDayAvailable;
-    }
-  }
+    /** 64 bit Power PC little endian */
+    PPC64LE,
 
-  /** @see LibCLoader */
-  private static class PosixLoader {
-    @SuppressWarnings(""VariableNameSameAsType"")
-    private static final jnr.posix.POSIX POSIX;
+    /** 32 bit Sun sparc */
+    SPARC,
 
-    private static final boolean GET_PID_AVAILABLE;
+    /** 64 bit Sun sparc */
+    SPARCV9,
 
-    static {
-      jnr.posix.POSIX posix;
-      try {
-        posix = POSIXFactory.getPOSIX(new DefaultPOSIXHandler(), true);
-      } catch (Throwable t) {
-        posix = null;
-        LOG.debug(""Error loading POSIX"", t);
-      }
-      POSIX = posix;
-      boolean getPidAvailable = false;
-      if (POSIX != null) {
-        try {
-          POSIX.getpid();
-          getPidAvailable = true;
-        } catch (Throwable t) {
-          LOG.debug(""Error accessing posix.getpid()"", t);
-        }
-      }
-      GET_PID_AVAILABLE = getPidAvailable;
-    }
-  }
+    /** IBM zSeries S/390 */
+    S390X,
 
-  private static class PlatformLoader {
+    /** 32 bit MIPS (used by nestedvm) */
+    MIPS32,
 
-    private static final Platform PLATFORM;
+    /** 32 bit ARM */
+    ARM,
 
-    static {
-      Platform platform = null;
-      try {
-        Class<?> platformClass = Reflection.loadClass(null, ""jnr.ffi.Platform"");
-        if (platformClass != null) {
-          Method getNativePlatform = platformClass.getMethod(""getNativePlatform"");
-          platform = (Platform) getNativePlatform.invoke(null);
-        }
-      } catch (Throwable t) {
-        LOG.debug(""Error loading jnr.ffi.Platform class, this class will not be available."", t);
+    /** 64 bit ARM */
+    AARCH64,
+
+    /**
+     * Unknown CPU architecture. A best effort will be made to infer architecture specific values
+     * such as address and long size.
+     */
+    UNKNOWN;
+  }
+
+  private static CPU determineCPU() {
+    String archString = System.getProperty(""os.arch"");","[{'comment': 'maybe we could extract this whole logic and provide a 3rd implementation of `AbstractNativeImpl`?\r\nSo we would have: `jnr`, `unknown` and `graal` implementations?', 'commenter': 'tomekl007'}]"
1422,core/src/main/java/com/datastax/oss/driver/internal/core/os/AbstractNativeImpl.java,"@@ -0,0 +1,35 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.os;
+
+import java.util.function.Supplier;
+
+public abstract class AbstractNativeImpl implements NativeImpl {
+
+  Supplier<IllegalStateException> gettimeofdaySupplier =","[{'comment': ""1. These suppliers are only used in one of the child classes, maybe move them there?\r\n2. Storing functional interfaces as lambdas is precisely one of the new error-prone rules, I wonder if these won't generate a compile error?\r\n3. The error messages mention `isCurrentTimeMicrosAvailable` and `isGetProcessIdAvailable` but these methods are not declared in this class hierarchy.\r\n4. If the errors go away I think this class could be removed altogether."", 'commenter': 'adutra'}]"
1422,core/src/main/java/com/datastax/oss/driver/internal/core/os/JnrNativeImpl.java,"@@ -0,0 +1,103 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.os;
+
+import java.util.Optional;
+import jnr.posix.POSIX;
+import jnr.posix.POSIXFactory;
+import jnr.posix.Timeval;
+import jnr.posix.util.DefaultPOSIXHandler;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class JnrNativeImpl extends AbstractNativeImpl {
+
+  private static final Logger LOG = LoggerFactory.getLogger(JnrNativeImpl.class);
+
+  private final Optional<POSIX> posix;","[{'comment': 'I observe that you strived to give a functional look to this class, but I\'m not convinced the result looks better than plain old Java.\r\n\r\n1. Storing `Optional` as a field: my IDE complains, and since `Optional` is not serializable, I would rather avoid this situation, thinking that projects like Spark would love to see the entire driver serializable :-) \r\n2. The functional style is probably a matter of taste, but in the present case we must admit it\'s more verbose than imperative style, e.g. (also applying my suggestion to change `NativeImpl` API):\r\n\r\n```\r\n  @Nullable private final POSIX posix;\r\n\r\n  public JnrNativeImpl() {\r\n    this.posix = loadPosix();\r\n  }\r\n\r\n  private static POSIX loadPosix() {\r\n    POSIX posix;\r\n    try {\r\n      posix = POSIXFactory.getPOSIX(new DefaultPOSIXHandler(), true);\r\n    } catch (Throwable t) {\r\n      LOG.debug(""Error loading POSIX"", t);\r\n      return null;\r\n    }\r\n    try {\r\n      posix.getpid();\r\n    } catch (Throwable t) {\r\n      LOG.debug(""Error calling getpid()"", t);\r\n      return null;\r\n    }\r\n    try {\r\n      gettimeofdayInternal(posix);\r\n    } catch (Throwable t) {\r\n      LOG.debug(""Error calling gettimeofday()"", t);\r\n      return null;\r\n    }\r\n    return posix;\r\n  }\r\n\r\n  @Override\r\n  public OptionalLong gettimeofday() {\r\n    if (posix == null) {\r\n      return OptionalLong.empty();\r\n    }\r\n    return OptionalLong.of(gettimeofdayInternal(posix));\r\n  }\r\n\r\n  private static long gettimeofdayInternal(@NonNull POSIX posix) {\r\n    Timeval tv = posix.allocateTimeval();\r\n    int rv = posix.gettimeofday(tv);\r\n    if (rv != 0) {\r\n      throw new IllegalStateException(\r\n          ""Expected 0 return value from gettimeofday(), observed "" + rv);\r\n    }\r\n    return tv.sec() * 1_000_000 + tv.usec();\r\n  }\r\n\r\n  @Override\r\n  public OptionalInt getpid() {\r\n    if (posix == null) {\r\n      return OptionalInt.empty();\r\n    }\r\n    return OptionalInt.of(posix.getpid());\r\n  }\r\n```', 'commenter': 'adutra'}]"
1422,core/src/main/java/com/datastax/oss/driver/internal/core/os/Native.java,"@@ -34,172 +24,172 @@
 
   private static final Logger LOG = LoggerFactory.getLogger(Native.class);
 
+  private static class ImplLoader {
+
+    public NativeImpl load() {
+      try {
+        return new JnrNativeImpl();
+      } catch (Throwable t) {
+        LOG.info(
+            ""Unable to load JNR native implementation.  This could be normal if JNR is excluded from the classpath"",
+            t);
+        return new EmptyNativeImpl();
+      }
+    }
+  }
+
+  /* Copied from equivalent op in jnr.ffi.Platform.  We have to have this here as it has to be defined
+   * before it's (multiple) uses in determineCpu() */","[{'comment': 'Nit: ""before its (multiple)""', 'commenter': 'adutra'}]"
1422,core/src/main/java/com/datastax/oss/driver/internal/core/os/Native.java,"@@ -34,172 +24,172 @@
 
   private static final Logger LOG = LoggerFactory.getLogger(Native.class);
 
+  private static class ImplLoader {
+
+    public NativeImpl load() {
+      try {
+        return new JnrNativeImpl();
+      } catch (Throwable t) {
+        LOG.info(
+            ""Unable to load JNR native implementation.  This could be normal if JNR is excluded from the classpath"",
+            t);
+        return new EmptyNativeImpl();
+      }
+    }
+  }
+
+  /* Copied from equivalent op in jnr.ffi.Platform.  We have to have this here as it has to be defined
+   * before it's (multiple) uses in determineCpu() */
+  private static final Locale LOCALE = java.util.Locale.ENGLISH;","[{'comment': 'Nit: Can we use regular imports or is there a requirement to use FQCNs here?', 'commenter': 'adutra'}]"
1422,core/src/main/java/com/datastax/oss/driver/internal/core/os/Native.java,"@@ -34,172 +24,172 @@
 
   private static final Logger LOG = LoggerFactory.getLogger(Native.class);
 
+  private static class ImplLoader {
+
+    public NativeImpl load() {
+      try {
+        return new JnrNativeImpl();
+      } catch (Throwable t) {
+        LOG.info(
+            ""Unable to load JNR native implementation.  This could be normal if JNR is excluded from the classpath"",","[{'comment': 'Nit: two spaces after the dot.', 'commenter': 'adutra'}]"
1425,core/src/main/java/com/datastax/oss/driver/internal/core/os/NativeImpl.java,"@@ -15,13 +15,15 @@
  */
 package com.datastax.oss.driver.internal.core.os;
 
-public interface NativeImpl {
+import java.util.Optional;
 
-  public boolean gettimeofdayAvailable();
+public interface NativeImpl {
 
-  public long gettimeofday();
+  /* Maintained to allow Native.isXAvailable() functionality without trying to make a native call if
+   * the underlying support _is_ available. */
+  public boolean available();
 
-  public boolean getpidAvailable();
+  public Optional<Long> gettimeofday();","[{'comment': 'Why not `OptionalLong` and `OptionalInt`?', 'commenter': 'adutra'}, {'comment': ""Because OptionalLong and OptionalInt do not subclass Optional; they are their own type hierarchy.  As a result the map/flatMap ops off Optional<POSIX> can't generate these types."", 'commenter': 'absurdfarce'}, {'comment': 'Oh you are flatmapping them down the line. Ok then.', 'commenter': 'adutra'}]"
1425,core/src/main/java/com/datastax/oss/driver/internal/core/os/JnrNativeImpl.java,"@@ -34,70 +35,54 @@ public JnrNativeImpl() {
     this.posix = loadPosix();
   }
 
+  @Override
+  public Optional<Long> gettimeofday() {
+
+    return this.posix.flatMap(this::gettimeofdayImpl);
+  }
+
+  @Override
+  public Optional<Integer> getpid() {
+
+    return this.posix.map(POSIX::getpid);
+  }
+
+  @Override
+  public boolean available() {
+    return this.posix.isPresent();
+  }
+
   private Optional<POSIX> loadPosix() {
 
     try {
       return Optional.of(POSIXFactory.getPOSIX(new DefaultPOSIXHandler(), true))
-          .flatMap(this::validatePosix);
+          .flatMap(p -> catchAll(p, posix -> posix.getpid(), ""Error calling getpid()""))
+          .flatMap(p -> catchAll(p, this::gettimeofdayImpl, ""Error calling gettimeofday()""));
     } catch (Throwable t) {
       LOG.debug(""Error loading POSIX"", t);
       return Optional.empty();
     }
   }
 
-  private Optional<POSIX> validatePosix(POSIX posix) {
-
+  private Optional<POSIX> catchAll(POSIX posix, Consumer<POSIX> fn, String debugStr) {
     try {
-
-      posix.getpid();
+      fn.accept(posix);
+      return Optional.of(posix);
     } catch (Throwable t) {
 
-      LOG.debug(""Error calling getpid()"", t);
+      LOG.debug(debugStr, t);
       return Optional.empty();
     }
-
-    try {
-
-      Timeval tv = posix.allocateTimeval();
-      int rv = posix.gettimeofday(tv);
-      if (rv != 0) {
-
-        LOG.debug(""Expected getitimeofday() to return zero, observed {}"", rv);
-        return Optional.empty();
-      }
-    } catch (Throwable t) {
-
-      LOG.debug(""Error calling gettimeofday()"", t);
-      return Optional.empty();
-    }
-
-    return Optional.of(posix);
-  }
-
-  @Override
-  public boolean gettimeofdayAvailable() {
-    return posix.isPresent();
   }
 
-  @Override
-  public long gettimeofday() {
+  private Optional<Long> gettimeofdayImpl(POSIX posix) {
 
-    Timeval tv = this.posix.map(POSIX::allocateTimeval).orElseThrow(gettimeofdaySupplier);
-    int rv = this.posix.map(p -> p.gettimeofday(tv)).orElseThrow(gettimeofdaySupplier);
+    Timeval tv = posix.allocateTimeval();
+    int rv = posix.gettimeofday(tv);
     if (rv != 0) {
-      throw new IllegalStateException(
-          ""Expected 0 return value from gettimeofday(), observed "" + rv);
+      LOG.info(""Expected 0 return value from gettimeofday(), observed "" + rv);","[{'comment': '`Log.debug` maybe?', 'commenter': 'adutra'}, {'comment': 'Agreed, will change', 'commenter': 'absurdfarce'}]"
1430,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoMethodGenerator.java,"@@ -146,13 +146,6 @@ protected void populateBuilderWithFunction(
     }
   }
 
-  protected void populateBuilderWithProfile(CodeBlock.Builder builder) {
-    builder.beginControlFlow(""if(context.getExecutionProfileName() != null)"");
-    builder.addStatement(
-        ""boundStatementBuilder = boundStatementBuilder.setExecutionProfileName(context.getExecutionProfileName())"");
-    builder.endControlFlow();
-  }
-","[{'comment': ""One more thing I hadn't caught previously: it's easier to do this in `DaoBase`."", 'commenter': 'olim7t'}]"
1430,mapper-runtime/src/main/java/com/datastax/oss/driver/internal/mapper/DaoBase.java,"@@ -60,6 +60,11 @@
 
   protected static CompletionStage<PreparedStatement> prepare(
       SimpleStatement statement, MapperContext context) {
+    if (context.getExecutionProfileName() != null) {
+      statement = statement.setExecutionProfileName(context.getExecutionProfileName());
+    } else if (context.getExecutionProfile() != null) {
+      statement = statement.setExecutionProfile(context.getExecutionProfile());
+    }
     return context.getSession().prepareAsync(statement);","[{'comment': 'Do we prevent users from setting both `profileName` and profile (`class`)?\r\nIf no, did we document the fact that `getExecutionProfileName` has priority over `getExecutionProfile`?', 'commenter': 'tomekl007'}, {'comment': ""It's enforced implicitly by the mapper processor: only one `@DaoProfile`-annotated parameter is allowed per DAO factory method.\r\n\r\nI'll add a comment on the context methods to explain that."", 'commenter': 'olim7t'}, {'comment': ""Now that I think of it, we could also allow injection of the profile via `MapperBuilder`, like we do with the default keyspace. I'll add that. "", 'commenter': 'olim7t'}]"
1430,mapper-runtime/revapi.json,"@@ -60,8 +60,13 @@
       {
         ""code"": ""java.method.addedToInterface"",
         ""new"": ""method java.lang.String com.datastax.oss.driver.api.mapper.MapperContext::getExecutionProfileName()"",
-        ""justification"": ""JAVA-2633 Adding execution profile to mapper""
+        ""justification"": ""JAVA-2633: Add execution profile argument to DAO factory method (accept API break -- it's unlikely that MapperContext will be implemented outside of the driver)""","[{'comment': ""Can't we use `customState` to propagate this? We would not need to break backward compatibility in such a case"", 'commenter': 'tomekl007'}, {'comment': ""Already discussed [on the original PR](https://github.com/datastax/java-driver/pull/1409#discussion_r389558576).\r\n\r\nTo expand: `MapperContext` can't be implemented by the user in any useful way (they can't inject their implementation in mapper-generated code), so we deemed it acceptable to break the API."", 'commenter': 'olim7t'}]"
1430,mapper-runtime/src/main/java/com/datastax/oss/driver/api/mapper/annotations/DaoProfile.java,"@@ -26,18 +27,16 @@
  *
  * <p>Example:
  *
- * <p>*
- *
  * <pre>
  *  * &#64;Mapper
  *  * public interface InventoryMapper {
  *  *   ProductDao productDao(@DaoTable String executionProfile);","[{'comment': ""Shouldn't this be `ProductDao productDao(@DaoProfile String executionProfile);`?"", 'commenter': 'adutra'}, {'comment': 'ðŸ‘ Good catch, will fix', 'commenter': 'olim7t'}]"
1430,integration-tests/src/test/java/com/datastax/oss/driver/mapper/ProfileIT.java,"@@ -35,37 +36,55 @@
 import com.datastax.oss.driver.api.mapper.annotations.PartitionKey;
 import com.datastax.oss.driver.api.mapper.annotations.Query;
 import com.datastax.oss.driver.api.mapper.annotations.Select;
+import com.datastax.oss.driver.api.mapper.annotations.StatementAttributes;
 import com.datastax.oss.driver.api.mapper.annotations.Update;
+import com.datastax.oss.driver.api.testinfra.session.SessionRule;
 import com.datastax.oss.driver.api.testinfra.session.SessionUtils;
 import com.datastax.oss.driver.api.testinfra.simulacron.SimulacronRule;
 import com.datastax.oss.driver.categories.ParallelizableTests;
 import com.datastax.oss.protocol.internal.Message;
 import com.datastax.oss.protocol.internal.request.Execute;
-import com.datastax.oss.simulacron.common.cluster.ClusterQueryLogReport;
 import com.datastax.oss.simulacron.common.cluster.ClusterSpec;
 import com.datastax.oss.simulacron.common.cluster.QueryLog;
 import com.datastax.oss.simulacron.common.stubbing.PrimeDsl;
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.Lists;
+import java.util.List;
 import java.util.Map;
 import java.util.Objects;
 import java.util.UUID;
 import java.util.concurrent.TimeUnit;
+import java.util.function.UnaryOperator;
 import org.junit.Before;
 import org.junit.BeforeClass;
 import org.junit.ClassRule;
 import org.junit.Test;
 import org.junit.experimental.categories.Category;
+import org.junit.rules.RuleChain;
+import org.junit.rules.TestRule;
 
 @Category(ParallelizableTests.class)
 public class ProfileIT {","[{'comment': ""I've completely rearranged this test to cover all combinations of mapper-level, dao-level, and method-level (via the existing statement attributes mechanism)."", 'commenter': 'olim7t'}]"
1430,mapper-runtime/src/main/java/com/datastax/oss/driver/internal/mapper/DefaultMapperContext.java,"@@ -70,11 +56,6 @@ public DefaultMapperContext(
         NullAllowingImmutableMap.copyOf(customState));
   }
 
-  public DefaultMapperContext(
-      @NonNull CqlSession session, @NonNull Map<Object, Object> customState) {
-    this(session, null, customState);
-  }
-","[{'comment': ""This constructor wasn't used anymore, no need to keep it."", 'commenter': 'olim7t'}]"
1430,core/src/main/java/com/datastax/oss/driver/api/core/cql/StatementBuilder.java,"@@ -91,6 +91,7 @@ protected StatementBuilder(StatementT template) {
   @NonNull
   public SelfT setExecutionProfileName(@Nullable String executionProfileName) {
     this.executionProfileName = executionProfileName;
+    this.executionProfile = null;
     return self;
   }","[{'comment': 'This solved a bug when combining DAO-level profile and `@StatementAttributes`, but I think it also makes sense for general usage of statement builders.', 'commenter': 'olim7t'}]"
1431,core/src/main/java/com/datastax/dse/driver/internal/core/graph/ContinuousGraphRequestHandler.java,"@@ -99,7 +99,7 @@ protected Duration getGlobalTimeout() {
   @NonNull
   @Override
   protected Duration getPageTimeout(int pageNumber) {
-    return Duration.ZERO;
+    return this.globalTimeout;","[{'comment': ""I have concerns about the full effect of this change. Currently, a value of 0 means that the driver will wait forever for a response to successive page requests (the first page timeout is controlled by the globalTimeout).\r\n\r\nI changed the return value here to be the globalTimeout value as the handler doesn't have a separate page timeouts (like [ContinuousCqlRequestHandler](https://github.com/datastax/java-driver/blob/4.x/core/src/main/java/com/datastax/dse/driver/internal/core/cql/continuous/ContinuousCqlRequestHandler.java#L51-L52)).\r\n\r\nThe test that fails is [here](https://github.com/datastax/java-driver/pull/1431/files#diff-c96ea9340f349d35ca1642eb4f50c76aR490), as it expects the second page request to timeout while CCM has been paused. But with this method returning a Duration of 0, it never times out the request. Instead, we get a Heartbeat timeout 30 seconds later. This also generally causes many of the DatatProvider tests in that class to fail as more queries are attempted before the session has been fully re-established after CMM is resumed, which causes the session to die with an AllNodesFailed exception.\r\n\r\nI'm not sure if this is the best solution, as maybe GraphPaging needs to support setting a timeout for `otherPages` like the ContiuousCqlRequestHandler does now."", 'commenter': 'emerkle826'}]"
1431,integration-tests/src/test/java/com/datastax/dse/driver/api/core/graph/GraphPagingIT.java,"@@ -69,6 +69,9 @@
                   .withStringList(
                       DefaultDriverOption.METRICS_NODE_ENABLED,
                       Collections.singletonList(DseNodeMetric.GRAPH_MESSAGES.getPath()))
+                  // Increase heartbeat interval to avoid interferences since we pause/stop the
+                  // server in this test
+                  .withDuration(DefaultDriverOption.HEARTBEAT_INTERVAL, Duration.ofMinutes(10))","[{'comment': ""If we remove the invalid test below, I don't think we need to increase the heartbeat here. The rest of the tests in this class pass fine without even coming close to tripping the 30 second heartbeat timeout."", 'commenter': 'emerkle826'}]"
1431,integration-tests/src/test/java/com/datastax/dse/driver/api/core/graph/GraphPagingIT.java,"@@ -461,38 +464,6 @@ public void should_trigger_global_timeout_async() throws InterruptedException {
     }
   }
 
-  @Test
-  public void should_trigger_global_timeout_async_after_first_page() throws InterruptedException {","[{'comment': 'Why do you want to remove this test? It exercises the global timeout for async paging.', 'commenter': 'tomekl007'}, {'comment': 'Discussed in Slack: TLDR is this test is too broken to be kept.', 'commenter': 'adutra'}]"
1431,core/src/test/java/com/datastax/oss/driver/internal/core/metadata/MetadataManagerTest.java,"@@ -308,7 +308,7 @@ private void waitForPendingAdminTasks() {
     // This works because the event loop group is single-threaded
     Future<?> f = adminEventLoopGroup.schedule(() -> null, 5, TimeUnit.NANOSECONDS);
     try {
-      Uninterruptibles.getUninterruptibly(f, 100, TimeUnit.MILLISECONDS);
+      Uninterruptibles.getUninterruptibly(f, 500, TimeUnit.MILLISECONDS);","[{'comment': 'Are we waiting for some specific event to occur? If so, would it be possible to use Awaitility to wait on that condition?', 'commenter': 'adutra'}, {'comment': ""I'm not sure,but I'll investigate further. With the timeout set to 100, I couldn't reproduce the failure locally, and it failed sporadically on Jenkins/CloudBees.\r\nI can lower the value and reproduce it and figure out what I need to be waiting on."", 'commenter': 'emerkle826'}]"
1431,integration-tests/src/test/java/com/datastax/oss/driver/osgi/support/BundleOptions.java,"@@ -167,6 +168,14 @@ public static CompositeOption simulacronBundles() {
                 simulacronVersion));
   }
 
+  public static CompositeOption awaitilityBundles() {","[{'comment': 'Note to myself: I need to port this to JAVA-2658.', 'commenter': 'adutra'}, {'comment': 'Done.', 'commenter': 'adutra'}]"
1431,core/src/test/java/com/datastax/oss/driver/internal/core/metadata/MetadataManagerTest.java,"@@ -290,6 +287,8 @@ public void should_remove_node() {
   private static class TestMetadataManager extends MetadataManager {
 
     private List<MetadataRefresh> refreshes = new CopyOnWriteArrayList<>();
+    private int addNodeCount = 0;","[{'comment': 'These 2 fields probably should be volatile.', 'commenter': 'adutra'}, {'comment': ""Good catch. I did try to verify that the tests run in parallel don't trample each other here, and it seemed like they didn't, but `volatile` here makes sense."", 'commenter': 'emerkle826'}]"
1431,core/src/test/java/com/datastax/oss/driver/internal/core/metadata/MetadataManagerTest.java,"@@ -301,18 +300,24 @@ Void apply(MetadataRefresh refresh) {
       refreshes.add(refresh);
       return null;
     }
+
+    @Override
+    public void addNode(InetSocketAddress broadcastRpcAddress) {
+      // Keep track of addNode calls for condition checking
+      ++addNodeCount;
+      super.addNode(broadcastRpcAddress);
+    }
+
+    @Override
+    public void removeNode(InetSocketAddress broadcastRpcAddress) {
+      // Keep track of removeNode calls for condition checking
+      ++removeNodeCount;
+      super.removeNode(broadcastRpcAddress);
+    }
   }
 
   // Wait for all the tasks on the pool's admin executor to complete.
-  private void waitForPendingAdminTasks() {
-    // This works because the event loop group is single-threaded
-    Future<?> f = adminEventLoopGroup.schedule(() -> null, 5, TimeUnit.NANOSECONDS);
-    try {
-      Uninterruptibles.getUninterruptibly(f, 100, TimeUnit.MILLISECONDS);
-    } catch (ExecutionException e) {
-      fail(""unexpected error"", e.getCause());
-    } catch (TimeoutException e) {
-      fail(""timed out while waiting for admin tasks to complete"", e);
-    }
+  private void waitForPendingAdminTasks(Callable<Boolean> condition) {
+    await().atMost(500, TimeUnit.MILLISECONDS).until(condition);","[{'comment': 'Nice rework! ðŸ‘ ', 'commenter': 'adutra'}]"
1433,integration-tests/src/test/java/com/datastax/oss/driver/api/core/metadata/SerializableMetadataIT.java,"@@ -0,0 +1,105 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core.metadata;
+
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertTrue;
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.metadata.schema.KeyspaceMetadata;
+import com.datastax.oss.driver.api.core.metadata.schema.TableMetadata;
+import com.datastax.oss.driver.api.testinfra.ccm.CcmRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionRule;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import com.datastax.oss.driver.internal.core.metadata.schema.DefaultKeyspaceMetadata;
+import com.datastax.oss.driver.internal.core.metadata.schema.DefaultTableMetadata;
+import com.google.common.collect.ImmutableList;
+import java.util.List;
+import java.util.Optional;
+import org.apache.commons.lang3.SerializationUtils;
+import org.junit.Before;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.junit.rules.RuleChain;
+import org.junit.rules.TestRule;
+
+@Category(ParallelizableTests.class)
+public class SerializableMetadataIT {
+
+  private static final CcmRule ccmRule = CcmRule.getInstance();
+
+  private static final SessionRule<CqlSession> sessionRule = SessionRule.builder(ccmRule).build();
+
+  @ClassRule public static final TestRule chain = RuleChain.outerRule(ccmRule).around(sessionRule);
+
+  @Before
+  public void setupKeyspace() {
+    List<String> statements =
+        ImmutableList.of(
+            ""CREATE KEYSPACE IF NOT EXISTS numbers WITH REPLICATION = { 'class' : 'NetworkTopologyStrategy', 'datacenter1' : 1 }"",
+            ""CREATE TABLE numbers.all (val int, doub int, str text, primary key (val,doub)) WITH CLUSTERING ORDER BY (doub DESC)"",
+            ""CREATE TYPE numbers.numbers_data (doub int, str text)"",
+            ""CREATE TABLE numbers.smaller (val int primary key, data numbers_data)"",
+            ""CREATE INDEX doub_idx on numbers.all (doub)"",
+            ""CREATE OR REPLACE FUNCTION numbers.avgState ( state tuple<int,bigint>, val int ) CALLED ON NULL INPUT RETURNS tuple<int,bigint> LANGUAGE java AS""
+                + "" 'if (val !=null) { state.setInt(0, state.getInt(0)+1); state.setLong(1, state.getLong(1)+val.intValue()); } return state;'"",
+            ""CREATE OR REPLACE FUNCTION numbers.avgFinal ( state tuple<int,bigint> ) CALLED ON NULL INPUT RETURNS double LANGUAGE java AS""
+                + "" 'double r = 0; if (state.getInt(0) == 0) return null; r = state.getLong(1); r/= state.getInt(0); return Double.valueOf(r);'"",
+            ""CREATE AGGREGATE IF NOT EXISTS numbers.average ( int ) SFUNC avgState STYPE tuple<int,bigint> FINALFUNC avgFinal INITCOND (0,0)"",
+            ""INSERT INTO numbers.all (val,doub,str) VALUES (1,2,'one')"",
+            ""INSERT INTO numbers.all (val,doub,str) VALUES (2,4,'two')"",
+            ""INSERT INTO numbers.all (val,doub,str) VALUES (3,6,'three')"",
+            ""CREATE MATERIALIZED VIEW IF NOT EXISTS numbers.big AS SELECT FROM numbers.all""
+                + "" WHERE val is not null and doub is not null and val > 2""
+                + "" PRIMARY KEY (val,doub)"");
+
+    statements.stream().forEach((stmt) -> sessionRule.session().execute(stmt));
+  }
+
+  @Test
+  public void should_contain_serializable_metadata() {
+
+    Optional<KeyspaceMetadata> ksOption =
+        sessionRule.session().getMetadata().getKeyspace(""numbers"");
+    assertTrue(ksOption.isPresent());
+    KeyspaceMetadata ks = ksOption.get();
+    assertTrue(ks instanceof DefaultKeyspaceMetadata);
+
+    /* Validate that the keyspace metadata is fully populated */
+    assertFalse(ks.getUserDefinedTypes().isEmpty());
+    assertFalse(ks.getTables().isEmpty());
+    assertFalse(ks.getViews().isEmpty());
+    assertFalse(ks.getFunctions().isEmpty());
+    assertFalse(ks.getAggregates().isEmpty());
+
+    Optional<TableMetadata> tableOption = ks.getTable(""all"");
+    assertTrue(tableOption.isPresent());
+    TableMetadata table = tableOption.get();
+    assertTrue(table instanceof DefaultTableMetadata);
+
+    /* Validate that the table metadata is fully populated */
+    assertFalse(table.getPartitionKey().isEmpty());
+    assertFalse(table.getClusteringColumns().isEmpty());
+    assertFalse(table.getColumns().isEmpty());
+    // TODO: What kind of objects can be set for options here?  How do we guarantee they're
+    // serializable?","[{'comment': ""Another odd case.  Options are presently defined as Objects and at the moment it looks like all of them are serializable (based on the contents of TableParser).  But this is hardly a guarantee going forward.  It'd be nice if we could change RelationMetadata.getOptions() to return a Map<CqlIdentifier, Serializable> but I can already hear @olim7t pointing out that that would be an API change (and with good reason).\r\n\r\nPresumably we can just do the same thing here we did with the interfaces... instances of these interfaces are serializable as long as the concrete type is itself serializable.  In this case we'd have something in the docs saying the default table metadata impl is serializable as long as the options used are (which should be the common case).  Does that seem adequate?"", 'commenter': 'absurdfarce'}, {'comment': ""> I can already hear @olim7t pointing out that that would be an API change\r\n\r\nI've thought it so loud that you heard me ðŸ˜‰ \r\n\r\n+1 to add a note in the javadocs.\r\n\r\nNote that this should remain mostly the responsibility of the driver; it's technically possible to plug custom schema logic that parses custom table options, but I don't think it's very likely to happen. So it will be up to us to keep that in mind and test it correctly if new option types get added."", 'commenter': 'olim7t'}, {'comment': ""> I've thought it so loud that you heard me :wink: \r\n\r\nI mean, there's also the fact that I agree with you... the two work together nicely :)"", 'commenter': 'absurdfarce'}]"
1433,integration-tests/src/test/java/com/datastax/oss/driver/api/core/metadata/SerializableMetadataIT.java,"@@ -0,0 +1,105 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core.metadata;
+
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertTrue;
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.metadata.schema.KeyspaceMetadata;
+import com.datastax.oss.driver.api.core.metadata.schema.TableMetadata;
+import com.datastax.oss.driver.api.testinfra.ccm.CcmRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionRule;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import com.datastax.oss.driver.internal.core.metadata.schema.DefaultKeyspaceMetadata;
+import com.datastax.oss.driver.internal.core.metadata.schema.DefaultTableMetadata;
+import com.google.common.collect.ImmutableList;
+import java.util.List;
+import java.util.Optional;
+import org.apache.commons.lang3.SerializationUtils;
+import org.junit.Before;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.junit.rules.RuleChain;
+import org.junit.rules.TestRule;
+
+@Category(ParallelizableTests.class)
+public class SerializableMetadataIT {
+
+  private static final CcmRule ccmRule = CcmRule.getInstance();
+
+  private static final SessionRule<CqlSession> sessionRule = SessionRule.builder(ccmRule).build();
+
+  @ClassRule public static final TestRule chain = RuleChain.outerRule(ccmRule).around(sessionRule);
+
+  @Before
+  public void setupKeyspace() {
+    List<String> statements =
+        ImmutableList.of(
+            ""CREATE KEYSPACE IF NOT EXISTS numbers WITH REPLICATION = { 'class' : 'NetworkTopologyStrategy', 'datacenter1' : 1 }"",
+            ""CREATE TABLE numbers.all (val int, doub int, str text, primary key (val,doub)) WITH CLUSTERING ORDER BY (doub DESC)"",
+            ""CREATE TYPE numbers.numbers_data (doub int, str text)"",
+            ""CREATE TABLE numbers.smaller (val int primary key, data numbers_data)"",
+            ""CREATE INDEX doub_idx on numbers.all (doub)"",
+            ""CREATE OR REPLACE FUNCTION numbers.avgState ( state tuple<int,bigint>, val int ) CALLED ON NULL INPUT RETURNS tuple<int,bigint> LANGUAGE java AS""
+                + "" 'if (val !=null) { state.setInt(0, state.getInt(0)+1); state.setLong(1, state.getLong(1)+val.intValue()); } return state;'"",
+            ""CREATE OR REPLACE FUNCTION numbers.avgFinal ( state tuple<int,bigint> ) CALLED ON NULL INPUT RETURNS double LANGUAGE java AS""
+                + "" 'double r = 0; if (state.getInt(0) == 0) return null; r = state.getLong(1); r/= state.getInt(0); return Double.valueOf(r);'"",
+            ""CREATE AGGREGATE IF NOT EXISTS numbers.average ( int ) SFUNC avgState STYPE tuple<int,bigint> FINALFUNC avgFinal INITCOND (0,0)"",
+            ""INSERT INTO numbers.all (val,doub,str) VALUES (1,2,'one')"",
+            ""INSERT INTO numbers.all (val,doub,str) VALUES (2,4,'two')"",
+            ""INSERT INTO numbers.all (val,doub,str) VALUES (3,6,'three')"",
+            ""CREATE MATERIALIZED VIEW IF NOT EXISTS numbers.big AS SELECT FROM numbers.all""
+                + "" WHERE val is not null and doub is not null and val > 2""
+                + "" PRIMARY KEY (val,doub)"");
+
+    statements.stream().forEach((stmt) -> sessionRule.session().execute(stmt));
+  }
+
+  @Test
+  public void should_contain_serializable_metadata() {
+
+    Optional<KeyspaceMetadata> ksOption =
+        sessionRule.session().getMetadata().getKeyspace(""numbers"");
+    assertTrue(ksOption.isPresent());
+    KeyspaceMetadata ks = ksOption.get();
+    assertTrue(ks instanceof DefaultKeyspaceMetadata);
+
+    /* Validate that the keyspace metadata is fully populated */","[{'comment': '""fully populated"" here means ""all the maps in DefaultKeyspaceMetadata have data in them"".  This definition is based on the impl of the class and therefore isn\'t super-flexible.  I\'d love to have a definition based on the class (i.e. if the concrete instance had some kind of isFullyPopulated() method or something) but doing so seemed to lead down some unpleasant avenues... so I decided to leave it at this for now.\r\n\r\nI\'m certainly open to better suggestions here.', 'commenter': 'absurdfarce'}, {'comment': 'Would that notion of ""fully populated"" be of any use outside of tests though?\r\n\r\nIf this is more about how deep we should go with the checks, see my suggestion in another comment to merge this with `DescribeIT`.', 'commenter': 'olim7t'}, {'comment': 'No, the idea of ""fully populated"" doesn\'t really have any meaning outside of tests.  That\'s part of the reason I\'m not lobbying for it... it feels like a @VisibleForTesting kind of thing of limited utility\r\n\r\nI\'ll look into this more as I look to migrating this stuff to DescribeIT', 'commenter': 'absurdfarce'}]"
1433,core/src/main/java/com/datastax/oss/driver/internal/core/metadata/schema/DefaultAggregateMetadata.java,"@@ -40,7 +41,7 @@
   @NonNull private final DataType returnType;
   @NonNull private final FunctionSignature stateFuncSignature;
   @NonNull private final DataType stateType;
-  @NonNull private final TypeCodec<Object> stateTypeCodec;
+  @NonNull private final transient TypeCodec<Object> stateTypeCodec;","[{'comment': ""Perhaps the most significant change in an otherwise straightforward PR.  It didn't seem reasonable to make the codec serializable so I went with this approach right now.  The IT still passes even though the codec won't be populated after deserialization becuase stateTypeCodec isn't included in the equals() impl.\r\n\r\nMy working assumption was that this change wouldn't be terribly significant in practice.  A missing codec here will cause AggregateMetadata.describe() will misbehave on deserialized instances.  But the presumed motivation for making these classes serializable is (at least largely) to use them directly as DTOs... and their performance in that role should still be intact here.\r\n\r\nPerhaps another not worth documenting... but not much more?"", 'commenter': 'absurdfarce'}, {'comment': 'Correction: AggregateMetadata.describe() will be fine in this case.  It will make an attempt to use the codec but if that results in an exception (which it will in this case by way of an NPE) it falls back to toString() instead.  Still seems worthwhile adding an explicit test for this functionality as a guard against future breakage.', 'commenter': 'absurdfarce'}]"
1433,core/src/main/java/com/datastax/oss/driver/internal/core/metadata/schema/DefaultAggregateMetadata.java,"@@ -109,6 +110,13 @@ public DataType getStateType() {
     if (initCond == null) {
       return Optional.empty();
     }
+    if (stateTypeCodec == null) {
+      LOG.debug(
+          String.format(
+              ""Null codec for INITCOND for %s.%s, using toString instead.  This often indicates use of a deserialized AggregateMetadata instance."",
+              keyspace.asInternal(), signature.getName().asInternal()));
+      return Optional.of(initCond.toString());
+    }","[{'comment': 'This is not strictly necessary; the try/catch block below will catch the NPE in cases where stateTypeCodec is null from deserialization.  I added the explicit case here only to avoid adding messages at WARN to the logging infrastructure... DEBUG seemed like a more reasonable level for this (admittedly probably rare in practice) case.', 'commenter': 'absurdfarce'}, {'comment': ""Maybe we should pre-compute the result of `formatInitCond()` in the constructor, and store that as a field instead of the codec.\r\nThe original intent in storing the codec was to avoid formatting eagerly if it's never going to be used, but I'll take that over a deserialized object that doesn't describe correctly."", 'commenter': 'olim7t'}, {'comment': ""Yeah, I think I agree.  I mean the describe() op does have a fallback to toString() if the codec chucks an exception while doing it's formatting... but I agree it's probably better to avoid the transient/nullable stuff if we can.\r\n\r\nI'll make the change."", 'commenter': 'absurdfarce'}]"
1433,integration-tests/src/test/java/com/datastax/oss/driver/api/core/metadata/SerializableMetadataIT.java,"@@ -0,0 +1,152 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core.metadata;
+
+import static org.junit.Assert.*;
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.metadata.schema.AggregateMetadata;
+import com.datastax.oss.driver.api.core.metadata.schema.KeyspaceMetadata;
+import com.datastax.oss.driver.api.core.metadata.schema.TableMetadata;
+import com.datastax.oss.driver.api.core.type.DataTypes;
+import com.datastax.oss.driver.api.testinfra.ccm.CcmRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionRule;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import com.datastax.oss.driver.internal.core.metadata.schema.DefaultKeyspaceMetadata;
+import com.datastax.oss.driver.internal.core.metadata.schema.DefaultTableMetadata;
+import com.google.common.collect.ImmutableList;
+import java.io.ByteArrayInputStream;
+import java.io.ByteArrayOutputStream;
+import java.io.ObjectInputStream;
+import java.io.ObjectOutputStream;
+import java.util.List;
+import java.util.Optional;
+import org.apache.commons.lang3.SerializationUtils;
+import org.junit.Before;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.junit.rules.RuleChain;
+import org.junit.rules.TestRule;
+
+@Category(ParallelizableTests.class)
+public class SerializableMetadataIT {
+
+  private static final CcmRule ccmRule = CcmRule.getInstance();
+
+  private static final SessionRule<CqlSession> sessionRule = SessionRule.builder(ccmRule).build();
+
+  @ClassRule public static final TestRule chain = RuleChain.outerRule(ccmRule).around(sessionRule);
+
+  @Before
+  public void setupKeyspace() {
+    List<String> statements =
+        ImmutableList.of(
+            ""CREATE TABLE all (val int, doub int, str text, primary key (val,doub)) WITH CLUSTERING ORDER BY (doub DESC)"",
+            ""CREATE TYPE numbers_data (doub int, str text)"",
+            ""CREATE TABLE smaller (val int primary key, data numbers_data)"",
+            ""CREATE INDEX doub_idx on all (doub)"",
+            ""CREATE OR REPLACE FUNCTION avgState ( state tuple<int,bigint>, val int ) CALLED ON NULL INPUT RETURNS tuple<int,bigint> LANGUAGE java AS""
+                + "" 'if (val !=null) { state.setInt(0, state.getInt(0)+1); state.setLong(1, state.getLong(1)+val.intValue()); } return state;'"",
+            ""CREATE OR REPLACE FUNCTION avgFinal ( state tuple<int,bigint> ) CALLED ON NULL INPUT RETURNS double LANGUAGE java AS""
+                + "" 'double r = 0; if (state.getInt(0) == 0) return null; r = state.getLong(1); r/= state.getInt(0); return Double.valueOf(r);'"",
+            ""CREATE AGGREGATE IF NOT EXISTS average ( int ) SFUNC avgState STYPE tuple<int,bigint> FINALFUNC avgFinal INITCOND (0,0)"",
+            ""INSERT INTO all (val,doub,str) VALUES (1,2,'one')"",
+            ""INSERT INTO all (val,doub,str) VALUES (2,4,'two')"",
+            ""INSERT INTO all (val,doub,str) VALUES (3,6,'three')"",
+            ""CREATE MATERIALIZED VIEW IF NOT EXISTS big AS SELECT * FROM all""
+                + "" WHERE val is not null and doub is not null and val > 2""
+                + "" PRIMARY KEY (val,doub)"");
+
+    CqlSession session = sessionRule.session();
+    statements.forEach(session::execute);
+  }
+
+  private KeyspaceMetadata getAndValidateKsMetadata() {
+
+    Optional<KeyspaceMetadata> ksOption =
+            sessionRule.session().getMetadata().getKeyspace(sessionRule.session().getKeyspace().get());
+    assertTrue(ksOption.isPresent());
+    KeyspaceMetadata ks = ksOption.get();
+    assertTrue(ks instanceof DefaultKeyspaceMetadata);
+    return ks;
+  }
+
+  @Test
+  public void should_contain_serializable_metadata() {
+
+    KeyspaceMetadata ks = getAndValidateKsMetadata();
+
+    /* Validate that the keyspace metadata is fully populated */
+    assertFalse(ks.getUserDefinedTypes().isEmpty());
+    assertFalse(ks.getTables().isEmpty());
+    assertFalse(ks.getViews().isEmpty());
+    assertFalse(ks.getFunctions().isEmpty());
+    assertFalse(ks.getAggregates().isEmpty());
+
+    Optional<TableMetadata> tableOption = ks.getTable(""all"");
+    assertTrue(tableOption.isPresent());
+    TableMetadata table = tableOption.get();
+    assertTrue(table instanceof DefaultTableMetadata);
+
+    /* Validate that the table metadata is fully populated */
+    assertFalse(table.getPartitionKey().isEmpty());
+    assertFalse(table.getClusteringColumns().isEmpty());
+    assertFalse(table.getColumns().isEmpty());
+    // TODO: What kind of objects can be set for options here?  How do we guarantee they're
+    // serializable?
+    assertFalse(table.getOptions().isEmpty());
+    assertFalse(table.getIndexes().isEmpty());
+
+    SerializationUtils.roundtrip((DefaultKeyspaceMetadata) ks);
+  }
+
+  @Test
+  public void should_support_describe_for_aggregates_with_deserialized_instances() {","[{'comment': ""This test already passed due to the exception handling in DefaultAggregateMetadata.describe().  Adding this as an explicit integration test as a guard against future breakage in functionality (since that's the only area affected by the transient declaration)."", 'commenter': 'absurdfarce'}]"
1433,integration-tests/src/test/java/com/datastax/oss/driver/api/core/metadata/SerializableMetadataIT.java,"@@ -0,0 +1,153 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core.metadata;
+
+import static org.junit.Assert.*;
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.metadata.schema.AggregateMetadata;
+import com.datastax.oss.driver.api.core.metadata.schema.KeyspaceMetadata;
+import com.datastax.oss.driver.api.core.metadata.schema.TableMetadata;
+import com.datastax.oss.driver.api.core.type.DataTypes;
+import com.datastax.oss.driver.api.testinfra.ccm.CcmRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionRule;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import com.datastax.oss.driver.internal.core.metadata.schema.DefaultKeyspaceMetadata;
+import com.datastax.oss.driver.internal.core.metadata.schema.DefaultTableMetadata;
+import com.google.common.collect.ImmutableList;
+import java.io.ByteArrayInputStream;
+import java.io.ByteArrayOutputStream;
+import java.io.ObjectInputStream;
+import java.io.ObjectOutputStream;
+import java.util.List;
+import java.util.Optional;
+import org.apache.commons.lang3.SerializationUtils;
+import org.junit.Before;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.junit.rules.RuleChain;
+import org.junit.rules.TestRule;
+
+@Category(ParallelizableTests.class)
+public class SerializableMetadataIT {
+
+  private static final CcmRule ccmRule = CcmRule.getInstance();
+
+  private static final SessionRule<CqlSession> sessionRule = SessionRule.builder(ccmRule).build();
+
+  @ClassRule public static final TestRule chain = RuleChain.outerRule(ccmRule).around(sessionRule);
+
+  @Before
+  public void setupKeyspace() {
+    List<String> statements =
+        ImmutableList.of(
+            ""CREATE TABLE IF NOT EXISTS all (val int, doub int, str text, primary key (val,doub)) WITH CLUSTERING ORDER BY (doub DESC)"",
+            ""CREATE TYPE IF NOT EXISTS numbers_data (doub int, str text)"",
+            ""CREATE TABLE IF NOT EXISTS smaller (val int primary key, data numbers_data)"",
+            ""CREATE INDEX IF NOT EXISTS doub_idx on all (doub)"",
+            ""CREATE OR REPLACE FUNCTION avgState ( state tuple<int,bigint>, val int ) CALLED ON NULL INPUT RETURNS tuple<int,bigint> LANGUAGE java AS""
+                + "" 'if (val !=null) { state.setInt(0, state.getInt(0)+1); state.setLong(1, state.getLong(1)+val.intValue()); } return state;'"",
+            ""CREATE OR REPLACE FUNCTION avgFinal ( state tuple<int,bigint> ) CALLED ON NULL INPUT RETURNS double LANGUAGE java AS""
+                + "" 'double r = 0; if (state.getInt(0) == 0) return null; r = state.getLong(1); r/= state.getInt(0); return Double.valueOf(r);'"",
+            ""CREATE AGGREGATE IF NOT EXISTS average ( int ) SFUNC avgState STYPE tuple<int,bigint> FINALFUNC avgFinal INITCOND (0,0)"",
+            ""TRUNCATE all"",
+            ""INSERT INTO all (val,doub,str) VALUES (1,2,'one')"",
+            ""INSERT INTO all (val,doub,str) VALUES (2,4,'two')"",
+            ""INSERT INTO all (val,doub,str) VALUES (3,6,'three')"",
+            ""CREATE MATERIALIZED VIEW IF NOT EXISTS big AS SELECT * FROM all""
+                + "" WHERE val is not null and doub is not null and val > 2""
+                + "" PRIMARY KEY (val,doub)"");
+
+    CqlSession session = sessionRule.session();
+    statements.forEach(session::execute);
+  }
+
+  private KeyspaceMetadata getAndValidateKsMetadata() {
+
+    Optional<KeyspaceMetadata> ksOption =
+        sessionRule.session().getMetadata().getKeyspace(sessionRule.session().getKeyspace().get());
+    assertTrue(ksOption.isPresent());
+    KeyspaceMetadata ks = ksOption.get();
+    assertTrue(ks instanceof DefaultKeyspaceMetadata);
+    return ks;
+  }
+
+  @Test
+  public void should_contain_serializable_metadata() {
+
+    KeyspaceMetadata ks = getAndValidateKsMetadata();
+
+    /* Validate that the keyspace metadata is fully populated */
+    assertFalse(ks.getUserDefinedTypes().isEmpty());
+    assertFalse(ks.getTables().isEmpty());
+    assertFalse(ks.getViews().isEmpty());
+    assertFalse(ks.getFunctions().isEmpty());
+    assertFalse(ks.getAggregates().isEmpty());","[{'comment': ""Nit: we've pretty much settled on AssertJ as de facto standard by now, please use it instead of JUnit assertions. As a bonus, it has more fluent alternatives for most of those checks:\r\n```java\r\nassertThat(ks.getUserDefinedTypes()).isEmpty();\r\n```\r\n\\+ the optional example from my other comment"", 'commenter': 'olim7t'}, {'comment': ""Bah, that's on me.  I know AssertJ is preferred... I used the JUnit checks when I was getting this test going and forgot to convert before I threw this open to review. :facepalm: \r\n\r\nWill fix."", 'commenter': 'absurdfarce'}]"
1433,integration-tests/src/test/java/com/datastax/oss/driver/api/core/metadata/SerializableMetadataIT.java,"@@ -0,0 +1,153 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core.metadata;
+
+import static org.junit.Assert.*;
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.metadata.schema.AggregateMetadata;
+import com.datastax.oss.driver.api.core.metadata.schema.KeyspaceMetadata;
+import com.datastax.oss.driver.api.core.metadata.schema.TableMetadata;
+import com.datastax.oss.driver.api.core.type.DataTypes;
+import com.datastax.oss.driver.api.testinfra.ccm.CcmRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionRule;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import com.datastax.oss.driver.internal.core.metadata.schema.DefaultKeyspaceMetadata;
+import com.datastax.oss.driver.internal.core.metadata.schema.DefaultTableMetadata;
+import com.google.common.collect.ImmutableList;
+import java.io.ByteArrayInputStream;
+import java.io.ByteArrayOutputStream;
+import java.io.ObjectInputStream;
+import java.io.ObjectOutputStream;
+import java.util.List;
+import java.util.Optional;
+import org.apache.commons.lang3.SerializationUtils;
+import org.junit.Before;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.junit.rules.RuleChain;
+import org.junit.rules.TestRule;
+
+@Category(ParallelizableTests.class)
+public class SerializableMetadataIT {
+
+  private static final CcmRule ccmRule = CcmRule.getInstance();
+
+  private static final SessionRule<CqlSession> sessionRule = SessionRule.builder(ccmRule).build();
+
+  @ClassRule public static final TestRule chain = RuleChain.outerRule(ccmRule).around(sessionRule);
+
+  @Before
+  public void setupKeyspace() {
+    List<String> statements =
+        ImmutableList.of(
+            ""CREATE TABLE IF NOT EXISTS all (val int, doub int, str text, primary key (val,doub)) WITH CLUSTERING ORDER BY (doub DESC)"",
+            ""CREATE TYPE IF NOT EXISTS numbers_data (doub int, str text)"",
+            ""CREATE TABLE IF NOT EXISTS smaller (val int primary key, data numbers_data)"",
+            ""CREATE INDEX IF NOT EXISTS doub_idx on all (doub)"",
+            ""CREATE OR REPLACE FUNCTION avgState ( state tuple<int,bigint>, val int ) CALLED ON NULL INPUT RETURNS tuple<int,bigint> LANGUAGE java AS""
+                + "" 'if (val !=null) { state.setInt(0, state.getInt(0)+1); state.setLong(1, state.getLong(1)+val.intValue()); } return state;'"",
+            ""CREATE OR REPLACE FUNCTION avgFinal ( state tuple<int,bigint> ) CALLED ON NULL INPUT RETURNS double LANGUAGE java AS""
+                + "" 'double r = 0; if (state.getInt(0) == 0) return null; r = state.getLong(1); r/= state.getInt(0); return Double.valueOf(r);'"",
+            ""CREATE AGGREGATE IF NOT EXISTS average ( int ) SFUNC avgState STYPE tuple<int,bigint> FINALFUNC avgFinal INITCOND (0,0)"",
+            ""TRUNCATE all"",
+            ""INSERT INTO all (val,doub,str) VALUES (1,2,'one')"",
+            ""INSERT INTO all (val,doub,str) VALUES (2,4,'two')"",
+            ""INSERT INTO all (val,doub,str) VALUES (3,6,'three')"",
+            ""CREATE MATERIALIZED VIEW IF NOT EXISTS big AS SELECT * FROM all""
+                + "" WHERE val is not null and doub is not null and val > 2""
+                + "" PRIMARY KEY (val,doub)"");
+
+    CqlSession session = sessionRule.session();
+    statements.forEach(session::execute);
+  }
+
+  private KeyspaceMetadata getAndValidateKsMetadata() {
+
+    Optional<KeyspaceMetadata> ksOption =
+        sessionRule.session().getMetadata().getKeyspace(sessionRule.session().getKeyspace().get());
+    assertTrue(ksOption.isPresent());
+    KeyspaceMetadata ks = ksOption.get();
+    assertTrue(ks instanceof DefaultKeyspaceMetadata);
+    return ks;
+  }
+
+  @Test
+  public void should_contain_serializable_metadata() {
+
+    KeyspaceMetadata ks = getAndValidateKsMetadata();
+
+    /* Validate that the keyspace metadata is fully populated */
+    assertFalse(ks.getUserDefinedTypes().isEmpty());
+    assertFalse(ks.getTables().isEmpty());
+    assertFalse(ks.getViews().isEmpty());
+    assertFalse(ks.getFunctions().isEmpty());
+    assertFalse(ks.getAggregates().isEmpty());
+
+    Optional<TableMetadata> tableOption = ks.getTable(""all"");
+    assertTrue(tableOption.isPresent());
+    TableMetadata table = tableOption.get();
+    assertTrue(table instanceof DefaultTableMetadata);
+
+    /* Validate that the table metadata is fully populated */
+    assertFalse(table.getPartitionKey().isEmpty());
+    assertFalse(table.getClusteringColumns().isEmpty());
+    assertFalse(table.getColumns().isEmpty());
+    // TODO: What kind of objects can be set for options here?  How do we guarantee they're
+    // serializable?
+    assertFalse(table.getOptions().isEmpty());
+    assertFalse(table.getIndexes().isEmpty());
+
+    SerializationUtils.roundtrip((DefaultKeyspaceMetadata) ks);","[{'comment': ""Nit -- we have a built-in utility for that: `SerializationHelper.serializeAndDeserialize`.\r\n\r\nIt's essentially the same thing, but we wrote it at a time when commons-lang wasn't yet available in the classpath (I'm not even sure how it landed here TBH). Let's use it everywhere for the sake of consistency."", 'commenter': 'olim7t'}, {'comment': ""Looks like it's Gremlin's fault:\r\n\r\n```\r\n[INFO] +- org.apache.tinkerpop:gremlin-core:jar:3.4.5:compile\r\n[INFO] |  +- org.apache.tinkerpop:gremlin-shaded:jar:3.4.5:compile\r\n[INFO] |  +- commons-configuration:commons-configuration:jar:1.10:compile\r\n[INFO] |  |  \\- commons-lang:commons-lang:jar:2.6:compile\r\n[INFO] |  +- commons-collections:commons-collections:jar:3.2.2:compile\r\n[INFO] |  +- org.apache.commons:commons-lang3:jar:3.8.1:compile\r\n[INFO] |  +- org.yaml:snakeyaml:jar:1.15:compile\r\n[INFO] |  +- com.carrotsearch:hppc:jar:0.7.1:compile\r\n[INFO] |  +- com.jcabi:jcabi-manifests:jar:1.1:compile\r\n[INFO] |  |  \\- com.jcabi:jcabi-log:jar:0.14:compile\r\n[INFO] |  +- com.squareup:javapoet:jar:1.11.1:compile\r\n[INFO] |  +- net.objecthunter:exp4j:jar:0.4.8:compile\r\n[INFO] |  \\- org.slf4j:jcl-over-slf4j:jar:1.7.25:compile\r\n```\r\n\r\nI love that the same artifact brings in both commons-lang2 (via a transitive dep) and commons-lang3 directly. :rage: "", 'commenter': 'absurdfarce'}]"
1433,integration-tests/src/test/java/com/datastax/oss/driver/api/core/metadata/SerializableMetadataIT.java,"@@ -0,0 +1,153 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core.metadata;
+
+import static org.junit.Assert.*;
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.metadata.schema.AggregateMetadata;
+import com.datastax.oss.driver.api.core.metadata.schema.KeyspaceMetadata;
+import com.datastax.oss.driver.api.core.metadata.schema.TableMetadata;
+import com.datastax.oss.driver.api.core.type.DataTypes;
+import com.datastax.oss.driver.api.testinfra.ccm.CcmRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionRule;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import com.datastax.oss.driver.internal.core.metadata.schema.DefaultKeyspaceMetadata;
+import com.datastax.oss.driver.internal.core.metadata.schema.DefaultTableMetadata;
+import com.google.common.collect.ImmutableList;
+import java.io.ByteArrayInputStream;
+import java.io.ByteArrayOutputStream;
+import java.io.ObjectInputStream;
+import java.io.ObjectOutputStream;
+import java.util.List;
+import java.util.Optional;
+import org.apache.commons.lang3.SerializationUtils;
+import org.junit.Before;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.junit.rules.RuleChain;
+import org.junit.rules.TestRule;
+
+@Category(ParallelizableTests.class)
+public class SerializableMetadataIT {
+
+  private static final CcmRule ccmRule = CcmRule.getInstance();
+
+  private static final SessionRule<CqlSession> sessionRule = SessionRule.builder(ccmRule).build();
+
+  @ClassRule public static final TestRule chain = RuleChain.outerRule(ccmRule).around(sessionRule);
+
+  @Before
+  public void setupKeyspace() {
+    List<String> statements =
+        ImmutableList.of(
+            ""CREATE TABLE IF NOT EXISTS all (val int, doub int, str text, primary key (val,doub)) WITH CLUSTERING ORDER BY (doub DESC)"",
+            ""CREATE TYPE IF NOT EXISTS numbers_data (doub int, str text)"",
+            ""CREATE TABLE IF NOT EXISTS smaller (val int primary key, data numbers_data)"",
+            ""CREATE INDEX IF NOT EXISTS doub_idx on all (doub)"",
+            ""CREATE OR REPLACE FUNCTION avgState ( state tuple<int,bigint>, val int ) CALLED ON NULL INPUT RETURNS tuple<int,bigint> LANGUAGE java AS""
+                + "" 'if (val !=null) { state.setInt(0, state.getInt(0)+1); state.setLong(1, state.getLong(1)+val.intValue()); } return state;'"",
+            ""CREATE OR REPLACE FUNCTION avgFinal ( state tuple<int,bigint> ) CALLED ON NULL INPUT RETURNS double LANGUAGE java AS""
+                + "" 'double r = 0; if (state.getInt(0) == 0) return null; r = state.getLong(1); r/= state.getInt(0); return Double.valueOf(r);'"",
+            ""CREATE AGGREGATE IF NOT EXISTS average ( int ) SFUNC avgState STYPE tuple<int,bigint> FINALFUNC avgFinal INITCOND (0,0)"",
+            ""TRUNCATE all"",
+            ""INSERT INTO all (val,doub,str) VALUES (1,2,'one')"",
+            ""INSERT INTO all (val,doub,str) VALUES (2,4,'two')"",
+            ""INSERT INTO all (val,doub,str) VALUES (3,6,'three')"",
+            ""CREATE MATERIALIZED VIEW IF NOT EXISTS big AS SELECT * FROM all""
+                + "" WHERE val is not null and doub is not null and val > 2""
+                + "" PRIMARY KEY (val,doub)"");","[{'comment': ""Schema-related testing is always annoying because it has to work against all the server versions in our CI suite. In particular, the functions, aggregates and views used here are going to be a problem with legacy Cassandra versions.\r\n\r\nI don't like to piggyback on existing tests, but for this particular case we could avoid a lot of headaches by reusing `DescribeIT`, which already has the logic to deal with backward compatibility. It could be as simple as throwing a serialization roundtrip in the middle of the existing method, or we could add a separate one.\r\n\r\nAs a bonus, comparing the `describeWithChildren` output of the top-level keyspace gives us a pretty reliable deep comparison, without having to enumerate all the children."", 'commenter': 'olim7t'}, {'comment': ""Hmmm, interesting... I'll look more at DescribeIT and see what's involved in such a migration.  Doesn't sound too bad on the face of it."", 'commenter': 'absurdfarce'}]"
1433,integration-tests/src/test/java/com/datastax/oss/driver/core/metadata/DescribeIT.java,"@@ -158,4 +202,35 @@ private File getScriptFile() {
         ""Using {} to test against {} {}"", bestFile, isDse ? ""DSE"" : ""Cassandra"", serverVersion);
     return bestFile;
   }
+
+  private static String getScriptContents() {
+
+    try {
+
+      return Files.asCharSource(scriptFile, Charsets.UTF_8)
+          .read()
+          .trim()
+          .replaceAll(""ks_0"", SESSION_RULE.keyspace().asCql(true));
+    } catch (IOException ioe) {
+      fail(""Exception reading script file "" + scriptFile, ioe);
+      return null;
+    }
+  }
+
+  private static String setupDatabase() {
+
+    List<String> statements = STATEMENT_SPLITTER.splitToList(scriptContents);
+
+    // Skip the first statement (CREATE KEYSPACE), we already have a keyspace
+    for (int i = 1; i < statements.size(); i++) {
+      String statement = statements.get(i);
+      try {
+        SESSION_RULE.session().execute(statement);
+      } catch (Exception e) {
+        fail(""Error executing statement %s (%s)"", statement, e);
+      }
+    }
+
+    return scriptContents;","[{'comment': 'nit: return value is not used', 'commenter': 'olim7t'}, {'comment': ""Yeah, this was put in there to make IntelliJ happy.  I'll change the method to just return a list of failed statements and do the fail() at the caller."", 'commenter': 'absurdfarce'}, {'comment': 'Why not just make the method void?', 'commenter': 'olim7t'}]"
1438,driver-core/src/test/java/com/datastax/driver/core/ReplicationFactorTest.java,"@@ -0,0 +1,44 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.driver.core;
+
+import static org.assertj.core.api.Assertions.assertThat;
+
+import org.testng.annotations.Test;
+","[{'comment': 'Is it possible to add an Integration test with a transient replica that runs only on > `OSS 4.0.0` and validates that the replication factor is parsed correctly?', 'commenter': 'tomekl007'}, {'comment': 'I should be able to do that.', 'commenter': 'emerkle826'}, {'comment': 'I think that would be welcome on 4.x branch as well. But you can do this in a follow-up ticket.', 'commenter': 'adutra'}, {'comment': 'Added: https://datastax-oss.atlassian.net/browse/JAVA-2746', 'commenter': 'emerkle826'}]"
1438,driver-core/src/main/java/com/datastax/driver/core/ReplicationFactor.java,"@@ -0,0 +1,85 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.driver.core;
+
+// This class is a subset of server version at org.apache.cassandra.locator.ReplicationFactor
+
+import com.google.common.base.Preconditions;
+
+class ReplicationFactor {
+  private final int allReplicas;
+  private final int fullReplicas;
+  private final int transientReplicas;
+
+  ReplicationFactor(int allReplicas, int transientReplicas) {
+    this.allReplicas = allReplicas;
+    this.transientReplicas = transientReplicas;
+    this.fullReplicas = allReplicas - transientReplicas;
+  }
+
+  ReplicationFactor(int allReplicas) {
+    this(allReplicas, 0);
+  }
+
+  int fullReplicas() {
+    return fullReplicas;
+  }
+
+  int transientReplicas() {
+    return transientReplicas;
+  }
+
+  boolean hasTransientReplicas() {
+    return transientReplicas > 0;
+  }
+
+  static ReplicationFactor fromString(String s) {
+    if (s.contains(""/"")) {
+
+      int slash = s.indexOf('/');
+      String allPart = s.substring(0, slash);
+      String transientPart = s.substring(slash + 1);
+      Preconditions.checkArgument(
+          allPart != null && transientPart != null,","[{'comment': 'This is an old version of driver 4\'s `ReplicationFactor`. This logic can/should be simplified to\r\n\r\n```\r\n    if (s.contains(""/"")) {\r\n      int slash = s.indexOf(\'/\');\r\n      String allPart = s.substring(0, slash);\r\n      String transientPart = s.substring(slash + 1);\r\n      return new ReplicationFactor(Integer.parseInt(allPart), Integer.parseInt(transientPart));\r\n    } else {\r\n      return new ReplicationFactor(Integer.parseInt(s), 0);\r\n    }\r\n```', 'commenter': 'adutra'}, {'comment': ""I'll update it."", 'commenter': 'emerkle826'}]"
1438,driver-core/src/main/java/com/datastax/driver/core/ReplicationFactor.java,"@@ -0,0 +1,85 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.driver.core;
+
+// This class is a subset of server version at org.apache.cassandra.locator.ReplicationFactor
+
+import com.google.common.base.Preconditions;
+
+class ReplicationFactor {
+  private final int allReplicas;
+  private final int fullReplicas;
+  private final int transientReplicas;
+
+  ReplicationFactor(int allReplicas, int transientReplicas) {
+    this.allReplicas = allReplicas;
+    this.transientReplicas = transientReplicas;
+    this.fullReplicas = allReplicas - transientReplicas;
+  }
+
+  ReplicationFactor(int allReplicas) {
+    this(allReplicas, 0);
+  }
+
+  int fullReplicas() {
+    return fullReplicas;
+  }
+
+  int transientReplicas() {
+    return transientReplicas;
+  }
+
+  boolean hasTransientReplicas() {
+    return transientReplicas > 0;
+  }
+
+  static ReplicationFactor fromString(String s) {
+    if (s.contains(""/"")) {
+
+      int slash = s.indexOf('/');
+      String allPart = s.substring(0, slash);
+      String transientPart = s.substring(slash + 1);
+      Preconditions.checkArgument(
+          allPart != null && transientPart != null,
+          ""Replication factor format is <replicas> or <replicas>/<transient>"");
+      return new ReplicationFactor(Integer.parseInt(allPart), Integer.parseInt(transientPart));
+    } else {
+      return new ReplicationFactor(Integer.parseInt(s), 0);
+    }
+  }
+
+  @Override
+  public String toString() {
+    return allReplicas + (hasTransientReplicas() ? ""/"" + transientReplicas() : """");
+  }
+
+  @Override
+  public boolean equals(Object o) {
+    if (this == o) {
+      return true;
+    }
+    if (!(o instanceof ReplicationFactor)) {
+      return false;
+    }
+    ReplicationFactor that = (ReplicationFactor) o;
+    return allReplicas == that.allReplicas && transientReplicas == that.transientReplicas;
+  }
+
+  @Override
+  public int hashCode() {
+    return allReplicas ^ transientReplicas;","[{'comment': ""That's a fancy hash code, shouldn't we go for the more mainstream ` Objects.hash(allReplicas, fullReplicas)`?"", 'commenter': 'adutra'}, {'comment': ""I did use that at first, but there is a JDK6 check in the compile for 3.x and `Objects.hash()` was introduced in JDK7, so it errors the compile with this:\r\n```\r\n[INFO] --- animal-sniffer-maven-plugin:1.15:check (check-jdk6) @ cassandra-driver-core ---\r\n[INFO] Checking unresolved references to org.codehaus.mojo.signature:java16:1.0\r\n[ERROR] /home/emerkle/workspace/java-driver-3.x/driver-core/src/main/java/com/datastax/driver/core/ReplicationFactor.java:85: Undefined reference: int java.util.Objects.hash(Object[])\r\n```\r\n\r\nSo I went with the old-school hash computation of XOR'ing the 2 attributes used to construct the object."", 'commenter': 'emerkle826'}, {'comment': 'Ah right, thanks for the explanation. ', 'commenter': 'adutra'}]"
1438,driver-core/src/main/java/com/datastax/driver/core/schemabuilder/TableOptions.java,"@@ -1362,4 +1413,38 @@ public String value() {
       return value;
     }
   }
+
+  /** Read Repair modes. Possible values: BLOCKING, NONE. */
+  public static enum ReadRepairValue {
+    BLOCKING(""'BLOCKING'""),
+    NONE(""'NONE'"");
+
+    private String value;
+
+    private ReadRepairValue(String value) {
+      this.value = value;
+    }
+
+    public String value() {
+      return value;
+    }
+
+    @Override
+    public String toString() {
+      return value;
+    }
+  }
+
+  /** Additional Write Policy. Default value is 99p */
+  public static class AdditionalWritePolicyValue {","[{'comment': ""Shouldn't we create an enum with possible options for this `AdditionalWritePolicyValue`?\r\n(similar to https://github.com/datastax/java-driver/pull/1438/files/175ee2f219caf1a8a475d8ad005935aa6b93d3ca..62d5b0cad267e3746b316858037fcf540ab8b7b2#diff-89e075b9ba33eed1af4b80ebefe597ccR1419-R1420)"", 'commenter': 'tomekl007'}, {'comment': ""I would, but there are many possible values. This `additional_write_policy` value, from what I can tell, is parsed and treated like `speculative_retry`, to the point that invalid values result in an error message like this:\r\n```\r\nConfigurationException: Invalid value ALL for option 'speculative_retry'\r\n```\r\nValid values can be percentiles ('99p', '50p', '32p'), time ('400ms', '200ms'), 'ALWAYS' and 'NEVER', so I don't think it's feasible to create an enum for all values.\r\n\r\nThat said, we could add more support to `SchemaBuilder` for this and do something similar to what we do for speculative retry:\r\n\r\nSchemaBuilder.[noSpeculativeRetry()](https://github.com/datastax/java-driver/blob/3.x/driver-core/src/main/java/com/datastax/driver/core/schemabuilder/SchemaBuilder.java#L321-L323)\r\nSchemaBuilder.[always()](https://github.com/datastax/java-driver/blob/3.x/driver-core/src/main/java/com/datastax/driver/core/schemabuilder/SchemaBuilder.java#L331-L333)\r\nSchemaBuilder.[percentile(int percentile)](https://github.com/datastax/java-driver/blob/3.x/driver-core/src/main/java/com/datastax/driver/core/schemabuilder/SchemaBuilder.java#L341-L347)\r\nSchemaBuilder.[millisecs(int millisecs)](https://github.com/datastax/java-driver/blob/3.x/driver-core/src/main/java/com/datastax/driver/core/schemabuilder/SchemaBuilder.java#L355-L361)"", 'commenter': 'emerkle826'}]"
1439,core/src/main/java/com/datastax/oss/driver/internal/core/os/GraalGetpid.java,"@@ -0,0 +1,45 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.os;
+
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import org.graalvm.nativeimage.c.CContext;
+import org.graalvm.nativeimage.c.function.CFunction;
+
+@CContext(GraalGetpid.Directives.class)
+public class GraalGetpid {
+
+  static class Directives implements CContext.Directives {
+
+    @Override
+    public List<String> getHeaderFiles() {
+
+      return Collections.unmodifiableList(Arrays.asList(""<unistd.h>""));
+    }
+
+    /* TODO: I don't know that these are required... */
+    @Override
+    public List<String> getMacroDefinitions() {
+
+      return Collections.unmodifiableList(Arrays.asList(""_GNU_SOURCE"", ""_LARGEFILE64_SOURCE""));","[{'comment': ""We explicitly avoid Guava in these methods since this code will be evaluated by Graal analysis at build time... and we don't want to force app authors to include a bunch of Guava classes as build-time requiremenets."", 'commenter': 'absurdfarce'}, {'comment': ""Isn't the shaded guava already included in the build via java-driver dependency?"", 'commenter': 'tomekl007'}, {'comment': ""It is, but we're just creating a simple unmodifiable list here, either approach will do."", 'commenter': 'olim7t'}, {'comment': ""It's not a question of whether the classes are available @tomekl007 ... you're correct, they certainly are in the classpath.  The point is that this code will be eval'd at build-time (since we have to build C source off of it).  And if we include the Guava classes here you'll get errors in the native image build operation saying that some Guava classes were unexpectedly initialized at build-time, which you then have to work around by specifying the --initialize-at-build-time flag for each of those classes.  I'm trying to avoid all of that by keeping Guava out of this section of the code base."", 'commenter': 'absurdfarce'}]"
1439,core/src/main/java/com/datastax/oss/driver/internal/core/os/Native.java,"@@ -25,8 +25,24 @@
 
   private static class LibcLoader {
 
+    /* These values come from Graal's imageinfo API which aims to offer the ability to detect
+    * when we're in the Graal build/run time via system props.  The maintainers of Graal have
+    * agreed that this API will not change over time.  We reference these props as literals
+    * to avoid introducing a dependency on Graal code for non-Graal users here. */
+    private static final String GRAAL_STATUS_PROP = ""org.graalvm.nativeimage.imagecode"";
+    private static final String GRAAL_BUILDTIME_STATUS = ""buildtime"";
+    private static final String GRAAL_RUNTIME_STATUS = ""runtime"";
+
     public Libc load() {
       try {
+        if (isGraalBuildTime()) {
+          LOG.info(""Using Graal-native functions for Graal build time"");
+          return new GraalLibc();
+        }
+        if (isGraalRunTime()) {","[{'comment': ""I don't understand why do we need to create two conditions. \r\nIf the graal is available in the build time, it is also available in the build time, isn't it?\r\nIf so we can coalesce those into one if that checks:  \r\n```\r\nreturn System.getProperty(GRAAL_STATUS_PROP).equals(    return System.getProperty(GRAAL_STATUS_PROP).equals(GRAAL_BUILDTIME_STATUS) || \r\nreturn System.getProperty(GRAAL_STATUS_PROP).equals(GRAAL_RUNTIME_STATUS)\r\n```"", 'commenter': 'tomekl007'}, {'comment': 'Yeah I tend to agree. Maybe if we needed different implementations at some point in the future, but that sounds far-fetched and we could always make the change later...', 'commenter': 'olim7t'}, {'comment': ""My original thinking was that we'd need two checks to address the following cases:\r\n\r\n* At build-time we want Graal's build-time analysis logic to follow that path rather than worry about the jnr path\r\n* At run-time we wan to select the impl in order to access the underlying System/libc functionality\r\n\r\nI don't think the run-time point above is terribly controversial... but as I reflect on it more I think the build-time requirement is bogus.  Built into my logic above is a belief that the build-time analysis done by the native image build is aware of these system properties and can detect branching logic based on them.  This is... somewhat silly.  A much more plausible explanation is that the analysis process just considers _both_ paths (realizing that the actual selection will be made at run-time).  As a result there's no real reason to worry about build-time at all.\r\n\r\nLocal testing seems to confirm this hypothesis, so I'll remove the build-time constraint all together."", 'commenter': 'absurdfarce'}, {'comment': 'I think you need to leave the `build-time`. Without it, the quarkus app that is using JNR will not compile with incomplete classpath errors.', 'commenter': 'tomekl007'}, {'comment': ""It shouldn't result in incomplete classpath errors, but you do raise an interesting point @tomekl007 ... it's quite possible my sample app isn't doing build-time initialization of any of the native classes.  Which means we might not be exercising everything we need for testing here.\r\n\r\nYup, looks like that might be the case.  Quarkus does apparently trigger build-time init of these classes (via Uuids) which leads us to the following:\r\n\r\n```\r\nError: Class initialization of com.datastax.oss.driver.api.core.uuid.Uuids failed. Use the option --initialize-at-run-time=com.datastax.oss.driver.api.core.uuid.Uuids to explicit\r\nly request delayed initialization of this class.                                                                                                    \r\nDetailed message:                                                      \r\n                                                                                                \r\ncom.oracle.svm.core.util.UserError$UserException: Class initialization of com.datastax.oss.driver.api.core.uuid.Uuids failed. Use the option --initialize-at-run-time=com.datastax\r\n.oss.driver.api.core.uuid.Uuids to explicitly request delayed initialization of this class. \r\nDetailed message:                                                                                  \r\n                                                                                              \r\n        at com.oracle.svm.core.util.UserError.abort(UserError.java:75)                     \r\n        at com.oracle.svm.hosted.FallbackFeature.reportAsFallback(FallbackFeature.java:221)                                                                                               at com.oracle.svm.hosted.NativeImageGenerator.runPointsToAnalysis(NativeImageGenerator.java:736)\r\n        at com.oracle.svm.hosted.NativeImageGenerator.doRun(NativeImageGenerator.java:530)                                                                                       \r\n        at com.oracle.svm.hosted.NativeImageGenerator.lambda$run$0(NativeImageGenerator.java:445)                                                                                \r\n        at java.util.concurrent.ForkJoinTask$AdaptedRunnableAction.exec(ForkJoinTask.java:1386)                   \r\n        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)                                                                                                       \r\n        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)                                                                                           \r\n        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)               \r\n        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)                                                                                          \r\nCaused by: com.oracle.graal.pointsto.constraints.UnsupportedFeatureException: Class initialization of com.datastax.oss.driver.api.core.uuid.Uuids failed. Use the option --initial\r\nize-at-run-time=com.datastax.oss.driver.api.core.uuid.Uuids to explicitly request delayed initialization of this class.                                                          \r\nDetailed message:                                                                                                                                                                \r\n                                                                                                                                                                                 \r\n        at com.oracle.graal.pointsto.constraints.UnsupportedFeatures.report(UnsupportedFeatures.java:126)                                                                        \r\n        at com.oracle.svm.hosted.NativeImageGenerator.runPointsToAnalysis(NativeImageGenerator.java:733)\r\n        ... 7 more                                                        \r\nCaused by: java.lang.UnsatisfiedLinkError: com.datastax.oss.driver.internal.core.os.GraalGetpid.getpid()I\r\n        at com.datastax.oss.driver.internal.core.os.GraalGetpid.getpid(Native Method)                                                                                            \r\n        at com.datastax.oss.driver.internal.core.os.GraalLibc.getpid(GraalLibc.java:36)                                                                                          \r\n        at com.datastax.oss.driver.internal.core.os.Native.getProcessId(Native.java:82)  \r\n        at com.datastax.oss.driver.api.core.uuid.Uuids.getProcessPiece(Uuids.java:193)        \r\n        at com.datastax.oss.driver.api.core.uuid.Uuids.makeNode(Uuids.java:163)             \r\n        at com.datastax.oss.driver.api.core.uuid.Uuids.makeClockSeqAndNode(Uuids.java:226)                                              \r\n        at com.datastax.oss.driver.api.core.uuid.Uuids.<clinit>(Uuids.java:116)      \r\n        at sun.misc.Unsafe.ensureClassInitialized(Native Method)            \r\n...\r\n```\r\n\r\nLooks like Uuids attempt to get at the PID is leading to a build-time attempt to access the native code which likely isn't available yet.\r\n\r\nI've got an idea for how to work around this, need to do some more testing."", 'commenter': 'absurdfarce'}]"
1439,core/src/main/java/com/datastax/oss/driver/internal/core/os/Native.java,"@@ -35,6 +51,14 @@ public Libc load() {
         return new EmptyLibc();
       }
     }
+
+    private boolean isGraalBuildTime() {","[{'comment': 'Is this https://github.com/datastax/java-driver/pull/1439/files#diff-357e4854869b2e21c38b1b437f11095aR130-R134 dependency always present if one of those settings evaluates to true?', 'commenter': 'tomekl007'}, {'comment': ""Yes, that's correct.  It has to be the case since at build-time the annotations are eval'd for C generation and at run-time the necessary infrastructure for the C calls has to be in place."", 'commenter': 'absurdfarce'}]"
1439,pom.xml,"@@ -377,6 +378,11 @@
         <artifactId>wiremock</artifactId>
         <version>2.25.0</version>
       </dependency>
+      <dependency>
+        <groupId>org.graalvm.sdk</groupId>
+        <artifactId>graal-sdk</artifactId>
+        <version>${graal-sdk.version}</version>
+      </dependency>","[{'comment': 'nit: extracting the version to a property was done for dependencies that are used in OSGi tests (and those are going away soon anyway).\r\n\r\nWhen only one dependency uses the version, we can keep things simple and inline it.', 'commenter': 'olim7t'}, {'comment': ""My working assumption was that we kept the versions in props like this to allow users to override them at build-time.  It seemed like it _might_ be desirable to support something like this for the Graal functionality... although that's just intuition with no real data behind it."", 'commenter': 'absurdfarce'}, {'comment': ""> My working assumption was that we kept the versions in props like this to allow users to override them at build-time\r\n\r\nNo, it's mostly for our internal use (apart from OSGi, one other use was testing against different versions of Guava on 3.x, but that's not a thing anymore).\r\nI don't think many users build from source, and if they start changing dependency versions it's almost already a fork anyway, we don't need to have built-in support for this.\r\n\r\nNo strong opinion here, but as a heads up I might do a pass on all those properties and remove unnecessary ones as part of #1424."", 'commenter': 'olim7t'}]"
1439,core/src/main/java/com/datastax/oss/driver/internal/core/os/GraalLibc.java,"@@ -0,0 +1,38 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.os;
+
+import java.util.Optional;
+
+public class GraalLibc implements Libc {
+
+  @Override
+  public boolean available() {
+    return true;
+  }
+
+  /* Substrate includes a substitution for Linux + Darwin which redefines System.nanoTime() to use
+   * gettimeofday() (unless platform-specific higher-res clocks are available, which is even better). */
+  @Override
+  public Optional<Long> gettimeofday() {
+    return Optional.of(Math.round(System.nanoTime() / 1_000d));","[{'comment': '`gettimeofday()` has microsecond resolution. If `nanoTime()` uses it, it likely returns a multiple of 1000. So an integer division should be sufficient.', 'commenter': 'olim7t'}, {'comment': ""gettimeofday() does indeed have microsecond resolution, but note the comment above about trying to use the higher resolution Linux clocks if they are available.  That's the clock_gettime() call in https://github.com/oracle/graal/blob/master/substratevm/src/com.oracle.svm.core.posix/src/com/oracle/svm/core/posix/linux/LinuxSubstitutions.java#L47-L51.  If that succeeds then we'd need rounding to get to microseconds."", 'commenter': 'absurdfarce'}, {'comment': 'I had missed that, makes sense then ðŸ‘ ', 'commenter': 'olim7t'}, {'comment': '1. What happens if Graal changes its substitution for `System.nanoTime()` to use something that is not a wall-clock time? It looks to me that we are relying on implementation details that could change later.\r\n2. What happens for platforms that do not have a similar substitution, Windows for example? Is this method going to return the actual call to `System.nanoTime()`?', 'commenter': 'adutra'}]"
1439,core/src/main/java/com/datastax/oss/driver/internal/core/os/GraalGetpid.java,"@@ -0,0 +1,45 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.os;
+
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import org.graalvm.nativeimage.c.CContext;
+import org.graalvm.nativeimage.c.function.CFunction;
+
+@CContext(GraalGetpid.Directives.class)
+public class GraalGetpid {
+
+  static class Directives implements CContext.Directives {
+
+    @Override
+    public List<String> getHeaderFiles() {
+
+      return Collections.unmodifiableList(Arrays.asList(""<unistd.h>""));","[{'comment': 'I get an IDE warning suggesting this:\r\n```suggestion\r\n      return Collections.unmodifiableList(Collections.singletonList(""<unistd.h>""));\r\n```', 'commenter': 'olim7t'}, {'comment': ""Hey, I'm happy to do what I can to make the IDE happy. :)"", 'commenter': 'absurdfarce'}]"
1439,core/src/main/java/com/datastax/oss/driver/internal/core/os/Native.java,"@@ -25,8 +25,24 @@
 
   private static class LibcLoader {
 
+    /* These values come from Graal's imageinfo API which aims to offer the ability to detect
+    * when we're in the Graal build/run time via system props.  The maintainers of Graal have
+    * agreed that this API will not change over time.  We reference these props as literals
+    * to avoid introducing a dependency on Graal code for non-Graal users here. */","[{'comment': ""```suggestion\r\n     * when we're in the Graal build/run time via system props.  The maintainers of Graal have\r\n     * agreed that this API will not change over time.  We reference these props as literals\r\n     * to avoid introducing a dependency on Graal code for non-Graal users here. */\r\n```\r\nTo make the formatter happy ðŸ™‚ "", 'commenter': 'olim7t'}, {'comment': ""Like I said earlier, man... I'm all about making the tools happy. ;)"", 'commenter': 'absurdfarce'}]"
1439,core/src/main/java/com/datastax/oss/driver/internal/core/os/Native.java,"@@ -35,6 +51,14 @@ public Libc load() {
         return new EmptyLibc();
       }
     }
+
+    private boolean isGraalBuildTime() {
+      return System.getProperty(GRAAL_STATUS_PROP).equals(GRAAL_BUILDTIME_STATUS);","[{'comment': 'This throws an NPE outside of Graal, `getProperty` returns null if the property does not exist.', 'commenter': 'olim7t'}, {'comment': ':facepalm: Fixed, good catch', 'commenter': 'absurdfarce'}]"
1439,core/src/main/java/com/datastax/oss/driver/internal/core/os/Native.java,"@@ -25,8 +25,19 @@
 
   private static class LibcLoader {
 
+    /* These values come from Graal's imageinfo API which aims to offer the ability to detect
+     * when we're in the Graal build/run time via system props.  The maintainers of Graal have
+     * agreed that this API will not change over time.  We reference these props as literals
+     * to avoid introducing a dependency on Graal code for non-Graal users here. */
+    private static final String GRAAL_STATUS_PROP = ""org.graalvm.nativeimage.imagecode"";
+    private static final String GRAAL_RUNTIME_STATUS = ""runtime"";
+
     public Libc load() {","[{'comment': ""I was considering adding a config option that would allow the user to disable this functionality even for Graal VM apps if they desired.  A user might want to do something like this if they don't want to install the LLVM toolchain for some reason (I'm not sure why that would be an issue but let's assume it is) or they don't want to rely on a newer feature.\r\n\r\nIs this worthwhile?  I can see pretty clear arguments for and against... and the more I think about it the more it seems like it might not be worth it."", 'commenter': 'absurdfarce'}, {'comment': ""Yeah I agree the use case sounds contrived... Let's keep it like this for now, if someone needs it they will ask for it."", 'commenter': 'olim7t'}]"
1439,core/src/main/java/com/datastax/oss/driver/internal/core/os/GraalGetpid.java,"@@ -0,0 +1,45 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.os;
+
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import org.graalvm.nativeimage.c.CContext;
+import org.graalvm.nativeimage.c.function.CFunction;
+
+@CContext(GraalGetpid.Directives.class)
+public class GraalGetpid {
+
+  static class Directives implements CContext.Directives {
+
+    @Override
+    public List<String> getHeaderFiles() {
+
+      return Collections.unmodifiableList(Collections.singletonList(""<unistd.h>""));
+    }
+
+    /* TODO: I don't know that these are required... */","[{'comment': ""After pondering this for a few minutes it became pretty clear that they shouldn't be... it's not like you have to specify these when writing apps in C and it might be weird in the Darwin case.  Regardless things seem to work well without them so I'm removing them."", 'commenter': 'absurdfarce'}]"
1439,core/src/main/java/com/datastax/oss/driver/api/core/uuid/Uuids.java,"@@ -19,6 +19,7 @@
 import com.datastax.oss.driver.internal.core.util.Loggers;","[{'comment': ""Changes in his file are an attempt to handle the UnsatisfiedLinkErrors reported elsewhere on this PR.  By deferring initialization of state necessary to support time-based UUIDs until run-time we prevent the build-time analysis from detecting any attempt to use the linked LLVM native code.  The drawback here is that we've replaced a simple field access with a method call to a Supplier plus some (at the moment manual) unboxing into a primitive, so there is some slight performance degradation here.\r\n\r\n@olim7t are you okay with this?"", 'commenter': 'absurdfarce'}, {'comment': ""Additional note: I originally tried to implement this via some Graal substitutions for the underlying methods in Uuids, but the complexity of that approach escalated quickly.  You'd want to delete the field performing static initialization and inject a new field which uses the supplier-based approach here, but you can't inject static fields.  You also want to preserve access to the underlying functionality (to use in your supplier) without copying code... which proves to be difficult/impossible to do."", 'commenter': 'absurdfarce'}, {'comment': ""I think it is possible to substitute static fields, see `@Alias` annotation and it's usages in the `quarkus` repo"", 'commenter': 'tomekl007'}]"
1443,pom.xml,"@@ -693,7 +698,7 @@ limitations under the License.]]></inlineHeader>
           <verbose>false</verbose>
           <quiet>true</quiet>
           <doclint>all,-missing</doclint>
-          <excludePackageNames>com.datastax.*.driver.internal*</excludePackageNames>
+          <excludePackageNames>com.datastax.*.driver.internal*,com.datastax.oss.driver.shaded.guava.common.primitives</excludePackageNames>","[{'comment': ""Weirdly this was required... without this change Javadoc generation started failing the build:\r\n\r\n```\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] BUILD FAILURE\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] Total time:  28.949 s\r\n[INFO] Finished at: 2020-05-19T13:18:42-05:00\r\n[INFO] ------------------------------------------------------------------------\r\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-javadoc-plugin:3.1.1:jar (attach-javadocs) on project java-driver-core: MavenReportException: Error while generating Javadoc: \r\n[ERROR] Exit code: 1 - javadoc: error - class file for com.datastax.oss.driver.shaded.guava.errorprone.annotations.CheckReturnValue not found\r\n[ERROR] \r\n[ERROR] Command line was: /work/local/graalvm-ce-java8-19.3.1/jre/../bin/javadoc @options @packages\r\n[ERROR] \r\n[ERROR] Refer to the generated Javadoc files in '/work/git/java-driver/core/target/apidocs' dir.\r\n[ERROR] \r\n[ERROR] -> [Help 1]\r\n```\r\n\r\nI'm not sure where the reference to that class is coming from; it isn't introduced by the new dependency added in this change and isn't in the shaded Guava JAR."", 'commenter': 'absurdfarce'}, {'comment': 'I think Graal generates the substituted class inside driver-core own classes, i.e. in `driver-core/target/classes` which is why many tools (javadoc, bundle) are picking it up.\r\n\r\nI think you can actually exclude everything that is shaded here, just in case we generate some more substitutions in the future:\r\n\r\n```\r\n<excludePackageNames>com.datastax.*.driver.internal*,com.datastax.*.driver.shaded*</excludePackageNames>\r\n```\r\n', 'commenter': 'adutra'}]"
1443,core/pom.xml,"@@ -132,6 +132,10 @@
       <artifactId>graal-sdk</artifactId>
       <scope>provided</scope>
     </dependency>
+    <dependency>
+      <groupId>org.graalvm.nativeimage</groupId>
+      <artifactId>svm</artifactId>
+    </dependency>","[{'comment': 'TODO: this could probably also be changed to use a ""provided"" scope', 'commenter': 'absurdfarce'}]"
1443,pom.xml,"@@ -382,6 +382,11 @@
         <artifactId>graal-sdk</artifactId>
         <version>20.0.0</version>
       </dependency>
+      <dependency>
+        <groupId>org.graalvm.nativeimage</groupId>
+        <artifactId>svm</artifactId>
+        <version>20.0.0</version>","[{'comment': 'Is it safe to hardcode the version of the graal here?\r\nThe quarkus takes it from the env runtime variable. \r\nWhat will happen if someone will try to compile it in earlier versions? ', 'commenter': 'tomekl007'}, {'comment': ""This lib is only required to gain access to the annotations needed to implement the substitution.  So long as the Graal native build respects those annotations there shouldn't be an issue.  I've tested this with a toolchain going back to 19.2.1 and it worked without issue so we should be fine.\r\n\r\nNote that Quarkus only supports 19.3.1 so even with 19.2.1 we'd be talking about users writing their own apps for Graal and not using Quarkus."", 'commenter': 'absurdfarce'}]"
1445,core/src/main/java/com/datastax/oss/driver/api/core/config/DriverConfigLoader.java,"@@ -265,6 +266,28 @@ static DriverConfigLoader fromMap(@NonNull OptionsMap source) {
     return new MapBasedDriverConfigLoader(source, source.asRawMap());
   }
 
+  /**
+   * Composes two existing config loaders to form a new one.
+   *
+   * <p>When the driver reads an option, the ""primary"" config will be queried first. If the option
+   * is missing, then it will be looked up in the ""fallback"" config.
+   *
+   * <p>All execution profiles will be surfaced in the new config. If a profile is defined both in
+   * the primary and the fallback config, its options will be merged using the same precedence rules
+   * as described above.
+   *
+   * <p>The new config is reloadable if at least one of the input configs is. If you invoke {@link
+   * DriverConfigLoader#reload()} on the new loader, it will reload whatever is reloadable, or fail
+   * if nothing is. If the input loaders have periodic reloading built-in, each one will reload at
+   * its own pace, and the changes will be reflected in the new config.
+   */
+  @NonNull
+  static DriverConfigLoader compose(
+      @NonNull DriverConfigLoader primaryConfigLoader,
+      @NonNull DriverConfigLoader fallbackConfigLoader) {","[{'comment': ""I was pondering adding a warning in the javadocs.\r\n\r\nThe feature works and I think it's good to have for completeness, but still it feels pretty convoluted."", 'commenter': 'olim7t'}, {'comment': 'While at it, why not introduce a varargs parameter here and allow to compose more than 2 loaders together?', 'commenter': 'adutra'}, {'comment': ""Because it's overkill IMHO.\r\n\r\nFrankly I already have doubts about composing two loaders (per my previous comment). Composing more than two sounds even more wrong.\r\n\r\nYou can always chain `compose()` calls."", 'commenter': 'olim7t'}]"
1445,core/src/main/java/com/datastax/oss/driver/api/core/config/DriverExecutionProfile.java,"@@ -201,4 +219,109 @@ default Duration getDuration(@NonNull DriverOption option, @Nullable Duration de
    */
   @NonNull
   SortedSet<Map.Entry<String, Object>> entrySet();
+
+  @NonNull
+  @Override
+  default DriverExecutionProfile withBoolean(@NonNull DriverOption option, boolean value) {
+    return DerivedExecutionProfile.with(this, option, value);
+  }","[{'comment': ""I realized the logic for derived profiles could be extracted in a generic implementation, so now it doesn't have to be done for every implementation, which is nice.\r\n\r\nI still kept the overrides for the Typesafe implementation, because it can be done a bit better there."", 'commenter': 'olim7t'}]"
1445,core/src/main/java/com/datastax/oss/driver/internal/core/config/composite/CompositeDriverConfig.java,"@@ -0,0 +1,60 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.config.composite;
+
+import com.datastax.oss.driver.api.core.config.DriverConfig;
+import com.datastax.oss.driver.api.core.config.DriverExecutionProfile;
+import com.datastax.oss.driver.shaded.guava.common.collect.Sets;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import java.util.Collections;
+import java.util.Map;
+import java.util.Objects;
+import java.util.concurrent.ConcurrentHashMap;
+
+public class CompositeDriverConfig implements DriverConfig {
+
+  private final DriverConfig primaryConfig;
+  private final DriverConfig fallbackConfig;
+  private final Map<String, CompositeDriverExecutionProfile> profiles = new ConcurrentHashMap<>();
+
+  public CompositeDriverConfig(
+      @NonNull DriverConfig primaryConfig, @NonNull DriverConfig fallbackConfig) {
+    this.primaryConfig = Objects.requireNonNull(primaryConfig);
+    this.fallbackConfig = Objects.requireNonNull(fallbackConfig);
+  }
+
+  @NonNull
+  @Override
+  public DriverExecutionProfile getProfile(@NonNull String profileName) {
+    return profiles.compute(
+        profileName,
+        (k, v) ->
+            (v == null)
+                ? new CompositeDriverExecutionProfile(primaryConfig, fallbackConfig, profileName)
+                : v.refresh());
+  }
+
+  @NonNull
+  @Override
+  public Map<String, ? extends DriverExecutionProfile> getProfiles() {
+    // The map is updated lazily, if we want all the profiles we need to fetch them explicitly
+    for (String name :
+        Sets.union(primaryConfig.getProfiles().keySet(), fallbackConfig.getProfiles().keySet())) {
+      getProfile(name);","[{'comment': 'it looks a bit odd that we are ignoring the result from `getProfile()` call. Can we refactor it to not have side-effects, but to actually use the returned value? Or otherwise, change the method name to smth like: `refreshProfile()`?', 'commenter': 'tomekl007'}, {'comment': ""It is a bit odd, the map acts as a loading cache, we need to fetch to make sure everything is up-to-date, but we end up using the map itself to build the result.\r\n\r\nIt's all private and within 15 lines of code though, I think it will be fine."", 'commenter': 'olim7t'}]"
1445,core/src/main/java/com/datastax/oss/driver/internal/core/config/composite/CompositeDriverConfigLoader.java,"@@ -0,0 +1,85 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.config.composite;
+
+import com.datastax.oss.driver.api.core.config.DriverConfig;
+import com.datastax.oss.driver.api.core.config.DriverConfigLoader;
+import com.datastax.oss.driver.api.core.context.DriverContext;
+import com.datastax.oss.driver.internal.core.util.concurrent.CompletableFutures;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import java.util.Objects;
+import java.util.concurrent.CompletionStage;
+
+public class CompositeDriverConfigLoader implements DriverConfigLoader {
+
+  private final DriverConfigLoader primaryConfigLoader;
+  private final DriverConfigLoader fallbackConfigLoader;
+
+  public CompositeDriverConfigLoader(
+      @NonNull DriverConfigLoader primaryConfigLoader,
+      @NonNull DriverConfigLoader fallbackConfigLoader) {
+    this.primaryConfigLoader = Objects.requireNonNull(primaryConfigLoader);
+    this.fallbackConfigLoader = Objects.requireNonNull(fallbackConfigLoader);
+  }
+
+  @NonNull
+  @Override
+  public DriverConfig getInitialConfig() {
+    DriverConfig primaryConfig = primaryConfigLoader.getInitialConfig();
+    DriverConfig fallbackConfig = fallbackConfigLoader.getInitialConfig();
+    return new CompositeDriverConfig(primaryConfig, fallbackConfig);
+  }
+
+  @Override
+  public void onDriverInit(@NonNull DriverContext context) {
+    fallbackConfigLoader.onDriverInit(context);
+    primaryConfigLoader.onDriverInit(context);
+  }
+
+  @NonNull
+  @Override
+  public CompletionStage<Boolean> reload() {
+    if (!primaryConfigLoader.supportsReloading() && !fallbackConfigLoader.supportsReloading()) {
+      return CompletableFutures.failedFuture(
+          new UnsupportedOperationException(
+              ""Reloading is not supported (this is a composite config, ""
+                  + ""and neither the primary nor the fallback are reloadable)""));
+    } else if (!primaryConfigLoader.supportsReloading()) {
+      return fallbackConfigLoader.reload();
+    } else if (!fallbackConfigLoader.supportsReloading()) {
+      return primaryConfigLoader.reload();
+    } else {
+      return fallbackConfigLoader","[{'comment': '- is there a specific reason for this order of reloading? ( I mean that fallback first, then primary)\r\n- to double-check: the `thenCompose` will execute also if the fallback `reload()` will fail, right?\r\n- Should we log it if there will be an error while reloading? (for example this:\r\nhttps://github.com/datastax/java-driver/pull/1445/files#diff-dc57c9b73b57019d14f9babae5c5d7b2R72-R77)', 'commenter': 'tomekl007'}, {'comment': ""The order does not matter, but you bring a good point about error handling, `thenCompose` does not execute the callback if the first future is failed. I'll fix the code to reload both independently and only combine the result after.\r\n\r\nIf an error occurs, it will be passed to the caller via the resulting stage, so we should not log it."", 'commenter': 'olim7t'}]"
1445,core/src/main/java/com/datastax/oss/driver/internal/core/config/typesafe/TypesafeDriverExecutionProfile.java,"@@ -269,7 +270,7 @@ public DriverExecutionProfile without(@NonNull DriverOption option) {
   @NonNull
   @Override
   public Object getComparisonKey(@NonNull DriverOption option) {
-    // No need to cache this, it's only used for policy initialization
+    // Override the default implementation because we can do much simpler","[{'comment': 'nit: maybe could you expand on that comment? When I am reading code it does not give me a lot of info. I think that there is too little context here.', 'commenter': 'tomekl007'}, {'comment': ""The implementation here is a one-liner, vs 8 lines in the default impl, that's what the comment is referring to."", 'commenter': 'adutra'}, {'comment': ""ðŸ‘ I'll rephrase the comment."", 'commenter': 'olim7t'}]"
1445,core/src/main/java/com/datastax/oss/driver/api/core/config/DriverExecutionProfile.java,"@@ -182,12 +184,28 @@ default Duration getDuration(@NonNull DriverOption option, @Nullable Duration de
   /**
    * Returns a representation of all the child options under a given option.
    *
-   * <p>This is only used to compare configuration sections across profiles, so the actual
-   * implementation does not matter, as long as identical sections (same options with same values,
-   * regardless of order) compare as equal and have the same {@code hashCode()}.
+   * <p>This is used by the driver at initialization time, to compare profiles and determine if it
+   * must create per-profile policies. For example, if two profiles have the same options in the
+   * {@code basic.load-balancing-policy} section, they will share the same policy instance. But if
+   * their options differ, two separate instances will be created.
+   *
+   * <p>The runtime return type does not matter, as long as identical sections (same options with
+   * same values, regardless of order) compare as equal and have the same {@code hashCode()}. The
+   * default implementation builds a map based on the entries from {@link #entrySet()}, it should be
+   * good for most cases.
    */
   @NonNull
-  Object getComparisonKey(@NonNull DriverOption option);
+  default Object getComparisonKey(@NonNull DriverOption option) {","[{'comment': 'The implementation in `MapBasedDriverExecutionProfile` is now identical. I think you can remove it, revapi seems to accept it.', 'commenter': 'adutra'}, {'comment': 'That was my intention, I forgot. Thanks for pointing it out.', 'commenter': 'olim7t'}]"
1445,core/src/main/java/com/datastax/oss/driver/internal/core/config/composite/CompositeDriverConfig.java,"@@ -0,0 +1,60 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.config.composite;
+
+import com.datastax.oss.driver.api.core.config.DriverConfig;
+import com.datastax.oss.driver.api.core.config.DriverExecutionProfile;
+import com.datastax.oss.driver.shaded.guava.common.collect.Sets;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import java.util.Collections;
+import java.util.Map;
+import java.util.Objects;
+import java.util.concurrent.ConcurrentHashMap;
+
+public class CompositeDriverConfig implements DriverConfig {
+
+  private final DriverConfig primaryConfig;
+  private final DriverConfig fallbackConfig;
+  private final Map<String, CompositeDriverExecutionProfile> profiles = new ConcurrentHashMap<>();
+
+  public CompositeDriverConfig(
+      @NonNull DriverConfig primaryConfig, @NonNull DriverConfig fallbackConfig) {","[{'comment': 'Again, if we are going through all this hassle, I feel like we should allow composition of N configs, not just 2.', 'commenter': 'adutra'}]"
1445,core/src/main/java/com/datastax/oss/driver/internal/core/config/composite/CompositeDriverConfigLoader.java,"@@ -0,0 +1,85 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.config.composite;
+
+import com.datastax.oss.driver.api.core.config.DriverConfig;
+import com.datastax.oss.driver.api.core.config.DriverConfigLoader;
+import com.datastax.oss.driver.api.core.context.DriverContext;
+import com.datastax.oss.driver.internal.core.util.concurrent.CompletableFutures;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import java.util.Objects;
+import java.util.concurrent.CompletionStage;
+
+public class CompositeDriverConfigLoader implements DriverConfigLoader {
+
+  private final DriverConfigLoader primaryConfigLoader;
+  private final DriverConfigLoader fallbackConfigLoader;
+
+  public CompositeDriverConfigLoader(
+      @NonNull DriverConfigLoader primaryConfigLoader,
+      @NonNull DriverConfigLoader fallbackConfigLoader) {
+    this.primaryConfigLoader = Objects.requireNonNull(primaryConfigLoader);
+    this.fallbackConfigLoader = Objects.requireNonNull(fallbackConfigLoader);
+  }
+
+  @NonNull
+  @Override
+  public DriverConfig getInitialConfig() {
+    DriverConfig primaryConfig = primaryConfigLoader.getInitialConfig();
+    DriverConfig fallbackConfig = fallbackConfigLoader.getInitialConfig();
+    return new CompositeDriverConfig(primaryConfig, fallbackConfig);
+  }
+
+  @Override
+  public void onDriverInit(@NonNull DriverContext context) {
+    fallbackConfigLoader.onDriverInit(context);
+    primaryConfigLoader.onDriverInit(context);
+  }
+
+  @NonNull
+  @Override
+  public CompletionStage<Boolean> reload() {
+    if (!primaryConfigLoader.supportsReloading() && !fallbackConfigLoader.supportsReloading()) {","[{'comment': ""I find it a bit odd that a reload can be successful if only one of the inner loaders supports reloading, but would fail if both don't. Is there a specific reason why you think this is desirable? Imo the logic would seem easier to grasp if we failed as soon as one loader doesn't support reloading."", 'commenter': 'adutra'}, {'comment': ""I guess it's a matter of interpretation, but the way I see it is that if something has dynamic parts and static parts, then the thing as a whole is still dynamic. Kind of like a dynamic web page, you wouldn't instruct the browser to cache it indefinitely because some of the resources are static."", 'commenter': 'olim7t'}]"
1445,core/src/main/java/com/datastax/oss/driver/internal/core/config/composite/CompositeDriverExecutionProfile.java,"@@ -0,0 +1,196 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.config.composite;
+
+import com.datastax.oss.driver.api.core.config.DriverConfig;
+import com.datastax.oss.driver.api.core.config.DriverExecutionProfile;
+import com.datastax.oss.driver.api.core.config.DriverOption;
+import com.datastax.oss.driver.shaded.guava.common.base.Preconditions;
+import com.datastax.oss.driver.shaded.guava.common.collect.ImmutableSortedSet;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import java.time.Duration;
+import java.util.List;
+import java.util.Map;
+import java.util.Objects;
+import java.util.SortedSet;
+import java.util.TreeSet;
+import java.util.function.BiFunction;
+
+public class CompositeDriverExecutionProfile implements DriverExecutionProfile {
+
+  private final DriverConfig primaryConfig;
+  private final DriverConfig fallbackConfig;
+  private final String profileName;
+
+  private volatile DriverExecutionProfile primaryProfile;
+  private volatile DriverExecutionProfile fallbackProfile;
+
+  public CompositeDriverExecutionProfile(
+      @NonNull DriverConfig primaryConfig,
+      @NonNull DriverConfig fallbackConfig,
+      @NonNull String profileName) {
+    this.primaryConfig = Objects.requireNonNull(primaryConfig);
+    this.fallbackConfig = Objects.requireNonNull(fallbackConfig);
+    this.profileName = Objects.requireNonNull(profileName);
+    refresh();
+  }
+
+  /**
+   * Fetches the underlying profiles again from the two backing configs. This is because some config
+   * implementations support adding/removing profiles at runtime.
+   *
+   * <p>For efficiency reasons this is only done when the user fetches the profile again from the
+   * main config, not every time an option is fetched from the profile.
+   */
+  public CompositeDriverExecutionProfile refresh() {
+    // There's no `hasProfile()` in the public API because it didn't make sense until now. So","[{'comment': 'Can/should we add it?', 'commenter': 'adutra'}, {'comment': ""Outside of this implementation I don't see a lot of value in the general case. I think users create profiles in their configuration and use them, it's not like you get a random opaque config and have to figure out what's in it."", 'commenter': 'olim7t'}]"
1445,core/src/main/java/com/datastax/oss/driver/internal/core/config/composite/CompositeDriverExecutionProfile.java,"@@ -0,0 +1,196 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.config.composite;
+
+import com.datastax.oss.driver.api.core.config.DriverConfig;
+import com.datastax.oss.driver.api.core.config.DriverExecutionProfile;
+import com.datastax.oss.driver.api.core.config.DriverOption;
+import com.datastax.oss.driver.shaded.guava.common.base.Preconditions;
+import com.datastax.oss.driver.shaded.guava.common.collect.ImmutableSortedSet;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import java.time.Duration;
+import java.util.List;
+import java.util.Map;
+import java.util.Objects;
+import java.util.SortedSet;
+import java.util.TreeSet;
+import java.util.function.BiFunction;
+
+public class CompositeDriverExecutionProfile implements DriverExecutionProfile {
+
+  private final DriverConfig primaryConfig;
+  private final DriverConfig fallbackConfig;
+  private final String profileName;
+
+  private volatile DriverExecutionProfile primaryProfile;
+  private volatile DriverExecutionProfile fallbackProfile;
+
+  public CompositeDriverExecutionProfile(
+      @NonNull DriverConfig primaryConfig,
+      @NonNull DriverConfig fallbackConfig,
+      @NonNull String profileName) {
+    this.primaryConfig = Objects.requireNonNull(primaryConfig);
+    this.fallbackConfig = Objects.requireNonNull(fallbackConfig);
+    this.profileName = Objects.requireNonNull(profileName);
+    refresh();","[{'comment': 'Calling a public method from a constructor is error-prone, could you extract the default implementation of `refresh()` to a private method?.', 'commenter': 'adutra'}, {'comment': 'Will do.', 'commenter': 'olim7t'}]"
1446,mapper-runtime/src/main/java/com/datastax/oss/driver/api/mapper/MappedResultProducer.java,"@@ -0,0 +1,31 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.mapper;
+
+import com.datastax.oss.driver.api.core.cql.Statement;
+import com.datastax.oss.driver.api.core.type.reflect.GenericType;
+import com.datastax.oss.driver.api.mapper.entity.EntityHelper;
+
+public interface MappedResultProducer {
+
+  boolean canProduce(GenericType<?> resultType);
+
+  @SuppressWarnings(""TypeParameterUnusedInFormals"")
+  <EntityT> Object execute(","[{'comment': 'For the JAVA-2754 the type that we want to support is `MutinyMappedReactiveResultSet<EntityT>`. The goal is to wrap the `Publisher<>` (in fact the `MappedReactiveResultSet` that extends `Publisher`) within the `MutinyMappedReactiveResultSet` API, see: (https://github.com/datastax/cassandra-quarkus/pull/11/files#diff-81a76325271bcd182f8cf5f0cfc5dd45R43 and https://github.com/datastax/cassandra-quarkus/pull/11/files#diff-833ec0655b43a8e6649bceb026d9a14dR41)\r\n\r\nIdeally, I would like to be able to execute the `executeReactiveAndMap(entityHelper, statement)` and wrap the result within the `new DefaultMutinyMappedReactiveResultSet`. \r\nCurrently, it is possible but requires some code duplication:\r\nhttps://github.com/datastax/cassandra-quarkus/pull/11/commits/c73996f2d0fd401c261bd2ece6c5b013fd73c0d8#diff-e788c33b4c50e6b07ff072f711ce4532R45-R51\r\nand\r\nhttps://github.com/datastax/cassandra-quarkus/pull/11/commits/c73996f2d0fd401c261bd2ece6c5b013fd73c0d8#diff-e788c33b4c50e6b07ff072f711ce4532R36-R38\r\n\r\nI was thinking that maybe the `MappedResultProducer` should extend the `DaoBase` somehow to allow the client to use those methods without duplicating them?', 'commenter': 'tomekl007'}, {'comment': ""Even after inlining callees, `executeReactiveAndMap` is just two lines of trivial code:\r\n```java\r\nReactiveResultSet source = context.getSession().executeReactive(statement);\r\nreturn new DefaultMappedReactiveResultSet<>(source, entityHelper::get);\r\n```\r\nSo there's not much duplication.\r\nBesides, `DaoBase` is internal, I'd rather have implementors use the public session API."", 'commenter': 'olim7t'}]"
1446,integration-tests/src/test/java/com/datastax/oss/driver/mapper/CustomResultTypeIT.java,"@@ -0,0 +1,243 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.mapper;
+
+import static org.assertj.core.api.Assertions.assertThat;
+
+import com.datastax.oss.driver.api.core.CqlIdentifier;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.cql.AsyncResultSet;
+import com.datastax.oss.driver.api.core.cql.Row;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.api.core.cql.Statement;
+import com.datastax.oss.driver.api.core.type.reflect.GenericType;
+import com.datastax.oss.driver.api.mapper.MappedResultProducer;
+import com.datastax.oss.driver.api.mapper.MapperBuilder;
+import com.datastax.oss.driver.api.mapper.MapperContext;
+import com.datastax.oss.driver.api.mapper.annotations.Dao;
+import com.datastax.oss.driver.api.mapper.annotations.DaoFactory;
+import com.datastax.oss.driver.api.mapper.annotations.DaoKeyspace;
+import com.datastax.oss.driver.api.mapper.annotations.Delete;
+import com.datastax.oss.driver.api.mapper.annotations.Insert;
+import com.datastax.oss.driver.api.mapper.annotations.Mapper;
+import com.datastax.oss.driver.api.mapper.annotations.Query;
+import com.datastax.oss.driver.api.mapper.annotations.Select;
+import com.datastax.oss.driver.api.mapper.annotations.Update;
+import com.datastax.oss.driver.api.mapper.entity.EntityHelper;
+import com.datastax.oss.driver.api.testinfra.ccm.CcmRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionRule;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import com.datastax.oss.driver.shaded.guava.common.util.concurrent.Futures;
+import com.datastax.oss.driver.shaded.guava.common.util.concurrent.ListenableFuture;
+import com.datastax.oss.driver.shaded.guava.common.util.concurrent.SettableFuture;
+import java.util.UUID;
+import java.util.concurrent.ExecutionException;
+import org.junit.BeforeClass;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.junit.rules.RuleChain;
+import org.junit.rules.TestRule;
+
+@Category(ParallelizableTests.class)
+public class CustomResultTypeIT extends InventoryITBase {
+
+  private static final CcmRule CCM_RULE = CcmRule.getInstance();
+
+  private static final SessionRule<CqlSession> SESSION_RULE = SessionRule.builder(CCM_RULE).build();
+
+  @ClassRule
+  public static final TestRule CHAIN = RuleChain.outerRule(CCM_RULE).around(SESSION_RULE);
+
+  private static ProductDao dao;
+
+  @BeforeClass
+  public static void setup() {
+    CqlSession session = SESSION_RULE.session();
+
+    for (String query : createStatements(CCM_RULE)) {
+      session.execute(
+          SimpleStatement.builder(query).setExecutionProfile(SESSION_RULE.slowProfile()).build());
+    }
+
+    InventoryMapper mapper =
+        InventoryMapper.builder(SESSION_RULE.session())
+            .withResultProducers(
+                // Note that order matters, both producers operate on ListenableFuture<Something>,
+                // the most specific must come first.
+                new VoidListenableFutureProducer(), new SingleEntityListenableFutureProducer())
+            .build();
+    dao = mapper.productDao(SESSION_RULE.keyspace());
+  }
+
+  @Test
+  public void should_use_custom_result_for_insert_method()
+      throws ExecutionException, InterruptedException {
+
+    ListenableFuture<Void> insertFuture = dao.insert(FLAMETHROWER);
+    insertFuture.get();
+
+    Row row = SESSION_RULE.session().execute(""SELECT id FROM product"").one();
+    UUID insertedId = row.getUuid(0);
+    assertThat(insertedId).isEqualTo(FLAMETHROWER.getId());
+  }
+
+  @Test
+  public void should_use_custom_result_for_select_method()
+      throws ExecutionException, InterruptedException {
+
+    dao.insert(FLAMETHROWER).get();
+
+    ListenableFuture<Product> selectFuture = dao.select(FLAMETHROWER.getId());
+    Product selectedProduct = selectFuture.get();
+    assertThat(selectedProduct).isEqualTo(FLAMETHROWER);
+  }
+
+  @Test
+  public void should_use_custom_result_for_update_method()
+      throws ExecutionException, InterruptedException {
+
+    dao.insert(FLAMETHROWER).get();
+
+    Product productToUpdate = dao.select(FLAMETHROWER.getId()).get();
+    productToUpdate.setDescription(""changed description"");
+    ListenableFuture<Void> updateFuture = dao.update(productToUpdate);
+    updateFuture.get();
+
+    Product selectedProduct = dao.select(FLAMETHROWER.getId()).get();
+    assertThat(selectedProduct.getDescription()).isEqualTo(""changed description"");
+  }
+
+  @Test
+  public void should_use_custom_result_for_delete_method()
+      throws ExecutionException, InterruptedException {
+    dao.insert(FLAMETHROWER).get();
+
+    ListenableFuture<Void> deleteFuture = dao.delete(FLAMETHROWER);
+    deleteFuture.get();
+
+    Product selectedProduct = dao.select(FLAMETHROWER.getId()).get();
+    assertThat(selectedProduct).isNull();
+  }
+
+  @Test
+  public void should_use_custom_result_for_query_method()
+      throws ExecutionException, InterruptedException {
+    dao.insert(FLAMETHROWER).get();
+
+    ListenableFuture<Void> deleteFuture = dao.deleteById(FLAMETHROWER.getId());
+    deleteFuture.get();
+
+    Product selectedProduct = dao.select(FLAMETHROWER.getId()).get();
+    assertThat(selectedProduct).isNull();
+  }
+
+  public interface ListenableFutureDao<EntityT> {
+
+    @Select
+    ListenableFuture<EntityT> select(UUID id);
+
+    @Update
+    ListenableFuture<Void> update(EntityT entity);
+
+    @Insert
+    ListenableFuture<Void> insert(EntityT entity);
+
+    @Delete
+    ListenableFuture<Void> delete(EntityT entity);
+  }
+
+  @Dao
+  public interface ProductDao extends ListenableFutureDao<Product> {
+
+    // We could do this easier with @Delete, but the goal here is to test @Query
+    @Query(""DELETE FROM ${keyspaceId}.product WHERE id = :id"")
+    ListenableFuture<Void> deleteById(UUID id);
+  }
+
+  @Mapper
+  public interface InventoryMapper {
+
+    @DaoFactory
+    ProductDao productDao(@DaoKeyspace CqlIdentifier keyspace);
+
+    static MapperBuilder<InventoryMapper> builder(CqlSession session) {
+      return new CustomResultTypeIT_InventoryMapperBuilder(session);
+    }
+  }
+
+  public abstract static class ListenableFutureProducer implements MappedResultProducer {","[{'comment': 'For JAVA-2754 we need to provide producers for a client to use them in the mapper:\r\nhttps://github.com/datastax/cassandra-quarkus/pull/11/commits/c73996f2d0fd401c261bd2ece6c5b013fd73c0d8#diff-e788c33b4c50e6b07ff072f711ce4532R28\r\nTo make it work, the client will need to pass the `GenericType` of a required entity class to the Producer instance:\r\nhttps://github.com/datastax/cassandra-quarkus/pull/11/commits/c73996f2d0fd401c261bd2ece6c5b013fd73c0d8#diff-3f2336e51cf1fcd26707d79b6d633cb5R38-R41\r\nI wonder if we can simplify it somehow (make `MappedResultProducer` generic?) or are we ok with this requirement of using `GenericType` in the clients of `MappedResultProducer` classes?\r\n\r\nOr using the `resultType.isSubtypeOf` of `?` is a required way of using this:\r\n- https://github.com/datastax/cassandra-quarkus/pull/11/commits/83e47bb8f0e30703f7267b1643d4fcbdec267e21#diff-e788c33b4c50e6b07ff072f711ce4532R39\r\n- https://github.com/datastax/cassandra-quarkus/pull/11/commits/83e47bb8f0e30703f7267b1643d4fcbdec267e21#diff-e788c33b4c50e6b07ff072f711ce4532R30\r\n\r\n?', 'commenter': 'tomekl007'}, {'comment': ""The client should not register a `MutinyMappedReactiveResultSet<Fruit>`, but instead a producer that can handle a reactive result set of _any entity_. See `CustomResultTypeIT.SingleEntityListenableFutureProducer` for an example.\r\n\r\nThe idea is that we would provide those Mutiny producers out of the box with the Quarkus extension, users should not have to implement them.\r\n\r\nPS:  There's not any good way to make the interface generic, and at the end of the day it all happens at runtime anyway, there would always be an unchecked cast somewhere."", 'commenter': 'olim7t'}]"
1446,mapper-runtime/src/main/java/com/datastax/oss/driver/api/mapper/MappedResultProducer.java,"@@ -0,0 +1,31 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.mapper;
+
+import com.datastax.oss.driver.api.core.cql.Statement;
+import com.datastax.oss.driver.api.core.type.reflect.GenericType;
+import com.datastax.oss.driver.api.mapper.entity.EntityHelper;
+
+public interface MappedResultProducer {","[{'comment': 'NIt is the `Mapped` a proper prefix here? It can be used also for not mapped results sets, see this conversion from `CompletionStage` to `Uni`:\r\nhttps://github.com/datastax/cassandra-quarkus/pull/11/commits/ea171cb528a3dc5d75f8e2062b765f0b897b688e#diff-91bdea213ffff0bc78a4bc81a5f6ed1fR37', 'commenter': 'tomekl007'}, {'comment': 'I was meant as ""a result produced by the mapper"".\r\nMaybe `MapperResultProducer` would be a better name. I\'ll give it more thought, I wasn\'t completely satisfied with naming.', 'commenter': 'olim7t'}]"
1446,integration-tests/src/test/java/com/datastax/oss/driver/mapper/CustomResultTypeIT.java,"@@ -0,0 +1,243 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.mapper;
+
+import static org.assertj.core.api.Assertions.assertThat;
+
+import com.datastax.oss.driver.api.core.CqlIdentifier;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.cql.AsyncResultSet;
+import com.datastax.oss.driver.api.core.cql.Row;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.api.core.cql.Statement;
+import com.datastax.oss.driver.api.core.type.reflect.GenericType;
+import com.datastax.oss.driver.api.mapper.MappedResultProducer;
+import com.datastax.oss.driver.api.mapper.MapperBuilder;
+import com.datastax.oss.driver.api.mapper.MapperContext;
+import com.datastax.oss.driver.api.mapper.annotations.Dao;
+import com.datastax.oss.driver.api.mapper.annotations.DaoFactory;
+import com.datastax.oss.driver.api.mapper.annotations.DaoKeyspace;
+import com.datastax.oss.driver.api.mapper.annotations.Delete;
+import com.datastax.oss.driver.api.mapper.annotations.Insert;
+import com.datastax.oss.driver.api.mapper.annotations.Mapper;
+import com.datastax.oss.driver.api.mapper.annotations.Query;
+import com.datastax.oss.driver.api.mapper.annotations.Select;
+import com.datastax.oss.driver.api.mapper.annotations.Update;
+import com.datastax.oss.driver.api.mapper.entity.EntityHelper;
+import com.datastax.oss.driver.api.testinfra.ccm.CcmRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionRule;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import com.datastax.oss.driver.shaded.guava.common.util.concurrent.Futures;
+import com.datastax.oss.driver.shaded.guava.common.util.concurrent.ListenableFuture;
+import com.datastax.oss.driver.shaded.guava.common.util.concurrent.SettableFuture;
+import java.util.UUID;
+import java.util.concurrent.ExecutionException;
+import org.junit.BeforeClass;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.junit.rules.RuleChain;
+import org.junit.rules.TestRule;
+
+@Category(ParallelizableTests.class)
+public class CustomResultTypeIT extends InventoryITBase {
+
+  private static final CcmRule CCM_RULE = CcmRule.getInstance();
+
+  private static final SessionRule<CqlSession> SESSION_RULE = SessionRule.builder(CCM_RULE).build();
+
+  @ClassRule
+  public static final TestRule CHAIN = RuleChain.outerRule(CCM_RULE).around(SESSION_RULE);
+
+  private static ProductDao dao;
+
+  @BeforeClass
+  public static void setup() {
+    CqlSession session = SESSION_RULE.session();
+
+    for (String query : createStatements(CCM_RULE)) {
+      session.execute(
+          SimpleStatement.builder(query).setExecutionProfile(SESSION_RULE.slowProfile()).build());
+    }
+
+    InventoryMapper mapper =
+        InventoryMapper.builder(SESSION_RULE.session())
+            .withResultProducers(","[{'comment': 'Can we abstract away that `withResultProducers`? I am thinking that it could be more UX friendly if their will be set on the `*Mapper` class itself.\r\n\r\nFor JAVA-2754 the use case is that if someone would like to use provided mapped producers (`toMulti` and `toUni`) he needs to remember that and call `withResultProducer` every time the Mapper is created:\r\nhttps://github.com/datastax/cassandra-quarkus/pull/11/files#diff-3f2336e51cf1fcd26707d79b6d633cb5R36-R38 \r\nThere could be a use case for more than 2 mapped producers as well.\r\n\r\nIt could be alleviated by providing a way for a user to just extends their mapper (via `@Mapper` or just class extends) so instead of this:\r\n```\r\n@Mapper\r\npublic interface FruitMapper {\r\n}\r\n```\r\nand then when constructing this:\r\n```\r\n return new FruitMapperBuilder(cqlSession)\r\n        .withResultProducers(new MutinyMappedResultProducer(), new UniMappedResultSetProducer())\r\n        .build();\r\n```\r\nthe quarkus framework would provide a class \r\n```\r\ninterface ResultSetProducers{\r\n  MappedResultSetProducer producers();\r\n}\r\nclass QuarkusResultSetProducers implement ResultSetProducers{\r\n    @Override\r\n    MappedResultSetProducer producers(){\r\n       return Arrays.ofList(new MutinyMappedResultProducer(), new UniMappedResultSetProducer())\r\n    }\r\n}\r\n```\r\nand then a user when creating a Mapper could do \r\n```\r\n@Mapper(withProducers = QuarkusResultSetProducers.class)\r\npublic interface FruitMapper {\r\n}\r\n```\r\nor:\r\n```\r\npublic interface FruitMapper extend QuarkusResultSetProducers{\r\n}\r\n```\r\nThe object-mapper would need to use the returned `producers()` instead of the builder method `withResultProducers`.\r\nThen there will be no need for the client to remember and add all mappedProducers that he needs to use in:\r\n```\r\n.withResultProducers(new MutinyMappedResultProducer(), new UniMappedResultSetProducer())\r\n```\r\n\r\nThe pros of this solution that I see are:\r\n- the mappedProducers are encapsulated in the quarkus framework.\r\n- adding a new mapped producer in the future would require no work from the client-side.\r\n\r\n', 'commenter': 'tomekl007'}, {'comment': 'See my other answer, we can write producers that will handle any kind of entity.', 'commenter': 'olim7t'}, {'comment': 'yes, I agree but this is not related. We still need to provide two `mapperResultsProducers` and my suggestion was to hide it from the client:\r\n(this call `.withResultProducers(new MutinyMappedResultProducer(), new UniMappedResultSetProducer())`)', 'commenter': 'tomekl007'}, {'comment': ""Aren't result producers good candidates for loading through the Service Loader API?"", 'commenter': 'adutra'}, {'comment': ""Sorry @tomekl007 I glanced over your initial comment a bit too quickly. The idea of a `@Mapper` attribute is interesting, but I find the service loader idea even more compelling: we could publish specialized modules like `mapper-mutiny`,  `mapper-rxjava`, etc, and all users would have to do is put them in their classpath. I'm going to explore that route."", 'commenter': 'olim7t'}]"
1446,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoReturnTypeKind.java,"@@ -32,9 +33,10 @@
    *
    * @param methodBuilder the method to add the code to.
    * @param helperFieldName the name of the helper for entity conversions (might not get used for
-   *     certain kinds, in that case it's ok to pass null).
+   * @param returnTypeName","[{'comment': 'The previous param description was accidentally truncated it seems.', 'commenter': 'adutra'}]"
1446,mapper-processor/src/test/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoDeleteMethodGeneratorTest.java,"@@ -163,4 +171,32 @@ public void should_warn_when_non_bind_marker_has_cql_name() {
             .addModifiers(Modifier.PUBLIC, Modifier.ABSTRACT)
             .build());
   }
+
+  @Test
+  public void should_not_fail_on_unsupported_result_when_custom_results_enabled() {
+
+    MethodSpec methodSpec =
+        MethodSpec.methodBuilder(""delete"")
+            .addAnnotation(Delete.class)
+            .addModifiers(Modifier.PUBLIC, Modifier.ABSTRACT)
+            .addParameter(ENTITY_CLASS_NAME, ""entity"")
+            .returns(Integer.class) // not a built-in return type
+            .build();
+    TypeSpec daoSpec =
+        TypeSpec.interfaceBuilder(ClassName.get(""test"", ""ProductDao""))
+            .addModifiers(Modifier.PUBLIC)
+            .addAnnotation(Dao.class)
+            .addMethod(methodSpec)
+            .build();
+
+    for (List<String> compilerOptions :
+        ImmutableList.of(
+            ImmutableList.of(""-Acom.datastax.oss.driver.mapper.customResults.enabled=true""),
+            // The option default to true, so it should also work without explicit options:","[{'comment': 'Nit: defaults to true.', 'commenter': 'adutra'}]"
1446,mapper-runtime/src/main/java/com/datastax/oss/driver/internal/mapper/DefaultMapperContext.java,"@@ -140,6 +150,20 @@ public NameConverter getNameConverter(Class<? extends NameConverter> converterCl
     return customState;
   }
 
+  @NonNull
+  @Override
+  public MapperResultProducer getResultProducer(GenericType<?> resultToProduce) {
+    for (MapperResultProducer resultProducer : resultProducers) {","[{'comment': ""This is a bit inefficient, shouldn't we cache the results?"", 'commenter': 'adutra'}]"
1446,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DaoMethodGenerator.java,"@@ -234,4 +244,30 @@ protected boolean isFromClassFile() {
     TypeElement enclosingElement = (TypeElement) methodElement.getEnclosingElement();
     return Reflection.loadClass(null, enclosingElement.getQualifiedName().toString()) != null;
   }
+
+  /**
+   * Common pattern for CRUD methods that build a bound statement, execute it and convert the result
+   * into a target type.
+   *
+   * @param createStatementBlock the code that creates the statement. It must store it into a
+   *     variable named ""boundStatement"".
+   */
+  protected Optional<MethodSpec> crudMethod(
+      CodeBlock.Builder createStatementBlock, DaoReturnType returnType, String helperFieldName) {
+
+    MethodSpec.Builder method = GeneratedCodePatterns.override(methodElement, typeParameters);
+    TypeName returnTypeName = null;
+    if (returnType.getKind() == CUSTOM) {","[{'comment': ""it's a bit odd the the `producer` variable declaration is generated here, but only accessed by code generated inside `addExecuteStatement`. Instead of special-casing CUSTOM here, can't you generate the `producer` variable delcaration inside `DefaultDaoReturnTypeKind.CUSTOM.addExecuteStatement()` ?"", 'commenter': 'adutra'}, {'comment': 'The output of `addExecuteStatement()` is enclosed in the `try` block. We also need the producer in the `catch` block to wrap errors.', 'commenter': 'olim7t'}]"
1446,mapper-runtime/src/main/java/com/datastax/oss/driver/api/mapper/MapperResultProducer.java,"@@ -0,0 +1,107 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.mapper;
+
+import com.datastax.oss.driver.api.core.cql.Statement;
+import com.datastax.oss.driver.api.core.data.GettableByName;
+import com.datastax.oss.driver.api.core.type.reflect.GenericType;
+import com.datastax.oss.driver.api.mapper.entity.EntityHelper;
+import java.util.concurrent.CompletionStage;
+
+/**
+ * A component that can be plugged into the object mapper, in order to return custom result types
+ * from DAO methods.
+ *
+ * <p>For example, this could be used to substitute a 3rd-party future implementation for {@link
+ * CompletionStage}:
+ *
+ * <pre>
+ * public class CustomFutureProducer implements MapperResultProducer {
+ *   ...
+ * }
+ * </pre>
+ *
+ * <p>Producers are registered at construction time with {@link
+ * MapperBuilder#withResultProducers(MapperResultProducer...)}:
+ *
+ * <pre>
+ * InventoryMapper inventoryMapper = new InventoryMapperBuilder(session)
+ *     .withResultProducers(new CustomFutureProducer())
+ *     .build();
+ * </pre>
+ *
+ * DAO methods can then use the new type:
+ *
+ * <pre>
+ * &#64;Dao
+ * public interface ProductDao {
+ *   &#64;Select
+ *   CustomFuture&lt;Product&gt; findById(UUID productId);
+ * }
+ * </pre>
+ *
+ * See the javadocs of the methods in this interface for more explanations.
+ */
+public interface MapperResultProducer {
+
+  /**
+   * Checks if this producer can handle a particular result type.
+   *
+   * <p>This will be invoked at runtime to select a producer: if a DAO method declares a return type
+   * that is not supported natively, then the mapper generates an implementation which, for every
+   * invocation, iterates through all the producers <em>in the order that they were registered</em>,
+   * and picks the first one where {@code canProduce()} returns true.
+   *
+   * @param resultType the DAO method's declared return type. If checking the top-level type is
+   *     sufficient, then {@link GenericType#getRawType()} should do the trick. If you need to
+   *     recurse into the type arguments, call {@link GenericType#getType()} and use the {@code
+   *     java.lang.reflect} APIs.
+   */
+  boolean canProduce(GenericType<?> resultType);
+
+  /**
+   * Executes the statement generated by the mapper, and converts the result to the expected type.
+   *
+   * <p>This will be executed at runtime, every time the DAO method is called.
+   *
+   * @param statement the statement, ready to execute: the mapper has already bound all the values,
+   *     and set all the necessary attributes (consistency, page size, etc).
+   * @param context the context in which the DAO method is executed. In particular, this is how you
+   *     get access to the {@linkplain MapperContext#getSession() session}.
+   * @param entityHelper if the type to produce contains a mapped entity (e.g. {@code
+   *     ListenableFuture<Product>}), an instance of the helper class to manipulate that entity. In
+   *     particular, {@link EntityHelper#get(GettableByName) entityHelper.get()} allows you to
+   *     convert rows into entity instances. If the type to produce does not contain an entity, this
+   *     will be {@code null}.
+   * @return the object to return from the DAO method. This must match the type that this producer
+   *     was selected for, there will be an unchecked cast at runtime.
+   */
+  @SuppressWarnings(""TypeParameterUnusedInFormals"")
+  <EntityT> Object execute(","[{'comment': 'Maybe add nullability annotations in this interface?', 'commenter': 'adutra'}, {'comment': 'Right, I forgot.', 'commenter': 'olim7t'}]"
1446,mapper-runtime/src/main/java/com/datastax/oss/driver/api/mapper/MapperResultProducer.java,"@@ -0,0 +1,107 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.mapper;
+
+import com.datastax.oss.driver.api.core.cql.Statement;
+import com.datastax.oss.driver.api.core.data.GettableByName;
+import com.datastax.oss.driver.api.core.type.reflect.GenericType;
+import com.datastax.oss.driver.api.mapper.entity.EntityHelper;
+import java.util.concurrent.CompletionStage;
+
+/**
+ * A component that can be plugged into the object mapper, in order to return custom result types
+ * from DAO methods.
+ *
+ * <p>For example, this could be used to substitute a 3rd-party future implementation for {@link
+ * CompletionStage}:
+ *
+ * <pre>
+ * public class CustomFutureProducer implements MapperResultProducer {
+ *   ...
+ * }
+ * </pre>
+ *
+ * <p>Producers are registered at construction time with {@link
+ * MapperBuilder#withResultProducers(MapperResultProducer...)}:
+ *
+ * <pre>
+ * InventoryMapper inventoryMapper = new InventoryMapperBuilder(session)
+ *     .withResultProducers(new CustomFutureProducer())
+ *     .build();
+ * </pre>
+ *
+ * DAO methods can then use the new type:
+ *
+ * <pre>
+ * &#64;Dao
+ * public interface ProductDao {
+ *   &#64;Select
+ *   CustomFuture&lt;Product&gt; findById(UUID productId);
+ * }
+ * </pre>
+ *
+ * See the javadocs of the methods in this interface for more explanations.
+ */
+public interface MapperResultProducer {
+
+  /**
+   * Checks if this producer can handle a particular result type.
+   *
+   * <p>This will be invoked at runtime to select a producer: if a DAO method declares a return type
+   * that is not supported natively, then the mapper generates an implementation which, for every
+   * invocation, iterates through all the producers <em>in the order that they were registered</em>,
+   * and picks the first one where {@code canProduce()} returns true.
+   *
+   * @param resultType the DAO method's declared return type. If checking the top-level type is
+   *     sufficient, then {@link GenericType#getRawType()} should do the trick. If you need to
+   *     recurse into the type arguments, call {@link GenericType#getType()} and use the {@code
+   *     java.lang.reflect} APIs.
+   */
+  boolean canProduce(GenericType<?> resultType);
+
+  /**
+   * Executes the statement generated by the mapper, and converts the result to the expected type.
+   *
+   * <p>This will be executed at runtime, every time the DAO method is called.
+   *
+   * @param statement the statement, ready to execute: the mapper has already bound all the values,
+   *     and set all the necessary attributes (consistency, page size, etc).
+   * @param context the context in which the DAO method is executed. In particular, this is how you
+   *     get access to the {@linkplain MapperContext#getSession() session}.
+   * @param entityHelper if the type to produce contains a mapped entity (e.g. {@code
+   *     ListenableFuture<Product>}), an instance of the helper class to manipulate that entity. In
+   *     particular, {@link EntityHelper#get(GettableByName) entityHelper.get()} allows you to
+   *     convert rows into entity instances. If the type to produce does not contain an entity, this
+   *     will be {@code null}.
+   * @return the object to return from the DAO method. This must match the type that this producer
+   *     was selected for, there will be an unchecked cast at runtime.
+   */
+  @SuppressWarnings(""TypeParameterUnusedInFormals"")
+  <EntityT> Object execute(
+      Statement<?> statement, MapperContext context, EntityHelper<EntityT> entityHelper);
+
+  /**
+   * Surfaces any error encountered in the DAO method (either in the generated mapper code that
+   * builds the statement, or during invocation of {@link #execute}).
+   *
+   * <p>For some result types, it is expected that errors will be wrapped in some sort of container
+   * instead of thrown directly; for example a failed future or publisher.
+   *
+   * <p>If rethrowing is the right thing to do, then it is perfectly fine to do so from this method
+   * (note however that you'll need to wrap checked exceptions).
+   */
+  Object wrapError(Throwable error);","[{'comment': ""Wouldn't it be a bit more user-friendly if this method declared to throw `Exception`? We could then catch it in call sites and convert the thrown error into something more appropriate."", 'commenter': 'adutra'}, {'comment': ""Great idea, I hadn't thought of doing that in the generated code. We could even check if the DAO method declares any exceptions, and rethrow those without wrapping."", 'commenter': 'olim7t'}]"
1446,mapper-runtime/src/main/java/com/datastax/oss/driver/internal/mapper/DefaultMapperContext.java,"@@ -155,4 +184,12 @@ private static NameConverter buildNameConverter(Class<? extends NameConverter> c
           e);
     }
   }
+
+  private static List<MapperResultProducer> getResultProducers() {
+    ImmutableList.Builder<MapperResultProducer> result = ImmutableList.builder();
+    ServiceLoader<MapperResultProducerService> loader =
+        ServiceLoader.load(MapperResultProducerService.class);
+    loader.iterator().forEachRemaining(provider -> result.addAll(provider.getProducers()));","[{'comment': 'Do we really need the `MapperResultProducerService` interface? We could load instances of `MapperResultProducer` directly using the Service Loader API.', 'commenter': 'adutra'}, {'comment': ""Indeed, I hadn't thought of that. If the order in the descriptor is deterministic (it probably is), then I'm fine loading producers directly. I'll check."", 'commenter': 'olim7t'}, {'comment': ""The order is not formally defined by the `ServiceLoader` javadocs.\r\n\r\nIn practice, the Oracle JDK implementation parses the lines and puts them in a list, so they come out in the original order. But to play devil's advocate, it's not unrealistic to imagine another implementation that would use a hash set in order to eliminate duplicates (which are supposed to be skipped). \r\n\r\nSo just to err on the safe side I think it's better to keep the top-level service interface, it doesn't add much complexity."", 'commenter': 'olim7t'}]"
1446,mapper-runtime/src/main/java/com/datastax/oss/driver/api/mapper/result/MapperResultProducerService.java,"@@ -0,0 +1,36 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.mapper.result;
+
+/**
+ * Provides the custom mapper result types that will be used in an application.
+ *
+ * <p>This class is loaded with the Java Service Provider Interface mechanism, you must reference it
+ * via a service descriptor: create a file {@code
+ * META-INF/services/com.datastax.oss.driver.api.mapper.result.MapperResultProducerService}, with a
+ * single line that contains the name of the implementing class.
+ */
+public interface MapperResultProducerService {","[{'comment': 'See my other comment, not sure if this interface is required.', 'commenter': 'adutra'}, {'comment': ""Oh it just occurred to me: is it because you want to have a deterministic order when loading producers? In this case, OK, I'm fine with this interface."", 'commenter': 'adutra'}]"
1446,mapper-runtime/src/main/java/com/datastax/oss/driver/api/mapper/result/MapperResultProducerService.java,"@@ -0,0 +1,36 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.mapper.result;
+
+/**
+ * Provides the custom mapper result types that will be used in an application.
+ *
+ * <p>This class is loaded with the Java Service Provider Interface mechanism, you must reference it
+ * via a service descriptor: create a file {@code
+ * META-INF/services/com.datastax.oss.driver.api.mapper.result.MapperResultProducerService}, with a
+ * single line that contains the name of the implementing class.","[{'comment': 'You can have more than one line, if there are more than one implementations to load.', 'commenter': 'adutra'}]"
1446,mapper-runtime/src/main/java/com/datastax/oss/driver/api/mapper/MapperContext.java,"@@ -83,4 +85,18 @@
    */
   @NonNull
   Map<Object, Object> getCustomState();
+
+  /**
+   * Returns a component that will execute a statement and convert it into a custom result of the
+   * given type.
+   *
+   * <p>These components must be registered at build time with {@link
+   * MapperBuilder#withResultProducers(MapperResultProducer...)}.","[{'comment': 'This method was removed.', 'commenter': 'adutra'}]"
1446,mapper-processor/src/main/java/com/datastax/oss/driver/internal/mapper/processor/dao/DefaultDaoReturnTypeKind.java,"@@ -18,236 +18,433 @@
 import com.datastax.dse.driver.internal.core.cql.reactive.FailedReactiveResultSet;
 import com.datastax.dse.driver.internal.mapper.reactive.FailedMappedReactiveResultSet;
 import com.datastax.oss.driver.internal.core.util.concurrent.CompletableFutures;
+import com.datastax.oss.driver.internal.mapper.processor.util.generation.GeneratedCodePatterns;
+import com.datastax.oss.driver.shaded.guava.common.base.Throwables;
 import com.squareup.javapoet.CodeBlock;
+import com.squareup.javapoet.TypeName;
+import java.util.Map;
+import javax.lang.model.element.ExecutableElement;
+import javax.lang.model.element.Name;
+import javax.lang.model.element.TypeElement;
+import javax.lang.model.type.TypeMirror;
 
 public enum DefaultDaoReturnTypeKind implements DaoReturnTypeKind {
   VOID {
     @Override
-    public void addExecuteStatement(CodeBlock.Builder methodBuilder, String helperFieldName) {
+    public void addExecuteStatement(
+        CodeBlock.Builder methodBuilder,
+        String helperFieldName,
+        ExecutableElement methodElement,
+        Map<Name, TypeElement> typeParameters) {
       // Note that the execute* methods in the generated code are defined in DaoBase
       methodBuilder.addStatement(""execute(boundStatement)"");
     }
 
     @Override
-    public CodeBlock wrapWithErrorHandling(CodeBlock innerBlock) {
+    public CodeBlock wrapWithErrorHandling(
+        CodeBlock innerBlock,
+        ExecutableElement methodElement,
+        Map<Name, TypeElement> typeParameters) {
       return innerBlock;
     }
   },
   BOOLEAN {
     @Override
-    public void addExecuteStatement(CodeBlock.Builder methodBuilder, String helperFieldName) {
+    public void addExecuteStatement(
+        CodeBlock.Builder methodBuilder,
+        String helperFieldName,
+        ExecutableElement methodElement,
+        Map<Name, TypeElement> typeParameters) {
       methodBuilder.addStatement(""return executeAndMapWasAppliedToBoolean(boundStatement)"");
     }
 
     @Override
-    public CodeBlock wrapWithErrorHandling(CodeBlock innerBlock) {
+    public CodeBlock wrapWithErrorHandling(
+        CodeBlock innerBlock,
+        ExecutableElement methodElement,
+        Map<Name, TypeElement> typeParameters) {
       return innerBlock;
     }
   },
   LONG {
     @Override
-    public void addExecuteStatement(CodeBlock.Builder methodBuilder, String helperFieldName) {
+    public void addExecuteStatement(
+        CodeBlock.Builder methodBuilder,
+        String helperFieldName,
+        ExecutableElement methodElement,
+        Map<Name, TypeElement> typeParameters) {
       methodBuilder.addStatement(""return executeAndMapFirstColumnToLong(boundStatement)"");
     }
 
     @Override
-    public CodeBlock wrapWithErrorHandling(CodeBlock innerBlock) {
+    public CodeBlock wrapWithErrorHandling(
+        CodeBlock innerBlock,
+        ExecutableElement methodElement,
+        Map<Name, TypeElement> typeParameters) {
       return innerBlock;
     }
   },
   ROW {
     @Override
-    public void addExecuteStatement(CodeBlock.Builder methodBuilder, String helperFieldName) {
+    public void addExecuteStatement(
+        CodeBlock.Builder methodBuilder,
+        String helperFieldName,
+        ExecutableElement methodElement,
+        Map<Name, TypeElement> typeParameters) {
       methodBuilder.addStatement(""return executeAndExtractFirstRow(boundStatement)"");
     }
 
     @Override
-    public CodeBlock wrapWithErrorHandling(CodeBlock innerBlock) {
+    public CodeBlock wrapWithErrorHandling(
+        CodeBlock innerBlock,
+        ExecutableElement methodElement,
+        Map<Name, TypeElement> typeParameters) {
       return innerBlock;
     }
   },
   ENTITY {
     @Override
-    public void addExecuteStatement(CodeBlock.Builder methodBuilder, String helperFieldName) {
+    public void addExecuteStatement(
+        CodeBlock.Builder methodBuilder,
+        String helperFieldName,
+        ExecutableElement methodElement,
+        Map<Name, TypeElement> typeParameters) {
       methodBuilder.addStatement(
           ""return executeAndMapToSingleEntity(boundStatement, $L)"", helperFieldName);
     }
 
     @Override
-    public CodeBlock wrapWithErrorHandling(CodeBlock innerBlock) {
+    public CodeBlock wrapWithErrorHandling(
+        CodeBlock innerBlock,
+        ExecutableElement methodElement,
+        Map<Name, TypeElement> typeParameters) {
       return innerBlock;
     }
   },
   OPTIONAL_ENTITY {
     @Override
-    public void addExecuteStatement(CodeBlock.Builder methodBuilder, String helperFieldName) {
+    public void addExecuteStatement(
+        CodeBlock.Builder methodBuilder,
+        String helperFieldName,
+        ExecutableElement methodElement,
+        Map<Name, TypeElement> typeParameters) {
       methodBuilder.addStatement(
           ""return executeAndMapToOptionalEntity(boundStatement, $L)"", helperFieldName);
     }
 
     @Override
-    public CodeBlock wrapWithErrorHandling(CodeBlock innerBlock) {
+    public CodeBlock wrapWithErrorHandling(
+        CodeBlock innerBlock,
+        ExecutableElement methodElement,
+        Map<Name, TypeElement> typeParameters) {
       return innerBlock;
     }
   },
   RESULT_SET {
     @Override
-    public void addExecuteStatement(CodeBlock.Builder methodBuilder, String helperFieldName) {
+    public void addExecuteStatement(
+        CodeBlock.Builder methodBuilder,
+        String helperFieldName,
+        ExecutableElement methodElement,
+        Map<Name, TypeElement> typeParameters) {
       methodBuilder.addStatement(""return execute(boundStatement)"");
     }
 
     @Override
-    public CodeBlock wrapWithErrorHandling(CodeBlock innerBlock) {
+    public CodeBlock wrapWithErrorHandling(
+        CodeBlock innerBlock,
+        ExecutableElement methodElement,
+        Map<Name, TypeElement> typeParameters) {
       return innerBlock;
     }
   },
   BOUND_STATEMENT {
     @Override
-    public void addExecuteStatement(CodeBlock.Builder methodBuilder, String helperFieldName) {
+    public void addExecuteStatement(
+        CodeBlock.Builder methodBuilder,
+        String helperFieldName,
+        ExecutableElement methodElement,
+        Map<Name, TypeElement> typeParameters) {
       methodBuilder.addStatement(""return boundStatement"");
     }
 
     @Override
-    public CodeBlock wrapWithErrorHandling(CodeBlock innerBlock) {
+    public CodeBlock wrapWithErrorHandling(
+        CodeBlock innerBlock,
+        ExecutableElement methodElement,
+        Map<Name, TypeElement> typeParameters) {
       return innerBlock;
     }
   },
   PAGING_ITERABLE {
     @Override
-    public void addExecuteStatement(CodeBlock.Builder methodBuilder, String helperFieldName) {
+    public void addExecuteStatement(
+        CodeBlock.Builder methodBuilder,
+        String helperFieldName,
+        ExecutableElement methodElement,
+        Map<Name, TypeElement> typeParameters) {
       methodBuilder.addStatement(
           ""return executeAndMapToEntityIterable(boundStatement, $L)"", helperFieldName);
     }
 
     @Override
-    public CodeBlock wrapWithErrorHandling(CodeBlock innerBlock) {
+    public CodeBlock wrapWithErrorHandling(
+        CodeBlock innerBlock,
+        ExecutableElement methodElement,
+        Map<Name, TypeElement> typeParameters) {
       return innerBlock;
     }
   },
 
   FUTURE_OF_VOID {
     @Override
-    public void addExecuteStatement(CodeBlock.Builder methodBuilder, String helperFieldName) {
+    public void addExecuteStatement(
+        CodeBlock.Builder methodBuilder,
+        String helperFieldName,
+        ExecutableElement methodElement,
+        Map<Name, TypeElement> typeParameters) {
       methodBuilder.addStatement(""return executeAsyncAndMapToVoid(boundStatement)"");
     }
 
     @Override
-    public CodeBlock wrapWithErrorHandling(CodeBlock innerBlock) {
+    public CodeBlock wrapWithErrorHandling(
+        CodeBlock innerBlock,
+        ExecutableElement methodElement,
+        Map<Name, TypeElement> typeParameters) {
       return wrapWithErrorHandling(innerBlock, FAILED_FUTURE);
     }
   },
   FUTURE_OF_BOOLEAN {
     @Override
-    public void addExecuteStatement(CodeBlock.Builder methodBuilder, String helperFieldName) {
+    public void addExecuteStatement(
+        CodeBlock.Builder methodBuilder,
+        String helperFieldName,
+        ExecutableElement methodElement,
+        Map<Name, TypeElement> typeParameters) {
       methodBuilder.addStatement(""return executeAsyncAndMapWasAppliedToBoolean(boundStatement)"");
     }
 
     @Override
-    public CodeBlock wrapWithErrorHandling(CodeBlock innerBlock) {
+    public CodeBlock wrapWithErrorHandling(
+        CodeBlock innerBlock,
+        ExecutableElement methodElement,
+        Map<Name, TypeElement> typeParameters) {
       return wrapWithErrorHandling(innerBlock, FAILED_FUTURE);
     }
   },
   FUTURE_OF_LONG {
     @Override
-    public void addExecuteStatement(CodeBlock.Builder methodBuilder, String helperFieldName) {
+    public void addExecuteStatement(
+        CodeBlock.Builder methodBuilder,
+        String helperFieldName,
+        ExecutableElement methodElement,
+        Map<Name, TypeElement> typeParameters) {
       methodBuilder.addStatement(""return executeAsyncAndMapFirstColumnToLong(boundStatement)"");
     }
 
     @Override
-    public CodeBlock wrapWithErrorHandling(CodeBlock innerBlock) {
+    public CodeBlock wrapWithErrorHandling(
+        CodeBlock innerBlock,
+        ExecutableElement methodElement,
+        Map<Name, TypeElement> typeParameters) {
       return wrapWithErrorHandling(innerBlock, FAILED_FUTURE);
     }
   },
   FUTURE_OF_ROW {
     @Override
-    public void addExecuteStatement(CodeBlock.Builder methodBuilder, String helperFieldName) {
+    public void addExecuteStatement(
+        CodeBlock.Builder methodBuilder,
+        String helperFieldName,
+        ExecutableElement methodElement,
+        Map<Name, TypeElement> typeParameters) {
       methodBuilder.addStatement(""return executeAsyncAndExtractFirstRow(boundStatement)"");
     }
 
     @Override
-    public CodeBlock wrapWithErrorHandling(CodeBlock innerBlock) {
+    public CodeBlock wrapWithErrorHandling(
+        CodeBlock innerBlock,
+        ExecutableElement methodElement,
+        Map<Name, TypeElement> typeParameters) {
       return wrapWithErrorHandling(innerBlock, FAILED_FUTURE);
     }
   },
   FUTURE_OF_ENTITY {
     @Override
-    public void addExecuteStatement(CodeBlock.Builder methodBuilder, String helperFieldName) {
+    public void addExecuteStatement(
+        CodeBlock.Builder methodBuilder,
+        String helperFieldName,
+        ExecutableElement methodElement,
+        Map<Name, TypeElement> typeParameters) {
       methodBuilder.addStatement(
           ""return executeAsyncAndMapToSingleEntity(boundStatement, $L)"", helperFieldName);
     }
 
     @Override
-    public CodeBlock wrapWithErrorHandling(CodeBlock innerBlock) {
+    public CodeBlock wrapWithErrorHandling(
+        CodeBlock innerBlock,
+        ExecutableElement methodElement,
+        Map<Name, TypeElement> typeParameters) {
       return wrapWithErrorHandling(innerBlock, FAILED_FUTURE);
     }
   },
   FUTURE_OF_OPTIONAL_ENTITY {
     @Override
-    public void addExecuteStatement(CodeBlock.Builder methodBuilder, String helperFieldName) {
+    public void addExecuteStatement(
+        CodeBlock.Builder methodBuilder,
+        String helperFieldName,
+        ExecutableElement methodElement,
+        Map<Name, TypeElement> typeParameters) {
       methodBuilder.addStatement(
           ""return executeAsyncAndMapToOptionalEntity(boundStatement, $L)"", helperFieldName);
     }
 
     @Override
-    public CodeBlock wrapWithErrorHandling(CodeBlock innerBlock) {
+    public CodeBlock wrapWithErrorHandling(
+        CodeBlock innerBlock,
+        ExecutableElement methodElement,
+        Map<Name, TypeElement> typeParameters) {
       return wrapWithErrorHandling(innerBlock, FAILED_FUTURE);
     }
   },
   FUTURE_OF_ASYNC_RESULT_SET {
     @Override
-    public void addExecuteStatement(CodeBlock.Builder methodBuilder, String helperFieldName) {
+    public void addExecuteStatement(
+        CodeBlock.Builder methodBuilder,
+        String helperFieldName,
+        ExecutableElement methodElement,
+        Map<Name, TypeElement> typeParameters) {
       methodBuilder.addStatement(""return executeAsync(boundStatement)"");
     }
 
     @Override
-    public CodeBlock wrapWithErrorHandling(CodeBlock innerBlock) {
+    public CodeBlock wrapWithErrorHandling(
+        CodeBlock innerBlock,
+        ExecutableElement methodElement,
+        Map<Name, TypeElement> typeParameters) {
       return wrapWithErrorHandling(innerBlock, FAILED_FUTURE);
     }
   },
   FUTURE_OF_ASYNC_PAGING_ITERABLE {
     @Override
-    public void addExecuteStatement(CodeBlock.Builder methodBuilder, String helperFieldName) {
+    public void addExecuteStatement(
+        CodeBlock.Builder methodBuilder,
+        String helperFieldName,
+        ExecutableElement methodElement,
+        Map<Name, TypeElement> typeParameters) {
       methodBuilder.addStatement(
           ""return executeAsyncAndMapToEntityIterable(boundStatement, $L)"", helperFieldName);
     }
 
     @Override
-    public CodeBlock wrapWithErrorHandling(CodeBlock innerBlock) {
+    public CodeBlock wrapWithErrorHandling(
+        CodeBlock innerBlock,
+        ExecutableElement methodElement,
+        Map<Name, TypeElement> typeParameters) {
       return wrapWithErrorHandling(innerBlock, FAILED_FUTURE);
     }
   },
   REACTIVE_RESULT_SET {
     @Override
-    public void addExecuteStatement(CodeBlock.Builder methodBuilder, String helperFieldName) {
+    public void addExecuteStatement(
+        CodeBlock.Builder methodBuilder,
+        String helperFieldName,
+        ExecutableElement methodElement,
+        Map<Name, TypeElement> typeParameters) {
       methodBuilder.addStatement(""return executeReactive(boundStatement)"");
     }
 
     @Override
-    public CodeBlock wrapWithErrorHandling(CodeBlock innerBlock) {
+    public CodeBlock wrapWithErrorHandling(
+        CodeBlock innerBlock,
+        ExecutableElement methodElement,
+        Map<Name, TypeElement> typeParameters) {
       return wrapWithErrorHandling(innerBlock, FAILED_REACTIVE_RESULT_SET);
     }
   },
   MAPPED_REACTIVE_RESULT_SET {
     @Override
-    public void addExecuteStatement(CodeBlock.Builder methodBuilder, String helperFieldName) {
+    public void addExecuteStatement(
+        CodeBlock.Builder methodBuilder,
+        String helperFieldName,
+        ExecutableElement methodElement,
+        Map<Name, TypeElement> typeParameters) {
       methodBuilder.addStatement(
           ""return executeReactiveAndMap(boundStatement, $L)"", helperFieldName);
     }
 
     @Override
-    public CodeBlock wrapWithErrorHandling(CodeBlock innerBlock) {
+    public CodeBlock wrapWithErrorHandling(
+        CodeBlock innerBlock,
+        ExecutableElement methodElement,
+        Map<Name, TypeElement> typeParameters) {
       return wrapWithErrorHandling(innerBlock, FAILED_MAPPED_REACTIVE_RESULT_SET);
     }
   },
 
+  CUSTOM {
+    @Override
+    public void addExecuteStatement(
+        CodeBlock.Builder methodBuilder,
+        String helperFieldName,
+        ExecutableElement methodElement,
+        Map<Name, TypeElement> typeParameters) {
+      TypeName returnTypeName =
+          GeneratedCodePatterns.getTypeName(methodElement.getReturnType(), typeParameters);
+      methodBuilder.addStatement(
+          ""return ($T) producer.execute(boundStatement, context, $L)"",","[{'comment': 'This statement produces compiler warnings:\r\n\r\n```\r\n[INFO] [...] FruitDaoReactiveImpl__MapperGenerated.java uses unchecked or unsafe operations.\r\n[INFO] [...]  Recompile with -Xlint:unchecked for details.\r\n---\r\n```\r\n\r\nWould it be possible to extract the result to a local variable and annotate it with `@SuppressWarnings(""unchecked"")`? Alternatively, maybe we should consider annotating the whole class with `@SuppressWarnings(""all"")`.', 'commenter': 'adutra'}, {'comment': 'I think we can extract local variables in the code generated by `DefaultDaoReturnTypeKind.CUSTOM`, and annotate them individually.', 'commenter': 'olim7t'}, {'comment': ""Done.\r\nI wasn't getting any warnings when compiling our integration tests, but maybe the project is configured differently."", 'commenter': 'olim7t'}]"
1447,manual/core/bom/README.md,"@@ -0,0 +1,105 @@
+## Bill of Materials (BOM)
+
+A ""Bill Of Materials"" is a special Maven descriptor that defines the versions of a set of related
+artifacts.
+
+To import the driver's BOM, add the following section in your application's own POM:
+
+```xml
+<project>
+  ...
+  <dependencyManagement>
+    <dependencies>
+      <dependency>
+        <groupId>com.datastax.oss</groupId>
+        <artifactId>java-driver-bom</artifactId>
+        <version>4.6.1</version>
+        <type>pom</type>
+        <scope>import</scope>
+      </dependency>
+    </dependencies>
+  </dependencyManagement>
+```
+
+This allows you to omit the version when you later reference the driver artifacts:
+
+```xml
+  <dependencies>
+    <dependency>
+      <groupId>com.datastax.oss</groupId>
+      <artifactId>java-driver-query-builder</artifactId>
+    </dependency>
+  </dependencies>
+```
+
+The advantage is that this also applies to transitive dependencies. For example, if you there is a","[{'comment': 'Typo: ""if you there is""', 'commenter': 'adutra'}]"
1447,manual/core/integration/README.md,"@@ -8,6 +8,73 @@
 
 -----
 
+### Which artifact(s) should I use?
+
+There are multiple driver artifacts under the group id
+[com.datastax.oss](https://search.maven.org/search?q=g:com.datastax.oss). Here's how to pick the
+right dependencies:
+
+<table>
+<tr><th>Feature</th><th>Artifact(s)</th><th>Comments</th></tr>
+<tr>
+  <td>
+    Core functionality: executing queries with <code>CqlSession.execute()</code>, processing the
+    results with <code>ResultSet</code>, etc.
+  </td>
+  <td><code>java-driver-core</code></td>","[{'comment': 'The hyphens break and the table renders poorly. I suggest using non-breaking hyphens for artifact ids (`&#8209;`).', 'commenter': 'adutra'}]"
1447,manual/core/integration/README.md,"@@ -8,6 +8,73 @@
 
 -----
 
+### Which artifact(s) should I use?
+
+There are multiple driver artifacts under the group id
+[com.datastax.oss](https://search.maven.org/search?q=g:com.datastax.oss). Here's how to pick the
+right dependencies:
+
+<table>
+<tr><th>Feature</th><th>Artifact(s)</th><th>Comments</th></tr>
+<tr>
+  <td>
+    Core functionality: executing queries with <code>CqlSession.execute()</code>, processing the
+    results with <code>ResultSet</code>, etc.
+  </td>
+  <td><code>java-driver-core</code></td>
+  <td></td>
+</tr>
+<tr>
+  <td>
+    Same as the above, but without an explicit dependency to Netty. ","[{'comment': ""It currently also shades ESRI and Jackson, both legacy and new. Maybe mention Jackson as well, since it's a popular library?"", 'commenter': 'adutra'}]"
1447,manual/core/integration/README.md,"@@ -8,6 +8,73 @@
 
 -----
 
+### Which artifact(s) should I use?
+
+There are multiple driver artifacts under the group id
+[com.datastax.oss](https://search.maven.org/search?q=g:com.datastax.oss). Here's how to pick the
+right dependencies:
+
+<table>
+<tr><th>Feature</th><th>Artifact(s)</th><th>Comments</th></tr>
+<tr>
+  <td>
+    Core functionality: executing queries with <code>CqlSession.execute()</code>, processing the
+    results with <code>ResultSet</code>, etc.
+  </td>
+  <td><code>java-driver-core</code></td>
+  <td></td>
+</tr>
+<tr>
+  <td>
+    Same as the above, but without an explicit dependency to Netty. 
+  </td>
+  <td><code>java-driver-core-shaded</code></td>
+  <td>Replaces the regular core.<br/>See <a href=""../shaded_jar/"">this page</a></td>","[{'comment': '```suggestion\r\n  <td>Replaces the regular core driver artifact (<code>java-driver-core</code>).<br/>See <a href=""../shaded_jar/"">this page</a>.</td>\r\n```', 'commenter': 'adutra'}]"
1447,manual/core/integration/README.md,"@@ -8,6 +8,73 @@
 
 -----
 
+### Which artifact(s) should I use?
+
+There are multiple driver artifacts under the group id
+[com.datastax.oss](https://search.maven.org/search?q=g:com.datastax.oss). Here's how to pick the
+right dependencies:
+
+<table>
+<tr><th>Feature</th><th>Artifact(s)</th><th>Comments</th></tr>
+<tr>
+  <td>
+    Core functionality: executing queries with <code>CqlSession.execute()</code>, processing the
+    results with <code>ResultSet</code>, etc.
+  </td>
+  <td><code>java-driver-core</code></td>
+  <td></td>
+</tr>
+<tr>
+  <td>
+    Same as the above, but without an explicit dependency to Netty. 
+  </td>
+  <td><code>java-driver-core-shaded</code></td>
+  <td>Replaces the regular core.<br/>See <a href=""../shaded_jar/"">this page</a></td>
+</tr>
+<tr>
+  <td>
+    <a href=""../../query_builder"">Query builder</a>: generating CQL query strings programmatically. 
+  </td>
+  <td><code>java-driver-query-builder</code></td>
+  <td></td>
+</tr>
+<tr>
+  <td>
+    <a href=""../../mapper"">Object mapper</a>: generating the boilerplate to execute queries and
+    convert the results into your own domain classes.
+  </td>
+  <td>
+    <code>java-driver-mapper-processor</code><br/>
+    <code>java-driver-mapper-runtime</code>
+  </td>
+  <td>
+    Both artifacts are needed. Follow the link on the left for more details on how to configure
+    them.
+  </td>
+</tr>
+<tr>
+  <td>
+    ""Bill Of Materials"": can help manage versions if you use multiple driver artifacts.
+  </td>
+  <td><code>java-driver-bom</code></td>
+  <td>See <a href=""../bom/"">this page</a></td>","[{'comment': '```suggestion\r\n  <td>See <a href=""../bom/"">this page</a>.</td>\r\n```', 'commenter': 'adutra'}]"
1447,manual/mapper/README.md,"@@ -13,6 +13,7 @@ It is published as two artifacts:
       <groupId>com.datastax.oss</groupId>
       <artifactId>java-driver-mapper-processor</artifactId>
       <version>${driver.version}</version>
+      <scope>provided</scope>","[{'comment': ""Shouldn't we remove this XML snippet instead? It doesn't showcase the preferred way of referencing the annotation processor."", 'commenter': 'adutra'}, {'comment': 'Yes, the goal on this page was just to show the coordinates (which is why I omitted the scope initially), but maybe using XML is confusing.\r\n\r\nI\'ll rearrange it to move everything to the ""configuring"" page.', 'commenter': 'olim7t'}]"
1455,examples/src/main/java/com/datastax/oss/driver/examples/astra/AstraReadCassandraVersion.java,"@@ -13,36 +13,38 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package com.datastax.oss.driver.examples.apollo;
+package com.datastax.oss.driver.examples.astra;
 
 import com.datastax.oss.driver.api.core.CqlSession;
 import com.datastax.oss.driver.api.core.cql.ResultSet;
 import com.datastax.oss.driver.api.core.cql.Row;
 import java.nio.file.Paths;
 
 /**
- * Connects to a DataStax Apollo cluster and extracts basic information from it.
+ * Connects to a DataStax Astra cluster and extracts basic information from it.
  *
  * <p>Preconditions:
  *
  * <ul>
- *   <li>A DataStax Apollo cluster is running and accessible.
- *   <li>A DataStax Apollo secure connect bundle for the running cluster.
+ *   <li>A DataStax Astra cluster is running and accessible.
+ *   <li>A DataStax Astra secure connect bundle for the running cluster.
  * </ul>
  *
  * <p>Side effects: none.
  *
  * @see <a
- *     href=""https://helpdocs.datastax.com/gcp/dscloud/apollo/dscloudGettingStarted.html#dscloudCreateCluster"">
- *     Creating an Apollo Database</a>
- * @see <a href=""https://helpdocs.datastax.com/gcp/dscloud/apollo/dscloudShareClusterDetails.html"">
- *     Providing access to Apollo databases</a>
- * @see <a href=""https://helpdocs.datastax.com/gcp/dscloud/apollo/dscloudObtainingCredentials.html"">
- *     Obtaining Apollo secure connect bundle</a>
- * @see <a href=""https://docs.datastax.com/en/developer/java-driver/4.3"">Java driver online
+ *     href=""https://docs.datastax.com/en/astra/gcp/doc/dscloud/astra/dscloudGettingStarted.html#dscloudCreateCluster"">
+ *     Creating an Astra Database</a>
+ * @see <a
+ *     href=""https://docs.datastax.com/en/astra/gcp/doc/dscloud/astra/dscloudShareClusterDetails.html"">
+ *     Providing access to Astra databases</a>
+ * @see <a
+ *     href=""https://docs.datastax.com/en/astra/gcp/doc/dscloud/astra/dscloudObtainingCredentials.html"">
+ *     Obtaining Astra secure connect bundle</a>
+ * @see <a href=""https://docs.datastax.com/en/developer/java-driver/4.7"">Java driver online","[{'comment': '```suggestion\r\n * @see <a href=""https://docs.datastax.com/en/developer/java-driver/latest"">Java driver online\r\n```', 'commenter': 'adutra'}, {'comment': 'done', 'commenter': 'msmygit'}]"
1455,examples/src/main/java/com/datastax/oss/driver/examples/astra/AstraReadCassandraVersion.java,"@@ -13,36 +13,38 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package com.datastax.oss.driver.examples.apollo;
+package com.datastax.oss.driver.examples.astra;
 
 import com.datastax.oss.driver.api.core.CqlSession;
 import com.datastax.oss.driver.api.core.cql.ResultSet;
 import com.datastax.oss.driver.api.core.cql.Row;
 import java.nio.file.Paths;
 
 /**
- * Connects to a DataStax Apollo cluster and extracts basic information from it.
+ * Connects to a DataStax Astra cluster and extracts basic information from it.
  *
  * <p>Preconditions:
  *
  * <ul>
- *   <li>A DataStax Apollo cluster is running and accessible.
- *   <li>A DataStax Apollo secure connect bundle for the running cluster.
+ *   <li>A DataStax Astra cluster is running and accessible.
+ *   <li>A DataStax Astra secure connect bundle for the running cluster.
  * </ul>
  *
  * <p>Side effects: none.
  *
  * @see <a
- *     href=""https://helpdocs.datastax.com/gcp/dscloud/apollo/dscloudGettingStarted.html#dscloudCreateCluster"">
- *     Creating an Apollo Database</a>
- * @see <a href=""https://helpdocs.datastax.com/gcp/dscloud/apollo/dscloudShareClusterDetails.html"">
- *     Providing access to Apollo databases</a>
- * @see <a href=""https://helpdocs.datastax.com/gcp/dscloud/apollo/dscloudObtainingCredentials.html"">
- *     Obtaining Apollo secure connect bundle</a>
- * @see <a href=""https://docs.datastax.com/en/developer/java-driver/4.3"">Java driver online
+ *     href=""https://docs.datastax.com/en/astra/gcp/doc/dscloud/astra/dscloudGettingStarted.html#dscloudCreateCluster"">","[{'comment': ""These instructions are for GCP it seems, don't we have more general instructions for any cloud provider?"", 'commenter': 'adutra'}, {'comment': ""Unfortunately, we don't have generic instructions. We only have AWS and GCP atm."", 'commenter': 'msmygit'}, {'comment': 'OK, how about we change the link texts to ""blahblah (GCP)""? ', 'commenter': 'adutra'}, {'comment': 'Done', 'commenter': 'msmygit'}]"
1460,core/src/main/java/com/datastax/oss/driver/api/core/session/SessionBuilder.java,"@@ -633,73 +637,103 @@ public SessionT build() {
     return CompletableFutures.getUninterruptibly(buildAsync());
   }
 
+  /**
+   * Creates an uninitialized session with the options set by this builder.
+   *
+   * <p>The returned session will be disconnected; it must be manually {@linkplain
+   * LazyCqlSession#init() initialized} before it is used to perform queries.
+   *
+   * @throws UnsupportedOperationException it this builder does not support creating uninitialized
+   *     sessions.
+   */
+  @NonNull
+  public SessionT buildLazy() {
+    LazyCqlSession defaultSession = buildDefaultSessionLazy();
+    return wrapLazy(defaultSession);
+  }
+
   protected abstract SessionT wrap(@NonNull CqlSession defaultSession);
 
+  protected SessionT wrapLazy(@NonNull LazyCqlSession defaultSession) {
+    throw new UnsupportedOperationException(""Building uninitialized sessions not supported"");
+  }
+
   @NonNull
   protected final CompletionStage<CqlSession> buildDefaultSessionAsync() {
     try {
+      LazyCqlSession defaultSession = buildDefaultSessionLazy();
+      return defaultSession.init().thenApply(v -> defaultSession);
+    } catch (Throwable t) {
+      // We construct the session synchronously (until the init() call), but async clients expect a
+      // failed future if anything goes wrong. So wrap any error from that synchronous part.
+      return CompletableFutures.failedFuture(t);
+    }
+  }
 
-      ProgrammaticArguments programmaticArguments = programmaticArgumentsBuilder.build();
+  @NonNull
+  protected LazyCqlSession buildDefaultSessionLazy() {","[{'comment': ""`buildDefaultSessionAsync` is final, but I don't see why it should be; and making this method final would make revapi angry."", 'commenter': 'adutra'}]"
1460,core/src/main/java/com/datastax/oss/driver/api/core/session/SessionBuilder.java,"@@ -633,73 +637,103 @@ public SessionT build() {
     return CompletableFutures.getUninterruptibly(buildAsync());
   }
 
+  /**
+   * Creates an uninitialized session with the options set by this builder.
+   *
+   * <p>The returned session will be disconnected; it must be manually {@linkplain
+   * LazyCqlSession#init() initialized} before it is used to perform queries.
+   *
+   * @throws UnsupportedOperationException it this builder does not support creating uninitialized
+   *     sessions.
+   */
+  @NonNull
+  public SessionT buildLazy() {
+    LazyCqlSession defaultSession = buildDefaultSessionLazy();
+    return wrapLazy(defaultSession);
+  }
+
   protected abstract SessionT wrap(@NonNull CqlSession defaultSession);
 
+  protected SessionT wrapLazy(@NonNull LazyCqlSession defaultSession) {
+    throw new UnsupportedOperationException(""Building uninitialized sessions not supported"");
+  }
+
   @NonNull
   protected final CompletionStage<CqlSession> buildDefaultSessionAsync() {
     try {
+      LazyCqlSession defaultSession = buildDefaultSessionLazy();
+      return defaultSession.init().thenApply(v -> defaultSession);
+    } catch (Throwable t) {
+      // We construct the session synchronously (until the init() call), but async clients expect a
+      // failed future if anything goes wrong. So wrap any error from that synchronous part.
+      return CompletableFutures.failedFuture(t);
+    }
+  }
 
-      ProgrammaticArguments programmaticArguments = programmaticArgumentsBuilder.build();
+  @NonNull
+  protected LazyCqlSession buildDefaultSessionLazy() {
 
-      DriverConfigLoader configLoader =
-          this.configLoader != null
-              ? this.configLoader
-              : defaultConfigLoader(programmaticArguments.getClassLoader());
+    ProgrammaticArguments programmaticArguments = programmaticArgumentsBuilder.build();
 
-      DriverExecutionProfile defaultConfig = configLoader.getInitialConfig().getDefaultProfile();
-      if (cloudConfigInputStream == null) {
-        String configUrlString =
-            defaultConfig.getString(DefaultDriverOption.CLOUD_SECURE_CONNECT_BUNDLE, null);
-        if (configUrlString != null) {
-          cloudConfigInputStream = () -> getURL(configUrlString).openStream();
-        }
+    DriverConfigLoader configLoader1 =
+        this.configLoader != null
+            ? this.configLoader
+            : defaultConfigLoader(programmaticArguments.getClassLoader());
+
+    DriverExecutionProfile defaultConfig = configLoader1.getInitialConfig().getDefaultProfile();
+    if (cloudConfigInputStream == null) {
+      String configUrlString =
+          defaultConfig.getString(DefaultDriverOption.CLOUD_SECURE_CONNECT_BUNDLE, null);
+      if (configUrlString != null) {
+        cloudConfigInputStream = () -> getURL(configUrlString).openStream();
       }
-      List<String> configContactPoints =
-          defaultConfig.getStringList(DefaultDriverOption.CONTACT_POINTS, Collections.emptyList());
-      if (cloudConfigInputStream != null) {
-        if (!programmaticContactPoints.isEmpty() || !configContactPoints.isEmpty()) {
-          throw new IllegalStateException(
-              ""Can't use withCloudSecureConnectBundle and addContactPoint(s). They are mutually exclusive."");
-        }
-        String configuredSSLFactory =
-            defaultConfig.getString(DefaultDriverOption.SSL_ENGINE_FACTORY_CLASS, null);
-        if (sslConfigured || configuredSSLFactory != null) {
-          throw new IllegalStateException(
-              ""Can't use withCloudSecureConnectBundle and explicitly specify ssl configuration. They are mutually exclusive."");
-        }
-        CloudConfig cloudConfig =
-            new CloudConfigFactory().createCloudConfig(cloudConfigInputStream.call());
+    }
+    List<String> configContactPoints =
+        defaultConfig.getStringList(DefaultDriverOption.CONTACT_POINTS, Collections.emptyList());
+    if (cloudConfigInputStream != null) {
+      if (!programmaticContactPoints.isEmpty() || !configContactPoints.isEmpty()) {
+        throw new IllegalStateException(
+            ""Can't use withCloudSecureConnectBundle and addContactPoint(s). They are mutually exclusive."");
+      }
+      String configuredSSLFactory =
+          defaultConfig.getString(DefaultDriverOption.SSL_ENGINE_FACTORY_CLASS, null);
+      if (sslConfigured || configuredSSLFactory != null) {
+        throw new IllegalStateException(
+            ""Can't use withCloudSecureConnectBundle and explicitly specify ssl configuration. They are mutually exclusive."");
+      }
+      try {
+        InputStream is = cloudConfigInputStream.call();
+        CloudConfig cloudConfig = new CloudConfigFactory().createCloudConfig(is);
         addContactEndPoints(cloudConfig.getEndPoints());
         withLocalDatacenter(cloudConfig.getLocalDatacenter());
         withSslEngineFactory(cloudConfig.getSslEngineFactory());
         withCloudProxyAddress(cloudConfig.getProxyAddress());
         if (cloudConfig.getAuthProvider().isPresent()) {
           withAuthProvider(cloudConfig.getAuthProvider().get());
         }
-        programmaticArguments = programmaticArgumentsBuilder.build();
+      } catch (IOException e) {
+        throw new UncheckedIOException(e);","[{'comment': ""I'm changing this slightly so that this method does not throw checked exceptions; it seems overkill to impose to catch checked exceptions on callers of `buildLazy()`."", 'commenter': 'adutra'}]"
1463,core/src/main/java/com/datastax/oss/driver/api/core/cql/ColumnDefinitions.java,"@@ -97,22 +100,59 @@ default ColumnDefinition get(@NonNull CqlIdentifier name) {
   /** Whether there is a definition using the given CQL identifier. */
   boolean contains(@NonNull CqlIdentifier id);
 
+  /**
+   * Returns the indices of all columns that use the given name.
+   *
+   * <p>Because raw strings are ambiguous with regard to case-sensitivity, the argument will be
+   * interpreted according to the rules described in {@link AccessibleByName}.
+   *
+   * @return the indices, or an empty list if no column uses this name.
+   * @apiNote the default implementation only exists for backward compatibility. It wraps the result
+   *     of {@link #firstIndexOf(String)} in a singleton list, which is not entirely correct, as it
+   *     will only return the first occurrence. Therefore it also logs a warning.
+   *     <p>Implementors should always override this method (all built-in driver implementations
+   *     do).
+   */
+  @NonNull
+  default List<Integer> allIndicesOf(@NonNull String name) {
+    Loggers.COLUMN_DEFINITIONS.warn(
+        ""Implementors should always override this method, ""
+            + ""the default implementation only returns the first occurrence"");
+    return Collections.singletonList(firstIndexOf(name));","[{'comment': ""I wouldn't use this as a general solution to breaking API changes, but a number of things make me think it's OK for this case:\r\n* this only matters for prepared and bound statements (UDTs can't have duplicate field names, and rows are not settable).\r\n* there are probably very few (if any) custom implementations of `PreparedStatement` and `BoundStatement` outside of the driver.\r\n* even so, the implementation is only incorrect when there are multiple occurrences of the same variable, which I still think is an edge case.\r\n* finally, the default implementation is not completely broken, it just functions like previous 4.x releases."", 'commenter': 'olim7t'}]"
1463,pom.xml,"@@ -714,6 +714,10 @@ limitations under the License.]]></inlineHeader>
           <doclint>all,-missing</doclint>
           <excludePackageNames>com.datastax.*.driver.internal*</excludePackageNames>
           <tags>
+            <tag>
+              <name>apiNote</name>
+              <placement>A</placement>
+            </tag>","[{'comment': 'This is supposed to be a ""semi-official"" tag in Java 8, IDEA recognizes it but the build doesn\'t.', 'commenter': 'olim7t'}, {'comment': 'This tag is not being rendered in generated javadocs, is that on purpose?', 'commenter': 'adutra'}, {'comment': ""No, I thought it would but I didn't check. I might switch back to a regular paragraph."", 'commenter': 'olim7t'}, {'comment': 'OK, it just needed an additional `<head>` element.', 'commenter': 'olim7t'}]"
1463,core/src/main/java/com/datastax/oss/driver/api/core/data/SettableById.java,"@@ -59,7 +59,12 @@
   @NonNull
   @CheckReturnValue
   default SelfT setBytesUnsafe(@NonNull CqlIdentifier id, @Nullable ByteBuffer v) {
-    return setBytesUnsafe(firstIndexOf(id), v);
+    SelfT result = null;
+    for (Integer i : allIndicesOf(id)) {
+      result = (result == null ? this : result).setBytesUnsafe(i, v);
+    }
+    assert result != null; // allIndices throws if there are no results
+    return result;","[{'comment': ""A couple of comments on performance:\r\n\r\n* for bound statements (immutable), this will create a new copy for each iteration in the loop. That's fine, because repeated names are rare anyway, and if they happen it will probably be 2-3 times at most. Also `BoundStatementBuilder` is available as a workaround.\r\n* this quirky pattern to deal with the self type is copy-pasted in every method. I could extract it to a utility method that takes a `BiFunction<SelfT, Integer, SelfT>`, but creating a capturing lambda for every method call bothers me."", 'commenter': 'olim7t'}]"
1463,integration-tests/src/test/java/com/datastax/oss/driver/core/cql/BoundStatementCcmIT.java,"@@ -365,6 +366,44 @@ public void should_compute_routing_key_when_indices_randomly_distributed() {
     }
   }
 
+  @Test
+  public void should_set_all_occurrence_of_variable() {
+    CqlSession session = sessionRule.session();
+    PreparedStatement ps = session.prepare(""INSERT INTO test3 (pk1, pk2, v) VALUES (:i, :i, :i)"");
+
+    CqlIdentifier id = CqlIdentifier.fromCql(""i"");
+    ColumnDefinitions variableDefinitions = ps.getVariableDefinitions();
+    assertThat(variableDefinitions.allIndicesOf(id)).containsExactly(0, 1, 2);
+
+    should_set_all_occurrences_of_variable(ps.bind().setInt(id, 12));
+    should_set_all_occurrences_of_variable(ps.boundStatementBuilder().setInt(id, 12).build());
+  }
+
+  private void should_set_all_occurrences_of_variable(BoundStatement bs) {
+    assertThat(bs.getInt(0)).isEqualTo(12);
+    assertThat(bs.getInt(1)).isEqualTo(12);
+    assertThat(bs.getInt(2)).isEqualTo(12);
+
+    // Nothing should be shared internally (this would be a bug if the client later retrieves a
+    // buffer with getBytesUnsafe and modifies it)","[{'comment': 'I want emphasize this. I think reusing the same buffer would be a mistake, but just to check everyone agrees.', 'commenter': 'olim7t'}, {'comment': 'Agreed, while using names or ids you would always modify the values of all occurrences, when using indices you still can modify just one of the occurrences.', 'commenter': 'adutra'}]"
1463,core/src/main/java/com/datastax/oss/driver/api/core/cql/ColumnDefinitions.java,"@@ -97,22 +100,59 @@ default ColumnDefinition get(@NonNull CqlIdentifier name) {
   /** Whether there is a definition using the given CQL identifier. */
   boolean contains(@NonNull CqlIdentifier id);
 
+  /**
+   * Returns the indices of all columns that use the given name.
+   *
+   * <p>Because raw strings are ambiguous with regard to case-sensitivity, the argument will be
+   * interpreted according to the rules described in {@link AccessibleByName}.
+   *
+   * @return the indices, or an empty list if no column uses this name.
+   * @apiNote the default implementation only exists for backward compatibility. It wraps the result
+   *     of {@link #firstIndexOf(String)} in a singleton list, which is not entirely correct, as it
+   *     will only return the first occurrence. Therefore it also logs a warning.
+   *     <p>Implementors should always override this method (all built-in driver implementations
+   *     do).
+   */
+  @NonNull
+  default List<Integer> allIndicesOf(@NonNull String name) {
+    Loggers.COLUMN_DEFINITIONS.warn(
+        ""Implementors should always override this method, ""","[{'comment': 'Nit: ""this method"" will look a bit ambiguous in a log file, shouldn\'t we name it explicitly?', 'commenter': 'adutra'}, {'comment': 'Agreed, my brain was in ""exception message"" mode, but we don\'t have a stacktrace for that log.', 'commenter': 'olim7t'}]"
1463,core/src/main/java/com/datastax/oss/driver/internal/core/util/Loggers.java,"@@ -38,4 +43,10 @@ public static void warnWithException(Logger logger, String format, Object... arg
       }
     }
   }
+
+  // Loggers for API interfaces, declared here in order to keep them internal.
+  public static Logger COLUMN_DEFINITIONS = LoggerFactory.getLogger(ColumnDefinitions.class);","[{'comment': 'Very clever ðŸ‘ ', 'commenter': 'adutra'}]"
1463,integration-tests/src/test/java/com/datastax/oss/driver/core/cql/BoundStatementCcmIT.java,"@@ -365,6 +366,44 @@ public void should_compute_routing_key_when_indices_randomly_distributed() {
     }
   }
 
+  @Test
+  public void should_set_all_occurrence_of_variable() {","[{'comment': '```suggestion\r\n  public void should_set_all_occurrences_of_variable() {\r\n```', 'commenter': 'adutra'}]"
1464,core/src/main/java/com/datastax/oss/driver/api/core/config/DefaultDriverOption.java,"@@ -808,6 +808,20 @@
    * <p>Value-type: int
    */
   SESSION_LEAK_THRESHOLD(""advanced.session-leak.threshold""),
+
+  /**
+   * The fully-qualified classname of the desired MetricsFactory implementation.
+   *
+   * <p>Value-type: {@link String}
+   */
+  METRICS_FACTORY_CLASS(""advanced.metrics.factory.class""),
+
+  /**
+   * The fully-qualified classname of the desired metrics registry implementation.
+   *
+   * <p>Value-type: {@link String}
+   */
+  METRICS_FACTORY_REGISTRY_CLASS(""advanced.metrics.factory.registry.class""),","[{'comment': 'do we really need those two parameters? I think that `METRICS_FACTORY_CLASS` should be enough.\r\nYou wrote:\r\n```\r\nhttps://github.com/datastax/java-driver/pull/1464/files#diff-8b8ff589cb369ea902b39dcf4601a449R1242-R1245\r\n```\r\ncould you elaborate more on that?', 'commenter': 'tomekl007'}, {'comment': ""My explanation is long, the TL;DR is this:\r\nI wanted to try to avoid having a hard-coded metrics registry implementation for MicroProfile (i.e. smallrye). I'm not sure it's worth the effort.\r\n\r\nLonger explanation:\r\nI added this to allow for configuring a class that could be instantiated as a metric registry, mostly for the case of MicroProfile. Each metrics framework (DropWizard, Micrometer and MicroProfile) has a registry for holding the collected metrics, and each requires access to that registry in order to create the desired metrics.\r\n\r\nIn the case of DropWizzard and Micrometer, a default or global registry is provided with the framework, so the MetricsFactory implementation for those frameworks does not need a configured registry class.\r\n\r\nDropWizard: https://github.com/datastax/java-driver/pull/1464/files#diff-a204e0c3b6601fba49c1b8f3414563cbR69\r\n\r\nMicrometer: https://github.com/datastax/java-driver/pull/1464/files#diff-7823a53975fd907c9990f1e6a327fd56R71\r\n\r\nBut for MicroProfile, there is no included concrete implementation of a MeterRegistry. We use Smallye for the implementation, but there are other libraries that could be used. Currently, I have the registry hardcoded here:\r\n\r\nhttps://github.com/datastax/java-driver/pull/1464/files#diff-c3b4e1da8e315c17af71d25d32983dafR71\r\n\r\nHowever, if the registry class is configurable, I can remove the Smallrye dependency and allow users to configure whatever registry they want to use. But as I'm typing this explanation, I'm realizing that if we want this MicroProfile module to be drop-in, it will need an implementation of the registry, so smallrye will have to be included.\r\n\r\nSo the only reason to keep the registry class configurable is to allow for different implementations of a given metrics framework's registry, which may not really be applicable outside of MicroProfile anyway."", 'commenter': 'emerkle826'}, {'comment': ""I agree that two options sounds a bit complicated. Maybe there should be one factory per available MicroProfile implementation, e.g. `MicroprofileSmallRyeMetricsFactory implements MetricsFactory`. Are there any other implementation than SmallRye out there, and is it likely that we'd need to support them?"", 'commenter': 'olim7t'}, {'comment': ""> Are there any other implementation than SmallRye out there, and is it likely that we'd need to support them?\r\n\r\nI think this is the list: https://wiki.eclipse.org/MicroProfile/Implementation#MP_Metrics_implementations\r\nSo just 2 others that implement the latest version of MicroProfile metrics."", 'commenter': 'emerkle826'}, {'comment': 'So it seems the other two MicroProfile implementations are other apps, not necessarily libraries intended to be used as a metrics implementation. So, it looks like smallrye is the only one we will need to handle for now.', 'commenter': 'emerkle826'}]"
1464,metrics/micrometer/src/main/java/com/datastax/oss/driver/metrics/micrometer/MicrometerSessionMetricUpdater.java,"@@ -0,0 +1,130 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.metrics.micrometer;
+
+import com.datastax.dse.driver.api.core.metrics.DseSessionMetric;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import com.datastax.oss.driver.api.core.metrics.DefaultSessionMetric;
+import com.datastax.oss.driver.api.core.metrics.SessionMetric;
+import com.datastax.oss.driver.api.core.session.throttling.RequestThrottler;
+import com.datastax.oss.driver.internal.core.context.InternalDriverContext;
+import com.datastax.oss.driver.internal.core.cql.CqlPrepareAsyncProcessor;
+import com.datastax.oss.driver.internal.core.cql.CqlPrepareSyncProcessor;
+import com.datastax.oss.driver.internal.core.metrics.SessionMetricUpdater;
+import com.datastax.oss.driver.internal.core.session.RequestProcessor;
+import com.datastax.oss.driver.internal.core.session.throttling.ConcurrencyLimitingRequestThrottler;
+import com.datastax.oss.driver.internal.core.session.throttling.RateLimitingRequestThrottler;
+import com.datastax.oss.driver.shaded.guava.common.cache.Cache;
+import edu.umd.cs.findbugs.annotations.Nullable;
+import io.micrometer.core.instrument.MeterRegistry;
+import java.util.Set;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class MicrometerSessionMetricUpdater extends MicrometerMetricUpdater<SessionMetric>
+    implements SessionMetricUpdater {
+
+  private static final Logger LOG = LoggerFactory.getLogger(MicrometerSessionMetricUpdater.class);
+
+  private final String metricNamePrefix;
+
+  public MicrometerSessionMetricUpdater(
+      Set<SessionMetric> enabledMetrics, MeterRegistry registry, InternalDriverContext context) {
+    super(enabledMetrics, registry);
+    this.metricNamePrefix = context.getSessionName() + ""."";
+
+    if (enabledMetrics.contains(DefaultSessionMetric.CONNECTED_NODES)) {
+      this.registry.gauge(
+          buildFullName(DefaultSessionMetric.CONNECTED_NODES, null),
+          context,
+          c -> {
+            int count = 0;
+            for (Node node : c.getMetadataManager().getMetadata().getNodes().values()) {
+              if (node.getOpenConnections() > 0) {
+                ++count;
+              }
+            }
+            return count;
+          });
+    }
+    if (enabledMetrics.contains(DefaultSessionMetric.THROTTLING_QUEUE_SIZE)) {
+      this.registry.gauge(
+          buildFullName(DefaultSessionMetric.THROTTLING_QUEUE_SIZE, null),
+          context,","[{'comment': 'nit: refactor to a separate method', 'commenter': 'tomekl007'}]"
1464,metrics/micrometer/src/main/java/com/datastax/oss/driver/metrics/micrometer/MicrometerSessionMetricUpdater.java,"@@ -0,0 +1,130 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.metrics.micrometer;
+
+import com.datastax.dse.driver.api.core.metrics.DseSessionMetric;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import com.datastax.oss.driver.api.core.metrics.DefaultSessionMetric;
+import com.datastax.oss.driver.api.core.metrics.SessionMetric;
+import com.datastax.oss.driver.api.core.session.throttling.RequestThrottler;
+import com.datastax.oss.driver.internal.core.context.InternalDriverContext;
+import com.datastax.oss.driver.internal.core.cql.CqlPrepareAsyncProcessor;
+import com.datastax.oss.driver.internal.core.cql.CqlPrepareSyncProcessor;
+import com.datastax.oss.driver.internal.core.metrics.SessionMetricUpdater;
+import com.datastax.oss.driver.internal.core.session.RequestProcessor;
+import com.datastax.oss.driver.internal.core.session.throttling.ConcurrencyLimitingRequestThrottler;
+import com.datastax.oss.driver.internal.core.session.throttling.RateLimitingRequestThrottler;
+import com.datastax.oss.driver.shaded.guava.common.cache.Cache;
+import edu.umd.cs.findbugs.annotations.Nullable;
+import io.micrometer.core.instrument.MeterRegistry;
+import java.util.Set;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class MicrometerSessionMetricUpdater extends MicrometerMetricUpdater<SessionMetric>
+    implements SessionMetricUpdater {
+
+  private static final Logger LOG = LoggerFactory.getLogger(MicrometerSessionMetricUpdater.class);
+
+  private final String metricNamePrefix;
+
+  public MicrometerSessionMetricUpdater(
+      Set<SessionMetric> enabledMetrics, MeterRegistry registry, InternalDriverContext context) {
+    super(enabledMetrics, registry);
+    this.metricNamePrefix = context.getSessionName() + ""."";
+
+    if (enabledMetrics.contains(DefaultSessionMetric.CONNECTED_NODES)) {
+      this.registry.gauge(
+          buildFullName(DefaultSessionMetric.CONNECTED_NODES, null),
+          context,
+          c -> {
+            int count = 0;
+            for (Node node : c.getMetadataManager().getMetadata().getNodes().values()) {
+              if (node.getOpenConnections() > 0) {
+                ++count;
+              }
+            }
+            return count;
+          });
+    }
+    if (enabledMetrics.contains(DefaultSessionMetric.THROTTLING_QUEUE_SIZE)) {
+      this.registry.gauge(
+          buildFullName(DefaultSessionMetric.THROTTLING_QUEUE_SIZE, null),
+          context,
+          c -> {
+            RequestThrottler requestThrottler = c.getRequestThrottler();
+            String logPrefix = c.getSessionName();
+            if (requestThrottler instanceof ConcurrencyLimitingRequestThrottler) {
+              return ((ConcurrencyLimitingRequestThrottler) requestThrottler).getQueueSize();
+            }
+            if (requestThrottler instanceof RateLimitingRequestThrottler) {
+              return ((RateLimitingRequestThrottler) requestThrottler).getQueueSize();
+            }
+            LOG.warn(
+                ""[{}] Metric {} does not support {}, it will always return 0"",
+                logPrefix,
+                DefaultSessionMetric.THROTTLING_QUEUE_SIZE.getPath(),
+                requestThrottler.getClass().getName());
+            return 0;
+          });
+    }
+    if (enabledMetrics.contains(DefaultSessionMetric.CQL_PREPARED_CACHE_SIZE)) {
+      this.registry.gauge(","[{'comment': 'nit: refactor to a separate method', 'commenter': 'tomekl007'}]"
1464,metrics/microprofile/pom.xml,"@@ -0,0 +1,66 @@
+<?xml version=""1.0"" encoding=""UTF-8""?>
+<!--
+
+    Copyright DataStax, Inc.
+
+    Licensed under the Apache License, Version 2.0 (the ""License"");
+    you may not use this file except in compliance with the License.
+    You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+    Unless required by applicable law or agreed to in writing, software
+    distributed under the License is distributed on an ""AS IS"" BASIS,
+    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+    See the License for the specific language governing permissions and
+    limitations under the License.
+
+-->
+<project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"">
+  <modelVersion>4.0.0</modelVersion>
+  <parent>
+    <groupId>com.datastax.oss</groupId>
+    <artifactId>java-driver-parent</artifactId>
+    <version>4.8.0-SNAPSHOT</version>
+  </parent>
+  <artifactId>java-driver-metrics-microprofile</artifactId>
+  <packaging>bundle</packaging>
+  <name>DataStax Java driver for Apache Cassandra(R) - Metrics - microprofile</name>
+  <properties>
+    <microprofile.version>2.3</microprofile.version>","[{'comment': ""shouldn't it work for provided dependencies to not hardcode versions of the library in this module? (same for micro profile) "", 'commenter': 'tomekl007'}, {'comment': ""Maven won't compile if I remove the dependency version. I can change it to a range and it will pick the latest."", 'commenter': 'emerkle826'}, {'comment': 'You can also use `RELEASE`, but usually people prefer a fixed version because variable versions or version ranges tend to produce non-repeatable builds.', 'commenter': 'adutra'}]"
1464,metrics/micrometer/src/test/java/com/datastax/oss/driver/metrics/micrometer/MicrometerMetricsFactoryIT.java,"@@ -0,0 +1,91 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.metrics.micrometer;
+
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.awaitility.Awaitility.await;
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
+import com.datastax.oss.driver.api.core.config.DriverConfigLoader;
+import com.datastax.oss.driver.api.core.metrics.DefaultSessionMetric;
+import com.datastax.oss.driver.api.testinfra.ccm.CcmRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionUtils;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import io.micrometer.core.instrument.MeterRegistry;
+import io.micrometer.core.instrument.Timer;
+import io.micrometer.core.instrument.simple.SimpleMeterRegistry;
+import java.util.Collections;
+import java.util.concurrent.TimeUnit;
+import org.assertj.core.api.Condition;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+@Category(ParallelizableTests.class)
+public class MicrometerMetricsFactoryIT {
+
+  @ClassRule public static final CcmRule CCM_RULE = CcmRule.getInstance();
+  private static final MeterRegistry METER_REGISTRY = new SimpleMeterRegistry();
+","[{'comment': 'I think that the IT test should be more extensive and validate all possible metrics types.\r\nPlease see the test in the spring starter', 'commenter': 'tomekl007'}]"
1464,metrics/microprofile/src/test/java/com/datastax/oss/driver/netrics/microprofile/MicroProfileMetricsFactoryIT.java,"@@ -0,0 +1,91 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.netrics.microprofile;
+
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.awaitility.Awaitility.await;
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
+import com.datastax.oss.driver.api.core.config.DriverConfigLoader;
+import com.datastax.oss.driver.api.core.metrics.DefaultSessionMetric;
+import com.datastax.oss.driver.api.testinfra.ccm.CcmRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionUtils;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import com.datastax.oss.driver.metrics.microprofile.MicroProfileMetricsSessionBuilder;
+import io.smallrye.metrics.MetricsRegistryImpl;
+import java.util.Collections;
+import java.util.Map.Entry;
+import java.util.concurrent.TimeUnit;
+import org.assertj.core.api.Condition;
+import org.eclipse.microprofile.metrics.Metric;
+import org.eclipse.microprofile.metrics.MetricID;
+import org.eclipse.microprofile.metrics.MetricRegistry;
+import org.eclipse.microprofile.metrics.Timer;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+@Category(ParallelizableTests.class)
+public class MicroProfileMetricsFactoryIT {
+
+  @ClassRule public static final CcmRule CCM_RULE = CcmRule.getInstance();
+  private static final MetricRegistry METRIC_REGISTRY = new MetricsRegistryImpl();
+","[{'comment': 'Same here, the IT test does not validate all metric types.\r\nIt is beneficial to have such a test - we may catch some errors.', 'commenter': 'tomekl007'}]"
1464,metrics/micrometer/src/main/java/com/datastax/oss/driver/metrics/micrometer/MicrometerDriverContext.java,"@@ -0,0 +1,42 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.metrics.micrometer;
+
+import com.datastax.oss.driver.api.core.config.DriverConfigLoader;
+import com.datastax.oss.driver.api.core.context.DriverContext;
+import com.datastax.oss.driver.api.core.metrics.MetricsFactory;
+import com.datastax.oss.driver.api.core.session.ProgrammaticArguments;
+import com.datastax.oss.driver.internal.core.context.DefaultDriverContext;
+import io.micrometer.core.instrument.MeterRegistry;
+
+/** Implementation of {@link DriverContext} that provides for a Micrometer {@link MeterRegistry}. */","[{'comment': 'The `DefaultDriverContext` is using `Dropwizard` metrics. \r\nMaybe to make it consistent we should create a `DropwizardDriverContext extends DefaultDriverContext ` to provide our users with a consistent API for using metrics:\r\n- `DropwizardDriverContext`\r\n- `MicrometerDriverContext`\r\n- `MicroProfileDriverContext`\r\nIf not, then we should at least document that.', 'commenter': 'tomekl007'}]"
1464,core/src/main/java/com/datastax/oss/driver/internal/core/metrics/DropwizardMetricsFactory.java,"@@ -49,9 +50,9 @@
   @Nullable private final Metrics metrics;
   private final SessionMetricUpdater sessionUpdater;
 
-  public DropwizardMetricsFactory(InternalDriverContext context) {
+  public DropwizardMetricsFactory(DriverContext context) {
     this.logPrefix = context.getSessionName();
-    this.context = context;
+    this.context = (InternalDriverContext) context;","[{'comment': ""This change can be reverted as we're not exposing a config option to instantiate a MetricsFactory"", 'commenter': 'emerkle826'}]"
1464,metrics/micrometer/src/main/java/com/datastax/oss/driver/internal/metrics/micrometer/MicrometerMetricsFactory.java,"@@ -0,0 +1,124 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.metrics.micrometer;
+
+import com.datastax.dse.driver.api.core.metrics.DseNodeMetric;
+import com.datastax.dse.driver.api.core.metrics.DseSessionMetric;
+import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
+import com.datastax.oss.driver.api.core.config.DriverExecutionProfile;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import com.datastax.oss.driver.api.core.metrics.DefaultNodeMetric;
+import com.datastax.oss.driver.api.core.metrics.DefaultSessionMetric;
+import com.datastax.oss.driver.api.core.metrics.NodeMetric;
+import com.datastax.oss.driver.api.core.metrics.SessionMetric;
+import com.datastax.oss.driver.internal.core.context.InternalDriverContext;
+import com.datastax.oss.driver.internal.core.metrics.MetricsFactory;
+import com.datastax.oss.driver.internal.core.metrics.NodeMetricUpdater;
+import com.datastax.oss.driver.internal.core.metrics.NoopNodeMetricUpdater;
+import com.datastax.oss.driver.internal.core.metrics.NoopSessionMetricUpdater;
+import com.datastax.oss.driver.internal.core.metrics.SessionMetricUpdater;
+import io.micrometer.core.instrument.MeterRegistry;
+import java.util.Collections;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Optional;
+import java.util.Set;
+import net.jcip.annotations.ThreadSafe;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+@ThreadSafe
+public class MicrometerMetricsFactory implements MetricsFactory {
+
+  private static final Logger LOG = LoggerFactory.getLogger(MicrometerMetricsFactory.class);
+
+  private final String logPrefix;
+  private final InternalDriverContext context;
+  private final Set<NodeMetric> enabledNodeMetrics;
+  private final MeterRegistry registry;
+  private final SessionMetricUpdater sessionUpdater;
+
+  public MicrometerMetricsFactory(InternalDriverContext context, MeterRegistry registry) {
+    this.logPrefix = context.getSessionName();
+    this.context = context;
+
+    DriverExecutionProfile config = context.getConfig().getDefaultProfile();
+    Set<SessionMetric> enabledSessionMetrics =
+        parseSessionMetricPaths(config.getStringList(DefaultDriverOption.METRICS_SESSION_ENABLED));
+    this.enabledNodeMetrics =
+        parseNodeMetricPaths(config.getStringList(DefaultDriverOption.METRICS_NODE_ENABLED));
+
+    if (enabledSessionMetrics.isEmpty() && enabledNodeMetrics.isEmpty()) {
+      LOG.debug(""[{}] All metrics are disabled, Session.getMetrics will be empty"", logPrefix);
+      this.registry = null;
+      this.sessionUpdater = NoopSessionMetricUpdater.INSTANCE;
+    } else {
+      this.registry = registry;
+      this.sessionUpdater =
+          new MicrometerSessionMetricUpdater(enabledSessionMetrics, this.registry, this.context);
+    }
+  }
+
+  @Override
+  public Optional<com.datastax.oss.driver.api.core.metrics.Metrics> getMetrics() {","[{'comment': 'Any reason to not import `Metrics`?', 'commenter': 'adutra'}, {'comment': ""I think at one point I was also referencing Micrometer's `Metrics` class and already had that imported. So one of them was going to need to be fully qualified. It doesn't look like I am referencing Micrometer's class any more, so I can change this now."", 'commenter': 'emerkle826'}]"
1464,metrics/microprofile/src/main/java/com/datastax/oss/driver/internal/metrics/microprofile/MicroProfileMetricUpdater.java,"@@ -0,0 +1,87 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.metrics.microprofile;
+
+import com.datastax.oss.driver.api.core.config.DriverExecutionProfile;
+import com.datastax.oss.driver.internal.core.metrics.MetricUpdater;
+import java.util.Set;
+import java.util.concurrent.TimeUnit;
+import net.jcip.annotations.ThreadSafe;
+import org.eclipse.microprofile.metrics.MetricRegistry;
+
+@ThreadSafe
+public abstract class MicroProfileMetricUpdater<MetricT> implements MetricUpdater<MetricT> {
+
+  public static final String CASSANDRA_METRICS_PREFIX = ""cassandra"";","[{'comment': ""Why this? isn't the metrics prefix computed in subclasses enough?"", 'commenter': 'adutra'}, {'comment': 'This was just the pattern I followed from what we did with the quarkus extension. I can remove it. Just to clarify, removing this would result in metrics paths going from something like:\r\n```\r\ncassandra.s0.cql-requests\r\n```\r\nto something like:\r\n```\r\ns0.cql-requests\r\n```', 'commenter': 'emerkle826'}, {'comment': 'I see. Is it possible to let the user ""nest"" the driver metrics inside a general prefix of their choice?', 'commenter': 'adutra'}, {'comment': 'It is to the extent that a user application can override the Session name. But this implementation is just a fixed static prefix of ""cassandra"".', 'commenter': 'emerkle826'}]"
1464,metrics/microprofile/src/test/java/com/datastax/oss/driver/api/netrics/microprofile/MicroProfileMetricsTest.java,"@@ -0,0 +1,213 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.netrics.microprofile;
+
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.awaitility.Awaitility.await;
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.config.DriverConfigLoader;
+import com.datastax.oss.driver.api.core.metrics.DefaultNodeMetric;
+import com.datastax.oss.driver.api.core.metrics.DefaultSessionMetric;
+import com.datastax.oss.driver.api.core.metrics.NodeMetric;
+import com.datastax.oss.driver.api.core.metrics.SessionMetric;
+import com.datastax.oss.driver.api.metrics.microprofile.MicroProfileMetricsSessionBuilder;
+import com.datastax.oss.driver.api.testinfra.ccm.CcmRule;
+import com.datastax.oss.driver.api.testinfra.session.SessionUtils;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import com.datastax.oss.driver.internal.metrics.microprofile.MicroProfileMetricUpdater;
+import io.smallrye.metrics.MetricsRegistryImpl;
+import java.util.Map.Entry;
+import java.util.concurrent.TimeUnit;
+import java.util.function.Function;
+import java.util.regex.Pattern;
+import org.assertj.core.api.Condition;
+import org.eclipse.microprofile.metrics.Counter;
+import org.eclipse.microprofile.metrics.Gauge;
+import org.eclipse.microprofile.metrics.Meter;
+import org.eclipse.microprofile.metrics.Metric;
+import org.eclipse.microprofile.metrics.MetricID;
+import org.eclipse.microprofile.metrics.MetricRegistry;
+import org.eclipse.microprofile.metrics.Timer;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+@Category(ParallelizableTests.class)
+public class MicroProfileMetricsTest {","[{'comment': 'The tests are nice, however they are integration tests. Following our conventions, they should be moved to the integration tests module.\r\nI wonder how to add true unit tests here. The Dropwizard factory is only tested indirectly.', 'commenter': 'adutra'}, {'comment': ""I'll move these to the integration test module. True unit tests were difficult to design as the metrics are pegged throughout the driver code with access to the Session and Node Updater classes. Making a unit test that simulates that is just testing the metric/meter marking code, so it doesn't provide much value. I'll try to come up with something that provides at least a little value."", 'commenter': 'emerkle826'}]"
1464,metrics/micrometer/src/main/java/com/datastax/oss/driver/api/metrics/micrometer/MicrometerMetricsSessionBuilder.java,"@@ -0,0 +1,46 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.metrics.micrometer;
+
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.config.DriverConfigLoader;
+import com.datastax.oss.driver.api.core.context.DriverContext;
+import com.datastax.oss.driver.api.core.session.ProgrammaticArguments;
+import com.datastax.oss.driver.api.core.session.SessionBuilder;
+import com.datastax.oss.driver.internal.metrics.micrometer.MicrometerDriverContext;
+import io.micrometer.core.instrument.MeterRegistry;
+
+public class MicrometerMetricsSessionBuilder
+    extends SessionBuilder<MicrometerMetricsSessionBuilder, CqlSession> {
+
+  private MeterRegistry registry;
+
+  public MicrometerMetricsSessionBuilder withMeterRegistry(MeterRegistry registry) {
+    this.registry = registry;
+    return this;
+  }
+
+  @Override
+  protected CqlSession wrap(CqlSession defaultSession) {
+    return defaultSession;
+  }
+
+  @Override
+  protected DriverContext buildContext(
+      DriverConfigLoader configLoader, ProgrammaticArguments programmaticArguments) {
+    return new MicrometerDriverContext(configLoader, programmaticArguments, registry);","[{'comment': ""What happens if `registry` is null? Afaict we'll have a NPE later on. I think it's better to throw the NPE here. Same for Microprofile."", 'commenter': 'adutra'}, {'comment': ""If the registry is required to be non-null, maybe it's better to inject it in the constructor."", 'commenter': 'adutra'}, {'comment': ""Good catch, I'll fix it."", 'commenter': 'emerkle826'}]"
1464,metrics/micrometer/pom.xml,"@@ -0,0 +1,86 @@
+<?xml version=""1.0"" encoding=""UTF-8""?>
+<!--
+
+    Copyright DataStax, Inc.
+
+    Licensed under the Apache License, Version 2.0 (the ""License"");
+    you may not use this file except in compliance with the License.
+    You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+    Unless required by applicable law or agreed to in writing, software
+    distributed under the License is distributed on an ""AS IS"" BASIS,
+    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+    See the License for the specific language governing permissions and
+    limitations under the License.
+
+-->
+<project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"">
+  <modelVersion>4.0.0</modelVersion>
+  <parent>
+    <groupId>com.datastax.oss</groupId>
+    <artifactId>java-driver-parent</artifactId>
+    <version>4.8.0-SNAPSHOT</version>
+    <relativePath>../../</relativePath>
+  </parent>
+  <artifactId>java-driver-metrics-micrometer</artifactId>
+  <packaging>bundle</packaging>
+  <name>DataStax Java driver for Apache Cassandra(R) - Metrics - Micrometer</name>
+  <properties>
+    <micrometer.version>1.5.0</micrometer.version>
+  </properties>
+  <dependencyManagement>
+    <dependencies>
+      <dependency>
+        <groupId>${project.groupId}</groupId>
+        <artifactId>java-driver-bom</artifactId>
+        <version>${project.version}</version>
+        <type>pom</type>
+        <scope>import</scope>
+      </dependency>
+    </dependencies>
+  </dependencyManagement>
+  <dependencies>
+    <dependency>
+      <groupId>io.micrometer</groupId>
+      <artifactId>micrometer-core</artifactId>
+      <version>${micrometer.version}</version>
+    </dependency>
+    <dependency>
+      <groupId>com.datastax.oss</groupId>
+      <artifactId>java-driver-core</artifactId>
+      <exclusions>
+        <exclusion>
+          <groupId>io.dropwizard.metrics</groupId>
+          <artifactId>metrics-core</artifactId>
+        </exclusion>
+      </exclusions>
+    </dependency>
+    <dependency>","[{'comment': 'A few unnecessary dependencies here, I will fix myself.', 'commenter': 'adutra'}]"
1464,metrics/micrometer/src/test/java/com/datastax/oss/driver/internal/metrics/micrometer/LoggerTest.java,"@@ -0,0 +1,63 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.metrics.micrometer;
+
+import static org.mockito.Mockito.mock;
+
+import ch.qos.logback.classic.Level;
+import ch.qos.logback.classic.Logger;
+import ch.qos.logback.classic.spi.ILoggingEvent;
+import ch.qos.logback.core.Appender;
+import org.mockito.ArgumentCaptor;
+import org.slf4j.LoggerFactory;
+
+public class LoggerTest {","[{'comment': 'Not required, you can use the one from driver-core. I will fix myself.', 'commenter': 'adutra'}]"
1464,metrics/microprofile/src/test/java/com/datastax/oss/driver/internal/metrics/microprofile/LoggerTest.java,"@@ -0,0 +1,63 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.metrics.microprofile;
+
+import static org.mockito.Mockito.mock;
+
+import ch.qos.logback.classic.Level;
+import ch.qos.logback.classic.Logger;
+import ch.qos.logback.classic.spi.ILoggingEvent;
+import ch.qos.logback.core.Appender;
+import org.mockito.ArgumentCaptor;
+import org.slf4j.LoggerFactory;
+
+public class LoggerTest {","[{'comment': 'Same here.', 'commenter': 'adutra'}]"
1464,pom.xml,"@@ -396,6 +399,11 @@
         <artifactId>svm</artifactId>
         <version>${graalapi.version}</version>
       </dependency>
+      <dependency>","[{'comment': 'Now that you declared smallrye here, I think it would make sense to also declare microprofile and micrometer here. I will fix myself.', 'commenter': 'adutra'}]"
1464,core/src/main/java/com/datastax/oss/driver/internal/core/metrics/DropwizardMetricsFactory.java,"@@ -136,36 +128,4 @@ public NodeMetricUpdater newNodeUpdater(Node node) {
       return dropwizardNodeMetricUpdater;
     }
   }
-
-  protected Set<SessionMetric> parseSessionMetricPaths(List<String> paths) {","[{'comment': 'Nit: these overridable methods were a hook to add new metrics by subclassing. In particular this is how the DSE driver did it when it was a separate project.\r\nNow that OSS and DSE are unified this is less of a concern, but maybe still nice to have. We can keep the protected methods but simply have them delegate to the utility class.', 'commenter': 'olim7t'}]"
1465,core/src/main/java/com/datastax/oss/driver/internal/core/channel/DefaultWriteCoalescer.java,"@@ -84,8 +94,10 @@ private Flusher(EventLoop eventLoop) {
 
     private void enqueue(Write write) {
       boolean added = writes.offer(write);
+      System.out.println(""writes size: "" + writes.size() + "" for Flusher: "" + this);
       assert added; // always true (see MpscLinkedAtomicQueue implementation)
-      if (running.compareAndSet(false, true)) {
+      if (write.isWritable() && running.compareAndSet(false, true)) {
+        System.out.println(""runOnEventLoop#100"");
         eventLoop.execute(this::runOnEventLoop);","[{'comment': ""The problem is that there is no rescheduling here: if `!write.isWritable()`, the writes will simply be left pending.\r\n\r\nOf course if another request is executed after the channel becomes writable again, it will dequeue all the accumulated writes. But this is not guaranteed to happen; if it doesn't the requests will hang forever."", 'commenter': 'olim7t'}, {'comment': 'yes, that is why we need to register on the `channelWritabilityChanged()` callback - it signals that `isWritable()` changed; in such a case we should start the event loop', 'commenter': 'tomekl007'}, {'comment': ""I've propagated the `channelWritabilityChanged` in the: https://github.com/datastax/java-driver/pull/1465/commits/341e7d7927e6f0021094c1324386f94d998e4261 - please take a look. The requests are no left pending - when the buffer is able to accept the messages, the event loop is restarted"", 'commenter': 'tomekl007'}]"
1465,core/src/main/java/com/datastax/oss/driver/internal/core/channel/InFlightHandler.java,"@@ -91,6 +95,21 @@ public void channelActive(ChannelHandlerContext ctx) throws Exception {
     this.logPrefix = ownerLogPrefix + ""|"" + channelId.substring(1, channelId.length() - 1);
   }
 
+  @Override
+  public void channelWritabilityChanged(ChannelHandlerContext ctx) throws Exception {
+    System.out.println(""Channel writability changed "" + ctx.channel().isWritable());
+    if (ctx.channel().isWritable()) {
+      driverChannel.thenAccept(
+          d -> {
+            WriteCoalescer writeCoalescer = d.getWriteCoalescer();
+            if (writeCoalescer instanceof DefaultWriteCoalescer) {
+              ((DefaultWriteCoalescer) writeCoalescer).restartFlushers();
+            }","[{'comment': ""It seems that this TCP needs to be implemented only in the `DefaultWriteCoalescer`.\r\nIn such a case, we don't need to change the interface of the flusher"", 'commenter': 'tomekl007'}]"
1465,integration-tests/src/test/java/com/datastax/oss/driver/core/TCPFlowControlIT.java,"@@ -0,0 +1,253 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.core;
+
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.noRows;
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.when;
+import static java.util.Collections.emptyList;
+import static java.util.Collections.emptyMap;
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.assertj.core.api.Assertions.assertThatThrownBy;
+
+import com.codahale.metrics.Gauge;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.DriverTimeoutException;
+import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
+import com.datastax.oss.driver.api.core.cql.AsyncResultSet;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.api.core.data.ByteUtils;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import com.datastax.oss.driver.api.core.metrics.DefaultNodeMetric;
+import com.datastax.oss.driver.api.testinfra.session.SessionUtils;
+import com.datastax.oss.driver.api.testinfra.simulacron.SimulacronRule;
+import com.datastax.oss.driver.categories.IsolatedTests;
+import com.datastax.oss.driver.internal.core.session.DefaultSession;
+import com.datastax.oss.simulacron.common.cluster.ClusterSpec;
+import com.datastax.oss.simulacron.common.request.Query;
+import com.datastax.oss.simulacron.server.BoundNode;
+import java.net.SocketAddress;
+import java.nio.ByteBuffer;
+import java.time.Duration;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.concurrent.CompletionStage;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
+import java.util.stream.Collectors;
+import org.awaitility.Awaitility;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+@Category(IsolatedTests.class)
+public class TCPFlowControlIT {
+  @ClassRule
+  public static final SimulacronRule SIMULACRON_RULE =
+      new SimulacronRule(ClusterSpec.builder().withNodes(3));
+
+  // this number is calculated empirically to be high enough to saturate the underling TCP buffer
+  private static int NUMBER_OF_SUBMITTED_REQUESTS = 2048;
+
+  @Test
+  public void should_not_write_more_requests_to_the_socket_after_the_server_paused_reading()
+      throws InterruptedException, ExecutionException, TimeoutException {
+    byte[] buffer = new byte[10240];
+    Arrays.fill(buffer, (byte) 1);
+    ByteBuffer buffer10Kb = ByteBuffer.wrap(buffer);
+
+    String queryString =
+        String.format(""INSERT INTO table1 (id) VALUES (%s)"", ByteUtils.toHexString(buffer10Kb));
+    Query query = new Query(queryString, emptyList(), emptyMap(), emptyMap());
+
+    SIMULACRON_RULE.cluster().prime(when(query).then(noRows()));
+
+    try (CqlSession session =
+        SessionUtils.newSession(SIMULACRON_RULE, SessionUtils.configLoaderBuilder().build())) {","[{'comment': 'I _think_ this can be simplified to `SessionUtils.newSession(SIMULACRON_RULE)`.', 'commenter': 'adutra'}]"
1465,integration-tests/src/test/java/com/datastax/oss/driver/core/TCPFlowControlIT.java,"@@ -0,0 +1,253 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.core;
+
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.noRows;
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.when;
+import static java.util.Collections.emptyList;
+import static java.util.Collections.emptyMap;
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.assertj.core.api.Assertions.assertThatThrownBy;
+
+import com.codahale.metrics.Gauge;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.DriverTimeoutException;
+import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
+import com.datastax.oss.driver.api.core.cql.AsyncResultSet;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.api.core.data.ByteUtils;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import com.datastax.oss.driver.api.core.metrics.DefaultNodeMetric;
+import com.datastax.oss.driver.api.testinfra.session.SessionUtils;
+import com.datastax.oss.driver.api.testinfra.simulacron.SimulacronRule;
+import com.datastax.oss.driver.categories.IsolatedTests;
+import com.datastax.oss.driver.internal.core.session.DefaultSession;
+import com.datastax.oss.simulacron.common.cluster.ClusterSpec;
+import com.datastax.oss.simulacron.common.request.Query;
+import com.datastax.oss.simulacron.server.BoundNode;
+import java.net.SocketAddress;
+import java.nio.ByteBuffer;
+import java.time.Duration;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.concurrent.CompletionStage;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
+import java.util.stream.Collectors;
+import org.awaitility.Awaitility;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+@Category(IsolatedTests.class)
+public class TCPFlowControlIT {","[{'comment': ""I wouldn't mind keep these tests â€“ they are very nicely designed btw â€“ but:\r\n\r\n1. We are testing rather Netty than the driver here;\r\n2. I wonder how deterministic this test really is? I think it could fail in many ways depending on how the underlying socket is implemented. I'm a bit worried that this test will be flaky forever."", 'commenter': 'adutra'}, {'comment': ""1. I don't have strong opinion here. But since the netty is the core of our driver, sometimes it may be beneficial to test the feature of netty that we strongly rely on. Should the netty change the behavior of TCP buffering, we will notice it immediately. I see it a bit similar to performance tests that are we are running if the netty version is incremented. Here, we are not testing performance directly but another important aspect like buffering (it can have impact on performance as well). So maybe it is beneficial to have it automated via IT?\r\n2. I run them a lot of times and they are constantly passing. The tests are also green on Jenkins. I didn't observe the flaky behavior of those ITs."", 'commenter': 'tomekl007'}, {'comment': ""Good arguments. Let's have the test then, and if we ever notice any flakiness we can revisit this later."", 'commenter': 'adutra'}]"
1465,core/src/main/java/com/datastax/oss/driver/internal/core/channel/DriverChannel.java,"@@ -271,6 +271,10 @@ public String toString() {
     return channel.toString();
   }
 
+  public Channel getChannel() {","[{'comment': 'Can you annotate this with `@VisibleForTesting`?', 'commenter': 'olim7t'}]"
1465,integration-tests/src/test/java/com/datastax/oss/driver/core/TCPFlowControlIT.java,"@@ -0,0 +1,251 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.core;
+
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.noRows;
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.when;
+import static java.util.Collections.emptyList;
+import static java.util.Collections.emptyMap;
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.assertj.core.api.Assertions.assertThatThrownBy;
+
+import com.codahale.metrics.Gauge;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.DriverTimeoutException;
+import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
+import com.datastax.oss.driver.api.core.cql.AsyncResultSet;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.api.core.data.ByteUtils;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import com.datastax.oss.driver.api.core.metrics.DefaultNodeMetric;
+import com.datastax.oss.driver.api.testinfra.session.SessionUtils;
+import com.datastax.oss.driver.api.testinfra.simulacron.SimulacronRule;
+import com.datastax.oss.driver.categories.IsolatedTests;
+import com.datastax.oss.driver.internal.core.session.DefaultSession;
+import com.datastax.oss.simulacron.common.cluster.ClusterSpec;
+import com.datastax.oss.simulacron.common.request.Query;
+import com.datastax.oss.simulacron.server.BoundNode;
+import java.net.SocketAddress;
+import java.nio.ByteBuffer;
+import java.time.Duration;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.concurrent.CompletionStage;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
+import java.util.stream.Collectors;
+import org.awaitility.Awaitility;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+@Category(IsolatedTests.class)
+public class TCPFlowControlIT {
+  @ClassRule
+  public static final SimulacronRule SIMULACRON_RULE =
+      new SimulacronRule(ClusterSpec.builder().withNodes(3));
+
+  // this number is calculated empirically to be high enough to saturate the underling TCP buffer
+  private static int NUMBER_OF_SUBMITTED_REQUESTS = 2048;
+
+  @Test
+  public void should_not_write_more_requests_to_the_socket_after_the_server_paused_reading()
+      throws InterruptedException, ExecutionException, TimeoutException {
+    byte[] buffer = new byte[10240];
+    Arrays.fill(buffer, (byte) 1);
+    ByteBuffer buffer10Kb = ByteBuffer.wrap(buffer);
+
+    String queryString =
+        String.format(""INSERT INTO table1 (id) VALUES (%s)"", ByteUtils.toHexString(buffer10Kb));
+    Query query = new Query(queryString, emptyList(), emptyMap(), emptyMap());
+
+    SIMULACRON_RULE.cluster().prime(when(query).then(noRows()));
+
+    try (CqlSession session = SessionUtils.newSession(SIMULACRON_RULE)) {
+      SIMULACRON_RULE.cluster().pauseRead();
+
+      // The TCP send and receive buffer size depends on the OS
+      // We don't know how much data is needed to be flushed in order for OS to signal as full (and
+      // Channel#isWritable return false)
+      // Send 20Mb+ to each node
+      List<CompletionStage<AsyncResultSet>> pendingRequests = new ArrayList<>();
+      for (int i = 0; i < NUMBER_OF_SUBMITTED_REQUESTS; i++) {
+        pendingRequests.add(session.executeAsync(SimpleStatement.newInstance(queryString)));
+      }
+
+      // Assert that there are still requests that haven't been written
+      waitForWriteQueueToStabilize(session);
+      assertThat(getWriteQueueSize(session)).isGreaterThan(1);
+
+      SIMULACRON_RULE.cluster().resumeRead();
+
+      int numberOfFinished = 0;
+      for (CompletionStage<AsyncResultSet> result : pendingRequests) {
+        result.toCompletableFuture().get(1, TimeUnit.SECONDS);
+        numberOfFinished = numberOfFinished + 1;
+      }
+
+      int writeQueueSize = getWriteQueueSize(session);
+      assertThat(writeQueueSize).isEqualTo(0);
+      assertThat(numberOfFinished).isEqualTo(NUMBER_OF_SUBMITTED_REQUESTS);
+    }
+  }
+
+  @Test
+  public void should_process_requests_successfully_on_non_paused_nodes()
+      throws InterruptedException, ExecutionException, TimeoutException {
+    byte[] buffer = new byte[10240];
+    Arrays.fill(buffer, (byte) 1);
+    ByteBuffer buffer10Kb = ByteBuffer.wrap(buffer);
+
+    String queryString =
+        String.format(""INSERT INTO table1 (id) VALUES (%s)"", ByteUtils.toHexString(buffer10Kb));
+    Query query = new Query(queryString, emptyList(), emptyMap(), emptyMap());
+
+    SIMULACRON_RULE.cluster().prime(when(query).then(noRows()));
+
+    try (CqlSession session =
+        SessionUtils.newSession(
+            SIMULACRON_RULE,
+            SessionUtils.configLoaderBuilder()
+                .withStringList(
+                    DefaultDriverOption.METRICS_NODE_ENABLED,
+                    Collections.singletonList(""pool.in-flight""))
+                .build())) {
+      int hostIndex = 2;
+      BoundNode pausedNode = SIMULACRON_RULE.cluster().node(hostIndex);
+      SocketAddress pausedHostAddress = pausedNode.getAddress();
+      List<Node> nonPausedNodes =
+          session.getMetadata().getNodes().values().stream()
+              .filter(n -> !pausedHostAddress.equals(n.getBroadcastRpcAddress().get()))
+              .collect(Collectors.toList());
+      assertThat(nonPausedNodes.size()).isEqualTo(2);
+
+      SIMULACRON_RULE.cluster().node(2).pauseRead();
+
+      List<CompletionStage<AsyncResultSet>> pendingRequests = new ArrayList<>();
+      for (int i = 0; i < NUMBER_OF_SUBMITTED_REQUESTS; i++) {
+        pendingRequests.add(session.executeAsync(SimpleStatement.newInstance(queryString)));
+      }
+
+      // Non-paused nodes should process the requests correctly
+      for (Node n : nonPausedNodes) {
+        Awaitility.await()","[{'comment': 'Nit: you can static import this.', 'commenter': 'olim7t'}]"
1465,integration-tests/src/test/java/com/datastax/oss/driver/core/TCPFlowControlIT.java,"@@ -0,0 +1,251 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.core;
+
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.noRows;
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.when;
+import static java.util.Collections.emptyList;
+import static java.util.Collections.emptyMap;
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.assertj.core.api.Assertions.assertThatThrownBy;
+
+import com.codahale.metrics.Gauge;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.DriverTimeoutException;
+import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
+import com.datastax.oss.driver.api.core.cql.AsyncResultSet;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.api.core.data.ByteUtils;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import com.datastax.oss.driver.api.core.metrics.DefaultNodeMetric;
+import com.datastax.oss.driver.api.testinfra.session.SessionUtils;
+import com.datastax.oss.driver.api.testinfra.simulacron.SimulacronRule;
+import com.datastax.oss.driver.categories.IsolatedTests;
+import com.datastax.oss.driver.internal.core.session.DefaultSession;
+import com.datastax.oss.simulacron.common.cluster.ClusterSpec;
+import com.datastax.oss.simulacron.common.request.Query;
+import com.datastax.oss.simulacron.server.BoundNode;
+import java.net.SocketAddress;
+import java.nio.ByteBuffer;
+import java.time.Duration;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.concurrent.CompletionStage;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
+import java.util.stream.Collectors;
+import org.awaitility.Awaitility;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+@Category(IsolatedTests.class)
+public class TCPFlowControlIT {
+  @ClassRule
+  public static final SimulacronRule SIMULACRON_RULE =
+      new SimulacronRule(ClusterSpec.builder().withNodes(3));
+
+  // this number is calculated empirically to be high enough to saturate the underling TCP buffer
+  private static int NUMBER_OF_SUBMITTED_REQUESTS = 2048;
+
+  @Test
+  public void should_not_write_more_requests_to_the_socket_after_the_server_paused_reading()
+      throws InterruptedException, ExecutionException, TimeoutException {
+    byte[] buffer = new byte[10240];
+    Arrays.fill(buffer, (byte) 1);
+    ByteBuffer buffer10Kb = ByteBuffer.wrap(buffer);
+
+    String queryString =
+        String.format(""INSERT INTO table1 (id) VALUES (%s)"", ByteUtils.toHexString(buffer10Kb));
+    Query query = new Query(queryString, emptyList(), emptyMap(), emptyMap());
+
+    SIMULACRON_RULE.cluster().prime(when(query).then(noRows()));
+
+    try (CqlSession session = SessionUtils.newSession(SIMULACRON_RULE)) {
+      SIMULACRON_RULE.cluster().pauseRead();
+
+      // The TCP send and receive buffer size depends on the OS
+      // We don't know how much data is needed to be flushed in order for OS to signal as full (and
+      // Channel#isWritable return false)
+      // Send 20Mb+ to each node
+      List<CompletionStage<AsyncResultSet>> pendingRequests = new ArrayList<>();
+      for (int i = 0; i < NUMBER_OF_SUBMITTED_REQUESTS; i++) {
+        pendingRequests.add(session.executeAsync(SimpleStatement.newInstance(queryString)));
+      }
+
+      // Assert that there are still requests that haven't been written
+      waitForWriteQueueToStabilize(session);
+      assertThat(getWriteQueueSize(session)).isGreaterThan(1);
+
+      SIMULACRON_RULE.cluster().resumeRead();
+
+      int numberOfFinished = 0;
+      for (CompletionStage<AsyncResultSet> result : pendingRequests) {
+        result.toCompletableFuture().get(1, TimeUnit.SECONDS);
+        numberOfFinished = numberOfFinished + 1;
+      }
+
+      int writeQueueSize = getWriteQueueSize(session);
+      assertThat(writeQueueSize).isEqualTo(0);
+      assertThat(numberOfFinished).isEqualTo(NUMBER_OF_SUBMITTED_REQUESTS);
+    }
+  }
+
+  @Test
+  public void should_process_requests_successfully_on_non_paused_nodes()
+      throws InterruptedException, ExecutionException, TimeoutException {
+    byte[] buffer = new byte[10240];
+    Arrays.fill(buffer, (byte) 1);
+    ByteBuffer buffer10Kb = ByteBuffer.wrap(buffer);
+
+    String queryString =
+        String.format(""INSERT INTO table1 (id) VALUES (%s)"", ByteUtils.toHexString(buffer10Kb));
+    Query query = new Query(queryString, emptyList(), emptyMap(), emptyMap());
+
+    SIMULACRON_RULE.cluster().prime(when(query).then(noRows()));
+
+    try (CqlSession session =
+        SessionUtils.newSession(
+            SIMULACRON_RULE,
+            SessionUtils.configLoaderBuilder()
+                .withStringList(
+                    DefaultDriverOption.METRICS_NODE_ENABLED,
+                    Collections.singletonList(""pool.in-flight""))
+                .build())) {
+      int hostIndex = 2;
+      BoundNode pausedNode = SIMULACRON_RULE.cluster().node(hostIndex);
+      SocketAddress pausedHostAddress = pausedNode.getAddress();
+      List<Node> nonPausedNodes =
+          session.getMetadata().getNodes().values().stream()
+              .filter(n -> !pausedHostAddress.equals(n.getBroadcastRpcAddress().get()))
+              .collect(Collectors.toList());
+      assertThat(nonPausedNodes.size()).isEqualTo(2);
+
+      SIMULACRON_RULE.cluster().node(2).pauseRead();
+
+      List<CompletionStage<AsyncResultSet>> pendingRequests = new ArrayList<>();
+      for (int i = 0; i < NUMBER_OF_SUBMITTED_REQUESTS; i++) {
+        pendingRequests.add(session.executeAsync(SimpleStatement.newInstance(queryString)));
+      }
+
+      // Non-paused nodes should process the requests correctly
+      for (Node n : nonPausedNodes) {
+        Awaitility.await()
+            .until(
+                () -> {
+                  int inFlightRequests = getInFlightRequests(session, n);
+                  return inFlightRequests == 0;
+                });
+      }
+
+      // There are still requests that haven't been written
+      waitForWriteQueueToStabilize(session);
+      assertThat(getWriteQueueSize(session)).isGreaterThan(1);
+
+      SIMULACRON_RULE.cluster().resumeRead();
+
+      int numberOfFinished = 0;
+      for (CompletionStage<AsyncResultSet> result : pendingRequests) {
+        result.toCompletableFuture().get(1, TimeUnit.SECONDS);
+        numberOfFinished = numberOfFinished + 1;
+      }
+
+      int writeQueueSize = getWriteQueueSize(session);
+      assertThat(writeQueueSize).isEqualTo(0);
+      assertThat(numberOfFinished).isEqualTo(NUMBER_OF_SUBMITTED_REQUESTS);
+    }
+  }
+
+  @Test
+  public void should_timeout_requests_when_the_server_paused_reading_without_resuming()
+      throws InterruptedException {
+    byte[] buffer = new byte[10240];
+    Arrays.fill(buffer, (byte) 1);
+    ByteBuffer buffer10Kb = ByteBuffer.wrap(buffer);
+
+    String queryString =
+        String.format(""INSERT INTO table1 (id) VALUES (%s)"", ByteUtils.toHexString(buffer10Kb));
+    Query query = new Query(queryString, emptyList(), emptyMap(), emptyMap());
+
+    SIMULACRON_RULE.cluster().prime(when(query).then(noRows()));
+
+    try (CqlSession session = SessionUtils.newSession(SIMULACRON_RULE)) {
+      SIMULACRON_RULE.cluster().pauseRead();
+
+      List<CompletionStage<AsyncResultSet>> pendingRequests = new ArrayList<>();
+      for (int i = 0; i < NUMBER_OF_SUBMITTED_REQUESTS; i++) {
+        pendingRequests.add(session.executeAsync(SimpleStatement.newInstance(queryString)));
+      }
+
+      // Assert that there are still requests that haven't been written
+      waitForWriteQueueToStabilize(session);
+      assertThat(getWriteQueueSize(session)).isGreaterThan(1);
+
+      // do not resumeRead
+      for (CompletionStage<AsyncResultSet> result : pendingRequests) {
+        assertThatThrownBy(() -> result.toCompletableFuture().get())
+            .isInstanceOf(ExecutionException.class)
+            .hasCauseInstanceOf(DriverTimeoutException.class);
+      }
+
+      // write queue size is still non empty
+      int writeQueueSize = getWriteQueueSize(session);
+      assertThat(writeQueueSize).isGreaterThan(1);
+      SIMULACRON_RULE.cluster().stop();
+    }
+  }
+
+  @SuppressWarnings(""unchecked"")
+  private int getInFlightRequests(CqlSession session, Node n) {
+    return ((Gauge<Integer>)
+            session.getMetrics().get().getNodeMetric(n, DefaultNodeMetric.IN_FLIGHT).get())
+        .getValue();
+  }
+
+  private int getWriteQueueSize(CqlSession session) {
+    int writeQueueSize = 0;
+    for (Node n : session.getMetadata().getNodes().values()) {
+      writeQueueSize +=
+          ((DefaultSession) session)
+              .getChannel(n, ""ignore"")
+              .getChannel()
+              .unsafe()
+              .outboundBuffer()
+              .size();
+    }
+    return writeQueueSize;
+  }
+
+  private void waitForWriteQueueToStabilize(CqlSession cqlSession) throws InterruptedException {
+    final Integer[] lastWriteQueueValue = {getWriteQueueSize(cqlSession)};
+    // initial delay
+    Thread.sleep(500);","[{'comment': ""Isn't that also done by the `await().pollDelay()` below?"", 'commenter': 'olim7t'}]"
1465,integration-tests/src/test/java/com/datastax/oss/driver/core/TCPFlowControlIT.java,"@@ -0,0 +1,251 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.core;
+
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.noRows;
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.when;
+import static java.util.Collections.emptyList;
+import static java.util.Collections.emptyMap;
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.assertj.core.api.Assertions.assertThatThrownBy;
+
+import com.codahale.metrics.Gauge;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.DriverTimeoutException;
+import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
+import com.datastax.oss.driver.api.core.cql.AsyncResultSet;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.api.core.data.ByteUtils;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import com.datastax.oss.driver.api.core.metrics.DefaultNodeMetric;
+import com.datastax.oss.driver.api.testinfra.session.SessionUtils;
+import com.datastax.oss.driver.api.testinfra.simulacron.SimulacronRule;
+import com.datastax.oss.driver.categories.IsolatedTests;
+import com.datastax.oss.driver.internal.core.session.DefaultSession;
+import com.datastax.oss.simulacron.common.cluster.ClusterSpec;
+import com.datastax.oss.simulacron.common.request.Query;
+import com.datastax.oss.simulacron.server.BoundNode;
+import java.net.SocketAddress;
+import java.nio.ByteBuffer;
+import java.time.Duration;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.concurrent.CompletionStage;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
+import java.util.stream.Collectors;
+import org.awaitility.Awaitility;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+@Category(IsolatedTests.class)
+public class TCPFlowControlIT {
+  @ClassRule
+  public static final SimulacronRule SIMULACRON_RULE =
+      new SimulacronRule(ClusterSpec.builder().withNodes(3));
+
+  // this number is calculated empirically to be high enough to saturate the underling TCP buffer
+  private static int NUMBER_OF_SUBMITTED_REQUESTS = 2048;
+
+  @Test
+  public void should_not_write_more_requests_to_the_socket_after_the_server_paused_reading()
+      throws InterruptedException, ExecutionException, TimeoutException {
+    byte[] buffer = new byte[10240];
+    Arrays.fill(buffer, (byte) 1);
+    ByteBuffer buffer10Kb = ByteBuffer.wrap(buffer);
+
+    String queryString =
+        String.format(""INSERT INTO table1 (id) VALUES (%s)"", ByteUtils.toHexString(buffer10Kb));
+    Query query = new Query(queryString, emptyList(), emptyMap(), emptyMap());
+
+    SIMULACRON_RULE.cluster().prime(when(query).then(noRows()));
+
+    try (CqlSession session = SessionUtils.newSession(SIMULACRON_RULE)) {
+      SIMULACRON_RULE.cluster().pauseRead();
+
+      // The TCP send and receive buffer size depends on the OS
+      // We don't know how much data is needed to be flushed in order for OS to signal as full (and
+      // Channel#isWritable return false)
+      // Send 20Mb+ to each node
+      List<CompletionStage<AsyncResultSet>> pendingRequests = new ArrayList<>();
+      for (int i = 0; i < NUMBER_OF_SUBMITTED_REQUESTS; i++) {
+        pendingRequests.add(session.executeAsync(SimpleStatement.newInstance(queryString)));
+      }
+
+      // Assert that there are still requests that haven't been written
+      waitForWriteQueueToStabilize(session);
+      assertThat(getWriteQueueSize(session)).isGreaterThan(1);
+
+      SIMULACRON_RULE.cluster().resumeRead();
+
+      int numberOfFinished = 0;
+      for (CompletionStage<AsyncResultSet> result : pendingRequests) {
+        result.toCompletableFuture().get(1, TimeUnit.SECONDS);
+        numberOfFinished = numberOfFinished + 1;
+      }
+
+      int writeQueueSize = getWriteQueueSize(session);
+      assertThat(writeQueueSize).isEqualTo(0);
+      assertThat(numberOfFinished).isEqualTo(NUMBER_OF_SUBMITTED_REQUESTS);
+    }
+  }
+
+  @Test
+  public void should_process_requests_successfully_on_non_paused_nodes()
+      throws InterruptedException, ExecutionException, TimeoutException {
+    byte[] buffer = new byte[10240];
+    Arrays.fill(buffer, (byte) 1);
+    ByteBuffer buffer10Kb = ByteBuffer.wrap(buffer);
+
+    String queryString =
+        String.format(""INSERT INTO table1 (id) VALUES (%s)"", ByteUtils.toHexString(buffer10Kb));
+    Query query = new Query(queryString, emptyList(), emptyMap(), emptyMap());
+
+    SIMULACRON_RULE.cluster().prime(when(query).then(noRows()));
+
+    try (CqlSession session =
+        SessionUtils.newSession(
+            SIMULACRON_RULE,
+            SessionUtils.configLoaderBuilder()
+                .withStringList(
+                    DefaultDriverOption.METRICS_NODE_ENABLED,
+                    Collections.singletonList(""pool.in-flight""))
+                .build())) {
+      int hostIndex = 2;
+      BoundNode pausedNode = SIMULACRON_RULE.cluster().node(hostIndex);
+      SocketAddress pausedHostAddress = pausedNode.getAddress();
+      List<Node> nonPausedNodes =
+          session.getMetadata().getNodes().values().stream()
+              .filter(n -> !pausedHostAddress.equals(n.getBroadcastRpcAddress().get()))
+              .collect(Collectors.toList());
+      assertThat(nonPausedNodes.size()).isEqualTo(2);
+
+      SIMULACRON_RULE.cluster().node(2).pauseRead();
+
+      List<CompletionStage<AsyncResultSet>> pendingRequests = new ArrayList<>();
+      for (int i = 0; i < NUMBER_OF_SUBMITTED_REQUESTS; i++) {
+        pendingRequests.add(session.executeAsync(SimpleStatement.newInstance(queryString)));
+      }
+
+      // Non-paused nodes should process the requests correctly
+      for (Node n : nonPausedNodes) {
+        Awaitility.await()
+            .until(
+                () -> {
+                  int inFlightRequests = getInFlightRequests(session, n);
+                  return inFlightRequests == 0;
+                });
+      }
+
+      // There are still requests that haven't been written
+      waitForWriteQueueToStabilize(session);
+      assertThat(getWriteQueueSize(session)).isGreaterThan(1);
+
+      SIMULACRON_RULE.cluster().resumeRead();
+
+      int numberOfFinished = 0;
+      for (CompletionStage<AsyncResultSet> result : pendingRequests) {
+        result.toCompletableFuture().get(1, TimeUnit.SECONDS);
+        numberOfFinished = numberOfFinished + 1;
+      }
+
+      int writeQueueSize = getWriteQueueSize(session);
+      assertThat(writeQueueSize).isEqualTo(0);
+      assertThat(numberOfFinished).isEqualTo(NUMBER_OF_SUBMITTED_REQUESTS);
+    }
+  }
+
+  @Test
+  public void should_timeout_requests_when_the_server_paused_reading_without_resuming()
+      throws InterruptedException {
+    byte[] buffer = new byte[10240];
+    Arrays.fill(buffer, (byte) 1);
+    ByteBuffer buffer10Kb = ByteBuffer.wrap(buffer);
+
+    String queryString =
+        String.format(""INSERT INTO table1 (id) VALUES (%s)"", ByteUtils.toHexString(buffer10Kb));
+    Query query = new Query(queryString, emptyList(), emptyMap(), emptyMap());
+
+    SIMULACRON_RULE.cluster().prime(when(query).then(noRows()));
+
+    try (CqlSession session = SessionUtils.newSession(SIMULACRON_RULE)) {
+      SIMULACRON_RULE.cluster().pauseRead();
+
+      List<CompletionStage<AsyncResultSet>> pendingRequests = new ArrayList<>();
+      for (int i = 0; i < NUMBER_OF_SUBMITTED_REQUESTS; i++) {
+        pendingRequests.add(session.executeAsync(SimpleStatement.newInstance(queryString)));
+      }
+
+      // Assert that there are still requests that haven't been written
+      waitForWriteQueueToStabilize(session);
+      assertThat(getWriteQueueSize(session)).isGreaterThan(1);
+
+      // do not resumeRead
+      for (CompletionStage<AsyncResultSet> result : pendingRequests) {
+        assertThatThrownBy(() -> result.toCompletableFuture().get())
+            .isInstanceOf(ExecutionException.class)
+            .hasCauseInstanceOf(DriverTimeoutException.class);
+      }
+
+      // write queue size is still non empty
+      int writeQueueSize = getWriteQueueSize(session);
+      assertThat(writeQueueSize).isGreaterThan(1);
+      SIMULACRON_RULE.cluster().stop();
+    }
+  }
+
+  @SuppressWarnings(""unchecked"")
+  private int getInFlightRequests(CqlSession session, Node n) {
+    return ((Gauge<Integer>)
+            session.getMetrics().get().getNodeMetric(n, DefaultNodeMetric.IN_FLIGHT).get())
+        .getValue();","[{'comment': 'You can pass an explicit type argument to `getNodeMetric` call to avoid the unchecked cast. Also using the optional API:\r\n```suggestion\r\n    return session\r\n        .getMetrics()\r\n        .flatMap(metrics -> metrics.<Gauge<Integer>>getNodeMetric(n, DefaultNodeMetric.IN_FLIGHT))\r\n        .map(Gauge::getValue)\r\n        .orElseThrow(() -> new AssertionError(""Expected in-flight metric to be enabled""));\r\n```', 'commenter': 'olim7t'}]"
1465,integration-tests/src/test/java/com/datastax/oss/driver/core/TCPFlowControlIT.java,"@@ -0,0 +1,251 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.core;
+
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.noRows;
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.when;
+import static java.util.Collections.emptyList;
+import static java.util.Collections.emptyMap;
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.assertj.core.api.Assertions.assertThatThrownBy;
+
+import com.codahale.metrics.Gauge;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.DriverTimeoutException;
+import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
+import com.datastax.oss.driver.api.core.cql.AsyncResultSet;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.api.core.data.ByteUtils;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import com.datastax.oss.driver.api.core.metrics.DefaultNodeMetric;
+import com.datastax.oss.driver.api.testinfra.session.SessionUtils;
+import com.datastax.oss.driver.api.testinfra.simulacron.SimulacronRule;
+import com.datastax.oss.driver.categories.IsolatedTests;
+import com.datastax.oss.driver.internal.core.session.DefaultSession;
+import com.datastax.oss.simulacron.common.cluster.ClusterSpec;
+import com.datastax.oss.simulacron.common.request.Query;
+import com.datastax.oss.simulacron.server.BoundNode;
+import java.net.SocketAddress;
+import java.nio.ByteBuffer;
+import java.time.Duration;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.concurrent.CompletionStage;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
+import java.util.stream.Collectors;
+import org.awaitility.Awaitility;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+@Category(IsolatedTests.class)
+public class TCPFlowControlIT {
+  @ClassRule
+  public static final SimulacronRule SIMULACRON_RULE =
+      new SimulacronRule(ClusterSpec.builder().withNodes(3));
+
+  // this number is calculated empirically to be high enough to saturate the underling TCP buffer
+  private static int NUMBER_OF_SUBMITTED_REQUESTS = 2048;
+
+  @Test
+  public void should_not_write_more_requests_to_the_socket_after_the_server_paused_reading()
+      throws InterruptedException, ExecutionException, TimeoutException {
+    byte[] buffer = new byte[10240];
+    Arrays.fill(buffer, (byte) 1);
+    ByteBuffer buffer10Kb = ByteBuffer.wrap(buffer);
+
+    String queryString =
+        String.format(""INSERT INTO table1 (id) VALUES (%s)"", ByteUtils.toHexString(buffer10Kb));","[{'comment': 'These 4 lines are repeated at the beginning of every test, you can extract them to a constant. I think this will produce the same string:\r\n```java\r\n  private static final String QUERY_STRING =\r\n      String.format(""INSERT INTO table1 (id) VALUES (0x%s)"", Strings.repeat(""01"", 10240));\r\n```', 'commenter': 'olim7t'}]"
1465,integration-tests/src/test/java/com/datastax/oss/driver/core/TCPFlowControlIT.java,"@@ -0,0 +1,251 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.core;
+
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.noRows;
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.when;
+import static java.util.Collections.emptyList;
+import static java.util.Collections.emptyMap;
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.assertj.core.api.Assertions.assertThatThrownBy;
+
+import com.codahale.metrics.Gauge;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.DriverTimeoutException;
+import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
+import com.datastax.oss.driver.api.core.cql.AsyncResultSet;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.api.core.data.ByteUtils;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import com.datastax.oss.driver.api.core.metrics.DefaultNodeMetric;
+import com.datastax.oss.driver.api.testinfra.session.SessionUtils;
+import com.datastax.oss.driver.api.testinfra.simulacron.SimulacronRule;
+import com.datastax.oss.driver.categories.IsolatedTests;
+import com.datastax.oss.driver.internal.core.session.DefaultSession;
+import com.datastax.oss.simulacron.common.cluster.ClusterSpec;
+import com.datastax.oss.simulacron.common.request.Query;
+import com.datastax.oss.simulacron.server.BoundNode;
+import java.net.SocketAddress;
+import java.nio.ByteBuffer;
+import java.time.Duration;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.concurrent.CompletionStage;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
+import java.util.stream.Collectors;
+import org.awaitility.Awaitility;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+@Category(IsolatedTests.class)
+public class TCPFlowControlIT {
+  @ClassRule
+  public static final SimulacronRule SIMULACRON_RULE =
+      new SimulacronRule(ClusterSpec.builder().withNodes(3));
+
+  // this number is calculated empirically to be high enough to saturate the underling TCP buffer
+  private static int NUMBER_OF_SUBMITTED_REQUESTS = 2048;
+
+  @Test
+  public void should_not_write_more_requests_to_the_socket_after_the_server_paused_reading()
+      throws InterruptedException, ExecutionException, TimeoutException {
+    byte[] buffer = new byte[10240];
+    Arrays.fill(buffer, (byte) 1);
+    ByteBuffer buffer10Kb = ByteBuffer.wrap(buffer);
+
+    String queryString =
+        String.format(""INSERT INTO table1 (id) VALUES (%s)"", ByteUtils.toHexString(buffer10Kb));
+    Query query = new Query(queryString, emptyList(), emptyMap(), emptyMap());
+
+    SIMULACRON_RULE.cluster().prime(when(query).then(noRows()));","[{'comment': 'This is also repeated in every test. You could move it to a `@BeforeClass` method.', 'commenter': 'olim7t'}]"
1465,integration-tests/src/test/java/com/datastax/oss/driver/core/TCPFlowControlIT.java,"@@ -0,0 +1,251 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.core;
+
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.noRows;
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.when;
+import static java.util.Collections.emptyList;
+import static java.util.Collections.emptyMap;
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.assertj.core.api.Assertions.assertThatThrownBy;
+
+import com.codahale.metrics.Gauge;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.DriverTimeoutException;
+import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
+import com.datastax.oss.driver.api.core.cql.AsyncResultSet;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.api.core.data.ByteUtils;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import com.datastax.oss.driver.api.core.metrics.DefaultNodeMetric;
+import com.datastax.oss.driver.api.testinfra.session.SessionUtils;
+import com.datastax.oss.driver.api.testinfra.simulacron.SimulacronRule;
+import com.datastax.oss.driver.categories.IsolatedTests;
+import com.datastax.oss.driver.internal.core.session.DefaultSession;
+import com.datastax.oss.simulacron.common.cluster.ClusterSpec;
+import com.datastax.oss.simulacron.common.request.Query;
+import com.datastax.oss.simulacron.server.BoundNode;
+import java.net.SocketAddress;
+import java.nio.ByteBuffer;
+import java.time.Duration;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.concurrent.CompletionStage;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
+import java.util.stream.Collectors;
+import org.awaitility.Awaitility;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+@Category(IsolatedTests.class)
+public class TCPFlowControlIT {
+  @ClassRule
+  public static final SimulacronRule SIMULACRON_RULE =
+      new SimulacronRule(ClusterSpec.builder().withNodes(3));
+
+  // this number is calculated empirically to be high enough to saturate the underling TCP buffer
+  private static int NUMBER_OF_SUBMITTED_REQUESTS = 2048;
+
+  @Test
+  public void should_not_write_more_requests_to_the_socket_after_the_server_paused_reading()
+      throws InterruptedException, ExecutionException, TimeoutException {
+    byte[] buffer = new byte[10240];
+    Arrays.fill(buffer, (byte) 1);
+    ByteBuffer buffer10Kb = ByteBuffer.wrap(buffer);
+
+    String queryString =
+        String.format(""INSERT INTO table1 (id) VALUES (%s)"", ByteUtils.toHexString(buffer10Kb));
+    Query query = new Query(queryString, emptyList(), emptyMap(), emptyMap());
+
+    SIMULACRON_RULE.cluster().prime(when(query).then(noRows()));
+
+    try (CqlSession session = SessionUtils.newSession(SIMULACRON_RULE)) {
+      SIMULACRON_RULE.cluster().pauseRead();
+
+      // The TCP send and receive buffer size depends on the OS
+      // We don't know how much data is needed to be flushed in order for OS to signal as full (and
+      // Channel#isWritable return false)
+      // Send 20Mb+ to each node
+      List<CompletionStage<AsyncResultSet>> pendingRequests = new ArrayList<>();
+      for (int i = 0; i < NUMBER_OF_SUBMITTED_REQUESTS; i++) {
+        pendingRequests.add(session.executeAsync(SimpleStatement.newInstance(queryString)));
+      }
+
+      // Assert that there are still requests that haven't been written
+      waitForWriteQueueToStabilize(session);
+      assertThat(getWriteQueueSize(session)).isGreaterThan(1);
+
+      SIMULACRON_RULE.cluster().resumeRead();","[{'comment': ""I was wondering about the risk of query timeouts while the server is paused. But I see that the integration-test module's `application.conf` sets `request.timeout` to 10 seconds, so I think we're good."", 'commenter': 'olim7t'}]"
1465,integration-tests/src/test/java/com/datastax/oss/driver/core/TCPFlowControlIT.java,"@@ -0,0 +1,251 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.core;
+
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.noRows;
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.when;
+import static java.util.Collections.emptyList;
+import static java.util.Collections.emptyMap;
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.assertj.core.api.Assertions.assertThatThrownBy;
+
+import com.codahale.metrics.Gauge;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.DriverTimeoutException;
+import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
+import com.datastax.oss.driver.api.core.cql.AsyncResultSet;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.api.core.data.ByteUtils;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import com.datastax.oss.driver.api.core.metrics.DefaultNodeMetric;
+import com.datastax.oss.driver.api.testinfra.session.SessionUtils;
+import com.datastax.oss.driver.api.testinfra.simulacron.SimulacronRule;
+import com.datastax.oss.driver.categories.IsolatedTests;
+import com.datastax.oss.driver.internal.core.session.DefaultSession;
+import com.datastax.oss.simulacron.common.cluster.ClusterSpec;
+import com.datastax.oss.simulacron.common.request.Query;
+import com.datastax.oss.simulacron.server.BoundNode;
+import java.net.SocketAddress;
+import java.nio.ByteBuffer;
+import java.time.Duration;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.concurrent.CompletionStage;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
+import java.util.stream.Collectors;
+import org.awaitility.Awaitility;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+@Category(IsolatedTests.class)
+public class TCPFlowControlIT {
+  @ClassRule
+  public static final SimulacronRule SIMULACRON_RULE =
+      new SimulacronRule(ClusterSpec.builder().withNodes(3));
+
+  // this number is calculated empirically to be high enough to saturate the underling TCP buffer
+  private static int NUMBER_OF_SUBMITTED_REQUESTS = 2048;
+
+  @Test
+  public void should_not_write_more_requests_to_the_socket_after_the_server_paused_reading()
+      throws InterruptedException, ExecutionException, TimeoutException {
+    byte[] buffer = new byte[10240];
+    Arrays.fill(buffer, (byte) 1);
+    ByteBuffer buffer10Kb = ByteBuffer.wrap(buffer);
+
+    String queryString =
+        String.format(""INSERT INTO table1 (id) VALUES (%s)"", ByteUtils.toHexString(buffer10Kb));
+    Query query = new Query(queryString, emptyList(), emptyMap(), emptyMap());
+
+    SIMULACRON_RULE.cluster().prime(when(query).then(noRows()));
+
+    try (CqlSession session = SessionUtils.newSession(SIMULACRON_RULE)) {
+      SIMULACRON_RULE.cluster().pauseRead();
+
+      // The TCP send and receive buffer size depends on the OS
+      // We don't know how much data is needed to be flushed in order for OS to signal as full (and
+      // Channel#isWritable return false)
+      // Send 20Mb+ to each node
+      List<CompletionStage<AsyncResultSet>> pendingRequests = new ArrayList<>();
+      for (int i = 0; i < NUMBER_OF_SUBMITTED_REQUESTS; i++) {
+        pendingRequests.add(session.executeAsync(SimpleStatement.newInstance(queryString)));
+      }
+
+      // Assert that there are still requests that haven't been written
+      waitForWriteQueueToStabilize(session);
+      assertThat(getWriteQueueSize(session)).isGreaterThan(1);
+
+      SIMULACRON_RULE.cluster().resumeRead();
+
+      int numberOfFinished = 0;
+      for (CompletionStage<AsyncResultSet> result : pendingRequests) {
+        result.toCompletableFuture().get(1, TimeUnit.SECONDS);
+        numberOfFinished = numberOfFinished + 1;","[{'comment': ""Nit: you don't need to count the number of futures, the code will throw if one of them is failed or still incomplete. If you reach the final assertion it's guaranteed to succeed.\r\n\r\nAlso, `com.datastax.oss.driver.Assertions` has an `assertThatStage` with a few convenient methods. Here you could use `isSuccess`, which already has a built-in 2-second timeout:\r\n\r\n```java\r\nassertThatStage(result).isSuccess();\r\n```"", 'commenter': 'olim7t'}]"
1465,integration-tests/src/test/java/com/datastax/oss/driver/core/TCPFlowControlIT.java,"@@ -0,0 +1,251 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.core;
+
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.noRows;
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.when;
+import static java.util.Collections.emptyList;
+import static java.util.Collections.emptyMap;
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.assertj.core.api.Assertions.assertThatThrownBy;
+
+import com.codahale.metrics.Gauge;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.DriverTimeoutException;
+import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
+import com.datastax.oss.driver.api.core.cql.AsyncResultSet;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.api.core.data.ByteUtils;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import com.datastax.oss.driver.api.core.metrics.DefaultNodeMetric;
+import com.datastax.oss.driver.api.testinfra.session.SessionUtils;
+import com.datastax.oss.driver.api.testinfra.simulacron.SimulacronRule;
+import com.datastax.oss.driver.categories.IsolatedTests;
+import com.datastax.oss.driver.internal.core.session.DefaultSession;
+import com.datastax.oss.simulacron.common.cluster.ClusterSpec;
+import com.datastax.oss.simulacron.common.request.Query;
+import com.datastax.oss.simulacron.server.BoundNode;
+import java.net.SocketAddress;
+import java.nio.ByteBuffer;
+import java.time.Duration;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.concurrent.CompletionStage;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
+import java.util.stream.Collectors;
+import org.awaitility.Awaitility;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+@Category(IsolatedTests.class)
+public class TCPFlowControlIT {
+  @ClassRule
+  public static final SimulacronRule SIMULACRON_RULE =
+      new SimulacronRule(ClusterSpec.builder().withNodes(3));
+
+  // this number is calculated empirically to be high enough to saturate the underling TCP buffer
+  private static int NUMBER_OF_SUBMITTED_REQUESTS = 2048;
+
+  @Test
+  public void should_not_write_more_requests_to_the_socket_after_the_server_paused_reading()
+      throws InterruptedException, ExecutionException, TimeoutException {
+    byte[] buffer = new byte[10240];
+    Arrays.fill(buffer, (byte) 1);
+    ByteBuffer buffer10Kb = ByteBuffer.wrap(buffer);
+
+    String queryString =
+        String.format(""INSERT INTO table1 (id) VALUES (%s)"", ByteUtils.toHexString(buffer10Kb));
+    Query query = new Query(queryString, emptyList(), emptyMap(), emptyMap());
+
+    SIMULACRON_RULE.cluster().prime(when(query).then(noRows()));
+
+    try (CqlSession session = SessionUtils.newSession(SIMULACRON_RULE)) {
+      SIMULACRON_RULE.cluster().pauseRead();
+
+      // The TCP send and receive buffer size depends on the OS
+      // We don't know how much data is needed to be flushed in order for OS to signal as full (and
+      // Channel#isWritable return false)
+      // Send 20Mb+ to each node
+      List<CompletionStage<AsyncResultSet>> pendingRequests = new ArrayList<>();
+      for (int i = 0; i < NUMBER_OF_SUBMITTED_REQUESTS; i++) {
+        pendingRequests.add(session.executeAsync(SimpleStatement.newInstance(queryString)));
+      }
+
+      // Assert that there are still requests that haven't been written
+      waitForWriteQueueToStabilize(session);
+      assertThat(getWriteQueueSize(session)).isGreaterThan(1);
+
+      SIMULACRON_RULE.cluster().resumeRead();
+
+      int numberOfFinished = 0;
+      for (CompletionStage<AsyncResultSet> result : pendingRequests) {
+        result.toCompletableFuture().get(1, TimeUnit.SECONDS);
+        numberOfFinished = numberOfFinished + 1;
+      }
+
+      int writeQueueSize = getWriteQueueSize(session);
+      assertThat(writeQueueSize).isEqualTo(0);
+      assertThat(numberOfFinished).isEqualTo(NUMBER_OF_SUBMITTED_REQUESTS);
+    }
+  }
+
+  @Test
+  public void should_process_requests_successfully_on_non_paused_nodes()
+      throws InterruptedException, ExecutionException, TimeoutException {
+    byte[] buffer = new byte[10240];
+    Arrays.fill(buffer, (byte) 1);
+    ByteBuffer buffer10Kb = ByteBuffer.wrap(buffer);
+
+    String queryString =
+        String.format(""INSERT INTO table1 (id) VALUES (%s)"", ByteUtils.toHexString(buffer10Kb));
+    Query query = new Query(queryString, emptyList(), emptyMap(), emptyMap());
+
+    SIMULACRON_RULE.cluster().prime(when(query).then(noRows()));
+
+    try (CqlSession session =
+        SessionUtils.newSession(
+            SIMULACRON_RULE,
+            SessionUtils.configLoaderBuilder()
+                .withStringList(
+                    DefaultDriverOption.METRICS_NODE_ENABLED,
+                    Collections.singletonList(""pool.in-flight""))
+                .build())) {
+      int hostIndex = 2;
+      BoundNode pausedNode = SIMULACRON_RULE.cluster().node(hostIndex);
+      SocketAddress pausedHostAddress = pausedNode.getAddress();
+      List<Node> nonPausedNodes =
+          session.getMetadata().getNodes().values().stream()
+              .filter(n -> !pausedHostAddress.equals(n.getBroadcastRpcAddress().get()))
+              .collect(Collectors.toList());
+      assertThat(nonPausedNodes.size()).isEqualTo(2);
+
+      SIMULACRON_RULE.cluster().node(2).pauseRead();","[{'comment': ""It's the right value but I guess it would be better to reuse the existing variable?\r\n```suggestion\r\n      SIMULACRON_RULE.cluster().node(hostIndex).pauseRead();\r\n```"", 'commenter': 'olim7t'}]"
1465,integration-tests/src/test/java/com/datastax/oss/driver/core/TCPFlowControlIT.java,"@@ -0,0 +1,251 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.core;
+
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.noRows;
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.when;
+import static java.util.Collections.emptyList;
+import static java.util.Collections.emptyMap;
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.assertj.core.api.Assertions.assertThatThrownBy;
+
+import com.codahale.metrics.Gauge;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.DriverTimeoutException;
+import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
+import com.datastax.oss.driver.api.core.cql.AsyncResultSet;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.api.core.data.ByteUtils;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import com.datastax.oss.driver.api.core.metrics.DefaultNodeMetric;
+import com.datastax.oss.driver.api.testinfra.session.SessionUtils;
+import com.datastax.oss.driver.api.testinfra.simulacron.SimulacronRule;
+import com.datastax.oss.driver.categories.IsolatedTests;
+import com.datastax.oss.driver.internal.core.session.DefaultSession;
+import com.datastax.oss.simulacron.common.cluster.ClusterSpec;
+import com.datastax.oss.simulacron.common.request.Query;
+import com.datastax.oss.simulacron.server.BoundNode;
+import java.net.SocketAddress;
+import java.nio.ByteBuffer;
+import java.time.Duration;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.concurrent.CompletionStage;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
+import java.util.stream.Collectors;
+import org.awaitility.Awaitility;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+@Category(IsolatedTests.class)
+public class TCPFlowControlIT {
+  @ClassRule
+  public static final SimulacronRule SIMULACRON_RULE =
+      new SimulacronRule(ClusterSpec.builder().withNodes(3));
+
+  // this number is calculated empirically to be high enough to saturate the underling TCP buffer
+  private static int NUMBER_OF_SUBMITTED_REQUESTS = 2048;
+
+  @Test
+  public void should_not_write_more_requests_to_the_socket_after_the_server_paused_reading()
+      throws InterruptedException, ExecutionException, TimeoutException {
+    byte[] buffer = new byte[10240];
+    Arrays.fill(buffer, (byte) 1);
+    ByteBuffer buffer10Kb = ByteBuffer.wrap(buffer);
+
+    String queryString =
+        String.format(""INSERT INTO table1 (id) VALUES (%s)"", ByteUtils.toHexString(buffer10Kb));
+    Query query = new Query(queryString, emptyList(), emptyMap(), emptyMap());
+
+    SIMULACRON_RULE.cluster().prime(when(query).then(noRows()));
+
+    try (CqlSession session = SessionUtils.newSession(SIMULACRON_RULE)) {
+      SIMULACRON_RULE.cluster().pauseRead();
+
+      // The TCP send and receive buffer size depends on the OS
+      // We don't know how much data is needed to be flushed in order for OS to signal as full (and
+      // Channel#isWritable return false)
+      // Send 20Mb+ to each node
+      List<CompletionStage<AsyncResultSet>> pendingRequests = new ArrayList<>();
+      for (int i = 0; i < NUMBER_OF_SUBMITTED_REQUESTS; i++) {
+        pendingRequests.add(session.executeAsync(SimpleStatement.newInstance(queryString)));","[{'comment': '```suggestion\r\n        pendingRequests.add(session.executeAsync(queryString));\r\n```', 'commenter': 'olim7t'}]"
1465,integration-tests/src/test/java/com/datastax/oss/driver/core/TCPFlowControlIT.java,"@@ -0,0 +1,251 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.core;
+
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.noRows;
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.when;
+import static java.util.Collections.emptyList;
+import static java.util.Collections.emptyMap;
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.assertj.core.api.Assertions.assertThatThrownBy;
+
+import com.codahale.metrics.Gauge;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.DriverTimeoutException;
+import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
+import com.datastax.oss.driver.api.core.cql.AsyncResultSet;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.api.core.data.ByteUtils;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import com.datastax.oss.driver.api.core.metrics.DefaultNodeMetric;
+import com.datastax.oss.driver.api.testinfra.session.SessionUtils;
+import com.datastax.oss.driver.api.testinfra.simulacron.SimulacronRule;
+import com.datastax.oss.driver.categories.IsolatedTests;
+import com.datastax.oss.driver.internal.core.session.DefaultSession;
+import com.datastax.oss.simulacron.common.cluster.ClusterSpec;
+import com.datastax.oss.simulacron.common.request.Query;
+import com.datastax.oss.simulacron.server.BoundNode;
+import java.net.SocketAddress;
+import java.nio.ByteBuffer;
+import java.time.Duration;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.concurrent.CompletionStage;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
+import java.util.stream.Collectors;
+import org.awaitility.Awaitility;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+@Category(IsolatedTests.class)
+public class TCPFlowControlIT {
+  @ClassRule
+  public static final SimulacronRule SIMULACRON_RULE =
+      new SimulacronRule(ClusterSpec.builder().withNodes(3));
+
+  // this number is calculated empirically to be high enough to saturate the underling TCP buffer
+  private static int NUMBER_OF_SUBMITTED_REQUESTS = 2048;
+
+  @Test
+  public void should_not_write_more_requests_to_the_socket_after_the_server_paused_reading()
+      throws InterruptedException, ExecutionException, TimeoutException {
+    byte[] buffer = new byte[10240];
+    Arrays.fill(buffer, (byte) 1);
+    ByteBuffer buffer10Kb = ByteBuffer.wrap(buffer);
+
+    String queryString =
+        String.format(""INSERT INTO table1 (id) VALUES (%s)"", ByteUtils.toHexString(buffer10Kb));
+    Query query = new Query(queryString, emptyList(), emptyMap(), emptyMap());
+
+    SIMULACRON_RULE.cluster().prime(when(query).then(noRows()));
+
+    try (CqlSession session = SessionUtils.newSession(SIMULACRON_RULE)) {
+      SIMULACRON_RULE.cluster().pauseRead();
+
+      // The TCP send and receive buffer size depends on the OS
+      // We don't know how much data is needed to be flushed in order for OS to signal as full (and
+      // Channel#isWritable return false)
+      // Send 20Mb+ to each node
+      List<CompletionStage<AsyncResultSet>> pendingRequests = new ArrayList<>();
+      for (int i = 0; i < NUMBER_OF_SUBMITTED_REQUESTS; i++) {
+        pendingRequests.add(session.executeAsync(SimpleStatement.newInstance(queryString)));
+      }
+
+      // Assert that there are still requests that haven't been written
+      waitForWriteQueueToStabilize(session);
+      assertThat(getWriteQueueSize(session)).isGreaterThan(1);
+
+      SIMULACRON_RULE.cluster().resumeRead();
+
+      int numberOfFinished = 0;
+      for (CompletionStage<AsyncResultSet> result : pendingRequests) {
+        result.toCompletableFuture().get(1, TimeUnit.SECONDS);
+        numberOfFinished = numberOfFinished + 1;
+      }
+
+      int writeQueueSize = getWriteQueueSize(session);
+      assertThat(writeQueueSize).isEqualTo(0);
+      assertThat(numberOfFinished).isEqualTo(NUMBER_OF_SUBMITTED_REQUESTS);
+    }
+  }
+
+  @Test
+  public void should_process_requests_successfully_on_non_paused_nodes()
+      throws InterruptedException, ExecutionException, TimeoutException {
+    byte[] buffer = new byte[10240];
+    Arrays.fill(buffer, (byte) 1);
+    ByteBuffer buffer10Kb = ByteBuffer.wrap(buffer);
+
+    String queryString =
+        String.format(""INSERT INTO table1 (id) VALUES (%s)"", ByteUtils.toHexString(buffer10Kb));
+    Query query = new Query(queryString, emptyList(), emptyMap(), emptyMap());
+
+    SIMULACRON_RULE.cluster().prime(when(query).then(noRows()));
+
+    try (CqlSession session =
+        SessionUtils.newSession(
+            SIMULACRON_RULE,
+            SessionUtils.configLoaderBuilder()
+                .withStringList(
+                    DefaultDriverOption.METRICS_NODE_ENABLED,
+                    Collections.singletonList(""pool.in-flight""))
+                .build())) {
+      int hostIndex = 2;
+      BoundNode pausedNode = SIMULACRON_RULE.cluster().node(hostIndex);
+      SocketAddress pausedHostAddress = pausedNode.getAddress();
+      List<Node> nonPausedNodes =
+          session.getMetadata().getNodes().values().stream()
+              .filter(n -> !pausedHostAddress.equals(n.getBroadcastRpcAddress().get()))
+              .collect(Collectors.toList());
+      assertThat(nonPausedNodes.size()).isEqualTo(2);
+
+      SIMULACRON_RULE.cluster().node(2).pauseRead();
+
+      List<CompletionStage<AsyncResultSet>> pendingRequests = new ArrayList<>();
+      for (int i = 0; i < NUMBER_OF_SUBMITTED_REQUESTS; i++) {
+        pendingRequests.add(session.executeAsync(SimpleStatement.newInstance(queryString)));
+      }
+
+      // Non-paused nodes should process the requests correctly
+      for (Node n : nonPausedNodes) {
+        Awaitility.await()
+            .until(
+                () -> {
+                  int inFlightRequests = getInFlightRequests(session, n);
+                  return inFlightRequests == 0;
+                });
+      }
+
+      // There are still requests that haven't been written
+      waitForWriteQueueToStabilize(session);
+      assertThat(getWriteQueueSize(session)).isGreaterThan(1);
+
+      SIMULACRON_RULE.cluster().resumeRead();
+
+      int numberOfFinished = 0;
+      for (CompletionStage<AsyncResultSet> result : pendingRequests) {
+        result.toCompletableFuture().get(1, TimeUnit.SECONDS);
+        numberOfFinished = numberOfFinished + 1;
+      }
+
+      int writeQueueSize = getWriteQueueSize(session);
+      assertThat(writeQueueSize).isEqualTo(0);
+      assertThat(numberOfFinished).isEqualTo(NUMBER_OF_SUBMITTED_REQUESTS);
+    }
+  }
+
+  @Test
+  public void should_timeout_requests_when_the_server_paused_reading_without_resuming()
+      throws InterruptedException {
+    byte[] buffer = new byte[10240];
+    Arrays.fill(buffer, (byte) 1);
+    ByteBuffer buffer10Kb = ByteBuffer.wrap(buffer);
+
+    String queryString =
+        String.format(""INSERT INTO table1 (id) VALUES (%s)"", ByteUtils.toHexString(buffer10Kb));
+    Query query = new Query(queryString, emptyList(), emptyMap(), emptyMap());
+
+    SIMULACRON_RULE.cluster().prime(when(query).then(noRows()));
+
+    try (CqlSession session = SessionUtils.newSession(SIMULACRON_RULE)) {
+      SIMULACRON_RULE.cluster().pauseRead();
+
+      List<CompletionStage<AsyncResultSet>> pendingRequests = new ArrayList<>();
+      for (int i = 0; i < NUMBER_OF_SUBMITTED_REQUESTS; i++) {
+        pendingRequests.add(session.executeAsync(SimpleStatement.newInstance(queryString)));
+      }
+
+      // Assert that there are still requests that haven't been written
+      waitForWriteQueueToStabilize(session);
+      assertThat(getWriteQueueSize(session)).isGreaterThan(1);
+
+      // do not resumeRead
+      for (CompletionStage<AsyncResultSet> result : pendingRequests) {
+        assertThatThrownBy(() -> result.toCompletableFuture().get())
+            .isInstanceOf(ExecutionException.class)
+            .hasCauseInstanceOf(DriverTimeoutException.class);","[{'comment': 'Using the same driver class that I mentioned earlier:\r\n```java\r\n        assertThatStage(result)\r\n            .isFailed(e -> assertThat(e).isInstanceOf(DriverTimeoutException.class));\r\n```\r\nOr you could do it in pure AssertJ:\r\n```java\r\n        assertThat(result).hasFailedWithThrowableThat().isInstanceOf(DriverTimeoutException.class);\r\n```', 'commenter': 'olim7t'}, {'comment': ""None of the above versions worked for me, the test hangs. As I don't have much time to investigate I will just leave the code as is now."", 'commenter': 'adutra'}, {'comment': 'Actually this test hangs very often. An assertion fails, then there is a problem with the session closing sequence that never ends.', 'commenter': 'adutra'}]"
1465,integration-tests/src/test/java/com/datastax/oss/driver/core/TCPFlowControlIT.java,"@@ -0,0 +1,251 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.core;
+
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.noRows;
+import static com.datastax.oss.simulacron.common.stubbing.PrimeDsl.when;
+import static java.util.Collections.emptyList;
+import static java.util.Collections.emptyMap;
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.assertj.core.api.Assertions.assertThatThrownBy;
+
+import com.codahale.metrics.Gauge;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.DriverTimeoutException;
+import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
+import com.datastax.oss.driver.api.core.cql.AsyncResultSet;
+import com.datastax.oss.driver.api.core.cql.SimpleStatement;
+import com.datastax.oss.driver.api.core.data.ByteUtils;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import com.datastax.oss.driver.api.core.metrics.DefaultNodeMetric;
+import com.datastax.oss.driver.api.testinfra.session.SessionUtils;
+import com.datastax.oss.driver.api.testinfra.simulacron.SimulacronRule;
+import com.datastax.oss.driver.categories.IsolatedTests;
+import com.datastax.oss.driver.internal.core.session.DefaultSession;
+import com.datastax.oss.simulacron.common.cluster.ClusterSpec;
+import com.datastax.oss.simulacron.common.request.Query;
+import com.datastax.oss.simulacron.server.BoundNode;
+import java.net.SocketAddress;
+import java.nio.ByteBuffer;
+import java.time.Duration;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.concurrent.CompletionStage;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
+import java.util.stream.Collectors;
+import org.awaitility.Awaitility;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+@Category(IsolatedTests.class)
+public class TCPFlowControlIT {
+  @ClassRule
+  public static final SimulacronRule SIMULACRON_RULE =
+      new SimulacronRule(ClusterSpec.builder().withNodes(3));
+
+  // this number is calculated empirically to be high enough to saturate the underling TCP buffer
+  private static int NUMBER_OF_SUBMITTED_REQUESTS = 2048;
+
+  @Test
+  public void should_not_write_more_requests_to_the_socket_after_the_server_paused_reading()
+      throws InterruptedException, ExecutionException, TimeoutException {
+    byte[] buffer = new byte[10240];
+    Arrays.fill(buffer, (byte) 1);
+    ByteBuffer buffer10Kb = ByteBuffer.wrap(buffer);
+
+    String queryString =
+        String.format(""INSERT INTO table1 (id) VALUES (%s)"", ByteUtils.toHexString(buffer10Kb));
+    Query query = new Query(queryString, emptyList(), emptyMap(), emptyMap());
+
+    SIMULACRON_RULE.cluster().prime(when(query).then(noRows()));
+
+    try (CqlSession session = SessionUtils.newSession(SIMULACRON_RULE)) {
+      SIMULACRON_RULE.cluster().pauseRead();
+
+      // The TCP send and receive buffer size depends on the OS
+      // We don't know how much data is needed to be flushed in order for OS to signal as full (and
+      // Channel#isWritable return false)
+      // Send 20Mb+ to each node
+      List<CompletionStage<AsyncResultSet>> pendingRequests = new ArrayList<>();
+      for (int i = 0; i < NUMBER_OF_SUBMITTED_REQUESTS; i++) {
+        pendingRequests.add(session.executeAsync(SimpleStatement.newInstance(queryString)));
+      }
+
+      // Assert that there are still requests that haven't been written
+      waitForWriteQueueToStabilize(session);
+      assertThat(getWriteQueueSize(session)).isGreaterThan(1);
+
+      SIMULACRON_RULE.cluster().resumeRead();
+
+      int numberOfFinished = 0;
+      for (CompletionStage<AsyncResultSet> result : pendingRequests) {
+        result.toCompletableFuture().get(1, TimeUnit.SECONDS);
+        numberOfFinished = numberOfFinished + 1;
+      }
+
+      int writeQueueSize = getWriteQueueSize(session);
+      assertThat(writeQueueSize).isEqualTo(0);
+      assertThat(numberOfFinished).isEqualTo(NUMBER_OF_SUBMITTED_REQUESTS);
+    }
+  }
+
+  @Test
+  public void should_process_requests_successfully_on_non_paused_nodes()
+      throws InterruptedException, ExecutionException, TimeoutException {
+    byte[] buffer = new byte[10240];
+    Arrays.fill(buffer, (byte) 1);
+    ByteBuffer buffer10Kb = ByteBuffer.wrap(buffer);
+
+    String queryString =
+        String.format(""INSERT INTO table1 (id) VALUES (%s)"", ByteUtils.toHexString(buffer10Kb));
+    Query query = new Query(queryString, emptyList(), emptyMap(), emptyMap());
+
+    SIMULACRON_RULE.cluster().prime(when(query).then(noRows()));
+
+    try (CqlSession session =
+        SessionUtils.newSession(
+            SIMULACRON_RULE,
+            SessionUtils.configLoaderBuilder()
+                .withStringList(
+                    DefaultDriverOption.METRICS_NODE_ENABLED,
+                    Collections.singletonList(""pool.in-flight""))
+                .build())) {
+      int hostIndex = 2;
+      BoundNode pausedNode = SIMULACRON_RULE.cluster().node(hostIndex);
+      SocketAddress pausedHostAddress = pausedNode.getAddress();
+      List<Node> nonPausedNodes =
+          session.getMetadata().getNodes().values().stream()
+              .filter(n -> !pausedHostAddress.equals(n.getBroadcastRpcAddress().get()))
+              .collect(Collectors.toList());
+      assertThat(nonPausedNodes.size()).isEqualTo(2);
+
+      SIMULACRON_RULE.cluster().node(2).pauseRead();
+
+      List<CompletionStage<AsyncResultSet>> pendingRequests = new ArrayList<>();
+      for (int i = 0; i < NUMBER_OF_SUBMITTED_REQUESTS; i++) {
+        pendingRequests.add(session.executeAsync(SimpleStatement.newInstance(queryString)));
+      }
+
+      // Non-paused nodes should process the requests correctly
+      for (Node n : nonPausedNodes) {
+        Awaitility.await()
+            .until(
+                () -> {
+                  int inFlightRequests = getInFlightRequests(session, n);
+                  return inFlightRequests == 0;
+                });
+      }
+
+      // There are still requests that haven't been written
+      waitForWriteQueueToStabilize(session);
+      assertThat(getWriteQueueSize(session)).isGreaterThan(1);
+
+      SIMULACRON_RULE.cluster().resumeRead();
+
+      int numberOfFinished = 0;
+      for (CompletionStage<AsyncResultSet> result : pendingRequests) {
+        result.toCompletableFuture().get(1, TimeUnit.SECONDS);
+        numberOfFinished = numberOfFinished + 1;
+      }
+
+      int writeQueueSize = getWriteQueueSize(session);
+      assertThat(writeQueueSize).isEqualTo(0);
+      assertThat(numberOfFinished).isEqualTo(NUMBER_OF_SUBMITTED_REQUESTS);
+    }
+  }
+
+  @Test
+  public void should_timeout_requests_when_the_server_paused_reading_without_resuming()
+      throws InterruptedException {
+    byte[] buffer = new byte[10240];
+    Arrays.fill(buffer, (byte) 1);
+    ByteBuffer buffer10Kb = ByteBuffer.wrap(buffer);
+
+    String queryString =
+        String.format(""INSERT INTO table1 (id) VALUES (%s)"", ByteUtils.toHexString(buffer10Kb));
+    Query query = new Query(queryString, emptyList(), emptyMap(), emptyMap());
+
+    SIMULACRON_RULE.cluster().prime(when(query).then(noRows()));
+
+    try (CqlSession session = SessionUtils.newSession(SIMULACRON_RULE)) {
+      SIMULACRON_RULE.cluster().pauseRead();
+
+      List<CompletionStage<AsyncResultSet>> pendingRequests = new ArrayList<>();
+      for (int i = 0; i < NUMBER_OF_SUBMITTED_REQUESTS; i++) {
+        pendingRequests.add(session.executeAsync(SimpleStatement.newInstance(queryString)));
+      }
+
+      // Assert that there are still requests that haven't been written
+      waitForWriteQueueToStabilize(session);
+      assertThat(getWriteQueueSize(session)).isGreaterThan(1);
+
+      // do not resumeRead
+      for (CompletionStage<AsyncResultSet> result : pendingRequests) {
+        assertThatThrownBy(() -> result.toCompletableFuture().get())
+            .isInstanceOf(ExecutionException.class)
+            .hasCauseInstanceOf(DriverTimeoutException.class);
+      }
+
+      // write queue size is still non empty
+      int writeQueueSize = getWriteQueueSize(session);
+      assertThat(writeQueueSize).isGreaterThan(1);
+      SIMULACRON_RULE.cluster().stop();","[{'comment': 'I think this is a leftover, the rule will take care of that.', 'commenter': 'olim7t'}]"
1467,core/src/main/java/com/datastax/oss/driver/api/core/session/SessionBuilder.java,"@@ -488,6 +489,12 @@ public SelfT withCloudSecureConnectBundle(@NonNull Path cloudConfigPath) {
     return self;
   }
 
+  /** Registers a CodecRegistry to use for the session. */","[{'comment': 'Can you explain the interaction with `addTypeCodecs`, when both are called?', 'commenter': 'olim7t'}, {'comment': ""I've expanded the javadoc to cover this."", 'commenter': 'wimtie'}]"
1467,core/src/main/java/com/datastax/oss/driver/internal/core/context/DefaultDriverContext.java,"@@ -563,9 +562,12 @@ protected RequestProcessorRegistry buildRequestProcessorRegistry() {
     return new RequestProcessorRegistry(logPrefix, processors.toArray(new RequestProcessor[0]));
   }
 
-  protected CodecRegistry buildCodecRegistry(String logPrefix, List<TypeCodec<?>> codecs) {
-    MutableCodecRegistry registry = new DefaultCodecRegistry(logPrefix);
-    registry.register(codecs);
+  protected CodecRegistry getCodecRegistry(ProgrammaticArguments arguments) {","[{'comment': 'Why change the method name? This is not a huge deal, but the rest of the class follows the convention `buildXxx` for the method that creates the object (either at construction time or when the lazy reference gets accessed), and `getXxx` for the public method that clients call to get the component. We already have a no-arg `getCodecRegistry` for the latter.', 'commenter': 'olim7t'}, {'comment': 'Fully agreed, undid the rename.', 'commenter': 'wimtie'}]"
1467,core/src/main/java/com/datastax/oss/driver/internal/core/context/DefaultDriverContext.java,"@@ -563,9 +562,12 @@ protected RequestProcessorRegistry buildRequestProcessorRegistry() {
     return new RequestProcessorRegistry(logPrefix, processors.toArray(new RequestProcessor[0]));
   }
 
-  protected CodecRegistry buildCodecRegistry(String logPrefix, List<TypeCodec<?>> codecs) {
-    MutableCodecRegistry registry = new DefaultCodecRegistry(logPrefix);
-    registry.register(codecs);
+  protected CodecRegistry getCodecRegistry(ProgrammaticArguments arguments) {
+    MutableCodecRegistry registry = (MutableCodecRegistry) arguments.getCodecRegistry();","[{'comment': ""So, that `MutableCodecRegistry` type is a bit awkward, it only exists for backward compatibility issue (maybe you've seen that in the javadocs already). While all registry implementations *should* extend it, it's possible for someone to pass an implementation that doesn't, we can't risk an exception here.\r\n\r\nOne option is to do an `instanceof` check, and skip the registration of the codecs if it fails. We should also log a warning, because if someone provided codecs they expect them to work.\r\n\r\nAnother is to require the mutable type directly at build time: `SessionBuilder.withCodecRegistry(MutableCodecRegistry)`.\r\n\r\nI prefer the latter, WDYT?"", 'commenter': 'olim7t'}, {'comment': ""The former option feels more correct, while the second adds less clutter to the code. I find it hard to decide, so I'll defer to you and go with MutableCodecRegistry all the way."", 'commenter': 'wimtie'}, {'comment': 'Sounds good. Another way to put it is that MutableCodecRegistry is the type that should have been exposed publicly from the start.', 'commenter': 'olim7t'}, {'comment': 'All the changes should be there, thanks again for reviewing!', 'commenter': 'wimtie'}]"
1467,core/src/main/java/com/datastax/oss/driver/api/core/session/SessionBuilder.java,"@@ -488,6 +489,18 @@ public SelfT withCloudSecureConnectBundle(@NonNull Path cloudConfigPath) {
     return self;
   }
 
+  /**
+   * Registers a CodecRegistry to use for the session.
+   *
+   * <p>When both this and {@link #addTypeCodecs(TypeCodec[])} are called, the added type codecs
+   * will be registered on the provided CodecRegistry.
+   */
+  @NonNull
+  public SelfT withCodecRegistry(@Nullable MutableCodecRegistry codecRegistry) {","[{'comment': ""I changed the parameter from NonNull to Nullable. This would allow someone to clear the registry on a builder where it was configured previously. Not very likely but the implementation allows it, so why not.\r\n\r\n(there are other attributes on the builder that don't follow this pattern, I think it's an oversight on our side, I'll take a look)"", 'commenter': 'olim7t'}]"
1469,core/src/main/java/com/datastax/oss/driver/internal/core/config/typesafe/DefaultDriverConfigLoader.java,"@@ -58,8 +60,10 @@
         ConfigFactory.invalidateCaches();
         // The thread's context class loader will be used for application classpath resources,
         // while the driver class loader will be used for reference classpath resources.
-        return ConfigFactory.defaultApplication()","[{'comment': 'We have also a similar pattern in the `DefaultProgrammaticDriverConfigLoaderBuilder`:\r\n```\r\n  public static final Supplier<Config> DEFAULT_FALLBACK_SUPPLIER =\r\n      () ->\r\n          ConfigFactory.defaultApplication()\r\n              // Do not remove root path here, it must be done after merging configs\r\n              .withFallback(ConfigFactory.defaultReference(CqlSession.class.getClassLoader()));\r\n```\r\nand \r\n```\r\n  public DefaultProgrammaticDriverConfigLoaderBuilder(@NonNull ClassLoader appClassLoader) {\r\n    this(\r\n        () ->\r\n            ConfigFactory.defaultApplication(appClassLoader)\r\n                .withFallback(ConfigFactory.defaultReference(CqlSession.class.getClassLoader())),\r\n        DefaultDriverConfigLoader.DEFAULT_ROOT_PATH);\r\n  }\r\n```\r\nI am not sure if progrmatticConfigLoader should also accept `ConfigFactory.defaultOverrides()`, wdyt?', 'commenter': 'tomekl007'}, {'comment': 'No it should not change, the default overrides are applied later on, see [here](https://github.com/datastax/java-driver/blob/1427be868bcbb115258e87cccb1cffcf54c82e04/core/src/main/java/com/datastax/oss/driver/internal/core/config/typesafe/DefaultProgrammaticDriverConfigLoaderBuilder.java#L250).', 'commenter': 'adutra'}]"
1469,core/src/main/java/com/datastax/oss/driver/internal/core/config/typesafe/DefaultDriverConfigLoader.java,"@@ -58,8 +60,10 @@
         ConfigFactory.invalidateCaches();
         // The thread's context class loader will be used for application classpath resources,
         // while the driver class loader will be used for reference classpath resources.
-        return ConfigFactory.defaultApplication()
-            .withFallback(ConfigFactory.defaultReference(CqlSession.class.getClassLoader()))
+        return ConfigFactory.defaultOverrides()
+            .withFallback(
+                ConfigFactory.defaultApplication()
+                    .withFallback(defaultReference(CqlSession.class.getClassLoader())))","[{'comment': 'I just noticed: we are also missing a `.resolve()` at the end.', 'commenter': 'adutra'}]"
1469,core/src/main/java/com/datastax/oss/driver/internal/core/config/typesafe/DefaultDriverConfigLoader.java,"@@ -95,8 +99,11 @@ public DefaultDriverConfigLoader(@NonNull ClassLoader appClassLoader) {
     this(
         () -> {
           ConfigFactory.invalidateCaches();
-          return ConfigFactory.defaultApplication(appClassLoader)
-              .withFallback(ConfigFactory.defaultReference(CqlSession.class.getClassLoader()))
+          return ConfigFactory.defaultOverrides()
+              .withFallback(
+                  ConfigFactory.defaultApplication(appClassLoader)
+                      .withFallback(
+                          ConfigFactory.defaultReference(CqlSession.class.getClassLoader())))","[{'comment': 'Same here we need to call `.resolve()`', 'commenter': 'adutra'}]"
1469,core/src/test/java/com/datastax/oss/driver/internal/core/config/map/MapBasedDriverConfigLoaderTest.java,"@@ -58,7 +60,14 @@ public void should_fill_default_profile_like_reference_file() {
     DriverExecutionProfile mapBasedConfig =
         DriverConfigLoader.fromMap(optionsMap).getInitialConfig().getDefaultProfile();
     DriverExecutionProfile fileBasedConfig =
-        new DefaultDriverConfigLoader().getInitialConfig().getDefaultProfile();
+        new DefaultDriverConfigLoader(
+                () -> {
+                  // Only load reference.conf since we are focusing on driver defaults
+                  ConfigFactory.invalidateCaches();","[{'comment': 'required because the addition of an application.conf to the classpath breaks this test.', 'commenter': 'adutra'}]"
1469,core/src/main/java/com/datastax/oss/driver/internal/core/config/typesafe/DefaultDriverConfigLoader.java,"@@ -58,8 +60,11 @@
         ConfigFactory.invalidateCaches();
         // The thread's context class loader will be used for application classpath resources,
         // while the driver class loader will be used for reference classpath resources.
-        return ConfigFactory.defaultApplication()
-            .withFallback(ConfigFactory.defaultReference(CqlSession.class.getClassLoader()))
+        return ConfigFactory.defaultOverrides()
+            .withFallback(
+                ConfigFactory.defaultApplication()
+                    .withFallback(defaultReference(CqlSession.class.getClassLoader())))","[{'comment': ""Nit: withFallback calls can be chained, this is more readable. I'll fix it directly as part of the merge."", 'commenter': 'olim7t'}]"
1476,core/src/main/java/com/datastax/oss/driver/api/core/session/SessionBuilder.java,"@@ -671,8 +675,11 @@ public SessionT build() {
           defaultConfig.getStringList(DefaultDriverOption.CONTACT_POINTS, Collections.emptyList());
       if (cloudConfigInputStream != null) {
         if (!programmaticContactPoints.isEmpty() || !configContactPoints.isEmpty()) {
-          throw new IllegalStateException(
-              ""Can't use withCloudSecureConnectBundle and addContactPoint(s). They are mutually exclusive."");
+          LOG.info(
+              ""The withCloudSecureConnectBundle and addContactPoint(s) were provided. They are mutually exclusive. The addContactPoint(s) setting will be ignored."");
+          // clear the contact points provided in the setting file and via addContactPoints
+          configContactPoints = Collections.emptyList();
+          programmaticContactPoints = new HashSet<>();
         }
         String configuredSSLFactory =","[{'comment': 'I am not sure if we should ignore (not fail) for SSL as well.\r\nThe `defaultConfig.getString(DefaultDriverOption.SSL_ENGINE_FACTORY_CLASS, null);` \r\nsetting is used in multiple places, for example:\r\n`DefaultDriverContext`:\r\n```\r\nprotected Optional<SslEngineFactory> buildSslEngineFactory(SslEngineFactory factoryFromBuilder) {\r\n    return (factoryFromBuilder != null)\r\n        ? Optional.of(factoryFromBuilder)\r\n        : Reflection.buildFromConfig(\r\n            this,\r\n            DefaultDriverOption.SSL_ENGINE_FACTORY_CLASS,\r\n            SslEngineFactory.class,\r\n            ""com.datastax.oss.driver.internal.core.ssl"");\r\n  }\r\n```\r\nIgnoring this property will work now, but it seems fragile and may lead to some problems in the future (If we would change one of the usages of `DefaultDriverOption.SSL_ENGINE_FACTORY_CLASS`)\r\nFor `contactPoints` ignoring is not problematic because usage of `configContactPoints` and `programmaticContactPoints` is encapsulated in the `SessionBuilder`, wdty?', 'commenter': 'tomekl007'}, {'comment': 'Any SSL configuration will be overridden by line 694 if a secure connect bundle is present:\r\n\r\n```\r\nwithSslEngineFactory(cloudConfig.getSslEngineFactory());\r\n```\r\n\r\nSo I think we can just replace lines 687-688 by a warning and be done with it :-)', 'commenter': 'adutra'}, {'comment': 'However we should add a warning as well for local datacenter. Same pattern as for contact points.', 'commenter': 'adutra'}, {'comment': 'It is a bit problematic for contact points because it fails with:\r\n```\r\n Multiple entries with same key: default=dc1 and default=dc-ignored\r\n```\r\nIt is using the \r\n`\r\nprivate ImmutableMap.Builder<String, String> localDatacentersBuilder = ImmutableMap.builder();\r\n`\r\nunderneath and it does not support removing. Are we ok with changing this?', 'commenter': 'tomekl007'}, {'comment': 'You can add a new method `ProgrammaticArguments.Builder.clearDatacenters()`.', 'commenter': 'olim7t'}]"
1476,core/src/main/java/com/datastax/oss/driver/api/core/session/SessionBuilder.java,"@@ -671,8 +675,11 @@ public SessionT build() {
           defaultConfig.getStringList(DefaultDriverOption.CONTACT_POINTS, Collections.emptyList());
       if (cloudConfigInputStream != null) {
         if (!programmaticContactPoints.isEmpty() || !configContactPoints.isEmpty()) {
-          throw new IllegalStateException(
-              ""Can't use withCloudSecureConnectBundle and addContactPoint(s). They are mutually exclusive."");
+          LOG.info(
+              ""The withCloudSecureConnectBundle and addContactPoint(s) were provided. They are mutually exclusive. The addContactPoint(s) setting will be ignored."");","[{'comment': '```suggestion\r\n              ""Both a secure connect bundle and contact points were provided. These are mutually exclusive. All contact points will be ignored."");\r\n```', 'commenter': 'adutra'}]"
1476,examples/src/main/java/com/datastax/oss/driver/examples/astra/AstraReadCassandraVersion.java,"@@ -57,7 +56,6 @@ public static void main(String[] args) {
             .withCloudSecureConnectBundle(Paths.get(""/path/to/secure-connect-database_name.zip""))
             // Change the user_name and password here for the Astra instance
             .withAuthCredentials(""user_name"", ""fakePasswordForTests"")
-            .withConfigLoader(DriverConfigLoader.fromClasspath(""application-astra""))","[{'comment': 'Nice that you remembered to change this ðŸ‘ ', 'commenter': 'adutra'}]"
1476,integration-tests/src/test/java/com/datastax/oss/driver/api/core/cloud/CloudIT.java,"@@ -210,18 +216,32 @@ public void should_connect_to_proxy_using_url_with_http_protocol_provided_in_the
   }
 
   @Test
-  public void should_error_when_contact_points_and_secure_bundle_used() {
+  public void should_connect_and_log_info_when_contact_points_and_secure_bundle_used() {","[{'comment': 'Add a similar test for local datacenter and SSL.', 'commenter': 'adutra'}]"
1476,integration-tests/src/test/java/com/datastax/oss/driver/api/core/cloud/CloudIT.java,"@@ -210,18 +216,32 @@ public void should_connect_to_proxy_using_url_with_http_protocol_provided_in_the
   }
 
   @Test
-  public void should_error_when_contact_points_and_secure_bundle_used() {
+  public void should_connect_and_log_info_when_contact_points_and_secure_bundle_used() {
     // given
+    LoggerTest.LoggerSetup logger = setupTestLogger(SessionBuilder.class, Level.INFO);
+
     Path bundle = proxyRule.getProxy().getBundleWithoutCredentialsPath();
-    CqlSessionBuilder builder =
+
+    ResultSet set;","[{'comment': 'Why declare this variable outside of the try-catch block?', 'commenter': 'adutra'}]"
1476,core/src/main/java/com/datastax/oss/driver/api/core/session/SessionBuilder.java,"@@ -671,18 +677,27 @@ public SessionT build() {
           defaultConfig.getStringList(DefaultDriverOption.CONTACT_POINTS, Collections.emptyList());
       if (cloudConfigInputStream != null) {
         if (!programmaticContactPoints.isEmpty() || !configContactPoints.isEmpty()) {
-          throw new IllegalStateException(
-              ""Can't use withCloudSecureConnectBundle and addContactPoint(s). They are mutually exclusive."");
+          LOG.info(
+              ""Both a secure connect bundle and contact points were provided. These are mutually exclusive. The contact points from a secure bundle will have priority."");","[{'comment': '```suggestion\r\n              ""Both a secure connect bundle and contact points were provided. These are mutually exclusive. The contact points from the secure bundle will have priority."");\r\n```', 'commenter': 'adutra'}]"
1476,core/src/main/java/com/datastax/oss/driver/api/core/session/SessionBuilder.java,"@@ -671,18 +677,27 @@ public SessionT build() {
           defaultConfig.getStringList(DefaultDriverOption.CONTACT_POINTS, Collections.emptyList());
       if (cloudConfigInputStream != null) {
         if (!programmaticContactPoints.isEmpty() || !configContactPoints.isEmpty()) {
-          throw new IllegalStateException(
-              ""Can't use withCloudSecureConnectBundle and addContactPoint(s). They are mutually exclusive."");
+          LOG.info(
+              ""Both a secure connect bundle and contact points were provided. These are mutually exclusive. The contact points from a secure bundle will have priority."");
+          // clear the contact points provided in the setting file and via addContactPoints
+          configContactPoints = Collections.emptyList();
+          programmaticContactPoints = new HashSet<>();
         }
         String configuredSSLFactory =
             defaultConfig.getString(DefaultDriverOption.SSL_ENGINE_FACTORY_CLASS, null);
         if (sslConfigured || configuredSSLFactory != null) {
-          throw new IllegalStateException(
-              ""Can't use withCloudSecureConnectBundle and explicitly specify ssl configuration. They are mutually exclusive."");
+          LOG.info(
+              ""Both withCloudSecureConnectBundle and explicitly specified ssl configuration were provided. They are mutually exclusive. The ssl settings from a secure bundle will have priority."");","[{'comment': '```suggestion\r\n              ""Both a secure connect bundle and SSL options were provided. They are mutually exclusive. The SSL options from the secure bundle will have priority."");\r\n```', 'commenter': 'adutra'}]"
1476,core/src/main/java/com/datastax/oss/driver/api/core/session/SessionBuilder.java,"@@ -671,18 +677,27 @@ public SessionT build() {
           defaultConfig.getStringList(DefaultDriverOption.CONTACT_POINTS, Collections.emptyList());
       if (cloudConfigInputStream != null) {
         if (!programmaticContactPoints.isEmpty() || !configContactPoints.isEmpty()) {
-          throw new IllegalStateException(
-              ""Can't use withCloudSecureConnectBundle and addContactPoint(s). They are mutually exclusive."");
+          LOG.info(
+              ""Both a secure connect bundle and contact points were provided. These are mutually exclusive. The contact points from a secure bundle will have priority."");
+          // clear the contact points provided in the setting file and via addContactPoints
+          configContactPoints = Collections.emptyList();
+          programmaticContactPoints = new HashSet<>();
         }
         String configuredSSLFactory =
             defaultConfig.getString(DefaultDriverOption.SSL_ENGINE_FACTORY_CLASS, null);
         if (sslConfigured || configuredSSLFactory != null) {
-          throw new IllegalStateException(
-              ""Can't use withCloudSecureConnectBundle and explicitly specify ssl configuration. They are mutually exclusive."");
+          LOG.info(
+              ""Both withCloudSecureConnectBundle and explicitly specified ssl configuration were provided. They are mutually exclusive. The ssl settings from a secure bundle will have priority."");
         }
         CloudConfig cloudConfig =
             new CloudConfigFactory().createCloudConfig(cloudConfigInputStream.call());
         addContactEndPoints(cloudConfig.getEndPoints());
+
+        if (localDatacenterConfigured) {","[{'comment': 'I think you should also inspect the configuration (same pattern done above for contact points and SSL).\r\n\r\nBut there is one difference: since you are clearing all datacenters, you should inspect all profiles for a datacenter.', 'commenter': 'adutra'}, {'comment': 'Following my suggestion above: rename to `programmaticLocalDatacenter`.', 'commenter': 'adutra'}]"
1476,core/src/main/java/com/datastax/oss/driver/api/core/session/SessionBuilder.java,"@@ -671,18 +677,27 @@ public SessionT build() {
           defaultConfig.getStringList(DefaultDriverOption.CONTACT_POINTS, Collections.emptyList());
       if (cloudConfigInputStream != null) {
         if (!programmaticContactPoints.isEmpty() || !configContactPoints.isEmpty()) {
-          throw new IllegalStateException(
-              ""Can't use withCloudSecureConnectBundle and addContactPoint(s). They are mutually exclusive."");
+          LOG.info(
+              ""Both a secure connect bundle and contact points were provided. These are mutually exclusive. The contact points from a secure bundle will have priority."");
+          // clear the contact points provided in the setting file and via addContactPoints
+          configContactPoints = Collections.emptyList();
+          programmaticContactPoints = new HashSet<>();
         }
         String configuredSSLFactory =
             defaultConfig.getString(DefaultDriverOption.SSL_ENGINE_FACTORY_CLASS, null);
         if (sslConfigured || configuredSSLFactory != null) {
-          throw new IllegalStateException(
-              ""Can't use withCloudSecureConnectBundle and explicitly specify ssl configuration. They are mutually exclusive."");
+          LOG.info(
+              ""Both withCloudSecureConnectBundle and explicitly specified ssl configuration were provided. They are mutually exclusive. The ssl settings from a secure bundle will have priority."");
         }
         CloudConfig cloudConfig =
             new CloudConfigFactory().createCloudConfig(cloudConfigInputStream.call());
         addContactEndPoints(cloudConfig.getEndPoints());
+
+        if (localDatacenterConfigured) {
+          LOG.info(
+              ""Both withCloudSecureConnectBundle and explicitly specified local datacenter configuration were provided. They are mutually exclusive. The local datacenter settings from a secure bundle will have priority."");","[{'comment': '```suggestion\r\n              ""Both a secure connect bundle and a local datacenter were provided. They are mutually exclusive. The local datacenter from the secure bundle will have priority."");\r\n```', 'commenter': 'adutra'}]"
1476,core/src/main/java/com/datastax/oss/driver/api/core/session/SessionBuilder.java,"@@ -671,18 +677,27 @@ public SessionT build() {
           defaultConfig.getStringList(DefaultDriverOption.CONTACT_POINTS, Collections.emptyList());
       if (cloudConfigInputStream != null) {
         if (!programmaticContactPoints.isEmpty() || !configContactPoints.isEmpty()) {
-          throw new IllegalStateException(
-              ""Can't use withCloudSecureConnectBundle and addContactPoint(s). They are mutually exclusive."");
+          LOG.info(
+              ""Both a secure connect bundle and contact points were provided. These are mutually exclusive. The contact points from a secure bundle will have priority."");
+          // clear the contact points provided in the setting file and via addContactPoints
+          configContactPoints = Collections.emptyList();
+          programmaticContactPoints = new HashSet<>();
         }
         String configuredSSLFactory =
             defaultConfig.getString(DefaultDriverOption.SSL_ENGINE_FACTORY_CLASS, null);
         if (sslConfigured || configuredSSLFactory != null) {","[{'comment': ""Minor suggestion: rename these variables to match the variable names used for contact points:\r\n\r\n```\r\nsslConfigured -> programmaticSslFactory\r\nconfiguredSSLFactory -> configSslFactory\r\n```\r\n\r\n(Also: `configuredSSLFactory` doesn't respect naming rules)"", 'commenter': 'adutra'}, {'comment': ""`programmaticSslFactory` for a boolean doesn't seem to be a good name. Maybe `isProgrammaticSslFactorySet` or similar?"", 'commenter': 'tomekl007'}]"
1478,core/src/main/java/com/datastax/oss/driver/internal/core/metrics/DropwizardMetricUpdater.java,"@@ -33,46 +40,80 @@
   private static final Logger LOG = LoggerFactory.getLogger(DropwizardMetricUpdater.class);
 
   protected final Set<MetricT> enabledMetrics;
+
   protected final MetricRegistry registry;
 
-  protected DropwizardMetricUpdater(Set<MetricT> enabledMetrics, MetricRegistry registry) {
+  protected final Cache<String, Metric> metricsCache;
+
+  protected DropwizardMetricUpdater(
+      Set<MetricT> enabledMetrics, MetricRegistry registry, Ticker ticker) {
+    this(enabledMetrics, registry, ticker, Duration.ofHours(1));
+  }
+
+  protected DropwizardMetricUpdater(
+      Set<MetricT> enabledMetrics, MetricRegistry registry, Ticker ticker, Duration expiresAfter) {
     this.enabledMetrics = enabledMetrics;
     this.registry = registry;
+    metricsCache =","[{'comment': ""I followed @adutra suggestion from this jira comment: https://datastax-oss.atlassian.net/browse/JAVA-2331?focusedCommentId=51936 to implement eviction of metrics using cache but I have doubts about this approach:\r\n- How to approach configuring the eviction time? \r\n- This solution is eventually consistent - our users will need to tolerate some non-negligible amount of time when there will be stale metrics - are we ok with it?\r\n- Are all metrics frequently updated? If we will pick 1 hour as a default eviction policy, can we assure that all metrics have a shorter update period?\r\n- What if the user will set the eviction time to very short amount? (i.e. 1 second) There will be a situation in which metrics will be removed when they shouldn't. We should probably restrict the lowest value that could be set to some high value (i.e. 1 hour)\r\n- keeping all metrics for all nodes in a cluster in the guava cache will mean that the cache can grow to thousands of entries - are we ok with that additional memory footprint?\r\nwdyt?"", 'commenter': 'tomekl007'}]"
1478,core/src/main/java/com/datastax/oss/driver/internal/core/metrics/DropwizardMetricUpdater.java,"@@ -33,46 +40,80 @@
   private static final Logger LOG = LoggerFactory.getLogger(DropwizardMetricUpdater.class);
 
   protected final Set<MetricT> enabledMetrics;
+
   protected final MetricRegistry registry;
 
-  protected DropwizardMetricUpdater(Set<MetricT> enabledMetrics, MetricRegistry registry) {
+  protected final Cache<String, Metric> metricsCache;
+
+  protected DropwizardMetricUpdater(
+      Set<MetricT> enabledMetrics, MetricRegistry registry, Ticker ticker) {
+    this(enabledMetrics, registry, ticker, Duration.ofHours(1));","[{'comment': '`Duration.ofHours(1)` - this will be parameterized (to be able to set via `reference.conf`)', 'commenter': 'tomekl007'}]"
1478,core/src/main/java/com/datastax/oss/driver/internal/core/metrics/DropwizardNodeMetricUpdater.java,"@@ -116,4 +154,40 @@ private void initializePoolGauge(
               });
     }
   }
+
+  public void cleanupNodeMetrics() {
+    String profileName = context.getConfig().getDefaultProfile().getName();
+    registry.remove(buildFullName(DefaultNodeMetric.OPEN_CONNECTIONS, null));
+    registry.remove(buildFullName(DefaultNodeMetric.AVAILABLE_STREAMS, null));
+    registry.remove(buildFullName(DefaultNodeMetric.IN_FLIGHT, null));
+    registry.remove(buildFullName(DefaultNodeMetric.ORPHANED_STREAMS, null));
+
+    registry.remove(buildFullName(DefaultNodeMetric.CQL_MESSAGES, profileName));
+
+    registry.remove(buildFullName(DefaultNodeMetric.UNSENT_REQUESTS, null));
+    registry.remove(buildFullName(DefaultNodeMetric.ABORTED_REQUESTS, null));
+    registry.remove(buildFullName(DefaultNodeMetric.WRITE_TIMEOUTS, null));
+    registry.remove(buildFullName(DefaultNodeMetric.READ_TIMEOUTS, null));
+    registry.remove(buildFullName(DefaultNodeMetric.UNAVAILABLES, null));
+    registry.remove(buildFullName(DefaultNodeMetric.OTHER_ERRORS, null));
+    registry.remove(buildFullName(DefaultNodeMetric.RETRIES, null));
+    registry.remove(buildFullName(DefaultNodeMetric.RETRIES_ON_ABORTED, null));
+    registry.remove(buildFullName(DefaultNodeMetric.RETRIES_ON_READ_TIMEOUT, null));
+    registry.remove(buildFullName(DefaultNodeMetric.RETRIES_ON_WRITE_TIMEOUT, null));
+    registry.remove(buildFullName(DefaultNodeMetric.RETRIES_ON_UNAVAILABLE, null));
+    registry.remove(buildFullName(DefaultNodeMetric.RETRIES_ON_OTHER_ERROR, null));
+    registry.remove(buildFullName(DefaultNodeMetric.IGNORES, null));
+    registry.remove(buildFullName(DefaultNodeMetric.IGNORES_ON_ABORTED, null));
+    registry.remove(buildFullName(DefaultNodeMetric.IGNORES_ON_READ_TIMEOUT, null));
+    registry.remove(buildFullName(DefaultNodeMetric.IGNORES_ON_WRITE_TIMEOUT, null));
+    registry.remove(buildFullName(DefaultNodeMetric.IGNORES_ON_UNAVAILABLE, null));
+    registry.remove(buildFullName(DefaultNodeMetric.IGNORES_ON_OTHER_ERROR, null));
+    registry.remove(buildFullName(DefaultNodeMetric.SPECULATIVE_EXECUTIONS, null));
+    registry.remove(buildFullName(DefaultNodeMetric.CONNECTION_INIT_ERRORS, null));
+    registry.remove(buildFullName(DefaultNodeMetric.AUTHENTICATION_ERRORS, null));
+    registry.remove(buildFullName(DseNodeMetric.GRAPH_MESSAGES, profileName));
+
+    registry.remove(buildFullName(DefaultNodeMetric.BYTES_RECEIVED, null));
+    registry.remove(buildFullName(DefaultNodeMetric.BYTES_SENT, null));","[{'comment': 'I think you can remove all the metrics in one line:\r\n```java\r\nregistry.removeMatching((name, metric) -> name.startsWith(metricNamePrefix));\r\n```', 'commenter': 'olim7t'}, {'comment': 'nice, thx', 'commenter': 'tomekl007'}]"
1478,core/src/main/resources/reference.conf,"@@ -1611,6 +1611,15 @@ datastax-java-driver {
         significant-digits = 3
         refresh-interval = 5 minutes
       }
+
+      # The time after the node level metrics will be evicted.
+      # The eviction will happen only if none of the enabled node-level metrics is updated for a given node for a given time.
+      # When the eviction-time will elapse, all metrics for a given node are removed.
+      #
+      # Required: no (defaults to 1 hour)
+      # Modifiable at runtime: no
+      # Overridable in a profile: no
+      eviction-time = 1 hour","[{'comment': 'I wonder if we need to expose this in the configuration. Maybe 1 hour is a reasonable one-size-fit-all value that we could hard-code instead.\r\n\r\nWe\'ve had that philosophy of ""when in doubt, make it configurable"" in driver 4, but the config file is getting huge. I think we should be more opinionated sometimes.', 'commenter': 'olim7t'}, {'comment': 'Regarding one hour, I am not sure if this is `one-size-fit-all`. \r\n- Maybe there will be some clients that will not tolerate stale metrics for one hour but also tolerate some risk of flickering and set it to N minutes?\r\n- Or on the other hand, maybe someone that uses our driver in a context of batch jobs (i.e. spark) that are executed every N hours will need to increment that parameters to couple hours to retain metrics between those batch job runs?', 'commenter': 'tomekl007'}]"
1478,core/src/main/java/com/datastax/oss/driver/api/core/config/DefaultDriverOption.java,"@@ -517,7 +517,6 @@
    * <p>Value-type: {@link java.time.Duration Duration}
    */
   METRICS_NODE_CQL_MESSAGES_INTERVAL(""advanced.metrics.node.cql-messages.refresh-interval""),
-","[{'comment': 'This change looks spurious.', 'commenter': 'adutra'}]"
1478,core/src/main/java/com/datastax/oss/driver/api/core/config/DefaultDriverOption.java,"@@ -808,6 +807,14 @@
    * <p>Value-type: int
    */
   SESSION_LEAK_THRESHOLD(""advanced.session-leak.threshold""),
+  /**
+   * The time after the node level metrics will be evicted. The eviction will happen only if none of","[{'comment': '```suggestion\r\n   * The period of inactivity after which the node level metrics will be evicted. The eviction will happen only if none of\r\n```', 'commenter': 'adutra'}]"
1478,core/src/main/java/com/datastax/oss/driver/api/core/config/DefaultDriverOption.java,"@@ -808,6 +807,14 @@
    * <p>Value-type: int
    */
   SESSION_LEAK_THRESHOLD(""advanced.session-leak.threshold""),
+  /**
+   * The time after the node level metrics will be evicted. The eviction will happen only if none of
+   * the enabled node-level metrics is updated for a given node for a given time. When the
+   * eviction-time will elapse, all metrics for a given node are removed.","[{'comment': '```suggestion\r\n   * the enabled node-level metrics is updated for a given node within this time window.\r\n```\r\n\r\nThe last sentence seems redundant.', 'commenter': 'adutra'}]"
1478,core/src/main/java/com/datastax/oss/driver/api/core/config/DefaultDriverOption.java,"@@ -808,6 +807,14 @@
    * <p>Value-type: int
    */
   SESSION_LEAK_THRESHOLD(""advanced.session-leak.threshold""),
+  /**
+   * The time after the node level metrics will be evicted. The eviction will happen only if none of
+   * the enabled node-level metrics is updated for a given node for a given time. When the
+   * eviction-time will elapse, all metrics for a given node are removed.
+   *
+   * <p>Value-type: {@link java.time.Duration Duration}
+   */
+  METRICS_NODE_EVICTION_TIME(""advanced.metrics.node.eviction-time""),","[{'comment': '```suggestion\r\n  METRICS_NODE_EXPIRE_AFTER(""advanced.metrics.node.expire-after""),\r\n```\r\n\r\nI don\'t like much the usage of the word ""time"" in the sense of ""expiration interval"". Also ""eviction"" is a cache vocabulary, but users don\'t need to know we are using a cache with eviction based on activity, this is an implementation detail.\r\n', 'commenter': 'adutra'}]"
1478,core/src/main/java/com/datastax/oss/driver/internal/core/metrics/DropwizardMetricsFactory.java,"@@ -73,6 +96,20 @@ public DropwizardMetricsFactory(InternalDriverContext context) {
     }
   }
 
+  private Duration getAndValidateEvictionTime(DriverExecutionProfile config) {
+    Duration evictionTime = config.getDuration(DefaultDriverOption.METRICS_NODE_EVICTION_TIME);
+
+    if (evictionTime.compareTo(LOWEST_ACCEPTABLE_EVICTION_TIME) < 0) {
+      LOG.warn(
+          ""The {} setting was provided with too low value. Consider increasing it to at least {} hour. ""
+              + ""Having lower value may cause disappearing and reappearing of your node-level metrics."",","[{'comment': '```suggestion\r\n              + ""Having lower values may cause your node-level metrics to keep disappearing and reappearing."",\r\n```', 'commenter': 'adutra'}]"
1478,core/src/main/java/com/datastax/oss/driver/internal/core/metrics/DropwizardMetricsFactory.java,"@@ -73,6 +96,20 @@ public DropwizardMetricsFactory(InternalDriverContext context) {
     }
   }
 
+  private Duration getAndValidateEvictionTime(DriverExecutionProfile config) {
+    Duration evictionTime = config.getDuration(DefaultDriverOption.METRICS_NODE_EVICTION_TIME);
+
+    if (evictionTime.compareTo(LOWEST_ACCEPTABLE_EVICTION_TIME) < 0) {
+      LOG.warn(
+          ""The {} setting was provided with too low value. Consider increasing it to at least {} hour. ""","[{'comment': ""Shouldn't we display the offending value in the message?"", 'commenter': 'adutra'}]"
1478,integration-tests/src/test/java/com/datastax/oss/driver/core/metrics/MetricsSimulacronIT.java,"@@ -0,0 +1,271 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.core.metrics;
+
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.mockito.Mockito.timeout;
+import static org.mockito.Mockito.verify;
+
+import ch.qos.logback.classic.Level;
+import com.codahale.metrics.Meter;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
+import com.datastax.oss.driver.api.core.config.DriverConfigLoader;
+import com.datastax.oss.driver.api.core.context.DriverContext;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import com.datastax.oss.driver.api.core.metrics.DefaultNodeMetric;
+import com.datastax.oss.driver.api.core.metrics.DefaultSessionMetric;
+import com.datastax.oss.driver.api.core.metrics.Metrics;
+import com.datastax.oss.driver.api.core.session.ProgrammaticArguments;
+import com.datastax.oss.driver.api.core.session.SessionBuilder;
+import com.datastax.oss.driver.api.testinfra.session.SessionUtils;
+import com.datastax.oss.driver.api.testinfra.simulacron.SimulacronRule;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import com.datastax.oss.driver.internal.core.context.DefaultDriverContext;
+import com.datastax.oss.driver.internal.core.context.InternalDriverContext;
+import com.datastax.oss.driver.internal.core.metrics.DropwizardMetricsFactory;
+import com.datastax.oss.driver.internal.core.metrics.MetricsFactory;
+import com.datastax.oss.driver.internal.core.util.LoggerTest;
+import com.datastax.oss.driver.shaded.guava.common.base.Ticker;
+import com.datastax.oss.simulacron.common.cluster.ClusterSpec;
+import com.google.common.collect.Lists;
+import com.tngtech.java.junit.dataprovider.DataProvider;
+import com.tngtech.java.junit.dataprovider.DataProviderRunner;
+import com.tngtech.java.junit.dataprovider.UseDataProvider;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import java.time.Duration;
+import org.awaitility.Awaitility;
+import org.junit.Before;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.junit.runner.RunWith;
+
+@Category(ParallelizableTests.class)
+@RunWith(DataProviderRunner.class)
+public class MetricsSimulacronIT {","[{'comment': 'Nice tests ðŸ‘ ', 'commenter': 'adutra'}]"
1478,core/src/main/resources/reference.conf,"@@ -1611,6 +1611,15 @@ datastax-java-driver {
         significant-digits = 3
         refresh-interval = 5 minutes
       }
+
+      # The time after the node level metrics will be evicted.
+      # The eviction will happen only if none of the enabled node-level metrics is updated for a given node for a given time.","[{'comment': 'Nit: could you reformat to avoid lines of more than 100 characters?', 'commenter': 'olim7t'}]"
1478,core/src/main/resources/reference.conf,"@@ -1611,6 +1611,15 @@ datastax-java-driver {
         significant-digits = 3
         refresh-interval = 5 minutes
       }
+
+      # The time after the node level metrics will be evicted.","[{'comment': '```suggestion\r\n      # The time after which the node level metrics will be evicted.\r\n```', 'commenter': 'olim7t'}]"
1478,core/src/main/resources/reference.conf,"@@ -1611,6 +1611,15 @@ datastax-java-driver {
         significant-digits = 3
         refresh-interval = 5 minutes
       }
+
+      # The time after the node level metrics will be evicted.
+      # The eviction will happen only if none of the enabled node-level metrics is updated for a given node for a given time.
+      # When the expire-after will elapse, all metrics for a given node are removed.
+      #","[{'comment': 'Also specify that it will be forced to 1 hour (currently, see my other comment) if you set a lower value.', 'commenter': 'olim7t'}]"
1478,core/src/main/java/com/datastax/oss/driver/internal/core/metrics/DropwizardMetricsFactory.java,"@@ -41,24 +46,42 @@
 public class DropwizardMetricsFactory implements MetricsFactory {
 
   private static final Logger LOG = LoggerFactory.getLogger(DropwizardMetricsFactory.class);
+  private static final Duration LOWEST_ACCEPTABLE_EVICTION_TIME = Duration.ofHours(1);","[{'comment': 'That seems a bit aggressive. They are probably many applications where hundreds of requests per second is the norm, I can see how someone might want to go lower in those cases.\r\n\r\nWe want to prevent absurd values, like less than a few seconds. But we could go as low as 5 minutes, WDYT?', 'commenter': 'olim7t'}, {'comment': 'good idea, it is too strict now. Changed', 'commenter': 'tomekl007'}]"
1478,core/src/main/java/com/datastax/oss/driver/internal/core/metrics/DropwizardMetricsFactory.java,"@@ -41,24 +46,42 @@
 public class DropwizardMetricsFactory implements MetricsFactory {
 
   private static final Logger LOG = LoggerFactory.getLogger(DropwizardMetricsFactory.class);
+  private static final Duration LOWEST_ACCEPTABLE_EVICTION_TIME = Duration.ofHours(1);
 
   private final String logPrefix;
   private final InternalDriverContext context;
   private final Set<NodeMetric> enabledNodeMetrics;
   private final MetricRegistry registry;
   @Nullable private final Metrics metrics;
   private final SessionMetricUpdater sessionUpdater;
+  private Cache<Node, DropwizardNodeMetricUpdater> metricsCache;
 
-  public DropwizardMetricsFactory(InternalDriverContext context) {
+  public DropwizardMetricsFactory(InternalDriverContext context, Ticker ticker) {
     this.logPrefix = context.getSessionName();
     this.context = context;
 
     DriverExecutionProfile config = context.getConfig().getDefaultProfile();
     Set<SessionMetric> enabledSessionMetrics =
         parseSessionMetricPaths(config.getStringList(DefaultDriverOption.METRICS_SESSION_ENABLED));
+    Duration evictionTime = getAndValidateEvictionTime(config);
+
     this.enabledNodeMetrics =
         parseNodeMetricPaths(config.getStringList(DefaultDriverOption.METRICS_NODE_ENABLED));
 
+    metricsCache =
+        CacheBuilder.newBuilder()
+            .ticker(ticker)
+            .expireAfterAccess(evictionTime)
+            .removalListener(
+                (RemovalNotification<Node, DropwizardNodeMetricUpdater> notification) -> {
+                  LOG.debug(
+                      ""Removing metrics for node: {} from cache after {}"",
+                      notification.getKey(),
+                      evictionTime);","[{'comment': '```suggestion\r\n                  LOG.debug(\r\n                      ""[{}] Removing metrics for node: {} from cache after {}"",\r\n                      logPrefix,\r\n                      notification.getKey(),\r\n                      evictionTime);\r\n```', 'commenter': 'olim7t'}]"
1478,core/src/main/java/com/datastax/oss/driver/internal/core/metrics/DropwizardMetricsFactory.java,"@@ -73,6 +96,21 @@ public DropwizardMetricsFactory(InternalDriverContext context) {
     }
   }
 
+  private Duration getAndValidateEvictionTime(DriverExecutionProfile config) {
+    Duration evictionTime = config.getDuration(DefaultDriverOption.METRICS_NODE_EXPIRE_AFTER);
+
+    if (evictionTime.compareTo(LOWEST_ACCEPTABLE_EVICTION_TIME) < 0) {
+      LOG.warn(
+          ""The {} setting was provided with too low value: {}. Consider increasing it to at least {} hour. ""
+              + ""Having lower values may cause your node-level metrics to keep disappearing and reappearing."",
+          DefaultDriverOption.METRICS_NODE_EXPIRE_AFTER,
+          evictionTime,
+          LOWEST_ACCEPTABLE_EVICTION_TIME.toHours());","[{'comment': '```suggestion\r\n      LOG.warn(\r\n          ""[{}] Value too low for {}: {}. Forcing to {} instead."",\r\n          logPrefix,\r\n          DefaultDriverOption.METRICS_NODE_EXPIRE_AFTER.getPath(),\r\n          evictionTime,\r\n          LOWEST_ACCEPTABLE_EVICTION_TIME);\r\n```\r\nLog prefix + using the option path instead of the programmatic name + shortening the error message (I don\'t think it\'s necessary to explain the consequences) + indicating that we force the value, it\'s not just a suggestion + keep the raw duration, per my other comment to maybe use less than one hour.\r\n\r\nNote that we have a pending ticket to improve the formatting of durations: [JAVA-2845](https://datastax-oss.atlassian.net/browse/JAVA-2845).', 'commenter': 'olim7t'}]"
1478,core/src/main/java/com/datastax/oss/driver/internal/core/metrics/DropwizardMetricsFactory.java,"@@ -41,24 +46,42 @@
 public class DropwizardMetricsFactory implements MetricsFactory {
 
   private static final Logger LOG = LoggerFactory.getLogger(DropwizardMetricsFactory.class);
+  private static final Duration LOWEST_ACCEPTABLE_EVICTION_TIME = Duration.ofHours(1);
 
   private final String logPrefix;
   private final InternalDriverContext context;
   private final Set<NodeMetric> enabledNodeMetrics;
   private final MetricRegistry registry;
   @Nullable private final Metrics metrics;
   private final SessionMetricUpdater sessionUpdater;
+  private Cache<Node, DropwizardNodeMetricUpdater> metricsCache;","[{'comment': '```suggestion\r\n  private final Cache<Node, DropwizardNodeMetricUpdater> metricsCache;\r\n```', 'commenter': 'olim7t'}]"
1478,integration-tests/src/test/java/com/datastax/oss/driver/core/metrics/MetricsSimulacronIT.java,"@@ -0,0 +1,274 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.core.metrics;
+
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.mockito.Mockito.timeout;
+import static org.mockito.Mockito.verify;
+
+import ch.qos.logback.classic.Level;
+import com.codahale.metrics.Meter;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
+import com.datastax.oss.driver.api.core.config.DriverConfigLoader;
+import com.datastax.oss.driver.api.core.context.DriverContext;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import com.datastax.oss.driver.api.core.metrics.DefaultNodeMetric;
+import com.datastax.oss.driver.api.core.metrics.DefaultSessionMetric;
+import com.datastax.oss.driver.api.core.metrics.Metrics;
+import com.datastax.oss.driver.api.core.session.ProgrammaticArguments;
+import com.datastax.oss.driver.api.core.session.SessionBuilder;
+import com.datastax.oss.driver.api.testinfra.session.SessionUtils;
+import com.datastax.oss.driver.api.testinfra.simulacron.SimulacronRule;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import com.datastax.oss.driver.internal.core.context.DefaultDriverContext;
+import com.datastax.oss.driver.internal.core.context.InternalDriverContext;
+import com.datastax.oss.driver.internal.core.metrics.DropwizardMetricsFactory;
+import com.datastax.oss.driver.internal.core.metrics.MetricsFactory;
+import com.datastax.oss.driver.internal.core.util.LoggerTest;
+import com.datastax.oss.driver.shaded.guava.common.base.Ticker;
+import com.datastax.oss.simulacron.common.cluster.ClusterSpec;
+import com.google.common.collect.Lists;
+import com.tngtech.java.junit.dataprovider.DataProvider;
+import com.tngtech.java.junit.dataprovider.DataProviderRunner;
+import com.tngtech.java.junit.dataprovider.UseDataProvider;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import java.time.Duration;
+import org.awaitility.Awaitility;
+import org.junit.Before;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.junit.runner.RunWith;
+
+@Category(ParallelizableTests.class)
+@RunWith(DataProviderRunner.class)
+public class MetricsSimulacronIT {
+
+  @ClassRule
+  public static final SimulacronRule SIMULACRON_RULE =
+      new SimulacronRule(ClusterSpec.builder().withNodes(1));
+
+  @Before
+  public void clearPrimes() {
+    SIMULACRON_RULE.cluster().clearLogs();
+    SIMULACRON_RULE.cluster().clearPrimes(true);
+  }
+
+  @Test
+  public void should_remove_node_metrics_and_not_remove_session_metrics_after_eviction_time() {
+
+    // given
+    DriverConfigLoader loader =
+        SessionUtils.configLoaderBuilder()
+            .withStringList(
+                DefaultDriverOption.METRICS_SESSION_ENABLED,
+                Lists.newArrayList(""bytes-sent"", ""bytes-received""))
+            .withStringList(
+                DefaultDriverOption.METRICS_NODE_ENABLED,
+                Lists.newArrayList(""bytes-sent"", ""bytes-received""))
+            .withDuration(DefaultDriverOption.METRICS_NODE_EXPIRE_AFTER, Duration.ofHours(1))
+            .build();
+    FakeTicker fakeTicker = new FakeTicker();
+    try (CqlSession session =
+        new MetricsTestContextBuilder()
+            .addContactEndPoints(SIMULACRON_RULE.getContactPoints())
+            .withConfigLoader(loader)
+            .withTicker(fakeTicker)
+            .build()) {
+      for (int i = 0; i < 10; i++) {
+        session.execute(""SELECT release_version FROM system.local"");
+      }
+
+      // when
+      fakeTicker.advance(Duration.ofHours(2));
+
+      // then session metrics are not evicted
+      assertThat(session.getMetrics())
+          .hasValueSatisfying(
+              metrics -> {
+                assertThat(metrics.<Meter>getSessionMetric(DefaultSessionMetric.BYTES_SENT))
+                    .hasValueSatisfying(
+                        bytesSent -> assertThat(bytesSent.getCount()).isGreaterThan(0));
+                assertThat(metrics.<Meter>getSessionMetric(DefaultSessionMetric.BYTES_RECEIVED))
+                    .hasValueSatisfying(
+                        bytesReceived -> assertThat(bytesReceived.getCount()).isGreaterThan(0));
+              });
+
+      // and node metrics are evicted
+      Awaitility.await()","[{'comment': ""You can add a static import for the method. I forgot to add it in the contribution guidelines, but we've done it in other tests."", 'commenter': 'olim7t'}]"
1478,integration-tests/src/test/java/com/datastax/oss/driver/core/metrics/MetricsSimulacronIT.java,"@@ -0,0 +1,274 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.core.metrics;
+
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.mockito.Mockito.timeout;
+import static org.mockito.Mockito.verify;
+
+import ch.qos.logback.classic.Level;
+import com.codahale.metrics.Meter;
+import com.datastax.oss.driver.api.core.CqlSession;
+import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
+import com.datastax.oss.driver.api.core.config.DriverConfigLoader;
+import com.datastax.oss.driver.api.core.context.DriverContext;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import com.datastax.oss.driver.api.core.metrics.DefaultNodeMetric;
+import com.datastax.oss.driver.api.core.metrics.DefaultSessionMetric;
+import com.datastax.oss.driver.api.core.metrics.Metrics;
+import com.datastax.oss.driver.api.core.session.ProgrammaticArguments;
+import com.datastax.oss.driver.api.core.session.SessionBuilder;
+import com.datastax.oss.driver.api.testinfra.session.SessionUtils;
+import com.datastax.oss.driver.api.testinfra.simulacron.SimulacronRule;
+import com.datastax.oss.driver.categories.ParallelizableTests;
+import com.datastax.oss.driver.internal.core.context.DefaultDriverContext;
+import com.datastax.oss.driver.internal.core.context.InternalDriverContext;
+import com.datastax.oss.driver.internal.core.metrics.DropwizardMetricsFactory;
+import com.datastax.oss.driver.internal.core.metrics.MetricsFactory;
+import com.datastax.oss.driver.internal.core.util.LoggerTest;
+import com.datastax.oss.driver.shaded.guava.common.base.Ticker;
+import com.datastax.oss.simulacron.common.cluster.ClusterSpec;
+import com.google.common.collect.Lists;
+import com.tngtech.java.junit.dataprovider.DataProvider;
+import com.tngtech.java.junit.dataprovider.DataProviderRunner;
+import com.tngtech.java.junit.dataprovider.UseDataProvider;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import java.time.Duration;
+import org.awaitility.Awaitility;
+import org.junit.Before;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.junit.runner.RunWith;
+
+@Category(ParallelizableTests.class)
+@RunWith(DataProviderRunner.class)
+public class MetricsSimulacronIT {
+
+  @ClassRule
+  public static final SimulacronRule SIMULACRON_RULE =
+      new SimulacronRule(ClusterSpec.builder().withNodes(1));
+
+  @Before
+  public void clearPrimes() {
+    SIMULACRON_RULE.cluster().clearLogs();
+    SIMULACRON_RULE.cluster().clearPrimes(true);
+  }
+
+  @Test
+  public void should_remove_node_metrics_and_not_remove_session_metrics_after_eviction_time() {
+
+    // given
+    DriverConfigLoader loader =
+        SessionUtils.configLoaderBuilder()
+            .withStringList(
+                DefaultDriverOption.METRICS_SESSION_ENABLED,
+                Lists.newArrayList(""bytes-sent"", ""bytes-received""))
+            .withStringList(
+                DefaultDriverOption.METRICS_NODE_ENABLED,
+                Lists.newArrayList(""bytes-sent"", ""bytes-received""))
+            .withDuration(DefaultDriverOption.METRICS_NODE_EXPIRE_AFTER, Duration.ofHours(1))
+            .build();
+    FakeTicker fakeTicker = new FakeTicker();
+    try (CqlSession session =
+        new MetricsTestContextBuilder()
+            .addContactEndPoints(SIMULACRON_RULE.getContactPoints())
+            .withConfigLoader(loader)
+            .withTicker(fakeTicker)
+            .build()) {
+      for (int i = 0; i < 10; i++) {
+        session.execute(""SELECT release_version FROM system.local"");
+      }
+
+      // when
+      fakeTicker.advance(Duration.ofHours(2));
+
+      // then session metrics are not evicted
+      assertThat(session.getMetrics())
+          .hasValueSatisfying(
+              metrics -> {
+                assertThat(metrics.<Meter>getSessionMetric(DefaultSessionMetric.BYTES_SENT))
+                    .hasValueSatisfying(
+                        bytesSent -> assertThat(bytesSent.getCount()).isGreaterThan(0));
+                assertThat(metrics.<Meter>getSessionMetric(DefaultSessionMetric.BYTES_RECEIVED))
+                    .hasValueSatisfying(
+                        bytesReceived -> assertThat(bytesReceived.getCount()).isGreaterThan(0));
+              });
+
+      // and node metrics are evicted
+      Awaitility.await()
+          .until(
+              () -> {
+                // get only node in a cluster and evaluate its metrics.
+                Node node = session.getMetadata().getNodes().values().iterator().next();
+                Metrics metrics = session.getMetrics().get();
+                return !metrics.<Meter>getNodeMetric(node, DefaultNodeMetric.BYTES_SENT).isPresent()
+                    && !metrics
+                        .<Meter>getNodeMetric(node, DefaultNodeMetric.BYTES_RECEIVED)
+                        .isPresent();
+              });
+    }
+  }
+
+  @Test
+  public void
+      should_not_evict_not_updated_node_metric_if_any_other_node_level_metric_was_updated() {
+    // given
+    DriverConfigLoader loader =
+        SessionUtils.configLoaderBuilder()
+            .withStringList(
+                DefaultDriverOption.METRICS_NODE_ENABLED,
+                Lists.newArrayList(""bytes-sent"", ""errors.request.aborted""))
+            .withDuration(DefaultDriverOption.METRICS_NODE_EXPIRE_AFTER, Duration.ofHours(1))
+            .build();
+    FakeTicker fakeTicker = new FakeTicker();
+    try (CqlSession session =
+        new MetricsTestContextBuilder()
+            .addContactEndPoints(SIMULACRON_RULE.getContactPoints())
+            .withConfigLoader(loader)
+            .withTicker(fakeTicker)
+            .build()) {
+      for (int i = 0; i < 10; i++) {
+        session.execute(""SELECT release_version FROM system.local"");
+      }
+
+      // when advance time to before eviction
+      fakeTicker.advance(Duration.ofMinutes(59));
+      // execute query that update only bytes-sent
+      session.execute(""SELECT release_version FROM system.local"");
+      // advance time to after eviction
+      fakeTicker.advance(Duration.ofMinutes(2));
+
+      // then all node-level metrics should not be evicted
+      Awaitility.await()
+          .until(
+              () -> {
+                // get only node in a cluster and evaluate its metrics.
+                Node node = session.getMetadata().getNodes().values().iterator().next();
+                Metrics metrics = session.getMetrics().get();
+                return metrics.<Meter>getNodeMetric(node, DefaultNodeMetric.BYTES_SENT).isPresent()
+                    && metrics
+                        .<Meter>getNodeMetric(node, DefaultNodeMetric.ABORTED_REQUESTS)
+                        .isPresent();
+              });
+    }
+  }
+
+  @Test
+  public void should_log_warning_when_provided_eviction_time_setting_is_too_low() {","[{'comment': 'If you make the method static in `DropwizardMetricsFactory`,  you can cover this (and the next method) in a unit test instead.', 'commenter': 'olim7t'}]"
1478,core/src/main/java/com/datastax/oss/driver/api/core/config/TypedDriverOption.java,"@@ -701,6 +701,10 @@ public String toString() {
       new TypedDriverOption<>(
           DseDriverOption.METRICS_NODE_GRAPH_MESSAGES_INTERVAL, GenericType.DURATION);
 
+  /** The time after the node level metrics will be evicted. */","[{'comment': '```suggestion\r\n  /** The time after which the node level metrics will be evicted. */\r\n```', 'commenter': 'adutra'}]"
1478,core/src/main/java/com/datastax/oss/driver/internal/core/metrics/DropwizardMetricsFactory.java,"@@ -73,6 +99,23 @@ public DropwizardMetricsFactory(InternalDriverContext context) {
     }
   }
 
+  @VisibleForTesting
+  static Duration getAndValidateEvictionTime(DriverExecutionProfile config, String logPrefix) {
+    Duration evictionTime = config.getDuration(DefaultDriverOption.METRICS_NODE_EXPIRE_AFTER);
+
+    if (evictionTime.compareTo(LOWEST_ACCEPTABLE_EXPIRE_AFTER) < 0) {
+      LOG.warn(
+          ""[{}] Value too low for {}: {} (It should be higher than {}). Forcing to {} instead."",
+          logPrefix,
+          DefaultDriverOption.METRICS_NODE_EXPIRE_AFTER.getPath(),
+          evictionTime,
+          LOWEST_ACCEPTABLE_EXPIRE_AFTER,
+          DEFAULT_EXPIRE_AFTER);","[{'comment': 'If the user value is lesser than the lowest acceptable one, I think it would instead make sense to force it to be equal to the lowest acceptable value no? IOW, force to 5 minutes instead of 1 hour.', 'commenter': 'adutra'}, {'comment': 'Yes, I was thinking about it but on the other hand, if someone provides the wrong value maybe it is better to just set it to default?', 'commenter': 'tomekl007'}, {'comment': ""If someone overrides the default value to, say, 1 minute, it's probably because they really want an aggressive eviction policy. I think it would make sense to provide them with the most aggressive interval that we support, so 5 minutes, not 1 hour."", 'commenter': 'adutra'}, {'comment': 'It does make sense. Done', 'commenter': 'tomekl007'}]"
1478,core/src/main/resources/reference.conf,"@@ -1611,6 +1611,17 @@ datastax-java-driver {
         significant-digits = 3
         refresh-interval = 5 minutes
       }
+
+      # The time after which the node level metrics will be evicted.
+      # The eviction will happen only if none of the enabled node-level
+      # metrics is updated for a given node for a given time.
+      # When the expire-after will elapse, all metrics for a given node are removed.","[{'comment': '```suggestion\r\n      # When this interval elapses, all metrics for the idle node are removed.\r\n```', 'commenter': 'adutra'}]"
1478,core/src/main/resources/reference.conf,"@@ -1611,6 +1611,17 @@ datastax-java-driver {
         significant-digits = 3
         refresh-interval = 5 minutes
       }
+
+      # The time after which the node level metrics will be evicted.
+      # The eviction will happen only if none of the enabled node-level
+      # metrics is updated for a given node for a given time.
+      # When the expire-after will elapse, all metrics for a given node are removed.
+      # If you set it to a value lower than 5 minutes, it will be forced to a default 1 hour.","[{'comment': 'See my other comment; I find this logic a bit twisted. This would make more sense imo: `If you set it to a value lower than 5 minutes, it will be forced to 5 minutes.`', 'commenter': 'adutra'}]"
1479,fallout/benchmarks/benchmark.yaml,"@@ -0,0 +1,81 @@
+ensemble:
+  server:
+    node.count: 3
+    provisioner:
+      name: ctool
+      properties:","[{'comment': 'Per conversation with @adutra we likely want to explicitly locate these servers in the ""performance"" tenant', 'commenter': 'absurdfarce'}, {'comment': 'These servers have now been moved to the moonshot tenant', 'commenter': 'absurdfarce'}]"
1479,fallout/benchmarks/benchmark.yaml,"@@ -0,0 +1,81 @@
+ensemble:
+  server:
+    node.count: 3
+    provisioner:
+      name: ctool
+      properties:
+        cloud.provider: nebula
+        cloud.instance.type: m3.large","[{'comment': 'Confirm with Fallout team whether this is the appropriate size for servers in this case', 'commenter': 'absurdfarce'}, {'comment': 'Current size is ms2.small based on recommendations from the Fallout team', 'commenter': 'absurdfarce'}]"
1479,fallout/benchmarks/benchmark.yaml,"@@ -0,0 +1,81 @@
+ensemble:
+  server:
+    node.count: 3
+    provisioner:
+      name: ctool
+      properties:
+        cloud.provider: nebula
+        cloud.instance.type: m3.large
+    configuration_manager:
+      - name: ctool
+        properties:
+          product.type: cassandra
+          product.install.type: tarball
+          product.version: ""3.11.6""
+  client:
+    node.count: 1
+    provisioner:
+      name: ctool
+      properties:
+        cloud.provider: nebula
+        cloud.instance.type: m3.xlarge
+    configuration_manager:
+      - name: ctool
+        properties:
+          install.maven: true
+      - name: gatling
+        properties:
+          git.repository: git@github.com:riptano/dse-benchmarks.git
+          # master uses a Gatling DSE plugin based on 3.x
+          #git.branch: master
+          # This feature branch uses a Gatling DSE plugin based on the unified driver (>= 4.5.x)
+          git.branch: DSP-17972-unified-driver","[{'comment': 'Branch used for a given test run should be converted to a Fallout parameter', 'commenter': 'absurdfarce'}]"
1479,fallout/gatling/gatling_timeseries_latency.yaml,"@@ -0,0 +1,84 @@
+# master uses a Gatling DSE plugin based on 3.x
+# DSP-17972-unified-driver uses a Gatling DSE plugin based on the unified driver (>= 4.5.x)
+branch: ""master""
+
+---
+
+ensemble:
+  server:
+    node.count: 3
+    provisioner:
+      name: ctool
+      properties:
+        cloud.provider: nebula
+        cloud.instance.type: m3.large
+    configuration_manager:
+      - name: ctool
+        properties:
+          product.type: cassandra
+          product.install.type: tarball
+          product.version: ""3.11.6""
+  client:
+    node.count: 1
+    provisioner:
+      name: ctool
+      properties:
+        cloud.provider: nebula
+        cloud.instance.type: m3.xlarge
+    configuration_manager:
+      - name: ctool
+        properties:
+          install.maven: true
+      - name: gatling
+        properties:
+          git.repository: git@github.com:riptano/dse-benchmarks.git
+          git.branch: {{branch}}
+","[{'comment': ""don't you want to add the `observer` node with grafana and send results from Gatling there?\r\nOr are we ok with auto-generated HTML files that Gatling creates after every test run?"", 'commenter': 'tomekl007'}, {'comment': ""Gatling records HDRHistogram data files of it's run that can be used to compute a histogram of request latencies as well as throughput.  So my original plan was to just rely on those files.  I still think that will work for latency but it doesn't really get at the desire to step-wise increase throughput and record when things fall off the rails.\r\n\r\nMy _new_ thinking is very much along the lines of what you describe: use the approach I have right now for latency histograms but change the throughput to a step-wise increasing function, then observe throughput as reported to an external monitor, likely Grafana."", 'commenter': 'absurdfarce'}]"
1479,fallout/gatling/gatling_timeseries_latency.yaml,"@@ -0,0 +1,84 @@
+# master uses a Gatling DSE plugin based on 3.x
+# DSP-17972-unified-driver uses a Gatling DSE plugin based on the unified driver (>= 4.5.x)
+branch: ""master""
+
+---
+
+ensemble:
+  server:
+    node.count: 3
+    provisioner:
+      name: ctool
+      properties:
+        cloud.provider: nebula
+        cloud.instance.type: m3.large
+    configuration_manager:
+      - name: ctool
+        properties:
+          product.type: cassandra
+          product.install.type: tarball
+          product.version: ""3.11.6""
+  client:
+    node.count: 1
+    provisioner:
+      name: ctool
+      properties:
+        cloud.provider: nebula
+        cloud.instance.type: m3.xlarge
+    configuration_manager:
+      - name: ctool
+        properties:
+          install.maven: true
+      - name: gatling
+        properties:
+          git.repository: git@github.com:riptano/dse-benchmarks.git
+          git.branch: {{branch}}
+
+workload:
+  phases:
+    # This simulation isn't necessary in a single-node case; Timeseries_Write will
+    # create the infrastructure that's needed.  We do need it in the multi-node","[{'comment': ""`create the infrastructure that's needed.` - what does it mean? Will it setup keyspace/tables?"", 'commenter': 'tomekl007'}, {'comment': ""Yes, that's correct.  Both Timeseries_Write and Timeseries_Read will create the keyspace and execute any required DDL statements.  Obviously the read test won't do much of interest unless data has previously been populated.\r\n\r\nI'll re-work this line to make that clearer."", 'commenter': 'absurdfarce'}]"
1479,fallout/gatling/gatling_timeseries_latency.yaml,"@@ -0,0 +1,84 @@
+# master uses a Gatling DSE plugin based on 3.x
+# DSP-17972-unified-driver uses a Gatling DSE plugin based on the unified driver (>= 4.5.x)
+branch: ""master""
+
+---
+
+ensemble:
+  server:
+    node.count: 3
+    provisioner:
+      name: ctool
+      properties:
+        cloud.provider: nebula
+        cloud.instance.type: m3.large
+    configuration_manager:
+      - name: ctool
+        properties:
+          product.type: cassandra
+          product.install.type: tarball
+          product.version: ""3.11.6""
+  client:
+    node.count: 1
+    provisioner:
+      name: ctool
+      properties:
+        cloud.provider: nebula
+        cloud.instance.type: m3.xlarge
+    configuration_manager:
+      - name: ctool
+        properties:
+          install.maven: true
+      - name: gatling
+        properties:
+          git.repository: git@github.com:riptano/dse-benchmarks.git
+          git.branch: {{branch}}
+
+workload:
+  phases:
+    # This simulation isn't necessary in a single-node case; Timeseries_Write will
+    # create the infrastructure that's needed.  We do need it in the multi-node
+    # client case; otherwise you can see weird timing errors around the existence
+    # of the expected keyspace/table on various nodes.
+    - timeseries-schema:
+        module: gatling
+        properties:
+          maven_args: >
+              scala:testCompile
+              gatling:execute
+              -Dgatling.disableCompiler
+              -Dgatling.simulationClass=core.timeseries.Timeseries_Schema
+    - timeseries-write:
+        module: gatling
+        properties:
+          maven_args: >
+              scala:testCompile
+              gatling:execute","[{'comment': ""where the results(HTML file with charts) from the gatling run will be stored?\r\nDon't we want to persist it between fallout restarts? If yes, they should be copied into `${FALLOUT_ARTIFACT_DIR}`"", 'commenter': 'tomekl007'}, {'comment': ""Test artifacts are preserved even across Fallout restarts aren't they?  They were in my testing when using a local Fallout server so for now I think what's there is fine.  This may change if/when the Grafana server is introduced."", 'commenter': 'absurdfarce'}, {'comment': 'Yes, if you want to persists results this is exactly why `${FALLOUT_ARTIFACT_DIR}` was created. \r\nAfter each test run, the fallout is creating an artifact directory persisting all data there.', 'commenter': 'tomekl007'}]"
1479,fallout/gatling/gatling_timeseries_latency.yaml,"@@ -0,0 +1,84 @@
+# master uses a Gatling DSE plugin based on 3.x
+# DSP-17972-unified-driver uses a Gatling DSE plugin based on the unified driver (>= 4.5.x)
+branch: ""master""
+
+---
+
+ensemble:
+  server:
+    node.count: 3
+    provisioner:
+      name: ctool
+      properties:
+        cloud.provider: nebula
+        cloud.instance.type: m3.large
+    configuration_manager:
+      - name: ctool
+        properties:
+          product.type: cassandra
+          product.install.type: tarball
+          product.version: ""3.11.6""
+  client:
+    node.count: 1
+    provisioner:
+      name: ctool
+      properties:
+        cloud.provider: nebula
+        cloud.instance.type: m3.xlarge
+    configuration_manager:
+      - name: ctool
+        properties:
+          install.maven: true
+      - name: gatling
+        properties:
+          git.repository: git@github.com:riptano/dse-benchmarks.git
+          git.branch: {{branch}}
+
+workload:
+  phases:
+    # This simulation isn't necessary in a single-node case; Timeseries_Write will
+    # create the infrastructure that's needed.  We do need it in the multi-node
+    # client case; otherwise you can see weird timing errors around the existence
+    # of the expected keyspace/table on various nodes.
+    - timeseries-schema:
+        module: gatling
+        properties:
+          maven_args: >
+              scala:testCompile
+              gatling:execute
+              -Dgatling.disableCompiler
+              -Dgatling.simulationClass=core.timeseries.Timeseries_Schema
+    - timeseries-write:
+        module: gatling
+        properties:
+          maven_args: >
+              scala:testCompile
+              gatling:execute
+              -Dgatling.disableCompiler
+              -Dgatling.simulationClass=core.timeseries.Timeseries_Write
+              -DconnectionsPerHost=8","[{'comment': 'I think that the base test should rely on the default parameters of the driver under test. Why change it to non-default 8?', 'commenter': 'tomekl007'}, {'comment': ""This prop was set in a few other Fallout runs using Timeseries_(Read|Write) so I preserved it here for sake of continuity.  I don't think it's required by any means."", 'commenter': 'absurdfarce'}]"
1479,fallout/gatling/gatling_timeseries_latency.yaml,"@@ -0,0 +1,84 @@
+# master uses a Gatling DSE plugin based on 3.x
+# DSP-17972-unified-driver uses a Gatling DSE plugin based on the unified driver (>= 4.5.x)
+branch: ""master""
+
+---
+
+ensemble:
+  server:
+    node.count: 3
+    provisioner:
+      name: ctool
+      properties:
+        cloud.provider: nebula
+        cloud.instance.type: m3.large
+    configuration_manager:
+      - name: ctool
+        properties:
+          product.type: cassandra
+          product.install.type: tarball
+          product.version: ""3.11.6""
+  client:
+    node.count: 1
+    provisioner:
+      name: ctool
+      properties:
+        cloud.provider: nebula
+        cloud.instance.type: m3.xlarge
+    configuration_manager:
+      - name: ctool
+        properties:
+          install.maven: true
+      - name: gatling
+        properties:
+          git.repository: git@github.com:riptano/dse-benchmarks.git
+          git.branch: {{branch}}
+
+workload:
+  phases:
+    # This simulation isn't necessary in a single-node case; Timeseries_Write will
+    # create the infrastructure that's needed.  We do need it in the multi-node
+    # client case; otherwise you can see weird timing errors around the existence
+    # of the expected keyspace/table on various nodes.
+    - timeseries-schema:
+        module: gatling
+        properties:
+          maven_args: >
+              scala:testCompile
+              gatling:execute
+              -Dgatling.disableCompiler
+              -Dgatling.simulationClass=core.timeseries.Timeseries_Schema
+    - timeseries-write:
+        module: gatling
+        properties:
+          maven_args: >
+              scala:testCompile
+              gatling:execute
+              -Dgatling.disableCompiler
+              -Dgatling.simulationClass=core.timeseries.Timeseries_Write
+              -DconnectionsPerHost=8
+              -DrowsPerPartition=1000
+              -DinjectionType=constant
+              -Dthroughput=10000
+              -DbenchmarkDurationInMinutes=10","[{'comment': '10 minutes as a time for performance tests seems too low. I think that we should increase it to have more durable/long-running tests.', 'commenter': 'tomekl007'}, {'comment': ""Same as above: this value will likely change once I'm satisfied with the mechanism."", 'commenter': 'absurdfarce'}]"
1479,fallout/gatling/gatling_timeseries_latency.yaml,"@@ -0,0 +1,84 @@
+# master uses a Gatling DSE plugin based on 3.x
+# DSP-17972-unified-driver uses a Gatling DSE plugin based on the unified driver (>= 4.5.x)
+branch: ""master""
+
+---
+
+ensemble:
+  server:
+    node.count: 3
+    provisioner:
+      name: ctool
+      properties:
+        cloud.provider: nebula
+        cloud.instance.type: m3.large
+    configuration_manager:
+      - name: ctool
+        properties:
+          product.type: cassandra
+          product.install.type: tarball
+          product.version: ""3.11.6""
+  client:
+    node.count: 1
+    provisioner:
+      name: ctool
+      properties:
+        cloud.provider: nebula
+        cloud.instance.type: m3.xlarge
+    configuration_manager:
+      - name: ctool
+        properties:
+          install.maven: true
+      - name: gatling
+        properties:
+          git.repository: git@github.com:riptano/dse-benchmarks.git
+          git.branch: {{branch}}
+
+workload:
+  phases:
+    # This simulation isn't necessary in a single-node case; Timeseries_Write will
+    # create the infrastructure that's needed.  We do need it in the multi-node
+    # client case; otherwise you can see weird timing errors around the existence
+    # of the expected keyspace/table on various nodes.
+    - timeseries-schema:
+        module: gatling
+        properties:
+          maven_args: >
+              scala:testCompile
+              gatling:execute
+              -Dgatling.disableCompiler
+              -Dgatling.simulationClass=core.timeseries.Timeseries_Schema
+    - timeseries-write:
+        module: gatling
+        properties:
+          maven_args: >
+              scala:testCompile
+              gatling:execute
+              -Dgatling.disableCompiler
+              -Dgatling.simulationClass=core.timeseries.Timeseries_Write
+              -DconnectionsPerHost=8
+              -DrowsPerPartition=1000
+              -DinjectionType=constant
+              -Dthroughput=10000
+              -DbenchmarkDurationInMinutes=10
+    - timeseries-read:
+        module: gatling
+        properties:
+          maven_args: >
+              scala:testCompile
+              gatling:execute
+              -Dgatling.disableCompiler
+              -Dgatling.simulationClass=core.timeseries.Timeseries_Read
+              -DconnectionsPerHost=8","[{'comment': 'same comments as for `Timeseries_Write`', 'commenter': 'tomekl007'}]"
1479,fallout/gatling/gatling_timeseries_latency.yaml,"@@ -0,0 +1,84 @@
+# master uses a Gatling DSE plugin based on 3.x
+# DSP-17972-unified-driver uses a Gatling DSE plugin based on the unified driver (>= 4.5.x)
+branch: ""master""
+
+---
+
+ensemble:
+  server:
+    node.count: 3
+    provisioner:
+      name: ctool
+      properties:
+        cloud.provider: nebula
+        cloud.instance.type: m3.large
+    configuration_manager:
+      - name: ctool
+        properties:
+          product.type: cassandra
+          product.install.type: tarball
+          product.version: ""3.11.6""
+  client:
+    node.count: 1
+    provisioner:
+      name: ctool
+      properties:
+        cloud.provider: nebula
+        cloud.instance.type: m3.xlarge
+    configuration_manager:
+      - name: ctool
+        properties:
+          install.maven: true
+      - name: gatling
+        properties:
+          git.repository: git@github.com:riptano/dse-benchmarks.git
+          git.branch: {{branch}}
+
+workload:
+  phases:
+    # This simulation isn't necessary in a single-node case; Timeseries_Write will
+    # create the infrastructure that's needed.  We do need it in the multi-node
+    # client case; otherwise you can see weird timing errors around the existence
+    # of the expected keyspace/table on various nodes.
+    - timeseries-schema:
+        module: gatling
+        properties:
+          maven_args: >
+              scala:testCompile
+              gatling:execute
+              -Dgatling.disableCompiler
+              -Dgatling.simulationClass=core.timeseries.Timeseries_Schema
+    - timeseries-write:
+        module: gatling
+        properties:
+          maven_args: >
+              scala:testCompile
+              gatling:execute
+              -Dgatling.disableCompiler
+              -Dgatling.simulationClass=core.timeseries.Timeseries_Write
+              -DconnectionsPerHost=8
+              -DrowsPerPartition=1000
+              -DinjectionType=constant
+              -Dthroughput=10000","[{'comment': 'This parameter should be a lot higher. Kafka or dsbulk are able to generate traffic that is > 200_000 per second. We should validate at least this number of requests ', 'commenter': 'tomekl007'}, {'comment': 'This value (like the timeout mentioned above) will also need some tweaking once the mechanism is fully in place and working.', 'commenter': 'absurdfarce'}]"
1479,fallout/gatling/gatling_timeseries_latency.yaml,"@@ -0,0 +1,84 @@
+# master uses a Gatling DSE plugin based on 3.x
+# DSP-17972-unified-driver uses a Gatling DSE plugin based on the unified driver (>= 4.5.x)
+branch: ""master""
+
+---
+
+ensemble:
+  server:
+    node.count: 3
+    provisioner:
+      name: ctool
+      properties:
+        cloud.provider: nebula
+        cloud.instance.type: m3.large
+    configuration_manager:
+      - name: ctool
+        properties:
+          product.type: cassandra
+          product.install.type: tarball
+          product.version: ""3.11.6""
+  client:
+    node.count: 1
+    provisioner:
+      name: ctool
+      properties:
+        cloud.provider: nebula
+        cloud.instance.type: m3.xlarge
+    configuration_manager:
+      - name: ctool
+        properties:
+          install.maven: true
+      - name: gatling
+        properties:
+          git.repository: git@github.com:riptano/dse-benchmarks.git
+          git.branch: {{branch}}
+
+workload:
+  phases:","[{'comment': 'you may want to increase the timeout of this fallout step', 'commenter': 'tomekl007'}, {'comment': 'Understood.  Most of the knobs here (timeouts, throughput, etc.) will need another pass or two of refinement once I get the details of the mechanism fully sorted out.', 'commenter': 'absurdfarce'}]"
1479,fallout/gatling/gatling_timeseries_latency.yaml,"@@ -0,0 +1,99 @@
+# master uses a Gatling DSE plugin based on 3.x
+# DSP-17972-unified-driver uses a Gatling DSE plugin based on the unified driver (>= 4.5.x)
+branch: ""master""
+
+---
+
+ensemble:
+  observer:
+    node.count: 1
+    provisioner:
+      name: ctool
+      properties:
+        cloud.provider: nebula
+        cloud.instance.type: m3.large
+    configuration_manager:
+      - name: ctool_monitoring
+        properties:
+          graphite.create_server: true
+  server:
+    node.count: 3
+    provisioner:
+      name: ctool
+      properties:
+        cloud.provider: nebula
+        cloud.tenant: moonshot-v2
+        cloud.instance.type: ms2.small","[{'comment': ""Nice that we are using the new moonshot-v2 tenant, we'll have to eventually migrate everything there."", 'commenter': 'adutra'}, {'comment': ""Hmm I'm unable to access this tenant; how did you create your app credentials?"", 'commenter': 'adutra'}, {'comment': ""Nevermind, I found it, it's on https://nebula.sjc.dsinternal.org/dashboard/identity/application_credentials/"", 'commenter': 'adutra'}]"
1479,fallout/gatling/gatling_timeseries_latency.yaml,"@@ -0,0 +1,99 @@
+# master uses a Gatling DSE plugin based on 3.x
+# DSP-17972-unified-driver uses a Gatling DSE plugin based on the unified driver (>= 4.5.x)
+branch: ""master""
+
+---
+
+ensemble:
+  observer:
+    node.count: 1
+    provisioner:
+      name: ctool
+      properties:
+        cloud.provider: nebula
+        cloud.instance.type: m3.large","[{'comment': 'No tenant/project specified, do you assume that the observer can live in whichever project is the default one for the user?', 'commenter': 'adutra'}, {'comment': 'Yes, that was the intent.  Only the servers really need to be in the moonshot tenant, everything else can just run in whatever the default tenant might be.', 'commenter': 'absurdfarce'}]"
1479,fallout/gatling/gatling_timeseries_latency.yaml,"@@ -0,0 +1,99 @@
+# master uses a Gatling DSE plugin based on 3.x
+# DSP-17972-unified-driver uses a Gatling DSE plugin based on the unified driver (>= 4.5.x)
+branch: ""master""
+
+---
+
+ensemble:
+  observer:
+    node.count: 1
+    provisioner:
+      name: ctool
+      properties:
+        cloud.provider: nebula
+        cloud.instance.type: m3.large
+    configuration_manager:
+      - name: ctool_monitoring
+        properties:
+          graphite.create_server: true
+  server:
+    node.count: 3
+    provisioner:
+      name: ctool
+      properties:
+        cloud.provider: nebula
+        cloud.tenant: moonshot-v2
+        cloud.instance.type: ms2.small
+    configuration_manager:
+      - name: ctool
+        properties:
+          product.type: cassandra
+          product.install.type: tarball
+          product.version: ""3.11.6""
+      - name: ctool_monitoring
+        properties:
+          components: OS,JVM,GRAPH","[{'comment': 'Does `Graph` in this context mean ""DSE Graph""? If so maybe we can remove it.', 'commenter': 'adutra'}, {'comment': ""However according to the docs [here](https://fallout.sjc.dsinternal.org/components/configurationmanagers/ctool_monitoring) maybe we should add `cassandra`? We are not really interested in the cluster metrics, but maybe it's a nice to have."", 'commenter': 'adutra'}]"
1479,fallout/gatling/gatling_timeseries_latency.yaml,"@@ -0,0 +1,99 @@
+# master uses a Gatling DSE plugin based on 3.x
+# DSP-17972-unified-driver uses a Gatling DSE plugin based on the unified driver (>= 4.5.x)
+branch: ""master""
+
+---
+
+ensemble:
+  observer:
+    node.count: 1
+    provisioner:
+      name: ctool
+      properties:
+        cloud.provider: nebula
+        cloud.instance.type: m3.large
+    configuration_manager:
+      - name: ctool_monitoring
+        properties:
+          graphite.create_server: true
+  server:
+    node.count: 3
+    provisioner:
+      name: ctool
+      properties:
+        cloud.provider: nebula
+        cloud.tenant: moonshot-v2
+        cloud.instance.type: ms2.small
+    configuration_manager:
+      - name: ctool
+        properties:
+          product.type: cassandra
+          product.install.type: tarball
+          product.version: ""3.11.6""
+      - name: ctool_monitoring
+        properties:
+          components: OS,JVM,GRAPH
+  client:
+    node.count: 1
+    provisioner:
+      name: ctool
+      properties:
+        cloud.provider: nebula
+        cloud.instance.type: m3.xlarge","[{'comment': 'same here, no tenant.', 'commenter': 'adutra'}]"
1479,fallout/gatling/gatling_timeseries_latency.yaml,"@@ -0,0 +1,99 @@
+# master uses a Gatling DSE plugin based on 3.x
+# DSP-17972-unified-driver uses a Gatling DSE plugin based on the unified driver (>= 4.5.x)
+branch: ""master""
+
+---
+
+ensemble:
+  observer:
+    node.count: 1
+    provisioner:
+      name: ctool
+      properties:
+        cloud.provider: nebula
+        cloud.instance.type: m3.large
+    configuration_manager:
+      - name: ctool_monitoring
+        properties:
+          graphite.create_server: true
+  server:
+    node.count: 3
+    provisioner:
+      name: ctool
+      properties:
+        cloud.provider: nebula
+        cloud.tenant: moonshot-v2
+        cloud.instance.type: ms2.small
+    configuration_manager:
+      - name: ctool
+        properties:
+          product.type: cassandra
+          product.install.type: tarball
+          product.version: ""3.11.6""
+      - name: ctool_monitoring
+        properties:
+          components: OS,JVM,GRAPH
+  client:
+    node.count: 1
+    provisioner:
+      name: ctool
+      properties:
+        cloud.provider: nebula
+        cloud.instance.type: m3.xlarge
+    configuration_manager:
+      - name: ctool
+        properties:
+          install.maven: true
+      - name: gatling
+        properties:
+          git.repository: git@github.com:riptano/dse-benchmarks.git
+          git.branch: {{branch}}
+
+workload:
+  phases:
+    # This simulation isn't necessary in a single-node case; Timeseries_Write will
+    # create the infrastructure that's needed.  We do need it in the multi-node
+    # client case; otherwise you can see weird timing errors around the existence
+    # of the expected keyspace/table on various nodes.
+    - timeseries-schema:
+        module: gatling
+        properties:
+          maven_args: >
+              scala:testCompile
+              gatling:execute
+              -Dgatling.disableCompiler
+              -Dgatling.simulationClass=core.timeseries.Timeseries_Schema
+    - timeseries-write:
+        module: gatling
+        properties:
+          maven_args: >
+              scala:testCompile
+              gatling:execute
+              -Dgatling.disableCompiler
+              -Dgatling.simulationClass=core.timeseries.Timeseries_Write
+              -DconnectionsPerHost=8
+              -DrowsPerPartition=1000
+              -DinjectionType=constant","[{'comment': ""I've been experimenting with throughput parameters [here](https://github.com/riptano/dse-benchmarks/blob/417eb6b411a34d583f970644c1a3825b9f41d14c/src/test/scala/core/ThroughputParameters.scala#L8-L35). It seems `linear-ramp-up` is not a valid value for time series. However `custom-ramp-up` should be close to what we want. I will experiment more when Fallout is back online (it's currently being redeployed)."", 'commenter': 'adutra'}]"
1480,core/src/main/java/com/datastax/oss/driver/api/core/config/OptionsMap.java,"@@ -255,7 +255,7 @@ protected static void fillWithDriverDefaults(OptionsMap map) {
     map.put(TypedDriverOption.LOAD_BALANCING_POLICY_SLOW_AVOIDANCE, true);
     map.put(TypedDriverOption.SESSION_LEAK_THRESHOLD, 4);
     map.put(TypedDriverOption.CONNECTION_CONNECT_TIMEOUT, Duration.ofSeconds(5));
-    map.put(TypedDriverOption.CONNECTION_INIT_QUERY_TIMEOUT, Duration.ofMillis(500));
+    map.put(TypedDriverOption.CONNECTION_INIT_QUERY_TIMEOUT, Duration.ofSeconds(5));","[{'comment': '`reference.conf` uses HOCON substitutions to define a few other options in terms of this one. You need to update them too: `CONNECTION_SET_KEYSPACE_TIMEOUT`, `HEARTBEAT_TIMEOUT`, `CONTROL_CONNECTION_TIMEOUT` and `REPREPARE_TIMEOUT`.\r\n\r\nIn fact we should probably extract a variable:\r\n```java\r\n    Duration initQueryTimeout = Duration.ofSeconds(5);\r\n    map.put(TypedDriverOption.CONNECTION_INIT_QUERY_TIMEOUT, initQueryTimeout);\r\n    map.put(TypedDriverOption.CONNECTION_SET_KEYSPACE_TIMEOUT, initQueryTimeout);\r\n    ...\r\n```\r\n\r\nCan you also do a pass to handle all other substitutions the same way, and commit that separately?\r\n\r\n`MapBasedDriverConfigLoaderTest.should_fill_default_profile_like_reference_file` will let you know if everything is in sync.', 'commenter': 'olim7t'}, {'comment': 'done, what do you mean in a separate commit? Do you want to have two commits in the `4.x` after this branch will be merged?', 'commenter': 'tomekl007'}, {'comment': 'Yes, because the refactoring to handle other substitutions by extracting a local variable is unrelated to this PR.', 'commenter': 'adutra'}]"
1482,core/src/test/java/com/datastax/oss/driver/internal/core/metadata/DefaultEndPointTest.java,"@@ -41,7 +45,32 @@ public void should_create_from_literal_ipv4_address() {
   @Test
   public void should_create_from_literal_ipv6_address() {
     DefaultEndPoint endPoint = new DefaultEndPoint(new InetSocketAddress(""::1"", 9042));
-    assertThat(endPoint.asMetricPrefix()).isEqualTo(""0:0:0:0:0:0:0:1:9042"");
+    assertThat(endPoint.asMetricPrefix()).isEqualTo(""0:0:0:0:0:0:0:1:localhost:9042"");
+  }
+
+  @Test
+  public void should_create_from_ipv4_exact_type_address() throws UnknownHostException {
+    // given
+    byte[] ipAddr = new byte[] {127, 0, 0, 1};
+    InetAddress inet4Address = Inet4Address.getByAddress(""localhost"", ipAddr);
+    // when
+    DefaultEndPoint defaultEndPoint =
+        new DefaultEndPoint(new InetSocketAddress(inet4Address, 9042));
+    // then
+    assertThat(defaultEndPoint.asMetricPrefix()).isEqualTo(""localhost:9042"");
+  }
+
+  @Test
+  public void should_create_from_ipv6_exact_type_address() throws UnknownHostException {
+    // given
+    byte[] ipAddr = new byte[] {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16};
+    InetAddress inet4Address = Inet6Address.getByAddress(""localhost"", ipAddr);
+    // when
+    DefaultEndPoint defaultEndPoint =
+        new DefaultEndPoint(new InetSocketAddress(inet4Address, 9042));
+    // then
+    assertThat(defaultEndPoint.asMetricPrefix())
+        .isEqualTo(""localhost:102:304:506:708:90a:b0c:d0e:f10:9042"");","[{'comment': 'without this change, this was evaluating to `locahost:9042` as well, causing `llegalArgumentException` when trying to register duplicated metric name in the `MetricRegistry`:\r\n```\r\n public <T extends Metric> T register(String name, T metric) throws IllegalArgumentException {\r\n        if (metric instanceof MetricSet) {\r\n            registerAll(name, (MetricSet) metric);\r\n        } else {\r\n            final Metric existing = metrics.putIfAbsent(name, metric);\r\n            if (existing == null) {\r\n                onMetricAdded(name, metric);\r\n            } else {\r\n                throw new IllegalArgumentException(""A metric named "" + name + "" already exists"");\r\n            }\r\n        }\r\n        return metric;\r\n    }\r\n```\r\n- This change is covering only the case when one IP address is resolving to two: ipv4 and ipv6. I am not sure if it is possible that one address will resolve to more addresses with the same port - I think it is not. \r\n- We have the same logic in the `SniEndPoint#SniEndPoint` but I think that this problem should not occur there.\r\n\r\nlet me know wdyt', 'commenter': 'tomekl007'}]"
1482,core/src/main/java/com/datastax/oss/driver/internal/core/metadata/DefaultEndPoint.java,"@@ -81,7 +82,16 @@ private static String buildMetricPrefix(InetSocketAddress address) {
       throw new IllegalArgumentException(
           ""Could not extract a host string from provided address "" + address);
     }
-    // Append the port since Cassandra 4 supports nodes with different ports
-    return hostString.replace('.', '_') + ':' + address.getPort();
+    if (address.getAddress() instanceof Inet6Address) {
+      // append getCanonicalHostName() to distinguish ip6 address from ip4
+      return hostString.replace('.', '_')
+          + ':'
+          + address.getAddress().getHostAddress()","[{'comment': '`getAddress()` may return null if the address is unresolved.\r\n\r\nI was rather thinking about something like below:\r\n\r\n```\r\n    String hostString =\r\n        address.isUnresolved() ? address.getHostString() : address.getAddress().getHostAddress();\r\n```\r\n\r\nIOW: if the address is unresolved, there is not much we can do about it, so use `getHostString()`; if the address is resolved however, instead of relying on the original host string as provided by the user, switch to the raw IP address, since it is unique. This will solve the localhost problem.\r\n\r\nI also think that using raw IP addresses whenever possible for metric prefixes may be beneficial since host names tend to create extremely long metric names (I tried recently with Astra, the metric names were a nightmare to read).', 'commenter': 'adutra'}, {'comment': ""Also, it's worth noting that apart from contact points, any other nodes (i.e., those discovered in system.peers) will have raw IP addresses anyways. So my change above would also standardize the metrics representation of contact points with that of other nodes."", 'commenter': 'adutra'}, {'comment': ""@olim7t what's your opinion on this? Is it still time to include this in 4.8?"", 'commenter': 'adutra'}, {'comment': ""@olim7t what's your opinion on using raw IP addresses for metric names, including for contact points? As I see it, this would have the double benefit of fixing the original issue _and_ producing homogenous metric names.\r\nIf possible let's include this in 4.9.\r\n"", 'commenter': 'adutra'}]"
1489,Jenkinsfile,"@@ -457,7 +457,11 @@ pipeline {
             values '2.1',       // Legacy Apache Cassandraâ“‡
                    '3.0',       // Previous Apache Cassandraâ“‡
                    '3.11',      // Current Apache Cassandraâ“‡
-                   '4.0'        // Development Apache Cassandraâ“‡
+                   '4.0',       // Development Apache Cassandraâ“‡
+                   'dse-5.1',   // Legacy DataStax Enterprise
+                   'dse-6.0',   // Previous DataStax Enterprise
+                   'dse-6.7',   // Previous DataStax Enterprise
+                   'dse-6.8.0'  // Current DataStax Enterprise","[{'comment': 'Around line 218-222, we list out the Cassandra versions that you can choose from for triggering a specific build. It would be more complete to add the DSE versions there as well.', 'commenter': 'emerkle826'}]"
1489,driver-core/src/main/java/com/datastax/driver/core/TableOptionsMetadata.java,"@@ -183,11 +182,12 @@
       this.extensions = ImmutableMap.copyOf(row.getMap(EXTENSIONS, String.class, ByteBuffer.class));
     else this.extensions = ImmutableMap.of();
 
-    if (is380OrHigher) this.cdc = isNullOrAbsent(row, CDC) ? DEFAULT_CDC : row.getBool(CDC);
-    else this.cdc = DEFAULT_CDC;
+    this.cdc = isNullOrAbsent(row, CDC) ? DEFAULT_CDC : row.getBool(CDC);
 
-    if (is400OrHigher) this.additionalWritePolicy = row.getString(ADDITIONAL_WRITE_POLICY);
-    else this.additionalWritePolicy = DEFAULT_ADDITIONAL_WRITE_POLICY;
+    this.additionalWritePolicy =
+        isNullOrAbsent(row, ADDITIONAL_WRITE_POLICY)
+            ? DEFAULT_ADDITIONAL_WRITE_POLICY
+            : row.getString(ADDITIONAL_WRITE_POLICY);","[{'comment': ""One (quite possibly minor) point: we're losing a bit of insurance against aberrant cases by excluding the version guards, right?  The original code _expected_ fields to be present for certain versions.  The new code will silently handle missing fields silently due to isNullOrAbsent() even if a field should be present for a specific version.\r\n\r\nThis should only matter if, say, a server bug causes expected fields to not be returned for some reason.  I'm merely mentioning it to say we do lose that notion of version-based expectation here."", 'commenter': 'absurdfarce'}, {'comment': ""On a related point: it seems a bit odd that after this change some fields still have version guards while some don't.  Perhaps we should make this change wholesale to all fields examined by this class?"", 'commenter': 'absurdfarce'}, {'comment': ""> I'm merely mentioning it to say we do lose that notion of version-based expectation here.\r\n\r\nCorrect, but as you pointed out, I don't see how this would matter unless the server is misbehaving.\r\n\r\n> it seems a bit odd that after this change some fields still have version guards while some don't.\r\n\r\nIndeed, but for these fields, we distinguish 3 states: \r\n\r\n1. the option is supported and present: use the value in the row\r\n2. the option is supported but not present: use the default value instead\r\n3. the option is not supported: use null\r\n\r\nThe version guard is required for item 3."", 'commenter': 'adutra'}, {'comment': 'Fair point on the remaining version bounds.  And I don\'t feel super-strongly about the fact that we\'re no longer considering whether a field _should be_ provided for the changes here.  I agree that it should only figure in when the server is misbehaving... my concern was that if that were to happen it might be better for my client to fail loudly.\r\n\r\nRegardless, I should\'ve prefixed all of this with the ""nit"" label; none of it is fatal.  What you have should be fine.', 'commenter': 'absurdfarce'}]"
1489,driver-core/src/test/java/com/datastax/driver/core/CCMBridge.java,"@@ -1047,13 +1047,15 @@ public CCMBridge build() {
         cassandraConfiguration.remove(""rpc_port"");
         cassandraConfiguration.remove(""thrift_prepared_statements_cache_size_mb"");
       }
-      if (isMaterializedViewsDisabledByDefault(cassandraVersion)) {
-        // enable materialized views
-        cassandraConfiguration.put(""enable_materialized_views"", true);
-      }
-      if (isSasiConfigEnablementRequired(cassandraVersion)) {
-        // enable SASI indexing in config (disabled by default in C* 4.0)
-        cassandraConfiguration.put(""enable_sasi_indexes"", true);
+      if (!dse) {","[{'comment': 'Perhaps the DSE check should be folded into isMaterializedViewsDisabledByDefault() and isSasiConfigEnablementRequired() instead?  Seems like that would make those two methods more accurate in general.', 'commenter': 'absurdfarce'}, {'comment': ""That was actually my first version of this snippet :-) But the implementations felt clumsy to me as both would start with a `if (dse) return false;` statement that seemed disconnected from the rest. Anyways, it doesn't matter much, there is no version of DSE as of today that support these config options."", 'commenter': 'adutra'}, {'comment': 'It\'s the fact that no DSE version uses these configs that formed the foundation of my concern... that\'s why you want those methods to be as accurate as possible.  But we\'re talking about private methods in CCMBridge so it\'s hard to get too concerned... this also falls squarely under the category of ""nit"".', 'commenter': 'absurdfarce'}]"
1494,core/src/main/java/com/datastax/oss/driver/api/core/config/DefaultDriverOption.java,"@@ -816,6 +816,13 @@
    * <p>Value-type: {@link java.time.Duration Duration}
    */
   METRICS_NODE_EXPIRE_AFTER(""advanced.metrics.node.expire-after""),
+
+  /**
+   * The fully-qualified classname of the desired MetricsFactory implementation.","[{'comment': 'The implementation does not need a fully-qualified classname for implementations that exist in `com.datastax.oss.driver.internal.core.metrics`. Remove `fully-qualified` from the javadoc.', 'commenter': 'emerkle826'}]"
1494,core/src/main/java/com/datastax/oss/driver/api/core/config/TypedDriverOption.java,"@@ -705,6 +705,10 @@ public String toString() {
   public static final TypedDriverOption<Duration> METRICS_NODE_EXPIRE_AFTER =
       new TypedDriverOption<>(DefaultDriverOption.METRICS_NODE_EXPIRE_AFTER, GenericType.DURATION);
 
+  /** The fully-qualified classname of the desired MetricsFactory implementation. */","[{'comment': 'remove `fully-qualified`.', 'commenter': 'emerkle826'}]"
1494,core/src/main/java/com/datastax/oss/driver/api/core/session/SessionBuilder.java,"@@ -650,6 +650,12 @@ public SelfT withApplicationVersion(@Nullable String applicationVersion) {
     return self;
   }
 
+  @NonNull","[{'comment': 'Needs Javadocs.', 'commenter': 'emerkle826'}]"
1494,core/src/main/java/com/datastax/oss/driver/internal/core/context/InternalDriverContext.java,"@@ -180,4 +180,7 @@
    */
   @NonNull
   RequestLogFormatter getRequestLogFormatter();
+
+  @Nullable","[{'comment': 'Needs Javadocs.', 'commenter': 'emerkle826'}]"
1494,core/src/main/java/com/datastax/oss/driver/api/core/session/ProgrammaticArguments.java,"@@ -289,6 +298,12 @@ public Builder withCodecRegistry(@Nullable MutableCodecRegistry codecRegistry) {
       return this;
     }
 
+    @NonNull
+    public Builder withMetricRegistry(@Nullable Object metricRegistry) {","[{'comment': 'How is this design going to solve the Spring Boot issue? You would need access to the metric registry bean at session build time.', 'commenter': 'adutra'}, {'comment': 'A possible solution would be to provide a `Supplier<Object>`.', 'commenter': 'adutra'}, {'comment': 'Oooh I see. `SessionBuilderCustomizer` FTW! Very smart :-) ', 'commenter': 'adutra'}]"
1494,metrics/micrometer/src/main/java/com/datastax/oss/driver/internal/metrics/micrometer/MicrometerMetricsFactory.java,"@@ -87,9 +91,16 @@ public MicrometerMetricsFactory(
       this.registry = null;
       this.sessionUpdater = NoopSessionMetricUpdater.INSTANCE;
     } else {
-      this.registry = registry;
-      this.sessionUpdater =
-          new MicrometerSessionMetricUpdater(enabledSessionMetrics, this.registry, this.context);
+      // try to get the mertic registry from the context
+      Object possibleMetricRegistry = context.getMetricRegistry();
+      if (possibleMetricRegistry instanceof MeterRegistry) {
+        this.registry = (MeterRegistry) possibleMetricRegistry;
+        this.sessionUpdater =
+            new MicrometerSessionMetricUpdater(enabledSessionMetrics, this.registry, this.context);
+      } else {
+        this.registry = null;","[{'comment': 'Maybe we should throw an error if an object was present but was not of the expected type?', 'commenter': 'adutra'}, {'comment': ""Will do. But I just also realized that the registry could be NULL, which will cause an NPE here as well. I'll fix this and write some unit tests to catch it."", 'commenter': 'emerkle826'}]"
1494,core/src/main/java/com/datastax/oss/driver/api/core/session/SessionBuilder.java,"@@ -650,6 +651,25 @@ public SelfT withApplicationVersion(@Nullable String applicationVersion) {
     return self;
   }
 
+  /**
+   * The metrics registry object for storing driver metrics.","[{'comment': 'Very good javadocs ðŸ‘ ', 'commenter': 'adutra'}]"
1494,core/src/main/resources/reference.conf,"@@ -1228,6 +1228,15 @@ datastax-java-driver {
   }
 
   advanced.metrics {
+    # Metrics Factory configuration.
+    factory {
+      # The class for the metrics factory.
+      #
+      # The driver provides Dropwizard metrics out of the box. If you would like to use a
+      # different metrics framework, change the factory class to the fully-qualified name of a
+      # class that implements com.datastax.oss.driver.internal.core.metrics.MetricsFactory.
+      class = DropwizardMetricsFactory","[{'comment': 'I think you could add: \r\n\r\n```\r\n    # Required: yes\r\n    # Modifiable at runtime: no\r\n    # Overridable in a profile: no\r\n```\r\n\r\nAlso, when you say ""The driver provides Dropwizard metrics out of the box"" I think you should also mention the 2 alternative factories that are also provided now (micrometer et microprofile).', 'commenter': 'adutra'}]"
1494,core/src/main/java/com/datastax/oss/driver/internal/core/context/InternalDriverContext.java,"@@ -180,4 +180,20 @@
    */
   @NonNull
   RequestLogFormatter getRequestLogFormatter();
+
+  /**
+   * A metrics registry for storing metrics.
+   *
+   * <p>This will return the object from {@link
+   * SessionBuilder#withMetricRegistry(java.lang.Object)}. Access to this registry object is only
+   * intended for {@link MetricsFactory} implementations that need to expose a way to specify the
+   * registry external to the Factory implementation itself.
+   *
+   * <p>The default metrics framework used by the Driver is DropWizard and does not need an external
+   * metrics registry object.
+   */
+  @Nullable
+  default Object getMetricRegistry() {","[{'comment': 'I\'m going to sound a bit pedantic but I wonder if we shouldn\'t put ""metrics"" in the plural everywhere: `getMetricsRegistry`, etc.. Dropwizard and Microprofile have `MetricsRegistry` while Microprofile is the only one to have `MetricRegistry` in the singular.', 'commenter': 'adutra'}, {'comment': ""I never shy from a pedantry contest ðŸ˜„ \r\n\r\nDropwizard actually uses the singular form: [MetricRegistry](https://www.javadoc.io/doc/io.dropwizard.metrics/metrics-core/4.1.2/com/codahale/metrics/MetricRegistry.html). I like it a bit better, it's like apple pie, ClassLoader, LoggerFactory, etc.\r\n\r\nPS -- I did write a `MetricsFactory` interface in the driver, but it returns an instance of the `Metrics` object."", 'commenter': 'olim7t'}, {'comment': "":-) You are right, let's keep the singular form then: `MetricRegistry`. Sorry for initiating this storm in a teacup."", 'commenter': 'adutra'}, {'comment': 'Will do!', 'commenter': 'emerkle826'}]"
1494,metrics/microprofile/src/main/java/com/datastax/oss/driver/internal/metrics/microprofile/MicroProfileMetricsFactory.java,"@@ -86,9 +90,16 @@ public MicroProfileMetricsFactory(
       this.registry = null;
       this.sessionUpdater = NoopSessionMetricUpdater.INSTANCE;
     } else {
-      this.registry = registry;
-      this.sessionUpdater =
-          new MicroProfileSessionMetricUpdater(enabledSessionMetrics, this.registry, this.context);
+      Object possibleMetricRegistry = context.getMetricRegistry();
+      if (possibleMetricRegistry instanceof MetricRegistry) {
+        this.registry = (MetricRegistry) possibleMetricRegistry;
+        this.sessionUpdater =
+            new MicroProfileSessionMetricUpdater(
+                enabledSessionMetrics, this.registry, this.context);
+      } else {
+        this.registry = null;","[{'comment': 'Same here.', 'commenter': 'adutra'}]"
1494,core/src/main/java/com/datastax/oss/driver/api/core/session/SessionBuilder.java,"@@ -650,6 +651,25 @@ public SelfT withApplicationVersion(@Nullable String applicationVersion) {
     return self;
   }
 
+  /**
+   * The metrics registry object for storing driver metrics.
+   *
+   * <p>The default driver metrics framework is DropWizard and does not need an external metrics
+   * registry object to be provided.
+   *
+   * <p>If using a non-default metrics framework, it is necessary to call this method <b>only if</b>
+   * the {@link MetricsFactory} implementation does not provide its own registry for metrics
+   * storage.
+   *
+   * <p>If using Micrometer or MicroProfile driver implementations, an acceptable registry object is
+   * expected to be set with this method.
+   */
+  @NonNull
+  public SelfT withMetricsRegistry(@Nullable Object metricRegistry) {","[{'comment': 'OK, we lose a bit of precision on the type but the ability to pass a registry even with alternative implementations is worth it.', 'commenter': 'olim7t'}]"
1494,core/src/main/java/com/datastax/oss/driver/api/core/session/SessionBuilder.java,"@@ -650,6 +651,25 @@ public SelfT withApplicationVersion(@Nullable String applicationVersion) {
     return self;
   }
 
+  /**
+   * The metrics registry object for storing driver metrics.
+   *
+   * <p>The default driver metrics framework is DropWizard and does not need an external metrics
+   * registry object to be provided.","[{'comment': 'It does not _need_ it, but maybe we should use the object if it is provided. Right now our implementation creates an instance no matter what, but Dropwizard can publish to an existing registry like the other frameworks.', 'commenter': 'olim7t'}, {'comment': ""In general, should the MetricsFactory implementations try to create a Registry object, even if one isn't set on the Context? That is what DropWizardMetricsFactory does now. I can do it for Micrometer as well. But unless we add a compile time dependency on SmallRye, I can't spin up a concrete instance for MircoProfile.\r\n\r\nThe reason I ask is for consistency. We can add SmallRye as a dependency, and then each of the Factory classes can use a registry object if provided, or use a default implementation if not. Or we could require an appropriate registry object to be set on the context. I'm thinking the former approach may be best as it will capture metrics if an application forgets to set a registry object."", 'commenter': 'emerkle826'}, {'comment': ""> But unless we add a compile time dependency on SmallRye, I can't spin up a concrete instance for MircoProfile.\r\n\r\nMaybe use reflection for that?"", 'commenter': 'adutra'}, {'comment': ""Looking at this a bit more, it's going to be difficult to really test Micrometer and MicroProfile metrics if an explicit Registry object is not provided. The Factory classes currently do _not_ implement `getMetrics()`, only Dropwizard does. So if applications setup Micrometer or MicroProfile and do not also specify the metric Registry, they won't have a way to access driver metrics collected.\r\n\r\nFor Dropwizard, even if an application doe not provide a Registry, the application can still access the metrics from `getMterics()`.\r\n\r\n@adutra @olim7t Should I add an accessor method to the MetricsFactory, or some other way to access the metrics when an explicit registry is not specified (for Micrometer and MicroProfile)? Or should we rstick with requiring an external Registry object to be provided in these cases, and only allow Dropwizard to not specify a Registry object?"", 'commenter': 'emerkle826'}, {'comment': ""> The Factory classes currently do not implement getMetrics()\r\n\r\nImplementing `getMetrics()` may be an interesting addition that stems quite naturally from the general trend of opening up to different metrics implementations. However I don't think that it's required to be done within this PR. It can safely be implemented later as a follow-up ticket. Our current work on Spring Boot does not require that either, so I'd suggest that we keep this PR focused on providing an external registry.\r\n\r\n> Should I add an accessor method to the MetricsFactory, or some other way to access the metrics when an explicit registry is not specified (for Micrometer and MicroProfile)? Or should we stick with requiring an external Registry object to be provided in these cases, and only allow Dropwizard to not specify a Registry object?\r\n\r\nMy suggestions are:\r\n\r\n1. For consistency, allow all `MetricsFactory` implementations to use the external registry, if provided; if not, they should either instantiate one on the fly, or error out if the external registry is required.\r\n    a. For microprofile, you _might_ get away by instantiating the SmallRye registry by reflection; but throwing an error is fine too imo.\r\n3. For integration tests, always pass an external registry; this would hopefully make verifications of the registry's contents straightforward, even in the absence of a proper `getMetrics()` implementation."", 'commenter': 'adutra'}, {'comment': 'What I have in mind for MicroProfile:\r\n\r\n```\r\n      Object possibleMetricRegistry = context.getMetricsRegistry();\r\n      if (possibleMetricRegistry == null) {\r\n        // metrics are enabled, but a metrics registry was not supplied to the context;\r\n        // in this case, attempt to instantiate SmallRye\'s implementation\r\n        Class<?> smallRyeRegistryClass =\r\n            Reflection.loadClass(\r\n                context.getClassLoader(), ""io.smallrye.metrics.MetricsRegistryImpl"");\r\n        if (smallRyeRegistryClass != null) {\r\n          try {\r\n            possibleMetricRegistry = smallRyeRegistryClass.newInstance();\r\n          } catch (Exception ignored) {\r\n          }\r\n          if (possibleMetricRegistry == null) {\r\n            throw new IllegalArgumentException(\r\n                ""Metrics registry object is NULL. Expected registry object to be of type \'""\r\n                    + MetricRegistry.class.getCanonicalName()\r\n                    + ""\'"");\r\n          }\r\n        }\r\n      }\r\n```\r\n\r\nThe code above could be simplified if we move this instantiation logic to `Reflection`. Actually, the logic is already there in `buildFromConfig` but we would need to extract a `buildFromClassName` method out of it.', 'commenter': 'adutra'}, {'comment': 'And Micrometer:\r\n\r\n```\r\n      // try to get the metric registry from the context\r\n      Object possibleMetricRegistry = context.getMetricsRegistry();\r\n      if (possibleMetricRegistry == null) {\r\n        // metrics are enabled, but a metrics registry was not supplied to the context;\r\n        // use global registry in this case.\r\n        possibleMetricRegistry = io.micrometer.core.instrument.Metrics.globalRegistry;\r\n      }\r\n```', 'commenter': 'adutra'}, {'comment': ""The only problem with this approach is access to the Registry if it's not explicitly set on the context. Metrics will be collected, but the reference to the Registry is private inside the Factory. And since there is no `getRegistry()` method on the Factory, client code wouldn't have any way to get at the Metrics being stored.\r\n\r\nIt's different for Dropwizard, because we have `getMetrics()` on MetricsFactory. So client code doesn't need access to the Registry to get at the stored metrics.\r\n\r\nI only realized this when writing the tests for it as I had implemented almost exactly what you suggest, but I wasn't able to get the test to verify the metrics."", 'commenter': 'emerkle826'}, {'comment': ""I've implemented the suggestion to error out for Micrometer and MicroProfile if no external registry is provided. For Dropwizard, I create a registry to keep the behavior the same. See this [commit](https://github.com/datastax/java-driver/pull/1494/commits/247aa46145c5f7afc88b4ab73948aa9ef32dca18)"", 'commenter': 'emerkle826'}, {'comment': ""> The only problem with this approach is access to the Registry if it's not explicitly set on the context. Metrics will be collected, but the reference to the Registry is private inside the Factory. And since there is no `getRegistry()` method on the Factory, client code wouldn't have any way to get at the Metrics being stored.\r\n\r\nYou are right, sorry for not anticipating that. \r\n\r\nThat said, unless I'm mistaken in Micrometer the default registry is global, and hence accessible even without `getMetrics()`. So at least for Micrometer I think it's safe to default to the global registry.\r\n\r\nFor MicroProfile, I'd suggest that we error out as you did already; and if/when `getMetrics()` is implemented for this framework, we could revisit the logic and create a SmallRye registry on the fly.\r\n\r\n"", 'commenter': 'adutra'}, {'comment': ""Good point about Micrometer. I'll change it as you suggest."", 'commenter': 'emerkle826'}]"
1494,core/src/main/resources/reference.conf,"@@ -1228,6 +1228,25 @@ datastax-java-driver {
   }
 
   advanced.metrics {
+    # Metrics Factory configuration.
+    factory {
+      # The class for the metrics factory.
+      #
+      # The driver provides Dropwizard, Micrometer and MicroProfile metrics out of the box.
+      # To use Dropwizard, this value should be set to ""DropwizardMetricsFactory"". To use
+      # Micrometer or MicroProfile, this value needs to be set to the fully qualified class
+      # name ""com.datastax.oss.driver.internal.metrics.micrometer.MicrometerMetricsFactory""
+      # or ""com.datastax.oss.driver.internal.metrics.microprofile.MicroProfileMetricsFactory"".","[{'comment': 'You can declare those two packages as additional defaults, so that all of our built-in implementations work with the unqualified name.\r\n\r\nSee `DefaultDriverContext.buildMetricsFactory`, the last parameter of `Reflection.buildFromConfig` is a vararg.', 'commenter': 'olim7t'}]"
1494,metrics/micrometer/src/main/java/com/datastax/oss/driver/internal/metrics/micrometer/MicrometerMetricsFactory.java,"@@ -87,9 +91,28 @@ public MicrometerMetricsFactory(
       this.registry = null;
       this.sessionUpdater = NoopSessionMetricUpdater.INSTANCE;
     } else {
-      this.registry = registry;
-      this.sessionUpdater =
-          new MicrometerSessionMetricUpdater(enabledSessionMetrics, this.registry, this.context);
+      // try to get the mertic registry from the context
+      Object possibleMetricRegistry = context.getMetricsRegistry();
+      if (possibleMetricRegistry == null) {
+        // metrics are enabled, but a metrics registry was not supplied to the context
+        throw new IllegalArgumentException(
+            ""Metrics registry object is NULL. Expected registry object to be of type '""
+                + MeterRegistry.class.getCanonicalName()
+                + ""'"");
+      }
+      if (possibleMetricRegistry instanceof MeterRegistry) {
+        this.registry = (MeterRegistry) possibleMetricRegistry;
+        this.sessionUpdater =
+            new MicrometerSessionMetricUpdater(enabledSessionMetrics, this.registry, this.context);
+      } else {
+        // Metrics are enabled, but the registry object is not an expected type
+        throw new IllegalArgumentException(
+            ""Unexpected Metrics registry object. Expected registry object to be of type '""
+                + MeterRegistry.class.getCanonicalName()
+                + ""', but was '""
+                + possibleMetricRegistry.getClass().getCanonicalName()","[{'comment': 'Prefer `getName()` to `getCanonicalName()` as the latter would return null for anonymous inner classes (which could very well be the case for the object passed to the factory).', 'commenter': 'adutra'}]"
1495,core/src/test/java/com/datastax/oss/driver/internal/core/protocol/CompressSubstitutionsTest.java,"@@ -0,0 +1,159 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.protocol;
+
+import static org.assertj.core.api.Assertions.assertThat;
+
+import com.datastax.oss.driver.shaded.guava.common.collect.Sets;
+import com.datastax.oss.driver.shaded.guava.common.collect.Sets.SetView;
+import java.lang.reflect.Method;
+import java.lang.reflect.Modifier;
+import java.lang.reflect.Parameter;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+import java.util.Objects;
+import java.util.Set;
+import java.util.function.Predicate;
+import java.util.stream.Collectors;
+import org.junit.Test;
+
+public class CompressSubstitutionsTest {
+
+  @Test
+  public void Lz4SubstitutionShouldSubstituteAllProtectedMethodsFromLz4Compressor() {","[{'comment': ""Instead of this, wouldn't it be simpler to make `Lz4Substitution` and `SnappySubstitution` implement `Compressor<ByteBuf>`?"", 'commenter': 'adutra'}, {'comment': 'I was thinking about it but I assumed that not implementing `Compressor<ByteBuf>` was done on purpose and has some valid reason, @absurdfarce  can we do it without problems?', 'commenter': 'tomekl007'}, {'comment': 'Hmm I think you meant @absurdfarce.', 'commenter': 'adutra'}, {'comment': ""This code came into the Java driver with JAVA-2803, and while I didn't see anything precisely on point in the record If I recall correctly the main issue here is that Lz4Substitution shouldn't be thought of as a Compressor itself.  It's just a substitution for certain methods that apply in certain circumstances, in this case when the lz4 libs aren't present.  There's no real cause for this class to implement the algorithm() method, for example, since whatever class it's used as a substitution for will already handle that.\r\n\r\nAlso worth noting: implementing the interface (arguably) provides a false security.  There's nothing stopping Lz4Compressor from implementing some new interface that's similar to (but not the same as) the hierarchy built up by Compressor<ByteBuf> while the substitution remains unchanged.  In that case everything would continue to type check but the substitution almost certainly wouldn't behave as expected.  That fact alone argues for automated testing of some kind to catch this.\r\n\r\nFinally, I'd note that if we want to pursue this path (and I'm not fundamentally opposed to it) then Compressor<ByteBuf> seems like the wrong interface.  Note that SnappySubstitution and Lz4Substitution both have the same methods with the same signatures.... it seems like there's an interface begging to be born there.  I also note that the same interface could be applied to ByteBufCompressor to handle the declared abstract methods in that class as well.  Note, however, that pulling out such an interface would still be subject to the implementation  divergence mentioned above.\r\n\r\nTo sum up: I don't think of this class as a compressor itself but as a provider of replacement methods for a compressor.  We could readily pull out an interface to define those methods for this replacement (and the Snappy one as well) but without better testing we're still vulnerable to impls diverging."", 'commenter': 'absurdfarce'}, {'comment': 'So I suggest keeping the test that I created in this PR.', 'commenter': 'tomekl007'}, {'comment': '> There\'s no real cause for this class to implement the algorithm() method\r\n\r\nBut it doesn\'t hurt to do so, does it? It would simply return ""lz4"".\r\n\r\n> There\'s nothing stopping Lz4Compressor from implementing some new interface\r\n\r\nGranted, although highly unlikely. But I get your point: some methods in `ByteBufCompressor` are not present in `Compressor` so implementing `Compressor` is not enough. OK.\r\n\r\nIn this case can we make `Lz4Substitution` extend `ByteBufCompressor`? If that\'s possible I sill think that for the sake of simplicity that would be enough for now, given that these classes are not expected to change anymore... until protocol v6 :-).', 'commenter': 'adutra'}, {'comment': ""The methods in question are marked as abstract in ByteBufCompressor so I'd still argue it's worthwhile to pull those out into an interface which everyone should implement.  There's no reason for either of the substitutions to worry about the other implementation details in ByteBufCompressor; they just need to implement the methods in question.  And I'd argue it's a clearer expression of intent to leave them focused on just that.\r\n\r\nTo make this more concrete I pulled out a simple version of what this interface might look like and put a PR up against @JiriOndrusek 's fork.  I'd argue this PR (https://github.com/JiriOndrusek/java-driver/pull/1) is a cleaner expression of the intent we're after here."", 'commenter': 'absurdfarce'}, {'comment': ""@olim7t what would be your opinion? @absurdfarce's solution is elegant; but we need this ticket in 4.9 so maybe we could just merge the PR as is and leave this improvement for a follow-up PR. "", 'commenter': 'adutra'}, {'comment': ""> There's nothing stopping Lz4Compressor from implementing some new interface\r\n\r\nTrue, but that seems very unlikely. The rest of the driver interacts with `Compressor<ByteBuf>` as a completely opaque abstraction. If Lz4Compressor implements a new interface, it means that someone, somewhere would use that interface, so it would probably surface through the parent types as well.\r\nThis was the case for the ticket that caused this regression (JAVA-2773), the changes propagated all the way up to `Compressor`.\r\n\r\nAlso, even if we introduce a `ByteBufCompressOps`, the same thing can happen. If someone completely forgets about substitutions (ðŸ˜³ ), they can still modify the concrete classes in a way that doesn't break at compile time but breaks at substitution time.\r\n\r\nI think extending `ByteBufCompressor` is a good middle ground. It doesn't cover every imaginable case, but it would have caught this bug, and most future problems will be of a similar nature given how the type hierarchy is designed. For the rest, we have the unit test."", 'commenter': 'olim7t'}, {'comment': '@olim7t \r\nI did make Lz4Substitution and SnappySubstitution extend `ByteBufCompressor`. \r\nregarding:\r\n>For the rest, we have the unit test.\r\n\r\nwhich unit test do you mean? Are you suggesting that we should also keep the `CompressSubstitutionsTest` (https://github.com/datastax/java-driver/pull/1495/files#diff-f7188280f40e3b891f3337cb2e13ba5eR33)?', 'commenter': 'tomekl007'}, {'comment': 'It turns out that the solution with extends ByteBufCompressor will not work with graal because of the:\r\n```\r\ncom.oracle.svm.core.util.UserError$UserException: Annotated class must inherit directly from Object: class com.datastax.oss.driver.internal.core.protocol.Lz4Substitution\r\n```\r\nSo we would need to extend Lz4Compressor directly, but this is not possible because the constructor of `Lz4Compressor` is using the lz4 dependency (and we want to make it work when this dependency is not present on the classpath):\r\n```\r\n Lz4Compressor(String sessionName) {\r\n    if (DependencyCheck.LZ4.isPresent()) {\r\n    ....\r\n```\r\n(same for SnappyCompressor), and the constructor needs to be called from the child class:\r\n```\r\n@Substitute\r\npublic Lz4Substitution(DriverContext context) {\r\n    super(context);\r\n}\r\n```\r\nSo one feasible solution is what @absurdfarce suggested: extracting a new `ByteBufCompressOps` interface. The other is leaving this code as it is. wdyt?', 'commenter': 'tomekl007'}, {'comment': 'We are running out of time, I suggest that we do not introduce any interface for now, and keep the test.', 'commenter': 'adutra'}]"
1495,core/src/test/java/com/datastax/oss/driver/internal/core/protocol/CompressSubstitutionsTest.java,"@@ -0,0 +1,159 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.protocol;
+
+import static org.assertj.core.api.Assertions.assertThat;
+
+import com.datastax.oss.driver.shaded.guava.common.collect.Sets;
+import com.datastax.oss.driver.shaded.guava.common.collect.Sets.SetView;
+import java.lang.reflect.Method;
+import java.lang.reflect.Modifier;
+import java.lang.reflect.Parameter;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+import java.util.Objects;
+import java.util.Set;
+import java.util.function.Predicate;
+import java.util.stream.Collectors;
+import org.junit.Test;
+
+public class CompressSubstitutionsTest {","[{'comment': 'OK to keep the test then, but the current test is way too complex, and method names do not comply with our guidelines.\r\n\r\nSuggested change:\r\n\r\n```\r\n@RunWith(DataProviderRunner.class)\r\npublic class CompressSubstitutionsTest {\r\n\r\n  private static final Set<String> EXCLUDED_METHOD_NAMES =\r\n      ImmutableSet.of(""algorithm"", ""readUncompressedLength"");\r\n\r\n  private static final Comparator<Method> METHOD_SIGNATURE_COMPARATOR =\r\n      Comparator.comparing(Method::getName)\r\n          .thenComparing(Method::getReturnType, (ret1, ret2) -> ret1.equals(ret2) ? 0 : -1)\r\n          .thenComparing(\r\n              Method::getParameterTypes,\r\n              (params1, params2) -> Arrays.deepEquals(params1, params2) ? 0 : -1);\r\n\r\n  @Test\r\n  @UseDataProvider(value = ""substitutionClasses"")\r\n  public void should_substitute_compressor_methods(\r\n      Class<?> substitutedClass, Class<?> substitutionClass) {\r\n\r\n    Set<Method> methodsToSubstitute =\r\n        Arrays.stream(substitutedClass.getDeclaredMethods())\r\n            .filter(method -> !EXCLUDED_METHOD_NAMES.contains(method.getName()))\r\n            .collect(Collectors.toSet());\r\n\r\n    Set<Method> substitutedMethods =\r\n        Arrays.stream(substitutionClass.getDeclaredMethods()).collect(Collectors.toSet());\r\n\r\n    assertThat(methodsToSubstitute)\r\n        .usingElementComparator(METHOD_SIGNATURE_COMPARATOR)\r\n        .containsExactlyInAnyOrderElementsOf(substitutedMethods);\r\n  }\r\n\r\n  @DataProvider\r\n  public static Object[][] substitutionClasses() {\r\n    return new Object[][] {\r\n      {Lz4Compressor.class, Lz4Substitution.class},\r\n      {SnappyCompressor.class, SnappySubstitution.class}\r\n    };\r\n  }\r\n}\r\n```', 'commenter': 'adutra'}, {'comment': 'done, thx', 'commenter': 'tomekl007'}]"
1507,driver-examples/src/main/java/com/datastax/driver/examples/apollo/AstraReadCassandraVersion.java,"@@ -14,7 +14,7 @@
  * limitations under the License.
  */
 
-package com.datastax.driver.examples.apollo;
+package com.datastax.driver.examples.astra;","[{'comment': 'Hmm there is a compilation issue here, the package was renamed to `com.datastax.driver.examples.astra` but the directory is still `com/datastax/driver/examples/apollo` -> should be `com/datastax/driver/examples/astra`.', 'commenter': 'adutra'}, {'comment': 'ah, missed it. Resolved now', 'commenter': 'msmygit'}]"
1507,manual/cloud/README.md,"@@ -1,22 +1,19 @@
-## Connecting to Apollo (Cloud)
+## Connecting to Astra (Cloud)
 
-Using the DataStax Java Driver to connect to a DataStax Apollo database is almost identical to using
+Using the DataStax Java Driver to connect to a DataStax Astra database is almost identical to using
 the driver to connect to any normal Apache CassandraÂ® database. The only differences are in how the
 driver is configured in an application and that you will need to obtain a `secure connect bundle`.
 
-The following is a Quick Start guide to writing a simple application that can connect to an Apollo
+The following is a Quick Start guide to writing a simple application that can connect to an Astra
 database.
 
 ### Prerequisites
 
 1. [Download][Download Maven] and [install][Install Maven] Maven.
-1. Create an Apollo database on [GCP][Create an Apollo database - GCP] or
-   [AWS][Create an Apollo database - AWS]; alternatively, have a team member provide access to their
-   Apollo database (instructions for [GCP][Access an Apollo database - GCP] and 
-   [AWS][Access an Apollo database - AWS]) to obtain database connection details.
+1. Create an Astra database on [GCP/AWS/Azure][Create an Astra database - GCP/AWS/Azure]; alternatively, have a team member provide access to their","[{'comment': ""Nit: try to keep line widths below 100 characters for better readability of the source code (but it doesn't affect the rendered result, of course)."", 'commenter': 'adutra'}, {'comment': 'done', 'commenter': 'msmygit'}]"
1544,core/src/test/java/com/datastax/dse/driver/internal/core/cql/reactive/ReactiveResultSetSubscriptionTest.java,"@@ -145,4 +145,31 @@ public void should_report_error_on_intermediary_page() {
     assertThat(wasAppliedSubscriber.getElements()).hasSize(1).containsExactly(true);
     assertThat(wasAppliedSubscriber.getError()).isNull();
   }
+
+  @Test
+  public void should_handle_empty_non_final_pages() {
+    CompletableFuture<AsyncResultSet> future1 = new CompletableFuture<>();
+    CompletableFuture<AsyncResultSet> future2 = new CompletableFuture<>();
+    CompletableFuture<AsyncResultSet> future3 = new CompletableFuture<>();
+    MockAsyncResultSet page1 = new MockAsyncResultSet(10, future2);
+    MockAsyncResultSet page2 = new MockAsyncResultSet(0, future3);
+    MockAsyncResultSet page3 = new MockAsyncResultSet(0, null);
+    TestSubscriber<ReactiveRow> mainSubscriber = new TestSubscriber<>(1);
+    TestSubscriber<ColumnDefinitions> colDefsSubscriber = new TestSubscriber<>();
+    TestSubscriber<ExecutionInfo> execInfosSubscriber = new TestSubscriber<>();
+    TestSubscriber<Boolean> wasAppliedSubscriber = new TestSubscriber<>();
+    ReactiveResultSetSubscription<AsyncResultSet> subscription =
+        new ReactiveResultSetSubscription<>(
+            mainSubscriber, colDefsSubscriber, execInfosSubscriber, wasAppliedSubscriber);
+    mainSubscriber.onSubscribe(subscription);
+    subscription.start(() -> future1);
+    future1.complete(page1);
+    future2.complete(page2);
+    // emulate backpressure
+    subscription.request(1);
+    future3.complete(page3);
+    subscription.request(Long.MAX_VALUE);
+    mainSubscriber.awaitTermination();
+    assertThat(mainSubscriber.isCompleted()).isTrue();
+  }","[{'comment': 'It\'s a minor thing but I prefer to not change `TestSubscriber` and rewrite the test as suggested.\r\n\r\n```suggestion\r\n  public void should_handle_empty_non_final_pages() {\r\n    CompletableFuture<AsyncResultSet> future1 = new CompletableFuture<>();\r\n    CompletableFuture<AsyncResultSet> future2 = new CompletableFuture<>();\r\n    CompletableFuture<AsyncResultSet> future3 = new CompletableFuture<>();\r\n    MockAsyncResultSet page1 = new MockAsyncResultSet(1, future2);\r\n    MockAsyncResultSet page2 = new MockAsyncResultSet(0, future3); // empty\r\n    MockAsyncResultSet page3 = new MockAsyncResultSet(1, null);\r\n    TestSubscriber<ReactiveRow> mainSubscriber = new TestSubscriber<>();\r\n    TestSubscriber<ColumnDefinitions> colDefsSubscriber = new TestSubscriber<>();\r\n    TestSubscriber<ExecutionInfo> execInfosSubscriber = new TestSubscriber<>();\r\n    TestSubscriber<Boolean> wasAppliedSubscriber = new TestSubscriber<>();\r\n    ReactiveResultSetSubscription<AsyncResultSet> subscription =\r\n        new ReactiveResultSetSubscription<>(\r\n            mainSubscriber, colDefsSubscriber, execInfosSubscriber, wasAppliedSubscriber);\r\n    mainSubscriber.onSubscribe(subscription);\r\n    subscription.start(() -> future1);\r\n    future1.complete(page1);\r\n    future2.complete(page2);\r\n    // row 1 should be served from page1\r\n    assertThat(mainSubscriber.getElements()).hasSize(1);\r\n    // page1 and page2 should have been discarded, queue should be empty\r\n    assertThat(subscription)\r\n        .extracting(""pages.elements"")\r\n        .satisfies(queue -> assertThat(((Queue<?>) queue)).isEmpty());\r\n    // row 2 should be served from page3\r\n    future3.complete(page3);\r\n    mainSubscriber.awaitTermination();\r\n    // should have completed normally\r\n    assertThat(mainSubscriber.getError()).isNull();\r\n    // should have received 2 rows\r\n    List<Row> expected = new ArrayList<>(page1.currentPage());\r\n    expected.addAll(page3.currentPage());\r\n    assertThat(mainSubscriber.getElements()).hasSize(2).extracting(""row"").isEqualTo(expected);\r\n  }\r\n```', 'commenter': 'adutra'}, {'comment': ""If I rewrite test how you suggested it will pass with previous code. Because `future3.complete(page3);` will call `drain` -> `tryNext()` and code successfully complete.\r\n\r\nI tried to rewrite this test without changing `TestSubscriber`, but didn't come up with anything"", 'commenter': 'dssysolyatin'}, {'comment': 'I tried my changes before suggesting them. They work as expected, feel free to cherry-pick: \r\n\r\nhttps://github.com/adutra/java-driver/commit/466e5ddc276576495198be0d5a89a0e2d538c482', 'commenter': 'adutra'}, {'comment': '@adutra Yes, it works. But it works for case\r\n```\r\ncurrent = pages.peek();\r\n        // if the next page is readily available,\r\n        // serve its first row now, no need to wait\r\n        // for the next drain.\r\n        if (current != null && current.hasMoreRows()) {\r\n          return current.nextRow();\r\n        }\r\n```\r\nas well', 'commenter': 'dssysolyatin'}, {'comment': 'I meant if you change branch to `4.x` branch and try to use your test. It will work, but it should fail', 'commenter': 'dssysolyatin'}, {'comment': 'Ah right, now I understand what you had in mind. Please disregard my previous comments :-) ', 'commenter': 'adutra'}]"
1544,core/src/test/java/com/datastax/dse/driver/internal/core/cql/reactive/TestSubscriber.java,"@@ -31,16 +31,26 @@
 
   private final List<T> elements = new ArrayList<>();
   private final CountDownLatch latch = new CountDownLatch(1);
+  private long demand;","[{'comment': 'Please revert changes in this file, see my other comment + suggestion.', 'commenter': 'adutra'}, {'comment': '```suggestion\r\n  private final long demand;\r\n```', 'commenter': 'adutra'}]"
1544,core/src/test/java/com/datastax/dse/driver/internal/core/cql/reactive/TestSubscriber.java,"@@ -69,6 +80,10 @@ public Throwable getError() {
     return elements;
   }
 
+  public boolean isCompleted() {","[{'comment': 'I don\'t like much this new boolean property because it is ambiguous. Some people consider that a flux that terminates with an error is completed. I don\'t think we need this field imo; but if you absolutely want to expose a boolean property to indicate the termination status, I\'d suggest instead to use the word ""terminated"":\r\n\r\n```\r\n  public boolean isTerminated() {\r\n    return latch.getCount() == 0;\r\n  }\r\n``` \r\n\r\nIf you want to further distinguish normal vs abnormal termination, you can then inspect `getError() == null`.', 'commenter': 'adutra'}, {'comment': ""I need it because those two lines don't work for my test case\r\n```\r\nmainSubscriber.awaitTermination();\r\nassertThat(mainSubscriber.getError()).isNull();\r\n```\r\n\r\n`mainSubscriber.awaitTermination()` does not throw any exception if after timeout there is still data in publisher.  In this regard, the question, is it correct behaviour that `mainSubscriber.awaitTermination()` does not throw exception after timeout ?\r\n\r\n"", 'commenter': 'dssysolyatin'}, {'comment': '> is it correct behaviour that mainSubscriber.awaitTermination() does not throw exception after timeout ?\r\n\r\nArguably not :-)\r\nFeel free to change the method to throw throw an assertion error if the timeout fires.', 'commenter': 'adutra'}, {'comment': 'I deleted isCompleted method and fixed `awaitTermination`', 'commenter': 'dssysolyatin'}]"
1544,core/src/test/java/com/datastax/dse/driver/internal/core/cql/reactive/ReactiveResultSetSubscriptionTest.java,"@@ -145,4 +145,31 @@ public void should_report_error_on_intermediary_page() {
     assertThat(wasAppliedSubscriber.getElements()).hasSize(1).containsExactly(true);
     assertThat(wasAppliedSubscriber.getError()).isNull();
   }
+
+  @Test
+  public void should_handle_empty_non_final_pages() {
+    CompletableFuture<AsyncResultSet> future1 = new CompletableFuture<>();
+    CompletableFuture<AsyncResultSet> future2 = new CompletableFuture<>();
+    CompletableFuture<AsyncResultSet> future3 = new CompletableFuture<>();
+    MockAsyncResultSet page1 = new MockAsyncResultSet(10, future2);
+    MockAsyncResultSet page2 = new MockAsyncResultSet(0, future3);
+    MockAsyncResultSet page3 = new MockAsyncResultSet(0, null);
+    TestSubscriber<ReactiveRow> mainSubscriber = new TestSubscriber<>(1);
+    TestSubscriber<ColumnDefinitions> colDefsSubscriber = new TestSubscriber<>();
+    TestSubscriber<ExecutionInfo> execInfosSubscriber = new TestSubscriber<>();
+    TestSubscriber<Boolean> wasAppliedSubscriber = new TestSubscriber<>();
+    ReactiveResultSetSubscription<AsyncResultSet> subscription =
+        new ReactiveResultSetSubscription<>(
+            mainSubscriber, colDefsSubscriber, execInfosSubscriber, wasAppliedSubscriber);
+    mainSubscriber.onSubscribe(subscription);
+    subscription.start(() -> future1);
+    future1.complete(page1);
+    future2.complete(page2);
+    // emulate backpressure
+    subscription.request(1);
+    future3.complete(page3);
+    subscription.request(Long.MAX_VALUE);
+    mainSubscriber.awaitTermination();
+    assertThat(mainSubscriber.isCompleted()).isTrue();","[{'comment': 'Even if you don\'t change the entire test, at least you could add the following assertions at the end:\r\n\r\n```\r\n    assertThat(mainSubscriber.getError()).isNull();\r\n    List<Row> expected = new ArrayList<>(page1.currentPage());\r\n    expected.addAll(page3.currentPage());\r\n    assertThat(mainSubscriber.getElements()).hasSize(...).extracting(""row"").isEqualTo(expected);\r\n```', 'commenter': 'adutra'}, {'comment': 'added', 'commenter': 'dssysolyatin'}]"
1544,core/src/test/java/com/datastax/dse/driver/internal/core/cql/reactive/ReactiveResultSetSubscriptionTest.java,"@@ -145,4 +145,31 @@ public void should_report_error_on_intermediary_page() {
     assertThat(wasAppliedSubscriber.getElements()).hasSize(1).containsExactly(true);
     assertThat(wasAppliedSubscriber.getError()).isNull();
   }
+
+  @Test
+  public void should_handle_empty_non_final_pages() {
+    CompletableFuture<AsyncResultSet> future1 = new CompletableFuture<>();
+    CompletableFuture<AsyncResultSet> future2 = new CompletableFuture<>();
+    CompletableFuture<AsyncResultSet> future3 = new CompletableFuture<>();
+    MockAsyncResultSet page1 = new MockAsyncResultSet(10, future2);
+    MockAsyncResultSet page2 = new MockAsyncResultSet(0, future3);
+    MockAsyncResultSet page3 = new MockAsyncResultSet(0, null);","[{'comment': 'I think it could be interesting to test the case where `page3 = new MockAsyncResultSet(10, null);`. From your discussions with the Scylla devs, it seems that this is something that could happen.', 'commenter': 'adutra'}, {'comment': 'Yeap, I changed 0 to 10', 'commenter': 'dssysolyatin'}]"
1545,core/src/test/java/com/datastax/oss/driver/internal/core/session/PoolManagerTest.java,"@@ -0,0 +1,72 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.session;
+
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.mockito.Mockito.spy;
+import static org.mockito.Mockito.when;
+
+import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
+import com.datastax.oss.driver.api.core.config.DriverConfig;
+import com.datastax.oss.driver.api.core.config.DriverExecutionProfile;
+import com.datastax.oss.driver.internal.core.context.EventBus;
+import com.datastax.oss.driver.internal.core.context.InternalDriverContext;
+import com.datastax.oss.driver.internal.core.context.NettyOptions;
+import io.netty.channel.DefaultEventLoopGroup;
+import java.util.concurrent.ConcurrentHashMap;
+import org.junit.Before;
+import org.junit.Test;
+import org.mockito.Mock;
+import org.mockito.MockitoAnnotations;
+
+public class PoolManagerTest {
+  @Mock private InternalDriverContext context;
+  @Mock private NettyOptions nettyOptions;
+  @Mock private DriverConfig config;
+  @Mock private DriverExecutionProfile defaultProfile;
+
+  private EventBus eventBus;
+
+  @Before
+  public void setup() {
+    MockitoAnnotations.initMocks(this);
+
+    DefaultEventLoopGroup adminEventLoopGroup = new DefaultEventLoopGroup(1);
+    when(nettyOptions.adminEventExecutorGroup()).thenReturn(adminEventLoopGroup);
+    when(context.getNettyOptions()).thenReturn(nettyOptions);
+    eventBus = spy(new EventBus(""test""));
+    when(context.getEventBus()).thenReturn(eventBus);
+    when(config.getDefaultProfile()).thenReturn(defaultProfile);
+    when(context.getConfig()).thenReturn(config);
+  }
+
+  @Test
+  public void should_use_weak_values_if_config_is_true_or_undefined() {
+    when(defaultProfile.getBoolean(DefaultDriverOption.PREPARED_CACHE_WEAK_VALUES, true))
+        .thenReturn(true);
+    // As weak values map class is MapMakerInternalMap
+    assertThat(new PoolManager(context).getRepreparePayloads())
+        .isNotInstanceOf(ConcurrentHashMap.class);
+  }
+
+  @Test
+  public void should_not_use_weak_values_if_config_is_false() {
+    when(defaultProfile.getBoolean(DefaultDriverOption.PREPARED_CACHE_WEAK_VALUES, true))
+        .thenReturn(false);
+    assertThat(new PoolManager(context).getRepreparePayloads())
+        .isInstanceOf(ConcurrentHashMap.class);","[{'comment': 'Can we check weak or strong reference directly?', 'commenter': 'songdongsheng'}, {'comment': 'It is all implemented by MapMakerInternalMap, cannot just the value is weak or strong references.', 'commenter': 'hnlzq163'}, {'comment': ""OK, it use guava's sealed implementation, no simple method to check it's strong reference of weak reference values."", 'commenter': 'songdongsheng'}]"
1545,core/src/main/java/com/datastax/oss/driver/api/core/config/DefaultDriverOption.java,"@@ -687,6 +687,12 @@
    * <p>Value-type: {@link java.time.Duration Duration}
    */
   REPREPARE_TIMEOUT(""advanced.prepared-statements.reprepare-on-up.timeout""),
+  /**
+   * Whether the prepared statements cache use weak values.
+   *
+   * <p>Value-type: boolean
+   */
+  PREPARED_CACHE_WEAK_VALUES(""advanced.prepared-statements.prepared-cache.weak-values""),","[{'comment': 'You must add new enum constants at the end of the list, otherwise the `revapi` plugin will complain that this is a breaking change, and `mvn clean verify` will fail.', 'commenter': 'adutra'}, {'comment': 'OK, already updated', 'commenter': 'hnlzq163'}]"
1545,core/src/main/java/com/datastax/oss/driver/internal/core/session/PoolManager.java,"@@ -95,6 +94,13 @@ public PoolManager(InternalDriverContext context) {
     this.adminExecutor = context.getNettyOptions().adminEventExecutorGroup().next();
     this.config = context.getConfig().getDefaultProfile();
     this.singleThreaded = new SingleThreaded(context);
+
+    if (config.getBoolean(DefaultDriverOption.PREPARED_CACHE_WEAK_VALUES, true)) {
+      this.repreparePayloads = new MapMaker().weakValues().makeMap();
+    } else {
+      LOG.debug(""[{}] Strong values are used for prepared statements cache."", logPrefix);
+      this.repreparePayloads = new MapMaker().makeMap();
+    }","[{'comment': '```suggestion\r\n    if (config.getBoolean(DefaultDriverOption.PREPARED_CACHE_WEAK_VALUES, true)) {\r\n      LOG.debug(""[{}] Prepared statements cache configured to use weak values"", logPrefix);\r\n      this.repreparePayloads = new MapMaker().weakValues().makeMap();\r\n    } else {\r\n      LOG.debug(""[{}] Prepared statements cache configured to use strong values"", logPrefix);\r\n      this.repreparePayloads = new MapMaker().makeMap();\r\n    }\r\n```', 'commenter': 'adutra'}]"
1545,core/src/main/resources/reference.conf,"@@ -2161,6 +2161,18 @@ datastax-java-driver {
       # Overridable in a profile: no
       timeout = ${datastax-java-driver.advanced.connection.init-query-timeout}
     }
+
+    # How to build the cache of prepared statements.
+    prepared-cache {
+      # Whether use the weak reference values for the prepared statements cache.
+      #
+      # If this option is absent, the weak reference values will be used.","[{'comment': '```suggestion\r\n      # Whether to use weak references for the prepared statements cache values.\r\n      #\r\n      # If this option is absent, weak references will be used.\r\n```', 'commenter': 'adutra'}]"
1545,core/src/test/java/com/datastax/oss/driver/internal/core/session/PoolManagerTest.java,"@@ -0,0 +1,72 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.session;
+
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.mockito.Mockito.spy;
+import static org.mockito.Mockito.when;
+
+import com.datastax.oss.driver.api.core.config.DefaultDriverOption;
+import com.datastax.oss.driver.api.core.config.DriverConfig;
+import com.datastax.oss.driver.api.core.config.DriverExecutionProfile;
+import com.datastax.oss.driver.internal.core.context.EventBus;
+import com.datastax.oss.driver.internal.core.context.InternalDriverContext;
+import com.datastax.oss.driver.internal.core.context.NettyOptions;
+import io.netty.channel.DefaultEventLoopGroup;
+import java.util.concurrent.ConcurrentHashMap;
+import org.junit.Before;
+import org.junit.Test;
+import org.mockito.Mock;
+import org.mockito.MockitoAnnotations;
+
+public class PoolManagerTest {
+  @Mock private InternalDriverContext context;
+  @Mock private NettyOptions nettyOptions;
+  @Mock private DriverConfig config;
+  @Mock private DriverExecutionProfile defaultProfile;
+
+  private EventBus eventBus;
+
+  @Before
+  public void setup() {
+    MockitoAnnotations.initMocks(this);
+
+    DefaultEventLoopGroup adminEventLoopGroup = new DefaultEventLoopGroup(1);
+    when(nettyOptions.adminEventExecutorGroup()).thenReturn(adminEventLoopGroup);
+    when(context.getNettyOptions()).thenReturn(nettyOptions);
+    eventBus = spy(new EventBus(""test""));
+    when(context.getEventBus()).thenReturn(eventBus);
+    when(config.getDefaultProfile()).thenReturn(defaultProfile);
+    when(context.getConfig()).thenReturn(config);
+  }","[{'comment': '```suggestion\r\n  @Mock private InternalDriverContext context;\r\n  @Mock private NettyOptions nettyOptions;\r\n  @Mock private DriverConfig config;\r\n  @Mock private DriverExecutionProfile defaultProfile;\r\n\r\n  @Before\r\n  public void setup() {\r\n    MockitoAnnotations.initMocks(this);\r\n\r\n    DefaultEventLoopGroup adminEventLoopGroup = new DefaultEventLoopGroup(1);\r\n    when(nettyOptions.adminEventExecutorGroup()).thenReturn(adminEventLoopGroup);\r\n    when(context.getNettyOptions()).thenReturn(nettyOptions);\r\n    when(context.getEventBus()).thenReturn(new EventBus(""test""));\r\n    when(config.getDefaultProfile()).thenReturn(defaultProfile);\r\n    when(context.getConfig()).thenReturn(config);\r\n  }\r\n```', 'commenter': 'adutra'}]"
1547,core/src/main/java/com/datastax/oss/driver/api/core/servererrors/OverloadedException.java,"@@ -34,8 +34,8 @@
  */
 public class OverloadedException extends QueryExecutionException {
 
-  public OverloadedException(@NonNull Node coordinator) {
-    super(coordinator, String.format(""%s is bootstrapping"", coordinator), null, false);
+  public OverloadedException(@NonNull Node coordinator, @NonNull String message) {
+    super(coordinator, message, null, false);","[{'comment': 'I think we could do as in 3.x and augment the original message with the coordinator address:\r\n\r\n```suggestion\r\n    super(coordinator, String.format(""%s was overloaded: %s"", coordinator, message), null, false);\r\n```', 'commenter': 'adutra'}, {'comment': 'Agree. Uploaded a new version of the commit, used ""is"" instead of ""was"" to be in-synch with the other error message (bootstraping). But let me know if you prefer was to be in synch with 3.x ', 'commenter': 'chicobento'}]"
1568,driver-core/src/main/java/com/datastax/driver/core/exceptions/ReadTimeoutException.java,"@@ -42,8 +42,11 @@ public ReadTimeoutException(
     super(
         endPoint,
         String.format(
+            ""Cassandra timeout during read query at consistency %s (%s). ""
+                + ""In case this was generated during read repair, the consistency level is not representative of the actual consistency."",
             ""Cassandra timeout during read query at consistency %s (%s)"",","[{'comment': 'this is a redundant line. I believe this line is supposed to be replaced by the new changes.', 'commenter': 'justinchuch'}, {'comment': 'Thanks for pointing out. Probably skipped while switching/raising multiple PR on this. Taken care now. ', 'commenter': 'AnkitBarsainyaPSL'}]"
1568,driver-core/src/main/java/com/datastax/driver/core/exceptions/ReadTimeoutException.java,"@@ -42,8 +42,11 @@ public ReadTimeoutException(
     super(
         endPoint,
         String.format(
+            ""Cassandra timeout during read query at consistency %s (%s). ""
+                + ""In case this was generated during read repair, the consistency level is not representative of the actual consistency."",
             ""Cassandra timeout during read query at consistency %s (%s)"",
-            consistency, formatDetails(received, required, dataPresent)),
+            consistency,","[{'comment': 'another comment is that this is an unnecessary formatting change not related to the problem we want to fix. please remove.', 'commenter': 'berndocklin'}, {'comment': 'Thanks. Taken care of now.', 'commenter': 'AnkitBarsainyaPSL'}]"
1570,core/src/test/java/com/datastax/oss/driver/internal/core/cql/CqlRequestHandlerTest.java,"@@ -105,6 +110,28 @@ public void should_fail_if_no_node_available() {
     }
   }
 
+  @Test
+  public void should_fail_if_nodes_unavailable() {
+    RequestHandlerTestHarness.Builder harnessBuilder = RequestHandlerTestHarness.builder();
+    try (RequestHandlerTestHarness harness = harnessBuilder.withEmptyPool(node1).build()) {
+      CompletionStage<AsyncResultSet> resultSetFuture =
+              new CqlRequestHandler(
+                      UNDEFINED_IDEMPOTENCE_STATEMENT,
+                      harness.getSession(),
+                      harness.getContext(),
+                      ""test"")
+                      .handle();
+      assertThatStage(resultSetFuture)
+              .isFailed(error -> {
+                assertThat(error).isInstanceOf(AllNodesFailedException.class);","[{'comment': ""Clearly haven't worked with assertj before, I'd love feedback on how to make this block more readable!"", 'commenter': 'akhaku'}, {'comment': 'That\'s very well written, congrats :-)\r\n\r\nJust as a suggestion, how about we include 2 nodes in this test? Something like:\r\n\r\n```\r\n@Test\r\npublic void should_fail_if_nodes_unavailable() {\r\n  RequestHandlerTestHarness.Builder harnessBuilder = RequestHandlerTestHarness.builder();\r\n  try (RequestHandlerTestHarness harness =\r\n      harnessBuilder.withEmptyPool(node1).withEmptyPool(node2).build()) {\r\n    CompletionStage<AsyncResultSet> resultSetFuture =\r\n        new CqlRequestHandler(\r\n                UNDEFINED_IDEMPOTENCE_STATEMENT,\r\n                harness.getSession(),\r\n                harness.getContext(),\r\n                ""test"")\r\n            .handle();\r\n    assertThatStage(resultSetFuture)\r\n        .isFailed(\r\n            error -> {\r\n              assertThat(error).isInstanceOf(AllNodesFailedException.class);\r\n              Map<Node, List<Throwable>> allErrors =\r\n                  ((AllNodesFailedException) error).getAllErrors();\r\n              assertThat(allErrors).hasSize(2);\r\n              assertThat(allErrors)\r\n                  .hasEntrySatisfying(\r\n                      node1,\r\n                      nodeErrors ->\r\n                          assertThat(nodeErrors)\r\n                              .singleElement()\r\n                              .isInstanceOf(NodeUnavailableException.class));\r\n              assertThat(allErrors)\r\n                  .hasEntrySatisfying(\r\n                      node2,\r\n                      nodeErrors ->\r\n                          assertThat(nodeErrors)\r\n                              .singleElement()\r\n                              .isInstanceOf(NodeUnavailableException.class));\r\n            });\r\n  }\r\n}\r\n```', 'commenter': 'adutra'}, {'comment': 'thanks, done', 'commenter': 'akhaku'}]"
1570,core/src/main/java/com/datastax/oss/driver/api/core/NodeUnavailableException.java,"@@ -0,0 +1,54 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core;
+
+import com.datastax.oss.driver.api.core.metadata.Node;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import java.util.Objects;
+
+/**
+ * Indicates that a Node encountered as part of processing a query plan has no connections
+ * available.
+ *
+ * <p>This might happen under heavy load. The driver will automatically try the next node in the
+ * query plan. Therefore, the only way that the client can observe this exception is as part of a
+ * {@link AllNodesFailedException}.
+ * <p>The default load balancer will attempt to evenly distribute requests across nodes, so a common","[{'comment': 'You need to run `mvn fmt:format` on your branch, I see that a few classes are not compliant.', 'commenter': 'adutra'}, {'comment': 'That\'s a good explanation, but I wouldn\'t mention ""load balancer"" and ""evenly distribute"" here. I would rephrase this paragraph and the following one as follows:\r\n\r\n```\r\n * <p>A common reason to encounter this error is when the configured number of connections per node\r\n * and requests per connection is not high enough to absorb the overall request rate. This can be\r\n * mitigated by tuning the following options:\r\n *\r\n * <ul>\r\n *   <li>{@code advanced.connection.pool.local.size};\r\n *   <li>{@code advanced.connection.pool.remote.size};\r\n *   <li>{@code advanced.connection.max-requests-per-connection}.\r\n * </ul>\r\n *\r\n * See {@code reference.conf} for more details.\r\n *\r\n * <p>Another possibility is when you are trying to direct a request {@linkplain\r\n * com.datastax.oss.driver.api.core.cql.Statement#setNode(Node) to a particular node}, but that node\r\n * has no connections available.\r\n```\r\n', 'commenter': 'adutra'}, {'comment': 'done and done', 'commenter': 'akhaku'}]"
1570,core/src/main/java/com/datastax/oss/driver/api/core/NodeUnavailableException.java,"@@ -0,0 +1,54 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core;
+
+import com.datastax.oss.driver.api.core.metadata.Node;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import java.util.Objects;
+
+/**
+ * Indicates that a Node encountered as part of processing a query plan has no connections","[{'comment': 'Just trying to improve the style a bit:\r\n\r\n```suggestion\r\n * Indicates that a {@link Node} was selected in a query plan, but it had no connection available.\r\n```', 'commenter': 'adutra'}, {'comment': 'done, thanks', 'commenter': 'akhaku'}]"
1570,changelog/README.md,"@@ -2,6 +2,10 @@
 
 <!-- Note: contrary to 3.x, insert new entries *first* in their section -->
 
+### 4.14.0","[{'comment': '```suggestion\r\n### 4.14.0 (in progress)\r\n```', 'commenter': 'adutra'}, {'comment': 'done', 'commenter': 'akhaku'}]"
1570,upgrade_guide/README.md,"@@ -1,5 +1,15 @@
 ## Upgrade guide
 
+### 4.14.0
+
+#### AllNodesFailedException instead of NoNodeAvailableException in certain cases
+
+[JAVA-2959](https://datastax-oss.atlassian.net/browse/JAVA-2959) changed the behavior for when a
+request is directed to a particular node (via `Statement#setNode(Node)`) and all connections to that
+node are busy. Previously you would get back a `NoNodeAvailableException` but you will now get back
+an `AllNodesFailedException` where the `getAllErrors` map contains a `NodeUnavailableException`
+for that node.","[{'comment': '`Statement#setNode(Node)` is not the only way by which you could observe the new behavior. I suggest a more generic rephrasing:\r\n\r\n```suggestion\r\n[JAVA-2959](https://datastax-oss.atlassian.net/browse/JAVA-2959) changed the behavior for when a\r\nrequest cannot be executed because all nodes tried were busy. Previously you would get back a \r\n`NoNodeAvailableException` but you will now get back an `AllNodesFailedException` where the \r\n`getAllErrors` map contains a `NodeUnavailableException` for that node.\r\n```', 'commenter': 'adutra'}, {'comment': 'done', 'commenter': 'akhaku'}]"
1573,driver-core/src/main/java/com/datastax/driver/core/ControlConnection.java,"@@ -956,7 +975,11 @@ private boolean isValidPeer(Row peerRow, boolean logIfInvalid) {
               && !peerRow.isNull(""native_port"");
     } else {
       isValid &=
-          peerRow.getColumnDefinitions().contains(""rpc_address"") && !peerRow.isNull(""rpc_address"");
+          (peerRow.getColumnDefinitions().contains(""rpc_address"") && !peerRow.isNull(""rpc_address""))
+              || (peerRow.getColumnDefinitions().contains(""native_transport_address"")
+                  && peerRow.getColumnDefinitions().contains(""native_transport_port"")
+                  && !peerRow.isNull(""native_transport_address"")
+                  && !peerRow.isNull(""native_transport_port""));","[{'comment': 'How about a row that has `native_transport_port_ssl` but not `native_transport_port`? Can this even happen?', 'commenter': 'adutra'}, {'comment': ""native_transport_port has a default value of 9042 so even if nothing is specified for it in cassandra.yaml the corresponding system.peers row will still have a value.  I don't think there is a way to create a row here that doesn't have a native_transport_port entry so this logic should hold up.  I'll take a look at DSE code in a bit to confirm.\r\n\r\nI also just fired up my local 3 node DSE 6.8.13 cluster (with no value specified for native_transport_port on any node) to confirm... and it does look like the value is populated as expected."", 'commenter': 'absurdfarce'}]"
1573,driver-core/src/main/java/com/datastax/driver/core/ControlConnection.java,"@@ -611,6 +611,17 @@ private static void updateInfo(
       InetAddress nativeAddress = row.getInet(""native_address"");
       int nativePort = row.getInt(""native_port"");
       broadcastRpcAddress = new InetSocketAddress(nativeAddress, nativePort);
+    } else if (row.getColumnDefinitions().contains(""native_transport_address"")) {
+      // DSE 6.8 introduced native_transport_address and native_transport_port for the
+      // listen address.  Also included is native_transport_port_ssl (in case users
+      // want to setup a different port for SSL and non-SSL conns).
+      InetAddress nativeAddress = row.getInet(""native_transport_address"");
+      int nativePort = row.getInt(""native_transport_port"");
+      if (cluster.getCluster().getConfiguration().getProtocolOptions().getSSLOptions() != null
+          && !row.isNull(""native_transport_port_ssl"")) {
+        nativePort = row.getInt(""native_transport_port_ssl"");
+      }
+      broadcastRpcAddress = new InetSocketAddress(nativeAddress, nativePort);","[{'comment': ""Location of this block (and similar logic in the other blocks) was very deliberate.  We can't go underneath the base case handler of rpc_address + default port but it does seem like this specific case will be less likely than the native_address/port case above us."", 'commenter': 'absurdfarce'}]"
1573,driver-core/src/test/java/com/datastax/driver/core/exceptions/ExceptionsTest.java,"@@ -284,7 +284,7 @@ public void should_create_proper_read_timeout_exception() {
     ReadTimeoutException e = new ReadTimeoutException(endPoint1, LOCAL_QUORUM, 2, 3, true);
     assertThat(e.getMessage())
         .isEqualTo(
-            ""Cassandra timeout during read query at consistency LOCAL_QUORUM (3 responses were required but only 2 replica responded)"");
+            ""Cassandra timeout during read query at consistency LOCAL_QUORUM (3 responses were required but only 2 replica responded). In case this was generated during read repair, the consistency level is not representative of the actual consistency."");","[{'comment': 'This and the similar change below address some minor fallout from https://github.com/datastax/java-driver/pull/1571', 'commenter': 'absurdfarce'}, {'comment': 'Ouch, and I approved that PR :-D.\r\nAlternatively, you could use `assertThat(...).contains(...)` to keep the code concise.', 'commenter': 'adutra'}, {'comment': ""Hmmmm... that might actually be desirable anyway.  It makes the test a purer expression of the thing we're actually testing for rather than worrying about the total structure of the log message.  I think I like that."", 'commenter': 'absurdfarce'}]"
1573,driver-core/src/test/java/com/datastax/driver/core/ControlConnectionTest.java,"@@ -560,6 +571,292 @@ public void should_connect_when_peers_v2_table_not_present() {
     }
   }
 
+  /**
+   * Cassandra 4.0 supports native_address and native_port columns in system.peers_v2. We want to
+   * validate our ability to build correct metadata when drawing data from these tables.
+   */
+  @Test(groups = ""short"")
+  @CCMConfig(createCcm = false)
+  public void should_extract_hosts_using_native_address_port_from_peersv2()
+      throws UnknownHostException {
+
+    InetAddress expectedAddress = InetAddress.getByName(""4.3.2.1"");
+    int expectedPort = 2409;
+    PeerRowState state =
+        PeerRowState.builder()
+            .peersV2(""native_address"", expectedAddress)
+            .peersV2(""native_port"", expectedPort)
+            .expectedAddress(expectedAddress)
+            .expectedPort(expectedPort)
+            .build();
+    runPeerTest(state);
+  }
+
+  /** DSE 6.8 includes native_transport_address and native_transport_port in system.peers. */
+  @Test(groups = ""short"")
+  @CCMConfig(createCcm = false)
+  public void should_extract_hosts_using_native_transport_address_port_from_peers()
+      throws UnknownHostException {
+
+    InetAddress expectedAddress = InetAddress.getByName(""4.3.2.1"");
+    int expectedPort = 2409;
+    PeerRowState state =
+        PeerRowState.builder()
+            .peers(""native_transport_address"", expectedAddress)
+            .peers(""native_transport_port"", expectedPort)
+            .expectedAddress(expectedAddress)
+            .expectedPort(expectedPort)
+            .build();
+    runPeerTest(state);
+  }
+
+  /**
+   * If both native_transport_port and native_transport_port_ssl are present we expect the SSL
+   * version to win. Note that we don't make this conditional on SSL actually being configured.
+   */
+  @Test(groups = ""short"")
+  @CCMConfig(createCcm = false)
+  @Ignore(""Requires SSL support in scassandra"")
+  public void should_extract_hosts_using_native_transport_address_port_ssl_from_peers()
+      throws UnknownHostException {
+
+    InetAddress expectedAddress = InetAddress.getByName(""4.3.2.1"");
+    int expectedPort = 2409;
+    PeerRowState state =
+        PeerRowState.builder()
+            .peers(""native_transport_address"", expectedAddress)
+            .peers(""native_transport_port"", expectedPort - 100)
+            .peers(""native_transport_port_ssl"", expectedPort)
+            .expectedAddress(expectedAddress)
+            .expectedPort(expectedPort)
+            .build();
+    runPeerTest(state);
+  }
+
+  /**
+   * The default case. If we can't get native_address/port out of system.peers_v2 or
+   * native_transport_address/port out of system.peers the fall back to rpc_address + a default port
+   */
+  @Test(groups = ""short"")
+  @CCMConfig(createCcm = false)
+  public void should_extract_hosts_using_rpc_address_from_peers() throws UnknownHostException {
+
+    InetAddress expectedAddress = InetAddress.getByName(""4.3.2.1"");
+    PeerRowState state =
+        PeerRowState.builder()
+            .peers(""rpc_address"", expectedAddress)
+            /* DefaultEndPointFactory isn't happy if we don't have both a value for
+             * both peer and rpc_address */
+            .peers(""peer"", InetAddress.getByName(""1.2.3.4""))
+            .expectedAddress(expectedAddress)
+            .build();
+    runPeerTest(state);
+  }
+
+  private void runPeerTest(PeerRowState state) {
+
+    ScassandraCluster scassandras =
+        ScassandraCluster.builder().withNodes(2).withPeersV2(state.usePeersV2()).build();
+    scassandras.init();
+
+    Cluster cluster = null;
+    try {
+
+      scassandras.node(1).primingClient().clearAllPrimes();
+
+      PrimingClient primingClient = scassandras.node(1).primingClient();
+
+      /* Note that we always prime system.local; ControlConnection.refreshNodeAndTokenMap() gets angry
+       * if this is empty */
+      primingClient.prime(
+          PrimingRequest.queryBuilder()
+              .withQuery(""SELECT * FROM system.local WHERE key='local'"")
+              .withThen(then().withColumnTypes(SELECT_LOCAL).withRows(state.getLocalRow()).build())
+              .build());
+
+      if (state.shouldPrimePeers()) {
+
+        primingClient.prime(
+            PrimingRequest.queryBuilder()
+                .withQuery(""SELECT * FROM system.peers"")
+                .withThen(
+                    then()
+                        .withColumnTypes(state.isDse68() ? SELECT_PEERS_DSE68 : SELECT_PEERS)
+                        .withRows(state.getPeersRow())
+                        .build())
+                .build());
+      }
+      if (state.shouldPrimePeersV2()) {
+
+        primingClient.prime(
+            PrimingRequest.queryBuilder()
+                .withQuery(""SELECT * FROM system.peers_v2"")
+                .withThen(
+                    then().withColumnTypes(SELECT_PEERS_V2).withRows(state.getPeersV2Row()).build())
+                .build());
+      } else {
+
+        /* Must return an error code in this case in order to trigger the driver's downgrade to system.peers */
+        primingClient.prime(
+            PrimingRequest.queryBuilder()
+                .withQuery(""SELECT * FROM system.peers_v2"")
+                .withThen(then().withResult(Result.invalid).build()));
+      }
+
+      cluster =
+          Cluster.builder()
+              .addContactPoints(scassandras.address(1).getAddress())
+              .withPort(scassandras.getBinaryPort())
+              .withNettyOptions(nonQuietClusterCloseOptions)
+              .build();
+      cluster.connect();
+
+      Collection<EndPoint> hostEndPoints =
+          Collections2.transform(
+              cluster.getMetadata().allHosts(),
+              new Function<Host, EndPoint>() {
+                public EndPoint apply(Host host) {
+                  return host.getEndPoint();
+                }
+              });
+      assertThat(hostEndPoints).contains(state.getExpectedEndPoint(scassandras));
+    } finally {
+      if (cluster != null) cluster.close();
+      scassandras.stop();
+    }
+  }
+
+  static class PeerRowState {","[{'comment': ""State object + builder is a bit more convoluted than I'd like but it has the effect of nicely isolating changes required for an individual test.  Without this class I found myself passing around multiple config options (all of which were either related or similar to each other) in order to setup a given test.  Introducing this class had the effect of making the intent of a single test case above very clear."", 'commenter': 'absurdfarce'}, {'comment': 'Very clean design imo :-)', 'commenter': 'adutra'}]"
1573,driver-core/src/test/java/com/datastax/driver/core/ControlConnectionTest.java,"@@ -560,6 +571,292 @@ public void should_connect_when_peers_v2_table_not_present() {
     }
   }
 
+  /**
+   * Cassandra 4.0 supports native_address and native_port columns in system.peers_v2. We want to
+   * validate our ability to build correct metadata when drawing data from these tables.
+   */
+  @Test(groups = ""short"")
+  @CCMConfig(createCcm = false)
+  public void should_extract_hosts_using_native_address_port_from_peersv2()
+      throws UnknownHostException {
+
+    InetAddress expectedAddress = InetAddress.getByName(""4.3.2.1"");
+    int expectedPort = 2409;
+    PeerRowState state =
+        PeerRowState.builder()
+            .peersV2(""native_address"", expectedAddress)
+            .peersV2(""native_port"", expectedPort)
+            .expectedAddress(expectedAddress)
+            .expectedPort(expectedPort)
+            .build();
+    runPeerTest(state);
+  }
+
+  /** DSE 6.8 includes native_transport_address and native_transport_port in system.peers. */
+  @Test(groups = ""short"")
+  @CCMConfig(createCcm = false)
+  public void should_extract_hosts_using_native_transport_address_port_from_peers()
+      throws UnknownHostException {
+
+    InetAddress expectedAddress = InetAddress.getByName(""4.3.2.1"");
+    int expectedPort = 2409;
+    PeerRowState state =
+        PeerRowState.builder()
+            .peers(""native_transport_address"", expectedAddress)
+            .peers(""native_transport_port"", expectedPort)
+            .expectedAddress(expectedAddress)
+            .expectedPort(expectedPort)
+            .build();
+    runPeerTest(state);
+  }
+
+  /**
+   * If both native_transport_port and native_transport_port_ssl are present we expect the SSL
+   * version to win. Note that we don't make this conditional on SSL actually being configured.
+   */
+  @Test(groups = ""short"")
+  @CCMConfig(createCcm = false)
+  @Ignore(""Requires SSL support in scassandra"")","[{'comment': 'Really no good way to test this without SSL support for scassandra.  https://github.com/scassandra/scassandra-server/issues/65 is of relevance here.\r\n\r\nA minor addendum; this might be obvious but any change here would also require tweaks to runPeerTest() in order to allow it to (optionally) enable SSL based on the contents of the PeerRowState instance.', 'commenter': 'absurdfarce'}]"
1573,driver-core/src/test/java/com/datastax/driver/core/ControlConnectionTest.java,"@@ -560,6 +571,292 @@ public void should_connect_when_peers_v2_table_not_present() {
     }
   }
 
+  /**
+   * Cassandra 4.0 supports native_address and native_port columns in system.peers_v2. We want to
+   * validate our ability to build correct metadata when drawing data from these tables.
+   */
+  @Test(groups = ""short"")
+  @CCMConfig(createCcm = false)
+  public void should_extract_hosts_using_native_address_port_from_peersv2()
+      throws UnknownHostException {
+
+    InetAddress expectedAddress = InetAddress.getByName(""4.3.2.1"");
+    int expectedPort = 2409;
+    PeerRowState state =
+        PeerRowState.builder()
+            .peersV2(""native_address"", expectedAddress)
+            .peersV2(""native_port"", expectedPort)
+            .expectedAddress(expectedAddress)
+            .expectedPort(expectedPort)
+            .build();
+    runPeerTest(state);
+  }
+
+  /** DSE 6.8 includes native_transport_address and native_transport_port in system.peers. */
+  @Test(groups = ""short"")
+  @CCMConfig(createCcm = false)
+  public void should_extract_hosts_using_native_transport_address_port_from_peers()
+      throws UnknownHostException {
+
+    InetAddress expectedAddress = InetAddress.getByName(""4.3.2.1"");
+    int expectedPort = 2409;
+    PeerRowState state =
+        PeerRowState.builder()
+            .peers(""native_transport_address"", expectedAddress)
+            .peers(""native_transport_port"", expectedPort)
+            .expectedAddress(expectedAddress)
+            .expectedPort(expectedPort)
+            .build();
+    runPeerTest(state);
+  }
+
+  /**
+   * If both native_transport_port and native_transport_port_ssl are present we expect the SSL
+   * version to win. Note that we don't make this conditional on SSL actually being configured.
+   */
+  @Test(groups = ""short"")
+  @CCMConfig(createCcm = false)
+  @Ignore(""Requires SSL support in scassandra"")
+  public void should_extract_hosts_using_native_transport_address_port_ssl_from_peers()
+      throws UnknownHostException {
+
+    InetAddress expectedAddress = InetAddress.getByName(""4.3.2.1"");
+    int expectedPort = 2409;
+    PeerRowState state =
+        PeerRowState.builder()
+            .peers(""native_transport_address"", expectedAddress)
+            .peers(""native_transport_port"", expectedPort - 100)
+            .peers(""native_transport_port_ssl"", expectedPort)
+            .expectedAddress(expectedAddress)
+            .expectedPort(expectedPort)
+            .build();
+    runPeerTest(state);
+  }
+
+  /**
+   * The default case. If we can't get native_address/port out of system.peers_v2 or
+   * native_transport_address/port out of system.peers the fall back to rpc_address + a default port
+   */
+  @Test(groups = ""short"")
+  @CCMConfig(createCcm = false)
+  public void should_extract_hosts_using_rpc_address_from_peers() throws UnknownHostException {
+
+    InetAddress expectedAddress = InetAddress.getByName(""4.3.2.1"");
+    PeerRowState state =
+        PeerRowState.builder()
+            .peers(""rpc_address"", expectedAddress)
+            /* DefaultEndPointFactory isn't happy if we don't have both a value for","[{'comment': ""```suggestion\r\n            /* DefaultEndPointFactory isn't happy if we don't have a value for\r\n```"", 'commenter': 'adutra'}]"
1573,driver-core/src/test/java/com/datastax/driver/core/ControlConnectionTest.java,"@@ -560,6 +571,292 @@ public void should_connect_when_peers_v2_table_not_present() {
     }
   }
 
+  /**
+   * Cassandra 4.0 supports native_address and native_port columns in system.peers_v2. We want to
+   * validate our ability to build correct metadata when drawing data from these tables.
+   */
+  @Test(groups = ""short"")
+  @CCMConfig(createCcm = false)
+  public void should_extract_hosts_using_native_address_port_from_peersv2()
+      throws UnknownHostException {
+
+    InetAddress expectedAddress = InetAddress.getByName(""4.3.2.1"");
+    int expectedPort = 2409;
+    PeerRowState state =
+        PeerRowState.builder()
+            .peersV2(""native_address"", expectedAddress)
+            .peersV2(""native_port"", expectedPort)
+            .expectedAddress(expectedAddress)
+            .expectedPort(expectedPort)
+            .build();
+    runPeerTest(state);
+  }
+
+  /** DSE 6.8 includes native_transport_address and native_transport_port in system.peers. */
+  @Test(groups = ""short"")
+  @CCMConfig(createCcm = false)
+  public void should_extract_hosts_using_native_transport_address_port_from_peers()
+      throws UnknownHostException {
+
+    InetAddress expectedAddress = InetAddress.getByName(""4.3.2.1"");
+    int expectedPort = 2409;
+    PeerRowState state =
+        PeerRowState.builder()
+            .peers(""native_transport_address"", expectedAddress)
+            .peers(""native_transport_port"", expectedPort)
+            .expectedAddress(expectedAddress)
+            .expectedPort(expectedPort)
+            .build();
+    runPeerTest(state);
+  }
+
+  /**
+   * If both native_transport_port and native_transport_port_ssl are present we expect the SSL
+   * version to win. Note that we don't make this conditional on SSL actually being configured.","[{'comment': ""> Note that we don't make this conditional on SSL actually being configured\r\n\r\nAre we sure? I see SSL being tested in a few places with the expression:\r\n\r\n```\r\ncluster.getCluster().getConfiguration().getProtocolOptions().getSSLOptions() != null\r\n```"", 'commenter': 'adutra'}, {'comment': ""Agreed.  I refreshed my recollection after writing that comment and just forgot to go clean it up.  I'll clean this up so that it makes sense."", 'commenter': 'absurdfarce'}]"
1573,driver-core/src/test/java/com/datastax/driver/core/ControlConnectionTest.java,"@@ -560,6 +571,292 @@ public void should_connect_when_peers_v2_table_not_present() {
     }
   }
 
+  /**
+   * Cassandra 4.0 supports native_address and native_port columns in system.peers_v2. We want to
+   * validate our ability to build correct metadata when drawing data from these tables.
+   */
+  @Test(groups = ""short"")
+  @CCMConfig(createCcm = false)
+  public void should_extract_hosts_using_native_address_port_from_peersv2()
+      throws UnknownHostException {
+
+    InetAddress expectedAddress = InetAddress.getByName(""4.3.2.1"");
+    int expectedPort = 2409;
+    PeerRowState state =
+        PeerRowState.builder()
+            .peersV2(""native_address"", expectedAddress)
+            .peersV2(""native_port"", expectedPort)
+            .expectedAddress(expectedAddress)
+            .expectedPort(expectedPort)
+            .build();
+    runPeerTest(state);
+  }
+
+  /** DSE 6.8 includes native_transport_address and native_transport_port in system.peers. */
+  @Test(groups = ""short"")
+  @CCMConfig(createCcm = false)
+  public void should_extract_hosts_using_native_transport_address_port_from_peers()
+      throws UnknownHostException {
+
+    InetAddress expectedAddress = InetAddress.getByName(""4.3.2.1"");
+    int expectedPort = 2409;
+    PeerRowState state =
+        PeerRowState.builder()
+            .peers(""native_transport_address"", expectedAddress)
+            .peers(""native_transport_port"", expectedPort)
+            .expectedAddress(expectedAddress)
+            .expectedPort(expectedPort)
+            .build();
+    runPeerTest(state);
+  }
+
+  /**
+   * If both native_transport_port and native_transport_port_ssl are present we expect the SSL
+   * version to win. Note that we don't make this conditional on SSL actually being configured.
+   */
+  @Test(groups = ""short"")
+  @CCMConfig(createCcm = false)
+  @Ignore(""Requires SSL support in scassandra"")
+  public void should_extract_hosts_using_native_transport_address_port_ssl_from_peers()
+      throws UnknownHostException {
+
+    InetAddress expectedAddress = InetAddress.getByName(""4.3.2.1"");
+    int expectedPort = 2409;
+    PeerRowState state =
+        PeerRowState.builder()
+            .peers(""native_transport_address"", expectedAddress)
+            .peers(""native_transport_port"", expectedPort - 100)
+            .peers(""native_transport_port_ssl"", expectedPort)
+            .expectedAddress(expectedAddress)
+            .expectedPort(expectedPort)
+            .build();
+    runPeerTest(state);
+  }
+
+  /**
+   * The default case. If we can't get native_address/port out of system.peers_v2 or
+   * native_transport_address/port out of system.peers the fall back to rpc_address + a default port
+   */
+  @Test(groups = ""short"")
+  @CCMConfig(createCcm = false)
+  public void should_extract_hosts_using_rpc_address_from_peers() throws UnknownHostException {
+
+    InetAddress expectedAddress = InetAddress.getByName(""4.3.2.1"");
+    PeerRowState state =
+        PeerRowState.builder()
+            .peers(""rpc_address"", expectedAddress)
+            /* DefaultEndPointFactory isn't happy if we don't have both a value for
+             * both peer and rpc_address */
+            .peers(""peer"", InetAddress.getByName(""1.2.3.4""))
+            .expectedAddress(expectedAddress)
+            .build();
+    runPeerTest(state);
+  }
+
+  private void runPeerTest(PeerRowState state) {
+
+    ScassandraCluster scassandras =
+        ScassandraCluster.builder().withNodes(2).withPeersV2(state.usePeersV2()).build();
+    scassandras.init();
+
+    Cluster cluster = null;
+    try {
+
+      scassandras.node(1).primingClient().clearAllPrimes();
+
+      PrimingClient primingClient = scassandras.node(1).primingClient();
+
+      /* Note that we always prime system.local; ControlConnection.refreshNodeAndTokenMap() gets angry
+       * if this is empty */
+      primingClient.prime(
+          PrimingRequest.queryBuilder()
+              .withQuery(""SELECT * FROM system.local WHERE key='local'"")
+              .withThen(then().withColumnTypes(SELECT_LOCAL).withRows(state.getLocalRow()).build())
+              .build());
+
+      if (state.shouldPrimePeers()) {
+
+        primingClient.prime(
+            PrimingRequest.queryBuilder()
+                .withQuery(""SELECT * FROM system.peers"")
+                .withThen(
+                    then()
+                        .withColumnTypes(state.isDse68() ? SELECT_PEERS_DSE68 : SELECT_PEERS)
+                        .withRows(state.getPeersRow())
+                        .build())
+                .build());
+      }
+      if (state.shouldPrimePeersV2()) {
+
+        primingClient.prime(
+            PrimingRequest.queryBuilder()
+                .withQuery(""SELECT * FROM system.peers_v2"")
+                .withThen(
+                    then().withColumnTypes(SELECT_PEERS_V2).withRows(state.getPeersV2Row()).build())
+                .build());
+      } else {
+
+        /* Must return an error code in this case in order to trigger the driver's downgrade to system.peers */
+        primingClient.prime(
+            PrimingRequest.queryBuilder()
+                .withQuery(""SELECT * FROM system.peers_v2"")
+                .withThen(then().withResult(Result.invalid).build()));
+      }
+
+      cluster =
+          Cluster.builder()
+              .addContactPoints(scassandras.address(1).getAddress())
+              .withPort(scassandras.getBinaryPort())
+              .withNettyOptions(nonQuietClusterCloseOptions)
+              .build();
+      cluster.connect();
+
+      Collection<EndPoint> hostEndPoints =
+          Collections2.transform(
+              cluster.getMetadata().allHosts(),
+              new Function<Host, EndPoint>() {
+                public EndPoint apply(Host host) {
+                  return host.getEndPoint();
+                }
+              });
+      assertThat(hostEndPoints).contains(state.getExpectedEndPoint(scassandras));
+    } finally {
+      if (cluster != null) cluster.close();
+      scassandras.stop();
+    }
+  }
+
+  static class PeerRowState {
+
+    public static final int PEERS_FLAG = 0;
+    public static final int PEERS_V2_FLAG = 1;
+
+    private final ImmutableMap<String, Object> peers;
+    private final ImmutableMap<String, Object> peersV2;
+    private final ImmutableMap<String, Object> local;
+
+    private final InetAddress expectedAddress;
+    private final Optional<Integer> expectedPort;
+
+    private final BitSet shouldPrime;
+
+    private PeerRowState(
+        ImmutableMap<String, Object> peers,
+        ImmutableMap<String, Object> peersV2,
+        ImmutableMap<String, Object> local,
+        InetAddress expectedAddress,
+        Optional<Integer> expectedPort,
+        BitSet shouldPrime) {
+      this.peers = peers;
+      this.peersV2 = peersV2;
+      this.local = local;
+
+      this.expectedAddress = expectedAddress;
+      this.expectedPort = expectedPort;
+
+      this.shouldPrime = shouldPrime;
+    }
+
+    public static Builder builder() {
+      return new Builder();
+    }
+
+    public boolean usePeersV2() {
+      return !this.peersV2.isEmpty();
+    }
+
+    public boolean isDse68() {
+      return this.peers.containsKey(""native_transport_address"")
+          || this.peers.containsKey(""native_transport_port"")
+          || this.peers.containsKey(""native_transport_port_ssl"");
+    }
+
+    public boolean shouldPrimePeers() {
+      return this.shouldPrime.get(PEERS_FLAG);
+    }
+
+    public boolean shouldPrimePeersV2() {
+      return this.shouldPrime.get(PEERS_V2_FLAG);
+    }
+
+    public ImmutableMap<String, Object> getPeersRow() {
+      return this.peers;
+    }
+
+    public ImmutableMap<String, Object> getPeersV2Row() {
+      return this.peersV2;
+    }
+
+    public ImmutableMap<String, Object> getLocalRow() {
+      return this.local;
+    }
+
+    public EndPoint getExpectedEndPoint(ScassandraCluster cluster) {
+      return new TranslatedAddressEndPoint(
+          new InetSocketAddress(
+              this.expectedAddress, this.expectedPort.or(cluster.getBinaryPort())));
+    }
+
+    static class Builder {
+
+      private ImmutableMap.Builder<String, Object> peers = this.basePeerRow();
+      private ImmutableMap.Builder<String, Object> peersV2 = this.basePeerRow();
+      private ImmutableMap.Builder<String, Object> local = this.basePeerRow();
+
+      private InetAddress expectedAddress;
+      private Optional<Integer> expectedPort = Optional.absent();
+
+      private BitSet shouldPrime = new BitSet(3);","[{'comment': 'What is the third bit used for? If only 2 are required, maybe you could simplify and use 2 booleans?', 'commenter': 'adutra'}, {'comment': ""This is also a holdover that prolly doesn't make sense anymore.  The third bit originally referred to system.local, but as discussed above we have to _always_ prime system.local now.\r\n\r\nIt also probably does make sense to just switch to booleans now.  With three vals it seemed worth encapsulating everything in a single data structure... but by the time you add in static flags for each name the benefit is somewhat lost."", 'commenter': 'absurdfarce'}]"
1580,core-shaded/pom.xml,"@@ -311,6 +300,11 @@
                   <artifactId>jctools-core</artifactId>
                   <version>2.1.2</version>
                 </additionalDependency>
+                <additionalDependency>
+                  <groupId>com.esri.geometry</groupId>
+                  <artifactId>esri-geometry-api</artifactId>
+                  <version>1.2.1</version>
+                </additionalDependency>","[{'comment': 'javadoc generation was throwing warnings about missing classes without this declaration in place', 'commenter': 'absurdfarce'}]"
1580,manual/core/integration/README.md,"@@ -479,8 +479,9 @@ don't use any of the above features, you can safely exclude the dependency:
 Our [geospatial types](../dse/geotypes/) implementation is based on the [Esri Geometry
 API](https://github.com/Esri/geometry-api-java).
 
-Esri is declared as a required dependency, but the driver can operate normally without it. If you
-don't use geospatial types anywhere in your application, you can exclude the dependency:
+For driver versions >= 4.4.0 and < 4.14.0 Esri is declared as a required dependency,","[{'comment': ""The TinkerPop example explicitly referred to an inclusive range, but in this case I wanted the range to be inclusive on the low end but exclusive on the high end (in order to explicitly convey the idea that all drivers before the named version used a different behaviour).  Since this changed the semantics slightly I thought that perhaps identifying versions via the >= and < syntax might be more explicit.\r\n\r\nI'm open to other ideas of presenting this information if this isn't clear."", 'commenter': 'absurdfarce'}]"
1580,upgrade_guide/README.md,"@@ -9,6 +9,26 @@ request cannot be executed because all nodes tried were busy. Previously you wou
 `NoNodeAvailableException` but you will now get back an `AllNodesFailedException` where the
 `getAllErrors` map contains a `NodeUnavailableException` for that node.
 
+#### Esri Geometry dependency now optional
+
+Previous versions of the Java driver defined a mandatory dependency on the Esri geometry library.
+This library offered support for primitive geometric types supported by DSE.  As of driver 4.14.0
+this dependency is now optional.
+
+If you do not use DSE (or if you do but do not use the support for geometric types within DSE) you
+should experience no disruption.  If you are using geometric types with DSE you'll now need to
+explicitly declare a dependency on the Esri library:
+
+```xml
+<dependency>
+  <groupId>com.esri.geometry</groupId>
+  <artifactId>esri-geometry-api</artifactId>
+  <version>${esri.version}</version>
+</dependency>
+```
+
+See the [integration](../manual/core/integration/#esri) section in the manual for more details.
+","[{'comment': ""Very much based on the example used when TinkerPop was made optional.\r\n\r\nI've explicitly added this in the 4.14.0 section under the assumption that changing a dependency from required to optional breaks backwards compat and thus requires at least a minor release."", 'commenter': 'absurdfarce'}]"
1580,core-shaded/pom.xml,"@@ -146,7 +146,6 @@
                   -->
                   <include>com.datastax.oss:java-driver-core</include>
                   <include>io.netty:*</include>
-                  <include>com.esri.geometry:*</include>","[{'comment': ""I'm a bit hesitant about this. Making a dependency optional and shading it are different concerns.\r\n\r\nInitially, the decision to shade a dependency was taken by looking at the likelihood of a version conflict for a given dependency. For Esri, we deemed it high because the driver requires a very old Esri version. \r\n\r\nI think we should keep shading Esri even if it is optional, wdyt? This means that in the shaded jar Esri would be present and shaded, just like it is now. I can't see a situation where this could be a problem for the end user."", 'commenter': 'adutra'}, {'comment': ""I see your point here... I'm just not sure I agree.\r\n\r\nTo be clear, the case you're describing is fairly precise.  It would have to be something like the following:\r\n\r\n* The user's app uses ESRI 2.x for some other functionality in their app\r\n* The user's app also uses the (shaded) Java driver\r\n* The user runs this app against DSE\r\n* The user wants to leverage the geometric type support in DSE\r\n\r\nThis user will (possibly, even likely) experience a class conflict.  But the cost for supporting this (quite limited case) is that _every user of the shaded JAR_ has a larger JAR on their classpath.  Initially it seemed like optimizing for the broad case made sense rather than doing so for the extremely precise case but I'm certainly open to argument."", 'commenter': 'absurdfarce'}, {'comment': ""> every user of the shaded JAR has a larger JAR on their classpath\r\n\r\nWell the shaded jar already has all of Netty in it, so adding Esri on top of it won't really make a noticeable size difference :-)\r\n\r\nBut OK, I don't mind if we stop shading Esri."", 'commenter': 'adutra'}]"
1580,manual/core/integration/README.md,"@@ -496,6 +497,18 @@ don't use geospatial types anywhere in your application, you can exclude the dep
 </dependency>
 ```
 
+Starting with driver 4.14.0 Esri has been changed to an optional dependency.  You no longer have to","[{'comment': ""Now that we are doing this for Esri, I wonder why we wouldn't do the same for Jackson. Do you mind opening a ticket for that, if it doesn't exist yet?"", 'commenter': 'adutra'}, {'comment': ""This isn't quite as clear cut for me.  Jackson is directly used at several spots within the driver:\r\n\r\n* Insights code\r\n* Graph code\r\n* Cloud config code\r\n* JsonCodec\r\n\r\nThe first is a no-op now that Insights is defunct; if anything I'd argue the driving code on our side should be removed, but that's a distraction from our current conversation.  The second usage above fits nicely into the model we're establishing here: dependencies of DSE features should be made optional.  If we stopped there I'd be more inclined to agree.\r\n\r\nIt's the last two that give me pause.  I don't know that we want to require users to manually add a dep when using the driver against Astra.  I'd make a similar argument for JsonCodec, although I readily agree it's less significant than the Astra use case.\r\n\r\nWorth noting: the user who created the original PR to make ESRI optional did so because of the old Jackson version that came along with it.  Just making ESRI optional resolves that by using the more recent version we explicitly include.\r\n\r\nWhaddya think?"", 'commenter': 'absurdfarce'}, {'comment': 'JsonCodec is optional, the driver doesn\'t register it by default, so I still think it falls under the ""could-be-optional"" category. Insights and Graph too.\r\n\r\nIt\'s only Astra that bugs me. \r\n\r\nAfter some more thought, OK to keep it mandatory. I\'d make a case for creating a ticket to remove the dependency to Jackson in `CloudConfigFactory` first, and only then make Jackson optional.\r\n\r\n> the user who created the original PR to make ESRI optional did so because of the old Jackson version that came along with it.\r\n\r\nIndeed, Esri comes with two annoying dependencies because they are old and popular at the same time:\r\n\r\n* org.json:json:20210307\r\n* org.codehaus.jackson:jackson-core-asl:1.9.12\r\n\r\n2 additional questions:\r\n\r\n* Are we sure that the above dependencies are now absent from the shaded jar as well?\r\n* Have you seen this? https://github.com/spring-projects/spring-boot/issues/29387#issuecomment-1015332594 In short: `org.json:json-api` was promoted to direct dependency for obscure OSGi-related reasons. Please check what\'s the current status of this dependency in the core pom.xml, in the shaded jar and in the OSGi tests.', 'commenter': 'adutra'}, {'comment': ""I naively thought that these would be removed by my earlier changes.  As I look back on it I'm not sure why I thought that but I did. :(  Regardless, this should be fixed now in the most recent push.\r\n\r\nI'll look into the spring-boot issue as well."", 'commenter': 'absurdfarce'}, {'comment': ""I naively thought that these would be removed by my earlier changes.  As I look back on it I'm not sure why I thought that but I did. :(  Regardless, this should be fixed now in the most recent push.\r\n\r\nI'll look into the spring-boot issue as well."", 'commenter': 'absurdfarce'}, {'comment': ""I can also confirm that making ESRI optional has the effect of making it's transitive dependencies optional.  That means that org.json:json is now an optional dependency for core and core-shaded.  I'll add a comment onto that Spring ticket confirming that fact."", 'commenter': 'absurdfarce'}]"
1580,manual/core/integration/README.md,"@@ -496,6 +497,18 @@ don't use geospatial types anywhere in your application, you can exclude the dep
 </dependency>
 ```
 
+Starting with driver 4.14.0 Esri has been changed to an optional dependency.  You no longer have to
+explicitly exclude the dependency if it's not used, but if you do wish to make use of the Esri
+library you must now explicitly specify it as a dependency :
+
+```xml
+<dependency>
+  <groupId>com.esri.geometry</groupId>
+  <artifactId>esri-geometry-api</artifactId>
+  <version>${esri.version}</version>","[{'comment': ""We could add a note here about the fact that the driver requires Esri 1.2.x, wdyt? This could be useful for developers since it's such an old version."", 'commenter': 'adutra'}, {'comment': ""I completely agree with this.  My original version of these docs spelled out the version of ESRI to include but I shifted over the the prop-based model in use here to avoid customer trying to use a newer version.  That said, I readily agree that an additional warning here about the older version would be worthwhile... I'll add that in."", 'commenter': 'absurdfarce'}, {'comment': 'Added the trailing note here re: the use of an older ESRI version.  Does that address your concerns here?', 'commenter': 'absurdfarce'}, {'comment': 'We are good thanks!', 'commenter': 'adutra'}]"
1580,upgrade_guide/README.md,"@@ -9,6 +9,26 @@ request cannot be executed because all nodes tried were busy. Previously you wou
 `NoNodeAvailableException` but you will now get back an `AllNodesFailedException` where the
 `getAllErrors` map contains a `NodeUnavailableException` for that node.
 
+#### Esri Geometry dependency now optional","[{'comment': ""Now that we are doing this for Esri, I wonder why we wouldn't do the same for Jackson and Reactive Streams. Indeed, these 2 dependencies are handled the same way as Esri used to be handled: declared as mandatory in pom.xml, but made optional by the magic of reflection. \r\n\r\nDo you mind opening a ticket for that, if it doesn't exist yet?"", 'commenter': 'adutra'}, {'comment': ""See above re: my hesitation about doing so with Jackson but the reactive streams dependency would seem to make a lot of sense on this front.  I'll file a ticket for that."", 'commenter': 'absurdfarce'}, {'comment': 'Ticket is https://datastax-oss.atlassian.net/browse/JAVA-2981', 'commenter': 'absurdfarce'}]"
1580,manual/core/integration/README.md,"@@ -496,6 +497,22 @@ don't use geospatial types anywhere in your application, you can exclude the dep
 </dependency>
 ```
 
+Starting with driver 4.14.0 Esri has been changed to an optional dependency.  You no longer have to
+explicitly exclude the dependency if it's not used, but if you do wish to make use of the Esri
+library you must now explicitly specify it as a dependency :
+
+```xml
+<dependency>
+  <groupId>com.esri.geometry</groupId>
+  <artifactId>esri-geometry-api</artifactId>
+  <version>${esri.version}</version>
+</dependency>
+```
+
+The dependency specification above will introduce a dependency on version 1.2.1 of the Esri","[{'comment': 'Small suggestion to better link this to the above snippet:\r\n\r\n> In the dependency specification above, `esri.version` should be any version in the range `[1.2,2.0)`, for example 1.2.1. While this version is now old, it is the only version that is guaranteed to be fully compatible with DSE.', 'commenter': 'adutra'}, {'comment': ""Oh, I like that _much_ better.  I tried to re-phrase what I had a couple of times but I never really liked what I came up with... I'll switch to your language instead."", 'commenter': 'absurdfarce'}, {'comment': ""I basically used the language you provided with a (hopefully small and non-obtrusive) tweak to avoid specifying the version range with the [) syntax; I was worried it might be confusing to some users.  Take a look and let me know if you're okay with that usage."", 'commenter': 'absurdfarce'}, {'comment': ""That's excellent, thanks!"", 'commenter': 'adutra'}]"
1580,core-shaded/pom.xml,"@@ -340,7 +312,7 @@
                   <!--
                   1) Don't import packages shaded in the driver bundle. Note that shaded-guava lives
                   in its own bundle, so we must explicitly *not* mention it here.
-                  -->!com.datastax.oss.driver.shaded.netty.*, !com.datastax.oss.driver.shaded.esri.*, !com.datastax.oss.driver.shaded.json.*, !com.datastax.oss.driver.shaded.codehaus.jackson.*, !com.datastax.oss.driver.shaded.fasterxml.jackson.*,
+                  -->!com.datastax.oss.driver.shaded.netty.*, !com.datastax.oss.driver.shaded.json.*, !com.datastax.oss.driver.shaded.codehaus.jackson.*, !com.datastax.oss.driver.shaded.fasterxml.jackson.*,","[{'comment': 'I just noticed: we are still mentioning here two shaded packages that do not exist anymore:\r\n\r\n* com.datastax.oss.driver.shaded.json\r\n* com.datastax.oss.driver.shaded.codehaus.jackson', 'commenter': 'adutra'}, {'comment': ""Ah, good catch, you're absolutely right!  I'll clean this up (and the one below) shortly."", 'commenter': 'absurdfarce'}]"
1580,core-shaded/pom.xml,"@@ -366,7 +338,7 @@
                 1) The driver's packages (API and internal);
                 2) All shaded packages, except Guava which resides in a separate bundle.
                 -->
-                <Export-Package>com.datastax.oss.driver.api.core.*, com.datastax.oss.driver.internal.core.*, com.datastax.dse.driver.api.core.*, com.datastax.dse.driver.internal.core.*, com.datastax.oss.driver.shaded.netty.*, com.datastax.oss.driver.shaded.esri.*, com.datastax.oss.driver.shaded.json.*, com.datastax.oss.driver.shaded.codehaus.jackson.*, com.datastax.oss.driver.shaded.fasterxml.jackson.*,</Export-Package>
+                <Export-Package>com.datastax.oss.driver.api.core.*, com.datastax.oss.driver.internal.core.*, com.datastax.dse.driver.api.core.*, com.datastax.dse.driver.internal.core.*, com.datastax.oss.driver.shaded.netty.*, com.datastax.oss.driver.shaded.json.*, com.datastax.oss.driver.shaded.codehaus.jackson.*, com.datastax.oss.driver.shaded.fasterxml.jackson.*,</Export-Package>","[{'comment': 'Same here.', 'commenter': 'adutra'}]"
1586,bom/pom.xml,"@@ -71,7 +71,7 @@
       <dependency>
         <groupId>com.datastax.oss</groupId>
         <artifactId>native-protocol</artifactId>
-        <version>1.5.0</version>
+        <version>1.5.1-SNAPSHOT</version>","[{'comment': 'Just a placeholder for now to use a locally-built version of native-protocol that includes https://github.com/datastax/native-protocol/pull/42.  To be replaced by an actual version once we release a new version of native-protocol.', 'commenter': 'absurdfarce'}, {'comment': 'native-protocol 1.5.1 has now been released', 'commenter': 'absurdfarce'}]"
1586,core/src/main/java/com/datastax/oss/driver/api/core/servererrors/CASWriteUnknownException.java,"@@ -0,0 +1,84 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core.servererrors;
+
+import com.datastax.oss.driver.api.core.AllNodesFailedException;
+import com.datastax.oss.driver.api.core.ConsistencyLevel;
+import com.datastax.oss.driver.api.core.DriverException;
+import com.datastax.oss.driver.api.core.cql.ExecutionInfo;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import com.datastax.oss.driver.api.core.retry.RetryPolicy;
+import com.datastax.oss.driver.api.core.session.Request;
+import edu.umd.cs.findbugs.annotations.NonNull;
+
+/**
+ * The result of a CAS operation is in an unknown state
+ *
+ * <p>This exception is processed by {@link RetryPolicy#onReadTimeoutVerdict(Request,","[{'comment': 'This is not accurate, it is processed by `onErrorResponseVerdict`.', 'commenter': 'adutra'}, {'comment': 'Ah, another good catch... and another copy/paste error.  Fixing.', 'commenter': 'absurdfarce'}]"
1586,core/src/main/java/com/datastax/oss/driver/api/core/servererrors/CASWriteUnknownException.java,"@@ -0,0 +1,84 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core.servererrors;
+
+import com.datastax.oss.driver.api.core.AllNodesFailedException;
+import com.datastax.oss.driver.api.core.ConsistencyLevel;
+import com.datastax.oss.driver.api.core.DriverException;
+import com.datastax.oss.driver.api.core.cql.ExecutionInfo;
+import com.datastax.oss.driver.api.core.metadata.Node;
+import com.datastax.oss.driver.api.core.retry.RetryPolicy;
+import com.datastax.oss.driver.api.core.session.Request;
+import edu.umd.cs.findbugs.annotations.NonNull;
+
+/**
+ * The result of a CAS operation is in an unknown state","[{'comment': '```suggestion\r\n * The result of a CAS operation is in an unknown state.\r\n```', 'commenter': 'adutra'}]"
1597,core/src/main/java/com/datastax/oss/driver/internal/core/addresstranslation/PrivateLinkAddressTranslator.java,"@@ -0,0 +1,50 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.addresstranslation;
+
+import com.datastax.oss.driver.api.core.addresstranslation.AddressTranslator;
+import com.datastax.oss.driver.api.core.config.DriverExecutionProfile;
+import com.datastax.oss.driver.api.core.config.DriverOption;
+import com.datastax.oss.driver.api.core.context.DriverContext;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import java.net.InetSocketAddress;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class PrivateLinkAddressTranslator implements AddressTranslator {
+
+  private static final Logger LOG = LoggerFactory.getLogger(PrivateLinkAddressTranslator.class);
+
+  public static final DriverOption ADVERTISED_HOSTNAME_OPTION =
+      () -> ""advanced.address-translator.advertised-hostname"";
+  private final String advertisedHostname;
+
+  public PrivateLinkAddressTranslator(@NonNull DriverContext context) {
+    DriverExecutionProfile config = context.getConfig().getDefaultProfile();
+    advertisedHostname = config.getString(ADVERTISED_HOSTNAME_OPTION);
+  }
+
+  @NonNull
+  @Override
+  public InetSocketAddress translate(@NonNull InetSocketAddress address) {
+    final int port = address.getPort();
+    LOG.debug(String.format(""translating %s:%d to %s:%d"", address, port, advertisedHostname, port));","[{'comment': 'You should avoid `String.format`:\r\n```suggestion\r\n    LOG.debug(""Translating {}:{} to {}:{}"", address, port, advertisedHostname, port));\r\n```', 'commenter': 'adutra'}]"
1597,core/src/main/java/com/datastax/oss/driver/internal/core/addresstranslation/PrivateLinkAddressTranslator.java,"@@ -0,0 +1,50 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.addresstranslation;
+
+import com.datastax.oss.driver.api.core.addresstranslation.AddressTranslator;
+import com.datastax.oss.driver.api.core.config.DriverExecutionProfile;
+import com.datastax.oss.driver.api.core.config.DriverOption;
+import com.datastax.oss.driver.api.core.context.DriverContext;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import java.net.InetSocketAddress;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class PrivateLinkAddressTranslator implements AddressTranslator {
+
+  private static final Logger LOG = LoggerFactory.getLogger(PrivateLinkAddressTranslator.class);
+
+  public static final DriverOption ADVERTISED_HOSTNAME_OPTION =","[{'comment': 'Since this is going to be part of the core driver, you should rather add this new option to `DefaultDriverOption`.\r\n\r\nThen you should also declare a new `TypedDriverOption` so that folks using programmatic configuration can use the new option too.', 'commenter': 'adutra'}]"
1597,core/src/main/java/com/datastax/oss/driver/internal/core/addresstranslation/PrivateLinkAddressTranslator.java,"@@ -0,0 +1,50 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.addresstranslation;
+
+import com.datastax.oss.driver.api.core.addresstranslation.AddressTranslator;
+import com.datastax.oss.driver.api.core.config.DriverExecutionProfile;
+import com.datastax.oss.driver.api.core.config.DriverOption;
+import com.datastax.oss.driver.api.core.context.DriverContext;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import java.net.InetSocketAddress;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class PrivateLinkAddressTranslator implements AddressTranslator {","[{'comment': 'As discussed in the Jira ticket, you may want to give this a more generic name. Also, a bit of javadoc explaining why it is useful would be nice.', 'commenter': 'adutra'}]"
1639,core/src/test/java/com/datastax/oss/driver/internal/core/type/codec/CqlVectorCodecTest.java,"@@ -0,0 +1,110 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.type.codec;
+
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.assertj.core.api.Assertions.assertThatThrownBy;
+
+import com.datastax.oss.driver.api.core.data.CqlVector;
+import com.datastax.oss.driver.api.core.type.CqlVectorType;
+import com.datastax.oss.driver.api.core.type.DataTypes;
+import com.datastax.oss.driver.api.core.type.codec.TypeCodecs;
+import com.datastax.oss.driver.api.core.type.reflect.GenericType;
+import org.junit.Test;
+
+public class CqlVectorCodecTest extends CodecTestBase<CqlVector> {","[{'comment': '#nit - should this also check to make sure `null` cannot be accepted [per these discussions](https://lists.apache.org/list?dev@cassandra.apache.org:lte=1M:vector)?', 'commenter': 'msmygit'}, {'comment': 'We have checks around the [encode](https://github.com/datastax/java-driver/blob/cep-vsearch/core/src/test/java/com/datastax/oss/driver/internal/core/type/codec/CqlVectorCodecTest.java#L43) and [decode](https://github.com/datastax/java-driver/blob/cep-vsearch/core/src/test/java/com/datastax/oss/driver/internal/core/type/codec/CqlVectorCodecTest.java#L50) functions for null values.  Were you thinking of something else here?', 'commenter': 'absurdfarce'}]"
1639,core/src/main/java/com/datastax/oss/driver/internal/core/type/codec/CqlVectorCodec.java,"@@ -0,0 +1,105 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.internal.core.type.codec;
+
+import com.datastax.oss.driver.api.core.ProtocolVersion;
+import com.datastax.oss.driver.api.core.data.CqlVector;
+import com.datastax.oss.driver.api.core.type.CqlVectorType;
+import com.datastax.oss.driver.api.core.type.DataType;
+import com.datastax.oss.driver.api.core.type.codec.TypeCodec;
+import com.datastax.oss.driver.api.core.type.reflect.GenericType;
+import com.datastax.oss.driver.shaded.guava.common.base.Preconditions;
+import com.datastax.oss.driver.shaded.guava.common.base.Splitter;
+import com.datastax.oss.driver.shaded.guava.common.collect.Iterables;
+import edu.umd.cs.findbugs.annotations.NonNull;
+import edu.umd.cs.findbugs.annotations.Nullable;
+import java.nio.ByteBuffer;
+import java.util.Arrays;
+
+public class CqlVectorCodec implements TypeCodec<CqlVector> {
+
+  @NonNull
+  @Override
+  public GenericType<CqlVector> getJavaType() {
+    return GenericType.of(CqlVector.class);
+  }
+
+  /* Since we've overridden accepts() this shouldn't ever actually be used */
+  @NonNull
+  @Override
+  public DataType getCqlType() {
+    return new CqlVectorType(0);
+  }
+
+  @NonNull
+  @Override
+  public boolean accepts(@NonNull DataType cqlType) {
+    Preconditions.checkNotNull(cqlType);
+    return cqlType.getClass().equals(CqlVectorType.class);
+  }
+
+  @Nullable
+  @Override
+  public ByteBuffer encode(@Nullable CqlVector value, @NonNull ProtocolVersion protocolVersion) {
+    if (value == null) {
+      return null;
+    }
+    float[] values = value.getValues();
+    ByteBuffer bytes = ByteBuffer.allocate(4 * values.length);
+    for (int i = 0; i < values.length; ++i) bytes.putFloat(values[i]);
+    bytes.rewind();
+    return bytes;
+  }
+
+  @Nullable
+  @Override
+  public CqlVector decode(@Nullable ByteBuffer bytes, @NonNull ProtocolVersion protocolVersion) {
+    if (bytes == null || bytes.remaining() == 0) {
+      return null;
+    }
+    int length = bytes.limit();
+    if (length % 4 != 0)
+      throw new IllegalArgumentException(""Expected CqlVector to consist of a multiple of 4 bytes"");
+    int valuesCnt = length / 4;
+    float[] values = new float[valuesCnt];
+    for (int i = 0; i < valuesCnt; ++i) {
+      values[i] = bytes.getFloat();
+    }
+    /* Restore the input ByteBuffer to its original state */
+    bytes.rewind();
+    return new CqlVector(values);
+  }","[{'comment': ""Note that encode/decode above represent something of a compromise.  There's no correlation between the number of floats we try to read or write and the number that are actually expected according to the type.  We do extract the expected number of dimensions for a given vector and we do store it in a CqlVectorType instance but that information isn't available to the encode/decode impls above.  This isn't a huge surprise; in the past parameterized types have been used exclusively to indicate subtypes, not meta-information about a type.\r\n\r\nThe really robust answer is probably to expand this API so that the underlying type is provided to encode() and decode().  In most cases that type will be ignored, but in this case we'd use it to see how many floats we should expect to read and write.  In the interest of time I avoided this larger-scale refactoring and settled on the compromise above.  I'm open to a follow-up ticket which would do things the right way."", 'commenter': 'absurdfarce'}, {'comment': 'The above statement is no longer true with the changes made to support CEP-7 syntax.', 'commenter': 'absurdfarce'}]"
1639,core/src/main/java/com/datastax/oss/driver/api/core/type/DataTypes.java,"@@ -51,13 +53,24 @@ public class DataTypes {
   public static final DataType TINYINT = new PrimitiveType(ProtocolConstants.DataType.TINYINT);
   public static final DataType DURATION = new PrimitiveType(ProtocolConstants.DataType.DURATION);
 
+  /* Vector types are currently expressed as a custom type but the class name in that custom type
+   * if followed by the number of dimensions for this specific type of vector enclosed by parentheses.
+   * A vector with M dimensions is of a different type than a vector with N dimensions (M != N) so
+   * we include the number of dimensions as part of the type.  */
+  private static final Pattern VECTOR_PATTERN =
+      Pattern.compile(CqlVectorType.CQLVECTOR_CLASS_NAME + ""(\\d+)"");","[{'comment': ""I don't _love_ the idea of a regex compare here but it's not as bad as it appears at first blush.  We're only doing this for custom types so hopefully it should be a pretty rare event even for users of the vector type.  More importantly, most of the regex in question is a long literal string so eval'ing it as a regex shouldn't be vastly less performant than a base string compare.  Since it gives us the ability to extract the dimensions easily it seemed like a reasonable compromise, although we certainly can implement the extraction logic manually if need be."", 'commenter': 'absurdfarce'}, {'comment': ""The regex is no longer used; we've addressed this issue via basic string ops."", 'commenter': 'absurdfarce'}]"
1639,core/src/main/java/com/datastax/oss/driver/api/core/data/CqlVector.java,"@@ -0,0 +1,91 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core.data;
+
+import com.datastax.oss.driver.shaded.guava.common.base.Joiner;
+import com.datastax.oss.driver.shaded.guava.common.collect.ImmutableList;
+import com.datastax.oss.driver.shaded.guava.common.collect.Iterators;
+import java.util.Arrays;
+
+/** An n-dimensional vector defined in CQL */
+public class CqlVector<T> {","[{'comment': ""I'd like to make this a simple value type (immutable, final, ...) and javadoc it as such."", 'commenter': 'jshook'}]"
1639,core/src/main/java/com/datastax/oss/driver/api/core/data/CqlVector.java,"@@ -0,0 +1,91 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core.data;
+
+import com.datastax.oss.driver.shaded.guava.common.base.Joiner;
+import com.datastax.oss.driver.shaded.guava.common.collect.ImmutableList;
+import com.datastax.oss.driver.shaded.guava.common.collect.Iterators;
+import java.util.Arrays;
+
+/** An n-dimensional vector defined in CQL */
+public class CqlVector<T> {
+
+  private final ImmutableList<T> values;","[{'comment': ""There is no strong reason I can see here for hoisting Guava in, since immutability can be offered with standard lib conventions in practice. (You're not going to get through a dev cycle treating it differently)"", 'commenter': 'jshook'}]"
1639,core/src/main/java/com/datastax/oss/driver/api/core/data/CqlVector.java,"@@ -0,0 +1,91 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core.data;
+
+import com.datastax.oss.driver.shaded.guava.common.base.Joiner;
+import com.datastax.oss.driver.shaded.guava.common.collect.ImmutableList;
+import com.datastax.oss.driver.shaded.guava.common.collect.Iterators;
+import java.util.Arrays;
+
+/** An n-dimensional vector defined in CQL */
+public class CqlVector<T> {
+
+  private final ImmutableList<T> values;
+
+  private CqlVector(ImmutableList<T> values) {","[{'comment': 'Instead of this, why not\r\n```\r\npublic CqlVector(List<T> values) { ...\r\n```\r\nand\r\n```\r\npublic CqlVector(float... components) { ...\r\n```\r\n[update: I realized the varargs signatures were distinct enough and changed this ^^]\r\nNot sure if can easily combine varargs and type params... ', 'commenter': 'jshook'}]"
1639,core/src/main/java/com/datastax/oss/driver/api/core/data/CqlVector.java,"@@ -0,0 +1,91 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core.data;
+
+import com.datastax.oss.driver.shaded.guava.common.base.Joiner;
+import com.datastax.oss.driver.shaded.guava.common.collect.ImmutableList;
+import com.datastax.oss.driver.shaded.guava.common.collect.Iterators;
+import java.util.Arrays;
+
+/** An n-dimensional vector defined in CQL */
+public class CqlVector<T> {
+
+  private final ImmutableList<T> values;
+
+  private CqlVector(ImmutableList<T> values) {
+    this.values = values;
+  }
+
+  public static Builder builder() {","[{'comment': ""Requiring the builder here for such a simple container type causes contorted patterns in the caller's code. Without requiring this, usage patterns can be way easier."", 'commenter': 'jshook'}]"
1639,core/src/main/java/com/datastax/oss/driver/api/core/data/CqlVector.java,"@@ -0,0 +1,91 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core.data;
+
+import com.datastax.oss.driver.shaded.guava.common.base.Joiner;
+import com.datastax.oss.driver.shaded.guava.common.collect.ImmutableList;
+import com.datastax.oss.driver.shaded.guava.common.collect.Iterators;
+import java.util.Arrays;
+
+/** An n-dimensional vector defined in CQL */
+public class CqlVector<T> {
+
+  private final ImmutableList<T> values;
+
+  private CqlVector(ImmutableList<T> values) {
+    this.values = values;
+  }
+
+  public static Builder builder() {
+    return new Builder();
+  }
+
+  public Iterable<T> getValues() {","[{'comment': 'This could be implemented on the value type directly for simplicity, as\r\n```\r\npublic class CqlVector<T> implements Iterable<T> {\r\n```', 'commenter': 'jshook'}]"
1639,core/src/main/java/com/datastax/oss/driver/api/core/data/CqlVector.java,"@@ -0,0 +1,91 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core.data;
+
+import com.datastax.oss.driver.shaded.guava.common.base.Joiner;
+import com.datastax.oss.driver.shaded.guava.common.collect.ImmutableList;
+import com.datastax.oss.driver.shaded.guava.common.collect.Iterators;
+import java.util.Arrays;
+
+/** An n-dimensional vector defined in CQL */
+public class CqlVector<T> {
+
+  private final ImmutableList<T> values;
+
+  private CqlVector(ImmutableList<T> values) {
+    this.values = values;
+  }
+
+  public static Builder builder() {
+    return new Builder();
+  }
+
+  public Iterable<T> getValues() {
+    return values;","[{'comment': 'Requiring the use of the iterator pattern here for simple collections types is also making the user write more code that is unnecessary. There should be a simple collection type that can be accessed without all the machinations.', 'commenter': 'jshook'}]"
1639,core/src/main/java/com/datastax/oss/driver/api/core/data/CqlVector.java,"@@ -0,0 +1,91 @@
+/*
+ * Copyright DataStax, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.datastax.oss.driver.api.core.data;
+
+import com.datastax.oss.driver.shaded.guava.common.base.Joiner;
+import com.datastax.oss.driver.shaded.guava.common.collect.ImmutableList;
+import com.datastax.oss.driver.shaded.guava.common.collect.Iterators;
+import java.util.Arrays;
+
+/** An n-dimensional vector defined in CQL */","[{'comment': 'For the server type ""n"" is part of the type definition.  Should it be on the client as well?  Vector<float, 2> and Vector<float, 3> are different types.\r\nYou need to ensure that someone is not trying to bind a Vector<float, 3> into a Vector<float, 2>', 'commenter': 'JeremiahDJordan'}, {'comment': ""This is only the value type.  CqlVectorType actually defines the specific type of this vector, and there we include both subtype and dimension.  Also note that this is the type returned by CqlVectorCodec.getCqlType(), which in turn means that it's used as the comparison to determine whether or not a given DataType can be handled by the codec.  As a consequence the codec will reject any type which doesn't match it specifically.  You can see this tested in CqlVectorCodecTest.should_accept_vector_type_correct_dimension_only()."", 'commenter': 'absurdfarce'}]"
1641,core/src/main/java/com/datastax/oss/driver/api/core/data/CqlVector.java,"@@ -56,36 +62,47 @@ public int hashCode() {
 
   @Override
   public String toString() {
+    StringBuilder builder = new StringBuilder(""CqlVector{"");
+    for (T value : values) {
+      builder.append(value).append("", "");
+    }
+    builder.setLength(builder.length() - "", "".length());
+    return builder.toString();","[{'comment': 'missing ""}"", I\'ll push an update with a simple test', 'commenter': 'jshook'}, {'comment': 'fixed and covered', 'commenter': 'jshook'}]"
1641,core/src/main/java/com/datastax/oss/driver/api/core/data/CqlVector.java,"@@ -15,25 +15,31 @@
  */
 package com.datastax.oss.driver.api.core.data;
 
-import com.datastax.oss.driver.shaded.guava.common.base.Joiner;
-import com.datastax.oss.driver.shaded.guava.common.collect.ImmutableList;
-import com.datastax.oss.driver.shaded.guava.common.collect.Iterators;
-import java.util.Arrays;
+import java.util.*;","[{'comment': 'nit; consider specific types', 'commenter': 'jeffbanks'}]"
1641,core/src/main/java/com/datastax/oss/driver/api/core/data/CqlVector.java,"@@ -56,36 +62,48 @@ public int hashCode() {
 
   @Override
   public String toString() {
+    StringBuilder builder = new StringBuilder(""CqlVector{"");
+    for (T value : values) {
+      builder.append(value).append("", "");","[{'comment': 'nit: Consider `String.join()` for the `values`? This will prevent having to strip-off the trailing `, `', 'commenter': 'ShaunakDas88'}, {'comment': 'I tried this, but the generic parameter broke it, as there is no bound on T which makes it a CharSequence. ', 'commenter': 'jshook'}]"
1641,core/src/main/java/com/datastax/oss/driver/api/core/data/CqlVector.java,"@@ -15,25 +15,31 @@
  */
 package com.datastax.oss.driver.api.core.data;
 
-import com.datastax.oss.driver.shaded.guava.common.base.Joiner;
-import com.datastax.oss.driver.shaded.guava.common.collect.ImmutableList;
-import com.datastax.oss.driver.shaded.guava.common.collect.Iterators;
-import java.util.Arrays;
+import java.util.*;
 
-/** An n-dimensional vector defined in CQL */
-public class CqlVector<T> {
+/**
+ * An n-dimensional vector defined in CQL. You can use either {@link CqlVector#builder()} for an
+ * iterable interface or {@link #CqlVector(List)} directly. Once created, a CqlVector is immutable.
+ */
+public class CqlVector<T> implements Iterable<T> {
 
-  private final ImmutableList<T> values;
+  private final List<T> values;","[{'comment': ""Mentioned in my [original review](https://datastax.slack.com/archives/C053MUQHLAK/p1685736386479569?thread_ts=1685666615.431329&cid=C053MUQHLAK) of what became this PR: I don't see any reason for this change.  We're giving up informing the type system that the list in question is immutable for no obvious benefit."", 'commenter': 'absurdfarce'}, {'comment': ""Sorry, I wasn't looking in chat for review content after our first convo. I thought it would be here. This is a design choice. Here are the reasons for my suggestion:\r\n\r\nI've preferred to avoid dependencies on Guava due to the instability, unneccesary complexity, library bloat, and packaging problems (not strictly backwards compatible, etc) that have arisin in other projects that depend on it. In this case, informing the type system doesn't really do anything, since the users doesn't see this in the CqlVector contract, and the effective behavior is the same. (The implemented type is effectively immutable.) Depending on this library also puts deeper roots into the Guava type system which is starting to overlap significantly with more modern Java idioms. Based on having to disentagle Guava from other projects for the reasons above, I've taken a stricter approach for if and when I would rely on it.\r\n\r\nBut this isn't something that is important enough to block a PR for, so I'll just follow your lead on the actual change."", 'commenter': 'jshook'}, {'comment': 'Are we already using Guava in the driver today?', 'commenter': 'jbellis'}, {'comment': ""I'll only mention that my comment about having the immutability of that collection reflected in the type system had to do with code internal to CqlVector rather than consumers of that library.  Which leads beautifully into my answer to the question posed by @jbellis :)\r\n\r\nThe Java driver does use Guava fairly extensively.  In the 3.x driver there are a few spots that returned Guava types directly from methods but this practice has been discontinued for the 4.x driver.  In 4.x we use a shaded version of Guava (an older one actually) and we only use it internally."", 'commenter': 'absurdfarce'}, {'comment': 'Understood, but the desire to disentangle Guava as a form of incremental simplification still stands for me, not to protect users directly, but more to make the code easier to maintain over time. This item is mostly an academic discussion on this point, but I wanted to be transparent on my reasons for suggesting the change.', 'commenter': 'jshook'}]"
1641,core/src/main/java/com/datastax/oss/driver/api/core/data/CqlVector.java,"@@ -15,25 +15,31 @@
  */
 package com.datastax.oss.driver.api.core.data;
 
-import com.datastax.oss.driver.shaded.guava.common.base.Joiner;
-import com.datastax.oss.driver.shaded.guava.common.collect.ImmutableList;
-import com.datastax.oss.driver.shaded.guava.common.collect.Iterators;
-import java.util.Arrays;
+import java.util.*;
 
-/** An n-dimensional vector defined in CQL */
-public class CqlVector<T> {
+/**
+ * An n-dimensional vector defined in CQL. You can use either {@link CqlVector#builder()} for an
+ * iterable interface or {@link #CqlVector(List)} directly. Once created, a CqlVector is immutable.
+ */
+public class CqlVector<T> implements Iterable<T> {
 
-  private final ImmutableList<T> values;
+  private final List<T> values;
 
-  private CqlVector(ImmutableList<T> values) {
-    this.values = values;
+  /**
+   * Create a CqlVector from a list of values.
+   *
+   * @param values
+   */
+  public CqlVector(List<T> values) {
+    this.values = Collections.unmodifiableList(values);
   }
 
-  public static Builder builder() {
-    return new Builder();
+  public static <T> CqlVector of(T... values) {
+    return new CqlVector(Collections.unmodifiableList(Arrays.asList(values)));
   }
 
-  public Iterable<T> getValues() {
+  /** @return the (immutable) list of values in the vector */
+  public List<T> getValues() {
     return values;
   }","[{'comment': ""Upon reflection I'm not sure we need a method to just return the contained values here.  Supporting Iterable should cover the overwhelming majority of cases; my inclination is to just go with that until we see we need something else."", 'commenter': 'absurdfarce'}, {'comment': ""[EDIT: I improved this comment to take out the unnecessarily strong tone and to focus it on the accessor]\r\n\r\nThis, I emphatically disagree with. This is the most important aspect of the change we need. Here is a scenario to illustrate:\r\n\r\n- We need to run an intensive performance test that is logically and operationally repeatable.\r\n- We need to run a variation of the performance test which has the simplest possible change, from un-normalized vectors to normalized vectors. The way to do that is, arguably, to simply allow for a CqlVector to be normalized. \r\n- We provide a basic function to do that ^, but because we are forced to use an iterator interface with (with no size data, even), we must:\r\n  - create a collection on heap, which will have to be dynamically resized and reallocated each time we exceed its backing capacity, which is incredibly wasteful for larger vectors.\r\n  - iteratively walk the data and store it into the collection\r\n  - run our computation against the separate copy of the data to compute the normalization factor\r\n  - run another scaling computation against those values\r\n  - <s>(and with the forced builder pattern) construct a whole new object, install the values into, and ask it for our new CqlVector</s> The newInstance approach can cover this.\r\n- Aspects of our testing mean that we're spending *significant* cycles on just creating and modifying vectors.\r\n\r\nIf we can access the vector data as a raw value, we could do this instead:\r\n- create an array of values of the correct size, _once_ as a copy of the original data.\r\n- scan over it to compute the normalization factor.\r\n- Update that array in place with the recalculated component lengths.\r\n- (and with the newInstance pattern elsewhere) just create a new instance from the array.\r\n\r\nAs an application developer using the vector search capability for the first time, I would want the datatype to be reasonably forgiving, meaning easy to create and access, not a hot spot for typical data sizes (over 1K or higher elements). In short, the closest thing to List<Float> or Float[] you can get, the happier I'll be as a user. I would ask you to consider our usage as a meaningful application study on what others will do. We are, after all, running an application.\r\n\r\nIn order to overcome the testing impact of the iterative accessor, we've had to create our own branches and build from them. We could jump through some hoops to prove out the impacts, but I thought they were evident enough that we wouldn't need to spend hours or days to justify such a direct and simple improvement."", 'commenter': 'jshook'}, {'comment': 'Understood, this explanation is helpful... thanks @jshook!\r\n\r\nFTR in my working PR I\'ve tried to address this by adding a method to export data from a vector to an input collection.  If I understand correctly (always a question) this will allow users to do the ""could to this instead"" section above while still making no assumptions about the type of collection involved or any of it\'s properties (other than the fact that it\'s, you know, a Collection).\r\n\r\nWe\'ll see if that flies.', 'commenter': 'absurdfarce'}, {'comment': 'without knowing the details -- maybe?\r\n\r\nthe core issue is we need to be able to modify it w/o copying it\r\n\r\ncopies of large collections like vectors really hurt perf', 'commenter': 'jbellis'}, {'comment': 'And copying them incrementally with a dynamically sized backing store, one element at a time, is a good way to test your GC, because this is a lot of small allocations.\r\n\r\n> As elements are added to an ArrayList, its capacity grows automatically. The details of the growth policy are not specified beyond the fact that adding an element has constant amortized time cost.\r\n\r\nAnd the nitty gritty details are almost always ""the existing backing memory is copied into a newly allocated region"", so you can see how this would create a heap allocation trampoline.', 'commenter': 'jshook'}, {'comment': ""I believe this entire discussion is now rendered moot by virtue of the most recent round of changes to my working PR for JAVA-3061.  I won't repeat the discussion in my most [recent comment on the topic](https://github.com/datastax/java-driver/pull/1656#issuecomment-1605121141) but the upshot is that CqlVector no longer exists and the Java driver now just represents CQL vectors as Java Lists."", 'commenter': 'absurdfarce'}]"
1641,core/src/main/java/com/datastax/oss/driver/api/core/data/CqlVector.java,"@@ -15,25 +15,31 @@
  */
 package com.datastax.oss.driver.api.core.data;
 
-import com.datastax.oss.driver.shaded.guava.common.base.Joiner;
-import com.datastax.oss.driver.shaded.guava.common.collect.ImmutableList;
-import com.datastax.oss.driver.shaded.guava.common.collect.Iterators;
-import java.util.Arrays;
+import java.util.*;
 
-/** An n-dimensional vector defined in CQL */
-public class CqlVector<T> {
+/**
+ * An n-dimensional vector defined in CQL. You can use either {@link CqlVector#builder()} for an
+ * iterable interface or {@link #CqlVector(List)} directly. Once created, a CqlVector is immutable.
+ */
+public class CqlVector<T> implements Iterable<T> {
 
-  private final ImmutableList<T> values;
+  private final List<T> values;
 
-  private CqlVector(ImmutableList<T> values) {
-    this.values = values;
+  /**
+   * Create a CqlVector from a list of values.
+   *
+   * @param values
+   */
+  public CqlVector(List<T> values) {
+    this.values = Collections.unmodifiableList(values);
   }
 
-  public static Builder builder() {
-    return new Builder();
+  public static <T> CqlVector of(T... values) {
+    return new CqlVector(Collections.unmodifiableList(Arrays.asList(values)));","[{'comment': 'This should be newInstance() in order to match up more closely with the other custom types with impls defined in the driver.  CqlDuration is [an excellent example](https://github.com/datastax/java-driver/blob/4.x/core/src/main/java/com/datastax/oss/driver/api/core/data/CqlDuration.java#L96-L108) here.', 'commenter': 'absurdfarce'}, {'comment': 'sure, that works too', 'commenter': 'jshook'}]"
1641,core/src/main/java/com/datastax/oss/driver/api/core/data/CqlVector.java,"@@ -56,36 +62,48 @@ public int hashCode() {
 
   @Override
   public String toString() {
+    StringBuilder builder = new StringBuilder(""CqlVector{"");
+    for (T value : values) {
+      builder.append(value).append("", "");
+    }
+    builder.setLength(builder.length() - "", "".length());
+    builder.append(""}"");
+    return builder.toString();
+  }","[{'comment': ""Note that the TypeCodec interface provides methods for converting a specific type to a String and back.  Upshot is that whatever is used here has to play nice with what's in [CqlVectorCodec](https://github.com/datastax/java-driver/blob/4.x/core/src/main/java/com/datastax/oss/driver/internal/core/type/codec/CqlVectorCodec.java#L118-L134).\r\n\r\nCqlDurationCodec provides [a useful example](https://github.com/datastax/java-driver/blob/4.x/core/src/main/java/com/datastax/oss/driver/internal/core/type/codec/CqlDurationCodec.java#L103-L115) of the interaction here."", 'commenter': 'absurdfarce'}, {'comment': 'noted', 'commenter': 'jshook'}]"
1642,pom.xml,"@@ -61,7 +61,7 @@
     <jackson-databind.version>2.13.2.2</jackson-databind.version>
     <legacy-jackson.version>1.9.12</legacy-jackson.version>
     <!-- optional dependencies -->
-    <snappy.version>1.1.7.3</snappy.version>
+    <snappy.version>1.1.8.4</snappy.version>","[{'comment': ""note to myself to remove this dependency update since it's unrelated to jdk17"", 'commenter': 'hhughes'}]"
1642,pom.xml,"@@ -714,6 +715,7 @@ limitations under the License.]]></inlineHeader>
             </property>
           </properties>
           <skip>${skipUnitTests}</skip>
+          <argLine>--add-opens java.base/jdk.internal.util.random=ALL-UNNAMED</argLine>","[{'comment': 'this line will cause jdk8 build to fail :(', 'commenter': 'hhughes'}]"
1642,Jenkinsfile,"@@ -25,9 +25,16 @@ def initializeEnvironment() {
     . ${JABBA_SHELL}
     jabba which 1.8''', returnStdout: true).trim()
 
+  env.TEST_JAVA_HOME = sh(label: 'Get TEST_JAVA_HOME',script: '''#!/bin/bash -le
+    . ${JABBA_SHELL}
+    jabba which ${JABBA_VERSION}''', returnStdout: true).trim()
+  env.TEST_JAVA_VERSION = sh(label: 'Get TEST_JAVA_HOME',script: '''#!/bin/bash -le
+    echo ""${JABBA_VERSION##*.}""''', returnStdout: true).trim()
+
+
   sh label: 'Download Apache Cassandraâ“‡ or DataStax Enterprise',script: '''#!/bin/bash -le
     . ${JABBA_SHELL}
-    jabba use ${JABBA_VERSION}
+    jabba use 1.8","[{'comment': 'Shall we have a comment explaining the difference between JABBA_VERSION, TEST_JAVA_VERSION, etal?', 'commenter': 'weideng1'}, {'comment': ""Done. I've also filed JAVA-3064 to tidy up how jabba is used in Jenkinsfile since after this change we no longer need the all the `JAVA_HOME` switching stuff in `buildDriver` and `executeTests`"", 'commenter': 'hhughes'}]"
1642,integration-tests/src/test/java/com/datastax/dse/driver/api/core/cql/continuous/ContinuousPagingITBase.java,"@@ -61,7 +62,9 @@ protected static void initialize(CqlSession session, DriverExecutionProfile slow
     }
     int count = 0;
     for (int i = 0; i < 200; i++) {
-      BatchStatement batch = BatchStatement.newInstance(DefaultBatchType.UNLOGGED);
+      BatchStatement batch =
+          BatchStatement.newInstance(DefaultBatchType.UNLOGGED)
+              .setConsistencyLevel(ConsistencyLevel.ALL);","[{'comment': 'Which test failure this CL_ALL is trying to address?', 'commenter': 'weideng1'}, {'comment': ""This was an experiement to attempt to fix flakiness where the queries in these tests fail to find all the items added in the test set-up. It wasn't supposed to be included in this PR so should be removed, thanks for catching this."", 'commenter': 'hhughes'}]"
1642,Jenkinsfile,"@@ -18,16 +18,40 @@ def initializeEnvironment() {
 
   env.MAVEN_HOME = ""${env.HOME}/.mvn/apache-maven-3.3.9""
   env.PATH = ""${env.MAVEN_HOME}/bin:${env.PATH}""
+
+  /*
+  * As of JAVA-3042 JAVA_HOME is always set to JDK8 and this is currently necessary for mvn compile and DSE Search/Graph.
+  * To facilitate testing with JDK11/17 we feed the appropriate JAVA_HOME into the maven build via commandline.
+  *
+  * Maven command-line flags:
+  * - -DtestJavaHome=/path/to/java/home: overrides JAVA_HOME for surefire/failsafe tests, defaults to environment JAVA_HOME.
+  * - -Ptest-jdk-N: enables profile for running tests with a specific JDK version (substitute N for 8/11/17).
+  *
+  * Note test-jdk-N is also automatically loaded based off JAVA_HOME SDK version so testing with an older SDK is not supported.
+  *
+  * Environment variables:
+  * - JAVA_HOME: Path to JDK used for mvn (all steps except surefire/failsafe), Cassandra, DSE.
+  * - JAVA8_HOME: Path to JDK8 used for Cassandra/DSE if ccm determines JAVA_HOME is not compatible with the chosen backend.
+  * - TEST_JAVA_HOME: PATH to JDK used for surefire/failsafe testing.
+  * - TEST_JAVA_VERSION: TEST_JAVA_HOME SDK version number [8/11/17], used to configure test-jdk-N profile in maven (see above)
+  */
+
   env.JAVA_HOME = sh(label: 'Get JAVA_HOME',script: '''#!/bin/bash -le
     . ${JABBA_SHELL}
     jabba which ${JABBA_VERSION}''', returnStdout: true).trim()
   env.JAVA8_HOME = sh(label: 'Get JAVA8_HOME',script: '''#!/bin/bash -le
     . ${JABBA_SHELL}
     jabba which 1.8''', returnStdout: true).trim()
 
+  env.TEST_JAVA_HOME = sh(label: 'Get TEST_JAVA_HOME',script: '''#!/bin/bash -le
+    . ${JABBA_SHELL}
+    jabba which ${JABBA_VERSION}''', returnStdout: true).trim()
+  env.TEST_JAVA_VERSION = sh(label: 'Get TEST_JAVA_HOME',script: '''#!/bin/bash -le
+    echo ""${JABBA_VERSION##*.}""''', returnStdout: true).trim()
+
   sh label: 'Download Apache Cassandraâ“‡ or DataStax Enterprise',script: '''#!/bin/bash -le
     . ${JABBA_SHELL}
-    jabba use ${JABBA_VERSION}
+    jabba use 1.8","[{'comment': ""Worth noting that this is the setup for the attempt to install the requested C* server version via ccm_environment.  This operation assumes that a Jabba install with the requested version is available (and uses that as JAVA_HOME for the C* invocation).  See [here](https://github.com/riptano/openstack-jenkins-drivers/blob/master/shared/scripts/ccm_environment.sh#L273) and [here](https://github.com/riptano/openstack-jenkins-drivers/blob/master/shared/scripts/ccm_environment.sh#L49-L54) and [here](https://github.com/riptano/openstack-jenkins-drivers/blob/master/shared/scripts/ccm_environment.sh#L278-L281) for more on that point.\r\n\r\nBottom line: it doesn't matter what JDK is setup here... we'll wind up using Java8 for the ccm invocation regardless."", 'commenter': 'absurdfarce'}, {'comment': ""Makes sense. Here I'm really making sure that jabba is not accidentally loading some other version of JDK since we now always want JAVA_HOME to be 1.8 during the jenkins run. I'd like to leave them all in for now to reduce the complexity of this change and I've filed [JAVA-3064](https://datastax-oss.atlassian.net/browse/JAVA-3064) to clean up all these extra usages of jabba in this script. "", 'commenter': 'hhughes'}]"
1642,Jenkinsfile,"@@ -18,16 +18,40 @@ def initializeEnvironment() {
 
   env.MAVEN_HOME = ""${env.HOME}/.mvn/apache-maven-3.3.9""
   env.PATH = ""${env.MAVEN_HOME}/bin:${env.PATH}""
+
+  /*
+  * As of JAVA-3042 JAVA_HOME is always set to JDK8 and this is currently necessary for mvn compile and DSE Search/Graph.
+  * To facilitate testing with JDK11/17 we feed the appropriate JAVA_HOME into the maven build via commandline.
+  *
+  * Maven command-line flags:
+  * - -DtestJavaHome=/path/to/java/home: overrides JAVA_HOME for surefire/failsafe tests, defaults to environment JAVA_HOME.
+  * - -Ptest-jdk-N: enables profile for running tests with a specific JDK version (substitute N for 8/11/17).
+  *
+  * Note test-jdk-N is also automatically loaded based off JAVA_HOME SDK version so testing with an older SDK is not supported.
+  *
+  * Environment variables:
+  * - JAVA_HOME: Path to JDK used for mvn (all steps except surefire/failsafe), Cassandra, DSE.
+  * - JAVA8_HOME: Path to JDK8 used for Cassandra/DSE if ccm determines JAVA_HOME is not compatible with the chosen backend.
+  * - TEST_JAVA_HOME: PATH to JDK used for surefire/failsafe testing.
+  * - TEST_JAVA_VERSION: TEST_JAVA_HOME SDK version number [8/11/17], used to configure test-jdk-N profile in maven (see above)
+  */
+
   env.JAVA_HOME = sh(label: 'Get JAVA_HOME',script: '''#!/bin/bash -le
     . ${JABBA_SHELL}
     jabba which ${JABBA_VERSION}''', returnStdout: true).trim()
   env.JAVA8_HOME = sh(label: 'Get JAVA8_HOME',script: '''#!/bin/bash -le
     . ${JABBA_SHELL}
     jabba which 1.8''', returnStdout: true).trim()
 
+  env.TEST_JAVA_HOME = sh(label: 'Get TEST_JAVA_HOME',script: '''#!/bin/bash -le
+    . ${JABBA_SHELL}
+    jabba which ${JABBA_VERSION}''', returnStdout: true).trim()
+  env.TEST_JAVA_VERSION = sh(label: 'Get TEST_JAVA_HOME',script: '''#!/bin/bash -le","[{'comment': ""Presumably this should be 'Get TEST_JAVA_VERSION'... ?"", 'commenter': 'absurdfarce'}]"
1642,pom.xml,"@@ -936,6 +936,69 @@ height=""0"" width=""0"" style=""display:none;visibility:hidden""></iframe></noscript>
         <revapi.skip>true</revapi.skip>
       </properties>
     </profile>
+    <!-- macOS-specific build workarounds -->
+    <profile>
+      <id>build-macos</id>
+      <activation>
+        <os>
+          <name>mac os x</name>
+        </os>
+      </activation>
+      <properties>
+        <snappy.version>1.1.8.4</snappy.version>
+      </properties>
+    </profile>
+    <!-- Use $JAVA_HOME as the default JDK to run surefire/failsafe to maintain portability -->
+    <profile>
+      <id>test-jdk-environment</id>
+      <activation>
+        <property>
+          <name>!testJavaHome</name>
+        </property>
+      </activation>
+      <properties>
+        <testing.jvm>${env.JAVA_HOME}</testing.jvm>
+      </properties>
+    </profile>
+    <!-- set -DtestJdkHome=/path/to/jdk/home to use a different JDK for surefire/failsafe -->","[{'comment': 'Believe this should be ""testJavaHome"" rather than ""testJdkHome"" here, right?', 'commenter': 'absurdfarce'}]"
1642,pom.xml,"@@ -533,12 +533,12 @@
         <plugin>
           <groupId>org.jacoco</groupId>
           <artifactId>jacoco-maven-plugin</artifactId>
-          <version>0.8.5</version>
+          <version>0.8.10</version>
         </plugin>
         <plugin>
           <groupId>org.apache.felix</groupId>
           <artifactId>maven-bundle-plugin</artifactId>
-          <version>4.2.1</version>
+          <version>5.1.1</version>","[{'comment': 'Does this change have any implications on the minimum supported Maven version?', 'commenter': 'absurdfarce'}, {'comment': ""This is the changelog https://github.com/apache/felix-dev/blob/master/tools/maven-bundle-plugin/changelog.txt#L59\r\n\r\nThere's nothing calling out bumping maven version or other dependencies"", 'commenter': 'hhughes'}]"
1650,test-infra/src/main/java/com/datastax/oss/driver/api/testinfra/ccm/CcmBridge.java,"@@ -52,7 +52,7 @@ public class CcmBridge implements AutoCloseable {
   private static final Logger LOG = LoggerFactory.getLogger(CcmBridge.class);
 
   public static final Version VERSION =
-      Objects.requireNonNull(Version.parse(System.getProperty(""ccm.version"", ""4.0.0"")));
+      Objects.requireNonNull(Version.parse(System.getProperty(""ccm.version"", ""4.0.10"")));","[{'comment': 'Bringing default Cassandra version used in testing to be same 4.0 one used by Jenkins.\r\n\r\nThis is why this test passed locally but not in 4.0/3.11 in Jenkins.\r\n\r\nHappy to remove if this change is too disruptive.', 'commenter': 'hhughes'}, {'comment': ""Since we're talking about a default here I'd prefer to avoid changing it for now.  Worth noting that on Jenkins at least this value is always provided [by the runner](https://github.com/riptano/openstack-jenkins-drivers/blob/master/shared/scripts/ccm_environment.sh#L260) by way of the [Jenkinsfile](https://github.com/datastax/java-driver/blob/4.x/Jenkinsfile#LL101C1-L101C1).  So for our purposes it will always be set to the expected version based on Jenkins args."", 'commenter': 'absurdfarce'}, {'comment': 'Ack, removed', 'commenter': 'hhughes'}]"
1650,integration-tests/src/test/java/com/datastax/oss/driver/core/cql/PreparedStatementIT.java,"@@ -444,9 +447,32 @@ public void should_fail_fast_if_id_changes_on_reprepare() {
       executeDdl(""DROP TABLE prepared_statement_test"");
       executeDdl(""CREATE TABLE prepared_statement_test (a int PRIMARY KEY, b int, c int)"");
 
-      assertThatThrownBy(() -> session.execute(preparedStatement.bind(1)))
-          .isInstanceOf(IllegalStateException.class)
-          .hasMessageContaining(""ID mismatch while trying to reprepare"");
+      // JAVA-3065: determine if cassandra/dse version has CASSANDRA-15252 fix
+      boolean isCassandra15252Fixed = false;
+      if (ccmRule.getDseVersion().isPresent()) {
+        // no DSE versions have recent enough cassandra yet
+        isCassandra15252Fixed = false;
+      } else {
+        // cassandra must be 3.0.26+, 3.11.12+ or 4.0.2+
+        Version cassandraVersion = ccmRule.getCassandraVersion();
+        if (cassandraVersion.compareTo(Version.parse(""4.0.0"")) >= 0) {
+          isCassandra15252Fixed = cassandraVersion.compareTo(Version.parse(""4.0.2"")) >= 0;
+        } else if (cassandraVersion.compareTo(Version.parse(""3.11.0"")) >= 0) {
+          isCassandra15252Fixed = cassandraVersion.compareTo(Version.parse(""3.11.12"")) >= 0;
+        } else if (cassandraVersion.compareTo(Version.parse(""3.0.0"")) >= 0) {
+          isCassandra15252Fixed = cassandraVersion.compareTo(Version.parse(""3.0.26"")) >= 0;
+        }
+      }
+
+      ThrowableAssert.ThrowingCallable errorTrigger =
+          () -> session.execute(preparedStatement.bind(1));
+      if (isCassandra15252Fixed) {
+        assertThatCode(errorTrigger).doesNotThrowAnyException();
+      } else {
+        assertThatThrownBy(errorTrigger)
+            .isInstanceOf(IllegalStateException.class)
+            .hasMessageContaining(""ID mismatch while trying to reprepare"");
+      }","[{'comment': ""Within the integration test framework this kind of thing is normally handled via the [CassandraRequirement annotation](https://github.com/datastax/java-driver/blob/4.x/test-infra/src/main/java/com/datastax/oss/driver/api/testinfra/CassandraRequirement.java).  You can find examples in [other test methods of PreparedStatementIT](https://github.com/datastax/java-driver/blob/4.x/integration-tests/src/test/java/com/datastax/oss/driver/core/cql/PreparedStatementIT.java#L154) (as well as other integration tests).  Enforcement of these annotations can be found in [BaseCcmRule](https://github.com/datastax/java-driver/blob/4.x/test-infra/src/main/java/com/datastax/oss/driver/api/testinfra/ccm/BaseCcmRule.java#L76-L132).\r\n\r\nI'd very strongly prefer not to introduce multiple methods for supporting this kind of behaviour unless it's absolutely necessary, but it's worth pointing out that the current impl does have a limitation of relevance for us here.  To really do this right you'd want to be able to (a) attach multiple annotations to a given test method, one for each major.minor version, with a min or max version specified as appropriate and (b) have BaseCcmRule evaluate _all_ annotation on the test method and enforce all of them.  After a quick review of the code I don't think that change is actually that bad, so maybe this is an opportunity to add a new feature (support for multiple annotations) in order to avoid having to introduce another way to handle version constraints."", 'commenter': 'absurdfarce'}, {'comment': ""Added new annotation: `BackendRequirement` which supports annotation repetition and am now using this with `should_fail_fast_if_id_changes_on_reprepare`. I've left the previous anotations in for now because I think there is some more tidying up, e.g. for tests which have been copied to support multiple c*/dse requirement, I've filed JAVA-3069 to track this work."", 'commenter': 'hhughes'}, {'comment': 'Wrong version/backend error in intellij test output looks like:\r\n```\r\norg.junit.AssumptionViolatedException: Test requires one of:\r\n  - C* 3.0.26 or greater, but less than 3.11.0\r\n  - C* 3.11.12 or greater, but less than 4.0.0\r\n  - C* 4.0.2 or greater\r\nbut configuration is C* 4.0.0.\r\n```', 'commenter': 'hhughes'}]"
1650,integration-tests/src/test/java/com/datastax/oss/driver/core/cql/PreparedStatementIT.java,"@@ -444,12 +445,38 @@ public void should_fail_fast_if_id_changes_on_reprepare() {
       executeDdl(""DROP TABLE prepared_statement_test"");
       executeDdl(""CREATE TABLE prepared_statement_test (a int PRIMARY KEY, b int, c int)"");
 
-      assertThatThrownBy(() -> session.execute(preparedStatement.bind(1)))
-          .isInstanceOf(IllegalStateException.class)
-          .hasMessageContaining(""ID mismatch while trying to reprepare"");
+      return assertThatCode(() -> session.execute(preparedStatement.bind(1)));","[{'comment': 'Slick, I like this... ðŸ‘ ', 'commenter': 'absurdfarce'}]"
1650,integration-tests/src/test/java/com/datastax/oss/driver/core/cql/PreparedStatementIT.java,"@@ -444,12 +445,38 @@ public void should_fail_fast_if_id_changes_on_reprepare() {
       executeDdl(""DROP TABLE prepared_statement_test"");
       executeDdl(""CREATE TABLE prepared_statement_test (a int PRIMARY KEY, b int, c int)"");
 
-      assertThatThrownBy(() -> session.execute(preparedStatement.bind(1)))
-          .isInstanceOf(IllegalStateException.class)
-          .hasMessageContaining(""ID mismatch while trying to reprepare"");
+      return assertThatCode(() -> session.execute(preparedStatement.bind(1)));
     }
   }
 
+  @BackendRequirement(type = BackendType.DSE)","[{'comment': ""DSE is actually a different code base; it has to explicitly bring changes from the OSS C* project along.  I'm wondering if DSE has even brought the relevant change over to its code base (let alone released it yet).\r\n\r\nIt doesn't _look like_ that's happened yet (although I haven't made an exhaustive search).  Perhaps add a comment here saying that we'll need to add version boundaries to this guy as well once DSE brings the change over."", 'commenter': 'absurdfarce'}, {'comment': ""Yes it's currently `4.0.0`: https://github.com/riptano/bdp/blob/6.8-dev/build.properties#L31, updated the description and added a comment"", 'commenter': 'hhughes'}]"
1656,core/src/main/java/com/datastax/oss/driver/api/core/data/CqlVector.java,"@@ -15,26 +15,78 @@
  */
 package com.datastax.oss.driver.api.core.data;
 
-import com.datastax.oss.driver.shaded.guava.common.base.Joiner;
+import com.datastax.oss.driver.api.core.type.codec.TypeCodec;
+import com.datastax.oss.driver.shaded.guava.common.base.Splitter;
 import com.datastax.oss.driver.shaded.guava.common.collect.ImmutableList;
+import com.datastax.oss.driver.shaded.guava.common.collect.Iterables;
 import com.datastax.oss.driver.shaded.guava.common.collect.Iterators;
+import com.datastax.oss.driver.shaded.guava.common.collect.Streams;
+import edu.umd.cs.findbugs.annotations.NonNull;
 import java.util.Arrays;
+import java.util.Collection;
+import java.util.Iterator;
 
-/** An n-dimensional vector defined in CQL */
-public class CqlVector<T> {
+/**
+ * An immutable n-dimensional vector representation of the CQL vector type.
+ *
+ * <p>Instances may be created iteratively using {@link CqlVector.Builder} or in a single method
+ * call via {@link #newInstance(Object[])} or {@link #newInstance(Collection)}.
+ */
+public class CqlVector<T> implements Iterable<T> {","[{'comment': ""There is still a critical need to have simple access to the underlying values. The iterative accessor is simply not sufficient for our application's needs."", 'commenter': 'jshook'}, {'comment': ' \r\nLooking at the pros/cons, my understanding is:\r\n\r\n* Iterable access provides a generic approach and could be used where lazy evaluation and large datasets are known. \r\n* List<T> commonly used interface, easy way to work with collections, random access, specific list operations.  This also allows for getting an idea of the size.\r\n\r\nSince our full set of vector search use cases are somewhat vague at this point, +1 for having both options.\r\n', 'commenter': 'jeffbanks'}, {'comment': ""I attempted to address this by adding an export(Collection) method which dumps the vectors content into a collection type of your choosing.  This is significantly more flexible than exporting a single collection type; whatever type we export may have properties (list vs. set vs. queue, immutable vs. immutable, linked lists vs. array lists and so on) which almost certainly will not apply in even a majority of situations.  This approach allows API users to create the list in whatever way they see fit (and even perform some initial operations on said list if they choose to do so) and then hand it over to our vector for supplying the data.\r\n\r\nIt's my hope that this addresses your immediate concern @jshook while also maximizing our flexibility for the many Java driver users who will have use cases with different parameters.  If I outright missed the boat here and didn't understand something please let me know."", 'commenter': 'absurdfarce'}, {'comment': ""This is an ergonomic improvement, and quite flexible. However, it doesn't improve the situation around the heap allocation acrobatics for large (assume 2K nominal) vectors. I'm working up some microbenches to quantify this better. It seems worth it at least for something that, once settled, will be how CqlVector works for a long time."", 'commenter': 'jshook'}]"
1656,core/src/main/java/com/datastax/oss/driver/api/core/data/SettableByIndex.java,"@@ -423,8 +423,9 @@ default SelfT setCqlDuration(int i, @Nullable CqlDuration v) {
    */
   @NonNull
   @CheckReturnValue
-  default SelfT setCqlVector(int i, @Nullable CqlVector<?> v) {
-    return set(i, v, CqlVector.class);
+  default <ElementT> SelfT setVector(","[{'comment': '```\r\n* Sets the {@code i}th value to the provided duration.\r\n```\r\nShould this be `vector`?\r\n', 'commenter': 'hhughes'}, {'comment': 'It should indeed... good catch @hhughes!', 'commenter': 'absurdfarce'}, {'comment': 'Done.', 'commenter': 'absurdfarce'}]"
1656,core/src/main/java/com/datastax/oss/driver/api/core/data/SettableByName.java,"@@ -570,10 +570,11 @@ default SelfT setCqlDuration(@NonNull String name, @Nullable CqlDuration v) {
    */
   @NonNull
   @CheckReturnValue
-  default SelfT setCqlVector(@NonNull String name, @Nullable CqlVector<?> v) {
+  default <ElementT> SelfT setVector(","[{'comment': 'Replace `duration` with `vector` in the javadoc', 'commenter': 'hhughes'}, {'comment': 'Yup, this one too!', 'commenter': 'absurdfarce'}, {'comment': 'Done.', 'commenter': 'absurdfarce'}]"
1656,core/src/main/java/com/datastax/oss/driver/internal/core/type/codec/VectorCodec.java,"@@ -57,12 +58,12 @@ public DataType getCqlType() {
   @Nullable
   @Override
   public ByteBuffer encode(
-      @Nullable CqlVector<SubtypeT> value, @NonNull ProtocolVersion protocolVersion) {
+      @Nullable List<SubtypeT> value, @NonNull ProtocolVersion protocolVersion) {
     if (value == null || cqlType.getDimensions() <= 0) {
       return null;
     }
     ByteBuffer[] valueBuffs = new ByteBuffer[cqlType.getDimensions()];
-    Iterator<SubtypeT> values = value.getValues().iterator();
+    Iterator<SubtypeT> values = value.iterator();
     int allValueBuffsSize = 0;
     for (int i = 0; i < cqlType.getDimensions(); ++i) {
       ByteBuffer valueBuff = this.subtypeCodec.encode(values.next(), protocolVersion);","[{'comment': 'ListCodec and MapCodec have some handling around null elements and classcastexceptions, do we want to have the same checks here?', 'commenter': 'hhughes'}, {'comment': ""My original thought was that this wasn't necessary because the codecs were pretty good about not meeting these error cases.  Then I went back and looked at the code and realized that recollection was entirely wrong.  Added that code in there (as well as check for a few other error conditions) and beefed up the codec test a bit to detect them."", 'commenter': 'absurdfarce'}]"
1677,upgrade_guide/README.md,"@@ -1,5 +1,60 @@
 ## Upgrade guide
 
+### 4.17.0
+
+#### Java17 is now a supported platform
+
+With [JAVA-3042](https://datastax-oss.atlassian.net/browse/JAVA-3042) the driver is now tested against Java17 as
+well as the other LTS JVM releases Java8 and Java11.  Based on our testing the Java driver appears to function
+normally on Java17, but of course bugs are always a possibility.  Issues arising from the Java17 platform will be
+triaged and addressed just like any other issue.","[{'comment': ""I'm not super-happy with the way this language came out.  If anybody has any suggestions on a way to say all this in something that sounds more like English I'm all ears."", 'commenter': 'absurdfarce'}, {'comment': ""I'm happy with this from a technical sentiment pov"", 'commenter': 'hhughes'}]"
1677,manual/core/README.md,"@@ -231,34 +231,35 @@ See [AccessibleByName] for an explanation of the conversion rules.
 
 ##### CQL to Java type mapping
 
-| CQL3 data type      | Getter name    | Java type            | See also                            |
-|---------------------|----------------|----------------------|-------------------------------------|
-| ascii               | getString      | java.lang.String     |                                     |
-| bigint              | getLong        | long                 |                                     |
-| blob                | getByteBuffer  | java.nio.ByteBuffer  |                                     |
-| boolean             | getBoolean     | boolean              |                                     |
-| counter             | getLong        | long                 |                                     |
-| date                | getLocalDate   | java.time.LocalDate  | [Temporal types](temporal_types/)   |
-| decimal             | getBigDecimal  | java.math.BigDecimal |                                     |
-| double              | getDouble      | double               |                                     |
-| duration            | getCqlDuration | [CqlDuration]        | [Temporal types](temporal_types/)   |
-| float               | getFloat       | float                |                                     |
-| inet                | getInetAddress | java.net.InetAddress |                                     |
-| int                 | getInt         | int                  |                                     |
-| list                | getList        | java.util.List<T>    |                                     |
-| map                 | getMap         | java.util.Map<K, V>  |                                     |
-| set                 | getSet         | java.util.Set<T>     |                                     |
-| smallint            | getShort       | short                |                                     |
-| text                | getString      | java.lang.String     |                                     |
-| time                | getLocalTime   | java.time.LocalTime  | [Temporal types](temporal_types/)   |
-| timestamp           | getInstant     | java.time.Instant    | [Temporal types](temporal_types/)   |
-| timeuuid            | getUuid        | java.util.UUID       |                                     |
-| tinyint             | getByte        | byte                 |                                     |
-| tuple               | getTupleValue  | [TupleValue]         | [Tuples](tuples/)                   |
-| user-defined types  | getUDTValue    | [UDTValue]           | [User-defined types](udts/)         |
-| uuid                | getUuid        | java.util.UUID       |                                     |
-| varchar             | getString      | java.lang.String     |                                     |
-| varint              | getBigInteger  | java.math.BigInteger |                                     |
+| CQL3 data type     | Getter name    | Java type            | See also                          |
+|--------------------|----------------|----------------------|-----------------------------------|
+| ascii              | getString      | java.lang.String     |                                   |
+| bigint             | getLong        | long                 |                                   |
+| blob               | getByteBuffer  | java.nio.ByteBuffer  |                                   |
+| boolean            | getBoolean     | boolean              |                                   |
+| counter            | getLong        | long                 |                                   |
+| date               | getLocalDate   | java.time.LocalDate  | [Temporal types](temporal_types/) |
+| decimal            | getBigDecimal  | java.math.BigDecimal |                                   |
+| double             | getDouble      | double               |                                   |
+| duration           | getCqlDuration | [CqlDuration]        | [Temporal types](temporal_types/) |
+| float              | getFloat       | float                |                                   |
+| inet               | getInetAddress | java.net.InetAddress |                                   |
+| int                | getInt         | int                  |                                   |
+| list               | getList        | java.util.List<T>    |                                   |
+| map                | getMap         | java.util.Map<K, V>  |                                   |
+| set                | getSet         | java.util.Set<T>     |                                   |
+| smallint           | getShort       | short                |                                   |
+| text               | getString      | java.lang.String     |                                   |
+| time               | getLocalTime   | java.time.LocalTime  | [Temporal types](temporal_types/) |
+| timestamp          | getInstant     | java.time.Instant    | [Temporal types](temporal_types/) |
+| timeuuid           | getUuid        | java.util.UUID       |                                   |
+| tinyint            | getByte        | byte                 |                                   |
+| tuple              | getTupleValue  | [TupleValue]         | [Tuples](tuples/)                 |
+| user-defined types | getUDTValue    | [UDTValue]           | [User-defined types](udts/)       |
+| uuid               | getUuid        | java.util.UUID       |                                   |
+| varchar            | getString      | java.lang.String     |                                   |
+| varint             | getBigInteger  | java.math.BigInteger |                                   |
+| vector             | getVector      | [CqlVector]          | [Custom Codecs](custom_codecs/)   |","[{'comment': 'Should we add a dangling link to CqlVector in the section at the bottom? And what is the process for getting those existing links updated to 4.17.0?', 'commenter': 'hhughes'}, {'comment': 'The ""custom_codecs"" link refers to another manual page; all the links at the bottom point directly at Javadoc.  I thought about adding a local ref to the URL but note that we don\'t do that with any of the other value types here (including CqlDuration, which serves as something of our model).\r\n\r\nAs for updating the links to point at the current release: that\'s something we do as part of the release process.  We haven\'t done it the last few times around because the doc updates were delayed for various reasons (mostly build-related) that we believe are now resolved.\r\n\r\nThis conversation certainly does highlight that CqlVector needs to have a link at the bottom; I\'ll add that for sure.', 'commenter': 'absurdfarce'}]"
1677,manual/core/custom_codecs/README.md,"@@ -256,6 +256,18 @@ that maps instances of that class to Json strings, using a newly-allocated, defa
 It is also possible to pass a custom `ObjectMapper` instance using [ExtraTypeCodecs.json(Class,
 ObjectMapper)] instead.
 
+#### Mapping CQL vectors to Java array
+
+By default, the driver maps CQL `vector` to the [CqlVector] value type. If you prefer to deal with
+arrays, the driver offers the following codec:
+
+| Codec               | CQL type        | Java type |
+|---------------------|-----------------|-----------|
+| [ExtraTypeCodecs.floatVectorToArray(dimensions)] | `vector<float>` | `float[]` |","[{'comment': 'Similar - no dangling links defined', 'commenter': 'hhughes'}, {'comment': 'As above, I should\'ve added the Javadoc link below when I put this together originally.  That\'s what I get for writing docs instead of getting some sleep. :(\r\n\r\nLink added.  I also changed the arg for floatVectorToArray() to be the type (""int"") rather than the name (""dimensions"") to match the usage elsewhere in this doc.', 'commenter': 'absurdfarce'}]"
1677,upgrade_guide/README.md,"@@ -1,5 +1,60 @@
 ## Upgrade guide
 
+### 4.17.0
+
+#### Java17 is now a supported platform
+
+With [JAVA-3042](https://datastax-oss.atlassian.net/browse/JAVA-3042) the driver is now tested against Java17 as
+well as the other LTS JVM releases Java8 and Java11.  Based on our testing the Java driver appears to function
+normally on Java17, but of course bugs are always a possibility.  Issues arising from the Java17 platform will be
+triaged and addressed just like any other issue.
+
+#### Updated API for vector search
+
+The 4.16.0 release introduced support for the CQL vector datatype (see
+[JAVA-3060](https://datastax-oss.atlassian.net/browse/JAVA-3060) for more detail).  This release modifies the CqlVector
+value type used to represent a CQL vector to make it easier to use.  CqlVector now implements the Iterable interface
+as well as several methods modelled on the JDK's List interface, so it should have a familiar feel to it.  We've also
+done away with the builder interface, replacing it with some factory methods that resemble similar methods on CqlDuration.
+
+As an example, the following code will create a keyspace and table, populate that table with some data and then execute
+a query which will return a vector type.  This data is retrieved directly via Row.getVector() and the resulting
+CqlVector value object can be interrogated directly.
+
+```java
+try (CqlSession session = new CqlSessionBuilder().withLocalDatacenter(""datacenter1"").build()){
+
+    session.execute(""drop keyspace if exists test"");
+    session.execute(""create keyspace test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1}"");
+    session.execute(""create table test.foo(i int primary key, j vector<float, 3>)"");
+    session.execute(""create custom index ann_index on test.foo(j) using 'StorageAttachedIndex'"");
+    session.execute(""insert into test.foo (i, j) values (1, [8, 2.3, 58])"");
+    session.execute(""insert into test.foo (i, j) values (2, [1.2, 3.4, 5.6])"");
+    session.execute(""insert into test.foo (i, j) values (5, [23, 18, 3.9])"");
+    ResultSet rs=session.execute(""select j from test.foo where j ann of [3.4, 7.8, 9.1] limit 1"");","[{'comment': 'Nit: upper case CQL keywords to match the style used elsewhere in this file', 'commenter': 'hhughes'}, {'comment': ""Yeah, that's reasonable.\r\n\r\nUpdated."", 'commenter': 'absurdfarce'}]"
1677,upgrade_guide/README.md,"@@ -1,5 +1,60 @@
 ## Upgrade guide
 
+### 4.17.0
+
+#### Java17 is now a supported platform
+
+With [JAVA-3042](https://datastax-oss.atlassian.net/browse/JAVA-3042) the driver is now tested against Java17 as
+well as the other LTS JVM releases Java8 and Java11.  Based on our testing the Java driver appears to function
+normally on Java17, but of course bugs are always a possibility.  Issues arising from the Java17 platform will be
+triaged and addressed just like any other issue.
+
+#### Updated API for vector search
+
+The 4.16.0 release introduced support for the CQL vector datatype (see
+[JAVA-3060](https://datastax-oss.atlassian.net/browse/JAVA-3060) for more detail).  This release modifies the CqlVector
+value type used to represent a CQL vector to make it easier to use.  CqlVector now implements the Iterable interface
+as well as several methods modelled on the JDK's List interface, so it should have a familiar feel to it.  We've also
+done away with the builder interface, replacing it with some factory methods that resemble similar methods on CqlDuration.
+
+As an example, the following code will create a keyspace and table, populate that table with some data and then execute
+a query which will return a vector type.  This data is retrieved directly via Row.getVector() and the resulting
+CqlVector value object can be interrogated directly.
+
+```java
+try (CqlSession session = new CqlSessionBuilder().withLocalDatacenter(""datacenter1"").build()){
+
+    session.execute(""drop keyspace if exists test"");
+    session.execute(""create keyspace test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1}"");
+    session.execute(""create table test.foo(i int primary key, j vector<float, 3>)"");
+    session.execute(""create custom index ann_index on test.foo(j) using 'StorageAttachedIndex'"");
+    session.execute(""insert into test.foo (i, j) values (1, [8, 2.3, 58])"");
+    session.execute(""insert into test.foo (i, j) values (2, [1.2, 3.4, 5.6])"");
+    session.execute(""insert into test.foo (i, j) values (5, [23, 18, 3.9])"");
+    ResultSet rs=session.execute(""select j from test.foo where j ann of [3.4, 7.8, 9.1] limit 1"");
+    for(Row row:rs){
+        CqlVector<Float> v = row.getVector(0,Float.class);
+        System.out.println(v);
+        if(Iterables.size(v) != 3){
+            throw new RuntimeException(""Expected vector with three dimensions"");
+        }
+    }
+}","[{'comment': 'Nit: Fix spacing in this code to conform to our usual code standards', 'commenter': 'hhughes'}, {'comment': ""The samples in this doc appear to vary quite a bit.  Most use an indent of 4 spaces (which is what I use here) while a few spots use an indent of two spaces.  I find a 4 space indent to be more readable as well so I think I'll update all the samples in the doc to use a consistent 4 space indent."", 'commenter': 'absurdfarce'}, {'comment': 'To clarify I was mostly thinking about the spacing within the lines, eg between variable and `=`, in the loop syntax etc', 'commenter': 'hhughes'}, {'comment': ""Makes sense @hhughes.  It's probably a good exercise to standardize the docs on a (mostly) uniform Java code style anyway. :)"", 'commenter': 'absurdfarce'}]"
1677,upgrade_guide/README.md,"@@ -1,5 +1,60 @@
 ## Upgrade guide
 
+### 4.17.0
+
+#### Java17 is now a supported platform
+
+With [JAVA-3042](https://datastax-oss.atlassian.net/browse/JAVA-3042) the driver is now tested against Java17 as
+well as the other LTS JVM releases Java8 and Java11.  Based on our testing the Java driver appears to function
+normally on Java17, but of course bugs are always a possibility.  Issues arising from the Java17 platform will be
+triaged and addressed just like any other issue.
+
+#### Updated API for vector search
+
+The 4.16.0 release introduced support for the CQL vector datatype (see","[{'comment': ""Nit: do we mention anywhere which cassandra/dse versions are needed for CqlVector? If not should we? I realized when copy and pasting your example into an integration test that I don't know what version of cassandra I need"", 'commenter': 'hhughes'}, {'comment': ""Eventually yes, but for now the code isn't released in any Cassandra or DSE version.  It will be available in Astra but nowhere else yet.\r\n\r\nSo eventually yeah, we'll prolly need to add more detailed information regarding dependencies... but I don't think we're there yet."", 'commenter': 'absurdfarce'}]"
1677,upgrade_guide/README.md,"@@ -1,5 +1,63 @@
 ## Upgrade guide
 
+### 4.17.0
+
+#### Beta support for Java17
+
+With the completion of [JAVA-3042](https://datastax-oss.atlassian.net/browse/JAVA-3042) the driver now passes our
+test suite when run on Java17.  We also now include Java17 in our automated test matrix for Java driver releases.
+All features appear to function normally when run with Java17 but we do not want to declare full support for this
+platform until we've received feedback from other users in the ecosystem.
+
+If you discover an issue with the Java driver running on Java17 please let us know!  Java17 issues will be triaged
+and addressed just like issues from any supported Java version.
+
+#### Updated API for vector search
+
+The 4.16.0 release introduced support for the CQL vector datatype (see
+[JAVA-3060](https://datastax-oss.atlassian.net/browse/JAVA-3060) for more detail).  This release modifies the CqlVector
+value type used to represent a CQL vector to make it easier to use.  CqlVector now implements the Iterable interface
+as well as several methods modelled on the JDK's List interface, so it should have a familiar feel to it.  We've also
+done away with the builder interface, replacing it with some factory methods that resemble similar methods on CqlDuration.
+
+As an example, the following code will create a keyspace and table, populate that table with some data and then execute
+a query which will return a vector type.  This data is retrieved directly via Row.getVector() and the resulting
+CqlVector value object can be interrogated directly.
+
+```java
+try (CqlSession session = new CqlSessionBuilder().withLocalDatacenter(""datacenter1"").build()){
+
+    session.execute(""drop keyspace if exists test"");
+    session.execute(""CREATE KEYSPACE test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1}"");
+    session.execute(""CREATE TABLE test.foo(i int primary key, j vector<float, 3>)"");
+    session.execute(""CREAT CUSTOM INDEX ann_index ON test.foo(j) USING 'StorageAttachedIndex'"");
+    session.execute(""INSERT INTO test.foo (i, j) VALUES (1, [8, 2.3, 58])"");
+    session.execute(""INSERT INTO test.foo (i, j) VALUES (2, [1.2, 3.4, 5.6])"");
+    session.execute(""INSERT INTO test.foo (i, j) VALUES (5, [23, 18, 3.9])"");
+    ResultSet rs=session.execute(""SELECT j FROM test.foo WHERE j ann of [3.4, 7.8, 9.1] limit 1"");
+    for (Row row:rs){","[{'comment': 'Nit: spacing around `:` and before  `{` (and also on lines 28 + 41)', 'commenter': 'hhughes'}, {'comment': ""Agreed, will fix.  Something in one of the automated formatting processes did something to the sample code in this readme... I think it might've been IntelliJ actually.  Everything got heavily concatenated. :("", 'commenter': 'absurdfarce'}]"
1677,upgrade_guide/README.md,"@@ -1,5 +1,63 @@
 ## Upgrade guide
 
+### 4.17.0
+
+#### Beta support for Java17
+
+With the completion of [JAVA-3042](https://datastax-oss.atlassian.net/browse/JAVA-3042) the driver now passes our
+test suite when run on Java17.  We also now include Java17 in our automated test matrix for Java driver releases.
+All features appear to function normally when run with Java17 but we do not want to declare full support for this
+platform until we've received feedback from other users in the ecosystem.
+
+If you discover an issue with the Java driver running on Java17 please let us know!  Java17 issues will be triaged
+and addressed just like issues from any supported Java version.
+
+#### Updated API for vector search
+
+The 4.16.0 release introduced support for the CQL vector datatype (see
+[JAVA-3060](https://datastax-oss.atlassian.net/browse/JAVA-3060) for more detail).  This release modifies the CqlVector
+value type used to represent a CQL vector to make it easier to use.  CqlVector now implements the Iterable interface
+as well as several methods modelled on the JDK's List interface, so it should have a familiar feel to it.  We've also
+done away with the builder interface, replacing it with some factory methods that resemble similar methods on CqlDuration.
+
+As an example, the following code will create a keyspace and table, populate that table with some data and then execute
+a query which will return a vector type.  This data is retrieved directly via Row.getVector() and the resulting
+CqlVector value object can be interrogated directly.
+
+```java
+try (CqlSession session = new CqlSessionBuilder().withLocalDatacenter(""datacenter1"").build()){
+
+    session.execute(""drop keyspace if exists test"");
+    session.execute(""CREATE KEYSPACE test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1}"");
+    session.execute(""CREATE TABLE test.foo(i int primary key, j vector<float, 3>)"");
+    session.execute(""CREAT CUSTOM INDEX ann_index ON test.foo(j) USING 'StorageAttachedIndex'"");
+    session.execute(""INSERT INTO test.foo (i, j) VALUES (1, [8, 2.3, 58])"");
+    session.execute(""INSERT INTO test.foo (i, j) VALUES (2, [1.2, 3.4, 5.6])"");
+    session.execute(""INSERT INTO test.foo (i, j) VALUES (5, [23, 18, 3.9])"");
+    ResultSet rs=session.execute(""SELECT j FROM test.foo WHERE j ann of [3.4, 7.8, 9.1] limit 1"");
+    for (Row row:rs){
+        CqlVector<Float> v = row.getVector(0,Float.class);","[{'comment': 'Nit: spacing after `,`', 'commenter': 'hhughes'}]"
1677,upgrade_guide/README.md,"@@ -1,5 +1,63 @@
 ## Upgrade guide
 
+### 4.17.0
+
+#### Beta support for Java17
+
+With the completion of [JAVA-3042](https://datastax-oss.atlassian.net/browse/JAVA-3042) the driver now passes our
+test suite when run on Java17.  We also now include Java17 in our automated test matrix for Java driver releases.
+All features appear to function normally when run with Java17 but we do not want to declare full support for this
+platform until we've received feedback from other users in the ecosystem.
+
+If you discover an issue with the Java driver running on Java17 please let us know!  Java17 issues will be triaged
+and addressed just like issues from any supported Java version.
+
+#### Updated API for vector search
+
+The 4.16.0 release introduced support for the CQL vector datatype (see
+[JAVA-3060](https://datastax-oss.atlassian.net/browse/JAVA-3060) for more detail).  This release modifies the CqlVector
+value type used to represent a CQL vector to make it easier to use.  CqlVector now implements the Iterable interface
+as well as several methods modelled on the JDK's List interface, so it should have a familiar feel to it.  We've also
+done away with the builder interface, replacing it with some factory methods that resemble similar methods on CqlDuration.
+
+As an example, the following code will create a keyspace and table, populate that table with some data and then execute
+a query which will return a vector type.  This data is retrieved directly via Row.getVector() and the resulting
+CqlVector value object can be interrogated directly.
+
+```java
+try (CqlSession session = new CqlSessionBuilder().withLocalDatacenter(""datacenter1"").build()){
+
+    session.execute(""drop keyspace if exists test"");
+    session.execute(""CREATE KEYSPACE test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1}"");
+    session.execute(""CREATE TABLE test.foo(i int primary key, j vector<float, 3>)"");
+    session.execute(""CREAT CUSTOM INDEX ann_index ON test.foo(j) USING 'StorageAttachedIndex'"");
+    session.execute(""INSERT INTO test.foo (i, j) VALUES (1, [8, 2.3, 58])"");
+    session.execute(""INSERT INTO test.foo (i, j) VALUES (2, [1.2, 3.4, 5.6])"");
+    session.execute(""INSERT INTO test.foo (i, j) VALUES (5, [23, 18, 3.9])"");
+    ResultSet rs=session.execute(""SELECT j FROM test.foo WHERE j ann of [3.4, 7.8, 9.1] limit 1"");
+    for (Row row:rs){
+        CqlVector<Float> v = row.getVector(0,Float.class);
+        System.out.println(v);
+        if (Iterables.size(v) != 3){","[{'comment': 'Nit: Is this size check necessary for the code snippet?', 'commenter': 'hhughes'}, {'comment': ""It's not strictly necessary.  I added it here to show that v implements Iterable and can be used in anything expecting that interface.  The other obvious way to do that was to iterate over it in a for loop but it wasn't clear to me that that would be a useful exercise for a vector."", 'commenter': 'absurdfarce'}]"
1677,upgrade_guide/README.md,"@@ -1,5 +1,63 @@
 ## Upgrade guide
 
+### 4.17.0
+
+#### Beta support for Java17
+
+With the completion of [JAVA-3042](https://datastax-oss.atlassian.net/browse/JAVA-3042) the driver now passes our
+test suite when run on Java17.  We also now include Java17 in our automated test matrix for Java driver releases.
+All features appear to function normally when run with Java17 but we do not want to declare full support for this
+platform until we've received feedback from other users in the ecosystem.
+
+If you discover an issue with the Java driver running on Java17 please let us know!  Java17 issues will be triaged
+and addressed just like issues from any supported Java version.
+
+#### Updated API for vector search
+
+The 4.16.0 release introduced support for the CQL vector datatype (see
+[JAVA-3060](https://datastax-oss.atlassian.net/browse/JAVA-3060) for more detail).  This release modifies the CqlVector
+value type used to represent a CQL vector to make it easier to use.  CqlVector now implements the Iterable interface
+as well as several methods modelled on the JDK's List interface, so it should have a familiar feel to it.  We've also
+done away with the builder interface, replacing it with some factory methods that resemble similar methods on CqlDuration.
+
+As an example, the following code will create a keyspace and table, populate that table with some data and then execute
+a query which will return a vector type.  This data is retrieved directly via Row.getVector() and the resulting
+CqlVector value object can be interrogated directly.
+
+```java
+try (CqlSession session = new CqlSessionBuilder().withLocalDatacenter(""datacenter1"").build()){
+
+    session.execute(""drop keyspace if exists test"");
+    session.execute(""CREATE KEYSPACE test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1}"");
+    session.execute(""CREATE TABLE test.foo(i int primary key, j vector<float, 3>)"");
+    session.execute(""CREAT CUSTOM INDEX ann_index ON test.foo(j) USING 'StorageAttachedIndex'"");
+    session.execute(""INSERT INTO test.foo (i, j) VALUES (1, [8, 2.3, 58])"");
+    session.execute(""INSERT INTO test.foo (i, j) VALUES (2, [1.2, 3.4, 5.6])"");
+    session.execute(""INSERT INTO test.foo (i, j) VALUES (5, [23, 18, 3.9])"");
+    ResultSet rs=session.execute(""SELECT j FROM test.foo WHERE j ann of [3.4, 7.8, 9.1] limit 1"");
+    for (Row row:rs){
+        CqlVector<Float> v = row.getVector(0,Float.class);
+        System.out.println(v);
+        if (Iterables.size(v) != 3){
+            throw new RuntimeException(""Expected vector with three dimensions"");
+        }
+    }
+}
+```
+
+You can also use the CqlVector type with prepared statements:
+
+```java
+PreparedStatement preparedInsert = session.prepare(""insert into test.foo (i, j) values (?,?)"");","[{'comment': 'Nit: casing of cql keywords', 'commenter': 'hhughes'}]"
1677,upgrade_guide/README.md,"@@ -1,5 +1,63 @@
 ## Upgrade guide
 
+### 4.17.0
+
+#### Beta support for Java17
+
+With the completion of [JAVA-3042](https://datastax-oss.atlassian.net/browse/JAVA-3042) the driver now passes our
+test suite when run on Java17.  We also now include Java17 in our automated test matrix for Java driver releases.
+All features appear to function normally when run with Java17 but we do not want to declare full support for this
+platform until we've received feedback from other users in the ecosystem.
+
+If you discover an issue with the Java driver running on Java17 please let us know!  Java17 issues will be triaged
+and addressed just like issues from any supported Java version.","[{'comment': ""```suggestion\r\nWith the completion of [JAVA-3042](https://datastax-oss.atlassian.net/browse/JAVA-3042), the driver now passes our automated test matrix for Java driver releases.\r\nWhile all features function normally when run with Java 17 tests, we do not offer full support for this\r\nplatform until we've received feedback from other users in the ecosystem.\r\n\r\nIf you discover an issue with the Java driver running on Java 17, please let us know. We will traige and address Java 17 issues.\r\n```"", 'commenter': 'jgillenwater'}, {'comment': 'How should they let us know? Is there an email address we can include?', 'commenter': 'jgillenwater'}, {'comment': ""Any of the regular reporting mechanisms would work.  Probably the default way would be to contact support but posting tickets to JIRA, posting to the Google group/mailing list, etc.... any of those would be fine.\r\n\r\nI didn't specify 'cause I didn't want to create the perception of limiting it to any one mechanism.  I'm open to other ideas here, though."", 'commenter': 'absurdfarce'}]"
1677,upgrade_guide/README.md,"@@ -1,5 +1,63 @@
 ## Upgrade guide
 
+### 4.17.0
+
+#### Beta support for Java17
+
+With the completion of [JAVA-3042](https://datastax-oss.atlassian.net/browse/JAVA-3042) the driver now passes our
+test suite when run on Java17.  We also now include Java17 in our automated test matrix for Java driver releases.
+All features appear to function normally when run with Java17 but we do not want to declare full support for this
+platform until we've received feedback from other users in the ecosystem.
+
+If you discover an issue with the Java driver running on Java17 please let us know!  Java17 issues will be triaged
+and addressed just like issues from any supported Java version.
+
+#### Updated API for vector search
+
+The 4.16.0 release introduced support for the CQL vector datatype (see
+[JAVA-3060](https://datastax-oss.atlassian.net/browse/JAVA-3060) for more detail).  This release modifies the CqlVector
+value type used to represent a CQL vector to make it easier to use.  CqlVector now implements the Iterable interface
+as well as several methods modelled on the JDK's List interface, so it should have a familiar feel to it.  We've also
+done away with the builder interface, replacing it with some factory methods that resemble similar methods on CqlDuration.
+
+As an example, the following code will create a keyspace and table, populate that table with some data and then execute
+a query which will return a vector type.  This data is retrieved directly via Row.getVector() and the resulting
+CqlVector value object can be interrogated directly.","[{'comment': ""```suggestion\r\nThe 4.16.0 release introduced support for the CQL `vector` datatype. This release modifies the `CqlVector`\r\nvalue type used to represent a CQL vector to make it easier to use.  `CqlVector` now implements the Iterable interface\r\nas well as several methods modelled on the JDK's List interface. For more, see\r\n[JAVA-3060](https://datastax-oss.atlassian.net/browse/JAVA-3060). \r\n\r\nThe builder interface was replaced with factory methods that resemble similar methods on `CqlDuration`.\r\nFor example, the following code will create a keyspace and table, populate that table with some data, and then execute\r\na query which will return a `vector` type.  This data is retrieved directly via `Row.getVector()` and the resulting\r\n`CqlVector` value object can be interrogated directly.\r\n```"", 'commenter': 'jgillenwater'}]"
1677,upgrade_guide/README.md,"@@ -1,5 +1,63 @@
 ## Upgrade guide
 
+### 4.17.0
+
+#### Beta support for Java17
+
+With the completion of [JAVA-3042](https://datastax-oss.atlassian.net/browse/JAVA-3042) the driver now passes our
+test suite when run on Java17.  We also now include Java17 in our automated test matrix for Java driver releases.
+All features appear to function normally when run with Java17 but we do not want to declare full support for this
+platform until we've received feedback from other users in the ecosystem.
+
+If you discover an issue with the Java driver running on Java17 please let us know!  Java17 issues will be triaged
+and addressed just like issues from any supported Java version.
+
+#### Updated API for vector search
+
+The 4.16.0 release introduced support for the CQL vector datatype (see
+[JAVA-3060](https://datastax-oss.atlassian.net/browse/JAVA-3060) for more detail).  This release modifies the CqlVector
+value type used to represent a CQL vector to make it easier to use.  CqlVector now implements the Iterable interface
+as well as several methods modelled on the JDK's List interface, so it should have a familiar feel to it.  We've also
+done away with the builder interface, replacing it with some factory methods that resemble similar methods on CqlDuration.
+
+As an example, the following code will create a keyspace and table, populate that table with some data and then execute
+a query which will return a vector type.  This data is retrieved directly via Row.getVector() and the resulting
+CqlVector value object can be interrogated directly.
+
+```java
+try (CqlSession session = new CqlSessionBuilder().withLocalDatacenter(""datacenter1"").build()){
+
+    session.execute(""drop keyspace if exists test"");
+    session.execute(""CREATE KEYSPACE test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1}"");
+    session.execute(""CREATE TABLE test.foo(i int primary key, j vector<float, 3>)"");
+    session.execute(""CREAT CUSTOM INDEX ann_index ON test.foo(j) USING 'StorageAttachedIndex'"");
+    session.execute(""INSERT INTO test.foo (i, j) VALUES (1, [8, 2.3, 58])"");
+    session.execute(""INSERT INTO test.foo (i, j) VALUES (2, [1.2, 3.4, 5.6])"");
+    session.execute(""INSERT INTO test.foo (i, j) VALUES (5, [23, 18, 3.9])"");
+    ResultSet rs=session.execute(""SELECT j FROM test.foo WHERE j ann of [3.4, 7.8, 9.1] limit 1"");
+    for (Row row:rs){
+        CqlVector<Float> v = row.getVector(0,Float.class);
+        System.out.println(v);
+        if (Iterables.size(v) != 3){
+            throw new RuntimeException(""Expected vector with three dimensions"");
+        }
+    }
+}
+```
+
+You can also use the CqlVector type with prepared statements:","[{'comment': '```suggestion\r\nYou can also use the `CqlVector` type with prepared statements:\r\n```', 'commenter': 'jgillenwater'}]"
1677,upgrade_guide/README.md,"@@ -1,5 +1,63 @@
 ## Upgrade guide
 
+### 4.17.0
+
+#### Beta support for Java17
+
+With the completion of [JAVA-3042](https://datastax-oss.atlassian.net/browse/JAVA-3042) the driver now passes our
+test suite when run on Java17.  We also now include Java17 in our automated test matrix for Java driver releases.
+All features appear to function normally when run with Java17 but we do not want to declare full support for this
+platform until we've received feedback from other users in the ecosystem.
+
+If you discover an issue with the Java driver running on Java17 please let us know!  Java17 issues will be triaged
+and addressed just like issues from any supported Java version.
+
+#### Updated API for vector search
+
+The 4.16.0 release introduced support for the CQL vector datatype (see
+[JAVA-3060](https://datastax-oss.atlassian.net/browse/JAVA-3060) for more detail).  This release modifies the CqlVector
+value type used to represent a CQL vector to make it easier to use.  CqlVector now implements the Iterable interface
+as well as several methods modelled on the JDK's List interface, so it should have a familiar feel to it.  We've also
+done away with the builder interface, replacing it with some factory methods that resemble similar methods on CqlDuration.
+
+As an example, the following code will create a keyspace and table, populate that table with some data and then execute
+a query which will return a vector type.  This data is retrieved directly via Row.getVector() and the resulting
+CqlVector value object can be interrogated directly.
+
+```java
+try (CqlSession session = new CqlSessionBuilder().withLocalDatacenter(""datacenter1"").build()){
+
+    session.execute(""drop keyspace if exists test"");
+    session.execute(""CREATE KEYSPACE test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1}"");
+    session.execute(""CREATE TABLE test.foo(i int primary key, j vector<float, 3>)"");
+    session.execute(""CREAT CUSTOM INDEX ann_index ON test.foo(j) USING 'StorageAttachedIndex'"");
+    session.execute(""INSERT INTO test.foo (i, j) VALUES (1, [8, 2.3, 58])"");
+    session.execute(""INSERT INTO test.foo (i, j) VALUES (2, [1.2, 3.4, 5.6])"");
+    session.execute(""INSERT INTO test.foo (i, j) VALUES (5, [23, 18, 3.9])"");
+    ResultSet rs=session.execute(""SELECT j FROM test.foo WHERE j ann of [3.4, 7.8, 9.1] limit 1"");
+    for (Row row:rs){
+        CqlVector<Float> v = row.getVector(0,Float.class);
+        System.out.println(v);
+        if (Iterables.size(v) != 3){
+            throw new RuntimeException(""Expected vector with three dimensions"");
+        }
+    }
+}
+```
+
+You can also use the CqlVector type with prepared statements:
+
+```java
+PreparedStatement preparedInsert = session.prepare(""insert into test.foo (i, j) values (?,?)"");
+CqlVector<Float> vector = CqlVector.newInstance(1.4f, 2.5f, 3.6f);
+session.execute(preparedInsert.bind(3, vector));
+```
+
+In some cases it may be more appropriate to access the vector directly as an array of some numerical type.  This version
+supports such use cases by providing a codec which translates a CQL vector to and from a primitive array.  In this release
+only float arrays are supported but additional codecs may be added for other primitive numerical types in future releases.
+You can find more information about this codec in the manual documentation on [custom codecs](../manual/core/custom_codecs/)","[{'comment': '```suggestion\r\nIn some cases, it makes sense to access the vector directly as an array of some numerical type. This version\r\nsupports such use cases by providing a codec which translates a CQL vector to and from a primitive array. Only float arrays are supported. \r\nYou can find more information about this codec in the manual documentation on [custom codecs](../manual/core/custom_codecs/)\r\n```', 'commenter': 'jgillenwater'}]"
1677,upgrade_guide/README.md,"@@ -1,5 +1,60 @@
 ## Upgrade guide
 
+### 4.17.0
+
+#### Beta support for Java17
+
+With the completion of [JAVA-3042](https://datastax-oss.atlassian.net/browse/JAVA-3042), the driver now passes our automated test matrix for Java driver releases.
+While all features function normally when run with Java 17 tests, we do not offer full support for this
+platform until we've received feedback from other users in the ecosystem.
+
+If you discover an issue with the Java driver running on Java 17, please let us know. We will triage and address Java 17 issues.
+
+#### Updated API for vector search
+
+The 4.16.0 release introduced support for the CQL `vector` datatype. This release modifies the `CqlVector`
+value type used to represent a CQL vector to make it easier to use.  `CqlVector` now implements the Iterable interface
+as well as several methods modelled on the JDK's List interface. For more, see
+[JAVA-3060](https://datastax-oss.atlassian.net/browse/JAVA-3060). 
+
+The builder interface was replaced with factory methods that resemble similar methods on `CqlDuration`.
+For example, the following code will create a keyspace and table, populate that table with some data, and then execute
+a query which will return a `vector` type.  This data is retrieved directly via `Row.getVector()` and the resulting
+`CqlVector` value object can be interrogated directly.
+
+```java
+try (CqlSession session = new CqlSessionBuilder().withLocalDatacenter(""datacenter1"").build()) {
+
+    session.execute(""drop keyspace if exists test"");","[{'comment': 'nit: is the drop keyspace line neccessary for this example?', 'commenter': 'hhughes'}]"
1677,manual/core/custom_codecs/README.md,"@@ -256,6 +256,18 @@ that maps instances of that class to Json strings, using a newly-allocated, defa
 It is also possible to pass a custom `ObjectMapper` instance using [ExtraTypeCodecs.json(Class,
 ObjectMapper)] instead.
 
+#### Mapping CQL vectors to Java array
+
+By default, the driver maps CQL `vector` to the [CqlVector] value type. If you prefer to deal with
+arrays, the driver offers the following codec:
+
+| Codec                                     | CQL type        | Java type |
+|-------------------------------------------|-----------------|-----------|
+| [ExtraTypeCodecs.floatVectorToArray(int)] | `vector<float>` | `float[]` |
+
+This release only provides a codec for vectors and arrays containing float values.  Support for other primitive
+numerical types may be added in future releases.","[{'comment': ""```suggestion\r\nThis release only provides a codec for vectors and arrays containing float values. \r\n```\r\n\r\nWe don't predict what might be released later in the docs. "", 'commenter': 'jgillenwater'}]"
1677,upgrade_guide/README.md,"@@ -1,5 +1,60 @@
 ## Upgrade guide
 
+### 4.17.0
+
+#### Beta support for Java17
+
+With the completion of [JAVA-3042](https://datastax-oss.atlassian.net/browse/JAVA-3042), the driver now passes our automated test matrix for Java driver releases.
+While all features function normally when run with Java 17 tests, we do not offer full support for this
+platform until we've received feedback from other users in the ecosystem.
+
+If you discover an issue with the Java driver running on Java 17, please let us know. We will triage and address Java 17 issues.","[{'comment': 'How are they supposed to let us know?', 'commenter': 'jgillenwater'}]"
1677,upgrade_guide/README.md,"@@ -1,5 +1,60 @@
 ## Upgrade guide
 
+### 4.17.0
+
+#### Beta support for Java17
+
+With the completion of [JAVA-3042](https://datastax-oss.atlassian.net/browse/JAVA-3042), the driver now passes our automated test matrix for Java driver releases.
+While all features function normally when run with Java 17 tests, we do not offer full support for this
+platform until we've received feedback from other users in the ecosystem.
+
+If you discover an issue with the Java driver running on Java 17, please let us know. We will triage and address Java 17 issues.
+
+#### Updated API for vector search
+
+The 4.16.0 release introduced support for the CQL `vector` datatype. This release modifies the `CqlVector`
+value type used to represent a CQL vector to make it easier to use.  `CqlVector` now implements the Iterable interface
+as well as several methods modelled on the JDK's List interface. For more, see
+[JAVA-3060](https://datastax-oss.atlassian.net/browse/JAVA-3060). 
+
+The builder interface was replaced with factory methods that resemble similar methods on `CqlDuration`.
+For example, the following code will create a keyspace and table, populate that table with some data, and then execute
+a query which will return a `vector` type.  This data is retrieved directly via `Row.getVector()` and the resulting","[{'comment': '```suggestion\r\na query that will return a `vector` type.  This data is retrieved directly via `Row.getVector()` and the resulting\r\n```', 'commenter': 'jgillenwater'}]"
1689,metrics/micrometer/src/main/java/com/datastax/oss/driver/internal/metrics/micrometer/MicrometerNodeMetricUpdater.java,"@@ -97,7 +97,7 @@ protected void cancelMetricsExpirationTimeout() {
   protected Timer.Builder configureTimer(Timer.Builder builder, NodeMetric metric, MetricId id) {
     DriverExecutionProfile profile = context.getConfig().getDefaultProfile();
     if (metric == DefaultNodeMetric.CQL_MESSAGES) {
-      return builder
+      builder
           .publishPercentileHistogram()","[{'comment': 'I feel like we\'re only getting at half the solution if we leave this method call in unmodified.\r\n\r\nI agree with your point that you can avoid the _bucket vals if you specify something in publish-percentiles but in that case you\'re (a) asking Micrometer to collect data for a histogram that you know up front you\'ll never use and (b) having to build logic in to select only certain tags for scraping.  Neither of those seem great.\r\n\r\nYou can address the second point by changing the export process to remove the histogram data... say by pruning out any line that contains the literal string ""_bucket"".  But (c) you\'re now introducing a lot of string processing every time you scrape metrics and (d) you haven\'t really addressed point (a) above; there\'s really no reason for Micrometer to collect this data if you aren\'t going to use it.\r\n\r\nSo really I think you\'re asking for _two_ things here:\r\n\r\n1. The ability to specify percentiles for timers (and to have those percentiles published)\r\n2. The ability to shut off the percentile histogram if desired\r\n\r\nYour PR implements the first, but it sure seems like you should have the second as well to really solve this problem.\r\n', 'commenter': 'absurdfarce'}, {'comment': 'Regarding second problem. we can always whitelist what kind of metrics we allow. That is what we are doing in Walmart.  so we already have solution for second. So also this is configurable and adding new 3 metrics series. \r\n', 'commenter': 'nparaddi-walmart'}, {'comment': 'also am ok to remove publishPercentileHistogram if percentiles are defined. let me know if that is the path we want to go.', 'commenter': 'nparaddi-walmart'}, {'comment': 'I agree with @absurdfarce - if you\'re trying to turn off the metrics produced by `publishPercentileHistogram` because it\'s producing too many values there will be others in the same situation too. As pointed out earlier there is an efficiency benefit of not having those produced in the first place if they are not needed, and additionally I think there is a usability benefit of having that configured directly in the driver, rather than directing users to another configuration system to strip out these the metrics they don\'t want.\r\n\r\nI also wonder that since micrometer supports publishing both agregable histogram and local histogram metrics simultaneously that having the driver configure only one or the other might be too restrictive. Instead, perhaps you could add a new config entry for toggling `publishPercentileHistogram` in each scenario, maybe named something along the lines of ""publish aggregable histogram"" to contrast with the `publishPercentiles` settings which could be re-named ""publish local percentiles""?', 'commenter': 'hhughes'}, {'comment': ' Yes. because too many metrics, none of the walmart teams are enabled the metrics.\r\nAs i understand correctly, you would like two config one for ""publish local percentiles"" and another one for """"publish aggregable histogram""\r\ni am ok to that.. correct me if I am wrong.\r\n\r\n\r\n', 'commenter': 'nparaddi-walmart'}, {'comment': 'Another general clarification i would like to see that is this going to be merged before handing to over apache foundation?', 'commenter': 'nparaddi-walmart'}, {'comment': ""After some more thought and discussion on our side I think we've landed on one new configuration option which switches aggregable histogram generation on/off for all metric flavors [default=on], in addition to the per-metric publishPercentile settings you have already pulled together in this PR. If you could add that to this PR and tackle some of the other newly added feedback around doc strings then I think we'll be good to go here :)"", 'commenter': 'hhughes'}, {'comment': 'Thank you. added addtional configuration option for aggregable histogram generation', 'commenter': 'nparaddi-walmart'}, {'comment': 'Note could not add junit cases for this new configuration as micrometer SimpleMeterRegistry does not generate _bucket metrics . I have changes ready to add junit test cases but it requires PrometheusMeterRegistry( prometheus dependency). I can add those changes this PR if needed', 'commenter': 'nparaddi-walmart'}, {'comment': 'i have this junit test case id different branch. It needs micrometer-registry-prometheus dependecy. I can pull junit cases in this PR if thats ok\r\nhttps://github.com/nparaddi-walmart/java-driver/blob/f5a9b39ff37831a61b8dcd0f4ba5e871fa2ccf8c/metrics/micrometer/src/test/java/com/datastax/oss/driver/internal/metrics/micrometer/MicrometerNodeMetricUpdaterTest.java#L294', 'commenter': 'nparaddi-walmart'}]"
1689,core/src/main/resources/reference.conf,"@@ -1578,6 +1578,13 @@ datastax-java-driver {
         # Valid for: Micrometer.
         // slo = [ 100 milliseconds, 500 milliseconds, 1 second ]
         
+        # An optional list of percentiles to be published by Micrometer.
+        #
+        # If defined, the histogram is guaranteed to contain these percentiles alongside other
+        # buckets used to generate aggregable percentile approximations.","[{'comment': 'We should probably be very explicit here in saying that these percentiles can **_NOT_** be aggregated across multiple nodes in the way that percentile histograms can.', 'commenter': 'absurdfarce'}, {'comment': 'thanks. added comment', 'commenter': 'nparaddi-walmart'}]"
1689,metrics/micrometer/src/main/java/com/datastax/oss/driver/internal/metrics/micrometer/MicrometerNodeMetricUpdater.java,"@@ -111,8 +111,17 @@ protected Timer.Builder configureTimer(Timer.Builder builder, NodeMetric metric,
                   : null)
           .percentilePrecision(
               profile.getInt(DefaultDriverOption.METRICS_NODE_CQL_MESSAGES_DIGITS));
+
+      if (profile.isDefined(DefaultDriverOption.METRICS_NODE_CQL_MESSAGES_PUBLISH_PERCENTILES)) {
+        builder.publishPercentiles(
+            profile.getDoubleList(DefaultDriverOption.METRICS_NODE_CQL_MESSAGES_PUBLISH_PERCENTILES)
+                .stream()
+                .mapToDouble(Double::doubleValue)
+                .toArray());
+      }","[{'comment': ""There's already a lot of repetition in this file but let's start to minimize it by abstracting this out to a helper method:\r\n\r\n```java\r\nprivate double[] toDoubleArray(List<Double> doubleList) {                                          \r\n  return doubleList.stream().mapToDouble(Double::doubleValue).toArray();               \r\n} \r\n```\r\n\r\nI would've thought we could have avoided some of the intermediate object creation here by just calling publishPercentiles() multiple times on the builder:\r\n\r\n```java\r\n       if (profile.isDefined(DseDriverOption.METRICS_SESSION_GRAPH_REQUESTS_PUBLISH_PERCENTILES)) {\r\n         profile          \r\n           .getDoubleList(DseDriverOption.METRICS_SESSION_GRAPH_REQUESTS_PUBLISH_PERCENTILES)\r\n           .forEach(builder::publishPercentiles);\r\n       } \r\n```\r\n\r\nBut weirdly that didn't work.  With this change the unit tests fail to pass, claiming that this assert:\r\n\r\n```java\r\n    assertThat(snapshot.percentileValues()).hasSize(3);\r\n```\r\n\r\nis only finding one item rather than three... which suggests that percentiles aren't just added together."", 'commenter': 'absurdfarce'}, {'comment': 'Thank you. added helper method. let me know anything else needed', 'commenter': 'nparaddi-walmart'}, {'comment': 'optional: it would be nice not to have to repeat the driver option name in the conditional and in the getter, particularly with these long similar names, consider breaking the repeated pattern of `if(isDefined(option)) { ... profile.getDoubleList(option) ...` into a method', 'commenter': 'hhughes'}, {'comment': 'created separate static method', 'commenter': 'nparaddi-walmart'}]"
1689,core/src/main/java/com/datastax/oss/driver/api/core/config/TypedDriverOption.java,"@@ -409,6 +409,12 @@ public String toString() {
       new TypedDriverOption<>(
           DefaultDriverOption.METRICS_SESSION_CQL_REQUESTS_SLO,
           GenericType.listOf(GenericType.DURATION));
+  /** Optional pre-defined percentile of cql resquests to publish, as a list of percentiles . */","[{'comment': 'nit: `resquests` is mis-spelled', 'commenter': 'hhughes'}, {'comment': 'fixed it', 'commenter': 'nparaddi-walmart'}]"
1689,core/src/main/java/com/datastax/oss/driver/api/core/config/DefaultDriverOption.java,"@@ -939,6 +939,30 @@ public enum DefaultDriverOption implements DriverOption {
    * <p>Value-type: List of {@link String}
    */
   METADATA_SCHEMA_CHANGE_LISTENER_CLASSES(""advanced.schema-change-listener.classes""),
+  /**
+   * List of percentiles in double to be published by the cql-requests metric. 95th percentile to
+   * represented as 0.95.
+   *
+   * <p>Value type: {@link java.util.List List}&#60;{@link Double}&#62;
+   */
+  METRICS_SESSION_CQL_REQUESTS_PUBLISH_PERCENTILES(
+      ""advanced.metrics.session.cql-requests.publish-percentiles""),
+  /**
+   * List of percentiles in double to be published by the node cql-messages metric. 95th percentile
+   * to represented as 0.95.
+   *
+   * <p>Value type: {@link java.util.List List}&#60;{@link Double}&#62;
+   */
+  METRICS_NODE_CQL_MESSAGES_PUBLISH_PERCENTILES(
+      ""advanced.metrics.node.cql-messages.publish-percentiles""),
+  /**
+   * List of percentiles in double to be published by the throttling delay metric. 95th percentile
+   * to represented as 0.95.","[{'comment': ""Can you rework the javadoc descriptions for enums in this class and in `DseDriverOption.java`? They are a little hard to understand. For example I don't think we need to repeat that the value type as this is stated in the `Value type:` section of the description."", 'commenter': 'hhughes'}, {'comment': 'updated the java doc', 'commenter': 'nparaddi-walmart'}]"
1689,metrics/micrometer/src/main/java/com/datastax/oss/driver/internal/metrics/micrometer/MicrometerMetricUpdater.java,"@@ -159,4 +160,8 @@ protected DistributionSummary.Builder configureDistributionSummary(
       DistributionSummary.Builder builder, MetricT metric, MetricId id) {
     return builder.publishPercentileHistogram();
   }
+
+  protected double[] toDoubleArray(List<Double> doubleList) {","[{'comment': 'nit: this could be static', 'commenter': 'hhughes'}, {'comment': 'moved to static', 'commenter': 'nparaddi-walmart'}]"
1689,metrics/micrometer/src/test/java/com/datastax/oss/driver/internal/metrics/micrometer/MicrometerNodeMetricUpdaterTest.java,"@@ -190,6 +195,60 @@ public void should_create_timer(
     assertThat(timer.count()).isEqualTo(10);
     HistogramSnapshot snapshot = timer.takeSnapshot();
     assertThat(snapshot.histogramCounts()).hasSize(2);
+    assertThat(snapshot.percentileValues()).hasSize(3);","[{'comment': 'nit: it would provide better coverage if you could assert the exact set of percentiles you expect here instead of just the number of items (i.e. `[0.75, 0.95, 0.99]`) - same feedback to the other test in `MicrometerSessionMetricUpdaterTest`', 'commenter': 'hhughes'}, {'comment': 'added the validation', 'commenter': 'nparaddi-walmart'}]"
1689,metrics/micrometer/src/test/java/com/datastax/oss/driver/internal/metrics/micrometer/MicrometerNodeMetricUpdaterTest.java,"@@ -174,6 +175,10 @@ public void should_create_timer(
     when(profile.isDefined(sla)).thenReturn(true);
     when(profile.getDurationList(sla))
         .thenReturn(Arrays.asList(Duration.ofMillis(100), Duration.ofMillis(500)));
+    when(profile.isDefined(percentiles)).thenReturn(true);
+    when(profile.getDoubleList(percentiles))
+        .thenReturn(
+            Arrays.asList(Double.valueOf(0.75), Double.valueOf(0.95), Double.valueOf(0.99)));","[{'comment': 'nit: unneccessary boxing of doubles (and again in the other tests)', 'commenter': 'hhughes'}, {'comment': 'removed boxing', 'commenter': 'nparaddi-walmart'}]"
1729,docs.yaml,"@@ -143,3 +143,46 @@ versions:
     ref: '2.1'
   - name: '2.0'
     ref: '2.0'
+checks:
+  external_links:
+    exclude:
+      - 'https://twitter.com/dsJavaDriver'
+      - 'https://twitter.com/datastaxeng'
+      - 'https://projectreactor.io'
+      - 'https://docs.datastax.com/en/drivers/java/4.[0-9]+/com/datastax/oss/driver/internal/'
+      - 'http://www.planetcassandra.org/blog/user-defined-functions-in-cassandra-3-0/'
+      - 'http://www.planetcassandra.org/making-the-change-from-thrift-to-cql/'
+      - 'https://academy.datastax.com/slack'
+      - 'https://micrometer.io/docs'
+      - 'http://datastax.github.io/java-driver/features/shaded_jar/'
+  internal_links:
+    exclude:
+      - 'netty_pipeline/'
+      - '../core/'
+      - '%5Bguava%20eviction%5D'
+
+rewrites:
+  - search: 'https://(helpdocs|docs).datastax.com/(en/astra/)?(aws|gcp)/(dscloud/apollo|doc/dscloud/astra)/dscloudGettingStarted.html'
+    replace: 'https://awesome-astra.github.io/docs/pages/astra/'
+  - search: 'https://(helpdocs|docs).datastax.com/(en/astra/)?(aws|gcp)/(dscloud/apollo|doc/dscloud/astra)/dscloudObtainingCredentials.html'
+    replace: 'https://awesome-astra.github.io/docs/pages/astra/download-scb/'
+  - search: 'https://(helpdocs|docs).datastax.com/(en/astra/)?(aws|gcp)/(dscloud/apollo|doc/dscloud/astra)/dscloudShareClusterDetails.html'
+    replace: 'https://awesome-astra.github.io/docs/pages/astra/create-token/'
+  - search: 'http://downloads.datastax.com/java-driver/'
+    replace: 'https://downloads.datastax.com/#datastax-drivers'
+  - search: 'https://docs.datastax.com/en/drivers/java/(4.[0-9]+)/com/datastax/oss/driver/api/mapper/EntityHelper.html'
+    replace: 'https://docs.datastax.com/en/drivers/java/\1/com/datastax/oss/driver/api/mapper/entity/EntityHelper.html'
+  - search: '(http|https)://www.datastax.com/drivers/java/'
+    replace: 'https://docs.datastax.com/en/drivers/java/'
+  - search: 'http://docs.datastax.com/en/drivers/java'
+    replace: 'https://docs.datastax.com/en/drivers/java'
+  - search: 'https://docs.astra.datastax.com/docs/creating-your-astra-database'
+    replace: 'https://awesome-astra.github.io/docs/pages/astra/create-instance/'
+  - search: 'https://docs.astra.datastax.com/docs/obtaining-database-credentials'
+    replace: 'https://awesome-astra.github.io/docs/pages/astra/download-scb/'
+  - search: 'https://docs.datastax.com/en/drivers/java/(4.[0-9]+)/com/datastax/oss/driver/api/core/cql/SyncCqlSession.html%60'
+    replace: 'https://docs.datastax.com/en/drivers/java/\1/com/datastax/oss/driver/api/core/cql/SyncCqlSession.html'","[{'comment': ""```suggestion\r\n  - search: 'https://(helpdocs|docs).datastax.com/(en/astra/)?(aws|gcp)/(dscloud/apollo|doc/dscloud/astra)/dscloudGettingStarted.html'\r\n    replace: 'https://docs.datastax.com/en/astra-serverless/docs/getting-started/getting-started.html'\r\n  - search: 'https://(helpdocs|docs).datastax.com/(en/astra/)?(aws|gcp)/(dscloud/apollo|doc/dscloud/astra)/dscloudObtainingCredentials.html'\r\n    replace: 'https://docs.datastax.com/en/astra-serverless/docs/connect/secure-connect-bundle.html'\r\n  - search: 'https://(helpdocs|docs).datastax.com/(en/astra/)?(aws|gcp)/(dscloud/apollo|doc/dscloud/astra)/dscloudShareClusterDetails.html'\r\n    replace: 'https://docs.datastax.com/en/astra-serverless/docs/manage/org/manage-tokens.html'\r\n  - search: 'http://downloads.datastax.com/java-driver/'\r\n    replace: 'https://downloads.datastax.com/#datastax-drivers'\r\n  - search: 'https://docs.datastax.com/en/drivers/java/(4.[0-9]+)/com/datastax/oss/driver/api/mapper/EntityHelper.html'\r\n    replace: 'https://docs.datastax.com/en/drivers/java/\\1/com/datastax/oss/driver/api/mapper/entity/EntityHelper.html'\r\n  - search: '(http|https)://www.datastax.com/drivers/java/'\r\n    replace: 'https://docs.datastax.com/en/drivers/java/'\r\n  - search: 'http://docs.datastax.com/en/drivers/java'\r\n    replace: 'https://docs.datastax.com/en/drivers/java'\r\n  - search: 'https://docs.astra.datastax.com/docs/creating-your-astra-database'\r\n    replace: 'https://docs.datastax.com/en/astra-serverless/docs/getting-started/create-db-choices.html'\r\n  - search: 'https://docs.astra.datastax.com/docs/obtaining-database-credentials'\r\n    replace: 'https://docs.datastax.com/en/astra-serverless/docs/connect/secure-connect-bundle.html'\r\n  - search: 'https://docs.datastax.com/en/drivers/java/(4.[0-9]+)/com/datastax/oss/driver/api/core/cql/SyncCqlSession.html%60'\r\n    replace: 'https://docs.datastax.com/en/drivers/java/\\1/com/datastax/oss/driver/api/core/cql/SyncCqlSession.html'\r\n```"", 'commenter': 'jgillenwater'}]"
