Pull,Path,Diff_hunk,Comment
28,ambari-server/src/main/python/ambari_server_main.py,"@@ -60,6 +60,7 @@
     ""-XX:+UseConcMarkSweepGC "" + \
     ""-XX:-UseGCOverheadLimit -XX:CMSInitiatingOccupancyFraction=60 "" \
     ""-Dsun.zip.disableMemoryMapping=true "" + \
+    ""-Djava.security.egd=file:/dev/./urandom "" + \","[{'comment': ""Although it might be true that reading from /dev/urandom is faster since it is non-blocking, we must understand that it is non-blocking because it's not doing the work to generate entropy for randomness. If the use of /dev/random is blocking, this means that the entropy pool is drained and it needs to generate more. \r\n\r\nSince any customer can do this on their own, I would rather not make this a product change since it could have security implications."", 'commenter': 'jonathan-hurley'}]"
77,ambari-server/src/main/java/org/apache/ambari/server/utils/PasswordUtils.java,"@@ -0,0 +1,137 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.utils;
+
+import java.io.File;
+import java.io.IOException;
+import java.nio.charset.Charset;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+
+import org.apache.ambari.server.AmbariException;
+import org.apache.ambari.server.configuration.Configuration;
+import org.apache.ambari.server.security.encryption.CredentialProvider;
+import org.apache.commons.io.FileUtils;
+import org.apache.commons.lang.StringUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.inject.Inject;
+
+/**
+ * Utility class to read passwords from files or the Credential Store
+ */
+public class PasswordUtils {
+
+  private static final Logger LOG = LoggerFactory.getLogger(PasswordUtils.class);
+  private static final Lock LOCK = new ReentrantLock();
+  private static final PasswordUtils INSTANCE = new PasswordUtils();
+
+  /**
+   * The constructor we need for creating a singleton instance
+   */
+  private PasswordUtils() {
+  }
+
+  @Inject
+  private static Configuration configuration;
+
+  private volatile CredentialProvider credentialProvider = null;
+
+  public static PasswordUtils getInstance() {
+    return INSTANCE;
+  }
+
+  public String readPassword(String passwordProperty, String defaultPassword) {","[{'comment': 'Missing JavaDoc', 'commenter': 'rlevas'}, {'comment': 'Fixed all missing Javadoc issues', 'commenter': 'smolnar82'}]"
77,ambari-server/src/main/java/org/apache/ambari/server/utils/PasswordUtils.java,"@@ -0,0 +1,137 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.utils;
+
+import java.io.File;
+import java.io.IOException;
+import java.nio.charset.Charset;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+
+import org.apache.ambari.server.AmbariException;
+import org.apache.ambari.server.configuration.Configuration;
+import org.apache.ambari.server.security.encryption.CredentialProvider;
+import org.apache.commons.io.FileUtils;
+import org.apache.commons.lang.StringUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.inject.Inject;
+
+/**
+ * Utility class to read passwords from files or the Credential Store
+ */
+public class PasswordUtils {
+
+  private static final Logger LOG = LoggerFactory.getLogger(PasswordUtils.class);
+  private static final Lock LOCK = new ReentrantLock();
+  private static final PasswordUtils INSTANCE = new PasswordUtils();
+
+  /**
+   * The constructor we need for creating a singleton instance
+   */
+  private PasswordUtils() {
+  }
+
+  @Inject
+  private static Configuration configuration;
+
+  private volatile CredentialProvider credentialProvider = null;
+
+  public static PasswordUtils getInstance() {
+    return INSTANCE;
+  }
+
+  public String readPassword(String passwordProperty, String defaultPassword) {
+    if (StringUtils.isNotBlank(passwordProperty)) {
+      if (CredentialProvider.isAliasString(passwordProperty)) {
+        return readPasswordFromStore(passwordProperty);
+      } else {
+        return readPasswordFromFile(passwordProperty, defaultPassword);
+      }
+    }
+    return defaultPassword;
+  }
+
+  public String readPasswordFromFile(String filePath, String defaultPassword) {","[{'comment': 'Missing JavaDoc', 'commenter': 'rlevas'}, {'comment': ""I would have put this into something like Password.fromFile(path, default), because this part doesn't need the Configuration/CredentialStore dependency. \r\nAlso please consider returning a Password objects instead of String/char[] arrays.\r\n\r\nStoring password in strings is not perfectly safe because there is no way of deleting it from the memory after it is not needed on the other hand a char[] can be nulled out. So a Password class that wraps a char[] array would be easier to enhance with this ability."", 'commenter': 'zeroflag'}, {'comment': 'Generally I agree...\r\nHowever this is out of scope of this task. Let me create a new JIRA to cover this topic.\r\n  ', 'commenter': 'smolnar82'}]"
77,ambari-server/src/main/java/org/apache/ambari/server/utils/PasswordUtils.java,"@@ -0,0 +1,137 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.utils;
+
+import java.io.File;
+import java.io.IOException;
+import java.nio.charset.Charset;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+
+import org.apache.ambari.server.AmbariException;
+import org.apache.ambari.server.configuration.Configuration;
+import org.apache.ambari.server.security.encryption.CredentialProvider;
+import org.apache.commons.io.FileUtils;
+import org.apache.commons.lang.StringUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.inject.Inject;
+
+/**
+ * Utility class to read passwords from files or the Credential Store
+ */
+public class PasswordUtils {
+
+  private static final Logger LOG = LoggerFactory.getLogger(PasswordUtils.class);
+  private static final Lock LOCK = new ReentrantLock();
+  private static final PasswordUtils INSTANCE = new PasswordUtils();
+
+  /**
+   * The constructor we need for creating a singleton instance
+   */
+  private PasswordUtils() {
+  }
+
+  @Inject
+  private static Configuration configuration;
+
+  private volatile CredentialProvider credentialProvider = null;
+
+  public static PasswordUtils getInstance() {
+    return INSTANCE;
+  }
+
+  public String readPassword(String passwordProperty, String defaultPassword) {
+    if (StringUtils.isNotBlank(passwordProperty)) {
+      if (CredentialProvider.isAliasString(passwordProperty)) {
+        return readPasswordFromStore(passwordProperty);
+      } else {
+        return readPasswordFromFile(passwordProperty, defaultPassword);
+      }
+    }
+    return defaultPassword;
+  }
+
+  public String readPasswordFromFile(String filePath, String defaultPassword) {
+    if (filePath == null || !fileExistsAndCanBeRead(filePath)) {
+      LOG.debug(""DB password file not specified or does not exist/can not be read - using default"");
+      return defaultPassword;
+    } else {
+      LOG.debug(""Reading password from file {}"", filePath);
+      String password = null;
+      try {
+        password = FileUtils.readFileToString(new File(filePath), Charset.defaultCharset());
+        return StringUtils.chomp(password);
+      } catch (IOException e) {
+        throw new RuntimeException(""Unable to read password from file ["" + filePath + ""]"", e);
+      }
+    }
+  }
+
+  private boolean fileExistsAndCanBeRead(String filePath) {
+    final File passwordFile = new File(filePath);
+    return passwordFile.exists() && passwordFile.canRead() && passwordFile.isFile();
+  }
+
+  private String readPasswordFromStore(String aliasStr) {
+    return readPasswordFromStore(aliasStr, configuration.getMasterKeyLocation(), configuration.isMasterKeyPersisted(), configuration.getMasterKeyStoreLocation());
+  }
+
+  public String readPasswordFromStore(String aliasStr, File masterKeyLocation, boolean isMasterKeyPersisted, File masterKeyStoreLocation) {","[{'comment': 'Missing JavaDoc', 'commenter': 'rlevas'}]"
77,ambari-server/src/main/java/org/apache/ambari/server/utils/PasswordUtils.java,"@@ -0,0 +1,137 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.utils;
+
+import java.io.File;
+import java.io.IOException;
+import java.nio.charset.Charset;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+
+import org.apache.ambari.server.AmbariException;
+import org.apache.ambari.server.configuration.Configuration;
+import org.apache.ambari.server.security.encryption.CredentialProvider;
+import org.apache.commons.io.FileUtils;
+import org.apache.commons.lang.StringUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.inject.Inject;
+
+/**
+ * Utility class to read passwords from files or the Credential Store
+ */
+public class PasswordUtils {
+
+  private static final Logger LOG = LoggerFactory.getLogger(PasswordUtils.class);
+  private static final Lock LOCK = new ReentrantLock();
+  private static final PasswordUtils INSTANCE = new PasswordUtils();
+
+  /**
+   * The constructor we need for creating a singleton instance
+   */
+  private PasswordUtils() {
+  }
+
+  @Inject
+  private static Configuration configuration;
+
+  private volatile CredentialProvider credentialProvider = null;
+
+  public static PasswordUtils getInstance() {
+    return INSTANCE;
+  }
+
+  public String readPassword(String passwordProperty, String defaultPassword) {
+    if (StringUtils.isNotBlank(passwordProperty)) {
+      if (CredentialProvider.isAliasString(passwordProperty)) {
+        return readPasswordFromStore(passwordProperty);
+      } else {
+        return readPasswordFromFile(passwordProperty, defaultPassword);
+      }
+    }
+    return defaultPassword;
+  }
+
+  public String readPasswordFromFile(String filePath, String defaultPassword) {
+    if (filePath == null || !fileExistsAndCanBeRead(filePath)) {
+      LOG.debug(""DB password file not specified or does not exist/can not be read - using default"");
+      return defaultPassword;
+    } else {
+      LOG.debug(""Reading password from file {}"", filePath);
+      String password = null;
+      try {
+        password = FileUtils.readFileToString(new File(filePath), Charset.defaultCharset());
+        return StringUtils.chomp(password);
+      } catch (IOException e) {
+        throw new RuntimeException(""Unable to read password from file ["" + filePath + ""]"", e);
+      }
+    }
+  }
+
+  private boolean fileExistsAndCanBeRead(String filePath) {
+    final File passwordFile = new File(filePath);
+    return passwordFile.exists() && passwordFile.canRead() && passwordFile.isFile();
+  }
+
+  private String readPasswordFromStore(String aliasStr) {
+    return readPasswordFromStore(aliasStr, configuration.getMasterKeyLocation(), configuration.isMasterKeyPersisted(), configuration.getMasterKeyStoreLocation());
+  }
+
+  public String readPasswordFromStore(String aliasStr, File masterKeyLocation, boolean isMasterKeyPersisted, File masterKeyStoreLocation) {
+    String password = null;
+    loadCredentialProvider(masterKeyLocation, isMasterKeyPersisted, masterKeyStoreLocation);
+    if (credentialProvider != null) {
+      char[] result = null;
+      try {
+        result = credentialProvider.getPasswordForAlias(aliasStr);
+      } catch (AmbariException e) {
+        LOG.error(""Error reading from credential store."");
+        e.printStackTrace();","[{'comment': 'Do we need both printStackTrace + logging here?', 'commenter': 'zeroflag'}, {'comment': 'Fixed', 'commenter': 'smolnar82'}]"
77,ambari-server/src/main/java/org/apache/ambari/server/utils/PasswordUtils.java,"@@ -0,0 +1,186 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.utils;
+
+import java.io.File;
+import java.io.IOException;
+import java.nio.charset.Charset;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+
+import org.apache.ambari.server.AmbariException;
+import org.apache.ambari.server.configuration.Configuration;
+import org.apache.ambari.server.security.encryption.CredentialProvider;
+import org.apache.commons.io.FileUtils;
+import org.apache.commons.lang.StringUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.inject.Inject;
+
+/**
+ * Utility class to read passwords from files or the Credential Store
+ */
+public class PasswordUtils {
+
+  private static final Logger LOG = LoggerFactory.getLogger(PasswordUtils.class);
+  private static final Lock LOCK = new ReentrantLock();
+  private static final PasswordUtils INSTANCE = new PasswordUtils();
+
+  /**
+   * The constructor we need for creating a singleton instance
+   */
+  private PasswordUtils() {
+  }
+
+  @Inject
+  private static Configuration configuration;
+
+  private volatile CredentialProvider credentialProvider = null;
+
+  public static PasswordUtils getInstance() {
+    return INSTANCE;
+  }
+
+  /**
+   * Reading the password belong to the given password property
+   *
+   * @param passwordProperty
+   *          this is either the Credential Store alias or the password file name
+   *          you want to read the password for/from
+   * @param defaultPassword
+   *          the default password this function returns in case the given
+   *          <code>passwordProperty</code> is <blank> or the password file cannot
+   *          be read for any reason
+   * @return in case <code>passwordProperty</code> belongs to a Credential Store
+   *         alias this function returns the password of the given Credential
+   *         Store alias or <code>null</code> (if the given alias is
+   *         <code>blank</code> or there is no password found in CS); otherwise
+   *         either the password found in the given password file is returned or
+   *         <code>defaultPassword</code> if the given path is <code>blank</code>
+   *         or cannot be read for any reason
+   * @throws RuntimeException
+   *           if any error occurred while reading the password file
+   */
+  public String readPassword(String passwordProperty, String defaultPassword) {
+    if (StringUtils.isNotBlank(passwordProperty)) {
+      if (CredentialProvider.isAliasString(passwordProperty)) {
+        return readPasswordFromStore(passwordProperty);
+      } else {
+        return readPasswordFromFile(passwordProperty, defaultPassword);
+      }
+    }
+    return defaultPassword;
+  }
+
+  /**
+   * Reading password from the given password file
+   *
+   * @param filePath
+   *          the path of the file to read the password from
+   * @param defaultPassword
+   *          the default password this function returns in case the given
+   *          <code>filePath</code> is <code>blank</code> or the password file
+   *          cannot be read for any reason
+   * @return the password found in the given password file or
+   *         <code>defaultPassword</code> if the given path is <code>blank</code>
+   *         or cannot be read for any reason
+   * @throws RuntimeException
+   *           when any error occurred while reading the password file
+   */
+  public String readPasswordFromFile(String filePath, String defaultPassword) {
+    if (StringUtils.isBlank(filePath) || !fileExistsAndCanBeRead(filePath)) {
+      LOG.debug(""DB password file not specified or does not exist/can not be read - using default"");
+      return defaultPassword;
+    } else {
+      LOG.debug(""Reading password from file {}"", filePath);
+      String password = null;
+      try {
+        password = FileUtils.readFileToString(new File(filePath), Charset.defaultCharset());
+        return StringUtils.chomp(password);
+      } catch (IOException e) {
+        throw new RuntimeException(""Unable to read password from file ["" + filePath + ""]"", e);
+      }
+    }
+  }
+
+  private boolean fileExistsAndCanBeRead(String filePath) {
+    final File passwordFile = new File(filePath);
+    return passwordFile.exists() && passwordFile.canRead() && passwordFile.isFile();
+  }
+
+  private String readPasswordFromStore(String aliasStr) {
+    return readPasswordFromStore(aliasStr, configuration.getMasterKeyLocation(), configuration.isMasterKeyPersisted(), configuration.getMasterKeyStoreLocation());
+  }
+
+  /**
+   * Reading the password from Credential Store for the given alias
+   *
+   * @param aliasStr
+   *          the Credential Store alias you want to read the password for
+   * @param masterKeyLocation
+   *          the master key location file
+   * @param isMasterKeyPersisted
+   *          a flag indicating whether the master key is persisted
+   * @param masterKeyStoreLocation
+   *          the master key-store location file
+   * @return the password of the given alias if it is not <code>blank</code> and
+   *         there is password stored for this alias in Credential Store;
+   *         <code>null</code> otherwise
+   */
+  public String readPasswordFromStore(String aliasStr, File masterKeyLocation, boolean isMasterKeyPersisted, File masterKeyStoreLocation) {
+    String password = null;
+    loadCredentialProvider(masterKeyLocation, isMasterKeyPersisted, masterKeyStoreLocation);
+    if (credentialProvider != null) {
+      char[] result = null;
+      try {
+        result = credentialProvider.getPasswordForAlias(aliasStr);
+      } catch (AmbariException e) {
+        LOG.error(""Error reading from credential store."", e);
+      }
+      if (result != null) {
+        password = new String(result);
+      } else {
+        if (CredentialProvider.isAliasString(aliasStr)) {
+          LOG.error(""Cannot read password for alias = "" + aliasStr);
+        } else {
+          LOG.warn(""Raw password provided, not an alias. It cannot be read from credential store."");
+        }
+      }
+    }
+    return password;
+  }
+
+  private void loadCredentialProvider(File masterKeyLocation, boolean isMasterKeyPersisted, File masterKeyStoreLocation) {
+    if (credentialProvider == null) {
+      try {
+        LOCK.lock();
+        credentialProvider = new CredentialProvider(null, masterKeyLocation, isMasterKeyPersisted, masterKeyStoreLocation);
+      } catch (Exception e) {
+        LOG.info(""Credential provider creation failed. Reason: "" + e.getMessage());
+        if (LOG.isDebugEnabled()) {
+          e.printStackTrace();","[{'comment': 'We should be using logging methods rather than printStackTrace.', 'commenter': 'rlevas'}, {'comment': 'Fixed', 'commenter': 'smolnar82'}]"
86,ambari-server/src/main/java/org/apache/ambari/server/agent/CommandRepository.java,"@@ -234,6 +234,13 @@ public void setPreInstalled(String isPreInstalled) {
       this.m_isPreInstalled = isPreInstalled.equalsIgnoreCase(""true"");
     }
 
+    public Boolean getM_isPreInstalled() {","[{'comment': ' * does not fit naming convention\r\n * method is unused', 'commenter': 'adoroszlai'}]"
86,ambari-server/src/main/java/org/apache/ambari/server/agent/CommandRepository.java,"@@ -234,6 +234,13 @@ public void setPreInstalled(String isPreInstalled) {
       this.m_isPreInstalled = isPreInstalled.equalsIgnoreCase(""true"");
     }
 
+    public Boolean getM_isPreInstalled() {
+      return m_isPreInstalled;
+    }
+
+    public boolean isM_isScoped() {","[{'comment': ' * does not fit naming convention\r\n * method is unused', 'commenter': 'adoroszlai'}]"
86,ambari-server/src/main/java/org/apache/ambari/server/agent/CommandRepository.java,"@@ -234,6 +234,13 @@ public void setPreInstalled(String isPreInstalled) {
       this.m_isPreInstalled = isPreInstalled.equalsIgnoreCase(""true"");
     }
 
+    public Boolean getPreInstalled() {","[{'comment': 'why this getter is needed? it is not used anywhere and the only one role he have - be serialized to json', 'commenter': 'hapylestat'}]"
86,ambari-server/src/main/java/org/apache/ambari/server/agent/CommandRepository.java,"@@ -234,6 +234,13 @@ public void setPreInstalled(String isPreInstalled) {
       this.m_isPreInstalled = isPreInstalled.equalsIgnoreCase(""true"");
     }
 
+    public Boolean getPreInstalled() {
+      return m_isPreInstalled;
+    }
+
+    public boolean isScoped() {","[{'comment': 'why this getter is needed? it is not used anywhere and the only one role he have - be serialized to json', 'commenter': 'hapylestat'}]"
106,ambari-server/src/main/java/org/apache/ambari/server/configuration/Configuration.java,"@@ -2851,6 +2854,41 @@ private static Properties readConfigFile() {
     return properties;
   }
 
+  /**
+   * Removing the given properties from ambari.properties (i.e. at upgrade time)
+   *
+   * @param propertiesToBeCleared
+   *          the properties to be removed
+   * @throws AmbariException
+   *           if there was any issue when clearing ambari.properties
+   */
+  public void removePropertiesFromAmbariProperties(Collection<String> propertiesToBeRemoved) throws AmbariException {
+    try {
+      final File ambariPropertiesFile = new File(Configuration.class.getClassLoader().getResource(Configuration.CONFIG_FILE).getPath());
+      final List<String> ambariPropertiesLines = FileUtils.readLines(ambariPropertiesFile, Charset.defaultCharset());
+      final StringBuilder newAmbariProperties = new StringBuilder();
+      for (String line : ambariPropertiesLines) {
+        if (!lineNeedsToBeRemoved(line, propertiesToBeRemoved)) {
+          newAmbariProperties.append(line).append(System.getProperty(""line.separator""));
+        }
+      }
+      FileUtils.deleteQuietly(ambariPropertiesFile);
+      FileUtils.writeStringToFile(ambariPropertiesFile, newAmbariProperties.toString(), Charset.defaultCharset());
+
+      // reloading properties
+      this.properties = readConfigFile();
+    } catch (IOException e) {
+      throw new AmbariException(""Error while clearing ambari.properties"", e);
+    }
+  }
+
+  private boolean lineNeedsToBeRemoved(String line, Collection<String> propertiesToBeRemoved) {
+    if (StringUtils.isNotBlank(line)) {
+      return propertiesToBeRemoved.stream().filter(propertyToBeCleared -> line.trim().indexOf(propertyToBeCleared) > -1).findFirst().isPresent();","[{'comment': 'maybe anyMatch() would be shorter', 'commenter': 'zeroflag'}, {'comment': 'This implementation also removes properties with prefix and/or suffix, eg. `security.server.crt_pass` would match `security.server.crt_pass_file`, too, due to using `indexOf()`.  It may not be a problem in this use case, but I think it should be addressed, since it is a generic method.', 'commenter': 'adoroszlai'}, {'comment': 'Fixed', 'commenter': 'smolnar82'}]"
106,ambari-server/src/main/java/org/apache/ambari/server/upgrade/UpgradeCatalog300.java,"@@ -383,4 +389,94 @@ protected void updateKerberosConfigurations() throws AmbariException {
       }
     }
   }
+
+  /**
+   * Moves LDAP related properties from ambari.properties to ambari_congiuration DB table
+   * @throws AmbariException if there was any issue when clearing ambari.properties
+   */
+  protected void upgradeLdapConfiguration() throws AmbariException {
+    LOG.info(""Moving LDAP related properties from ambari.properties to ambari_congiuration DB table..."");","[{'comment': 'typo in `ambari_congiuration`', 'commenter': 'adoroszlai'}, {'comment': 'Fixed', 'commenter': 'smolnar82'}]"
106,ambari-server/src/main/java/org/apache/ambari/server/configuration/Configuration.java,"@@ -2851,6 +2854,41 @@ private static Properties readConfigFile() {
     return properties;
   }
 
+  /**
+   * Removing the given properties from ambari.properties (i.e. at upgrade time)
+   *
+   * @param propertiesToBeCleared
+   *          the properties to be removed
+   * @throws AmbariException
+   *           if there was any issue when clearing ambari.properties
+   */
+  public void removePropertiesFromAmbariProperties(Collection<String> propertiesToBeRemoved) throws AmbariException {
+    try {
+      final File ambariPropertiesFile = new File(Configuration.class.getClassLoader().getResource(Configuration.CONFIG_FILE).getPath());
+      final List<String> ambariPropertiesLines = FileUtils.readLines(ambariPropertiesFile, Charset.defaultCharset());
+      final StringBuilder newAmbariProperties = new StringBuilder();
+      for (String line : ambariPropertiesLines) {
+        if (!lineNeedsToBeRemoved(line, propertiesToBeRemoved)) {
+          newAmbariProperties.append(line).append(System.getProperty(""line.separator""));
+        }
+      }
+      FileUtils.deleteQuietly(ambariPropertiesFile);
+      FileUtils.writeStringToFile(ambariPropertiesFile, newAmbariProperties.toString(), Charset.defaultCharset());","[{'comment': 'Deleting the file before writing the new one may lead to data loss.', 'commenter': 'adoroszlai'}, {'comment': 'Fixed', 'commenter': 'smolnar82'}]"
106,ambari-server/src/main/java/org/apache/ambari/server/upgrade/UpgradeCatalog300.java,"@@ -383,4 +389,94 @@ protected void updateKerberosConfigurations() throws AmbariException {
       }
     }
   }
+
+  /**
+   * Moves LDAP related properties from ambari.properties to ambari_configuration DB table
+   * @throws AmbariException if there was any issue when clearing ambari.properties
+   */
+  protected void upgradeLdapConfiguration() throws AmbariException {","[{'comment': 'Upgrade logic should be idempotent:\r\n\r\n1. Running upgrade on already upgraded setup should be no-op (no errors etc.).  I guess this is the case, since properties removed at the end will not be processed on next try.\r\n2. Retrying partially completed upgrade should resume without errors.  This _might_ be a  problem here, since all properties are left until the end, duplicate properties may be created.  Can you please confirm if this handled correctly?  (Test by throwing some exception in the middle of property processing.)  If not, can you please make sure to ""create or update"" the properties instead of ""create"".', 'commenter': 'adoroszlai'}, {'comment': ""These are good notices; thanks for bringing them up.\r\nPoint 1: like you indicated there won't be more properties to be upgraded = no-op\r\nPoint 2: I'll check it out and modify the code if needed"", 'commenter': 'smolnar82'}, {'comment': 'Fixed', 'commenter': 'smolnar82'}]"
106,ambari-server/src/main/java/org/apache/ambari/server/configuration/Configuration.java,"@@ -2853,6 +2855,40 @@ private static Properties readConfigFile() {
     return properties;
   }
 
+  /**
+   * Removing the given properties from ambari.properties (i.e. at upgrade time)
+   *
+   * @param propertiesToBeCleared
+   *          the properties to be removed
+   * @throws AmbariException
+   *           if there was any issue when clearing ambari.properties
+   */
+  public void removePropertiesFromAmbariProperties(Collection<String> propertiesToBeRemoved) throws AmbariException {
+    try {
+      final File ambariPropertiesFile = new File(Configuration.class.getClassLoader().getResource(Configuration.CONFIG_FILE).getPath());","[{'comment': 'You should use `org.apache.ambari.server.configuration.Configuration#readConfigFile` to read the properties from the `ambari.properties file` into a `java.util.Properties` object.   Then later write out the properties using `org.apache.ambari.server.configuration.Configuration#writeConfigFile` (which does not yet exist). \r\n', 'commenter': 'rlevas'}]"
106,ambari-server/src/main/java/org/apache/ambari/server/utils/HostAndPort.java,"@@ -25,4 +25,12 @@ public HostAndPort(String host, int port) {
     this.host = host;
     this.port = port;
   }
+
+  public static HostAndPort fromUrl(String url) {","[{'comment': 'Can `java.net.URL` be used instead?', 'commenter': 'rlevas'}, {'comment': 'Using HostAndPort from Guava instead', 'commenter': 'smolnar82'}]"
106,ambari-server/src/main/java/org/apache/ambari/server/utils/HostAndPort.java,"@@ -25,4 +25,12 @@ public HostAndPort(String host, int port) {
     this.host = host;
     this.port = port;
   }
+
+  public static HostAndPort fromUrl(String url) {
+    return new HostAndPort(url.split("":"")[0], Integer.valueOf(url.split("":"")[1]));
+  }
+
+  public String portAsString() {","[{'comment': 'Can `java.net.URL` be used instead?', 'commenter': 'rlevas'}, {'comment': 'Using HostAndPort from Guava instead', 'commenter': 'smolnar82'}]"
109,ambari-server/src/main/resources/Ambari-DDL-Derby-CREATE.sql,"@@ -601,15 +582,49 @@ CREATE TABLE requestschedulebatchrequest (
   return_message varchar(20000),
   CONSTRAINT PK_requestschedulebatchrequest PRIMARY KEY (schedule_id, batch_id),
   CONSTRAINT FK_rsbatchrequest_schedule_id FOREIGN KEY (schedule_id) REFERENCES requestschedule (schedule_id));
+  CONSTRAINT FK_rsbatchrequest_schedule_id FOREIGN KEY (schedule_id) REFERENCES requestschedule (schedule_id));","[{'comment': 'Looks like a duplicate row.', 'commenter': 'adoroszlai'}, {'comment': 'Fixed', 'commenter': 'benyoka'}]"
109,ambari-server/src/main/java/org/apache/ambari/server/orm/entities/BlueprintServiceConfigEntityPk.java,"@@ -0,0 +1,51 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.orm.entities;
+
+import javax.persistence.Column;
+import javax.persistence.Id;
+
+public class BlueprintServiceConfigEntityPk {
+
+  @Id
+  @Column(name = ""service_id"", nullable = false, insertable = true, updatable = false)
+  private Long serviceId;
+
+  @Id
+  @Column(name = ""type_name"", nullable = false, insertable = true, updatable = false, length = 100)
+  private String type;
+
+  @Override
+  public boolean equals(Object o) {
+    if (this == o) return true;
+    if (o == null || getClass() != o.getClass()) return false;
+
+    BlueprintServiceConfigEntityPk that = (BlueprintServiceConfigEntityPk) o;
+
+    if (serviceId != null ? !serviceId.equals(that.serviceId) : that.serviceId != null) return false;
+    return type != null ? type.equals(that.type) : that.type == null;
+  }
+
+  @Override
+  public int hashCode() {
+    int result = serviceId != null ? serviceId.hashCode() : 0;","[{'comment': 'Please simplify using `Objects.hash()`.', 'commenter': 'adoroszlai'}, {'comment': 'Fixed.', 'commenter': 'benyoka'}]"
109,ambari-server/src/main/java/org/apache/ambari/server/orm/entities/BlueprintServiceConfigEntityPk.java,"@@ -0,0 +1,51 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.orm.entities;
+
+import javax.persistence.Column;
+import javax.persistence.Id;
+
+public class BlueprintServiceConfigEntityPk {
+
+  @Id
+  @Column(name = ""service_id"", nullable = false, insertable = true, updatable = false)
+  private Long serviceId;
+
+  @Id
+  @Column(name = ""type_name"", nullable = false, insertable = true, updatable = false, length = 100)
+  private String type;
+
+  @Override
+  public boolean equals(Object o) {
+    if (this == o) return true;
+    if (o == null || getClass() != o.getClass()) return false;
+
+    BlueprintServiceConfigEntityPk that = (BlueprintServiceConfigEntityPk) o;
+
+    if (serviceId != null ? !serviceId.equals(that.serviceId) : that.serviceId != null) return false;","[{'comment': 'Please simplify using `Objects.equals()`.', 'commenter': 'adoroszlai'}, {'comment': 'Fixed.', 'commenter': 'benyoka'}]"
109,ambari-server/src/main/java/org/apache/ambari/server/orm/entities/BlueprintMpackConfigEntityPk.java,"@@ -0,0 +1,50 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.orm.entities;
+
+import javax.persistence.Column;
+import javax.persistence.Id;
+
+public class BlueprintMpackConfigEntityPk {
+  @Id
+  @Column(name = ""mpack_ref_id"", nullable = false, insertable = true, updatable = false)
+  private Long mpackRefId;
+
+  @Id
+  @Column(name = ""type_name"", nullable = false, insertable = true, updatable = false, length = 100)
+  private String type;
+
+  @Override
+  public boolean equals(Object o) {
+    if (this == o) return true;
+    if (o == null || getClass() != o.getClass()) return false;
+
+    BlueprintMpackConfigEntityPk that = (BlueprintMpackConfigEntityPk) o;
+
+    if (mpackRefId != null ? !mpackRefId.equals(that.mpackRefId) : that.mpackRefId != null) return false;","[{'comment': 'Please simplify using `Objects.equals()`.', 'commenter': 'adoroszlai'}, {'comment': 'Fixed.', 'commenter': 'benyoka'}]"
109,ambari-server/src/main/java/org/apache/ambari/server/orm/entities/BlueprintMpackConfigEntityPk.java,"@@ -0,0 +1,50 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.orm.entities;
+
+import javax.persistence.Column;
+import javax.persistence.Id;
+
+public class BlueprintMpackConfigEntityPk {
+  @Id
+  @Column(name = ""mpack_ref_id"", nullable = false, insertable = true, updatable = false)
+  private Long mpackRefId;
+
+  @Id
+  @Column(name = ""type_name"", nullable = false, insertable = true, updatable = false, length = 100)
+  private String type;
+
+  @Override
+  public boolean equals(Object o) {
+    if (this == o) return true;
+    if (o == null || getClass() != o.getClass()) return false;
+
+    BlueprintMpackConfigEntityPk that = (BlueprintMpackConfigEntityPk) o;
+
+    if (mpackRefId != null ? !mpackRefId.equals(that.mpackRefId) : that.mpackRefId != null) return false;
+    return type != null ? type.equals(that.type) : that.type == null;
+  }
+
+  @Override
+  public int hashCode() {
+    int result = mpackRefId != null ? mpackRefId.hashCode() : 0;","[{'comment': 'Please simplify using `Objects.hash()`.', 'commenter': 'adoroszlai'}, {'comment': 'Fixed.', 'commenter': 'benyoka'}]"
109,ambari-server/src/main/resources/Ambari-DDL-MySQL-CREATE.sql,"@@ -635,12 +649,17 @@ CREATE TABLE hostgroup (
   CONSTRAINT FK_hg_blueprint_name FOREIGN KEY (blueprint_name) REFERENCES blueprint(blueprint_name));
 
 CREATE TABLE hostgroup_component (
-  blueprint_name VARCHAR(100) NOT NULL,
-  hostgroup_name VARCHAR(100) NOT NULL,
-  name VARCHAR(100) NOT NULL,
-  provision_action VARCHAR(100),
-  CONSTRAINT PK_hostgroup_component PRIMARY KEY (blueprint_name, hostgroup_name, name),
-  CONSTRAINT FK_hgc_blueprint_name FOREIGN KEY (blueprint_name, hostgroup_name) REFERENCES hostgroup(blueprint_name, name));
+  id BIGINT NOT NULL,
+  blueprint_name VARCHAR(255) NOT NULL,
+  hostgroup_name VARCHAR(255) NOT NULL,
+  name VARCHAR(255) NOT NULL,
+  mpack_name VARCHAR(255),
+  mpack_version VARCHAR(100),
+  service_name VARCHAR(255),
+  provision_action VARCHAR(255),
+  CONSTRAINT PK_hostgroup_component PRIMARY KEY (id),
+  CONSTRAINT FK_hgc_blueprint_name FOREIGN KEY (blueprint_name, hostgroup_name) REFERENCES hostgroup (blueprint_name, name),
+  CONSTRAINT FK_hgc_blueprint_service FOREIGN KEY (blueprint_service_id) REFERENCES blueprint_service(id));","[{'comment': ""```\r\nKey column 'blueprint_service_id' doesn't exist in table\r\n```"", 'commenter': 'adoroszlai'}, {'comment': 'Good catch. This FK is no more needed, will drop.', 'commenter': 'benyoka'}, {'comment': 'Two more of these still left in the Derby and SQLServer scripts.', 'commenter': 'adoroszlai'}, {'comment': 'Done.', 'commenter': 'benyoka'}]"
109,ambari-server/src/main/java/org/apache/ambari/server/orm/entities/BlueprintEntity.java,"@@ -63,17 +64,20 @@
    * Unidirectional one-to-one association to {@link StackEntity}
    */
   @OneToOne
-  @JoinColumn(name = ""stack_id"", unique = false, nullable = false, insertable = true, updatable = false)
+  @JoinColumn(name = ""stack_id"", unique = false, nullable = true, insertable = true, updatable = false)","[{'comment': ""How can stack be null? It's still necessary to specify this when creating a blueprint - even if it's just an mpack association..."", 'commenter': 'jonathan-hurley'}, {'comment': 'This field is kept only for compatibility purposes. A blueprint will reference mpacks instead of stacks and it can have multiple mpacks.', 'commenter': 'benyoka'}, {'comment': 'So if a blueprint references a stack but there are multiple mpacks associated with that stack, how is it determined which to use?', 'commenter': 'jonathan-hurley'}, {'comment': 'I think when mpacks are defined the stack property should be ignored. Another option is to validate that one of the mpack declarations point to the stack.\r\n\r\nDo you want to remove the reference to stack?', 'commenter': 'benyoka'}, {'comment': ""I think that leaving the stack without the mpack makes it ambiguous. Do we have problems removing this field? If we're keeping it only for compatibility, then don't we still have to specify it in addition to the mpack?"", 'commenter': 'jonathan-hurley'}]"
109,ambari-server/src/main/java/org/apache/ambari/server/orm/entities/BlueprintMpackConfigEntity.java,"@@ -0,0 +1,110 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.orm.entities;
+
+import javax.persistence.Basic;
+import javax.persistence.Column;
+import javax.persistence.Entity;
+import javax.persistence.FetchType;
+import javax.persistence.Id;
+import javax.persistence.IdClass;
+import javax.persistence.JoinColumn;
+import javax.persistence.Lob;
+import javax.persistence.ManyToOne;
+import javax.persistence.Table;
+
+@Entity
+@Table(name = ""blueprint_mpack_configuration"")
+@IdClass(BlueprintMpackConfigEntityPk.class)
+public class BlueprintMpackConfigEntity implements BlueprintConfiguration {","[{'comment': 'This class needs documentation.', 'commenter': 'jonathan-hurley'}]"
109,ambari-server/src/main/java/org/apache/ambari/server/orm/entities/BlueprintMpackConfigEntityPk.java,"@@ -0,0 +1,48 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.orm.entities;
+
+import java.util.Objects;
+
+import javax.persistence.Column;
+import javax.persistence.Id;
+
+public class BlueprintMpackConfigEntityPk {","[{'comment': 'This class needs documentation.', 'commenter': 'jonathan-hurley'}]"
109,ambari-server/src/main/java/org/apache/ambari/server/orm/entities/BlueprintMpackReferenceEntity.java,"@@ -0,0 +1,121 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.orm.entities;
+
+import java.util.ArrayList;
+import java.util.Collection;
+
+import javax.persistence.CascadeType;
+import javax.persistence.Column;
+import javax.persistence.Entity;
+import javax.persistence.GeneratedValue;
+import javax.persistence.GenerationType;
+import javax.persistence.Id;
+import javax.persistence.JoinColumn;
+import javax.persistence.ManyToOne;
+import javax.persistence.OneToMany;
+import javax.persistence.Table;
+import javax.persistence.TableGenerator;
+
+@Entity
+@Table(name = ""blueprint_mpack_reference"")
+@TableGenerator(name = ""blueprint_mpack_reference_id_generator"", table = ""ambari_sequences"", pkColumnName = ""sequence_name"",
+  valueColumnName = ""sequence_value"", pkColumnValue = ""blueprint_mpack_ref_id_seq"", initialValue = 1)
+public class BlueprintMpackReferenceEntity {","[{'comment': 'This class needs documentation.', 'commenter': 'jonathan-hurley'}]"
109,ambari-server/src/main/java/org/apache/ambari/server/orm/entities/BlueprintMpackReferenceEntity.java,"@@ -0,0 +1,121 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.orm.entities;
+
+import java.util.ArrayList;
+import java.util.Collection;
+
+import javax.persistence.CascadeType;
+import javax.persistence.Column;
+import javax.persistence.Entity;
+import javax.persistence.GeneratedValue;
+import javax.persistence.GenerationType;
+import javax.persistence.Id;
+import javax.persistence.JoinColumn;
+import javax.persistence.ManyToOne;
+import javax.persistence.OneToMany;
+import javax.persistence.Table;
+import javax.persistence.TableGenerator;
+
+@Entity
+@Table(name = ""blueprint_mpack_reference"")
+@TableGenerator(name = ""blueprint_mpack_reference_id_generator"", table = ""ambari_sequences"", pkColumnName = ""sequence_name"",
+  valueColumnName = ""sequence_value"", pkColumnValue = ""blueprint_mpack_ref_id_seq"", initialValue = 1)
+public class BlueprintMpackReferenceEntity {
+  @Id
+  @GeneratedValue(strategy = GenerationType.TABLE, generator = ""blueprint_mpack_reference_id_generator"")
+  @Column(name = ""id"", nullable = false, updatable = false)
+  private Long id;
+
+  @Column(name = ""mpack_name"")","[{'comment': ""Why do we have duplicated fields here? Shouldn't this just be a foreign key to an mpack?"", 'commenter': 'jonathan-hurley'}, {'comment': ""I wanted to decouple the blueprint lifecycle from the mpack lifecycle. This is a valid case:\r\n\r\n- user posts a blueprint (mpack does not exist at this time as automatic mpack download will happen at cluster installation time)\r\n- user installs the necessary mpacks (or they are auto-dowloaded at cluster installation)\r\n\r\nI am not sure this can be modelled with foreign keys as after the first step there are no mpacks in the DB the FK's can refer to.\r\n\r\n"", 'commenter': 'benyoka'}, {'comment': ""Is that really a valid use case? Ambari can't auto download the correct management pack because there could be multiple places it's hosted. The mpacks have to be installed first for any blueprint installation, right?"", 'commenter': 'jonathan-hurley'}, {'comment': '@rnettleton @jayush  Could you pls. clarify if blueprints have an independent lifecycle from mpacks, such as the user can send in a blueprint referencing an mpack that is not yet installed.', 'commenter': 'benyoka'}]"
109,ambari-server/src/main/java/org/apache/ambari/server/orm/entities/BlueprintServiceConfigEntity.java,"@@ -0,0 +1,108 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.orm.entities;
+
+import javax.persistence.Basic;
+import javax.persistence.Column;
+import javax.persistence.Entity;
+import javax.persistence.FetchType;
+import javax.persistence.Id;
+import javax.persistence.IdClass;
+import javax.persistence.JoinColumn;
+import javax.persistence.Lob;
+import javax.persistence.ManyToOne;
+import javax.persistence.Table;
+
+@Entity
+@Table(name = ""blueprint_service_config"")
+@IdClass(BlueprintServiceConfigEntityPk.class)
+public class BlueprintServiceConfigEntity implements BlueprintConfiguration {","[{'comment': 'This class needs documentation.', 'commenter': 'jonathan-hurley'}]"
109,ambari-server/src/main/java/org/apache/ambari/server/orm/entities/BlueprintServiceConfigEntityPk.java,"@@ -0,0 +1,49 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.orm.entities;
+
+import java.util.Objects;
+
+import javax.persistence.Column;
+import javax.persistence.Id;
+
+public class BlueprintServiceConfigEntityPk {","[{'comment': 'This class needs documentation.', 'commenter': 'jonathan-hurley'}]"
109,ambari-server/src/main/java/org/apache/ambari/server/orm/entities/BlueprintServiceEntity.java,"@@ -0,0 +1,96 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.orm.entities;
+
+import java.util.ArrayList;
+import java.util.Collection;
+
+import javax.persistence.CascadeType;
+import javax.persistence.Column;
+import javax.persistence.Entity;
+import javax.persistence.GeneratedValue;
+import javax.persistence.GenerationType;
+import javax.persistence.Id;
+import javax.persistence.JoinColumn;
+import javax.persistence.ManyToOne;
+import javax.persistence.OneToMany;
+import javax.persistence.Table;
+import javax.persistence.TableGenerator;
+
+@Entity
+@Table(name = ""blueprint_service"")
+@TableGenerator(name = ""blueprint_service_id_generator"", table = ""ambari_sequences"", pkColumnName = ""sequence_name"",
+  valueColumnName = ""sequence_value"", pkColumnValue = ""blueprint_service_id_seq"", initialValue = 1)
+public class BlueprintServiceEntity {","[{'comment': 'This class needs documentation.', 'commenter': 'jonathan-hurley'}]"
109,ambari-server/src/main/java/org/apache/ambari/server/orm/entities/HostGroupComponentEntity.java,"@@ -20,44 +20,57 @@
 
 import javax.persistence.Column;
 import javax.persistence.Entity;
+import javax.persistence.GeneratedValue;
+import javax.persistence.GenerationType;
 import javax.persistence.Id;
-import javax.persistence.IdClass;
 import javax.persistence.JoinColumn;
 import javax.persistence.JoinColumns;
 import javax.persistence.ManyToOne;
 import javax.persistence.Table;
+import javax.persistence.TableGenerator;
 
 /**
  * Represents a Host Group Component which is embedded in a Blueprint.
  */
-@IdClass(HostGroupComponentEntityPK.class)
-@Table(name = ""hostgroup_component"")
 @Entity
+@Table(name = ""hostgroup_component"")
+@TableGenerator(name = ""hostgroup_component_id_generator"", table = ""ambari_sequences"", pkColumnName = ""sequence_name"",
+  valueColumnName = ""sequence_value"", pkColumnValue = ""hostgroup_component_id_seq"", initialValue = 1)
 public class HostGroupComponentEntity {
 
   @Id
+  @GeneratedValue(strategy = GenerationType.TABLE, generator = ""hostgroup_component_id_generator"")
+  @Column(name = ""id"", nullable = false, updatable = false)
+  private Long id;
+
   @Column(name = ""hostgroup_name"", nullable = false, insertable = false, updatable = false)
   private String hostGroupName;
 
-  @Id
   @Column(name = ""blueprint_name"", nullable = false, insertable = false, updatable = false)
   private String blueprintName;
 
-  @Id
   @Column(name = ""name"", nullable = false, insertable = true, updatable = false)
   private String name;
 
   @Column(name = ""provision_action"", nullable = true, insertable = true, updatable = false)
   private String provisionAction;
 
+  @Column(name = ""mpack_name"", nullable = true, insertable = true, updatable = false)","[{'comment': 'Why no an mpack association here by foriegn key - why duplicated fields?', 'commenter': 'jonathan-hurley'}]"
125,ambari-server/src/main/java/org/apache/ambari/server/serveraction/kerberos/KerberosOperationHandler.java,"@@ -354,8 +354,9 @@ protected Keytab createKeytab(String principal, String password, Integer keyNumb
 
         keytab.setEntries(keytabEntries);
       }
+    } else {
+      throw new KerberosOperationException(""Failed to create keytab file, the key encryption types you set are unsupported"");","[{'comment': 'i think that this is too late in the process to be useful.  However you might want to take a look at `org.apache.ambari.server.serveraction.kerberos.KerberosOperationHandler#translateEncryptionType` and throw an exception there if the requested encryption type is not valid.  \r\n\r\nOr maybe do it in `org.apache.ambari.server.serveraction.kerberos.KerberosOperationHandler#translateEncryptionTypes` if none of the requested encryption types yield a supported type. \r\n\r\nI think you might want to log a message in `org.apache.ambari.server.serveraction.kerberos.KerberosOperationHandler#translateEncryptionType` and throw the exception in `org.apache.ambari.server.serveraction.kerberos.KerberosOperationHandler#translateEncryptionTypes`.\r\n', 'commenter': 'rlevas'}, {'comment': 'Thanks you for your review!\r\nUpdated to log a message in `#translateEncryptionType` and throw the exception in `#translateEncryptionTypes`. Throwing the exception in `#translateEncryptionType` seems overkill.', 'commenter': 'aajisaka'}]"
133,ambari-server/src/test/java/org/apache/ambari/server/topology/BlueprintImplTest.java,"@@ -178,6 +178,8 @@ public void testValidateConfigurations__hostGroupConfigForNameNodeHAPositive() t
     Configuration group2Configuration = new Configuration(group2Props, EMPTY_ATTRIBUTES, configuration);
     expect(group2.getConfiguration()).andReturn(group2Configuration).atLeastOnce();
 
+    org.apache.ambari.server.configuration.Configuration serverConfig = setupConfigurationWithGPLLicense(true);","[{'comment': 'It may make sense to convert `serverConfig` to a member variable initialized in `setup()`.', 'commenter': 'adoroszlai'}]"
155,ambari-server/src/test/java/org/apache/ambari/server/utils/TestVersionUtils.java,"@@ -168,4 +167,98 @@ public void testVersionCompareError() {
     expectedException.expectMessage(""maxLengthToCompare cannot be less than 0"");
     VersionUtils.compareVersions(""2"", ""1"", -1);
   }
+
+  @Test
+  public void testCompareVersionsWithHotfixAndBuildNumber() {","[{'comment': 'Would be nice to extract various test cases in this method into separate methods with descriptive method names.  (Pros: avoid need to reset `errMessage`, finer grained info about success/failure, may even be shorter)', 'commenter': 'adoroszlai'}]"
155,ambari-server/src/test/java/org/apache/ambari/server/utils/TestVersionUtils.java,"@@ -168,4 +167,98 @@ public void testVersionCompareError() {
     expectedException.expectMessage(""maxLengthToCompare cannot be less than 0"");
     VersionUtils.compareVersions(""2"", ""1"", -1);
   }
+
+  @Test
+  public void testCompareVersionsWithHotfixAndBuildNumber() {
+    String errMessage = null;
+    try {
+      MpackVersion.parse(null);
+    } catch (IllegalArgumentException e) {
+      errMessage = e.getMessage();
+    }
+    Assert.assertTrue(""Mpack version can't be empty or null"".equals(errMessage));","[{'comment': 'Please consider extracting such messages to constants.', 'commenter': 'adoroszlai'}]"
155,ambari-server/src/main/java/org/apache/ambari/server/utils/MpackVersion.java,"@@ -0,0 +1,130 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.utils;
+
+
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+import org.apache.commons.lang.StringUtils;
+
+public class MpackVersion implements Comparable<MpackVersion> {
+
+  private final static String versionWithHotfixAndBuildPattern = ""^([0-9]+).([0-9]+).([0-9]+)-h([0-9]+)-b([0-9]+)"";
+  private final static String versionWithBuildPattern = ""^([0-9]+).([0-9]+).([0-9]+)-b([0-9]+)"";
+  private final static String legacyStackVersionPattern = ""^([0-9]+).([0-9]+).([0-9]+).([0-9]+)-([0-9]+)"";
+
+  private int major;
+  private int minor;
+  private int maint;
+  private int hotfix;
+  private int build;
+
+
+  public MpackVersion(int major, int minor, int maint, int hotfix, int build) {
+    this.major = major;
+    this.minor = minor;
+    this.maint = maint;
+    this.hotfix = hotfix;
+    this.build = build;
+  }
+
+  public static MpackVersion parse(String mpackVersion) {
+    Matcher versionMatcher = validateMpackVersion(mpackVersion);
+    MpackVersion result = null;
+
+    if (versionMatcher.pattern().pattern().equals(versionWithBuildPattern)) {
+      result = new MpackVersion(Integer.parseInt(versionMatcher.group(1)), Integer.parseInt(versionMatcher.group(2)),
+              Integer.parseInt(versionMatcher.group(3)), 0, Integer.parseInt(versionMatcher.group(4)));
+
+    } else {
+      result = new MpackVersion(Integer.parseInt(versionMatcher.group(1)), Integer.parseInt(versionMatcher.group(2)),
+              Integer.parseInt(versionMatcher.group(3)), Integer.parseInt(versionMatcher.group(4)), Integer.parseInt(versionMatcher.group(5)));
+
+    }
+
+    return result;
+  }
+
+  public static MpackVersion parseStackVersion(String stackVersion) {
+    Matcher versionMatcher = validateStackVersion(stackVersion);
+    MpackVersion result = new MpackVersion(Integer.parseInt(versionMatcher.group(1)), Integer.parseInt(versionMatcher.group(2)),
+          Integer.parseInt(versionMatcher.group(3)), Integer.parseInt(versionMatcher.group(4)), Integer.parseInt(versionMatcher.group(5)));
+
+    return result;
+  }
+
+  private static Matcher validateStackVersion(String version) {
+    if (StringUtils.isEmpty(version)) {
+      throw new IllegalArgumentException(""Stack version can't be empty or null"");
+    }
+
+    String stackVersion = StringUtils.trim(version);
+
+    Pattern patternWithHotfix = Pattern.compile(versionWithHotfixAndBuildPattern);
+    Pattern patternLegacyStackVersion = Pattern.compile(legacyStackVersionPattern);
+
+    Matcher versionMatcher = patternWithHotfix.matcher(stackVersion);
+    if (!versionMatcher.find()) {
+      versionMatcher = patternLegacyStackVersion.matcher(stackVersion);
+      if (!versionMatcher.find()) {
+        throw new IllegalArgumentException(""Wrong format for stack version, should be N.N.N.N-N or N.N.N-hN-bN"");
+      }
+    }
+
+    return versionMatcher;
+  }
+
+  private static Matcher validateMpackVersion(String version) {
+    if (StringUtils.isEmpty(version)) {
+      throw new IllegalArgumentException(""Mpack version can't be empty or null"");
+    }
+
+    String mpackVersion = StringUtils.trim(version);
+
+    Pattern patternWithHotfix = Pattern.compile(versionWithHotfixAndBuildPattern);
+    Pattern patternWithoutHotfix = Pattern.compile(versionWithBuildPattern);
+
+    Matcher versionMatcher = patternWithHotfix.matcher(mpackVersion);
+    if (!versionMatcher.find()) {
+      versionMatcher = patternWithoutHotfix.matcher(mpackVersion);
+      if (!versionMatcher.find()) {
+        throw new IllegalArgumentException(""Wrong format for mpack version, should be N.N.N-bN or N.N.N-hN-bN"");
+      }
+    }
+
+    return versionMatcher;
+  }
+
+  @Override
+  public int compareTo(MpackVersion other) {","[{'comment': 'I think `equals()` (and `hashCode()`) should be overridden, too, for consistency.', 'commenter': 'adoroszlai'}]"
155,ambari-server/src/main/java/org/apache/ambari/server/utils/MpackVersion.java,"@@ -0,0 +1,130 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.utils;
+
+
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+import org.apache.commons.lang.StringUtils;
+
+public class MpackVersion implements Comparable<MpackVersion> {
+
+  private final static String versionWithHotfixAndBuildPattern = ""^([0-9]+).([0-9]+).([0-9]+)-h([0-9]+)-b([0-9]+)"";","[{'comment': 'Constant name should be in `UPPER_CASE`.', 'commenter': 'adoroszlai'}]"
155,ambari-server/src/test/java/org/apache/ambari/server/utils/TestVersionUtils.java,"@@ -17,13 +17,12 @@
  */
 package org.apache.ambari.server.utils;
 
+import junit.framework.Assert;","[{'comment': ""Moving this here would cause Checkstyle error:\r\n\r\n```\r\nWrong order for 'org.apache.ambari.server.bootstrap.BootStrapImpl' import. [ImportOrder]\r\n```\r\n\r\nPlease use `org.junit` instead of `junit.framework` (latter is only for backwards compatibility)."", 'commenter': 'adoroszlai'}]"
155,ambari-server/src/test/java/org/apache/ambari/server/utils/TestVersionUtils.java,"@@ -168,4 +167,98 @@ public void testVersionCompareError() {
     expectedException.expectMessage(""maxLengthToCompare cannot be less than 0"");
     VersionUtils.compareVersions(""2"", ""1"", -1);
   }
+
+  @Test
+  public void testCompareVersionsWithHotfixAndBuildNumber() {
+    String errMessage = null;
+    try {
+      MpackVersion.parse(null);
+    } catch (IllegalArgumentException e) {
+      errMessage = e.getMessage();
+    }
+    Assert.assertTrue(""Mpack version can't be empty or null"".equals(errMessage));
+
+
+    try {
+      errMessage = null;
+      MpackVersion.parse("""");
+    } catch (IllegalArgumentException e) {
+      errMessage = e.getMessage();
+    }
+    Assert.assertTrue(""Mpack version can't be empty or null"".equals(errMessage));","[{'comment': '`assertTrue(x.equals(y))` is better written as `assertEquals(x, y)`, not only shorter, but also provides values in case of failure.', 'commenter': 'adoroszlai'}]"
155,ambari-server/src/main/java/org/apache/ambari/server/utils/MpackVersion.java,"@@ -0,0 +1,130 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.utils;
+
+
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+import org.apache.commons.lang.StringUtils;
+
+public class MpackVersion implements Comparable<MpackVersion> {
+
+  private final static String versionWithHotfixAndBuildPattern = ""^([0-9]+).([0-9]+).([0-9]+)-h([0-9]+)-b([0-9]+)"";
+  private final static String versionWithBuildPattern = ""^([0-9]+).([0-9]+).([0-9]+)-b([0-9]+)"";
+  private final static String legacyStackVersionPattern = ""^([0-9]+).([0-9]+).([0-9]+).([0-9]+)-([0-9]+)"";
+
+  private int major;
+  private int minor;
+  private int maint;
+  private int hotfix;
+  private int build;
+
+
+  public MpackVersion(int major, int minor, int maint, int hotfix, int build) {
+    this.major = major;
+    this.minor = minor;
+    this.maint = maint;
+    this.hotfix = hotfix;
+    this.build = build;
+  }
+
+  public static MpackVersion parse(String mpackVersion) {
+    Matcher versionMatcher = validateMpackVersion(mpackVersion);
+    MpackVersion result = null;
+
+    if (versionMatcher.pattern().pattern().equals(versionWithBuildPattern)) {
+      result = new MpackVersion(Integer.parseInt(versionMatcher.group(1)), Integer.parseInt(versionMatcher.group(2)),
+              Integer.parseInt(versionMatcher.group(3)), 0, Integer.parseInt(versionMatcher.group(4)));
+
+    } else {
+      result = new MpackVersion(Integer.parseInt(versionMatcher.group(1)), Integer.parseInt(versionMatcher.group(2)),
+              Integer.parseInt(versionMatcher.group(3)), Integer.parseInt(versionMatcher.group(4)), Integer.parseInt(versionMatcher.group(5)));
+
+    }
+
+    return result;
+  }
+
+  public static MpackVersion parseStackVersion(String stackVersion) {
+    Matcher versionMatcher = validateStackVersion(stackVersion);
+    MpackVersion result = new MpackVersion(Integer.parseInt(versionMatcher.group(1)), Integer.parseInt(versionMatcher.group(2)),
+          Integer.parseInt(versionMatcher.group(3)), Integer.parseInt(versionMatcher.group(4)), Integer.parseInt(versionMatcher.group(5)));
+
+    return result;
+  }
+
+  private static Matcher validateStackVersion(String version) {
+    if (StringUtils.isEmpty(version)) {
+      throw new IllegalArgumentException(""Stack version can't be empty or null"");
+    }
+
+    String stackVersion = StringUtils.trim(version);
+
+    Pattern patternWithHotfix = Pattern.compile(versionWithHotfixAndBuildPattern);","[{'comment': 'Please consider creating `Pattern` constants instead of `String` to avoid the need to compile each time.', 'commenter': 'adoroszlai'}]"
155,ambari-server/src/main/java/org/apache/ambari/server/utils/ModuleVersion.java,"@@ -0,0 +1,110 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.utils;
+
+
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+import org.apache.commons.lang.StringUtils;","[{'comment': ""Checkstyle says:\r\n\r\n```\r\n'org.apache.commons.lang.StringUtils' should be separated from previous imports. [ImportOrder]\r\n```"", 'commenter': 'adoroszlai'}]"
155,ambari-server/src/main/java/org/apache/ambari/server/utils/MpackVersion.java,"@@ -0,0 +1,130 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.utils;
+
+
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+import org.apache.commons.lang.StringUtils;","[{'comment': ""Checkstyle says:\r\n\r\n```\r\n'org.apache.commons.lang.StringUtils' should be separated from previous imports. [ImportOrder]\r\n```"", 'commenter': 'adoroszlai'}]"
155,ambari-server/src/main/java/org/apache/ambari/server/utils/ModuleVersion.java,"@@ -20,12 +20,16 @@
 
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
+
 import org.apache.commons.lang.StringUtils;
 
 public class ModuleVersion implements Comparable<ModuleVersion> {
 
-  private static final String versionWithHotfixAndBuildPattern = ""^([0-9]+).([0-9]+).([0-9]+).([0-9]+)-h([0-9]+)-b([0-9]+)"";
-  private static final String versionWithBuildPattern = ""^([0-9]+).([0-9]+).([0-9]+).([0-9]+)-b([0-9]+)"";
+  private static final String VERSION_WITH_HOTFIX_AND_BUILD_PATTERN = ""^([0-9]+).([0-9]+).([0-9]+).([0-9]+)-h([0-9]+)-b([0-9]+)"";
+  private static final String VERSION_WITH_BUILD_PATTERN = ""^([0-9]+).([0-9]+).([0-9]+).([0-9]+)-b([0-9]+)"";
+
+  private static final Pattern patternWithHotfix = Pattern.compile(VERSION_WITH_HOTFIX_AND_BUILD_PATTERN);","[{'comment': 'The new constants should also observe the same naming convention.', 'commenter': 'adoroszlai'}]"
155,ambari-server/src/main/java/org/apache/ambari/server/utils/ModuleVersion.java,"@@ -106,5 +107,31 @@ public int compareTo(ModuleVersion other) {
     return result > 0 ? 1 : result < 0 ? -1 : 0;
   }
 
+  @Override
+  public boolean equals(Object o) {
+    if (this == o) return true;
+    if (o == null || getClass() != o.getClass()) return false;
+
+    ModuleVersion that = (ModuleVersion) o;
+
+    if (apacheMajor != that.apacheMajor) return false;","[{'comment': 'Can be simplified using `&&`, eg.\r\n\r\n```\r\nreturn apacheMajor == that.apacheMajor &&\r\n  apacheMinor == that.apacheMinor &&\r\n  ...;\r\n```', 'commenter': 'adoroszlai'}]"
155,ambari-server/src/main/java/org/apache/ambari/server/utils/ModuleVersion.java,"@@ -106,5 +107,31 @@ public int compareTo(ModuleVersion other) {
     return result > 0 ? 1 : result < 0 ? -1 : 0;
   }
 
+  @Override
+  public boolean equals(Object o) {
+    if (this == o) return true;
+    if (o == null || getClass() != o.getClass()) return false;
+
+    ModuleVersion that = (ModuleVersion) o;
+
+    if (apacheMajor != that.apacheMajor) return false;
+    if (apacheMinor != that.apacheMinor) return false;
+    if (build != that.build) return false;
+    if (hotfix != that.hotfix) return false;
+    if (internalMaint != that.internalMaint) return false;
+    if (internalMinor != that.internalMinor) return false;
 
+    return true;
+  }
+
+  @Override
+  public int hashCode() {
+    int result = apacheMajor;","[{'comment': 'Can be simplified using `Objects.hash()`.', 'commenter': 'adoroszlai'}, {'comment': '@adoroszlai nope, why to box primitives to calculate hash?   it is not effective', 'commenter': 'hapylestat'}]"
155,ambari-server/src/main/java/org/apache/ambari/server/utils/MpackVersion.java,"@@ -0,0 +1,155 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.utils;
+
+
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+import org.apache.commons.lang.StringUtils;
+
+public class MpackVersion implements Comparable<MpackVersion> {","[{'comment': 'Add documentation to class and public methods.', 'commenter': 'jayush'}, {'comment': 'Done', 'commenter': 'vbrodetskyi'}]"
155,ambari-common/src/main/python/resource_management/libraries/functions/module_version.py,"@@ -0,0 +1,152 @@
+""""""
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  ""License""); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an ""AS IS"" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+""""""
+
+import re
+
+
+class ModuleVersion(object):","[{'comment': 'Add python documentation for ModuleVersion class', 'commenter': 'jayush'}, {'comment': 'Done', 'commenter': 'vbrodetskyi'}]"
155,ambari-common/src/main/python/resource_management/libraries/functions/mpack_version.py,"@@ -0,0 +1,200 @@
+""""""
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  ""License""); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an ""AS IS"" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+""""""
+
+import re
+
+
+class MpackVersion(object):","[{'comment': 'Add python documentation for MpackVersion class', 'commenter': 'jayush'}, {'comment': 'Done', 'commenter': 'vbrodetskyi'}]"
155,ambari-server/src/main/java/org/apache/ambari/server/utils/MpackVersion.java,"@@ -0,0 +1,155 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.utils;
+
+
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+import org.apache.commons.lang.StringUtils;
+
+public class MpackVersion implements Comparable<MpackVersion> {
+
+  private final static String VERSION_WITH_HOTFIX_AND_BUILD_PATTERN = ""^([0-9]+).([0-9]+).([0-9]+)-h([0-9]+)-b([0-9]+)"";
+  private final static String VERSION_WITH_BUILD_PATTERN = ""^([0-9]+).([0-9]+).([0-9]+)-b([0-9]+)"";
+  private final static String LEGACY_STACK_VERSION_PATTERN = ""^([0-9]+).([0-9]+).([0-9]+).([0-9]+)-([0-9]+)"";
+
+  private final static Pattern PATTERN_WITH_HOTFIX = Pattern.compile(VERSION_WITH_HOTFIX_AND_BUILD_PATTERN);
+  private final static Pattern PATTERN_LEGACY_STACK_VERSION = Pattern.compile(LEGACY_STACK_VERSION_PATTERN);
+  private final static Pattern PATTERN_WITHOUT_HOTFIX = Pattern.compile(VERSION_WITH_BUILD_PATTERN);
+
+  private int major;
+  private int minor;
+  private int maint;
+  private int hotfix;
+  private int build;
+
+
+  public MpackVersion(int major, int minor, int maint, int hotfix, int build) {","[{'comment': 'Add java documentation to class and public methods', 'commenter': 'jayush'}, {'comment': 'Done', 'commenter': 'vbrodetskyi'}, {'comment': 'Hi @jayush , Do we need to handle modules without versions like MYSQL?', 'commenter': 'mradha25'}, {'comment': ""@mradha25 \r\nThis is a utility function, we shouldn't add the special casing for external modules (i.e. MYSQL) in the utility function. "", 'commenter': 'jayush'}]"
155,ambari-common/src/main/python/resource_management/libraries/functions/mpack_version.py,"@@ -0,0 +1,207 @@
+""""""
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  ""License""); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an ""AS IS"" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+""""""
+
+import re
+
+""""""
+ This class should be used to compare mpack and stack versions.
+ Base method which should be used is parse/parse_stack_version, depends
+ on which versions you want to compare. This method will validate and parse
+ version which you will pass as parameter, and return object of current class with
+ parsed version. Same thing you should do with another version, with which you are
+ planning to compare previous one. After that, use ""=="", "">"", ""<"" to get final result.
+""""""
+class MpackVersion(object):
+  __module_version_pattern = ""(?P<major>[0-9]+).(?P<minor>[0-9]+).(?P<maint>[0-9]+)(-h(?P<hotfix>[0-9]+))*-b(?P<build>[0-9]+)""","[{'comment': 'Hi,\r\nIf we want to support for eg HDP-2.6.1.0 then we will have __module_legacy_stack_version_pattern to match that and __mpack_version_pattern to match new schema for eg hdpcore 3.0.0-b123 / hdpcore 3.0.0-h7-b111\r\nI am not sure I understand the need for module version pattern since you have a separate class to handle it.', 'commenter': 'mradha25'}, {'comment': 'I believe this should be called mpack_version_pattern, mpack_legacy_stack_version_pattern, mpack_version_regex etc instead of ""module""', 'commenter': 'jayush'}, {'comment': 'Done', 'commenter': 'vbrodetskyi'}]"
155,ambari-common/src/main/python/resource_management/libraries/functions/mpack_version.py,"@@ -0,0 +1,207 @@
+""""""
+  Licensed to the Apache Software Foundation (ASF) under one
+  or more contributor license agreements.  See the NOTICE file
+  distributed with this work for additional information
+  regarding copyright ownership.  The ASF licenses this file
+  to you under the Apache License, Version 2.0 (the
+  ""License""); you may not use this file except in compliance
+  with the License.  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an ""AS IS"" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+""""""
+
+import re
+
+""""""
+ This class should be used to compare mpack and stack versions.
+ Base method which should be used is parse/parse_stack_version, depends
+ on which versions you want to compare. This method will validate and parse
+ version which you will pass as parameter, and return object of current class with
+ parsed version. Same thing you should do with another version, with which you are
+ planning to compare previous one. After that, use ""=="", "">"", ""<"" to get final result.
+""""""
+class MpackVersion(object):
+  __mpack_version_pattern = ""(?P<major>[0-9]+).(?P<minor>[0-9]+).(?P<maint>[0-9]+)(-h(?P<hotfix>[0-9]+))*-b(?P<build>[0-9]+)""
+  __mpack_legacy_stack_version_pattern = ""(?P<major>[0-9]+).(?P<minor>[0-9]+).(?P<maint>[0-9]+).(?P<hotfix>[0-9]+)(-(?P<build>[0-9]+))""","[{'comment': 'Hi @vbrodetskyi ,\r\n__mpack_version_pattern and __mpack_legacy_stack_version_pattern show the same pattern. I think \r\n__mpack_legacy_stack_version_pattern = ^([0-9]+).([0-9]+).([0-9]+).([0-9]+)-([0-9]+) (same as the java counterpart)', 'commenter': 'mradha25'}, {'comment': ""No it's not the same pattern. \r\n__mpack_version_pattern = N.N.N-hN-bN\r\n__mpack_legacy_stack_version_pattern = N.N.N.N-N\r\nAnd it's the same as we have in java."", 'commenter': 'vbrodetskyi'}]"
156,ambari-server/src/main/python/ambari-server.py,"@@ -524,8 +524,10 @@ def init_ldap_sync_parser_options(parser):
 
 @OsFamilyFuncImpl(OsFamilyImpl.DEFAULT)
 def init_ldap_setup_parser_options(parser):
-  parser.add_option('--ldap-url', default=None, help=""Primary url for LDAP"", dest=""ldap_url"")
-  parser.add_option('--ldap-secondary-url', default=None, help=""Secondary url for LDAP"", dest=""ldap_secondary_url"")
+  parser.add_option('--ldap-url-host', default=None, help=""Primary Host for LDAP"", dest=""ldap_url_host"")","[{'comment': 'Nit: ldap-url-host --> ldap-primary-host', 'commenter': 'rlevas'}]"
156,ambari-server/src/main/python/ambari-server.py,"@@ -524,8 +524,10 @@ def init_ldap_sync_parser_options(parser):
 
 @OsFamilyFuncImpl(OsFamilyImpl.DEFAULT)
 def init_ldap_setup_parser_options(parser):
-  parser.add_option('--ldap-url', default=None, help=""Primary url for LDAP"", dest=""ldap_url"")
-  parser.add_option('--ldap-secondary-url', default=None, help=""Secondary url for LDAP"", dest=""ldap_secondary_url"")
+  parser.add_option('--ldap-url-host', default=None, help=""Primary Host for LDAP"", dest=""ldap_url_host"")
+  parser.add_option('--ldap-url-port', default=None, help=""Primary Port for LDAP"", dest=""ldap_url_port"")","[{'comment': 'Nit: ldap-url-port --> ldap-primary-port', 'commenter': 'rlevas'}]"
156,ambari-server/src/main/python/ambari-server.py,"@@ -524,8 +524,10 @@ def init_ldap_sync_parser_options(parser):
 
 @OsFamilyFuncImpl(OsFamilyImpl.DEFAULT)
 def init_ldap_setup_parser_options(parser):
-  parser.add_option('--ldap-url', default=None, help=""Primary url for LDAP"", dest=""ldap_url"")
-  parser.add_option('--ldap-secondary-url', default=None, help=""Secondary url for LDAP"", dest=""ldap_secondary_url"")
+  parser.add_option('--ldap-url-host', default=None, help=""Primary Host for LDAP"", dest=""ldap_url_host"")
+  parser.add_option('--ldap-url-port', default=None, help=""Primary Port for LDAP"", dest=""ldap_url_port"")
+  parser.add_option('--ldap-secondary-url-host', default=None, help=""Secondary Host for LDAP"", dest=""ldap_secondary_url_host"")","[{'comment': 'Nit: ldap-secondary-url-host --> ldap-secondary-host', 'commenter': 'rlevas'}]"
156,ambari-server/src/main/python/ambari-server.py,"@@ -524,8 +524,10 @@ def init_ldap_sync_parser_options(parser):
 
 @OsFamilyFuncImpl(OsFamilyImpl.DEFAULT)
 def init_ldap_setup_parser_options(parser):
-  parser.add_option('--ldap-url', default=None, help=""Primary url for LDAP"", dest=""ldap_url"")
-  parser.add_option('--ldap-secondary-url', default=None, help=""Secondary url for LDAP"", dest=""ldap_secondary_url"")
+  parser.add_option('--ldap-url-host', default=None, help=""Primary Host for LDAP"", dest=""ldap_url_host"")
+  parser.add_option('--ldap-url-port', default=None, help=""Primary Port for LDAP"", dest=""ldap_url_port"")
+  parser.add_option('--ldap-secondary-url-host', default=None, help=""Secondary Host for LDAP"", dest=""ldap_secondary_url_host"")
+  parser.add_option('--ldap-secondary-url-port', default=None, help=""Secondary Port for LDAP"", dest=""ldap_secondary_url_port"")","[{'comment': 'Nit: ldap-secondary-url-port --> ldap-secondary-port', 'commenter': 'rlevas'}]"
156,ambari-server/src/main/python/ambari_server/setupSecurity.py,"@@ -278,7 +281,7 @@ def sync_ldap(options):
 
   properties = get_ambari_properties()
 
-  if get_value_from_properties(properties,CLIENT_SECURITY_KEY,"""") == 'pam':
+  if get_value_from_properties(properties,CLIENT_SECURITY,"""") == 'pam':","[{'comment': 'This block should probably be more generic.  Rather than checking for PAM, it should ensure the type is LDAP and fail if it is not.  In the future, PAM may not be the only alternative. ', 'commenter': 'rlevas'}]"
156,ambari-server/src/main/python/ambari_server/setupSecurity.py,"@@ -587,45 +590,89 @@ def __init__(self, properties, i_option, i_prop_name, i_prop_val_pattern, i_prom
 def init_ldap_properties_list_reqd(properties, options):
   # python2.x dict is not ordered
   ldap_properties = [
-    LdapPropTemplate(properties, options.ldap_url, ""authentication.ldap.primaryUrl"", ""Primary URL* {{host:port}} {0}: "", REGEX_HOSTNAME_PORT, False),
-    LdapPropTemplate(properties, options.ldap_secondary_url, ""authentication.ldap.secondaryUrl"", ""Secondary URL {{host:port}} {0}: "", REGEX_HOSTNAME_PORT, True),
-    LdapPropTemplate(properties, options.ldap_ssl, ""authentication.ldap.useSSL"", ""Use SSL* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false""),
-    LdapPropTemplate(properties, options.ldap_user_attr, ""authentication.ldap.usernameAttribute"", ""User name attribute* {0}: "", REGEX_ANYTHING, False, ""uid""),
-    LdapPropTemplate(properties, options.ldap_base_dn, ""authentication.ldap.baseDn"", ""Base DN* {0}: "", REGEX_ANYTHING, False),
-    LdapPropTemplate(properties, options.ldap_referral, ""authentication.ldap.referral"", ""Referral method [follow/ignore] {0}: "", REGEX_REFERRAL, True),
-    LdapPropTemplate(properties, options.ldap_bind_anonym, ""authentication.ldap.bindAnonymously"" ""Bind anonymously* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false"")
+    LdapPropTemplate(properties, options.ldap_url_host, ""ambari.ldap.connectivity.server.host"", ""Primary URL Host* {0}: "", REGEX_HOSTNAME, False),
+    LdapPropTemplate(properties, options.ldap_url_port, ""ambari.ldap.connectivity.server.port"", ""Primary URL Port* {0}: "", REGEX_PORT, False),
+    LdapPropTemplate(properties, options.ldap_secondary_url_host, ""ambari.ldap.connectivity.secondary.server.host"", ""Secondary URL Host {0}: "", REGEX_HOSTNAME, True),
+    LdapPropTemplate(properties, options.ldap_secondary_url_port, ""ambari.ldap.connectivity.secondary.server.port"", ""Secondary URL Port {0}: "", REGEX_PORT, True),
+    LdapPropTemplate(properties, options.ldap_ssl, ""ambari.ldap.connectivity.use_ssl"", ""Use SSL* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false""),
+    LdapPropTemplate(properties, options.ldap_user_attr, ""ambari.ldap.attributes.user.name_attr"", ""User name attribute* {0}: "", REGEX_ANYTHING, False, ""uid""),
+    LdapPropTemplate(properties, options.ldap_base_dn, ""ambari.ldap.attributes.user.search_base"", ""Base DN* {0}: "", REGEX_ANYTHING, False, ""dc=ambari,dc=apache,dc=org""),
+    LdapPropTemplate(properties, options.ldap_referral, ""ambari.ldap.advanced.referrals"", ""Referral method [follow/ignore] {0}: "", REGEX_REFERRAL, True),
+    LdapPropTemplate(properties, options.ldap_bind_anonym, ""ambari.ldap.connectivity.anonymous_bind"" ""Bind anonymously* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false"")
   ]
   return ldap_properties
 
 @OsFamilyFuncImpl(OsFamilyImpl.DEFAULT)
 def init_ldap_properties_list_reqd(properties, options):
   ldap_properties = [
-    LdapPropTemplate(properties, options.ldap_url, LDAP_PRIMARY_URL_PROPERTY, ""Primary URL* {{host:port}} {0}: "", REGEX_HOSTNAME_PORT, False),
-    LdapPropTemplate(properties, options.ldap_secondary_url, ""authentication.ldap.secondaryUrl"", ""Secondary URL {{host:port}} {0}: "", REGEX_HOSTNAME_PORT, True),
-    LdapPropTemplate(properties, options.ldap_ssl, ""authentication.ldap.useSSL"", ""Use SSL* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false""),
-    LdapPropTemplate(properties, options.ldap_user_class, ""authentication.ldap.userObjectClass"", ""User object class* {0}: "", REGEX_ANYTHING, False, ""posixAccount""),
-    LdapPropTemplate(properties, options.ldap_user_attr, ""authentication.ldap.usernameAttribute"", ""User name attribute* {0}: "", REGEX_ANYTHING, False, ""uid""),
-    LdapPropTemplate(properties, options.ldap_group_class, ""authentication.ldap.groupObjectClass"", ""Group object class* {0}: "", REGEX_ANYTHING, False, ""posixGroup""),
-    LdapPropTemplate(properties, options.ldap_group_attr, ""authentication.ldap.groupNamingAttr"", ""Group name attribute* {0}: "", REGEX_ANYTHING, False, ""cn""),
-    LdapPropTemplate(properties, options.ldap_member_attr, ""authentication.ldap.groupMembershipAttr"", ""Group member attribute* {0}: "", REGEX_ANYTHING, False, ""memberUid""),
-    LdapPropTemplate(properties, options.ldap_dn, ""authentication.ldap.dnAttribute"", ""Distinguished name attribute* {0}: "", REGEX_ANYTHING, False, ""dn""),
-    LdapPropTemplate(properties, options.ldap_base_dn, ""authentication.ldap.baseDn"", ""Base DN* {0}: "", REGEX_ANYTHING, False),
-    LdapPropTemplate(properties, options.ldap_referral, ""authentication.ldap.referral"", ""Referral method [follow/ignore] {0}: "", REGEX_REFERRAL, True),
-    LdapPropTemplate(properties, options.ldap_bind_anonym, ""authentication.ldap.bindAnonymously"", ""Bind anonymously* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false""),
-    LdapPropTemplate(properties, options.ldap_sync_username_collisions_behavior, ""ldap.sync.username.collision.behavior"", ""Handling behavior for username collisions [convert/skip] for LDAP sync* {0}: "", REGEX_SKIP_CONVERT, False, ""convert""),
+    LdapPropTemplate(properties, options.ldap_url_host, ""ambari.ldap.connectivity.server.host"", ""Primary URL Host* {0}: "", REGEX_HOSTNAME, False),
+    LdapPropTemplate(properties, options.ldap_url_port, ""ambari.ldap.connectivity.server.port"", ""Primary URL Port* {0}: "", REGEX_PORT, False),
+    LdapPropTemplate(properties, options.ldap_secondary_url_host, ""ambari.ldap.connectivity.secondary.server.host"", ""Secondary URL Host {0}: "", REGEX_HOSTNAME, True),
+    LdapPropTemplate(properties, options.ldap_secondary_url_port, ""ambari.ldap.connectivity.secondary.server.port"", ""Secondary URL Port {0}: "", REGEX_PORT, True),
+    LdapPropTemplate(properties, options.ldap_ssl, ""ambari.ldap.connectivity.use_ssl"", ""Use SSL* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false""),
+    LdapPropTemplate(properties, options.ldap_user_class, ""ambari.ldap.attributes.user.object_class"", ""User object class* {0}: "", REGEX_ANYTHING, False, ""person""),
+    LdapPropTemplate(properties, options.ldap_user_attr, ""ambari.ldap.attributes.user.name_attr"", ""User name attribute* {0}: "", REGEX_ANYTHING, False, ""uid""),
+    LdapPropTemplate(properties, options.ldap_group_class, ""ambari.ldap.attributes.group.object_class"", ""Group object class* {0}: "", REGEX_ANYTHING, False, ""ou=groups,dc=ambari,dc=apache,dc=org""),
+    LdapPropTemplate(properties, options.ldap_group_attr, ""ambari.ldap.attributes.group.name_attr"", ""Group name attribute* {0}: "", REGEX_ANYTHING, False, ""cn""),
+    LdapPropTemplate(properties, options.ldap_member_attr, ""ambari.ldap.attributes.group.member_attr"", ""Group member attribute* {0}: "", REGEX_ANYTHING, False, ""memberUid""),
+    LdapPropTemplate(properties, options.ldap_dn, ""ambari.ldap.attributes.dn_attr"", ""Distinguished name attribute* {0}: "", REGEX_ANYTHING, False, ""dn""),
+    LdapPropTemplate(properties, options.ldap_base_dn, ""ambari.ldap.attributes.user.search_base"", ""Base DN* {0}: "", REGEX_ANYTHING, False, ""dc=ambari,dc=apache,dc=org""),
+    LdapPropTemplate(properties, options.ldap_referral, ""ambari.ldap.advanced.referrals"", ""Referral method [follow/ignore] {0}: "", REGEX_REFERRAL, True),
+    LdapPropTemplate(properties, options.ldap_bind_anonym, ""ambari.ldap.connectivity.anonymous_bind"", ""Bind anonymously* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false""),
+    LdapPropTemplate(properties, options.ldap_sync_username_collisions_behavior, ""ambari.ldap.advance.collision_behavior"", ""Handling behavior for username collisions [convert/skip] for LDAP sync* {0}: "", REGEX_SKIP_CONVERT, False, ""convert""),
+    LdapPropTemplate(properties, options.ldap_force_lowercase_usernames, ""ambari.ldap.advanced.force_lowercase_usernames"", ""Force lower-case user names [true/false] {0}:"", REGEX_TRUE_FALSE, True),
+    LdapPropTemplate(properties, options.ldap_pagination_enabled, ""ambari.ldap.advanced.pagination_enabled"", ""Results from LDAP are paginated when requested [true/false] {0}:"", REGEX_TRUE_FALSE, True)
   ]
   return ldap_properties
 
+def update_ldap_configuration(properties, ldap_property_value_map):
+  admin_login = get_validated_string_input(""Enter Ambari Admin login: "", None, None, None, False, False)
+  admin_password = get_validated_string_input(""Enter Ambari Admin password: "", None, None, None, True, False)
+  url = get_ambari_server_api_base(properties) + SETUP_LDAP_CONFIG_URL
+  admin_auth = base64.encodestring('%s:%s' % (admin_login, admin_password)).replace('\n', '')
+  request = urllib2.Request(url)
+  request.add_header('Authorization', 'Basic %s' % admin_auth)
+  request.add_header('X-Requested-By', 'ambari')
+  data = {
+    ""Configuration"": {
+      ""service_name"": ""AMBARI"",","[{'comment': 'This is not needed.', 'commenter': 'rlevas'}]"
156,ambari-server/src/main/python/ambari_server/setupSecurity.py,"@@ -587,45 +590,89 @@ def __init__(self, properties, i_option, i_prop_name, i_prop_val_pattern, i_prom
 def init_ldap_properties_list_reqd(properties, options):
   # python2.x dict is not ordered
   ldap_properties = [
-    LdapPropTemplate(properties, options.ldap_url, ""authentication.ldap.primaryUrl"", ""Primary URL* {{host:port}} {0}: "", REGEX_HOSTNAME_PORT, False),
-    LdapPropTemplate(properties, options.ldap_secondary_url, ""authentication.ldap.secondaryUrl"", ""Secondary URL {{host:port}} {0}: "", REGEX_HOSTNAME_PORT, True),
-    LdapPropTemplate(properties, options.ldap_ssl, ""authentication.ldap.useSSL"", ""Use SSL* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false""),
-    LdapPropTemplate(properties, options.ldap_user_attr, ""authentication.ldap.usernameAttribute"", ""User name attribute* {0}: "", REGEX_ANYTHING, False, ""uid""),
-    LdapPropTemplate(properties, options.ldap_base_dn, ""authentication.ldap.baseDn"", ""Base DN* {0}: "", REGEX_ANYTHING, False),
-    LdapPropTemplate(properties, options.ldap_referral, ""authentication.ldap.referral"", ""Referral method [follow/ignore] {0}: "", REGEX_REFERRAL, True),
-    LdapPropTemplate(properties, options.ldap_bind_anonym, ""authentication.ldap.bindAnonymously"" ""Bind anonymously* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false"")
+    LdapPropTemplate(properties, options.ldap_url_host, ""ambari.ldap.connectivity.server.host"", ""Primary URL Host* {0}: "", REGEX_HOSTNAME, False),
+    LdapPropTemplate(properties, options.ldap_url_port, ""ambari.ldap.connectivity.server.port"", ""Primary URL Port* {0}: "", REGEX_PORT, False),
+    LdapPropTemplate(properties, options.ldap_secondary_url_host, ""ambari.ldap.connectivity.secondary.server.host"", ""Secondary URL Host {0}: "", REGEX_HOSTNAME, True),
+    LdapPropTemplate(properties, options.ldap_secondary_url_port, ""ambari.ldap.connectivity.secondary.server.port"", ""Secondary URL Port {0}: "", REGEX_PORT, True),
+    LdapPropTemplate(properties, options.ldap_ssl, ""ambari.ldap.connectivity.use_ssl"", ""Use SSL* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false""),
+    LdapPropTemplate(properties, options.ldap_user_attr, ""ambari.ldap.attributes.user.name_attr"", ""User name attribute* {0}: "", REGEX_ANYTHING, False, ""uid""),
+    LdapPropTemplate(properties, options.ldap_base_dn, ""ambari.ldap.attributes.user.search_base"", ""Base DN* {0}: "", REGEX_ANYTHING, False, ""dc=ambari,dc=apache,dc=org""),
+    LdapPropTemplate(properties, options.ldap_referral, ""ambari.ldap.advanced.referrals"", ""Referral method [follow/ignore] {0}: "", REGEX_REFERRAL, True),
+    LdapPropTemplate(properties, options.ldap_bind_anonym, ""ambari.ldap.connectivity.anonymous_bind"" ""Bind anonymously* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false"")
   ]
   return ldap_properties
 
 @OsFamilyFuncImpl(OsFamilyImpl.DEFAULT)
 def init_ldap_properties_list_reqd(properties, options):
   ldap_properties = [
-    LdapPropTemplate(properties, options.ldap_url, LDAP_PRIMARY_URL_PROPERTY, ""Primary URL* {{host:port}} {0}: "", REGEX_HOSTNAME_PORT, False),
-    LdapPropTemplate(properties, options.ldap_secondary_url, ""authentication.ldap.secondaryUrl"", ""Secondary URL {{host:port}} {0}: "", REGEX_HOSTNAME_PORT, True),
-    LdapPropTemplate(properties, options.ldap_ssl, ""authentication.ldap.useSSL"", ""Use SSL* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false""),
-    LdapPropTemplate(properties, options.ldap_user_class, ""authentication.ldap.userObjectClass"", ""User object class* {0}: "", REGEX_ANYTHING, False, ""posixAccount""),
-    LdapPropTemplate(properties, options.ldap_user_attr, ""authentication.ldap.usernameAttribute"", ""User name attribute* {0}: "", REGEX_ANYTHING, False, ""uid""),
-    LdapPropTemplate(properties, options.ldap_group_class, ""authentication.ldap.groupObjectClass"", ""Group object class* {0}: "", REGEX_ANYTHING, False, ""posixGroup""),
-    LdapPropTemplate(properties, options.ldap_group_attr, ""authentication.ldap.groupNamingAttr"", ""Group name attribute* {0}: "", REGEX_ANYTHING, False, ""cn""),
-    LdapPropTemplate(properties, options.ldap_member_attr, ""authentication.ldap.groupMembershipAttr"", ""Group member attribute* {0}: "", REGEX_ANYTHING, False, ""memberUid""),
-    LdapPropTemplate(properties, options.ldap_dn, ""authentication.ldap.dnAttribute"", ""Distinguished name attribute* {0}: "", REGEX_ANYTHING, False, ""dn""),
-    LdapPropTemplate(properties, options.ldap_base_dn, ""authentication.ldap.baseDn"", ""Base DN* {0}: "", REGEX_ANYTHING, False),
-    LdapPropTemplate(properties, options.ldap_referral, ""authentication.ldap.referral"", ""Referral method [follow/ignore] {0}: "", REGEX_REFERRAL, True),
-    LdapPropTemplate(properties, options.ldap_bind_anonym, ""authentication.ldap.bindAnonymously"", ""Bind anonymously* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false""),
-    LdapPropTemplate(properties, options.ldap_sync_username_collisions_behavior, ""ldap.sync.username.collision.behavior"", ""Handling behavior for username collisions [convert/skip] for LDAP sync* {0}: "", REGEX_SKIP_CONVERT, False, ""convert""),
+    LdapPropTemplate(properties, options.ldap_url_host, ""ambari.ldap.connectivity.server.host"", ""Primary URL Host* {0}: "", REGEX_HOSTNAME, False),
+    LdapPropTemplate(properties, options.ldap_url_port, ""ambari.ldap.connectivity.server.port"", ""Primary URL Port* {0}: "", REGEX_PORT, False),
+    LdapPropTemplate(properties, options.ldap_secondary_url_host, ""ambari.ldap.connectivity.secondary.server.host"", ""Secondary URL Host {0}: "", REGEX_HOSTNAME, True),
+    LdapPropTemplate(properties, options.ldap_secondary_url_port, ""ambari.ldap.connectivity.secondary.server.port"", ""Secondary URL Port {0}: "", REGEX_PORT, True),
+    LdapPropTemplate(properties, options.ldap_ssl, ""ambari.ldap.connectivity.use_ssl"", ""Use SSL* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false""),
+    LdapPropTemplate(properties, options.ldap_user_class, ""ambari.ldap.attributes.user.object_class"", ""User object class* {0}: "", REGEX_ANYTHING, False, ""person""),
+    LdapPropTemplate(properties, options.ldap_user_attr, ""ambari.ldap.attributes.user.name_attr"", ""User name attribute* {0}: "", REGEX_ANYTHING, False, ""uid""),
+    LdapPropTemplate(properties, options.ldap_group_class, ""ambari.ldap.attributes.group.object_class"", ""Group object class* {0}: "", REGEX_ANYTHING, False, ""ou=groups,dc=ambari,dc=apache,dc=org""),
+    LdapPropTemplate(properties, options.ldap_group_attr, ""ambari.ldap.attributes.group.name_attr"", ""Group name attribute* {0}: "", REGEX_ANYTHING, False, ""cn""),
+    LdapPropTemplate(properties, options.ldap_member_attr, ""ambari.ldap.attributes.group.member_attr"", ""Group member attribute* {0}: "", REGEX_ANYTHING, False, ""memberUid""),
+    LdapPropTemplate(properties, options.ldap_dn, ""ambari.ldap.attributes.dn_attr"", ""Distinguished name attribute* {0}: "", REGEX_ANYTHING, False, ""dn""),
+    LdapPropTemplate(properties, options.ldap_base_dn, ""ambari.ldap.attributes.user.search_base"", ""Base DN* {0}: "", REGEX_ANYTHING, False, ""dc=ambari,dc=apache,dc=org""),
+    LdapPropTemplate(properties, options.ldap_referral, ""ambari.ldap.advanced.referrals"", ""Referral method [follow/ignore] {0}: "", REGEX_REFERRAL, True),
+    LdapPropTemplate(properties, options.ldap_bind_anonym, ""ambari.ldap.connectivity.anonymous_bind"", ""Bind anonymously* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false""),
+    LdapPropTemplate(properties, options.ldap_sync_username_collisions_behavior, ""ambari.ldap.advance.collision_behavior"", ""Handling behavior for username collisions [convert/skip] for LDAP sync* {0}: "", REGEX_SKIP_CONVERT, False, ""convert""),
+    LdapPropTemplate(properties, options.ldap_force_lowercase_usernames, ""ambari.ldap.advanced.force_lowercase_usernames"", ""Force lower-case user names [true/false] {0}:"", REGEX_TRUE_FALSE, True),
+    LdapPropTemplate(properties, options.ldap_pagination_enabled, ""ambari.ldap.advanced.pagination_enabled"", ""Results from LDAP are paginated when requested [true/false] {0}:"", REGEX_TRUE_FALSE, True)
   ]
   return ldap_properties
 
+def update_ldap_configuration(properties, ldap_property_value_map):
+  admin_login = get_validated_string_input(""Enter Ambari Admin login: "", None, None, None, False, False)
+  admin_password = get_validated_string_input(""Enter Ambari Admin password: "", None, None, None, True, False)
+  url = get_ambari_server_api_base(properties) + SETUP_LDAP_CONFIG_URL
+  admin_auth = base64.encodestring('%s:%s' % (admin_login, admin_password)).replace('\n', '')
+  request = urllib2.Request(url)
+  request.add_header('Authorization', 'Basic %s' % admin_auth)
+  request.add_header('X-Requested-By', 'ambari')
+  data = {
+    ""Configuration"": {
+      ""service_name"": ""AMBARI"",
+      ""component_name"": ""AMBARI_SERVER"",","[{'comment': 'This is not needed', 'commenter': 'rlevas'}]"
156,ambari-server/src/main/python/ambari_server/setupSecurity.py,"@@ -587,45 +590,89 @@ def __init__(self, properties, i_option, i_prop_name, i_prop_val_pattern, i_prom
 def init_ldap_properties_list_reqd(properties, options):
   # python2.x dict is not ordered
   ldap_properties = [
-    LdapPropTemplate(properties, options.ldap_url, ""authentication.ldap.primaryUrl"", ""Primary URL* {{host:port}} {0}: "", REGEX_HOSTNAME_PORT, False),
-    LdapPropTemplate(properties, options.ldap_secondary_url, ""authentication.ldap.secondaryUrl"", ""Secondary URL {{host:port}} {0}: "", REGEX_HOSTNAME_PORT, True),
-    LdapPropTemplate(properties, options.ldap_ssl, ""authentication.ldap.useSSL"", ""Use SSL* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false""),
-    LdapPropTemplate(properties, options.ldap_user_attr, ""authentication.ldap.usernameAttribute"", ""User name attribute* {0}: "", REGEX_ANYTHING, False, ""uid""),
-    LdapPropTemplate(properties, options.ldap_base_dn, ""authentication.ldap.baseDn"", ""Base DN* {0}: "", REGEX_ANYTHING, False),
-    LdapPropTemplate(properties, options.ldap_referral, ""authentication.ldap.referral"", ""Referral method [follow/ignore] {0}: "", REGEX_REFERRAL, True),
-    LdapPropTemplate(properties, options.ldap_bind_anonym, ""authentication.ldap.bindAnonymously"" ""Bind anonymously* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false"")
+    LdapPropTemplate(properties, options.ldap_url_host, ""ambari.ldap.connectivity.server.host"", ""Primary URL Host* {0}: "", REGEX_HOSTNAME, False),
+    LdapPropTemplate(properties, options.ldap_url_port, ""ambari.ldap.connectivity.server.port"", ""Primary URL Port* {0}: "", REGEX_PORT, False),
+    LdapPropTemplate(properties, options.ldap_secondary_url_host, ""ambari.ldap.connectivity.secondary.server.host"", ""Secondary URL Host {0}: "", REGEX_HOSTNAME, True),
+    LdapPropTemplate(properties, options.ldap_secondary_url_port, ""ambari.ldap.connectivity.secondary.server.port"", ""Secondary URL Port {0}: "", REGEX_PORT, True),
+    LdapPropTemplate(properties, options.ldap_ssl, ""ambari.ldap.connectivity.use_ssl"", ""Use SSL* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false""),
+    LdapPropTemplate(properties, options.ldap_user_attr, ""ambari.ldap.attributes.user.name_attr"", ""User name attribute* {0}: "", REGEX_ANYTHING, False, ""uid""),
+    LdapPropTemplate(properties, options.ldap_base_dn, ""ambari.ldap.attributes.user.search_base"", ""Base DN* {0}: "", REGEX_ANYTHING, False, ""dc=ambari,dc=apache,dc=org""),
+    LdapPropTemplate(properties, options.ldap_referral, ""ambari.ldap.advanced.referrals"", ""Referral method [follow/ignore] {0}: "", REGEX_REFERRAL, True),
+    LdapPropTemplate(properties, options.ldap_bind_anonym, ""ambari.ldap.connectivity.anonymous_bind"" ""Bind anonymously* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false"")
   ]
   return ldap_properties
 
 @OsFamilyFuncImpl(OsFamilyImpl.DEFAULT)
 def init_ldap_properties_list_reqd(properties, options):
   ldap_properties = [
-    LdapPropTemplate(properties, options.ldap_url, LDAP_PRIMARY_URL_PROPERTY, ""Primary URL* {{host:port}} {0}: "", REGEX_HOSTNAME_PORT, False),
-    LdapPropTemplate(properties, options.ldap_secondary_url, ""authentication.ldap.secondaryUrl"", ""Secondary URL {{host:port}} {0}: "", REGEX_HOSTNAME_PORT, True),
-    LdapPropTemplate(properties, options.ldap_ssl, ""authentication.ldap.useSSL"", ""Use SSL* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false""),
-    LdapPropTemplate(properties, options.ldap_user_class, ""authentication.ldap.userObjectClass"", ""User object class* {0}: "", REGEX_ANYTHING, False, ""posixAccount""),
-    LdapPropTemplate(properties, options.ldap_user_attr, ""authentication.ldap.usernameAttribute"", ""User name attribute* {0}: "", REGEX_ANYTHING, False, ""uid""),
-    LdapPropTemplate(properties, options.ldap_group_class, ""authentication.ldap.groupObjectClass"", ""Group object class* {0}: "", REGEX_ANYTHING, False, ""posixGroup""),
-    LdapPropTemplate(properties, options.ldap_group_attr, ""authentication.ldap.groupNamingAttr"", ""Group name attribute* {0}: "", REGEX_ANYTHING, False, ""cn""),
-    LdapPropTemplate(properties, options.ldap_member_attr, ""authentication.ldap.groupMembershipAttr"", ""Group member attribute* {0}: "", REGEX_ANYTHING, False, ""memberUid""),
-    LdapPropTemplate(properties, options.ldap_dn, ""authentication.ldap.dnAttribute"", ""Distinguished name attribute* {0}: "", REGEX_ANYTHING, False, ""dn""),
-    LdapPropTemplate(properties, options.ldap_base_dn, ""authentication.ldap.baseDn"", ""Base DN* {0}: "", REGEX_ANYTHING, False),
-    LdapPropTemplate(properties, options.ldap_referral, ""authentication.ldap.referral"", ""Referral method [follow/ignore] {0}: "", REGEX_REFERRAL, True),
-    LdapPropTemplate(properties, options.ldap_bind_anonym, ""authentication.ldap.bindAnonymously"", ""Bind anonymously* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false""),
-    LdapPropTemplate(properties, options.ldap_sync_username_collisions_behavior, ""ldap.sync.username.collision.behavior"", ""Handling behavior for username collisions [convert/skip] for LDAP sync* {0}: "", REGEX_SKIP_CONVERT, False, ""convert""),
+    LdapPropTemplate(properties, options.ldap_url_host, ""ambari.ldap.connectivity.server.host"", ""Primary URL Host* {0}: "", REGEX_HOSTNAME, False),
+    LdapPropTemplate(properties, options.ldap_url_port, ""ambari.ldap.connectivity.server.port"", ""Primary URL Port* {0}: "", REGEX_PORT, False),
+    LdapPropTemplate(properties, options.ldap_secondary_url_host, ""ambari.ldap.connectivity.secondary.server.host"", ""Secondary URL Host {0}: "", REGEX_HOSTNAME, True),
+    LdapPropTemplate(properties, options.ldap_secondary_url_port, ""ambari.ldap.connectivity.secondary.server.port"", ""Secondary URL Port {0}: "", REGEX_PORT, True),
+    LdapPropTemplate(properties, options.ldap_ssl, ""ambari.ldap.connectivity.use_ssl"", ""Use SSL* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false""),
+    LdapPropTemplate(properties, options.ldap_user_class, ""ambari.ldap.attributes.user.object_class"", ""User object class* {0}: "", REGEX_ANYTHING, False, ""person""),
+    LdapPropTemplate(properties, options.ldap_user_attr, ""ambari.ldap.attributes.user.name_attr"", ""User name attribute* {0}: "", REGEX_ANYTHING, False, ""uid""),
+    LdapPropTemplate(properties, options.ldap_group_class, ""ambari.ldap.attributes.group.object_class"", ""Group object class* {0}: "", REGEX_ANYTHING, False, ""ou=groups,dc=ambari,dc=apache,dc=org""),
+    LdapPropTemplate(properties, options.ldap_group_attr, ""ambari.ldap.attributes.group.name_attr"", ""Group name attribute* {0}: "", REGEX_ANYTHING, False, ""cn""),
+    LdapPropTemplate(properties, options.ldap_member_attr, ""ambari.ldap.attributes.group.member_attr"", ""Group member attribute* {0}: "", REGEX_ANYTHING, False, ""memberUid""),
+    LdapPropTemplate(properties, options.ldap_dn, ""ambari.ldap.attributes.dn_attr"", ""Distinguished name attribute* {0}: "", REGEX_ANYTHING, False, ""dn""),
+    LdapPropTemplate(properties, options.ldap_base_dn, ""ambari.ldap.attributes.user.search_base"", ""Base DN* {0}: "", REGEX_ANYTHING, False, ""dc=ambari,dc=apache,dc=org""),
+    LdapPropTemplate(properties, options.ldap_referral, ""ambari.ldap.advanced.referrals"", ""Referral method [follow/ignore] {0}: "", REGEX_REFERRAL, True),
+    LdapPropTemplate(properties, options.ldap_bind_anonym, ""ambari.ldap.connectivity.anonymous_bind"", ""Bind anonymously* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false""),
+    LdapPropTemplate(properties, options.ldap_sync_username_collisions_behavior, ""ambari.ldap.advance.collision_behavior"", ""Handling behavior for username collisions [convert/skip] for LDAP sync* {0}: "", REGEX_SKIP_CONVERT, False, ""convert""),
+    LdapPropTemplate(properties, options.ldap_force_lowercase_usernames, ""ambari.ldap.advanced.force_lowercase_usernames"", ""Force lower-case user names [true/false] {0}:"", REGEX_TRUE_FALSE, True),
+    LdapPropTemplate(properties, options.ldap_pagination_enabled, ""ambari.ldap.advanced.pagination_enabled"", ""Results from LDAP are paginated when requested [true/false] {0}:"", REGEX_TRUE_FALSE, True)
   ]
   return ldap_properties
 
+def update_ldap_configuration(properties, ldap_property_value_map):
+  admin_login = get_validated_string_input(""Enter Ambari Admin login: "", None, None, None, False, False)
+  admin_password = get_validated_string_input(""Enter Ambari Admin password: "", None, None, None, True, False)
+  url = get_ambari_server_api_base(properties) + SETUP_LDAP_CONFIG_URL
+  admin_auth = base64.encodestring('%s:%s' % (admin_login, admin_password)).replace('\n', '')
+  request = urllib2.Request(url)
+  request.add_header('Authorization', 'Basic %s' % admin_auth)
+  request.add_header('X-Requested-By', 'ambari')
+  data = {
+    ""Configuration"": {
+      ""service_name"": ""AMBARI"",
+      ""component_name"": ""AMBARI_SERVER"",
+      ""category"": ""ldap-configuration"",
+      ""properties"": {
+      }
+    }
+  }
+  data['Configuration']['properties'] = ldap_property_value_map
+  request.add_data(json.dumps(data))
+  request.get_method = lambda: 'PUT'
+
+  try:
+    response = urllib2.urlopen(request)
+  except Exception as e:
+    err = 'Updating LDAP configuration failed. Error details: %s' % e
+    raise FatalException(1, err)
+
+  response_status_code = response.getcode()
+  if response_status_code != 200:
+    err = 'Error during syncing. Http status code - ' + str(response_status_code)","[{'comment': 'Copy/paste issue? :)', 'commenter': 'rlevas'}]"
156,ambari-server/src/main/python/ambari_server/setupSecurity.py,"@@ -587,45 +590,89 @@ def __init__(self, properties, i_option, i_prop_name, i_prop_val_pattern, i_prom
 def init_ldap_properties_list_reqd(properties, options):
   # python2.x dict is not ordered
   ldap_properties = [
-    LdapPropTemplate(properties, options.ldap_url, ""authentication.ldap.primaryUrl"", ""Primary URL* {{host:port}} {0}: "", REGEX_HOSTNAME_PORT, False),
-    LdapPropTemplate(properties, options.ldap_secondary_url, ""authentication.ldap.secondaryUrl"", ""Secondary URL {{host:port}} {0}: "", REGEX_HOSTNAME_PORT, True),
-    LdapPropTemplate(properties, options.ldap_ssl, ""authentication.ldap.useSSL"", ""Use SSL* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false""),
-    LdapPropTemplate(properties, options.ldap_user_attr, ""authentication.ldap.usernameAttribute"", ""User name attribute* {0}: "", REGEX_ANYTHING, False, ""uid""),
-    LdapPropTemplate(properties, options.ldap_base_dn, ""authentication.ldap.baseDn"", ""Base DN* {0}: "", REGEX_ANYTHING, False),
-    LdapPropTemplate(properties, options.ldap_referral, ""authentication.ldap.referral"", ""Referral method [follow/ignore] {0}: "", REGEX_REFERRAL, True),
-    LdapPropTemplate(properties, options.ldap_bind_anonym, ""authentication.ldap.bindAnonymously"" ""Bind anonymously* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false"")
+    LdapPropTemplate(properties, options.ldap_url_host, ""ambari.ldap.connectivity.server.host"", ""Primary URL Host* {0}: "", REGEX_HOSTNAME, False),
+    LdapPropTemplate(properties, options.ldap_url_port, ""ambari.ldap.connectivity.server.port"", ""Primary URL Port* {0}: "", REGEX_PORT, False),
+    LdapPropTemplate(properties, options.ldap_secondary_url_host, ""ambari.ldap.connectivity.secondary.server.host"", ""Secondary URL Host {0}: "", REGEX_HOSTNAME, True),
+    LdapPropTemplate(properties, options.ldap_secondary_url_port, ""ambari.ldap.connectivity.secondary.server.port"", ""Secondary URL Port {0}: "", REGEX_PORT, True),
+    LdapPropTemplate(properties, options.ldap_ssl, ""ambari.ldap.connectivity.use_ssl"", ""Use SSL* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false""),
+    LdapPropTemplate(properties, options.ldap_user_attr, ""ambari.ldap.attributes.user.name_attr"", ""User name attribute* {0}: "", REGEX_ANYTHING, False, ""uid""),
+    LdapPropTemplate(properties, options.ldap_base_dn, ""ambari.ldap.attributes.user.search_base"", ""Base DN* {0}: "", REGEX_ANYTHING, False, ""dc=ambari,dc=apache,dc=org""),
+    LdapPropTemplate(properties, options.ldap_referral, ""ambari.ldap.advanced.referrals"", ""Referral method [follow/ignore] {0}: "", REGEX_REFERRAL, True),
+    LdapPropTemplate(properties, options.ldap_bind_anonym, ""ambari.ldap.connectivity.anonymous_bind"" ""Bind anonymously* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false"")
   ]
   return ldap_properties
 
 @OsFamilyFuncImpl(OsFamilyImpl.DEFAULT)
 def init_ldap_properties_list_reqd(properties, options):
   ldap_properties = [
-    LdapPropTemplate(properties, options.ldap_url, LDAP_PRIMARY_URL_PROPERTY, ""Primary URL* {{host:port}} {0}: "", REGEX_HOSTNAME_PORT, False),
-    LdapPropTemplate(properties, options.ldap_secondary_url, ""authentication.ldap.secondaryUrl"", ""Secondary URL {{host:port}} {0}: "", REGEX_HOSTNAME_PORT, True),
-    LdapPropTemplate(properties, options.ldap_ssl, ""authentication.ldap.useSSL"", ""Use SSL* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false""),
-    LdapPropTemplate(properties, options.ldap_user_class, ""authentication.ldap.userObjectClass"", ""User object class* {0}: "", REGEX_ANYTHING, False, ""posixAccount""),
-    LdapPropTemplate(properties, options.ldap_user_attr, ""authentication.ldap.usernameAttribute"", ""User name attribute* {0}: "", REGEX_ANYTHING, False, ""uid""),
-    LdapPropTemplate(properties, options.ldap_group_class, ""authentication.ldap.groupObjectClass"", ""Group object class* {0}: "", REGEX_ANYTHING, False, ""posixGroup""),
-    LdapPropTemplate(properties, options.ldap_group_attr, ""authentication.ldap.groupNamingAttr"", ""Group name attribute* {0}: "", REGEX_ANYTHING, False, ""cn""),
-    LdapPropTemplate(properties, options.ldap_member_attr, ""authentication.ldap.groupMembershipAttr"", ""Group member attribute* {0}: "", REGEX_ANYTHING, False, ""memberUid""),
-    LdapPropTemplate(properties, options.ldap_dn, ""authentication.ldap.dnAttribute"", ""Distinguished name attribute* {0}: "", REGEX_ANYTHING, False, ""dn""),
-    LdapPropTemplate(properties, options.ldap_base_dn, ""authentication.ldap.baseDn"", ""Base DN* {0}: "", REGEX_ANYTHING, False),
-    LdapPropTemplate(properties, options.ldap_referral, ""authentication.ldap.referral"", ""Referral method [follow/ignore] {0}: "", REGEX_REFERRAL, True),
-    LdapPropTemplate(properties, options.ldap_bind_anonym, ""authentication.ldap.bindAnonymously"", ""Bind anonymously* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false""),
-    LdapPropTemplate(properties, options.ldap_sync_username_collisions_behavior, ""ldap.sync.username.collision.behavior"", ""Handling behavior for username collisions [convert/skip] for LDAP sync* {0}: "", REGEX_SKIP_CONVERT, False, ""convert""),
+    LdapPropTemplate(properties, options.ldap_url_host, ""ambari.ldap.connectivity.server.host"", ""Primary URL Host* {0}: "", REGEX_HOSTNAME, False),
+    LdapPropTemplate(properties, options.ldap_url_port, ""ambari.ldap.connectivity.server.port"", ""Primary URL Port* {0}: "", REGEX_PORT, False),
+    LdapPropTemplate(properties, options.ldap_secondary_url_host, ""ambari.ldap.connectivity.secondary.server.host"", ""Secondary URL Host {0}: "", REGEX_HOSTNAME, True),
+    LdapPropTemplate(properties, options.ldap_secondary_url_port, ""ambari.ldap.connectivity.secondary.server.port"", ""Secondary URL Port {0}: "", REGEX_PORT, True),
+    LdapPropTemplate(properties, options.ldap_ssl, ""ambari.ldap.connectivity.use_ssl"", ""Use SSL* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false""),
+    LdapPropTemplate(properties, options.ldap_user_class, ""ambari.ldap.attributes.user.object_class"", ""User object class* {0}: "", REGEX_ANYTHING, False, ""person""),
+    LdapPropTemplate(properties, options.ldap_user_attr, ""ambari.ldap.attributes.user.name_attr"", ""User name attribute* {0}: "", REGEX_ANYTHING, False, ""uid""),
+    LdapPropTemplate(properties, options.ldap_group_class, ""ambari.ldap.attributes.group.object_class"", ""Group object class* {0}: "", REGEX_ANYTHING, False, ""ou=groups,dc=ambari,dc=apache,dc=org""),
+    LdapPropTemplate(properties, options.ldap_group_attr, ""ambari.ldap.attributes.group.name_attr"", ""Group name attribute* {0}: "", REGEX_ANYTHING, False, ""cn""),
+    LdapPropTemplate(properties, options.ldap_member_attr, ""ambari.ldap.attributes.group.member_attr"", ""Group member attribute* {0}: "", REGEX_ANYTHING, False, ""memberUid""),
+    LdapPropTemplate(properties, options.ldap_dn, ""ambari.ldap.attributes.dn_attr"", ""Distinguished name attribute* {0}: "", REGEX_ANYTHING, False, ""dn""),
+    LdapPropTemplate(properties, options.ldap_base_dn, ""ambari.ldap.attributes.user.search_base"", ""Base DN* {0}: "", REGEX_ANYTHING, False, ""dc=ambari,dc=apache,dc=org""),
+    LdapPropTemplate(properties, options.ldap_referral, ""ambari.ldap.advanced.referrals"", ""Referral method [follow/ignore] {0}: "", REGEX_REFERRAL, True),
+    LdapPropTemplate(properties, options.ldap_bind_anonym, ""ambari.ldap.connectivity.anonymous_bind"", ""Bind anonymously* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false""),
+    LdapPropTemplate(properties, options.ldap_sync_username_collisions_behavior, ""ambari.ldap.advance.collision_behavior"", ""Handling behavior for username collisions [convert/skip] for LDAP sync* {0}: "", REGEX_SKIP_CONVERT, False, ""convert""),
+    LdapPropTemplate(properties, options.ldap_force_lowercase_usernames, ""ambari.ldap.advanced.force_lowercase_usernames"", ""Force lower-case user names [true/false] {0}:"", REGEX_TRUE_FALSE, True),
+    LdapPropTemplate(properties, options.ldap_pagination_enabled, ""ambari.ldap.advanced.pagination_enabled"", ""Results from LDAP are paginated when requested [true/false] {0}:"", REGEX_TRUE_FALSE, True)
   ]
   return ldap_properties
 
+def update_ldap_configuration(properties, ldap_property_value_map):
+  admin_login = get_validated_string_input(""Enter Ambari Admin login: "", None, None, None, False, False)
+  admin_password = get_validated_string_input(""Enter Ambari Admin password: "", None, None, None, True, False)
+  url = get_ambari_server_api_base(properties) + SETUP_LDAP_CONFIG_URL
+  admin_auth = base64.encodestring('%s:%s' % (admin_login, admin_password)).replace('\n', '')
+  request = urllib2.Request(url)
+  request.add_header('Authorization', 'Basic %s' % admin_auth)
+  request.add_header('X-Requested-By', 'ambari')
+  data = {
+    ""Configuration"": {
+      ""service_name"": ""AMBARI"",
+      ""component_name"": ""AMBARI_SERVER"",
+      ""category"": ""ldap-configuration"",
+      ""properties"": {
+      }
+    }
+  }
+  data['Configuration']['properties'] = ldap_property_value_map
+  request.add_data(json.dumps(data))
+  request.get_method = lambda: 'PUT'
+
+  try:
+    response = urllib2.urlopen(request)
+  except Exception as e:
+    err = 'Updating LDAP configuration failed. Error details: %s' % e
+    raise FatalException(1, err)
+
+  response_status_code = response.getcode()
+  if response_status_code != 200:
+    err = 'Error during syncing. Http status code - ' + str(response_status_code)
+    raise FatalException(1, err)
+
 def setup_ldap(options):
   logger.info(""Setup LDAP."")
+
   if not is_root():","[{'comment': 'This block is not needed.  The local user account is not relevant here. ', 'commenter': 'rlevas'}]"
156,ambari-server/src/main/python/ambari_server/setupSecurity.py,"@@ -587,45 +590,89 @@ def __init__(self, properties, i_option, i_prop_name, i_prop_val_pattern, i_prom
 def init_ldap_properties_list_reqd(properties, options):
   # python2.x dict is not ordered
   ldap_properties = [
-    LdapPropTemplate(properties, options.ldap_url, ""authentication.ldap.primaryUrl"", ""Primary URL* {{host:port}} {0}: "", REGEX_HOSTNAME_PORT, False),
-    LdapPropTemplate(properties, options.ldap_secondary_url, ""authentication.ldap.secondaryUrl"", ""Secondary URL {{host:port}} {0}: "", REGEX_HOSTNAME_PORT, True),
-    LdapPropTemplate(properties, options.ldap_ssl, ""authentication.ldap.useSSL"", ""Use SSL* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false""),
-    LdapPropTemplate(properties, options.ldap_user_attr, ""authentication.ldap.usernameAttribute"", ""User name attribute* {0}: "", REGEX_ANYTHING, False, ""uid""),
-    LdapPropTemplate(properties, options.ldap_base_dn, ""authentication.ldap.baseDn"", ""Base DN* {0}: "", REGEX_ANYTHING, False),
-    LdapPropTemplate(properties, options.ldap_referral, ""authentication.ldap.referral"", ""Referral method [follow/ignore] {0}: "", REGEX_REFERRAL, True),
-    LdapPropTemplate(properties, options.ldap_bind_anonym, ""authentication.ldap.bindAnonymously"" ""Bind anonymously* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false"")
+    LdapPropTemplate(properties, options.ldap_url_host, ""ambari.ldap.connectivity.server.host"", ""Primary URL Host* {0}: "", REGEX_HOSTNAME, False),
+    LdapPropTemplate(properties, options.ldap_url_port, ""ambari.ldap.connectivity.server.port"", ""Primary URL Port* {0}: "", REGEX_PORT, False),
+    LdapPropTemplate(properties, options.ldap_secondary_url_host, ""ambari.ldap.connectivity.secondary.server.host"", ""Secondary URL Host {0}: "", REGEX_HOSTNAME, True),
+    LdapPropTemplate(properties, options.ldap_secondary_url_port, ""ambari.ldap.connectivity.secondary.server.port"", ""Secondary URL Port {0}: "", REGEX_PORT, True),
+    LdapPropTemplate(properties, options.ldap_ssl, ""ambari.ldap.connectivity.use_ssl"", ""Use SSL* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false""),
+    LdapPropTemplate(properties, options.ldap_user_attr, ""ambari.ldap.attributes.user.name_attr"", ""User name attribute* {0}: "", REGEX_ANYTHING, False, ""uid""),
+    LdapPropTemplate(properties, options.ldap_base_dn, ""ambari.ldap.attributes.user.search_base"", ""Base DN* {0}: "", REGEX_ANYTHING, False, ""dc=ambari,dc=apache,dc=org""),
+    LdapPropTemplate(properties, options.ldap_referral, ""ambari.ldap.advanced.referrals"", ""Referral method [follow/ignore] {0}: "", REGEX_REFERRAL, True),
+    LdapPropTemplate(properties, options.ldap_bind_anonym, ""ambari.ldap.connectivity.anonymous_bind"" ""Bind anonymously* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false"")
   ]
   return ldap_properties
 
 @OsFamilyFuncImpl(OsFamilyImpl.DEFAULT)
 def init_ldap_properties_list_reqd(properties, options):
   ldap_properties = [
-    LdapPropTemplate(properties, options.ldap_url, LDAP_PRIMARY_URL_PROPERTY, ""Primary URL* {{host:port}} {0}: "", REGEX_HOSTNAME_PORT, False),
-    LdapPropTemplate(properties, options.ldap_secondary_url, ""authentication.ldap.secondaryUrl"", ""Secondary URL {{host:port}} {0}: "", REGEX_HOSTNAME_PORT, True),
-    LdapPropTemplate(properties, options.ldap_ssl, ""authentication.ldap.useSSL"", ""Use SSL* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false""),
-    LdapPropTemplate(properties, options.ldap_user_class, ""authentication.ldap.userObjectClass"", ""User object class* {0}: "", REGEX_ANYTHING, False, ""posixAccount""),
-    LdapPropTemplate(properties, options.ldap_user_attr, ""authentication.ldap.usernameAttribute"", ""User name attribute* {0}: "", REGEX_ANYTHING, False, ""uid""),
-    LdapPropTemplate(properties, options.ldap_group_class, ""authentication.ldap.groupObjectClass"", ""Group object class* {0}: "", REGEX_ANYTHING, False, ""posixGroup""),
-    LdapPropTemplate(properties, options.ldap_group_attr, ""authentication.ldap.groupNamingAttr"", ""Group name attribute* {0}: "", REGEX_ANYTHING, False, ""cn""),
-    LdapPropTemplate(properties, options.ldap_member_attr, ""authentication.ldap.groupMembershipAttr"", ""Group member attribute* {0}: "", REGEX_ANYTHING, False, ""memberUid""),
-    LdapPropTemplate(properties, options.ldap_dn, ""authentication.ldap.dnAttribute"", ""Distinguished name attribute* {0}: "", REGEX_ANYTHING, False, ""dn""),
-    LdapPropTemplate(properties, options.ldap_base_dn, ""authentication.ldap.baseDn"", ""Base DN* {0}: "", REGEX_ANYTHING, False),
-    LdapPropTemplate(properties, options.ldap_referral, ""authentication.ldap.referral"", ""Referral method [follow/ignore] {0}: "", REGEX_REFERRAL, True),
-    LdapPropTemplate(properties, options.ldap_bind_anonym, ""authentication.ldap.bindAnonymously"", ""Bind anonymously* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false""),
-    LdapPropTemplate(properties, options.ldap_sync_username_collisions_behavior, ""ldap.sync.username.collision.behavior"", ""Handling behavior for username collisions [convert/skip] for LDAP sync* {0}: "", REGEX_SKIP_CONVERT, False, ""convert""),
+    LdapPropTemplate(properties, options.ldap_url_host, ""ambari.ldap.connectivity.server.host"", ""Primary URL Host* {0}: "", REGEX_HOSTNAME, False),
+    LdapPropTemplate(properties, options.ldap_url_port, ""ambari.ldap.connectivity.server.port"", ""Primary URL Port* {0}: "", REGEX_PORT, False),
+    LdapPropTemplate(properties, options.ldap_secondary_url_host, ""ambari.ldap.connectivity.secondary.server.host"", ""Secondary URL Host {0}: "", REGEX_HOSTNAME, True),
+    LdapPropTemplate(properties, options.ldap_secondary_url_port, ""ambari.ldap.connectivity.secondary.server.port"", ""Secondary URL Port {0}: "", REGEX_PORT, True),
+    LdapPropTemplate(properties, options.ldap_ssl, ""ambari.ldap.connectivity.use_ssl"", ""Use SSL* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false""),
+    LdapPropTemplate(properties, options.ldap_user_class, ""ambari.ldap.attributes.user.object_class"", ""User object class* {0}: "", REGEX_ANYTHING, False, ""person""),
+    LdapPropTemplate(properties, options.ldap_user_attr, ""ambari.ldap.attributes.user.name_attr"", ""User name attribute* {0}: "", REGEX_ANYTHING, False, ""uid""),
+    LdapPropTemplate(properties, options.ldap_group_class, ""ambari.ldap.attributes.group.object_class"", ""Group object class* {0}: "", REGEX_ANYTHING, False, ""ou=groups,dc=ambari,dc=apache,dc=org""),
+    LdapPropTemplate(properties, options.ldap_group_attr, ""ambari.ldap.attributes.group.name_attr"", ""Group name attribute* {0}: "", REGEX_ANYTHING, False, ""cn""),
+    LdapPropTemplate(properties, options.ldap_member_attr, ""ambari.ldap.attributes.group.member_attr"", ""Group member attribute* {0}: "", REGEX_ANYTHING, False, ""memberUid""),
+    LdapPropTemplate(properties, options.ldap_dn, ""ambari.ldap.attributes.dn_attr"", ""Distinguished name attribute* {0}: "", REGEX_ANYTHING, False, ""dn""),
+    LdapPropTemplate(properties, options.ldap_base_dn, ""ambari.ldap.attributes.user.search_base"", ""Base DN* {0}: "", REGEX_ANYTHING, False, ""dc=ambari,dc=apache,dc=org""),
+    LdapPropTemplate(properties, options.ldap_referral, ""ambari.ldap.advanced.referrals"", ""Referral method [follow/ignore] {0}: "", REGEX_REFERRAL, True),
+    LdapPropTemplate(properties, options.ldap_bind_anonym, ""ambari.ldap.connectivity.anonymous_bind"", ""Bind anonymously* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false""),
+    LdapPropTemplate(properties, options.ldap_sync_username_collisions_behavior, ""ambari.ldap.advance.collision_behavior"", ""Handling behavior for username collisions [convert/skip] for LDAP sync* {0}: "", REGEX_SKIP_CONVERT, False, ""convert""),
+    LdapPropTemplate(properties, options.ldap_force_lowercase_usernames, ""ambari.ldap.advanced.force_lowercase_usernames"", ""Force lower-case user names [true/false] {0}:"", REGEX_TRUE_FALSE, True),
+    LdapPropTemplate(properties, options.ldap_pagination_enabled, ""ambari.ldap.advanced.pagination_enabled"", ""Results from LDAP are paginated when requested [true/false] {0}:"", REGEX_TRUE_FALSE, True)
   ]
   return ldap_properties
 
+def update_ldap_configuration(properties, ldap_property_value_map):
+  admin_login = get_validated_string_input(""Enter Ambari Admin login: "", None, None, None, False, False)
+  admin_password = get_validated_string_input(""Enter Ambari Admin password: "", None, None, None, True, False)
+  url = get_ambari_server_api_base(properties) + SETUP_LDAP_CONFIG_URL
+  admin_auth = base64.encodestring('%s:%s' % (admin_login, admin_password)).replace('\n', '')
+  request = urllib2.Request(url)
+  request.add_header('Authorization', 'Basic %s' % admin_auth)
+  request.add_header('X-Requested-By', 'ambari')
+  data = {
+    ""Configuration"": {
+      ""service_name"": ""AMBARI"",
+      ""component_name"": ""AMBARI_SERVER"",
+      ""category"": ""ldap-configuration"",
+      ""properties"": {
+      }
+    }
+  }
+  data['Configuration']['properties'] = ldap_property_value_map
+  request.add_data(json.dumps(data))
+  request.get_method = lambda: 'PUT'
+
+  try:
+    response = urllib2.urlopen(request)
+  except Exception as e:
+    err = 'Updating LDAP configuration failed. Error details: %s' % e
+    raise FatalException(1, err)
+
+  response_status_code = response.getcode()
+  if response_status_code != 200:
+    err = 'Error during syncing. Http status code - ' + str(response_status_code)
+    raise FatalException(1, err)
+
 def setup_ldap(options):
   logger.info(""Setup LDAP."")
+
   if not is_root():
     err = 'Ambari-server setup-ldap should be run with ' \
           'root-level privileges'
     raise FatalException(4, err)
 
   properties = get_ambari_properties()
 
-  if get_value_from_properties(properties,CLIENT_SECURITY_KEY,"""") == 'pam':
+  server_status, pid = is_server_runing()
+  if not server_status:
+    err = 'Ambari Server is not running.'
+    raise FatalException(1, err)
+
+  if get_value_from_properties(properties,CLIENT_SECURITY,"""") == 'pam':","[{'comment': 'This should be changed to something like\r\n\r\n```\r\nclient_security = get_value_from_properties(properties,CLIENT_SECURITY,"""")\r\nif client_security != \'local\' or client_security != \'ldap\':\r\n query = client_security . "" is currently configured, do you wish to use LDAP instead [y/n] (n)? ""\r\n ...\r\n```\r\n\r\nYou might need to do a `None` check in there as well. ', 'commenter': 'rlevas'}]"
156,ambari-server/src/main/python/ambari_server/setupSecurity.py,"@@ -637,20 +684,16 @@ def setup_ldap(options):
 
   ldap_property_list_reqd = init_ldap_properties_list_reqd(properties, options)
 
-  ldap_property_list_opt = [""authentication.ldap.managerDn"",
+  ldap_property_list_opt = [""ambari.ldap.connectivity.bind_dn"",","[{'comment': 'There should be a constant created for ""ambari.ldap.connectivity.bind_dn"" to help avoid typo errors. ', 'commenter': 'rlevas'}]"
156,ambari-server/src/main/python/ambari_server/setupSecurity.py,"@@ -663,7 +706,7 @@ def setup_ldap(options):
     if input is not None and input != """":
       ldap_property_value_map[ldap_prop.prop_name] = input
 
-  bindAnonymously = ldap_property_value_map[""authentication.ldap.bindAnonymously""]
+  bindAnonymously = ldap_property_value_map[""ambari.ldap.connectivity.anonymous_bind""]","[{'comment': 'There should be a constant created for ""ambari.ldap.connectivity.anonymous_bind"" to help avoid typo errors.\r\n\r\n', 'commenter': 'rlevas'}]"
156,ambari-server/src/main/python/ambari_server/setupSecurity.py,"@@ -587,70 +592,104 @@ def __init__(self, properties, i_option, i_prop_name, i_prop_val_pattern, i_prom
 def init_ldap_properties_list_reqd(properties, options):
   # python2.x dict is not ordered
   ldap_properties = [
-    LdapPropTemplate(properties, options.ldap_url, ""authentication.ldap.primaryUrl"", ""Primary URL* {{host:port}} {0}: "", REGEX_HOSTNAME_PORT, False),
-    LdapPropTemplate(properties, options.ldap_secondary_url, ""authentication.ldap.secondaryUrl"", ""Secondary URL {{host:port}} {0}: "", REGEX_HOSTNAME_PORT, True),
-    LdapPropTemplate(properties, options.ldap_ssl, ""authentication.ldap.useSSL"", ""Use SSL* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false""),
-    LdapPropTemplate(properties, options.ldap_user_attr, ""authentication.ldap.usernameAttribute"", ""User name attribute* {0}: "", REGEX_ANYTHING, False, ""uid""),
-    LdapPropTemplate(properties, options.ldap_base_dn, ""authentication.ldap.baseDn"", ""Base DN* {0}: "", REGEX_ANYTHING, False),
-    LdapPropTemplate(properties, options.ldap_referral, ""authentication.ldap.referral"", ""Referral method [follow/ignore] {0}: "", REGEX_REFERRAL, True),
-    LdapPropTemplate(properties, options.ldap_bind_anonym, ""authentication.ldap.bindAnonymously"" ""Bind anonymously* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false"")
+    LdapPropTemplate(properties, options.ldap_url_host, ""ambari.ldap.connectivity.server.host"", ""Primary URL Host* {0}: "", REGEX_HOSTNAME, False),","[{'comment': ""Shouldn't `options.ldap_url_host` be `options.ldap_primary_host`?"", 'commenter': 'rlevas'}, {'comment': 'Fixed them all', 'commenter': 'smolnar82'}]"
156,ambari-server/src/main/python/ambari_server/setupSecurity.py,"@@ -587,70 +592,104 @@ def __init__(self, properties, i_option, i_prop_name, i_prop_val_pattern, i_prom
 def init_ldap_properties_list_reqd(properties, options):
   # python2.x dict is not ordered
   ldap_properties = [
-    LdapPropTemplate(properties, options.ldap_url, ""authentication.ldap.primaryUrl"", ""Primary URL* {{host:port}} {0}: "", REGEX_HOSTNAME_PORT, False),
-    LdapPropTemplate(properties, options.ldap_secondary_url, ""authentication.ldap.secondaryUrl"", ""Secondary URL {{host:port}} {0}: "", REGEX_HOSTNAME_PORT, True),
-    LdapPropTemplate(properties, options.ldap_ssl, ""authentication.ldap.useSSL"", ""Use SSL* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false""),
-    LdapPropTemplate(properties, options.ldap_user_attr, ""authentication.ldap.usernameAttribute"", ""User name attribute* {0}: "", REGEX_ANYTHING, False, ""uid""),
-    LdapPropTemplate(properties, options.ldap_base_dn, ""authentication.ldap.baseDn"", ""Base DN* {0}: "", REGEX_ANYTHING, False),
-    LdapPropTemplate(properties, options.ldap_referral, ""authentication.ldap.referral"", ""Referral method [follow/ignore] {0}: "", REGEX_REFERRAL, True),
-    LdapPropTemplate(properties, options.ldap_bind_anonym, ""authentication.ldap.bindAnonymously"" ""Bind anonymously* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false"")
+    LdapPropTemplate(properties, options.ldap_url_host, ""ambari.ldap.connectivity.server.host"", ""Primary URL Host* {0}: "", REGEX_HOSTNAME, False),
+    LdapPropTemplate(properties, options.ldap_url_port, ""ambari.ldap.connectivity.server.port"", ""Primary URL Port* {0}: "", REGEX_PORT, False),","[{'comment': ""Shouldn't `options.ldap_url_port` be `options.ldap_primary_port`?"", 'commenter': 'rlevas'}, {'comment': 'That is correct; lack of unit test case under WIN. Let me write it and fix this issue and send a new commit.', 'commenter': 'smolnar82'}]"
156,ambari-server/src/main/python/ambari_server/setupSecurity.py,"@@ -587,70 +592,104 @@ def __init__(self, properties, i_option, i_prop_name, i_prop_val_pattern, i_prom
 def init_ldap_properties_list_reqd(properties, options):
   # python2.x dict is not ordered
   ldap_properties = [
-    LdapPropTemplate(properties, options.ldap_url, ""authentication.ldap.primaryUrl"", ""Primary URL* {{host:port}} {0}: "", REGEX_HOSTNAME_PORT, False),
-    LdapPropTemplate(properties, options.ldap_secondary_url, ""authentication.ldap.secondaryUrl"", ""Secondary URL {{host:port}} {0}: "", REGEX_HOSTNAME_PORT, True),
-    LdapPropTemplate(properties, options.ldap_ssl, ""authentication.ldap.useSSL"", ""Use SSL* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false""),
-    LdapPropTemplate(properties, options.ldap_user_attr, ""authentication.ldap.usernameAttribute"", ""User name attribute* {0}: "", REGEX_ANYTHING, False, ""uid""),
-    LdapPropTemplate(properties, options.ldap_base_dn, ""authentication.ldap.baseDn"", ""Base DN* {0}: "", REGEX_ANYTHING, False),
-    LdapPropTemplate(properties, options.ldap_referral, ""authentication.ldap.referral"", ""Referral method [follow/ignore] {0}: "", REGEX_REFERRAL, True),
-    LdapPropTemplate(properties, options.ldap_bind_anonym, ""authentication.ldap.bindAnonymously"" ""Bind anonymously* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false"")
+    LdapPropTemplate(properties, options.ldap_url_host, ""ambari.ldap.connectivity.server.host"", ""Primary URL Host* {0}: "", REGEX_HOSTNAME, False),
+    LdapPropTemplate(properties, options.ldap_url_port, ""ambari.ldap.connectivity.server.port"", ""Primary URL Port* {0}: "", REGEX_PORT, False),
+    LdapPropTemplate(properties, options.ldap_secondary_url_host, ""ambari.ldap.connectivity.secondary.server.host"", ""Secondary URL Host {0}: "", REGEX_HOSTNAME, True),","[{'comment': ""Shouldn't `options.ldap_secondary_url_host` be `options.ldap_secondary_host`?"", 'commenter': 'rlevas'}]"
156,ambari-server/src/main/python/ambari_server/setupSecurity.py,"@@ -587,70 +592,104 @@ def __init__(self, properties, i_option, i_prop_name, i_prop_val_pattern, i_prom
 def init_ldap_properties_list_reqd(properties, options):
   # python2.x dict is not ordered
   ldap_properties = [
-    LdapPropTemplate(properties, options.ldap_url, ""authentication.ldap.primaryUrl"", ""Primary URL* {{host:port}} {0}: "", REGEX_HOSTNAME_PORT, False),
-    LdapPropTemplate(properties, options.ldap_secondary_url, ""authentication.ldap.secondaryUrl"", ""Secondary URL {{host:port}} {0}: "", REGEX_HOSTNAME_PORT, True),
-    LdapPropTemplate(properties, options.ldap_ssl, ""authentication.ldap.useSSL"", ""Use SSL* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false""),
-    LdapPropTemplate(properties, options.ldap_user_attr, ""authentication.ldap.usernameAttribute"", ""User name attribute* {0}: "", REGEX_ANYTHING, False, ""uid""),
-    LdapPropTemplate(properties, options.ldap_base_dn, ""authentication.ldap.baseDn"", ""Base DN* {0}: "", REGEX_ANYTHING, False),
-    LdapPropTemplate(properties, options.ldap_referral, ""authentication.ldap.referral"", ""Referral method [follow/ignore] {0}: "", REGEX_REFERRAL, True),
-    LdapPropTemplate(properties, options.ldap_bind_anonym, ""authentication.ldap.bindAnonymously"" ""Bind anonymously* [true/false] {0}: "", REGEX_TRUE_FALSE, False, ""false"")
+    LdapPropTemplate(properties, options.ldap_url_host, ""ambari.ldap.connectivity.server.host"", ""Primary URL Host* {0}: "", REGEX_HOSTNAME, False),
+    LdapPropTemplate(properties, options.ldap_url_port, ""ambari.ldap.connectivity.server.port"", ""Primary URL Port* {0}: "", REGEX_PORT, False),
+    LdapPropTemplate(properties, options.ldap_secondary_url_host, ""ambari.ldap.connectivity.secondary.server.host"", ""Secondary URL Host {0}: "", REGEX_HOSTNAME, True),
+    LdapPropTemplate(properties, options.ldap_secondary_url_port, ""ambari.ldap.connectivity.secondary.server.port"", ""Secondary URL Port {0}: "", REGEX_PORT, True),","[{'comment': ""Shouldn't `options.ldap_secondary_url_port` be `options.ldap_secondary_port`?"", 'commenter': 'rlevas'}]"
176,ambari-server/src/test/java/org/apache/ambari/server/security/authorization/AmbariAuthorizationFilterTest.java,"@@ -60,6 +61,11 @@
 import junit.framework.Assert;
 
 public class AmbariAuthorizationFilterTest {
+  @Before
+  public void setUp() {
+    SecurityContextHolder.getContext().setAuthentication(null);","[{'comment': 'Technically, this is not the correct solution since the test case appears to use a mocked `SecurityContext`, not the one from the `SecurityContextHolder`. See `org.apache.ambari.server.security.authorization.AmbariAuthorizationFilterTest#performGeneralDoFilterTest`.\r\n\r\nI think we need to remove the mocked SecurityContext and use the one provided by Spring.  For example, see `org.apache.ambari.server.controller.internal.UserResourceProviderTest#deleteResourcesTest`.  \r\n\r\nIn cases like the referenced method, we can do the following rather than create a mocked SecurityContext:\r\n```\r\n    SecurityContextHolder.getContext().setAuthentication(authentication);\r\n```\r\n\r\nSince the test case explictly sets the authenticated user before performing the test, there is no real need to explicitly clear it in the _`before`_ method. \r\n', 'commenter': 'rlevas'}]"
177,ambari-server/src/main/resources/stacks/HDP/2.0.6/properties/stack_features.json,"@@ -433,31 +433,16 @@
         ""description"": ""Registry remove root path setting"",
         ""min_version"": ""2.6.3.0""
       },
-      {
-        ""name"": ""registry_allowed_resources_support"",
-        ""description"": ""Registry allowed resources"",
-        ""min_version"": ""2.6.3.0""
-      },
-      {
-        ""name"": ""registry_rewriteuri_filter_support"",
-        ""description"": ""Registry RewriteUri servlet filter"",
-        ""min_version"": ""2.6.3.0""
-      },
-      {
-        ""name"": ""sam_storage_core_in_registry"",
-        ""description"": ""Storage core module moved to registry"",
-        ""min_version"": ""2.6.3.0""
-      },
-      {
-        ""name"": ""kafka_extended_sasl_support"",","[{'comment': '`kafka_extended_sasl_support` is used:\r\n\r\nhttps://github.com/apache/ambari/blob/3730c03f5c0ae3a9ac510baf53a5c2666e16012e/ambari-common/src/main/python/resource_management/libraries/functions/constants.py#L123\r\n\r\nhttps://github.com/apache/ambari/blob/3730c03f5c0ae3a9ac510baf53a5c2666e16012e/ambari-server/src/main/resources/common-services/KAFKA/0.8.1/package/scripts/params.py#L168', 'commenter': 'adoroszlai'}, {'comment': '@adoroszlai for HDP stack the challenge is version 2.6.5 will not support the feature as initially expected (kafka version will still be current version and not 0.10.2 or higher).  What is the best way to ensure that feature will not be enabled for that version of this stack yet ensure code is in place for other stacks that may be able to leverage it? ', 'commenter': 'YolandaMDavis'}, {'comment': 'The simplest one is probably keeping the feature with increased `min_version`.\r\n\r\nNote that Ambari 2.6.1 already has this code (and your change will only go into 2.6.2), which means this mismatch should probably be documented in HDP 2.6.5 release notes.', 'commenter': 'adoroszlai'}, {'comment': '@adoroszlai I can increase the min version not a problem; perhaps use 2.6.99.99? Concerning the documentation I will ensure that is addressed as well and create a followup Apache Jira for it.', 'commenter': 'YolandaMDavis'}, {'comment': 'I think that would make sense.  Thanks.', 'commenter': 'adoroszlai'}]"
182,ambari-server/src/main/java/org/apache/ambari/server/upgrade/UpgradeCatalog300.java,"@@ -542,6 +580,65 @@ protected void addAmbariConfigurationTable() throws SQLException {
     dbAccessor.addPKConstraint(AMBARI_CONFIGURATION_TABLE, ""PK_ambari_configuration"", AMBARI_CONFIGURATION_CATEGORY_NAME_COLUMN, AMBARI_CONFIGURATION_PROPERTY_NAME_COLUMN);
   }
 
+  /**
+   * Creates new tables for changed kerberos data.
+   *
+   * @throws SQLException
+   */
+  protected void upgradeKerberosTables() throws SQLException {
+    boolean keytabPrincipalTableCreated = false;
+    boolean mappingTableCreated = false;
+
+    if (!dbAccessor.tableExists(KERBEROS_KEYTAB_TABLE)) {","[{'comment': ""I believe that `dbAccessor.createTable` and `dbAccessor.addPKContraint` are safe to call without checking for this. Each ensure they are no re-adding the requested construct.   I don't think it hurts to do this though."", 'commenter': 'rlevas'}]"
182,ambari-server/src/main/java/org/apache/ambari/server/upgrade/UpgradeCatalog300.java,"@@ -542,6 +580,65 @@ protected void addAmbariConfigurationTable() throws SQLException {
     dbAccessor.addPKConstraint(AMBARI_CONFIGURATION_TABLE, ""PK_ambari_configuration"", AMBARI_CONFIGURATION_CATEGORY_NAME_COLUMN, AMBARI_CONFIGURATION_PROPERTY_NAME_COLUMN);
   }
 
+  /**
+   * Creates new tables for changed kerberos data.
+   *
+   * @throws SQLException
+   */
+  protected void upgradeKerberosTables() throws SQLException {
+    boolean keytabPrincipalTableCreated = false;
+    boolean mappingTableCreated = false;
+
+    if (!dbAccessor.tableExists(KERBEROS_KEYTAB_TABLE)) {
+      List<DBAccessor.DBColumnInfo> kerberosKeytabColumns = new ArrayList<>();
+      kerberosKeytabColumns.add(new DBAccessor.DBColumnInfo(KEYTAB_PATH_FIELD, String.class, 255, null, false));
+      kerberosKeytabColumns.add(new DBAccessor.DBColumnInfo(OWNER_NAME_FIELD, String.class, 255, null, true));
+      kerberosKeytabColumns.add(new DBAccessor.DBColumnInfo(OWNER_ACCESS_FIELD, String.class, 255, null, true));
+      kerberosKeytabColumns.add(new DBAccessor.DBColumnInfo(GROUP_NAME_FIELD, String.class, 255, null, true));
+      kerberosKeytabColumns.add(new DBAccessor.DBColumnInfo(GROUP_ACCESS_FIELD, String.class, 255, null, true));
+      kerberosKeytabColumns.add(new DBAccessor.DBColumnInfo(IS_AMBARI_KEYTAB_FIELD, Integer.class, null, 0, false));
+      kerberosKeytabColumns.add(new DBAccessor.DBColumnInfo(WRITE_AMBARI_JAAS_FIELD, Integer.class, null, 0, false));
+      dbAccessor.createTable(KERBEROS_KEYTAB_TABLE, kerberosKeytabColumns);
+      dbAccessor.addPKConstraint(KERBEROS_KEYTAB_TABLE, PK_KERBEROS_KEYTAB, KEYTAB_PATH_FIELD);
+    }
+
+    if (!dbAccessor.tableExists(KERBEROS_KEYTAB_PRINCIPAL_TABLE)) {","[{'comment': ""I believe that `dbAccessor.createTable`, `dbAccessor.addPKContraint`, and  `dbAccessor.addUniqueConstraint` are safe to call without checking or this. Each ensure they are no re-adding the requested construct.   I don't think it hurts to do this though."", 'commenter': 'rlevas'}, {'comment': 'Yes - they should all be safe to call without the check ahead of time.', 'commenter': 'jonathan-hurley'}, {'comment': '@rlevas @jonathan-hurley thanks for pointing to this, now I see that everything already have a check for existence. Fixed.', 'commenter': 'echekanskiy'}]"
182,ambari-server/src/main/java/org/apache/ambari/server/upgrade/UpgradeCatalog300.java,"@@ -542,6 +580,65 @@ protected void addAmbariConfigurationTable() throws SQLException {
     dbAccessor.addPKConstraint(AMBARI_CONFIGURATION_TABLE, ""PK_ambari_configuration"", AMBARI_CONFIGURATION_CATEGORY_NAME_COLUMN, AMBARI_CONFIGURATION_PROPERTY_NAME_COLUMN);
   }
 
+  /**
+   * Creates new tables for changed kerberos data.
+   *
+   * @throws SQLException
+   */
+  protected void upgradeKerberosTables() throws SQLException {
+    boolean keytabPrincipalTableCreated = false;
+    boolean mappingTableCreated = false;
+
+    if (!dbAccessor.tableExists(KERBEROS_KEYTAB_TABLE)) {
+      List<DBAccessor.DBColumnInfo> kerberosKeytabColumns = new ArrayList<>();
+      kerberosKeytabColumns.add(new DBAccessor.DBColumnInfo(KEYTAB_PATH_FIELD, String.class, 255, null, false));
+      kerberosKeytabColumns.add(new DBAccessor.DBColumnInfo(OWNER_NAME_FIELD, String.class, 255, null, true));
+      kerberosKeytabColumns.add(new DBAccessor.DBColumnInfo(OWNER_ACCESS_FIELD, String.class, 255, null, true));
+      kerberosKeytabColumns.add(new DBAccessor.DBColumnInfo(GROUP_NAME_FIELD, String.class, 255, null, true));
+      kerberosKeytabColumns.add(new DBAccessor.DBColumnInfo(GROUP_ACCESS_FIELD, String.class, 255, null, true));
+      kerberosKeytabColumns.add(new DBAccessor.DBColumnInfo(IS_AMBARI_KEYTAB_FIELD, Integer.class, null, 0, false));
+      kerberosKeytabColumns.add(new DBAccessor.DBColumnInfo(WRITE_AMBARI_JAAS_FIELD, Integer.class, null, 0, false));
+      dbAccessor.createTable(KERBEROS_KEYTAB_TABLE, kerberosKeytabColumns);
+      dbAccessor.addPKConstraint(KERBEROS_KEYTAB_TABLE, PK_KERBEROS_KEYTAB, KEYTAB_PATH_FIELD);
+    }
+
+    if (!dbAccessor.tableExists(KERBEROS_KEYTAB_PRINCIPAL_TABLE)) {
+      List<DBAccessor.DBColumnInfo> kkpColumns = new ArrayList<>();
+      kkpColumns.add(new DBAccessor.DBColumnInfo(KKP_ID_COLUMN, Long.class, null, 0L, false));
+      kkpColumns.add(new DBAccessor.DBColumnInfo(KEYTAB_PATH_FIELD, String.class, 255, null, false));
+      kkpColumns.add(new DBAccessor.DBColumnInfo(PRINCIPAL_NAME_COLUMN, String.class, 255, null, false));
+      kkpColumns.add(new DBAccessor.DBColumnInfo(HOST_ID_COLUMN, Long.class, null, null, true));
+      kkpColumns.add(new DBAccessor.DBColumnInfo(IS_DISTRIBUTED_COLUMN, Integer.class, null, 0, false));
+      dbAccessor.createTable(KERBEROS_KEYTAB_PRINCIPAL_TABLE, kkpColumns);
+      dbAccessor.addPKConstraint(KERBEROS_KEYTAB_PRINCIPAL_TABLE, PK_KKP, KKP_ID_COLUMN);
+      dbAccessor.addUniqueConstraint(KERBEROS_KEYTAB_PRINCIPAL_TABLE, UNI_KKP, KEYTAB_PATH_FIELD, PRINCIPAL_NAME_COLUMN, HOST_ID_COLUMN);
+      keytabPrincipalTableCreated=true;
+    }
+
+    if (!dbAccessor.tableExists(KKP_MAPPING_SERVICE_TABLE)) {","[{'comment': ""I believe that `dbAccessor.createTable` and `dbAccessor.addPKContraint` are safe to call without checking for this. Each ensure they are no re-adding the requested construct.   I don't think it hurts to do this though."", 'commenter': 'rlevas'}]"
182,ambari-server/src/main/java/org/apache/ambari/server/upgrade/UpgradeCatalog300.java,"@@ -784,9 +888,50 @@ protected void updateKerberosConfigurations() throws AmbariException {
                   true, false);
             }
           }
+          if (doPrincipalsCreation) {","[{'comment': ""rather that use a Boolean argument for this, wouldn't it be better it use a mocked `PrepareKerberosIdentitiesServerAction` instance in the injector?  \r\n\r\nThen you can do\r\n```\r\nPrepareKerberosIdentitiesServerAction prepareIdentities = injector.getInstance(PrepareKerberosIdentitiesServerAction.class)\r\n```\r\n\r\nRather than\r\n```\r\nPrepareKerberosIdentitiesServerAction prepareIdentities = new PrepareKerberosIdentitiesServerAction();\r\n...\r\n// inject whatever we need for calling desired server action\r\ninjector.injectMembers(prepareIdentities);\r\n```\r\n\r\nAnd you do not have to introduce an argument used only for testing purposes - which is never actually set to false. \r\n"", 'commenter': 'rlevas'}]"
182,ambari-server/src/main/java/org/apache/ambari/server/upgrade/UpgradeCatalog300.java,"@@ -784,9 +888,50 @@ protected void updateKerberosConfigurations() throws AmbariException {
                   true, false);
             }
           }
+          if (doPrincipalsCreation) {
+            PrepareKerberosIdentitiesServerAction prepareIdentities = new PrepareKerberosIdentitiesServerAction();
+            ExecutionCommand executionCommand = new ExecutionCommand();
+            executionCommand.setCommandParams(new HashMap<String, String>(){{
+              put(KerberosServerAction.DEFAULT_REALM, config.getProperties().get(""realm""));
+            }});
+            prepareIdentities.setExecutionCommand(executionCommand);
+
+            // inject whatever we need for calling desired server action
+            injector.injectMembers(prepareIdentities);
+            KerberosHelper kerberosHelper = injector.getInstance(KerberosHelper.class);
+
+            injector.getInstance(AmbariServer.class).performStaticInjection();
+            AmbariServer.setController(injector.getInstance(AmbariManagementController.class));
+
+            KerberosDescriptor kerberosDescriptor = kerberosHelper.getKerberosDescriptor(cluster, false);
+            Map<String, Map<String, String>> kerberosConfigurations = new HashMap<>();
+            Map<String, Set<String>> propertiesToIgnore = new HashMap<>();
+            List<ServiceComponentHost> schToProcess = kerberosHelper.getServiceComponentHostsToProcess(cluster, kerberosDescriptor, null, null);
+            Map<String, Map<String, String>> configurations = kerberosHelper.calculateConfigurations(cluster, null, kerberosDescriptor, false, false);
+            boolean includeAmbariIdentity = true;
+            String dataDirectory = kerberosHelper.createTemporaryDirectory().getAbsolutePath();
+            List<Boolean> failed = new ArrayList<>();","[{'comment': 'Why is this a List?', 'commenter': 'rlevas'}, {'comment': 'runnable is an anonymous inner class, so failed must be final, which does not allow to modify variable in runnable. But as far as I see this is not needed at all, I will just re-raise AmbariException as RuntimeException in run method and re-wrap it in AmbariException again', 'commenter': 'echekanskiy'}]"
182,ambari-server/src/main/java/org/apache/ambari/server/upgrade/UpgradeCatalog300.java,"@@ -542,6 +580,65 @@ protected void addAmbariConfigurationTable() throws SQLException {
     dbAccessor.addPKConstraint(AMBARI_CONFIGURATION_TABLE, ""PK_ambari_configuration"", AMBARI_CONFIGURATION_CATEGORY_NAME_COLUMN, AMBARI_CONFIGURATION_PROPERTY_NAME_COLUMN);
   }
 
+  /**
+   * Creates new tables for changed kerberos data.
+   *
+   * @throws SQLException
+   */
+  protected void upgradeKerberosTables() throws SQLException {
+    boolean keytabPrincipalTableCreated = false;
+    boolean mappingTableCreated = false;
+
+    if (!dbAccessor.tableExists(KERBEROS_KEYTAB_TABLE)) {
+      List<DBAccessor.DBColumnInfo> kerberosKeytabColumns = new ArrayList<>();
+      kerberosKeytabColumns.add(new DBAccessor.DBColumnInfo(KEYTAB_PATH_FIELD, String.class, 255, null, false));
+      kerberosKeytabColumns.add(new DBAccessor.DBColumnInfo(OWNER_NAME_FIELD, String.class, 255, null, true));
+      kerberosKeytabColumns.add(new DBAccessor.DBColumnInfo(OWNER_ACCESS_FIELD, String.class, 255, null, true));
+      kerberosKeytabColumns.add(new DBAccessor.DBColumnInfo(GROUP_NAME_FIELD, String.class, 255, null, true));
+      kerberosKeytabColumns.add(new DBAccessor.DBColumnInfo(GROUP_ACCESS_FIELD, String.class, 255, null, true));
+      kerberosKeytabColumns.add(new DBAccessor.DBColumnInfo(IS_AMBARI_KEYTAB_FIELD, Integer.class, null, 0, false));
+      kerberosKeytabColumns.add(new DBAccessor.DBColumnInfo(WRITE_AMBARI_JAAS_FIELD, Integer.class, null, 0, false));
+      dbAccessor.createTable(KERBEROS_KEYTAB_TABLE, kerberosKeytabColumns);
+      dbAccessor.addPKConstraint(KERBEROS_KEYTAB_TABLE, PK_KERBEROS_KEYTAB, KEYTAB_PATH_FIELD);
+    }
+
+    if (!dbAccessor.tableExists(KERBEROS_KEYTAB_PRINCIPAL_TABLE)) {
+      List<DBAccessor.DBColumnInfo> kkpColumns = new ArrayList<>();
+      kkpColumns.add(new DBAccessor.DBColumnInfo(KKP_ID_COLUMN, Long.class, null, 0L, false));
+      kkpColumns.add(new DBAccessor.DBColumnInfo(KEYTAB_PATH_FIELD, String.class, 255, null, false));
+      kkpColumns.add(new DBAccessor.DBColumnInfo(PRINCIPAL_NAME_COLUMN, String.class, 255, null, false));
+      kkpColumns.add(new DBAccessor.DBColumnInfo(HOST_ID_COLUMN, Long.class, null, null, true));
+      kkpColumns.add(new DBAccessor.DBColumnInfo(IS_DISTRIBUTED_COLUMN, Integer.class, null, 0, false));
+      dbAccessor.createTable(KERBEROS_KEYTAB_PRINCIPAL_TABLE, kkpColumns);
+      dbAccessor.addPKConstraint(KERBEROS_KEYTAB_PRINCIPAL_TABLE, PK_KKP, KKP_ID_COLUMN);
+      dbAccessor.addUniqueConstraint(KERBEROS_KEYTAB_PRINCIPAL_TABLE, UNI_KKP, KEYTAB_PATH_FIELD, PRINCIPAL_NAME_COLUMN, HOST_ID_COLUMN);
+      keytabPrincipalTableCreated=true;
+    }
+
+    if (!dbAccessor.tableExists(KKP_MAPPING_SERVICE_TABLE)) {
+      List<DBAccessor.DBColumnInfo> kkpMappingColumns = new ArrayList<>();
+      kkpMappingColumns.add(new DBAccessor.DBColumnInfo(KKP_ID_COLUMN, Long.class, null, 0L, false));
+      kkpMappingColumns.add(new DBAccessor.DBColumnInfo(SERVICE_NAME_COLUMN, String.class, 255, null, false));
+      kkpMappingColumns.add(new DBAccessor.DBColumnInfo(COMPONENT_NAME_COLUMN, String.class, 255, null, false));
+      dbAccessor.createTable(KKP_MAPPING_SERVICE_TABLE, kkpMappingColumns);
+      dbAccessor.addPKConstraint(KKP_MAPPING_SERVICE_TABLE, PK_KKP_MAPPING_SERVICE, KKP_ID_COLUMN, SERVICE_NAME_COLUMN, COMPONENT_NAME_COLUMN);
+      mappingTableCreated = true;
+    }
+
+    //  cross tables constraints
+    if (keytabPrincipalTableCreated) {","[{'comment': ""I don't think this is idempotent - in the first run if the table is created but something happens after, these might not be created. Then, on subsequent runs they will not execute."", 'commenter': 'jonathan-hurley'}]"
189,ambari-server/src/main/resources/Ambari-DDL-Derby-CREATE.sql,"@@ -628,12 +628,12 @@ CREATE TABLE blueprint_service_config (
   CONSTRAINT FK_bp_svc_config_to_service FOREIGN KEY (service_id) REFERENCES blueprint_service (id));
 
 CREATE TABLE blueprint_mpack_configuration (
-  mpack_ref_id BIGINT NOT NULL,
+  mpack_instance_id BIGINT NOT NULL,","[{'comment': ""Doesn't this require a change in `BlueprintMpackConfigEntity`, too?\r\n\r\nhttps://github.com/apache/ambari/blob/80d840e5d50614a0f148a650c9bfc7d21363a218/ambari-server/src/main/java/org/apache/ambari/server/orm/entities/BlueprintMpackConfigEntity.java#L41"", 'commenter': 'adoroszlai'}, {'comment': 'Fixed. Thanks.', 'commenter': 'benyoka'}]"
189,ambari-server/src/main/resources/Ambari-DDL-Derby-CREATE.sql,"@@ -613,11 +613,11 @@ CREATE TABLE blueprint_mpack_instance(
 
 CREATE TABLE blueprint_service (
   id BIGINT NOT NULL,
-  mpack_ref_id BIGINT NOT NULL,
+  mpack_instance_id BIGINT NOT NULL,","[{'comment': ""Doesn't this require a change in `BlueprintServiceEntity`, too?\r\n\r\nhttps://github.com/apache/ambari/blob/80d840e5d50614a0f148a650c9bfc7d21363a218/ambari-server/src/main/java/org/apache/ambari/server/orm/entities/BlueprintServiceEntity.java#L50"", 'commenter': 'adoroszlai'}, {'comment': 'Fixed. Thanks.', 'commenter': 'benyoka'}]"
194,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/StackServiceComponentResourceProvider.java,"@@ -62,6 +62,9 @@
   private static final String COMPONENT_CATEGORY_PROPERTY_ID = PropertyHelper.getPropertyId(
       ""StackServiceComponents"", ""component_category"");
 
+  private static final String COMPONENT_VERSION_PROPERTY_ID = PropertyHelper.getPropertyId(","[{'comment': 'Should also be added to `propertyIds`:\r\n\r\nhttps://github.com/apache/ambari/blob/48d882d1b797cbb355a1cbc53389e2991b72ee04/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/StackServiceComponentResourceProvider.java#L124-L127', 'commenter': 'adoroszlai'}]"
194,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/StackServiceResourceProvider.java,"@@ -54,6 +54,9 @@
   protected static final String SERVICE_TYPE_PROPERTY_ID = PropertyHelper.getPropertyId(
 		  ""StackServices"", ""service_type"");
 
+  protected static final String SERVICE_CATEGORY_PROPERTY_ID = PropertyHelper.getPropertyId(","[{'comment': 'Should also be added to `propertyIds`:\r\n\r\nhttps://github.com/apache/ambari/blob/48d882d1b797cbb355a1cbc53389e2991b72ee04/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/StackServiceResourceProvider.java#L117-L120', 'commenter': 'adoroszlai'}]"
194,ambari-server/src/main/java/org/apache/ambari/server/state/ServiceInfo.java,"@@ -503,6 +517,9 @@ public ComponentInfo getComponentByName(String componentName){
     return null;
   }
   public boolean isClientOnlyService() {
+    if (category.equals(ServiceCategory.CLIENT)) {","[{'comment': 'Can `category` be `null`?', 'commenter': 'adoroszlai'}, {'comment': 'Will change to ServiceCategory.CLIENT.equals(category)', 'commenter': 'd0zen1'}]"
194,ambari-server/src/main/resources/properties.json,"@@ -189,6 +189,7 @@
         ""StackServices/stack_name"",
         ""StackServices/stack_version"",
         ""StackServices/service_name"",
+        ""StackServices/service_category"",","[{'comment': '`properties.json` is no longer used, see e77a31ab0a.', 'commenter': 'adoroszlai'}, {'comment': ""Didn't know about this change. Thanks for noticing"", 'commenter': 'd0zen1'}]"
194,ambari-server/src/main/java/org/apache/ambari/server/stack/ServiceModule.java,"@@ -209,6 +210,10 @@ public void resolveInternal(
       ServiceInfo.ServiceAdvisorType serviceAdvisorType = parent.getServiceAdvisorType();
       serviceInfo.setServiceAdvisorType(serviceAdvisorType == null ? ServiceInfo.ServiceAdvisorType.PYTHON : serviceAdvisorType);
     }
+    if (serviceInfo.getCategory() == null) {
+      ServiceCategory category = parent.getCategory();
+      serviceInfo.setCategory(category == null ? ServiceCategory.LEGACY : category);","[{'comment': ""Hi Dmitry,\r\nI have a query. Inheritance will be present only for Legacy Stacks and hence services. So, when we say ServiceCategory category = parent.getCategory();, category will be null for new mpacks. In which case we will use the category from current module's metainfo.xml. Considering we have inserted a category to all the existing metainfo.xml files, what is the scenario for Category = LEGACY? Is it just for the very first version of a stack? Even for that, it should take its category from metainfo.xml instead of terming it LEGACY, correct?"", 'commenter': 'mradha25'}, {'comment': ""I haven't completely understood the issue here. \r\n\r\nIn case the service is not inherited from other service this code will never be reached, instead the one from StackModule will be executed: https://github.com/d0zen1/ambari/blob/137d82ebfb8db273706c23a352803a8ed671a09b/ambari-server/src/main/java/org/apache/ambari/server/stack/StackModule.java#L356-L364\r\n\r\nThe general logic follows the following priority rules, given 1 is the highest priority:\r\n1. Current service category (from service metainfo.xml if defined)\r\n2. Parent service category (from parent service metainfo.xml if defined)\r\n3. Default service category (LEGACY)"", 'commenter': 'd0zen1'}]"
197,ambari-server/src/main/resources/common-services/YARN/2.1.0.2.0/package/scripts/params_linux.py,"@@ -347,7 +347,9 @@ def get_spark_version(service_name, component_name, yarn_version):
   rm_kinit_cmd = format(""{kinit_path_local} -kt {rm_keytab} {rm_principal_name};"")
   yarn_jaas_file = os.path.join(config_dir, 'yarn_jaas.conf')
   if stack_supports_zk_security:
-    rm_security_opts = format('-Dzookeeper.sasl.client=true -Dzookeeper.sasl.client.username=zookeeper -Djava.security.auth.login.config={yarn_jaas_file} -Dzookeeper.sasl.clientconfig=Client')
+    zk_principal_name = default(""/configurations/zookeeper-env/zookeeper_principal_name"", ""zookeeper/_HOST@EXAMPLE.COM"")
+    zk_principal_user = zk_principal_name.split('/')[0]
+    rm_security_opts = format('-Dzookeeper.sasl.client=true -Dzookeeper.sasl.client.username=' + zk_principal_user + ' -Djava.security.auth.login.config={yarn_jaas_file} -Dzookeeper.sasl.clientconfig=Client')","[{'comment': ""For consistency, this should be \r\n```\r\nrm_security_opts = format('-Dzookeeper.sasl.client=true -Dzookeeper.sasl.client.username={zk_principal_user} -Djava.security.auth.login.config={yarn_jaas_file} -Dzookeeper.sasl.clientconfig=Client')\r\n```"", 'commenter': 'rlevas'}, {'comment': 'Done.', 'commenter': 'smolnar82'}]"
197,ambari-server/src/main/resources/common-services/YARN/3.0.0.3.0/package/scripts/params_linux.py,"@@ -345,7 +345,9 @@ def get_spark_version(service_name, component_name, yarn_version):
   rm_keytab = config['configurations']['yarn-site']['yarn.resourcemanager.keytab']
   rm_kinit_cmd = format(""{kinit_path_local} -kt {rm_keytab} {rm_principal_name};"")
   yarn_jaas_file = os.path.join(config_dir, 'yarn_jaas.conf')
-  rm_security_opts = format('-Dzookeeper.sasl.client=true -Dzookeeper.sasl.client.username=zookeeper -Djava.security.auth.login.config={yarn_jaas_file} -Dzookeeper.sasl.clientconfig=Client')
+  zk_principal_name = default(""/configurations/zookeeper-env/zookeeper_principal_name"", ""zookeeper/_HOST@EXAMPLE.COM"")
+  zk_principal_user = zk_principal_name.split('/')[0]
+  rm_security_opts = format('-Dzookeeper.sasl.client=true -Dzookeeper.sasl.client.username=' + zk_principal_user + ' -Djava.security.auth.login.config={yarn_jaas_file} -Dzookeeper.sasl.clientconfig=Client')","[{'comment': ""For consistency, this should be \r\n```\r\nrm_security_opts = format('-Dzookeeper.sasl.client=true -Dzookeeper.sasl.client.username={zk_principal_user} -Djava.security.auth.login.config={yarn_jaas_file} -Dzookeeper.sasl.clientconfig=Client')\r\n```"", 'commenter': 'rlevas'}]"
213,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/BlueprintResourceProvider.java,"@@ -176,14 +190,12 @@
    *
    * @param factory   blueprint factory
    * @param dao       blueprint data access object
-   * @param gson      json serializer
    */
   public static void init(BlueprintFactory factory, BlueprintDAO dao, SecurityConfigurationFactory
-    securityFactory, Gson gson, AmbariMetaInfo metaInfo) {
+    securityFactory,AmbariMetaInfo metaInfo) {","[{'comment': 'missing space', 'commenter': 'adoroszlai'}, {'comment': 'fixed', 'commenter': 'benyoka'}]"
213,ambari-server/src/main/java/org/apache/ambari/server/topology/BlueprintValidatorImpl.java,"@@ -63,34 +64,40 @@ public BlueprintValidatorImpl(Blueprint blueprint) {
   @Override
   public void validateTopology() throws InvalidTopologyException {
     LOGGER.info(""Validating topology for blueprint: [{}]"", blueprint.getName());
-    Collection<HostGroup> hostGroups = blueprint.getHostGroups().values();
-    Map<String, Map<String, Collection<DependencyInfo>>> missingDependencies = new HashMap<>();
-
-    for (HostGroup group : hostGroups) {
-      Map<String, Collection<DependencyInfo>> missingGroupDependencies = validateHostGroup(group);
-      if (!missingGroupDependencies.isEmpty()) {
-        missingDependencies.put(group.getName(), missingGroupDependencies);
+    if (blueprint.isAllMpacksResolved()) {","[{'comment': 'To minimize the code change, can you please change to:\r\n\r\n```\r\nif (!blueprint.isAllMpacksResolved()) {\r\n   warn\r\n   return\r\n}\r\n```', 'commenter': 'adoroszlai'}, {'comment': 'fixed', 'commenter': 'benyoka'}]"
213,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/BlueprintResourceProvider.java,"@@ -351,9 +369,37 @@ protected Resource toResource(BlueprintEntity entity, Set<String> requestedIds)
       setResourceProperty(resource, BLUEPRINT_SECURITY_PROPERTY_ID, securityConfigMap, requestedIds);
     }
 
+    Collection<Map<String, Object>> mpacks = entity.getMpackInstances().stream().map(mpackEntity -> {
+      MpackInstance mpack = MpackInstance.fromEntity(mpackEntity);
+      Map<String, Object> mpackAsMap = fromJson(toJson(mpack), Map.class);
+      return mpackAsMap;
+    } ).collect(toList());
+    setResourceProperty(resource, MPACK_INSTANCES_PROPERTY_ID, mpacks, requestedIds);
+
     return resource;
   }
 
+  private static <T> T fromJson(String json, Class<? extends T> valueType) {
+    if (null == json) {
+      return  null;
+    }
+    try {
+      return jsonSerializer.readValue(json, valueType);
+    }
+    catch (IOException ex) {
+      throw new RuntimeException(ex);","[{'comment': ""I'd prefer wrapping in `UncheckedIOException`.  (Also in `toJson`.)"", 'commenter': 'adoroszlai'}, {'comment': 'fixed', 'commenter': 'benyoka'}]"
213,ambari-server/src/main/java/org/apache/ambari/server/orm/entities/BlueprintEntity.java,"@@ -57,7 +57,6 @@
   @Basic
   @Column(name = ""security_descriptor_reference"", nullable = true, insertable = true, updatable = true)
   private String securityDescriptorReference;
-  ","[{'comment': ""Please don't remove this line."", 'commenter': 'adoroszlai'}, {'comment': 'fixed', 'commenter': 'benyoka'}]"
213,ambari-server/src/main/java/org/apache/ambari/server/state/UpgradeHelper.java,"@@ -37,7 +37,7 @@
 import org.apache.ambari.server.api.services.AmbariMetaInfo;
 import org.apache.ambari.server.controller.AmbariManagementController;
 import org.apache.ambari.server.controller.internal.TaskResourceProvider;
-import org.apache.ambari.server.controller.predicate.AndPredicate;
+import org.apache.ambari.server.controller.predicate  .AndPredicate;","[{'comment': ""Please don't add this unnecessary space."", 'commenter': 'adoroszlai'}, {'comment': 'fixed', 'commenter': 'benyoka'}]"
213,ambari-server/src/main/java/org/apache/ambari/server/topology/BlueprintValidatorImpl.java,"@@ -178,27 +185,33 @@ public void validateRequiredProperties() throws InvalidTopologyException, GPLLic
               }
           }
 
-        if (component.equals(""HIVE_METASTORE"")) {
-          Map<String, String> hiveEnvConfig = clusterConfigurations.get(""hive-env"");
-          if (hiveEnvConfig != null && !hiveEnvConfig.isEmpty() && hiveEnvConfig.get(""hive_database"") != null
-            && hiveEnvConfig.get(""hive_database"").equals(""Existing SQL Anywhere Database"")
-            && VersionUtils.compareVersions(stack.getVersion(), ""2.3.0.0"") < 0
-            && stack.getName().equalsIgnoreCase(""HDP"")) {
-            throw new InvalidTopologyException(""Incorrect configuration: SQL Anywhere db is available only for stack HDP-2.3+ "" +
-              ""and repo version 2.3.2+!"");
+        if (blueprint.isAllMpacksResolved()) {","[{'comment': ""I'd prefer not making this change here now, since the HIVE/OOZIE-specific checks are most likely going to be removed anyway."", 'commenter': 'adoroszlai'}, {'comment': 'Leaving as is for the moment as not sure about backwards compatibility implications (e.g. breaking tests)', 'commenter': 'benyoka'}]"
213,ambari-server/src/main/java/org/apache/ambari/server/topology/Component.java,"@@ -19,21 +19,40 @@
 package org.apache.ambari.server.topology;
 
 
+import java.util.Objects;
+
+import javax.annotation.Nullable;
+
 import org.apache.ambari.server.controller.internal.ProvisionAction;
 
 public class Component {
 
   private final String name;
+  @Nullable
+  private final String mpackInstance;
+  @Nullable
+  private final String serviceInstance;
 
   private final ProvisionAction provisionAction;
 
+  @Deprecated
   public Component(String name) {
-    this(name, null);
+    this(name, null, null, null);
   }
 
-
+  @Deprecated
   public Component(String name, ProvisionAction provisionAction) {
+    this(name, null, null, provisionAction);
+  }
+
+  public Component(String name, String mpackInstance, String serviceInstance) {","[{'comment': 'Do we really need to add this constructor?', 'commenter': 'adoroszlai'}, {'comment': 'removed', 'commenter': 'benyoka'}]"
213,ambari-server/src/main/java/org/apache/ambari/server/topology/Component.java,"@@ -19,21 +19,40 @@
 package org.apache.ambari.server.topology;
 
 
+import java.util.Objects;
+
+import javax.annotation.Nullable;
+
 import org.apache.ambari.server.controller.internal.ProvisionAction;
 
 public class Component {
 
   private final String name;
+  @Nullable
+  private final String mpackInstance;
+  @Nullable
+  private final String serviceInstance;
 
   private final ProvisionAction provisionAction;
 
+  @Deprecated
   public Component(String name) {
-    this(name, null);
+    this(name, null, null, null);
   }
 
-
+  @Deprecated
   public Component(String name, ProvisionAction provisionAction) {","[{'comment': 'Seems to be unused.', 'commenter': 'adoroszlai'}, {'comment': 'fixed', 'commenter': 'benyoka'}]"
213,ambari-server/src/main/java/org/apache/ambari/server/topology/HostGroupImpl.java,"@@ -52,12 +64,13 @@
   /**
    * components contained in the host group
    */
-  private Map<String, Component> components = new HashMap<>();
+  private List<Component> components = new ArrayList<>();","[{'comment': 'Can be `final`?', 'commenter': 'adoroszlai'}, {'comment': 'fixed', 'commenter': 'benyoka'}]"
213,ambari-server/src/main/java/org/apache/ambari/server/topology/HostGroupImpl.java,"@@ -148,75 +161,121 @@ public static String formatAbsoluteName(String bpName, String hgName) {
   }
 
   /**
-   * Add a component to the host group.
-   *
-   * @param component  component to add
-   *
-   * @return true if component was added; false if component already existed
+   * Adds a component to the host group. The component is successfully added if it is not a duplicate or ambiguous (as of
+   * Ambari 3.1 multiple components of the same type can exist in a hostgroup. However, they have to come from different
+   * management packs or belong to different service instances)
+   * @param component the component to add
+   * @return a boolean to indicate if addition was successful
    */
-  @Override
-  public boolean addComponent(String component) {
-    return this.addComponent(component, null);
+  public boolean addComponent(Component component) {
+    // Exclude ambiguous component definitions
+    boolean ambigous = components.stream().filter(c -> {
+      if (c.getName().equals(component.getName())) { // found another component with the same name
+        if (c.getMpackInstance() == null || component.getMpackInstance() == null) {
+          return true; // if either of them has no mpack instance defined it is ambiguous
+        }
+        if (c.getMpackInstance().equals(component.getMpackInstance())) {
+          // both components are in the same mpack, and one of them does not declare a service instance or
+          // both declare the same service instance --> ambiguous
+          return  c.getServiceInstance() != null && component.getServiceInstance() != null &&
+            c.getServiceInstance().equals(component.getServiceInstance());
+        }
+        else {
+          return false; // different mpacks --> no ambiguity
+        }
+      }
+      else {
+        return false; // different name --> no ambiguity
+      }
+    }).findAny().isPresent();
+    if (ambigous) {
+      return false;
+    }
+    addComponent(component, getStackForComponent(component));
+    return true;
   }
 
-  /**
-   * Add a component with the specified provision action to the
-   *   host group.
-   *
-   * @param component  component name
-   * @param provisionAction provision action for this component
-   *
-   * @return true if component was added; false if component already existed
-   */
-  public boolean addComponent(String component, ProvisionAction provisionAction) {
-    boolean added;
-    if (!components.containsKey(component)) {
-      components.put(component, new Component(component, provisionAction));
-      added = true;
-    } else {
-      added = false;
+  private Optional<Stack> getStackForComponent(Component component) {
+    // Look for the stack of this component
+    if (component.getMpackInstance() == null) {
+      // Component does not declare its stack. Let's find it.
+      Collection<Stack> candidateStacks =
+        stackMap.values().stream().filter(stack -> stack.getServiceForComponent(component.getName()) != null).collect(toList());
+      switch (candidateStacks.size()) {
+        case 0:
+          // no stack (no service) for this component
+          LOG.warn(""No stack/service found for component: {}"", component);
+          return Optional.empty();
+        case 1:
+          return Optional.of(candidateStacks.iterator().next());
+        default:
+          LOG.warn(""Ambiguous stack resolution for component: {}, stacks: {}"", component, candidateStacks);
+          return Optional.empty();
+      }
     }
-
-    if (stack.isMasterComponent(component)) {
-      containsMasterComponent = true;
+    else {
+      // TODO: refine this logic
+      Stack stack = stackMap.get(component.getMpackInstance());
+      if (null == stack) {
+        LOG.warn(""Component declared an invalid stack: {}"", component);
+      }
+      return Optional.ofNullable(stack);
     }
-    if (added) {
-      String service = stack.getServiceForComponent(component);
-      if (service != null) {
-        // an example of a component without a service in the stack is AMBARI_SERVER
-        Set<String> serviceComponents = componentsForService.get(service);
-        if (serviceComponents == null) {
-          serviceComponents = new HashSet<>();
-          componentsForService.put(service, serviceComponents);
-        }
-        serviceComponents.add(component);
+  }
+
+  private void addComponent(Component component, Optional<Stack> stack) {
+    if (stack.isPresent()) {
+      String serviceName = stack.get().getServiceForComponent(component.getName());
+      if (!componentsForService.containsKey(serviceName)) {
+        componentsForService.put(serviceName, Sets.newHashSet(component));
+      }
+      else {
+        componentsForService.get(serviceName).add(component);
+      }
+      if (stack.get().isMasterComponent(component.getName())) {
+        containsMasterComponent = true;
       }
     }
-    return added;
+    components.add(component);
   }
 
   /**
    * Get the components for the specified service which are associated with the host group.
    *
    * @param service  service name
    *
-   * @return set of component names
+   * @return set of components
    */
   @Override
-  public Collection<String> getComponents(String service) {
+  public Collection<Component> getComponents(String service) {
     return componentsForService.containsKey(service) ?
       new HashSet<>(componentsForService.get(service)) :
         Collections.emptySet();
   }
 
+  /**
+   * Get the names components for the specified service which are associated with the host group.
+   *
+   * @param service  service name
+   *
+   * @return set of component names
+   */
+  @Override
+  @Deprecated
+  public Collection<String> getComponentNames(String service) {
+    return componentsForService.containsKey(service) ?
+      new HashSet<>(componentsForService.get(service).stream().map(Component::getName).collect(toList())) :","[{'comment': 'How about `collect(toSet())` instead of wrapping it in a `HashSet`?', 'commenter': 'adoroszlai'}, {'comment': 'fixed', 'commenter': 'benyoka'}]"
213,ambari-server/src/main/java/org/apache/ambari/server/topology/Blueprint.java,"@@ -170,4 +196,15 @@
   BlueprintEntity toEntity();
 
   List<RepositorySetting> getRepositorySettings();
+
+  /**
+   * @return a boolean indicating if all mpack referenced by the blueprints are resolved (installed on the system)
+   */
+  boolean isAllMpacksResolved();","[{'comment': 'Minor issue:  \r\n\r\nisAllMpacksResolved() should be ""areAllMpacksResolved()""', 'commenter': 'rnettleton'}, {'comment': ""Thought all boolean getters should start with 'is' even if grammatically incorrect. Not sure though."", 'commenter': 'benyoka'}]"
213,ambari-server/src/main/java/org/apache/ambari/server/topology/HostGroup.java,"@@ -79,35 +81,38 @@
    * @return collection of component names as String that are associated with
    *           the specified provision action
    */
+  @Deprecated","[{'comment': 'Why is this method deprecated?  ', 'commenter': 'rnettleton'}, {'comment': ""Eventually I think we'll need to pass Component objects (which can contain information about mpack and service instance as needed) as passing simply the component name could be ambiguous in the multi-everything world."", 'commenter': 'benyoka'}]"
221,ambari-server/src/main/java/org/apache/ambari/server/orm/entities/RepoDefinitionEntity.java,"@@ -0,0 +1,188 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.orm.entities;
+
+import java.util.ArrayList;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Set;
+
+import javax.persistence.CascadeType;
+import javax.persistence.Column;
+import javax.persistence.Entity;
+import javax.persistence.GeneratedValue;
+import javax.persistence.GenerationType;
+import javax.persistence.Id;
+import javax.persistence.JoinColumn;
+import javax.persistence.ManyToOne;
+import javax.persistence.OneToMany;
+import javax.persistence.Table;
+import javax.persistence.TableGenerator;
+
+import org.apache.ambari.server.state.stack.RepoTag;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+@Entity
+@Table(name = ""repo_definition"", uniqueConstraints = {
+})
+@TableGenerator(name = ""repo_definition_id_generator"",
+    table = ""ambari_sequences"",
+    pkColumnName = ""sequence_name"",
+    valueColumnName = ""sequence_value"",
+    pkColumnValue = ""repo_definition_id_seq"",
+    initialValue = 0
+)
+public class RepoDefinitionEntity {
+  private static final Logger LOG = LoggerFactory.getLogger(RepoDefinitionEntity.class);
+
+  @Id
+  @Column(name = ""repo_definition_id"", nullable = false)
+  @GeneratedValue(strategy = GenerationType.TABLE, generator = ""repo_definition_id_generator"")
+  private Long id;
+
+  @ManyToOne
+  @JoinColumn(name = ""repo_os_id"", referencedColumnName = ""repo_os_id"", nullable = false)
+  private RepoOsEntity repoOs;
+
+  @Column(name = ""repo_name"", nullable = false)
+  private String repoName;
+
+  @Column(name = ""repo_id"", nullable = false)
+  private String repoID;
+
+  @Column(name = ""base_url"", nullable = false)
+  private String baseUrl;
+
+  @Column(name = ""mirrors"")
+  private String mirrors;
+
+  @Column(name = ""distribution"")
+  private String distribution;
+
+  @Column(name = ""components"")
+  private String components;
+
+  @Column(name = ""unique_repo"", nullable = false)
+  private short unique = 0;
+
+  @OneToMany(cascade = CascadeType.ALL, mappedBy = ""repoDefinitionEntity"")
+  private List<RepoTagEntity> repoTagEntities = new ArrayList<>();","[{'comment': 'This should not be a whole new entity.  You should be able to use a CollectionTable (see AlertTargetEntity alertStates for an example)', 'commenter': 'ncole'}]"
221,ambari-server/src/main/resources/Ambari-DDL-Postgres-CREATE.sql,"@@ -155,12 +155,39 @@ CREATE TABLE clusterstate (
   CONSTRAINT FK_clusterstate_cluster_id FOREIGN KEY (cluster_id) REFERENCES clusters (cluster_id),
   CONSTRAINT FK_cs_current_stack_id FOREIGN KEY (current_stack_id) REFERENCES stack(stack_id));
 
+CREATE TABLE repo_tag (
+  repo_tag_id BIGINT NOT NULL,
+  repo_definition_id BIGINT NOT NULL,
+  tag VARCHAR(255) NOT NULL DEFAULT '',
+  CONSTRAINT PK_repo_tag_id PRIMARY KEY (repo_tag_id));
+  CONSTRAINT FK_repo_tag_id_repo_definition_id FOREIGN KEY (repo_definition_id) REFERENCES repo_definition (repo_definition_id));
+
+CREATE TABLE repo_os (
+  repo_os_id BIGINT NOT NULL,","[{'comment': 'just ""id"" ?', 'commenter': 'ncole'}]"
221,ambari-server/src/main/resources/Ambari-DDL-Postgres-CREATE.sql,"@@ -155,12 +155,39 @@ CREATE TABLE clusterstate (
   CONSTRAINT FK_clusterstate_cluster_id FOREIGN KEY (cluster_id) REFERENCES clusters (cluster_id),
   CONSTRAINT FK_cs_current_stack_id FOREIGN KEY (current_stack_id) REFERENCES stack(stack_id));
 
+CREATE TABLE repo_tag (
+  repo_tag_id BIGINT NOT NULL,
+  repo_definition_id BIGINT NOT NULL,
+  tag VARCHAR(255) NOT NULL DEFAULT '',
+  CONSTRAINT PK_repo_tag_id PRIMARY KEY (repo_tag_id));
+  CONSTRAINT FK_repo_tag_id_repo_definition_id FOREIGN KEY (repo_definition_id) REFERENCES repo_definition (repo_definition_id));
+
+CREATE TABLE repo_os (
+  repo_os_id BIGINT NOT NULL,
+  repo_version_id BIGINT NOT NULL,
+  family VARCHAR(255) NOT NULL DEFAULT '',
+  ambari_managed SMALLINT DEFAULT 1,
+  CONSTRAINT PK_repo_os_id PRIMARY KEY (repo_os_id));
+  CONSTRAINT FK_repo_os_id_repo_version_id FOREIGN KEY (repo_version_id) REFERENCES repo_version (repo_version_id));
+
+CREATE TABLE repo_definition (
+  repo_definition_id BIGINT NOT NULL,","[{'comment': 'just ""id"" ?', 'commenter': 'ncole'}]"
221,ambari-server/src/main/java/org/apache/ambari/server/orm/entities/RepoDefinitionEntity.java,"@@ -0,0 +1,198 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.orm.entities;
+
+import java.util.HashSet;
+import java.util.Set;
+
+import javax.persistence.CollectionTable;
+import javax.persistence.Column;
+import javax.persistence.ElementCollection;
+import javax.persistence.Entity;
+import javax.persistence.EnumType;
+import javax.persistence.Enumerated;
+import javax.persistence.FetchType;
+import javax.persistence.GeneratedValue;
+import javax.persistence.GenerationType;
+import javax.persistence.Id;
+import javax.persistence.JoinColumn;
+import javax.persistence.ManyToOne;
+import javax.persistence.Table;
+import javax.persistence.TableGenerator;
+
+import org.apache.ambari.server.state.stack.RepoTag;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+@Entity
+@Table(name = ""repo_definition"")
+@TableGenerator(name = ""repo_definition_id_generator"",
+    table = ""ambari_sequences"",
+    pkColumnName = ""sequence_name"",
+    valueColumnName = ""sequence_value"",
+    pkColumnValue = ""repo_definition_id_seq""
+)
+public class RepoDefinitionEntity {
+  @Id
+  @Column(name = ""id"", nullable = false)
+  @GeneratedValue(strategy = GenerationType.TABLE, generator = ""repo_definition_id_generator"")
+  private Long id;
+
+  @Enumerated(value = EnumType.STRING)
+  @ElementCollection(targetClass = RepoTag.class)
+  @CollectionTable(name = ""repo_tag_states"", joinColumns = @JoinColumn(name = ""repo_definition_id""))
+  @Column(name = ""tag_state"")
+  private Set<RepoTag> repoTags = new HashSet<>();
+
+  @ManyToOne(fetch = FetchType.LAZY)
+  @JoinColumn(name = ""repo_os_id"", nullable = false)
+  private RepoOsEntity repoOs;
+
+  @Column(name = ""repo_name"", nullable = false)
+  private String repoName;
+
+  @Column(name = ""repo_id"", nullable = false)
+  private String repoID;
+
+  @Column(name = ""base_url"", nullable = false)
+  private String baseUrl;
+
+  @Column(name = ""mirrors"")
+  private String mirrors;
+
+  @Column(name = ""distribution"")
+  private String distribution;
+
+  @Column(name = ""components"")
+  private String components;
+
+  @Column(name = ""unique_repo"", nullable = false)
+  private short unique = 0;
+
+
+
+  public String getDistribution() {
+    return distribution;
+  }
+
+  public void setDistribution(String distribution) {
+    this.distribution = distribution;
+  }
+
+  public RepoOsEntity getRepoOs() {
+    return repoOs;
+  }
+
+  public void setRepoOs(RepoOsEntity repoOs) {
+    this.repoOs = repoOs;
+  }
+
+  public String getRepoName() {
+    return repoName;
+  }
+
+  public void setRepoName(String repoName) {
+    this.repoName = repoName;
+  }
+
+  public String getRepoID() {
+    return repoID;
+  }
+
+  public void setRepoID(String repoID) {
+    this.repoID = repoID;
+  }
+
+  public String getBaseUrl() {
+    return baseUrl;
+  }
+
+  public void setBaseUrl(String baseUrl) {
+    this.baseUrl = baseUrl;
+  }
+
+  public String getMirrors() {
+    return mirrors;
+  }
+
+  public void setMirrors(String mirrors) {
+    this.mirrors = mirrors;
+  }
+
+  public Long getId() {
+    return id;
+  }
+
+  public void setId(Long id) {
+    this.id = id;
+  }
+
+  public String getComponents() {
+    return components;
+  }
+
+  public void setComponents(String components) {
+    this.components = components;
+  }
+
+  public boolean isUnique() {
+    return unique == 1;
+  }
+
+  public void setUnique(boolean unique) {
+    this.unique = (short) (unique ? 1 : 0);
+  }
+
+  public Set<RepoTag> getTags() {
+    return repoTags;
+  }
+
+  public void setTags(Set<RepoTag> repoTags) {
+    this.repoTags = repoTags;
+  }
+
+  @Override
+  public boolean equals(Object o) {","[{'comment': 'We should use Objects.equals() for this', 'commenter': 'jonathan-hurley'}]"
221,ambari-server/src/main/java/org/apache/ambari/server/orm/entities/RepoDefinitionEntity.java,"@@ -0,0 +1,198 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.orm.entities;
+
+import java.util.HashSet;
+import java.util.Set;
+
+import javax.persistence.CollectionTable;
+import javax.persistence.Column;
+import javax.persistence.ElementCollection;
+import javax.persistence.Entity;
+import javax.persistence.EnumType;
+import javax.persistence.Enumerated;
+import javax.persistence.FetchType;
+import javax.persistence.GeneratedValue;
+import javax.persistence.GenerationType;
+import javax.persistence.Id;
+import javax.persistence.JoinColumn;
+import javax.persistence.ManyToOne;
+import javax.persistence.Table;
+import javax.persistence.TableGenerator;
+
+import org.apache.ambari.server.state.stack.RepoTag;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+@Entity
+@Table(name = ""repo_definition"")
+@TableGenerator(name = ""repo_definition_id_generator"",
+    table = ""ambari_sequences"",
+    pkColumnName = ""sequence_name"",
+    valueColumnName = ""sequence_value"",
+    pkColumnValue = ""repo_definition_id_seq""
+)
+public class RepoDefinitionEntity {
+  @Id
+  @Column(name = ""id"", nullable = false)
+  @GeneratedValue(strategy = GenerationType.TABLE, generator = ""repo_definition_id_generator"")
+  private Long id;
+
+  @Enumerated(value = EnumType.STRING)
+  @ElementCollection(targetClass = RepoTag.class)
+  @CollectionTable(name = ""repo_tag_states"", joinColumns = @JoinColumn(name = ""repo_definition_id""))
+  @Column(name = ""tag_state"")
+  private Set<RepoTag> repoTags = new HashSet<>();
+
+  @ManyToOne(fetch = FetchType.LAZY)
+  @JoinColumn(name = ""repo_os_id"", nullable = false)
+  private RepoOsEntity repoOs;
+
+  @Column(name = ""repo_name"", nullable = false)
+  private String repoName;
+
+  @Column(name = ""repo_id"", nullable = false)
+  private String repoID;
+
+  @Column(name = ""base_url"", nullable = false)
+  private String baseUrl;
+
+  @Column(name = ""mirrors"")
+  private String mirrors;
+
+  @Column(name = ""distribution"")
+  private String distribution;
+
+  @Column(name = ""components"")
+  private String components;
+
+  @Column(name = ""unique_repo"", nullable = false)
+  private short unique = 0;
+
+
+
+  public String getDistribution() {
+    return distribution;
+  }
+
+  public void setDistribution(String distribution) {
+    this.distribution = distribution;
+  }
+
+  public RepoOsEntity getRepoOs() {
+    return repoOs;
+  }
+
+  public void setRepoOs(RepoOsEntity repoOs) {
+    this.repoOs = repoOs;
+  }
+
+  public String getRepoName() {
+    return repoName;
+  }
+
+  public void setRepoName(String repoName) {
+    this.repoName = repoName;
+  }
+
+  public String getRepoID() {
+    return repoID;
+  }
+
+  public void setRepoID(String repoID) {
+    this.repoID = repoID;
+  }
+
+  public String getBaseUrl() {
+    return baseUrl;
+  }
+
+  public void setBaseUrl(String baseUrl) {
+    this.baseUrl = baseUrl;
+  }
+
+  public String getMirrors() {
+    return mirrors;
+  }
+
+  public void setMirrors(String mirrors) {
+    this.mirrors = mirrors;
+  }
+
+  public Long getId() {
+    return id;
+  }
+
+  public void setId(Long id) {
+    this.id = id;
+  }
+
+  public String getComponents() {
+    return components;
+  }
+
+  public void setComponents(String components) {
+    this.components = components;
+  }
+
+  public boolean isUnique() {
+    return unique == 1;
+  }
+
+  public void setUnique(boolean unique) {
+    this.unique = (short) (unique ? 1 : 0);
+  }
+
+  public Set<RepoTag> getTags() {
+    return repoTags;
+  }
+
+  public void setTags(Set<RepoTag> repoTags) {
+    this.repoTags = repoTags;
+  }
+
+  @Override
+  public boolean equals(Object o) {
+    if (this == o) return true;
+    if (o == null || getClass() != o.getClass()) return false;
+
+    RepoDefinitionEntity that = (RepoDefinitionEntity) o;
+
+    if (unique != that.unique) return false;
+    if (repoTags != null ? !repoTags.equals(that.repoTags) : that.repoTags != null) return false;
+    if (repoName != null ? !repoName.equals(that.repoName) : that.repoName != null) return false;
+    if (repoID != null ? !repoID.equals(that.repoID) : that.repoID != null) return false;
+    if (baseUrl != null ? !baseUrl.equals(that.baseUrl) : that.baseUrl != null) return false;
+    if (mirrors != null ? !mirrors.equals(that.mirrors) : that.mirrors != null) return false;
+    if (distribution != null ? !distribution.equals(that.distribution) : that.distribution != null) return false;
+    return components != null ? components.equals(that.components) : that.components == null;
+  }
+
+  @Override
+  public int hashCode() {
+    int result = repoTags != null ? repoTags.hashCode() : 0;","[{'comment': 'We should use Objects.hash() for this', 'commenter': 'jonathan-hurley'}]"
221,ambari-server/src/main/java/org/apache/ambari/server/orm/entities/RepoOsEntity.java,"@@ -0,0 +1,130 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.orm.entities;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import javax.persistence.CascadeType;
+import javax.persistence.Column;
+import javax.persistence.Entity;
+import javax.persistence.FetchType;
+import javax.persistence.GeneratedValue;
+import javax.persistence.GenerationType;
+import javax.persistence.Id;
+import javax.persistence.JoinColumn;
+import javax.persistence.ManyToOne;
+import javax.persistence.OneToMany;
+import javax.persistence.Table;
+import javax.persistence.TableGenerator;
+
+@Entity
+@Table(name = ""repo_os"")
+@TableGenerator(name = ""repo_os_id_generator"",
+    table = ""ambari_sequences"",
+    pkColumnName = ""sequence_name"",
+    valueColumnName = ""sequence_value"",
+    pkColumnValue = ""repo_os_id_seq""
+)
+public class RepoOsEntity {
+  @Id
+  @Column(name = ""id"", nullable = false)
+  @GeneratedValue(strategy = GenerationType.TABLE, generator = ""repo_os_id_generator"")
+  private Long id;
+
+  @Column(name = ""family"")
+  private String family;
+
+  @Column(name = ""ambari_managed"", nullable = false)
+  private short ambariManaged = 0;
+
+  @OneToMany(orphanRemoval = true, fetch = FetchType.LAZY, cascade = CascadeType.ALL, mappedBy = ""repoOs"")","[{'comment': ""I think we'd always want to retrieve the definitions for an OS, right? So maybe we should make this EAGER instead of LAZY"", 'commenter': 'jonathan-hurley'}]"
221,ambari-server/src/main/resources/Ambari-DDL-Derby-CREATE.sql,"@@ -172,6 +171,31 @@ CREATE TABLE repo_version (
   CONSTRAINT UQ_repo_version_display_name UNIQUE (display_name),
   CONSTRAINT UQ_repo_version_stack_id UNIQUE (stack_id, version));
 
+CREATE TABLE repo_os (
+  id BIGINT NOT NULL,
+  repo_version_id BIGINT NOT NULL,
+  family VARCHAR(255) NOT NULL DEFAULT '',
+  ambari_managed SMALLINT DEFAULT 1,
+  CONSTRAINT PK_repo_os_id PRIMARY KEY (id));
+  CONSTRAINT FK_repo_os_id_repo_version_id FOREIGN KEY (repo_version_id) REFERENCES repo_version (repo_version_id));
+
+CREATE TABLE repo_definition (
+  id BIGINT NOT NULL,
+  repo_os_id BIGINT,
+  repo_name VARCHAR(255) NOT NULL,
+  repo_id VARCHAR(255) NOT NULL,
+  base_url VARCHAR(2048) NOT NULL,
+  distribution VARCHAR(2048),
+  components VARCHAR(2048),
+  unique_repo SMALLINT DEFAULT 1,
+  mirrors VARCHAR(2048),
+  CONSTRAINT PK_repo_definition_id PRIMARY KEY (id));
+  CONSTRAINT FK_repo_definition_repo_os_id FOREIGN KEY (repo_os_id) REFERENCES repo_os (id));
+
+CREATE TABLE repo_tag_states (","[{'comment': 'Let\'s call this ""repo_tags"" instead of ""repo_tag_states"" ... they are not really states, they are just tags. ', 'commenter': 'jonathan-hurley'}]"
221,ambari-server/src/main/java/org/apache/ambari/server/orm/entities/RepoTagEntity.java,"@@ -0,0 +1,84 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.orm.entities;
+
+import javax.persistence.Column;
+import javax.persistence.Entity;
+import javax.persistence.GeneratedValue;
+import javax.persistence.GenerationType;
+import javax.persistence.Id;
+import javax.persistence.JoinColumn;
+import javax.persistence.ManyToOne;
+import javax.persistence.Table;
+import javax.persistence.TableGenerator;
+
+@Entity
+@Table(name = ""repo_tag"")
+@TableGenerator(name = ""repo_tag_id_generator"",
+    table = ""ambari_sequences"",
+    pkColumnName = ""sequence_name"",
+    valueColumnName = ""sequence_value"",
+    pkColumnValue = ""repo_tag_id_seq"",
+    initialValue = 0
+)
+public class RepoTagEntity {
+
+  @Id","[{'comment': ""Does an entity-mapping table need its own surrogate IDs? I don't think it does. (If it does, then you also need to add this sequence into the SQL file) ... but I don't think it needs this."", 'commenter': 'jonathan-hurley'}, {'comment': 'I think that the ID value need to be removed here, right?', 'commenter': 'jonathan-hurley'}]"
231,ambari-server/src/main/java/org/apache/ambari/server/api/services/stackadvisor/StackAdvisorHelper.java,"@@ -122,9 +122,7 @@ public synchronized RecommendationResponse recommend(StackAdvisorRequest request
       throws StackAdvisorException {
       requestId = generateRequestId();
 
-    // TODO, need to pass the service Name that was modified.
-    // For now, hardcode
-    String serviceName = ""ZOOKEEPER"";
+    String serviceName = Iterables.getFirst(request.getServices(), null);","[{'comment': 'Should we consider using a JDK 8 Stream to get the first element, rather than add a new dependency? \r\n\r\nMaybe we should use the ""findFirst()"" method: \r\n\r\nhttps://docs.oracle.com/javase/8/docs/api/java/util/stream/Stream.html#findFirst--\r\n\r\n', 'commenter': 'rnettleton'}, {'comment': 'Good idea, I changed it.', 'commenter': 'adoroszlai'}]"
231,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/StackDefinition.java,"@@ -0,0 +1,218 @@
+package org.apache.ambari.server.controller.internal;
+
+import java.util.Collection;
+import java.util.Map;
+import java.util.Set;
+import java.util.stream.Stream;
+
+import org.apache.ambari.server.state.AutoDeployInfo;
+import org.apache.ambari.server.state.ComponentInfo;
+import org.apache.ambari.server.state.DependencyInfo;
+import org.apache.ambari.server.state.PropertyInfo;
+import org.apache.ambari.server.state.StackId;
+import org.apache.ambari.server.topology.Cardinality;
+import org.apache.ambari.server.topology.Configuration;
+
+import com.google.common.collect.Iterables;
+
+/**
+ * Encapsulates stack information.
+ */
+public interface StackDefinition {
+
+  /**
+   * @return the IDs for the set of stacks that this stacks is (possibly) composed of.
+   */
+  Set<StackId> getStackIds();
+
+  /**
+   * @return the IDs of the set of stacks that the given service is defined in
+   */
+  Set<StackId> getStacksForService(String serviceName);
+
+  /**
+   * @return the names of services defined the given stack
+   */
+  Set<String> getServices(StackId stackId);
+
+  /**
+   * Get services contained in the stack.
+   *
+   * @return collection of all services for the stack
+   */
+  Collection<String> getServices();
+
+  /**
+   * Get components contained in the stack for the specified service.
+   *
+   * @param service  service name
+   *
+   * @return collection of component names for the specified service
+   */
+  Collection<String> getComponents(String service);
+
+  /**
+   * Get all service components
+   *
+   * @return map of service to associated components
+   */
+  Map<String, Collection<String>> getComponents();
+
+  /**
+   * Get info for the specified component.
+   *
+   * @param component  component name
+   *
+   * @return component information for the requested component
+   *         or null if the component doesn't exist in the stack
+   */
+  ComponentInfo getComponentInfo(String component);
+
+  /**
+   * Get all configuration types, including excluded types for the specified service.
+   *
+   * @param service  service name
+   *
+   * @return collection of all configuration types for the specified service
+   */
+  Collection<String> getAllConfigurationTypes(String service);
+
+  /**
+   * Get configuration types for the specified service.
+   * This doesn't include any service excluded types.
+   *
+   * @param service  service name
+   * @return collection of all configuration types for the specified service
+   */
+  Collection<String> getConfigurationTypes(String service);
+
+  /**
+   * Get the set of excluded configuration types for this service.
+   *
+   * @param service service name
+   * @return Set of names of excluded config types. Will not return null.
+   */
+  Set<String> getExcludedConfigurationTypes(String service);
+
+  /**
+   * Get config properties for the specified service and configuration type.
+   *
+   * @param service  service name
+   * @param type     configuration type
+   * @return map of property names to values for the specified service and configuration type
+   */
+  Map<String, String> getConfigurationProperties(String service, String type);
+
+  Map<String, Stack.ConfigProperty> getConfigurationPropertiesWithMetadata(String service, String type);","[{'comment': 'This method should have javadocs. \r\n\r\n', 'commenter': 'rnettleton'}]"
231,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/StackDefinition.java,"@@ -0,0 +1,218 @@
+package org.apache.ambari.server.controller.internal;
+
+import java.util.Collection;
+import java.util.Map;
+import java.util.Set;
+import java.util.stream.Stream;
+
+import org.apache.ambari.server.state.AutoDeployInfo;
+import org.apache.ambari.server.state.ComponentInfo;
+import org.apache.ambari.server.state.DependencyInfo;
+import org.apache.ambari.server.state.PropertyInfo;
+import org.apache.ambari.server.state.StackId;
+import org.apache.ambari.server.topology.Cardinality;
+import org.apache.ambari.server.topology.Configuration;
+
+import com.google.common.collect.Iterables;
+
+/**
+ * Encapsulates stack information.
+ */
+public interface StackDefinition {
+
+  /**
+   * @return the IDs for the set of stacks that this stacks is (possibly) composed of.
+   */
+  Set<StackId> getStackIds();
+
+  /**
+   * @return the IDs of the set of stacks that the given service is defined in
+   */
+  Set<StackId> getStacksForService(String serviceName);
+
+  /**
+   * @return the names of services defined the given stack
+   */
+  Set<String> getServices(StackId stackId);
+
+  /**
+   * Get services contained in the stack.
+   *
+   * @return collection of all services for the stack
+   */
+  Collection<String> getServices();
+
+  /**
+   * Get components contained in the stack for the specified service.
+   *
+   * @param service  service name
+   *
+   * @return collection of component names for the specified service
+   */
+  Collection<String> getComponents(String service);
+
+  /**
+   * Get all service components
+   *
+   * @return map of service to associated components
+   */
+  Map<String, Collection<String>> getComponents();
+
+  /**
+   * Get info for the specified component.
+   *
+   * @param component  component name
+   *
+   * @return component information for the requested component
+   *         or null if the component doesn't exist in the stack
+   */
+  ComponentInfo getComponentInfo(String component);
+
+  /**
+   * Get all configuration types, including excluded types for the specified service.
+   *
+   * @param service  service name
+   *
+   * @return collection of all configuration types for the specified service
+   */
+  Collection<String> getAllConfigurationTypes(String service);
+
+  /**
+   * Get configuration types for the specified service.
+   * This doesn't include any service excluded types.
+   *
+   * @param service  service name
+   * @return collection of all configuration types for the specified service
+   */
+  Collection<String> getConfigurationTypes(String service);
+
+  /**
+   * Get the set of excluded configuration types for this service.
+   *
+   * @param service service name
+   * @return Set of names of excluded config types. Will not return null.
+   */
+  Set<String> getExcludedConfigurationTypes(String service);
+
+  /**
+   * Get config properties for the specified service and configuration type.
+   *
+   * @param service  service name
+   * @param type     configuration type
+   * @return map of property names to values for the specified service and configuration type
+   */
+  Map<String, String> getConfigurationProperties(String service, String type);
+
+  Map<String, Stack.ConfigProperty> getConfigurationPropertiesWithMetadata(String service, String type);
+
+  /**
+   * Get all required config properties for the specified service.
+   *
+   * @param service  service name
+   * @return collection of all required properties for the given service
+   */
+  Collection<Stack.ConfigProperty> getRequiredConfigurationProperties(String service);
+
+  /**
+   * Get required config properties for the specified service which belong to the specified property type.
+   *
+   * @param service       service name
+   * @param propertyType  property type
+   *
+   * @return collection of required properties for the given service and property type
+   */
+  Collection<Stack.ConfigProperty> getRequiredConfigurationProperties(String service, PropertyInfo.PropertyType propertyType);
+
+  boolean isPasswordProperty(String service, String type, String propertyName);","[{'comment': 'These three methods should have javadocs. \r\n\r\n', 'commenter': 'rnettleton'}]"
231,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/StackDefinition.java,"@@ -0,0 +1,218 @@
+package org.apache.ambari.server.controller.internal;
+
+import java.util.Collection;
+import java.util.Map;
+import java.util.Set;
+import java.util.stream.Stream;
+
+import org.apache.ambari.server.state.AutoDeployInfo;
+import org.apache.ambari.server.state.ComponentInfo;
+import org.apache.ambari.server.state.DependencyInfo;
+import org.apache.ambari.server.state.PropertyInfo;
+import org.apache.ambari.server.state.StackId;
+import org.apache.ambari.server.topology.Cardinality;
+import org.apache.ambari.server.topology.Configuration;
+
+import com.google.common.collect.Iterables;
+
+/**
+ * Encapsulates stack information.
+ */
+public interface StackDefinition {
+
+  /**
+   * @return the IDs for the set of stacks that this stacks is (possibly) composed of.
+   */
+  Set<StackId> getStackIds();
+
+  /**
+   * @return the IDs of the set of stacks that the given service is defined in
+   */
+  Set<StackId> getStacksForService(String serviceName);
+
+  /**
+   * @return the names of services defined the given stack
+   */
+  Set<String> getServices(StackId stackId);
+
+  /**
+   * Get services contained in the stack.
+   *
+   * @return collection of all services for the stack
+   */
+  Collection<String> getServices();
+
+  /**
+   * Get components contained in the stack for the specified service.
+   *
+   * @param service  service name
+   *
+   * @return collection of component names for the specified service
+   */
+  Collection<String> getComponents(String service);
+
+  /**
+   * Get all service components
+   *
+   * @return map of service to associated components
+   */
+  Map<String, Collection<String>> getComponents();
+
+  /**
+   * Get info for the specified component.
+   *
+   * @param component  component name
+   *
+   * @return component information for the requested component
+   *         or null if the component doesn't exist in the stack
+   */
+  ComponentInfo getComponentInfo(String component);
+
+  /**
+   * Get all configuration types, including excluded types for the specified service.
+   *
+   * @param service  service name
+   *
+   * @return collection of all configuration types for the specified service
+   */
+  Collection<String> getAllConfigurationTypes(String service);
+
+  /**
+   * Get configuration types for the specified service.
+   * This doesn't include any service excluded types.
+   *
+   * @param service  service name
+   * @return collection of all configuration types for the specified service
+   */
+  Collection<String> getConfigurationTypes(String service);
+
+  /**
+   * Get the set of excluded configuration types for this service.
+   *
+   * @param service service name
+   * @return Set of names of excluded config types. Will not return null.
+   */
+  Set<String> getExcludedConfigurationTypes(String service);
+
+  /**
+   * Get config properties for the specified service and configuration type.
+   *
+   * @param service  service name
+   * @param type     configuration type
+   * @return map of property names to values for the specified service and configuration type
+   */
+  Map<String, String> getConfigurationProperties(String service, String type);
+
+  Map<String, Stack.ConfigProperty> getConfigurationPropertiesWithMetadata(String service, String type);
+
+  /**
+   * Get all required config properties for the specified service.
+   *
+   * @param service  service name
+   * @return collection of all required properties for the given service
+   */
+  Collection<Stack.ConfigProperty> getRequiredConfigurationProperties(String service);
+
+  /**
+   * Get required config properties for the specified service which belong to the specified property type.
+   *
+   * @param service       service name
+   * @param propertyType  property type
+   *
+   * @return collection of required properties for the given service and property type
+   */
+  Collection<Stack.ConfigProperty> getRequiredConfigurationProperties(String service, PropertyInfo.PropertyType propertyType);
+
+  boolean isPasswordProperty(String service, String type, String propertyName);
+
+  Map<String, String> getStackConfigurationProperties(String type);
+
+  boolean isKerberosPrincipalNameProperty(String service, String type, String propertyName);
+
+  /**
+   * Get config attributes for the specified service and configuration type.
+   *
+   * @param service  service name
+   * @param type     configuration type
+   *
+   * @return  map of attribute names to map of property names to attribute values
+   *          for the specified service and configuration type
+   */
+  Map<String, Map<String, String>> getConfigurationAttributes(String service, String type);
+
+  Map<String, Map<String, String>> getStackConfigurationAttributes(String type);","[{'comment': 'Missing javadoc. ', 'commenter': 'rnettleton'}]"
231,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/StackDefinition.java,"@@ -0,0 +1,218 @@
+package org.apache.ambari.server.controller.internal;
+
+import java.util.Collection;
+import java.util.Map;
+import java.util.Set;
+import java.util.stream.Stream;
+
+import org.apache.ambari.server.state.AutoDeployInfo;
+import org.apache.ambari.server.state.ComponentInfo;
+import org.apache.ambari.server.state.DependencyInfo;
+import org.apache.ambari.server.state.PropertyInfo;
+import org.apache.ambari.server.state.StackId;
+import org.apache.ambari.server.topology.Cardinality;
+import org.apache.ambari.server.topology.Configuration;
+
+import com.google.common.collect.Iterables;
+
+/**
+ * Encapsulates stack information.
+ */
+public interface StackDefinition {
+
+  /**
+   * @return the IDs for the set of stacks that this stacks is (possibly) composed of.
+   */
+  Set<StackId> getStackIds();
+
+  /**
+   * @return the IDs of the set of stacks that the given service is defined in
+   */
+  Set<StackId> getStacksForService(String serviceName);
+
+  /**
+   * @return the names of services defined the given stack
+   */
+  Set<String> getServices(StackId stackId);
+
+  /**
+   * Get services contained in the stack.
+   *
+   * @return collection of all services for the stack
+   */
+  Collection<String> getServices();
+
+  /**
+   * Get components contained in the stack for the specified service.
+   *
+   * @param service  service name
+   *
+   * @return collection of component names for the specified service
+   */
+  Collection<String> getComponents(String service);
+
+  /**
+   * Get all service components
+   *
+   * @return map of service to associated components
+   */
+  Map<String, Collection<String>> getComponents();
+
+  /**
+   * Get info for the specified component.
+   *
+   * @param component  component name
+   *
+   * @return component information for the requested component
+   *         or null if the component doesn't exist in the stack
+   */
+  ComponentInfo getComponentInfo(String component);
+
+  /**
+   * Get all configuration types, including excluded types for the specified service.
+   *
+   * @param service  service name
+   *
+   * @return collection of all configuration types for the specified service
+   */
+  Collection<String> getAllConfigurationTypes(String service);
+
+  /**
+   * Get configuration types for the specified service.
+   * This doesn't include any service excluded types.
+   *
+   * @param service  service name
+   * @return collection of all configuration types for the specified service
+   */
+  Collection<String> getConfigurationTypes(String service);
+
+  /**
+   * Get the set of excluded configuration types for this service.
+   *
+   * @param service service name
+   * @return Set of names of excluded config types. Will not return null.
+   */
+  Set<String> getExcludedConfigurationTypes(String service);
+
+  /**
+   * Get config properties for the specified service and configuration type.
+   *
+   * @param service  service name
+   * @param type     configuration type
+   * @return map of property names to values for the specified service and configuration type
+   */
+  Map<String, String> getConfigurationProperties(String service, String type);
+
+  Map<String, Stack.ConfigProperty> getConfigurationPropertiesWithMetadata(String service, String type);
+
+  /**
+   * Get all required config properties for the specified service.
+   *
+   * @param service  service name
+   * @return collection of all required properties for the given service
+   */
+  Collection<Stack.ConfigProperty> getRequiredConfigurationProperties(String service);
+
+  /**
+   * Get required config properties for the specified service which belong to the specified property type.
+   *
+   * @param service       service name
+   * @param propertyType  property type
+   *
+   * @return collection of required properties for the given service and property type
+   */
+  Collection<Stack.ConfigProperty> getRequiredConfigurationProperties(String service, PropertyInfo.PropertyType propertyType);
+
+  boolean isPasswordProperty(String service, String type, String propertyName);
+
+  Map<String, String> getStackConfigurationProperties(String type);
+
+  boolean isKerberosPrincipalNameProperty(String service, String type, String propertyName);
+
+  /**
+   * Get config attributes for the specified service and configuration type.
+   *
+   * @param service  service name
+   * @param type     configuration type
+   *
+   * @return  map of attribute names to map of property names to attribute values
+   *          for the specified service and configuration type
+   */
+  Map<String, Map<String, String>> getConfigurationAttributes(String service, String type);
+
+  Map<String, Map<String, String>> getStackConfigurationAttributes(String type);
+
+  /**
+   * Get the service for the specified component.
+   *
+   * @param component  component name
+   *
+   * @return service name that contains tha specified component
+   */
+  String getServiceForComponent(String component);
+
+  /**
+   * Get the names of the services which contains the specified components.
+   *
+   * @param components collection of components
+   *
+   * @return collection of services which contain the specified components
+   */
+  Collection<String> getServicesForComponents(Collection<String> components);
+
+  /**
+   * Obtain the service name which corresponds to the specified configuration.
+   *
+   * @param config  configuration type
+   *
+   * @return name of service which corresponds to the specified configuration type
+   */
+  String getServiceForConfigType(String config);
+
+  Stream<String> getServicesForConfigType(String config);","[{'comment': 'Missing javadocs.  ', 'commenter': 'rnettleton'}]"
231,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/StackDefinition.java,"@@ -0,0 +1,218 @@
+package org.apache.ambari.server.controller.internal;
+
+import java.util.Collection;
+import java.util.Map;
+import java.util.Set;
+import java.util.stream.Stream;
+
+import org.apache.ambari.server.state.AutoDeployInfo;
+import org.apache.ambari.server.state.ComponentInfo;
+import org.apache.ambari.server.state.DependencyInfo;
+import org.apache.ambari.server.state.PropertyInfo;
+import org.apache.ambari.server.state.StackId;
+import org.apache.ambari.server.topology.Cardinality;
+import org.apache.ambari.server.topology.Configuration;
+
+import com.google.common.collect.Iterables;
+
+/**
+ * Encapsulates stack information.
+ */
+public interface StackDefinition {
+
+  /**
+   * @return the IDs for the set of stacks that this stacks is (possibly) composed of.
+   */
+  Set<StackId> getStackIds();
+
+  /**
+   * @return the IDs of the set of stacks that the given service is defined in
+   */
+  Set<StackId> getStacksForService(String serviceName);
+
+  /**
+   * @return the names of services defined the given stack
+   */
+  Set<String> getServices(StackId stackId);
+
+  /**
+   * Get services contained in the stack.
+   *
+   * @return collection of all services for the stack
+   */
+  Collection<String> getServices();
+
+  /**
+   * Get components contained in the stack for the specified service.
+   *
+   * @param service  service name
+   *
+   * @return collection of component names for the specified service
+   */
+  Collection<String> getComponents(String service);
+
+  /**
+   * Get all service components
+   *
+   * @return map of service to associated components
+   */
+  Map<String, Collection<String>> getComponents();
+
+  /**
+   * Get info for the specified component.
+   *
+   * @param component  component name
+   *
+   * @return component information for the requested component
+   *         or null if the component doesn't exist in the stack
+   */
+  ComponentInfo getComponentInfo(String component);
+
+  /**
+   * Get all configuration types, including excluded types for the specified service.
+   *
+   * @param service  service name
+   *
+   * @return collection of all configuration types for the specified service
+   */
+  Collection<String> getAllConfigurationTypes(String service);
+
+  /**
+   * Get configuration types for the specified service.
+   * This doesn't include any service excluded types.
+   *
+   * @param service  service name
+   * @return collection of all configuration types for the specified service
+   */
+  Collection<String> getConfigurationTypes(String service);
+
+  /**
+   * Get the set of excluded configuration types for this service.
+   *
+   * @param service service name
+   * @return Set of names of excluded config types. Will not return null.
+   */
+  Set<String> getExcludedConfigurationTypes(String service);
+
+  /**
+   * Get config properties for the specified service and configuration type.
+   *
+   * @param service  service name
+   * @param type     configuration type
+   * @return map of property names to values for the specified service and configuration type
+   */
+  Map<String, String> getConfigurationProperties(String service, String type);
+
+  Map<String, Stack.ConfigProperty> getConfigurationPropertiesWithMetadata(String service, String type);
+
+  /**
+   * Get all required config properties for the specified service.
+   *
+   * @param service  service name
+   * @return collection of all required properties for the given service
+   */
+  Collection<Stack.ConfigProperty> getRequiredConfigurationProperties(String service);
+
+  /**
+   * Get required config properties for the specified service which belong to the specified property type.
+   *
+   * @param service       service name
+   * @param propertyType  property type
+   *
+   * @return collection of required properties for the given service and property type
+   */
+  Collection<Stack.ConfigProperty> getRequiredConfigurationProperties(String service, PropertyInfo.PropertyType propertyType);
+
+  boolean isPasswordProperty(String service, String type, String propertyName);
+
+  Map<String, String> getStackConfigurationProperties(String type);
+
+  boolean isKerberosPrincipalNameProperty(String service, String type, String propertyName);
+
+  /**
+   * Get config attributes for the specified service and configuration type.
+   *
+   * @param service  service name
+   * @param type     configuration type
+   *
+   * @return  map of attribute names to map of property names to attribute values
+   *          for the specified service and configuration type
+   */
+  Map<String, Map<String, String>> getConfigurationAttributes(String service, String type);
+
+  Map<String, Map<String, String>> getStackConfigurationAttributes(String type);
+
+  /**
+   * Get the service for the specified component.
+   *
+   * @param component  component name
+   *
+   * @return service name that contains tha specified component
+   */
+  String getServiceForComponent(String component);
+
+  /**
+   * Get the names of the services which contains the specified components.
+   *
+   * @param components collection of components
+   *
+   * @return collection of services which contain the specified components
+   */
+  Collection<String> getServicesForComponents(Collection<String> components);
+
+  /**
+   * Obtain the service name which corresponds to the specified configuration.
+   *
+   * @param config  configuration type
+   *
+   * @return name of service which corresponds to the specified configuration type
+   */
+  String getServiceForConfigType(String config);
+
+  Stream<String> getServicesForConfigType(String config);
+
+  /**
+   * Return the dependencies specified for the given component.
+   *
+   * @param component  component to get dependency information for
+   *
+   * @return collection of dependency information for the specified component
+   */
+  //todo: full dependency graph
+  Collection<DependencyInfo> getDependenciesForComponent(String component);
+
+  /**
+   * Get the service, if any, that a component dependency is conditional on.
+   *
+   * @param dependency  dependency to get conditional service for
+   *
+   * @return conditional service for provided component or null if dependency
+   *         is not conditional on a service
+   */
+  String getConditionalServiceForDependency(DependencyInfo dependency);
+
+  String getExternalComponentConfig(String component);","[{'comment': 'Missing javadocs. \r\n\r\n', 'commenter': 'rnettleton'}]"
231,ambari-server/src/main/java/org/apache/ambari/server/metadata/RoleCommandOrder.java,"@@ -137,6 +141,16 @@ public void initialize(Cluster cluster, LinkedHashSet<String> sectionKeys) {
     for (Service service : cluster.getServices().values()) {
       stackIds.add(service.getDesiredStackId());
     }
+    Set<String> components = cluster.getServices().values().stream()
+      .flatMap(s -> s.getServiceComponents().values().stream())
+      .map(ServiceComponent::getName)
+      .collect(toSet());
+    // FIXME ugly workaround
+    StackInfo hdp = ambariMetaInfo.getStacks().stream()","[{'comment': 'Is this hard-coding still required, now that the new BP parsing changes are merged into the feature branch? ', 'commenter': 'rnettleton'}, {'comment': ""Yes, unfortunately it's still needed until `hdp-select` is replaced (or added to all mpacks)."", 'commenter': 'adoroszlai'}]"
231,ambari-server/src/main/java/org/apache/ambari/server/stack/NoSuchStackException.java,"@@ -18,10 +18,15 @@
 
 package org.apache.ambari.server.stack;
 
+import org.apache.ambari.server.state.StackId;
+
 /**
- * Indicates that the requested Stack doesn't esist.
+ * Indicates that the requested Stack doesn't exist.
  */
-public class NoSuchStackException extends Exception {
+public class NoSuchStackException extends IllegalArgumentException {","[{'comment': 'Why is this being changed to a runtime exception?  Is this required due to the JDK8 stream usages in the new code?  \r\n\r\n', 'commenter': 'rnettleton'}, {'comment': 'Exactly.', 'commenter': 'adoroszlai'}]"
231,ambari-server/src/main/java/org/apache/ambari/server/topology/BlueprintValidator.java,"@@ -29,7 +27,7 @@
    *
    * @throws InvalidTopologyException if the topology is invalid
    */
-  void validateTopology() throws InvalidTopologyException;
+  void validateTopology(Blueprint blueprint) throws InvalidTopologyException;","[{'comment': 'I would recommend updating the javadoc for these methods, to document the Blueprint parameter. \r\n', 'commenter': 'rnettleton'}]"
231,ambari-server/src/main/java/org/apache/ambari/server/topology/BlueprintValidatorImpl.java,"@@ -315,16 +278,21 @@ public void validateRequiredProperties() throws InvalidTopologyException, GPLLic
    * Verify that a component meets cardinality requirements.  For components that are
    * auto-install enabled, will add component to topology if needed.
    *
+   *
+   * @param stack
    * @param component    component to validate
    * @param cardinality  required cardinality
    * @param autoDeploy   auto-deploy information for component
    *
    * @return collection of missing component information
    */
-  public Collection<String> verifyComponentCardinalityCount(Component component,
-                                                            Cardinality cardinality,
-                                                            AutoDeployInfo autoDeploy) {
-
+  private Collection<String> verifyComponentCardinalityCount(
+    Blueprint blueprint,","[{'comment': 'javadoc needs to be updated for this method. ', 'commenter': 'rnettleton'}]"
231,ambari-server/src/main/resources/stack-hooks/before-ANY/scripts/params.py,"@@ -242,8 +242,8 @@ def is_secure_port(port):
 
 user_to_gid_dict = collections.defaultdict(lambda:user_group)
 
-user_list = json.loads(config['hostLevelParams']['user_list'])
-group_list = json.loads(config['hostLevelParams']['group_list'])
+user_list = set(json.loads(config['hostLevelParams']['user_list']) + [smoke_user])","[{'comment': ""Why is this change included in this PR?  It doesn't appear to be related to the other Blueprint-specific changes in this PR.  "", 'commenter': 'rnettleton'}, {'comment': ""At some point I ran into an error where `smoke_user` was not being created, but I think the role-command order workaround addresses it, too.  So we don't need this anymore."", 'commenter': 'adoroszlai'}]"
231,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/CompositeStack.java,"@@ -0,0 +1,299 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.controller.internal;
+
+import static java.util.stream.Collectors.toMap;
+import static java.util.stream.Collectors.toSet;
+
+import java.util.Collection;
+import java.util.Map;
+import java.util.Objects;
+import java.util.Optional;
+import java.util.Set;
+import java.util.stream.Stream;
+
+import org.apache.ambari.server.state.AutoDeployInfo;
+import org.apache.ambari.server.state.ComponentInfo;
+import org.apache.ambari.server.state.ConfigHelper;
+import org.apache.ambari.server.state.DependencyInfo;
+import org.apache.ambari.server.state.PropertyInfo;
+import org.apache.ambari.server.state.StackId;
+import org.apache.ambari.server.topology.Cardinality;
+import org.apache.ambari.server.topology.Configuration;
+import org.apache.commons.lang3.tuple.Pair;
+
+import com.google.common.collect.ImmutableMap;
+import com.google.common.collect.ImmutableSet;
+
+/** Combines multiple mpacks into a single stack. */
+// TODO move to topology package
+public class CompositeStack implements StackDefinition {
+
+  private final Set<Stack> stacks;
+
+  public CompositeStack(Set<Stack> stacks) {
+    this.stacks = stacks;
+  }
+
+  @Override
+  public Set<StackId> getStacksForService(String serviceName) {
+    return stacks.stream()
+      .map(m -> Pair.of(m.getStackId(), m.getServices()))
+      .filter(p -> p.getRight().contains(serviceName))
+      .map(Pair::getLeft)
+      .collect(toSet());
+  }
+
+  @Override
+  public Set<String> getServices(StackId stackId) {
+    return stacks.stream()
+      .filter(m -> stackId.equals(m.getStackId()))
+      .findAny()
+      .flatMap(m -> Optional.of(ImmutableSet.copyOf(m.getServices())))
+      .orElse(ImmutableSet.of());
+  }
+
+  @Override
+  public Set<StackId> getStackIds() {
+    return stacks.stream()
+      .map(Stack::getStackId)
+      .collect(toSet());
+  }
+
+  @Override
+  public Collection<String> getServices() {
+    return stacks.stream()
+      .flatMap(s -> s.getServices().stream())
+      .collect(toSet());
+  }
+
+  @Override
+  public Collection<String> getComponents(String service) {
+    return stacks.stream()
+      .map(Stack::getComponents)
+      .map(m -> m.get(service))
+      .filter(Objects::nonNull)
+      .flatMap(Collection::stream)
+      .collect(toSet());
+  }
+
+  @Override
+  public Map<String, Collection<String>> getComponents() {
+    return stacks.stream()
+      .map(Stack::getComponents)
+      .reduce(ImmutableMap.of(), (m1, m2) -> ImmutableMap.<String, Collection<String>>builder().putAll(m1).putAll(m2).build());
+  }
+
+  @Override
+  public ComponentInfo getComponentInfo(String component) {
+    return stacks.stream()
+      .map(m -> m.getComponentInfo(component))
+      .filter(Objects::nonNull)
+      .findAny()
+      .orElse(null);
+  }
+
+  @Override
+  public Collection<String> getAllConfigurationTypes(String service) {
+    return stacks.stream()
+      .flatMap(m -> m.getAllConfigurationTypes(service).stream())
+      .collect(toSet());
+  }
+
+  @Override
+  public Collection<String> getConfigurationTypes(String service) {
+    return stacks.stream()
+      .flatMap(m -> m.getConfigurationTypes(service).stream())
+      .collect(toSet());
+  }
+
+  @Override
+  public Set<String> getExcludedConfigurationTypes(String service) {
+    return stacks.stream()
+      .flatMap(m -> m.getExcludedConfigurationTypes(service).stream())
+      .collect(toSet());
+  }
+
+  @Override
+  public Map<String, String> getConfigurationProperties(String service, String type) {
+    return stacks.stream()
+      .flatMap(m -> m.getConfigurationProperties(service, type).entrySet().stream())
+      .collect(toMap(Map.Entry::getKey, Map.Entry::getValue));
+  }
+
+  @Override
+  public Map<String, Stack.ConfigProperty> getConfigurationPropertiesWithMetadata(String service, String type) {
+    return stacks.stream()
+      .flatMap(m -> m.getConfigurationPropertiesWithMetadata(service, type).entrySet().stream())
+      .collect(toMap(Map.Entry::getKey, Map.Entry::getValue));
+  }
+
+  @Override
+  public Collection<Stack.ConfigProperty> getRequiredConfigurationProperties(String service) {
+    return stacks.stream()
+      .flatMap(m -> m.getRequiredConfigurationProperties(service).stream())
+      .collect(toSet());
+  }
+
+  @Override
+  public Collection<Stack.ConfigProperty> getRequiredConfigurationProperties(String service, PropertyInfo.PropertyType propertyType) {
+    return stacks.stream()
+      .flatMap(m -> m.getRequiredConfigurationProperties(service, propertyType).stream())
+      .collect(toSet());
+  }
+
+  @Override
+  public boolean isPasswordProperty(String service, String type, String propertyName) {
+    return stacks.stream()
+      .anyMatch(s -> s.isPasswordProperty(service, type, propertyName));
+  }
+
+  @Override
+  public Map<String, String> getStackConfigurationProperties(String type) {
+    return stacks.stream()
+      .flatMap(m -> m.getStackConfigurationProperties(type).entrySet().stream())
+      .collect(toMap(Map.Entry::getKey, Map.Entry::getValue));
+  }
+
+  @Override
+  public boolean isKerberosPrincipalNameProperty(String service, String type, String propertyName) {
+    return stacks.stream()
+      .anyMatch(s -> s.isKerberosPrincipalNameProperty(service, type, propertyName));
+  }
+
+  @Override
+  public Map<String, Map<String, String>> getConfigurationAttributes(String service, String type) {
+    return stacks.stream()
+      .flatMap(m -> m.getConfigurationAttributes(service, type).entrySet().stream())
+      .collect(toMap(Map.Entry::getKey, Map.Entry::getValue));
+  }
+
+  @Override
+  public Map<String, Map<String, String>> getStackConfigurationAttributes(String type) {
+    return stacks.stream()
+      .flatMap(m -> m.getStackConfigurationAttributes(type).entrySet().stream())
+      .collect(toMap(Map.Entry::getKey, Map.Entry::getValue));
+  }
+
+  @Override
+  public String getServiceForComponent(String component) {
+    return stacks.stream()
+      .map(m -> m.getServiceForComponent(component))
+      .filter(Objects::nonNull)
+      .findAny()
+      .orElse(null);
+  }
+
+  @Override
+  public Collection<String> getServicesForComponents(Collection<String> components) {
+    return stacks.stream()
+      .flatMap(m -> m.getServicesForComponents(components).stream())
+      .collect(toSet());
+  }
+
+  @Override
+  public String getServiceForConfigType(String config) {
+    if (ConfigHelper.CLUSTER_ENV.equals(config)) { // for backwards compatibility
+      return null;
+    }
+    return getServicesForConfigType(config)
+      .findAny()
+      .orElseThrow(() -> new IllegalArgumentException(Stack.formatMissingServiceForConfigType(config, ""ANY"")));
+  }
+
+  @Override
+  public Stream<String> getServicesForConfigType(String config) {
+    if (ConfigHelper.CLUSTER_ENV.equals(config)) { // for backwards compatibility
+      return Stream.empty();
+    }
+    return stacks.stream()
+      .map(m -> {
+        try {
+          return m.getServiceForConfigType(config);
+        } catch (IllegalArgumentException e) {
+          return null;
+        }
+      })
+      .filter(Objects::nonNull);
+  }
+
+  @Override
+  public Collection<DependencyInfo> getDependenciesForComponent(String component) {
+    return stacks.stream()
+      .flatMap(m -> m.getDependenciesForComponent(component).stream())
+      .collect(toSet());
+  }
+
+  @Override
+  public String getConditionalServiceForDependency(DependencyInfo dependency) {
+    return stacks.stream()
+      .map(m -> m.getConditionalServiceForDependency(dependency))
+      .filter(Objects::nonNull)
+      .findAny()
+      .orElse(null);
+  }
+
+  @Override
+  public String getExternalComponentConfig(String component) {
+    return stacks.stream()
+      .map(m -> m.getExternalComponentConfig(component))
+      .filter(Objects::nonNull)
+      .findAny()
+      .orElse(null);
+  }
+
+  @Override
+  public Cardinality getCardinality(String component) {
+    return stacks.stream()
+      .map(m -> m.getCardinality(component))
+      .filter(Objects::nonNull)
+      .findAny()
+      .orElse(null);
+  }
+
+  @Override
+  public AutoDeployInfo getAutoDeployInfo(String component) {
+    return stacks.stream()
+      .map(m -> m.getAutoDeployInfo(component))
+      .filter(Objects::nonNull)
+      .findAny()
+      .orElse(null);
+  }
+
+  @Override
+  public boolean isMasterComponent(String component) {
+    return stacks.stream()
+      .anyMatch(s -> s.isMasterComponent(component));
+  }
+
+  @Override
+  public Configuration getConfiguration(Collection<String> services) {
+    // FIXME probably too costly
+    return stacks.stream()
+      .map(m -> m.getConfiguration(services))
+      .reduce(Configuration.createEmpty(), Configuration::combine);
+  }
+
+  @Override
+  public Configuration getConfiguration() {
+    // FIXME probably too costly
+    return stacks.stream()
+      .map(StackDefinition::getConfiguration)
+      .reduce(Configuration.createEmpty(), Configuration::combine);
+  }
+}","[{'comment': 'This CompositeStack is a great idea!\r\nOne question though: Do we plan to have a sanity check somewhere to make sure a component/service/dependency/etc. is only defined in only one stack to avoid non-deterministic behavior?', 'commenter': 'benyoka'}, {'comment': 'Added check for service and component.', 'commenter': 'adoroszlai'}]"
231,ambari-server/src/main/java/org/apache/ambari/server/topology/AmbariContext.java,"@@ -213,97 +213,114 @@ public HostRoleCommand getPhysicalTask(long id) {
 
   public void createAmbariResources(ClusterTopology topology, String clusterName, SecurityType securityType,
                                     String repoVersionString, Long repoVersionId) {
-    Stack stack = topology.getBlueprint().getStack();
-    StackId stackId = new StackId(stack.getName(), stack.getVersion());
+    Map<StackId, Long> repoVersionByStack = new HashMap<>();
 
-    RepositoryVersionEntity repoVersion = null;
-    if (StringUtils.isEmpty(repoVersionString) && null == repoVersionId) {
-      List<RepositoryVersionEntity> stackRepoVersions = repositoryVersionDAO.findByStack(stackId);
+    Set<StackId> stackIds = topology.getBlueprint().getStackIds();
+    for (StackId stackId : stackIds) {
+      RepositoryVersionEntity repoVersion = null;
+      if (stackIds.size() == 1) {
+        repoVersion = findSpecifiedRepo(repoVersionString, repoVersionId, stackId);
+      }
+      if (null == repoVersion) {
+        repoVersion = findRepoForStack(stackId);
+      }
+      Preconditions.checkNotNull(repoVersion);
+      // only use a STANDARD repo when creating a new cluster
+      if (repoVersion.getType() != RepositoryType.STANDARD) {
+        throw new IllegalArgumentException(String.format(
+          ""Unable to create a cluster using the following repository since it is not a STANDARD type: %s"",
+          repoVersion
+        ));
+      }
+    }
 
-      if (stackRepoVersions.isEmpty()) {
-        // !!! no repos, try to get the version for the stack
-        VersionDefinitionResourceProvider vdfProvider = getVersionDefinitionResourceProvider();
+    StackId stackId = Iterables.getFirst(topology.getBlueprint().getStackIds(), null);
+    createAmbariClusterResource(clusterName, stackId, securityType);
+    createAmbariServiceAndComponentResources(topology, clusterName, repoVersionByStack);
+  }
 
-        Map<String, Object> properties = new HashMap<>();
-        properties.put(VersionDefinitionResourceProvider.VERSION_DEF_AVAILABLE_DEFINITION, stackId.toString());
+  private RepositoryVersionEntity findRepoForStack(StackId stackId) {
+    RepositoryVersionEntity repoVersion;
+    List<RepositoryVersionEntity> stackRepoVersions = repositoryVersionDAO.findByStack(stackId);
+    if (stackRepoVersions.isEmpty()) {
+      // !!! no repos, try to get the version for the stack
+      VersionDefinitionResourceProvider vdfProvider = getVersionDefinitionResourceProvider();
 
-        Request request = new RequestImpl(Collections.<String>emptySet(),
-            Collections.singleton(properties), Collections.<String, String>emptyMap(), null);
+      Map<String, Object> properties = new HashMap<>();
+      properties.put(VersionDefinitionResourceProvider.VERSION_DEF_AVAILABLE_DEFINITION, stackId.toString());
 
-        Long defaultRepoVersionId = null;
+      Request request = new RequestImpl(Collections.emptySet(),
+        Collections.singleton(properties), Collections.emptyMap(), null
+      );
 
-        try {
-          RequestStatus requestStatus = vdfProvider.createResources(request);
-          if (!requestStatus.getAssociatedResources().isEmpty()) {
-            Resource resource = requestStatus.getAssociatedResources().iterator().next();
-            defaultRepoVersionId = (Long) resource.getPropertyValue(VersionDefinitionResourceProvider.VERSION_DEF_ID);
-          }
-        } catch (Exception e) {
-          throw new IllegalArgumentException(String.format(
-              ""Failed to create a default repository version definition for stack %s. ""
-              + ""This typically is a result of not loading the stack correctly or being able ""
-              + ""to load information about released versions.  Create a repository version ""
-              + "" and try again."", stackId), e);
-        }
+      Long defaultRepoVersionId = null;
 
-        repoVersion = repositoryVersionDAO.findByPK(defaultRepoVersionId);
-        // !!! better not!
-        if (null == repoVersion) {
-          throw new IllegalArgumentException(String.format(
-              ""Failed to load the default repository version definition for stack %s. ""
-              + ""Check for a valid repository version and try again."", stackId));
+      try {
+        RequestStatus requestStatus = vdfProvider.createResources(request);
+        if (!requestStatus.getAssociatedResources().isEmpty()) {
+          Resource resource = requestStatus.getAssociatedResources().iterator().next();
+          defaultRepoVersionId = (Long) resource.getPropertyValue(VersionDefinitionResourceProvider.VERSION_DEF_ID);
         }
+      } catch (Exception e) {
+        throw new IllegalArgumentException(String.format(
+          ""Failed to create a default repository version definition for stack %s. ""
+            + ""This typically is a result of not loading the stack correctly or being able ""
+            + ""to load information about released versions.  Create a repository version ""
+            + "" and try again."", stackId), e);
+      }
 
-      } else if (stackRepoVersions.size() > 1) {
+      repoVersion = repositoryVersionDAO.findByPK(defaultRepoVersionId);
+      // !!! better not!
+      if (null == repoVersion) {
+        throw new IllegalArgumentException(String.format(
+          ""Failed to load the default repository version definition for stack %s. ""
+            + ""Check for a valid repository version and try again."", stackId));
+      }
 
-        Function<RepositoryVersionEntity, String> function = new Function<RepositoryVersionEntity, String>() {
-          @Override
-          public String apply(RepositoryVersionEntity input) {
-            return input.getVersion();
-          }
-        };
+    } else if (stackRepoVersions.size() > 1) {
+      String versions = stackRepoVersions.stream()
+        .map(RepositoryVersionEntity::getVersion)
+        .collect(joining("", ""));
 
-        Collection<String> versions = Collections2.transform(stackRepoVersions, function);
+      throw new IllegalArgumentException(String.format(
+        ""Several repositories were found for %s:  %s.  Specify the version with '%s'"",
+        stackId, versions, ProvisionClusterRequest.REPO_VERSION_PROPERTY
+      ));
+    } else {
+      repoVersion = stackRepoVersions.get(0);
+      LOG.info(""Found single matching repository version {} for stack {}"", repoVersion.getVersion(), stackId);
+    }
+    return repoVersion;
+  }
 
-        throw new IllegalArgumentException(String.format(""Several repositories were found for %s:  %s.  Specify the version""
-            + "" with '%s'"", stackId, StringUtils.join(versions, "", ""), ProvisionClusterRequest.REPO_VERSION_PROPERTY));
-      } else {
-        repoVersion = stackRepoVersions.get(0);
-        LOG.warn(""Cluster is being provisioned using the single matching repository version {}"", repoVersion.getVersion());
-      }
-    } else if (null != repoVersionId){
+  private RepositoryVersionEntity findSpecifiedRepo(String repoVersionString, Long repoVersionId, StackId stackId) {
+    RepositoryVersionEntity repoVersion = null;
+    if (null != repoVersionId) {
       repoVersion = repositoryVersionDAO.findByPK(repoVersionId);
 
       if (null == repoVersion) {
         throw new IllegalArgumentException(String.format(
           ""Could not identify repository version with repository version id %s for installing services. ""
             + ""Specify a valid repository version id with '%s'"",
-          repoVersionId, ProvisionClusterRequest.REPO_VERSION_ID_PROPERTY));
+          repoVersionId, ProvisionClusterRequest.REPO_VERSION_ID_PROPERTY
+        ));
       }
-    } else {
+    } else if (Strings.isNotEmpty(repoVersionString)) {
       repoVersion = repositoryVersionDAO.findByStackAndVersion(stackId, repoVersionString);","[{'comment': ""Shouldn't there be a third else branch that throws an IllegalArgimentException? Looks like if null == repoVersion and repoVersionString is null it will fall through while in other error cases it throws an exception."", 'commenter': 'benyoka'}, {'comment': 'If both are null (ie. no repo version was specified in the request), `createAmbariResources` tries to find any repo for the stack (`findRepoForStack`).  If both of these attempts fail, then an `IllegalArgumentException` is thrown by way of calling `Preconditions.checkNotNull`.\r\n\r\nThis is basically a slightly reorganized version of the existing logic, applied to each stack:\r\n\r\nhttps://github.com/apache/ambari/blob/995b5d4251bf76963d6793a5fd58a4643b352761/ambari-server/src/main/java/org/apache/ambari/server/topology/AmbariContext.java#L219-L299', 'commenter': 'adoroszlai'}, {'comment': 'I see, thanks.', 'commenter': 'benyoka'}]"
231,ambari-server/src/main/java/org/apache/ambari/server/utils/JsonUtils.java,"@@ -27,8 +32,13 @@
  */
 public class JsonUtils {
 
+  /**
+   * Used to serialize to/from json.
+   */
   public static JsonParser jsonParser = new JsonParser();
 
+  private static final ObjectMapper JSON_SERIALIZER = new ObjectMapper();
+","[{'comment': 'Probably we should use the immutable ObjectReader / ObjectWriter classes if we want to make this a general utility. ObjectMapper is said to be thread safe but uses internal locking that can be a perf bottleneck (according to StackOverflow).', 'commenter': 'benyoka'}, {'comment': 'Done.', 'commenter': 'adoroszlai'}]"
233,ambari-server/src/main/java/org/apache/ambari/server/upgrade/UpgradeCatalog300.java,"@@ -561,6 +564,32 @@ protected void executeDMLUpdates() throws AmbariException, SQLException {
     updateKerberosConfigurations();
     upgradeLdapConfiguration();
     createRoleAuthorizations();
+    addUserAuthenticationSequence();
+  }
+
+  protected void addUserAuthenticationSequence() throws SQLException {
+    final long maxUserAuthenticationId = fetchMaxUserAuthenticationId();
+    LOG.info(""Maximum user authentication ID = "" + maxUserAuthenticationId);
+    addSequence(""user_authentication_id_seq"", maxUserAuthenticationId + 1, false);
+  }
+
+  private long fetchMaxUserAuthenticationId() throws SQLException {","[{'comment': 'Would it make sense to create a reusable method that takes the ID column name and table name?', 'commenter': 'adoroszlai'}, {'comment': ""Good idea; I'll push a new version"", 'commenter': 'smolnar82'}, {'comment': '+1 for a reusable method.', 'commenter': 'rlevas'}]"
233,ambari-server/src/main/java/org/apache/ambari/server/upgrade/UpgradeCatalog300.java,"@@ -561,6 +564,32 @@ protected void executeDMLUpdates() throws AmbariException, SQLException {
     updateKerberosConfigurations();
     upgradeLdapConfiguration();
     createRoleAuthorizations();
+    addUserAuthenticationSequence();
+  }
+
+  protected void addUserAuthenticationSequence() throws SQLException {
+    final long maxUserAuthenticationId = fetchMaxUserAuthenticationId();
+    LOG.info(""Maximum user authentication ID = "" + maxUserAuthenticationId);
+    addSequence(""user_authentication_id_seq"", maxUserAuthenticationId + 1, false);
+  }
+
+  private long fetchMaxUserAuthenticationId() throws SQLException {
+    Statement statement = null;
+    ResultSet rs = null;
+    try {","[{'comment': 'Can be simplified a bit using ""try-with"":\r\n\r\n```\r\n    try (Statement stmt = dbAccessor.getConnection().createStatement();\r\n         ResultSet rs = stmt.executeQuery(String.format(""SELECT MAX(user_authentication_id) FROM %s"", USER_AUTHENTICATION_TABLE))\r\n    ) {\r\n      if (rs.next()) {\r\n        return rs.getLong(1);\r\n      }\r\n    }\r\n```', 'commenter': 'adoroszlai'}, {'comment': 'Good point', 'commenter': 'smolnar82'}]"
233,ambari-server/src/main/java/org/apache/ambari/server/upgrade/UpgradeCatalog300.java,"@@ -561,6 +561,13 @@ protected void executeDMLUpdates() throws AmbariException, SQLException {
     updateKerberosConfigurations();
     upgradeLdapConfiguration();
     createRoleAuthorizations();
+    addUserAuthenticationSequence();
+  }
+
+  protected void addUserAuthenticationSequence() throws SQLException {
+    final long maxUserAuthenticationId = fetchMaxId(USER_AUTHENTICATION_TABLE, ""user_authentication_id"");","[{'comment': 'Please use `USER_AUTHENTICATION_USER_AUTHENTICATION_ID_COLUMN` instead of `""user_authentication_id""`.', 'commenter': 'adoroszlai'}, {'comment': 'Done', 'commenter': 'smolnar82'}]"
234,ambari-server/src/main/java/org/apache/ambari/server/controller/ServiceGroupRequest.java,"@@ -57,9 +61,30 @@ public void setServiceGroupName(String serviceGroupName) {
     this.serviceGroupName = serviceGroupName;
   }
 
+  /**
+   * @return a list of associated mpack names
+   */
+  public Set<String> getMpackNames() {
+    return mpackNames;
+  }
+
+  /**
+   * @param mpackNames a list of associated mpack names
+   */
+  public void setMpackNames(Set<String> mpackNames) {
+    this.mpackNames.addAll(mpackNames);
+  }
+
   @Override
   public String toString() {
-    return String.format(""clusterName=%s, serviceGroupName=%s"", clusterName, serviceGroupName);
+    StringBuilder sb = new StringBuilder();
+    sb.append(""clusterName=""+clusterName+"",serviceGroupName=""+serviceGroupName);
+    if (!mpackNames.isEmpty()) {
+      sb.append("",mpackNames="");
+      mpackNames.stream().map(mpackName -> sb.append(mpackName + "",""));
+      sb.deleteCharAt(sb.length()-1);","[{'comment': 'I think simply using built-in `Set.toString()` would be OK.  If you prefer explicit formatting, instead of `map(append)` please use `Collectors.joining`, which is side-effect free, and also makes `deleteCharAt` unnecessary.', 'commenter': 'adoroszlai'}, {'comment': 'Updated', 'commenter': 'scottduan'}]"
234,ambari-server/src/main/java/org/apache/ambari/server/controller/ServiceGroupRequest.java,"@@ -57,9 +61,30 @@ public void setServiceGroupName(String serviceGroupName) {
     this.serviceGroupName = serviceGroupName;
   }
 
+  /**
+   * @return a list of associated mpack names
+   */
+  public Set<String> getMpackNames() {
+    return mpackNames;
+  }
+
+  /**
+   * @param mpackNames a list of associated mpack names
+   */
+  public void setMpackNames(Set<String> mpackNames) {
+    this.mpackNames.addAll(mpackNames);
+  }
+
   @Override
   public String toString() {
-    return String.format(""clusterName=%s, serviceGroupName=%s"", clusterName, serviceGroupName);
+    StringBuilder sb = new StringBuilder();
+    sb.append(""clusterName=""+clusterName+"",serviceGroupName=""+serviceGroupName);","[{'comment': 'Please avoid `+` inside `append()`.', 'commenter': 'adoroszlai'}]"
234,ambari-server/src/main/java/org/apache/ambari/server/controller/ServiceGroupRequest.java,"@@ -57,9 +61,30 @@ public void setServiceGroupName(String serviceGroupName) {
     this.serviceGroupName = serviceGroupName;
   }
 
+  /**
+   * @return a list of associated mpack names
+   */
+  public Set<String> getMpackNames() {
+    return mpackNames;
+  }
+
+  /**
+   * @param mpackNames a list of associated mpack names
+   */
+  public void setMpackNames(Set<String> mpackNames) {
+    this.mpackNames.addAll(mpackNames);","[{'comment': 'The method name says `set`, but it `add`s to existing ones.', 'commenter': 'adoroszlai'}, {'comment': 'You are right. I copied and wrote,but forgot to update the name.', 'commenter': 'scottduan'}]"
234,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/ServiceGroupResourceProvider.java,"@@ -269,6 +271,11 @@ private ServiceGroupRequest getRequest(Map<String, Object> properties) {
     String clusterName = (String) properties.get(SERVICE_GROUP_CLUSTER_NAME_PROPERTY_ID);
     String serviceGroupName = (String) properties.get(SERVICE_GROUP_SERVICE_GROUP_NAME_PROPERTY_ID);
     ServiceGroupRequest svcRequest = new ServiceGroupRequest(clusterName, serviceGroupName);
+    Set<Object> mpackNames = (Set<Object>) properties.get(SERVICE_GROUP_SERVICE_GROUP_MPACKNAME_PROPERTY_ID);
+    if (mpackNames != null) {
+      Set<String> mpackNamesSet = mpackNames.stream().flatMap(mpack -> ((Map<String, String>)mpack).values().stream()).collect(Collectors.toSet());","[{'comment': 'Can you minimize the number of unchecked casts by avoiding the intermediate `Set<Object>` type, and casting to `Set<Map<String,String>>` (or better yet, `Collection<Map<String,String>>`) in the first place?', 'commenter': 'adoroszlai'}]"
234,ambari-server/src/main/java/org/apache/ambari/server/orm/entities/ServiceGroupEntity.java,"@@ -80,6 +83,10 @@
   @OneToMany(mappedBy=""serviceGroupDependency"")
   private List<ServiceGroupDependencyEntity> dependencies;
 
+  @ElementCollection
+  @Column(name = ""name"")","[{'comment': 'Don\'t we need a corresponding change in the DDL scripts?\r\n(`ambari-server/src/main/resources/Ambari-DDL-*-CREATE.sql`)\r\n\r\n```\r\nERROR: relation ""servicegroupentity_mpacknames"" does not exist\r\n```', 'commenter': 'adoroszlai'}]"
234,ambari-server/src/main/java/org/apache/ambari/server/state/ServiceGroup.java,"@@ -46,6 +46,14 @@
 
   Set<ServiceGroupDependencyResponse> getServiceGroupDependencyResponses();
 
+  Set<String> getServiceGroupMpackNames();
+
+  void addServiceGroupMpackName(String mpackName);
+
+  void addServiceGroupMpackNames(Set<String> mpackNames);
+
+  void setServiceGroupMpackNames(Set<String> mpackNames);","[{'comment': 'Can you please omit `ServiceGroup` from the method names?', 'commenter': 'adoroszlai'}, {'comment': 'Seems other methods have ServiceGroup, anyway, I will remove it.', 'commenter': 'scottduan'}]"
234,ambari-server/src/main/java/org/apache/ambari/server/state/ServiceGroupImpl.java,"@@ -217,7 +246,14 @@ public Cluster getCluster() {
   @Override
   public void debugDump(StringBuilder sb) {
     sb.append(""ServiceGroup={ serviceGroupName="" + getServiceGroupName() + "", clusterName=""
-      + cluster.getClusterName() + "", clusterId="" + cluster.getClusterId() + ""}"");
+      + cluster.getClusterName() + "", clusterId="" + cluster.getClusterId() );
+    Set<String> mpackNames = getServiceGroupMpackNames();
+    if (!mpackNames.isEmpty()) {
+      sb.append("", mpackNames="");
+      mpackNames.stream().map(mpackName ->sb.append(mpackName + "",""));
+      sb.deleteCharAt(sb.length()-1);","[{'comment': 'Same as above for similar logic in `ServiceGroupRequest.toString()`.', 'commenter': 'adoroszlai'}, {'comment': 'Updated', 'commenter': 'scottduan'}]"
234,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/ServiceGroupResourceProvider.java,"@@ -422,6 +430,11 @@ private void validateCreateRequests(Set<ServiceGroupRequest> requests, Clusters
       }
       serviceGroupNames.get(clusterName).add(serviceGroupName);
 
+      if (request.getMpackNames().size() != 1) {
+        String errmsg = ""Invalid arguments, only one mpack is allowed in the service group "" + serviceGroupName;","[{'comment': 'Please distinguish between 0 mpacks and 2+ mpacks.  The error message ""only one mpack is allowed"" is a bit confusing when the request doesn\'t specify any mpacks for the service group.', 'commenter': 'adoroszlai'}, {'comment': 'I did not write the description of this bug clearly. At this time, temporarily we only support one mpack per servicegroup, but finally each servicegroup must have one or more mpacks when multiple mpacks support code is implemented. Again, I will add more clearer errmsg here.', 'commenter': 'scottduan'}, {'comment': 'if (request.getMpackNames().size() < 1)\r\nError should be that ""Invalid arguments, atleast one mpack should be specified for a service group""\r\nFrom backend, we should not restrict the number of mpacks associated to a service group.\r\n@jayush , please correct me if wrong.', 'commenter': 'mradha25'}]"
234,ambari-server/src/test/java/org/apache/ambari/server/controller/internal/ServiceGroupResourceProviderTest.java,"@@ -42,4 +75,83 @@ public static void createServiceGroup(AmbariManagementController controller, Str
     createServiceGroups(controller, Collections.singleton(request));
   }
 
+  @Test
+  public void testCreateServiceGroupsWithMpacks() throws Exception {
+    // Valid Requesty
+    String body = ""[{\""ServiceGroupInfo\"":{\""service_group_name\"": \""CORE\"",\""mpacks\"": [{\""name\"": \""HDPCORE\""}]}},{\""ServiceGroupInfo\"": {\""service_group_name\"": \""EDW-MKTG\"",\""mpacks\"": [{\""name\"": \""EDW2\""}]}}]"";
+    runTestCreateServiceGroupsWithMpacks(body);
+    System.out.println(""++++++ Test valid request successfully ++++++"");
+
+    // Invalid Request
+    body = ""[{\""ServiceGroupInfo\"":{\""service_group_name\"": \""CORE\"",\""mpacks\"": [{\""name\"": \""HDPCORE\""},{\""name\"": \""EDW\""}]}},{\""ServiceGroupInfo\"": {\""service_group_name\"": \""EDW-MKTG\"",\""mpacks\"": [{\""name\"": \""EDW2\""}]}}]"";
+    runTestCreateServiceGroupsWithMpacks(body);
+    System.out.println(""++++++ Test invalid request successfully ++++++"");
+  }
+
+  private static void runTestCreateServiceGroupsWithMpacks(String body) throws Exception{","[{'comment': 'If the same sequence is OK for both valid and invalid request, then what is being tested?', 'commenter': 'adoroszlai'}, {'comment': 'For invalid request, this is the negative test. As I mentioned above, actually at this time we only support 1 mpck per servicegroup. So in invalid test case, if we can detect it is invalid request, this test case succeeds. I will update the code to make it more clear.', 'commenter': 'scottduan'}]"
234,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/ServiceGroupResourceProvider.java,"@@ -78,6 +79,7 @@
   public static final String SERVICE_GROUP_CLUSTER_NAME_PROPERTY_ID = RESPONSE_KEY + PropertyHelper.EXTERNAL_PATH_SEP + ""cluster_name"";
   public static final String SERVICE_GROUP_SERVICE_GROUP_ID_PROPERTY_ID = RESPONSE_KEY + PropertyHelper.EXTERNAL_PATH_SEP + ""service_group_id"";
   public static final String SERVICE_GROUP_SERVICE_GROUP_NAME_PROPERTY_ID = RESPONSE_KEY + PropertyHelper.EXTERNAL_PATH_SEP + ""service_group_name"";
+  public static final String SERVICE_GROUP_SERVICE_GROUP_MPACKNAME_PROPERTY_ID = RESPONSE_KEY + PropertyHelper.EXTERNAL_PATH_SEP + ""mpacks"";","[{'comment': 'Can we simply call it SERVICE_GROUP_MPACKNAME_PROPERTY_ID?', 'commenter': 'mradha25'}, {'comment': 'We need to add this property to -\r\n1. static {\r\n     // properties\r\nhttps://github.com/apache/ambari/pull/234/files#diff-b9a4d90adcd0c31956baab7c90540e53R107\r\n2. To the response of createResourcesAuthorized\r\nhttps://github.com/apache/ambari/pull/234/files#diff-b9a4d90adcd0c31956baab7c90540e53L162\r\n3. To the response of getResourcesAuthorized\r\nhttps://github.com/apache/ambari/pull/234/files#diff-b9a4d90adcd0c31956baab7c90540e53R202\r\n', 'commenter': 'mradha25'}, {'comment': 'OK, I will change it to SERVICE_GROUP_MPACKNAME_PROPERTY_ID', 'commenter': 'scottduan'}]"
234,ambari-server/src/test/java/org/apache/ambari/server/controller/internal/ServiceGroupResourceProviderTest.java,"@@ -42,4 +75,90 @@ public static void createServiceGroup(AmbariManagementController controller, Str
     createServiceGroups(controller, Collections.singleton(request));
   }
 
+  @Test
+  public void testCreateServiceGroupsWithMpacks() throws Exception {
+    // Valid Requesty with one mpack name per service group
+    String body = ""[{\""ServiceGroupInfo\"":{\""service_group_name\"": \""CORE\"",\""mpacks\"": [{\""name\"": \""HDPCORE\""}]}},{\""ServiceGroupInfo\"": {\""service_group_name\"": \""EDW-MKTG\"",\""mpacks\"": [{\""name\"": \""EDW2\""}]}}]"";
+    try {
+      runTestCreateServiceGroupsWithMpacks(body);
+    } catch (Exception e) {
+      System.out.println(""++++++ Test valid request unsuccessfully ++++++"");
+      throw e;
+    }
+    System.out.println(""++++++ Test valid request successfully ++++++"");
+
+    // Invalid Request with two mpack names in the service group CORE
+    body = ""[{\""ServiceGroupInfo\"":{\""service_group_name\"": \""CORE\"",\""mpacks\"": [{\""name\"": \""HDPCORE\""},{\""name\"": \""EDW\""}]}},{\""ServiceGroupInfo\"": {\""service_group_name\"": \""EDW-MKTG\"",\""mpacks\"": [{\""name\"": \""EDW2\""}]}}]"";
+    try {
+      runTestCreateServiceGroupsWithMpacks(body);
+    } catch (IllegalArgumentException e) {
+      System.out.println(""++++++ Test invalid request successfully ++++++"");","[{'comment': 'Thanks, the test is much clearer now for the reader.  However, unit tests should fail in case of unexpected results, not print a message.  Can you please:\r\n\r\n* extract the 2 test cases into separate methods\r\n* get rid of the try-catch in both cases\r\n* add `@Test(expected = IllegalArgumentException.class)` for the invalid request case\r\n\r\nThis will allow the test suite to catch any regression automatically, instead of relying on the developer to run it and look for text output.  Thanks.', 'commenter': 'adoroszlai'}]"
234,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/ServiceGroupResourceProvider.java,"@@ -422,6 +436,11 @@ private void validateCreateRequests(Set<ServiceGroupRequest> requests, Clusters
       }
       serviceGroupNames.get(clusterName).add(serviceGroupName);
 
+      if (request.getMpackNames().size() != 1) {","[{'comment': 'Since this change introduces mpacks for service groups, and it also makes exactly one mpack a requirement, it breaks existing code that uses `ServiceGroupResourceProvider` to create mpacks.  For one, it increases the number of failing/erroring unit tests:\r\n\r\n```\r\n[ERROR] Tests run: 5122, Failures: 39, Errors: 231, Skipped: 37\r\n->\r\n[ERROR] Tests run: 5123, Failures: 42, Errors: 296, Skipped: 37\r\n```\r\n\r\nCan you please address those?  I think most could be fixed by adding some dummy mpack name in https://github.com/apache/ambari/blob/00f29b9a4f7d5829f1c5a45fad6da587f599a224/ambari-server/src/test/java/org/apache/ambari/server/controller/internal/ServiceGroupResourceProviderTest.java#L39-L43', 'commenter': 'adoroszlai'}]"
234,ambari-server/src/main/resources/Ambari-DDL-Derby-CREATE.sql,"@@ -1156,6 +1156,16 @@ CREATE TABLE alert_notice (
   FOREIGN KEY (history_id) REFERENCES alert_history(alert_id)
 );
 
+CREATE TABLE servicegroup_mpackname (
+  service_group_id BIGINT NOT NULL,
+  service_group_cluster_id BIGINT NOT NULL,
+  mpack_name VARCHAR(255) NOT NULL,
+  CONSTRAINT PK_servicegroup_mpackname PRIMARY KEY (service_group_id, service_group_cluster_id, mpack_name),
+  FOREIGN KEY (service_group_id) REFERENCES servicegroups(service_group_id),
+  FOREIGN KEY (service_group_cluster_id) REFERENCES servicegroups(service_group_cluster_id)","[{'comment': 'Constraints/foreign keys in this table are different than in the other DBMS schemas.', 'commenter': 'adoroszlai'}]"
234,ambari-server/src/main/resources/Ambari-DDL-Postgres-CREATE.sql,"@@ -1119,6 +1119,15 @@ CREATE TABLE alert_notice (
   FOREIGN KEY (history_id) REFERENCES alert_history(alert_id)
 );
 
+CREATE TABLE servicegroup_mpacknames (
+  service_group_id BIGINT NOT NULL,
+  service_group_cluster_id BIGINT NOT NULL,
+  mpack_name VARCHAR(255) NOT NULL UNIQUE,
+  CONSTRAINT PK_servicegroup_mpacknames PRIMARY KEY (service_group_id, service_group_cluster_id, mpack_name),
+  CONSTRAINT UQ_servicegroup_mpacknames UNIQUE (service_group_id, service_group_cluster_id),","[{'comment': 'These unique constraints will require upgrade logic for dropping them when Ambari wants to allow multiple mpacks per service group.', 'commenter': 'adoroszlai'}]"
234,ambari-server/src/main/resources/Ambari-DDL-Derby-CREATE.sql,"@@ -1156,6 +1156,16 @@ CREATE TABLE alert_notice (
   FOREIGN KEY (history_id) REFERENCES alert_history(alert_id)
 );
 
+CREATE TABLE servicegroup_mpackname (","[{'comment': 'Table name is different in this schema (`servicegroup_mpackname` vs `servicegroup_mpacknames`).', 'commenter': 'adoroszlai'}]"
234,ambari-server/src/main/resources/Ambari-DDL-SQLServer-CREATE.sql,"@@ -1173,6 +1173,15 @@ CREATE TABLE alert_notice (
   FOREIGN KEY (history_id) REFERENCES alert_history(alert_id)
 );
 
+CREATE TABLE servicegroup_mpacknames (
+  service_group_id BIGINT NOT NULL,
+  service_group_cluster_id BIGINT NOT NULL,
+  mpack_name VARCHAR(255) NOT NULL UNIQUE,
+  CONSTRAINT PK_servicegroup_mpacknames PRIMARY KEY (service_group_id, service_group_cluster_id, mpack_name),
+  CONSTRAINT UQ_servicegroup_mpacknames UNIQUE (service_group_id, service_group_cluster_id),
+  CONSTRAINT FK_servicgroups FOREIGN KEY (service_group_id, service_group_cluster_id) REFERENCES servicegroups(id, cluster_id) ON DELETE CASCADE
+);DatabaseConsistencyCheckHelper","[{'comment': '`DatabaseConsistencyCheckHelper` does not seem to belong here, and will cause syntax error.', 'commenter': 'adoroszlai'}]"
234,ambari-server/src/main/java/org/apache/ambari/server/orm/entities/ServiceGroupEntity.java,"@@ -80,6 +84,14 @@
   @OneToMany(mappedBy=""serviceGroupDependency"")
   private List<ServiceGroupDependencyEntity> dependencies;
 
+  @ElementCollection()
+  @CollectionTable(name = ""servicegroup_mpacknames"",
+                   joinColumns = {@JoinColumn(name = ""service_group_id"", referencedColumnName = ""id""),
+                                  @JoinColumn(name = ""service_group_cluster_id"", referencedColumnName = ""cluster_id"")})
+  @Column(name = ""mpack_name"", unique = true, nullable = false)","[{'comment': 'I don\'t think the `mpack_name` in itself should be unique.  This causes unit test failures with the current constant `""dummy""` mpack name for tests.  (eg. `AmbariManagementControllerTest`)', 'commenter': 'adoroszlai'}]"
234,ambari-server/src/main/java/org/apache/ambari/server/controller/ServiceGroupRequest.java,"@@ -57,9 +61,30 @@ public void setServiceGroupName(String serviceGroupName) {
     this.serviceGroupName = serviceGroupName;
   }
 
+  /**
+   * @return a list of associated mpack names
+   */
+  public Set<String> getMpackNames() {
+    return mpackNames;
+  }
+
+  /**
+   * @param mpackNames a list of associated mpack names
+   */
+  public void addMpackNames(Set<String> mpackNames) {
+    if (mpackNames != null) {
+      this.mpackNames.addAll(mpackNames);
+    }
+  }
+
   @Override
   public String toString() {
-    return String.format(""clusterName=%s, serviceGroupName=%s"", clusterName, serviceGroupName);
+    StringBuilder sb = new StringBuilder();","[{'comment': ""We've been gradually moving to ToStringBuilder here instead"", 'commenter': 'ncole'}]"
234,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/ServiceGroupResourceProvider.java,"@@ -78,6 +80,7 @@
   public static final String SERVICE_GROUP_CLUSTER_NAME_PROPERTY_ID = RESPONSE_KEY + PropertyHelper.EXTERNAL_PATH_SEP + ""cluster_name"";
   public static final String SERVICE_GROUP_SERVICE_GROUP_ID_PROPERTY_ID = RESPONSE_KEY + PropertyHelper.EXTERNAL_PATH_SEP + ""service_group_id"";
   public static final String SERVICE_GROUP_SERVICE_GROUP_NAME_PROPERTY_ID = RESPONSE_KEY + PropertyHelper.EXTERNAL_PATH_SEP + ""service_group_name"";
+  public static final String SERVICE_GROUP_MPACKNAME_PROPERTY_ID = RESPONSE_KEY + PropertyHelper.EXTERNAL_PATH_SEP + ""mpacks"";","[{'comment': 'This is an awkward construct.  Either do the string directly or use PropertyHelper.getPropertyId(...).', 'commenter': 'ncole'}]"
234,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/ServiceGroupResourceProvider.java,"@@ -422,6 +436,11 @@ private void validateCreateRequests(Set<ServiceGroupRequest> requests, Clusters
       }
       serviceGroupNames.get(clusterName).add(serviceGroupName);
 
+      if (request.getMpackNames().size() != 1) {
+        String errmsg = ""Invalid arguments, "" + request.getMpackNames().size() + "" mpack(s) found in the service group "" + serviceGroupName + "", only one mpack is allowed"";","[{'comment': 'Use String.format() here.', 'commenter': 'ncole'}]"
234,ambari-server/src/main/java/org/apache/ambari/server/orm/entities/ServiceGroupEntity.java,"@@ -80,6 +84,14 @@
   @OneToMany(mappedBy=""serviceGroupDependency"")
   private List<ServiceGroupDependencyEntity> dependencies;
 
+  @ElementCollection()","[{'comment': 'Formatting', 'commenter': 'ncole'}]"
239,ambari-server/src/main/python/ambari_server/setupSecurity.py,"@@ -79,6 +79,7 @@
 PAM_CONFIG_FILE = 'pam.configuration'
 
 IS_LDAP_CONFIGURED = ""ambari.ldap.authentication.enabled""
+IS_LDAP_CONFIGURED_IN_AMBARI_PROPERTIES = ""ambari.ldap.isConfigured""","[{'comment': 'Why do we need to check the ambari.properties file?  The values stored in the Ambari DB should be good enough.', 'commenter': 'rlevas'}, {'comment': 'Did not you tell, that it should be backward compatible?', 'commenter': 'smolnar82'}, {'comment': 'It should be backwards compatible from the perspective of the user.  However the relevant data can all be stored in the Ambari DB so it is in one place.   By retrieving the properties from the Ambari DB rather that looking at the ambari.properties file should suffice. ', 'commenter': 'rlevas'}, {'comment': 'Fixed', 'commenter': 'smolnar82'}]"
239,ambari-server/src/main/python/ambari_server/setupSecurity.py,"@@ -274,6 +275,56 @@ def __init__(self, options):
   def no_ldap_sync_options_set(self):
     return not self.ldap_sync_all and not self.ldap_sync_existing and self.ldap_sync_users is None and self.ldap_sync_groups is None
 
+def getLdapPropertyFromDB(properties, admin_login, admin_password, property_name):
+  ldapProperty = None
+  url = get_ambari_server_api_base(properties) + SETUP_LDAP_CONFIG_URL
+  admin_auth = base64.encodestring('%s:%s' % (admin_login, admin_password)).replace('\n', '')
+  request = urllib2.Request(url)
+  request.add_header('Authorization', 'Basic %s' % admin_auth)
+  request.add_header('X-Requested-By', 'ambari')
+  request.get_method = lambda: 'GET'
+  request_in_progress = True
+
+  sys.stdout.write('\nFetching LDAP configuration from DB')
+  numOfTries = 0
+  while request_in_progress:
+    numOfTries += 1
+    if (numOfTries == 60):
+      raise FatalException(1, ""Could not fetch LDAP configuration within a minute; giving up!"")
+    sys.stdout.write('.')
+    sys.stdout.flush()
+
+    try:
+      response = urllib2.urlopen(request)","[{'comment': 'The response object is not closed.\r\n\r\nYou can use:\r\n\r\n```python\r\nfrom contextlib import closing\r\nwith closing(urllib2.urlopen(request)) as response:\r\n  ...\r\n  response.read()\r\n  ...\r\n```', 'commenter': 'zeroflag'}, {'comment': ""Thanks for this catch; it's good to learn these kind of stuff."", 'commenter': 'smolnar82'}, {'comment': 'Fixed', 'commenter': 'smolnar82'}]"
244,ambari-common/src/main/python/ambari_commons/shell.py,"@@ -231,42 +231,92 @@ def runPowershell(self, file=None, script_block=None, args=[]):
     return _dict_to_object({'exitCode': code, 'output': out, 'error': err})
 
 
-#linux specific code
-@OsFamilyFuncImpl(os_family=OsFamilyImpl.DEFAULT)
-def kill_process_with_children(parent_pid):
-  def kill_tree_function(pid, signal):
-    '''
-    Kills process tree starting from a given pid.
-    '''
-    # The command below starts 'ps' linux utility and then parses it's
-    # output using 'awk'. AWK recursively extracts PIDs of all children of
-    # a given PID and then passes list of ""kill -<SIGNAL> PID"" commands to 'sh'
-    # shell.
-    CMD = """"""ps xf | awk -v PID="""""" + str(pid) + \
-          """""" ' $1 == PID { P = $1; next } P && /_/ { P = P "" "" $1;"""""" + \
-          """"""K=P } P && !/_/ { P="""" }  END { print ""kill -"""""" \
-          + str(signal) + """""" ""K }' | sh """"""
-    process = subprocess.Popen(CMD, stdout=subprocess.PIPE,
-                               stderr=subprocess.PIPE, shell=True)
-    process.communicate()
-
-  _run_kill_function(kill_tree_function, parent_pid)
-
-
-def _run_kill_function(kill_function, pid):
-  try:
-    kill_function(pid, signal.SIGTERM)
-  except Exception, e:
-    logger.warn(""Failed to kill PID %d"" % (pid))
-    logger.warn(""Reported error: "" + repr(e))
+def get_all_childrens(base_pid):
+  """"""
+  Return all child's pids of base_pid process
+
+  :type base_pid int
+  :rtype list[(int, str, str)]
+  """"""
+  parent_pid_path_pattern = ""/proc/{0}/task/{0}/children""
+  comm_path_pattern = ""/proc/{0}/comm""
+  cmdline_path_pattern = ""/proc/{0}/cmdline""
+
+  def read_childrens(pid):
+    try:
+      with open(parent_pid_path_pattern.format(pid), ""r"") as f:
+        return [int(item) for item in f.readline().strip().split("" "")]
+    except (IOError, ValueError):
+      return []
+
+  def read_command(pid):
+    try:
+      with open(comm_path_pattern.format(pid), ""r"") as f:
+        return f.readline().strip()
+    except IOError:
+      return """"
+
+  def read_cmdline(pid):
+    try:
+      with open(cmdline_path_pattern.format(pid), ""r"") as f:
+        return f.readline().strip()
+    except IOError:
+      return """"
 
-  time.sleep(gracefull_kill_delay)
+  done = []
+  pending = [int(base_pid)]
 
+  while pending:
+    mypid = pending.pop(0)
+    children = read_childrens(mypid)
+
+    done.append((mypid, read_command(mypid), read_cmdline(mypid)))
+    pending.extend(children)
+
+  return done
+
+
+def is_pid_life(pid):
+  """"""
+  check if process with pid still exists (not counting it real state)
+
+  :type pid int
+  """"""
+  pid_path = ""/proc/{0}""
   try:
-    kill_function(pid, signal.SIGKILL)
-  except Exception, e:
-    logger.error(""Failed to send SIGKILL to PID %d. Process exited?"" % (pid))
-    logger.error(""Reported error: "" + repr(e))
+    return os.path.exists(pid_path.format(pid))
+  except Exception:
+    logger.debug(""Failed to check pid state"")
+    return False
+
+
+# linux specific code
+@OsFamilyFuncImpl(os_family=OsFamilyImpl.DEFAULT)
+def kill_process_with_children(parent_pid):
+  exception_list = [""apt-get"", ""apt"", ""yum"", ""zypper"", ""zypp""]
+  signals_to_post = [signal.SIGTERM, signal.SIGKILL]
+  all_chield_pids = [item[0] for item in get_all_childrens(parent_pid) if item[1].lower() not in exception_list and item[0] != os.getpid()]
+  clean_kill = True
+  last_error = """"
+
+  for sig in signals_to_post:
+    # we need to kill processes from the bottom of the tree
+    pids_to_kill = sorted(all_chield_pids, reverse=True)
+    for pid in pids_to_kill:
+      try:
+        if is_pid_life(pid):","[{'comment': '> is_pid_life\r\nmaybe is_pid_alive?', 'commenter': 'Unknown'}]"
244,ambari-common/src/main/python/ambari_commons/shell.py,"@@ -231,42 +231,92 @@ def runPowershell(self, file=None, script_block=None, args=[]):
     return _dict_to_object({'exitCode': code, 'output': out, 'error': err})
 
 
-#linux specific code
-@OsFamilyFuncImpl(os_family=OsFamilyImpl.DEFAULT)
-def kill_process_with_children(parent_pid):
-  def kill_tree_function(pid, signal):
-    '''
-    Kills process tree starting from a given pid.
-    '''
-    # The command below starts 'ps' linux utility and then parses it's
-    # output using 'awk'. AWK recursively extracts PIDs of all children of
-    # a given PID and then passes list of ""kill -<SIGNAL> PID"" commands to 'sh'
-    # shell.
-    CMD = """"""ps xf | awk -v PID="""""" + str(pid) + \
-          """""" ' $1 == PID { P = $1; next } P && /_/ { P = P "" "" $1;"""""" + \
-          """"""K=P } P && !/_/ { P="""" }  END { print ""kill -"""""" \
-          + str(signal) + """""" ""K }' | sh """"""
-    process = subprocess.Popen(CMD, stdout=subprocess.PIPE,
-                               stderr=subprocess.PIPE, shell=True)
-    process.communicate()
-
-  _run_kill_function(kill_tree_function, parent_pid)
-
-
-def _run_kill_function(kill_function, pid):
-  try:
-    kill_function(pid, signal.SIGTERM)
-  except Exception, e:
-    logger.warn(""Failed to kill PID %d"" % (pid))
-    logger.warn(""Reported error: "" + repr(e))
+def get_all_childrens(base_pid):
+  """"""
+  Return all child's pids of base_pid process
+
+  :type base_pid int
+  :rtype list[(int, str, str)]
+  """"""
+  parent_pid_path_pattern = ""/proc/{0}/task/{0}/children""
+  comm_path_pattern = ""/proc/{0}/comm""
+  cmdline_path_pattern = ""/proc/{0}/cmdline""
+
+  def read_childrens(pid):","[{'comment': 'def read_children(pid)', 'commenter': 'ncole'}]"
244,ambari-common/src/main/python/ambari_commons/shell.py,"@@ -231,42 +231,92 @@ def runPowershell(self, file=None, script_block=None, args=[]):
     return _dict_to_object({'exitCode': code, 'output': out, 'error': err})
 
 
-#linux specific code
-@OsFamilyFuncImpl(os_family=OsFamilyImpl.DEFAULT)
-def kill_process_with_children(parent_pid):
-  def kill_tree_function(pid, signal):
-    '''
-    Kills process tree starting from a given pid.
-    '''
-    # The command below starts 'ps' linux utility and then parses it's
-    # output using 'awk'. AWK recursively extracts PIDs of all children of
-    # a given PID and then passes list of ""kill -<SIGNAL> PID"" commands to 'sh'
-    # shell.
-    CMD = """"""ps xf | awk -v PID="""""" + str(pid) + \
-          """""" ' $1 == PID { P = $1; next } P && /_/ { P = P "" "" $1;"""""" + \
-          """"""K=P } P && !/_/ { P="""" }  END { print ""kill -"""""" \
-          + str(signal) + """""" ""K }' | sh """"""
-    process = subprocess.Popen(CMD, stdout=subprocess.PIPE,
-                               stderr=subprocess.PIPE, shell=True)
-    process.communicate()
-
-  _run_kill_function(kill_tree_function, parent_pid)
-
-
-def _run_kill_function(kill_function, pid):
-  try:
-    kill_function(pid, signal.SIGTERM)
-  except Exception, e:
-    logger.warn(""Failed to kill PID %d"" % (pid))
-    logger.warn(""Reported error: "" + repr(e))
+def get_all_childrens(base_pid):","[{'comment': 'def get_all_children(base_pid)', 'commenter': 'ncole'}, {'comment': ""@ncole no, actually same as previous implementation  (popen didn't use ambari_sudo or sudo command)\r\n\r\nIt is another kind of problem which would need to be solved via sudo.py..."", 'commenter': 'hapylestat'}]"
244,ambari-common/src/main/python/ambari_commons/shell.py,"@@ -231,42 +231,92 @@ def runPowershell(self, file=None, script_block=None, args=[]):
     return _dict_to_object({'exitCode': code, 'output': out, 'error': err})
 
 
-#linux specific code
-@OsFamilyFuncImpl(os_family=OsFamilyImpl.DEFAULT)
-def kill_process_with_children(parent_pid):
-  def kill_tree_function(pid, signal):
-    '''
-    Kills process tree starting from a given pid.
-    '''
-    # The command below starts 'ps' linux utility and then parses it's
-    # output using 'awk'. AWK recursively extracts PIDs of all children of
-    # a given PID and then passes list of ""kill -<SIGNAL> PID"" commands to 'sh'
-    # shell.
-    CMD = """"""ps xf | awk -v PID="""""" + str(pid) + \
-          """""" ' $1 == PID { P = $1; next } P && /_/ { P = P "" "" $1;"""""" + \
-          """"""K=P } P && !/_/ { P="""" }  END { print ""kill -"""""" \
-          + str(signal) + """""" ""K }' | sh """"""
-    process = subprocess.Popen(CMD, stdout=subprocess.PIPE,
-                               stderr=subprocess.PIPE, shell=True)
-    process.communicate()
-
-  _run_kill_function(kill_tree_function, parent_pid)
-
-
-def _run_kill_function(kill_function, pid):
-  try:
-    kill_function(pid, signal.SIGTERM)
-  except Exception, e:
-    logger.warn(""Failed to kill PID %d"" % (pid))
-    logger.warn(""Reported error: "" + repr(e))
+def get_all_childrens(base_pid):
+  """"""
+  Return all child's pids of base_pid process
+
+  :type base_pid int
+  :rtype list[(int, str, str)]
+  """"""
+  parent_pid_path_pattern = ""/proc/{0}/task/{0}/children""
+  comm_path_pattern = ""/proc/{0}/comm""
+  cmdline_path_pattern = ""/proc/{0}/cmdline""
+
+  def read_childrens(pid):
+    try:
+      with open(parent_pid_path_pattern.format(pid), ""r"") as f:
+        return [int(item) for item in f.readline().strip().split("" "")]
+    except (IOError, ValueError):
+      return []
+
+  def read_command(pid):
+    try:
+      with open(comm_path_pattern.format(pid), ""r"") as f:
+        return f.readline().strip()
+    except IOError:
+      return """"
+
+  def read_cmdline(pid):
+    try:
+      with open(cmdline_path_pattern.format(pid), ""r"") as f:
+        return f.readline().strip()
+    except IOError:
+      return """"
 
-  time.sleep(gracefull_kill_delay)
+  done = []
+  pending = [int(base_pid)]
 
+  while pending:
+    mypid = pending.pop(0)
+    children = read_childrens(mypid)
+
+    done.append((mypid, read_command(mypid), read_cmdline(mypid)))
+    pending.extend(children)
+
+  return done
+
+
+def is_pid_life(pid):
+  """"""
+  check if process with pid still exists (not counting it real state)
+
+  :type pid int
+  """"""
+  pid_path = ""/proc/{0}""
   try:
-    kill_function(pid, signal.SIGKILL)
-  except Exception, e:
-    logger.error(""Failed to send SIGKILL to PID %d. Process exited?"" % (pid))
-    logger.error(""Reported error: "" + repr(e))
+    return os.path.exists(pid_path.format(pid))
+  except Exception:
+    logger.debug(""Failed to check pid state"")
+    return False
+
+
+# linux specific code
+@OsFamilyFuncImpl(os_family=OsFamilyImpl.DEFAULT)
+def kill_process_with_children(parent_pid):
+  exception_list = [""apt-get"", ""apt"", ""yum"", ""zypper"", ""zypp""]
+  signals_to_post = [signal.SIGTERM, signal.SIGKILL]
+  all_chield_pids = [item[0] for item in get_all_childrens(parent_pid) if item[1].lower() not in exception_list and item[0] != os.getpid()]","[{'comment': 'Can we ever end up with never-dying yum here?', 'commenter': 'ncole'}, {'comment': 'It is rare situation that yum hangs. Most likey it can slowly download package by holding lock. Ambari able to check package manager lock and retry package installation over time. However if some serious problem happen, what cause yum to hang, manual user debugging is needed anyway.\r\n\r\nIt is lesser evil, than killing it in the middle of the work and then screw the whole system.', 'commenter': 'hapylestat'}]"
252,ambari-server/src/main/java/org/apache/ambari/server/api/services/AmbariMetaInfo.java,"@@ -1655,4 +1656,25 @@ KerberosDescriptor readKerberosDescriptorFromFile(String fileLocation) throws Am
   public File getCommonWidgetsDescriptorFile() {
     return commonWidgetsDescriptorFile;
   }
+
+  /***
+   * Fetch all mpacks from mpackMap
+   * @return
+   */
+  public Collection<Mpack> getMpacks() {
+    return mpackManager.getMpackMap().values();
+  }
+
+  /***
+   * Fetch a particular mpack based on mpackid
+   * @return
+   */
+  public Mpack getMpack(Long mpackId) {
+    if (mpackManager.getMpackMap().containsKey(mpackId)) {
+      return mpackManager.getMpackMap().get(mpackId);
+    }
+    else {
+      return null;","[{'comment': 'I think the if-else is not needed, simply `return mpackManager.getMpackMap().get(mpackId);` would return `null` anyway if the key is not present.', 'commenter': 'adoroszlai'}]"
252,ambari-server/src/main/java/org/apache/ambari/server/api/services/AmbariMetaInfo.java,"@@ -1655,4 +1656,25 @@ KerberosDescriptor readKerberosDescriptorFromFile(String fileLocation) throws Am
   public File getCommonWidgetsDescriptorFile() {
     return commonWidgetsDescriptorFile;
   }
+
+  /***
+   * Fetch all mpacks from mpackMap
+   * @return","[{'comment': ""Please don't add useless javadoc tags (`@param something`, `@return`, `@throws SomeException` without description).  The same information is visible in the method signature.  The doc should describe eg. the conditions when the exception is thrown, etc.  If it's obvious, then simply omit the javadoc tags."", 'commenter': 'adoroszlai'}]"
252,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/MpackResourceProvider.java,"@@ -108,7 +109,7 @@
     PROPERTY_IDS.add(MPACK_NAME);
     PROPERTY_IDS.add(MPACK_VERSION);
     PROPERTY_IDS.add(MPACK_URI);
-    PROPERTY_IDS.add(PACKLETS);
+    PROPERTY_IDS.add(MODULES);","[{'comment': ""Shouldn't `MPACK_DESC` be added to `PROPERTY_IDS` to be able to explicitly request this field?"", 'commenter': 'adoroszlai'}, {'comment': 'Thanks for catching this!', 'commenter': 'mradha25'}]"
252,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/MpackResourceProvider.java,"@@ -240,17 +242,18 @@ private String getMpackUri(MpackRequest mpackRequest) throws AmbariException {
     Long mpackId = null;
     if (predicate == null) {
       // Fetch all mpacks
-      List<MpackEntity> entities = mpackDAO.findAll();
-      if (null == entities) {
-        entities = Collections.emptyList();
+      Set<MpackResponse> responses = (HashSet)getManagementController().getMpacks();","[{'comment': 'Either change `AmbariManagementController#getMpacks()` to return `Set` instead of `Collection`, or declare `responses` as `Collection`.  Both are better than this unchecked cast.', 'commenter': 'adoroszlai'}, {'comment': 'Yes looks like i changed in AmbariManagementControllerImpl but not in AmbariManagementController. Thanks!', 'commenter': 'mradha25'}, {'comment': 'Resolved.', 'commenter': 'mradha25'}]"
252,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/StackDefinition.java,"@@ -1,3 +1,21 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0","[{'comment': 'Thanks for catching this.', 'commenter': 'adoroszlai'}]"
252,ambari-server/src/main/java/org/apache/ambari/server/mpack/MpackManager.java,"@@ -405,9 +420,16 @@ private Boolean createMpackDirectory(Mpack mpack, Path mpackTarPath)
   private void createSymLinks(Mpack mpack) throws IOException {
 
     String stackId = mpack.getStackId();
-    String[] stackMetaData = stackId.split(""-"");
-    String stackName = stackMetaData[0];
-    String stackVersion = stackMetaData[1];
+    String stackName = """";
+    String stackVersion = """";
+    if (stackId == null) {
+      stackName = mpack.getName();
+      stackVersion = mpack.getVersion();
+    } else {
+      String[] stackMetaData = stackId.split(""-"");
+      stackName = stackMetaData[0];
+      stackVersion = stackMetaData[1];","[{'comment': 'Why not use `StackId` object instead of manually splitting?  Eg.:\r\n\r\n```\r\nStackId id = new StackId(stackId);\r\nstackName  = id.getStackName();\r\nstackVersion = id.getStackVersion();\r\n```', 'commenter': 'adoroszlai'}]"
252,ambari-server/src/main/java/org/apache/ambari/server/mpack/MpackManager.java,"@@ -425,14 +447,15 @@ private void createSymLinks(Mpack mpack) throws IOException {
    * Download the mpack from the given uri
    *
    * @param mpackURI
+   * @param mpackDefintionLocation
    * @return
    */
-  public Path downloadMpack(String mpackURI) throws IOException {
+  public Path downloadMpack(String mpackURI, String mpackDefintionLocation) throws IOException {","[{'comment': 'Typo: `Defintion` -> `Definition`', 'commenter': 'adoroszlai'}]"
252,ambari-server/src/main/java/org/apache/ambari/server/mpack/MpackManager.java,"@@ -506,9 +529,16 @@ protected Long populateDB(Mpack mpack) throws IOException {
   protected void populateStackDB(Mpack mpack) throws IOException {
 
     String stackId = mpack.getStackId();
-    String[] stackMetaData = stackId.split(""-"");
-    String stackName = stackMetaData[0];
-    String stackVersion = stackMetaData[1];
+    String stackName = """";
+    String stackVersion = """";
+    if (stackId == null) {
+      stackName = mpack.getName();
+      stackVersion = mpack.getVersion();
+    } else {
+      String[] stackMetaData = stackId.split(""-"");
+      stackName = stackMetaData[0];
+      stackVersion = stackMetaData[1];
+    }","[{'comment': 'This logic is duplicated.  Can you please extract to a function in `Mpack` that returns a `StackId`?', 'commenter': 'adoroszlai'}]"
252,ambari-server/src/main/java/org/apache/ambari/server/mpack/MpackManager.java,"@@ -528,16 +558,16 @@ protected void populateStackDB(Mpack mpack) throws IOException {
   }
 
   /**
-   * Fetches the packlet info stored in the memory for mpacks/{mpack_id} call.
+   * Fetches the mpack info stored in the memory for mpacks/{mpack_id} call.
    *
    * @param mpackId
-   * @return list of {@link Packlet}
+   * @return list of {@link Module}
    */
-  public List<Packlet> getPacklets(Long mpackId) {
+  public List<Module> getModules(Long mpackId) {
 
     Mpack mpack = mpackMap.get(mpackId);
-    if (mpack.getPacklets() != null) {
-      return mpack.getPacklets();
+    if (mpack.getModules() != null) {
+      return mpack.getModules();
     }
     return null;","[{'comment': 'No need for the `if`.', 'commenter': 'adoroszlai'}]"
252,ambari-server/src/main/java/org/apache/ambari/server/state/Module.java,"@@ -0,0 +1,177 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.state;
+
+import java.util.List;
+
+import com.google.gson.annotations.SerializedName;
+
+public class Module {
+  public enum Category {
+    @SerializedName(""SERVER"")
+    SERVER,
+    @SerializedName(""CLIENT"")
+    CLIENT,
+    @SerializedName(""LIBRARY"")
+    LIBRARY
+  }
+
+  @SerializedName(""id"")
+  private String id;
+  @SerializedName(""displayName"")
+  private String displayName;
+  @SerializedName(""description"")
+  private String description;
+  @SerializedName(""category"")
+  private Category category;
+  @SerializedName(""name"")
+  private String name;
+  @SerializedName(""version"")
+  private String version;
+  @SerializedName(""definition"")
+  private String definition;
+  @SerializedName(""dependencies"")
+  private List<ModuleDependency> moduleDependencyList;
+  @SerializedName(""components"")
+  private List<ModuleComponent> moduleComponentList;
+
+  public Category getCategory() {
+    return category;
+  }
+
+  public void setType(Category category) {
+    this.category = category;
+  }
+
+  public String getName() {
+    return name;
+  }
+
+  public void setName(String name) {
+    this.name = name;
+  }
+
+  public String getVersion() {
+    return version;
+  }
+
+  public void setVersion(String version) {
+    this.version = version;
+  }
+
+  public String getDefinition() {
+    return definition;
+  }
+
+  public void setDefinition(String definition) {
+    this.definition = definition;
+  }
+
+  public String getId() {
+    return id;
+  }
+
+  public void setId(String id) {
+    this.id = id;
+  }
+
+  public String getDisplayName() {
+    return displayName;
+  }
+
+  public void setDisplayName(String displayName) {
+    this.displayName = displayName;
+  }
+
+  public String getDescription() {
+    return description;
+  }
+
+  public void setDescription(String description) {
+    this.description = description;
+  }
+
+  public void setCategory(Category category) {
+    this.category = category;
+  }
+
+  public List<ModuleDependency> getModuleDependencyList() {
+    return moduleDependencyList;
+  }
+
+  public void setModuleDependencyList(List<ModuleDependency> moduleDependencyList) {
+    this.moduleDependencyList = moduleDependencyList;
+  }
+
+  public List<ModuleComponent> getModuleComponentList() {
+    return moduleComponentList;
+  }
+
+  public void setModuleComponentList(List<ModuleComponent> moduleComponentList) {
+    this.moduleComponentList = moduleComponentList;
+  }
+
+
+  @Override
+  public boolean equals(Object o) {
+    if (this == o) return true;
+    if (o == null || getClass() != o.getClass()) return false;
+
+    Module module = (Module) o;
+
+    if (!id.equals(module.id)) return false;
+    if (!displayName.equals(module.displayName)) return false;
+    if (!description.equals(module.description)) return false;
+    if (category != module.category) return false;
+    if (!name.equals(module.name)) return false;
+    if (!version.equals(module.version)) return false;
+    if (!definition.equals(module.definition)) return false;
+    if (moduleDependencyList != null ? !moduleDependencyList.equals(module.moduleDependencyList) : module.moduleDependencyList != null)
+      return false;
+    return moduleComponentList.equals(module.moduleComponentList);","[{'comment': 'Some of the checks, eg. `!definition.equals(module.definition)` are prone to `NullPointerException`.\r\n\r\nAnd\r\n\r\n```\r\nObjects.equals(moduleDependencyList, module.moduleDependencyList)\r\n```\r\n\r\nis the same as\r\n\r\n```\r\nmoduleDependencyList != null ? moduleDependencyList.equals(module.moduleDependencyList) : module.moduleDependencyList == null\r\n```\r\n\r\nSo the second part of the `equals()` implementation can be simplified (for the attributes where `null` check is present) and made safer (for the attributes where `null` check is missing):\r\n\r\n```\r\nreturn \r\n  ...\r\n  Objects.equals(definition, module.definition) &&\r\n  Objects.equals(moduleDependencyList, module.moduleDependencyList) &&\r\n  ...;\r\n```', 'commenter': 'adoroszlai'}]"
252,ambari-server/src/main/java/org/apache/ambari/server/state/Module.java,"@@ -0,0 +1,177 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.state;
+
+import java.util.List;
+
+import com.google.gson.annotations.SerializedName;
+
+public class Module {
+  public enum Category {
+    @SerializedName(""SERVER"")
+    SERVER,
+    @SerializedName(""CLIENT"")
+    CLIENT,
+    @SerializedName(""LIBRARY"")
+    LIBRARY
+  }
+
+  @SerializedName(""id"")
+  private String id;
+  @SerializedName(""displayName"")
+  private String displayName;
+  @SerializedName(""description"")
+  private String description;
+  @SerializedName(""category"")
+  private Category category;
+  @SerializedName(""name"")
+  private String name;
+  @SerializedName(""version"")
+  private String version;
+  @SerializedName(""definition"")
+  private String definition;
+  @SerializedName(""dependencies"")
+  private List<ModuleDependency> moduleDependencyList;
+  @SerializedName(""components"")
+  private List<ModuleComponent> moduleComponentList;
+
+  public Category getCategory() {
+    return category;
+  }
+
+  public void setType(Category category) {
+    this.category = category;
+  }
+
+  public String getName() {
+    return name;
+  }
+
+  public void setName(String name) {
+    this.name = name;
+  }
+
+  public String getVersion() {
+    return version;
+  }
+
+  public void setVersion(String version) {
+    this.version = version;
+  }
+
+  public String getDefinition() {
+    return definition;
+  }
+
+  public void setDefinition(String definition) {
+    this.definition = definition;
+  }
+
+  public String getId() {
+    return id;
+  }
+
+  public void setId(String id) {
+    this.id = id;
+  }
+
+  public String getDisplayName() {
+    return displayName;
+  }
+
+  public void setDisplayName(String displayName) {
+    this.displayName = displayName;
+  }
+
+  public String getDescription() {
+    return description;
+  }
+
+  public void setDescription(String description) {
+    this.description = description;
+  }
+
+  public void setCategory(Category category) {
+    this.category = category;
+  }
+
+  public List<ModuleDependency> getModuleDependencyList() {
+    return moduleDependencyList;
+  }
+
+  public void setModuleDependencyList(List<ModuleDependency> moduleDependencyList) {
+    this.moduleDependencyList = moduleDependencyList;
+  }
+
+  public List<ModuleComponent> getModuleComponentList() {
+    return moduleComponentList;
+  }
+
+  public void setModuleComponentList(List<ModuleComponent> moduleComponentList) {
+    this.moduleComponentList = moduleComponentList;
+  }
+
+
+  @Override
+  public boolean equals(Object o) {
+    if (this == o) return true;
+    if (o == null || getClass() != o.getClass()) return false;
+
+    Module module = (Module) o;
+
+    if (!id.equals(module.id)) return false;
+    if (!displayName.equals(module.displayName)) return false;
+    if (!description.equals(module.description)) return false;
+    if (category != module.category) return false;
+    if (!name.equals(module.name)) return false;
+    if (!version.equals(module.version)) return false;
+    if (!definition.equals(module.definition)) return false;
+    if (moduleDependencyList != null ? !moduleDependencyList.equals(module.moduleDependencyList) : module.moduleDependencyList != null)
+      return false;
+    return moduleComponentList.equals(module.moduleComponentList);
+  }
+
+  @Override
+  public int hashCode() {
+    int result = id.hashCode();
+    result = 31 * result + displayName.hashCode();
+    result = 31 * result + description.hashCode();
+    result = 31 * result + category.hashCode();
+    result = 31 * result + name.hashCode();
+    result = 31 * result + version.hashCode();
+    result = 31 * result + definition.hashCode();
+    result = 31 * result + (moduleDependencyList != null ? moduleDependencyList.hashCode() : 0);
+    result = 31 * result + moduleComponentList.hashCode();","[{'comment': 'Can be simplified using `Objects#hash`:\r\n\r\n```\r\nreturn Objects.hash(..., definition, moduleDependencyList, ...);\r\n```', 'commenter': 'adoroszlai'}]"
252,ambari-server/src/main/java/org/apache/ambari/server/state/Mpack.java,"@@ -295,8 +248,23 @@ public void copyFrom(Mpack mpack) {
     if (this.prerequisites == null) {
       this.prerequisites = mpack.getPrerequisites();
     }
-    if (this.packlets == null) {
-      this.packlets = mpack.getPacklets();
+    if (this.modules == null) {
+      this.modules = mpack.getModules();
+    }
+    if (this.artifactsPath == null) {
+      this.artifactsPath = mpack.getArtifactsPath();
+    }
+    if (this.prerequisites == null) {
+      this.prerequisites = mpack.getPrerequisites();","[{'comment': '`prerequisites` is already copied a few lines above this.', 'commenter': 'adoroszlai'}]"
252,ambari-server/src/test/java/org/apache/ambari/server/state/MpackTest.java,"@@ -90,7 +90,7 @@ public void testMpacksUsingGson() {
     Assert.assertEquals(""3.0.0.0-111"", mpack.getVersion());
     Assert.assertEquals(""HDF 3.0.0 Ambari Management Pack"", mpack.getDescription());
     Assert.assertEquals(expectedPrereq, mpack.getPrerequisites());
-    Assert.assertEquals(expectedPacklets.toString(), mpack.getPacklets().toString());
+    Assert.assertEquals(expectedPacklets.toString(), mpack.getModules().toString());","[{'comment': '```\r\n[ERROR]   MpackTest.testMpacksUsingGson:93 NullPointer\r\n```\r\n\r\nI guess `packlets` should be changed to `modules` in `mpackJsonContents`.', 'commenter': 'adoroszlai'}, {'comment': 'Fixed.', 'commenter': 'mradha25'}]"
252,ambari-server/src/test/java/org/apache/ambari/server/controller/internal/MpackResourceProviderTest.java,"@@ -220,7 +222,8 @@ public void testCreateResources() throws Exception {
     replay(m_amc,request);
     // end expectations
 
-    MpackResourceProvider provider = (MpackResourceProvider) AbstractControllerResourceProvider.getResourceProvider(Resource.Type.Mpack, m_amc);
+    MpackResourceProvider provider = (MpackResourceProvider) AbstractControllerResourceProvider.getResourceProvider(","[{'comment': '```\r\n[ERROR]   MpackResourceProviderTest.testCreateResources:225  NullPointer\r\n```', 'commenter': 'adoroszlai'}]"
252,ambari-server/src/test/java/org/apache/ambari/server/controller/internal/MpackResourceProviderTest.java,"@@ -112,7 +111,9 @@ public void testGetResourcesMpacks() throws Exception {
     // replay
     replay(m_dao);
 
-    ResourceProvider provider = AbstractControllerResourceProvider.getResourceProvider(type, m_amc);
+    ResourceProvider provider = AbstractControllerResourceProvider.getResourceProvider(
+            type","[{'comment': '```\r\n[ERROR]   MpackResourceProviderTest.testGetResourcesMpacks:114  NullPointer\r\n```', 'commenter': 'adoroszlai'}, {'comment': 'Hi,\r\nSo far I am not able to repro this error. I get \r\nCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.metrics2.sink.timeline.TimelineMetric\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\r\nI will look into this test class in my subsequent patch. Thanks', 'commenter': 'mradha25'}]"
252,ambari-server/src/test/java/org/apache/ambari/server/controller/internal/MpackResourceProviderTest.java,"@@ -158,30 +159,31 @@ public void testGetResourcesMpackId() throws Exception {
     entity.setMpackVersion(""3.0"");
 
 
-    ArrayList<Packlet> packletArrayList = new ArrayList<>();
-    Packlet packlet = new Packlet();
-    packlet.setName(""testService"");
-    packlet.setType(Packlet.PackletType.SERVICE_PACKLET);
-    packlet.setSourceLocation(""testDir/testDir"");
-    packlet.setVersion(""3.0"");
-    packletArrayList.add(packlet);
+    ArrayList<Module> packletArrayList = new ArrayList<>();
+    Module module = new Module();
+    module.setName(""testService"");
+    //module.setType(Module.PackletType.SERVICE_PACKLET);
+    module.setDefinition(""testDir"");
+    module.setVersion(""3.0"");
+    packletArrayList.add(module);
 
     Resource resourceExpected1 = new ResourceImpl(Resource.Type.Mpack);
     resourceExpected1.setProperty(MpackResourceProvider.MPACK_ID, (long)1);
     resourceExpected1.setProperty(MpackResourceProvider.MPACK_NAME, ""TestMpack1"");
     resourceExpected1.setProperty(MpackResourceProvider.MPACK_VERSION, ""3.0"");
     resourceExpected1.setProperty(MpackResourceProvider.MPACK_URI, ""abcd.tar.gz"");
     resourceExpected1.setProperty(MpackResourceProvider.REGISTRY_ID, null);
-    resourceExpected1.setProperty(MpackResourceProvider.PACKLETS,packletArrayList);
+    resourceExpected1.setProperty(MpackResourceProvider.MODULES,packletArrayList);
 
     // set expectations
     EasyMock.expect(m_dao.findById((long)1)).andReturn(entity).anyTimes();
-    EasyMock.expect(m_amc.getPacklets((long)1)).andReturn(packletArrayList).anyTimes();
+    EasyMock.expect(m_amc.getModules((long)1)).andReturn(packletArrayList).anyTimes();
 
     // replay
     replay(m_dao,m_amc);
 
-    ResourceProvider provider = AbstractControllerResourceProvider.getResourceProvider(type, m_amc);
+    ResourceProvider provider = AbstractControllerResourceProvider.getResourceProvider(","[{'comment': '```\r\n[ERROR]   MpackResourceProviderTest.testGetResourcesMpackId:185  NullPointer\r\n```', 'commenter': 'adoroszlai'}]"
252,ambari-server/src/main/java/org/apache/ambari/server/api/services/AmbariMetaInfo.java,"@@ -1655,4 +1656,20 @@ KerberosDescriptor readKerberosDescriptorFromFile(String fileLocation) throws Am
   public File getCommonWidgetsDescriptorFile() {
     return commonWidgetsDescriptorFile;
   }
+
+  /***
+   * Fetch all mpacks from mpackMap
+   * @return all mpacks from mpackMap - in memory data structure
+   */
+  public Collection<Mpack> getMpacks() {
+    return mpackManager.getMpackMap().values();
+  }
+
+  /***
+   * Fetch a particular mpack based on mpackid
+   * @return a single mpack
+   */
+  public Mpack getMpack(Long mpackId) {
+    return mpackManager.getMpackMap().get(mpackId);","[{'comment': 'Add null check', 'commenter': 'jayush'}]"
252,ambari-server/src/main/java/org/apache/ambari/server/api/services/AmbariMetaInfo.java,"@@ -1655,4 +1656,20 @@ KerberosDescriptor readKerberosDescriptorFromFile(String fileLocation) throws Am
   public File getCommonWidgetsDescriptorFile() {
     return commonWidgetsDescriptorFile;
   }
+
+  /***
+   * Fetch all mpacks from mpackMap
+   * @return all mpacks from mpackMap - in memory data structure
+   */
+  public Collection<Mpack> getMpacks() {
+    return mpackManager.getMpackMap().values();","[{'comment': 'Add null check.', 'commenter': 'jayush'}]"
252,ambari-server/src/main/java/org/apache/ambari/server/state/Module.java,"@@ -0,0 +1,163 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.state;
+
+import java.util.List;
+import java.util.Objects;
+
+import com.google.gson.annotations.SerializedName;
+
+public class Module {
+  public enum Category {
+    @SerializedName(""SERVER"")
+    SERVER,
+    @SerializedName(""CLIENT"")
+    CLIENT,
+    @SerializedName(""LIBRARY"")
+    LIBRARY
+  }
+
+  @SerializedName(""id"")
+  private String id;
+  @SerializedName(""displayName"")
+  private String displayName;
+  @SerializedName(""description"")
+  private String description;
+  @SerializedName(""category"")
+  private Category category;
+  @SerializedName(""name"")
+  private String name;
+  @SerializedName(""version"")
+  private String version;
+  @SerializedName(""definition"")
+  private String definition;
+  @SerializedName(""dependencies"")
+  private List<ModuleDependency> moduleDependencyList;
+  @SerializedName(""components"")
+  private List<ModuleComponent> moduleComponentList;
+
+  public Category getCategory() {
+    return category;
+  }
+
+  public void setType(Category category) {
+    this.category = category;
+  }
+
+  public String getName() {
+    return name;
+  }
+
+  public void setName(String name) {
+    this.name = name;
+  }
+
+  public String getVersion() {
+    return version;
+  }
+
+  public void setVersion(String version) {
+    this.version = version;
+  }
+
+  public String getDefinition() {
+    return definition;
+  }
+
+  public void setDefinition(String definition) {
+    this.definition = definition;
+  }
+
+  public String getId() {
+    return id;
+  }
+
+  public void setId(String id) {
+    this.id = id;
+  }
+
+  public String getDisplayName() {
+    return displayName;
+  }
+
+  public void setDisplayName(String displayName) {
+    this.displayName = displayName;
+  }
+
+  public String getDescription() {
+    return description;
+  }
+
+  public void setDescription(String description) {
+    this.description = description;
+  }
+
+  public void setCategory(Category category) {
+    this.category = category;
+  }
+
+  public List<ModuleDependency> getModuleDependencyList() {
+    return moduleDependencyList;
+  }
+
+  public void setModuleDependencyList(List<ModuleDependency> moduleDependencyList) {
+    this.moduleDependencyList = moduleDependencyList;
+  }
+
+  public List<ModuleComponent> getModuleComponentList() {
+    return moduleComponentList;
+  }
+
+  public void setModuleComponentList(List<ModuleComponent> moduleComponentList) {
+    this.moduleComponentList = moduleComponentList;
+  }
+
+
+  @Override
+  public boolean equals(Object o) {
+    if (this == o) return true;
+    if (o == null || getClass() != o.getClass()) return false;
+
+    Module module = (Module) o;
+
+    return Objects.equals(id, module.id) && Objects.equals(displayName, module.displayName) &&
+            Objects.equals(description, module.description) && Objects.equals(category, module.category) &&
+            Objects.equals(name, module.name) && Objects.equals(version, module.version) && Objects.equals(definition, module.definition)
+            && Objects.equals(moduleDependencyList, module.moduleDependencyList) && Objects.equals(moduleComponentList, module.moduleComponentList);
+  }
+
+  @Override
+  public int hashCode() {
+    return Objects.hash(id, displayName, description, category, name, version, definition, moduleComponentList, moduleComponentList);
+  }
+
+  @Override
+  public String toString() {
+    return ""Module{"" +
+            ""id='"" + id + '\'' +
+            "", displayName='"" + displayName + '\'' +
+            "", description='"" + description + '\'' +
+            "", category="" + category +
+            "", name='"" + name + '\'' +
+            "", version='"" + version + '\'' +
+            "", definition='"" + definition + '\'' +
+            "", moduleDependencyList="" + moduleDependencyList +","[{'comment': 'rename moduleDependencyList to dependencies', 'commenter': 'jayush'}]"
252,ambari-server/src/main/java/org/apache/ambari/server/state/Module.java,"@@ -0,0 +1,163 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.state;
+
+import java.util.List;
+import java.util.Objects;
+
+import com.google.gson.annotations.SerializedName;
+
+public class Module {
+  public enum Category {
+    @SerializedName(""SERVER"")
+    SERVER,
+    @SerializedName(""CLIENT"")
+    CLIENT,
+    @SerializedName(""LIBRARY"")
+    LIBRARY
+  }
+
+  @SerializedName(""id"")
+  private String id;
+  @SerializedName(""displayName"")
+  private String displayName;
+  @SerializedName(""description"")
+  private String description;
+  @SerializedName(""category"")
+  private Category category;
+  @SerializedName(""name"")
+  private String name;
+  @SerializedName(""version"")
+  private String version;
+  @SerializedName(""definition"")
+  private String definition;
+  @SerializedName(""dependencies"")
+  private List<ModuleDependency> moduleDependencyList;
+  @SerializedName(""components"")
+  private List<ModuleComponent> moduleComponentList;
+
+  public Category getCategory() {
+    return category;
+  }
+
+  public void setType(Category category) {
+    this.category = category;
+  }
+
+  public String getName() {
+    return name;
+  }
+
+  public void setName(String name) {
+    this.name = name;
+  }
+
+  public String getVersion() {
+    return version;
+  }
+
+  public void setVersion(String version) {
+    this.version = version;
+  }
+
+  public String getDefinition() {
+    return definition;
+  }
+
+  public void setDefinition(String definition) {
+    this.definition = definition;
+  }
+
+  public String getId() {
+    return id;
+  }
+
+  public void setId(String id) {
+    this.id = id;
+  }
+
+  public String getDisplayName() {
+    return displayName;
+  }
+
+  public void setDisplayName(String displayName) {
+    this.displayName = displayName;
+  }
+
+  public String getDescription() {
+    return description;
+  }
+
+  public void setDescription(String description) {
+    this.description = description;
+  }
+
+  public void setCategory(Category category) {
+    this.category = category;
+  }
+
+  public List<ModuleDependency> getModuleDependencyList() {
+    return moduleDependencyList;
+  }
+
+  public void setModuleDependencyList(List<ModuleDependency> moduleDependencyList) {
+    this.moduleDependencyList = moduleDependencyList;
+  }
+
+  public List<ModuleComponent> getModuleComponentList() {
+    return moduleComponentList;
+  }
+
+  public void setModuleComponentList(List<ModuleComponent> moduleComponentList) {
+    this.moduleComponentList = moduleComponentList;
+  }
+
+
+  @Override
+  public boolean equals(Object o) {
+    if (this == o) return true;
+    if (o == null || getClass() != o.getClass()) return false;
+
+    Module module = (Module) o;
+
+    return Objects.equals(id, module.id) && Objects.equals(displayName, module.displayName) &&
+            Objects.equals(description, module.description) && Objects.equals(category, module.category) &&
+            Objects.equals(name, module.name) && Objects.equals(version, module.version) && Objects.equals(definition, module.definition)
+            && Objects.equals(moduleDependencyList, module.moduleDependencyList) && Objects.equals(moduleComponentList, module.moduleComponentList);
+  }
+
+  @Override
+  public int hashCode() {
+    return Objects.hash(id, displayName, description, category, name, version, definition, moduleComponentList, moduleComponentList);
+  }
+
+  @Override
+  public String toString() {
+    return ""Module{"" +
+            ""id='"" + id + '\'' +
+            "", displayName='"" + displayName + '\'' +
+            "", description='"" + description + '\'' +
+            "", category="" + category +
+            "", name='"" + name + '\'' +
+            "", version='"" + version + '\'' +
+            "", definition='"" + definition + '\'' +
+            "", moduleDependencyList="" + moduleDependencyList +
+            "", moduleComponentList="" + moduleComponentList +","[{'comment': 'Rename moduleComponentList to components', 'commenter': 'jayush'}]"
252,ambari-server/src/main/java/org/apache/ambari/server/state/Module.java,"@@ -0,0 +1,163 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.state;
+
+import java.util.List;
+import java.util.Objects;
+
+import com.google.gson.annotations.SerializedName;
+
+public class Module {
+  public enum Category {
+    @SerializedName(""SERVER"")
+    SERVER,
+    @SerializedName(""CLIENT"")
+    CLIENT,
+    @SerializedName(""LIBRARY"")
+    LIBRARY
+  }
+
+  @SerializedName(""id"")
+  private String id;
+  @SerializedName(""displayName"")
+  private String displayName;
+  @SerializedName(""description"")
+  private String description;
+  @SerializedName(""category"")
+  private Category category;
+  @SerializedName(""name"")
+  private String name;
+  @SerializedName(""version"")
+  private String version;
+  @SerializedName(""definition"")
+  private String definition;
+  @SerializedName(""dependencies"")
+  private List<ModuleDependency> moduleDependencyList;","[{'comment': 'Rename moduleDependencyList to dependencies', 'commenter': 'jayush'}]"
252,ambari-server/src/main/java/org/apache/ambari/server/state/Module.java,"@@ -0,0 +1,163 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.state;
+
+import java.util.List;
+import java.util.Objects;
+
+import com.google.gson.annotations.SerializedName;
+
+public class Module {
+  public enum Category {
+    @SerializedName(""SERVER"")
+    SERVER,
+    @SerializedName(""CLIENT"")
+    CLIENT,
+    @SerializedName(""LIBRARY"")
+    LIBRARY
+  }
+
+  @SerializedName(""id"")
+  private String id;
+  @SerializedName(""displayName"")
+  private String displayName;
+  @SerializedName(""description"")
+  private String description;
+  @SerializedName(""category"")
+  private Category category;
+  @SerializedName(""name"")
+  private String name;
+  @SerializedName(""version"")
+  private String version;
+  @SerializedName(""definition"")
+  private String definition;
+  @SerializedName(""dependencies"")
+  private List<ModuleDependency> moduleDependencyList;
+  @SerializedName(""components"")
+  private List<ModuleComponent> moduleComponentList;","[{'comment': 'Rename moduleComponentList to components', 'commenter': 'jayush'}]"
252,ambari-server/src/main/java/org/apache/ambari/server/state/Module.java,"@@ -0,0 +1,163 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.state;
+
+import java.util.List;
+import java.util.Objects;
+
+import com.google.gson.annotations.SerializedName;
+
+public class Module {
+  public enum Category {
+    @SerializedName(""SERVER"")
+    SERVER,
+    @SerializedName(""CLIENT"")
+    CLIENT,
+    @SerializedName(""LIBRARY"")
+    LIBRARY
+  }
+
+  @SerializedName(""id"")
+  private String id;
+  @SerializedName(""displayName"")
+  private String displayName;
+  @SerializedName(""description"")
+  private String description;
+  @SerializedName(""category"")
+  private Category category;
+  @SerializedName(""name"")
+  private String name;
+  @SerializedName(""version"")
+  private String version;
+  @SerializedName(""definition"")
+  private String definition;
+  @SerializedName(""dependencies"")
+  private List<ModuleDependency> moduleDependencyList;
+  @SerializedName(""components"")
+  private List<ModuleComponent> moduleComponentList;
+
+  public Category getCategory() {
+    return category;
+  }
+
+  public void setType(Category category) {
+    this.category = category;
+  }
+
+  public String getName() {
+    return name;
+  }
+
+  public void setName(String name) {
+    this.name = name;
+  }
+
+  public String getVersion() {
+    return version;
+  }
+
+  public void setVersion(String version) {
+    this.version = version;
+  }
+
+  public String getDefinition() {
+    return definition;
+  }
+
+  public void setDefinition(String definition) {
+    this.definition = definition;
+  }
+
+  public String getId() {
+    return id;
+  }
+
+  public void setId(String id) {
+    this.id = id;
+  }
+
+  public String getDisplayName() {
+    return displayName;
+  }
+
+  public void setDisplayName(String displayName) {
+    this.displayName = displayName;
+  }
+
+  public String getDescription() {
+    return description;
+  }
+
+  public void setDescription(String description) {
+    this.description = description;
+  }
+
+  public void setCategory(Category category) {
+    this.category = category;
+  }
+
+  public List<ModuleDependency> getModuleDependencyList() {","[{'comment': 'Rename getModuleDependencyList() to getDependencies()', 'commenter': 'jayush'}]"
252,ambari-server/src/main/java/org/apache/ambari/server/state/Module.java,"@@ -0,0 +1,163 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.state;
+
+import java.util.List;
+import java.util.Objects;
+
+import com.google.gson.annotations.SerializedName;
+
+public class Module {
+  public enum Category {
+    @SerializedName(""SERVER"")
+    SERVER,
+    @SerializedName(""CLIENT"")
+    CLIENT,
+    @SerializedName(""LIBRARY"")
+    LIBRARY
+  }
+
+  @SerializedName(""id"")
+  private String id;
+  @SerializedName(""displayName"")
+  private String displayName;
+  @SerializedName(""description"")
+  private String description;
+  @SerializedName(""category"")
+  private Category category;
+  @SerializedName(""name"")
+  private String name;
+  @SerializedName(""version"")
+  private String version;
+  @SerializedName(""definition"")
+  private String definition;
+  @SerializedName(""dependencies"")
+  private List<ModuleDependency> moduleDependencyList;
+  @SerializedName(""components"")
+  private List<ModuleComponent> moduleComponentList;
+
+  public Category getCategory() {
+    return category;
+  }
+
+  public void setType(Category category) {
+    this.category = category;
+  }
+
+  public String getName() {
+    return name;
+  }
+
+  public void setName(String name) {
+    this.name = name;
+  }
+
+  public String getVersion() {
+    return version;
+  }
+
+  public void setVersion(String version) {
+    this.version = version;
+  }
+
+  public String getDefinition() {
+    return definition;
+  }
+
+  public void setDefinition(String definition) {
+    this.definition = definition;
+  }
+
+  public String getId() {
+    return id;
+  }
+
+  public void setId(String id) {
+    this.id = id;
+  }
+
+  public String getDisplayName() {
+    return displayName;
+  }
+
+  public void setDisplayName(String displayName) {
+    this.displayName = displayName;
+  }
+
+  public String getDescription() {
+    return description;
+  }
+
+  public void setDescription(String description) {
+    this.description = description;
+  }
+
+  public void setCategory(Category category) {
+    this.category = category;
+  }
+
+  public List<ModuleDependency> getModuleDependencyList() {
+    return moduleDependencyList;","[{'comment': 'Rename moduleDependencyList to dependencies', 'commenter': 'jayush'}]"
252,ambari-server/src/main/java/org/apache/ambari/server/state/Module.java,"@@ -0,0 +1,163 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.state;
+
+import java.util.List;
+import java.util.Objects;
+
+import com.google.gson.annotations.SerializedName;
+
+public class Module {
+  public enum Category {
+    @SerializedName(""SERVER"")
+    SERVER,
+    @SerializedName(""CLIENT"")
+    CLIENT,
+    @SerializedName(""LIBRARY"")
+    LIBRARY
+  }
+
+  @SerializedName(""id"")
+  private String id;
+  @SerializedName(""displayName"")
+  private String displayName;
+  @SerializedName(""description"")
+  private String description;
+  @SerializedName(""category"")
+  private Category category;
+  @SerializedName(""name"")
+  private String name;
+  @SerializedName(""version"")
+  private String version;
+  @SerializedName(""definition"")
+  private String definition;
+  @SerializedName(""dependencies"")
+  private List<ModuleDependency> moduleDependencyList;
+  @SerializedName(""components"")
+  private List<ModuleComponent> moduleComponentList;
+
+  public Category getCategory() {
+    return category;
+  }
+
+  public void setType(Category category) {
+    this.category = category;
+  }
+
+  public String getName() {
+    return name;
+  }
+
+  public void setName(String name) {
+    this.name = name;
+  }
+
+  public String getVersion() {
+    return version;
+  }
+
+  public void setVersion(String version) {
+    this.version = version;
+  }
+
+  public String getDefinition() {
+    return definition;
+  }
+
+  public void setDefinition(String definition) {
+    this.definition = definition;
+  }
+
+  public String getId() {
+    return id;
+  }
+
+  public void setId(String id) {
+    this.id = id;
+  }
+
+  public String getDisplayName() {
+    return displayName;
+  }
+
+  public void setDisplayName(String displayName) {
+    this.displayName = displayName;
+  }
+
+  public String getDescription() {
+    return description;
+  }
+
+  public void setDescription(String description) {
+    this.description = description;
+  }
+
+  public void setCategory(Category category) {
+    this.category = category;
+  }
+
+  public List<ModuleDependency> getModuleDependencyList() {
+    return moduleDependencyList;
+  }
+
+  public void setModuleDependencyList(List<ModuleDependency> moduleDependencyList) {","[{'comment': 'Rename setModuleDependencyList to setDependencies', 'commenter': 'jayush'}]"
255,AMBARI-22894-trunk.patch,"@@ -0,0 +1,13 @@
+diff --git a/ambari-server/src/main/resources/stack-hooks/before-START/scripts/params.py b/ambari-server/src/main/resources/stack-hooks/before-START/scripts/params.py","[{'comment': 'Please do not add this patch file.  (You can remove it in an another commit, no need for a new PR.)', 'commenter': 'adoroszlai'}]"
255,ambari-server/src/main/resources/stack-hooks/before-START/scripts/ AMBARI-22894-trunk.patch,"@@ -0,0 +1,7 @@
+ ambari_libs_dir = ""/var/lib/ambari-agent/lib""
+ is_webhdfs_enabled = config['configurations']['hdfs-site']['dfs.webhdfs.enabled']
+-default_fs = config['configurations']['core-site']['fs.defaultFS']
++default_fs = default('/configurations/core-site/fs.defaultFS',None)","[{'comment': 'Please do not add this patch file either.', 'commenter': 'adoroszlai'}]"
255,ambari-server/src/main/resources/stack-hooks/before-START/scripts/params.py,"@@ -269,7 +269,7 @@
   
 ambari_libs_dir = ""/var/lib/ambari-agent/lib""
 is_webhdfs_enabled = config['configurations']['hdfs-site']['dfs.webhdfs.enabled']
-default_fs = config['configurations']['core-site']['fs.defaultFS']
+default_fs = default('/configurations/core-site/fs.defaultFS',None)","[{'comment': 'Can you please explain the need for this change? Please include steps to reproduce the problem it is trying to solve.', 'commenter': 'adoroszlai'}, {'comment': 'Step as follows:\r\nwhen deploy a cluster with only \'zookeeper\'zookeeper started failed\r\n\r\nTraceback (most recent call last):\r\nFile ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-START/scripts/hook.py"", line 40, in <module>\r\nBeforeStartHook().execute()\r\nFile ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 314, in execute\r\nmethod(env)\r\nFile ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-START/scripts/hook.py"", line 28, in hook\r\nimport params\r\nFile ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-START/scripts/params.py"", line 320, in <module>\r\nif namenode_rpc:\r\nFile ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/config_dictionary.py"", line 73, in _getattr_\r\nraise Fail(""Configuration parameter \'"" + self.name + ""\' was not found in configurations dictionary!"")\r\nresource_management.core.exceptions.Fail: Configuration parameter \'core-site\' was not found in configurations dictionary!\r\nError: Error: Unable to run the custom hook script [\'/usr/bin/python\', \'/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-START/scripts/hook.py\', \'START\', \'/var/lib/ambari-agent/data/command-31.json\', \'/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-START\', \'/var/lib/ambari-agent/data/structured-out-31.json\', \'INFO\', \'/var/lib/ambari-agent/tmp\', \'PROTOCOL_TLSv1\']', 'commenter': 'AliceXiaoLu'}, {'comment': 'Thanks, the traceback helps a lot.  This problem is already fixed by 108ad9b4a0d and c35a834d7bf:\r\n\r\nhttps://github.com/apache/ambari/blob/79504f74e83164a9c70713ea54cfc2f257d5be04/ambari-server/src/main/resources/stack-hooks/before-START/scripts/params.py#L336-L337\r\n\r\nand\r\n\r\nhttps://github.com/apache/ambari/blob/2019a5c0d824c73167535ac7091fc1af765c4826/ambari-server/src/main/resources/stacks/HDP/2.0.6/hooks/before-START/scripts/params.py#L330-L331', 'commenter': 'adoroszlai'}]"
262,ambari-server/src/main/python/ambari-server.py,"@@ -555,6 +555,8 @@ def init_ldap_setup_parser_options(parser):
   parser.add_option('--ldap-sync-username-collisions-behavior', default=None, help=""Handling behavior for username collisions [convert/skip] for LDAP sync"", dest=""ldap_sync_username_collisions_behavior"")
   parser.add_option('--ldap-force-lowercase-usernames', default=None, help=""Declares whether to force the ldap user name to be lowercase or leave as-is"", dest=""ldap_force_lowercase_usernames"")
   parser.add_option('--ldap-pagination-enabled', default=None, help=""Determines whether results from LDAP are paginated when requested"", dest=""ldap_pagination_enabled"")
+  parser.add_option('--ldap-setup-admin-name', default=None, help=""Ambari username for LDAP setup"", dest=""ldap_setup_admin_name"")","[{'comment': ""Trying to be more generic about this, maybe rename to 'ambari-admin-username'.\r\n\r\nThis may come in handy in the future when other sets of properties are moved to the Ambari DB."", 'commenter': 'rlevas'}, {'comment': ""I'm OK with this change...but this generates another question: shall we then remove/rename the one we already have for sync-ldap?"", 'commenter': 'smolnar82'}, {'comment': 'If there is a command line option to set the Ambari username and password when syncing ldap, lets leave that alone for now. In the future we will need to converge on the more generic approach and allow those properties to be _global_ as opposed to operation-specific. ', 'commenter': 'rlevas'}, {'comment': 'ok', 'commenter': 'smolnar82'}]"
262,ambari-server/src/main/python/ambari-server.py,"@@ -555,6 +555,8 @@ def init_ldap_setup_parser_options(parser):
   parser.add_option('--ldap-sync-username-collisions-behavior', default=None, help=""Handling behavior for username collisions [convert/skip] for LDAP sync"", dest=""ldap_sync_username_collisions_behavior"")
   parser.add_option('--ldap-force-lowercase-usernames', default=None, help=""Declares whether to force the ldap user name to be lowercase or leave as-is"", dest=""ldap_force_lowercase_usernames"")
   parser.add_option('--ldap-pagination-enabled', default=None, help=""Determines whether results from LDAP are paginated when requested"", dest=""ldap_pagination_enabled"")
+  parser.add_option('--ldap-setup-admin-name', default=None, help=""Ambari username for LDAP setup"", dest=""ldap_setup_admin_name"")
+  parser.add_option('--ldap-setup-admin-password', default=None, help=""Ambari password for LDAP setup"", dest=""ldap_setup_admin_password"")","[{'comment': ""Trying to be more generic about this, maybe rename to 'ambari-admin-password'.\r\n\r\nThis may come in handy in the future when other sets of properties are moved to the Ambari DB."", 'commenter': 'rlevas'}]"
269,ambari-server/src/main/java/org/apache/ambari/server/stack/HostsType.java,"@@ -47,15 +49,118 @@
    * That is to say, a downgrade only occurs where the current version is not
    * the target version.
    */
-  public LinkedHashSet<String> hosts = new LinkedHashSet<>();
+  private LinkedHashSet<String> hosts;
 
   /**
-   * Unhealthy hosts are those which are explicitely put into maintenance mode.
+   * Unhealthy hosts are those which are explicitly put into maintenance mode.
    * If there is a host which is not heartbeating (or is generally unhealthy)
    * but not in maintenance mode, then the prerequisite upgrade checks will let
    * the administrator know that it must be put into maintenance mode before an
    * upgrade can begin.
    */
   public List<ServiceComponentHost> unhealthy = new ArrayList<>();
 
+  public boolean hasMasters() {","[{'comment': 'javadoc for these methods would be helpful', 'commenter': 'ncole'}]"
269,ambari-server/src/main/java/org/apache/ambari/server/stack/HostsType.java,"@@ -47,15 +49,118 @@
    * That is to say, a downgrade only occurs where the current version is not
    * the target version.
    */
-  public LinkedHashSet<String> hosts = new LinkedHashSet<>();
+  private LinkedHashSet<String> hosts;
 
   /**
-   * Unhealthy hosts are those which are explicitely put into maintenance mode.
+   * Unhealthy hosts are those which are explicitly put into maintenance mode.
    * If there is a host which is not heartbeating (or is generally unhealthy)
    * but not in maintenance mode, then the prerequisite upgrade checks will let
    * the administrator know that it must be put into maintenance mode before an
    * upgrade can begin.
    */
   public List<ServiceComponentHost> unhealthy = new ArrayList<>();
 
+  public boolean hasMasters() {
+    return !getMasters().isEmpty();
+  }
+
+  public List<HaHosts> getHaHosts() {
+    return haHosts;
+  }
+
+  /**
+   * Order the hosts so that for each HA host the secondaries come first.
+   * For example: [sec1, sec2, master1, sec3, sec4, master2]
+   */
+  public void arrangeHostSecondariesFirst() {
+    this.hosts = getHaHosts().stream()
+      .flatMap(each -> Stream.concat(each.getSecondaries().stream(), Stream.of(each.getMaster())))
+      .collect(toCollection(LinkedHashSet::new));
+  }
+
+  public boolean hasMastersAndSecondaries() {
+    return !getMasters().isEmpty() && !getSecondaries().isEmpty();
+  }
+
+  /**
+   * A master and secondary host(s). In HA mode there is one master and one secondary host,
+   * in federated mode there can be more than one secondaries.
+   */
+  public static class HaHosts {
+    private final String master;
+    private final List<String> secondaries;
+
+    public HaHosts(String master, List<String> secondaries) {
+      if (master == null) {
+        throw new IllegalArgumentException(""Master host is missing"");
+      }
+      this.master = master;
+      this.secondaries = secondaries;
+    }
+
+    public String getMaster() {
+      return master;
+    }
+
+    public List<String> getSecondaries() {
+      return secondaries;
+    }
+  }
+
+  public static HostsType from(String master, String secondary, LinkedHashSet<String> hosts) {","[{'comment': 'javadoc on these statics would be helpful', 'commenter': 'ncole'}]"
269,ambari-server/src/main/java/org/apache/ambari/server/stack/MasterHostResolver.java,"@@ -268,94 +258,79 @@ public boolean isNameNodeHA() throws AmbariException {
   }
 
   /**
-   * Get mapping of the HDFS Namenodes from the state (""active"" or ""standby"") to the hostname.
-   * @return Returns a map from the state (""active"" or ""standby"" to the hostname with that state if exactly
-   * one active and one standby host were found, otherwise, return null.
-   * The hostnames are returned in lowercase.
+   * Get the NameNode NameSpaces (master->secondaries hosts).
+   * In each NameSpace there should be exactly 1 master and at least one secondary host.
    */
-  private Map<Status, String> getNameNodePair(Set<String> componentHosts) throws AmbariException {
-    Map<Status, String> stateToHost = new HashMap<>();
-    Cluster cluster = getCluster();
+  private List<HostsType.HaHosts> nameSpaces(Set<String> componentHosts) {
+    return NameService.fromConfig(m_configHelper, getCluster()).stream()
+      .map(each -> findMasterAndSecondaries(each, componentHosts))
+      .collect(Collectors.toList());
 
-    String nameService = m_configHelper.getValueFromDesiredConfigurations(cluster, ConfigHelper.HDFS_SITE, ""dfs.internal.nameservices"");
-    if (nameService == null || nameService.isEmpty()) {
-      return null;
-    }
+  }
 
-    String nnUniqueIDstring = m_configHelper.getValueFromDesiredConfigurations(cluster, ConfigHelper.HDFS_SITE, ""dfs.ha.namenodes."" + nameService);
-    if (nnUniqueIDstring == null || nnUniqueIDstring.isEmpty()) {
-      return null;
+  /**
+   * Find the master and secondary namenode(s) based on JMX NameNodeStatus.
+   */
+  private HostsType.HaHosts findMasterAndSecondaries(NameService nameService, Set<String> componentHosts) throws ClassifyNameNodeException {
+    String master = null;
+    List<String> secondaries = new ArrayList<>();
+    for (NameService.NameNode nameNode : nameService.getNameNodes()) {
+      checkForDualNetworkCards(componentHosts, nameNode);
+      String state = queryJmxBeanValue(nameNode.getHost(), nameNode.getPort(), ""Hadoop:service=NameNode,name=NameNodeStatus"", ""State"", true, nameNode.isEncrypted());
+      if (Status.ACTIVE.toString().equalsIgnoreCase(state)) {
+        master = nameNode.getHost();
+      } else if (Status.STANDBY.toString().equalsIgnoreCase(state)) {
+        secondaries.add(nameNode.getHost());
+      } else {
+        LOG.error(String.format(""Could not retrieve state for NameNode %s from property %s by querying JMX."", nameNode.getHost(), nameNode.getPropertyName()));
+      }
     }
-
-    String[] nnUniqueIDs = nnUniqueIDstring.split("","");
-    if (nnUniqueIDs == null || nnUniqueIDs.length != 2) {
-      return null;
+    if (erverythingIsClassified(componentHosts, master, secondaries)) {
+      return new HostsType.HaHosts(master, secondaries);
     }
+    throw new ClassifyNameNodeException(nameService);
+  }
 
-    String policy = m_configHelper.getValueFromDesiredConfigurations(cluster, ConfigHelper.HDFS_SITE, ""dfs.http.policy"");
-    boolean encrypted = (policy != null && policy.equalsIgnoreCase(ConfigHelper.HTTPS_ONLY));
-
-    String namenodeFragment = ""dfs.namenode."" + (encrypted ? ""https-address"" : ""http-address"") + "".{0}.{1}"";
-
-    for (String nnUniqueID : nnUniqueIDs) {
-      String key = MessageFormat.format(namenodeFragment, nameService, nnUniqueID);
-      String value = m_configHelper.getValueFromDesiredConfigurations(cluster, ConfigHelper.HDFS_SITE, key);
-
-      try {
-        HostAndPort hp = HTTPUtils.getHostAndPortFromProperty(value);
-        if (hp == null) {
-          throw new MalformedURLException(""Could not parse host and port from "" + value);
-        }
-
-        if (!componentHosts.contains(hp.host)){
-          //This may happen when NN HA is configured on dual network card machines with public/private FQDNs.
-          LOG.error(
-              MessageFormat.format(
-                  ""Hadoop NameNode HA configuration {0} contains host {1} that does not exist in the NameNode hosts list {3}"",
-                  key, hp.host, componentHosts.toString()));
-        }
-        String state = queryJmxBeanValue(hp.host, hp.port, ""Hadoop:service=NameNode,name=NameNodeStatus"", ""State"", true, encrypted);
-
-        if (null != state && (state.equalsIgnoreCase(Status.ACTIVE.toString()) || state.equalsIgnoreCase(Status.STANDBY.toString()))) {
-          Status status = Status.valueOf(state.toUpperCase());
-          stateToHost.put(status, hp.host.toLowerCase());
-        } else {
-          LOG.error(String.format(""Could not retrieve state for NameNode %s from property %s by querying JMX."", hp.host, key));
-        }
-      } catch (MalformedURLException e) {
-        LOG.error(e.getMessage());
-      }
+  private static void checkForDualNetworkCards(Set<String> componentHosts, NameService.NameNode nameNode) {
+    if (!componentHosts.contains(nameNode.getHost())) {
+      //This may happen when NN HA is configured on dual network card machines with public/private FQDNs.
+      LOG.error(
+        MessageFormat.format(
+          ""Hadoop NameNode HA configuration {0} contains host {1} that does not exist in the NameNode hosts list {3}"",
+          nameNode.getPropertyName(), nameNode.getHost(), componentHosts.toString()));
     }
+  }
+
+  private static boolean erverythingIsClassified(Set<String> componentHosts, String master, List<String> secondaries) {","[{'comment': 'everythingIsClassified?', 'commenter': 'ncole'}]"
269,ambari-server/src/main/java/org/apache/ambari/server/stack/NameService.java,"@@ -0,0 +1,153 @@
+/*
+ *
+ *  * Licensed to the Apache Software Foundation (ASF) under one
+ *  * or more contributor license agreements.  See the NOTICE file
+ *  * distributed with this work for additional information
+ *  * regarding copyright ownership.  The ASF licenses this file
+ *  * to you under the Apache License, Version 2.0 (the
+ *  * ""License""); you may not use this file except in compliance
+ *  * with the License.  You may obtain a copy of the License at
+ *  *
+ *  *     http://www.apache.org/licenses/LICENSE-2.0
+ *  *
+ *  * Unless required by applicable law or agreed to in writing, software
+ *  * distributed under the License is distributed on an ""AS IS"" BASIS,
+ *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *  * See the License for the specific language governing permissions and
+ *  * limitations under the License.
+ *
+ */
+
+package org.apache.ambari.server.stack;
+
+import static org.apache.commons.lang.StringUtils.isBlank;
+
+import java.text.MessageFormat;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.stream.Collectors;
+
+import org.apache.ambari.server.state.Cluster;
+import org.apache.ambari.server.state.ConfigHelper;
+import org.apache.ambari.server.utils.HTTPUtils;
+import org.apache.ambari.server.utils.HostAndPort;
+import org.apache.commons.lang.builder.ToStringBuilder;
+
+public class NameService {","[{'comment': 'please add javadoc to this class', 'commenter': 'ncole'}]"
269,ambari-project/pom.xml,"@@ -206,6 +206,12 @@
         <artifactId>mockito-core</artifactId>
         <version>1.10.19</version>
       </dependency>
+      <dependency>
+        <groupId>org.hamcrest</groupId>","[{'comment': 'I think that this BSD license is compatible. Just wanted to mention it for a double-check.', 'commenter': 'jonathan-hurley'}, {'comment': ""Yes it's BSD, also used in logsearch."", 'commenter': 'zeroflag'}]"
269,ambari-server/src/main/java/org/apache/ambari/server/stack/HostsType.java,"@@ -47,15 +49,118 @@
    * That is to say, a downgrade only occurs where the current version is not
    * the target version.
    */
-  public LinkedHashSet<String> hosts = new LinkedHashSet<>();
+  private LinkedHashSet<String> hosts;
 
   /**
-   * Unhealthy hosts are those which are explicitely put into maintenance mode.
+   * Unhealthy hosts are those which are explicitly put into maintenance mode.
    * If there is a host which is not heartbeating (or is generally unhealthy)
    * but not in maintenance mode, then the prerequisite upgrade checks will let
    * the administrator know that it must be put into maintenance mode before an
    * upgrade can begin.
    */
   public List<ServiceComponentHost> unhealthy = new ArrayList<>();
 
+  public boolean hasMasters() {
+    return !getMasters().isEmpty();
+  }
+
+  public List<HaHosts> getHaHosts() {
+    return haHosts;
+  }
+
+  /**
+   * Order the hosts so that for each HA host the secondaries come first.
+   * For example: [sec1, sec2, master1, sec3, sec4, master2]
+   */
+  public void arrangeHostSecondariesFirst() {
+    this.hosts = getHaHosts().stream()
+      .flatMap(each -> Stream.concat(each.getSecondaries().stream(), Stream.of(each.getMaster())))
+      .collect(toCollection(LinkedHashSet::new));
+  }
+
+  public boolean hasMastersAndSecondaries() {
+    return !getMasters().isEmpty() && !getSecondaries().isEmpty();
+  }
+
+  /**
+   * A master and secondary host(s). In HA mode there is one master and one secondary host,
+   * in federated mode there can be more than one secondaries.
+   */
+  public static class HaHosts {","[{'comment': 'HaHosts is kind of a funny name - maybe just call it HighAvailabilityHosts', 'commenter': 'jonathan-hurley'}]"
269,ambari-server/src/main/java/org/apache/ambari/server/stack/HostsType.java,"@@ -47,15 +49,118 @@
    * That is to say, a downgrade only occurs where the current version is not
    * the target version.
    */
-  public LinkedHashSet<String> hosts = new LinkedHashSet<>();
+  private LinkedHashSet<String> hosts;
 
   /**
-   * Unhealthy hosts are those which are explicitely put into maintenance mode.
+   * Unhealthy hosts are those which are explicitly put into maintenance mode.
    * If there is a host which is not heartbeating (or is generally unhealthy)
    * but not in maintenance mode, then the prerequisite upgrade checks will let
    * the administrator know that it must be put into maintenance mode before an
    * upgrade can begin.
    */
   public List<ServiceComponentHost> unhealthy = new ArrayList<>();
 
+  public boolean hasMasters() {
+    return !getMasters().isEmpty();
+  }
+
+  public List<HaHosts> getHaHosts() {","[{'comment': ""Same as below - maybe just call this getHighAvailabilityHosts() so there's no ambiguity. HaHosts is just kind of odd."", 'commenter': 'jonathan-hurley'}]"
269,ambari-server/src/main/java/org/apache/ambari/server/stack/HostsType.java,"@@ -47,15 +49,118 @@
    * That is to say, a downgrade only occurs where the current version is not
    * the target version.
    */
-  public LinkedHashSet<String> hosts = new LinkedHashSet<>();
+  private LinkedHashSet<String> hosts;
 
   /**
-   * Unhealthy hosts are those which are explicitely put into maintenance mode.
+   * Unhealthy hosts are those which are explicitly put into maintenance mode.
    * If there is a host which is not heartbeating (or is generally unhealthy)
    * but not in maintenance mode, then the prerequisite upgrade checks will let
    * the administrator know that it must be put into maintenance mode before an
    * upgrade can begin.
    */
   public List<ServiceComponentHost> unhealthy = new ArrayList<>();
 
+  public boolean hasMasters() {
+    return !getMasters().isEmpty();
+  }
+
+  public List<HaHosts> getHaHosts() {
+    return haHosts;
+  }
+
+  /**
+   * Order the hosts so that for each HA host the secondaries come first.
+   * For example: [sec1, sec2, master1, sec3, sec4, master2]
+   */
+  public void arrangeHostSecondariesFirst() {
+    this.hosts = getHaHosts().stream()
+      .flatMap(each -> Stream.concat(each.getSecondaries().stream(), Stream.of(each.getMaster())))
+      .collect(toCollection(LinkedHashSet::new));
+  }
+
+  public boolean hasMastersAndSecondaries() {
+    return !getMasters().isEmpty() && !getSecondaries().isEmpty();
+  }
+
+  /**
+   * A master and secondary host(s). In HA mode there is one master and one secondary host,
+   * in federated mode there can be more than one secondaries.
+   */
+  public static class HaHosts {
+    private final String master;
+    private final List<String> secondaries;
+
+    public HaHosts(String master, List<String> secondaries) {
+      if (master == null) {
+        throw new IllegalArgumentException(""Master host is missing"");
+      }
+      this.master = master;
+      this.secondaries = secondaries;
+    }
+
+    public String getMaster() {","[{'comment': 'Is it possible that in an ""HA"" setup, there could potentially be multiple masters? Should we plan for that? We should try to make this class as non-HDFS specific as possible. In fact, we shouldn\'t use any HDFS terms in here.', 'commenter': 'jonathan-hurley'}, {'comment': ""It's possible and the user can use our REST API or the config editor on the UI to create such setup, even if our wizards don't support it. The 1 master + 1 secondary setup is a special case of the more general multiple master+secondaries setup."", 'commenter': 'zeroflag'}, {'comment': ""So do you think that we'd need to account for that here then by having the Masters also be a collection."", 'commenter': 'jonathan-hurley'}, {'comment': 'No, I think that would be speculative generality. We might or might not need that in the future, but we already need to have multiple secondaries.', 'commenter': 'zeroflag'}, {'comment': 'Fair enough.', 'commenter': 'jonathan-hurley'}]"
269,ambari-server/src/main/java/org/apache/ambari/server/stack/NameService.java,"@@ -0,0 +1,153 @@
+/*
+ *
+ *  * Licensed to the Apache Software Foundation (ASF) under one
+ *  * or more contributor license agreements.  See the NOTICE file
+ *  * distributed with this work for additional information
+ *  * regarding copyright ownership.  The ASF licenses this file
+ *  * to you under the Apache License, Version 2.0 (the
+ *  * ""License""); you may not use this file except in compliance
+ *  * with the License.  You may obtain a copy of the License at
+ *  *
+ *  *     http://www.apache.org/licenses/LICENSE-2.0
+ *  *
+ *  * Unless required by applicable law or agreed to in writing, software
+ *  * distributed under the License is distributed on an ""AS IS"" BASIS,
+ *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *  * See the License for the specific language governing permissions and
+ *  * limitations under the License.
+ *
+ */
+
+package org.apache.ambari.server.stack;
+
+import static org.apache.commons.lang.StringUtils.isBlank;
+
+import java.text.MessageFormat;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.stream.Collectors;
+
+import org.apache.ambari.server.state.Cluster;
+import org.apache.ambari.server.state.ConfigHelper;
+import org.apache.ambari.server.utils.HTTPUtils;
+import org.apache.ambari.server.utils.HostAndPort;
+import org.apache.commons.lang.builder.ToStringBuilder;
+
+public class NameService {
+
+  public static class NameNode {
+    private final String uniqueId;
+    private final String address;
+    private final boolean encrypted;
+    private final String propertyName;
+
+    public static NameNode fromConfig(String nameServiceId, String nnUniqueId, ConfigHelper config, Cluster cluster) {","[{'comment': ""I feel like having a class for a specific component of a given stack is a bad practice. Can this be made into some metainfo that ships with the stack so it's not hard coded?"", 'commenter': 'jonathan-hurley'}, {'comment': ""I don't think writing ambari in a fully component or stack agnostic way is possible (we already have lots of switch cases on service or component names). This feature is about handling a component specific setup. We need to know if the namenode is in federated mode and we only know this based on a component specific config file. This class represents a real domain concept and it just makes easier to extract this information from the config. Do you have a concrete idea how could we extract this info in a more dynamic way?\r\n"", 'commenter': 'zeroflag'}, {'comment': 'Alerts did something similar with HA modes where it was driven by stack properties, like this:\r\n\r\n```\r\n            ""high_availability"": {\r\n              ""nameservice"": ""{{hdfs-site/dfs.internal.nameservices}}"",\r\n              ""alias_key"" : ""{{hdfs-site/dfs.ha.namenodes.{{ha-nameservice}}}}"",\r\n              ""http_pattern"" : ""{{hdfs-site/dfs.namenode.http-address.{{ha-nameservice}}.{{alias}}}}"",\r\n              ""https_pattern"" : ""{{hdfs-site/dfs.namenode.https-address.{{ha-nameservice}}.{{alias}}}}""\r\n            }\r\n```\r\n\r\nIf we added this information to the HDFS metainfo.xml for NameNode, then you wouldn\'t need to hardcode it in here. You could instead make this a more generic class that could work for other pairs, like YARN\'s RM. ', 'commenter': 'jonathan-hurley'}, {'comment': ""This makes sense but I think this is out of scope now. Anyways most of these properties were already there, I've just modified the way we parse one of them (in federation mode one can contain comma separated ids) and moved them to a separate class. How about making a jira about this improvement?"", 'commenter': 'zeroflag'}, {'comment': ""Ok - let's do that. I'm totally fine following up on this with a 2nd Jira. "", 'commenter': 'jonathan-hurley'}]"
285,ambari-server/src/main/java/org/apache/ambari/server/api/query/render/HostInfoSummaryRenderer.java,"@@ -0,0 +1,95 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.api.query.render;
+
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.ambari.server.api.query.QueryInfo;
+import org.apache.ambari.server.api.services.Request;
+import org.apache.ambari.server.api.services.Result;
+import org.apache.ambari.server.api.services.ResultImpl;
+import org.apache.ambari.server.api.services.ResultPostProcessor;
+import org.apache.ambari.server.api.services.ResultPostProcessorImpl;
+import org.apache.ambari.server.api.util.TreeNode;
+import org.apache.ambari.server.api.util.TreeNodeImpl;
+import org.apache.ambari.server.controller.internal.HostInfoSummaryResourceProvider;
+import org.apache.ambari.server.controller.internal.ResourceImpl;
+import org.apache.ambari.server.controller.spi.Resource;
+import org.apache.commons.lang.StringUtils;
+
+public class HostInfoSummaryRenderer extends BaseRenderer implements Renderer {","[{'comment': 'Javadoc missing ...', 'commenter': 'jonathan-hurley'}, {'comment': 'oops, I will add it.', 'commenter': 'scottduan'}]"
285,ambari-server/src/main/java/org/apache/ambari/server/api/query/render/HostInfoSummaryRenderer.java,"@@ -0,0 +1,95 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.api.query.render;
+
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.ambari.server.api.query.QueryInfo;
+import org.apache.ambari.server.api.services.Request;
+import org.apache.ambari.server.api.services.Result;
+import org.apache.ambari.server.api.services.ResultImpl;
+import org.apache.ambari.server.api.services.ResultPostProcessor;
+import org.apache.ambari.server.api.services.ResultPostProcessorImpl;
+import org.apache.ambari.server.api.util.TreeNode;
+import org.apache.ambari.server.api.util.TreeNodeImpl;
+import org.apache.ambari.server.controller.internal.HostInfoSummaryResourceProvider;
+import org.apache.ambari.server.controller.internal.ResourceImpl;
+import org.apache.ambari.server.controller.spi.Resource;
+import org.apache.commons.lang.StringUtils;
+
+public class HostInfoSummaryRenderer extends BaseRenderer implements Renderer {
+
+  /**
+   * {@inheritDoc}
+   */
+  @Override
+  public TreeNode<Set<String>> finalizeProperties(
+      TreeNode<QueryInfo> queryTree, boolean isCollection) {","[{'comment': ""I don't think that you need to break the line so soon - maybe extend out your columns?"", 'commenter': 'jonathan-hurley'}, {'comment': 'OK, just copy/paste, I will make it one line.', 'commenter': 'scottduan'}]"
285,ambari-server/src/main/java/org/apache/ambari/server/api/query/render/HostInfoSummaryRenderer.java,"@@ -0,0 +1,95 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.api.query.render;
+
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.ambari.server.api.query.QueryInfo;
+import org.apache.ambari.server.api.services.Request;
+import org.apache.ambari.server.api.services.Result;
+import org.apache.ambari.server.api.services.ResultImpl;
+import org.apache.ambari.server.api.services.ResultPostProcessor;
+import org.apache.ambari.server.api.services.ResultPostProcessorImpl;
+import org.apache.ambari.server.api.util.TreeNode;
+import org.apache.ambari.server.api.util.TreeNodeImpl;
+import org.apache.ambari.server.controller.internal.HostInfoSummaryResourceProvider;
+import org.apache.ambari.server.controller.internal.ResourceImpl;
+import org.apache.ambari.server.controller.spi.Resource;
+import org.apache.commons.lang.StringUtils;
+
+public class HostInfoSummaryRenderer extends BaseRenderer implements Renderer {
+
+  /**
+   * {@inheritDoc}
+   */
+  @Override
+  public TreeNode<Set<String>> finalizeProperties(
+      TreeNode<QueryInfo> queryTree, boolean isCollection) {
+
+    QueryInfo queryInfo = queryTree.getObject();
+    TreeNode<Set<String>> resultTree = new TreeNodeImpl<>(
+        null, queryInfo.getProperties(), queryTree.getName());
+
+    copyPropertiesToResult(queryTree, resultTree);
+
+    return resultTree;
+  }
+
+  @Override
+  public boolean requiresPropertyProviderInput() {
+    return false;
+  }
+
+  @Override
+  public Result finalizeResult(Result queryResult) {
+    // Convert fully qualified properties into short property names and flat the queryResult datastructure
+    Map<String, Object> summaryMap = new HashMap<>();","[{'comment': 'Do you care about ordering here? If so, maybe use a different kind of map?', 'commenter': 'jonathan-hurley'}, {'comment': 'Not care the ordering. What different kind of map? ', 'commenter': 'scottduan'}, {'comment': ""If the ordering isn't important, then it's fine. Just was curious if you wanted ordered by something."", 'commenter': 'jonathan-hurley'}]"
285,ambari-server/src/main/java/org/apache/ambari/server/api/query/render/HostInfoSummaryRenderer.java,"@@ -0,0 +1,95 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.api.query.render;
+
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.ambari.server.api.query.QueryInfo;
+import org.apache.ambari.server.api.services.Request;
+import org.apache.ambari.server.api.services.Result;
+import org.apache.ambari.server.api.services.ResultImpl;
+import org.apache.ambari.server.api.services.ResultPostProcessor;
+import org.apache.ambari.server.api.services.ResultPostProcessorImpl;
+import org.apache.ambari.server.api.util.TreeNode;
+import org.apache.ambari.server.api.util.TreeNodeImpl;
+import org.apache.ambari.server.controller.internal.HostInfoSummaryResourceProvider;
+import org.apache.ambari.server.controller.internal.ResourceImpl;
+import org.apache.ambari.server.controller.spi.Resource;
+import org.apache.commons.lang.StringUtils;
+
+public class HostInfoSummaryRenderer extends BaseRenderer implements Renderer {
+
+  /**
+   * {@inheritDoc}
+   */
+  @Override
+  public TreeNode<Set<String>> finalizeProperties(
+      TreeNode<QueryInfo> queryTree, boolean isCollection) {
+
+    QueryInfo queryInfo = queryTree.getObject();
+    TreeNode<Set<String>> resultTree = new TreeNodeImpl<>(
+        null, queryInfo.getProperties(), queryTree.getName());
+
+    copyPropertiesToResult(queryTree, resultTree);
+
+    return resultTree;
+  }
+
+  @Override
+  public boolean requiresPropertyProviderInput() {
+    return false;
+  }
+
+  @Override
+  public Result finalizeResult(Result queryResult) {
+    // Convert fully qualified properties into short property names and flat the queryResult datastructure
+    Map<String, Object> summaryMap = new HashMap<>();
+    TreeNode<Resource> resultTree = queryResult.getResultTree();
+    Collection<TreeNode<Resource>> nodes = resultTree.getChildren();
+    if (nodes != null && !nodes.isEmpty()) {
+      Resource resource = (Resource)((TreeNode)nodes.iterator().next()).getObject();
+      Object o = resource.getPropertyValue(HostInfoSummaryResourceProvider.CLUSTER_NAME);
+      if (o != null && o instanceof String &&  StringUtils.isNotBlank((String)o)) {
+        summaryMap.put(""cluster_name"", (String)o);
+      }
+      o = resource.getPropertiesMap().get(HostInfoSummaryResourceProvider.HOSTS_SUMMARY);
+      if (o != null && o instanceof Map<?,?> &&  ((Map<String, Object>)o).size() > 0) {
+        summaryMap.putAll((Map<String, Object>)o);
+      }
+    }
+
+    Resource resultResource = new ResourceImpl(Resource.Type.HostSummary);
+    resultResource.setProperty(""hosts_summary"", summaryMap);","[{'comment': 'Extract out `hosts_summary` to a constant since you use it a few places.', 'commenter': 'jonathan-hurley'}, {'comment': 'Ok, I will use constant variable to replace it.', 'commenter': 'scottduan'}]"
285,ambari-server/src/main/java/org/apache/ambari/server/api/resources/HostInfoSummaryResourceDefinition.java,"@@ -0,0 +1,65 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.api.resources;
+
+import java.util.Collection;
+import java.util.Collections;
+
+import org.apache.ambari.server.api.query.render.HostInfoSummaryRenderer;
+import org.apache.ambari.server.api.query.render.Renderer;
+import org.apache.ambari.server.controller.spi.Request;
+import org.apache.ambari.server.controller.spi.Resource;
+
+public class HostInfoSummaryResourceDefinition extends BaseResourceDefinition {","[{'comment': 'Missing Javadoc', 'commenter': 'jonathan-hurley'}, {'comment': 'Will add it.', 'commenter': 'scottduan'}]"
285,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/DefaultProviderModule.java,"@@ -132,6 +132,10 @@ protected ResourceProvider createResourceProvider(Resource.Type type) {
         return new ArtifactResourceProvider(managementController);
       case RemoteCluster:
         return new RemoteClusterResourceProvider();
+      case Host:
+        return new HostResourceProvider(managementController);
+      case HostSummary:
+        return new HostInfoSummaryResourceProvider(managementController);","[{'comment': 'This is interesting - I would have expected you to use the same ResourceProvider here (`HostResourceProvider`) and simply change the renderer in the `HostResourceDefinition`. Is there a reason a whole new ResourceProvider is needed instead of just a new renderer?', 'commenter': 'jonathan-hurley'}, {'comment': 'At begining, I thought to reuse HostResourceProvider, but I found they are totoally different resource provider: \r\n(1) HostResourceProvider focuses on create, update and delete Host, but HostInfoSummaryResourceProvider is a read-only provider\r\n(2) HostResourceProvider is only on single host, but the latter will aggregate info from all hosts in cluster or accross clusters\r\nSo I think it is more clear to separete these two kinds of host resource providers.', 'commenter': 'scottduan'}, {'comment': ""HostResourceProvider does do multiple hosts though:\r\n\r\n```\r\n    Set<HostResponse> responses = getResources(new Command<Set<HostResponse>>() {\r\n      @Override\r\n      public Set<HostResponse> invoke() throws AmbariException {\r\n        return getHosts(requests);\r\n      }\r\n    });\r\n```\r\n\r\nIt iterates over the `HostResponse` instances, creating resources for each. So, wouldn't you only need to supply a renderer in this case to post-process the results?"", 'commenter': 'jonathan-hurley'}, {'comment': 'The reason is performance. My implementation is to sum os_type of all the hosts on DB level with single sql call. If we do post-process for all the hosts after creating resource for each, in large number of hosts scenario, it will take too long to get resources for all hosts.', 'commenter': 'scottduan'}, {'comment': 'I also thought we will be adding a new renderer like we do for Alerts. I understand that this is more performant but it is a tradeoff between performance & maintenance. If the host summary API is not going to be used very often then we should consider just going with a renderer. Can you check with the UI team on how often the host summary api will be called in the UI, if its perriodic or only on a specific user action. ', 'commenter': 'jayush'}, {'comment': 'I will check UI about the use case. In considerationof maintenance, I think the new implementation is more clear and clean. The host resouce provider is onlyb for single host crud, the hosts summary resource provider is a read provideer and works like a aggregation service and provide overall hosts summary in/across clusters. Now I only implement os_type sum, but it is very easy to add more inforamtion in the future.', 'commenter': 'scottduan'}, {'comment': ""The host resource provider does provide information for multiple hosts. If we're introducing a new resource provider, then we shouldn't need a renderer since the default renderer can be used (but it should be a new endpoint). So I think we need to either:\r\n- Switch to using a renderer with the current resource provider\r\n\r\nOR\r\n\r\n- Create a new endpoint and new resource provider with a default renderer for the host summary"", 'commenter': 'jonathan-hurley'}]"
285,ambari-server/src/test/java/org/apache/ambari/server/api/services/HostServiceTest.java,"@@ -31,6 +31,7 @@
 import org.apache.ambari.server.api.resources.ResourceInstance;
 import org.apache.ambari.server.api.services.parsers.RequestBodyParser;
 import org.apache.ambari.server.api.services.serializers.ResultSerializer;
+import org.apache.ambari.server.controller.spi.Resource;","[{'comment': 'Unused.', 'commenter': 'adoroszlai'}]"
285,ambari-server/src/main/java/org/apache/ambari/server/orm/models/HostInfoSummary.java,"@@ -0,0 +1,62 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.orm.models;
+
+import java.util.AbstractMap;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.stream.Collectors;
+import java.util.stream.Stream;
+
+import org.apache.ambari.server.StaticallyInject;
+import org.apache.ambari.server.orm.dao.HostInfoSummaryDAO;
+import org.apache.ambari.server.orm.dao.HostInfoSummaryDTO;
+
+import com.google.inject.Inject;
+
+@StaticallyInject
+public class HostInfoSummary {
+
+  static public String HOST_INFO_SUMMARY_OS = ""operating_systems"";
+
+  @Inject
+  private static HostInfoSummaryDAO hostInfoSummaryDAO;
+
+  private List<Object> summary = new ArrayList<>();
+
+  public  HostInfoSummary getHostInfoSummary(String cluster_name) {
+
+    List<HostInfoSummaryDTO> summaryDTOS = hostInfoSummaryDAO.findHostInfoSummary(cluster_name);
+    List<Map<String, Integer>> osSummaryList = new ArrayList<>();
+    for (HostInfoSummaryDTO summaryDTO : summaryDTOS) {
+      osSummaryList.add(Stream.of(new AbstractMap.SimpleImmutableEntry<>(summaryDTO.getOsType(), summaryDTO.getOsTypeCount())).collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue)));","[{'comment': ""Seems unnecessarily complicated.  I don't think you need both the loop and the stream."", 'commenter': 'adoroszlai'}]"
285,ambari-server/src/main/java/org/apache/ambari/server/orm/models/HostInfoSummary.java,"@@ -0,0 +1,62 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.orm.models;
+
+import java.util.AbstractMap;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.stream.Collectors;
+import java.util.stream.Stream;
+
+import org.apache.ambari.server.StaticallyInject;
+import org.apache.ambari.server.orm.dao.HostInfoSummaryDAO;
+import org.apache.ambari.server.orm.dao.HostInfoSummaryDTO;
+
+import com.google.inject.Inject;
+
+@StaticallyInject
+public class HostInfoSummary {
+
+  static public String HOST_INFO_SUMMARY_OS = ""operating_systems"";
+
+  @Inject
+  private static HostInfoSummaryDAO hostInfoSummaryDAO;
+
+  private List<Object> summary = new ArrayList<>();
+
+  public  HostInfoSummary getHostInfoSummary(String cluster_name) {
+
+    List<HostInfoSummaryDTO> summaryDTOS = hostInfoSummaryDAO.findHostInfoSummary(cluster_name);
+    List<Map<String, Integer>> osSummaryList = new ArrayList<>();
+    for (HostInfoSummaryDTO summaryDTO : summaryDTOS) {
+      osSummaryList.add(Stream.of(new AbstractMap.SimpleImmutableEntry<>(summaryDTO.getOsType(), summaryDTO.getOsTypeCount())).collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue)));
+    }
+    Map<String, Object> os = new HashMap<>();
+    os.put(HOST_INFO_SUMMARY_OS, osSummaryList);
+    summary.add(os);","[{'comment': 'Why does it need to keep `summary` as state?', 'commenter': 'adoroszlai'}]"
285,ambari-server/src/main/java/org/apache/ambari/server/orm/entities/ClusterHostMappingEntityPK.java,"@@ -0,0 +1,70 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.orm.entities;
+
+import java.io.Serializable;
+
+import javax.persistence.Column;
+import javax.persistence.Id;
+
+public class ClusterHostMappingEntityPK implements Serializable {
+
+  private Long clusterId;
+  private Long hostId;
+
+  @Id
+  @Column(name = ""cluster_id"", nullable = false, insertable = true, updatable = true)
+  public Long getClusterId() {
+    return clusterId;
+  }
+
+  public void setClusterId(Long clusterId) {
+    this.clusterId = clusterId;
+  }
+
+  @Id
+  @Column(name = ""host_id"", nullable = false, insertable = true, updatable = true)
+  public Long getHostId() {
+    return hostId;
+  }
+
+  public void setHostId(Long hostId) {
+    this.hostId = hostId;
+  }
+
+  @Override
+  public boolean equals(Object o) {
+    if (this == o) return true;
+    if (o == null || getClass() != o.getClass()) return false;
+
+    ClusterHostMappingEntityPK that = (ClusterHostMappingEntityPK) o;
+
+    if (!clusterId.equals(that.clusterId)) return false;
+    if (!hostId.equals(that.hostId)) return false;
+
+    return true;
+  }
+
+  @Override
+  public int hashCode() {
+    int result = clusterId.hashCode();
+    result = 31 * result + hostId.hashCode();","[{'comment': '`Objects.hash(...)`', 'commenter': 'adoroszlai'}]"
285,ambari-server/src/main/java/org/apache/ambari/server/orm/entities/ClusterHostMappingEntity.java,"@@ -0,0 +1,75 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.orm.entities;
+
+import javax.persistence.Column;
+import javax.persistence.Entity;
+import javax.persistence.Id;
+import javax.persistence.IdClass;
+import javax.persistence.Table;
+
+@Entity
+@Table(name = ""ClusterHostMapping"")
+@IdClass(ClusterHostMappingEntityPK.class)
+public class ClusterHostMappingEntity {
+
+  @Id
+  @Column(name = ""cluster_id"", nullable = false, insertable = true, updatable = true)
+  private Long clusterId;
+
+  @Id
+  @Column(name = ""host_id"", nullable = false, insertable = true, updatable = false)
+  private Long hostId;
+
+  public Long getClusterId() {
+    return clusterId;
+  }
+
+  public void setClusterId(Long clusterId) {
+    this.clusterId = clusterId;
+  }
+
+  public Long getHostId() {
+    return hostId;
+  }
+
+  public void setHostId(Long hostId) {
+    this.hostId = hostId;
+  }
+
+  @Override
+  public boolean equals(Object o) {
+    if (this == o) return true;
+    if (o == null || getClass() != o.getClass()) return false;
+
+    ClusterHostMappingEntity that = (ClusterHostMappingEntity) o;
+
+    if (!clusterId.equals(that.clusterId)) return false;
+    if (!hostId.equals(that.hostId)) return false;
+
+    return true;
+  }
+
+  @Override
+  public int hashCode() {
+    int result = clusterId.hashCode();
+    result = 31 * result + hostId.hashCode();","[{'comment': '`Objects.hash(...)`', 'commenter': 'adoroszlai'}]"
285,ambari-server/src/main/java/org/apache/ambari/server/orm/dao/HostInfoSummaryDTO.java,"@@ -0,0 +1,37 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.orm.dao;
+
+public class HostInfoSummaryDTO {
+
+  private String os_type;
+  private int os_type_count = 0;","[{'comment': 'Please use `camelCase`.', 'commenter': 'adoroszlai'}]"
285,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/HostResourceProvider.java,"@@ -238,6 +242,45 @@ public Void invoke() throws AmbariException, AuthorizationException {
   @Override
   protected Set<Resource> getResourcesAuthorized(Request request, Predicate predicate)
       throws SystemException, UnsupportedPropertyException, NoSuchResourceException, NoSuchParentResourceException {
+    if (request.getPropertyIds().contains(HOST_SUMMARY_PROPERTY_ID)) {
+      return getHostSummaryResource(request, predicate);
+    } else {
+      return getHostResource(request, predicate);
+    }
+  }
+
+  private Set<Resource> getHostSummaryResource(Request request, Predicate predicate)
+      throws SystemException, UnsupportedPropertyException, NoSuchResourceException, NoSuchParentResourceException {
+    Set<String> requestPropertyIds = getRequestPropertyIds(request, predicate);
+
+    // use a collection which preserves order since JPA sorts the results
+    Set<Resource> resources = new HashSet<>();","[{'comment': '""preserves order"" and `HashSet` seem to contradict each other.', 'commenter': 'adoroszlai'}]"
285,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/HostResourceProvider.java,"@@ -238,6 +242,45 @@ public Void invoke() throws AmbariException, AuthorizationException {
   @Override
   protected Set<Resource> getResourcesAuthorized(Request request, Predicate predicate)
       throws SystemException, UnsupportedPropertyException, NoSuchResourceException, NoSuchParentResourceException {
+    if (request.getPropertyIds().contains(HOST_SUMMARY_PROPERTY_ID)) {
+      return getHostSummaryResource(request, predicate);","[{'comment': 'Can you please document the response structure of host summary in `HostResponse` or a new interface using Swagger annotations?', 'commenter': 'adoroszlai'}]"
285,ambari-server/src/main/java/org/apache/ambari/server/orm/entities/ClusterHostMappingEntityPK.java,"@@ -0,0 +1,70 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.orm.entities;
+
+import java.io.Serializable;
+
+import javax.persistence.Column;
+import javax.persistence.Id;
+
+public class ClusterHostMappingEntityPK implements Serializable {
+
+  private Long clusterId;
+  private Long hostId;
+
+  @Id
+  @Column(name = ""cluster_id"", nullable = false, insertable = true, updatable = true)","[{'comment': 'Are these JPA annotations really required in both the PK and entity classes?', 'commenter': 'adoroszlai'}, {'comment': 'Not necessary at this time.', 'commenter': 'scottduan'}]"
285,ambari-server/src/main/java/org/apache/ambari/server/orm/dao/HostInfoSummaryDAO.java,"@@ -0,0 +1,75 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.orm.dao;
+
+import java.util.List;
+
+import javax.persistence.EntityManager;
+import javax.persistence.TypedQuery;
+
+import org.apache.ambari.server.orm.RequiresSession;
+import org.apache.commons.lang.StringUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.inject.Inject;
+import com.google.inject.Provider;
+import com.google.inject.Singleton;
+
+@Singleton
+public class HostInfoSummaryDAO {
+
+  private static final Logger LOG = LoggerFactory.getLogger(HostInfoSummaryDAO.class);","[{'comment': 'Unused.', 'commenter': 'adoroszlai'}, {'comment': 'Will remove', 'commenter': 'scottduan'}]"
305,ambari-server/src/main/resources/common-services/ZEPPELIN/0.7.0/package/scripts/alert_check_zeppelin.py,"@@ -37,7 +37,7 @@
 
 def execute(configurations={}, parameters={}, host_name=None):
   try:
-    pid_file = glob.glob(zeppelin_pid_dir + '/zeppelin-*.pid')[0]
+    pid_file = glob.glob(zeppelin_pid_dir + '/zeppelin-zeppelin-*.pid')[0]","[{'comment': ""The second `zeppelin` stands for Zeppelin service's unix user, which can be customised (see below).  So it should not be hard coded here, because that would trigger false alerts.  (The same applies to the 0.6.0 script.)\r\n\r\nhttps://github.com/apache/ambari/blob/622bb9eca3b9dbf91efb6e7d77c8b9bad04e7237/ambari-server/src/main/resources/common-services/ZEPPELIN/0.7.0/package/scripts/master.py#L269-L270"", 'commenter': 'adoroszlai'}]"
310,ambari-server/src/main/resources/common-services/RANGER/0.4.0/package/scripts/setup_ranger_xml.py,"@@ -120,6 +120,13 @@ def setup_ranger_admin(upgrade_type=None):
     create_parents=True
   )
 
+  File(format('{ranger_conf}/ranger-admin-env.sh'),
+    content = format(""export JAVA_HOME={java_home}""),
+    owner = params.unix_user,
+    group = params.unix_group,
+    mode = 0755
+  )
+","[{'comment': 'Instead of doing it like this as a one-off, why not just have Ambari manage the -env.sh file you? That way, future changes can be made directly to the template and the file will just be written out as needed.\r\n\r\nIt would look something like\r\n```\r\n# The java implementation to use.\r\nexport JAVA_HOME={{java64_home}}\r\n```\r\n', 'commenter': 'jonathan-hurley'}, {'comment': 'Hi @jonathan-hurley, this fix is currently for Ambari-2.6.2 release only. A consolidated -env.sh file is being planned for next major Ambari release.', 'commenter': 'fimugdha'}, {'comment': ""I suppose since it's only 1 property doing it this way is fine for 2.6.2"", 'commenter': 'jonathan-hurley'}]"
355,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/UpgradeResourceProvider.java,"@@ -755,7 +756,17 @@ major stack versions (e.g., HDP 2.2 -> 2.3), and then set config changes
     at the appropriate moment during the orchestration.
     */
     if (pack.getType() == UpgradeType.ROLLING || pack.getType() == UpgradeType.HOST_ORDERED) {
+      if (direction == Direction.UPGRADE) {
+        StackEntity targetStack = upgradeContext.getRepositoryVersion().getStack();
+        cluster.setDesiredStackVersion(
+            new StackId(targetStack.getStackName(), targetStack.getStackVersion()));
+      }
       s_upgradeHelper.updateDesiredRepositoriesAndConfigs(upgradeContext);
+      if (direction == Direction.DOWNGRADE) {
+        StackId targetStack = upgradeContext.getCluster().getCurrentStackVersion();
+        cluster.setDesiredStackVersion(","[{'comment': 'nit: you already have a StackId, no need to make a new one.', 'commenter': 'ncole'}, {'comment': 'Also, did you happen to test an EU?', 'commenter': 'ncole'}, {'comment': 'Tested EU, works well', 'commenter': 'Unknown'}]"
355,ambari-server/src/main/java/org/apache/ambari/server/upgrade/UpgradeCatalog262.java,"@@ -61,6 +66,29 @@ private void addHostRequestStatusColumn() throws SQLException {
 
   @Override
   protected void executePreDMLUpdates() throws AmbariException, SQLException {
+    fixDesiredStack();
+  }
+
+  /**
+   * if desired stack < current stack, set current stack as desired
+   *
+   * @throws AmbariException
+   */
+  private void fixDesiredStack() throws AmbariException {
+    AmbariManagementController ambariManagementController = injector.getInstance(AmbariManagementController.class);
+    Clusters clusters = ambariManagementController.getClusters();
+    if (clusters != null) {
+      Map<String, Cluster> clusterMap = getCheckedClusterMap(clusters);
+      if (clusterMap != null && !clusterMap.isEmpty()) {
+        for (final Cluster cluster : clusterMap.values()) {
+          StackId desiredStack = cluster.getDesiredStackVersion();
+          StackId currentStack = cluster.getCurrentStackVersion();
+          if (desiredStack.compareTo(currentStack) < 0) {
+            cluster.setDesiredStackVersion(currentStack);
+          }","[{'comment': ""If desired stack is > current stack, that's also a problem. Can you change this to a !equals() call instead?"", 'commenter': 'jonathan-hurley'}, {'comment': 'I assume that user is not expected to perform Ambari upgrade with a paused stack upgrade? Ok, will change the logic', 'commenter': 'Unknown'}, {'comment': '@jonathan-hurley , regarding \r\n\r\n    Why does express not have this problem? \r\n\r\nEU updates cluster desired version during upgrade and downgrade at `UpdateDesiredRepositoryAction`. This action is called only from EU upgrade packs.', 'commenter': 'Unknown'}, {'comment': ""Yes, that's what I expected. However, I didn't see it in there. Can you identify where exactly it's set? "", 'commenter': 'jonathan-hurley'}, {'comment': ""`org/apache/ambari/server/serveraction/upgrades/UpdateDesiredRepositoryAction.java:149`\r\nat `updateDesiredRepositoryVersion()`\r\n\r\n```\r\n        // move the cluster's desired stack as well\r\n        StackId targetStackId = targetRepositoryVersion.getStackId();\r\n        cluster.setDesiredStackVersion(targetStackId);\r\n      }\r\n```\r\n\r\nand below at the same file for downgrade.\r\nEU was not broken, this code is already at 2.6 branch"", 'commenter': 'Unknown'}, {'comment': 'I was looking in trunk :)\r\n\r\nThanks!', 'commenter': 'jonathan-hurley'}]"
377,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/HostComponentResourceProvider.java,"@@ -120,6 +120,8 @@
   public static final String HOST_COMPONENT_MAINTENANCE_STATE_PROPERTY_ID
       = ""HostRoles/maintenance_state"";
   public static final String HOST_COMPONENT_UPGRADE_STATE_PROPERTY_ID = ""HostRoles/upgrade_state"";
+  public static final String HOST_COMPONENT_CLUSTER_ID","[{'comment': 'We should not be adding service-specific properties to the ResourceProvider contract.', 'commenter': 'swagle'}]"
377,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/NameNodeClusterIdHelper.java,"@@ -0,0 +1,81 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.controller.internal;
+
+import com.google.gson.JsonObject;
+import com.google.gson.JsonParser;
+import org.apache.ambari.server.controller.AmbariManagementController;
+import org.apache.ambari.server.controller.ConfigurationRequest;
+import org.apache.ambari.server.controller.ConfigurationResponse;
+import org.apache.ambari.server.controller.spi.SystemException;
+import org.apache.ambari.server.state.HostConfig;
+
+import java.io.IOException;
+import java.io.InputStreamReader;
+import java.net.URL;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.Set;
+
+/**
+ * A helper class to find the clusterId of a Name Node.
+ */
+public class NameNodeClusterIdHelper {","[{'comment': 'This helper functionality is already covered by the JMXPropertyProvider and its implementations so that any JMX property can be exposed using a json file called metrics.json in the stack definition.', 'commenter': 'swagle'}]"
411,ambari-server/src/main/resources/common-services/YARN/2.1.0.2.0/configuration/container-executor.xml,"@@ -0,0 +1,36 @@
+<?xml version=""1.0""?>
+<?xml-stylesheet type=""text/xsl"" href=""configuration.xsl""?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration supports_adding_forbidden=""true"">
+  <property>
+    <name>content</name>
+    <display-name>container-executor configuration template</display-name>
+    <description>This is the jinja template for container-executor.cfg file</description>
+    <property-type>VALUE_FROM_PROPERTY_FILE</property-type>
+    <value/>
+    <value-attributes>
+	  <type>content</type>
+      <property-file-name>container-executor.cfg.j2</property-file-name>
+      <property-file-type>text</property-file-type>
+    </value-attributes>
+    <on-ambari-upgrade add=""true""/>","[{'comment': ""@dlysnichenko , @smolnar82 , doesn't this need to be set to false?\r\n"", 'commenter': 'rlevas'}, {'comment': 'yes, it should be false', 'commenter': 'Unknown'}, {'comment': 'fixed', 'commenter': 'smolnar82'}]"
417,ambari-infra/ambari-infra-solr-client/src/main/java/org/apache/ambari/infra/solr/AmbariSolrCloudCLI.java,"@@ -307,20 +314,33 @@ public static void main(String[] args) {
       .argName(""cluster prop value"")
       .build();
 
-    final Option copyFromZnodeOption = Option.builder(""cfz"")
-      .longOpt(""copy-from-znode"")
-      .desc(""Copy-from-znode"")
-      .numberOfArgs(1)
-      .argName(""/ambari-solr-secure"")
-      .build();
-
     final Option saslUsersOption = Option.builder(""su"")
       .longOpt(""sasl-users"")
       .desc(""Sasl users (comma separated list)"")
       .numberOfArgs(1)
       .argName(""atlas,ranger,logsearch-solr"")
       .build();
 
+    final Option copyScrOption = Option.builder(""cps"")
+      .longOpt(""copy-scr"")
+      .desc("""")
+      .argName(""/myznode"")
+      .build();
+
+    final Option copyDestOption = Option.builder(""cpd"")
+      .longOpt(""copy-dist"")
+      .desc("""")
+      .numberOfArgs(1)
+      .argName(""/myznode"")
+      .build();
+
+    final Option transferModeOption = Option.builder(""tm"")
+      .longOpt(""transfer-mode"")
+      .desc(""Transfer mode, if not used copy znode to znode."")
+      .numberOfArgs(1)
+      .argName(""copyToLocal|copyToLocal"")","[{'comment': 'Should be `copyFromLocal|copyToLocal` (or the other way around).', 'commenter': 'adoroszlai'}]"
417,ambari-infra/ambari-infra-solr-client/src/main/java/org/apache/ambari/infra/solr/AmbariSolrCloudCLI.java,"@@ -425,10 +448,13 @@ public static void main(String[] args) {
       } else if (cli.hasOption(""rah"")) {
         command = REMOVE_ADMIN_HANDLERS;
         validateRequiredOptions(cli, command, zkConnectStringOption, collectionOption);
-      } else {
+      } else if (cli.hasOption(""tf"")) {","[{'comment': ""Shouldn't this be `tz`?"", 'commenter': 'adoroszlai'}]"
417,ambari-infra/ambari-infra-solr-client/src/main/java/org/apache/ambari/infra/solr/AmbariSolrCloudCLI.java,"@@ -144,6 +146,11 @@ public static void main(String[] args) {
       .desc(""Remove AdminHandlers request handler from solrconfig.xml (command)"")
       .build();
 
+    final Option transferZnodeOption = Option.builder(""tz"")","[{'comment': 'These `Option`s should be reused below for`cli.hasOption` and `cli.getOptionValue`, instead of repeating the strings.', 'commenter': 'adoroszlai'}, {'comment': ""i think it's more like a global issue"", 'commenter': 'oleewere'}]"
417,ambari-infra/ambari-infra-solr-client/src/main/java/org/apache/ambari/infra/solr/AmbariSolrCloudCLI.java,"@@ -62,6 +63,7 @@
       + ""\n./solrCloudCli.sh --remove-admin-handlers -z host1:2181,host2:2181/ambari-solr -c collection""
       + ""\n./solrCloudCli.sh --create-znode -z host1:2181,host2:2181 -zn /ambari-solr""
       + ""\n./solrCloudCli.sh --check-znode -z host1:2181,host2:2181 -zn /ambari-solr""
+      + ""\n./solrCloudCli.sh --transfer-znode -z host1:2181,host2:2181 -tz -cps /ambari-solr -cpd /ambari-solr-backup""","[{'comment': ""Can you please clarify this example?  I think `-tz` is duplicated (as it's the short form of `--transfer-znode`), but `-tm` does not appear anywhere, nor does its argument."", 'commenter': 'adoroszlai'}]"
431,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/ExportBlueprintRequest.java,"@@ -154,14 +154,16 @@ private void createHostGroupInfo(Collection<ExportedHostGroup> exportedHostGroup
 
 
   private Stack parseStack(Resource clusterResource) throws InvalidTopologyTemplateException {
-    String[] stackTokens = String.valueOf(clusterResource.getPropertyValue(
-        ClusterResourceProvider.CLUSTER_VERSION_PROPERTY_ID)).split(""-"");
+    Splitter splitter = Splitter.on('-').limit(2);  // We are only interested to split on the first '-', e.g.","[{'comment': ""This logic is already implemented in `StackId`, which I think we should reuse.\r\n(I'm already doing that in my work-in-progress change.)"", 'commenter': 'adoroszlai'}, {'comment': 'fixed', 'commenter': 'benyoka'}]"
431,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/ExportBlueprintRequest.java,"@@ -46,11 +46,11 @@
 import org.apache.ambari.server.topology.HostGroup;
 import org.apache.ambari.server.topology.HostGroupImpl;
 import org.apache.ambari.server.topology.HostGroupInfo;
-import org.apache.ambari.server.topology.InvalidTopologyTemplateException;
-import org.apache.ambari.server.topology.TopologyRequest;
+import org.apache.ambari.server.topology.InvalidTopologyTemplateException;import org.apache.ambari.server.topology.TopologyRequest;","[{'comment': 'Looks like a newline is lost here.', 'commenter': 'adoroszlai'}, {'comment': 'fixed', 'commenter': 'benyoka'}]"
431,ambari-server/src/main/java/org/apache/ambari/server/api/query/render/ClusterBlueprintRenderer.java,"@@ -299,34 +323,36 @@ private Resource createBlueprintResource(TreeNode<Resource> clusterNode) {
         Map<String, Object> ServiceComponentInfoMap = component.getPropertiesMap().get(""ServiceComponentInfo"");
 
         if (ServiceComponentInfoMap.get(""recovery_enabled"").equals(""true"")) {
-          globalRecoveryEnabled = true;
-          property.put(""name"", ServiceInfoMap.get(""service_name"").toString());
-          property.put(""recovery_enabled"", ""true"");
+          serviceProperty.put(""name"", ServiceInfoMap.get(""service_name"").toString());
+          serviceProperty.put(""recovery_enabled"", ""true"");
 
           //component_settings population
-          componentProperty = new HashMap<>();
+          HashMap<String, String> componentProperty = new HashMap<>();
           componentProperty.put(""name"", ServiceComponentInfoMap.get(""component_name"").toString());
           componentProperty.put(""recovery_enabled"", ""true"");
+          componentSettingValue.add(componentProperty);
         }
       }
 
-      if (!property.isEmpty())
-        serviceSettingValue.add(property);
-      if (!componentProperty.isEmpty())
-        componentSettingValue.add(componentProperty);
+      if (!serviceProperty.isEmpty())
+        serviceSettingValue.add(serviceProperty);
     }
-    //recovery_settings population
-    property = new HashMap<>();
-    if (globalRecoveryEnabled) {
-      property.put(""recovery_enabled"", ""true"");
-    } else {
-      property.put(""recovery_enabled"", ""false"");
+
+    // Add cluster settings
+    Set<Map<String, String>> clusterSettings = new HashSet<>();
+    TreeNode<Resource> settingsNode = clusterNode.getChild(""settings"");
+    if (null != settingsNode) {
+      for (TreeNode<Resource> clusterSettingNode: settingsNode.getChildren()) {
+        Map<String, Object> nodeProperties = clusterSettingNode.getObject().getPropertiesMap().get(""ClusterSettingInfo"");
+        String key = Objects.toString(nodeProperties.get(""cluster_setting_name""));
+        String value = Objects.toString(nodeProperties.get(""cluster_setting_value""));","[{'comment': 'Should be reusing constants from `ClusterSettingResourceProvider`.', 'commenter': 'adoroszlai'}, {'comment': 'fixed', 'commenter': 'benyoka'}]"
431,ambari-server/src/main/java/org/apache/ambari/server/api/query/render/ClusterBlueprintRenderer.java,"@@ -106,17 +116,28 @@
       resultTree.addChild(new HashSet<>(), configType);
     }
 
+    if (resultTree.getChild(Resource.Type.ClusterSetting.name()) == null) {
+      resultTree.addChild(new HashSet<>(), Resource.Type.ClusterSetting.name());
+    }
+
+    String serviceGroupType = Resource.Type.ServiceGroup.name();
+    if (resultTree.getChild(serviceGroupType) == null) {
+      resultTree.addChild(new HashSet<>(), serviceGroupType);
+    }","[{'comment': 'Can you please extract and reuse this logic?', 'commenter': 'adoroszlai'}, {'comment': 'fixed', 'commenter': 'benyoka'}]"
431,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/BlueprintConfigurationProcessor.java,"@@ -522,7 +522,7 @@ public void doUpdateForBlueprintExport() {
 
       doFilterPriorToExport(configuration);
     }
-  }
+    }","[{'comment': 'Seems to be accidental.', 'commenter': 'adoroszlai'}]"
431,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/MpackResourceProvider.java,"@@ -159,12 +159,15 @@ public RequestStatus createResourcesAuthorized(final Request request)
         associatedResources.add(resource);
         return getRequestStatus(null, associatedResources);
       }
-    } catch (IOException e) {
-      if (e instanceof ConnectException)
-        throw new SystemException(""The Mpack Uri : "" + mpackRequest.getMpackUri() + "" is not valid. Please try again"");
-      e.printStackTrace();
-    } catch (BodyParseException e1) {
-      e1.printStackTrace();
+    }
+    catch (ConnectException e) {
+      throw new SystemException(""The Mpack Uri: "" + mpackRequest.getMpackUri() + "" is not valid. Please try again"", e);","[{'comment': ""Wouldn't a ConnectException also sometimes occur for a valid URI that happens to not be reachable with the given connection?  \r\n\r\nIts not a big deal for now, but maybe we should consider changing the error message here to make the error cause a little clearer.  "", 'commenter': 'rnettleton'}, {'comment': 'Agreed it could be improved. The reason why I made this change is that the original code was swallowing exceptions and I could only see an NPE thrown elsewhere and had to spend time debugging what was actually happening.', 'commenter': 'benyoka'}]"
431,ambari-server/src/main/java/org/apache/ambari/server/api/query/render/ClusterBlueprintRenderer.java,"@@ -198,11 +199,10 @@ private Resource createBlueprintResource(TreeNode<Resource> clusterNode) {
     configProcessor.doUpdateForBlueprintExport();
 
     Set<StackId> stackIds = topology.getBlueprint().getStackIds();
-    if (stackIds.size() == 1) {
-      StackId stackId = Iterables.getOnlyElement(stackIds);
-      blueprintResource.setProperty(""Blueprints/stack_name"", stackId.getStackName());
-      blueprintResource.setProperty(""Blueprints/stack_version"", stackId.getStackVersion());
-    }
+    // TODO: mpacks should come from service groups once https://github.com/apache/ambari/pull/234 will be committed
+    Collection<Map<String, String>> mpackInstances = stackIds.stream().","[{'comment': 'Will this query work for clusters that have been created by the Ambari UI?  ', 'commenter': 'rnettleton'}, {'comment': 'Yes, I am testing it with a cluster that has been created via UI.', 'commenter': 'benyoka'}]"
444,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/Stack.java,"@@ -380,13 +373,12 @@ public String getServiceForComponent(String component) {
   }
 
   @Override
-  public Collection<String> getServicesForComponents(Collection<String> components) {
-    Set<String> services = new HashSet<>();
-    for (String component : components) {
-      services.add(getServiceForComponent(component));
-    }
-
-    return services;
+  @Nonnull
+  public Stream<Pair<StackId, String>> getServicesForComponent(String component) {
+    String service = getServiceForComponent(component);
+    return service != null
+      ? Stream.of(Pair.of(getStackId(), service))
+      : Stream.empty();
   }","[{'comment': ""Why returning a stream if there is only a single instance? Also the method name suggests there can be multiple services. Wouldn't be more straightforward to return an Optional?"", 'commenter': 'benyoka'}, {'comment': 'Yes, there can be multiple services in `CompositeStack`.  The goal is to have a common interface `StackDefinition`.', 'commenter': 'adoroszlai'}, {'comment': 'Ok', 'commenter': 'benyoka'}]"
444,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/Stack.java,"@@ -610,11 +597,7 @@ private void parseExcludedConfigurations(ServiceInfo stackServiceResponse) {
     excludedConfigurationTypes.put(stackServiceResponse.getName(), stackServiceResponse.getExcludedConfigTypes());
   }
 
-  /**
-   * Register conditional dependencies.
-   */
-  //todo: This information should be specified in the stack definition.
-  void registerConditionalDependencies() {
+  private void registerConditionalDependencies() {
     dbDependencyInfo.put(""MYSQL_SERVER"", ""global/hive_database"");
   }","[{'comment': ""I'd prefer keeping the TODO comment. The rest of the comment is useless and can go in my opinion too."", 'commenter': 'benyoka'}, {'comment': 'OK.', 'commenter': 'adoroszlai'}]"
444,ambari-server/src/main/java/org/apache/ambari/server/topology/AmbariContext.java,"@@ -342,37 +349,35 @@ public Object call() throws Exception {
     }
   }
 
-  public void createAmbariServiceAndComponentResources(ClusterTopology topology, String clusterName, Map<StackId, Long> repoVersionByStack) {
-    Set<String> serviceGroups = Sets.newHashSet(DEFAULT_SERVICE_GROUP_NAME);
-    Collection<String> services = topology.getBlueprint().getServices();
+  // FIXME temporarily add default cluster settings -- should be provided by ClusterImpl itself
+  private void addDefaultClusterSettings(String clusterName) throws AmbariException {
+    Cluster cluster = getController().getClusters().getCluster(clusterName);
+    Set<Pair<String, String>> properties = getController().getAmbariMetaInfo().getClusterProperties().stream()
+      .map(p -> Pair.of(p.getName(), p.getValue()))
+      .collect(toSet());
 
-    try {
-      Cluster cluster = getController().getClusters().getCluster(clusterName);
-      serviceGroups.removeAll(cluster.getServiceGroups().keySet());
-      services.removeAll(cluster.getServices().keySet());
-    } catch (AmbariException e) {
-      throw new RuntimeException(""Failed to persist service and component resources: "" + e, e);
+    for (Pair<String, String> p : properties) {
+      cluster.addClusterSetting(p.getKey(), p.getValue());","[{'comment': ""Maybe a comment can be added to indicate the purpose of the double loop (I guess it's duplication removal)."", 'commenter': 'benyoka'}, {'comment': 'Nice catch.  It turns out to be unnecessary, so I changed to single loop.', 'commenter': 'adoroszlai'}]"
444,ambari-server/src/main/java/org/apache/ambari/server/topology/StackComponentResolver.java,"@@ -0,0 +1,144 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.topology;
+
+import static java.util.stream.Collectors.toSet;
+
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.stream.Stream;
+
+import org.apache.ambari.server.controller.internal.StackDefinition;
+import org.apache.ambari.server.state.StackId;
+import org.apache.commons.lang3.tuple.Pair;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.base.Joiner;
+import com.google.common.base.Strings;
+
+/**
+ * Resolves all incompletely specified host group components in the topology:
+ * finds stack and/or service type that each component is defined in.
+ */
+public class StackComponentResolver implements ComponentResolver {
+
+  private static final Logger LOG = LoggerFactory.getLogger(StackComponentResolver.class);
+
+  @Override
+  public Map<String, Set<ResolvedComponent>> resolveComponents(BlueprintBasedClusterProvisionRequest request) {","[{'comment': 'Please document the return value (what are the keys in the map)', 'commenter': 'benyoka'}, {'comment': 'Added comment on the interface.', 'commenter': 'adoroszlai'}]"
444,ambari-server/src/main/java/org/apache/ambari/server/topology/validators/GplPropertiesValidator.java,"@@ -0,0 +1,97 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.topology.validators;
+
+import java.util.Map;
+import java.util.Set;
+
+import javax.inject.Inject;
+
+import org.apache.ambari.server.configuration.Configuration;
+import org.apache.ambari.server.topology.ClusterTopology;
+import org.apache.ambari.server.topology.GPLLicenseNotAcceptedException;
+import org.apache.ambari.server.topology.InvalidTopologyException;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.collect.ImmutableSet;
+
+/**
+ * Properties that indicate usage of GPL software are
+ * allowed with explicit approval from user.
+ */
+public class GplPropertiesValidator implements TopologyValidator {
+
+  private static final Logger LOG = LoggerFactory.getLogger(GplPropertiesValidator.class);
+
+  private static final String CORE_SITE = ""core-site"";
+  static final String LZO_CODEC_CLASS_PROPERTY_NAME = ""io.compression.codec.lzo.class"";
+  static final String CODEC_CLASSES_PROPERTY_NAME = ""io.compression.codecs"";
+  static final String LZO_CODEC_CLASS = ""com.hadoop.compression.lzo.LzoCodec"";
+
+  private static final Set<String> PROPERTY_NAMES = ImmutableSet.of(
+    LZO_CODEC_CLASS_PROPERTY_NAME,
+    CODEC_CLASSES_PROPERTY_NAME
+  );
+
+  private static final String GPL_LICENSE_ERROR_MESSAGE =
+    ""Your Ambari server has not been configured to download LZO GPL software. "" +
+    ""Please refer to documentation to configure Ambari before proceeding."";
+
+  private final Configuration configuration;
+
+  @Inject
+  public GplPropertiesValidator(Configuration configuration) {
+    this.configuration = configuration;
+  }
+
+  @Override
+  public void validate(ClusterTopology topology) throws InvalidTopologyException {
+    // need to reject blueprints that have LZO enabled if the Ambari Server hasn't been configured for it
+    boolean gplEnabled = configuration.getGplLicenseAccepted();
+
+    if (gplEnabled) {
+      LOG.info(""GPL license accepted, skipping config check"");
+      return;
+    }
+
+    // we don't want to include default stack properties so we can't use full properties
+    Map<String, Map<String, String>> clusterConfigurations = topology.getConfiguration().getProperties();
+
+    if (clusterConfigurations != null) {
+      for (Map.Entry<String, Map<String, String>> configEntry : clusterConfigurations.entrySet()) {
+        String configType = configEntry.getKey();
+        if (!CORE_SITE.equals(configType)) {
+          continue;
+        }
+","[{'comment': 'Why iterate and continue if !CORE_SITE instead of getting CORE_SITE from the map?', 'commenter': 'benyoka'}, {'comment': ""Good catch.  This logic was intermixed with another kind of property validation that needed to loop over all configs.  When separating the two validations I didn't notice this one could be simplified."", 'commenter': 'adoroszlai'}]"
444,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/BlueprintConfigurationProcessor.java,"@@ -2883,13 +2905,13 @@ private static void addUnitPropertyUpdaters() {
    * @param stack
    */
   private void addExcludedConfigProperties(Configuration configuration, Set<String> configTypesUpdated, StackDefinition stack) {
-    Collection<String> blueprintServices = clusterTopology.getBlueprint().getServices();
+    Collection<String> blueprintServices = clusterTopology.getServices();
 
-    LOG.debug(""Handling excluded properties for blueprint services: {}"", blueprintServices);
+    LOG.info(""Handling excluded properties for blueprint services: {}"", blueprintServices);","[{'comment': 'It might be better to move these log statements back to debug level.  This may create a lot of noise in the ambari-server.log file.  ', 'commenter': 'rnettleton'}, {'comment': 'Changed back to debug.  I intended this only as a temporary change.  Thanks for noticing.', 'commenter': 'adoroszlai'}]"
444,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/ClusterResourceProvider.java,"@@ -538,12 +538,6 @@ private RequestStatusResponse processBlueprintCreate(Map<String, Object> propert
       throw new IllegalArgumentException(""Invalid Cluster Creation Template: "" + e, e);
     }
 
-    if (securityConfiguration != null && securityConfiguration.getType() == SecurityType.NONE &&","[{'comment': ""Why has this code block been removed?  Won't this break Kerberos deployments?  "", 'commenter': 'rnettleton'}, {'comment': ""It's just moved to:\r\n\r\nhttps://github.com/apache/ambari/blob/32707c74a7ef2e81429bc383c918ace17f434d78/ambari-server/src/main/java/org/apache/ambari/server/topology/BlueprintBasedClusterProvisionRequest.java#L202-L203"", 'commenter': 'adoroszlai'}, {'comment': 'ok, thanks for clarifying. Sorry, must have missed the move in the review.  ', 'commenter': 'rnettleton'}]"
444,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/Stack.java,"@@ -74,12 +77,6 @@
   private Map<String, Collection<DependencyInfo>> dependencies =
     new HashMap<>();
 
-  /**","[{'comment': 'Why is this block being removed? ', 'commenter': 'rnettleton'}]"
444,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/Stack.java,"@@ -422,11 +414,6 @@ static String formatMissingServiceForConfigType(String config, String stackId) {
         Collections.emptySet();
   }
 
-  @Override","[{'comment': 'Can you please provide some information on why the conditional dependency code is being removed with this patch?  ', 'commenter': 'rnettleton'}, {'comment': '`dependencyConditionalServiceMap` was a ""read-only"" map: always empty, because production code only queried it, never put any items into it.  I removed it in a separate commit d5347f6ae2, which can be inspected to verify this claim.', 'commenter': 'adoroszlai'}, {'comment': 'ok, thanks for clarifying this.  ', 'commenter': 'rnettleton'}]"
444,ambari-server/src/main/java/org/apache/ambari/server/topology/ResolvedComponent.java,"@@ -0,0 +1,75 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.topology;
+
+import java.util.Optional;
+
+import org.apache.ambari.server.state.StackId;
+import org.inferred.freebuilder.FreeBuilder;
+
+/**
+ * I provide additional information for a component specified in the blueprint,
+ * based on values resolved from the stack and sensible defaults.
+ */
+@FreeBuilder","[{'comment': 'Can you provide some more information about the usage of FreeBuilder in this context? ', 'commenter': 'rnettleton'}, {'comment': 'FreeBuilder takes an interface and generates 2 classes:\r\n\r\n1. an immutable implementation of the interface with `toString`, `equals` and `hashCode` (less boilerplate code for us to write)\r\n2. a mutable builder class to construct instances of the implementation ([support for collections](https://github.com/inferred/FreeBuilder#collections-and-maps) is especially convenient)\r\n\r\nUpon calling `build()`, the builder verifies that all required properties are initialized.  All properties are required by default.  [Optional properties](https://github.com/inferred/FreeBuilder#optional-values) can be defined as `Optional<T>` (preferred) or `@Nullable`.  [Custom constraints](https://github.com/inferred/FreeBuilder#defaults-and-constraints) can be added as needed by overriding setter methods (for single-property checks) or `build()` (for cross-property checks). \r\n\r\nThe builder can construct [partial objects](https://github.com/inferred/FreeBuilder#partials) for unit tests:\r\n\r\n> when testing a component which does not rely on the full state restrictions of the value type, partials can reduce the fragility of your test suite, allowing you to add new required fields or other constraints to an existing value type without breaking swathes of test code', 'commenter': 'adoroszlai'}, {'comment': 'Thanks for the info on FreeBuilder.  It does look promising for simpler object definition and maintenance, and the partial object support does look helpful for unit test maintenance. \r\n\r\nPerhaps we should consider using this across Blueprints, and eventually across Ambari as well.  ', 'commenter': 'rnettleton'}, {'comment': 'Sure.  Using it for new objects is a straightforward choice.  Refactoring existing ones can be done as time permits.', 'commenter': 'adoroszlai'}]"
476,ambari-server/src/main/java/org/apache/ambari/server/actionmanager/Request.java,"@@ -96,6 +98,7 @@ public Request(@Assisted long requestId, @Assisted(""clusterId"") Long clusterId,
     this.endTime = -1;
     this.exclusive = false;
     this.clusterHostInfo = ""{}"";
+    this.userName = AuthorizationHelper.getAuthenticatedName();","[{'comment': 'Since this is a part of the constructor, care should be taken to allow for nulls for a background operation lets say that is not initiated by a user.', 'commenter': 'swagle'}, {'comment': 'AuthorizationHelper.getAuthenticatedName() returned ""internal"" in such cases, as far as I could see. \r\nShould I just allow nulls?\r\nShould I move the AuthorizationHelper.getAuthenticatedName() call out of the constructor? ', 'commenter': 'majorendre'}, {'comment': 'Oh, you were quick to get to this while I was still reviewing :-)\r\nI think nulls are ok, maybe other reviewers can chime in.', 'commenter': 'swagle'}]"
476,ambari-server/src/main/java/org/apache/ambari/server/orm/entities/RequestEntity.java,"@@ -125,6 +125,9 @@
   @Column(name = ""exclusive_execution"", insertable = true, updatable = true, nullable = false)
   private Integer exclusive = 0;
 
+  @Column(name = ""user_name"", nullable = false)","[{'comment': 'We do set user of a BatchRequest as ""internal"" so this non-nullable might be ok but make sure to check other scenarios for this, IMO enforcing this as being non-nullable should not be needed.', 'commenter': 'swagle'}, {'comment': ""Thanks. I'll modify it tomorrow. "", 'commenter': 'majorendre'}]"
476,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/RequestStageContainer.java,"@@ -216,6 +217,10 @@ public void persist() throws AmbariException {
         request.setRequestContext(requestContext);
       }
 
+      if (request != null) { //request can be null at least in JUnit
+        request.setUserName(AuthorizationHelper.getAuthenticatedName());","[{'comment': '```\r\n[ERROR]   RequestStageContainerTest.testPersist:152\r\n  Unexpected method call Request.setUserName(null)\r\n```', 'commenter': 'adoroszlai'}]"
476,ambari-server/src/main/java/org/apache/ambari/server/actionmanager/Request.java,"@@ -422,6 +424,14 @@ public void setExclusive(boolean isExclusive) {
     exclusive = isExclusive;
   }
 
+  public String getUserName() {
+    return userName;
+  }
+
+  public void setUserName(String userName) {
+    this.userName = userName;
+  }
+","[{'comment': 'Javadoc.', 'commenter': 'jonathan-hurley'}]"
476,ambari-server/src/main/java/org/apache/ambari/server/orm/entities/RequestEntity.java,"@@ -299,6 +302,14 @@ public void setRequestScheduleId(Long scheduleId) {
     this.requestScheduleId = scheduleId;
   }
 
+  public String getUserName() {
+    return userName;
+  }
+
+  public void setUserName(String userName) {
+    this.userName = userName;
+  }
+","[{'comment': 'Javadoc.', 'commenter': 'jonathan-hurley'}]"
476,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/RequestResourceProvider.java,"@@ -851,6 +853,7 @@ private Resource getRequestResource(RequestEntity entity, String clusterName,
             hostRoleStatusCounters.get(HostRoleStatus.QUEUED), requestedPropertyIds);
     setResourceProperty(resource, REQUEST_COMPLETED_TASK_CNT_ID,
             hostRoleStatusCounters.get(HostRoleStatus.COMPLETED), requestedPropertyIds);
+    setResourceProperty(resource, REQUEST_USER_NAME, entity.getUserName(), requestedPropertyIds);","[{'comment': 'UI should make sure to handle null as something like Unknown user for readability.', 'commenter': 'swagle'}]"
477,ambari-server/src/main/java/org/apache/ambari/server/api/services/HostComponentService.java,"@@ -144,18 +144,18 @@ public Response createHostComponents(String body, @Context HttpHeaders headers,
    * @param body              http body
    * @param headers           http headers
    * @param ui                uri info
-   * @param hostComponentName host_component id
+   * @param hostComponentId   host_component id
    *
    * @return host_component resource representation
    */
   @POST @ApiIgnore // until documented
-  @Path(""{hostComponentName}"")
+  @Path(""{hostComponentId}"")
   @Produces(""text/plain"")
   public Response createHostComponent(String body, @Context HttpHeaders headers, @Context UriInfo ui,
-                                   @PathParam(""hostComponentName"") String hostComponentName) {
+                                   @PathParam(""hostComponentId"") String hostComponentId) {","[{'comment': 'Hi,\r\nFor POST call we will not have hostComponentId in the PathParam since it has not been created yet.', 'commenter': 'mradha25'}, {'comment': 'Removed this POST call.', 'commenter': 'swapanshridhar'}]"
477,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/HostComponentResourceProvider.java,"@@ -677,6 +684,8 @@ protected RequestStageContainer updateHostComponents(RequestStageContainer stage
             + "", clusterId="" + cluster.getClusterId()
             + "", serviceName="" + sch.getServiceName()
             + "", componentName="" + sch.getServiceComponentName()
+            + "", componentType="" + sch.getServiceComponentType()
+            + "", componentType="" + sch.getServiceComponentType()","[{'comment': 'Duplicate statement', 'commenter': 'mradha25'}, {'comment': 'Removed.', 'commenter': 'swapanshridhar'}]"
477,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/HostComponentResourceProvider.java,"@@ -722,13 +731,33 @@ protected RequestStageContainer updateHostComponents(RequestStageContainer stage
    * @return the component request object
    */
   private ServiceComponentHostRequest getRequest(Map<String, Object> properties) {
-    ServiceComponentHostRequest serviceComponentHostRequest = new ServiceComponentHostRequest(
-        (String) properties.get(HOST_COMPONENT_CLUSTER_NAME_PROPERTY_ID),
-        (String) properties.get(HOST_COMPONENT_SERVICE_GROUP_NAME_PROPERTY_ID),
-        (String) properties.get(HOST_COMPONENT_SERVICE_NAME_PROPERTY_ID),
-        (String) properties.get(HOST_COMPONENT_COMPONENT_NAME_PROPERTY_ID),
-        (String) properties.get(HOST_COMPONENT_HOST_NAME_PROPERTY_ID),
-        (String) properties.get(HOST_COMPONENT_DESIRED_STATE_PROPERTY_ID));
+    ServiceComponentHostRequest serviceComponentHostRequest = null;
+    if (properties.get(HOST_COMPONENT_HOST_COMPONENT_ID_PROPERTY_ID) != null) {
+      Long hostComponentId = properties.get(HOST_COMPONENT_HOST_COMPONENT_ID_PROPERTY_ID) instanceof String ?
+              Long.parseLong((String) properties.get(HOST_COMPONENT_HOST_COMPONENT_ID_PROPERTY_ID)) :","[{'comment': 'Here we are checking if its an instance of String, and then casting it to String and then using parseLong. If we have already checked that its an instance of String then we should be able to simply do \r\nLong.parseLong(properties.get(HOST_COMPONENT_HOST_COMPONENT_ID_PROPERTY_ID)).\r\nMight be simpler to just use : \r\nLong hostComponentId = Long.valueOf((String) properties.get(HOST_COMPONENT_HOST_COMPONENT_ID_PROPERTY_ID)\r\nparseLong returns a primitive type so you can choose to use valueOf to get an object of Long type.', 'commenter': 'mradha25'}]"
477,ambari-server/src/main/java/org/apache/ambari/server/orm/dao/ServiceComponentDesiredStateDAO.java,"@@ -92,6 +92,41 @@ public ServiceComponentDesiredStateEntity findByName(long clusterId, long servic
     query.setParameter(""serviceGroupId"", serviceGroupId);
     query.setParameter(""serviceId"", serviceId);
     query.setParameter(""componentName"", componentName);
+    query.setParameter(""componentType"", componentType);
+
+    ServiceComponentDesiredStateEntity entity = null;
+    List<ServiceComponentDesiredStateEntity> entities = daoUtils.selectList(query);
+    if (null != entities && !entities.isEmpty()) {
+      entity = entities.get(0);
+    }
+
+    return entity;
+  }
+
+  /**
+   * Finds a {@link ServiceComponentDesiredStateEntity} by a combination of
+   * cluster, service, and component.
+   *
+   * @param clusterId
+   *          the cluster ID
+   * @param serviceGroupId
+   *          the service group ID
+   * @param serviceId
+   *          the service ID
+   * @param componentId
+   *          the component id (not {@code null})
+   */
+  @RequiresSession
+  public ServiceComponentDesiredStateEntity findById(long clusterId, long serviceGroupId, long serviceId,
+                                                       Long componentId) {
+    EntityManager entityManager = entityManagerProvider.get();
+    TypedQuery<ServiceComponentDesiredStateEntity> query = entityManager.createNamedQuery(
+            ""ServiceComponentDesiredStateEntity.findById"", ServiceComponentDesiredStateEntity.class);
+
+    query.setParameter(""clusterId"", clusterId);
+    query.setParameter(""serviceGroupId"", serviceGroupId);
+    query.setParameter(""serviceId"", serviceId);
+    query.setParameter(""id"", componentId);","[{'comment': 'Do we need clusterId, serviceGroupId, serviceId in this case?', 'commenter': 'mradha25'}, {'comment': 'Fixed it.', 'commenter': 'swapanshridhar'}]"
477,ambari-server/src/main/java/org/apache/ambari/server/orm/entities/ServiceComponentDesiredStateEntity.java,"@@ -62,7 +63,15 @@
     query = ""SELECT scds FROM ServiceComponentDesiredStateEntity scds WHERE scds.clusterId = :clusterId "" +
       ""AND scds.serviceGroupId = :serviceGroupId "" +
       ""AND scds.serviceId = :serviceId "" +
-      ""AND scds.componentName = :componentName"") })
+      ""AND scds.componentName = :componentName "" +
+      ""AND scds.componentType = :componentType"" ),
+  @NamedQuery(
+    name = ""ServiceComponentDesiredStateEntity.findById"",","[{'comment': 'Id will already be unique. Do we need the clusterId, serviceGroupId and serviceId?', 'commenter': 'mradha25'}, {'comment': 'Fixed it. ', 'commenter': 'swapanshridhar'}]"
477,ambari-server/src/main/java/org/apache/ambari/server/state/cluster/ClusterImpl.java,"@@ -1187,6 +1187,58 @@ public Service getServiceByComponentName(String componentName) throws AmbariExce
     throw new ServiceNotFoundException(getClusterName(), ""component: "" + componentName);
   }
 
+  @Override
+  public Service getServiceByComponentId(Long componentId) throws AmbariException {
+    for (Service service : services.values()) {
+      for (ServiceComponent component : service.getServiceComponents().values()) {
+        if (component.getId().equals(componentId)) {
+          return service;
+        }
+      }
+    }
+
+    throw new ServiceNotFoundException(getClusterName(), ""component Id: "" + componentId);
+  }
+
+  @Override
+  public String getComponentName(Long componentId) throws AmbariException {
+    for (Service service : services.values()) {
+      for (ServiceComponent component : service.getServiceComponents().values()) {
+        if (component.getId() == componentId) {","[{'comment': 'use .equals', 'commenter': 'mradha25'}, {'comment': 'Done.', 'commenter': 'swapanshridhar'}]"
477,ambari-server/src/main/java/org/apache/ambari/server/state/cluster/ClusterImpl.java,"@@ -1187,6 +1187,58 @@ public Service getServiceByComponentName(String componentName) throws AmbariExce
     throw new ServiceNotFoundException(getClusterName(), ""component: "" + componentName);
   }
 
+  @Override
+  public Service getServiceByComponentId(Long componentId) throws AmbariException {
+    for (Service service : services.values()) {
+      for (ServiceComponent component : service.getServiceComponents().values()) {
+        if (component.getId().equals(componentId)) {
+          return service;
+        }
+      }
+    }
+
+    throw new ServiceNotFoundException(getClusterName(), ""component Id: "" + componentId);
+  }
+
+  @Override
+  public String getComponentName(Long componentId) throws AmbariException {
+    for (Service service : services.values()) {
+      for (ServiceComponent component : service.getServiceComponents().values()) {
+        if (component.getId() == componentId) {
+          return component.getName();
+        }
+      }
+    }
+
+    throw new ServiceNotFoundException(getClusterName(), ""component Id: "" + componentId);
+  }
+
+
+  @Override
+  public String getComponentType(Long componentId) throws AmbariException {
+    for (Service service : services.values()) {
+      for (ServiceComponent component : service.getServiceComponents().values()) {
+        if (component.getId() == componentId) {","[{'comment': 'use .equals', 'commenter': 'mradha25'}, {'comment': 'Done.', 'commenter': 'swapanshridhar'}]"
477,ambari-server/src/main/java/org/apache/ambari/server/ServiceComponentNotFoundException.java,"@@ -32,4 +32,14 @@ public ServiceComponentNotFoundException (String clusterName,
         + "", serviceComponentName="" + serviceComponentName);
   }
 
+  public ServiceComponentNotFoundException (String clusterName,
+      String serviceName, String serviceType, String serviceGroupName, Long serviceComponentId) {
+    super(""ServiceComponent not found""
+            + "", clusterName="" + clusterName
+            + "", serviceGroupName="" + serviceGroupName
+            + "", serviceName="" + serviceName
+            + "", serviceType="" + serviceType
+            + "", serviceComponentId="" + serviceComponentId);
+  }
+
 }","[{'comment': 'Where is serviceComponentId stored in the DB?', 'commenter': 'benyoka'}, {'comment': 'It should come from *servicecomponentdesiredstate* table.', 'commenter': 'swapanshridhar'}]"
477,ambari-server/src/main/java/org/apache/ambari/server/controller/metrics/MetricsCollectorHAClusterState.java,"@@ -60,11 +61,23 @@ public MetricsCollectorHAClusterState(String clusterName) {
     this.liveCollectorHosts = new CopyOnWriteArraySet<>();
     this.deadCollectorHosts = new CopyOnWriteArraySet<>();
     collectorDownRefreshCounter = new AtomicInteger(0);
+
+  }
+
+  private Long getComponentId(String componentName) {
+    Long componentId = null;
+    try {
+      componentId = managementController.getClusters().getCluster(clusterName).getComponentId(componentName);
+    } catch (AmbariException e) {
+      e.printStackTrace();","[{'comment': 'Can we log the exception instead of printStackTrace ? ', 'commenter': 'avijayanhwx'}]"
477,ambari-server/src/main/java/org/apache/ambari/server/api/services/RootClusterSettingService.java,"@@ -70,7 +70,7 @@ public RootClusterSettingService() {
           response = ReadOnlyConfigurationResponse.ReadOnlyConfigurationResponseSwagger.class, responseContainer = RESPONSE_CONTAINER_LIST)
   @ApiImplicitParams({
           @ApiImplicitParam(name = QUERY_FIELDS, value = QUERY_FILTER_DESCRIPTION, dataType = DATA_TYPE_STRING,
-                  paramType = PARAM_TYPE_QUERY, defaultValue = MpackResourceProvider.MPACK_RESOURCE_ID),
+                  paramType = PARAM_TYPE_QUERY, defaultValue = MpackResourceProvider.MPACK_ID),","[{'comment': 'You can remove this line completely.', 'commenter': 'mradha25'}]"
477,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/HostComponentResourceProvider.java,"@@ -722,13 +730,23 @@ protected RequestStageContainer updateHostComponents(RequestStageContainer stage
    * @return the component request object
    */
   private ServiceComponentHostRequest getRequest(Map<String, Object> properties) {
-    ServiceComponentHostRequest serviceComponentHostRequest = new ServiceComponentHostRequest(
-        (String) properties.get(HOST_COMPONENT_CLUSTER_NAME_PROPERTY_ID),
-        (String) properties.get(HOST_COMPONENT_SERVICE_GROUP_NAME_PROPERTY_ID),
-        (String) properties.get(HOST_COMPONENT_SERVICE_NAME_PROPERTY_ID),
-        (String) properties.get(HOST_COMPONENT_COMPONENT_NAME_PROPERTY_ID),
-        (String) properties.get(HOST_COMPONENT_HOST_NAME_PROPERTY_ID),
-        (String) properties.get(HOST_COMPONENT_DESIRED_STATE_PROPERTY_ID));
+    ServiceComponentHostRequest serviceComponentHostRequest = null;
+    if (properties.get(HOST_COMPONENT_HOST_COMPONENT_ID_PROPERTY_ID) != null) {
+      Long hostComponentId = properties.get(HOST_COMPONENT_HOST_COMPONENT_ID_PROPERTY_ID) instanceof String ?
+              Long.parseLong((String) properties.get(HOST_COMPONENT_HOST_COMPONENT_ID_PROPERTY_ID)) :
+              (Long) properties.get(HOST_COMPONENT_HOST_COMPONENT_ID_PROPERTY_ID);
+
+      serviceComponentHostRequest = new ServiceComponentHostRequest(
+              (String) properties.get(HOST_COMPONENT_CLUSTER_NAME_PROPERTY_ID),
+              (String) properties.get(HOST_COMPONENT_SERVICE_GROUP_NAME_PROPERTY_ID),
+              (String) properties.get(HOST_COMPONENT_SERVICE_NAME_PROPERTY_ID),
+              hostComponentId,
+              (String) properties.get(HOST_COMPONENT_COMPONENT_NAME_PROPERTY_ID),
+              (String) properties.get(HOST_COMPONENT_COMPONENT_TYPE_PROPERTY_ID),
+              (String) properties.get(HOST_COMPONENT_HOST_NAME_PROPERTY_ID),
+              (String) properties.get(HOST_COMPONENT_DESIRED_STATE_PROPERTY_ID));
+    }
+
     serviceComponentHostRequest.setState((String) properties.get(HOST_COMPONENT_STATE_PROPERTY_ID));","[{'comment': 'This results in NPE if `HOST_COMPONENT_HOST_COMPONENT_ID_PROPERTY_ID` is not present.', 'commenter': 'adoroszlai'}, {'comment': 'Fixed in https://github.com/apache/ambari/pull/566 and AMBARI-23158', 'commenter': 'swapanshridhar'}]"
485,ambari-server/src/test/java/org/apache/ambari/server/topology/ClusterTopologyImplTest.java,"@@ -219,6 +223,24 @@ public void testCreate_NNHAHostNameNotCorrectForStandbyWithActiveAsVariable() th
     new ClusterTopologyImpl(null, request);
   }
 
+  @Test
+  public void testHasHadoopCompatibleFsWhenThereIsHCFS() throws Exception {
+    ServiceInfo hcfs = new ServiceInfo();
+    hcfs.setServiceType(ServiceInfo.HADOOP_COMPATIBLE_FS);
+    expect(blueprint.getServiceInfos()).andReturn(Arrays.asList(hcfs)).anyTimes();
+    replayAll();
+    TestTopologyRequest request = new TestTopologyRequest(TopologyRequest.Type.PROVISION);
+    assertTrue(new ClusterTopologyImpl(null, request).hasHadoopCompatibleFileSystem());
+  }
+
+  @Test
+  public void testHasHadoopCompatibleFsWhenThereIsNoHCFS() throws Exception {","[{'comment': 'I guess this should be `testDoesNotHave...`', 'commenter': 'adoroszlai'}]"
485,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/BlueprintConfigurationProcessor.java,"@@ -1542,6 +1542,10 @@ public String updateForClusterCreate(String propertyName,
                 return origValue;
             }
 
+            if (topology.hasHadoopCompatibleFileSystem()) {
+              return origValue;","[{'comment': 'I think this condition should also include a check that the component being processed is one from OneFS.  Otherwise it will fail to throw `IllegalArgumentException` for other components when it should.\r\n\r\nTo verify this, make the following change in `BlueprintConfigurationProcessorTest` and run it:\r\n\r\n```diff\r\n@@ -8244,7 +8244,11 @@ public class BlueprintConfigurationProcessorTest extends EasyMockSupport {\r\n\r\n     replay(bp, topologyRequestMock);\r\n\r\n-    ClusterTopology topology = new ClusterTopologyImpl(ambariContext, topologyRequestMock);\r\n+    ClusterTopology topology = new ClusterTopologyImpl(ambariContext, topologyRequestMock) {\r\n+      public boolean hasHadoopCompatibleFileSystem() {\r\n+        return true;\r\n+      }\r\n+    };\r\n     topology.setConfigRecommendationStrategy(ConfigRecommendationStrategy.NEVER_APPLY);\r\n\r\n     return topology;\r\n```\r\n\r\nResult:\r\n\r\n```\r\n[ERROR]   BlueprintConfigurationProcessorTest.testDoUpdateForClusterCreate_SingleHostProperty__MissingComponent:2344 IllegalArgumentException should have been thrown\r\n[ERROR]   BlueprintConfigurationProcessorTest.testDoUpdateForClusterCreate_SingleHostProperty__MultipleMatchingHostGroupsError:2386 IllegalArgumentException should have been thrown\r\n```', 'commenter': 'adoroszlai'}, {'comment': 'Nice catch.', 'commenter': 'zeroflag'}]"
485,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/Stack.java,"@@ -259,6 +261,14 @@ public ComponentInfo getComponentInfo(String component) {
     return componentInfo;
   }
 
+  public Optional<ServiceInfo> getServiceInfo(String serviceName) {","[{'comment': 'Javadoc.', 'commenter': 'jonathan-hurley'}]"
494,ambari-common/src/main/python/resource_management/libraries/functions/security_commons.py,"@@ -57,7 +57,7 @@ def update_credential_provider_path(config, config_type, dest_provider_path, fil
     # make a copy of the config dictionary since it is read-only
     config_copy = config.copy()
     # overwrite the provider path with the path specified
-    config_copy[HADOOP_CREDENTIAL_PROVIDER_PROPERTY_NAME] = 'jceks://file{0}'.format(dest_provider_path)
+    config_copy[HADOOP_CREDENTIAL_PROVIDER_PROPERTY_NAME] = 'localjceks://file{0}'.format(dest_provider_path)","[{'comment': '`jceks://` is expected in several places.  This seems like an incomplete solution. ', 'commenter': 'rlevas'}]"
495,ambari-server/src/test/resources/stacks/HDP/0.1/services/PIG/metainfo.xml,"@@ -34,6 +34,16 @@
             <timeout>600</timeout>
           </commandScript>
         </component>
+        <component>
+          <name>SOME_CLIENT_FOR_SERVICE_CHECK</name>","[{'comment': 'This extra component should be accounted for in this assertion:\r\n\r\nhttps://github.com/apache/ambari/blob/04110f721437520eed50b1023e6ddda617a272d1/ambari-server/src/test/java/org/apache/ambari/server/stack/StackManagerTest.java#L246', 'commenter': 'adoroszlai'}, {'comment': 'Thanks for noticing', 'commenter': 'd0zen1'}]"
503,ambari-server/src/main/resources/common-services/ZEPPELIN/0.7.0/configuration/zeppelin-config.xml,"@@ -77,7 +77,7 @@
   </property>
   <property>
     <name>zeppelin.notebook.storage</name>
-    <value>org.apache.zeppelin.notebook.repo.FileSystemNotebookRepo</value>
+    <value>org.apache.zeppelin.notebook.repo.VFSNotebookRepo</value>","[{'comment': 'otherwise upgrade to HDP-2.6.1/2.6.2 would overwrite the value.\r\nDuring install, stack advisor will provide the right value (other jira by @vbrodetskyi )\r\n', 'commenter': 'Unknown'}]"
503,ambari-server/src/main/java/org/apache/ambari/server/serveraction/upgrades/FixNotebookStorage.java,"@@ -0,0 +1,83 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.serveraction.upgrades;
+
+import java.util.Map;
+import java.util.concurrent.ConcurrentMap;
+
+import org.apache.ambari.server.AmbariException;
+import org.apache.ambari.server.actionmanager.HostRoleStatus;
+import org.apache.ambari.server.agent.CommandReport;
+import org.apache.ambari.server.orm.entities.RepositoryVersionEntity;
+import org.apache.ambari.server.orm.entities.UpgradeEntity;
+import org.apache.ambari.server.serveraction.AbstractServerAction;
+import org.apache.ambari.server.state.Cluster;
+import org.apache.ambari.server.state.Clusters;
+import org.apache.ambari.server.state.Config;
+import org.apache.ambari.server.utils.VersionUtils;
+
+import com.google.inject.Inject;
+
+/**
+ * During stack upgrade,
+ * update zeppelin.notebook.storage
+ * to org.apache.zeppelin.notebook.repo.FileSystemNotebookRepo
+ * if stack HDP >=2.6.3 and old value org.apache.zeppelin.notebook.repo.VFSNotebookRepo
+ */
+public class FixNotebookStorage extends AbstractServerAction {
+
+  public static final String ZEPPELIN_NOTEBOOK_STORAGE = ""zeppelin.notebook.storage"";
+  public static final String ZEPPELIN_CONF = ""zeppelin-config"";
+  public static final String ORG_APACHE_ZEPPELIN_NOTEBOOK_REPO_VFSNOTEBOOK_REPO = ""org.apache.zeppelin.notebook.repo.VFSNotebookRepo"";
+  public static final String REC_VERSION = ""2.6.3.0"";
+  public static final String ORG_APACHE_ZEPPELIN_NOTEBOOK_REPO_FILE_SYSTEM_NOTEBOOK_REPO = ""org.apache.zeppelin.notebook.repo.FileSystemNotebookRepo"";
+
+
+  @Inject
+  private Clusters clusters;
+
+  @Override
+  public CommandReport execute(ConcurrentMap<String, Object> requestSharedDataContext)
+      throws AmbariException, InterruptedException {
+    String clusterName = getExecutionCommand().getClusterName();
+    Cluster cluster = clusters.getCluster(clusterName);
+    UpgradeEntity upgrade = cluster.getUpgradeInProgress();
+    RepositoryVersionEntity repositoryVersionEntity = upgrade.getRepositoryVersion();
+    String targetVersion = repositoryVersionEntity.getVersion();
+
+    Config config = cluster.getDesiredConfigByType(ZEPPELIN_CONF);
+    if (config == null || VersionUtils.compareVersions(targetVersion, REC_VERSION) < 0) {
+      return createCommandReport(0, HostRoleStatus.COMPLETED, ""{}"",
+          ""zeppelin.notebook.storage change not required"", """");
+    }
+    Map<String, String> properties = config.getProperties();
+    String oldContent = properties.get(ZEPPELIN_NOTEBOOK_STORAGE);
+    String newContent = ORG_APACHE_ZEPPELIN_NOTEBOOK_REPO_FILE_SYSTEM_NOTEBOOK_REPO;
+
+    if (ORG_APACHE_ZEPPELIN_NOTEBOOK_REPO_VFSNOTEBOOK_REPO.equals(oldContent)) {
+
+      properties.put(ZEPPELIN_NOTEBOOK_STORAGE, newContent);
+      config.setProperties(properties);
+      config.save();
+      return createCommandReport(0, HostRoleStatus.COMPLETED, ""{}"", ""set zeppelin.notebook.storage to org.apache.zeppelin.notebook.repo.FileSystemNotebookRepo"", """");","[{'comment': 'why do we hard code property names here if we have constant defined? should we use here formatted string instead', 'commenter': 'hapylestat'}]"
530,ambari-server/src/main/java/org/apache/ambari/server/api/rest/BootStrapResource.java,"@@ -66,13 +67,15 @@ public static void init(BootStrapImpl instance) {
    * @throws Exception
    */
   @POST @ApiIgnore // until documented
+  @Path(""/{validations}"")
   @Consumes(MediaType.APPLICATION_JSON)
   @Produces({MediaType.APPLICATION_JSON, MediaType.APPLICATION_XML})
-  public BSResponse bootStrap(SshHostInfo sshInfo, @Context UriInfo uriInfo) {
+  public BSResponse bootStrap(SshHostInfo sshInfo, @PathParam(""validations"") String validations,
+                              @Context UriInfo uriInfo) {
     
     normalizeHosts(sshInfo);
 
-    BSResponse resp = bsImpl.runBootStrap(sshInfo);
+    BSResponse resp = bsImpl.runBootStrap(sshInfo, validations == null ? false : true);","[{'comment': 'Simplify:\r\n\r\n```\r\nvalidations == null ? false : true\r\n```\r\n\r\nto:\r\n\r\n```\r\nvalidations != null\r\n```', 'commenter': 'adoroszlai'}]"
530,ambari-server/src/main/java/org/apache/ambari/server/bootstrap/BSHostStatusCollector.java,"@@ -98,6 +98,10 @@ public void run() {
           while (null != (line = reader.readLine())) {
             if (line.startsWith(""tcgetattr:"") || line.startsWith(""tput:""))
               continue;
+            if (line.startsWith(""ERROR MESSAGE:"") && !status.getStatusCode().equals(""0"")) {
+              // Remove ""ERROR MESSAGE:"" string
+              status.setError(line.substring(line.indexOf(""ERROR MESSAGE:"")+(new String(""ERROR MESSAGE: "")).length()));","[{'comment': '1. The string `""ERROR MESSAGE: ""` and its length do not change, so they should be constants.\r\n2. No need to find the string using `line.indexOf(...)` after checking that `line.startsWith(...)` it.', 'commenter': 'adoroszlai'}, {'comment': 'Good points.', 'commenter': 'scottduan'}, {'comment': 'At this point I believe exit codes are pretty much random selected for meaning.  Should we identify ranges of WARNING vs ERROR and not check text directly?', 'commenter': 'ncole'}]"
530,ambari-server/src/main/java/org/apache/ambari/server/bootstrap/BSRunner.java,"@@ -261,7 +264,8 @@ public void run() {
           requestIdDir + "" user="" + user + "" sshPort="" + sshPort + "" keyfile="" + this.sshKeyFile +
           "" passwordFile "" + this.passwordFile + "" server="" + this.ambariHostname +
           "" version="" + projectVersion + "" serverPort="" + this.serverPort + "" userRunAs="" + userRunAs +
-          "" timeout="" + bootstrapTimeout / 1000);
+          "" timeout="" + bootstrapTimeout / 1000 +
+          "" validation="" + String.valueOf(this.validationInstance));","[{'comment': 'No need for `String.valueOf()` when concatenating.', 'commenter': 'adoroszlai'}]"
530,ambari-server/src/main/java/org/apache/ambari/server/bootstrap/BSRunner.java,"@@ -282,17 +286,17 @@ public void run() {
 
       Process process = pb.start();
 
+      StringBuilder logInfoMessage = new StringBuilder(validationInstance ? ""Validation"" : ""Bootstrap"");","[{'comment': 'Please extract `validationInstance ? ""Validation"" : ""Bootstrap""` into a method, and reuse it below for `scriptLog`.', 'commenter': 'adoroszlai'}]"
530,ambari-server/src/main/python/bootstrap.py,"@@ -888,6 +921,10 @@ def main(argv=None):
   server_port = onlyargs[9]
   user_run_as = onlyargs[10]
   passwordFile = onlyargs[11]
+  if onlyargs[12]:
+    validate = True if onlyargs[12].lower() == ""true"" else False
+  else:
+    validate = False","[{'comment': 'Simplify to: `validate = onlyargs[12] and onlyargs[12].lower() == ""true""`', 'commenter': 'adoroszlai'}]"
530,ambari-server/src/test/java/org/apache/ambari/server/bootstrap/BootStrapTest.java,"@@ -172,7 +172,7 @@ public void testHostFailure() throws Exception {
     info.setUser(""user"");
     info.setUserRunAs(""root"");
     info.setPassword(""passwd"");
-    BSResponse response = impl.runBootStrap(info);
+    BSResponse response = impl.runBootStrap(info, false);","[{'comment': 'Can you please also add test case for validation-type bootstrap?', 'commenter': 'adoroszlai'}, {'comment': 'Add two unit tests, one for BootstrapResource.java and another for TestBootstrap.py', 'commenter': 'scottduan'}]"
530,ambari-server/src/main/java/org/apache/ambari/server/api/rest/BootStrapResource.java,"@@ -66,13 +67,15 @@ public static void init(BootStrapImpl instance) {
    * @throws Exception
    */
   @POST @ApiIgnore // until documented
+  @Path(""/{validations}"")","[{'comment': 'This would mean even a call such as POST /api/v1/bootstrap/SomeRandomString would run validations which is a side effect. \r\n\r\nAlso the general practice we have followed is to call such APIs as dry_run. For example do a dry run for deleting a host (https://issues.apache.org/jira/browse/AMBARI-16145) or do a dry run for VDF https://reviews.apache.org/r/46100/. \r\n\r\nI would recommend to make the API as follows\r\n\r\nPOST /api/v1/bootstrap \r\n{\r\n  ""verbose"" : true,\r\n  ""dryRun"" : true,\r\n  ""sshKey"" : ""sshKey"",\r\n  ""user"" : ""root"",\r\n ""sshPort"" : ""22"",\r\n""userRunAs"" : ""root"",\r\n""hosts"" : [""host1"", ""host2"", ""host3""]\r\n}\r\n\r\nTo maintain backward compatibility, dryRun should be an optional parameter and if not set should default to dryRun = false. ', 'commenter': 'jayush'}, {'comment': 'actually in my next patch, it will be:\r\n\r\n/**\r\n   * Run bootstrap on a list of hosts.\r\n   * @response.representation.200.doc\r\n   *\r\n   * @response.representation.200.mediaType application/json\r\n   * @response.representation.406.doc Error in format\r\n   * @response.representation.408.doc Request Timed out\r\n   * @throws Exception\r\n   */\r\n  @POST @ApiIgnore // until documented\r\n  @Consumes(MediaType.APPLICATION_JSON)\r\n  @Produces({MediaType.APPLICATION_JSON, MediaType.APPLICATION_XML})\r\n  public BSResponse bootStrap(SshHostInfo sshInfo, @PathParam(""validations"") String validations,\r\n                              @Context UriInfo uriInfo) {\r\n    \r\n    normalizeHosts(sshInfo);\r\n\r\n    BSResponse resp = bsImpl.runBootStrap(sshInfo, false);\r\n\r\n    return resp;\r\n  }\r\n  /**\r\n   * Run bootstrap on a list of hosts.\r\n   * @response.representation.200.doc\r\n   *\r\n   * @response.representation.200.mediaType application/json\r\n   * @response.representation.406.doc Error in format\r\n   * @response.representation.408.doc Request Timed out\r\n   * @throws Exception\r\n   */\r\n  @POST @ApiIgnore // until documented\r\n  @Path(""/validations"")\r\n  @Consumes(MediaType.APPLICATION_JSON)\r\n  @Produces({MediaType.APPLICATION_JSON, MediaType.APPLICATION_XML})\r\n  public BSResponse bootStrap(SshHostInfo sshInfo,\r\n                              @Context UriInfo uriInfo) {\r\n\r\n    normalizeHosts(sshInfo);\r\n\r\n    BSResponse resp = bsImpl.runBootStrap(sshInfo, true);\r\n\r\n    return resp;\r\n  }\r\n\r\nThe reason is that jax-rs seems not to support optional path param very well and it will redirect to GET api/v1/bootstrap. So I decide to seprate these two requests.', 'commenter': 'scottduan'}, {'comment': 'the new patch should have removed side effect of request path.', 'commenter': 'scottduan'}]"
530,ambari-server/src/main/java/org/apache/ambari/server/bootstrap/BSHostStatusCollector.java,"@@ -100,7 +100,7 @@ public void run() {
               continue;
             if (line.startsWith(""ERROR MESSAGE:"") && !status.getStatusCode().equals(""0"")) {
               // Remove ""ERROR MESSAGE:"" string
-              status.setError(line.substring(line.indexOf(""ERROR MESSAGE:"")+(new String(""ERROR MESSAGE: "")).length()));
+              status.setError(line.substring((new String(""ERROR MESSAGE: "")).length()));","[{'comment': 'This change addresses only one of the problems with this part (unnecessary `indexOf()` call).\r\n\r\nThe remaining problems in decreasing order of importance:\r\n\r\n * unnecessary creation of a `new String` instead of using literal `""ERROR MESSAGE: ""`\r\n * repeating the string `ERROR MESSAGE:` (with the minor variation of an added space) instead of using a constant\r\n * calculating the length of a fixed string instead of storing it in a constant', 'commenter': 'adoroszlai'}, {'comment': 'I use its length directly:\r\n\r\n// Remove ""ERROR MESSAGE: "" string whose length is 15\r\nstatus.setError(line.substring(15));', 'commenter': 'scottduan'}]"
530,ambari-server/src/main/java/org/apache/ambari/server/bootstrap/BSRunner.java,"@@ -179,6 +179,10 @@ private long calculateBSTimeout(int hostCount) {
     return Math.max(HOST_BS_TIMEOUT, HOST_BS_TIMEOUT * hostCount / PARALLEL_BS_COUNT);
   }
 
+  private String isValidateHostOperation() {
+    return validationInstance ? ""Validation"" : ""Bootstrap"";","[{'comment': 'Name suggests the method returns `boolean`.', 'commenter': 'adoroszlai'}, {'comment': 'Also - why not make these enums (or static at the very least)', 'commenter': 'jonathan-hurley'}]"
530,ambari-server/src/main/java/org/apache/ambari/server/bootstrap/BSRunner.java,"@@ -61,11 +61,12 @@
   private final String clusterOsFamily;
   private String projectVersion;
   private int serverPort;
+  private boolean validationInstance;
 
   public BSRunner(BootStrapImpl impl, SshHostInfo sshHostInfo, String bootDir,
       String bsScript, String agentSetupScript, String agentSetupPassword,
       int requestId, long timeout, String hostName, boolean isVerbose, String clusterOsFamily,
-      String projectVersion, int serverPort)
+      String projectVersion, int serverPort, boolean validationInstance)","[{'comment': ""Document what this variable does - it's not apparent. "", 'commenter': 'jonathan-hurley'}]"
530,ambari-server/src/main/java/org/apache/ambari/server/bootstrap/BootStrapImpl.java,"@@ -101,7 +101,7 @@ public synchronized void init() throws IOException {
     }
   }
 
-  public  synchronized BSResponse runBootStrap(SshHostInfo info) {
+  public  synchronized BSResponse runBootStrap(SshHostInfo info, boolean validate) {","[{'comment': 'Document the parameters here.', 'commenter': 'jonathan-hurley'}, {'comment': 'Added', 'commenter': 'scottduan'}]"
530,ambari-server/src/main/python/bootstrap.py,"@@ -793,25 +798,47 @@ def run(self):
     self.createDoneFile(last_retcode)
     self.status[""return_code""] = last_retcode
 
+class ValidateHost(Bootstrap):
+  def __new__(cls, *args, **kwargs):
+    return object.__new__(ValidateHost)
+
+  def __init__(self, hosts, sharedState):
+    super(ValidateHost, self).__init__(hosts, sharedState)
+    self.timeout = HOST_CONNECTIVITY_TIMEOUT
+
+  def login(self):
+    params = self.shared_state
+    self.host_log.write(""==========================\n"")
+    self.host_log.write(""Running login to the host {0} ..."".format(self.host))","[{'comment': 'No need for the word ""the""', 'commenter': 'jonathan-hurley'}]"
530,ambari-server/src/main/python/bootstrap.py,"@@ -793,25 +798,47 @@ def run(self):
     self.createDoneFile(last_retcode)
     self.status[""return_code""] = last_retcode
 
+class ValidateHost(Bootstrap):
+  def __new__(cls, *args, **kwargs):
+    return object.__new__(ValidateHost)
+
+  def __init__(self, hosts, sharedState):
+    super(ValidateHost, self).__init__(hosts, sharedState)
+    self.timeout = HOST_CONNECTIVITY_TIMEOUT
+
+  def login(self):
+    params = self.shared_state
+    self.host_log.write(""==========================\n"")
+    self.host_log.write(""Running login to the host {0} ..."".format(self.host))
+    ssh = SSH(params.user, params.sshPort, params.sshkey_file, self.host, ""exit"",
+              params.bootdir, self.host_log) #login and exit immediately
+    retcode = ssh.run()
+    self.host_log.write(""\n"")
+    return retcode
 
+  def run(self):
+    self.timeout = HOST_CONNECTIVITY_TIMEOUT
+    self.status[""start_time""] = time.time()
+    ret = self.try_to_execute(self.login)
+    retcode = ret[""exitstatus""]
+    err_msg = ret[""errormsg""]
+    std_out = ret[""log""]
+    if retcode != 0:
+      message = ""ERROR: Validation of host {0} fails because it finished with non-zero exit code ({1})\nERROR MESSAGE: {2}\nSTDOUT: {3}"".format(self.host, retcode, err_msg, std_out)","[{'comment': 'Why do we need the word ""ERROR"" here? Seems like it\'s just going to be stripped off? ', 'commenter': 'jonathan-hurley'}]"
546,ambari-server/src/main/resources/stacks/HDP/2.6/upgrades/config-upgrade.xml,"@@ -53,18 +53,14 @@
       </component>
       <component name=""DATANODE"">
         <changes>
-          <definition xsi:type=""configure"" id=""hdfs_set_data_transfer_protection""
-                      summary=""Enables SASL for authentication of data transfer protocol"">
+          <definition xsi:type=""configure"" id=""hdfs_set_data_transfer_protection"" summary=""Enables SASL for authentication of data transfer protocol"">
             <type>hdfs-site</type>
-            <set key=""dfs.data.transfer.protection"" value=""authentication,privacy"" if-type=""hdfs-site""
-                 if-key=""dfs.http.policy"" if-value=""HTTPS_ONLY""/>
+            <set key=""dfs.data.transfer.protection"" value=""authentication,privacy"" if-type=""cluster-env"" if-key=""security_enabled"" if-value=""true"" />
           </definition>
 
-          <definition xsi:type=""configure"" id=""hdfs_set_hadoop_rpc_protection_on_kerberized_cluster""
-                      summary=""Encrypting the data transfered between hadoop services and clients"">
+          <definition xsi:type=""configure"" id=""hdfs_set_hadoop_rpc_protection_on_kerberized_cluster"" summary=""Encrypting the data transfered between hadoop services and clients"">
             <type>core-site</type>
-            <set key=""hadoop.rpc.protection"" value=""authentication,privacy"" if-type=""cluster-env""
-                 if-key=""security_enabled"" if-value=""true""/>
+            <set key=""hadoop.rpc.protection"" value=""authentication,privacy"" if-type=""cluster-env"" if-key=""security_enabled"" if-value=""true"" />","[{'comment': ""I don't think this is correct. Security doesn't guarantee Kerberos. Instead, we have a <condition> element which should be used."", 'commenter': 'jonathan-hurley'}, {'comment': 'These are already protected via a kerberos xsi:security condition, do we need to also have a security_enabled=true check?', 'commenter': 'ncole'}, {'comment': ""You are right; instead I'll check if `hadoop.security.authentication` is set to `kerberos` in `core-site`. Are you OK with this approach?"", 'commenter': 'smolnar82'}, {'comment': ""Hm..just reading Nate's comment below and he is right too. We already have the kerberos xsi:security condition in `nonrolling-upgrade-3.0.xml` so that we can simply remove this check here. Any objection?"", 'commenter': 'smolnar82'}, {'comment': '@jonathan-hurley \r\nI implemented the change I mentioned above; if you had time could you please review? Thanks in advance!', 'commenter': 'smolnar82'}]"
588,ambari-common/src/main/python/resource_management/libraries/functions/get_not_managed_resources.py,"@@ -25,6 +25,7 @@
 from resource_management.libraries.script import Script
 from resource_management.core.logger import Logger
 from resource_management.libraries.functions.default import default
+from resource_management.libraries.functions.cluster_settings import *","[{'comment': ""Please don't `import *`.  Instead, explicitly specify the imported members."", 'commenter': 'adoroszlai'}]"
588,ambari-web/app/models/stack_service.js,"@@ -206,7 +206,7 @@ App.StackService = DS.Model.extend({
   }.property('coSelectedServices', 'serviceName'),
 
   isHiddenOnSelectServicePage: function () {
-    var hiddenServices = ['MAPREDUCE2'];
+    var hiddenServices = [];
     return hiddenServices.contains(this.get('serviceName')) || !this.get('isInstallable') || this.get('doNotShowAndInstall');","[{'comment': 'Have we made a decision to remove this? If yes, the fix would include removing the entire framework for co-selected services. ', 'commenter': 'ishanbha'}, {'comment': 'Hi,\r\nWe have to remove hard-coded services from UI code. I will not be closing the jira after this fix so that refactoring can be done as a separate task for this jira. As for the Yarn & Mr being co-selected, it is still in discussion. cc @jayush ', 'commenter': 'mradha25'}, {'comment': '@jayush @mradha25 So this means we are planning an API response for the list of co-selected services? Also, after this change along with https://github.com/mradha25/ambari/commit/28ea5b98afd08582a2eb8b5d4093b4fae8714fd5#diff-818191799511f5f6f1c55435409f4117R343 Yarn and MR will not be co-selected anymore. ', 'commenter': 'ishanbha'}]"
588,ambari-common/src/main/python/resource_management/libraries/functions/get_not_managed_resources.py,"@@ -34,8 +35,8 @@ def get_not_managed_resources():
   """"""
   config = Script.get_config()
   not_managed_hdfs_path_list = json.loads(config['hostLevelParams']['not_managed_hdfs_path_list'])[:]
-  if 'managed_hdfs_resource_property_names' in config['configurations']['cluster-env']:
-    managed_hdfs_resource_property_names = config['configurations']['cluster-env']['managed_hdfs_resource_property_names']
+  if get_cluster_setting('managed_hdfs_resource_property_names') not None:","[{'comment': 'I think `if ... not None:` is invalid syntax.  It should be `if ... is not None:` instead.  Sorry for not noticing it earlier.\r\n\r\n```\r\n>>> def x():\r\n...     return \'x\'\r\n...\r\n>>> if x() not None:\r\n  File ""<stdin>"", line 1\r\n    if x() not None:\r\n                  ^\r\nSyntaxError: invalid syntax\r\n```', 'commenter': 'adoroszlai'}]"
589,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/ConfigBasedJmxHostProvider.java,"@@ -0,0 +1,95 @@
+/*
+ *
+ *  * Licensed to the Apache Software Foundation (ASF) under one
+ *  * or more contributor license agreements.  See the NOTICE file
+ *  * distributed with this work for additional information
+ *  * regarding copyright ownership.  The ASF licenses this file
+ *  * to you under the Apache License, Version 2.0 (the
+ *  * ""License""); you may not use this file except in compliance
+ *  * with the License.  You may obtain a copy of the License at
+ *  *
+ *  *     http://www.apache.org/licenses/LICENSE-2.0
+ *  *
+ *  * Unless required by applicable law or agreed to in writing, software
+ *  * distributed under the License is distributed on an ""AS IS"" BASIS,
+ *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *  * See the License for the specific language governing permissions and
+ *  * limitations under the License.
+ *
+ */
+
+package org.apache.ambari.server.controller.internal;
+
+import static edu.emory.mathcs.backport.java.util.Collections.singleton;","[{'comment': ""Shouldn't this be simply `java.util.Collections.singleton`?"", 'commenter': 'adoroszlai'}, {'comment': 'yes, fixed', 'commenter': 'zeroflag'}]"
589,ambari-server/src/main/java/org/apache/ambari/server/state/alert/MetricsUri.java,"@@ -29,15 +29,15 @@
 import com.google.gson.annotations.SerializedName;
 
 /**
- * The {@link AlertUri} class is used to represent a complex URI structure where
+ * The {@link MetricsUri} class is used to represent a complex URI structure where
  * there can be both a plaintext and SSL URI. This is used in cases where the
  * alert definition needs a way to expose which URL (http or https) should be
  * used to gather data. Currently, only {@link MetricSource} uses this, but it
  * can be swapped out in other source types where a plain string is used for the
  * URI.
  */
 @JsonInclude(JsonInclude.Include.NON_EMPTY)
-public class AlertUri {
+public class MetricsUri {","[{'comment': ""I don't think it makes sense to change the name of this class to {{MetricsUri}}. This makes alerts which have nothing to do with metrics, like web alerts, look like they use a metric endpoint. If anything, we can abstract this class out to a regular URI."", 'commenter': 'jonathan-hurley'}, {'comment': ""I'm not seeing any Alert related information in this class, only URI-s and http(s) related configuration. Even MetricsUri sounds too specific if we consider what's inside."", 'commenter': 'zeroflag'}]"
589,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/ConfigBasedJmxHostProvider.java,"@@ -0,0 +1,95 @@
+/*
+ *
+ *  * Licensed to the Apache Software Foundation (ASF) under one
+ *  * or more contributor license agreements.  See the NOTICE file
+ *  * distributed with this work for additional information
+ *  * regarding copyright ownership.  The ASF licenses this file
+ *  * to you under the Apache License, Version 2.0 (the
+ *  * ""License""); you may not use this file except in compliance
+ *  * with the License.  You may obtain a copy of the License at
+ *  *
+ *  *     http://www.apache.org/licenses/LICENSE-2.0
+ *  *
+ *  * Unless required by applicable law or agreed to in writing, software
+ *  * distributed under the License is distributed on an ""AS IS"" BASIS,
+ *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *  * See the License for the specific language governing permissions and
+ *  * limitations under the License.
+ *
+ */
+
+package org.apache.ambari.server.controller.internal;
+
+import static java.util.Collections.singleton;
+
+import java.net.URI;
+import java.util.Map;
+import java.util.Optional;
+import java.util.Set;
+
+import org.apache.ambari.server.AmbariException;
+import org.apache.ambari.server.controller.jmx.JMXHostProvider;
+import org.apache.ambari.server.state.ConfigHelper;
+import org.apache.ambari.server.state.alert.UriInfo;
+
+/**
+ * I'm a special {@link JMXHostProvider} that resolves JMX URIs based on cluster configuration.","[{'comment': 'This is unusual for a javadoc comment.  Maybe just ""{@link JMXHostProvide} that resolves...""', 'commenter': 'ncole'}]"
603,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/BaseClusterRequest.java,"@@ -224,4 +210,73 @@ public ProvisionAction getProvisionAction() {
   public void setProvisionAction(ProvisionAction provisionAction) {
     this.provisionAction = provisionAction;
   }
+","[{'comment': 'This is moved here from PersistedStateImpl to take advantage of polymorphism instead of doing instanceof checks.', 'commenter': 'benyoka'}]"
603,ambari-server/src/main/java/org/apache/ambari/server/utils/JsonUtils.java,"@@ -39,6 +40,9 @@
   public static JsonParser jsonParser = new JsonParser();
 
   private static final ObjectMapper JSON_SERIALIZER = new ObjectMapper();
+  static {
+    JSON_SERIALIZER.setSerializationInclusion(JsonInclude.Include.NON_NULL);
+  }","[{'comment': '`setSerializationInclusion` and other similar methods return the mapper, so it can be chained like:\r\n\r\n```\r\nprivate static final ObjectMapper JSON_SERIALIZER = new ObjectMapper()\r\n  .setSerializationInclusion(JsonInclude.Include.NON_NULL);\r\n```', 'commenter': 'adoroszlai'}]"
603,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/BaseClusterRequest.java,"@@ -224,4 +210,73 @@ public ProvisionAction getProvisionAction() {
   public void setProvisionAction(ProvisionAction provisionAction) {
     this.provisionAction = provisionAction;
   }
+
+  /**
+   * @return the request converted to a {@link TopologyRequestEntity}
+   */
+  public TopologyRequestEntity toEntity() {
+    TopologyRequestEntity entity = new TopologyRequestEntity();
+
+    //todo: this isn't set for a scaling operation because we had intended to allow multiple
+    //todo: bp's to be used to scale a cluster although this isn't currently supported by
+    //todo: new topology infrastructure
+    entity.setAction(getType().name());
+    if (getBlueprint() != null) {
+      entity.setBlueprintName(getBlueprint().getName());
+    }
+
+    entity.setClusterAttributes(JsonUtils.toJson(getConfiguration().getAttributes()));
+    entity.setClusterId(getClusterId());
+    entity.setClusterProperties(JsonUtils.toJson(getConfiguration().getProperties()));
+    entity.setDescription(getDescription());
+
+    if (getProvisionAction() != null) {
+      entity.setProvisionAction(getProvisionAction());
+    }
+
+    // host groups
+    Collection<TopologyHostGroupEntity> hostGroupEntities = new ArrayList<>();
+    for (HostGroupInfo groupInfo : getHostGroupInfo().values())  {
+      hostGroupEntities.add(toHostGroupEntity(groupInfo, entity));
+    }
+    entity.setTopologyHostGroupEntities(hostGroupEntities);
+
+    return entity;
+  }
+
+  private TopologyHostGroupEntity toHostGroupEntity(HostGroupInfo groupInfo, TopologyRequestEntity topologyRequestEntity) {","[{'comment': 'Doc.', 'commenter': 'jonathan-hurley'}]"
603,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/BaseClusterRequest.java,"@@ -224,4 +210,73 @@ public ProvisionAction getProvisionAction() {
   public void setProvisionAction(ProvisionAction provisionAction) {
     this.provisionAction = provisionAction;
   }
+
+  /**
+   * @return the request converted to a {@link TopologyRequestEntity}
+   */
+  public TopologyRequestEntity toEntity() {
+    TopologyRequestEntity entity = new TopologyRequestEntity();
+
+    //todo: this isn't set for a scaling operation because we had intended to allow multiple
+    //todo: bp's to be used to scale a cluster although this isn't currently supported by
+    //todo: new topology infrastructure
+    entity.setAction(getType().name());
+    if (getBlueprint() != null) {
+      entity.setBlueprintName(getBlueprint().getName());
+    }
+
+    entity.setClusterAttributes(JsonUtils.toJson(getConfiguration().getAttributes()));
+    entity.setClusterId(getClusterId());
+    entity.setClusterProperties(JsonUtils.toJson(getConfiguration().getProperties()));
+    entity.setDescription(getDescription());
+
+    if (getProvisionAction() != null) {
+      entity.setProvisionAction(getProvisionAction());
+    }
+
+    // host groups
+    Collection<TopologyHostGroupEntity> hostGroupEntities = new ArrayList<>();
+    for (HostGroupInfo groupInfo : getHostGroupInfo().values())  {
+      hostGroupEntities.add(toHostGroupEntity(groupInfo, entity));
+    }
+    entity.setTopologyHostGroupEntities(hostGroupEntities);
+
+    return entity;
+  }
+
+  private TopologyHostGroupEntity toHostGroupEntity(HostGroupInfo groupInfo, TopologyRequestEntity topologyRequestEntity) {
+    TopologyHostGroupEntity entity = new TopologyHostGroupEntity();
+    entity.setGroupAttributes(JsonUtils.toJson(groupInfo.getConfiguration().getAttributes()));
+    entity.setGroupProperties(JsonUtils.toJson(groupInfo.getConfiguration().getProperties()));
+    entity.setName(groupInfo.getHostGroupName());
+    entity.setTopologyRequestEntity(topologyRequestEntity);
+
+    // host info
+    Collection<TopologyHostInfoEntity> hostInfoEntities = new ArrayList<>();
+    entity.setTopologyHostInfoEntities(hostInfoEntities);
+
+    Collection<String> hosts = groupInfo.getHostNames();
+    if (hosts.isEmpty()) {
+      TopologyHostInfoEntity hostInfoEntity = new TopologyHostInfoEntity();
+      hostInfoEntity.setTopologyHostGroupEntity(entity);
+      hostInfoEntity.setHostCount(groupInfo.getRequestedHostCount());
+      if (groupInfo.getPredicate() != null) {
+        hostInfoEntity.setPredicate(groupInfo.getPredicateString());
+      }
+      hostInfoEntities.add(hostInfoEntity);
+    } else {
+      for (String hostName : hosts) {
+        TopologyHostInfoEntity hostInfoEntity = new TopologyHostInfoEntity();
+        hostInfoEntity.setTopologyHostGroupEntity(entity);
+        if (groupInfo.getPredicate() != null) {","[{'comment': 'Is the if-check necessary? Can you just set it to the value of the predicate, even if the predicate is null?', 'commenter': 'jonathan-hurley'}, {'comment': ""I actually didn't write this method but copied it over from PersistedStateImpl. I can change it nevertheless. "", 'commenter': 'benyoka'}]"
603,ambari-server/src/main/java/org/apache/ambari/server/utils/JsonUtils.java,"@@ -70,6 +74,18 @@ public static boolean isValidJson(String jsonString) {
     }
   }
 
+  public static <T> T fromJson(String json, Class<?> valueType) {","[{'comment': 'Doc.', 'commenter': 'jonathan-hurley'}]"
603,ambari-server/src/main/resources/Ambari-DDL-Derby-CREATE.sql,"@@ -893,7 +893,7 @@ CREATE TABLE topology_request (
   action VARCHAR(255) NOT NULL,
   cluster_id BIGINT NOT NULL,
   bp_name VARCHAR(100) NOT NULL,
-  raw_request_body CLOB NOT NULL,
+  mpack_instances CLOB NOT NULL,","[{'comment': 'Can you give an example of what mpack_instances looks like? Does it make sense to try to normalize this data into its own table?', 'commenter': 'jonathan-hurley'}, {'comment': ""It has the same structure as for blueprints. For blueprints it is modelled by the BlueprintMpackInstanceEntity, BlueprintServiceEntity, BlueprintMpackConfigEntities. \r\n\r\nCurrently we are interested in the stack id's only, later also in configurations and service descriptors associated with mpacks (I saw configs are already dumped as json too).\r\n\r\nDo you think it is worth normalizing? (would mean 3 more tables, later maybe more) This information wouldn't be used outside of the scope of the replayed request."", 'commenter': 'benyoka'}, {'comment': ""My concern is always that when de-serializing this, if the object it's being mapped to changed in any way, the deserialization will fail? We've actually tried to move away from this model. Repositories used to be a solid chunk of JSON but we recently broke this out into 3 new tables to better manage it as entities. \r\n\r\nI hate giant blob of JSON, especially if we need to worry about deserializing it ourselves and pulling data out of it. "", 'commenter': 'jonathan-hurley'}, {'comment': ""Based on @jonathan-hurley 's feedback, and also based on looking at the BlueprintMPackInstanceEntity, it seems like this data should be normalized.  \r\n\r\nSince the MPack entities can be defined in either the Blueprint or the cluster creation template, it seems like it might make sense to use the same tables to store them.  \r\n\r\nIs there some technical reason for storing this data in the topology_request table?  It seems to me that we'd be better off persisting all MPack instance data in the same table, regardless of whether it is defined in the Blueprint or the Cluster Creation template. \r\n\r\nSince Service instance definitions and config can be specified in either document, I'm not sure why these should be modelled separately based on the location of the definition.  It looks like we should be using the existing tables for both cases.  "", 'commenter': 'rnettleton'}, {'comment': 'On the other hand, this shouldn\'t be a problem for blueprints, since backward compatibility is required.  If users should be able to successfully submit ""old"" blueprint / cluster creation requests even if code is changed, so should Ambari internally.', 'commenter': 'adoroszlai'}]"
603,ambari-server/src/main/java/org/apache/ambari/server/orm/entities/MpackInstanceEntity.java,"@@ -0,0 +1,182 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.orm.entities;
+
+import java.util.ArrayList;
+import java.util.Collection;
+
+import javax.annotation.Nullable;
+import javax.persistence.CascadeType;
+import javax.persistence.Column;
+import javax.persistence.DiscriminatorColumn;
+import javax.persistence.Entity;
+import javax.persistence.GeneratedValue;
+import javax.persistence.GenerationType;
+import javax.persistence.Id;
+import javax.persistence.Inheritance;
+import javax.persistence.InheritanceType;
+import javax.persistence.JoinColumn;
+import javax.persistence.ManyToOne;
+import javax.persistence.OneToMany;
+import javax.persistence.Table;
+import javax.persistence.TableGenerator;
+
+/**
+ * Entity to encapsulate a blueprint's or cluster template's use of an mpack. It contains the mpack name, version, url
+ * referencing a management pack from the blueprint. The reference contains the name and
+ * the version of the mpack, but no direct database reference to the mpack entity as a blueprint
+ * can be saved without the referenced mpack being present.
+ */
+@Entity
+@Table(name = ""mpack_instance"")
+@Inheritance(strategy = InheritanceType.SINGLE_TABLE)
+@DiscriminatorColumn(name = ""owner"", length = 20)
+@TableGenerator(name = ""mpack_instance_id_generator"", table = ""ambari_sequences"", pkColumnName = ""sequence_name"",
+  valueColumnName = ""sequence_value"", pkColumnValue = ""mpack_instance_id_seq"", initialValue = 1)
+public abstract class MpackInstanceEntity {
+
+  @Id
+  @GeneratedValue(strategy = GenerationType.TABLE, generator = ""mpack_instance_id_generator"")
+  @Column(name = ""id"", nullable = false, updatable = false)
+  private Long id;
+
+  @Column(name = ""mpack_name"", nullable = false)
+  private String mpackName;
+
+  @Column(name = ""mpack_version"", nullable = false)
+  private String mpackVersion;
+
+  @Column(name = ""mpack_uri"")
+  private String mpackUri;
+
+  @OneToMany(cascade = CascadeType.ALL, mappedBy = ""mpackInstance"")
+  private Collection<MpackInstanceServiceEntity> serviceInstances = new ArrayList<>();","[{'comment': 'Why make these a Collection instead of a List?', 'commenter': 'jonathan-hurley'}, {'comment': 'I prefer using the most general suitable interface.', 'commenter': 'benyoka'}]"
603,ambari-server/src/main/java/org/apache/ambari/server/topology/MpackInstance.java,"@@ -117,37 +121,59 @@ public void setUrl(String url) {
     this.url = url;
   }
 
-  public BlueprintMpackInstanceEntity toEntity() {
-    BlueprintMpackInstanceEntity mpackEntity = new BlueprintMpackInstanceEntity();
-    mpackEntity.setMpackUri(url);
-    mpackEntity.setMpackName(mpackName);
-    mpackEntity.setMpackVersion(mpackVersion);
-    Collection<BlueprintMpackConfigEntity> mpackConfigEntities =
-      BlueprintImpl.toConfigEntities(configuration, BlueprintMpackConfigEntity::new);
-    mpackConfigEntities.forEach( configEntity -> configEntity.setMpackInstance(mpackEntity) );
-    mpackEntity.setConfigurations(mpackConfigEntities);
+  /**
+   * Converts the mpack instance to a {@link BlueprintMpackInstanceEntity}
+   * @param blueprintEntity the blueprint entity will be associated to
+   * @return the resulting entity
+   */
+  public BlueprintMpackInstanceEntity toMpackInstanceEntity(BlueprintEntity blueprintEntity) {
+    BlueprintMpackInstanceEntity mpackInstanceEntity = new BlueprintMpackInstanceEntity();
+    mpackInstanceEntity.setBlueprint(blueprintEntity);
+    setCommonProperties(mpackInstanceEntity);
+    return mpackInstanceEntity;
+  }
+
+  /**
+   * Converts the mpack instance to a {@link TopologyRequestMpackInstanceEntity}
+   * @param topologyRequestEntity the topology request entity will be associated to
+   * @return the resulting entity
+   */
+  public TopologyRequestMpackInstanceEntity toMpackInstanceEntity(TopologyRequestEntity topologyRequestEntity) {
+    TopologyRequestMpackInstanceEntity mpackInstanceEntity = new TopologyRequestMpackInstanceEntity();
+    mpackInstanceEntity.setTopologyRequest(topologyRequestEntity);
+    setCommonProperties(mpackInstanceEntity);
+    return mpackInstanceEntity;
+  }
+
+  private void setCommonProperties(MpackInstanceEntity mpackInstanceEntity) {","[{'comment': 'Documentation of which properties are being set for the bi-directional relationship.', 'commenter': 'jonathan-hurley'}]"
603,ambari-server/src/main/resources/Ambari-DDL-Derby-CREATE.sql,"@@ -121,6 +111,16 @@ CREATE TABLE hosts (
   CONSTRAINT PK_hosts PRIMARY KEY (host_id),
   CONSTRAINT UQ_hosts_host_name UNIQUE (host_name));
 
+CREATE TABLE mpack_host_state (","[{'comment': 'Any reason you moved this?', 'commenter': 'jonathan-hurley'}, {'comment': 'To have the tables in the same order as in Ambari-DDL-Postgres-CREATE.sql for easier comparision.', 'commenter': 'benyoka'}]"
615,ambari-server/src/main/python/ambari-server.py,"@@ -558,6 +558,7 @@ def init_ldap_setup_parser_options(parser):
   parser.add_option('--ldap-sync-username-collisions-behavior', default=None, help=""Handling behavior for username collisions [convert/skip] for LDAP sync"", dest=""ldap_sync_username_collisions_behavior"")
   parser.add_option('--ldap-force-lowercase-usernames', default=None, help=""Declares whether to force the ldap user name to be lowercase or leave as-is"", dest=""ldap_force_lowercase_usernames"")
   parser.add_option('--ldap-pagination-enabled', default=None, help=""Determines whether results from LDAP are paginated when requested"", dest=""ldap_pagination_enabled"")
+  parser.add_option('--ldap-enforcement', default=None, help=""Forces the use of LDAP even if other (i.e. PAM) authentication method is configured already or if there is no authentication method configured at all"", dest=""ldap_enforcement"")","[{'comment': 'I think this should be a boolean, like `--silent`, which can be achieved by:\r\n\r\n```\r\naction=""store_true"", dest=""ldap_enforcement"", default=False, ...\r\n```\r\n\r\nAlso, I find the name `enforcement` a bit odd.  Usually this kind of option is called `force` something.  I suggest renaming it to `--force-ldap-setup`, or `--ldap-force-setup` if you want to stick to the `ldap` prefix.', 'commenter': 'adoroszlai'}, {'comment': ""Good idea; I'll change it.\r\nThanks!"", 'commenter': 'smolnar82'}, {'comment': 'Fixed.', 'commenter': 'smolnar82'}]"
615,ambari-server/src/main/python/ambari_server/setupSecurity.py,"@@ -716,14 +716,16 @@ def setup_ldap(options):
     err = 'Ambari Server is not running.'
     raise FatalException(1, err)
 
-  current_client_security = get_value_from_properties(properties,CLIENT_SECURITY,""no auth method"")
-  if current_client_security != 'ldap':
-    query = ""Currently '"" + current_client_security + ""' is configured, do you wish to use LDAP instead [y/n] (n)? ""
-    if get_YN_input(query, False):
-      pass
-    else:
-      err = ""Currently '"" + current_client_security + ""' configured. Can not setup LDAP.""
-      raise FatalException(1, err)
+  enforce_ldap = True if options.ldap_enforcement is not None and options.ldap_enforcement == 'true' else False
+  if not enforce_ldap:
+    current_client_security = get_value_from_properties(properties,CLIENT_SECURITY,""no auth method"")
+    if current_client_security != 'ldap':
+      query = ""Currently '"" + current_client_security + ""' is configured, do you wish to use LDAP instead [y/n] (n)? ""
+      if get_YN_input(query, False):","[{'comment': 'Instead of introducing a new flag to force LDAP setup, you might consider defaulting this input to `True`.  This would allow `setup-ldap --silent` to successfully complete, while still giving users a chance to quit at this point when invoked without `--silent`.  If this is too drastic to apply in all cases, then it still can be considered for the `no auth configured` case.', 'commenter': 'adoroszlai'}, {'comment': ""It's been defaulted to False for a reason; let me discuss this with @rlevas first."", 'commenter': 'smolnar82'}, {'comment': 'Implemented that for the case you suggested (when no auth. method is configured in the first place)', 'commenter': 'smolnar82'}]"
615,ambari-server/src/main/python/ambari_server/setupSecurity.py,"@@ -716,14 +716,17 @@ def setup_ldap(options):
     err = 'Ambari Server is not running.'
     raise FatalException(1, err)
 
-  current_client_security = get_value_from_properties(properties,CLIENT_SECURITY,""no auth method"")
-  if current_client_security != 'ldap':
-    query = ""Currently '"" + current_client_security + ""' is configured, do you wish to use LDAP instead [y/n] (n)? ""
-    if get_YN_input(query, False):
-      pass
-    else:
-      err = ""Currently '"" + current_client_security + ""' configured. Can not setup LDAP.""
-      raise FatalException(1, err)
+  enforce_ldap = options.ldap_force_setup if options.ldap_force_setup is not None else False
+  if not enforce_ldap:
+    current_client_security = get_value_from_properties(properties,CLIENT_SECURITY,""no auth method"")
+    if current_client_security != 'ldap':
+      query = ""Currently '"" + current_client_security + ""' is configured, do you wish to use LDAP instead [y/n] ({0})? ""","[{'comment': 'you could use string format using % instead of string concatenation', 'commenter': 'Unknown'}, {'comment': 'Or better yet, use `{}`-type placeholder, since already calling `query.format()` to add the default answer.', 'commenter': 'adoroszlai'}, {'comment': 'Done', 'commenter': 'smolnar82'}]"
615,ambari-server/src/main/python/ambari_server/setupSecurity.py,"@@ -716,14 +716,17 @@ def setup_ldap(options):
     err = 'Ambari Server is not running.'
     raise FatalException(1, err)
 
-  current_client_security = get_value_from_properties(properties,CLIENT_SECURITY,""no auth method"")
-  if current_client_security != 'ldap':
-    query = ""Currently '"" + current_client_security + ""' is configured, do you wish to use LDAP instead [y/n] (n)? ""
-    if get_YN_input(query, False):
-      pass
-    else:
-      err = ""Currently '"" + current_client_security + ""' configured. Can not setup LDAP.""
-      raise FatalException(1, err)
+  enforce_ldap = options.ldap_force_setup if options.ldap_force_setup is not None else False
+  if not enforce_ldap:
+    current_client_security = get_value_from_properties(properties,CLIENT_SECURITY,""no auth method"")
+    if current_client_security != 'ldap':
+      query = ""Currently '"" + current_client_security + ""' is configured, do you wish to use LDAP instead [y/n] ({0})? ""
+      ldap_setup_default = 'y' if current_client_security == ""no auth method"" else 'n'
+      if get_YN_input(query.format(ldap_setup_default), True if ldap_setup_default == 'y' else False):","[{'comment': ""```\r\nTrue if ldap_setup_default == 'y' else False\r\n```\r\n\r\ncan be simplified to:\r\n\r\n```\r\nldap_setup_default == 'y'\r\n```"", 'commenter': 'adoroszlai'}, {'comment': 'Done', 'commenter': 'smolnar82'}]"
615,ambari-server/src/main/python/ambari_server/setupSecurity.py,"@@ -716,14 +716,17 @@ def setup_ldap(options):
     err = 'Ambari Server is not running.'
     raise FatalException(1, err)
 
-  current_client_security = get_value_from_properties(properties,CLIENT_SECURITY,""no auth method"")
-  if current_client_security != 'ldap':
-    query = ""Currently '"" + current_client_security + ""' is configured, do you wish to use LDAP instead [y/n] (n)? ""
-    if get_YN_input(query, False):
-      pass
-    else:
-      err = ""Currently '"" + current_client_security + ""' configured. Can not setup LDAP.""
-      raise FatalException(1, err)
+  enforce_ldap = options.ldap_force_setup if options.ldap_force_setup is not None else False
+  if not enforce_ldap:
+    current_client_security = get_value_from_properties(properties,CLIENT_SECURITY,""no auth method"")
+    if current_client_security != 'ldap':
+      query = ""Currently '"" + current_client_security + ""' is configured, do you wish to use LDAP instead [y/n] ({0})? ""
+      ldap_setup_default = 'y' if current_client_security == ""no auth method"" else 'n'","[{'comment': 'Please extract the string `""no auth method""` (appears twice) to a variable.', 'commenter': 'adoroszlai'}, {'comment': 'Done', 'commenter': 'smolnar82'}]"
619,ambari-server/src/main/java/org/apache/ambari/server/api/services/AlertTargetService.java,"@@ -33,54 +33,165 @@
 import javax.ws.rs.core.UriInfo;
 
 import org.apache.ambari.annotations.ApiIgnore;
+import org.apache.ambari.server.api.resources.AlertTargetResourceDefinition;
 import org.apache.ambari.server.api.resources.ResourceInstance;
+import org.apache.ambari.server.controller.AlertTargetSwagger;
 import org.apache.ambari.server.controller.spi.Resource;
 import org.apache.ambari.server.state.alert.AlertTarget;
+import org.apache.http.HttpStatus;
+
+import io.swagger.annotations.Api;
+import io.swagger.annotations.ApiImplicitParam;
+import io.swagger.annotations.ApiImplicitParams;
+import io.swagger.annotations.ApiOperation;
+import io.swagger.annotations.ApiParam;
+import io.swagger.annotations.ApiResponse;
+import io.swagger.annotations.ApiResponses;
+
+
 
 /**
  * The {@link AlertTargetService} handles CRUD operation requests for alert
  * targets.
  */
 @Path(""/alert_targets/"")
+@Api(value = ""/alerts"", description = ""Endpoint for alert specific operations"")
 public class AlertTargetService extends BaseService {
 
-  @GET @ApiIgnore // until documented
+  public static final String ALERT_TARGET_REQUEST_TYPE = ""org.apache.ambari.server.controller.AlertTargetSwagger"";
+
+  /**
+   * Handles GET /alert_targets
+   * Get all alert targets.
+   *
+   * @param headers http headers
+   * @param ui      uri info
+   * @return alert target resource representation
+   */
+  @GET @ApiIgnore // documented below","[{'comment': 'I think `/alert_targets` and `/alert_targets/{targetId}` should be documented separately, since they have a few differences.', 'commenter': 'adoroszlai'}]"
619,ambari-server/src/main/java/org/apache/ambari/server/api/services/AlertTargetService.java,"@@ -33,54 +33,165 @@
 import javax.ws.rs.core.UriInfo;
 
 import org.apache.ambari.annotations.ApiIgnore;
+import org.apache.ambari.server.api.resources.AlertTargetResourceDefinition;
 import org.apache.ambari.server.api.resources.ResourceInstance;
+import org.apache.ambari.server.controller.AlertTargetSwagger;
 import org.apache.ambari.server.controller.spi.Resource;
 import org.apache.ambari.server.state.alert.AlertTarget;
+import org.apache.http.HttpStatus;
+
+import io.swagger.annotations.Api;
+import io.swagger.annotations.ApiImplicitParam;
+import io.swagger.annotations.ApiImplicitParams;
+import io.swagger.annotations.ApiOperation;
+import io.swagger.annotations.ApiParam;
+import io.swagger.annotations.ApiResponse;
+import io.swagger.annotations.ApiResponses;
+
+
 
 /**
  * The {@link AlertTargetService} handles CRUD operation requests for alert
  * targets.
  */
 @Path(""/alert_targets/"")
+@Api(value = ""/alerts"", description = ""Endpoint for alert specific operations"")
 public class AlertTargetService extends BaseService {
 
-  @GET @ApiIgnore // until documented
+  public static final String ALERT_TARGET_REQUEST_TYPE = ""org.apache.ambari.server.controller.AlertTargetSwagger"";
+
+  /**
+   * Handles GET /alert_targets
+   * Get all alert targets.
+   *
+   * @param headers http headers
+   * @param ui      uri info
+   * @return alert target resource representation
+   */
+  @GET @ApiIgnore // documented below
   @Produces(""text/plain"")
   public Response getTargets(@Context HttpHeaders headers,
       @Context UriInfo ui) {
     return handleRequest(headers, null, ui, Request.Type.GET,
         createAlertTargetResource(null));
   }
 
-  @GET @ApiIgnore // until documented
-  @Produces(""text/plain"")
+  /**
+   * Handles GET /alert_targets/{targetId}
+   * Get a specific alert target.
+   *
+   * @param headers   http headers
+   * @param ui        uri info
+   * @param targetId  alert target id
+   * @return alert target resource representation
+   */
+  @GET
   @Path(""{targetId}"")
+  @Produces(""text/plain"")
+  @ApiOperation(value = ""Returns a single alert target"", response = AlertTargetSwagger.class, responseContainer = ""List"")","[{'comment': 'List container is appropriate only for the ""all targets"" endpoint.', 'commenter': 'adoroszlai'}]"
619,ambari-server/src/main/java/org/apache/ambari/server/controller/AlertTargetSwagger.java,"@@ -0,0 +1,76 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.controller;
+
+import java.util.List;
+import java.util.Map;
+
+import org.apache.ambari.server.controller.internal.AlertGroupResourceProvider;
+import org.apache.ambari.server.controller.internal.AlertTargetResourceProvider;
+
+import io.swagger.annotations.ApiModelProperty;
+
+/**
+ * Request / response schema for AlertTarget API Swagger documentation generation. The interface only serves documentation
+ * generation purposes, it is not meant to be implemented.
+ */
+public interface AlertTargetSwagger extends ApiModel {","[{'comment': 'To more accurately reflect the JSON document, the outer `AlertTarget` map should be modelled, too.\r\n\r\n```\r\n""AlertTarget"" : {\r\n    ""name"" : ""asdf"",\r\n    ""notification_type"" : ""test"",\r\n    ...\r\n}\r\n```\r\n\r\nExample:\r\n\r\nhttps://github.com/apache/ambari/blob/c3363500db5a4d28490d5429832c3bfe0b55cf79/ambari-server/src/main/java/org/apache/ambari/server/controller/QuickLinksResponse.java#L31-L39', 'commenter': 'adoroszlai'}, {'comment': 'fixed', 'commenter': 'g-boros'}]"
619,ambari-server/src/main/java/org/apache/ambari/server/controller/AlertTargetSwagger.java,"@@ -0,0 +1,76 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.controller;
+
+import java.util.List;
+import java.util.Map;
+
+import org.apache.ambari.server.controller.internal.AlertGroupResourceProvider;
+import org.apache.ambari.server.controller.internal.AlertTargetResourceProvider;
+
+import io.swagger.annotations.ApiModelProperty;
+
+/**
+ * Request / response schema for AlertTarget API Swagger documentation generation. The interface only serves documentation
+ * generation purposes, it is not meant to be implemented.
+ */
+public interface AlertTargetSwagger extends ApiModel {
+
+    @ApiModelProperty(name = AlertTargetResourceProvider.ID_PROPERTY_ID)
+    Long getId();
+
+    @ApiModelProperty(name = AlertTargetResourceProvider.NAME_PROPERTY_ID)
+    String getName();
+
+    @ApiModelProperty(name = AlertTargetResourceProvider.DESCRIPTION_PROPERTY_ID)
+    String getDescription();
+
+    @ApiModelProperty(name = AlertTargetResourceProvider.NOTIFICATION_TYPE_PROPERTY_ID)
+    String getNotificationType();
+
+    @ApiModelProperty(name = AlertTargetResourceProvider.ENABLED_PROPERTY_ID)
+    Boolean isEnabled();
+
+    @ApiModelProperty(name = AlertTargetResourceProvider.PROPERTIES_PROPERTY_ID)
+    Map<String, String> getProperties();
+
+    // it should be Set<AlertState> but Swagger doesn't support list of enums
+    @ApiModelProperty(name = AlertTargetResourceProvider.STATES_PROPERTY_ID)
+    List<String> getAlertStates();
+
+    @ApiModelProperty(name = AlertTargetResourceProvider.GLOBAL_PROPERTY_ID)
+    Boolean isGlobal();
+
+    @ApiModelProperty(name = AlertTargetResourceProvider.GROUPS_PROPERTY_ID)
+    List<AlertGroup> getAlertGroups();
+
+    interface AlertGroup {
+        @ApiModelProperty(name = AlertGroupResourceProvider.ID_PROPERTY_ID)
+        Long getId();
+
+        @ApiModelProperty(name = ""cluster_id"")","[{'comment': 'You can use `ClusterResourceProvider.CLUSTER_ID`.', 'commenter': 'adoroszlai'}]"
619,ambari-server/src/main/java/org/apache/ambari/server/api/services/AlertTargetService.java,"@@ -33,54 +33,165 @@
 import javax.ws.rs.core.UriInfo;
 
 import org.apache.ambari.annotations.ApiIgnore;
+import org.apache.ambari.server.api.resources.AlertTargetResourceDefinition;
 import org.apache.ambari.server.api.resources.ResourceInstance;
+import org.apache.ambari.server.controller.AlertTargetSwagger;
 import org.apache.ambari.server.controller.spi.Resource;
 import org.apache.ambari.server.state.alert.AlertTarget;
+import org.apache.http.HttpStatus;
+
+import io.swagger.annotations.Api;
+import io.swagger.annotations.ApiImplicitParam;
+import io.swagger.annotations.ApiImplicitParams;
+import io.swagger.annotations.ApiOperation;
+import io.swagger.annotations.ApiParam;
+import io.swagger.annotations.ApiResponse;
+import io.swagger.annotations.ApiResponses;
+
+
 
 /**
  * The {@link AlertTargetService} handles CRUD operation requests for alert
  * targets.
  */
 @Path(""/alert_targets/"")
+@Api(value = ""/alerts"", description = ""Endpoint for alert specific operations"")
 public class AlertTargetService extends BaseService {
 
-  @GET @ApiIgnore // until documented
+  public static final String ALERT_TARGET_REQUEST_TYPE = ""org.apache.ambari.server.controller.AlertTargetSwagger"";
+
+  /**
+   * Handles GET /alert_targets
+   * Get all alert targets.
+   *
+   * @param headers http headers
+   * @param ui      uri info
+   * @return alert target resource representation
+   */
+  @GET @ApiIgnore // documented below
   @Produces(""text/plain"")
   public Response getTargets(@Context HttpHeaders headers,
       @Context UriInfo ui) {
     return handleRequest(headers, null, ui, Request.Type.GET,
         createAlertTargetResource(null));
   }
 
-  @GET @ApiIgnore // until documented
-  @Produces(""text/plain"")
+  /**
+   * Handles GET /alert_targets/{targetId}
+   * Get a specific alert target.
+   *
+   * @param headers   http headers
+   * @param ui        uri info
+   * @param targetId  alert target id
+   * @return alert target resource representation
+   */
+  @GET
   @Path(""{targetId}"")
+  @Produces(""text/plain"")
+  @ApiOperation(value = ""Returns a single alert target"", response = AlertTargetSwagger.class, responseContainer = ""List"")
+  @ApiImplicitParams({
+          @ApiImplicitParam(name = QUERY_FIELDS, value = QUERY_FILTER_DESCRIPTION, defaultValue = ""AlertTarget/*"", dataType = DATA_TYPE_STRING, paramType = PARAM_TYPE_QUERY),
+          @ApiImplicitParam(name = QUERY_SORT, value = QUERY_SORT_DESCRIPTION, dataType = DATA_TYPE_STRING, paramType = PARAM_TYPE_QUERY),
+          @ApiImplicitParam(name = QUERY_PAGE_SIZE, value = QUERY_PAGE_SIZE_DESCRIPTION, defaultValue = DEFAULT_PAGE_SIZE, dataType = DATA_TYPE_INT, paramType = PARAM_TYPE_QUERY),
+          @ApiImplicitParam(name = QUERY_FROM, value = QUERY_FROM_DESCRIPTION, allowableValues = QUERY_FROM_VALUES, defaultValue = DEFAULT_FROM, dataType = DATA_TYPE_INT, paramType = PARAM_TYPE_QUERY),
+          @ApiImplicitParam(name = QUERY_TO, value = QUERY_TO_DESCRIPTION, allowableValues = QUERY_TO_VALUES, dataType = DATA_TYPE_INT, paramType = PARAM_TYPE_QUERY),
+  })
+  @ApiResponses({
+          @ApiResponse(code = HttpStatus.SC_OK, message = MSG_SUCCESSFUL_OPERATION),
+          @ApiResponse(code = HttpStatus.SC_NOT_FOUND, message = MSG_CLUSTER_NOT_FOUND),
+          @ApiResponse(code = HttpStatus.SC_UNAUTHORIZED, message = MSG_NOT_AUTHENTICATED),
+          @ApiResponse(code = HttpStatus.SC_FORBIDDEN, message = MSG_PERMISSION_DENIED),
+          @ApiResponse(code = HttpStatus.SC_INTERNAL_SERVER_ERROR, message = MSG_SERVER_ERROR),
+          @ApiResponse(code = HttpStatus.SC_BAD_REQUEST, message = MSG_INVALID_ARGUMENTS),
+  })
   public Response getTargets(@Context HttpHeaders headers,","[{'comment': 'Ideally this method should be `getTarget`, since it returns a single item.  If you want to keep the existing name to avoid mixing documentation and refactoring, you can provide a `nickname` for Swagger.', 'commenter': 'adoroszlai'}, {'comment': 'renamed to `getTarget`', 'commenter': 'g-boros'}]"
619,ambari-server/src/main/java/org/apache/ambari/server/api/services/AlertTargetService.java,"@@ -32,55 +32,181 @@
 import javax.ws.rs.core.Response;
 import javax.ws.rs.core.UriInfo;
 
-import org.apache.ambari.annotations.ApiIgnore;
+import org.apache.ambari.server.api.resources.AlertTargetResourceDefinition;
 import org.apache.ambari.server.api.resources.ResourceInstance;
+import org.apache.ambari.server.controller.AlertTargetSwagger;
 import org.apache.ambari.server.controller.spi.Resource;
 import org.apache.ambari.server.state.alert.AlertTarget;
+import org.apache.http.HttpStatus;
+
+import io.swagger.annotations.Api;
+import io.swagger.annotations.ApiImplicitParam;
+import io.swagger.annotations.ApiImplicitParams;
+import io.swagger.annotations.ApiOperation;
+import io.swagger.annotations.ApiParam;
+import io.swagger.annotations.ApiResponse;
+import io.swagger.annotations.ApiResponses;
+
+
 
 /**
  * The {@link AlertTargetService} handles CRUD operation requests for alert
  * targets.
  */
 @Path(""/alert_targets/"")
+@Api(value = ""/alerts"", description = ""Endpoint for alert specific operations"")","[{'comment': 'I think @Api value should be ""Alerts""', 'commenter': 'benyoka'}, {'comment': 'thanks @benyoka. fixed by https://github.com/apache/ambari/pull/619/commits/0ad81d730878278501b70dde31a6f78408687229 \r\n\r\nswagger API resource values are not consistently capitalised, probably that confused me. thanks for checking. \r\n\r\n`ambari-server/docs/api/generated/swagger.json`:\r\n\r\n```\r\n{\r\n    ""name"" : ""User Authentication Sources"",\r\n    ""description"" : ""Endpoint for user specific authentication source operations""\r\n  }, {\r\n    ""name"" : ""Users"",\r\n    ""description"" : ""Endpoint for User specific operations""\r\n  }, {\r\n    ""name"" : ""Views""\r\n  }, {\r\n    ""name"" : ""clusters"",\r\n    ""description"" : ""Endpoint for cluster-specific operations""\r\n  }, {\r\n    ""name"" : ""hosts"",\r\n    ""description"" : ""Endpoint for host-specific operations""\r\n  },\r\n```', 'commenter': 'g-boros'}]"
619,ambari-server/src/main/java/org/apache/ambari/server/api/services/AlertTargetService.java,"@@ -54,7 +54,7 @@
  * targets.
  */
 @Path(""/alert_targets/"")
-@Api(value = ""/alerts"", description = ""Endpoint for alert specific operations"")
+@Api(value = ""/Alerts"", description = ""Endpoint for alert specific operations"")
 public class AlertTargetService extends BaseService {","[{'comment': 'I think the leading slash is also not needed, so it should be just ""Alerts"" (other @Api declarations don\'t use slashes).', 'commenter': 'benyoka'}]"
677,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/MpackResourceProvider.java,"@@ -272,42 +274,54 @@ private String getMpackUri(MpackRequest mpackRequest) throws AmbariException {
     } else {
       // Fetch a particular mpack based on id
       Map<String, Object> propertyMap = new HashMap<>(PredicateHelper.getProperties(predicate));
-      if (propertyMap.containsKey(STACK_NAME_PROPERTY_ID)
+      if (propertyMap.containsKey(MPACK_RESOURCE_ID)) {
+        Object objMpackId = propertyMap.get(MPACK_RESOURCE_ID);
+        if (objMpackId != null) {
+          mpackId = Long.valueOf((String) objMpackId);
+        }
+        MpackResponse response = getManagementController().getMpack(mpackId);
+        Resource resource = new ResourceImpl(Resource.Type.Mpack);
+        if (null != response) {
+          resource.setProperty(MPACK_RESOURCE_ID, response.getId());
+          resource.setProperty(MPACK_ID, response.getMpackId());
+          resource.setProperty(MPACK_NAME, response.getMpackName());
+          resource.setProperty(MPACK_VERSION, response.getMpackVersion());
+          resource.setProperty(MPACK_URI, response.getMpackUri());
+          resource.setProperty(MPACK_DESCRIPTION, response.getDescription());
+          resource.setProperty(MPACK_DISPLAY_NAME, response.getDisplayName());
+          resource.setProperty(REGISTRY_ID, response.getRegistryId());
+          List<Module> modules = getManagementController().getModules(response.getId());
+          resource.setProperty(MODULES, modules);
+          results.add(resource);
+        }
+      } //Fetch an mpack based on a stackVersion query
+      else if (propertyMap.containsKey(STACK_NAME_PROPERTY_ID)
           && propertyMap.containsKey(STACK_VERSION_PROPERTY_ID)) {
         String stackName = (String) propertyMap.get(STACK_NAME_PROPERTY_ID);
         String stackVersion = (String) propertyMap.get(STACK_VERSION_PROPERTY_ID);
         StackEntity stackEntity = stackDAO.find(stackName, stackVersion);
         mpackId = stackEntity.getMpackId();
-      } else if (propertyMap.containsKey(MPACK_RESOURCE_ID)) {
-        Object objMpackId = propertyMap.get(MPACK_RESOURCE_ID);
-        if (objMpackId != null) {
-          mpackId = Long.valueOf((String) objMpackId);
+        MpackResponse response = getManagementController().getMpack(mpackId);
+        Resource resource = new ResourceImpl(Resource.Type.Mpack);
+        if (null != response) {
+          resource.setProperty(MPACK_RESOURCE_ID, response.getId());
+          resource.setProperty(MPACK_ID, response.getMpackId());
+          resource.setProperty(MPACK_NAME, response.getMpackName());
+          resource.setProperty(MPACK_VERSION, response.getMpackVersion());
+          resource.setProperty(MPACK_URI, response.getMpackUri());
+          resource.setProperty(MPACK_DESCRIPTION, response.getDescription());
+          resource.setProperty(MPACK_DISPLAY_NAME, response.getDisplayName());
+          resource.setProperty(REGISTRY_ID, response.getRegistryId());
+          resource.setProperty(STACK_NAME_PROPERTY_ID, stackName);
+          resource.setProperty(STACK_VERSION_PROPERTY_ID, stackVersion);
+          results.add(resource);","[{'comment': 'Can you turn these setters into a single method that can be reused?', 'commenter': 'jonathan-hurley'}]"
681,ambari-agent/src/main/python/ambari_agent/FileCache.py,"@@ -247,7 +247,7 @@ def write_hash_sum(self, directory, new_hash):
     try:
       with open(hash_file, ""w"") as fh:
         fh.write(new_hash)
-      os.chmod(hash_file, 0o666)
+      os.chmod(hash_file, 0o755)","[{'comment': 'Why 755 and not 644?  Are these files executable?  Should the be executable by all?  Maybe 640 or 600 is in order here? ', 'commenter': 'rlevas'}, {'comment': 'Hi @rlevas ,\r\n\r\nThanks for your time reviewing the code,\r\nThe files are not executable , \r\nbut i see in ambari-server we are giving the permission 755 for hash files , \r\ncode ref : https://github.com/apache/ambari/blob/5b36cdfd87b756eba922dfd1ac5419552f4d375f/ambari-server/src/main/python/ambari_server/resourceFilesKeeper.py\r\n \r\nand i thought not to change the permissions defined by ambari-server and keep the same.\r\n\r\nLet me know your thoughts, IMHO its not executable files.\r\n', 'commenter': 'Akhilsnaik'}, {'comment': 'Maybe both places need to be fixed.  \r\n\r\nDo we know who/what needs to read these hash files?  Since this file would contain only a hash I guess it can be world-readable without any issues.  However I can imagine some \r\nsecurity audit to flag this and am leaning towards closing up access to this file as much as possible. I say we start with 640, or 600 if we are paranoid.  Then open it up further if we really need to.\r\n\r\n@aonishuk , @mpapirkovskyy , @jonathan-hurley... any thoughts on this?', 'commenter': 'rlevas'}, {'comment': 'I would think that 644 would be appropriate - or even 640 for that matter. ', 'commenter': 'jonathan-hurley'}]"
709,ambari-common/src/main/python/resource_management/libraries/execution_command/execution_command.py,"@@ -0,0 +1,95 @@
+#!/usr/bin/env python
+""""""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an ""AS IS"" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+""""""
+
+__all__ = [""ExecutionCommand""]
+
+import json
+
+from resource_management.libraries.execution_command import module_configs
+
+
+class ExecutionCommand:
+  """"""
+  The class maps to a command.json. All command related info should be retrieved from this class object
+  """"""
+
+  def __init__(self, command):
+    """"""
+    __execution_command is an internal dot access dict object maps to command.json
+    :param command: json string or a python dict object
+    """"""
+    self.__execution_command = command
+    self.__module_configs = module_configs.ModuleConfigs(self.__get_value(""configurations""))
+    # self.__StackSettings instance
+    # self.__ClusterSettins instance
+
+  def __get_value(self, key, default_value=None):
+    """"""
+    A private method to query value with the full path of key, if key does not exist, return default_value
+    :param key:
+    :param default_value:
+    :return:
+    """"""
+    sub_keys = key.split('/')
+    value = self.__execution_command
+    try:
+      for sub_key in sub_keys:
+        value = value[sub_key]
+      return value
+    except:
+      return default_value
+
+  def get_module_name(self):
+    return self.__get_value(""serviceName"")
+
+  def get_stack_name(self):
+    return self.__get_value(""hostLevelParams/stack_name"")
+
+  def get_stack_version(self):
+    return self.__get_value(""hostLevelParams/stack_version"")
+
+  def get_new_stack_version_for_upgrade(self):
+    """"""
+    New Cluster Stack Version that is defined during the RESTART of a Rolling Upgrade
+    :return:
+    """"""
+    return self.__get_value(""commandParams/version"")
+
+  def get_host_name(self):
+    return self.__get_value(""hostname"")
+
+  def get_java_home(self):
+    return self.__get_value(""hostLevelParams/java_home"")
+
+  def get_java_version(self):
+    java_version = self.__get_value(""hostLevelParams/java_version"")
+    return int(java_version) if java_version else None
+
+  def get_cluster_zookeeper_hosts(self):
+    return self.__get_value(""clusterHostInfo/zookeeper_hosts"")
+
+  def get_module_configs(self):
+    return self.__module_configs
+
+  def get_cluster_settins(self):","[{'comment': 'get_cluster_settings', 'commenter': 'ncole'}]"
709,ambari-common/src/main/python/resource_management/libraries/execution_command/execution_command.py,"@@ -0,0 +1,95 @@
+#!/usr/bin/env python
+""""""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an ""AS IS"" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+""""""
+
+__all__ = [""ExecutionCommand""]
+
+import json
+
+from resource_management.libraries.execution_command import module_configs
+
+
+class ExecutionCommand:
+  """"""
+  The class maps to a command.json. All command related info should be retrieved from this class object
+  """"""
+
+  def __init__(self, command):
+    """"""
+    __execution_command is an internal dot access dict object maps to command.json
+    :param command: json string or a python dict object
+    """"""
+    self.__execution_command = command
+    self.__module_configs = module_configs.ModuleConfigs(self.__get_value(""configurations""))
+    # self.__StackSettings instance
+    # self.__ClusterSettins instance
+
+  def __get_value(self, key, default_value=None):
+    """"""
+    A private method to query value with the full path of key, if key does not exist, return default_value
+    :param key:
+    :param default_value:
+    :return:
+    """"""
+    sub_keys = key.split('/')
+    value = self.__execution_command
+    try:
+      for sub_key in sub_keys:
+        value = value[sub_key]
+      return value
+    except:
+      return default_value
+
+  def get_module_name(self):
+    return self.__get_value(""serviceName"")
+
+  def get_stack_name(self):
+    return self.__get_value(""hostLevelParams/stack_name"")
+
+  def get_stack_version(self):
+    return self.__get_value(""hostLevelParams/stack_version"")
+
+  def get_new_stack_version_for_upgrade(self):
+    """"""
+    New Cluster Stack Version that is defined during the RESTART of a Rolling Upgrade
+    :return:
+    """"""
+    return self.__get_value(""commandParams/version"")
+
+  def get_host_name(self):
+    return self.__get_value(""hostname"")
+
+  def get_java_home(self):
+    return self.__get_value(""hostLevelParams/java_home"")
+
+  def get_java_version(self):
+    java_version = self.__get_value(""hostLevelParams/java_version"")
+    return int(java_version) if java_version else None
+
+  def get_cluster_zookeeper_hosts(self):
+    return self.__get_value(""clusterHostInfo/zookeeper_hosts"")
+
+  def get_module_configs(self):
+    return self.__module_configs
+
+  def get_cluster_settins(self):
+    pass
+
+  def get_stack_settins(self):","[{'comment': 'get_stack_settings', 'commenter': 'ncole'}]"
709,ambari-common/src/main/python/resource_management/libraries/execution_command/module_configs.py,"@@ -0,0 +1,41 @@
+#!/usr/bin/env python
+""""""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an ""AS IS"" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+""""""
+
+__all__ = [""ModuleConfigs""]
+
+
+class ModuleConfigs:","[{'comment': 'I think our convention (usually, not consistently) is class Foo(object)', 'commenter': 'ncole'}]"
709,ambari-common/src/main/python/resource_management/libraries/execution_command/execution_command.py,"@@ -0,0 +1,95 @@
+#!/usr/bin/env python
+""""""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an ""AS IS"" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+""""""
+
+__all__ = [""ExecutionCommand""]
+
+import json","[{'comment': 'import ambari_simplejson as json', 'commenter': 'ncole'}]"
709,ambari-common/src/main/python/resource_management/libraries/execution_command/execution_command.py,"@@ -0,0 +1,95 @@
+#!/usr/bin/env python
+""""""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an ""AS IS"" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+""""""
+
+__all__ = [""ExecutionCommand""]
+
+import json
+
+from resource_management.libraries.execution_command import module_configs
+
+
+class ExecutionCommand:
+  """"""
+  The class maps to a command.json. All command related info should be retrieved from this class object
+  """"""
+
+  def __init__(self, command):
+    """"""
+    __execution_command is an internal dot access dict object maps to command.json
+    :param command: json string or a python dict object
+    """"""
+    self.__execution_command = command
+    self.__module_configs = module_configs.ModuleConfigs(self.__get_value(""configurations""))
+    # self.__StackSettings instance
+    # self.__ClusterSettins instance
+
+  def __get_value(self, key, default_value=None):
+    """"""
+    A private method to query value with the full path of key, if key does not exist, return default_value
+    :param key:
+    :param default_value:
+    :return:
+    """"""
+    sub_keys = key.split('/')
+    value = self.__execution_command
+    try:
+      for sub_key in sub_keys:
+        value = value[sub_key]
+      return value
+    except:
+      return default_value
+
+  def get_module_name(self):
+    return self.__get_value(""serviceName"")
+
+  def get_stack_name(self):
+    return self.__get_value(""hostLevelParams/stack_name"")
+
+  def get_stack_version(self):
+    return self.__get_value(""hostLevelParams/stack_version"")
+
+  def get_new_stack_version_for_upgrade(self):
+    """"""
+    New Cluster Stack Version that is defined during the RESTART of a Rolling Upgrade
+    :return:
+    """"""
+    return self.__get_value(""commandParams/version"")
+
+  def get_host_name(self):
+    return self.__get_value(""hostname"")
+
+  def get_java_home(self):
+    return self.__get_value(""hostLevelParams/java_home"")
+
+  def get_java_version(self):
+    java_version = self.__get_value(""hostLevelParams/java_version"")
+    return int(java_version) if java_version else None
+
+  def get_cluster_zookeeper_hosts(self):
+    return self.__get_value(""clusterHostInfo/zookeeper_hosts"")","[{'comment': 'We could conceivably end up with tons of these, given the component list we support.  We should come up with a different way to supply this information that uses actual service/component names.  The command json keys for this type of information are random.  Please open another jira for this work.', 'commenter': 'ncole'}, {'comment': ""I agree with @ncole \r\nLets just focus on module configs in this patch and incrementally add support for other properties. \r\n\r\nAs much as possible, we should make this interface generic with no service specific functions. There might be one or two exceptions but shouldn't be the norm.\r\n\r\n"", 'commenter': 'jayush'}, {'comment': 'at this time execution_command only contains very general configuration info, it does not include service specific configuration settings, the only exception is ""clusterHostInfo/zookeeper_hosts"", but it is located in clusterHostInfo block, not in ""configurations"" block. The others are very basic configurations, such as stack_version, stack_name, java_home etc. If I remove them, only:\r\n\r\ndef __get_value(self, key, default_value=None)\r\ndef get_module_name(self)\r\ndef get_module_configs(self)\r\n\r\nthree methods left.\r\n ', 'commenter': 'scottduan'}]"
711,pom.xml,"@@ -472,8 +472,7 @@
         <module>ambari-funtest</module>
         <module>ambari-agent</module>
         <module>mpack-instance-manager</module>
-        <module>ambari-client</module>
-        <module>ambari-shell</module>
+        ","[{'comment': '`ambari-client` and `ambari-shell` modules were removed in trunk.  Should be removed from all profiles.', 'commenter': 'adoroszlai'}, {'comment': 'Fixed.', 'commenter': 'mradha25'}]"
711,ambari-server/src/main/java/org/apache/ambari/server/topology/Blueprint.java,"@@ -74,7 +76,59 @@
   Setting getSetting();
 
   /**
+<<<<<<< HEAD
    * @return the set of stack (mpack) IDs associated with the blueprint
+=======","[{'comment': 'Merge conflict.', 'commenter': 'adoroszlai'}, {'comment': 'Fixed.', 'commenter': 'mradha25'}, {'comment': 'Thanks.  Two further minor issues:\r\n\r\n1. The first `@return` (the one about mpacks) is not appropriate for `getServices()`.\r\n2. Can you please also remove\r\n    ```\r\n    >>>>>>> trunk\r\n    ```\r\n    from line 129?  It does not show up as compile error, because it happens to be in a javadoc comment.', 'commenter': 'adoroszlai'}]"
711,ambari-server/src/test/java/org/apache/ambari/server/upgrade/UpgradeCatalog270Test.java,"@@ -869,13 +1087,21 @@ public void testUpdateKerberosConfigurations() throws AmbariException, NoSuchFie
         .addMockedMethod(""createConfiguration"")
         .addMockedMethod(""getClusters"", new Class[]{})
         .addMockedMethod(""createConfig"")
+        .addMockedMethod(""getClusterMetadataOnConfigsUpdate"", Cluster.class)
         .createMock();
     expect(controller.getClusters()).andReturn(clusters).anyTimes();
+<<<<<<< HEAD:ambari-server/src/test/java/org/apache/ambari/server/upgrade/UpgradeCatalog300Test.java
     expect(controller.createConfig(eq(cluster1), eq(stackId), eq(""kerberos-env""), capture(capturedProperties), anyString(), anyObject(Map.class), anyLong())).andReturn(newConfig).once();
+=======","[{'comment': 'Merge conflict.', 'commenter': 'adoroszlai'}]"
711,ambari-server/src/test/java/org/apache/ambari/server/topology/ClusterTopologyImplTest.java,"@@ -18,20 +18,24 @@
 
 package org.apache.ambari.server.topology;
 
+import static java.util.Arrays.asList;
+import static java.util.Collections.singletonList;
 import static org.easymock.EasyMock.expect;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertTrue;
 import static org.powermock.api.easymock.PowerMock.createNiceMock;
 import static org.powermock.api.easymock.PowerMock.replay;
 import static org.powermock.api.easymock.PowerMock.reset;
 import static org.powermock.api.easymock.PowerMock.verify;
 
 import java.util.Collection;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.Map;
 import java.util.Set;
 
-import org.apache.ambari.server.state.StackId;","[{'comment': 'Deleting this causes compile error:\r\n\r\n```\r\n[ERROR] COMPILATION ERROR :\r\n[INFO] -------------------------------------------------------------\r\n[ERROR] ambari-server/src/test/java/org/apache/ambari/server/topology/ClusterTopologyImplTest.java:[107,67] cannot find symbol\r\n  symbol:   class StackId\r\n  location: class org.apache.ambari.server.topology.ClusterTopologyImplTest\r\n```', 'commenter': 'adoroszlai'}, {'comment': 'Fixed', 'commenter': 'mradha25'}]"
716,ambari-server/src/main/java/org/apache/ambari/server/topology/Component.java,"@@ -54,12 +59,20 @@ public String getName() {
   }
 
   /**
-   * @return the mpack associated with this component (can be {@code null} if component -> mpack mapping is unambiguous)
+   * @return the mpack associated with this component as {@link String} (can be {@code null} if component -> mpack mapping is unambiguous)","[{'comment': ""Minor issue in the javadocs: \r\n\r\nWouldn't this method return null if the mpack mapping was ambiguous, rather than unambiguous?  \r\n\r\n"", 'commenter': 'rnettleton'}]"
760,ambari-server/src/main/python/ambari_server/setupSso.py,"@@ -47,6 +47,8 @@
 JWT_PUBLIC_KEY_HEADER = ""-----BEGIN CERTIFICATE-----\n""
 JWT_PUBLIC_KEY_FOOTER = ""\n-----END CERTIFICATE-----\n""
 
+REGEX_URL = ""http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+""","[{'comment': 'This regular expression seems to be extremely lenient and I am not sure it it really doing all that much to validate a true URL other than ensure it begins with http.  Maybe just skip the URL validation or change it to something like \r\n\r\n```\r\nhttps?:\\/\\/.*\r\n```\r\nI think we can go really crazy here validating the URL but the regular expression will be huge and unreadable.  The host side of the equation is huge alone:\r\n\r\n```\r\n(([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\\-]*[a-zA-Z0-9])\\.)*([A-Za-z0-9]|[A-Za-z0-9][A-Za-z0-9\\-]*[A-Za-z0-9])\r\n```\r\n_From https://stackoverflow.com/questions/106179/regular-expression-to-match-dns-hostname-or-ip-address_\r\n\r\nBu the user can choose to use an IP address as well, so the expression will be even larger just to validate the host part of this.', 'commenter': 'rlevas'}, {'comment': ""I used the regex found on this page: http://www.noah.org/wiki/RegEx_Python#URL_regex_pattern\r\n\r\nI'd not drop URL validation in the tool, instead we may consider using django's `URLValidator` RE:\r\n```\r\nregex = re.compile(\r\n    r'^(?:http|ftp)s?://' # http:// or https://\r\n    r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|' # domain...\r\n    r'localhost|' # localhost...\r\n    r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}|' # ...or ipv4\r\n    r'\\[?[A-F0-9]*:[A-F0-9:]+\\]?)' # ...or ipv6\r\n    r'(?::\\d+)?' # optional port\r\n    r'(?:/?|[/?]\\S+)$', re.IGNORECASE)\r\n```\r\nOr, I'm not sure if importing 3rd party modules is an option, we may use the django lib.\r\nFurther info is available here: https://github.com/django/django/blob/master/django/core/validators.py\r\n\r\nWhat do you think?"", 'commenter': 'smolnar82'}, {'comment': 'ok.. well then leave what you have since others seem to think it is a useful Regex for URLs. ', 'commenter': 'rlevas'}]"
760,ambari-server/src/main/python/ambari_server/setupSso.py,"@@ -58,7 +60,7 @@ def validateOptions(options):
       errors.append(""Missing option: --sso-provider-url"")
     if not options.sso_public_cert_file:
       errors.append(""Missing option: --sso-public-cert-file"")
-    if options.sso_provider_url and not re.search(REGEX_HOSTNAME_PORT, options.sso_provider_url):
+    if options.sso_provider_url and not re.search(REGEX_URL, options.sso_provider_url):","[{'comment': 'why not re.match? \r\nhttps://docs.python.org/3/library/re.html#search-vs-match\r\n', 'commenter': 'Unknown'}, {'comment': 'any string that _contains_ url would pass the condition', 'commenter': 'Unknown'}, {'comment': 'Nice catch...let me fix it', 'commenter': 'smolnar82'}, {'comment': 'Fixed; could you please review again?', 'commenter': 'smolnar82'}]"
765,ambari-server/src/main/python/ambari_server/setupSso.py,"@@ -113,15 +125,101 @@ def populateJwtAudiences(options, properties):
     audiences = options.sso_jwt_audience_list
 
   properties.process_pair(JWT_AUDIENCES, audiences)
+  
+def get_eligible_services(properties, admin_login, admin_password, cluster_name):
+  url = get_ambari_server_api_base(properties) + URL_TO_FETCH_SERVICES_ELIGIBLE_FOR_SSO.replace("":CLUSTER_NAME"", cluster_name)
+  admin_auth = base64.encodestring('%s:%s' % (admin_login, admin_password)).replace('\n', '')
+  request = urllib2.Request(url)
+  request.add_header('Authorization', 'Basic %s' % admin_auth)
+  request.add_header('X-Requested-By', 'ambari')
+  request.get_method = lambda: 'GET'
+
+  services = []
+  sys.stdout.write('\nFetching SSO enabled services')
+  numOfTries = 0
+  request_in_progress = True
+  while request_in_progress:
+    numOfTries += 1
+    if (numOfTries == 60):
+      raise FatalException(1, ""Could not fetch eligible services within a minute; giving up!"")
+    sys.stdout.write('.')
+    sys.stdout.flush()
+
+    try:
+      with closing(urllib2.urlopen(request)) as response:
+        response_status_code = response.getcode()
+        if response_status_code != 200:
+          request_in_progress = False
+          err = 'Error while fetching eligible services. Http status code - ' + str(response_status_code)
+          raise FatalException(1, err)
+        else:
+            response_body = json.loads(response.read())
+            items = response_body['items']
+            if len(items) > 0:
+              for item in items:
+                services.append(item['ServiceInfo']['service_name'])
+            if not items:
+              time.sleep(1)
+            else:
+              request_in_progress = False
+
+    except Exception as e:
+      request_in_progress = False
+      err = 'Error while fetching eligible services. Error details: %s' % e
+      raise FatalException(1, err)
+  if (len(services) == 0):
+    sys.stdout.write('\nThere is no SSO enabled services found\n')
+  else:
+    sys.stdout.write('\nFound SSO enabled services: {0}\n'.format(', '.join(str(s) for s in services)))
+  return services
+
+def get_services_requires_sso(options, properties, admin_login, admin_password):","[{'comment': 'we usually put 2 blank lines between methods (PEP 8 and default Idea warnings)', 'commenter': 'Unknown'}]"
765,ambari-server/src/main/python/ambari_server/setupSso.py,"@@ -66,7 +78,7 @@ def validateOptions(options):
     raise FatalException(1, error_msg.format(str(errors)))
 
 
-def populateSsoProviderUrl(options, properties):
+def populate_sso_provide_url(options, properties):","[{'comment': ""This should probably be renamed to 'populate_sso_provider_url'"", 'commenter': 'rlevas'}, {'comment': ""yepp; I'll fix it."", 'commenter': 'smolnar82'}, {'comment': 'Fixed', 'commenter': 'smolnar82'}]"
765,ambari-server/src/main/python/ambari_server/setupSso.py,"@@ -113,15 +125,101 @@ def populateJwtAudiences(options, properties):
     audiences = options.sso_jwt_audience_list
 
   properties.process_pair(JWT_AUDIENCES, audiences)
+  
+def get_eligible_services(properties, admin_login, admin_password, cluster_name):
+  url = get_ambari_server_api_base(properties) + URL_TO_FETCH_SERVICES_ELIGIBLE_FOR_SSO.replace("":CLUSTER_NAME"", cluster_name)","[{'comment': '`clustername` should probably be URL encoded.', 'commenter': 'rlevas'}, {'comment': 'ok', 'commenter': 'smolnar82'}, {'comment': 'Done', 'commenter': 'smolnar82'}]"
765,ambari-server/src/main/python/ambari_server/setupSso.py,"@@ -113,15 +125,101 @@ def populateJwtAudiences(options, properties):
     audiences = options.sso_jwt_audience_list
 
   properties.process_pair(JWT_AUDIENCES, audiences)
+  
+def get_eligible_services(properties, admin_login, admin_password, cluster_name):
+  url = get_ambari_server_api_base(properties) + URL_TO_FETCH_SERVICES_ELIGIBLE_FOR_SSO.replace("":CLUSTER_NAME"", cluster_name)
+  admin_auth = base64.encodestring('%s:%s' % (admin_login, admin_password)).replace('\n', '')
+  request = urllib2.Request(url)
+  request.add_header('Authorization', 'Basic %s' % admin_auth)
+  request.add_header('X-Requested-By', 'ambari')
+  request.get_method = lambda: 'GET'
+
+  services = []
+  sys.stdout.write('\nFetching SSO enabled services')
+  numOfTries = 0
+  request_in_progress = True
+  while request_in_progress:
+    numOfTries += 1
+    if (numOfTries == 60):
+      raise FatalException(1, ""Could not fetch eligible services within a minute; giving up!"")
+    sys.stdout.write('.')
+    sys.stdout.flush()
+
+    try:
+      with closing(urllib2.urlopen(request)) as response:
+        response_status_code = response.getcode()
+        if response_status_code != 200:
+          request_in_progress = False
+          err = 'Error while fetching eligible services. Http status code - ' + str(response_status_code)
+          raise FatalException(1, err)
+        else:
+            response_body = json.loads(response.read())
+            items = response_body['items']
+            if len(items) > 0:
+              for item in items:
+                services.append(item['ServiceInfo']['service_name'])
+            if not items:
+              time.sleep(1)
+            else:
+              request_in_progress = False
+
+    except Exception as e:
+      request_in_progress = False
+      err = 'Error while fetching eligible services. Error details: %s' % e
+      raise FatalException(1, err)
+  if (len(services) == 0):
+    sys.stdout.write('\nThere is no SSO enabled services found\n')
+  else:
+    sys.stdout.write('\nFound SSO enabled services: {0}\n'.format(', '.join(str(s) for s in services)))
+  return services
+
+def get_services_requires_sso(options, properties, admin_login, admin_password):
+  if not options.sso_enabled_services:
+    configure_for_all_services = get_YN_input(""Use SSO for all services [y/n] (n): "", False)
+    if configure_for_all_services:
+      services = WILDCARD_FOR_ALL_SERVICES
+    else:
+      cluster_name = get_cluster_name(properties, admin_login, admin_password)
+      eligible_services = get_eligible_services(properties, admin_login, admin_password, cluster_name)
+      services = ''
+      for service in eligible_services:
+        question = ""Use SSO for {0} [y/n] (y): "".format(service)
+        if get_YN_input(question, True):
+          if len(services) > 0:
+            services = services + "", ""
+          services = services + service
+  else:
+    services = options.sso_enabled_services
+
+  return services
+
+
+def update_sso_conf(properties, services, admin_login, admin_password):
+  sso_configuration_properties = {}","[{'comment': 'Add `ambari.sso.manage_services` to the properties indicating ""true"" to trigger Ambari to manage services, or ""false"" to have Ambari do nothing.  ', 'commenter': 'rlevas'}, {'comment': 'ok', 'commenter': 'smolnar82'}, {'comment': 'Done', 'commenter': 'smolnar82'}]"
776,ambari-server/src/main/java/org/apache/ambari/server/mpack/MpackManager.java,"@@ -200,6 +201,11 @@ public MpackResponse registerMpack(MpackRequest mpackRequest)
 
       if (isValidMetadata) {
         mpackTarPath = downloadMpack(mpackRequest.getMpackUri(), mpack.getDefinition());
+        if (!validateMpackTarballChecksum(mpackTarPath.toString(), mpack.getChecksum())) {
+          StringBuilder message = new StringBuilder(""Incorrect checksum for downloaded mpack tarball "");
+          message.append(mpackName);","[{'comment': 'Maybe just a simple String.format here? No need for a builder...', 'commenter': 'jonathan-hurley'}]"
776,ambari-server/src/main/java/org/apache/ambari/server/mpack/MpackManager.java,"@@ -479,6 +489,29 @@ private void createSymLinks(Mpack mpack) throws IOException {
     Files.createSymbolicLink(stackPath, mpackPath);
   }
 
+  /***
+   * After mpack tarball is downloaded, before it is extacted, do md5 validation to make sure it is valid
+   * @param tarballPath mpack tarball path
+   * @param md5 checksum to be verified
+   * @return true if md5 matches mpack md5
+   */
+  private boolean validateMpackTarballChecksum(String tarballPath, String md5) {
+    boolean matched = true;
+    try {
+      FileInputStream fis = new FileInputStream(tarballPath);
+      String generatedMd5 = DigestUtils.md5Hex(fis).toLowerCase();
+      if (!md5.equals(generatedMd5)) {
+        LOG.error(""Mpack file {} does not match provided MD5 {}"", tarballPath, md5);
+        matched = false;
+      }
+      fis.close();","[{'comment': 'Close this safely. ', 'commenter': 'jonathan-hurley'}]"
776,ambari-server/src/main/java/org/apache/ambari/server/mpack/MpackManager.java,"@@ -212,7 +218,11 @@ public MpackResponse registerMpack(MpackRequest mpackRequest)
       // Mpack registration using direct download
       mpack = downloadMpackMetadata(mpackRequest.getMpackUri());
       mpackTarPath = downloadMpack(mpackRequest.getMpackUri(), mpack.getDefinition());
-
+      if (!validateMpackTarballChecksum(mpackTarPath.toString(), mpack.getChecksum())) {
+        StringBuilder message = new StringBuilder(""Incorrect checksum for downloaded mpack tarball "");
+        message.append(mpackName);
+        throw new IllegalArgumentException(message.toString()); //Mismatch in information","[{'comment': 'StringBuilder seems unnecessary here.', 'commenter': 'ncole'}]"
776,ambari-server/src/main/java/org/apache/ambari/server/mpack/MpackManager.java,"@@ -479,6 +489,29 @@ private void createSymLinks(Mpack mpack) throws IOException {
     Files.createSymbolicLink(stackPath, mpackPath);
   }
 
+  /***
+   * After mpack tarball is downloaded, before it is extacted, do md5 validation to make sure it is valid
+   * @param tarballPath mpack tarball path
+   * @param md5 checksum to be verified
+   * @return true if md5 matches mpack md5
+   */
+  private boolean validateMpackTarballChecksum(String tarballPath, String md5) {
+    boolean matched = true;
+    try {
+      FileInputStream fis = new FileInputStream(tarballPath);
+      String generatedMd5 = DigestUtils.md5Hex(fis).toLowerCase();
+      if (!md5.equals(generatedMd5)) {
+        LOG.error(""Mpack file {} does not match provided MD5 {}"", tarballPath, md5);","[{'comment': 'Apache process uses sha512, should we allow that other than just md5?', 'commenter': 'ncole'}]"
776,ambari-server/src/main/java/org/apache/ambari/server/state/Mpack.java,"@@ -244,6 +255,7 @@ public boolean equals(Object o) {
     equalsBuilder.append(description, that.description);
     equalsBuilder.append(mpackUri, that.mpackUri);
     equalsBuilder.append(displayName, that.displayName);
+    equalsBuilder.append(checksum, that.checksum);","[{'comment': 'Does checksum really need to be part of equals()?', 'commenter': 'ncole'}]"
783,ambari-common/src/main/python/resource_management/libraries/functions/get_user_call_output.py,"@@ -40,10 +41,11 @@ def get_user_call_output(command, user, quiet=False, is_checked_call=True, **cal
   try:
     out_files.append(tempfile.NamedTemporaryFile())
     out_files.append(tempfile.NamedTemporaryFile())
-    
+    uid = pwd.getpwnam(user).pw_uid
     # other user should be able to write to it","[{'comment': 'At some point someone thought that the generated files should be world writable...\r\n\r\n```\r\n# other user should be able to write to it\r\n```\r\n\r\nHowever, looking at the function it seems like only the owner of the process executing the command needs to write to them.  The trick though is setting the correct owner when the Ambari agent does not run as root. \r\n\r\n@Akhilsnaik , have you tested this when Ambari agent is not run as root?\r\n\r\n@aonishuk, and comment on this?', 'commenter': 'rlevas'}]"
783,ambari-common/src/main/python/resource_management/libraries/functions/get_user_call_output.py,"@@ -40,10 +41,11 @@ def get_user_call_output(command, user, quiet=False, is_checked_call=True, **cal
   try:
     out_files.append(tempfile.NamedTemporaryFile())
     out_files.append(tempfile.NamedTemporaryFile())
-    
+    uid = pwd.getpwnam(user).pw_uid
     # other user should be able to write to it
     for f in out_files:
-      os.chmod(f.name, 0666)
+      os.chown(f.name,uid,-1)
+      os.chmod(f.name, 0644)","[{'comment': 'why not 0600? Any process in system may read output that contains some sensitive data', 'commenter': 'Unknown'}, {'comment': '@dlysnichenko ,\r\n\r\nHi the permission is not 600 because this functions is in libraries and we are using it in few places.\r\none instance is that storm user will be writing the file and ambari-agent user will be reading it.\r\nso its necessary we give 0644 permissions for the /tmp file created.\r\n', 'commenter': 'Akhilsnaik'}, {'comment': 'ok, makes sense. ', 'commenter': 'Unknown'}]"
785,ambari-server/src/main/java/org/apache/ambari/server/api/services/HostComponentService.java,"@@ -35,20 +35,37 @@
 import javax.ws.rs.QueryParam;
 import javax.ws.rs.core.Context;
 import javax.ws.rs.core.HttpHeaders;
+import javax.ws.rs.core.MediaType;
 import javax.ws.rs.core.Response;
 import javax.ws.rs.core.UriInfo;
 
 import org.apache.ambari.annotations.ApiIgnore;
 import org.apache.ambari.server.api.resources.ResourceInstance;
 import org.apache.ambari.server.configuration.Configuration;
+import org.apache.ambari.server.controller.HostComponentProcessResponse;
+import org.apache.ambari.server.controller.HostComponentSwagger;
 import org.apache.ambari.server.controller.spi.Resource;
 import org.apache.commons.lang.StringUtils;
 import org.apache.commons.lang.Validate;
 
+import org.apache.http.HttpStatus;
+
+import io.swagger.annotations.Api;
+import io.swagger.annotations.ApiImplicitParam;
+import io.swagger.annotations.ApiImplicitParams;
+import io.swagger.annotations.ApiOperation;
+import io.swagger.annotations.ApiResponse;
+import io.swagger.annotations.ApiResponses;
+
 /**
  * Service responsible for host_components resource requests.
  */
+@Path(""/hosts"")
+@Api(value = ""Hosts"", description = ""Endpoint for host-specific operations"")","[{'comment': 'Actually, this is a nested API. The ""/hosts"" path is used by HostService. Nested API\'s don\'t have @Path annotations (except if they are top level API\'s at the same time).\r\n\r\nThe HostComponentService is accessible from HostService.getHostComponentHandler(). That means its path will be synthesized from HostService\'s top level path (""/hosts"") and getHostComponentHandler()\'s path (""{hostName}/host_components"").\r\n\r\nNested API\'s are not supported by Swagger out of the box but we have written a custom plugin that handles simple cases (only one level of nesting, single parent for nested API\'s.)\r\n\r\nPlease see ClusterService (a top level service) and ServiceService (nested under ClusterService) for example.', 'commenter': 'benyoka'}]"
785,ambari-utility/src/main/java/org/apache/ambari/swagger/AmbariSwaggerReader.java,"@@ -84,32 +85,54 @@ public Swagger read(Set<Class<?>> classes) {
           Class<?> returnType = method.getReturnType();
           Api nestedApi = AnnotationUtils.findAnnotation(returnType, Api.class);
           Path nestedApiPath = AnnotationUtils.findAnnotation(returnType, Path.class);
-          logger.debug(""Examinig API method {}#{}, path={}, returnType={}"", cls.getSimpleName(), method.getName(),
+          logger.debug(""Examining API method {}#{}, path={}, returnType={}"", cls.getSimpleName(), method.getName(),
               nestedApiPath != null ? nestedApiPath.value() : null, returnType.getSimpleName());
           if (null != nestedApi) {
             if (null != nestedApiPath) {
               logger.info(""This class exists both as top level and nested API: {}, treating it as top level API"",
                   returnType.getName());
             }
             else {
-              Path apiPath = AnnotationUtils.findAnnotation(cls, Path.class);
-              String apiPathValue;
-              if (null == apiPath) {
-                logger.warn(""Parent api {} also seems to be a nested API. The current version does not support "" +
-                    ""multi-level nesting."");
-                apiPathValue = """";
-              }
-              else {
-                apiPathValue = apiPath.value();
-              }
-              NestedApiRecord nar = new NestedApiRecord(returnType, cls, apiPathValue, method, methodPath.value());
+              Boolean skipAdd = false;
+              Class<?> preferredParentClass = cls;
+
+              // API is a nested API of multiple top level APIs
               if (nestedAPIs.containsKey(returnType)) {
-                logger.warn(""{} is a nested API of multiple top level API's. Ignoring top level API {}"", returnType, cls);
+                SwaggerPreferredParent preferredParentAnnotation = AnnotationUtils.findAnnotation(returnType,
+                        SwaggerPreferredParent.class);
+                if (null != preferredParentAnnotation) {
+                  preferredParentClass = preferredParentAnnotation.preferredParent();
+                  if (nestedAPIs.get(returnType).parentApi.getName().equals(preferredParentClass.getName())) {
+                    skipAdd = true;
+                  } else {
+                    logger.info(""Setting top level API of {} to {} based on @SwaggerPreferredParent "" +
+                            ""annotation"", returnType, preferredParentClass.getSimpleName());
+                    try {
+                      method = preferredParentClass.getMethod(method.getName(), method.getParameterTypes());
+                    } catch (NoSuchMethodException exc) {
+                      skipAdd = true;
+                      logger.error(""{} class defined as parent API is invalid due to method mismatch! Ignoring "" +
+                              ""API {}"", preferredParentClass, returnType);
+                    }
+                  }
+                } else {
+                  logger.warn(""{} is a nested API of multiple top level API's. Ignoring top level API {}"",
+                          returnType, cls);
+                  skipAdd = true;
+                }
               }
-              else {
-                logger.info(""Registering nested API: {}"", returnType);
-                nestedAPIs.put(returnType, nar);
+
+              if (skipAdd) {
+                continue;
+              } else {
+                nestedAPIs.remove(returnType);
+                cls = preferredParentClass;
               }
+
+              logger.info(""Registering nested API: {}"", returnType);
+              NestedApiRecord nar = new NestedApiRecord(returnType, cls, validateParentApiPath(cls), method,","[{'comment': 'For better readability I would skip the assignment in line 129 and would use preferredParentClass instead of cls in line 133.', 'commenter': 'benyoka'}]"
785,ambari-utility/src/main/java/org/apache/ambari/swagger/AmbariSwaggerReader.java,"@@ -84,32 +85,53 @@ public Swagger read(Set<Class<?>> classes) {
           Class<?> returnType = method.getReturnType();
           Api nestedApi = AnnotationUtils.findAnnotation(returnType, Api.class);
           Path nestedApiPath = AnnotationUtils.findAnnotation(returnType, Path.class);
-          logger.debug(""Examinig API method {}#{}, path={}, returnType={}"", cls.getSimpleName(), method.getName(),
+          logger.debug(""Examining API method {}#{}, path={}, returnType={}"", cls.getSimpleName(), method.getName(),
               nestedApiPath != null ? nestedApiPath.value() : null, returnType.getSimpleName());
           if (null != nestedApi) {
             if (null != nestedApiPath) {
               logger.info(""This class exists both as top level and nested API: {}, treating it as top level API"",
                   returnType.getName());
             }
             else {
-              Path apiPath = AnnotationUtils.findAnnotation(cls, Path.class);
-              String apiPathValue;
-              if (null == apiPath) {
-                logger.warn(""Parent api {} also seems to be a nested API. The current version does not support "" +
-                    ""multi-level nesting."");
-                apiPathValue = """";
-              }
-              else {
-                apiPathValue = apiPath.value();
-              }
-              NestedApiRecord nar = new NestedApiRecord(returnType, cls, apiPathValue, method, methodPath.value());
+              Boolean skipAdd = false;","[{'comment': 'Should be `boolean`', 'commenter': 'adoroszlai'}]"
785,ambari-utility/src/test/java/org/apache/ambari/swagger/AmbariSwaggerReaderTest.java,"@@ -88,6 +89,61 @@ public void swaggerConflictingNestedApis() {
     assertPathParamsExist(swagger, ""/toplevel/{param}/nested/list"", ""param"");
   }
 
+  /**
+   * Test conflicting nested API's (the same API's are returned from different top level API's) with
+   * {@link SwaggerPreferredParent} annotation.
+   * In this case the nested API should be associated to the preferred top level API.
+   */
+  @Test
+  public void swaggerConflictingNestedApisWithPreferredParent() {
+    AmbariSwaggerReader asr = new AmbariSwaggerReader(null, createMock(Log.class));
+    ListOrderedSet classes = ListOrderedSet.decorate(","[{'comment': '`ListOrderedSet` is a raw type, produces compiler warning.  Can you please use `LinkedHashSet` instead?', 'commenter': 'adoroszlai'}]"
788,ambari-logsearch/ambari-logsearch-logfeeder/src/main/scripts/logfeeder.sh,"@@ -56,7 +56,12 @@ if [ ! -z ""$LOGSEARCH_SOLR_CLIENT_SSL_INCLUDE"" ]; then
 fi
 
 if [ -z ""$LOGFEEDER_PID_FILE"" ]; then
-  LOGFEEDER_PID_DIR=$HOME
+  LOGFEEDER_DEFAULT_PID_DIR=""/var/run/ambari-logsearch-logfeeder""
+  if [ -d ""$LOGFEEDER_DEFAULT_PID_DIR"" ]; then
+    LOGFEEDER_PID_DIR=LOGFEEDER_DEFAULT_PID_DIR","[{'comment': 'Missing `$`', 'commenter': 'adoroszlai'}]"
808,ambari-server/src/main/java/org/apache/ambari/server/state/Module.java,"@@ -129,13 +131,15 @@ public void setComponents(List<ModuleComponent> components) {
     this.components = components;
   }
 
+  /**
+   * Fetch a particular module component by the component name.
+   * @param moduleComponentName
+   * @return
+   */
   public ModuleComponent getModuleComponent(String moduleComponentName) {
-    for (ModuleComponent moduleComponent : components) {
-      if (StringUtils.equals(moduleComponentName, moduleComponent.getName())) {
-        return moduleComponent;
-      }
+    if(componentHashMap.containsKey(moduleComponentName)){
+      return componentHashMap.get(moduleComponentName);","[{'comment': 'Why the if-check here, just return map.get()', 'commenter': 'jonathan-hurley'}]"
808,ambari-server/src/main/java/org/apache/ambari/server/state/Mpack.java,"@@ -189,12 +191,9 @@ public void setRepositoryXml(RepositoryXml repositoryXml) {
    * @return the module or {@code null}.
    */
   public Module getModule(String moduleName) {
-    for (Module module : modules) {
-      if (StringUtils.equals(moduleName, module.getName())) {
-        return module;
-      }
+    if(moduleHashMap.containsKey(moduleName)){
+      return moduleHashMap.get(moduleName);","[{'comment': 'No need for the if-check - just return map.get()', 'commenter': 'jonathan-hurley'}]"
808,ambari-server/src/main/java/org/apache/ambari/server/state/Mpack.java,"@@ -208,13 +207,12 @@ public Module getModule(String moduleName) {
    * @return the component or {@code null}.
    */
   public ModuleComponent getModuleComponent(String moduleName, String moduleComponentName) {
-    for (Module module : modules) {
-      ModuleComponent moduleComponent = module.getModuleComponent(moduleComponentName);
-      if (null != moduleComponent) {
-        return moduleComponent;
+    if(moduleHashMap.containsKey(moduleName)){
+      Module module = moduleHashMap.get(moduleName);
+      if(module.getModuleComponent(moduleComponentName) != null){
+        return module.getModuleComponent(moduleComponentName);","[{'comment': 'Same - just return map.get()', 'commenter': 'jonathan-hurley'}]"
824,ambari-server/src/test/java/org/apache/ambari/server/utils/PasswordUtilsTest.java,"@@ -90,7 +90,7 @@ public void shouldReadDefaultPasswordIfPasswordPropertyIsPasswordFilePathButItIs
     final String testPassword = ""ambariTest"";
     final File passwordFile = writeTestPasswordFile(testPassword);
     passwordFile.setReadable(false);
-    assertEquals(""testPasswordDefault"", passwordUtils.readPassword(passwordFile.getAbsolutePath(), ""testPasswordDefault""));
+    //assertEquals(""testPasswordDefault"", passwordUtils.readPassword(passwordFile.getAbsolutePath(), ""testPasswordDefault""));","[{'comment': 'Do you want to skip the test altogether? Like @Ignore', 'commenter': 'shavi71'}, {'comment': 'I agree with @shavi71 , use `@Ignore` rather then comment out the line.  At least the test will be logged as skipped this way.\r\n\r\nWhat is failing about this test?  Is there a JIRA file for fixing it?', 'commenter': 'rlevas'}, {'comment': 'Done', 'commenter': 'smolnar82'}]"
835,ambari-common/src/main/python/resource_management/libraries/execution_command/execution_command.py,"@@ -54,32 +54,98 @@ def __get_value(self, key, default_value=None):
     except:
       return default_value
 
+  """"""
+  Global variables section
+  """"""
+
+  def get_module_configs(self):
+    return self.__module_configs
+
   def get_module_name(self):
     return self.__get_value(""serviceName"")
 
-  def get_stack_name(self):
-    return self.__get_value(""hostLevelParams/stack_name"")
+  def get_component_type(self):
+    return self.__get_value(""role"")
 
-  def get_stack_version(self):
-    return self.__get_value(""hostLevelParams/stack_version"")
+  def get_component_instance_default_name(self):
+    return self.__get_value(""default"")
 
-  def get_new_stack_version_for_upgrade(self):
-    """"""
-    New Cluster Stack Version that is defined during the RESTART of a Rolling Upgrade
-    :return:
-    """"""
-    return self.__get_value(""commandParams/version"")
+  def get_servicegroup_name(self):
+    return self.__get_value(""serviceGroupName"")
 
   def get_host_name(self):
     return self.__get_value(""hostname"")
 
+  def get_cluster_name(self):
+    return self.__get_value(""clusterName"")
+
+  """"""
+  Host related variables section
+  """"""
+
+  def get_mpack_name(self):
+    return self.__get_value(""hostLevelParams/stack_name"")","[{'comment': 'Isnt this going to be called mpack_name and mpack_version in the json as well?', 'commenter': 'mradha25'}, {'comment': 'This should be clusterLevelParams/stack_name\r\n', 'commenter': 'jayush'}, {'comment': 'Not sure, we can change later if it is mpack_name in json', 'commenter': 'scottduan'}]"
835,ambari-common/src/main/python/resource_management/libraries/execution_command/execution_command.py,"@@ -54,32 +54,98 @@ def __get_value(self, key, default_value=None):
     except:
       return default_value
 
+  """"""
+  Global variables section
+  """"""
+
+  def get_module_configs(self):
+    return self.__module_configs
+
   def get_module_name(self):
     return self.__get_value(""serviceName"")
 
-  def get_stack_name(self):
-    return self.__get_value(""hostLevelParams/stack_name"")
+  def get_component_type(self):
+    return self.__get_value(""role"")
 
-  def get_stack_version(self):
-    return self.__get_value(""hostLevelParams/stack_version"")
+  def get_component_instance_default_name(self):
+    return self.__get_value(""default"")
 
-  def get_new_stack_version_for_upgrade(self):
-    """"""
-    New Cluster Stack Version that is defined during the RESTART of a Rolling Upgrade
-    :return:
-    """"""
-    return self.__get_value(""commandParams/version"")
+  def get_servicegroup_name(self):
+    return self.__get_value(""serviceGroupName"")
 
   def get_host_name(self):
     return self.__get_value(""hostname"")
 
+  def get_cluster_name(self):
+    return self.__get_value(""clusterName"")
+
+  """"""
+  Host related variables section
+  """"""
+
+  def get_mpack_name(self):
+    return self.__get_value(""hostLevelParams/stack_name"")
+
+  def get_mpack_version(self):
+    return self.__get_value(""hostLevelParams/stack_version"")
+
   def get_java_home(self):
     return self.__get_value(""hostLevelParams/java_home"")
 
   def get_java_version(self):
     java_version = self.__get_value(""hostLevelParams/java_version"")
     return int(java_version) if java_version else None
 
-  def get_module_configs(self):
-    return self.__module_configs
+  def get_jdk_location(self):
+    java_version = self.__get_value(""hostLevelParams/jdk_location"")","[{'comment': 'This might now be in  ambariLevelParams.put(JDK_LOCATION, getJdkResourceUrl());', 'commenter': 'mradha25'}, {'comment': 'Should be ambariLevelParams/jdk_location', 'commenter': 'jayush'}]"
835,ambari-common/src/main/python/resource_management/libraries/execution_command/execution_command.py,"@@ -54,32 +54,98 @@ def __get_value(self, key, default_value=None):
     except:
       return default_value
 
+  """"""
+  Global variables section
+  """"""
+
+  def get_module_configs(self):
+    return self.__module_configs
+
   def get_module_name(self):
     return self.__get_value(""serviceName"")
 
-  def get_stack_name(self):
-    return self.__get_value(""hostLevelParams/stack_name"")
+  def get_component_type(self):
+    return self.__get_value(""role"")
 
-  def get_stack_version(self):
-    return self.__get_value(""hostLevelParams/stack_version"")
+  def get_component_instance_default_name(self):
+    return self.__get_value(""default"")
 
-  def get_new_stack_version_for_upgrade(self):
-    """"""
-    New Cluster Stack Version that is defined during the RESTART of a Rolling Upgrade
-    :return:
-    """"""
-    return self.__get_value(""commandParams/version"")
+  def get_servicegroup_name(self):
+    return self.__get_value(""serviceGroupName"")
 
   def get_host_name(self):
     return self.__get_value(""hostname"")
 
+  def get_cluster_name(self):
+    return self.__get_value(""clusterName"")
+
+  """"""
+  Host related variables section
+  """"""
+
+  def get_mpack_name(self):
+    return self.__get_value(""hostLevelParams/stack_name"")
+
+  def get_mpack_version(self):
+    return self.__get_value(""hostLevelParams/stack_version"")
+
   def get_java_home(self):
     return self.__get_value(""hostLevelParams/java_home"")
 
   def get_java_version(self):
     java_version = self.__get_value(""hostLevelParams/java_version"")
     return int(java_version) if java_version else None
 
-  def get_module_configs(self):
-    return self.__module_configs
+  def get_jdk_location(self):
+    java_version = self.__get_value(""hostLevelParams/jdk_location"")
+    return int(java_version) if java_version else None
+
+  def check_agent_stack_want_retry_on_unavailability(self):
+    return bool('hostLevelParams/agent_stack_retry_on_unavailability')
+
+  def get_agent_stack_retry_count_on_unavailability(self):
+    return int('hostLevelParams/agent_stack_retry_count')
+
+  """"""
+  Cluster related variables section
+  """"""
+
+  def get_ambari_server_host(self):
+    return self.__get_value(""clusterHostInfo/ambari_server_host"")","[{'comment': 'This should be ""ambariLevelParams/ambari_server_host""\r\n\r\n    ""ambariLevelParams"": {\r\n        ""ambari_server_host"": ""jay-c7-1.c.pramod-thangali.internal"",\r\n        ""ambari_server_port"": ""8080"",\r\n        ""ambari_server_use_ssl"": ""false""\r\n    },', 'commenter': 'jayush'}]"
835,ambari-common/src/main/python/resource_management/libraries/execution_command/execution_command.py,"@@ -54,32 +54,98 @@ def __get_value(self, key, default_value=None):
     except:
       return default_value
 
+  """"""
+  Global variables section
+  """"""
+
+  def get_module_configs(self):
+    return self.__module_configs
+
   def get_module_name(self):
     return self.__get_value(""serviceName"")
 
-  def get_stack_name(self):
-    return self.__get_value(""hostLevelParams/stack_name"")
+  def get_component_type(self):
+    return self.__get_value(""role"")
 
-  def get_stack_version(self):
-    return self.__get_value(""hostLevelParams/stack_version"")
+  def get_component_instance_default_name(self):
+    return self.__get_value(""default"")
 
-  def get_new_stack_version_for_upgrade(self):
-    """"""
-    New Cluster Stack Version that is defined during the RESTART of a Rolling Upgrade
-    :return:
-    """"""
-    return self.__get_value(""commandParams/version"")
+  def get_servicegroup_name(self):
+    return self.__get_value(""serviceGroupName"")
 
   def get_host_name(self):
     return self.__get_value(""hostname"")
 
+  def get_cluster_name(self):
+    return self.__get_value(""clusterName"")
+
+  """"""
+  Host related variables section
+  """"""
+
+  def get_mpack_name(self):
+    return self.__get_value(""hostLevelParams/stack_name"")
+
+  def get_mpack_version(self):
+    return self.__get_value(""hostLevelParams/stack_version"")
+
   def get_java_home(self):
     return self.__get_value(""hostLevelParams/java_home"")
 
   def get_java_version(self):
     java_version = self.__get_value(""hostLevelParams/java_version"")
     return int(java_version) if java_version else None
 
-  def get_module_configs(self):
-    return self.__module_configs
+  def get_jdk_location(self):
+    java_version = self.__get_value(""hostLevelParams/jdk_location"")
+    return int(java_version) if java_version else None
+
+  def check_agent_stack_want_retry_on_unavailability(self):
+    return bool('hostLevelParams/agent_stack_retry_on_unavailability')","[{'comment': 'Can we revisit some of the fields based on this?\r\n   ""ambariLevelParams"": {\r\n        ""jdk_location"": ""http://swap-backedoutUItest-1.openstacklocal:8080/resources/"",\r\n        ""agent_stack_retry_count"": ""5"",\r\n        ""db_driver_filename"": ""mysql-connector-java.jar"",\r\n        ""agent_stack_retry_on_unavailability"": ""false"",\r\n        ""jce_name"": ""jce_policy-8.zip"",\r\n        ""java_version"": ""8"",\r\n        ""oracle_jdbc_url"": ""http://swap-backedoutUItest-1.openstacklocal:8080/resources//ojdbc6.jar"",\r\n        ""ambari_server_host"": ""swap-backedoutuitest-1.openstacklocal"",\r\n        ""ambari_server_port"": ""8080"",\r\n        ""host_sys_prepped"": ""false"",\r\n        ""db_name"": ""ambari"",\r\n        ""ambari_server_use_ssl"": ""false"",\r\n        ""gpl_license_accepted"": ""false"",\r\n        ""jdk_name"": ""jdk-8u112-linux-x64.tar.gz"",\r\n        ""java_home"": ""/usr/jdk64/jdk1.8.0_112"",\r\n        ""mysql_jdbc_url"": ""http://swap-backedoutUItest-1.openstacklocal:8080/resources//mysql-connector-java.jar""\r\n    },', 'commenter': 'mradha25'}, {'comment': 'This is incorrect.  \r\nhttps://stackoverflow.com/questions/715417/converting-from-a-string-to-boolean-in-python\r\n\r\n>>> boolString = ""false""\r\n>>> bool(boolString)\r\nTrue\r\n>>> boolString = ""False""\r\n>>> bool(boolString)\r\nTrue\r\n\r\nSee https://github.com/apache/ambari/blob/trunk/ambari-common/src/main/python/resource_management/libraries/script/config_dictionary.py#L57-L60 \r\nConfigDictionary will actually store a value as a boolean if string is ""true"" or ""false"". \r\n\r\nAlso should be ambariLevelParams/agent_stack_retry_on_unavailability.\r\n\r\nAlso handle None. ', 'commenter': 'jayush'}, {'comment': 'good catch.', 'commenter': 'scottduan'}]"
835,ambari-common/src/main/python/resource_management/libraries/execution_command/execution_command.py,"@@ -54,32 +54,98 @@ def __get_value(self, key, default_value=None):
     except:
       return default_value
 
+  """"""
+  Global variables section
+  """"""
+
+  def get_module_configs(self):
+    return self.__module_configs
+
   def get_module_name(self):
     return self.__get_value(""serviceName"")
 
-  def get_stack_name(self):
-    return self.__get_value(""hostLevelParams/stack_name"")
+  def get_component_type(self):
+    return self.__get_value(""role"")
 
-  def get_stack_version(self):
-    return self.__get_value(""hostLevelParams/stack_version"")
+  def get_component_instance_default_name(self):
+    return self.__get_value(""default"")
 
-  def get_new_stack_version_for_upgrade(self):
-    """"""
-    New Cluster Stack Version that is defined during the RESTART of a Rolling Upgrade
-    :return:
-    """"""
-    return self.__get_value(""commandParams/version"")
+  def get_servicegroup_name(self):
+    return self.__get_value(""serviceGroupName"")
 
   def get_host_name(self):
     return self.__get_value(""hostname"")
 
+  def get_cluster_name(self):
+    return self.__get_value(""clusterName"")
+
+  """"""
+  Host related variables section
+  """"""
+
+  def get_mpack_name(self):
+    return self.__get_value(""hostLevelParams/stack_name"")
+
+  def get_mpack_version(self):
+    return self.__get_value(""hostLevelParams/stack_version"")
+
   def get_java_home(self):
     return self.__get_value(""hostLevelParams/java_home"")
 
   def get_java_version(self):
     java_version = self.__get_value(""hostLevelParams/java_version"")
     return int(java_version) if java_version else None
 
-  def get_module_configs(self):
-    return self.__module_configs
+  def get_jdk_location(self):
+    java_version = self.__get_value(""hostLevelParams/jdk_location"")
+    return int(java_version) if java_version else None
+
+  def check_agent_stack_want_retry_on_unavailability(self):
+    return bool('hostLevelParams/agent_stack_retry_on_unavailability')
+
+  def get_agent_stack_retry_count_on_unavailability(self):
+    return int('hostLevelParams/agent_stack_retry_count')
+
+  """"""
+  Cluster related variables section
+  """"""
+
+  def get_ambari_server_host(self):
+    return self.__get_value(""clusterHostInfo/ambari_server_host"")
+
+  """"""
+  Command related variables section
+  """"""
+
+  def get_new_mpack_version_for_upgrade(self):
+    """"""
+    New Cluster Stack Version that is defined during the RESTART of a Rolling Upgrade
+    :return:
+    """"""
+    return self.__get_value(""commandParams/version"")
+
+  def check_command_retry_enabled(self):
+    return self.__get_value('commandParams/command_retry_enabled', False)
+
+  def check_upgrade_direction(self):
+    return self.__get_value('commandParams/upgrade_direction')
+
+  def check_upgrade_direction(self):","[{'comment': 'Duplicate function', 'commenter': 'jayush'}]"
835,ambari-common/src/main/python/resource_management/libraries/execution_command/execution_command.py,"@@ -54,32 +54,98 @@ def __get_value(self, key, default_value=None):
     except:
       return default_value
 
+  """"""
+  Global variables section
+  """"""
+
+  def get_module_configs(self):
+    return self.__module_configs
+
   def get_module_name(self):
     return self.__get_value(""serviceName"")
 
-  def get_stack_name(self):
-    return self.__get_value(""hostLevelParams/stack_name"")
+  def get_component_type(self):
+    return self.__get_value(""role"")
 
-  def get_stack_version(self):
-    return self.__get_value(""hostLevelParams/stack_version"")
+  def get_component_instance_default_name(self):","[{'comment': 'This should actually be get_component_instance_name(self) and for now it should return hardcoded value as ""default""', 'commenter': 'jayush'}]"
835,ambari-common/src/main/python/resource_management/libraries/execution_command/execution_command.py,"@@ -54,32 +54,98 @@ def __get_value(self, key, default_value=None):
     except:
       return default_value
 
+  """"""
+  Global variables section
+  """"""
+
+  def get_module_configs(self):
+    return self.__module_configs
+
   def get_module_name(self):
     return self.__get_value(""serviceName"")
 
-  def get_stack_name(self):
-    return self.__get_value(""hostLevelParams/stack_name"")
+  def get_component_type(self):
+    return self.__get_value(""role"")
 
-  def get_stack_version(self):
-    return self.__get_value(""hostLevelParams/stack_version"")
+  def get_component_instance_default_name(self):
+    return self.__get_value(""default"")
 
-  def get_new_stack_version_for_upgrade(self):
-    """"""
-    New Cluster Stack Version that is defined during the RESTART of a Rolling Upgrade
-    :return:
-    """"""
-    return self.__get_value(""commandParams/version"")
+  def get_servicegroup_name(self):
+    return self.__get_value(""serviceGroupName"")
 
   def get_host_name(self):
     return self.__get_value(""hostname"")
 
+  def get_cluster_name(self):
+    return self.__get_value(""clusterName"")
+
+  """"""
+  Host related variables section
+  """"""
+
+  def get_mpack_name(self):
+    return self.__get_value(""hostLevelParams/stack_name"")
+
+  def get_mpack_version(self):
+    return self.__get_value(""hostLevelParams/stack_version"")
+
   def get_java_home(self):
     return self.__get_value(""hostLevelParams/java_home"")","[{'comment': 'Should be ambariLevelParams/java_home', 'commenter': 'jayush'}]"
835,ambari-common/src/main/python/resource_management/libraries/execution_command/execution_command.py,"@@ -54,32 +54,98 @@ def __get_value(self, key, default_value=None):
     except:
       return default_value
 
+  """"""
+  Global variables section
+  """"""
+
+  def get_module_configs(self):
+    return self.__module_configs
+
   def get_module_name(self):
     return self.__get_value(""serviceName"")
 
-  def get_stack_name(self):
-    return self.__get_value(""hostLevelParams/stack_name"")
+  def get_component_type(self):
+    return self.__get_value(""role"")
 
-  def get_stack_version(self):
-    return self.__get_value(""hostLevelParams/stack_version"")
+  def get_component_instance_default_name(self):
+    return self.__get_value(""default"")
 
-  def get_new_stack_version_for_upgrade(self):
-    """"""
-    New Cluster Stack Version that is defined during the RESTART of a Rolling Upgrade
-    :return:
-    """"""
-    return self.__get_value(""commandParams/version"")
+  def get_servicegroup_name(self):
+    return self.__get_value(""serviceGroupName"")
 
   def get_host_name(self):
     return self.__get_value(""hostname"")
 
+  def get_cluster_name(self):
+    return self.__get_value(""clusterName"")
+
+  """"""
+  Host related variables section
+  """"""
+
+  def get_mpack_name(self):
+    return self.__get_value(""hostLevelParams/stack_name"")
+
+  def get_mpack_version(self):
+    return self.__get_value(""hostLevelParams/stack_version"")
+
   def get_java_home(self):
     return self.__get_value(""hostLevelParams/java_home"")
 
   def get_java_version(self):
     java_version = self.__get_value(""hostLevelParams/java_version"")","[{'comment': 'Should be ambariLevelParams/java_version', 'commenter': 'jayush'}]"
835,ambari-common/src/main/python/resource_management/libraries/execution_command/execution_command.py,"@@ -54,32 +54,98 @@ def __get_value(self, key, default_value=None):
     except:
       return default_value
 
+  """"""
+  Global variables section
+  """"""
+
+  def get_module_configs(self):
+    return self.__module_configs
+
   def get_module_name(self):
     return self.__get_value(""serviceName"")
 
-  def get_stack_name(self):
-    return self.__get_value(""hostLevelParams/stack_name"")
+  def get_component_type(self):
+    return self.__get_value(""role"")
 
-  def get_stack_version(self):
-    return self.__get_value(""hostLevelParams/stack_version"")
+  def get_component_instance_default_name(self):
+    return self.__get_value(""default"")
 
-  def get_new_stack_version_for_upgrade(self):
-    """"""
-    New Cluster Stack Version that is defined during the RESTART of a Rolling Upgrade
-    :return:
-    """"""
-    return self.__get_value(""commandParams/version"")
+  def get_servicegroup_name(self):
+    return self.__get_value(""serviceGroupName"")
 
   def get_host_name(self):
     return self.__get_value(""hostname"")
 
+  def get_cluster_name(self):
+    return self.__get_value(""clusterName"")
+
+  """"""
+  Host related variables section
+  """"""
+
+  def get_mpack_name(self):
+    return self.__get_value(""hostLevelParams/stack_name"")
+
+  def get_mpack_version(self):
+    return self.__get_value(""hostLevelParams/stack_version"")
+
   def get_java_home(self):
     return self.__get_value(""hostLevelParams/java_home"")
 
   def get_java_version(self):
     java_version = self.__get_value(""hostLevelParams/java_version"")
     return int(java_version) if java_version else None
 
-  def get_module_configs(self):
-    return self.__module_configs
+  def get_jdk_location(self):
+    java_version = self.__get_value(""hostLevelParams/jdk_location"")
+    return int(java_version) if java_version else None","[{'comment': ""Copy-paste error. \r\n\r\nget_jdk_location() shouldn't return int (jdk_version) :) "", 'commenter': 'jayush'}]"
835,ambari-common/src/main/python/resource_management/libraries/execution_command/execution_command.py,"@@ -54,32 +54,98 @@ def __get_value(self, key, default_value=None):
     except:
       return default_value
 
+  """"""
+  Global variables section
+  """"""
+
+  def get_module_configs(self):
+    return self.__module_configs
+
   def get_module_name(self):
     return self.__get_value(""serviceName"")
 
-  def get_stack_name(self):
-    return self.__get_value(""hostLevelParams/stack_name"")
+  def get_component_type(self):
+    return self.__get_value(""role"")
 
-  def get_stack_version(self):
-    return self.__get_value(""hostLevelParams/stack_version"")
+  def get_component_instance_default_name(self):
+    return self.__get_value(""default"")
 
-  def get_new_stack_version_for_upgrade(self):
-    """"""
-    New Cluster Stack Version that is defined during the RESTART of a Rolling Upgrade
-    :return:
-    """"""
-    return self.__get_value(""commandParams/version"")
+  def get_servicegroup_name(self):
+    return self.__get_value(""serviceGroupName"")
 
   def get_host_name(self):
     return self.__get_value(""hostname"")
 
+  def get_cluster_name(self):
+    return self.__get_value(""clusterName"")
+
+  """"""
+  Host related variables section
+  """"""
+
+  def get_mpack_name(self):
+    return self.__get_value(""hostLevelParams/stack_name"")
+
+  def get_mpack_version(self):
+    return self.__get_value(""hostLevelParams/stack_version"")
+
   def get_java_home(self):
     return self.__get_value(""hostLevelParams/java_home"")
 
   def get_java_version(self):
     java_version = self.__get_value(""hostLevelParams/java_version"")
     return int(java_version) if java_version else None
 
-  def get_module_configs(self):
-    return self.__module_configs
+  def get_jdk_location(self):
+    java_version = self.__get_value(""hostLevelParams/jdk_location"")
+    return int(java_version) if java_version else None
+
+  def check_agent_stack_want_retry_on_unavailability(self):
+    return bool('hostLevelParams/agent_stack_retry_on_unavailability')
+
+  def get_agent_stack_retry_count_on_unavailability(self):
+    return int('hostLevelParams/agent_stack_retry_count')","[{'comment': 'Should be ambariLevelParams/agent_stack_retry_count. \r\n\r\nAlso need to handle default value of None if incase the property is not present in command.json', 'commenter': 'jayush'}]"
835,ambari-common/src/main/python/resource_management/libraries/execution_command/execution_command.py,"@@ -54,32 +54,98 @@ def __get_value(self, key, default_value=None):
     except:
       return default_value
 
+  """"""
+  Global variables section
+  """"""
+
+  def get_module_configs(self):
+    return self.__module_configs
+
   def get_module_name(self):
     return self.__get_value(""serviceName"")
 
-  def get_stack_name(self):
-    return self.__get_value(""hostLevelParams/stack_name"")
+  def get_component_type(self):
+    return self.__get_value(""role"")
 
-  def get_stack_version(self):
-    return self.__get_value(""hostLevelParams/stack_version"")
+  def get_component_instance_default_name(self):
+    return self.__get_value(""default"")
 
-  def get_new_stack_version_for_upgrade(self):
-    """"""
-    New Cluster Stack Version that is defined during the RESTART of a Rolling Upgrade
-    :return:
-    """"""
-    return self.__get_value(""commandParams/version"")
+  def get_servicegroup_name(self):
+    return self.__get_value(""serviceGroupName"")
 
   def get_host_name(self):
     return self.__get_value(""hostname"")
 
+  def get_cluster_name(self):
+    return self.__get_value(""clusterName"")
+
+  """"""
+  Host related variables section
+  """"""
+
+  def get_mpack_name(self):
+    return self.__get_value(""hostLevelParams/stack_name"")
+
+  def get_mpack_version(self):
+    return self.__get_value(""hostLevelParams/stack_version"")","[{'comment': 'This should be clusterLevelParams/stack_version\r\n', 'commenter': 'jayush'}]"
835,ambari-common/src/main/python/resource_management/libraries/execution_command/execution_command.py,"@@ -54,32 +54,98 @@ def __get_value(self, key, default_value=None):
     except:
       return default_value
 
+  """"""
+  Global variables section
+  """"""
+
+  def get_module_configs(self):
+    return self.__module_configs
+
   def get_module_name(self):
     return self.__get_value(""serviceName"")
 
-  def get_stack_name(self):
-    return self.__get_value(""hostLevelParams/stack_name"")
+  def get_component_type(self):
+    return self.__get_value(""role"")
 
-  def get_stack_version(self):
-    return self.__get_value(""hostLevelParams/stack_version"")
+  def get_component_instance_default_name(self):
+    return self.__get_value(""default"")
 
-  def get_new_stack_version_for_upgrade(self):
-    """"""
-    New Cluster Stack Version that is defined during the RESTART of a Rolling Upgrade
-    :return:
-    """"""
-    return self.__get_value(""commandParams/version"")
+  def get_servicegroup_name(self):
+    return self.__get_value(""serviceGroupName"")
 
   def get_host_name(self):
     return self.__get_value(""hostname"")","[{'comment': 'This should now be agentLevelParams/hostname', 'commenter': 'jayush'}]"
835,ambari-common/src/main/python/resource_management/libraries/execution_command/execution_command.py,"@@ -54,32 +54,130 @@ def __get_value(self, key, default_value=None):
     except:
       return default_value
 
+  """"""
+  Global variables section
+  """"""
+
+  def get_module_configs(self):
+    return self.__module_configs
+
   def get_module_name(self):
     return self.__get_value(""serviceName"")
 
-  def get_stack_name(self):
-    return self.__get_value(""hostLevelParams/stack_name"")
+  def get_component_type(self):
+    return self.__get_value(""role"")
+
+  def get_component_instance_name(self):
+    """"""
+    At this time it returns hardcoded 'default' name
+    :return:
+    """"""
+    return self.__get_value(""default"")
+
+  def get_servicegroup_name(self):
+    return self.__get_value(""serviceGroupName"")
+
+  def get_cluster_name(self):
+    return self.__get_value(""clusterName"")
+
+  """"""
+  Ambari variables section
+  """"""
+
+  def get_jdk_location(self):
+    return self.__get_value(""ambariLevelParams/jdk_location"")
+
+  def get_jdk_name(self):
+    return self.__get_value(""ambariLevelParams/jdk_name"")
+
+  def get_java_home(self):
+    return self.__get_value(""ambariLevelParams/java_home"")
+
+  def get_java_version(self):
+    return int(self.__get_value(""ambariLevelParams/java_version""))
+
+  def get_jce_name(self):
+    return self.__get_value(""ambariLevelParams/jce_name"")
+
+  def get_db_driver_file_name(self):
+    return self.__get_value('ambariLevelParams/db_driver_filename')
+
+  def get_db_name(self):","[{'comment': 'We still need to handle default value ""None"" for all the params.', 'commenter': 'mradha25'}, {'comment': 'The default is None, if no key is found, please see:\r\n\r\n  def __get_value(self, key, default_value=None):\r\n    """"""\r\n    A private method to query value with the full path of key, if key does not exist, return default_value\r\n    :param key:\r\n    :param default_value:\r\n    :return:\r\n    """"""\r\n    sub_keys = key.split(\'/\')\r\n    value = self.__execution_command\r\n    try:\r\n      for sub_key in sub_keys:\r\n        value = value[sub_key]\r\n      return value\r\n    except:\r\n      return default_value\r\n', 'commenter': 'scottduan'}]"
835,ambari-common/src/main/python/resource_management/libraries/execution_command/execution_command.py,"@@ -54,32 +54,130 @@ def __get_value(self, key, default_value=None):
     except:
       return default_value
 
+  """"""
+  Global variables section
+  """"""
+
+  def get_module_configs(self):
+    return self.__module_configs
+
   def get_module_name(self):
     return self.__get_value(""serviceName"")
 
-  def get_stack_name(self):
-    return self.__get_value(""hostLevelParams/stack_name"")
+  def get_component_type(self):
+    return self.__get_value(""role"")
+
+  def get_component_instance_name(self):
+    """"""
+    At this time it returns hardcoded 'default' name
+    :return:
+    """"""
+    return self.__get_value(""default"")
+
+  def get_servicegroup_name(self):
+    return self.__get_value(""serviceGroupName"")
+
+  def get_cluster_name(self):
+    return self.__get_value(""clusterName"")
+
+  """"""
+  Ambari variables section
+  """"""
+
+  def get_jdk_location(self):
+    return self.__get_value(""ambariLevelParams/jdk_location"")
+
+  def get_jdk_name(self):
+    return self.__get_value(""ambariLevelParams/jdk_name"")
+
+  def get_java_home(self):
+    return self.__get_value(""ambariLevelParams/java_home"")
+
+  def get_java_version(self):
+    return int(self.__get_value(""ambariLevelParams/java_version""))
+
+  def get_jce_name(self):
+    return self.__get_value(""ambariLevelParams/jce_name"")
+
+  def get_db_driver_file_name(self):
+    return self.__get_value('ambariLevelParams/db_driver_filename')
+
+  def get_db_name(self):
+    return self.__get_value('ambariLevelParams/db_name')
+
+  def get_oracle_jdbc_url(self):
+    return self.__get_value('ambariLevelParams/oracle_jdbc_url')
+
+  def get_mysql_jdbc_url(self):
+    return self.__get_value('ambariLevelParams/mysql_jdbc_url')
+
+  def get_agent_stack_retry_count_on_unavailability(self):
+    return self.__get_value('ambariLevelParams/agent_stack_retry_count', 5)
 
-  def get_stack_version(self):
-    return self.__get_value(""hostLevelParams/stack_version"")
+  def check_agent_stack_want_retry_on_unavailability(self):
+    return self.__get_value('ambariLevelParams/agent_stack_retry_on_unavailability')
 
-  def get_new_stack_version_for_upgrade(self):
+  def get_ambari_server_host(self):
+    return self.__get_value(""ambariLevelParams/ambari_server_host"")
+
+  def get_ambari_server_port(self):
+    return self.__get_value(""ambariLevelParams/ambari_server_port"")
+
+  def is_ambari_server_use_ssl(self):
+    return self.__get_value(""ambariLevelParams/ambari_server_use_ssl"", False)
+
+  def is_host_system_prepared(self):
+    return self.__get_value(""ambariLevelParams/host_sys_prepped"", False)
+
+  def is_gpl_license_accepted(self):
+    return self.__get_value(""ambariLevelParams/gpl_license_accepted"", False)
+
+  """"""
+  Cluster related variables section
+  """"""
+
+  def get_mpack_name(self):
+    return self.__get_value(""clusterLevelParams/stack_name"")
+
+  def get_mpack_version(self):
+    return self.__get_value(""clusterLevelParams/stack_version"")
+
+  """"""
+  Agent related variable section
+  """"""
+
+  def get_host_name(self):
+    return self.__get_value(""agentLevelParams/hostname"")
+
+  """"""
+  Command related variables section
+  """"""
+
+  def get_new_mpack_version_for_upgrade(self):
     """"""
     New Cluster Stack Version that is defined during the RESTART of a Rolling Upgrade
     :return:
     """"""
     return self.__get_value(""commandParams/version"")
 
-  def get_host_name(self):
-    return self.__get_value(""hostname"")
+  def check_command_retry_enabled(self):
+    return self.__get_value('commandParams/command_retry_enabled', False)
 
-  def get_java_home(self):
-    return self.__get_value(""hostLevelParams/java_home"")
+  def check_upgrade_direction(self):
+    return self.__get_value('commandParams/upgrade_direction')
 
-  def get_java_version(self):
-    java_version = self.__get_value(""hostLevelParams/java_version"")
-    return int(java_version) if java_version else None
+  def is_rolling_restart_in_upgrade(self):
+    return self.__get_value('commandParams/rolling_restart', False)
 
-  def get_module_configs(self):
-    return self.__module_configs
+  def is_update_files_only(self):
+    return self.__get_value('commandParams/update_files_only', False)
+
+  def get_deploy_phase(self):
+    return self.__get_value('commandParams/phase', '')","[{'comment': 'I could not find phase, update_files_only, rolling_restart, upgrade_direction in the command.json.', 'commenter': 'mradha25'}, {'comment': 'I see this in hdfs params.py ', 'commenter': 'scottduan'}]"
851,ambari-server/src/main/java/org/apache/ambari/server/checks/KafkaPropertiesCheck.java,"@@ -0,0 +1,113 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.checks;
+
+import java.util.Arrays;
+import java.util.LinkedHashSet;
+import java.util.List;
+import java.util.regex.Pattern;
+
+import org.apache.ambari.server.AmbariException;
+import org.apache.ambari.server.controller.PrereqCheckRequest;
+import org.apache.ambari.server.state.Cluster;
+import org.apache.ambari.server.state.ServiceInfo;
+import org.apache.ambari.server.state.stack.PrereqCheckStatus;
+import org.apache.ambari.server.state.stack.PrerequisiteCheck;
+import org.apache.ambari.server.state.stack.upgrade.UpgradeType;
+
+import com.google.inject.Singleton;
+
+/**
+ * Check Kafka configuration properties before upgrade.
+ *
+ * Property to check:
+ *  inter.broker.protocol.version
+ *  log.message.format.version
+ *
+ * These configurations must exist and have a value (the value for both should be aligned with the Kafka version on the current stack).
+ *
+ * For inter.broker.protocol.version :
+ * value is not empty
+ * value is set to current Kafka version in the stack (e.g. for HDP 2.6.x value should be 0.10.1, HDP 2.5.x should be 0.10.0, HDP 2.3x - 2.4x should be 0.9.0)
+ *
+ * For log.message.format.version:
+ * value is not empty (version can vary from current stack version)
+ */
+@Singleton
+@UpgradeCheck(
+    group = UpgradeCheckGroup.REPOSITORY_VERSION,
+    required = { UpgradeType.ROLLING, UpgradeType.NON_ROLLING, UpgradeType.HOST_ORDERED })
+public class KafkaPropertiesCheck extends AbstractCheckDescriptor {
+  private static String KAFKA_BROKER_CONFIG = ""kafka-broker"";
+  private static String KAFKA_SERVICE_NAME = ""KAFKA"";
+
+  private interface KafkaProperties{
+    String INTER_BROKER_PROTOKOL_VERSION = ""inter.broker.protocol.version"";
+    String LOG_MESSAGE_FORMAT_VERSION = ""log.message.format.version"";
+    List<String> ALL_PROPERTIES = Arrays.asList(INTER_BROKER_PROTOKOL_VERSION, LOG_MESSAGE_FORMAT_VERSION);
+  }
+
+  /**
+   * Constructor.
+   */
+  public KafkaPropertiesCheck() {
+    super(CheckDescription.KAFKA_PROPERTIES_VALIDATION);
+  }
+
+  private String getKafkaServiceVersion(Cluster cluster)throws AmbariException{
+    ServiceInfo serviceInfo = ambariMetaInfo.get().getStack(cluster.getCurrentStackVersion()).getService(KAFKA_SERVICE_NAME);","[{'comment': 'this is the basic place where we can get a version, however we have a second one `repositoryVersionHelper.get().getRepositoryVersionEntity(cluster, cluster.getService(""KAFKA"").getServiceComponent(""KAFKA_BROKER"")).getRepositoryXml().manifests`, which feels better. I choosed first one as the change apply 2.3+ stacks, which may not have any vdf (not sure)', 'commenter': 'hapylestat'}, {'comment': 'But what about those versions that DO have a VDF?  That would be the more accurate check, no?  We should also try to validate that that version is one of the ""known"" ones.  What if the vdf/stack defined some 0.10.1.2 or something?', 'commenter': 'ncole'}, {'comment': 'We have no way to validate component version. Ambari not support component version retrieval, only stack version.   And internally component version in ambari = stack component version. Since this, the only source for the validation of ""known"" version would be meta info and vdf, where vdf can\'t be trusted as it is user-defined and may have no relation to actually supported versions in the checked properties.\r\n\r\nMeta information, is the source supported from the very beginning and any custom change to internal resources is a responsibility of the one who changed it. \r\n\r\nIn summary i prefer to follow jira description versions, and the ones mentioned in Kafka documentation.', 'commenter': 'hapylestat'}, {'comment': ""Ok. I figured I'd bring it up anyway.  Thanks!"", 'commenter': 'ncole'}]"
851,ambari-server/src/main/java/org/apache/ambari/server/checks/CheckDescription.java,"@@ -88,6 +88,11 @@
       .put(AbstractCheckDescriptor.DEFAULT,
           ""The following hosts must have version {{version}} installed: {{fails}}."").build());
 
+   public static CheckDescription KAFKA_PROPERTIES_VALIDATION = new CheckDescription(""KAFKA_PROPERTIES_VALIDATION"",","[{'comment': 'something with formatting', 'commenter': 'Unknown'}, {'comment': 'thx', 'commenter': 'hapylestat'}]"
851,ambari-server/src/main/java/org/apache/ambari/server/checks/KafkaPropertiesCheck.java,"@@ -0,0 +1,113 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.checks;
+
+import java.util.Arrays;
+import java.util.LinkedHashSet;
+import java.util.List;
+import java.util.regex.Pattern;
+
+import org.apache.ambari.server.AmbariException;
+import org.apache.ambari.server.controller.PrereqCheckRequest;
+import org.apache.ambari.server.state.Cluster;
+import org.apache.ambari.server.state.ServiceInfo;
+import org.apache.ambari.server.state.stack.PrereqCheckStatus;
+import org.apache.ambari.server.state.stack.PrerequisiteCheck;
+import org.apache.ambari.server.state.stack.upgrade.UpgradeType;
+
+import com.google.inject.Singleton;
+
+/**
+ * Check Kafka configuration properties before upgrade.
+ *
+ * Property to check:
+ *  inter.broker.protocol.version
+ *  log.message.format.version
+ *
+ * These configurations must exist and have a value (the value for both should be aligned with the Kafka version on the current stack).
+ *
+ * For inter.broker.protocol.version :
+ * value is not empty
+ * value is set to current Kafka version in the stack (e.g. for HDP 2.6.x value should be 0.10.1, HDP 2.5.x should be 0.10.0, HDP 2.3x - 2.4x should be 0.9.0)
+ *
+ * For log.message.format.version:
+ * value is not empty (version can vary from current stack version)
+ */
+@Singleton
+@UpgradeCheck(
+    group = UpgradeCheckGroup.REPOSITORY_VERSION,
+    required = { UpgradeType.ROLLING, UpgradeType.NON_ROLLING, UpgradeType.HOST_ORDERED })
+public class KafkaPropertiesCheck extends AbstractCheckDescriptor {
+  private static String KAFKA_BROKER_CONFIG = ""kafka-broker"";
+  private static String KAFKA_SERVICE_NAME = ""KAFKA"";
+
+  private interface KafkaProperties{
+    String INTER_BROKER_PROTOKOL_VERSION = ""inter.broker.protocol.version"";
+    String LOG_MESSAGE_FORMAT_VERSION = ""log.message.format.version"";
+    List<String> ALL_PROPERTIES = Arrays.asList(INTER_BROKER_PROTOKOL_VERSION, LOG_MESSAGE_FORMAT_VERSION);
+  }
+
+  /**
+   * Constructor.
+   */
+  public KafkaPropertiesCheck() {
+    super(CheckDescription.KAFKA_PROPERTIES_VALIDATION);
+  }
+
+  private String getKafkaServiceVersion(Cluster cluster)throws AmbariException{
+    ServiceInfo serviceInfo = ambariMetaInfo.get().getStack(cluster.getCurrentStackVersion()).getService(KAFKA_SERVICE_NAME);
+
+    if (serviceInfo == null) {
+      return null;
+    }
+
+    String[] version = serviceInfo.getVersion().split(Pattern.quote("".""));
+    if (version.length < 3) {
+      return null;
+    }
+    return String.format(""%s.%s.%s"", version[0], version[1], version[2]);
+  }
+
+  @Override
+  public void perform(PrerequisiteCheck prerequisiteCheck, PrereqCheckRequest request) throws AmbariException {
+    final String clusterName = request.getClusterName();
+    final Cluster cluster = clustersProvider.get().getCluster(clusterName);
+    LinkedHashSet<String> failedProperties = new LinkedHashSet<>();
+
+    for (String propertyName: KafkaProperties.ALL_PROPERTIES){
+      String propertyValue = getProperty(cluster, KAFKA_BROKER_CONFIG, propertyName);
+
+      if (propertyValue == null) {
+        failedProperties.add(propertyName);
+      } else if (propertyName.equals(KafkaProperties.INTER_BROKER_PROTOKOL_VERSION)) {
+       String stackKafkaVersion = getKafkaServiceVersion(cluster);","[{'comment': 'formatting broken', 'commenter': 'Unknown'}]"
851,ambari-server/src/main/java/org/apache/ambari/server/checks/CheckDescription.java,"@@ -88,6 +88,11 @@
       .put(AbstractCheckDescriptor.DEFAULT,
           ""The following hosts must have version {{version}} installed: {{fails}}."").build());
 
+   public static CheckDescription KAFKA_PROPERTIES_VALIDATION = new CheckDescription(""KAFKA_PROPERTIES_VALIDATION"",
+    PrereqCheckType.SERVICE,""Kafka properties should be set correctly"",
+    new ImmutableMap.Builder<String, String>().put( AbstractCheckDescriptor.DEFAULT,
+      ""The following kafka properties should be set properly: {{fails}}"").build());","[{'comment': 'nit: ""The following Kafka...""', 'commenter': 'ncole'}]"
851,ambari-server/src/main/java/org/apache/ambari/server/checks/KafkaPropertiesCheck.java,"@@ -0,0 +1,113 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.checks;
+
+import java.util.Arrays;
+import java.util.LinkedHashSet;
+import java.util.List;
+import java.util.regex.Pattern;
+
+import org.apache.ambari.server.AmbariException;
+import org.apache.ambari.server.controller.PrereqCheckRequest;
+import org.apache.ambari.server.state.Cluster;
+import org.apache.ambari.server.state.ServiceInfo;
+import org.apache.ambari.server.state.stack.PrereqCheckStatus;
+import org.apache.ambari.server.state.stack.PrerequisiteCheck;
+import org.apache.ambari.server.state.stack.upgrade.UpgradeType;
+
+import com.google.inject.Singleton;
+
+/**
+ * Check Kafka configuration properties before upgrade.
+ *
+ * Property to check:
+ *  inter.broker.protocol.version
+ *  log.message.format.version
+ *
+ * These configurations must exist and have a value (the value for both should be aligned with the Kafka version on the current stack).
+ *
+ * For inter.broker.protocol.version :
+ * value is not empty
+ * value is set to current Kafka version in the stack (e.g. for HDP 2.6.x value should be 0.10.1, HDP 2.5.x should be 0.10.0, HDP 2.3x - 2.4x should be 0.9.0)
+ *
+ * For log.message.format.version:
+ * value is not empty (version can vary from current stack version)
+ */
+@Singleton
+@UpgradeCheck(
+    group = UpgradeCheckGroup.REPOSITORY_VERSION,
+    required = { UpgradeType.ROLLING, UpgradeType.NON_ROLLING, UpgradeType.HOST_ORDERED })
+public class KafkaPropertiesCheck extends AbstractCheckDescriptor {
+  private static String KAFKA_BROKER_CONFIG = ""kafka-broker"";
+  private static String KAFKA_SERVICE_NAME = ""KAFKA"";
+
+  private interface KafkaProperties{
+    String INTER_BROKER_PROTOKOL_VERSION = ""inter.broker.protocol.version"";","[{'comment': 'INTER_BROKER_PROTOCOL_VERSION', 'commenter': 'ncole'}]"
851,ambari-server/src/main/java/org/apache/ambari/server/checks/KafkaPropertiesCheck.java,"@@ -0,0 +1,113 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.checks;
+
+import java.util.Arrays;
+import java.util.LinkedHashSet;
+import java.util.List;
+import java.util.regex.Pattern;
+
+import org.apache.ambari.server.AmbariException;
+import org.apache.ambari.server.controller.PrereqCheckRequest;
+import org.apache.ambari.server.state.Cluster;
+import org.apache.ambari.server.state.ServiceInfo;
+import org.apache.ambari.server.state.stack.PrereqCheckStatus;
+import org.apache.ambari.server.state.stack.PrerequisiteCheck;
+import org.apache.ambari.server.state.stack.upgrade.UpgradeType;
+
+import com.google.inject.Singleton;
+
+/**
+ * Check Kafka configuration properties before upgrade.
+ *
+ * Property to check:
+ *  inter.broker.protocol.version
+ *  log.message.format.version
+ *
+ * These configurations must exist and have a value (the value for both should be aligned with the Kafka version on the current stack).
+ *
+ * For inter.broker.protocol.version :
+ * value is not empty
+ * value is set to current Kafka version in the stack (e.g. for HDP 2.6.x value should be 0.10.1, HDP 2.5.x should be 0.10.0, HDP 2.3x - 2.4x should be 0.9.0)
+ *
+ * For log.message.format.version:
+ * value is not empty (version can vary from current stack version)
+ */
+@Singleton
+@UpgradeCheck(
+    group = UpgradeCheckGroup.REPOSITORY_VERSION,
+    required = { UpgradeType.ROLLING, UpgradeType.NON_ROLLING, UpgradeType.HOST_ORDERED })
+public class KafkaPropertiesCheck extends AbstractCheckDescriptor {","[{'comment': 'The JIRA indicates that HDP-2.6.5 should be considered.  This check could be done in the isApplicable() method.', 'commenter': 'ncole'}]"
855,ambari-server/src/main/resources/stack-hooks/after-INSTALL/scripts/params.py,"@@ -25,33 +25,37 @@
 from resource_management.libraries.functions import default
 from resource_management.libraries.functions import conf_select
 from resource_management.libraries.functions import stack_select
-from resource_management.libraries.functions import format_jvm_option
+from resource_management.libraries.functions.format_jvm_option import format_jvm_option_value
 from resource_management.libraries.functions.version import format_stack_version, get_major_version
 from resource_management.libraries.functions.cluster_settings import get_cluster_setting_value
+from resource_management.libraries.execution_command import execution_command
+from resource_management.libraries.execution_command import module_configs
 from string import lower
 
-config = Script.get_config()
+execution_command = Script.get_execution_command()
+module_configs = Script.get_module_configs()
+module_name = execution_command.get_module_name()
 tmp_dir = Script.get_tmp_dir()
 
-dfs_type = default(""/commandParams/dfs_type"", """")
+dfs_type = execution_command.get_dfs_type()
 
-is_parallel_execution_enabled = int(default(""/agentConfigParams/agent/parallel_execution"", 0)) == 1
-host_sys_prepped = default(""/ambariLevelParams/host_sys_prepped"", False)
+is_parallel_execution_enabled = execution_command.check_agent_config_execute_in_parallel() == 1
+host_sys_prepped = execution_command.is_host_system_prepared()
 
 sudo = AMBARI_SUDO_BINARY
 
-stack_version_unformatted = config['clusterLevelParams']['stack_version']
+stack_version_unformatted = execution_command.get_mpack_name()","[{'comment': '""stack_name"": ""HDPCORE"", we need stack_version here.', 'commenter': 'mradha25'}]"
855,ambari-server/src/main/resources/stack-hooks/after-INSTALL/scripts/params.py,"@@ -25,33 +25,37 @@
 from resource_management.libraries.functions import default
 from resource_management.libraries.functions import conf_select
 from resource_management.libraries.functions import stack_select
-from resource_management.libraries.functions import format_jvm_option
+from resource_management.libraries.functions.format_jvm_option import format_jvm_option_value
 from resource_management.libraries.functions.version import format_stack_version, get_major_version
 from resource_management.libraries.functions.cluster_settings import get_cluster_setting_value
+from resource_management.libraries.execution_command import execution_command
+from resource_management.libraries.execution_command import module_configs
 from string import lower
 
-config = Script.get_config()
+execution_command = Script.get_execution_command()
+module_configs = Script.get_module_configs()
+module_name = execution_command.get_module_name()
 tmp_dir = Script.get_tmp_dir()
 
-dfs_type = default(""/commandParams/dfs_type"", """")
+dfs_type = execution_command.get_dfs_type()
 
-is_parallel_execution_enabled = int(default(""/agentConfigParams/agent/parallel_execution"", 0)) == 1
-host_sys_prepped = default(""/ambariLevelParams/host_sys_prepped"", False)
+is_parallel_execution_enabled = execution_command.check_agent_config_execute_in_parallel() == 1
+host_sys_prepped = execution_command.is_host_system_prepared()
 
 sudo = AMBARI_SUDO_BINARY
 
-stack_version_unformatted = config['clusterLevelParams']['stack_version']
+stack_version_unformatted = execution_command.get_mpack_name()
 stack_version_formatted = format_stack_version(stack_version_unformatted)","[{'comment': 'In previous versions we needed to format the version but not anymore, our folders are infact created along with the build numbers so,\r\nstack_version_formatted = execution_command.get_mpack_version()', 'commenter': 'mradha25'}]"
855,ambari-server/src/main/resources/stack-hooks/after-INSTALL/scripts/params.py,"@@ -68,36 +72,36 @@
 security_enabled = get_cluster_setting_value('security_enabled')
 
 #java params
-java_home = config['ambariLevelParams']['java_home']
+java_home = execution_command.get_java_home()
 
 #hadoop params
-hdfs_log_dir_prefix = config['configurations']['hadoop-env']['hdfs_log_dir_prefix']
-hadoop_pid_dir_prefix = config['configurations']['hadoop-env']['hadoop_pid_dir_prefix']
-hadoop_root_logger = config['configurations']['hadoop-env']['hadoop_root_logger']
+hdfs_log_dir_prefix = module_configs.get_property_value(module_name, 'hadoop-env', 'hdfs_log_dir_prefix')
+hadoop_pid_dir_prefix = module_configs.get_property_value(module_name, 'hadoop-env', 'hadoop_pid_dir_prefix')
+hadoop_root_logger = module_configs.get_property_value(module_name, 'hadoop-env', 'hadoop_root_logger')
 
 jsvc_path = ""/usr/lib/bigtop-utils""
 
-hadoop_heapsize = config['configurations']['hadoop-env']['hadoop_heapsize']
-namenode_heapsize = config['configurations']['hadoop-env']['namenode_heapsize']
-namenode_opt_newsize = config['configurations']['hadoop-env']['namenode_opt_newsize']
-namenode_opt_maxnewsize = config['configurations']['hadoop-env']['namenode_opt_maxnewsize']
-namenode_opt_permsize = format_jvm_option(""/configurations/hadoop-env/namenode_opt_permsize"",""128m"")
-namenode_opt_maxpermsize = format_jvm_option(""/configurations/hadoop-env/namenode_opt_maxpermsize"",""256m"")
+hadoop_heapsize = module_configs.get_property_value(module_name, 'hadoop-env', 'hadoop_heapsize')","[{'comment': 'When this hook gets called for say Zookeeper-install, the module-name will be zookeeper. How will the hadoop related variables be resolved in this case? Problem is namenode, hdfs, hadoop related variables in this common hooks code.\r\nIs this something the instance manager needs to look at?\r\n@jayush , @d0zen1 ', 'commenter': 'mradha25'}, {'comment': 'At this time, module_name is not used, if you see module_config.py, it is not used and just a placeholder.', 'commenter': 'scottduan'}]"
855,ambari-server/src/main/resources/stack-hooks/before-ANY/scripts/params.py,"@@ -39,49 +39,55 @@
 from ambari_commons.constants import AMBARI_SUDO_BINARY, HADOOP_CLIENTS_MODULE_NAME, HADOOP_CLIENT_COMPONENT_TYPE
 import resource_management.libraries.functions.config_helper as config_helper
 from resource_management.libraries.functions.mpack_manager_helper import get_component_conf_path, get_component_home_path
-
+from resource_management.libraries.execution_command import execution_command
+from resource_management.libraries.execution_command import module_configs
 
 config = Script.get_config()
+execution_command = Script.get_execution_command()
+module_configs = Script.get_module_configs()
+module_name = execution_command.get_module_name()
 tmp_dir = Script.get_tmp_dir()
 
 stack_root = Script.get_stack_root()
 
 architecture = get_architecture()
 
-dfs_type = default(""/commandParams/dfs_type"", """")
+dfs_type = execution_command.get_dfs_type()
 
 artifact_dir = format(""{tmp_dir}/AMBARI-artifacts/"")
-jdk_name = default(""/ambariLevelParams/jdk_name"", None)
-java_home = config['ambariLevelParams']['java_home']
-java_version = expect(""/ambariLevelParams/java_version"", int)
-jdk_location = config['ambariLevelParams']['jdk_location']
+jdk_name = execution_command.get_jdk_name()
+java_home = execution_command.get_jdk_home()
+java_version = execution_command.get_java_version()
+jdk_location = execution_command.get_jdk_location()
 
-hadoop_custom_extensions_enabled = default(""/configurations/core-site/hadoop.custom-extensions.enabled"", False)
+hadoop_custom_extensions_enabled = module_configs.get_property_value(module_name, 'core-site', 'hadoop.custom-extensions.enabled', False)
 
 sudo = AMBARI_SUDO_BINARY
 
-ambari_server_hostname = config['ambariLevelParams']['ambari_server_host']
+ambari_server_hostname = execution_command.get_ambari_server_host()
 
-stack_version_unformatted = config['clusterLevelParams']['stack_version']
-stack_version_formatted = config['clusterLevelParams']['stack_version']
-#stack_version_formatted = format_stack_version(stack_version_unformatted)
+stack_version_unformatted = execution_command.get_mpack_version()
+stack_version_formatted = format_stack_version(stack_version_unformatted)","[{'comment': ""I don't think we need formatting anymore, we need to use the build number as well."", 'commenter': 'mradha25'}]"
855,ambari-server/src/main/resources/stack-hooks/before-ANY/scripts/params.py,"@@ -146,59 +151,59 @@ def is_secure_port(port):
     hadoop_secure_dn_user = '""""'
 
 #hadoop params
-hdfs_log_dir_prefix = config['configurations']['hadoop-env']['hdfs_log_dir_prefix']
-hadoop_pid_dir_prefix = config['configurations']['hadoop-env']['hadoop_pid_dir_prefix']
-hadoop_root_logger = config['configurations']['hadoop-env']['hadoop_root_logger']
+hdfs_log_dir_prefix = module_configs.get_property_value(module_name, 'hadoop-env', 'hdfs_log_dir_prefix')
+hadoop_pid_dir_prefix = module_configs.get_property_value(module_name, 'hadoop-env', 'hadoop_pid_dir_prefix')
+hadoop_root_logger = module_configs.get_property_value(module_name, 'hadoop-env', 'hadoop_root_logger')
 
 jsvc_path = ""/usr/lib/bigtop-utils""
 
-hadoop_heapsize = config['configurations']['hadoop-env']['hadoop_heapsize']
-namenode_heapsize = config['configurations']['hadoop-env']['namenode_heapsize']
-namenode_opt_newsize = config['configurations']['hadoop-env']['namenode_opt_newsize']
-namenode_opt_maxnewsize = config['configurations']['hadoop-env']['namenode_opt_maxnewsize']
-namenode_opt_permsize = format_jvm_option(""/configurations/hadoop-env/namenode_opt_permsize"",""128m"")
-namenode_opt_maxpermsize = format_jvm_option(""/configurations/hadoop-env/namenode_opt_maxpermsize"",""256m"")
+hadoop_heapsize = module_configs.get_property_value(module_name, 'hadoop-env', 'hadoop_heapsize')","[{'comment': 'Similar case, what happens if module_name is zookeeper?', 'commenter': 'mradha25'}, {'comment': 'This would be addressed based on service group dependencies and service instance dependencies \r\n\r\nFor example:\r\nODS1/HBASE -> ODS1/HADOOP_CLIENT -> HDPCORE1/HDFS \r\nODS2/HBASE -> ODS2/HADOOP_CLIENT -> HDPCORE2/HDFS\r\n\r\nWe will have to revisit this once we refactor our execution command to deal with multiple instances but for now we should can ignore the module_name since there is only one hadoop-env setting in the cluster.\r\n', 'commenter': 'jayush'}]"
883,ambari-server/src/main/java/org/apache/ambari/server/api/services/ConfigurationService.java,"@@ -27,17 +27,31 @@
 import javax.ws.rs.Produces;
 import javax.ws.rs.core.Context;
 import javax.ws.rs.core.HttpHeaders;
+import javax.ws.rs.core.MediaType;
 import javax.ws.rs.core.Response;
 import javax.ws.rs.core.UriInfo;
 
-import org.apache.ambari.annotations.ApiIgnore;
 import org.apache.ambari.server.api.resources.ResourceInstance;
+import org.apache.ambari.server.controller.ConfigurationResponse;
 import org.apache.ambari.server.controller.spi.Resource;
 
+import org.apache.http.HttpStatus;
+
+import io.swagger.annotations.Api;
+import io.swagger.annotations.ApiImplicitParam;
+import io.swagger.annotations.ApiImplicitParams;
+import io.swagger.annotations.ApiOperation;
+import io.swagger.annotations.ApiResponse;
+import io.swagger.annotations.ApiResponses;
+
 /**
  * Service responsible for services resource requests.
  */
+@Api(value = ""/clusters"", description = ""Endpoint for cluster-specific operations"")
 public class ConfigurationService extends BaseService {","[{'comment': 'Leading slash is not needed, also this description should belong to ClusterService', 'commenter': 'benyoka'}, {'comment': 'thanks for checking, fixed', 'commenter': 'g-boros'}]"
891,ambari-common/src/main/python/resource_management/libraries/execution_command/execution_command.py,"@@ -54,6 +54,15 @@ def __get_value(self, key, default_value=None):
     except:
       return default_value
 
+  def query_config_attri_directly(self, query_string, default_value=None):
+    """"""
+    Query config attribute from execution_command directly, i.e ""clusterHostInfo/zookeeper_server_hosts""","[{'comment': 'A better name would be `get_value`, since this method returns exactly the same as `__get_value`, the only difference is that it is public.', 'commenter': 'adoroszlai'}]"
891,ambari-common/src/main/python/resource_management/libraries/execution_command/execution_command.py,"@@ -231,10 +246,74 @@ def get_ambari_jdk_name(self):
   def need_refresh_topology(self):
     return self.__get_value('commandParams/refresh_topology', False)
 
+  def check_only_update_files(self):
+    return self.__get_value('commandParams/update_files_only', False)
+
 
   """"""
   Role related variables section
   """"""
 
   def is_upgrade_suspended(self):
-    return self.__get_value('roleParams/upgrade_suspended', False)
\ No newline at end of file
+    return self.__get_value('roleParams/upgrade_suspended', False)
+
+  """"""
+  Cluster Host Info
+  """"""
+
+  def get_namenode_hosts(self):","[{'comment': 'No we cannot have component names in methods. The method needs to be generic. You can add special-casing within the method to handle inconsistencies. Ideally we should fix inconsistencies in the future. But the library interface MUST BE generic. Otherwise how will it work for third-party mpacks which we are not even aware of the component names? For instance Lucidworks Solr mpack has SOLR server and SOLR_SERVER component https://github.com/lucidworks/solr-stack/blob/master/src/main/mpack/common-services/SOLR/5.5.5/metainfo.xml#L17. You should be able to call execution_command.get_component_hosts(""solr_server"") to get list of solr server component hosts. \r\n\r\nExample:\r\n\r\n```\r\ndef get_component_hosts(self, component_name): \r\n  key = ""clusterHostInfo/"" + component_name + ""_hosts""\r\n  if component_name == ""oozie_server"":\r\n    key = ""clusterHostInfo/"" + component_name\r\n  return self.__get_value(key, [])\r\n```', 'commenter': 'jayush'}, {'comment': 'Jayush, thanks. I thought wrongly when I implemented like this at that time. I just uploaded the  new patch.', 'commenter': 'scottduan'}]"
891,ambari-common/src/main/python/resource_management/libraries/execution_command/module_configs.py,"@@ -23,18 +23,40 @@
 
 class ModuleConfigs(object):
   """"""
-  This class maps to ""/configurations"" block in command.json which includes configuration information of a service
+  This class maps to ""/configurations"" and ""/configurationAttributes in command.json which includes configuration information of a service
   """"""
 
-  def __init__(self, config):
+  def __init__(self, config, configAttributes):","[{'comment': 'Nit-picking: Rename ""config"" to ""configs"" (plural instead of singular)\r\n\r\ndef __init__(self, **configs**, configAttributes):', 'commenter': 'jayush'}, {'comment': 'Done', 'commenter': 'scottduan'}]"
891,ambari-common/src/main/python/resource_management/libraries/execution_command/execution_command.py,"@@ -36,7 +36,7 @@ def __init__(self, command):
     :param command: json string or a python dict object
     """"""
     self._execution_command = command
-    self._module_configs = module_configs.ModuleConfigs(self.__get_value(""configurations""))
+    self._module_configs = module_configs.ModuleConfigs(self.__get_value(""configurations""), self.__get_value(""configAttributes""))","[{'comment': '""configurationAttributes"" and not ""configAttributes""\r\n\r\n```\r\n  ""configurationAttributes"": {\r\n        ""ranger-hdfs-audit"": {},\r\n        ""ssl-client"": {},\r\n        ""activity-zeppelin-site"": {},\r\n        ""ranger-hdfs-policymgr-ssl"": {},\r\n```', 'commenter': 'jayush'}]"
892,ambari-server/src/main/resources/stack-hooks/before-INSTALL/scripts/params.py,"@@ -58,19 +58,19 @@
 #hosts
 hostname = execution_command.get_host_name()
 ambari_server_hostname = execution_command.get_ambari_server_host()
-rm_host = execution_command._execution_command.__get_value(""clusterHostInfo/resourcemanager_hosts"", [])
-slave_hosts = execution_command._execution_command.__get_value(""clusterHostInfo/datanode_hosts"", [])
-oozie_servers = execution_command._execution_command.__get_value(""clusterHostInfo/oozie_server"", [])
-hcat_server_hosts = execution_command._execution_command.__get_value(""clusterHostInfo/webhcat_server_hosts"", [])
-hive_server_host =  execution_command._execution_command.__get_value(""clusterHostInfo/hive_server_hosts"", [])
-hbase_master_hosts = execution_command._execution_command.__get_value(""clusterHostInfo/hbase_master_hosts"", [])
-hs_host = execution_command._execution_command.__get_value(""clusterHostInfo/historyserver_hosts"", [])
-jtnode_host = execution_command._execution_command.__get_value(""clusterHostInfo/jtnode_hosts"", [])
-namenode_host = execution_command._execution_command.__get_value(""clusterHostInfo/namenode_hosts"", [])
-zk_hosts = execution_command._execution_command.__get_value(""clusterHostInfo/zookeeper_server_hosts"", [])
-ganglia_server_hosts = execution_command._execution_command.__get_value(""clusterHostInfo/ganglia_server_hosts"", [])
-storm_server_hosts = execution_command._execution_command.__get_value(""clusterHostInfo/nimbus_hosts"", [])
-falcon_host = execution_command._execution_command.__get_value(""clusterHostInfo/falcon_server_hosts"", [])
+rm_host = execution_command.get_value(""clusterHostInfo/resourcemanager_hosts"", [])
+slave_hosts = execution_command.get_value(""clusterHostInfo/datanode_hosts"", [])
+oozie_servers = execution_command.get_value(""clusterHostInfo/oozie_server"", [])
+hcat_server_hosts = execution_command.get_value(""clusterHostInfo/webhcat_server_hosts"", [])
+hive_server_host =  execution_command.get_value(""clusterHostInfo/hive_server_hosts"", [])
+hbase_master_hosts = execution_command.get_value(""clusterHostInfo/hbase_master_hosts"", [])
+hs_host = execution_command.get_value(""clusterHostInfo/historyserver_hosts"", [])
+jtnode_host = execution_command.get_value(""clusterHostInfo/jtnode_hosts"", [])
+namenode_host = execution_command.get_value(""clusterHostInfo/namenode_hosts"", [])
+zk_hosts = execution_command.get_value(""clusterHostInfo/zookeeper_server_hosts"", [])
+ganglia_server_hosts = execution_command.get_value(""clusterHostInfo/ganglia_server_hosts"", [])
+storm_server_hosts = execution_command.get_value(""clusterHostInfo/nimbus_hosts"", [])
+falcon_host = execution_command.get_value(""clusterHostInfo/falcon_server_hosts"", [])
 
 has_sqoop_client = 'sqoop-env' in module_configs","[{'comment': 'This line will also fail since `module_configs` is not iterable.', 'commenter': 'ncole'}, {'comment': '+1 on this.', 'commenter': 'jayush'}, {'comment': 'You are right. I will update the code', 'commenter': 'scottduan'}]"
892,ambari-server/src/main/resources/stack-hooks/before-INSTALL/scripts/params.py,"@@ -58,19 +58,19 @@
 #hosts
 hostname = execution_command.get_host_name()
 ambari_server_hostname = execution_command.get_ambari_server_host()
-rm_host = execution_command._execution_command.__get_value(""clusterHostInfo/resourcemanager_hosts"", [])
-slave_hosts = execution_command._execution_command.__get_value(""clusterHostInfo/datanode_hosts"", [])
-oozie_servers = execution_command._execution_command.__get_value(""clusterHostInfo/oozie_server"", [])
-hcat_server_hosts = execution_command._execution_command.__get_value(""clusterHostInfo/webhcat_server_hosts"", [])
-hive_server_host =  execution_command._execution_command.__get_value(""clusterHostInfo/hive_server_hosts"", [])
-hbase_master_hosts = execution_command._execution_command.__get_value(""clusterHostInfo/hbase_master_hosts"", [])
-hs_host = execution_command._execution_command.__get_value(""clusterHostInfo/historyserver_hosts"", [])
-jtnode_host = execution_command._execution_command.__get_value(""clusterHostInfo/jtnode_hosts"", [])
-namenode_host = execution_command._execution_command.__get_value(""clusterHostInfo/namenode_hosts"", [])
-zk_hosts = execution_command._execution_command.__get_value(""clusterHostInfo/zookeeper_server_hosts"", [])
-ganglia_server_hosts = execution_command._execution_command.__get_value(""clusterHostInfo/ganglia_server_hosts"", [])
-storm_server_hosts = execution_command._execution_command.__get_value(""clusterHostInfo/nimbus_hosts"", [])
-falcon_host = execution_command._execution_command.__get_value(""clusterHostInfo/falcon_server_hosts"", [])
+rm_host = execution_command.get_value(""clusterHostInfo/resourcemanager_hosts"", [])
+slave_hosts = execution_command.get_value(""clusterHostInfo/datanode_hosts"", [])
+oozie_servers = execution_command.get_value(""clusterHostInfo/oozie_server"", [])
+hcat_server_hosts = execution_command.get_value(""clusterHostInfo/webhcat_server_hosts"", [])
+hive_server_host =  execution_command.get_value(""clusterHostInfo/hive_server_hosts"", [])
+hbase_master_hosts = execution_command.get_value(""clusterHostInfo/hbase_master_hosts"", [])
+hs_host = execution_command.get_value(""clusterHostInfo/historyserver_hosts"", [])
+jtnode_host = execution_command.get_value(""clusterHostInfo/jtnode_hosts"", [])
+namenode_host = execution_command.get_value(""clusterHostInfo/namenode_hosts"", [])
+zk_hosts = execution_command.get_value(""clusterHostInfo/zookeeper_server_hosts"", [])
+ganglia_server_hosts = execution_command.get_value(""clusterHostInfo/ganglia_server_hosts"", [])
+storm_server_hosts = execution_command.get_value(""clusterHostInfo/nimbus_hosts"", [])
+falcon_host = execution_command.get_value(""clusterHostInfo/falcon_server_hosts"", [])
 
 has_sqoop_client = 'sqoop-env' in module_configs
 has_namenode = not len(namenode_host) == 0","[{'comment': 'This line will also fail since the field is late-accessed by config_dictionary\r\n```\r\nTraceback (most recent call last):\r\n  File ""/var/lib/ambari-agent/cache/stack-hooks/before-INSTALL/scripts/hook.py"", line 37, in <module>\r\n    BeforeInstallHook().execute()\r\n  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 349, in execute\r\n    method(env)\r\n  File ""/var/lib/ambari-agent/cache/stack-hooks/before-INSTALL/scripts/hook.py"", line 28, in hook\r\n    import params\r\n  File ""/var/lib/ambari-agent/cache/stack-hooks/before-INSTALL/scripts/params.py"", line 78, in <module>\r\n    has_namenode = not len(namenode_host) == 0\r\n  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/config_dictionary.py"", line 73, in __getattr__\r\n    raise Fail(""Configuration parameter \'"" + self.name + ""\' was not found in configurations dictionary!"")\r\n```\r\n```', 'commenter': 'ncole'}]"
892,ambari-server/src/main/resources/stack-hooks/before-INSTALL/scripts/params.py,"@@ -58,19 +58,19 @@
 #hosts
 hostname = execution_command.get_host_name()
 ambari_server_hostname = execution_command.get_ambari_server_host()
-rm_host = execution_command._execution_command.__get_value(""clusterHostInfo/resourcemanager_hosts"", [])
-slave_hosts = execution_command._execution_command.__get_value(""clusterHostInfo/datanode_hosts"", [])
-oozie_servers = execution_command._execution_command.__get_value(""clusterHostInfo/oozie_server"", [])
-hcat_server_hosts = execution_command._execution_command.__get_value(""clusterHostInfo/webhcat_server_hosts"", [])
-hive_server_host =  execution_command._execution_command.__get_value(""clusterHostInfo/hive_server_hosts"", [])
-hbase_master_hosts = execution_command._execution_command.__get_value(""clusterHostInfo/hbase_master_hosts"", [])
-hs_host = execution_command._execution_command.__get_value(""clusterHostInfo/historyserver_hosts"", [])
-jtnode_host = execution_command._execution_command.__get_value(""clusterHostInfo/jtnode_hosts"", [])
-namenode_host = execution_command._execution_command.__get_value(""clusterHostInfo/namenode_hosts"", [])
-zk_hosts = execution_command._execution_command.__get_value(""clusterHostInfo/zookeeper_server_hosts"", [])
-ganglia_server_hosts = execution_command._execution_command.__get_value(""clusterHostInfo/ganglia_server_hosts"", [])
-storm_server_hosts = execution_command._execution_command.__get_value(""clusterHostInfo/nimbus_hosts"", [])
-falcon_host = execution_command._execution_command.__get_value(""clusterHostInfo/falcon_server_hosts"", [])
+rm_host = execution_command.get_value(""clusterHostInfo/resourcemanager_hosts"", [])","[{'comment': 'Do we even need all these host variables in `before-INSTALL`?', 'commenter': 'ncole'}, {'comment': 'Not very clear these host variables yet, they are original code, at this time, just leave as is', 'commenter': 'scottduan'}, {'comment': 'If there are unused variables please remove them. ', 'commenter': 'jayush'}]"
892,ambari-server/src/main/resources/stack-hooks/before-ANY/scripts/params.py,"@@ -187,13 +187,13 @@ def is_secure_port(port):
 
 user_group = get_cluster_setting_value('user_group')
 
-ganglia_server_hosts = execution_command._execution_command.__get_value(""clusterHostInfo/ganglia_server_hosts"", [])
-namenode_host = execution_command._execution_command.__get_value(""clusterHostInfo/namenode_hosts"", [])
-hbase_master_hosts = execution_command._execution_command.__get_value(""clusterHostInfo/hbase_master_hosts"", [])
-oozie_servers = execution_command._execution_command.__get_value(""clusterHostInfo/oozie_server"", [])
-falcon_server_hosts = execution_command._execution_command.__get_value(""clusterHostInfo/falcon_server_hosts"", [])
-ranger_admin_hosts = execution_command._execution_command.__get_value(""clusterHostInfo/ranger_admin_hosts"", [])
-zeppelin_master_hosts = execution_command._execution_command.__get_value(""clusterHostInfo/zeppelin_master_hosts"", [])
+ganglia_server_hosts = execution_command.get_value(""clusterHostInfo/ganglia_server_hosts"", [])","[{'comment': 'lets just add a method to execution_command as following to handle this. \r\n\r\ndef get_component_hosts(component_name):', 'commenter': 'jayush'}, {'comment': 'OK, I will try to replace all the get_value() calls with specific apis in execution_command.', 'commenter': 'scottduan'}]"
892,ambari-server/src/main/resources/stack-hooks/before-START/scripts/params.py,"@@ -174,7 +174,7 @@
 if has_zk_host:
   if not zookeeper_quorum:
     zookeeper_clientPort = '2181'
-  zookeeper_quorum = (':' + zookeeper_clientPort + ',').join(execution_command._execution_command.__get_value(""clusterHostInfo/zookeeper_server_hosts""))
+  zookeeper_quorum = (':' + zookeeper_clientPort + ',').join(execution_command.get_value(""clusterHostInfo/zookeeper_server_hosts""))","[{'comment': 'Redundant call. Use zk_hosts variable\r\n', 'commenter': 'jayush'}]"
892,ambari-server/src/main/resources/stack-hooks/before-START/scripts/params.py,"@@ -254,10 +254,10 @@
 default_fs = module_configs.get_property_value(module_name, 'core-site', 'fs.defaultFS')
 
 #host info
-all_hosts = execution_command._execution_command.__get_value(""clusterHostInfo/all_hosts"", [])
-all_racks = execution_command._execution_command.__get_value(""clusterHostInfo/all_racks"", [])
-all_ipv4_ips = execution_command._execution_command.__get_value(""clusterHostInfo/all_ipv4_ips"", [])
-slave_hosts = execution_command._execution_command.__get_value(""clusterHostInfo/datanode_hosts"", [])
+all_hosts = execution_command.get_value(""clusterHostInfo/all_hosts"", [])","[{'comment': 'execution_command.get_all_hosts()', 'commenter': 'jayush'}]"
892,ambari-server/src/main/resources/stack-hooks/before-START/scripts/params.py,"@@ -254,10 +254,10 @@
 default_fs = module_configs.get_property_value(module_name, 'core-site', 'fs.defaultFS')
 
 #host info
-all_hosts = execution_command._execution_command.__get_value(""clusterHostInfo/all_hosts"", [])
-all_racks = execution_command._execution_command.__get_value(""clusterHostInfo/all_racks"", [])
-all_ipv4_ips = execution_command._execution_command.__get_value(""clusterHostInfo/all_ipv4_ips"", [])
-slave_hosts = execution_command._execution_command.__get_value(""clusterHostInfo/datanode_hosts"", [])
+all_hosts = execution_command.get_value(""clusterHostInfo/all_hosts"", [])
+all_racks = execution_command.get_value(""clusterHostInfo/all_racks"", [])","[{'comment': 'execution_command.get_all_racks()', 'commenter': 'jayush'}]"
892,ambari-server/src/main/resources/stack-hooks/before-START/scripts/params.py,"@@ -254,10 +254,10 @@
 default_fs = module_configs.get_property_value(module_name, 'core-site', 'fs.defaultFS')
 
 #host info
-all_hosts = execution_command._execution_command.__get_value(""clusterHostInfo/all_hosts"", [])
-all_racks = execution_command._execution_command.__get_value(""clusterHostInfo/all_racks"", [])
-all_ipv4_ips = execution_command._execution_command.__get_value(""clusterHostInfo/all_ipv4_ips"", [])
-slave_hosts = execution_command._execution_command.__get_value(""clusterHostInfo/datanode_hosts"", [])
+all_hosts = execution_command.get_value(""clusterHostInfo/all_hosts"", [])
+all_racks = execution_command.get_value(""clusterHostInfo/all_racks"", [])
+all_ipv4_ips = execution_command.get_value(""clusterHostInfo/all_ipv4_ips"", [])","[{'comment': 'execution_command.get_all_ips()', 'commenter': 'jayush'}]"
892,ambari-server/src/main/resources/stack-hooks/before-START/scripts/params.py,"@@ -254,10 +254,10 @@
 default_fs = module_configs.get_property_value(module_name, 'core-site', 'fs.defaultFS')
 
 #host info
-all_hosts = execution_command._execution_command.__get_value(""clusterHostInfo/all_hosts"", [])
-all_racks = execution_command._execution_command.__get_value(""clusterHostInfo/all_racks"", [])
-all_ipv4_ips = execution_command._execution_command.__get_value(""clusterHostInfo/all_ipv4_ips"", [])
-slave_hosts = execution_command._execution_command.__get_value(""clusterHostInfo/datanode_hosts"", [])
+all_hosts = execution_command.get_value(""clusterHostInfo/all_hosts"", [])
+all_racks = execution_command.get_value(""clusterHostInfo/all_racks"", [])
+all_ipv4_ips = execution_command.get_value(""clusterHostInfo/all_ipv4_ips"", [])
+slave_hosts = execution_command.get_value(""clusterHostInfo/datanode_hosts"", [])","[{'comment': ""Why is this variable called slave_hosts? Doesn't make sense.\r\n"", 'commenter': 'jayush'}, {'comment': 'this is original code:\r\nslave_hosts = default(""/clusterHostInfo/datanode_hosts"", [])', 'commenter': 'scottduan'}]"
892,ambari-server/src/main/resources/stack-hooks/after-INSTALL/scripts/shared_initialization.py,"@@ -84,7 +84,7 @@ def setup_config():
     XmlConfig(""core-site.xml"",
               conf_dir=params.hadoop_conf_dir,
               configurations=params.module_configs.get_property_value(params.module_name, 'core-site', ''),","[{'comment': ""This is wrong. \r\nThis should be module_configs.get_all_properties(params.module_name, 'core-site') which should return all core-site properties. \r\n"", 'commenter': 'jayush'}, {'comment': 'Now there is no get_all_properties() in execution_command module. Of course I can add it. However, there it seems different from your comment. Based on my understand, the configurations return from this call is a dict object:\r\n\r\noriginal code:\r\nconfigurations=params.config[\'configurations\'][\'core-site\']\r\nIf we use ""module_configs.get_all_properties(params.module_name, \'core-site\') which should return all core-site properties."", does that mean the return is a list of attributes?', 'commenter': 'scottduan'}, {'comment': 'The updated api in module_configs:\r\n\r\n  def get_properties(self, module_name, config_type, property_names):\r\n    return map(lambda property_name: self.get_property_value(module_name, config_type, property_name), property_names)\r\n\r\n  def get_property_value(self, module_name, config_type, property_name, default=None):\r\n    try:\r\n      if property_name:\r\n        return self.__module_configs[config_type][property_name]\r\n      else:\r\n        return self.__module_configs[config_type]\r\n    except:\r\n      return default', 'commenter': 'scottduan'}, {'comment': 'How can get_property_value() API return values for all properties for that config type. The method is for retrieving a single property value but also returns all property values? ', 'commenter': 'jayush'}, {'comment': 'This is the test I did:\r\n  \r\ndef test_access_to_module_configs(self):\r\n    module_configs = self.__execution_command.get_module_configs()\r\n    zoo_cfg = module_configs.get_property_value(""zookeeper"", ""zoo.cfg"", """")\r\n    self.assertTrue(isinstance(zoo_cfg, dict))\r\n\r\nzoo_cfg=\r\n{u\'clientPort\': u\'2181\', u\'autopurge.purgeInterval\': u\'24\', u\'syncLimit\': u\'5\', u\'dataDir\': u\'/hadoop/zookeeper\', u\'initLimit\': u\'10\', u\'tickTime\': u\'3000\', u\'autopurge.snapRetainCount\': u\'30\'}\r\n\r\nIt returns zoo_cfg dict.\r\n\r\nThe part of data file: command.json:\r\n""zoo.cfg"": {\r\n      ""clientPort"": ""2181"",\r\n      ""autopurge.purgeInterval"": ""24"",\r\n      ""syncLimit"": ""5"",\r\n      ""dataDir"": ""/hadoop/zookeeper"",\r\n      ""initLimit"": ""10"",\r\n      ""tickTime"": ""3000"",\r\n      ""autopurge.snapRetainCount"": ""30""\r\n    }', 'commenter': 'scottduan'}]"
892,ambari-server/src/main/resources/stack-hooks/after-INSTALL/scripts/shared_initialization.py,"@@ -84,7 +84,7 @@ def setup_config():
     XmlConfig(""core-site.xml"",
               conf_dir=params.hadoop_conf_dir,
               configurations=params.module_configs.get_property_value(params.module_name, 'core-site', ''),
-              configuration_attributes=params.execution_command._execution_command.__get_value(""configurationAttributes/core-site""),
+              configuration_attributes=params.execution_command.get_value(""configurationAttributes/core-site""),","[{'comment': 'Let ModuleConfigs deal with configs and config attributes and add necessary methods to support retrieving config properties and config attributes.\r\n\r\nThe whole purpose of having the execution_command library is to hide the schema of command.json and having to call with ""configurationAttributes/core-site"" defeats the purpose. \r\n\r\nExample: \r\n```\r\nclass ModuleConfigs(object):\r\n  """"""\r\n  This class maps to ""/configurations"" block in command.json which includes configuration information of a service\r\n  """"""\r\n\r\n  def __init__(self, configs, config_attributes):\r\n    self.__module_configs = configs\r\n    self.__module_config_attributes = config_attributes    \r\n\r\n  def get_attributes(self, module_name, config_type):\r\n     return  config_attributes(config_type)\r\n\r\n  def get_properties(self, module_name, config_type, property_names):\r\n    return map(lambda property_name: self.get_property_value(module_name, config_type, property_name), property_names)\r\n\r\n  def get_property_value(self, module_name, config_type, property_name, default=None):\r\n    try:\r\n      return self.__module_configs[config_type][property_name]\r\n    except:\r\n      return default\r\n```', 'commenter': 'jayush'}]"
892,ambari-server/src/main/resources/stack-hooks/before-START/scripts/params.py,"@@ -65,7 +65,7 @@
 
 dfs_type = execution_command.get_dfs_type()","[{'comment': 'I hope this method exists.  After other changes, this is the next error i see.', 'commenter': 'ncole'}]"
892,ambari-server/src/main/resources/stack-hooks/before-START/scripts/params.py,"@@ -96,7 +96,7 @@
 java_exec = ""{0}/bin/java"".format(java_home) if java_home is not None else ""/bin/java""
 
 #users and groups
-has_hadoop_env = 'hadoop-env' in module_configs
+has_hadoop_env = module_configs.get_all_properties(module_name, ""hadoop-env"") is not None","[{'comment': 'get_all_properties() never returns None. It returns an empty map if the config-type does not exist in the command.json\r\n', 'commenter': 'jayush'}]"
892,ambari-server/src/main/resources/stack-hooks/before-START/scripts/params.py,"@@ -316,7 +316,7 @@
   namenode_rpc = module_configs.get_property_value(module_name, 'hdfs-site', 'dfs.namenode.rpc-address', default_fs)
 
 # if HDFS is not installed in the cluster, then don't try to access namenode_rpc
-if has_namenode and namenode_rpc and 'core-site' in module_configs:
+if has_namenode and namenode_rpc and module_configs.get_all_properties(module_name, 'core-site') is not None:","[{'comment': 'get_all_properties() never returns None. It returns an empty map if the config-type does not exist in the command.json\r\n', 'commenter': 'jayush'}]"
892,ambari-server/src/main/resources/stack-hooks/before-ANY/scripts/params.py,"@@ -187,21 +187,21 @@ def is_secure_port(port):
 
 user_group = get_cluster_setting_value('user_group')
 
-ganglia_server_hosts = execution_command._execution_command.__get_value(""clusterHostInfo/ganglia_server_hosts"", [])
-namenode_host = execution_command._execution_command.__get_value(""clusterHostInfo/namenode_hosts"", [])
-hbase_master_hosts = execution_command._execution_command.__get_value(""clusterHostInfo/hbase_master_hosts"", [])
-oozie_servers = execution_command._execution_command.__get_value(""clusterHostInfo/oozie_server"", [])
-falcon_server_hosts = execution_command._execution_command.__get_value(""clusterHostInfo/falcon_server_hosts"", [])
-ranger_admin_hosts = execution_command._execution_command.__get_value(""clusterHostInfo/ranger_admin_hosts"", [])
-zeppelin_master_hosts = execution_command._execution_command.__get_value(""clusterHostInfo/zeppelin_master_hosts"", [])
+ganglia_server_hosts = execution_command.get_ganglia_server_hosts()
+namenode_host = execution_command.get_namenode_hosts()
+hbase_master_hosts = execution_command.get_hbase_master_hosts()
+oozie_servers = execution_command.get_oozie_server_hosts()
+falcon_server_hosts = execution_command.get_falcon_server_hosts()
+ranger_admin_hosts = execution_command.get_ranger_admin_hosts()
+zeppelin_master_hosts = execution_command.get_zeppelin_master_hosts()
 
 # get the correct version to use for checking stack features
 version_for_stack_feature_checks = get_stack_feature_version(config)
 
 
 has_namenode = not len(namenode_host) == 0
 has_ganglia_server = not len(ganglia_server_hosts) == 0
-has_tez = module_configs.get_property_value(module_name, 'tez-site', '') is not None
+has_tez = module_configs.get_all_properties(module_name, 'tez-site') is not None","[{'comment': 'get_all_properties() never returns None. It returns an empty map if the config-type does not exist in the command.json\r\n', 'commenter': 'jayush'}]"
892,ambari-server/src/main/resources/stack-hooks/before-INSTALL/scripts/params.py,"@@ -58,21 +58,21 @@
 #hosts
 hostname = execution_command.get_host_name()
 ambari_server_hostname = execution_command.get_ambari_server_host()
-rm_host = execution_command._execution_command.__get_value(""clusterHostInfo/resourcemanager_hosts"", [])
-slave_hosts = execution_command._execution_command.__get_value(""clusterHostInfo/datanode_hosts"", [])
-oozie_servers = execution_command._execution_command.__get_value(""clusterHostInfo/oozie_server"", [])
-hcat_server_hosts = execution_command._execution_command.__get_value(""clusterHostInfo/webhcat_server_hosts"", [])
-hive_server_host =  execution_command._execution_command.__get_value(""clusterHostInfo/hive_server_hosts"", [])
-hbase_master_hosts = execution_command._execution_command.__get_value(""clusterHostInfo/hbase_master_hosts"", [])
-hs_host = execution_command._execution_command.__get_value(""clusterHostInfo/historyserver_hosts"", [])
-jtnode_host = execution_command._execution_command.__get_value(""clusterHostInfo/jtnode_hosts"", [])
-namenode_host = execution_command._execution_command.__get_value(""clusterHostInfo/namenode_hosts"", [])
-zk_hosts = execution_command._execution_command.__get_value(""clusterHostInfo/zookeeper_server_hosts"", [])
-ganglia_server_hosts = execution_command._execution_command.__get_value(""clusterHostInfo/ganglia_server_hosts"", [])
-storm_server_hosts = execution_command._execution_command.__get_value(""clusterHostInfo/nimbus_hosts"", [])
-falcon_host = execution_command._execution_command.__get_value(""clusterHostInfo/falcon_server_hosts"", [])
-
-has_sqoop_client = 'sqoop-env' in module_configs
+rm_host = execution_command.get_resourcemanager_hosts()
+slave_hosts = execution_command.get_datanode_hosts()
+oozie_servers = execution_command.get_oozie_server_hosts
+hcat_server_hosts = execution_command.get_webhcat_server_hosts()
+hive_server_host =  execution_command.get_hive_server_hosts()
+hbase_master_hosts = execution_command.get_hbase_master_hosts()
+hs_host = execution_command.get_historyserver_hosts()
+jtnode_host = execution_command.get_jtnode_hosts()
+namenode_host = execution_command.get_namenode_hosts()
+zk_hosts = execution_command.get_zk_server_hosts()
+ganglia_server_hosts = execution_command.get_ganglia_server_hosts()
+storm_server_hosts = execution_command.get_nimbus_hosts()
+falcon_host = execution_command.get_falcon_server_hosts()
+
+has_sqoop_client = module_configs.get_all_properties(module_name, 'sqoop-env') is not None","[{'comment': 'get_all_properties() never returns None. It returns an empty map if the config-type does not exist in the command.json\r\n', 'commenter': 'jayush'}]"
892,ambari-server/src/main/resources/stack-hooks/before-INSTALL/scripts/params.py,"@@ -85,7 +85,7 @@
 has_ganglia_server = not len(ganglia_server_hosts) == 0
 has_storm_server = not len(storm_server_hosts) == 0
 has_falcon_server = not len(falcon_host) == 0
-has_tez = module_configs.get_property_value(module_name, 'tez-site', '') is not None
+has_tez = module_configs.get_all_properties(module_name, 'tez-site') is not None","[{'comment': 'get_all_properties() never returns None. It returns an empty map if the config-type does not exist in the command.json\r\n', 'commenter': 'jayush'}]"
892,ambari-server/src/main/resources/stack-hooks/before-START/scripts/params.py,"@@ -266,14 +266,14 @@
 net_topology_mapping_data_file_path = os.path.join(net_topology_script_dir, net_topology_mapping_data_file_name)
 
 #Added logic to create /tmp and /user directory for HCFS stack.  
-has_core_site = 'core-site' in module_configs
+has_core_site = module_configs.get_all_properties(module_name, ""core-site"") is not None","[{'comment': 'get_all_properties() never returns None. It returns an empty map if the config-type does not exist in the command.json\r\n', 'commenter': 'jayush'}, {'comment': 'You are right', 'commenter': 'scottduan'}]"
897,ambari-server/src/main/java/org/apache/ambari/server/serveraction/kerberos/ConfigureAmbariIdentitiesServerAction.java,"@@ -126,10 +126,8 @@ protected CommandReport processIdentity(ResolvedKerberosPrincipal resolvedPrinci
       String hostName = resolvedPrincipal.getHostName();
       for (Map.Entry<String, String> serviceMappingEntry : resolvedPrincipal.getServiceMapping().entries()){
         String serviceName = serviceMappingEntry.getKey();
-        // distribute ambari keytabs only if host id is null, otherwise they will
-        // be distributed by usual process using ambari-agent.
         // TODO check if changes needed for multiple principals in one keytab
-        if (resolvedPrincipal.getHostId() == null && hostName != null && serviceName.equals(RootService.AMBARI.name())) {
+        if (hostName != null && serviceName.equals(RootService.AMBARI.name())) {","[{'comment': ""This check should be to ensure the hostname is the same as the Ambari server's hostname.  This is being enforced below anyways:\r\n\r\n```\r\nif (StageUtils.getHostName().equals(hostName) && serviceName.equals(RootService.AMBARI.name()))\r\n```\r\n\r\nThe following code is being used below this, forcing hostname to be the Ambari server's hostname:\r\n```\r\n          hostName = StageUtils.getHostName();\r\n          File hostDirectory = new File(dataDirectory, hostName);\r\n```"", 'commenter': 'rlevas'}, {'comment': 'Done.\r\nRe-ran the same tests I described above, the result is the same:\r\n```\r\n[root@c7401 ~]# cat /etc/ambari-server/conf/krb5JAASLogin.conf\r\ncom.sun.security.jgss.krb5.initiate {\r\n    com.sun.security.auth.module.Krb5LoginModule required\r\n    renewTGT=false\r\n    doNotPrompt=true\r\n    useKeyTab=true\r\n    keyTab=""/etc/security/keytabs/ambari.server.keytab""\r\n    principal=""ambari-server-cluster1@AMBARI.APACHE.ORG""\r\n    storeKey=true\r\n    useTicketCache=false;\r\n};\r\n```', 'commenter': 'smolnar82'}]"
931,ambari-server/src/main/java/org/apache/ambari/server/utils/SecretReference.java,"@@ -172,4 +231,56 @@ public static void replacePasswordsWithReferencesForCustomProperties(Map<String,
       }
     }
   }
+
+  /**
+   * Replaces all password type properties in the given {@link Configuration} object. Creates a new Configuration
+   * object instead of mutating the input configuration.
+   * @param configuration the input configuration
+   * @param passwordProperties password type properties in a multimap.
+   *                           It has {@code config-type -> [password-prop-1, password-prop-2, ...]} structure.
+   * @return a new configuration with password properties replaced
+   */
+  public static Configuration replacePasswordsInConfigurations(Configuration configuration,
+                                                                     Multimap<String, String> passwordProperties) {
+    // replace passwords in config properties
+    Map<String, Map<String, String>> replacedProperties = configuration.getProperties().entrySet().stream().map(
+      entry -> {
+        String configType = entry.getKey();
+        Map<String, String> replacedConfigProps =
+          replacePasswordsInPropertyMap(entry.getValue(), configType, passwordProperties);
+        return new SimpleEntry<>(configType, replacedConfigProps);
+      }).collect(toMap(Map.Entry::getKey, Map.Entry::getValue));
+
+    // replace passwords in config attributes
+    Map<String, Map<String, Map<String, String>>> replacedAttributes = configuration.getAttributes().entrySet().stream().map(
+      configTypeEntry -> {
+        String configType = configTypeEntry.getKey();
+        Map<String, Map<String, String>> replacedConfigProps = configTypeEntry.getValue().entrySet().stream().collect(
+          toMap(Map.Entry::getKey, e -> replacePasswordsInPropertyMap(e.getValue(), configType, passwordProperties)));
+        return new SimpleEntry<>(configType, replacedConfigProps);
+      }).collect(toMap(Map.Entry::getKey, Map.Entry::getValue));
+
+    return new Configuration(replacedProperties, replacedAttributes);
+    }
+
+  /**
+   * Replaces all password type properties in the given property map. Creates a new map instead of mutating
+   * the input configuration.
+   * @param propertyMap the input property map
+   * @param passwordProperties password type properties in a multimap.
+   *                           It has {@code config-type -> [password-prop-1, password-prop-2, ...]} structure.
+   * @return a new property map with password properties replaced
+   */
+  public static Map<String, String> replacePasswordsInPropertyMap(Map<String, String> propertyMap,
+                                                                  String configType,
+                                                                  Multimap<String, String> passwordProperties) {
+    return propertyMap.entrySet().stream().map(entry -> {
+      String propertyType = entry.getKey();
+      String newValue = passwordProperties.get(configType).contains(propertyType) ?","[{'comment': 'Minor issue:  If a configuration type is listed in a Blueprint/Cluster Creation template that is not a match with any known configuration type, will this throw a NullPointerException? \r\n\r\nThis might be a problem in cases where the cluster creation template has an error (mis-spelled config type name) or if a custom config type is specified that the stacks do not include.  \r\n\r\nMaybe we should catch this error here and log appropriately, if the code doesn\'t already filter out ""unknown"" config types at some other level. \r\n\r\n', 'commenter': 'rnettleton'}, {'comment': ""Guava multimaps return an empty collection if the key is not in the map, that's one of their cool features."", 'commenter': 'benyoka'}]"
931,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/ArtifactResourceProvider.java,"@@ -75,6 +76,8 @@
   public static final String CLUSTER_NAME_PROPERTY = RESPONSE_KEY + PropertyHelper.EXTERNAL_PATH_SEP + CLUSTER_NAME;
   public static final String SERVICE_NAME_PROPERTY = RESPONSE_KEY + PropertyHelper.EXTERNAL_PATH_SEP + SERVICE_NAME;
 
+  public static final String PROVISION_REQUEST_ARTIFACT_NAME = ""provision_cluster_request"";","[{'comment': 'Minor concern: \r\n\r\nI think this change is fine the way it is, but we might want to consider refactoring this in the future.  \r\n\r\nIt may make more sense to keep any details about specific types of artifacts out of this class, and have a more generic strategy defined to allow for modifications of retrieved artifacts before the artifacts are returned to the caller.  \r\n\r\nEven having a static list of registered ""filtering"" or ""updating"" interfaces in this class might be better, and allow the resource provider code to only interact with that strategy interface.\r\n\r\nAs I mentioned, this current change is fine, but we might want to consider refactoring this if we find that other artifact types need to be persisted, and then retrieved in some custom fashion.  ', 'commenter': 'rnettleton'}]"
931,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/ClusterTemplateArtifactPasswordReplacer.java,"@@ -0,0 +1,138 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.controller.internal;
+
+import static java.util.Collections.emptyList;
+import static java.util.stream.Collectors.toList;
+import static java.util.stream.Collectors.toMap;
+
+import java.util.AbstractMap;
+import java.util.Collection;
+import java.util.List;
+import java.util.Map;
+import java.util.function.Function;
+
+import org.apache.ambari.server.state.StackId;
+import org.apache.ambari.server.topology.Configurable;
+import org.apache.ambari.server.topology.Configuration;
+import org.apache.ambari.server.utils.SecretReference;
+
+import com.google.common.collect.Multimap;
+
+/**
+ * Helper class for replacing password properties in cluster template artifacts. {@see #replacePasswords}
+ */
+public class ClusterTemplateArtifactPasswordReplacer {
+
+  /**
+   * Replaces all passwords (service config passwords and default password) in a received cluster creation template
+   * artifact. Mpack (stack) information is used to identify password type properties. If the cluster template does not
+   * specify mpacks, all installed stacks are queried for password type properties.
+   *
+   * @param artifactData the raw cluster template artifact as parsed json
+   * @return the cluster template artifact with passwords replaced
+   */
+  public Map<String, Object> replacePasswords(Map<String, Object> artifactData) {
+    Collection<StackId> stackIds = extractStackIdsFromClusterRequest(artifactData);
+    // get all password properties from the specified stacks or
+    // all stacks if the cluster template doesn't specify mpacks
+    Multimap<String, String> passwordProperties = stackIds.isEmpty() ?
+      SecretReference.getAllPasswordProperties() :
+      SecretReference.getAllPasswordProperties(stackIds);
+    Map<String, Object> passwordsReplaced = replacePasswordsInConfigurations(artifactData, passwordProperties);
+    passwordsReplaced.replace(""default_password"", SecretReference.SECRET_PREFIX + "":default_password"");
+    return passwordsReplaced;
+  }
+
+  /**
+   * Replaces passwords in the received cluster template artifact based on the received password information extracted
+   * from stacks.
+   * @param artifactData the cluster template artifact
+   * @param passwordProperties a multimap containing password type properties. The map has a structure of
+   *                           config type -> password properties.
+   * @return the cluster template artifact with passwords replaced
+   */
+  protected Map<String, Object> replacePasswordsInConfigurations(Map<String, Object> artifactData,
+                                                                 Multimap<String, String> passwordProperties) {
+    return (Map<String, Object>)applyToAllConfigurations(artifactData,
+      config -> {
+        Configuration configuration = Configurable.parseConfigs(config);
+        Configuration replacedConfiguration =
+          SecretReference.replacePasswordsInConfigurations(configuration, passwordProperties);
+        return Configurable.convertConfigToMap(replacedConfiguration);
+      }","[{'comment': 'Can you please extract this lambda expression to a function that can be referenced and tested more easily?', 'commenter': 'adoroszlai'}, {'comment': ""This method simply transforms a map with a lambda that is only 3 statements. I don't think further decomposition should be needed."", 'commenter': 'benyoka'}]"
931,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/ClusterTemplateArtifactPasswordReplacer.java,"@@ -0,0 +1,138 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.controller.internal;
+
+import static java.util.Collections.emptyList;
+import static java.util.stream.Collectors.toList;
+import static java.util.stream.Collectors.toMap;
+
+import java.util.AbstractMap;
+import java.util.Collection;
+import java.util.List;
+import java.util.Map;
+import java.util.function.Function;
+
+import org.apache.ambari.server.state.StackId;
+import org.apache.ambari.server.topology.Configurable;
+import org.apache.ambari.server.topology.Configuration;
+import org.apache.ambari.server.utils.SecretReference;
+
+import com.google.common.collect.Multimap;
+
+/**
+ * Helper class for replacing password properties in cluster template artifacts. {@see #replacePasswords}
+ */
+public class ClusterTemplateArtifactPasswordReplacer {
+
+  /**
+   * Replaces all passwords (service config passwords and default password) in a received cluster creation template
+   * artifact. Mpack (stack) information is used to identify password type properties. If the cluster template does not
+   * specify mpacks, all installed stacks are queried for password type properties.
+   *
+   * @param artifactData the raw cluster template artifact as parsed json
+   * @return the cluster template artifact with passwords replaced
+   */
+  public Map<String, Object> replacePasswords(Map<String, Object> artifactData) {
+    Collection<StackId> stackIds = extractStackIdsFromClusterRequest(artifactData);
+    // get all password properties from the specified stacks or
+    // all stacks if the cluster template doesn't specify mpacks
+    Multimap<String, String> passwordProperties = stackIds.isEmpty() ?
+      SecretReference.getAllPasswordProperties() :
+      SecretReference.getAllPasswordProperties(stackIds);
+    Map<String, Object> passwordsReplaced = replacePasswordsInConfigurations(artifactData, passwordProperties);
+    passwordsReplaced.replace(""default_password"", SecretReference.SECRET_PREFIX + "":default_password"");
+    return passwordsReplaced;
+  }
+
+  /**
+   * Replaces passwords in the received cluster template artifact based on the received password information extracted
+   * from stacks.
+   * @param artifactData the cluster template artifact
+   * @param passwordProperties a multimap containing password type properties. The map has a structure of
+   *                           config type -> password properties.
+   * @return the cluster template artifact with passwords replaced
+   */
+  protected Map<String, Object> replacePasswordsInConfigurations(Map<String, Object> artifactData,
+                                                                 Multimap<String, String> passwordProperties) {
+    return (Map<String, Object>)applyToAllConfigurations(artifactData,
+      config -> {
+        Configuration configuration = Configurable.parseConfigs(config);
+        Configuration replacedConfiguration =
+          SecretReference.replacePasswordsInConfigurations(configuration, passwordProperties);
+        return Configurable.convertConfigToMap(replacedConfiguration);
+      }
+    );
+  }
+
+  /**
+   * <p> Recursively scans the received data structure consisting of maps, lists ans simple values (a parsed Json) and
+   * applies the @{code transform} function to each configurations found. </p>
+   * <p> A value counts as configuration if it has a type of @{link java.util.List} and is a value in a map with
+   * {@code ""configurations""} key.
+   *
+   * </p>
+   * @param data the data to process recursively (is a structure of maps, lists and simple values)
+   * @param transform the transformation to apply to configuration values
+   * @return a replication of the input data with the transformation applied to all configuration values.
+   */
+  protected Object applyToAllConfigurations(Object data,
+                                            Function<List<Map<String, Object>>, Object> transform) {
+    // recursively call for lists
+    if (data instanceof List<?>) {
+      return ((List<Object>)data).stream().
+        map(item -> applyToAllConfigurations(item, transform)).collect(toList());
+    }
+    else if (data instanceof Map<?, ?>) {
+      return ((Map<String, Object>) data).entrySet().stream().map(
+        entry -> {
+          // apply transformation for configurations
+          if (""configurations"".equals(entry.getKey()) && entry.getValue() instanceof List<?>) {
+            return new AbstractMap.SimpleEntry<>(
+              entry.getKey(),
+              transform.apply((List<Map<String, Object>>)entry.getValue()));
+          }
+          // recursively call for non-configuration elements
+          else {
+            return new AbstractMap.SimpleEntry<>(
+              entry.getKey(),
+              applyToAllConfigurations(entry.getValue(), transform));
+          }
+        }).collect(toMap(Map.Entry::getKey, Map.Entry::getValue));","[{'comment': 'Can you please extract this lambda expression to a function that can be referenced and tested more easily?', 'commenter': 'adoroszlai'}, {'comment': ""I broke it up to smaller methods. I don't think it improves testing (the algorithm should be tested as a whole), on the other hand it hopefully improves readability."", 'commenter': 'benyoka'}]"
931,ambari-server/src/main/java/org/apache/ambari/server/utils/SecretReference.java,"@@ -150,6 +166,49 @@ public static void replacePasswordsWithReferences(Map<PropertyInfo.PropertyType,
     }
   }
 
+  /**
+   * Returns all password properties defined in the stacks specified by the given stack id's. Keys in the map are
+   * file names (e.g hadoop-env.xml) and values are property names.
+   * @param stackIds the stack ids to specify which stacks to look for
+   * @return A set multimap of password type properties.
+   * @throws IllegalArgumentException when a non-existing stack is specified
+   */
+  public static SetMultimap<String, String> getAllPasswordProperties(Collection<StackId> stackIds) throws IllegalArgumentException {
+    AmbariMetaInfo metaInfo = AmbariServer.getController().getAmbariMetaInfo();
+    Collection<StackInfo> stacks = stackIds.stream().map( stackId -> {
+      try {
+        return metaInfo.getStack(stackId);
+      }
+      catch (StackAccessException ex) {
+        throw new IllegalArgumentException(ex);
+      }
+    }).collect(toList());","[{'comment': 'Can you please extract this lambda expression to a function that can be referenced and tested more easily?', 'commenter': 'adoroszlai'}, {'comment': ""This is a very simple lambda (a simple method call within a try - catch - re-throw). I don't think it makes sense to decompose it further."", 'commenter': 'benyoka'}]"
931,ambari-server/src/main/java/org/apache/ambari/server/utils/SecretReference.java,"@@ -150,6 +166,49 @@ public static void replacePasswordsWithReferences(Map<PropertyInfo.PropertyType,
     }
   }
 
+  /**
+   * Returns all password properties defined in the stacks specified by the given stack id's. Keys in the map are
+   * file names (e.g hadoop-env.xml) and values are property names.
+   * @param stackIds the stack ids to specify which stacks to look for
+   * @return A set multimap of password type properties.
+   * @throws IllegalArgumentException when a non-existing stack is specified
+   */
+  public static SetMultimap<String, String> getAllPasswordProperties(Collection<StackId> stackIds) throws IllegalArgumentException {
+    AmbariMetaInfo metaInfo = AmbariServer.getController().getAmbariMetaInfo();
+    Collection<StackInfo> stacks = stackIds.stream().map( stackId -> {
+      try {
+        return metaInfo.getStack(stackId);
+      }
+      catch (StackAccessException ex) {
+        throw new IllegalArgumentException(ex);
+      }
+    }).collect(toList());
+    return getAllPasswordPropertiesInternal(stacks);
+  }
+
+  /**
+   * Returns all password properties defined in all stacks. Keys in the map are
+   * file names (e.g hadoop-env.xml) and values are property names.
+   * @return A set multimap of password type properties.
+   */
+  public static SetMultimap<String, String> getAllPasswordProperties() {
+    AmbariMetaInfo metaInfo = AmbariServer.getController().getAmbariMetaInfo();
+    return getAllPasswordPropertiesInternal(metaInfo.getStacks());
+  }
+
+  private static SetMultimap<String, String> getAllPasswordPropertiesInternal(Collection<StackInfo> stacks) {","[{'comment': 'Making this non-private would allow testing without the need for PowerMock.', 'commenter': 'adoroszlai'}, {'comment': 'Powermock is needed because of the static call to AmbariServer.getController(), not because this method is private.', 'commenter': 'benyoka'}, {'comment': 'I meant that this method could be tested instead of the one that calls `AmbariServer.getController()`.  Most of the logic is in this one.', 'commenter': 'adoroszlai'}, {'comment': 'Ok.', 'commenter': 'benyoka'}]"
931,ambari-server/src/main/java/org/apache/ambari/server/utils/SecretReference.java,"@@ -172,4 +231,56 @@ public static void replacePasswordsWithReferencesForCustomProperties(Map<String,
       }
     }
   }
+
+  /**
+   * Replaces all password type properties in the given {@link Configuration} object. Creates a new Configuration
+   * object instead of mutating the input configuration.
+   * @param configuration the input configuration
+   * @param passwordProperties password type properties in a multimap.
+   *                           It has {@code config-type -> [password-prop-1, password-prop-2, ...]} structure.
+   * @return a new configuration with password properties replaced
+   */
+  public static Configuration replacePasswordsInConfigurations(Configuration configuration,
+                                                                     Multimap<String, String> passwordProperties) {
+    // replace passwords in config properties
+    Map<String, Map<String, String>> replacedProperties = configuration.getProperties().entrySet().stream().map(
+      entry -> {
+        String configType = entry.getKey();
+        Map<String, String> replacedConfigProps =
+          replacePasswordsInPropertyMap(entry.getValue(), configType, passwordProperties);
+        return new SimpleEntry<>(configType, replacedConfigProps);
+      }).collect(toMap(Map.Entry::getKey, Map.Entry::getValue));
+
+    // replace passwords in config attributes
+    Map<String, Map<String, Map<String, String>>> replacedAttributes = configuration.getAttributes().entrySet().stream().map(
+      configTypeEntry -> {
+        String configType = configTypeEntry.getKey();
+        Map<String, Map<String, String>> replacedConfigProps = configTypeEntry.getValue().entrySet().stream().collect(
+          toMap(Map.Entry::getKey, e -> replacePasswordsInPropertyMap(e.getValue(), configType, passwordProperties)));
+        return new SimpleEntry<>(configType, replacedConfigProps);
+      }).collect(toMap(Map.Entry::getKey, Map.Entry::getValue));","[{'comment': 'Can you please extract these lambda expressions to a function that can be referenced and tested more easily?', 'commenter': 'adoroszlai'}, {'comment': 'I am thinking on a simplification.\r\n', 'commenter': 'benyoka'}]"
931,ambari-server/src/main/java/org/apache/ambari/server/utils/SecretReference.java,"@@ -172,4 +231,56 @@ public static void replacePasswordsWithReferencesForCustomProperties(Map<String,
       }
     }
   }
+
+  /**
+   * Replaces all password type properties in the given {@link Configuration} object. Creates a new Configuration
+   * object instead of mutating the input configuration.
+   * @param configuration the input configuration
+   * @param passwordProperties password type properties in a multimap.
+   *                           It has {@code config-type -> [password-prop-1, password-prop-2, ...]} structure.
+   * @return a new configuration with password properties replaced
+   */
+  public static Configuration replacePasswordsInConfigurations(Configuration configuration,
+                                                                     Multimap<String, String> passwordProperties) {
+    // replace passwords in config properties
+    Map<String, Map<String, String>> replacedProperties = configuration.getProperties().entrySet().stream().map(
+      entry -> {
+        String configType = entry.getKey();
+        Map<String, String> replacedConfigProps =
+          replacePasswordsInPropertyMap(entry.getValue(), configType, passwordProperties);
+        return new SimpleEntry<>(configType, replacedConfigProps);
+      }).collect(toMap(Map.Entry::getKey, Map.Entry::getValue));
+
+    // replace passwords in config attributes
+    Map<String, Map<String, Map<String, String>>> replacedAttributes = configuration.getAttributes().entrySet().stream().map(
+      configTypeEntry -> {
+        String configType = configTypeEntry.getKey();
+        Map<String, Map<String, String>> replacedConfigProps = configTypeEntry.getValue().entrySet().stream().collect(
+          toMap(Map.Entry::getKey, e -> replacePasswordsInPropertyMap(e.getValue(), configType, passwordProperties)));
+        return new SimpleEntry<>(configType, replacedConfigProps);
+      }).collect(toMap(Map.Entry::getKey, Map.Entry::getValue));
+
+    return new Configuration(replacedProperties, replacedAttributes);
+    }
+
+  /**
+   * Replaces all password type properties in the given property map. Creates a new map instead of mutating
+   * the input configuration.
+   * @param propertyMap the input property map
+   * @param passwordProperties password type properties in a multimap.
+   *                           It has {@code config-type -> [password-prop-1, password-prop-2, ...]} structure.
+   * @return a new property map with password properties replaced
+   */
+  public static Map<String, String> replacePasswordsInPropertyMap(Map<String, String> propertyMap,
+                                                                  String configType,
+                                                                  Multimap<String, String> passwordProperties) {
+    return propertyMap.entrySet().stream().map(entry -> {
+      String propertyType = entry.getKey();
+      String newValue = passwordProperties.get(configType).contains(propertyType) ?
+        SECRET_PREFIX + "":"" + configType + "":"" + propertyType :
+        entry.getValue();
+      return new SimpleEntry<>(propertyType, newValue);
+    }).collect(toMap(Map.Entry::getKey, Map.Entry::getValue));","[{'comment': 'Can you please extract this lambda expression to a function that can be referenced and tested more easily?', 'commenter': 'adoroszlai'}, {'comment': ""This is another a very simple map transformation written in java-8-lambda style rather as for-loop. I don't think further decomposition would add anything to readability or testability. I changed the indentation though to make it more readable."", 'commenter': 'benyoka'}]"
951,ambari-server/src/test/java/org/apache/ambari/server/serveraction/kerberos/KerberosTopologyUpdateTriggerServerActionTest.java,"@@ -0,0 +1,62 @@
+package org.apache.ambari.server.serveraction.kerberos;","[{'comment': 'Missing license header.', 'commenter': 'adoroszlai'}, {'comment': 'thx', 'commenter': 'smolnar82'}, {'comment': 'Done', 'commenter': 'smolnar82'}]"
951,ambari-server/src/main/java/org/apache/ambari/server/serveraction/kerberos/KerberosTopologyUpdateTriggerServerAction.java,"@@ -0,0 +1,46 @@
+package org.apache.ambari.server.serveraction.kerberos;","[{'comment': 'Missing license header.', 'commenter': 'adoroszlai'}, {'comment': 'thx', 'commenter': 'smolnar82'}, {'comment': 'Done', 'commenter': 'smolnar82'}]"
951,ambari-server/src/main/java/org/apache/ambari/server/controller/KerberosHelperImpl.java,"@@ -3913,6 +3935,11 @@ public long createStages(Cluster cluster,
       addUpdateConfigurationsStage(cluster, clusterHostInfoJson, hostParamsJson, event, commandParameters,
         roleCommandOrder, requestStageContainer);
 
+      // *****************************************************************
+      // Create stage to trigger a topology update event being published
+      addTopologyUpdateTriggerStage(cluster, clusterHostInfoJson, hostParamsJson, event, commandParameters,
+        roleCommandOrder, requestStageContainer);","[{'comment': '`KerberosHelperTest` needs to be changed accordingly.', 'commenter': 'adoroszlai'}, {'comment': 'ok', 'commenter': 'smolnar82'}, {'comment': 'Done', 'commenter': 'smolnar82'}]"
951,ambari-server/src/test/java/org/apache/ambari/server/serveraction/kerberos/KerberosTopologyUpdateTriggerServerActionTest.java,"@@ -0,0 +1,62 @@
+package org.apache.ambari.server.serveraction.kerberos;
+
+import static org.easymock.EasyMock.expect;
+import static org.easymock.EasyMock.replay;
+import static org.easymock.EasyMock.verify;
+import static org.mockito.Matchers.anyBoolean;
+
+import org.apache.ambari.server.agent.stomp.TopologyHolder;
+import org.apache.ambari.server.audit.AuditLogger;
+import org.apache.ambari.server.events.TopologyUpdateEvent;
+import org.apache.ambari.server.testutils.PartialNiceMockBinder;
+import org.easymock.EasyMock;
+import org.easymock.EasyMockSupport;
+import org.junit.Before;
+import org.junit.Test;
+
+import com.google.inject.AbstractModule;
+import com.google.inject.Guice;
+import com.google.inject.Injector;
+import com.google.inject.Provider;
+
+public class KerberosTopologyUpdateTriggerServerActionTest extends EasyMockSupport {
+
+  private static TopologyHolder topologyHolder = EasyMock.createNiceMock(TopologyHolder.class);
+
+  private KerberosTopologyUpdateTriggerServerAction action;
+
+  @Before
+  public void init() {
+    final Injector injector = createInjector();
+    action = injector.getInstance(KerberosTopologyUpdateTriggerServerAction.class);
+  }
+
+  @Test
+  public void shouldUpdateTopology() throws Exception {
+    final TopologyUpdateEvent event = createNiceMock(TopologyUpdateEvent.class);
+    expect(topologyHolder.getCurrentData()).andReturn(event).once();
+    expect(topologyHolder.updateData(event)).andReturn(anyBoolean()).once();
+    replay(topologyHolder);
+    action.execute(null);
+    verify(topologyHolder);
+  }
+
+  static class TestTopologyHolderProvider implements Provider<TopologyHolder> {
+    @Override
+    public TopologyHolder get() {
+      return topologyHolder;
+    }
+  }
+
+  private Injector createInjector() {","[{'comment': 'Would it be possible to test this without guice, just by passing the dependencies via constructor?', 'commenter': 'zeroflag'}, {'comment': 'We need Guice anyway...either the field would be injected or the constructor...how else would you get `TopologyHolder`? Please note this a Singleton scoped Guice bean.', 'commenter': 'smolnar82'}, {'comment': 'Why do we need guice to do this?\r\n\r\n```java\r\naction = new KerberosTopologyUpdateTriggerServerAction(topologyHolder);\r\n```', 'commenter': 'zeroflag'}, {'comment': 'I meant we need Guice anyway in order to retrieve `TopologyHolder` when running on the server. On the other hand it makes sense to pass it as a constructor argument (which allows unit testing with simple POJOs/Mocks) and let Guice inject the constructor within the server. Please review it again.', 'commenter': 'smolnar82'}]"
951,ambari-server/src/main/java/org/apache/ambari/server/serveraction/kerberos/KerberosTopologyUpdateTriggerServerAction.java,"@@ -43,14 +42,18 @@
 
   private final static Logger LOG = LoggerFactory.getLogger(KerberosTopologyUpdateTriggerServerAction.class);
 
+  private final TopologyHolder topologyHolder;
+
   @Inject
-  private Provider<TopologyHolder> topologyHolderProvider;
+  public KerberosTopologyUpdateTriggerServerAction(TopologyHolder topologyHolder) {
+    super();","[{'comment': 'Calling `super()` is unnecessary.', 'commenter': 'adoroszlai'}]"
959,ambari-common/src/main/python/ambari_commons/repo_manager/__init__.py,"@@ -61,6 +61,7 @@ def get_new_instance(cls, os_family=None):
 
     construct_rules = {
       OSConst.UBUNTU_FAMILY: AptManager,
+      OSConst.DEBIAN_FAMILY: AptManager,","[{'comment': ""it's strange, i have supposed that DEBIAN is a child of the UBUNTU family, as it extends it. Problem will repeat with *-PPC  variation of the system families.   Could we extend OsFamily to return the root os family, not the extended leaf and use it  in the constructor?"", 'commenter': 'hapylestat'}, {'comment': '@hapylestat, I don\'t fully understand what you suggest.\r\n\r\nThe problem is that OSConst.UBUNTU_FAMILY is a simple string so it won\'t match to OSConst.DEBIAN_FAMILY.\r\n\r\n```python\r\n>>> type(os_check.OSConst.OS_UBUNTU)\r\n<type \'str\'>\r\n```\r\n\r\nHowever this one works correctly and returns True on Debian.\r\n\r\n```python\r\n>>> os_check.OSCheck.is_ubuntu_family()\r\nTrue\r\n```\r\nSo instead of the map lookup we could modify it to something like this:\r\n\r\n```python\r\n  @classmethod\r\n  def get_new_instance(cls, os_family=None):\r\n    """"""\r\n    Construct new instance of Repository Manager object. Call is not thread-safe\r\n\r\n    :param os_family:  os family string; best used in combination with `OSCheck.get_os_family()`\r\n    :type os_family str\r\n    :rtype GenericManager\r\n    """"""\r\n    if not os_family:\r\n      os_family = OSCheck.get_os_family()\r\n\r\n    if OSCheck.is_in_family(os_family, OSConst.UBUNTU_FAMILY):\r\n      return AptManager()\r\n    if OSCheck.is_in_family(os_family, OSConst.SUSE_FAMILY):\r\n      return ZypperManager()\r\n    if OSCheck.is_in_family(os_family, OSConst.REDHAT_FAMILY):\r\n      return YumManager()\r\n    if OSCheck.is_in_family(os_family, OSConst.WINSRV_FAMILY):\r\n      return ChocoManager()\r\n\r\n    raise RuntimeError(""Not able to create Repository Manager object for unsupported OS family {0}"".format(os_family))```\r\n', 'commenter': 'zeroflag'}]"
966,ambari-agent/src/test/python/ambari_agent/TestRegistration.py,"@@ -50,17 +50,15 @@ def test_registration_build(self, get_os_version_mock, get_os_type_mock, run_os_
     run_os_cmd_mock.return_value = (3, """", """")
     register = Register(config)
     reference_version = '2.1.0'
-    data = register.build(reference_version, 1)
+    data = register.build()","[{'comment': 'This gives error on Mac.\r\n\r\n```\r\n  File ""ambari-agent/src/main/python/ambari_agent/HostInfo.py"", line 86, in checkLiveServices\r\n    if ""ntpd"" in service and is_redhat7_or_higher:\r\nTypeError: argument of type \'NoneType\' is not iterable\r\n```', 'commenter': 'adoroszlai'}, {'comment': '@adoroszlai  can you please provide reproduce of this?', 'commenter': 'aonishuk'}]"
966,ambari-agent/src/test/python/ambari_agent/TestCustomServiceOrchestrator.py,"@@ -408,14 +346,15 @@ def side_effect(*args, **kwargs):
       pass
 
   @patch.object(OSCheck, ""os_distribution"", new = MagicMock(return_value = os_distro_value))
+  @patch.object(ConfigurationBuilder, ""get_configuration"")
   @patch.object(CustomServiceOrchestrator, ""get_py_executor"")
   @patch(""ambari_commons.shell.kill_process_with_children"")
   @patch.object(FileCache, ""__init__"")
   @patch.object(CustomServiceOrchestrator, ""resolve_script_path"")
   @patch.object(CustomServiceOrchestrator, ""resolve_hook_script_path"")
   def test_cancel_backgound_command(self, resolve_hook_script_path_mock,
                                     resolve_script_path_mock, FileCache_mock, kill_process_with_children_mock,
-                                    get_py_executor_mock):
+                                    get_py_executor_mock, get_configuration_mock):","[{'comment': '`test_cancel_background_command` fails for me with:\r\n\r\n```\r\n  File ""ambari-agent/src/test/python/ambari_agent/TestCustomServiceOrchestrator.py"", line 409, in test_cancel_backgound_command\r\n    self.assertTrue(kill_process_with_children_mock.called)\r\nAssertionError: False is not true\r\n```', 'commenter': 'adoroszlai'}, {'comment': '@adoroszlai  can you please provide reproduce of this?', 'commenter': 'aonishuk'}, {'comment': 'fixed', 'commenter': 'aonishuk'}, {'comment': 'Where?', 'commenter': 'adoroszlai'}, {'comment': ""I don't remember the exact place. But it is fixed you can re-run it for yourself manually."", 'commenter': 'aonishuk'}]"
966,ambari-agent/src/test/python/ambari_agent/TestActionQueue.py,"@@ -45,7 +45,7 @@
 
 CLUSTER_ID = '0'
 
-class TestActionQueue:#(TestCase):
+class TestActionQueue(TestCase):","[{'comment': 'Can you please remove or fix the test cases with `@not_for_platform(PLATFORM_LINUX)`?  The following ones are erroring:\r\n\r\n```\r\ntest_execute_retryable_command\r\ntest_execute_retryable_command_fail_and_succeed\r\ntest_execute_retryable_command_succeed\r\n```\r\n\r\ndue to:\r\n\r\n```\r\n    actionQueue = ActionQueue(AmbariConfig(), dummy_controller)\r\nTypeError: __init__() takes exactly 2 arguments (3 given)\r\n```', 'commenter': 'adoroszlai'}, {'comment': 'removed the tests', 'commenter': 'aonishuk'}]"
966,ambari-agent/src/test/python/ambari_agent/TestRegistration.py,"@@ -38,18 +37,20 @@ class TestRegistration(TestCase):
   @patch.object(FacterLinux, ""facterInfo"", new = MagicMock(return_value={}))
   @patch.object(FacterLinux, ""__init__"", new = MagicMock(return_value = None))
   @patch(""resource_management.core.shell.call"")
+  @patch.object(OSCheck, ""get_os_family"")
   @patch.object(OSCheck, ""get_os_type"")
   @patch.object(OSCheck, ""get_os_version"")
-  def test_registration_build(self, get_os_version_mock, get_os_type_mock, run_os_cmd_mock, Popen_mock):
+  def test_registration_build(self, get_os_version_mock, get_os_family_mock, get_os_type_mock, run_os_cmd_mock, Popen_mock):
     config = AmbariConfig()
     tmpdir = tempfile.gettempdir()
     config.set('agent', 'prefix', tmpdir)
     config.set('agent', 'current_ping_port', '33777')
+    get_os_family_mock.return_value = ""suse""
     get_os_type_mock.return_value = ""suse""
     get_os_version_mock.return_value = ""11""
     run_os_cmd_mock.return_value = (3, """", """")
+    from ambari_agent.Register import Register","[{'comment': 'Unfortunately this fix works only if `TestRegistration` is run by itself.  If all tests are run, then it still fails with the same error.\r\n\r\nSo I think it\'s best to return `(""ntpd"",)` for other platforms from `get_ntp_service`.', 'commenter': 'adoroszlai'}, {'comment': 'done', 'commenter': 'aonishuk'}]"
966,ambari-agent/src/test/python/ambari_agent/TestActionQueue.py,"@@ -491,6 +494,8 @@ def test_do_not_log_execution_commands(self, command_status_dict_mock,
       [call(""out\n\nCommand completed successfully!\n"", ""9""), call(""stderr"", ""9"")], any_order=True)
     self.assertEqual(len(reports), 1)
     self.assertEqual(expected, reports[0])
+    import threading
+    print threading.enumerate()","[{'comment': 'Is this needed?', 'commenter': 'adoroszlai'}, {'comment': 'thanks. Removed.', 'commenter': 'aonishuk'}]"
975,ambari-server/src/main/resources/Ambari-DDL-Postgres-CREATE.sql,"@@ -159,9 +159,10 @@ CREATE TABLE clusterservices (
   cluster_id BIGINT NOT NULL,
   service_group_id BIGINT NOT NULL,
   service_enabled INTEGER NOT NULL,
-  CONSTRAINT PK_clusterservices PRIMARY KEY (id, service_group_id, cluster_id),
-  CONSTRAINT UQ_service_id UNIQUE (id),
-  CONSTRAINT FK_clusterservices_cluster_id FOREIGN KEY (service_group_id, cluster_id) REFERENCES servicegroups (id, cluster_id));
+  CONSTRAINT PK_clusterservices PRIMARY KEY (id),
+  CONSTRAINT UK_clusterservices_id UNIQUE (id, cluster_id, service_group_id, service_name),","[{'comment': ""Since `id` is the primary key, this is guaranteed to be unique even with duplicate values for the rest of the columns.  Shouldn't it constrain `(cluster_id, service_group_id, service_name)` instead?"", 'commenter': 'adoroszlai'}, {'comment': 'Fixed.', 'commenter': 'swapanshridhar'}]"
975,ambari-server/src/main/resources/Ambari-DDL-Derby-CREATE.sql,"@@ -134,10 +134,9 @@ CREATE TABLE servicegroups (
   service_group_name VARCHAR(255) NOT NULL,
   cluster_id BIGINT NOT NULL,
   stack_id BIGINT NOT NULL,
-  CONSTRAINT PK_servicegroups PRIMARY KEY (id, cluster_id),
-  CONSTRAINT FK_servicegroups_cluster_id FOREIGN KEY (cluster_id) REFERENCES clusters (cluster_id),
-  CONSTRAINT FK_servicegroups_stack_id FOREIGN KEY (stack_id) REFERENCES stack (stack_id),
-  CONSTRAINT UQ_TEMP_UNTIL_REAL_PK UNIQUE(id));
+  CONSTRAINT PK_servicegroups PRIMARY KEY (id),
+  CONSTRAINT UK_servicegroups_id UNIQUE (id, cluster_id, service_group_name),","[{'comment': 'This unique constraint is not needed unless you want to remove the ID value so that a service group name is unique in a cluster.', 'commenter': 'jonathan-hurley'}, {'comment': 'We want SG name to be unique across cluster, as the name is the only thing distinguishable from API perspective. CC @jayush ', 'commenter': 'swapanshridhar'}]"
975,ambari-server/src/main/resources/Ambari-DDL-Derby-CREATE.sql,"@@ -157,9 +156,9 @@ CREATE TABLE clusterservices (
   cluster_id BIGINT NOT NULL,
   service_group_id BIGINT NOT NULL,
   service_enabled INTEGER NOT NULL,
-  CONSTRAINT PK_clusterservices PRIMARY KEY (id, service_group_id, cluster_id),
-  CONSTRAINT UQ_service_id UNIQUE (id),
-  CONSTRAINT FK_clusterservices_cluster_id FOREIGN KEY (service_group_id, cluster_id) REFERENCES servicegroups (id, cluster_id));
+  CONSTRAINT PK_clusterservices PRIMARY KEY (id),
+  CONSTRAINT UK_clusterservices_id UNIQUE (id, cluster_id, service_group_id, service_name),","[{'comment': ""Same as above, don't use a PK as part of a unique clause."", 'commenter': 'jonathan-hurley'}, {'comment': 'Fixed.', 'commenter': 'swapanshridhar'}]"
975,ambari-server/src/main/java/org/apache/ambari/server/orm/dao/ServiceDesiredStateDAO.java,"@@ -39,8 +38,16 @@
   Provider<EntityManager> entityManagerProvider;
 
   @RequiresSession
-  public ServiceDesiredStateEntity findByPK(ServiceDesiredStateEntityPK primaryKey) {
-    return entityManagerProvider.get().find(ServiceDesiredStateEntity.class, primaryKey);
+  public ServiceDesiredStateEntity findByServiceId(Long serviceId) {
+    TypedQuery<ServiceDesiredStateEntity> query = entityManagerProvider.get()
+            .createNamedQuery(""ServiceDesiredStateByServiceId"", ServiceDesiredStateEntity.class);
+    query.setParameter(""serviceId"", serviceId);
+
+    try {
+      return query.getSingleResult();
+    } catch (NoResultException ignored) {
+      return null;
+    }","[{'comment': ""I'm not sure that this change is a good idea. Moving away from a PK for this table, it means that any lookup must hit the database instead of the L1 cache. Why was this necessary? "", 'commenter': 'jonathan-hurley'}, {'comment': 'Change was made as there is a way to simplify what constitutes as primary here. Servceid will be unique/primary here.\r\n\r\nWith my current change of using service_id as primary key, and querying it as : \r\n\r\n>>>>> return entityManagerProvider.get().find(ServiceDesiredStateEntity.class, serviceId);\r\n\r\nThat will get the query serviced via cache.', 'commenter': 'swapanshridhar'}, {'comment': ""But your final code here does not do this. Instead, to lookup a ServiceDesiredStateEntity via a NamedQuery. That means anytime you need the state of a service, it's hitting the DB."", 'commenter': 'jonathan-hurley'}, {'comment': 'Sure, I was not clear on my earlier comment. Given that we were discussing on whether I should change PK for the *servicedesiredstate* table or not and given that I had changed it, but with the change the change in PK, I was not using EntityManager. \r\n\r\nSo, I had posted the change in comment that if we want to go changing the Pk construct (which I had done earlier), and by using the EntityManager (suggested in comments), we can leverage the caching mechanism. \r\n\r\nSo, I have incorporated the EntityManager change now.', 'commenter': 'swapanshridhar'}]"
975,ambari-server/src/main/java/org/apache/ambari/server/orm/dao/ServiceGroupDAO.java,"@@ -43,15 +42,9 @@
   DaoUtils daoUtils;
 
   @RequiresSession
-  public ServiceGroupEntity findByPK(ServiceGroupEntityPK clusterServiceGroupEntityPK) {
-    return entityManagerProvider.get().find(ServiceGroupEntity.class, clusterServiceGroupEntityPK);
-  }
-
-  @RequiresSession
-  public ServiceGroupEntity findByClusterAndServiceGroupIds(Long clusterId, Long serviceGroupId) {
+  public ServiceGroupEntity findByPK(Long serviceGroupId) {
     TypedQuery<ServiceGroupEntity> query = entityManagerProvider.get()
-      .createNamedQuery(""serviceGroupByClusterAndServiceGroupIds"", ServiceGroupEntity.class);
-    query.setParameter(""clusterId"", clusterId);
+            .createNamedQuery(""serviceGroupById"", ServiceGroupEntity.class);
     query.setParameter(""serviceGroupId"", serviceGroupId);","[{'comment': ""this is a very bad idea - you're not actually looking up the SG by a PK, so EclipseLink will always make a DB query. You should use the EntityManager to findByPK ..."", 'commenter': 'jonathan-hurley'}, {'comment': 'Got it. Fetching it like this now.\r\n\r\n>> return entityManagerProvider.get().find(ServiceGroupEntity.class, serviceGroupId);\r\n\r\nSimilar change done for ServiceDAO as well.', 'commenter': 'swapanshridhar'}]"
975,ambari-server/src/main/java/org/apache/ambari/server/orm/entities/ClusterServiceEntity.java,"@@ -57,18 +53,22 @@
         ""AND clusterEntity.clusterName=:clusterName"")
 })
 @Entity
+@Table(
+    name = ""clusterservices"",
+    uniqueConstraints = @UniqueConstraint(
+        name = ""UK_clusterservices_id"",
+        columnNames = {""id"" , ""service_name"", ""service_group_id"", ""cluster_id""}))
+","[{'comment': 'No PK ID fields in unique clauses', 'commenter': 'jonathan-hurley'}, {'comment': 'Fixed.', 'commenter': 'swapanshridhar'}]"
975,ambari-server/src/main/java/org/apache/ambari/server/orm/entities/ServiceDesiredStateEntity.java,"@@ -25,23 +25,32 @@
 import javax.persistence.Enumerated;
 import javax.persistence.Id;
 import javax.persistence.JoinColumn;
-import javax.persistence.JoinColumns;
+import javax.persistence.NamedQueries;
+import javax.persistence.NamedQuery;
 import javax.persistence.OneToOne;
+import javax.persistence.Table;
+import javax.persistence.UniqueConstraint;
 
 import org.apache.ambari.server.state.MaintenanceState;
 import org.apache.ambari.server.state.State;
 
-@javax.persistence.IdClass(ServiceDesiredStateEntityPK.class)
-@javax.persistence.Table(name = ""servicedesiredstate"")
+@NamedQueries({
+        @NamedQuery(name = ""ServiceDesiredStateByServiceId"", query =
+                ""SELECT serviceDesiredState "" +
+                        ""FROM ServiceDesiredStateEntity serviceDesiredState "" +
+                        ""WHERE serviceDesiredState.serviceId=:serviceId "")
+})
+@Table(
+        name = ""servicedesiredstate"",
+        uniqueConstraints = @UniqueConstraint(name = ""UQ_servicedesiredstate"",
+                                              columnNames = {""service_id""}))
 @Entity","[{'comment': 'Spacing here looks off - 4 spaces instead of 2?', 'commenter': 'jonathan-hurley'}, {'comment': 'Done.', 'commenter': 'swapanshridhar'}]"
975,ambari-server/src/main/java/org/apache/ambari/server/orm/entities/ServiceGroupEntity.java,"@@ -35,32 +34,35 @@
 import javax.persistence.OneToOne;
 import javax.persistence.Table;
 import javax.persistence.TableGenerator;
+import javax.persistence.UniqueConstraint;
 
 import org.apache.ambari.server.orm.dao.ServiceGroupDAO;
 
 
-@IdClass(ServiceGroupEntityPK.class)
-@Table(name = ""servicegroups"")
 @NamedQueries({
-  @NamedQuery(name = ""serviceGroupByClusterAndServiceGroupIds"", query =
+  @NamedQuery(name = ""serviceGroupById"", query =
     ""SELECT serviceGroup "" +
       ""FROM ServiceGroupEntity serviceGroup "" +
-      ""JOIN serviceGroup.clusterEntity cluster "" +
-      ""WHERE serviceGroup.serviceGroupId=:serviceGroupId AND cluster.clusterId=:clusterId""),
+      ""WHERE serviceGroup.serviceGroupId=:serviceGroupId""),
   @NamedQuery(name = ServiceGroupDAO.SERVICE_GROUP_BY_CLUSTER_ID_AND_SERVICE_GROUP_NAME, query =
     ""SELECT serviceGroup "" +
       ""FROM ServiceGroupEntity serviceGroup "" +
       ""WHERE serviceGroup.serviceGroupName = :serviceGroupName AND serviceGroup.clusterId = :clusterId"")
 })
 @Entity
+@Table(
+    name = ""servicegroups"",
+    uniqueConstraints = @UniqueConstraint(
+                name = ""UK_servicegroups_id"",
+                columnNames = { ""id"" , ""cluster_id"", ""service_group_name"" }))
+","[{'comment': 'No PK ID fields in unique constraints.', 'commenter': 'jonathan-hurley'}, {'comment': 'Fixed.', 'commenter': 'swapanshridhar'}]"
975,ambari-server/src/main/java/org/apache/ambari/server/orm/entities/ClusterServiceEntity.java,"@@ -29,46 +29,46 @@
 import javax.persistence.GenerationType;
 import javax.persistence.Id;
 import javax.persistence.JoinColumn;
-import javax.persistence.JoinColumns;
 import javax.persistence.ManyToOne;
 import javax.persistence.NamedQueries;
 import javax.persistence.NamedQuery;
 import javax.persistence.OneToMany;
 import javax.persistence.OneToOne;
+import javax.persistence.Table;
 import javax.persistence.TableGenerator;
+import javax.persistence.UniqueConstraint;
 
-@javax.persistence.IdClass(ClusterServiceEntityPK.class)
-@javax.persistence.Table(name = ""clusterservices"")
 @NamedQueries({
   @NamedQuery(name = ""clusterServiceById"", query =
     ""SELECT clusterService "" +
       ""FROM ClusterServiceEntity clusterService "" +
-      ""JOIN clusterService.serviceGroupEntity serviceGroup "" +
-      ""WHERE clusterService.serviceId=:serviceId "" +
-      ""AND  serviceGroup.serviceGroupId=:serviceGroupId "" +
-      ""AND serviceGroup.clusterId=:clusterId""),
-   @NamedQuery(name = ""clusterServiceByName"", query =
-     ""SELECT clusterService "" +
-      ""FROM ClusterServiceEntity clusterService "" +
-       ""JOIN clusterService.serviceGroupEntity serviceGroup "" +
-        ""JOIN clusterService.clusterEntity clusterEntity "" +
-       ""WHERE clusterService.serviceName=:serviceName "" +
-       ""AND  serviceGroup.serviceGroupName=:serviceGroupName "" +
-        ""AND clusterEntity.clusterName=:clusterName"")
+      ""WHERE clusterService.serviceId=:serviceId ""),
+  @NamedQuery(name = ""clusterServiceByName"", query =
+    ""SELECT clusterService "" +
+    ""FROM ClusterServiceEntity clusterService "" +
+    ""JOIN clusterService.serviceGroupEntity serviceGroup "" +
+    ""JOIN clusterService.clusterEntity clusterEntity "" +
+    ""WHERE clusterService.serviceName=:serviceName "" +
+    ""AND  serviceGroup.serviceGroupName=:serviceGroupName "" +
+    ""AND clusterEntity.clusterName=:clusterName"")
 })","[{'comment': 'Instead of doing the `JOINS` yourself, can you do\r\n\r\n```\r\nSELECT clusterService FROM ClusterServiceEntity clusterService WHERE \r\n  clusterService.serviceName=:serviceName AND \r\n  clusterService.serviceGroupEntity.serviceGroupName=:serviceGroupName AND \r\n  clusterService.clusterEntity.clusterName:=clusterName\r\n```', 'commenter': 'jonathan-hurley'}, {'comment': 'Yep. Updated this part.', 'commenter': 'swapanshridhar'}]"
975,ambari-server/src/main/java/org/apache/ambari/server/orm/entities/ClusterServiceEntity.java,"@@ -29,46 +29,46 @@
 import javax.persistence.GenerationType;
 import javax.persistence.Id;
 import javax.persistence.JoinColumn;
-import javax.persistence.JoinColumns;
 import javax.persistence.ManyToOne;
 import javax.persistence.NamedQueries;
 import javax.persistence.NamedQuery;
 import javax.persistence.OneToMany;
 import javax.persistence.OneToOne;
+import javax.persistence.Table;
 import javax.persistence.TableGenerator;
+import javax.persistence.UniqueConstraint;
 
-@javax.persistence.IdClass(ClusterServiceEntityPK.class)
-@javax.persistence.Table(name = ""clusterservices"")
 @NamedQueries({
   @NamedQuery(name = ""clusterServiceById"", query =
     ""SELECT clusterService "" +
       ""FROM ClusterServiceEntity clusterService "" +
-      ""JOIN clusterService.serviceGroupEntity serviceGroup "" +
-      ""WHERE clusterService.serviceId=:serviceId "" +
-      ""AND  serviceGroup.serviceGroupId=:serviceGroupId "" +
-      ""AND serviceGroup.clusterId=:clusterId""),
-   @NamedQuery(name = ""clusterServiceByName"", query =
-     ""SELECT clusterService "" +
-      ""FROM ClusterServiceEntity clusterService "" +
-       ""JOIN clusterService.serviceGroupEntity serviceGroup "" +
-        ""JOIN clusterService.clusterEntity clusterEntity "" +
-       ""WHERE clusterService.serviceName=:serviceName "" +
-       ""AND  serviceGroup.serviceGroupName=:serviceGroupName "" +
-        ""AND clusterEntity.clusterName=:clusterName"")
+      ""WHERE clusterService.serviceId=:serviceId ""),
+  @NamedQuery(name = ""clusterServiceByName"", query =
+    ""SELECT clusterService "" +
+    ""FROM ClusterServiceEntity clusterService "" +
+    ""JOIN clusterService.serviceGroupEntity serviceGroup "" +
+    ""JOIN clusterService.clusterEntity clusterEntity "" +
+    ""WHERE clusterService.serviceName=:serviceName "" +
+    ""AND  serviceGroup.serviceGroupName=:serviceGroupName "" +
+    ""AND clusterEntity.clusterName=:clusterName"")
 })
 @Entity
+@Table(
+  name = ""clusterservices"",
+  uniqueConstraints = @UniqueConstraint(
+  name = ""UK_clusterservices_id"",
+  columnNames = {""service_name"", ""service_group_id"", ""cluster_id""}))
+","[{'comment': ""is cluster_id necessary in this unique clause? Aren't service group IDs unique anyway and only able to be associated with a single cluster?"", 'commenter': 'jonathan-hurley'}, {'comment': 'Correct. Removed cluster_id here and in DB table for UQ constraint.', 'commenter': 'swapanshridhar'}]"
975,ambari-server/src/main/java/org/apache/ambari/server/orm/entities/ServiceDesiredStateEntity.java,"@@ -25,23 +25,33 @@
 import javax.persistence.Enumerated;
 import javax.persistence.Id;
 import javax.persistence.JoinColumn;
-import javax.persistence.JoinColumns;
+import javax.persistence.NamedQueries;
+import javax.persistence.NamedQuery;
 import javax.persistence.OneToOne;
+import javax.persistence.Table;
+import javax.persistence.UniqueConstraint;
 
 import org.apache.ambari.server.state.MaintenanceState;
 import org.apache.ambari.server.state.State;
 
-@javax.persistence.IdClass(ServiceDesiredStateEntityPK.class)
-@javax.persistence.Table(name = ""servicedesiredstate"")
+@NamedQueries({
+  @NamedQuery(name = ""ServiceDesiredStateByServiceId"", query =
+    ""SELECT serviceDesiredState "" +
+    ""FROM ServiceDesiredStateEntity serviceDesiredState "" +
+    ""WHERE serviceDesiredState.serviceId=:serviceId "")
+})
+@Table(
+  name = ""servicedesiredstate"",
+  uniqueConstraints = @UniqueConstraint(name = ""UQ_servicedesiredstate"",
+                                        columnNames = {""service_id""}))","[{'comment': 'Does this unique constraint actually exist in the SQL files?', 'commenter': 'jonathan-hurley'}, {'comment': ""Nope. Doesnt exist.\r\nIn fact given that 'service_id' is the Primary key, Unique wouldn't be required. SO removed it from Entity."", 'commenter': 'swapanshridhar'}]"
975,ambari-server/src/main/java/org/apache/ambari/server/orm/entities/ServiceGroupEntity.java,"@@ -35,32 +34,35 @@
 import javax.persistence.OneToOne;
 import javax.persistence.Table;
 import javax.persistence.TableGenerator;
+import javax.persistence.UniqueConstraint;
 
 import org.apache.ambari.server.orm.dao.ServiceGroupDAO;
 
 
-@IdClass(ServiceGroupEntityPK.class)
-@Table(name = ""servicegroups"")
 @NamedQueries({
-  @NamedQuery(name = ""serviceGroupByClusterAndServiceGroupIds"", query =
+  @NamedQuery(name = ""serviceGroupById"", query =
     ""SELECT serviceGroup "" +
       ""FROM ServiceGroupEntity serviceGroup "" +
-      ""JOIN serviceGroup.clusterEntity cluster "" +
-      ""WHERE serviceGroup.serviceGroupId=:serviceGroupId AND cluster.clusterId=:clusterId""),
+      ""WHERE serviceGroup.serviceGroupId=:serviceGroupId""),
   @NamedQuery(name = ServiceGroupDAO.SERVICE_GROUP_BY_CLUSTER_ID_AND_SERVICE_GROUP_NAME, query =
     ""SELECT serviceGroup "" +
       ""FROM ServiceGroupEntity serviceGroup "" +
       ""WHERE serviceGroup.serviceGroupName = :serviceGroupName AND serviceGroup.clusterId = :clusterId"")
 })
 @Entity
+@Table(
+    name = ""servicegroups"",
+    uniqueConstraints = @UniqueConstraint(
+                name = ""UK_servicegroups_id"",","[{'comment': ""Shouldn't this be UQ_ ?"", 'commenter': 'jonathan-hurley'}, {'comment': 'Fixed.', 'commenter': 'swapanshridhar'}]"
975,ambari-server/src/main/java/org/apache/ambari/server/orm/entities/ClusterServiceEntity.java,"@@ -29,46 +29,44 @@
 import javax.persistence.GenerationType;
 import javax.persistence.Id;
 import javax.persistence.JoinColumn;
-import javax.persistence.JoinColumns;
 import javax.persistence.ManyToOne;
 import javax.persistence.NamedQueries;
 import javax.persistence.NamedQuery;
 import javax.persistence.OneToMany;
 import javax.persistence.OneToOne;
+import javax.persistence.Table;
 import javax.persistence.TableGenerator;
+import javax.persistence.UniqueConstraint;
 
-@javax.persistence.IdClass(ClusterServiceEntityPK.class)
-@javax.persistence.Table(name = ""clusterservices"")
 @NamedQueries({
   @NamedQuery(name = ""clusterServiceById"", query =
     ""SELECT clusterService "" +
       ""FROM ClusterServiceEntity clusterService "" +
-      ""JOIN clusterService.serviceGroupEntity serviceGroup "" +
-      ""WHERE clusterService.serviceId=:serviceId "" +
-      ""AND  serviceGroup.serviceGroupId=:serviceGroupId "" +
-      ""AND serviceGroup.clusterId=:clusterId""),
-   @NamedQuery(name = ""clusterServiceByName"", query =
-     ""SELECT clusterService "" +
-      ""FROM ClusterServiceEntity clusterService "" +
-       ""JOIN clusterService.serviceGroupEntity serviceGroup "" +
-        ""JOIN clusterService.clusterEntity clusterEntity "" +
-       ""WHERE clusterService.serviceName=:serviceName "" +
-       ""AND  serviceGroup.serviceGroupName=:serviceGroupName "" +
-        ""AND clusterEntity.clusterName=:clusterName"")
+      ""WHERE clusterService.serviceId=:serviceId ""),
+  @NamedQuery(name = ""clusterServiceByName"", query =
+    ""SELECT clusterService "" +
+    ""FROM ClusterServiceEntity clusterService "" +
+    ""WHERE clusterService.serviceName=:serviceName "" +
+    ""AND clusterService.serviceGroupEntity.serviceGroupName=:serviceGroupName "" +
+    ""AND clusterService.clusterEntity.clusterName=:clusterName"")
 })
 @Entity
+@Table(
+  name = ""clusterservices"",
+  uniqueConstraints = @UniqueConstraint(
+  name = ""UK_clusterservices_id"",
+  columnNames = {""service_name"", ""service_group_id""}))","[{'comment': ""Shouldn't this be UQ_?"", 'commenter': 'jonathan-hurley'}, {'comment': 'Fixed.', 'commenter': 'swapanshridhar'}]"
998,ambari-logsearch/ambari-logsearch-web/src/app/services/user-settings.service.ts,"@@ -87,7 +115,7 @@ export class UserSettingsService {
           filter: newConfig
         }, null, {
           clusterName
-        });
+        }).first().subscribe(addResponseHandler(clusterName), addResponseHandler(clusterName));","[{'comment': ""isn't `first()` redundant here, since it's already added to `request` method override in `HttpClientService`?"", 'commenter': 'aBabiichuk'}]"
1038,ambari-server/src/main/resources/common-services/KAFKA/0.8.1/package/scripts/setup_ranger_kafka.py,"@@ -84,14 +84,14 @@ def setup_ranger_kafka():
     if params.stack_supports_core_site_for_ranger_plugin and params.enable_ranger_kafka and params.kerberos_security_enabled:
       if params.has_namenode:
         Logger.info(""Stack supports core-site.xml creation for Ranger plugin and Namenode is installed, creating create core-site.xml from namenode configurations"")
-        setup_core_site_for_required_plugins(component_user = params.kafka_user, component_group = params.user_group,
+        setup_configuration_file_for_required_plugins(component_user = params.kafka_user, component_group = params.user_group,
                                              create_core_site_path = params.conf_dir, configurations = params.config['configurations']['core-site'],
-                                             configuration_attributes = params.config['configuration_attributes']['core-site'])
+                                             configuration_attributes = params.config['configuration_attributes']['core-site'], file_name='core-site.xml')","[{'comment': ""Need to change `config['configuration_attributes']` to `config['configurationAttributes']`"", 'commenter': 'fimugdha'}, {'comment': 'updated patch to address review comment.', 'commenter': 'vishalsuvagia'}]"
1055,ambari-web/app/controllers/main/admin/highAvailability/nameNode/rollbackHA/step2_controller.js,"@@ -27,7 +27,8 @@ App.RollbackHighAvailabilityWizardStep2Controller = App.HighAvailabilityWizardSt
       name: 'admin.high_availability.getNnCheckPointStatus',
       sender: this,
       data: {
-        hostName: hostName
+        hostName: hostName,
+        nameNodeId: App.HostComponent.find().findProperty('componentName', 'NAMENODE').get('compId')","[{'comment': 'I would prefer to see these lookups performed outside of the object literal. That way you can check if they return a value before you proceed.', 'commenter': 'jgolieb'}, {'comment': 'There will always be an id associated with a component which will be used in the host component endpoint. If not then it will be a BE bug and would be easily identified with this console error.', 'commenter': 'ishanbha'}]"
1055,ambari-web/app/mappers/service_metrics_mapper.js,"@@ -192,6 +192,7 @@ App.serviceMetricsMapper = App.QuickDataMapper.create({
     work_status: 'HostRoles.state',
     passive_state: 'HostRoles.maintenance_state',
     display_name: 'HostRoles.display_name',
+    comp_id: 'HostRoles.id',","[{'comment': 'Please name this component_id so it follows the pattern.', 'commenter': 'jgolieb'}, {'comment': 'The camel case auto conversion would convert this to compId which is the name of this attribute defined for this model', 'commenter': 'ishanbha'}]"
1055,ambari-web/app/models/host_component.js,"@@ -21,6 +21,7 @@ var App = require('app');
 App.HostComponent = DS.Model.extend({
   workStatus: DS.attr('string'),
   passiveState: DS.attr('string'),
+  compId: DS.attr('string'),","[{'comment': 'Please name this componentId and make it a number type', 'commenter': 'jgolieb'}, {'comment': 'I have used compId across the entire commit and should be fine as long as it is consistent. Also there is no particular need for this to be a number as this is an identifier and would not be used in any calculation. In the backend too this is a string. In future the id could not be a number but a combination of some string like a few other models. ', 'commenter': 'ishanbha'}]"
1073,ambari-agent/src/main/python/ambari_agent/CustomServiceOrchestrator.py,"@@ -556,6 +558,49 @@ def dump_command_to_json(self, command, retry=False):
       f.write(content)
     return file_path
 
+  def decompressClusterHostInfo(self, clusterHostInfo):
+    info = clusterHostInfo.copy()
+    #Pop info not related to host roles
+    hostsList = info.pop(self.HOSTS_LIST_KEY)
+    pingPorts = info.pop(self.PING_PORTS_KEY)
+    racks = info.pop(self.RACKS_KEY)
+    ipv4_addresses = info.pop(self.IPV4_ADDRESSES_KEY)
+
+    ambariServerHost = info.pop(self.AMBARI_SERVER_HOST)
+    ambariServerPort = info.pop(self.AMBARI_SERVER_PORT)
+    ambariServerUseSsl = info.pop(self.AMBARI_SERVER_USE_SSL)
+
+    decompressedMap = {}
+
+    for k,v in info.items():","[{'comment': 'how about adding some unit test for this logic?', 'commenter': 'Unknown'}]"
1114,ambari-server/src/main/java/org/apache/ambari/server/controller/utilities/NodeRefresher.java,"@@ -0,0 +1,147 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.controller.utilities;
+
+import static java.util.Collections.emptyList;
+import static java.util.Collections.emptyMap;
+import static java.util.Collections.singletonList;
+import static org.apache.ambari.server.Role.DATANODE;
+import static org.apache.ambari.server.Role.NAMENODE;
+
+import java.util.Collections;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.ambari.server.AmbariException;
+import org.apache.ambari.server.actionmanager.ActionManager;
+import org.apache.ambari.server.actionmanager.RequestFactory;
+import org.apache.ambari.server.actionmanager.Stage;
+import org.apache.ambari.server.actionmanager.StageFactory;
+import org.apache.ambari.server.controller.ActionExecutionContext;
+import org.apache.ambari.server.controller.AmbariCustomCommandExecutionHelper;
+import org.apache.ambari.server.controller.internal.RequestResourceFilter;
+import org.apache.ambari.server.controller.internal.RequestStageContainer;
+import org.apache.ambari.server.events.HostsRemovedEvent;
+import org.apache.ambari.server.events.publishers.AmbariEventPublisher;
+import org.apache.ambari.server.state.Cluster;
+import org.apache.ambari.server.state.ServiceComponent;
+import org.apache.ambari.server.utils.StageUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.eventbus.Subscribe;
+import com.google.inject.Inject;
+import com.google.inject.Singleton;
+
+/**
+ * Sends REFRESH_NODE command to the NAMENODE after deleting a host that was running a DATANODE
+ */
+@Singleton
+public class NodeRefresher {
+  private final static Logger LOG = LoggerFactory.getLogger(NodeRefresher.class);
+  private static final String REFRESH_NODE = ""REFRESH_NODES"";
+  private static final String REQUEST_CONTEXT = ""Refresh NameNode"";
+  private final AmbariCustomCommandExecutionHelper executionHelper;
+  private final StageFactory stageFactory;
+  private final ActionManager actionManager;
+  private final RequestFactory requestFactory;
+
+  @Inject
+  public NodeRefresher(AmbariCustomCommandExecutionHelper executionHelper, StageFactory stageFactory, ActionManager actionManager, RequestFactory requestFactory) {
+    this.executionHelper = executionHelper;
+    this.stageFactory = stageFactory;
+    this.actionManager = actionManager;
+    this.requestFactory = requestFactory;
+  }
+
+  public void register(AmbariEventPublisher eventPublisher) {
+    eventPublisher.register(this);
+  }
+
+  @Subscribe
+  public void onHostRemoved(HostsRemovedEvent event) {
+    if (event.hasComponent(DATANODE)) {
+      for (Cluster cluster : event.getClusters()) {
+        try {
+          LOG.info(""Sending {} command to NAMENODE after host was removed {}"", REFRESH_NODE, event);
+          refresh(nameNode(cluster), cluster);
+        } catch (AmbariException e) {
+          LOG.warn(""Could not send "" + REFRESH_NODE + "" to NAMENODE after deleting host: "" + event, e);
+        }
+      }
+    }
+  }
+
+  private ServiceComponent nameNode(Cluster cluster) throws AmbariException {
+    return cluster.getService(""HDFS"").getServiceComponent(NAMENODE.name());
+  }
+
+  protected void refresh(ServiceComponent namenode, Cluster cluster) throws AmbariException {","[{'comment': ""We've been operating without this option for years without a problem.  I think there should be a configuration knob to control this behavior since NN is going to eventually reconcile dead DN anyway."", 'commenter': 'ncole'}, {'comment': ""@ncole, can you add a comment to the bugdb jira? I'm happy to not add code if it's not absolutely needed."", 'commenter': 'zeroflag'}]"
1120,ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/util/FileUtil.java,"@@ -121,11 +121,11 @@ public static File getFileFromClasspath(String filename) {
             return files;
           }
         } catch (Exception e) {
-          LOG.warn(""Input file not found by pattern (exception thrown); {}, message: {}"", searchPath, e.getMessage());
+          LOG.warn(""Input file has not found by pattern (exception thrown); {}, message: {}"", searchPath, e.getMessage());","[{'comment': 'Original wording is better (applies to both log statements).  It\'s not the ""input file"" that is trying to find something, is it?  So please keep the original one, or change to ""was not found"" or ""has not been found"".', 'commenter': 'adoroszlai'}]"
1165,ambari-logsearch/ambari-logsearch-web/src/app/components/logs-container/logs-container.component.ts,"@@ -149,7 +149,7 @@ export class LogsContainerComponent implements OnInit, OnDestroy {
     );
 
     this.subscriptions.push(
-      Observable.fromEvent(window, 'scroll').throttleTime(10).subscribe(() => {
+      Observable.fromEvent(window, 'scroll').subscribe(() => {","[{'comment': ""Why to remove throttleTime? There are too many operations with DOM performing too often without it.\r\nLocally this subscription wasn't working for me with throttleTime, but adding its missing import fixed that."", 'commenter': 'aBabiichuk'}]"
1179,ambari-server/src/main/java/org/apache/ambari/server/orm/dao/RequestDAO.java,"@@ -447,11 +432,55 @@ public long cleanup(TimeBasedCleanupPolicy policy) {
       affectedRows += cleanTableByIds(requestIds, ""requestIds"", ""Request"", policy.getToDateInMillis(),
               ""RequestEntity.removeByRequestIds"", RequestEntity.class);
 
+      return affectedRows;
     } catch (AmbariException e) {
       LOG.error(""Error while looking up cluster with name: {}"", policy.getClusterName(), e);
       throw new IllegalStateException(e);
     }
+  }
 
-    return affectedRows;
+  private Set<Long> findHostTaskIds(Set<Long> taskIds) {
+    final Set<Long> hostTaskIds = new HashSet<>();
+    final Set<Long> partialTaskIds = new HashSet<>();
+    taskIds.forEach(taskId -> {
+      if (partialTaskIds.size() <= BATCH_SIZE) {
+        partialTaskIds.add(taskId);
+      } else {
+        hostTaskIds.addAll(topologyLogicalTaskDAO.findHostTaskIdsByPhysicalTaskIds(partialTaskIds));","[{'comment': ""I don't think these lookups executed for the last (or only) batch.  Can you please test with only a handful of records, too?"", 'commenter': 'adoroszlai'}, {'comment': ""Sure; I'll let you know the result (and double check the code before running the test; you might be right)"", 'commenter': 'smolnar82'}, {'comment': 'Coded; it was a nice catch; thanks again!', 'commenter': 'smolnar82'}]"
1194,ambari-logsearch/ambari-logsearch-web/src/app/components/main-container/main-container.component.less,"@@ -17,5 +17,5 @@
  */
 
 :host {
-  overflow-x: hidden;
+  ","[{'comment': ""This file (and reference to it in the component decorator as well) isn't needed anymore"", 'commenter': 'aBabiichuk'}]"
1212,ambari-server/src/main/resources/stacks/HDP/2.5/services/HIVE/kerberos.json,"@@ -150,7 +150,7 @@
             {
               ""webhcat-site"": {
                 ""templeton.kerberos.secret"": ""secret"",
-                ""templeton.hive.properties"": ""hive.metastore.local=false,hive.metastore.uris=${clusterHostInfo/hive_metastore_host|each(thrift://%s:9083, \\\\,, \\s*\\,\\s*)},hive.metastore.sasl.enabled=true,hive.metastore.execute.setugi=true,hive.metastore.warehouse.dir=/apps/hive/warehouse,hive.exec.mode.local.auto=false,hive.metastore.kerberos.principal=hive/_HOST@${realm}""
+                ""templeton.hive.properties"": ""hive.metastore.local=false,hive.metastore.uris=${clusterHostInfo/hive_metastore_hosts|each(thrift://%s:9083, \\\\,, \\s*\\,\\s*)},hive.metastore.sasl.enabled=true,hive.metastore.execute.setugi=true,hive.metastore.warehouse.dir=/apps/hive/warehouse,hive.exec.mode.local.auto=false,hive.metastore.kerberos.principal=hive/_HOST@${realm}""","[{'comment': 'This value will need to be updated in the user-suppled Kerberos descriptor when upgrading to Ambari 2.7.  This needs to be done in the UpgradeCatalog270 class. See `org.apache.ambari.server.upgrade.UpgradeCatalog270#updateKerberosDescriptorArtifact`.', 'commenter': 'rlevas'}, {'comment': 'Done', 'commenter': 'miklosgergely'}]"
1212,ambari-server/src/main/java/org/apache/ambari/server/upgrade/UpgradeCatalog270.java,"@@ -1079,6 +1081,17 @@ private boolean updateWebHCatHostsInHadoopProxyuserHttpHostsForHive(KerberosDesc
             updated = true;
           }
         }
+        final KerberosConfigurationDescriptor webhcatSiteConfiguration = webhcatServer.getConfiguration(CONFIGURATION_WEBHCAT_SITE);","[{'comment': 'You might want to rename the container method - `updateWebHCatHostsInHadoopProxyuserHttpHostsForHive` - to something like `updateWebHCatHostsInKerberosDescriptors` or something like that.  Or move the new block of code to a new method.  The current way is not very intuitive. \r\n', 'commenter': 'rlevas'}, {'comment': 'Modified the function name, also added a short description of it in a comment.', 'commenter': 'miklosgergely'}, {'comment': 'thanks', 'commenter': 'rlevas'}]"
1221,ambari-server/src/main/java/org/apache/ambari/server/checks/CheckDescription.java,"@@ -369,7 +369,15 @@
       new ImmutableMap.Builder<String, String>()
         .put(AbstractCheckDescriptor.DEFAULT,
             ""The following services are included in the upgrade but the repository is missing their dependencies:\n%s"").build());
-  
+
+
+  public static CheckDescription ATLAS_MIGRATION_PROPERTY_CHECK = new CheckDescription(""ATLAS_MIGRATION_PROPERTY_CHECK"",","[{'comment': 'General comment: So the absence of the filename property or value set to ""/etc/atlas/conf"" indicates what GraphDB is used? Seems a bit contrived. Isn\'t there a better property to check?', 'commenter': 'swagle'}, {'comment': 'This property atlas.migration.data.filename value is source for exported atlas metadata json from 2.6 which is used to import when Atlas is upgraded from 2.6 to 3.0. Atlas application code internally use this property for importing the data from path provided by this property.', 'commenter': 'fimugdha'}, {'comment': 'Right, I am going by the Failure comment which indicates path setting used and then talks about what backend database needs to be used which to the uninitiated eye seems very weird. I would have expected a separate config indicating the Titan vs Janos DB.', 'commenter': 'swagle'}]"
1221,ambari-server/src/main/java/org/apache/ambari/server/checks/AtlasMigrationPropertyCheck.java,"@@ -0,0 +1,75 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.checks;
+
+import java.util.Set;
+
+import org.apache.ambari.server.AmbariException;
+import org.apache.ambari.server.controller.PrereqCheckRequest;
+import org.apache.ambari.server.state.stack.PrereqCheckStatus;
+import org.apache.ambari.server.state.stack.PrerequisiteCheck;
+import org.apache.commons.lang.StringUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.collect.Sets;
+import com.google.inject.Singleton;
+
+
+/**
+ * Atlas needs to migrate existing data from TitanDB to JanusGraph.
+ * To migrate existing data to JanusGraph the property atlas.migration.data.filename needs to be present in Atlas applicaton.properties.
+ */
+@Singleton
+@UpgradeCheck(group = UpgradeCheckGroup.INFORMATIONAL_WARNING)
+public class AtlasMigrationPropertyCheck extends AbstractCheckDescriptor {
+
+    private static final Logger LOG = LoggerFactory.getLogger(AtlasMigrationPropertyCheck.class);
+    private static final String serviceName = ""ATLAS"";
+
+    /**
+     * Default constructor
+     */
+    public AtlasMigrationPropertyCheck(){  super(CheckDescription.ATLAS_MIGRATION_PROPERTY_CHECK); }
+
+    /**
+     * {@inheritDoc}
+     */
+    public Set<String> getApplicableServices() { return Sets.newHashSet(serviceName); }
+
+    /**
+     * {@inheritDoc}
+     */
+    @Override
+    public void perform(PrerequisiteCheck prerequisiteCheck, PrereqCheckRequest request) throws AmbariException {
+      String atlasMigrationProperty = getProperty(request,""application-properties"",""atlas.migration.data.filename"");
+      if(null == atlasMigrationProperty || StringUtils.isEmpty(atlasMigrationProperty.trim())) {","[{'comment': 'use StringUtils.isBlank() instead.', 'commenter': 'jonathan-hurley'}, {'comment': 'Updated in this [commit](https://github.com/apache/ambari/pull/1221/commits/8337e319a9ef7274519d0bb9f44d2dad1bfc087c).', 'commenter': 'fimugdha'}]"
1221,ambari-server/src/main/java/org/apache/ambari/server/checks/AtlasMigrationPropertyCheck.java,"@@ -0,0 +1,75 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.checks;
+
+import java.util.Set;
+
+import org.apache.ambari.server.AmbariException;
+import org.apache.ambari.server.controller.PrereqCheckRequest;
+import org.apache.ambari.server.state.stack.PrereqCheckStatus;
+import org.apache.ambari.server.state.stack.PrerequisiteCheck;
+import org.apache.commons.lang.StringUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.collect.Sets;
+import com.google.inject.Singleton;
+
+
+/**
+ * Atlas needs to migrate existing data from TitanDB to JanusGraph.
+ * To migrate existing data to JanusGraph the property atlas.migration.data.filename needs to be present in Atlas applicaton.properties.
+ */
+@Singleton
+@UpgradeCheck(group = UpgradeCheckGroup.INFORMATIONAL_WARNING)
+public class AtlasMigrationPropertyCheck extends AbstractCheckDescriptor {
+
+    private static final Logger LOG = LoggerFactory.getLogger(AtlasMigrationPropertyCheck.class);
+    private static final String serviceName = ""ATLAS"";
+
+    /**
+     * Default constructor
+     */
+    public AtlasMigrationPropertyCheck(){  super(CheckDescription.ATLAS_MIGRATION_PROPERTY_CHECK); }
+
+    /**
+     * {@inheritDoc}
+     */
+    public Set<String> getApplicableServices() { return Sets.newHashSet(serviceName); }
+
+    /**
+     * {@inheritDoc}
+     */
+    @Override
+    public void perform(PrerequisiteCheck prerequisiteCheck, PrereqCheckRequest request) throws AmbariException {
+      String atlasMigrationProperty = getProperty(request,""application-properties"",""atlas.migration.data.filename"");
+      if(null == atlasMigrationProperty || StringUtils.isEmpty(atlasMigrationProperty.trim())) {
+        LOG.info(""The property atlas.migration.data.filename is not found in application-properties, need to add the property before upgrade."");
+        prerequisiteCheck.getFailedOn().add(serviceName);
+        prerequisiteCheck.setStatus(PrereqCheckStatus.FAIL);
+        prerequisiteCheck.setFailReason(getFailReason(prerequisiteCheck, request));
+      } else if (atlasMigrationProperty.contains(""/etc/atlas/conf"")) {
+          LOG.info(""The property atlas.migration.data.filename is found in application-properties, but it contains with /etc/atlas/conf."");","[{'comment': 'This message is not very informative - can you log a better message that says what the problem is.', 'commenter': 'jonathan-hurley'}, {'comment': 'Updated in this [commit](https://github.com/apache/ambari/pull/1221/commits/8337e319a9ef7274519d0bb9f44d2dad1bfc087c).', 'commenter': 'fimugdha'}]"
1221,ambari-server/src/main/java/org/apache/ambari/server/checks/AtlasMigrationPropertyCheck.java,"@@ -0,0 +1,75 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.checks;
+
+import java.util.Set;
+
+import org.apache.ambari.server.AmbariException;
+import org.apache.ambari.server.controller.PrereqCheckRequest;
+import org.apache.ambari.server.state.stack.PrereqCheckStatus;
+import org.apache.ambari.server.state.stack.PrerequisiteCheck;
+import org.apache.commons.lang.StringUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.collect.Sets;
+import com.google.inject.Singleton;
+
+
+/**
+ * Atlas needs to migrate existing data from TitanDB to JanusGraph.
+ * To migrate existing data to JanusGraph the property atlas.migration.data.filename needs to be present in Atlas applicaton.properties.
+ */
+@Singleton
+@UpgradeCheck(group = UpgradeCheckGroup.INFORMATIONAL_WARNING)
+public class AtlasMigrationPropertyCheck extends AbstractCheckDescriptor {
+
+    private static final Logger LOG = LoggerFactory.getLogger(AtlasMigrationPropertyCheck.class);
+    private static final String serviceName = ""ATLAS"";
+
+    /**
+     * Default constructor
+     */
+    public AtlasMigrationPropertyCheck(){  super(CheckDescription.ATLAS_MIGRATION_PROPERTY_CHECK); }
+
+    /**
+     * {@inheritDoc}
+     */
+    public Set<String> getApplicableServices() { return Sets.newHashSet(serviceName); }
+
+    /**
+     * {@inheritDoc}
+     */
+    @Override
+    public void perform(PrerequisiteCheck prerequisiteCheck, PrereqCheckRequest request) throws AmbariException {
+      String atlasMigrationProperty = getProperty(request,""application-properties"",""atlas.migration.data.filename"");
+      if(null == atlasMigrationProperty || StringUtils.isEmpty(atlasMigrationProperty.trim())) {
+        LOG.info(""The property atlas.migration.data.filename is not found in application-properties, need to add the property before upgrade."");
+        prerequisiteCheck.getFailedOn().add(serviceName);
+        prerequisiteCheck.setStatus(PrereqCheckStatus.FAIL);
+        prerequisiteCheck.setFailReason(getFailReason(prerequisiteCheck, request));
+      } else if (atlasMigrationProperty.contains(""/etc/atlas/conf"")) {
+          LOG.info(""The property atlas.migration.data.filename is found in application-properties, but it contains with /etc/atlas/conf."");
+          prerequisiteCheck.getFailedOn().add(serviceName);
+          prerequisiteCheck.setStatus(PrereqCheckStatus.WARNING);
+          prerequisiteCheck.setFailReason(getFailReason(prerequisiteCheck, request));
+      } else {
+        LOG.info(""The property atlas.migration.data.filename is found, proceeding with upgrade."");","[{'comment': ""You can ditch this whole else-statement - it's not needed."", 'commenter': 'jonathan-hurley'}, {'comment': 'Updated in this [commit](https://github.com/apache/ambari/pull/1221/commits/8337e319a9ef7274519d0bb9f44d2dad1bfc087c).', 'commenter': 'fimugdha'}]"
1221,ambari-server/src/main/java/org/apache/ambari/server/checks/AtlasMigrationPropertyCheck.java,"@@ -58,18 +58,16 @@
     @Override
     public void perform(PrerequisiteCheck prerequisiteCheck, PrereqCheckRequest request) throws AmbariException {
       String atlasMigrationProperty = getProperty(request,""application-properties"",""atlas.migration.data.filename"");
-      if(null == atlasMigrationProperty || StringUtils.isEmpty(atlasMigrationProperty.trim())) {
+      if(null == atlasMigrationProperty || StringUtils.isBlank(atlasMigrationProperty.trim())) {","[{'comment': 'No need for the trim() or the null check when using isBlank()', 'commenter': 'jonathan-hurley'}, {'comment': 'Updated in this [commit](https://github.com/apache/ambari/pull/1221/commits/16ae709b40081b5c5e4fb815093979ba6bd2f531).', 'commenter': 'fimugdha'}]"
1222,ambari-logsearch/ambari-logsearch-web/src/app/classes/components/graph/time-graph.component.ts,"@@ -164,7 +171,10 @@ export class TimeGraphComponent extends GraphComponent implements OnInit {
    * @param {Date} endDate
    */
   protected setChartTimeGap(startDate: Date, endDate: Date): void {
-    this.chartTimeGap = this.getTimeGap(startDate, endDate);
+    const gap: ChartTimeGap = this.getTimeGap(startDate, endDate);
+    if (gap.value > 0) {
+      this.chartTimeGap = this.getTimeGap(startDate, endDate);","[{'comment': 'why not just `this.chartTimeGap = gap`?', 'commenter': 'aBabiichuk'}]"
1276,ambari-common/src/main/python/resource_management/libraries/functions/solr_cloud_util.py,"@@ -117,6 +120,10 @@ def create_collection(zookeeper_quorum, solr_znode, collection, config_set, java
   appendableDict[""--key-store-location""] = key_store_location
   appendableDict[""--key-store-password""] = None if key_store_password is None else '{key_store_password_param!p}'
   appendableDict[""--key-store-type""] = key_store_type
+  if default('configurations/infra-solr-env/infra_solr_ssl_enabled', False):","[{'comment': 'it would only work if solr server or solr truststore is on that host where solr_cloud_util.py runs. the idea with that tool is that you can use from hosts where atlas or ranger is located.\r\ntherefore it would be better if ranger / atlas would pass their truststore data through the create_collections method. (so its expected for the ranger / atlas truststore should contain solr certs)\r\n\r\nfor atlas:\r\nhttps://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/common-services/ATLAS/0.1.0.2.3/package/scripts/metadata.py#L241\r\n\r\nfor ranger:\r\nhttps://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/common-services/RANGER/0.4.0/package/scripts/setup_ranger_xml.py#L747', 'commenter': 'oleewere'}]"
1276,ambari-server/src/main/resources/common-services/ATLAS/0.1.0.2.3/package/scripts/metadata.py,"@@ -149,9 +151,26 @@ def metadata(type='server'):
                                        roles = [params.infra_solr_role_atlas, params.infra_solr_role_ranger_audit, params.infra_solr_role_dev],
                                        new_service_principals = [params.atlas_jaas_principal])
 
-      create_collection('vertex_index', 'atlas_configs', jaasFile)
-      create_collection('edge_index', 'atlas_configs', jaasFile)
-      create_collection('fulltext_index', 'atlas_configs', jaasFile)
+      if default('configurations/infra-solr-env/infra_solr_ssl_enabled', False):","[{'comment': 'i think ranger can run this command against external Solr as well, so maybe you can try out to just pass these truststore fields anyway (if they are not empty) and test that it cause any problem if solr is not using ssl (i guess it wont cause problem but lets check it)', 'commenter': 'oleewere'}, {'comment': '  private static final String TRUSTSTORE_LOCATION_ARG = ""javax.net.ssl.trustStore"";\r\n  private static final String TRUSTSTORE_PASSWORD_ARG = ""javax.net.ssl.trustStorePassword"";\r\n  private static final String TRUSTSTORE_TYPE_ARG = ""javax.net.ssl.trustStoreType"";\r\n\r\nThese properties map to the java ssl connection client. If they are passed and there is no SSL connection happening it wont hurt anyways. \r\n\r\nI guess we can adjust it so it does it for both external and internal, or have it done by default if the property exists ', 'commenter': 'amerissa'}, {'comment': 'Yes this will be needed to handle externally managed solr for Ranger Service', 'commenter': 'fimugdha'}, {'comment': 'I have changed it to supply the truststore information if they are defined regardless of the status of solr SSL/nonSSL Internal/External ', 'commenter': 'amerissa'}]"
1276,ambari-server/src/main/resources/common-services/RANGER/0.4.0/package/scripts/setup_ranger_xml.py,"@@ -741,8 +741,14 @@ def setup_ranger_audit_solr():
       solr_cloud_util.add_solr_roles(params.config,
                                      roles = [params.infra_solr_role_ranger_audit, params.infra_solr_role_dev],
                                      new_service_principals = service_principals)
-
-
+    if default('configurations/infra-solr-env/infra_solr_ssl_enabled', False) and not params.is_external_solrCloud_enabled:
+        solrtruststoretype = ""JKS""","[{'comment': 'Its ok to keep this hard-coded. Ranger does not have property to specify the type of truststore and the ranger-code base also expect it to be jks. May be in future need to update the hard-coded type.', 'commenter': 'fimugdha'}, {'comment': 'Exactly, Ranger and Atlas only support JKS for now ', 'commenter': 'amerissa'}]"
1276,ambari-server/src/main/resources/common-services/ATLAS/0.1.0.2.3/package/scripts/metadata.py,"@@ -149,9 +151,18 @@ def metadata(type='server'):
                                        roles = [params.infra_solr_role_atlas, params.infra_solr_role_ranger_audit, params.infra_solr_role_dev],
                                        new_service_principals = [params.atlas_jaas_principal])
 
-      create_collection('vertex_index', 'atlas_configs', jaasFile)
-      create_collection('edge_index', 'atlas_configs', jaasFile)
-      create_collection('fulltext_index', 'atlas_configs', jaasFile)
+      create_collection('vertex_index', 'atlas_configs', jaasFile,
+            trust_store_password =  default('configurations/ranger-atlas-policymgr-ssl/xasecure.policymgr.clientssl.truststore.password', None),","[{'comment': 'Why to use ranger-plugin related trustore properties ?\r\n\r\nadding @vishalsuvagia for review here.', 'commenter': 'fimugdha'}, {'comment': 'The plugin will need that information anyways to contact Solr later in to populate the audit entries\r\n', 'commenter': 'amerissa'}, {'comment': 'That ok, but need to use atlas-side property here. What if atlas is not using ranger-plugin.', 'commenter': 'fimugdha'}, {'comment': '@amerissa , @fimugdha seems right, in order to create a collection which would be used by Atlas Server, truststore setup for Ranger Atlas plugin should not be passed / used. The collection can even be in use by Atlas Server when Ranger Atlas plugin will not be enabled.', 'commenter': 'vishalsuvagia'}, {'comment': 'Makes sense. Atlas does have a truststore property truststore.file; however, it only uses jceks to store the password for it. Do you guys know a way in python to extract the password from JCEKS?', 'commenter': 'amerissa'}]"
1276,ambari-server/src/main/resources/common-services/ATLAS/0.1.0.2.3/package/scripts/params.py,"@@ -421,3 +429,10 @@ def configs_for_ha(atlas_hosts, metadata_port, is_atlas_ha_enabled, metadata_pro
 # atlas admin login username password
 atlas_admin_username = config['configurations']['atlas-env']['atlas.admin.username']
 atlas_admin_password = config['configurations']['atlas-env']['atlas.admin.password']
+
+# Atlas Passwords Extracted From Credential Store
+if credential_provider:
+    default_credential_shell_lib_path = '/var/lib/ambari-agent/cred/lib'
+    truststore_password = PasswordString(get_password_from_credential_store('truststore.password', credential_provider, os.path.join(default_credential_shell_lib_path, '*'), java64_home, default_credential_shell_lib_path))","[{'comment': 'Last parameter for get_password_from_credential_store should be jdk_location, ie jdk_location = config[\'ambariLevelParams\'][\'jdk_location\']\r\n\r\nWhat if the jar is not available in ""/var/lib/ambari-agent/cred/lib"" ? It can download from jdk_location.\r\n\r\nAs the alias is hard-coded .... need to document to use this alias, other-wise the password won\'t be available.\r\n', 'commenter': 'fimugdha'}, {'comment': ""https://github.com/apache/ambari/blob/906196b2568f21eb0ad18cc51f71f9c3006627bf/ambari-common/src/main/python/ambari_commons/credential_store_helper.py#L34\r\n\r\nThe function will autodownload to that folder. The JDK location in other scripts that use this point to this folder. When I was doing my testing, I was getting an error when it tried to get jdk_location = config['ambariLevelParams']['jdk_location']\r\n\r\nIf it works and it is just my testing environment I can remove the hardcoding and put it at as the jdk_location "", 'commenter': 'amerissa'}, {'comment': 'Please use jdk_location.\r\nCurrently the code is downloading and writing it in the same path ""/var/lib/ambari-agent/cred/lib"".\r\nIf in case the jar file is not available then it can download from jdk_location.', 'commenter': 'fimugdha'}, {'comment': 'Updated ', 'commenter': 'amerissa'}]"
1285,ambari-server/src/main/java/org/apache/ambari/server/state/host/HostImpl.java,"@@ -1256,14 +1256,14 @@ public void calculateHostStatus(Long clusterId) throws AmbariException {
       String category = componentInfo.getCategory();","[{'comment': 'Should there be some sort of logic to handle the case where `category` is `null`.... or is ignoring such components in this calculation ok?\r\n\r\n', 'commenter': 'rlevas'}, {'comment': 'Is logging as an error or warning would be enough?', 'commenter': 'kasakrisz'}, {'comment': 'A logged message would be nice.  However do we know what the effect is if a component is not accounted for due to a missing category.  Maybe this is a real show stopper and some exception should be thrown.  \r\n\r\n@mpapirkovskyy can you comment on this?', 'commenter': 'rlevas'}, {'comment': 'Added logging when category is null. Please review @rlevas', 'commenter': 'kasakrisz'}, {'comment': ""@rlevas I'm not sure if I can help here.\r\nI don't even get what null category means, since it looks like we never had such components before.\r\n\r\n@jonathan-hurley Since Java 7 theres Objects.equals(), you no longer need external lib for safe and easy check."", 'commenter': 'mpapirkovskyy'}, {'comment': 'I guess null category means faulty service definition, which should be fixed, but the goal is to handle it more gracefully.', 'commenter': 'adoroszlai'}, {'comment': ""Oh sure - Objects.equals() is fine too - I'm just used to StringUtils :)"", 'commenter': 'jonathan-hurley'}]"
1291,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/BlueprintConfigurationProcessor.java,"@@ -499,6 +499,27 @@ private void doNameNodeHAUpdateOnClusterCreation(Configuration clusterConfig,
         if (parsedNameServices.length > 1) {
           Set<String> activeNameNodeHostnames = new HashSet<>();
           Set<String> standbyNameNodeHostnames = new HashSet<>();
+          String tagsyncNameserviceMappingProperty   = ""ranger.tagsync.atlas.hdfs.instance."" + getClusterName() + "".%s.ranger.service"";","[{'comment': 'Will this configuration support be required for traditional HDFS NameNode HA as well, meaning a single HDFS nameservice defined?  If so, then this code to generate the properties for Ranger should probably also execute in the above block of code.  If this property generation should only occur for Federated deployments, then this block is in the right place. \r\n', 'commenter': 'rnettleton'}, {'comment': '@rnettleton, The properties will be required only for federation scenarios.\r\n\r\nThe service-advisor case will not be feasible here as we require the cluster-name for configuring the required properties. When the blueprint is deployed, the cluster-name will not be available in service-advisor to configure the required properties, so the deployment might fail.', 'commenter': 'vishalsuvagia'}, {'comment': 'Please store `clusterName` in a local variable and use it instead of multiple `getClusterName()` calls.', 'commenter': 'adoroszlai'}]"
1291,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/BlueprintConfigurationProcessor.java,"@@ -499,6 +499,27 @@ private void doNameNodeHAUpdateOnClusterCreation(Configuration clusterConfig,
         if (parsedNameServices.length > 1) {
           Set<String> activeNameNodeHostnames = new HashSet<>();
           Set<String> standbyNameNodeHostnames = new HashSet<>();
+          String tagsyncNameserviceMappingProperty   = ""ranger.tagsync.atlas.hdfs.instance."" + getClusterName() + "".%s.ranger.service"";
+
+          Map<String,String> rangerTagsyncPropertiesMap = clusterConfig.getFullProperties().get(""ranger-tagsync-site"");
+          Map<String, String> rangerHDFSPluginProperties = clusterConfig.getFullProperties().get(""ranger-hdfs-plugin-properties"");
+
+          String rangerHDFSPluginEnabledValue = rangerHDFSPluginProperties.getOrDefault(""ranger-hdfs-plugin-enabled"",""No"");
+          boolean isRangerHDFSPluginEnabled = (""yes"".equalsIgnoreCase(rangerHDFSPluginEnabledValue));
+
+          Map<String, String> rangerHDFSSecurityConfig = clusterConfig.getFullProperties().get(""ranger-hdfs-security"");","[{'comment': ""Please avoid calling `clusterConfig.getFullProperties()` repeatedly.  It's relatively expensive, since it creates a deep copy.  Use `clusterProps` instead."", 'commenter': 'adoroszlai'}]"
1291,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/BlueprintConfigurationProcessor.java,"@@ -525,9 +546,25 @@ private void doNameNodeHAUpdateOnClusterCreation(Configuration clusterConfig,
               // since HA is assumed, there should only be two NameNodes deployed per NameService
               activeNameNodeHostnames.add(hostNames.get(0));
               standbyNameNodeHostnames.add(hostNames.get(1));
+              if (isTagsyncPropertyConfigurationRequired && isRangerHDFSPluginEnabled) {
+                tagsyncNameserviceMappingProperty = String.format(tagsyncNameserviceMappingProperty, nameService);
+                String updatedRangerHDFSPluginServiceName = rangerHDFSPluginServiceName + ""_"" + nameService;
+                //rangerTagsyncPropertiesMap.put(tagsyncNameserviceMappingProperty, updatedRangerHDFSPluginServiceName);
+                clusterConfig.setProperty(""ranger-tagsync-site"", tagsyncNameserviceMappingProperty, updatedRangerHDFSPluginServiceName);
+                if (fsDefaultFSValue.contains(nameService)) {","[{'comment': 'I think this check is too permissive.  Consider name services called `X` and `XY`, with defaultFS set to `.../XY`.', 'commenter': 'adoroszlai'}]"
1291,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/BlueprintConfigurationProcessor.java,"@@ -499,6 +499,27 @@ private void doNameNodeHAUpdateOnClusterCreation(Configuration clusterConfig,
         if (parsedNameServices.length > 1) {
           Set<String> activeNameNodeHostnames = new HashSet<>();
           Set<String> standbyNameNodeHostnames = new HashSet<>();
+          String tagsyncNameserviceMappingProperty   = ""ranger.tagsync.atlas.hdfs.instance."" + getClusterName() + "".%s.ranger.service"";
+
+          Map<String,String> rangerTagsyncPropertiesMap = clusterConfig.getFullProperties().get(""ranger-tagsync-site"");
+          Map<String, String> rangerHDFSPluginProperties = clusterConfig.getFullProperties().get(""ranger-hdfs-plugin-properties"");
+
+          String rangerHDFSPluginEnabledValue = rangerHDFSPluginProperties.getOrDefault(""ranger-hdfs-plugin-enabled"",""No"");
+          boolean isRangerHDFSPluginEnabled = (""yes"".equalsIgnoreCase(rangerHDFSPluginEnabledValue));
+
+          Map<String, String> rangerHDFSSecurityConfig = clusterConfig.getFullProperties().get(""ranger-hdfs-security"");
+          String rangerHDFSPluginServiceName = rangerHDFSSecurityConfig.get(""ranger.plugin.hdfs.service.name"");
+          boolean isTagsyncPropertyConfigurationRequired = (clusterTopology.getBlueprint().getServices().contains(""RANGER_ADMIN"") &&
+                                                       clusterTopology.getBlueprint().getServices().contains(""RANGER_TAGSYNC"") &&
+                                                       clusterTopology.getBlueprint().getServices().contains(""ATLAS""));","[{'comment': 'Please use local variable for services.', 'commenter': 'adoroszlai'}]"
1291,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/BlueprintConfigurationProcessor.java,"@@ -525,9 +546,25 @@ private void doNameNodeHAUpdateOnClusterCreation(Configuration clusterConfig,
               // since HA is assumed, there should only be two NameNodes deployed per NameService
               activeNameNodeHostnames.add(hostNames.get(0));
               standbyNameNodeHostnames.add(hostNames.get(1));
+              if (isTagsyncPropertyConfigurationRequired && isRangerHDFSPluginEnabled) {
+                tagsyncNameserviceMappingProperty = String.format(tagsyncNameserviceMappingProperty, nameService);
+                String updatedRangerHDFSPluginServiceName = rangerHDFSPluginServiceName + ""_"" + nameService;
+                //rangerTagsyncPropertiesMap.put(tagsyncNameserviceMappingProperty, updatedRangerHDFSPluginServiceName);
+                clusterConfig.setProperty(""ranger-tagsync-site"", tagsyncNameserviceMappingProperty, updatedRangerHDFSPluginServiceName);
+                if (fsDefaultFSValue.contains(nameService)) {
+                  nameServiceInFsDefaultFSConfig = nameService;
+                }
+              }
             }
           }
 
+          if(isTagsyncPropertyConfigurationRequired && isRangerHDFSPluginEnabled) {
+            String rangerTagsyncAtlasNNServiceMappingProperty = ""ranger.tagsync.atlas.hdfs.instance.""+getClusterName() + "".ranger.service"";
+            String rangerTagsyncAtlasNNServiceName = rangerHDFSPluginServiceName + nameServiceInFsDefaultFSConfig;
+            clusterConfig.setProperty(""ranger-tagsync-site"", rangerTagsyncAtlasNNServiceMappingProperty, rangerTagsyncAtlasNNServiceName);
+            configTypesUpdated.add(""ranger-tagsync-site"");","[{'comment': 'I think `""ranger-tagsync-site""` should have its constant.', 'commenter': 'adoroszlai'}]"
1291,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/BlueprintConfigurationProcessor.java,"@@ -499,6 +502,42 @@ private void doNameNodeHAUpdateOnClusterCreation(Configuration clusterConfig,
         if (parsedNameServices.length > 1) {
           Set<String> activeNameNodeHostnames = new HashSet<>();
           Set<String> standbyNameNodeHostnames = new HashSet<>();
+          String clusterName = getClusterName();","[{'comment': 'For clarity, I think setting ranger stuff should have its own method.', 'commenter': 'benyoka'}]"
1291,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/BlueprintConfigurationProcessor.java,"@@ -499,6 +502,42 @@ private void doNameNodeHAUpdateOnClusterCreation(Configuration clusterConfig,
         if (parsedNameServices.length > 1) {
           Set<String> activeNameNodeHostnames = new HashSet<>();
           Set<String> standbyNameNodeHostnames = new HashSet<>();
+          String clusterName = getClusterName();
+","[{'comment': ""I agree with @benyoka's comment above:  I would recommend factoring out the Ranger-specific code into a set of convenience methods that are called from the update methods.  "", 'commenter': 'rnettleton'}]"
1291,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/BlueprintConfigurationProcessor.java,"@@ -499,6 +502,42 @@ private void doNameNodeHAUpdateOnClusterCreation(Configuration clusterConfig,
         if (parsedNameServices.length > 1) {
           Set<String> activeNameNodeHostnames = new HashSet<>();
           Set<String> standbyNameNodeHostnames = new HashSet<>();
+          String clusterName = getClusterName();
+
+          // Getting configuration and properties for adding configurations to ranger-tagsync-site
+          Map<String, String> rangerHDFSPluginProperties = null;
+          boolean isRangerHDFSPluginEnabled =  false;
+          Map<String, String> rangerHDFSSecurityConfig = null;","[{'comment': ""These variables like Map<String, String> rangerHDFSSecurityConfig = null are only used in a single place in an if-branch. I'd prefer creating them locally (at the same place where it is used) to minimize the scope of the variable."", 'commenter': 'benyoka'}]"
1291,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/BlueprintConfigurationProcessor.java,"@@ -542,11 +545,80 @@ private void doNameNodeHAUpdateOnClusterCreation(Configuration clusterConfig,
           } else {
             LOG.warn(""Error in processing the set of active/standby namenodes in this federated cluster, please check hdfs-site configuration"");
           }
+          // Need to configure ranger-tagsync-site properties, when Ranger-HDFS plugin is enabled
+          doTagSyncSiteUpdateForNamenodeNFederationEnabledOnClusterCreation(clusterConfig, clusterProps, configTypesUpdated);
         }
       }
     }
   }
 
+  /**
+   * Update ranger-tagsync-site properties when NN Federation is enabled and Ranger-HDFS plugin is enabled
+   * @throws ConfigurationTopologyException
+   */
+  private void doTagSyncSiteUpdateForNamenodeNFederationEnabledOnClusterCreation(Configuration clusterConfig,
+                                                                    Map<String, Map<String, String>> clusterProps,
+                                                                    Set<String> configTypesUpdated) throws ConfigurationTopologyException {
+    Map<String, String> hdfsSiteConfig = clusterConfig.getFullProperties().get(""hdfs-site"");
+    // parse out the nameservices value
+    String[] parsedNameServices = parseNameServices(hdfsSiteConfig);
+    String clusterName = getClusterName();
+
+    // Getting configuration and properties for adding configurations to ranger-tagsync-site
+    Map<String, String> rangerHDFSPluginProperties = null;
+    boolean isRangerHDFSPluginEnabled =  false;
+    Map<String, String> rangerHDFSSecurityConfig = null;
+    String rangerHDFSPluginServiceName = """";","[{'comment': 'Please declare rangerHDFSPluginProperties and rangerHDFSSecurityConfig with smaller scope (at the place they are used)', 'commenter': 'benyoka'}]"
1304,ambari-common/src/main/python/resource_management/libraries/script/script.py,"@@ -776,9 +776,20 @@ def load_available_packages(self):
     if self.available_packages_in_repos:
       return self.available_packages_in_repos
 
+    config = self.get_config()
+
+    service_name = config['serviceName'] if 'serviceName' in config else None
+    repos = CommandRepository(config['repositoryFile'])
+    repo_ids = [repo.repo_id for repo in repos.items]
+    Logger.info(""Command repositories: {0}"".format("", "".join(repo_ids)))
+    repos.items = [x for x in repos.items if (not x.applicable_services or service_name in x.applicable_services) ]
+    applicable_repo_ids = [repo.repo_id for repo in repos.items]
+    Logger.info(""Applicable repositories: {0}"".format("", "".join(applicable_repo_ids)))","[{'comment': 'What are these lines buying you?  ApplicableServices should be used only on the java side to determine what packages to use for installation.', 'commenter': 'ncole'}]"
1304,ambari-server/src/main/java/org/apache/ambari/server/agent/CommandRepository.java,"@@ -265,6 +266,11 @@ public void setPreInstalled(String isPreInstalled) {
     @JsonProperty(""mirrorsList"")
     private String m_mirrorsList;
 
+    @SerializedName(""applicableServices"")
+    @Experimental(feature = ExperimentalFeature.CUSTOM_SERVICE_REPOS,
+      comment = ""Remove logic for handling custom service repos after enabling multi-mpack cluster deployment"")
+    private List<String> m_applicableServices;","[{'comment': ""The agent shouldn't need this information"", 'commenter': 'ncole'}]"
1323,utility/pom.xml,"@@ -58,6 +58,26 @@
 
   <build>
     <plugins>
+        <groupId>org.vafer</groupId>","[{'comment': ""`<plugin>` is missing here, causing:\r\n\r\n```\r\n[FATAL] Non-parseable POM utility/pom.xml: Unrecognised tag: 'groupId' (position: START_TAG seen ...<plugins>\\n        <groupId>... @61:18)  @ line 61, column 18\r\n```"", 'commenter': 'adoroszlai'}, {'comment': 'My apologies for late reply.  Sure, I will make the necessary changes and give a single pull request.  Thank you very much for suggestions.', 'commenter': 'nareshgbhat'}]"
1337,ambari-logsearch/ambari-logsearch-web/src/app/services/auth.service.ts,"@@ -75,30 +76,48 @@ export class AuthService {
    * @param {string} password
    * @returns {Observable<Response>}
    */
-  login(username: string, password: string): Observable<Response> {
+  login(username: string, password: string): Observable<Boolean> {
     this.setLoginInProgressAppState(true);
     const response$ = this.httpClient.postFormData('login', {
       username: username,
       password: password
-    });
+    }).share();","[{'comment': ""Would be better to add `share()` globally for all requests (`request()` method of `HttpClientService`). It used to be implemented in such a way some time ago. Any reason why this attempt doesn't fit? `share()` was removed in scope of [AMBARI-23514](https://github.com/apache/ambari/commit/4f93c3c793da419b684a56b6e282947d7b974316#diff-73ad1b6cd007f2d3b4be75ff1c84e5d3R163)"", 'commenter': 'aBabiichuk'}]"
1337,ambari-logsearch/ambari-logsearch-web/src/app/services/auth.service.ts,"@@ -75,30 +76,48 @@ export class AuthService {
    * @param {string} password
    * @returns {Observable<Response>}
    */
-  login(username: string, password: string): Observable<Response> {
+  login(username: string, password: string): Observable<Boolean> {
     this.setLoginInProgressAppState(true);
     const response$ = this.httpClient.postFormData('login', {
       username: username,
       password: password
-    });
+    }).share();
     response$.subscribe(
       (resp: Response) => this.onLoginResponse(resp),
       (resp: Response) => this.onLoginError(resp)
     );
-    return response$;
+    return response$.switchMap((resp: Response) => {
+      return Observable.create((observer: Observer<boolean>) => {
+        if (resp.ok) {
+          observer.next(resp.ok);
+        } else {
+          observer.error(resp);
+        }
+        observer.complete();
+      });
+    });
   }
 
   /**
    * The single unique entry point to request a logout action
    * @returns {Observable<boolean | Error>}
    */
-  logout(): Observable<Response> {
-    const response$ = this.httpClient.get('logout');
+  logout(): Observable<Boolean> {
+    const response$ = this.httpClient.get('logout').share();","[{'comment': 'See comment for line 84, the same here', 'commenter': 'aBabiichuk'}]"
1337,ambari-logsearch/ambari-logsearch-web/src/app/components/login-form/login-form.component.html,"@@ -16,7 +16,7 @@
 -->
 
 <div class=""login-form well col-md-4 col-md-offset-4 col-sm-offset-4"">
-  <div class=""alert alert-danger"" *ngIf=""isLoginAlertDisplayed"" [innerHTML]=""'authorization.error' | translate""></div>
+  <div class=""alert alert-danger"" *ngIf=""isLoginAlertDisplayed"" [innerHTML]=""errorMessage""></div>","[{'comment': 'No need to use `innerHTML` attribute since there are no more HTML tags in the message', 'commenter': 'aBabiichuk'}]"
1337,ambari-logsearch/ambari-logsearch-web/src/assets/i18n/en.json,"@@ -219,6 +219,8 @@
   ""logIndexFilter.update.error"": ""Error at updating Log Index Filter for cluster <span class='cluster-name'>{{cluster}}</span>. {{message}}"",
 
   ""login.title"": ""Login"",
+  ""login.error.title"": ""Login error"",","[{'comment': '`""authorization.error""` prop can be removed', 'commenter': 'aBabiichuk'}]"
1353,ambari-common/src/main/python/ambari_commons/repo_manager/yum_manager.py,"@@ -63,7 +63,7 @@ class YumManagerProperties(GenericManagerProperties):
   }
 
   verify_dependency_cmd = [repo_manager_bin, '-d', '0', '-e', '0', 'check', 'dependencies']
-  installed_package_version_command = [pkg_manager_bin, ""-q"", ""--queryformat"", ""%{{version}}-%{{release}}""]
+  installed_package_version_command = [pkg_manager_bin, ""-q"", ""--queryformat"", ""%{version}-%{release}\n""]","[{'comment': 'This fixes only yum manager. Can we check if this change is needed in other OS families also? ', 'commenter': 'avijayanhwx'}]"
1366,ambari-server/src/main/java/org/apache/ambari/server/agent/stomp/dto/MetadataCluster.java,"@@ -87,7 +87,16 @@ public boolean updateServiceLevelParams(SortedMap<String, MetadataServiceInfo> u
         break;
       }
     }
+
+    for (String key : serviceLevelParams.keySet()) {
+      if (!update.containsKey(key) || !update.get(key).equals(update.get(key))) {","[{'comment': 'There is a typo in this line: `update.get(key)` is compared to itself (`update.get(key)`) instead of `serviceLevelParams.get(key)`.\r\n\r\nFurther, I think this comparison can be removed: values for keys present in both maps are already compared in the first loop.', 'commenter': 'adoroszlai'}, {'comment': 'Hm...this is weird..I fixed this issue long before I created the PR (it also broker unit tests). Let me check my stash.', 'commenter': 'smolnar82'}, {'comment': 'Done', 'commenter': 'smolnar82'}]"
1366,ambari-server/src/main/java/org/apache/ambari/server/agent/stomp/dto/MetadataCluster.java,"@@ -101,7 +110,16 @@ public boolean updateClusterLevelParams(SortedMap<String, String> update) {
         break;
       }
     }
+
+    for (String key : clusterLevelParams.keySet()) {
+      if (!update.containsKey(key) || !StringUtils.equals(update.get(key), clusterLevelParams.get(key))) {
+        changed = true;
+        break;
+      }
+    }
+","[{'comment': 'This method is almost identical to `updateServiceLevelParams`.  (The duplication is not introduced in this change, but is highlighted by the fact that the same problem needs to be fixed in two places.)  Can you please extract a method that takes 2 maps and updates one of them from the other?  Then please delegate to that method from these two.  Please make sure to keep the version that uses `null`-safe comparison.', 'commenter': 'adoroszlai'}, {'comment': ""Good idea; I'll extract it to a util method (maybe an existing/new class; will check this out)"", 'commenter': 'smolnar82'}, {'comment': 'Done', 'commenter': 'smolnar82'}]"
1366,ambari-server/src/main/java/org/apache/ambari/server/agent/stomp/dto/MetadataCluster.java,"@@ -18,37 +18,41 @@
 package org.apache.ambari.server.agent.stomp.dto;
 
 import java.util.HashSet;
+import java.util.Map;
 import java.util.Objects;
 import java.util.Set;
 import java.util.SortedMap;
 import java.util.TreeMap;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
 
 import org.apache.ambari.server.state.SecurityType;
-import org.apache.commons.lang.StringUtils;
 
 import com.fasterxml.jackson.annotation.JsonInclude;
 import com.fasterxml.jackson.annotation.JsonProperty;
 
 @JsonInclude(JsonInclude.Include.NON_EMPTY)
 public class MetadataCluster {
-  @JsonProperty(""status_commands_to_run"")
-  private Set<String> statusCommandsToRun = new HashSet<>();
-  private SortedMap<String, MetadataServiceInfo> serviceLevelParams = new TreeMap<>();
-  private SortedMap<String, String> clusterLevelParams = new TreeMap<>();
+  private final static Lock LOCK = new ReentrantLock();","[{'comment': 'Any reason for static lock?\r\nThere are multiple instances of this object and all of them are going use single lock instance.\r\nIt looks like theres better place for lock, and again not for static one:\r\norg.apache.ambari.server.agent.stomp.MetadataHolder#handleUpdate \r\n\r\nAnother possibility is to use ConcurrentSkipListMap', 'commenter': 'mpapirkovskyy'}, {'comment': ""It makes sense to have the lock a simple one (i.e. not static). However I'd keep it here to make sure we lock only the update piece and do not spend more time on locking where it is not necessary."", 'commenter': 'smolnar82'}]"
1366,ambari-server/src/main/java/org/apache/ambari/server/agent/stomp/dto/MetadataCluster.java,"@@ -67,44 +71,31 @@ public static MetadataCluster emptyMetadataCluster() {
     return clusterLevelParams;
   }
 
-  public void setClusterLevelParams(SortedMap<String, String> clusterLevelParams) {
-    this.clusterLevelParams = clusterLevelParams;
-  }
-
   public SortedMap<String, SortedMap<String, String>> getAgentConfigs() {
     return agentConfigs;
   }
 
-  public void setAgentConfigs(SortedMap<String, SortedMap<String, String>> agentConfigs) {
-    this.agentConfigs = agentConfigs;
-  }
-
   public boolean updateServiceLevelParams(SortedMap<String, MetadataServiceInfo> update) {
-    boolean changed = false;
-    for (String key : update.keySet()) {
-      if (!serviceLevelParams.containsKey(key) || !serviceLevelParams.get(key).equals(update.get(key))) {
-        changed = true;
-        break;
-      }
-    }
-    if (changed) {
-      serviceLevelParams.putAll(update);
-    }
-    return changed;
+    return updateMapIfNeeded(serviceLevelParams, update);
   }
 
   public boolean updateClusterLevelParams(SortedMap<String, String> update) {
-    boolean changed = false;
-    for (String key : update.keySet()) {
-      if (!clusterLevelParams.containsKey(key) || !StringUtils.equals(clusterLevelParams.get(key), update.get(key))) {
-        changed = true;
-        break;
+    return updateMapIfNeeded(clusterLevelParams, update);
+  }
+
+  private <T> boolean updateMapIfNeeded(Map<String, T> mapToBeUpdated, Map<String, T> updatedMap) {
+    try {
+      LOCK.lock();
+      final boolean changed = !Objects.equals(mapToBeUpdated, updatedMap);
+
+      if (changed) {
+        mapToBeUpdated.clear();","[{'comment': 'I agree with @adoroszlai that this is going to break partial metadata updates.\r\nSo we should either always send full metadata which will require additional modifications.\r\nOr change logic here.', 'commenter': 'mpapirkovskyy'}, {'comment': ""I'll fix it soon"", 'commenter': 'smolnar82'}]"
1366,ambari-server/src/main/java/org/apache/ambari/server/agent/stomp/dto/MetadataCluster.java,"@@ -67,44 +82,58 @@ public static MetadataCluster emptyMetadataCluster() {
     return clusterLevelParams;
   }
 
-  public void setClusterLevelParams(SortedMap<String, String> clusterLevelParams) {
-    this.clusterLevelParams = clusterLevelParams;
-  }
-
   public SortedMap<String, SortedMap<String, String>> getAgentConfigs() {
     return agentConfigs;
   }
 
-  public void setAgentConfigs(SortedMap<String, SortedMap<String, String>> agentConfigs) {
-    this.agentConfigs = agentConfigs;
+  public boolean isFullServiceLevelMetadata() {
+    return fullServiceLevelMetadata;
   }
 
-  public boolean updateServiceLevelParams(SortedMap<String, MetadataServiceInfo> update) {
-    boolean changed = false;
-    for (String key : update.keySet()) {
-      if (!serviceLevelParams.containsKey(key) || !serviceLevelParams.get(key).equals(update.get(key))) {
-        changed = true;
-        break;
+  public boolean updateServiceLevelParams(SortedMap<String, MetadataServiceInfo> update, boolean fullMetadataInUpdatedMap) {
+    if (update != null) {
+      if (this.serviceLevelParams == null) {
+        this.serviceLevelParams = new TreeMap<>();","[{'comment': 'I think these map creations should also be guarded by `lock`.', 'commenter': 'adoroszlai'}, {'comment': ""You are right (this was a last minute movement at certainly not thought over); it's fixed now."", 'commenter': 'smolnar82'}]"
1391,ambari-common/src/main/python/ambari_commons/credential_store_helper.py,"@@ -19,27 +19,66 @@
 """"""
 
 import os
+import re
 
-from resource_management.core.resources.system import File
+from resource_management.core.resources.system import File, Execute
 from resource_management.core.shell import checked_call
 from resource_management.core.source import DownloadSource
+from resource_management.core.utils import PasswordString
 
 credential_util_cmd = 'org.apache.ambari.server.credentialapi.CredentialUtil'
 credential_util_jar = 'CredentialUtil.jar'
 
-def get_password_from_credential_store(alias, provider_path, cs_lib_path, java_home, jdk_location):
+def removeloglines(lines):
+    regex = re.compile(r'^(([0-1][0-9])|([2][0-3])):([0-5][0-9])(:[0-5][0-9])[,]\d{1,3}')
+    cleanlines = filter(regex.search, lines)
+    return(cleanlines)
+
+def downloadjar(cs_lib_path, jdk_location):
     # Try to download CredentialUtil.jar from ambari-server resources
-    credential_util_dir = cs_lib_path.split('*')[0] # Remove the trailing '*'
+    credential_util_dir = cs_lib_path.split('*')[0].split("":"")[-1:][0] # Remove the trailing '*' and get the last directory if an entire path is passed","[{'comment': 'What\'s the difference between .split("":"")[-1:][0] and .split("":"")[-1] ?', 'commenter': 'zeroflag'}, {'comment': 'This is to account for the fact when the class path is multiple directories. Some services like Atlas require multiple directories to run the jar (webapps META-INF and the conf directory). \r\n\r\nThe change here specifically will find the last folder in the class path and download the CredentialAPI.jar to it. ', 'commenter': 'amerissa'}, {'comment': 'But if the intention was to find the last one then you can use [-1].\r\n\r\n```python\r\n>>> a = [1,2,3]\r\n>>> a[-1:][0]\r\n3\r\n>>> a[-1]\r\n3\r\n```', 'commenter': 'zeroflag'}, {'comment': 'You are right, This is more efficient. Would you like me to change it ? ', 'commenter': 'amerissa'}, {'comment': 'yes please', 'commenter': 'zeroflag'}]"
1391,ambari-common/src/main/python/ambari_commons/credential_store_helper.py,"@@ -36,7 +36,7 @@ def removeloglines(lines):
 
 def downloadjar(cs_lib_path, jdk_location):
     # Try to download CredentialUtil.jar from ambari-server resources
-    credential_util_dir = cs_lib_path.split('*')[0].split("":"")[-1:][0] # Remove the trailing '*' and get the last directory if an entire path is passed
+    credential_util_dir = cs_lib_path.split('*')[0].split("":"")[1:] # Remove the trailing '*' and get the last directory if an entire path is passed","[{'comment': '[1:] returns a list with all elements but the first one. I thought we need the last element. \r\n\r\nThis will get the last element.\r\n\r\n```python\r\ncs_lib_path.split(\'*\')[0].split("":"")[-1]\r\n```', 'commenter': 'zeroflag'}]"
1396,ambari-admin/src/main/resources/ui/admin-web/app/views/userManagement/groupEdit.html,"@@ -34,7 +34,7 @@
     <div class=""form-group"">
       <label class=""col-sm-2"">{{group.groupTypeName | translate}} {{'groups.members' | translate}}</label>
       <div class=""col-sm-10"">
-        <editable-list items-source=""group.editingUsers"" resource-type=""User"" editable=""group.group_type == 'LOCAL'""></editable-list>
+        <editable-list items-source=""group.editingUsers"" resource-type=""User"" editable=""group.group_type == 'LOCAL' || group.group_type == 'LDAP'""></editable-list>","[{'comment': 'Are we sure that we want a user to be able to do this?  Any user changes will be potentially reverted when the an LDAP sync operation occurs. \r\n', 'commenter': 'rlevas'}]"
1405,ambari-server/src/main/java/org/apache/ambari/server/checks/AmbariMetricsHadoopSinkVersionCompatibilityCheck.java,"@@ -0,0 +1,215 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.checks;
+
+
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.LinkedHashMap;
+import java.util.LinkedHashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.stream.Collectors;
+
+import org.apache.ambari.server.AmbariException;
+import org.apache.ambari.server.actionmanager.HostRoleStatus;
+import org.apache.ambari.server.controller.AmbariManagementController;
+import org.apache.ambari.server.controller.AmbariServer;
+import org.apache.ambari.server.controller.PrereqCheckRequest;
+import org.apache.ambari.server.controller.internal.AbstractControllerResourceProvider;
+import org.apache.ambari.server.controller.internal.RequestResourceProvider;
+import org.apache.ambari.server.controller.spi.Request;
+import org.apache.ambari.server.controller.spi.Resource;
+import org.apache.ambari.server.controller.spi.ResourceProvider;
+import org.apache.ambari.server.controller.utilities.PropertyHelper;
+import org.apache.ambari.server.orm.dao.HostRoleCommandDAO;
+import org.apache.ambari.server.orm.dao.RequestDAO;
+import org.apache.ambari.server.orm.entities.HostRoleCommandEntity;
+import org.apache.ambari.server.orm.entities.RequestEntity;
+import org.apache.ambari.server.state.stack.PrereqCheckStatus;
+import org.apache.ambari.server.state.stack.PrerequisiteCheck;
+import org.apache.ambari.server.state.stack.upgrade.UpgradeType;
+import org.apache.commons.lang.StringUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.collect.Sets;
+import com.google.inject.Inject;
+import com.google.inject.Singleton;
+
+@Singleton
+@UpgradeCheck(
+  group = UpgradeCheckGroup.DEFAULT,
+  required = { UpgradeType.ROLLING, UpgradeType.NON_ROLLING })","[{'comment': ""I don't think this check should be required - if it is, then it runs for all kinds of upgrades. Instead, it should be added explicitly to the upgrade pack."", 'commenter': 'jonathan-hurley'}, {'comment': 'Will remove the annotation since the check will be included in the HDP 2.6 -> 3.0 upgrade pack. ', 'commenter': 'avijayanhwx'}]"
1405,ambari-server/src/main/java/org/apache/ambari/server/checks/AmbariMetricsHadoopSinkVersionCompatibilityCheck.java,"@@ -0,0 +1,215 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.checks;
+
+
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.LinkedHashMap;
+import java.util.LinkedHashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.stream.Collectors;
+
+import org.apache.ambari.server.AmbariException;
+import org.apache.ambari.server.actionmanager.HostRoleStatus;
+import org.apache.ambari.server.controller.AmbariManagementController;
+import org.apache.ambari.server.controller.AmbariServer;
+import org.apache.ambari.server.controller.PrereqCheckRequest;
+import org.apache.ambari.server.controller.internal.AbstractControllerResourceProvider;
+import org.apache.ambari.server.controller.internal.RequestResourceProvider;
+import org.apache.ambari.server.controller.spi.Request;
+import org.apache.ambari.server.controller.spi.Resource;
+import org.apache.ambari.server.controller.spi.ResourceProvider;
+import org.apache.ambari.server.controller.utilities.PropertyHelper;
+import org.apache.ambari.server.orm.dao.HostRoleCommandDAO;
+import org.apache.ambari.server.orm.dao.RequestDAO;
+import org.apache.ambari.server.orm.entities.HostRoleCommandEntity;
+import org.apache.ambari.server.orm.entities.RequestEntity;
+import org.apache.ambari.server.state.stack.PrereqCheckStatus;
+import org.apache.ambari.server.state.stack.PrerequisiteCheck;
+import org.apache.ambari.server.state.stack.upgrade.UpgradeType;
+import org.apache.commons.lang.StringUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.collect.Sets;
+import com.google.inject.Inject;
+import com.google.inject.Singleton;
+
+@Singleton
+@UpgradeCheck(
+  group = UpgradeCheckGroup.DEFAULT,
+  required = { UpgradeType.ROLLING, UpgradeType.NON_ROLLING })
+public class AmbariMetricsHadoopSinkVersionCompatibilityCheck extends AbstractCheckDescriptor  {","[{'comment': 'Actually, I think this upgrade check, as it is specifically for HDP And Metrics, should be included as part of the HDP 3.0 / Ambari 2.7 stack. It should not be in public Apache. ', 'commenter': 'jonathan-hurley'}, {'comment': 'If moving this into your own JAR is too cumbersome right now, then we should remove all hard coded versions/stack names from this and make it a more generic check.', 'commenter': 'jonathan-hurley'}, {'comment': 'Even though this is a check for AMS-hadoop sink version, it entirely relies on Ambari Server infrastructure to perform this check. Hence, I had to keep the class in ambari-server. ', 'commenter': 'avijayanhwx'}, {'comment': 'The version incompatibility between Hadoop metrics sink package exists in Apache Hadoop as well, so doing this check on upgrade so that system works reliably after the upgrade is critical.', 'commenter': 'swagle'}, {'comment': ""Fair enough - let's keep it in Apache, but make it generic. "", 'commenter': 'jonathan-hurley'}]"
1405,ambari-server/src/main/java/org/apache/ambari/server/checks/AmbariMetricsHadoopSinkVersionCompatibilityCheck.java,"@@ -0,0 +1,215 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.checks;
+
+
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.LinkedHashMap;
+import java.util.LinkedHashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.stream.Collectors;
+
+import org.apache.ambari.server.AmbariException;
+import org.apache.ambari.server.actionmanager.HostRoleStatus;
+import org.apache.ambari.server.controller.AmbariManagementController;
+import org.apache.ambari.server.controller.AmbariServer;
+import org.apache.ambari.server.controller.PrereqCheckRequest;
+import org.apache.ambari.server.controller.internal.AbstractControllerResourceProvider;
+import org.apache.ambari.server.controller.internal.RequestResourceProvider;
+import org.apache.ambari.server.controller.spi.Request;
+import org.apache.ambari.server.controller.spi.Resource;
+import org.apache.ambari.server.controller.spi.ResourceProvider;
+import org.apache.ambari.server.controller.utilities.PropertyHelper;
+import org.apache.ambari.server.orm.dao.HostRoleCommandDAO;
+import org.apache.ambari.server.orm.dao.RequestDAO;
+import org.apache.ambari.server.orm.entities.HostRoleCommandEntity;
+import org.apache.ambari.server.orm.entities.RequestEntity;
+import org.apache.ambari.server.state.stack.PrereqCheckStatus;
+import org.apache.ambari.server.state.stack.PrerequisiteCheck;
+import org.apache.ambari.server.state.stack.upgrade.UpgradeType;
+import org.apache.commons.lang.StringUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.collect.Sets;
+import com.google.inject.Inject;
+import com.google.inject.Singleton;
+
+@Singleton
+@UpgradeCheck(
+  group = UpgradeCheckGroup.DEFAULT,
+  required = { UpgradeType.ROLLING, UpgradeType.NON_ROLLING })
+public class AmbariMetricsHadoopSinkVersionCompatibilityCheck extends AbstractCheckDescriptor  {
+
+  @Inject
+  private RequestDAO requestDAO;
+
+  @Inject
+  private HostRoleCommandDAO hostRoleCommandDAO;
+
+  private static final Logger LOG = LoggerFactory.getLogger(AmbariMetricsHadoopSinkVersionCompatibilityCheck.class);
+
+  private enum PreUpgradeCheckStatus {SUCCESS, FAILED, RUNNING}
+
+  private final String FAILED_REASON = ""Hadoop Sink version check for 2.7.0.0 failed. "" +","[{'comment': 'You should make this a parameter should it can be re-used in future upgrade packs if needed.', 'commenter': 'jonathan-hurley'}]"
1405,ambari-server/src/main/java/org/apache/ambari/server/checks/AmbariMetricsHadoopSinkVersionCompatibilityCheck.java,"@@ -0,0 +1,215 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.checks;
+
+
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.LinkedHashMap;
+import java.util.LinkedHashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.stream.Collectors;
+
+import org.apache.ambari.server.AmbariException;
+import org.apache.ambari.server.actionmanager.HostRoleStatus;
+import org.apache.ambari.server.controller.AmbariManagementController;
+import org.apache.ambari.server.controller.AmbariServer;
+import org.apache.ambari.server.controller.PrereqCheckRequest;
+import org.apache.ambari.server.controller.internal.AbstractControllerResourceProvider;
+import org.apache.ambari.server.controller.internal.RequestResourceProvider;
+import org.apache.ambari.server.controller.spi.Request;
+import org.apache.ambari.server.controller.spi.Resource;
+import org.apache.ambari.server.controller.spi.ResourceProvider;
+import org.apache.ambari.server.controller.utilities.PropertyHelper;
+import org.apache.ambari.server.orm.dao.HostRoleCommandDAO;
+import org.apache.ambari.server.orm.dao.RequestDAO;
+import org.apache.ambari.server.orm.entities.HostRoleCommandEntity;
+import org.apache.ambari.server.orm.entities.RequestEntity;
+import org.apache.ambari.server.state.stack.PrereqCheckStatus;
+import org.apache.ambari.server.state.stack.PrerequisiteCheck;
+import org.apache.ambari.server.state.stack.upgrade.UpgradeType;
+import org.apache.commons.lang.StringUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.collect.Sets;
+import com.google.inject.Inject;
+import com.google.inject.Singleton;
+
+@Singleton
+@UpgradeCheck(
+  group = UpgradeCheckGroup.DEFAULT,
+  required = { UpgradeType.ROLLING, UpgradeType.NON_ROLLING })
+public class AmbariMetricsHadoopSinkVersionCompatibilityCheck extends AbstractCheckDescriptor  {
+
+  @Inject
+  private RequestDAO requestDAO;
+
+  @Inject
+  private HostRoleCommandDAO hostRoleCommandDAO;
+
+  private static final Logger LOG = LoggerFactory.getLogger(AmbariMetricsHadoopSinkVersionCompatibilityCheck.class);
+
+  private enum PreUpgradeCheckStatus {SUCCESS, FAILED, RUNNING}
+
+  private final String FAILED_REASON = ""Hadoop Sink version check for 2.7.0.0 failed. "" +
+    ""To fix this, please upgrade 'ambari-metrics-hadoop-sink' package to 2.7.0.0 on all the failed hosts"";
+
+  /**
+   * Total wait time for Ambari Server request time to finish => 2 mins.
+   */
+  private final long RETRY_INTERVAL = 6000l; // 6 seconds sleep interval per retry.
+  private final int NUM_TRIES = 20; // 20 times the check will try to see if request finished.","[{'comment': 'These have to be parameterized.', 'commenter': 'ncole'}, {'comment': 'Will do.', 'commenter': 'avijayanhwx'}]"
1405,ambari-server/src/main/java/org/apache/ambari/server/checks/AmbariMetricsHadoopSinkVersionCompatibilityCheck.java,"@@ -0,0 +1,215 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.checks;
+
+
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.LinkedHashMap;
+import java.util.LinkedHashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.stream.Collectors;
+
+import org.apache.ambari.server.AmbariException;
+import org.apache.ambari.server.actionmanager.HostRoleStatus;
+import org.apache.ambari.server.controller.AmbariManagementController;
+import org.apache.ambari.server.controller.AmbariServer;
+import org.apache.ambari.server.controller.PrereqCheckRequest;
+import org.apache.ambari.server.controller.internal.AbstractControllerResourceProvider;
+import org.apache.ambari.server.controller.internal.RequestResourceProvider;
+import org.apache.ambari.server.controller.spi.Request;
+import org.apache.ambari.server.controller.spi.Resource;
+import org.apache.ambari.server.controller.spi.ResourceProvider;
+import org.apache.ambari.server.controller.utilities.PropertyHelper;
+import org.apache.ambari.server.orm.dao.HostRoleCommandDAO;
+import org.apache.ambari.server.orm.dao.RequestDAO;
+import org.apache.ambari.server.orm.entities.HostRoleCommandEntity;
+import org.apache.ambari.server.orm.entities.RequestEntity;
+import org.apache.ambari.server.state.stack.PrereqCheckStatus;
+import org.apache.ambari.server.state.stack.PrerequisiteCheck;
+import org.apache.ambari.server.state.stack.upgrade.UpgradeType;
+import org.apache.commons.lang.StringUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.collect.Sets;
+import com.google.inject.Inject;
+import com.google.inject.Singleton;
+
+@Singleton
+@UpgradeCheck(
+  group = UpgradeCheckGroup.DEFAULT,
+  required = { UpgradeType.ROLLING, UpgradeType.NON_ROLLING })
+public class AmbariMetricsHadoopSinkVersionCompatibilityCheck extends AbstractCheckDescriptor  {
+
+  @Inject
+  private RequestDAO requestDAO;
+
+  @Inject
+  private HostRoleCommandDAO hostRoleCommandDAO;
+
+  private static final Logger LOG = LoggerFactory.getLogger(AmbariMetricsHadoopSinkVersionCompatibilityCheck.class);
+
+  private enum PreUpgradeCheckStatus {SUCCESS, FAILED, RUNNING}
+
+  private final String FAILED_REASON = ""Hadoop Sink version check for 2.7.0.0 failed. "" +
+    ""To fix this, please upgrade 'ambari-metrics-hadoop-sink' package to 2.7.0.0 on all the failed hosts"";
+
+  /**
+   * Total wait time for Ambari Server request time to finish => 2 mins.
+   */
+  private final long RETRY_INTERVAL = 6000l; // 6 seconds sleep interval per retry.
+  private final int NUM_TRIES = 20; // 20 times the check will try to see if request finished.
+
+  /**
+   * Constructor.
+   */
+  public AmbariMetricsHadoopSinkVersionCompatibilityCheck() {
+    super(CheckDescription.AMS_HADOOP_SINK_VERSION_COMPATIBILITY);
+  }
+
+  /**
+   * {@inheritDoc}
+   */
+  @Override
+  public Set<String> getApplicableServices() {
+    return Sets.newHashSet(""AMBARI_METRICS"");
+  }
+
+  @Override
+  public void perform(PrerequisiteCheck prerequisiteCheck, PrereqCheckRequest prereqCheckRequest) throws AmbariException {
+
+    AmbariManagementController ambariManagementController = AmbariServer.getController();
+
+    ResourceProvider provider = AbstractControllerResourceProvider.getResourceProvider(
+      Resource.Type.Request,
+      ambariManagementController
+      );
+
+    String clusterName = prereqCheckRequest.getClusterName();
+
+    Set<String> hosts = ambariManagementController.getClusters()
+      .getCluster(clusterName).getHosts(""AMBARI_METRICS"", ""METRICS_MONITOR"");
+
+    Set<Map<String, Object>> propertiesSet = new HashSet<>();
+    Map<String, Object> properties = new LinkedHashMap<>();
+    properties.put(RequestResourceProvider.REQUEST_CLUSTER_NAME_PROPERTY_ID, clusterName);
+
+    Set<Map<String, Object>> filterSet = new HashSet<>();
+    Map<String, Object> filterMap = new HashMap<>();
+    filterMap.put(RequestResourceProvider.SERVICE_ID, ""AMBARI_METRICS"");
+    filterMap.put(RequestResourceProvider.COMPONENT_ID, ""METRICS_MONITOR"");
+    filterMap.put(RequestResourceProvider.HOSTS_ID, StringUtils.join(hosts,"",""));
+    filterSet.add(filterMap);
+
+    properties.put(RequestResourceProvider.REQUEST_RESOURCE_FILTER_ID, filterSet);
+    propertiesSet.add(properties);
+
+    Map<String, String> requestInfoProperties = new HashMap<>();
+    requestInfoProperties.put(RequestResourceProvider.COMMAND_ID, ""CHECK_HADOOP_SINK_VERSION"");
+    requestInfoProperties.put(RequestResourceProvider.REQUEST_CONTEXT_ID, ""Pre Upgrade check for compatible Hadoop Metric "" +
+      ""Sink version on all hosts."");
+
+    Request request = PropertyHelper.getCreateRequest(propertiesSet, requestInfoProperties);
+    try {
+      org.apache.ambari.server.controller.spi.RequestStatus response = provider.createResources(request);
+      Resource responseResource = response.getRequestResource();
+      String requestIdProp = PropertyHelper.getPropertyId(""Requests"", ""id"");
+      long requestId = (long) responseResource.getPropertyValue(requestIdProp);
+      LOG.debug(""RequestId for AMS Hadoop Sink version compatibility pre check : "" + requestId);
+
+      Thread.sleep(RETRY_INTERVAL);
+      PreUpgradeCheckStatus status;
+      int retry = 0;
+      LinkedHashSet<String> failedHosts = new LinkedHashSet<>();
+      while ((status = pollRequestStatus(requestId, failedHosts)).equals(PreUpgradeCheckStatus.RUNNING)
+        && retry++ < NUM_TRIES) {
+        if (retry != NUM_TRIES) {
+          Thread.sleep(RETRY_INTERVAL);
+        }
+      }
+
+      if (status.equals(PreUpgradeCheckStatus.SUCCESS)) {
+        prerequisiteCheck.setStatus(PrereqCheckStatus.PASS);
+      } else {
+        prerequisiteCheck.setStatus(PrereqCheckStatus.FAIL);
+        prerequisiteCheck.setFailReason(FAILED_REASON);
+        prerequisiteCheck.setFailedOn(failedHosts);
+      }
+    } catch (Exception e) {
+      LOG.error(""Error running Pre Upgrade check for AMS Hadoop Sink compatibility. "" + e);
+      prerequisiteCheck.setStatus(PrereqCheckStatus.FAIL);
+    }
+  }
+
+  /**
+   * Get the status of the requestId and also the set of failed hosts if any.
+   * @param requestId RequestId to track.
+   * @param failedHosts populate this argument for failed hosts.
+   * @return Status of the request.
+   * @throws Exception
+   */
+  private PreUpgradeCheckStatus pollRequestStatus(long requestId, Set<String> failedHosts) throws Exception {
+
+    List<RequestEntity> requestEntities = requestDAO.findByPks(Collections.singleton(requestId), true);
+    if (requestEntities != null && requestEntities.size() > 0) {","[{'comment': ""Why do this if you're only using the first one?  Just call `requestDAO.findByPK()`"", 'commenter': 'ncole'}, {'comment': 'This is because the findByPk has no option to refresh the JPA cache on find. ', 'commenter': 'avijayanhwx'}, {'comment': ""This is an interesting issue. I'm OK with the workaround of forcing a refresh from the DB, but would there be another way to do this? "", 'commenter': 'jonathan-hurley'}]"
1405,ambari-server/src/main/java/org/apache/ambari/server/checks/AmbariMetricsHadoopSinkVersionCompatibilityCheck.java,"@@ -0,0 +1,215 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.checks;
+
+
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.LinkedHashMap;
+import java.util.LinkedHashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.stream.Collectors;
+
+import org.apache.ambari.server.AmbariException;
+import org.apache.ambari.server.actionmanager.HostRoleStatus;
+import org.apache.ambari.server.controller.AmbariManagementController;
+import org.apache.ambari.server.controller.AmbariServer;
+import org.apache.ambari.server.controller.PrereqCheckRequest;
+import org.apache.ambari.server.controller.internal.AbstractControllerResourceProvider;
+import org.apache.ambari.server.controller.internal.RequestResourceProvider;
+import org.apache.ambari.server.controller.spi.Request;
+import org.apache.ambari.server.controller.spi.Resource;
+import org.apache.ambari.server.controller.spi.ResourceProvider;
+import org.apache.ambari.server.controller.utilities.PropertyHelper;
+import org.apache.ambari.server.orm.dao.HostRoleCommandDAO;
+import org.apache.ambari.server.orm.dao.RequestDAO;
+import org.apache.ambari.server.orm.entities.HostRoleCommandEntity;
+import org.apache.ambari.server.orm.entities.RequestEntity;
+import org.apache.ambari.server.state.stack.PrereqCheckStatus;
+import org.apache.ambari.server.state.stack.PrerequisiteCheck;
+import org.apache.ambari.server.state.stack.upgrade.UpgradeType;
+import org.apache.commons.lang.StringUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.collect.Sets;
+import com.google.inject.Inject;
+import com.google.inject.Singleton;
+
+@Singleton
+@UpgradeCheck(
+  group = UpgradeCheckGroup.DEFAULT,
+  required = { UpgradeType.ROLLING, UpgradeType.NON_ROLLING })
+public class AmbariMetricsHadoopSinkVersionCompatibilityCheck extends AbstractCheckDescriptor  {
+
+  @Inject
+  private RequestDAO requestDAO;
+
+  @Inject
+  private HostRoleCommandDAO hostRoleCommandDAO;
+
+  private static final Logger LOG = LoggerFactory.getLogger(AmbariMetricsHadoopSinkVersionCompatibilityCheck.class);
+
+  private enum PreUpgradeCheckStatus {SUCCESS, FAILED, RUNNING}
+
+  private final String FAILED_REASON = ""Hadoop Sink version check for 2.7.0.0 failed. "" +
+    ""To fix this, please upgrade 'ambari-metrics-hadoop-sink' package to 2.7.0.0 on all the failed hosts"";
+
+  /**
+   * Total wait time for Ambari Server request time to finish => 2 mins.
+   */
+  private final long RETRY_INTERVAL = 6000l; // 6 seconds sleep interval per retry.
+  private final int NUM_TRIES = 20; // 20 times the check will try to see if request finished.
+
+  /**
+   * Constructor.
+   */
+  public AmbariMetricsHadoopSinkVersionCompatibilityCheck() {
+    super(CheckDescription.AMS_HADOOP_SINK_VERSION_COMPATIBILITY);
+  }
+
+  /**
+   * {@inheritDoc}
+   */
+  @Override
+  public Set<String> getApplicableServices() {
+    return Sets.newHashSet(""AMBARI_METRICS"");
+  }
+
+  @Override
+  public void perform(PrerequisiteCheck prerequisiteCheck, PrereqCheckRequest prereqCheckRequest) throws AmbariException {
+
+    AmbariManagementController ambariManagementController = AmbariServer.getController();
+
+    ResourceProvider provider = AbstractControllerResourceProvider.getResourceProvider(
+      Resource.Type.Request,
+      ambariManagementController
+      );
+
+    String clusterName = prereqCheckRequest.getClusterName();
+
+    Set<String> hosts = ambariManagementController.getClusters()
+      .getCluster(clusterName).getHosts(""AMBARI_METRICS"", ""METRICS_MONITOR"");
+
+    Set<Map<String, Object>> propertiesSet = new HashSet<>();
+    Map<String, Object> properties = new LinkedHashMap<>();
+    properties.put(RequestResourceProvider.REQUEST_CLUSTER_NAME_PROPERTY_ID, clusterName);
+
+    Set<Map<String, Object>> filterSet = new HashSet<>();
+    Map<String, Object> filterMap = new HashMap<>();
+    filterMap.put(RequestResourceProvider.SERVICE_ID, ""AMBARI_METRICS"");
+    filterMap.put(RequestResourceProvider.COMPONENT_ID, ""METRICS_MONITOR"");
+    filterMap.put(RequestResourceProvider.HOSTS_ID, StringUtils.join(hosts,"",""));
+    filterSet.add(filterMap);
+
+    properties.put(RequestResourceProvider.REQUEST_RESOURCE_FILTER_ID, filterSet);
+    propertiesSet.add(properties);
+
+    Map<String, String> requestInfoProperties = new HashMap<>();
+    requestInfoProperties.put(RequestResourceProvider.COMMAND_ID, ""CHECK_HADOOP_SINK_VERSION"");
+    requestInfoProperties.put(RequestResourceProvider.REQUEST_CONTEXT_ID, ""Pre Upgrade check for compatible Hadoop Metric "" +
+      ""Sink version on all hosts."");
+
+    Request request = PropertyHelper.getCreateRequest(propertiesSet, requestInfoProperties);
+    try {
+      org.apache.ambari.server.controller.spi.RequestStatus response = provider.createResources(request);
+      Resource responseResource = response.getRequestResource();
+      String requestIdProp = PropertyHelper.getPropertyId(""Requests"", ""id"");
+      long requestId = (long) responseResource.getPropertyValue(requestIdProp);
+      LOG.debug(""RequestId for AMS Hadoop Sink version compatibility pre check : "" + requestId);
+
+      Thread.sleep(RETRY_INTERVAL);
+      PreUpgradeCheckStatus status;
+      int retry = 0;
+      LinkedHashSet<String> failedHosts = new LinkedHashSet<>();
+      while ((status = pollRequestStatus(requestId, failedHosts)).equals(PreUpgradeCheckStatus.RUNNING)
+        && retry++ < NUM_TRIES) {
+        if (retry != NUM_TRIES) {
+          Thread.sleep(RETRY_INTERVAL);
+        }","[{'comment': ""This construct makes me nervous.  If we're in a bad state the UI is going to wait up to 2 minutes just for this precheck to fail out."", 'commenter': 'ncole'}, {'comment': 'Only if the request is still in running state. If the request finishes (pass/fail), the check finishes then and there. ', 'commenter': 'avijayanhwx'}]"
1405,ambari-server/src/main/java/org/apache/ambari/server/checks/AmbariMetricsHadoopSinkVersionCompatibilityCheck.java,"@@ -0,0 +1,215 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.checks;
+
+
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.LinkedHashMap;
+import java.util.LinkedHashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.stream.Collectors;
+
+import org.apache.ambari.server.AmbariException;
+import org.apache.ambari.server.actionmanager.HostRoleStatus;
+import org.apache.ambari.server.controller.AmbariManagementController;
+import org.apache.ambari.server.controller.AmbariServer;
+import org.apache.ambari.server.controller.PrereqCheckRequest;
+import org.apache.ambari.server.controller.internal.AbstractControllerResourceProvider;
+import org.apache.ambari.server.controller.internal.RequestResourceProvider;
+import org.apache.ambari.server.controller.spi.Request;
+import org.apache.ambari.server.controller.spi.Resource;
+import org.apache.ambari.server.controller.spi.ResourceProvider;
+import org.apache.ambari.server.controller.utilities.PropertyHelper;
+import org.apache.ambari.server.orm.dao.HostRoleCommandDAO;
+import org.apache.ambari.server.orm.dao.RequestDAO;
+import org.apache.ambari.server.orm.entities.HostRoleCommandEntity;
+import org.apache.ambari.server.orm.entities.RequestEntity;
+import org.apache.ambari.server.state.stack.PrereqCheckStatus;
+import org.apache.ambari.server.state.stack.PrerequisiteCheck;
+import org.apache.ambari.server.state.stack.upgrade.UpgradeType;
+import org.apache.commons.lang.StringUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.collect.Sets;
+import com.google.inject.Inject;
+import com.google.inject.Singleton;
+
+@Singleton
+@UpgradeCheck(
+  group = UpgradeCheckGroup.DEFAULT,
+  required = { UpgradeType.ROLLING, UpgradeType.NON_ROLLING })
+public class AmbariMetricsHadoopSinkVersionCompatibilityCheck extends AbstractCheckDescriptor  {
+
+  @Inject
+  private RequestDAO requestDAO;
+
+  @Inject
+  private HostRoleCommandDAO hostRoleCommandDAO;
+
+  private static final Logger LOG = LoggerFactory.getLogger(AmbariMetricsHadoopSinkVersionCompatibilityCheck.class);
+
+  private enum PreUpgradeCheckStatus {SUCCESS, FAILED, RUNNING}
+
+  private final String FAILED_REASON = ""Hadoop Sink version check for 2.7.0.0 failed. "" +
+    ""To fix this, please upgrade 'ambari-metrics-hadoop-sink' package to 2.7.0.0 on all the failed hosts"";
+
+  /**
+   * Total wait time for Ambari Server request time to finish => 2 mins.
+   */
+  private final long RETRY_INTERVAL = 6000l; // 6 seconds sleep interval per retry.
+  private final int NUM_TRIES = 20; // 20 times the check will try to see if request finished.
+
+  /**
+   * Constructor.
+   */
+  public AmbariMetricsHadoopSinkVersionCompatibilityCheck() {
+    super(CheckDescription.AMS_HADOOP_SINK_VERSION_COMPATIBILITY);
+  }
+
+  /**
+   * {@inheritDoc}
+   */
+  @Override
+  public Set<String> getApplicableServices() {
+    return Sets.newHashSet(""AMBARI_METRICS"");
+  }
+
+  @Override
+  public void perform(PrerequisiteCheck prerequisiteCheck, PrereqCheckRequest prereqCheckRequest) throws AmbariException {
+
+    AmbariManagementController ambariManagementController = AmbariServer.getController();
+
+    ResourceProvider provider = AbstractControllerResourceProvider.getResourceProvider(
+      Resource.Type.Request,
+      ambariManagementController
+      );
+
+    String clusterName = prereqCheckRequest.getClusterName();
+
+    Set<String> hosts = ambariManagementController.getClusters()
+      .getCluster(clusterName).getHosts(""AMBARI_METRICS"", ""METRICS_MONITOR"");
+
+    Set<Map<String, Object>> propertiesSet = new HashSet<>();
+    Map<String, Object> properties = new LinkedHashMap<>();
+    properties.put(RequestResourceProvider.REQUEST_CLUSTER_NAME_PROPERTY_ID, clusterName);
+
+    Set<Map<String, Object>> filterSet = new HashSet<>();
+    Map<String, Object> filterMap = new HashMap<>();
+    filterMap.put(RequestResourceProvider.SERVICE_ID, ""AMBARI_METRICS"");
+    filterMap.put(RequestResourceProvider.COMPONENT_ID, ""METRICS_MONITOR"");
+    filterMap.put(RequestResourceProvider.HOSTS_ID, StringUtils.join(hosts,"",""));
+    filterSet.add(filterMap);
+
+    properties.put(RequestResourceProvider.REQUEST_RESOURCE_FILTER_ID, filterSet);
+    propertiesSet.add(properties);
+
+    Map<String, String> requestInfoProperties = new HashMap<>();
+    requestInfoProperties.put(RequestResourceProvider.COMMAND_ID, ""CHECK_HADOOP_SINK_VERSION"");
+    requestInfoProperties.put(RequestResourceProvider.REQUEST_CONTEXT_ID, ""Pre Upgrade check for compatible Hadoop Metric "" +
+      ""Sink version on all hosts."");
+
+    Request request = PropertyHelper.getCreateRequest(propertiesSet, requestInfoProperties);
+    try {
+      org.apache.ambari.server.controller.spi.RequestStatus response = provider.createResources(request);
+      Resource responseResource = response.getRequestResource();
+      String requestIdProp = PropertyHelper.getPropertyId(""Requests"", ""id"");
+      long requestId = (long) responseResource.getPropertyValue(requestIdProp);
+      LOG.debug(""RequestId for AMS Hadoop Sink version compatibility pre check : "" + requestId);
+
+      Thread.sleep(RETRY_INTERVAL);
+      PreUpgradeCheckStatus status;
+      int retry = 0;
+      LinkedHashSet<String> failedHosts = new LinkedHashSet<>();
+      while ((status = pollRequestStatus(requestId, failedHosts)).equals(PreUpgradeCheckStatus.RUNNING)
+        && retry++ < NUM_TRIES) {
+        if (retry != NUM_TRIES) {
+          Thread.sleep(RETRY_INTERVAL);
+        }
+      }
+
+      if (status.equals(PreUpgradeCheckStatus.SUCCESS)) {
+        prerequisiteCheck.setStatus(PrereqCheckStatus.PASS);
+      } else {
+        prerequisiteCheck.setStatus(PrereqCheckStatus.FAIL);
+        prerequisiteCheck.setFailReason(FAILED_REASON);
+        prerequisiteCheck.setFailedOn(failedHosts);","[{'comment': ""This check should be defined as a HOST one since you're using `failedOn` with hosts.  The CheckDescription as SERVICE should be a list of service names for `failedOn`"", 'commenter': 'ncole'}, {'comment': 'Will do. ', 'commenter': 'avijayanhwx'}]"
1405,ambari-server/src/main/java/org/apache/ambari/server/checks/AmbariMetricsHadoopSinkVersionCompatibilityCheck.java,"@@ -0,0 +1,233 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.checks;
+
+
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.LinkedHashMap;
+import java.util.LinkedHashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.stream.Collectors;
+
+import org.apache.ambari.server.AmbariException;
+import org.apache.ambari.server.actionmanager.HostRoleStatus;
+import org.apache.ambari.server.controller.AmbariManagementController;
+import org.apache.ambari.server.controller.AmbariServer;
+import org.apache.ambari.server.controller.PrereqCheckRequest;
+import org.apache.ambari.server.controller.internal.AbstractControllerResourceProvider;
+import org.apache.ambari.server.controller.internal.RequestResourceProvider;
+import org.apache.ambari.server.controller.spi.Request;
+import org.apache.ambari.server.controller.spi.Resource;
+import org.apache.ambari.server.controller.spi.ResourceProvider;
+import org.apache.ambari.server.controller.utilities.PropertyHelper;
+import org.apache.ambari.server.orm.dao.HostRoleCommandDAO;
+import org.apache.ambari.server.orm.dao.RequestDAO;
+import org.apache.ambari.server.orm.entities.HostRoleCommandEntity;
+import org.apache.ambari.server.orm.entities.RequestEntity;
+import org.apache.ambari.server.state.stack.PrereqCheckStatus;
+import org.apache.ambari.server.state.stack.PrerequisiteCheck;
+import org.apache.ambari.server.state.stack.UpgradePack;
+import org.apache.commons.lang.StringUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.collect.Sets;
+import com.google.inject.Inject;
+import com.google.inject.Singleton;
+
+@Singleton
+@UpgradeCheck(
+  group = UpgradeCheckGroup.DEFAULT)
+public class AmbariMetricsHadoopSinkVersionCompatibilityCheck extends AbstractCheckDescriptor  {
+
+  @Inject
+  private RequestDAO requestDAO;
+
+  @Inject
+  private HostRoleCommandDAO hostRoleCommandDAO;
+
+  private static final Logger LOG = LoggerFactory.getLogger(AmbariMetricsHadoopSinkVersionCompatibilityCheck.class);
+
+  private enum PreUpgradeCheckStatus {SUCCESS, FAILED, RUNNING}
+
+  private final String FAILED_REASON = ""Hadoop Sink version check failed. "" +
+    ""To fix this, please upgrade 'ambari-metrics-hadoop-sink' package to %s on all the failed hosts"";","[{'comment': 'This string should go with CheckDescription.AMS_HADOOP_SINK_VERSION_COMPATIBILITY.  We keep them all in one place, and use `getFailReason()` when needed.', 'commenter': 'ncole'}, {'comment': 'Done', 'commenter': 'avijayanhwx'}]"
1405,ambari-server/src/main/java/org/apache/ambari/server/checks/AmbariMetricsHadoopSinkVersionCompatibilityCheck.java,"@@ -0,0 +1,230 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.checks;
+
+
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.LinkedHashMap;
+import java.util.LinkedHashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.stream.Collectors;
+
+import org.apache.ambari.server.AmbariException;
+import org.apache.ambari.server.actionmanager.HostRoleStatus;
+import org.apache.ambari.server.controller.AmbariManagementController;
+import org.apache.ambari.server.controller.AmbariServer;
+import org.apache.ambari.server.controller.PrereqCheckRequest;
+import org.apache.ambari.server.controller.internal.AbstractControllerResourceProvider;
+import org.apache.ambari.server.controller.internal.RequestResourceProvider;
+import org.apache.ambari.server.controller.spi.Request;
+import org.apache.ambari.server.controller.spi.Resource;
+import org.apache.ambari.server.controller.spi.ResourceProvider;
+import org.apache.ambari.server.controller.utilities.PropertyHelper;
+import org.apache.ambari.server.orm.dao.HostRoleCommandDAO;
+import org.apache.ambari.server.orm.dao.RequestDAO;
+import org.apache.ambari.server.orm.entities.HostRoleCommandEntity;
+import org.apache.ambari.server.orm.entities.RequestEntity;
+import org.apache.ambari.server.state.stack.PrereqCheckStatus;
+import org.apache.ambari.server.state.stack.PrerequisiteCheck;
+import org.apache.ambari.server.state.stack.UpgradePack;
+import org.apache.commons.lang.StringUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.collect.Sets;
+import com.google.inject.Inject;
+import com.google.inject.Singleton;
+
+@Singleton
+@UpgradeCheck(
+  group = UpgradeCheckGroup.DEFAULT)","[{'comment': ""This will make sure that the warning is shown in the last group? Since this could involve a lot of time to fix, should this be changed to a different UpgradeCheckGroup?\r\n\r\nAlso - let's add some doc to this class."", 'commenter': 'jonathan-hurley'}]"
1405,ambari-server/src/main/java/org/apache/ambari/server/checks/AmbariMetricsHadoopSinkVersionCompatibilityCheck.java,"@@ -0,0 +1,230 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.checks;
+
+
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.LinkedHashMap;
+import java.util.LinkedHashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.stream.Collectors;
+
+import org.apache.ambari.server.AmbariException;
+import org.apache.ambari.server.actionmanager.HostRoleStatus;
+import org.apache.ambari.server.controller.AmbariManagementController;
+import org.apache.ambari.server.controller.AmbariServer;
+import org.apache.ambari.server.controller.PrereqCheckRequest;
+import org.apache.ambari.server.controller.internal.AbstractControllerResourceProvider;
+import org.apache.ambari.server.controller.internal.RequestResourceProvider;
+import org.apache.ambari.server.controller.spi.Request;
+import org.apache.ambari.server.controller.spi.Resource;
+import org.apache.ambari.server.controller.spi.ResourceProvider;
+import org.apache.ambari.server.controller.utilities.PropertyHelper;
+import org.apache.ambari.server.orm.dao.HostRoleCommandDAO;
+import org.apache.ambari.server.orm.dao.RequestDAO;
+import org.apache.ambari.server.orm.entities.HostRoleCommandEntity;
+import org.apache.ambari.server.orm.entities.RequestEntity;
+import org.apache.ambari.server.state.stack.PrereqCheckStatus;
+import org.apache.ambari.server.state.stack.PrerequisiteCheck;
+import org.apache.ambari.server.state.stack.UpgradePack;
+import org.apache.commons.lang.StringUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.collect.Sets;
+import com.google.inject.Inject;
+import com.google.inject.Singleton;
+
+@Singleton
+@UpgradeCheck(
+  group = UpgradeCheckGroup.DEFAULT)
+public class AmbariMetricsHadoopSinkVersionCompatibilityCheck extends AbstractCheckDescriptor  {
+
+  @Inject
+  private RequestDAO requestDAO;
+
+  @Inject
+  private HostRoleCommandDAO hostRoleCommandDAO;
+
+  private static final Logger LOG = LoggerFactory.getLogger(AmbariMetricsHadoopSinkVersionCompatibilityCheck.class);
+
+  private enum PreUpgradeCheckStatus {SUCCESS, FAILED, RUNNING}
+
+  static final String MIN_HADOOP_SINK_VERSION_PROPERTY_NAME = ""min-hadoop-sink-version"";
+  static final String RETRY_INTERVAL_PROPERTY_NAME = ""request-status-check-retry-interval"";
+  static final String NUM_TRIES_PROPERTY_NAME = ""request-status-check-num-retries"";
+
+  /**
+   * Total wait time for Ambari Server request time to finish => 2 mins.
+   */
+  private long retryInterval = 6000l; // 6 seconds sleep interval per retry.
+  private int numTries = 20; // 20 times the check will try to see if request finished.
+
+  /**
+   * Constructor.
+   */
+  public AmbariMetricsHadoopSinkVersionCompatibilityCheck() {
+    super(CheckDescription.AMS_HADOOP_SINK_VERSION_COMPATIBILITY);
+  }
+
+  /**
+   * {@inheritDoc}
+   */
+  @Override
+  public Set<String> getApplicableServices() {
+    return Sets.newHashSet(""AMBARI_METRICS"", ""HDFS"");
+  }
+
+  @Override
+  public void perform(PrerequisiteCheck prerequisiteCheck, PrereqCheckRequest prereqCheckRequest) throws AmbariException {
+
+    String minHadoopSinkVersion = null;
+
+    UpgradePack.PrerequisiteCheckConfig prerequisiteCheckConfig = prereqCheckRequest.getPrerequisiteCheckConfig();
+    Map<String, String> checkProperties = null;
+    if(prerequisiteCheckConfig != null) {
+      checkProperties = prerequisiteCheckConfig.getCheckProperties(this.getClass().getName());
+    }
+
+    if(checkProperties != null) {
+      minHadoopSinkVersion = checkProperties.getOrDefault(MIN_HADOOP_SINK_VERSION_PROPERTY_NAME, ""2.7.0.0"");","[{'comment': ""Let's fail the pre-req check if this isn't set - let's not hard code values for specific stacks anywhere."", 'commenter': 'jonathan-hurley'}]"
1405,ambari-server/src/main/java/org/apache/ambari/server/checks/AmbariMetricsHadoopSinkVersionCompatibilityCheck.java,"@@ -0,0 +1,230 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.checks;
+
+
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.LinkedHashMap;
+import java.util.LinkedHashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.stream.Collectors;
+
+import org.apache.ambari.server.AmbariException;
+import org.apache.ambari.server.actionmanager.HostRoleStatus;
+import org.apache.ambari.server.controller.AmbariManagementController;
+import org.apache.ambari.server.controller.AmbariServer;
+import org.apache.ambari.server.controller.PrereqCheckRequest;
+import org.apache.ambari.server.controller.internal.AbstractControllerResourceProvider;
+import org.apache.ambari.server.controller.internal.RequestResourceProvider;
+import org.apache.ambari.server.controller.spi.Request;
+import org.apache.ambari.server.controller.spi.Resource;
+import org.apache.ambari.server.controller.spi.ResourceProvider;
+import org.apache.ambari.server.controller.utilities.PropertyHelper;
+import org.apache.ambari.server.orm.dao.HostRoleCommandDAO;
+import org.apache.ambari.server.orm.dao.RequestDAO;
+import org.apache.ambari.server.orm.entities.HostRoleCommandEntity;
+import org.apache.ambari.server.orm.entities.RequestEntity;
+import org.apache.ambari.server.state.stack.PrereqCheckStatus;
+import org.apache.ambari.server.state.stack.PrerequisiteCheck;
+import org.apache.ambari.server.state.stack.UpgradePack;
+import org.apache.commons.lang.StringUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.collect.Sets;
+import com.google.inject.Inject;
+import com.google.inject.Singleton;
+
+@Singleton
+@UpgradeCheck(
+  group = UpgradeCheckGroup.DEFAULT)
+public class AmbariMetricsHadoopSinkVersionCompatibilityCheck extends AbstractCheckDescriptor  {
+
+  @Inject
+  private RequestDAO requestDAO;
+
+  @Inject
+  private HostRoleCommandDAO hostRoleCommandDAO;
+
+  private static final Logger LOG = LoggerFactory.getLogger(AmbariMetricsHadoopSinkVersionCompatibilityCheck.class);
+
+  private enum PreUpgradeCheckStatus {SUCCESS, FAILED, RUNNING}
+
+  static final String MIN_HADOOP_SINK_VERSION_PROPERTY_NAME = ""min-hadoop-sink-version"";
+  static final String RETRY_INTERVAL_PROPERTY_NAME = ""request-status-check-retry-interval"";
+  static final String NUM_TRIES_PROPERTY_NAME = ""request-status-check-num-retries"";
+
+  /**
+   * Total wait time for Ambari Server request time to finish => 2 mins.
+   */
+  private long retryInterval = 6000l; // 6 seconds sleep interval per retry.
+  private int numTries = 20; // 20 times the check will try to see if request finished.
+
+  /**
+   * Constructor.
+   */
+  public AmbariMetricsHadoopSinkVersionCompatibilityCheck() {
+    super(CheckDescription.AMS_HADOOP_SINK_VERSION_COMPATIBILITY);
+  }
+
+  /**
+   * {@inheritDoc}
+   */
+  @Override
+  public Set<String> getApplicableServices() {
+    return Sets.newHashSet(""AMBARI_METRICS"", ""HDFS"");
+  }
+
+  @Override
+  public void perform(PrerequisiteCheck prerequisiteCheck, PrereqCheckRequest prereqCheckRequest) throws AmbariException {
+
+    String minHadoopSinkVersion = null;
+
+    UpgradePack.PrerequisiteCheckConfig prerequisiteCheckConfig = prereqCheckRequest.getPrerequisiteCheckConfig();
+    Map<String, String> checkProperties = null;
+    if(prerequisiteCheckConfig != null) {
+      checkProperties = prerequisiteCheckConfig.getCheckProperties(this.getClass().getName());
+    }
+
+    if(checkProperties != null) {
+      minHadoopSinkVersion = checkProperties.getOrDefault(MIN_HADOOP_SINK_VERSION_PROPERTY_NAME, ""2.7.0.0"");
+      retryInterval = Long.valueOf(checkProperties.getOrDefault(RETRY_INTERVAL_PROPERTY_NAME, ""6000""));
+      numTries = Integer.valueOf(checkProperties.getOrDefault(NUM_TRIES_PROPERTY_NAME, ""20""));
+    }
+
+    LOG.debug(""Properties : "" + minHadoopSinkVersion + "", "" + retryInterval + "", "" + numTries);","[{'comment': 'Use {} notation.', 'commenter': 'jonathan-hurley'}]"
1405,ambari-server/src/main/java/org/apache/ambari/server/checks/AmbariMetricsHadoopSinkVersionCompatibilityCheck.java,"@@ -0,0 +1,230 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.checks;
+
+
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.LinkedHashMap;
+import java.util.LinkedHashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.stream.Collectors;
+
+import org.apache.ambari.server.AmbariException;
+import org.apache.ambari.server.actionmanager.HostRoleStatus;
+import org.apache.ambari.server.controller.AmbariManagementController;
+import org.apache.ambari.server.controller.AmbariServer;
+import org.apache.ambari.server.controller.PrereqCheckRequest;
+import org.apache.ambari.server.controller.internal.AbstractControllerResourceProvider;
+import org.apache.ambari.server.controller.internal.RequestResourceProvider;
+import org.apache.ambari.server.controller.spi.Request;
+import org.apache.ambari.server.controller.spi.Resource;
+import org.apache.ambari.server.controller.spi.ResourceProvider;
+import org.apache.ambari.server.controller.utilities.PropertyHelper;
+import org.apache.ambari.server.orm.dao.HostRoleCommandDAO;
+import org.apache.ambari.server.orm.dao.RequestDAO;
+import org.apache.ambari.server.orm.entities.HostRoleCommandEntity;
+import org.apache.ambari.server.orm.entities.RequestEntity;
+import org.apache.ambari.server.state.stack.PrereqCheckStatus;
+import org.apache.ambari.server.state.stack.PrerequisiteCheck;
+import org.apache.ambari.server.state.stack.UpgradePack;
+import org.apache.commons.lang.StringUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.collect.Sets;
+import com.google.inject.Inject;
+import com.google.inject.Singleton;
+
+@Singleton
+@UpgradeCheck(
+  group = UpgradeCheckGroup.DEFAULT)
+public class AmbariMetricsHadoopSinkVersionCompatibilityCheck extends AbstractCheckDescriptor  {
+
+  @Inject
+  private RequestDAO requestDAO;
+
+  @Inject
+  private HostRoleCommandDAO hostRoleCommandDAO;
+
+  private static final Logger LOG = LoggerFactory.getLogger(AmbariMetricsHadoopSinkVersionCompatibilityCheck.class);
+
+  private enum PreUpgradeCheckStatus {SUCCESS, FAILED, RUNNING}
+
+  static final String MIN_HADOOP_SINK_VERSION_PROPERTY_NAME = ""min-hadoop-sink-version"";
+  static final String RETRY_INTERVAL_PROPERTY_NAME = ""request-status-check-retry-interval"";
+  static final String NUM_TRIES_PROPERTY_NAME = ""request-status-check-num-retries"";
+
+  /**
+   * Total wait time for Ambari Server request time to finish => 2 mins.
+   */
+  private long retryInterval = 6000l; // 6 seconds sleep interval per retry.
+  private int numTries = 20; // 20 times the check will try to see if request finished.
+
+  /**
+   * Constructor.
+   */
+  public AmbariMetricsHadoopSinkVersionCompatibilityCheck() {
+    super(CheckDescription.AMS_HADOOP_SINK_VERSION_COMPATIBILITY);
+  }
+
+  /**
+   * {@inheritDoc}
+   */
+  @Override
+  public Set<String> getApplicableServices() {
+    return Sets.newHashSet(""AMBARI_METRICS"", ""HDFS"");
+  }
+
+  @Override
+  public void perform(PrerequisiteCheck prerequisiteCheck, PrereqCheckRequest prereqCheckRequest) throws AmbariException {
+
+    String minHadoopSinkVersion = null;
+
+    UpgradePack.PrerequisiteCheckConfig prerequisiteCheckConfig = prereqCheckRequest.getPrerequisiteCheckConfig();
+    Map<String, String> checkProperties = null;
+    if(prerequisiteCheckConfig != null) {
+      checkProperties = prerequisiteCheckConfig.getCheckProperties(this.getClass().getName());
+    }
+
+    if(checkProperties != null) {
+      minHadoopSinkVersion = checkProperties.getOrDefault(MIN_HADOOP_SINK_VERSION_PROPERTY_NAME, ""2.7.0.0"");
+      retryInterval = Long.valueOf(checkProperties.getOrDefault(RETRY_INTERVAL_PROPERTY_NAME, ""6000""));
+      numTries = Integer.valueOf(checkProperties.getOrDefault(NUM_TRIES_PROPERTY_NAME, ""20""));
+    }
+
+    LOG.debug(""Properties : "" + minHadoopSinkVersion + "", "" + retryInterval + "", "" + numTries);
+    AmbariManagementController ambariManagementController = AmbariServer.getController();
+
+    ResourceProvider provider = AbstractControllerResourceProvider.getResourceProvider(
+      Resource.Type.Request,
+      ambariManagementController
+      );
+
+    String clusterName = prereqCheckRequest.getClusterName();
+
+    Set<String> hosts = ambariManagementController.getClusters()","[{'comment': ""Sanity check for no hosts returned so that we don't hit errors later."", 'commenter': 'jonathan-hurley'}, {'comment': 'Fixed', 'commenter': 'avijayanhwx'}]"
1405,ambari-server/src/main/java/org/apache/ambari/server/orm/dao/HostRoleCommandDAO.java,"@@ -498,6 +500,17 @@ public HostRoleCommandEntity findByPK(long taskId) {
     return daoUtils.selectList(query, requestId);
   }
 
+  @RequiresSession
+  public List<HostRoleCommandEntity> findByRequest(long requestId, boolean refreshHint) {
+    TypedQuery<HostRoleCommandEntity> query = entityManagerProvider.get().createQuery(""SELECT command "" +","[{'comment': 'Convert into a NamedQuery', 'commenter': 'jonathan-hurley'}]"
1406,ambari-server/src/test/java/org/apache/ambari/server/state/MpackTest.java,"@@ -63,6 +63,7 @@
           ""    \""min-ambari-version\"": \""3.0.0.0\""\n"" +
           ""  },\n"" +
           ""  \""version\"": \""1.0.0-b16\""\n"" +","[{'comment': '`,` is missing before `\\n`, causing:\r\n\r\n```\r\nMalformedJsonException: Unterminated object at line 38 column 4\r\n```', 'commenter': 'adoroszlai'}, {'comment': 'Fixed', 'commenter': 'jayush'}]"
1424,ambari-common/src/main/python/resource_management/libraries/functions/mpack_manager_helper.py,"@@ -41,6 +43,31 @@ def get_component_conf_path(mpack_name, instance_name, module_name, components_i
   return conf_json[COMPONENTS_PLURAL_KEY_NAME][components_instance_type.lower()][COMPONENT_INSTANCES_PLURAL_KEY_NAME][
     component_instance_name][CONFIG_DIR_KEY_NAME]
 
+def get_component_log_path(mpack_name, instance_name, module_name, components_instance_type,
+                            subgroup_name='default', component_instance_name='default'):
+  """"""
+  :returns the single string that contains the path to the log folder of given component instance
+  :raises ValueError if the parameters doesn't match the mpack or instances structure
+  """"""
+
+  log_json = get_conf_dir(mpack_name, instance_name, subgroup_name, module_name,
+                           {components_instance_type: [component_instance_name]})
+
+  return log_json[COMPONENTS_PLURAL_KEY_NAME][components_instance_type.lower()][COMPONENT_INSTANCES_PLURAL_KEY_NAME][
+    component_instance_name][LOG_DIR_KEY_NAME]
+
+def get_component_pid_prefix_path(mpack_name, instance_name, module_name, components_instance_type,","[{'comment': ""pid_prefix isn't a suitable name here as it is not obvious that it means the root directory for pid. It could also mean /var/run/hadoop/hdfs- as prefix. Lets name it get_component_rundir_path"", 'commenter': 'jayush'}]"
1424,ambari-common/src/main/python/resource_management/libraries/functions/mpack_manager_helper.py,"@@ -41,6 +43,31 @@ def get_component_conf_path(mpack_name, instance_name, module_name, components_i
   return conf_json[COMPONENTS_PLURAL_KEY_NAME][components_instance_type.lower()][COMPONENT_INSTANCES_PLURAL_KEY_NAME][
     component_instance_name][CONFIG_DIR_KEY_NAME]
 
+def get_component_log_path(mpack_name, instance_name, module_name, components_instance_type,
+                            subgroup_name='default', component_instance_name='default'):
+  """"""
+  :returns the single string that contains the path to the log folder of given component instance
+  :raises ValueError if the parameters doesn't match the mpack or instances structure
+  """"""
+
+  log_json = get_conf_dir(mpack_name, instance_name, subgroup_name, module_name,","[{'comment': 'get_conf_dir() only returns config directories. We should be adding get_log_dir() and get_run_dir() to return log dir and run dir respectively', 'commenter': 'jayush'}, {'comment': ""get_conf_dir() return a directory, please see another piece of code:\r\n\r\n  def build_json_output(self, output_conf_dir, output_path):\r\n    result = {'name': self.name}\r\n    if output_conf_dir:\r\n      result['config_dir'] = os.path.join(self.component_path, CONFIGS_DIRECTORY_NAME)\r\n      result['log_dir'] = os.path.join(self.component_path, LOG_DIRECTORY_NAME)\r\n      result['run_dir'] = os.path.join(self.component_path, RUN_DIRECTORY_NAME)\r\n    if output_path:\r\n      result['path'] = self.path_exec\r\n    return result\r\n\r\n"", 'commenter': 'scottduan'}, {'comment': ""Ya output_conf_dir shouldn't return config_dir, log_dir and run_dir :)"", 'commenter': 'jayush'}]"
1424,ambari-common/src/main/python/resource_management/libraries/functions/mpack_manager_helper.py,"@@ -41,6 +43,31 @@ def get_component_conf_path(mpack_name, instance_name, module_name, components_i
   return conf_json[COMPONENTS_PLURAL_KEY_NAME][components_instance_type.lower()][COMPONENT_INSTANCES_PLURAL_KEY_NAME][
     component_instance_name][CONFIG_DIR_KEY_NAME]
 
+def get_component_log_path(mpack_name, instance_name, module_name, components_instance_type,
+                            subgroup_name='default', component_instance_name='default'):
+  """"""
+  :returns the single string that contains the path to the log folder of given component instance
+  :raises ValueError if the parameters doesn't match the mpack or instances structure
+  """"""
+
+  log_json = get_log_dir(mpack_name, instance_name, subgroup_name, module_name,
+                           {components_instance_type: [component_instance_name]})
+
+  return log_json[COMPONENTS_PLURAL_KEY_NAME][components_instance_type.lower()][COMPONENT_INSTANCES_PLURAL_KEY_NAME][
+    component_instance_name][LOG_DIR_KEY_NAME]
+
+def get_component_rundir_path(mpack_name, instance_name, module_name, components_instance_type,
+                            subgroup_name='default', component_instance_name='default'):
+  """"""
+  :returns the single string that contains the path to the rundir folder of given component instance
+  :raises ValueError if the parameters doesn't match the mpack or instances structure
+  """"""
+
+  run_json = get_run_dir(mpack_name, instance_name, subgroup_name, module_name,","[{'comment': ""Shouldn't it import `get_run_dir` and `get_log_dir`?"", 'commenter': 'adoroszlai'}]"
1483,ambari-server/src/main/java/org/apache/ambari/server/serveraction/upgrades/OozieConfigCalculation.java,"@@ -30,24 +31,103 @@
 import org.apache.ambari.server.state.Cluster;
 import org.apache.ambari.server.state.Config;
 import org.apache.ambari.server.state.Host;
+import org.apache.ambari.server.state.ServiceComponentSupport;
+import org.apache.ambari.server.state.StackId;
+import org.apache.ambari.server.state.UpgradeContext;
+import org.apache.commons.collections.CollectionUtils;
+
+import com.google.inject.Inject;
 
 /**
- * Changes oozie-env during upgrade (adds -Dhdp.version to $HADOOP_OPTS variable)
+ * Changes oozie-env (adds -Dhdp.version to $HADOOP_OPTS variable)
+ * and oozie-site (removes oozie.service.ELService.ext.functions.*) during upgrade
  */
 public class OozieConfigCalculation extends AbstractUpgradeServerAction {
-  private static final String TARGET_CONFIG_TYPE = ""oozie-env"";
+
+  private static final String FALCON_SERVICE_NAME = ""FALCON"";
+  @Inject
+  private ServiceComponentSupport serviceComponentSupport;
+
+  private static final String OOZIE_ENV_TARGET_CONFIG_TYPE = ""oozie-env"";
+  private static final String OOZIE_SITE_TARGET_CONFIG_TYPE = ""oozie-site"";
+  private static final String ELSERVICE_PROPERTIES_NAME_PREFIX = ""oozie.service.ELService.ext.functions."";
   private static final String CONTENT_PROPERTY_NAME = ""content"";
+  private boolean oozie_env_updated = false;
+  private boolean oozie_site_updated = false;
 
   @Override
   public CommandReport execute(ConcurrentMap<String, Object> requestSharedDataContext)
     throws AmbariException, InterruptedException {
     String clusterName = getExecutionCommand().getClusterName();
     Cluster cluster = getClusters().getCluster(clusterName);
-    Config config = cluster.getDesiredConfigByType(TARGET_CONFIG_TYPE);
+    StringBuilder stdOutBuilder = new StringBuilder();
 
-    if (config == null) {
+    try {
+      changeOozieEnv(cluster, stdOutBuilder);
+    } catch (Exception e) {
       return  createCommandReport(0, HostRoleStatus.FAILED,""{}"",
-                                   String.format(""Source type %s not found"", TARGET_CONFIG_TYPE), """");
+          String.format(""Source type %s not found"", OOZIE_ENV_TARGET_CONFIG_TYPE), """");
+    }
+
+    UpgradeContext upgradeContext = getUpgradeContext(cluster);
+    StackId targetStackId = upgradeContext.getTargetStack();
+
+
+    if (!serviceComponentSupport.isServiceSupported(FALCON_SERVICE_NAME, targetStackId.getStackName(), targetStackId.getStackVersion())) {
+      try {
+        removeFalconPropertiesFromOozieSize(cluster, stdOutBuilder);
+      } catch (AmbariException e) {
+        return createCommandReport(0, HostRoleStatus.FAILED, ""{}"",
+            String.format(""Source type %s not found"", OOZIE_SITE_TARGET_CONFIG_TYPE), """");
+      }
+    }
+
+    if (oozie_env_updated || oozie_site_updated) {
+      agentConfigsHolder.updateData(cluster.getClusterId(), cluster.getHosts().stream().map(Host::getHostId).collect(Collectors.toList()));
+    }
+
+    return createCommandReport(0, HostRoleStatus.COMPLETED, ""{}"",
+                  stdOutBuilder.toString(), """");
+  }
+
+  /**
+   * Changes oozie-site (removes oozie.service.ELService.ext.functions.*)
+   */
+  private void removeFalconPropertiesFromOozieSize(Cluster cluster, StringBuilder stringBuilder) throws AmbariException {
+    Config config = cluster.getDesiredConfigByType(OOZIE_SITE_TARGET_CONFIG_TYPE);
+
+    if (config == null) {
+      throw new AmbariException(String.format(""Target config not found %s"", OOZIE_SITE_TARGET_CONFIG_TYPE));
+    }
+
+    Map<String, String> properties = config.getProperties();
+    List<String> propertiesToRemove = properties.keySet().stream().filter(
+        s -> s.startsWith(ELSERVICE_PROPERTIES_NAME_PREFIX)).collect(Collectors.toList());
+
+    if (!CollectionUtils.isEmpty(propertiesToRemove)) {
+      stringBuilder.append(String.format(""Removed following properties from %s: %s"", OOZIE_SITE_TARGET_CONFIG_TYPE, propertiesToRemove));","[{'comment': 'Do you need to use StringUtils.join() on the list of `propertiesToRemove` ?', 'commenter': 'jonathan-hurley'}, {'comment': 'Currently the output looks like : ""Removed following properties from oozie-site: [oozie.service.ELService.ext.functions.workflow, oozie.service.ELService.ext.functions.coord-action-create]"", I think it should be fine.', 'commenter': 'd0zen1'}, {'comment': 'Sure', 'commenter': 'jonathan-hurley'}]"
1488,ambari-server/src/main/java/org/apache/ambari/server/api/services/stackadvisor/commands/KerberosConfigurationRecommendationCommand.java,"@@ -0,0 +1,121 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.api.services.stackadvisor.commands;
+
+import static org.apache.ambari.server.api.services.stackadvisor.recommendations.RecommendationResponse.BindingHostGroup;
+import static org.apache.ambari.server.api.services.stackadvisor.recommendations.RecommendationResponse.HostGroup;
+
+import java.io.File;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.ambari.server.api.services.AmbariMetaInfo;
+import org.apache.ambari.server.api.services.stackadvisor.StackAdvisorException;
+import org.apache.ambari.server.api.services.stackadvisor.StackAdvisorRequest;
+import org.apache.ambari.server.api.services.stackadvisor.StackAdvisorRunner;
+import org.apache.ambari.server.api.services.stackadvisor.recommendations.RecommendationResponse;
+import org.apache.ambari.server.controller.internal.AmbariServerConfigurationHandler;
+import org.apache.ambari.server.state.ServiceInfo;
+
+/**
+ * {@link KerberosConfigurationRecommendationCommand} implementation for
+ * Kerberos configuration recommendations.
+ */
+public class KerberosConfigurationRecommendationCommand extends
+    StackAdvisorCommand<RecommendationResponse> {
+
+  public KerberosConfigurationRecommendationCommand(File recommendationsDir,
+                                                    String recommendationsArtifactsLifetime,
+                                                    ServiceInfo.ServiceAdvisorType serviceAdvisorType,
+                                                    int requestId,
+                                                    StackAdvisorRunner saRunner,
+                                                    AmbariMetaInfo metaInfo, AmbariServerConfigurationHandler ambariServerConfigurationHandler) {
+    super(recommendationsDir, recommendationsArtifactsLifetime, serviceAdvisorType, requestId, saRunner, metaInfo, ambariServerConfigurationHandler);
+  }
+
+  @Override
+  protected StackAdvisorCommandType getCommandType() {
+    return StackAdvisorCommandType.RECOMMEND_CONFIGURATIONS_FOR_KERBEROS;
+  }
+
+  @Override
+  protected void validate(StackAdvisorRequest request) throws StackAdvisorException {
+    if (request.getHosts() == null || request.getHosts().isEmpty() || request.getServices() == null
+        || request.getServices().isEmpty()) {","[{'comment': 'CollectionUtils.isEmpty() ?', 'commenter': 'jonathan-hurley'}, {'comment': 'This was code copied from another class.. technically I should created an abstract class but some unit test was not happy with something about that.  I will either fix this or fix it in the abstract class I create to provide reusable code. ', 'commenter': 'rlevas'}]"
1488,ambari-server/src/main/java/org/apache/ambari/server/api/services/stackadvisor/commands/KerberosConfigurationRecommendationCommand.java,"@@ -0,0 +1,121 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.api.services.stackadvisor.commands;
+
+import static org.apache.ambari.server.api.services.stackadvisor.recommendations.RecommendationResponse.BindingHostGroup;
+import static org.apache.ambari.server.api.services.stackadvisor.recommendations.RecommendationResponse.HostGroup;
+
+import java.io.File;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.ambari.server.api.services.AmbariMetaInfo;
+import org.apache.ambari.server.api.services.stackadvisor.StackAdvisorException;
+import org.apache.ambari.server.api.services.stackadvisor.StackAdvisorRequest;
+import org.apache.ambari.server.api.services.stackadvisor.StackAdvisorRunner;
+import org.apache.ambari.server.api.services.stackadvisor.recommendations.RecommendationResponse;
+import org.apache.ambari.server.controller.internal.AmbariServerConfigurationHandler;
+import org.apache.ambari.server.state.ServiceInfo;
+
+/**
+ * {@link KerberosConfigurationRecommendationCommand} implementation for
+ * Kerberos configuration recommendations.
+ */
+public class KerberosConfigurationRecommendationCommand extends
+    StackAdvisorCommand<RecommendationResponse> {
+
+  public KerberosConfigurationRecommendationCommand(File recommendationsDir,
+                                                    String recommendationsArtifactsLifetime,
+                                                    ServiceInfo.ServiceAdvisorType serviceAdvisorType,
+                                                    int requestId,
+                                                    StackAdvisorRunner saRunner,
+                                                    AmbariMetaInfo metaInfo, AmbariServerConfigurationHandler ambariServerConfigurationHandler) {
+    super(recommendationsDir, recommendationsArtifactsLifetime, serviceAdvisorType, requestId, saRunner, metaInfo, ambariServerConfigurationHandler);
+  }
+
+  @Override
+  protected StackAdvisorCommandType getCommandType() {
+    return StackAdvisorCommandType.RECOMMEND_CONFIGURATIONS_FOR_KERBEROS;
+  }
+
+  @Override
+  protected void validate(StackAdvisorRequest request) throws StackAdvisorException {
+    if (request.getHosts() == null || request.getHosts().isEmpty() || request.getServices() == null
+        || request.getServices().isEmpty()) {
+      throw new StackAdvisorException(""Hosts and services must not be empty"");
+    }
+  }
+
+  @Override
+  protected RecommendationResponse updateResponse(StackAdvisorRequest request, RecommendationResponse response) {
+    response.getRecommendations().getBlueprint().setHostGroups(processHostGroups(request));
+    response.getRecommendations().getBlueprintClusterBinding().setHostGroups(processHostGroupBindings(request));
+    return response;
+  }
+
+  protected Set<HostGroup> processHostGroups(StackAdvisorRequest request) {
+    Set<HostGroup> resultSet = new HashSet<>();
+    for (Map.Entry<String, Set<String>> componentHost : request.getHostComponents().entrySet()) {
+      String hostGroupName = componentHost.getKey();
+      Set<String> components = componentHost.getValue();
+      if (hostGroupName != null && components != null) {
+        HostGroup hostGroup = new HostGroup();
+        Set<Map<String, String>> componentsSet = new HashSet<>();
+        for (String component : components) {
+          Map<String, String> componentMap = new HashMap<>();
+          componentMap.put(""name"", component);
+          componentsSet.add(componentMap);
+        }
+        hostGroup.setComponents(componentsSet);
+        hostGroup.setName(hostGroupName);
+        resultSet.add(hostGroup);
+      }
+    }
+    return resultSet;
+  }
+
+  private Set<BindingHostGroup> processHostGroupBindings(StackAdvisorRequest request) {","[{'comment': 'Can you doc the private/protected methods in here?', 'commenter': 'jonathan-hurley'}, {'comment': 'This was code copied from another class.. technically I should created an abstract class but some unit test was not happy with something about that.  I will try to figure out what the appropriate javadoc is and fix it in this or in the abstract class I create to provide reusable code. ', 'commenter': 'rlevas'}]"
1488,ambari-server/src/main/resources/stacks/stack_advisor.py,"@@ -1679,6 +1741,59 @@ def recommendConfigurationsForSSO(self, services, hosts):
 
     return recommendations
 
+  def recommendConfigurationsForKerberos(self, services, hosts):
+    self.services = services
+
+    stackName = services[""Versions""][""stack_name""]
+    stackVersion = services[""Versions""][""stack_version""]
+    hostsList = [host[""Hosts""][""host_name""] for host in hosts[""items""]]
+    servicesList = [service[""StackServices""][""service_name""] for service in services[""services""]]
+    components = [component[""StackServiceComponents""][""component_name""]
+                  for service in services[""services""]
+                  for component in service[""components""]]
+
+    clusterSummary = self.getConfigurationClusterSummary(servicesList, hosts, components, services)
+
+    recommendations = {
+      ""Versions"": {""stack_name"": stackName, ""stack_version"": stackVersion},
+      ""hosts"": hostsList,
+      ""services"": servicesList,
+      ""recommendations"": {
+        ""blueprint"": {
+          ""configurations"": {},
+          ""host_groups"": []
+        },
+        ""blueprint_cluster_binding"": {
+          ""host_groups"": []
+        }
+      }
+    }
+
+    # If recommendation for config groups
+    if ""config-groups"" in services:
+      self.recommendConfigGroupsConfigurations(recommendations, services, components, hosts,
+                                 servicesList)
+    else:
+      configurations = recommendations[""recommendations""][""blueprint""][""configurations""]","[{'comment': 'Is this structure always guaranteed to be here?', 'commenter': 'jonathan-hurley'}, {'comment': 'yes... at least that is what similar functions appear to believe as well.  This was copied from `recommendConfigurations` in the same python file. ', 'commenter': 'rlevas'}]"
1488,ambari-server/src/main/resources/stacks/stack_advisor.py,"@@ -341,6 +341,68 @@ def recommendConfigurationsForSSO(self, services, hosts):
     """"""
     pass
 
+  def recommendConfigurationsForKerberos(self, services, hosts):
+    """"""
+    Returns recommendation of Kerberos-related service configurations based on host-specific layout
+    of components.
+
+    This function takes as input all details about services being installed, and hosts
+    they are being installed into, to recommend host-specific configurations.
+
+    For backwards compatibility, this function redirects to recommendConfigurations. Implementations
+    should override this function to recommend Kerberos-specific property changes.
+
+    @type services: dictionary","[{'comment': 'in python  `dictionary` called  `dict`, to allow IDE type checking ', 'commenter': 'hapylestat'}]"
1488,ambari-server/src/main/resources/stacks/stack_advisor.py,"@@ -341,6 +341,68 @@ def recommendConfigurationsForSSO(self, services, hosts):
     """"""
     pass
 
+  def recommendConfigurationsForKerberos(self, services, hosts):
+    """"""
+    Returns recommendation of Kerberos-related service configurations based on host-specific layout
+    of components.
+
+    This function takes as input all details about services being installed, and hosts
+    they are being installed into, to recommend host-specific configurations.
+
+    For backwards compatibility, this function redirects to recommendConfigurations. Implementations
+    should override this function to recommend Kerberos-specific property changes.
+
+    @type services: dictionary
+    @param services: Dictionary containing all information about services and component layout selected by the user.
+    @type hosts: dictionary
+    @param hosts: Dictionary containing all information about hosts in this cluster","[{'comment': 'please use `reST` doc-string formatting instead of `Epytext`, as it is default for python and widely used in another python files within the project', 'commenter': 'hapylestat'}]"
1488,ambari-server/src/main/resources/stacks/stack_advisor.py,"@@ -1679,6 +1741,59 @@ def recommendConfigurationsForSSO(self, services, hosts):
 
     return recommendations
 
+  def recommendConfigurationsForKerberos(self, services, hosts):
+    self.services = services
+
+    stackName = services[""Versions""][""stack_name""]
+    stackVersion = services[""Versions""][""stack_version""]
+    hostsList = [host[""Hosts""][""host_name""] for host in hosts[""items""]]
+    servicesList = [service[""StackServices""][""service_name""] for service in services[""services""]]
+    components = [component[""StackServiceComponents""][""component_name""]","[{'comment': 'no, no, no...if you have such huge construction which is hardly readable with 2-cycles inside.....just use simple `for`. Generators generally r slower, while `for` is bit faster and more readable.  ', 'commenter': 'hapylestat'}, {'comment': 'This is copied code from `recommendConfigurations`...  I will attempt to fix.', 'commenter': 'rlevas'}]"
1488,ambari-server/src/main/resources/stacks/stack_advisor.py,"@@ -1679,6 +1741,59 @@ def recommendConfigurationsForSSO(self, services, hosts):
 
     return recommendations
 
+  def recommendConfigurationsForKerberos(self, services, hosts):
+    self.services = services
+
+    stackName = services[""Versions""][""stack_name""]
+    stackVersion = services[""Versions""][""stack_version""]
+    hostsList = [host[""Hosts""][""host_name""] for host in hosts[""items""]]
+    servicesList = [service[""StackServices""][""service_name""] for service in services[""services""]]
+    components = [component[""StackServiceComponents""][""component_name""]
+                  for service in services[""services""]
+                  for component in service[""components""]]
+
+    clusterSummary = self.getConfigurationClusterSummary(servicesList, hosts, components, services)
+
+    recommendations = {
+      ""Versions"": {""stack_name"": stackName, ""stack_version"": stackVersion},","[{'comment': 'lets format this as content below (not one line)', 'commenter': 'hapylestat'}]"
1497,ambari-server/src/main/java/org/apache/ambari/server/configuration/Configuration.java,"@@ -1866,14 +1866,28 @@
    */
   @Markdown(description = ""The maximum size of an incoming stomp text message. Default is 2 MB."")
   public static final ConfigurationProperty<Integer> STOMP_MAX_INCOMING_MESSAGE_SIZE = new ConfigurationProperty<>(
-      ""stomp.max.message.size"", 2*1024*1024);
+      ""stomp.max_incoming.message.size"", 2*1024*1024);
 
   /**
    * The maximum size of a buffer for stomp message sending. Default is 5 MB.
    */
   @Markdown(description = ""The maximum size of a buffer for stomp message sending. Default is 5 MB."")
   public static final ConfigurationProperty<Integer> STOMP_MAX_BUFFER_MESSAGE_SIZE = new ConfigurationProperty<>(
-      ""stomp.max.message.size"", 5*1024*1024);
+      ""stomp.max_buffer.message.size"", 5*1024*1024);
+
+  /**
+   * The number of attempts to emit execution command message to agent. Default is 4
+   */
+  @Markdown(description = ""The number of attempts to emit execution command message to agent. Default is 4"")
+  public static final ConfigurationProperty<Integer> EXECUTION_COMMANDS_RETRY_COUNT = new ConfigurationProperty<>(
+      ""stomp.commands.retry.count"", 4);","[{'comment': ""Minor nit: I would've named this as execution.command.retry.count instead of the protocol as config prefix."", 'commenter': 'swagle'}]"
1497,ambari-server/src/main/java/org/apache/ambari/server/events/DefaultMessageEmitter.java,"@@ -47,22 +52,55 @@
         put(STOMPEvent.Type.UPGRADE, ""/events/upgrade"");
         put(STOMPEvent.Type.AGENT_ACTIONS, ""/agent_actions"");
   }});
+  public static final Set<STOMPEvent.Type> DEFAULT_AGENT_TYPES =","[{'comment': 'Minor nit rename: DEFAULT_AGENT_EVENT_TYPES', 'commenter': 'swagle'}]"
1497,ambari-server/src/main/java/org/apache/ambari/server/events/MessageEmitter.java,"@@ -17,70 +17,248 @@
  */
 package org.apache.ambari.server.events;
 
+import java.util.concurrent.BlockingQueue;
+import java.util.concurrent.CancellationException;
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.concurrent.Future;
+import java.util.concurrent.LinkedBlockingQueue;
+import java.util.concurrent.ScheduledExecutorService;
+import java.util.concurrent.ScheduledFuture;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicLong;
+
 import org.apache.ambari.server.AmbariException;
 import org.apache.ambari.server.HostNotRegisteredException;
 import org.apache.ambari.server.agent.AgentSessionManager;
+import org.apache.ambari.server.agent.stomp.dto.AckReport;
+import org.apache.ambari.server.events.publishers.AmbariEventPublisher;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import org.springframework.messaging.MessageHeaders;
 import org.springframework.messaging.simp.SimpMessageHeaderAccessor;
 import org.springframework.messaging.simp.SimpMessageType;
 import org.springframework.messaging.simp.SimpMessagingTemplate;
 
+import com.google.common.eventbus.Subscribe;
+import com.google.common.util.concurrent.ThreadFactoryBuilder;
+
 /**
  * Is used to define a strategy for emitting message to subscribers.
  */
 public abstract class MessageEmitter {
   private final static Logger LOG = LoggerFactory.getLogger(MessageEmitter.class);
+
   protected final AgentSessionManager agentSessionManager;
   protected final SimpMessagingTemplate simpMessagingTemplate;
+  private AmbariEventPublisher ambariEventPublisher;
+
+  protected final ScheduledExecutorService emittersExecutor = Executors.newScheduledThreadPool(10,
+      new ThreadFactoryBuilder().setNameFormat(""agent-message-emitter-%d"").build());
+  protected final ExecutorService monitorsExecutor = Executors.newFixedThreadPool(10,
+      new ThreadFactoryBuilder().setNameFormat(""ambari-message-monitor-%d"").build());
+
+  protected static final AtomicLong MESSAGE_ID = new AtomicLong(0);
+  protected ConcurrentHashMap<Long, ScheduledFuture> unconfirmedMessages = new ConcurrentHashMap<>();
+  protected ConcurrentHashMap<Long, BlockingQueue<ExecutionCommandEvent>> messagesToEmit = new ConcurrentHashMap<>();
+
+  // is used to cancel agent queue check on unregistering
+  protected ConcurrentHashMap<Long, Future> monitors = new ConcurrentHashMap<>();
 
+  public final int retryCount;
+  public final int retryInterval;
 
-  public MessageEmitter(AgentSessionManager agentSessionManager, SimpMessagingTemplate simpMessagingTemplate) {
+  public MessageEmitter(AgentSessionManager agentSessionManager, SimpMessagingTemplate simpMessagingTemplate,
+                        AmbariEventPublisher ambariEventPublisher, int retryCount, int retryInterval) {
     this.agentSessionManager = agentSessionManager;
     this.simpMessagingTemplate = simpMessagingTemplate;
+    this.ambariEventPublisher = ambariEventPublisher;
+    this.retryCount = retryCount;
+    this.retryInterval = retryInterval;
+    ambariEventPublisher.register(this);
   }
 
   /**
    * Determines destinations and emits message.
    * @param event message should to be emitted.
    * @throws AmbariException
    */
-  abstract void emitMessage(STOMPEvent event) throws AmbariException;
+  abstract void emitMessage(STOMPEvent event) throws AmbariException, InterruptedException;
+
+  public void emitMessageRetriable(ExecutionCommandEvent event) throws AmbariException, InterruptedException {
+    // set message identifier used to recognize NACK/ACK agent response
+    event.setMessageId(MESSAGE_ID.getAndIncrement());
+
+    Long hostId = event.getHostId();
+    if (!messagesToEmit.containsKey(hostId)) {
+      LOG.error(""Trying to emit message to unregistered host with id {}"", hostId);
+      return;
+    }
+    messagesToEmit.get(hostId).add(event);
+  }
+
+  private class MessagesToEmitMonitor implements Runnable {
+
+    private final Long hostId;
+
+    public MessagesToEmitMonitor(Long hostId) {
+      this.hostId = hostId;
+    }
+
+    @Override
+    public void run() {
+      while (true) {
+        try {
+          ExecutionCommandEvent event = messagesToEmit.get(hostId).take();
+          EmitMessageTask emitMessageTask = new EmitMessageTask(event);
+          ScheduledFuture scheduledFuture =
+              emittersExecutor.scheduleAtFixedRate(emitMessageTask,
+                  0, retryInterval, TimeUnit.SECONDS);
+          emitMessageTask.setScheduledFuture(scheduledFuture);
+          unconfirmedMessages.put(event.getMessageId(), scheduledFuture);
+
+          scheduledFuture.get();
+        } catch (InterruptedException e) {
+          // can be interrupted when no responses were received from agent and HEARTBEAT_LOST will be fired
+          return;
+        } catch (CancellationException e) {
+          // scheduled tasks can be canceled
+          e.printStackTrace();","[{'comment': 'Should this not goto log file instead of out?', 'commenter': 'swagle'}]"
1497,ambari-server/src/main/java/org/apache/ambari/server/events/MessageEmitter.java,"@@ -17,70 +17,248 @@
  */
 package org.apache.ambari.server.events;
 
+import java.util.concurrent.BlockingQueue;
+import java.util.concurrent.CancellationException;
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.concurrent.Future;
+import java.util.concurrent.LinkedBlockingQueue;
+import java.util.concurrent.ScheduledExecutorService;
+import java.util.concurrent.ScheduledFuture;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicLong;
+
 import org.apache.ambari.server.AmbariException;
 import org.apache.ambari.server.HostNotRegisteredException;
 import org.apache.ambari.server.agent.AgentSessionManager;
+import org.apache.ambari.server.agent.stomp.dto.AckReport;
+import org.apache.ambari.server.events.publishers.AmbariEventPublisher;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import org.springframework.messaging.MessageHeaders;
 import org.springframework.messaging.simp.SimpMessageHeaderAccessor;
 import org.springframework.messaging.simp.SimpMessageType;
 import org.springframework.messaging.simp.SimpMessagingTemplate;
 
+import com.google.common.eventbus.Subscribe;
+import com.google.common.util.concurrent.ThreadFactoryBuilder;
+
 /**
  * Is used to define a strategy for emitting message to subscribers.
  */
 public abstract class MessageEmitter {
   private final static Logger LOG = LoggerFactory.getLogger(MessageEmitter.class);
+
   protected final AgentSessionManager agentSessionManager;
   protected final SimpMessagingTemplate simpMessagingTemplate;
+  private AmbariEventPublisher ambariEventPublisher;
+
+  protected final ScheduledExecutorService emittersExecutor = Executors.newScheduledThreadPool(10,
+      new ThreadFactoryBuilder().setNameFormat(""agent-message-emitter-%d"").build());
+  protected final ExecutorService monitorsExecutor = Executors.newFixedThreadPool(10,
+      new ThreadFactoryBuilder().setNameFormat(""ambari-message-monitor-%d"").build());
+
+  protected static final AtomicLong MESSAGE_ID = new AtomicLong(0);
+  protected ConcurrentHashMap<Long, ScheduledFuture> unconfirmedMessages = new ConcurrentHashMap<>();
+  protected ConcurrentHashMap<Long, BlockingQueue<ExecutionCommandEvent>> messagesToEmit = new ConcurrentHashMap<>();
+
+  // is used to cancel agent queue check on unregistering
+  protected ConcurrentHashMap<Long, Future> monitors = new ConcurrentHashMap<>();
 
+  public final int retryCount;
+  public final int retryInterval;
 
-  public MessageEmitter(AgentSessionManager agentSessionManager, SimpMessagingTemplate simpMessagingTemplate) {
+  public MessageEmitter(AgentSessionManager agentSessionManager, SimpMessagingTemplate simpMessagingTemplate,
+                        AmbariEventPublisher ambariEventPublisher, int retryCount, int retryInterval) {
     this.agentSessionManager = agentSessionManager;
     this.simpMessagingTemplate = simpMessagingTemplate;
+    this.ambariEventPublisher = ambariEventPublisher;
+    this.retryCount = retryCount;
+    this.retryInterval = retryInterval;
+    ambariEventPublisher.register(this);
   }
 
   /**
    * Determines destinations and emits message.
    * @param event message should to be emitted.
    * @throws AmbariException
    */
-  abstract void emitMessage(STOMPEvent event) throws AmbariException;
+  abstract void emitMessage(STOMPEvent event) throws AmbariException, InterruptedException;
+
+  public void emitMessageRetriable(ExecutionCommandEvent event) throws AmbariException, InterruptedException {
+    // set message identifier used to recognize NACK/ACK agent response
+    event.setMessageId(MESSAGE_ID.getAndIncrement());
+
+    Long hostId = event.getHostId();
+    if (!messagesToEmit.containsKey(hostId)) {
+      LOG.error(""Trying to emit message to unregistered host with id {}"", hostId);
+      return;
+    }
+    messagesToEmit.get(hostId).add(event);
+  }
+
+  private class MessagesToEmitMonitor implements Runnable {
+
+    private final Long hostId;
+
+    public MessagesToEmitMonitor(Long hostId) {
+      this.hostId = hostId;
+    }
+
+    @Override
+    public void run() {
+      while (true) {
+        try {
+          ExecutionCommandEvent event = messagesToEmit.get(hostId).take();
+          EmitMessageTask emitMessageTask = new EmitMessageTask(event);
+          ScheduledFuture scheduledFuture =
+              emittersExecutor.scheduleAtFixedRate(emitMessageTask,
+                  0, retryInterval, TimeUnit.SECONDS);
+          emitMessageTask.setScheduledFuture(scheduledFuture);
+          unconfirmedMessages.put(event.getMessageId(), scheduledFuture);
+
+          scheduledFuture.get();","[{'comment': 'Can we afford to wait without timeout here? The monitors could get exhausted, right?', 'commenter': 'swagle'}]"
1497,ambari-server/src/main/java/org/apache/ambari/server/events/MessageEmitter.java,"@@ -17,70 +17,248 @@
  */
 package org.apache.ambari.server.events;
 
+import java.util.concurrent.BlockingQueue;
+import java.util.concurrent.CancellationException;
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.concurrent.Future;
+import java.util.concurrent.LinkedBlockingQueue;
+import java.util.concurrent.ScheduledExecutorService;
+import java.util.concurrent.ScheduledFuture;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicLong;
+
 import org.apache.ambari.server.AmbariException;
 import org.apache.ambari.server.HostNotRegisteredException;
 import org.apache.ambari.server.agent.AgentSessionManager;
+import org.apache.ambari.server.agent.stomp.dto.AckReport;
+import org.apache.ambari.server.events.publishers.AmbariEventPublisher;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import org.springframework.messaging.MessageHeaders;
 import org.springframework.messaging.simp.SimpMessageHeaderAccessor;
 import org.springframework.messaging.simp.SimpMessageType;
 import org.springframework.messaging.simp.SimpMessagingTemplate;
 
+import com.google.common.eventbus.Subscribe;
+import com.google.common.util.concurrent.ThreadFactoryBuilder;
+
 /**
  * Is used to define a strategy for emitting message to subscribers.
  */
 public abstract class MessageEmitter {
   private final static Logger LOG = LoggerFactory.getLogger(MessageEmitter.class);
+
   protected final AgentSessionManager agentSessionManager;
   protected final SimpMessagingTemplate simpMessagingTemplate;
+  private AmbariEventPublisher ambariEventPublisher;
+
+  protected final ScheduledExecutorService emittersExecutor = Executors.newScheduledThreadPool(10,
+      new ThreadFactoryBuilder().setNameFormat(""agent-message-emitter-%d"").build());
+  protected final ExecutorService monitorsExecutor = Executors.newFixedThreadPool(10,
+      new ThreadFactoryBuilder().setNameFormat(""ambari-message-monitor-%d"").build());
+
+  protected static final AtomicLong MESSAGE_ID = new AtomicLong(0);
+  protected ConcurrentHashMap<Long, ScheduledFuture> unconfirmedMessages = new ConcurrentHashMap<>();
+  protected ConcurrentHashMap<Long, BlockingQueue<ExecutionCommandEvent>> messagesToEmit = new ConcurrentHashMap<>();
+
+  // is used to cancel agent queue check on unregistering
+  protected ConcurrentHashMap<Long, Future> monitors = new ConcurrentHashMap<>();
 
+  public final int retryCount;
+  public final int retryInterval;
 
-  public MessageEmitter(AgentSessionManager agentSessionManager, SimpMessagingTemplate simpMessagingTemplate) {
+  public MessageEmitter(AgentSessionManager agentSessionManager, SimpMessagingTemplate simpMessagingTemplate,
+                        AmbariEventPublisher ambariEventPublisher, int retryCount, int retryInterval) {
     this.agentSessionManager = agentSessionManager;
     this.simpMessagingTemplate = simpMessagingTemplate;
+    this.ambariEventPublisher = ambariEventPublisher;
+    this.retryCount = retryCount;
+    this.retryInterval = retryInterval;
+    ambariEventPublisher.register(this);
   }
 
   /**
    * Determines destinations and emits message.
    * @param event message should to be emitted.
    * @throws AmbariException
    */
-  abstract void emitMessage(STOMPEvent event) throws AmbariException;
+  abstract void emitMessage(STOMPEvent event) throws AmbariException, InterruptedException;
+
+  public void emitMessageRetriable(ExecutionCommandEvent event) throws AmbariException, InterruptedException {
+    // set message identifier used to recognize NACK/ACK agent response
+    event.setMessageId(MESSAGE_ID.getAndIncrement());
+
+    Long hostId = event.getHostId();
+    if (!messagesToEmit.containsKey(hostId)) {
+      LOG.error(""Trying to emit message to unregistered host with id {}"", hostId);
+      return;
+    }
+    messagesToEmit.get(hostId).add(event);
+  }
+
+  private class MessagesToEmitMonitor implements Runnable {
+
+    private final Long hostId;
+
+    public MessagesToEmitMonitor(Long hostId) {
+      this.hostId = hostId;
+    }
+
+    @Override
+    public void run() {
+      while (true) {
+        try {
+          ExecutionCommandEvent event = messagesToEmit.get(hostId).take();
+          EmitMessageTask emitMessageTask = new EmitMessageTask(event);
+          ScheduledFuture scheduledFuture =
+              emittersExecutor.scheduleAtFixedRate(emitMessageTask,
+                  0, retryInterval, TimeUnit.SECONDS);
+          emitMessageTask.setScheduledFuture(scheduledFuture);
+          unconfirmedMessages.put(event.getMessageId(), scheduledFuture);
+
+          scheduledFuture.get();
+        } catch (InterruptedException e) {
+          // can be interrupted when no responses were received from agent and HEARTBEAT_LOST will be fired
+          return;
+        } catch (CancellationException e) {
+          // scheduled tasks can be canceled
+          e.printStackTrace();
+        } catch (ExecutionException e) {
+          e.printStackTrace();
+          // generate delivery failed event
+          ambariEventPublisher.publish(new HeartbeatLostEvent(hostId));
+          return;
+        }
+      }
+    }
+  }
+
+  public void processReceiveReport(AckReport ackReport) {
+    Long messageId = ackReport.getMessageId();
+    if (AckReport.AckStatus.OK.equals(ackReport.getStatus())) {
+      if (unconfirmedMessages.containsKey(messageId)) {
+        unconfirmedMessages.get(messageId).cancel(true);
+        unconfirmedMessages.remove(messageId);
+      } else {
+        LOG.warn(""OK agent report was received again for already complete command with message id {}"", messageId);
+      }
+    } else {
+      LOG.error(""Received {} agent report for execution command with messageId {} with following reason: {}"",
+          ackReport.getStatus(), messageId, ackReport.getReason());
+    }
+  }
+
+  private class EmitMessageTask implements Runnable {
+
+    private final ExecutionCommandEvent executionCommandEvent;
+    private ScheduledFuture scheduledFuture;
+    private int retry_counter = 0;
+
+    public EmitMessageTask(ExecutionCommandEvent executionCommandEvent) {
+      this.executionCommandEvent = executionCommandEvent;
+    }
+
+    public void setScheduledFuture(ScheduledFuture scheduledFuture) {
+      this.scheduledFuture = scheduledFuture;
+    }
+
+    @Override
+    public void run() {
+      if (retry_counter >= retryCount) {
+        // generate delivery failed event and cancel emitter
+        ambariEventPublisher.publish(new HeartbeatLostEvent(executionCommandEvent.getHostId()));","[{'comment': ""Are we overloading the HB lost event here? Why don't we have a CommandFailedEvent here? "", 'commenter': 'swagle'}]"
1502,ambari-server/src/main/java/org/apache/ambari/server/view/ViewRegistry.java,"@@ -1135,6 +1135,12 @@ private boolean checkAutoInstanceConfig(AutoInstanceConfig autoConfig, StackId s
 
         String configStackId = autoConfig.getStackId();
 
+        if (configStackId.equals(""*"")) {","[{'comment': 'Please move this inside the `if (configStackId != null)` block to prevent NPE if `configStackId` is missing.  (But keep it before the `new StackId(configStackId)` call, since `*` cannot  be parsed as `StackId`).', 'commenter': 'adoroszlai'}, {'comment': 'Or just use StringUtils.equals() :)', 'commenter': 'jonathan-hurley'}, {'comment': ""Hi @adoroszlai , @jonathan-hurley ,\r\n\r\nThanks for catching this.  I've  updated the patch to account for this, and have uploaded to the review.  "", 'commenter': 'rnettleton'}]"
1519,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/ServiceGroupResourceProvider.java,"@@ -322,7 +322,7 @@ private ServiceGroupRequest getRequest(Map<String, Object> properties) {
   }
 
   // Get services from the given set of requests.
-  protected Set<ServiceGroupResponse> getServiceGroups(Set<ServiceGroupRequest> requests)
+  public Set<ServiceGroupResponse> getServiceGroups(Set<ServiceGroupRequest> requests)
     throws AmbariException {
     Set<ServiceGroupResponse> response = new HashSet<>();
     for (ServiceGroupRequest request : requests) {","[{'comment': ""I've seen another example making such methods public. The drawback with making it public is that it will bypass authentication. On the other hand, it makes sense to call the object oriented methods of resource managers internally instead of calling the tedious hashmap based ones. @rnettleton @adoroszlai what do you think? "", 'commenter': 'benyoka'}, {'comment': 'While I see the convenience of making these methods public for easy consumption by internal services, I am a little worried that this bypasses authentication.\r\n\r\nCan you ask @rlevas to review this change, since I think he implemented the authentication support in this area of the Ambari server-side code?  ', 'commenter': 'rnettleton'}, {'comment': 'I am fine with this, but it seems like getServiceGroups should be moved to some internal business logic class rather than remain in this class that handles API calls.   ', 'commenter': 'rlevas'}]"
1519,ambari-server/src/main/java/org/apache/ambari/server/state/cluster/ClusterImpl.java,"@@ -1610,14 +1610,9 @@ public void addConfig(Config config, Long serviceId) {
     }
     clusterGlobalLock.writeLock().lock();
     try {
-      if (!serviceConfigs.containsKey(serviceId)) {
-        ConcurrentMap<String, ConcurrentMap<String, Config>> allServiceConfigs = new ConcurrentHashMap<>();
-        serviceConfigs.put(serviceId, allServiceConfigs);
-      }
-      ConcurrentMap<String, ConcurrentMap<String, Config>> allServiceConfigs = serviceConfigs.get(serviceId);
-      allServiceConfigs.put(config.getType(), new ConcurrentHashMap<>());
-      allServiceConfigs.get(config.getType()).put(config.getTag(), config);
-      serviceConfigs.put(serviceId,allServiceConfigs);
+      serviceConfigs.computeIfAbsent(serviceId, __ -> new ConcurrentHashMap<>()).
+        computeIfAbsent(config.getType(), __ -> new ConcurrentHashMap<>()).
+        put(config.getTag(), config);
     } finally {","[{'comment': 'The previous version was broken as it overrode existing configuration with the new one with different tag.', 'commenter': 'benyoka'}]"
1519,ambari-server/src/main/java/org/apache/ambari/server/topology/tasks/ConfigureClusterTask.java,"@@ -70,12 +70,19 @@ public Boolean call() throws Exception {
 
     Collection<String> requiredHostGroups = getTopologyRequiredHostGroups();
 
+    String msg = null;
     if (!areHostGroupsResolved(requiredHostGroups)) {
-      String msg = ""Some host groups require more hosts, cluster configuration cannot begin"";
+      msg = ""Some host groups require more hosts, cluster configuration cannot begin"";
+    }
+      else if (topology.getConfigRecommendationStrategy().shouldUseAdvisor() && topology.getHostNames().isEmpty()) {
+      msg = ""Getting config recommendations requires at leas one host, cluster configuration cannot begin"";","[{'comment': 'Minor typo in the string value: \r\n\r\n""at leas""  should be ""at least"" \r\n\r\n', 'commenter': 'rnettleton'}, {'comment': 'Ok', 'commenter': 'benyoka'}]"
1519,ambari-server/src/main/java/org/apache/ambari/server/state/cluster/ClusterImpl.java,"@@ -1610,14 +1610,9 @@ public void addConfig(Config config, Long serviceId) {
     }
     clusterGlobalLock.writeLock().lock();
     try {
-      if (!serviceConfigs.containsKey(serviceId)) {
-        ConcurrentMap<String, ConcurrentMap<String, Config>> allServiceConfigs = new ConcurrentHashMap<>();
-        serviceConfigs.put(serviceId, allServiceConfigs);
-      }
-      ConcurrentMap<String, ConcurrentMap<String, Config>> allServiceConfigs = serviceConfigs.get(serviceId);
-      allServiceConfigs.put(config.getType(), new ConcurrentHashMap<>());
-      allServiceConfigs.get(config.getType()).put(config.getTag(), config);
-      serviceConfigs.put(serviceId,allServiceConfigs);
+      serviceConfigs.computeIfAbsent(serviceId, __ -> new ConcurrentHashMap<>()).
+        computeIfAbsent(config.getType(), __ -> new ConcurrentHashMap<>()).
+        put(config.getTag(), config);","[{'comment': 'Can you please put `.` at the start of the line instead of the end?  Ie.\r\n\r\n```\r\n.computeIfAbsent(...)\r\n.put(...)\r\n```', 'commenter': 'adoroszlai'}, {'comment': 'ok', 'commenter': 'benyoka'}]"
1519,ambari-server/src/main/java/org/apache/ambari/server/topology/ClusterConfigurationRequest.java,"@@ -145,7 +152,7 @@ public void process() throws AmbariException, ConfigurationTopologyException {
       }
 
       // obtain recommended configurations before config updates
-      if (!ConfigRecommendationStrategy.NEVER_APPLY.equals(this.clusterTopology.getConfigRecommendationStrategy())) {
+      if (clusterTopology.getConfigRecommendationStrategy().shouldUseAdvisor()) {","[{'comment': 'The existing check is `null`-safe, the new one is not.  Can you please make sure `clusterTopology.getConfigRecommendationStrategy()` never returns `null` (even when using mocks)?', 'commenter': 'adoroszlai'}, {'comment': 'Null check is intentionally ignored. I checked the code path and null is never expected. I added an additional guarantee in ClusterImpl.\r\nThere is now way to guaranteee this behavior for mocks (it is the responsibility for the writer of the mock to set up proper mocks).', 'commenter': 'benyoka'}, {'comment': 'I meant to ask you to verify that existing tests are not broken by this.', 'commenter': 'adoroszlai'}, {'comment': 'This is verified (I usually run a test suite on the feature branch and check if the number of failures increases with my changes)', 'commenter': 'benyoka'}, {'comment': 'Cool, thanks.', 'commenter': 'adoroszlai'}]"
1519,ambari-server/src/main/java/org/apache/ambari/server/topology/ClusterConfigurationRequest.java,"@@ -334,46 +341,84 @@ private Cluster getCluster() throws AmbariException {
    * @param clusterTopology  cluster topology
    * @param tag              config tag
    */
-  public void setConfigurationsOnCluster(ClusterTopology clusterTopology, String tag, Set<String> updatedConfigTypes)  {
+  public void setConfigurationsOnCluster(ClusterTopology clusterTopology, String tag, Set<String> updatedConfigTypes) {
+    // TODO: This version works with Ambari 3.0 where it is assumed that any service with a configuration can be identified
+    //   by its name. Even though the cluster is multi-stack (multi-mpack), service names should not conflict across mpacks,
+    //   except client services which have no configuration. In 3.1, mpack may have conflicting service names
     //todo: also handle setting of host group scoped configuration which is updated by config processor
-    List<BlueprintServiceConfigRequest> configurationRequests = new LinkedList<>();
+    List<Pair<String, ClusterRequest>> serviceNamesAndConfigurationRequests = new ArrayList<>();
 
     Configuration clusterConfiguration = clusterTopology.getConfiguration();
 
-    for (String service : clusterTopology.getServices()) {
-      //todo: remove intermediate request type
-      // one bp config request per service
-      BlueprintServiceConfigRequest blueprintConfigRequest = new BlueprintServiceConfigRequest(service);
-
-      for (String serviceConfigType : stack.getAllConfigurationTypes(service)) {
-        Set<String> excludedConfigTypes = stack.getExcludedConfigurationTypes(service);
-        if (!excludedConfigTypes.contains(serviceConfigType)) {
-          // skip handling of cluster-env here
-          if (! serviceConfigType.equals(""cluster-env"")) {
-            if (clusterConfiguration.getFullProperties().containsKey(serviceConfigType)) {
-              blueprintConfigRequest.addConfigElement(serviceConfigType,
-                  clusterConfiguration.getFullProperties().get(serviceConfigType),
-                  clusterConfiguration.getFullAttributes().get(serviceConfigType));
-            }
-          }
-        }
+    final Set<String> clusterConfigTypes = clusterConfiguration.getFullProperties().keySet();
+    final Set<String> globalConfigTypes = ImmutableSet.of(""cluster-env"");
+
+    // TODO: do we need to handle security type? In the previous version it was handled but in a broken way
+    for (ServiceResponse service : ambariContext.getServices(clusterTopology.getClusterName())) {
+      ClusterRequest clusterRequest =
+        new ClusterRequest(clusterTopology.getClusterId(), clusterTopology.getClusterName(), null, null, null, null);
+      clusterRequest.setDesiredConfig(new ArrayList<>());
+
+      Set<String> configTypes =
+        Sets.difference(
+          Sets.intersection(stack.getAllConfigurationTypes(service.getServiceName()), clusterConfigTypes),
+          Sets.union(stack.getExcludedConfigurationTypes(service.getServiceName()), globalConfigTypes)
+        );
+
+      for (String serviceConfigType: configTypes) {
+        Map<String, String> properties = clusterConfiguration.getFullProperties().get(serviceConfigType);
+        Map<String, Map<String, String>> attributes = clusterConfiguration.getFullAttributes().get(serviceConfigType);","[{'comment': ""Please call `getFullProperties` and `getFullAttributes` only once, as these are costly.  (Unless of course the changes need to be taken up in each iteration.  But I don't think that's the case, since a different `configType` is being processed each time.)"", 'commenter': 'adoroszlai'}, {'comment': 'Ok', 'commenter': 'benyoka'}]"
1519,ambari-server/src/main/java/org/apache/ambari/server/topology/ClusterConfigurationRequest.java,"@@ -334,46 +341,84 @@ private Cluster getCluster() throws AmbariException {
    * @param clusterTopology  cluster topology
    * @param tag              config tag
    */
-  public void setConfigurationsOnCluster(ClusterTopology clusterTopology, String tag, Set<String> updatedConfigTypes)  {
+  public void setConfigurationsOnCluster(ClusterTopology clusterTopology, String tag, Set<String> updatedConfigTypes) {
+    // TODO: This version works with Ambari 3.0 where it is assumed that any service with a configuration can be identified
+    //   by its name. Even though the cluster is multi-stack (multi-mpack), service names should not conflict across mpacks,
+    //   except client services which have no configuration. In 3.1, mpack may have conflicting service names
     //todo: also handle setting of host group scoped configuration which is updated by config processor
-    List<BlueprintServiceConfigRequest> configurationRequests = new LinkedList<>();
+    List<Pair<String, ClusterRequest>> serviceNamesAndConfigurationRequests = new ArrayList<>();
 
     Configuration clusterConfiguration = clusterTopology.getConfiguration();
 
-    for (String service : clusterTopology.getServices()) {
-      //todo: remove intermediate request type
-      // one bp config request per service
-      BlueprintServiceConfigRequest blueprintConfigRequest = new BlueprintServiceConfigRequest(service);
-
-      for (String serviceConfigType : stack.getAllConfigurationTypes(service)) {
-        Set<String> excludedConfigTypes = stack.getExcludedConfigurationTypes(service);
-        if (!excludedConfigTypes.contains(serviceConfigType)) {
-          // skip handling of cluster-env here
-          if (! serviceConfigType.equals(""cluster-env"")) {
-            if (clusterConfiguration.getFullProperties().containsKey(serviceConfigType)) {
-              blueprintConfigRequest.addConfigElement(serviceConfigType,
-                  clusterConfiguration.getFullProperties().get(serviceConfigType),
-                  clusterConfiguration.getFullAttributes().get(serviceConfigType));
-            }
-          }
-        }
+    final Set<String> clusterConfigTypes = clusterConfiguration.getFullProperties().keySet();
+    final Set<String> globalConfigTypes = ImmutableSet.of(""cluster-env"");
+
+    // TODO: do we need to handle security type? In the previous version it was handled but in a broken way
+    for (ServiceResponse service : ambariContext.getServices(clusterTopology.getClusterName())) {
+      ClusterRequest clusterRequest =
+        new ClusterRequest(clusterTopology.getClusterId(), clusterTopology.getClusterName(), null, null, null, null);
+      clusterRequest.setDesiredConfig(new ArrayList<>());
+
+      Set<String> configTypes =
+        Sets.difference(
+          Sets.intersection(stack.getAllConfigurationTypes(service.getServiceName()), clusterConfigTypes),
+          Sets.union(stack.getExcludedConfigurationTypes(service.getServiceName()), globalConfigTypes)
+        );
+
+      for (String serviceConfigType: configTypes) {
+        Map<String, String> properties = clusterConfiguration.getFullProperties().get(serviceConfigType);
+        Map<String, Map<String, String>> attributes = clusterConfiguration.getFullAttributes().get(serviceConfigType);
+
+        removeNullValues(properties, attributes);
+
+        ConfigurationRequest configurationRequest = new ConfigurationRequest(clusterTopology.getClusterName(),
+          serviceConfigType,
+          tag,
+          properties,
+          attributes,
+          service.getServiceId(),
+          service.getServiceGroupId());
+        clusterRequest.getDesiredConfig().add(configurationRequest);
       }
-
-      configurationRequests.add(blueprintConfigRequest);
+      serviceNamesAndConfigurationRequests.add(Pair.of(service.getServiceName(), clusterRequest));
     }
 
     // since the stack returns ""cluster-env"" with each service's config ensure that only one
     // ClusterRequest occurs for the global cluster-env configuration
-    BlueprintServiceConfigRequest globalConfigRequest = new BlueprintServiceConfigRequest(""GLOBAL-CONFIG"");
+    ClusterRequest globalConfigClusterRequest =
+      new ClusterRequest(clusterTopology.getClusterId(), clusterTopology.getClusterName(), null, null, null, null);
+
     Map<String, String> clusterEnvProps = clusterConfiguration.getFullProperties().get(""cluster-env"");
     Map<String, Map<String, String>> clusterEnvAttributes = clusterConfiguration.getFullAttributes().get(""cluster-env"");
 
-    globalConfigRequest.addConfigElement(""cluster-env"", clusterEnvProps,clusterEnvAttributes);
-    configurationRequests.add(globalConfigRequest);
+    removeNullValues(clusterEnvProps, clusterEnvAttributes);
+
+    ConfigurationRequest globalConfigurationRequest = new ConfigurationRequest(clusterTopology.getClusterName(),
+      ""cluster-env"",
+      tag,
+      clusterEnvProps,
+      clusterEnvAttributes,
+      null,
+      null);
+    globalConfigClusterRequest.setDesiredConfig(Lists.newArrayList(globalConfigurationRequest));
+    serviceNamesAndConfigurationRequests.add(Pair.of(""GLOBAL-CONFIG"", globalConfigClusterRequest));
+
+    // send configurations
+    setConfigurationsOnCluster(serviceNamesAndConfigurationRequests, tag, updatedConfigTypes);
+  }
 
-    setConfigurationsOnCluster(configurationRequests, tag, updatedConfigTypes);
+  private void removeNullValues(Map<String, String> configProperties, Map<String, Map<String, String>> configAttributes) {
+    if (null != configProperties) {
+      configProperties.values().removeIf(Objects::isNull);
+    }
+    if (null != configAttributes) {
+      configAttributes.values().removeIf(Objects::isNull);
+      configAttributes.values().forEach(map -> map.values().removeIf(Objects::isNull));
+      configAttributes.entrySet().removeIf(e -> e.getValue().isEmpty());","[{'comment': 'I think `values()` could be used here, too, instead of `entrySet()`.', 'commenter': 'adoroszlai'}, {'comment': 'Ok', 'commenter': 'benyoka'}]"
1519,ambari-server/src/main/java/org/apache/ambari/server/topology/ClusterConfigurationRequest.java,"@@ -478,12 +458,16 @@ private void setConfigurationsOnCluster(List<BlueprintServiceConfigRequest> conf
   private static class BlueprintServiceConfigRequest {
 
     private final String serviceName;
+    private final Long serviceId;
+    private final Long serviceGroupId;
 
     private List<BlueprintServiceConfigElement> configElements =
       new LinkedList<>();
 
-    BlueprintServiceConfigRequest(String serviceName) {
+    BlueprintServiceConfigRequest(String serviceName, Long serviceId, Long serviceGroupId) {","[{'comment': ""Can this class be removed?  I don't see it being used anymore."", 'commenter': 'adoroszlai'}, {'comment': 'Ok. Good catch.', 'commenter': 'benyoka'}]"
1532,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/UpgradeResourceProvider.java,"@@ -1654,8 +1654,11 @@ private ActionExecutionContext buildActionExecutionContext(Cluster cluster,
     actionContext.setStackId(stackId);
     actionContext.setTimeout(timeout);
     actionContext.setRetryAllowed(allowRetry);
-    actionContext.setAutoSkipFailures(context.isComponentFailureAutoSkipped());
-
+    if (StageWrapper.Type.SERVICE_CHECK.name().equals(role)) {","[{'comment': ""I wouldn't do this check here. I'd just set it directly in `makeServiceCheckStage(...)`. "", 'commenter': 'jonathan-hurley'}, {'comment': ""Actually, I'm OK with this change, but I'm worried that `makeServiceCheckStage(...)` takes a skip boolean too. We should try do set it all in one place."", 'commenter': 'jonathan-hurley'}]"
1556,ambari-server/src/main/java/org/apache/ambari/server/topology/ClusterTopologyImpl.java,"@@ -111,11 +113,42 @@ public ClusterTopologyImpl(
     stack = request.getStack();
     setting = request.getSetting();
     blueprint.getConfiguration().setParentConfiguration(stack.getConfiguration(getServices()));
+
+    checkForDuplicateHosts(request.getHostGroupInfo());
     registerHostGroupInfo(request.getHostGroupInfo());
   }
 
+  ClusterTopologyImpl copy() {
+    try {
+      return new ClusterTopologyImpl(ambariContext, provisionRequest, resolvedComponents);
+    } catch (InvalidTopologyException e) {
+      // cannot happen, since already validated
+      LOG.error(""Failed to copy cluster topology {}"", this);
+      return null;
+    }","[{'comment': 'Throwing an IllegalStateException instead of returning null?', 'commenter': 'benyoka'}, {'comment': ""Actually this method was not used, so I've removed it."", 'commenter': 'adoroszlai'}]"
1556,ambari-server/src/main/java/org/apache/ambari/server/topology/ClusterTopologyImpl.java,"@@ -111,11 +113,42 @@ public ClusterTopologyImpl(
     stack = request.getStack();
     setting = request.getSetting();
     blueprint.getConfiguration().setParentConfiguration(stack.getConfiguration(getServices()));
+
+    checkForDuplicateHosts(request.getHostGroupInfo());
     registerHostGroupInfo(request.getHostGroupInfo());
   }
 
+  ClusterTopologyImpl copy() {","[{'comment': 'What is the reason for making shallow copies? (At least some of) the data is mutable, so e.g. changing the configuration in the new instance will change it in the old instance too.', 'commenter': 'benyoka'}, {'comment': ""Actually this method was not used, so I've removed it."", 'commenter': 'adoroszlai'}]"
1556,ambari-server/src/main/java/org/apache/ambari/server/topology/ClusterTopology.java,"@@ -225,10 +231,13 @@
    */
   boolean containsMasterComponent(String hostGroup);
 
-  Collection<HostGroup> getHostGroups();
+  Set<String> getHostGroups();
 
   /*
    * @return true if the given component belongs to a service that has serviceType=HCFS
    */
   boolean isComponentHadoopCompatible(String component);
+
+  ClusterTopology withAdditionalComponents(Map<String, Set<ResolvedComponent>> additionalComponents) throws InvalidTopologyException;","[{'comment': 'Could you add some javadocs for this new method?  ', 'commenter': 'rnettleton'}]"
1556,ambari-server/src/main/java/org/apache/ambari/server/topology/ResolvedComponent_Builder.java,"@@ -536,40 +548,52 @@ public String componentName() {
     }
 
     @Override
-    public boolean masterComponent() {
-      return masterComponent;
+    public ServiceInfo serviceInfo() {
+      return serviceInfo;
+    }
+
+    @Override
+    public ComponentInfo componentInfo() {
+      return componentInfo;
     }
 
     @Override
     public Component component() {
       return component;
     }
 
+    @Override
+    public ResolvedComponent.Builder toBuilder() {
+      return new ResolvedComponent.Builder().mergeFrom(this);
+    }
+
     @Override
     public boolean equals(Object obj) {
       if (!(obj instanceof ResolvedComponent_Builder.Value)) {
         return false;
       }
       ResolvedComponent_Builder.Value other = (ResolvedComponent_Builder.Value) obj;
       return Objects.equals(stackId, other.stackId)
-          && Objects.equals(serviceGroupName, other.serviceGroupName)
-          && Objects.equals(serviceType, other.serviceType)
-          && Objects.equals(serviceName, other.serviceName)
+          //&& Objects.equals(serviceGroupName, other.serviceGroupName)","[{'comment': 'Please remove these commented-out lines.  ', 'commenter': 'rnettleton'}]"
1556,ambari-server/src/main/java/org/apache/ambari/server/topology/ResolvedComponent_Builder.java,"@@ -536,40 +548,52 @@ public String componentName() {
     }
 
     @Override
-    public boolean masterComponent() {
-      return masterComponent;
+    public ServiceInfo serviceInfo() {
+      return serviceInfo;
+    }
+
+    @Override
+    public ComponentInfo componentInfo() {
+      return componentInfo;
     }
 
     @Override
     public Component component() {
       return component;
     }
 
+    @Override
+    public ResolvedComponent.Builder toBuilder() {
+      return new ResolvedComponent.Builder().mergeFrom(this);
+    }
+
     @Override
     public boolean equals(Object obj) {
       if (!(obj instanceof ResolvedComponent_Builder.Value)) {
         return false;
       }
       ResolvedComponent_Builder.Value other = (ResolvedComponent_Builder.Value) obj;
       return Objects.equals(stackId, other.stackId)
-          && Objects.equals(serviceGroupName, other.serviceGroupName)
-          && Objects.equals(serviceType, other.serviceType)
-          && Objects.equals(serviceName, other.serviceName)
+          //&& Objects.equals(serviceGroupName, other.serviceGroupName)
+          //&& Objects.equals(serviceName, other.serviceName)
           && Objects.equals(componentName, other.componentName)
-          && Objects.equals(masterComponent, other.masterComponent)
-          && Objects.equals(component, other.component);
+          && Objects.equals(serviceInfo, other.serviceInfo)
+          //&& Objects.equals(componentInfo, other.componentInfo)
+          //&& Objects.equals(component, other.component)
+        ;
     }
 
     @Override
     public int hashCode() {
       return Objects.hash(
           stackId,
-          serviceGroupName,
-          serviceType,
-          serviceName,
+          //serviceGroupName,","[{'comment': 'Please remove the commented out lines below.  ', 'commenter': 'rnettleton'}]"
1556,ambari-server/src/main/java/org/apache/ambari/server/topology/validators/ClusterConfigTypeValidator.java,"@@ -30,15 +30,15 @@
   private static final Logger LOGGER = LoggerFactory.getLogger(ClusterConfigTypeValidator.class);
 
   @Override","[{'comment': 'Please add some javadoc to explain the updates to this method (return type change, etc).  \r\n\r\n', 'commenter': 'rnettleton'}]"
1575,ambari-server/src/main/java/org/apache/ambari/server/actionmanager/ActionDBAccessorImpl.java,"@@ -168,7 +172,18 @@ public ActionDBAccessorImpl(@Named(""executionCommandCacheSize"") long cacheLimit,
 
   @Inject
   void init() {
-    requestId = stageDAO.getLastRequestId();
+    if (jpaInitialized.get()) {
+      LOG.info(""Initializing last request ID from DB ..."");
+      requestId = stageDAO.getLastRequestId();
+    }
+  }
+
+  @Subscribe
+  public void jpaInitialized(JpaInitializedEvent event) {
+    LOG.info(""JPA initialized event received: {}"", event);
+    jpaInitialized.getAndSet(true);
+    init();
+    LOG.info(""Set latest request ID."");","[{'comment': ""* Shouldn't the call to `init()` be inside an `if (!jpaInitialized.getAndSet(true))` block to make sure it's executed only once?\r\n* Log message about request ID belongs to `init()`."", 'commenter': 'adoroszlai'}, {'comment': 'Ok.', 'commenter': 'smolnar82'}]"
1575,ambari-server/src/main/java/org/apache/ambari/server/actionmanager/ActionDBAccessorImpl.java,"@@ -154,6 +156,8 @@
   private Cache<Long, HostRoleCommand> hostRoleCommandCache;
   private long cacheLimit; //may be exceeded to store tasks from one request
 
+  private AtomicBoolean jpaInitialized = new AtomicBoolean(false);","[{'comment': 'Should be `final`.', 'commenter': 'adoroszlai'}, {'comment': 'Can you explain the need for this? There are really only a few odd  cases where this would be needed.', 'commenter': 'jonathan-hurley'}, {'comment': 'Ok.', 'commenter': 'smolnar82'}, {'comment': ""The reason I needed to add this is because from now on (if this change gets committed) `ActionDBAccessor` is injected in `TaskUpdateListener` and when the Guice injector is being created in `AmbariServer` the `init()` method has been called prematurely: before starting and initializing JPA context by invoking `injector.getInstance(GuiceJpaInitializer.class)`.  To avoid this we have to wait until this is happening and a `JpaInitializedEvent` is triggered.\r\n\r\nI'm open to any recommendation on how to resolve this in another way."", 'commenter': 'smolnar82'}, {'comment': 'Can you instead use a `Provider<>` to prevent this race condition?', 'commenter': 'jonathan-hurley'}]"
1579,ambari-server/src/main/resources/common-services/LOGSEARCH/0.5.0/properties/input.config-ambari.json.j2,"@@ -191,18 +191,58 @@
           ""map_date"":{
             ""target_date_pattern"":""yyyy-MM-dd HH:mm:ss.SSS""
           }
-
         },
-        ""level"":{
-          ""map_fieldvalue"":{
-            ""pre_value"":""Warning"",
-            ""post_value"":""Warn""
+        ""level"":[
+          {
+            ""map_fieldvalue"":{
+              ""pre_value"":""Severe"",
+              ""post_value"":""ERROR""
+            }
+          },
+          {
+            ""map_fieldvalue"":{
+              ""pre_value"":""Warning"",
+              ""post_value"":""WARN""
+            }
+          },
+          {
+            ""map_fieldvalue"":{
+              ""pre_value"":""Finer"",
+              ""post_value"":""WARN""
+            }
+          },
+          {
+            ""map_fieldvalue"":{
+              ""pre_value"":""Info"",
+              ""post_value"":""INFO""
+            }
+          },
+          {
+            ""map_fieldvalue"":{
+              ""pre_value"":""Config"",","[{'comment': 'I think logfeeder wont recognize this as a log level, as its not in the log level grok pattern', 'commenter': 'oleewere'}, {'comment': 'probably you can change the grok pattern to DATA from LOGLEVEl and then remap the values as you did', 'commenter': 'oleewere'}, {'comment': 'I changed the ambari_eclipselink grok pattern to DATA from LOGLEVEL.', 'commenter': 'kasakrisz'}]"
1579,ambari-server/src/main/resources/common-services/LOGSEARCH/0.5.0/configuration/logfeeder-env.xml,"@@ -150,6 +150,28 @@
     <property-type>KERBEROS_PRINCIPAL</property-type>
     <on-ambari-upgrade add=""true""/>
   </property>
+  <property>
+    <name>ambari_server_log_dir</name>
+    <value>/var/log/ambari-server</value>
+    <description>Log dir of Ambari Server. Use this property when bootstrapping Logsearch only.</description>
+    <display-name>Ambari Server log dir</display-name>
+    <value-attributes>
+      <type>directory</type>
+      <editable-only-at-install>true</editable-only-at-install>
+    </value-attributes>
+    <on-ambari-upgrade add=""true""/>
+  </property>
+  <property>
+    <name>ambari_agent_log_dir</name>
+    <value>/var/log/ambari-agent</value>
+    <description>Log dir of Ambari Agent. Use this property when bootstrapping Logsearch only.</description>
+    <display-name>Ambari Agent log dir</display-name>
+    <value-attributes>
+      <type>directory</type>
+      <editable-only-at-install>true</editable-only-at-install>
+    </value-attributes>
+    <on-ambari-upgrade add=""true""/>","[{'comment': ""If these properties are not editable after install, I guess they don't need to be added on upgrade."", 'commenter': 'adoroszlai'}]"
1584,ambari-server/src/main/java/org/apache/ambari/server/agent/stomp/AgentHostDataHolder.java,"@@ -42,32 +42,30 @@
   @Inject
   private STOMPUpdatePublisher STOMPUpdatePublisher;
 
-  private final Map<Long, T> data = new ConcurrentHashMap<>();
+  private final ConcurrentHashMap<Long, T> data = new ConcurrentHashMap<>();
 
   protected abstract T getCurrentData(Long hostId) throws AmbariException;
-  protected abstract boolean handleUpdate(T update) throws AmbariException;
+  protected abstract T handleUpdate(T current, T update) throws AmbariException;
 
   public T getUpdateIfChanged(String agentHash, Long hostId) throws AmbariException {
     T hostData = initializeDataIfNeeded(hostId, true);
     return !Objects.equals(agentHash, hostData.getHash()) ? hostData : getEmptyData();
   }
 
   public T initializeDataIfNeeded(Long hostId, boolean regenerateHash) throws AmbariException {
-    T hostData = data.get(hostId);
-    if (hostData == null) {
-      updateLock.lock();
-      try {
-        hostData = data.get(hostId);
-        if (hostData == null) {
-          hostData = getCurrentData(hostId);
-          if (regenerateHash) {
-            regenerateDataIdentifiers(hostData);
-          }
-          data.put(hostId, hostData);
-        }
-      } finally {
-        updateLock.unlock();
-      }
+    return data.computeIfAbsent(hostId, id -> initializeData(hostId, regenerateHash));","[{'comment': 'beautiful!', 'commenter': 'swagle'}]"
1584,ambari-server/src/main/java/org/apache/ambari/server/agent/stomp/AgentHostDataHolder.java,"@@ -42,32 +42,30 @@
   @Inject
   private STOMPUpdatePublisher STOMPUpdatePublisher;
 
-  private final Map<Long, T> data = new ConcurrentHashMap<>();
+  private final ConcurrentHashMap<Long, T> data = new ConcurrentHashMap<>();
 
   protected abstract T getCurrentData(Long hostId) throws AmbariException;
-  protected abstract boolean handleUpdate(T update) throws AmbariException;
+  protected abstract T handleUpdate(T current, T update) throws AmbariException;
 
   public T getUpdateIfChanged(String agentHash, Long hostId) throws AmbariException {
     T hostData = initializeDataIfNeeded(hostId, true);
     return !Objects.equals(agentHash, hostData.getHash()) ? hostData : getEmptyData();
   }
 
   public T initializeDataIfNeeded(Long hostId, boolean regenerateHash) throws AmbariException {
-    T hostData = data.get(hostId);
-    if (hostData == null) {
-      updateLock.lock();
-      try {
-        hostData = data.get(hostId);
-        if (hostData == null) {
-          hostData = getCurrentData(hostId);
-          if (regenerateHash) {
-            regenerateDataIdentifiers(hostData);
-          }
-          data.put(hostId, hostData);
-        }
-      } finally {
-        updateLock.unlock();
-      }
+    return data.computeIfAbsent(hostId, id -> initializeData(hostId, regenerateHash));
+  }
+
+  private T initializeData(Long hostId, boolean regenerateHash) {
+    T hostData;
+    try {
+      hostData = getCurrentData(hostId);
+    } catch (AmbariException e) {
+      LOG.error(""Error during retrieving initial value for host: {} and class {}"", hostId, getClass().getName(), e);
+      throw new RuntimeException(""Error during retrieving initial value for host: "" + hostId + "" and class: "" + getClass().getName(), e);","[{'comment': 'Can you please add a comment how the RuntimeException will be handled? Seems like very serious error.', 'commenter': 'swagle'}]"
1584,ambari-server/src/main/java/org/apache/ambari/server/agent/stomp/AgentHostDataHolder.java,"@@ -77,21 +75,30 @@ public T initializeDataIfNeeded(Long hostId, boolean regenerateHash) throws Amba
    * event to listeners.
    */
   public void updateData(T update) throws AmbariException {
-    //TODO need optimization for perf cluster
-    updateLock.lock();
-    try {
-      initializeDataIfNeeded(update.getHostId(), true);
-      if (handleUpdate(update)) {
-        T hostData = getData(update.getHostId());
-        regenerateDataIdentifiers(hostData);
-        setIdentifiersToEventUpdate(update, hostData);
-        if (update.getType().equals(STOMPEvent.Type.AGENT_CONFIGS)) {
-          LOG.info(""Configs update with hash {} will be sent to host {}"", update.getHash(), hostData.getHostId());
-        }
-        STOMPUpdatePublisher.publish(update);
+    data.compute(update.getHostId(), (id, current) -> {
+      if (current == null) {
+        current = initializeData(id, true);
+      }
+      T updated;
+      try {
+        updated = handleUpdate(current, update);
+      } catch (AmbariException e) {
+        LOG.error(""Error during handling update for host: {} and class {}"", id, getClass().getName(), e);
+        throw new RuntimeException(""Error during handling update for host: "" + id + "" and class: "" + getClass().getName(), e);","[{'comment': ""Again isn't this a behavior change, we are throwing a Runtime exception vs Ambari?"", 'commenter': 'swagle'}]"
1589,ambari-server/src/main/java/org/apache/ambari/server/checks/CheckDescription.java,"@@ -212,4 +244,42 @@ public String getText() {
   public String getFail(String key) {
     return m_fails.containsKey(key) ? m_fails.get(key) : """";
   }
+
+  /**
+   * {@inheritDoc}
+   */
+  @Override
+  public int hashCode() {
+    return Objects.hash(m_name);
+  }
+
+  /**
+   * {@inheritDoc}
+   */
+  @Override
+  public boolean equals(Object object) {
+    if (null == object) {
+      return false;
+    }
+
+    if (this == object) {
+      return true;
+    }
+
+    if (object.getClass() != getClass()) {
+      return false;
+    }
+
+    CheckDescription that = (CheckDescription) object;
+
+    return Objects.equals(m_name, that.m_name);","[{'comment': 'should not hashcode / equals / toString be based also on m_description and m_fails values?', 'commenter': 'Unknown'}, {'comment': ""No, since we're basically saying the name is what makes them unique."", 'commenter': 'jonathan-hurley'}]"
1589,ambari-server/src/main/java/org/apache/ambari/server/checks/ServicesUpCheck.java,"@@ -83,117 +84,124 @@ public ServicesUpCheck() {
    * {@inheritDoc}
    */
   @Override
-  public void perform(PrerequisiteCheck prerequisiteCheck, PrereqCheckRequest request)
+  public UpgradeCheckResult perform(PrereqCheckRequest request)
       throws AmbariException {
 
     final String clusterName = request.getClusterName();
     final Cluster cluster = clustersProvider.get().getCluster(clusterName);
     List<String> errorMessages = new ArrayList<>();
     LinkedHashSet<ServiceDetail> failedServices = new LinkedHashSet<>();
 
-    Set<String> servicesInUpgrade = getServicesInUpgrade(request);
-    for (String serviceName : servicesInUpgrade) {
-      final Service service = cluster.getService(serviceName);
+    Map<ServiceGroup, Set<String>> serviceGroupsInUpgrade = getServicesInUpgrade(request);
+    for (ServiceGroup serviceGroup : serviceGroupsInUpgrade.keySet()) {
+      for( String serviceName : serviceGroupsInUpgrade.get(serviceGroup) ) {
 
-      // Ignore services like Tez that are clientOnly.
-      if (service.isClientOnlyService()) {
-        continue;
-      }
-
-      Map<String, ServiceComponent> serviceComponents = service.getServiceComponents();
-      for (Map.Entry<String, ServiceComponent> component : serviceComponents.entrySet()) {
-
-        ServiceComponent serviceComponent = component.getValue();
-        // In Services like HDFS, ignore components like HDFS Client
-        if (serviceComponent.isClientComponent()) {
-          continue;
-        }
-
-        // skip if the component is not part of the finalization version check
-        if (!serviceComponent.isVersionAdvertised()) {
+        final Service service = cluster.getService(serviceName);
+  
+        // Ignore services like Tez that are clientOnly.
+        if (service.isClientOnlyService()) {
           continue;
         }
-
-        // TODO, add more logic that checks the Upgrade Pack.
-        // These components are not in the upgrade pack and do not advertise a
-        // version:
-        // ZKFC, Ambari Metrics, Kerberos, Atlas (right now).
-        // Generally, if it advertises a version => in the upgrade pack.
-        // So it can be in the Upgrade Pack but not advertise a version.
-        List<HostComponentSummary> hostComponentSummaries = HostComponentSummary.getHostComponentSummaries(
-          service.getClusterId(), service.getServiceGroupId(), service.getServiceId(), serviceComponent.getName());
-
-        // not installed, nothing to do
-        if (hostComponentSummaries.isEmpty()) {
-          continue;
-        }
-
-        // non-master, ""true"" slaves with cardinality 1+
-        boolean checkThreshold = false;
-        if (!serviceComponent.isMasterComponent()) {
-          StackId stackId = service.getStackId();
-          ComponentInfo componentInfo = ambariMetaInfo.get().getComponent(stackId.getStackName(),
-              stackId.getStackVersion(), serviceComponent.getServiceType(),
-              serviceComponent.getName());
-
-          String cardinality = componentInfo.getCardinality();
-          // !!! check if there can be more than one. This will match, say,
-          // datanodes but not ZKFC
-          if (null != cardinality
-              && (cardinality.equals(""ALL"") || cardinality.matches(""[1-9].*""))) {
-            checkThreshold = true;
+  
+        Map<String, ServiceComponent> serviceComponents = service.getServiceComponents();
+        for (Map.Entry<String, ServiceComponent> component : serviceComponents.entrySet()) {
+  
+          ServiceComponent serviceComponent = component.getValue();
+          // In Services like HDFS, ignore components like HDFS Client
+          if (serviceComponent.isClientComponent()) {
+            continue;
           }
-        }
-
-        // check threshold for slaves which have a non-0 cardinality
-        if (checkThreshold) {
-          int total = hostComponentSummaries.size();
-          int up = 0;
-          int down = 0;
-
-          for (HostComponentSummary summary : hostComponentSummaries) {
-            if (isConsideredDown(cluster, serviceComponent, summary)) {
-              down++;
-            } else {
-              up++;
-            }
+  
+          // skip if the component is not part of the finalization version check
+          if (!serviceComponent.isVersionAdvertised()) {
+            continue;
           }
-
-          if ((float) down / total > SLAVE_THRESHOLD) { // arbitrary
-            failedServices.add(new ServiceDetail(serviceName));
-
-            String message = MessageFormat.format(
-                ""{0}: {1} out of {2} {3} are started; there should be {4,number,percent} started before upgrading."",
-                service.getName(), up, total, serviceComponent.getName(), SLAVE_THRESHOLD);
-            errorMessages.add(message);
+  
+          // TODO, add more logic that checks the Upgrade Pack.
+          // These components are not in the upgrade pack and do not advertise a
+          // version:
+          // ZKFC, Ambari Metrics, Kerberos, Atlas (right now).
+          // Generally, if it advertises a version => in the upgrade pack.
+          // So it can be in the Upgrade Pack but not advertise a version.
+          List<HostComponentSummary> hostComponentSummaries = HostComponentSummary.getHostComponentSummaries(
+            service.getClusterId(), service.getServiceGroupId(), service.getServiceId(), serviceComponent.getName());
+  
+          // not installed, nothing to do
+          if (hostComponentSummaries.isEmpty()) {
+            continue;
+          }
+  
+          // non-master, ""true"" slaves with cardinality 1+
+          boolean checkThreshold = false;
+          if (!serviceComponent.isMasterComponent()) {
+            StackId stackId = service.getStackId();
+            ComponentInfo componentInfo = ambariMetaInfo.get().getComponent(stackId.getStackName(),
+                stackId.getStackVersion(), serviceComponent.getServiceType(),
+                serviceComponent.getName());
+  
+            String cardinality = componentInfo.getCardinality();
+            // !!! check if there can be more than one. This will match, say,
+            // datanodes but not ZKFC
+            if (null != cardinality
+                && (cardinality.equals(""ALL"") || cardinality.matches(""[1-9].*""))) {","[{'comment': 'These magic string/magic regex comparisons look unreliable in a long term perspective. How about scheduling refactoring to incapsulate Cardinality in a dedicated class with pre-defined constants like ALL and utility methods like moreThenZero()', 'commenter': 'Unknown'}, {'comment': ""I didn't change these - when we revisit upgrade checks, we can address workarounds like this then."", 'commenter': 'jonathan-hurley'}]"
1591,ambari-server/src/main/java/org/apache/ambari/server/topology/TopologyManager.java,"@@ -389,7 +376,7 @@ void saveOrUpdateQuickLinksProfile(String quickLinksProfileJson) {
   private void submitCredential(String clusterName, Credential credential) {
 
     ResourceProvider provider =
-        ambariContext.getClusterController().ensureResourceProvider(Resource.Type.Credential);
+        AmbariContext.getClusterController().ensureResourceProvider(Resource.Type.Credential);","[{'comment': 'Would it be better here to use the method on AmbariContext to obtain this value from the ""ambariContext"" field in this class?  ', 'commenter': 'rnettleton'}, {'comment': 'Access to `static` members is generally preferred via the class, not instances (eg. IDEA produces warnings for the latter).', 'commenter': 'adoroszlai'}]"
1591,ambari-server/src/main/java/org/apache/ambari/server/topology/BlueprintBasedClusterProvisionRequest.java,"@@ -51,13 +51,15 @@
   public BlueprintBasedClusterProvisionRequest(AmbariContext ambariContext, SecurityConfigurationFactory securityConfigurationFactory, Blueprint blueprint, ProvisionRequest request) {
     this.blueprint = blueprint;
     this.request = request;
-
     stackIds = ImmutableSet.copyOf(Sets.union(blueprint.getStackIds(), request.getStackIds()));
-    stack = ambariContext.composeStacks(stackIds);
     mpacks = ImmutableSet.<MpackInstance>builder().
       addAll(blueprint.getMpacks()).
       addAll(request.getMpacks()).build();
 
+    ambariContext.downloadMissingMpacks(mpacks);
+","[{'comment': 'I am a bit reluctant to put such side effects into constructors. I think TopologyManager.provisionCluster() was a better place for this.', 'commenter': 'benyoka'}, {'comment': ""I agree.  But the download needs to be performed during replay, too, in case mpacks were deleted in the meantime.  I'm not sure that's a scenario Ambari needs to support, we can defer that for now."", 'commenter': 'adoroszlai'}]"
1591,ambari-server/src/main/java/org/apache/ambari/server/topology/PersistedStateImpl.java,"@@ -311,7 +286,30 @@ private TopologyHostRequestEntity toEntity(HostRequest request, TopologyLogicalR
     return entity;
   }
 
-  static final class ReplayedTopologyRequest implements TopologyRequest {
+  // FIXME hard-code
+  static final class ReplayedProvisionRequest extends ReplayedTopologyRequest implements ProvisionRequest {
+
+    ReplayedProvisionRequest(TopologyRequestEntity entity, BlueprintFactory blueprintFactory) {
+      super(entity, blueprintFactory);
+    }
+
+    @Override
+    public ConfigRecommendationStrategy getConfigRecommendationStrategy() {
+      return ConfigRecommendationStrategy.NEVER_APPLY;
+    }
+
+    @Override
+    public String getDefaultPassword() {
+      return null;
+    }
+
+    @Override
+    public SecurityConfiguration getSecurityConfiguration() {
+      return SecurityConfiguration.NONE;
+    }
+  }
+","[{'comment': 'What is the purpose of the hard-coded overrides?', 'commenter': 'benyoka'}, {'comment': ""These are not overrides, but necessary implementations of interface methods.  I'm not sure they are used at all during request replay.\r\n\r\nChanged the hard-coded values to `null`."", 'commenter': 'adoroszlai'}]"
1591,ambari-server/src/test/java/org/apache/ambari/server/controller/internal/ExportBlueprintRequestTest.java,"@@ -49,69 +47,113 @@
 import org.apache.ambari.server.topology.Blueprint;
 import org.apache.ambari.server.topology.HostGroup;
 import org.apache.ambari.server.topology.HostGroupInfo;
-import org.junit.After;
 import org.junit.Before;
 import org.junit.Test;
 import org.junit.runner.RunWith;
 import org.powermock.api.easymock.PowerMock;
 import org.powermock.core.classloader.annotations.PrepareForTest;
 import org.powermock.modules.junit4.PowerMockRunner;
 
+import com.google.common.collect.ImmutableMap;
+import com.google.common.collect.ImmutableSet;
+
 /**
  * ExportBlueprintRequest unit tests.
  */
-@SuppressWarnings(""unchecked"")
 @RunWith(PowerMockRunner.class)
-@PrepareForTest({ExportBlueprintRequest.ExportedHostGroup.class})
+@PrepareForTest({ InetAddress.class })
 public class ExportBlueprintRequestTest {","[{'comment': 'According to powermock docs, when mocking system classes, the class that calls the system class should be prepared.\r\n\r\nhttps://github.com/powermock/powermock/wiki/Mock-System', 'commenter': 'benyoka'}, {'comment': 'Good point.', 'commenter': 'adoroszlai'}]"
1602,ambari-server/src/main/java/org/apache/ambari/server/checks/MissingOsInRepoVersionCheck.java,"@@ -0,0 +1,92 @@
+/*
+ *
+ *  * Licensed to the Apache Software Foundation (ASF) under one
+ *  * or more contributor license agreements.  See the NOTICE file
+ *  * distributed with this work for additional information
+ *  * regarding copyright ownership.  The ASF licenses this file
+ *  * to you under the Apache License, Version 2.0 (the
+ *  * ""License""); you may not use this file except in compliance
+ *  * with the License.  You may obtain a copy of the License at
+ *  *
+ *  *     http://www.apache.org/licenses/LICENSE-2.0
+ *  *
+ *  * Unless required by applicable law or agreed to in writing, software
+ *  * distributed under the License is distributed on an ""AS IS"" BASIS,
+ *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *  * See the License for the specific language governing permissions and
+ *  * limitations under the License.
+ *
+ */
+
+package org.apache.ambari.server.checks;
+
+import static java.util.stream.Collectors.toSet;
+import static org.apache.ambari.server.state.MaintenanceState.OFF;
+
+import java.util.LinkedHashSet;
+import java.util.Set;
+
+import org.apache.ambari.server.AmbariException;
+import org.apache.ambari.server.controller.PrereqCheckRequest;
+import org.apache.ambari.server.orm.entities.RepoOsEntity;
+import org.apache.ambari.server.state.Cluster;
+import org.apache.ambari.server.state.Host;
+import org.apache.ambari.server.state.stack.PrereqCheckStatus;
+import org.apache.ambari.server.state.stack.PrerequisiteCheck;
+import org.apache.ambari.server.state.stack.upgrade.UpgradeType;
+
+import com.google.inject.Singleton;
+
+/**
+ * This checks if the source and target version has an entry for each OS type in the cluster.
+ */
+@Singleton
+@UpgradeCheck(
+  group = UpgradeCheckGroup.REPOSITORY_VERSION,
+  required = { UpgradeType.NON_ROLLING })","[{'comment': 'Only required for express? I would think all the upgrades would need this check.', 'commenter': 'jonathan-hurley'}, {'comment': '@ncole suggested to apply it only for EU.', 'commenter': 'zeroflag'}, {'comment': ""@ncole  - any reason we're doing this only for EU? I would have suspected it would apply to all upgrades."", 'commenter': 'jonathan-hurley'}, {'comment': ""A restart for RU purposes should be pulling the new version for command purposes.  If we're not (should be tested) then we should adopt this for RU as well.  @zeroflag, make sure you test that case please and adjust as necessary."", 'commenter': 'ncole'}, {'comment': 'I added RU upgrade type because it failed during my test.', 'commenter': 'zeroflag'}]"
1602,ambari-server/src/main/java/org/apache/ambari/server/checks/MissingOsInRepoVersionCheck.java,"@@ -0,0 +1,92 @@
+/*
+ *
+ *  * Licensed to the Apache Software Foundation (ASF) under one
+ *  * or more contributor license agreements.  See the NOTICE file
+ *  * distributed with this work for additional information
+ *  * regarding copyright ownership.  The ASF licenses this file
+ *  * to you under the Apache License, Version 2.0 (the
+ *  * ""License""); you may not use this file except in compliance
+ *  * with the License.  You may obtain a copy of the License at
+ *  *
+ *  *     http://www.apache.org/licenses/LICENSE-2.0
+ *  *
+ *  * Unless required by applicable law or agreed to in writing, software
+ *  * distributed under the License is distributed on an ""AS IS"" BASIS,
+ *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *  * See the License for the specific language governing permissions and
+ *  * limitations under the License.
+ *
+ */
+
+package org.apache.ambari.server.checks;
+
+import static java.util.stream.Collectors.toSet;
+import static org.apache.ambari.server.state.MaintenanceState.OFF;
+
+import java.util.LinkedHashSet;
+import java.util.Set;
+
+import org.apache.ambari.server.AmbariException;
+import org.apache.ambari.server.controller.PrereqCheckRequest;
+import org.apache.ambari.server.orm.entities.RepoOsEntity;
+import org.apache.ambari.server.state.Cluster;
+import org.apache.ambari.server.state.Host;
+import org.apache.ambari.server.state.stack.PrereqCheckStatus;
+import org.apache.ambari.server.state.stack.PrerequisiteCheck;
+import org.apache.ambari.server.state.stack.upgrade.UpgradeType;
+
+import com.google.inject.Singleton;
+
+/**
+ * This checks if the source and target version has an entry for each OS type in the cluster.
+ */
+@Singleton
+@UpgradeCheck(
+  group = UpgradeCheckGroup.REPOSITORY_VERSION,
+  required = { UpgradeType.NON_ROLLING })
+public class MissingOsInRepoVersionCheck extends AbstractCheckDescriptor {
+  public static final String SOURCE_OS = ""source_os"";
+  public static final String TARGET_OS = ""target_os"";
+
+  public MissingOsInRepoVersionCheck() {
+    super(CheckDescription.MISSING_OS_IN_REPO_VERSION);
+  }
+
+  @Override
+  public void perform(PrerequisiteCheck prerequisiteCheck, PrereqCheckRequest request) throws AmbariException {
+    Set<String> osFamiliesInCluster = osFamiliesInCluster(cluster(prerequisiteCheck));
+    if (!targetOsFamilies(request).containsAll(osFamiliesInCluster)) {
+      prerequisiteCheck.setFailReason(getFailReason(TARGET_OS, prerequisiteCheck, request));
+      prerequisiteCheck.setStatus(PrereqCheckStatus.FAIL);
+      prerequisiteCheck.setFailedOn(new LinkedHashSet<>(osFamiliesInCluster));
+    } else if (!sourceOsFamilies(request).containsAll(osFamiliesInCluster)) {
+      prerequisiteCheck.setFailReason(getFailReason(SOURCE_OS, prerequisiteCheck, request));
+      prerequisiteCheck.setStatus(PrereqCheckStatus.FAIL);
+      prerequisiteCheck.setFailedOn(new LinkedHashSet<>(osFamiliesInCluster));
+    }
+  }
+
+  private Cluster cluster(PrerequisiteCheck prerequisiteCheck) throws AmbariException {
+    return clustersProvider.get().getCluster(prerequisiteCheck.getClusterName());
+  }
+
+  private Set<String> osFamiliesInCluster(Cluster cluster) {","[{'comment': 'Javadoc', 'commenter': 'jonathan-hurley'}, {'comment': 'Do we really need to add javadoc comments for private methods, especially short ones like these? I could have written one large method without using any privates and no one would have asked for javadocs in that case I suppose. IMHO if we require writing javadocs for every private methods it will discourage people to extract out logic into shorter methods.', 'commenter': 'zeroflag'}, {'comment': 'We absolutely do. Every method should be properly documented. A developer should only ever need to read documentation to determine the inputs, what they mean, and what the method does/expects. ', 'commenter': 'jonathan-hurley'}, {'comment': ""I added the missing javadocs, but I still think it's not just unnecessary but also harmful to enforce javadoc at private method level because it encourages people to write large methods without factoring out anything (not to mention the usual problems like extra maintenance cost and going out of sync with the code)."", 'commenter': 'zeroflag'}, {'comment': 'Just because you are writing a short description of what the method is doing and anything special that should be known about it (such as if it returns a copy of the underlying data set or an immutable one or if there are performance implications from calling it) should not preclude someone from writing concise methods. \r\n\r\nDocumentation is always better than none. Ambari is riddled with classes with zero documentation where you need to examine each line to figure out what the heck is going on.', 'commenter': 'jonathan-hurley'}, {'comment': '> should not preclude someone from writing concise methods.\r\n\r\nOne of the reason why people like writing small methods is to eliminate the need of writing a bunch of comments. No one will do it if they need to write those comments anyway.\r\n\r\nIf someone implements some logic using one big method and an other person implements the same using 10 smaller methods then the amount of required comments will be completely different (1 versus 10), even if the logic is the same. So, this rule makes the amount of required comments highly style dependent.\r\n\r\nDepending on the coding style, either people will complain about the missing comments even if the code is readable without them or they\'ll be completely fine with one unreadable ""God method"".\r\n\r\n> Documentation is always better than none.\r\n\r\nI disagree. It\'s a cost/benefit tradeoff. Comments need to be written and maintained and still they\'ll unavoidably go out of sync eventually, misleading the reader.\r\n\r\nMany times they don\'t add any extra value (like most of the comments on getter/setters) or they\'re untrue or the method is so simple than reading the code doesn\'t take longer than reading the comment. \r\n', 'commenter': 'zeroflag'}, {'comment': ""> One of the reason why people like writing small methods is to eliminate the need of writing a bunch of comments. No one will do it if they need to write those comments anyway.\r\n\r\nThat is called being lazy. We shouldn't accept undocumented code because developers want to take shortcuts. There is no cost/benefit trade off here. Code needs to be clear, maintainable and understandable by anyone - even people looking at it for the first time. A method can be clean and concise, but should still have documentation. \r\n\r\nI don't know why we're arguing about this. No documentation is sloppy and lazy - period."", 'commenter': 'jonathan-hurley'}, {'comment': ""> I don't know why we're arguing about this\r\n\r\nWe're not, because you're being dogmatic and not responding any of my arguments. So let's agree to disagree at least."", 'commenter': 'zeroflag'}, {'comment': 'I responded to your arguments directly by saying that asking developers to document methods should not lead to ""god"" methods. That even though smaller, concise methods are favorable, that should not preclude clear documentation so developers don\'t need to read every line of code to figure out what a method is or does.\r\n\r\nFrankly, I am sticking to my guns here because Ambari is an undocumented mess in a lot of areas. I can open up a file at random and see _ZERO_ documentation in it. \r\n\r\nI\'m sorry that you feel that spending the extra 20 seconds will lead to the unravelling of the universe, but in a project so large, we need to lead by example since it\'s too hard to enforce policies across the community. We can do this by having our code broken out into smaller, consumable methods that are still clearly documented.', 'commenter': 'jonathan-hurley'}, {'comment': ""Ok, I still don't see why adding javadocs to 3 lines long private methods will solve any of the problems, but nevertheless I added those. I'm merging and closing this issue."", 'commenter': 'zeroflag'}]"
1602,ambari-server/src/main/java/org/apache/ambari/server/checks/MissingOsInRepoVersionCheck.java,"@@ -0,0 +1,92 @@
+/*
+ *
+ *  * Licensed to the Apache Software Foundation (ASF) under one
+ *  * or more contributor license agreements.  See the NOTICE file
+ *  * distributed with this work for additional information
+ *  * regarding copyright ownership.  The ASF licenses this file
+ *  * to you under the Apache License, Version 2.0 (the
+ *  * ""License""); you may not use this file except in compliance
+ *  * with the License.  You may obtain a copy of the License at
+ *  *
+ *  *     http://www.apache.org/licenses/LICENSE-2.0
+ *  *
+ *  * Unless required by applicable law or agreed to in writing, software
+ *  * distributed under the License is distributed on an ""AS IS"" BASIS,
+ *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *  * See the License for the specific language governing permissions and
+ *  * limitations under the License.
+ *
+ */
+
+package org.apache.ambari.server.checks;
+
+import static java.util.stream.Collectors.toSet;
+import static org.apache.ambari.server.state.MaintenanceState.OFF;
+
+import java.util.LinkedHashSet;
+import java.util.Set;
+
+import org.apache.ambari.server.AmbariException;
+import org.apache.ambari.server.controller.PrereqCheckRequest;
+import org.apache.ambari.server.orm.entities.RepoOsEntity;
+import org.apache.ambari.server.state.Cluster;
+import org.apache.ambari.server.state.Host;
+import org.apache.ambari.server.state.stack.PrereqCheckStatus;
+import org.apache.ambari.server.state.stack.PrerequisiteCheck;
+import org.apache.ambari.server.state.stack.upgrade.UpgradeType;
+
+import com.google.inject.Singleton;
+
+/**
+ * This checks if the source and target version has an entry for each OS type in the cluster.
+ */
+@Singleton
+@UpgradeCheck(
+  group = UpgradeCheckGroup.REPOSITORY_VERSION,
+  required = { UpgradeType.NON_ROLLING })
+public class MissingOsInRepoVersionCheck extends AbstractCheckDescriptor {
+  public static final String SOURCE_OS = ""source_os"";
+  public static final String TARGET_OS = ""target_os"";
+
+  public MissingOsInRepoVersionCheck() {
+    super(CheckDescription.MISSING_OS_IN_REPO_VERSION);
+  }
+
+  @Override
+  public void perform(PrerequisiteCheck prerequisiteCheck, PrereqCheckRequest request) throws AmbariException {
+    Set<String> osFamiliesInCluster = osFamiliesInCluster(cluster(prerequisiteCheck));
+    if (!targetOsFamilies(request).containsAll(osFamiliesInCluster)) {
+      prerequisiteCheck.setFailReason(getFailReason(TARGET_OS, prerequisiteCheck, request));
+      prerequisiteCheck.setStatus(PrereqCheckStatus.FAIL);
+      prerequisiteCheck.setFailedOn(new LinkedHashSet<>(osFamiliesInCluster));
+    } else if (!sourceOsFamilies(request).containsAll(osFamiliesInCluster)) {
+      prerequisiteCheck.setFailReason(getFailReason(SOURCE_OS, prerequisiteCheck, request));
+      prerequisiteCheck.setStatus(PrereqCheckStatus.FAIL);
+      prerequisiteCheck.setFailedOn(new LinkedHashSet<>(osFamiliesInCluster));
+    }
+  }
+
+  private Cluster cluster(PrerequisiteCheck prerequisiteCheck) throws AmbariException {
+    return clustersProvider.get().getCluster(prerequisiteCheck.getClusterName());
+  }
+
+  private Set<String> osFamiliesInCluster(Cluster cluster) {
+    return cluster.getHosts().stream()
+      .filter(host -> host.getMaintenanceState(cluster.getClusterId()) == OFF)
+      .map(Host::getOsFamily)
+      .collect(toSet());
+  }
+
+  private Set<String> sourceOsFamilies(PrereqCheckRequest request) throws AmbariException {
+    return ambariMetaInfo.get().getStack(request.getSourceStackId()).getRepositoriesByOs().keySet();
+  }
+
+  private Set<String> targetOsFamilies(PrereqCheckRequest request) {","[{'comment': 'JavaDoc', 'commenter': 'jonathan-hurley'}]"
1612,ambari-logsearch/ambari-logsearch-web/src/app/components/app.component.html,"@@ -17,11 +17,11 @@
 
 <header>
   <nav class=""navbar"" [class.authorized]=""isAuthorized$ | async"">
-    <h1 [ngClass]=""{'full-flex-width': !(isAuthorized$ | async), 'pull-left': true}"">{{'common.title' | translate}}</h1>
-    <breadcrumbs *ngIf=""isAuthorized$ | async""></breadcrumbs>
-    <top-menu *ngIf=""isAuthorized$ | async""></top-menu>
+    <h1 [ngClass]=""{'full-flex-width': !(isAuthorized$ | async), 'pull-left': (isBaseDataAvailable$ | async)}"">{{'common.title' | translate}}</h1>
+    <breadcrumbs *ngIf=""(isAuthorized$ | async)"" [class.hide]=""!(isBaseDataAvailable$ | async)""></breadcrumbs>","[{'comment': 'Wouldn\'t `*ngIf=""(isAuthorized$ | async) && (isBaseDataAvailable$ | async)""` look better?', 'commenter': 'aBabiichuk'}]"
1612,ambari-web/app/views/common/modal_popups/log_tail_popup.js,"@@ -40,12 +40,12 @@ App.showLogTailPopup = function(content) {
       logSearchUrl: function() {
         var quickLink = App.QuickLinks.find().findProperty('site', 'logsearch-env'),
             logSearchServerHost = App.HostComponent.find().findProperty('componentName', 'LOGSEARCH_SERVER').get('hostName'),
-            queryParams = '';
+            params = '';
         if (quickLink) {
-          queryParams = 'hosts=' + this.get('content.hostName') + '&components=' + this.get('content.logComponentName')
-            + '&query=%5B%7B""id"":0,""name"":""path"",""label"":""Path"",""value"":""' + this.get('content.filePath')
-            + '"",""isExclude"":false%7D%5D';
-          return quickLink.get('template').fmt('http', logSearchServerHost, quickLink.get('default_http_port')) + '/#/logs/serviceLogs?' + queryParams;
+          params = 'hosts=' + encodeURIComponent(this.get('content.hostName'))
+            + ';components=' + encodeURIComponent(this.get('content.logComponentName'))
+            + ';query=[{""id"":0,""name"":""path"",""label"":""Path"",""value"":""' + encodeURIComponent(this.get('content.filePath')) + '"",""isExclude"":false}]';","[{'comment': ""Square brackets aren't escaped here, while in `host_progress_popup_body_view.js` and in `logs_view.js` they are"", 'commenter': 'aBabiichuk'}]"
1612,ambari-logsearch/ambari-logsearch-web/src/app/services/logs-container.service.ts,"@@ -885,32 +883,35 @@ export class LogsContainerService {
   }
 
   openServiceLog(log: ServiceLog): void {
-    const tab = {
-      id: log.id || `${log.host}-${log.type}`,
-      isCloseable: true,
-      label: `${log.host} >> ${log.type}`,
-      activeFilters: Object.assign(this.getFiltersData('serviceLogs'), {
-        components: this.filters.components.options.find((option: ListItem): boolean => {
-          return option.value === log.type;
-        }),
-        hosts: this.filters.hosts.options.find((option: ListItem): boolean => {
-          return option.value === log.host;
-        })
-      }),
-      appState: {
-        activeLogsType: 'serviceLogs',
-        isServiceLogsFileView: true,
-        activeLog: {
-          id: log.id,
-          host_name: log.host,
-          component_name: log.type
-        }
-      }
-    };
-    this.tabsStorage.addInstance(tab);
-    this.router.navigate(['/logs', tab.id], {
-      queryParams: this.logsFilteringUtilsService.getQueryParamsFromActiveFilter(tab.activeFilters, 'serviceLogs')
-    });
-  }
-
+    this.componentsService.findInCollection(component => (component.name || component.label) === log.type)
+      .map(component => component ? component.label || component.name : name)
+      .first()
+      .subscribe((componentName) => {
+        const tab = {
+          id: log.id || `${log.host}-${log.type}`,
+          isCloseable: true,
+          isActive: false,
+          label: `${log.host} >> ${componentName || log.type}`,
+          activeFilters: Object.assign({}, JSON.parse(JSON.stringify(this.filtersForm.value)), {","[{'comment': ""What's `JSON.parse(JSON.stringify(this.filtersForm.value))` for?"", 'commenter': 'aBabiichuk'}, {'comment': ""Poor man's deep copy :)\r\nIt is to cut off all the possibly references."", 'commenter': 'tobias-istvan'}]"
1617,ambari-server/src/main/resources/stacks/service_advisor.py,"@@ -115,6 +124,59 @@ def getServiceComponentLayoutValidations(self, services, hosts):
     """"""
     return []
 
+  def getServiceComponentCardinalityValidations(self, services, hosts, service_name):","[{'comment': 'Missing doc, which would be useful for a function like this. ', 'commenter': 'rlevas'}, {'comment': 'fixed', 'commenter': 'Unknown'}]"
1617,ambari-server/src/main/resources/stacks/service_advisor.py,"@@ -115,6 +124,59 @@ def getServiceComponentLayoutValidations(self, services, hosts):
     """"""
     return []
 
+  def getServiceComponentCardinalityValidations(self, services, hosts, service_name):
+    items = []
+    hostsSet = set(self.getActiveHosts(
+      [host[""Hosts""] for host in hosts[""items""]]))  # [host[""Hosts""][""host_name""] for host in hosts[""items""]]
+    hostsCount = len(hostsSet)
+
+    target_service = None
+    for service in services[""services""]:
+      if service[""StackServices""][""service_name""] is service_name:","[{'comment': '`is` seems to be too strong of an operator here.  Why is `==` not being used?', 'commenter': 'rlevas'}]"
1617,ambari-server/src/main/resources/stacks/service_advisor.py,"@@ -115,6 +124,59 @@ def getServiceComponentLayoutValidations(self, services, hosts):
     """"""
     return []
 
+  def getServiceComponentCardinalityValidations(self, services, hosts, service_name):
+    items = []
+    hostsSet = set(self.getActiveHosts(
+      [host[""Hosts""] for host in hosts[""items""]]))  # [host[""Hosts""][""host_name""] for host in hosts[""items""]]
+    hostsCount = len(hostsSet)
+
+    target_service = None
+    for service in services[""services""]:
+      if service[""StackServices""][""service_name""] is service_name:
+        target_service = service
+    componentsList = target_service[""components""]","[{'comment': '`target_service` could be `None`, causing an exception here. ', 'commenter': 'rlevas'}]"
1628,ambari-server/src/main/java/org/apache/ambari/server/api/services/stackadvisor/commands/StackAdvisorCommand.java,"@@ -228,6 +236,14 @@ private void populateConfigurations(ObjectNode root,
     root.put(GPL_LICENSE_ACCEPTED, request.getGplLicenseAccepted());
   }
 
+  protected void populateClusterEnvConfig(ObjectNode root, StackAdvisorRequest request) {
+    final StackId stackId = new StackId(request.getStackName(), request.getStackVersion());
+    final JsonNode propertiesNode = mapper.convertValue(ambariManagementController.getClusterEnvConfigurationByStackId(stackId), JsonNode.class);
+    final ObjectNode clusterEnvNode = mapper.createObjectNode();
+    clusterEnvNode.put(PROPERTIES_PROPERTY, propertiesNode);
+    root.put(ConfigHelper.CLUSTER_ENV, clusterEnvNode);","[{'comment': 'If cluster-env already exists in the configurations from the original request, it should not be overwritten.  \r\n', 'commenter': 'rlevas'}]"
1628,ambari-server/src/main/java/org/apache/ambari/server/controller/AmbariManagementControllerImpl.java,"@@ -5893,4 +5893,17 @@ public HostRepositories retrieveHostRepositories(Cluster cluster, Host host) thr
     }
     return new HostRepositories(hostRepositories, componentsRepos);
   }
+
+  @Override
+  public Map<String, String> getClusterEnvConfigurationByStackId(StackId stackId) {","[{'comment': 'This does not seem correct...   Maybe the correct information is not available here, but the exact cluster info is needed.  Guessing is not a good idea.  \r\n\r\nMaybe this logic needs to be bubbled up to `org.apache.ambari.server.controller.internal.StackAdvisorResourceProvider#calculateConfigGroups` if the cluster information is known.... else maybe the caller needs to be force to supply the cluster-env information. ', 'commenter': 'rlevas'}, {'comment': ""I asked the FE side to supply `cluster-env` (`security-enabled` at a minimum) within the request.\r\nUsing remote debug I have not found anything that we could use in this case other than what I implemented above); let's see what they respond."", 'commenter': 'smolnar82'}]"
1636,ambari-server/src/main/resources/common-services/STORM/0.9.1/package/scripts/params_linux.py,"@@ -152,27 +152,22 @@
   _ambari_principal_name = default('/configurations/cluster-env/ambari_principal_name', None)
   storm_keytab_path = config['configurations']['storm-env']['storm_keytab']
 
-  if stack_supports_storm_kerberos:
-    storm_ui_keytab_path = config['configurations']['storm-env']['storm_ui_keytab']
-    _storm_ui_jaas_principal_name = config['configurations']['storm-env']['storm_ui_principal_name']
-    storm_ui_jaas_principal = _storm_ui_jaas_principal_name.replace('_HOST',_hostname_lowercase)
-    storm_bare_jaas_principal = get_bare_principal(_storm_principal_name)
-    if _ambari_principal_name:
-      ambari_bare_jaas_principal = get_bare_principal(_ambari_principal_name)
-    _nimbus_principal_name = config['configurations']['storm-env']['nimbus_principal_name']
-    nimbus_jaas_principal = _nimbus_principal_name.replace('_HOST', _hostname_lowercase)
-    nimbus_bare_jaas_principal = get_bare_principal(_nimbus_principal_name)
-    nimbus_keytab_path = config['configurations']['storm-env']['nimbus_keytab']
+  storm_ui_keytab_path = config['configurations']['storm-env']['storm_ui_keytab']
+  _storm_ui_jaas_principal_name = config['configurations']['storm-env']['storm_ui_principal_name']
+  storm_ui_jaas_principal = _storm_ui_jaas_principal_name.replace('_HOST',_hostname_lowercase)
+  storm_bare_jaas_principal = get_bare_principal(_storm_principal_name)
+  if _ambari_principal_name:
+    ambari_bare_jaas_principal = get_bare_principal(_ambari_principal_name)
+  _nimbus_principal_name = config['configurations']['storm-env']['nimbus_principal_name']","[{'comment': ""Please adjust python unit tests accordingly.\r\n\r\n```\r\nFail: Configuration parameter 'nimbus_principal_name' was not found in configurations dictionary!\r\n...\r\nTotal run:1200\r\nTotal errors:21\r\nTotal failures:4\r\n```"", 'commenter': 'adoroszlai'}]"
1636,ambari-server/src/main/resources/common-services/STORM/0.9.1/package/scripts/params_linux.py,"@@ -152,27 +152,22 @@
   _ambari_principal_name = default('/configurations/cluster-env/ambari_principal_name', None)
   storm_keytab_path = config['configurations']['storm-env']['storm_keytab']
 
-  if stack_supports_storm_kerberos:
-    storm_ui_keytab_path = config['configurations']['storm-env']['storm_ui_keytab']
-    _storm_ui_jaas_principal_name = config['configurations']['storm-env']['storm_ui_principal_name']
-    storm_ui_jaas_principal = _storm_ui_jaas_principal_name.replace('_HOST',_hostname_lowercase)
-    storm_bare_jaas_principal = get_bare_principal(_storm_principal_name)
-    if _ambari_principal_name:
-      ambari_bare_jaas_principal = get_bare_principal(_ambari_principal_name)
-    _nimbus_principal_name = config['configurations']['storm-env']['nimbus_principal_name']
-    nimbus_jaas_principal = _nimbus_principal_name.replace('_HOST', _hostname_lowercase)
-    nimbus_bare_jaas_principal = get_bare_principal(_nimbus_principal_name)
-    nimbus_keytab_path = config['configurations']['storm-env']['nimbus_keytab']
+  storm_ui_keytab_path = config['configurations']['storm-env']['storm_ui_keytab']
+  _storm_ui_jaas_principal_name = config['configurations']['storm-env']['storm_ui_principal_name']
+  storm_ui_jaas_principal = _storm_ui_jaas_principal_name.replace('_HOST',_hostname_lowercase)
+  storm_bare_jaas_principal = get_bare_principal(_storm_principal_name)
+  if _ambari_principal_name:
+    ambari_bare_jaas_principal = get_bare_principal(_ambari_principal_name)
+  _nimbus_principal_name = config['configurations']['storm-env']['nimbus_principal_name']
+  nimbus_jaas_principal = _nimbus_principal_name.replace('_HOST', _hostname_lowercase)
+  nimbus_bare_jaas_principal = get_bare_principal(_nimbus_principal_name)
+  nimbus_keytab_path = config['configurations']['storm-env']['nimbus_keytab']
 
 kafka_bare_jaas_principal = None
-if stack_supports_storm_kerberos:
-  if security_enabled:
-    storm_thrift_transport = config['configurations']['storm-site']['_storm.thrift.secure.transport']
-    # generate KafkaClient jaas config if kafka is kerberoized
-    _kafka_principal_name = default(""/configurations/kafka-env/kafka_principal_name"", None)
-    kafka_bare_jaas_principal = get_bare_principal(_kafka_principal_name)
-  else:
-    storm_thrift_transport = config['configurations']['storm-site']['_storm.thrift.nonsecure.transport']
+if security_enabled:","[{'comment': ""`security_enabled` is set using\r\n```\r\nconfig['configurations']['cluster-env']['security_enabled']\r\n```\r\n\r\nIt seems like here is where you would rely on the service's Kerberos indicator, not Ambari's."", 'commenter': 'rlevas'}]"
1637,mpack-instance-manager/src/main/python/instance_manager/mpack-instance-manager.py,"@@ -152,24 +155,44 @@ def main(options, args):
                        components_map=parsed_components_map)
 
   elif action == GET_CONF_DIR_ACTION:
-    print get_conf_dir(mpack=options.mpack, mpack_instance=options.mpack_instance,
+    if options.format:
+      print json.dumps(get_conf_dir(mpack=options.mpack, mpack_instance=options.mpack_instance,
                        subgroup_name=options.subgroup_name, module_name=options.module_name,
-                       components_map=parsed_components_map)
+                       components_map=parsed_components_map), indent=INDENT)
+    else:
+      print json.dumps(get_conf_dir(mpack=options.mpack, mpack_instance=options.mpack_instance,
+                                    subgroup_name=options.subgroup_name, module_name=options.module_name,
+                                    components_map=parsed_components_map))","[{'comment': 'Can you please reduce code duplication by always passing the appropriate `indent`?\r\n\r\n```\r\nindent = INDENT if options.format else None\r\nprint json.dumps(get_conf_dir(...), indent=indent)\r\n```\r\n\r\n(Also in other similar places.)', 'commenter': 'adoroszlai'}, {'comment': 'Will update', 'commenter': 'scottduan'}]"
1637,mpack-instance-manager/src/main/python/instance_manager/instance_manager.py,"@@ -686,5 +689,12 @@ def build_json_output(self, output_conf_dir, output_log_dir, output_run_dir, out
     if output_run_dir:
       result['run_dir'] = os.path.join(self.component_path, RUN_DIRECTORY_NAME)
     if output_path:
-      result['path'] = self.path_exec
+      result['mpack_path'] = self.path_exec","[{'comment': 'This needs a change in `ambari-common`, too.\r\n\r\n```\r\n  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/mpack_manager_helper.py"", line 96, in get_component_home_path\r\n    component_instance_name=component_instance_name)\r\n  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/mpack_manager_helper.py"", line 83, in get_component_target_path\r\n    COMPONENT_INSTANCES_PLURAL_KEY_NAME][component_instance_name][PATH_KEY_NAME]\r\nKeyError: \'path\'\r\n```', 'commenter': 'adoroszlai'}, {'comment': 'Not sure why we need change in mpack_manager_helper.py, the change in instance_manager.py is to list_instance, in mpack_manager_helper.py, they are to get_component_home_path and  get_component_target_path. ', 'commenter': 'scottduan'}, {'comment': 'Because `get_component_target_path` calls `list_instances`?\r\n\r\nhttps://github.com/apache/ambari/blob/22c2e5b3d57a12ce23d8a9eee4b95370972b2f9e/ambari-common/src/main/python/resource_management/libraries/functions/mpack_manager_helper.py#L72-L80', 'commenter': 'adoroszlai'}, {'comment': 'You are right. Thanks.', 'commenter': 'scottduan'}]"
1637,mpack-instance-manager/src/main/python/instance_manager/mpack-instance-manager.py,"@@ -152,25 +157,24 @@ def main(options, args):
                        components_map=parsed_components_map)
 
   elif action == GET_CONF_DIR_ACTION:
-    print get_conf_dir(mpack=options.mpack, mpack_instance=options.mpack_instance,
+    print json.dumps(get_conf_dir(mpack=options.mpack, mpack_instance=options.mpack_instance,","[{'comment': 'The command line will still print an indented json output. That is not what is desirable. The command line should return a human readable output and NOT a JSON structure. \r\nFor example see the output of tree command\r\n```\r\ntree /usr/hwx/mpacks\r\n/usr/hwx/mpacks\r\n hdpcore\r\n\xa0\xa0  1.0.0-b415\r\n\xa0\xa0      app_timeline_server -> /usr/hwx/modules/yarn/3.0.0.0-b167\r\n\xa0\xa0      datanode -> /usr/hwx/modules/hdfs/3.0.0.0-b167\r\n\xa0\xa0      hadoop_client -> /usr/hwx/modules/hadoop_clients/3.0.0.0-b167\r\n\xa0\xa0      historyserver -> /usr/hwx/modules/mapreduce2/3.0.0.0-b167\r\n\xa0\xa0      journalnode -> /usr/hwx/modules/hdfs/3.0.0.0-b167\r\n\xa0\xa0      mpack.json\r\n\xa0\xa0      namenode -> /usr/hwx/modules/hdfs/3.0.0.0-b167\r\n\xa0\xa0      nfs_gateway -> /usr/hwx/modules/hdfs/3.0.0.0-b167\r\n\xa0\xa0      nodemanager -> /usr/hwx/modules/yarn/3.0.0.0-b167\r\n\xa0\xa0      resourcemanager -> /usr/hwx/modules/yarn/3.0.0.0-b167\r\n\xa0\xa0      secondary_namenode -> /usr/hwx/modules/hdfs/3.0.0.0-b167\r\n\xa0\xa0      timeline_reader -> /usr/hwx/modules/yarn/3.0.0.0-b167\r\n\xa0\xa0      yarn_registry_dns -> /usr/hwx/modules/yarn/3.0.0.0-b167\r\n\xa0\xa0      zkfc -> /usr/hwx/modules/hdfs/3.0.0.0-b167\r\n\xa0\xa0      zookeeper_client -> /usr/hwx/modules/zookeeper_clients/3.4.0.0-b38\r\n\xa0\xa0      zookeeper_server -> /usr/hwx/modules/zookeeper/3.4.0.0-b38\r\n ods\r\n     1.0.0-b279\r\n         hadoop_client -> /usr/hwx/modules/hadoop_clients/3.0.0.0-b166\r\n         hbase_client -> /usr/hwx/modules/hbase_clients/2.0.0.0-b219\r\n         hbase_master -> /usr/hwx/modules/hbase/2.0.0.0-b219\r\n         hbase_regionserver -> /usr/hwx/modules/hbase/2.0.0.0-b219\r\n         mpack.json\r\n         zookeeper_client -> /usr/hwx/modules/zookeeper_clients/3.4.0.0-b38\r\n```\r\n\r\nThe command line should print something like the following \r\n\r\n```\r\nhdpcore\r\n  |__ HDPCORE (default)\r\n      |__ zookeeper\r\n            |__ zookeeper_server (default)\r\n                     | mpack_version -> 1.0.0-b415\r\n                     | module_version -> 3.4.0.0-b38\r\n                     | instance_path -> /usr/hwx/instances/..../current\r\n                     | mpack_path -> /usr/hwx/mpacks/....\r\n                     | module_path -> /usr/hwx/modules/....\r\n                     | conf_dir -> /usr/hwx/instances/..../conf\r\n                     | log_dir -> /usr/hwx/instances/..../logs                            \r\n                     | run_dir -> /usr/hwx/instances/..../run\r\n```', 'commenter': 'jayush'}, {'comment': 'Lets work on the formatting of the command line output in a non-JSON format in a separate JIRA. ', 'commenter': 'jayush'}, {'comment': 'I agree, this jira is a blocker for some functionality missing in mpack_instance_manager, the format of output is just for cli stdout, the tree-like output will involve more work, I will create another jira for it.', 'commenter': 'scottduan'}, {'comment': 'A new JIRA: AMBARI-24196 has been filed to address this new requirement', 'commenter': 'scottduan'}, {'comment': ""OK, I'm fine with that."", 'commenter': 'jonathan-hurley'}]"
1637,mpack-instance-manager/src/main/python/instance_manager/mpack-instance-manager.py,"@@ -119,7 +121,8 @@ def init_get_parser_options(parser):
   parser.add_option('--components-map', default=None,
                     help=""map of 'component type' (eg: hive_server, metastore etc) as key and List of component instance name(s) to be given (eg: HS-1, finance_metastore) as value OR Empty map for all component instances present"",
                     dest=""components_map"")
-
+  parser.add_option('-f', '--format', action=""store_true"",","[{'comment': 'The desired usage is --format=json or --format=simple/plain/readable (by default the command line should output in simple/plain/readable format). Ambari will directly use the mpack-instance-manager python library and not use the command line and will always read in structured json format. \r\n ', 'commenter': 'jayush'}]"
1637,mpack-instance-manager/src/main/python/instance_manager/mpack-instance-manager.py,"@@ -152,25 +157,24 @@ def main(options, args):
                        components_map=parsed_components_map)
 
   elif action == GET_CONF_DIR_ACTION:
-    print get_conf_dir(mpack=options.mpack, mpack_instance=options.mpack_instance,
+    print json.dumps(get_conf_dir(mpack=options.mpack, mpack_instance=options.mpack_instance,
                        subgroup_name=options.subgroup_name, module_name=options.module_name,
-                       components_map=parsed_components_map)
+                       components_map=parsed_components_map), indent=indent)
 
   elif action == GET_LOG_DIR_ACTION:
-    print get_log_dir(mpack=options.mpack, mpack_instance=options.mpack_instance,
+    print json.dumps(get_log_dir(mpack=options.mpack, mpack_instance=options.mpack_instance,
                        subgroup_name=options.subgroup_name, module_name=options.module_name,
-                       components_map=parsed_components_map)
+                       components_map=parsed_components_map), indent=indent)
 
   elif action == GET_RUN_DIR_ACTION:
-    print get_run_dir(mpack=options.mpack, mpack_instance=options.mpack_instance,
+    print json.dumps(get_run_dir(mpack=options.mpack, mpack_instance=options.mpack_instance,
                        subgroup_name=options.subgroup_name, module_name=options.module_name,
-                       components_map=parsed_components_map)
+                       components_map=parsed_components_map), indent=indent)
 
   elif action == LIST_INSTANCES_ACTION:
-    print list_instances(mpack=options.mpack, mpack_instance=options.mpack_instance,
-                         subgroup_name=options.subgroup_name, module_name=options.module_name,
-                         components_map=parsed_components_map)
-
+    print json.dumps(list_instances(mpack=options.mpack, mpack_instance=options.mpack_instance,","[{'comment': ""We should simply update instance_manager.py::build_json_output() function to indent the json output. We shouldn't make changes to mpack_instance_manager.py to do this indentation after every function call. Please back out these indentation changes."", 'commenter': 'jayush'}, {'comment': '@jayush From programming point of view, indent is useless, it is only used to display readable output for the user, if we choose tree like output, the indent will be removed. instance_manager.py::build_json_output() only returns a python dict, I do not think indent  can be used here, I assume  indent can only apply to json.dumps.', 'commenter': 'scottduan'}]"
1642,ambari-logsearch/pom.xml,"@@ -131,6 +131,28 @@
       </plugins>
     </pluginManagement>
     <plugins>
+       <plugin>
+         <groupId>org.vafer</groupId>
+         <artifactId>jdeb</artifactId>
+         <version>1.0.1</version>","[{'comment': 'can you use version 1.4 for infra and logsearch?\r\nnote: just let you know, after 2.7.1 the ambari repo will be splitted into ambari, ambari-metrics,ambari-logsearch, ambari-infra, so those wont be built together (so possibly these changes wont be required for trunk)', 'commenter': 'oleewere'}, {'comment': 'OK sure I will fix it and re-push the patch, Thanks for the suggestion @oleewere ', 'commenter': 'nareshgbhat'}, {'comment': '@oleewere my apologies for delay.  As you have suggested I have modified and given pull request only for v2.7 branch https://github.com/apache/ambari/pull/1840\r\nShould we close this pull request  because it is for trunk ?', 'commenter': 'nareshgbhat'}, {'comment': ""maybe its ok to merge this to trunk, but long term it won't matter that much. on logsearch/infra side we want to move to gradle if possible (after the repo split), jdeb/rmp plugins will be used there properly with the right tasks"", 'commenter': 'oleewere'}, {'comment': '@oleewere ok Thanks,  I have updated the pull request.  Now the patch contains jdeb v1.4 for ambari-logsearch and ambari-infra', 'commenter': 'nareshgbhat'}]"
1646,ambari-server/src/test/java/org/apache/ambari/server/api/services/mpackadvisor/MpackAdvisorHelperTest.java,"@@ -0,0 +1,240 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.api.services.mpackadvisor;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+import static org.mockito.Mockito.doReturn;
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.spy;
+import static org.mockito.Mockito.when;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collection;
+
+import org.apache.ambari.server.api.services.AmbariMetaInfo;
+import org.apache.ambari.server.api.services.mpackadvisor.MpackAdvisorRequest.MpackAdvisorRequestType;
+import org.apache.ambari.server.api.services.mpackadvisor.commands.MpackAdvisorCommand;
+import org.apache.ambari.server.api.services.mpackadvisor.commands.MpackComponentLayoutRecommendationCommand;
+import org.apache.ambari.server.api.services.mpackadvisor.commands.MpackComponentLayoutValidationCommand;
+import org.apache.ambari.server.api.services.mpackadvisor.commands.MpackConfigurationValidationCommand;
+import org.apache.ambari.server.api.services.mpackadvisor.recommendations.MpackRecommendationResponse;
+import org.apache.ambari.server.api.services.mpackadvisor.validations.MpackValidationResponse;
+import org.apache.ambari.server.configuration.Configuration;
+import org.apache.ambari.server.state.ServiceInfo;
+import org.apache.ambari.server.topology.MpackInstance;
+import org.apache.ambari.server.topology.ServiceInstance;
+import org.junit.Test;
+import org.mockito.Mockito;
+
+/**
+ * MpackAdvisorHelper unit tests.
+ */
+public class MpackAdvisorHelperTest {
+  @Test
+  @SuppressWarnings(""unchecked"")
+  public void testValidate_returnsCommandResult() throws MpackAdvisorException, IOException {
+    Configuration configuration = mock(Configuration.class);
+    when(configuration.getRecommendationsArtifactsRolloverMax()).thenReturn(100);
+    MpackAdvisorRunner maRunner = mock(MpackAdvisorRunner.class);
+    AmbariMetaInfo metaInfo = mock(AmbariMetaInfo.class);
+    ServiceInfo service = mock(ServiceInfo.class);
+    when(metaInfo.getService(Mockito.anyString(), Mockito.anyString(), Mockito.anyString())).thenReturn(service);
+    when(service.getServiceAdvisorType()).thenReturn(ServiceInfo.ServiceAdvisorType.PYTHON);
+    MpackAdvisorHelper helper = mpackAdvisorHelperSpy(configuration, maRunner, metaInfo);
+
+    MpackAdvisorCommand<MpackValidationResponse> command = mock(MpackAdvisorCommand.class);
+    MpackValidationResponse expected = mock(MpackValidationResponse.class);
+
+    MpackAdvisorRequestType requestType = MpackAdvisorRequestType.HOST_GROUPS;
+    MpackAdvisorRequest request = MpackAdvisorRequest.MpackAdvisorRequestBuilder.forStack()
+        .ofType(requestType).build();
+
+    when(command.invoke(request)).thenReturn(expected);
+    doReturn(command).when(helper).createValidationCommand(request);
+    MpackValidationResponse response = helper.validate(request);
+    assertEquals(expected, response);
+  }
+
+  @Test(expected = MpackAdvisorException.class)
+  @SuppressWarnings(""unchecked"")
+  public void testValidate_commandThrowsException_throwsException() throws MpackAdvisorException,
+      IOException {
+    Configuration configuration = mock(Configuration.class);
+    when(configuration.getRecommendationsArtifactsRolloverMax()).thenReturn(100);
+    MpackAdvisorRunner maRunner = mock(MpackAdvisorRunner.class);
+    AmbariMetaInfo metaInfo = mock(AmbariMetaInfo.class);
+    ServiceInfo service = mock(ServiceInfo.class);
+    when(metaInfo.getService(Mockito.anyString(), Mockito.anyString(), Mockito.anyString())).thenReturn(service);
+    when(service.getServiceAdvisorType()).thenReturn(ServiceInfo.ServiceAdvisorType.PYTHON);
+    MpackAdvisorHelper helper = mpackAdvisorHelperSpy(configuration, maRunner, metaInfo);
+
+    MpackAdvisorCommand<MpackValidationResponse> command = mock(MpackAdvisorCommand.class);
+    MpackAdvisorRequest.MpackAdvisorRequestType requestType = MpackAdvisorRequest.MpackAdvisorRequestType.HOST_GROUPS;
+    MpackAdvisorRequest request = MpackAdvisorRequest.MpackAdvisorRequestBuilder.forStack().ofType(requestType).build();
+
+    when(command.invoke(request)).thenThrow(new MpackAdvisorException(""message""));
+    doReturn(command).when(helper).createValidationCommand(request);
+    helper.validate(request);
+
+    assertTrue(false);","[{'comment': 'This is unnecessary.', 'commenter': 'benyoka'}, {'comment': 'Removed.', 'commenter': 'swapanshridhar'}]"
1646,ambari-server/src/test/java/org/apache/ambari/server/api/services/mpackadvisor/MpackAdvisorRequestTypeTest.java,"@@ -0,0 +1,56 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.api.services.mpackadvisor;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+import org.apache.ambari.server.api.services.mpackadvisor.MpackAdvisorRequest.MpackAdvisorRequestType;
+import org.junit.Test;
+
+/**
+ * MpackAdvisorRequestTypeTest unit tests.
+ */
+public class MpackAdvisorRequestTypeTest {
+
+  @Test
+  public void testFromString_returnsHostGroupType() throws MpackAdvisorException {
+    String text = ""host_groups"";
+    MpackAdvisorRequestType type = MpackAdvisorRequestType.fromString(text);
+
+    assertEquals(type, MpackAdvisorRequestType.HOST_GROUPS);
+  }
+
+  @Test
+  public void testFromString_returnsConfigurationsType() throws MpackAdvisorException {
+    String text = ""configurations"";
+    MpackAdvisorRequestType type = MpackAdvisorRequestType.fromString(text);
+
+    assertEquals(type, MpackAdvisorRequestType.CONFIGURATIONS);
+  }
+
+  @Test(expected = MpackAdvisorException.class)
+  public void testFromString_throwsException() throws MpackAdvisorException {
+    String text = ""unknown_type"";
+    MpackAdvisorRequestType.fromString(text);
+
+    assertTrue(false);","[{'comment': 'Unnecessary', 'commenter': 'benyoka'}, {'comment': 'Removed.', 'commenter': 'swapanshridhar'}]"
1646,ambari-server/src/test/java/org/apache/ambari/server/api/services/mpackadvisor/MpackAdvisorRunnerTest.java,"@@ -0,0 +1,140 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.api.services.mpackadvisor;
+
+import static org.easymock.EasyMock.expect;
+import static org.junit.Assert.fail;
+import static org.powermock.api.easymock.PowerMock.createNiceMock;
+import static org.powermock.api.easymock.PowerMock.replay;
+import static org.powermock.api.support.membermodification.MemberModifier.stub;
+
+import java.io.File;
+import java.io.IOException;
+import java.util.HashMap;
+
+import org.apache.ambari.server.api.services.mpackadvisor.commands.MpackAdvisorCommandType;
+import org.apache.ambari.server.configuration.Configuration;
+import org.apache.ambari.server.state.ServiceInfo;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+import org.junit.rules.TemporaryFolder;
+import org.junit.runner.RunWith;
+import org.powermock.api.easymock.PowerMock;
+import org.powermock.core.classloader.annotations.PrepareForTest;
+import org.powermock.modules.junit4.PowerMockRunner;
+
+/**
+ * MpackAdvisorRunner unit tests.
+ */
+@RunWith(PowerMockRunner.class)
+@PrepareForTest(MpackAdvisorRunner.class)
+public class MpackAdvisorRunnerTest {
+
+  private TemporaryFolder temp = new TemporaryFolder();
+
+  @Before
+  public void setUp() throws IOException {
+    temp.create();
+  }
+
+  @After
+  public void tearDown() throws IOException {
+    temp.delete();
+  }
+
+  @Test(expected = MpackAdvisorException.class)
+  public void testRunScript_processStartThrowsException_returnFalse() throws Exception {
+    MpackAdvisorCommandType maCommandType = MpackAdvisorCommandType.RECOMMEND_COMPONENT_LAYOUT;","[{'comment': 'Method name indicates it returns false but it rather throws an exception.', 'commenter': 'benyoka'}, {'comment': 'Fixed.', 'commenter': 'swapanshridhar'}]"
1646,ambari-server/src/test/java/org/apache/ambari/server/api/services/mpackadvisor/MpackAdvisorRunnerTest.java,"@@ -0,0 +1,140 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.api.services.mpackadvisor;
+
+import static org.easymock.EasyMock.expect;
+import static org.junit.Assert.fail;
+import static org.powermock.api.easymock.PowerMock.createNiceMock;
+import static org.powermock.api.easymock.PowerMock.replay;
+import static org.powermock.api.support.membermodification.MemberModifier.stub;
+
+import java.io.File;
+import java.io.IOException;
+import java.util.HashMap;
+
+import org.apache.ambari.server.api.services.mpackadvisor.commands.MpackAdvisorCommandType;
+import org.apache.ambari.server.configuration.Configuration;
+import org.apache.ambari.server.state.ServiceInfo;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+import org.junit.rules.TemporaryFolder;
+import org.junit.runner.RunWith;
+import org.powermock.api.easymock.PowerMock;
+import org.powermock.core.classloader.annotations.PrepareForTest;
+import org.powermock.modules.junit4.PowerMockRunner;
+
+/**
+ * MpackAdvisorRunner unit tests.
+ */
+@RunWith(PowerMockRunner.class)
+@PrepareForTest(MpackAdvisorRunner.class)
+public class MpackAdvisorRunnerTest {
+
+  private TemporaryFolder temp = new TemporaryFolder();
+
+  @Before
+  public void setUp() throws IOException {
+    temp.create();
+  }
+
+  @After
+  public void tearDown() throws IOException {
+    temp.delete();
+  }
+
+  @Test(expected = MpackAdvisorException.class)
+  public void testRunScript_processStartThrowsException_returnFalse() throws Exception {
+    MpackAdvisorCommandType maCommandType = MpackAdvisorCommandType.RECOMMEND_COMPONENT_LAYOUT;
+    File actionDirectory = temp.newFolder(""actionDir"");
+    ProcessBuilder processBuilder = createNiceMock(ProcessBuilder.class);
+    MpackAdvisorRunner saRunner = new MpackAdvisorRunner();
+    Configuration configMock = createNiceMock(Configuration.class);
+    saRunner.setConfigs(configMock);
+    stub(PowerMock.method(MpackAdvisorRunner.class, ""prepareShellCommand""))
+        .toReturn(processBuilder);
+    expect(processBuilder.environment()).andReturn(new HashMap<>()).times(3);
+    expect(processBuilder.start()).andThrow(new IOException());
+    replay(processBuilder, configMock);
+    saRunner.runScript(ServiceInfo.ServiceAdvisorType.PYTHON, maCommandType, actionDirectory);
+  }
+
+  @Test(expected = MpackAdvisorRequestException.class)
+  public void testRunScript_processExitCode1_returnFalse() throws Exception {
+    String script = ""echo"";","[{'comment': 'Method name wrongly indicates expected outcome.', 'commenter': 'benyoka'}, {'comment': 'Fixed.', 'commenter': 'swapanshridhar'}]"
1646,ambari-server/src/test/java/org/apache/ambari/server/api/services/mpackadvisor/MpackAdvisorRunnerTest.java,"@@ -0,0 +1,140 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.api.services.mpackadvisor;
+
+import static org.easymock.EasyMock.expect;
+import static org.junit.Assert.fail;
+import static org.powermock.api.easymock.PowerMock.createNiceMock;
+import static org.powermock.api.easymock.PowerMock.replay;
+import static org.powermock.api.support.membermodification.MemberModifier.stub;
+
+import java.io.File;
+import java.io.IOException;
+import java.util.HashMap;
+
+import org.apache.ambari.server.api.services.mpackadvisor.commands.MpackAdvisorCommandType;
+import org.apache.ambari.server.configuration.Configuration;
+import org.apache.ambari.server.state.ServiceInfo;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+import org.junit.rules.TemporaryFolder;
+import org.junit.runner.RunWith;
+import org.powermock.api.easymock.PowerMock;
+import org.powermock.core.classloader.annotations.PrepareForTest;
+import org.powermock.modules.junit4.PowerMockRunner;
+
+/**
+ * MpackAdvisorRunner unit tests.
+ */
+@RunWith(PowerMockRunner.class)
+@PrepareForTest(MpackAdvisorRunner.class)
+public class MpackAdvisorRunnerTest {
+
+  private TemporaryFolder temp = new TemporaryFolder();
+
+  @Before
+  public void setUp() throws IOException {
+    temp.create();
+  }
+
+  @After
+  public void tearDown() throws IOException {
+    temp.delete();
+  }
+
+  @Test(expected = MpackAdvisorException.class)
+  public void testRunScript_processStartThrowsException_returnFalse() throws Exception {
+    MpackAdvisorCommandType maCommandType = MpackAdvisorCommandType.RECOMMEND_COMPONENT_LAYOUT;
+    File actionDirectory = temp.newFolder(""actionDir"");
+    ProcessBuilder processBuilder = createNiceMock(ProcessBuilder.class);
+    MpackAdvisorRunner saRunner = new MpackAdvisorRunner();
+    Configuration configMock = createNiceMock(Configuration.class);
+    saRunner.setConfigs(configMock);
+    stub(PowerMock.method(MpackAdvisorRunner.class, ""prepareShellCommand""))
+        .toReturn(processBuilder);
+    expect(processBuilder.environment()).andReturn(new HashMap<>()).times(3);
+    expect(processBuilder.start()).andThrow(new IOException());
+    replay(processBuilder, configMock);
+    saRunner.runScript(ServiceInfo.ServiceAdvisorType.PYTHON, maCommandType, actionDirectory);
+  }
+
+  @Test(expected = MpackAdvisorRequestException.class)
+  public void testRunScript_processExitCode1_returnFalse() throws Exception {
+    String script = ""echo"";
+    MpackAdvisorCommandType maCommandType = MpackAdvisorCommandType.RECOMMEND_COMPONENT_LAYOUT;
+    File actionDirectory = temp.newFolder(""actionDir"");
+    ProcessBuilder processBuilder = createNiceMock(ProcessBuilder.class);
+    Process process = createNiceMock(Process.class);
+    MpackAdvisorRunner maRunner = new MpackAdvisorRunner();
+    Configuration configMock = createNiceMock(Configuration.class);
+    maRunner.setConfigs(configMock);
+    stub(PowerMock.method(MpackAdvisorRunner.class, ""prepareShellCommand""))
+        .toReturn(processBuilder);
+    expect(processBuilder.environment()).andReturn(new HashMap<>()).times(3);
+    expect(processBuilder.start()).andReturn(process);
+    expect(process.waitFor()).andReturn(1);
+    replay(processBuilder, process, configMock);
+    maRunner.runScript(ServiceInfo.ServiceAdvisorType.PYTHON, maCommandType, actionDirectory);
+  }
+
+  @Test(expected = MpackAdvisorException.class)
+  public void testRunScript_processExitCode2_returnFalse() throws Exception {
+    String script = ""echo"";
+    MpackAdvisorCommandType maCommandType = MpackAdvisorCommandType.RECOMMEND_COMPONENT_LAYOUT;
+    File actionDirectory = temp.newFolder(""actionDir"");
+    ProcessBuilder processBuilder = createNiceMock(ProcessBuilder.class);
+    Process process = createNiceMock(Process.class);
+    MpackAdvisorRunner maRunner = new MpackAdvisorRunner();
+    Configuration configMock = createNiceMock(Configuration.class);
+    maRunner.setConfigs(configMock);
+
+    stub(PowerMock.method(MpackAdvisorRunner.class, ""prepareShellCommand""))
+        .toReturn(processBuilder);
+    expect(processBuilder.environment()).andReturn(new HashMap<>()).times(3);
+    expect(processBuilder.start()).andReturn(process);
+    expect(process.waitFor()).andReturn(2);
+    replay(processBuilder, process, configMock);
+    maRunner.runScript(ServiceInfo.ServiceAdvisorType.PYTHON, maCommandType, actionDirectory);
+  }
+
+  @Test
+  public void testRunScript_processExitCodeZero_returnTrue() throws Exception {
+    String script = ""echo"";
+    MpackAdvisorCommandType maCommandType = MpackAdvisorCommandType.RECOMMEND_COMPONENT_LAYOUT;
+    File actionDirectory = temp.newFolder(""actionDir"");
+    ProcessBuilder processBuilder = createNiceMock(ProcessBuilder.class);
+    Process process = createNiceMock(Process.class);
+    MpackAdvisorRunner maRunner = new MpackAdvisorRunner();
+    Configuration configMock = createNiceMock(Configuration.class);
+    maRunner.setConfigs(configMock);
+
+    stub(PowerMock.method(MpackAdvisorRunner.class, ""prepareShellCommand""))
+        .toReturn(processBuilder);
+    expect(processBuilder.environment()).andReturn(new HashMap<>()).times(3);
+    expect(processBuilder.start()).andReturn(process);
+    expect(process.waitFor()).andReturn(0);
+    replay(processBuilder, process, configMock);
+    try {
+      maRunner.runScript(ServiceInfo.ServiceAdvisorType.PYTHON, maCommandType, actionDirectory);
+    } catch (MpackAdvisorException ex) {
+      fail(""Should not fail with MpackAdvisorException"");
+    }
+  }","[{'comment': 'This try-catch is not necessary.', 'commenter': 'benyoka'}, {'comment': 'Removed.', 'commenter': 'swapanshridhar'}]"
1646,ambari-server/src/test/java/org/apache/ambari/server/api/services/mpackadvisor/MpackAdvisorRunnerTest.java,"@@ -0,0 +1,140 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.api.services.mpackadvisor;
+
+import static org.easymock.EasyMock.expect;
+import static org.junit.Assert.fail;
+import static org.powermock.api.easymock.PowerMock.createNiceMock;
+import static org.powermock.api.easymock.PowerMock.replay;
+import static org.powermock.api.support.membermodification.MemberModifier.stub;
+
+import java.io.File;
+import java.io.IOException;
+import java.util.HashMap;
+
+import org.apache.ambari.server.api.services.mpackadvisor.commands.MpackAdvisorCommandType;
+import org.apache.ambari.server.configuration.Configuration;
+import org.apache.ambari.server.state.ServiceInfo;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+import org.junit.rules.TemporaryFolder;
+import org.junit.runner.RunWith;
+import org.powermock.api.easymock.PowerMock;
+import org.powermock.core.classloader.annotations.PrepareForTest;
+import org.powermock.modules.junit4.PowerMockRunner;
+
+/**
+ * MpackAdvisorRunner unit tests.
+ */
+@RunWith(PowerMockRunner.class)
+@PrepareForTest(MpackAdvisorRunner.class)
+public class MpackAdvisorRunnerTest {
+
+  private TemporaryFolder temp = new TemporaryFolder();
+
+  @Before
+  public void setUp() throws IOException {
+    temp.create();
+  }
+
+  @After
+  public void tearDown() throws IOException {
+    temp.delete();
+  }
+
+  @Test(expected = MpackAdvisorException.class)
+  public void testRunScript_processStartThrowsException_returnFalse() throws Exception {
+    MpackAdvisorCommandType maCommandType = MpackAdvisorCommandType.RECOMMEND_COMPONENT_LAYOUT;
+    File actionDirectory = temp.newFolder(""actionDir"");
+    ProcessBuilder processBuilder = createNiceMock(ProcessBuilder.class);
+    MpackAdvisorRunner saRunner = new MpackAdvisorRunner();
+    Configuration configMock = createNiceMock(Configuration.class);
+    saRunner.setConfigs(configMock);
+    stub(PowerMock.method(MpackAdvisorRunner.class, ""prepareShellCommand""))
+        .toReturn(processBuilder);
+    expect(processBuilder.environment()).andReturn(new HashMap<>()).times(3);
+    expect(processBuilder.start()).andThrow(new IOException());
+    replay(processBuilder, configMock);
+    saRunner.runScript(ServiceInfo.ServiceAdvisorType.PYTHON, maCommandType, actionDirectory);
+  }
+
+  @Test(expected = MpackAdvisorRequestException.class)
+  public void testRunScript_processExitCode1_returnFalse() throws Exception {
+    String script = ""echo"";
+    MpackAdvisorCommandType maCommandType = MpackAdvisorCommandType.RECOMMEND_COMPONENT_LAYOUT;
+    File actionDirectory = temp.newFolder(""actionDir"");
+    ProcessBuilder processBuilder = createNiceMock(ProcessBuilder.class);
+    Process process = createNiceMock(Process.class);
+    MpackAdvisorRunner maRunner = new MpackAdvisorRunner();
+    Configuration configMock = createNiceMock(Configuration.class);
+    maRunner.setConfigs(configMock);
+    stub(PowerMock.method(MpackAdvisorRunner.class, ""prepareShellCommand""))
+        .toReturn(processBuilder);
+    expect(processBuilder.environment()).andReturn(new HashMap<>()).times(3);
+    expect(processBuilder.start()).andReturn(process);
+    expect(process.waitFor()).andReturn(1);
+    replay(processBuilder, process, configMock);
+    maRunner.runScript(ServiceInfo.ServiceAdvisorType.PYTHON, maCommandType, actionDirectory);
+  }
+
+  @Test(expected = MpackAdvisorException.class)
+  public void testRunScript_processExitCode2_returnFalse() throws Exception {
+    String script = ""echo"";
+    MpackAdvisorCommandType maCommandType = MpackAdvisorCommandType.RECOMMEND_COMPONENT_LAYOUT;
+    File actionDirectory = temp.newFolder(""actionDir"");
+    ProcessBuilder processBuilder = createNiceMock(ProcessBuilder.class);
+    Process process = createNiceMock(Process.class);
+    MpackAdvisorRunner maRunner = new MpackAdvisorRunner();
+    Configuration configMock = createNiceMock(Configuration.class);
+    maRunner.setConfigs(configMock);
+
+    stub(PowerMock.method(MpackAdvisorRunner.class, ""prepareShellCommand""))
+        .toReturn(processBuilder);
+    expect(processBuilder.environment()).andReturn(new HashMap<>()).times(3);
+    expect(processBuilder.start()).andReturn(process);
+    expect(process.waitFor()).andReturn(2);
+    replay(processBuilder, process, configMock);","[{'comment': 'This test is almost the same as the previous one. Both tests could call a method with the common logic that would take the exit code as parameter.', 'commenter': 'benyoka'}, {'comment': 'Created common fn.', 'commenter': 'swapanshridhar'}]"
1646,ambari-server/src/test/java/org/apache/ambari/server/api/services/mpackadvisor/commands/MpackAdvisorCommandTest.java,"@@ -0,0 +1,291 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.api.services.mpackadvisor.commands;
+
+import static org.easymock.EasyMock.expect;
+import static org.easymock.EasyMock.replay;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+import static org.mockito.Matchers.any;
+import static org.mockito.Matchers.anyString;
+import static org.mockito.Mockito.doAnswer;
+import static org.mockito.Mockito.doReturn;
+import static org.mockito.Mockito.doThrow;
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.spy;
+
+import java.io.File;
+import java.io.IOException;
+import java.nio.file.Files;
+import java.nio.file.Paths;
+import java.util.Collections;
+import java.util.Iterator;
+
+import javax.ws.rs.core.HttpHeaders;
+import javax.ws.rs.core.MediaType;
+import javax.ws.rs.core.Response;
+import javax.ws.rs.core.UriInfo;
+
+import org.apache.ambari.server.api.resources.ResourceInstance;
+import org.apache.ambari.server.api.services.AmbariMetaInfo;
+import org.apache.ambari.server.api.services.Request;
+import org.apache.ambari.server.api.services.mpackadvisor.MpackAdvisorException;
+import org.apache.ambari.server.api.services.mpackadvisor.MpackAdvisorRequest;
+import org.apache.ambari.server.api.services.mpackadvisor.MpackAdvisorResponse;
+import org.apache.ambari.server.api.services.mpackadvisor.MpackAdvisorRunner;
+import org.apache.ambari.server.controller.AmbariManagementController;
+import org.apache.ambari.server.controller.AmbariServer;
+import org.apache.ambari.server.controller.internal.AmbariServerConfigurationHandler;
+import org.apache.ambari.server.state.ServiceInfo;
+import org.apache.ambari.server.topology.AmbariContext;
+import org.apache.commons.io.FileUtils;
+import org.codehaus.jackson.JsonNode;
+import org.codehaus.jackson.annotate.JsonProperty;
+import org.codehaus.jackson.map.ObjectMapper;
+import org.codehaus.jackson.node.ArrayNode;
+import org.codehaus.jackson.node.ObjectNode;
+import org.easymock.EasyMock;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+import org.junit.rules.TemporaryFolder;
+import org.mockito.Mock;
+import org.mockito.invocation.InvocationOnMock;
+import org.mockito.stubbing.Answer;
+import org.powermock.core.classloader.annotations.PrepareForTest;
+
+import com.google.common.io.Resources;
+
+@PrepareForTest({AmbariContext.class, AmbariServer.class, AmbariManagementController.class})
+public class MpackAdvisorCommandTest {
+  String servicesJSON = null;
+  private TemporaryFolder temp = new TemporaryFolder();
+  @Mock
+  AmbariServerConfigurationHandler ambariServerConfigurationHandler;
+
+  @Before
+  public void setUp() throws IOException {
+    temp.create();
+  }
+
+  @After
+  public void tearDown() throws IOException {
+    temp.delete();
+  }
+
+  @Test(expected = MpackAdvisorException.class)
+  public void testInvoke_invalidRequest_throwsException() throws MpackAdvisorException {
+    File recommendationsDir = temp.newFolder(""recommendationDir"");
+    String recommendationsArtifactsLifetime = ""1w"";
+    int requestId = 0;
+    MpackAdvisorRunner maRunner = mock(MpackAdvisorRunner.class);
+    AmbariMetaInfo metaInfo = mock(AmbariMetaInfo.class);
+    doReturn(Collections.emptyList()).when(metaInfo).getStackParentVersions(anyString(), anyString());
+    MpackAdvisorCommand<MpackAdvisorCommandTest.TestResource> command = spy(new MpackAdvisorCommandTest.TestMpackAdvisorCommand(recommendationsDir, recommendationsArtifactsLifetime,
+        ServiceInfo.ServiceAdvisorType.PYTHON, requestId, maRunner, metaInfo));
+
+    MpackAdvisorRequest request = MpackAdvisorRequest.MpackAdvisorRequestBuilder.forStack().build();
+
+    doThrow(new MpackAdvisorException(""message"")).when(command).validate(request);
+    command.invoke(request);
+
+    assertTrue(false);","[{'comment': 'not necessary', 'commenter': 'benyoka'}, {'comment': 'Removed.', 'commenter': 'swapanshridhar'}]"
1646,ambari-server/src/test/java/org/apache/ambari/server/api/services/mpackadvisor/commands/MpackAdvisorCommandTest.java,"@@ -0,0 +1,291 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.api.services.mpackadvisor.commands;
+
+import static org.easymock.EasyMock.expect;
+import static org.easymock.EasyMock.replay;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+import static org.mockito.Matchers.any;
+import static org.mockito.Matchers.anyString;
+import static org.mockito.Mockito.doAnswer;
+import static org.mockito.Mockito.doReturn;
+import static org.mockito.Mockito.doThrow;
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.spy;
+
+import java.io.File;
+import java.io.IOException;
+import java.nio.file.Files;
+import java.nio.file.Paths;
+import java.util.Collections;
+import java.util.Iterator;
+
+import javax.ws.rs.core.HttpHeaders;
+import javax.ws.rs.core.MediaType;
+import javax.ws.rs.core.Response;
+import javax.ws.rs.core.UriInfo;
+
+import org.apache.ambari.server.api.resources.ResourceInstance;
+import org.apache.ambari.server.api.services.AmbariMetaInfo;
+import org.apache.ambari.server.api.services.Request;
+import org.apache.ambari.server.api.services.mpackadvisor.MpackAdvisorException;
+import org.apache.ambari.server.api.services.mpackadvisor.MpackAdvisorRequest;
+import org.apache.ambari.server.api.services.mpackadvisor.MpackAdvisorResponse;
+import org.apache.ambari.server.api.services.mpackadvisor.MpackAdvisorRunner;
+import org.apache.ambari.server.controller.AmbariManagementController;
+import org.apache.ambari.server.controller.AmbariServer;
+import org.apache.ambari.server.controller.internal.AmbariServerConfigurationHandler;
+import org.apache.ambari.server.state.ServiceInfo;
+import org.apache.ambari.server.topology.AmbariContext;
+import org.apache.commons.io.FileUtils;
+import org.codehaus.jackson.JsonNode;
+import org.codehaus.jackson.annotate.JsonProperty;
+import org.codehaus.jackson.map.ObjectMapper;
+import org.codehaus.jackson.node.ArrayNode;
+import org.codehaus.jackson.node.ObjectNode;
+import org.easymock.EasyMock;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+import org.junit.rules.TemporaryFolder;
+import org.mockito.Mock;
+import org.mockito.invocation.InvocationOnMock;
+import org.mockito.stubbing.Answer;
+import org.powermock.core.classloader.annotations.PrepareForTest;
+
+import com.google.common.io.Resources;
+
+@PrepareForTest({AmbariContext.class, AmbariServer.class, AmbariManagementController.class})
+public class MpackAdvisorCommandTest {
+  String servicesJSON = null;
+  private TemporaryFolder temp = new TemporaryFolder();
+  @Mock
+  AmbariServerConfigurationHandler ambariServerConfigurationHandler;
+
+  @Before
+  public void setUp() throws IOException {
+    temp.create();
+  }
+
+  @After
+  public void tearDown() throws IOException {
+    temp.delete();
+  }
+
+  @Test(expected = MpackAdvisorException.class)
+  public void testInvoke_invalidRequest_throwsException() throws MpackAdvisorException {
+    File recommendationsDir = temp.newFolder(""recommendationDir"");
+    String recommendationsArtifactsLifetime = ""1w"";
+    int requestId = 0;
+    MpackAdvisorRunner maRunner = mock(MpackAdvisorRunner.class);
+    AmbariMetaInfo metaInfo = mock(AmbariMetaInfo.class);
+    doReturn(Collections.emptyList()).when(metaInfo).getStackParentVersions(anyString(), anyString());
+    MpackAdvisorCommand<MpackAdvisorCommandTest.TestResource> command = spy(new MpackAdvisorCommandTest.TestMpackAdvisorCommand(recommendationsDir, recommendationsArtifactsLifetime,
+        ServiceInfo.ServiceAdvisorType.PYTHON, requestId, maRunner, metaInfo));
+
+    MpackAdvisorRequest request = MpackAdvisorRequest.MpackAdvisorRequestBuilder.forStack().build();
+
+    doThrow(new MpackAdvisorException(""message"")).when(command).validate(request);
+    command.invoke(request);
+
+    assertTrue(false);
+  }
+
+  @Test
+  public void testInvoke_success() throws Exception {
+    String expected = ""success"";
+    final String testResourceString = String.format(""{\""type\"": \""%s\""}"", expected);
+    final File recommendationsDir = temp.newFolder(""recommendationDir"");
+    String recommendationsArtifactsLifetime = ""1w"";
+    final int requestId = 2;
+    MpackAdvisorRunner maRunner = mock(MpackAdvisorRunner.class);
+    ObjectMapper mapper = mock(ObjectMapper.class);
+    AmbariMetaInfo metaInfo = mock(AmbariMetaInfo.class);
+    doReturn(Collections.emptyList()).when(metaInfo).getStackParentVersions(anyString(), anyString());
+    final MpackAdvisorCommand<MpackAdvisorCommandTest.TestResource> command = spy(new MpackAdvisorCommandTest.TestMpackAdvisorCommand(
+        recommendationsDir, recommendationsArtifactsLifetime, ServiceInfo.ServiceAdvisorType.PYTHON, requestId, maRunner, metaInfo));
+
+    MpackAdvisorRequest request = MpackAdvisorRequest.MpackAdvisorRequestBuilder.forStack().build();
+
+    String hostsFilePath = Resources.getResource(""mpack_advisor/hosts_count2.json"").getPath();
+    String hostsJSON = new String(Files.readAllBytes(Paths.get(hostsFilePath)));
+
+    String servicesFilePath = Resources.getResource(""mpack_advisor/ods_mpack_services.json"").getPath();
+    servicesJSON = new String(Files.readAllBytes(Paths.get(servicesFilePath)));
+
+    File file = mock(File.class);
+    AmbariMetaInfo ambariMetaInfo = mock(AmbariMetaInfo.class);
+
+    MpackAdvisorCommand<MpackAdvisorCommandTest.TestResource> cmd = new MpackAdvisorCommandTest.TestMpackAdvisorCommand(file, recommendationsArtifactsLifetime, ServiceInfo.ServiceAdvisorType.PYTHON, 1,
+        maRunner, ambariMetaInfo);
+    String objectNodeFilePath = Resources.getResource(""mpack_advisor/ods_mpack_objectnode.json"").getPath();
+    String objectNodeStr = new String(Files.readAllBytes(Paths.get(objectNodeFilePath)));
+    ObjectNode objectNode = (ObjectNode) cmd.mapper.readTree(objectNodeStr);
+
+    MpackAdvisorCommand.MpackAdvisorData data = new MpackAdvisorCommand.MpackAdvisorData(hostsJSON, servicesJSON);
+    doReturn(hostsJSON).when(command).getHostsInformation(request);
+    doReturn(data).when(command).getServicesInformation(request, hostsJSON);
+    doReturn(objectNode).when(command).adjust(servicesJSON, request);
+
+    doAnswer(new Answer() {
+      public Object answer(InvocationOnMock invocation) throws Throwable {
+        String resultFilePath = String.format(""%s/%s"", requestId, command.getResultFileName());
+        File resultFile = new File(recommendationsDir, resultFilePath);
+        resultFile.getParentFile().mkdirs();
+        FileUtils.writeStringToFile(resultFile, testResourceString);
+        return null;
+      }
+    }).when(maRunner).runScript(any(ServiceInfo.ServiceAdvisorType.class), any(MpackAdvisorCommandType.class), any(File.class));","[{'comment': 'You could use a lambda here.', 'commenter': 'benyoka'}]"
1646,ambari-server/src/test/java/org/apache/ambari/server/api/services/mpackadvisor/commands/MpackAdvisorCommandTest.java,"@@ -0,0 +1,291 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.api.services.mpackadvisor.commands;
+
+import static org.easymock.EasyMock.expect;
+import static org.easymock.EasyMock.replay;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+import static org.mockito.Matchers.any;
+import static org.mockito.Matchers.anyString;
+import static org.mockito.Mockito.doAnswer;
+import static org.mockito.Mockito.doReturn;
+import static org.mockito.Mockito.doThrow;
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.spy;
+
+import java.io.File;
+import java.io.IOException;
+import java.nio.file.Files;
+import java.nio.file.Paths;
+import java.util.Collections;
+import java.util.Iterator;
+
+import javax.ws.rs.core.HttpHeaders;
+import javax.ws.rs.core.MediaType;
+import javax.ws.rs.core.Response;
+import javax.ws.rs.core.UriInfo;
+
+import org.apache.ambari.server.api.resources.ResourceInstance;
+import org.apache.ambari.server.api.services.AmbariMetaInfo;
+import org.apache.ambari.server.api.services.Request;
+import org.apache.ambari.server.api.services.mpackadvisor.MpackAdvisorException;
+import org.apache.ambari.server.api.services.mpackadvisor.MpackAdvisorRequest;
+import org.apache.ambari.server.api.services.mpackadvisor.MpackAdvisorResponse;
+import org.apache.ambari.server.api.services.mpackadvisor.MpackAdvisorRunner;
+import org.apache.ambari.server.controller.AmbariManagementController;
+import org.apache.ambari.server.controller.AmbariServer;
+import org.apache.ambari.server.controller.internal.AmbariServerConfigurationHandler;
+import org.apache.ambari.server.state.ServiceInfo;
+import org.apache.ambari.server.topology.AmbariContext;
+import org.apache.commons.io.FileUtils;
+import org.codehaus.jackson.JsonNode;
+import org.codehaus.jackson.annotate.JsonProperty;
+import org.codehaus.jackson.map.ObjectMapper;
+import org.codehaus.jackson.node.ArrayNode;
+import org.codehaus.jackson.node.ObjectNode;
+import org.easymock.EasyMock;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+import org.junit.rules.TemporaryFolder;
+import org.mockito.Mock;
+import org.mockito.invocation.InvocationOnMock;
+import org.mockito.stubbing.Answer;
+import org.powermock.core.classloader.annotations.PrepareForTest;
+
+import com.google.common.io.Resources;
+
+@PrepareForTest({AmbariContext.class, AmbariServer.class, AmbariManagementController.class})
+public class MpackAdvisorCommandTest {
+  String servicesJSON = null;
+  private TemporaryFolder temp = new TemporaryFolder();
+  @Mock
+  AmbariServerConfigurationHandler ambariServerConfigurationHandler;
+
+  @Before
+  public void setUp() throws IOException {
+    temp.create();
+  }
+
+  @After
+  public void tearDown() throws IOException {
+    temp.delete();
+  }
+
+  @Test(expected = MpackAdvisorException.class)
+  public void testInvoke_invalidRequest_throwsException() throws MpackAdvisorException {
+    File recommendationsDir = temp.newFolder(""recommendationDir"");
+    String recommendationsArtifactsLifetime = ""1w"";
+    int requestId = 0;
+    MpackAdvisorRunner maRunner = mock(MpackAdvisorRunner.class);
+    AmbariMetaInfo metaInfo = mock(AmbariMetaInfo.class);
+    doReturn(Collections.emptyList()).when(metaInfo).getStackParentVersions(anyString(), anyString());
+    MpackAdvisorCommand<MpackAdvisorCommandTest.TestResource> command = spy(new MpackAdvisorCommandTest.TestMpackAdvisorCommand(recommendationsDir, recommendationsArtifactsLifetime,
+        ServiceInfo.ServiceAdvisorType.PYTHON, requestId, maRunner, metaInfo));
+
+    MpackAdvisorRequest request = MpackAdvisorRequest.MpackAdvisorRequestBuilder.forStack().build();
+
+    doThrow(new MpackAdvisorException(""message"")).when(command).validate(request);
+    command.invoke(request);
+
+    assertTrue(false);
+  }
+
+  @Test
+  public void testInvoke_success() throws Exception {
+    String expected = ""success"";
+    final String testResourceString = String.format(""{\""type\"": \""%s\""}"", expected);
+    final File recommendationsDir = temp.newFolder(""recommendationDir"");
+    String recommendationsArtifactsLifetime = ""1w"";
+    final int requestId = 2;
+    MpackAdvisorRunner maRunner = mock(MpackAdvisorRunner.class);
+    ObjectMapper mapper = mock(ObjectMapper.class);
+    AmbariMetaInfo metaInfo = mock(AmbariMetaInfo.class);
+    doReturn(Collections.emptyList()).when(metaInfo).getStackParentVersions(anyString(), anyString());
+    final MpackAdvisorCommand<MpackAdvisorCommandTest.TestResource> command = spy(new MpackAdvisorCommandTest.TestMpackAdvisorCommand(
+        recommendationsDir, recommendationsArtifactsLifetime, ServiceInfo.ServiceAdvisorType.PYTHON, requestId, maRunner, metaInfo));
+
+    MpackAdvisorRequest request = MpackAdvisorRequest.MpackAdvisorRequestBuilder.forStack().build();
+
+    String hostsFilePath = Resources.getResource(""mpack_advisor/hosts_count2.json"").getPath();
+    String hostsJSON = new String(Files.readAllBytes(Paths.get(hostsFilePath)));
+
+    String servicesFilePath = Resources.getResource(""mpack_advisor/ods_mpack_services.json"").getPath();
+    servicesJSON = new String(Files.readAllBytes(Paths.get(servicesFilePath)));
+
+    File file = mock(File.class);
+    AmbariMetaInfo ambariMetaInfo = mock(AmbariMetaInfo.class);
+
+    MpackAdvisorCommand<MpackAdvisorCommandTest.TestResource> cmd = new MpackAdvisorCommandTest.TestMpackAdvisorCommand(file, recommendationsArtifactsLifetime, ServiceInfo.ServiceAdvisorType.PYTHON, 1,
+        maRunner, ambariMetaInfo);
+    String objectNodeFilePath = Resources.getResource(""mpack_advisor/ods_mpack_objectnode.json"").getPath();
+    String objectNodeStr = new String(Files.readAllBytes(Paths.get(objectNodeFilePath)));
+    ObjectNode objectNode = (ObjectNode) cmd.mapper.readTree(objectNodeStr);
+
+    MpackAdvisorCommand.MpackAdvisorData data = new MpackAdvisorCommand.MpackAdvisorData(hostsJSON, servicesJSON);
+    doReturn(hostsJSON).when(command).getHostsInformation(request);
+    doReturn(data).when(command).getServicesInformation(request, hostsJSON);
+    doReturn(objectNode).when(command).adjust(servicesJSON, request);
+
+    doAnswer(new Answer() {
+      public Object answer(InvocationOnMock invocation) throws Throwable {
+        String resultFilePath = String.format(""%s/%s"", requestId, command.getResultFileName());
+        File resultFile = new File(recommendationsDir, resultFilePath);
+        resultFile.getParentFile().mkdirs();
+        FileUtils.writeStringToFile(resultFile, testResourceString);
+        return null;
+      }
+    }).when(maRunner).runScript(any(ServiceInfo.ServiceAdvisorType.class), any(MpackAdvisorCommandType.class), any(File.class));
+
+    MpackAdvisorCommandTest.TestResource result = command.invoke(request);
+
+    assertEquals(expected, result.getType());
+    assertEquals(requestId, result.getId());
+  }
+
+  @Test
+  public void testPopulateServiceAdvisors() throws Exception {
+    final File recommendationsDir = temp.newFolder(""recommendationDir"");
+    String recommendationsArtifactsLifetime = ""1w"";
+    final int requestId = 2;
+    MpackAdvisorRunner maRunner = mock(MpackAdvisorRunner.class);
+    AmbariMetaInfo metaInfo = EasyMock.createNiceMock(AmbariMetaInfo.class);
+    ServiceInfo serviceInfo = EasyMock.createNiceMock(ServiceInfo.class);
+
+    String advisorPath = ""/var/lib/ambari-server/resources/stacks/ODS/1.0.0-b340/services/HBASE/service_advisor.py"";
+    String advisorName = ""HBASEServiceAdvisor"";
+    expect(serviceInfo.getAdvisorFile()).andReturn(
+        new File(advisorPath)).anyTimes();
+    expect(serviceInfo.getAdvisorName()).andReturn(advisorName).anyTimes();
+    expect(metaInfo.getService(""ODS"", ""1.0.0-b340"", ""HBASE"")).andReturn(serviceInfo).anyTimes();
+    expect(metaInfo.getService(""ODS"", ""1.0.0-b340"", ""HDFS_CLIENTS"")).andReturn(serviceInfo).anyTimes();
+    expect(metaInfo.getService(""ODS"", ""1.0.0-b340"", ""HBASE_CLIENTS"")).andReturn(serviceInfo).anyTimes();
+    replay(metaInfo, serviceInfo);
+
+    final MpackAdvisorCommand<MpackAdvisorCommandTest.TestResource> command = spy(new MpackAdvisorCommandTest.TestMpackAdvisorCommand(
+        recommendationsDir, recommendationsArtifactsLifetime, ServiceInfo.ServiceAdvisorType.PYTHON, requestId, maRunner, metaInfo));
+
+    File file = mock(File.class);
+    AmbariMetaInfo ambariMetaInfo = mock(AmbariMetaInfo.class);
+
+    MpackAdvisorCommand<MpackAdvisorCommandTest.TestResource> cmd = new MpackAdvisorCommandTest.TestMpackAdvisorCommand(
+        file, recommendationsArtifactsLifetime, ServiceInfo.ServiceAdvisorType.PYTHON, 1, maRunner, ambariMetaInfo);
+    String objectNodeFilePath = Resources.getResource(""mpack_advisor/ods_mpack_objectnode1.json"").getPath();
+    String objectNodeStr = new String(Files.readAllBytes(Paths.get(objectNodeFilePath)));
+    ObjectNode objectNode = (ObjectNode) cmd.mapper.readTree(objectNodeStr);
+
+    command.populateServiceAdvisors(objectNode);
+
+    // Verification for updated objectNode.
+    ArrayNode services = (ArrayNode) objectNode.get(""services"");
+    Iterator<JsonNode> servicesIter = services.getElements();
+    while (servicesIter.hasNext()) {
+      JsonNode service = servicesIter.next();
+      ObjectNode serviceVersion = (ObjectNode) service.get(""StackServices"");
+      if (serviceVersion.get(""service_name"").getTextValue().equals(""HBASE"")) {
+        assertEquals(serviceVersion.get(""advisor_name"").getTextValue(), advisorName);
+        assertEquals(serviceVersion.get(""advisor_path"").getTextValue(), advisorPath);
+      }
+    }
+  }
+
+  @Test
+  public void testGetServicesInformation() throws Exception {
+    final File recommendationsDir = temp.newFolder(""recommendationDir"");
+    String recommendationsArtifactsLifetime = ""1w"";
+    final int requestId = 2;
+    MpackAdvisorRunner maRunner = mock(MpackAdvisorRunner.class);
+    AmbariMetaInfo metaInfo = mock(AmbariMetaInfo.class);
+    doReturn(Collections.emptyList()).when(metaInfo).getStackParentVersions(anyString(), anyString());
+    final MpackAdvisorCommand<MpackAdvisorCommandTest.TestResource> command = spy(new MpackAdvisorCommandTest.TestMpackAdvisorCommand(
+        recommendationsDir, recommendationsArtifactsLifetime, ServiceInfo.ServiceAdvisorType.PYTHON, requestId, maRunner, metaInfo));
+
+    MpackAdvisorRequest request = MpackAdvisorRequest.MpackAdvisorRequestBuilder.forStack().build();
+
+    String hostsFilePath = Resources.getResource(""mpack_advisor/hosts_count2.json"").getPath();
+    String hostsJSON = new String(Files.readAllBytes(Paths.get(hostsFilePath)));
+
+    String servicesFilePath = Resources.getResource(""mpack_advisor/ods_mpack_services.json"").getPath();
+    servicesJSON = new String(Files.readAllBytes(Paths.get(servicesFilePath)));
+
+    File file = mock(File.class);
+    AmbariMetaInfo ambariMetaInfo = mock(AmbariMetaInfo.class);
+    MpackAdvisorCommand<MpackAdvisorCommandTest.TestResource> cmd = new MpackAdvisorCommandTest.TestMpackAdvisorCommand(
+        file, recommendationsArtifactsLifetime, ServiceInfo.ServiceAdvisorType.PYTHON, 1, maRunner, ambariMetaInfo);
+    String objectNodeFilePath = Resources.getResource(""mpack_advisor/ods_mpack_objectnode.json"").getPath();
+    String objectNodeStr = new String(Files.readAllBytes(Paths.get(objectNodeFilePath)));
+    ObjectNode objectNode = (ObjectNode) cmd.mapper.readTree(objectNodeStr);
+
+    MpackAdvisorCommand.MpackAdvisorData data = new MpackAdvisorCommand.MpackAdvisorData(hostsJSON, servicesJSON);
+    doReturn(hostsJSON).when(command).getHostsInformation(request);
+    doReturn(data).when(command).getServicesInformation(request, hostsJSON);
+    doReturn(objectNode).when(command).adjust(servicesJSON, request);
+
+    MpackAdvisorCommand.MpackAdvisorData result = command.getServicesInformation(request, hostsJSON);
+    assertEquals(result.servicesJSON, servicesJSON);
+  }
+","[{'comment': 'Looks like it is the mock that is being tested, not production code.', 'commenter': 'benyoka'}, {'comment': 'Correct. Fixed it.', 'commenter': 'swapanshridhar'}]"
1646,ambari-server/src/test/resources/mpack_advisor/ods_mpack_services.json,"@@ -0,0 +1,1676 @@
+""{
+  \""services\"" : [ {","[{'comment': 'Why is this a string containing quoted/escaped json?', 'commenter': 'benyoka'}, {'comment': 'Renamed it to .txt. Contains String.', 'commenter': 'swapanshridhar'}]"
1646,ambari-server/src/test/java/org/apache/ambari/server/api/services/mpackadvisor/MpackAdvisorHelperTest.java,"@@ -0,0 +1,240 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.api.services.mpackadvisor;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+import static org.mockito.Mockito.doReturn;
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.spy;
+import static org.mockito.Mockito.when;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collection;
+
+import org.apache.ambari.server.api.services.AmbariMetaInfo;
+import org.apache.ambari.server.api.services.mpackadvisor.MpackAdvisorRequest.MpackAdvisorRequestType;
+import org.apache.ambari.server.api.services.mpackadvisor.commands.MpackAdvisorCommand;
+import org.apache.ambari.server.api.services.mpackadvisor.commands.MpackComponentLayoutRecommendationCommand;
+import org.apache.ambari.server.api.services.mpackadvisor.commands.MpackComponentLayoutValidationCommand;
+import org.apache.ambari.server.api.services.mpackadvisor.commands.MpackConfigurationValidationCommand;
+import org.apache.ambari.server.api.services.mpackadvisor.recommendations.MpackRecommendationResponse;
+import org.apache.ambari.server.api.services.mpackadvisor.validations.MpackValidationResponse;
+import org.apache.ambari.server.configuration.Configuration;
+import org.apache.ambari.server.state.ServiceInfo;
+import org.apache.ambari.server.topology.MpackInstance;
+import org.apache.ambari.server.topology.ServiceInstance;
+import org.junit.Test;
+import org.mockito.Mockito;
+
+/**
+ * MpackAdvisorHelper unit tests.
+ */
+public class MpackAdvisorHelperTest {
+  @Test
+  @SuppressWarnings(""unchecked"")
+  public void testValidate_returnsCommandResult() throws MpackAdvisorException, IOException {
+    Configuration configuration = mock(Configuration.class);
+    when(configuration.getRecommendationsArtifactsRolloverMax()).thenReturn(100);
+    MpackAdvisorRunner maRunner = mock(MpackAdvisorRunner.class);
+    AmbariMetaInfo metaInfo = mock(AmbariMetaInfo.class);
+    ServiceInfo service = mock(ServiceInfo.class);
+    when(metaInfo.getService(Mockito.anyString(), Mockito.anyString(), Mockito.anyString())).thenReturn(service);
+    when(service.getServiceAdvisorType()).thenReturn(ServiceInfo.ServiceAdvisorType.PYTHON);
+    MpackAdvisorHelper helper = mpackAdvisorHelperSpy(configuration, maRunner, metaInfo);
+
+    MpackAdvisorCommand<MpackValidationResponse> command = mock(MpackAdvisorCommand.class);
+    MpackValidationResponse expected = mock(MpackValidationResponse.class);
+
+    MpackAdvisorRequestType requestType = MpackAdvisorRequestType.HOST_GROUPS;
+    MpackAdvisorRequest request = MpackAdvisorRequest.MpackAdvisorRequestBuilder.forStack()
+        .ofType(requestType).build();
+
+    when(command.invoke(request)).thenReturn(expected);
+    doReturn(command).when(helper).createValidationCommand(request);
+    MpackValidationResponse response = helper.validate(request);
+    assertEquals(expected, response);
+  }
+
+  @Test(expected = MpackAdvisorException.class)
+  @SuppressWarnings(""unchecked"")
+  public void testValidate_commandThrowsException_throwsException() throws MpackAdvisorException,
+      IOException {
+    Configuration configuration = mock(Configuration.class);
+    when(configuration.getRecommendationsArtifactsRolloverMax()).thenReturn(100);
+    MpackAdvisorRunner maRunner = mock(MpackAdvisorRunner.class);
+    AmbariMetaInfo metaInfo = mock(AmbariMetaInfo.class);
+    ServiceInfo service = mock(ServiceInfo.class);
+    when(metaInfo.getService(Mockito.anyString(), Mockito.anyString(), Mockito.anyString())).thenReturn(service);
+    when(service.getServiceAdvisorType()).thenReturn(ServiceInfo.ServiceAdvisorType.PYTHON);
+    MpackAdvisorHelper helper = mpackAdvisorHelperSpy(configuration, maRunner, metaInfo);
+
+    MpackAdvisorCommand<MpackValidationResponse> command = mock(MpackAdvisorCommand.class);
+    MpackAdvisorRequest.MpackAdvisorRequestType requestType = MpackAdvisorRequest.MpackAdvisorRequestType.HOST_GROUPS;
+    MpackAdvisorRequest request = MpackAdvisorRequest.MpackAdvisorRequestBuilder.forStack().ofType(requestType).build();
+
+    when(command.invoke(request)).thenThrow(new MpackAdvisorException(""message""));
+    doReturn(command).when(helper).createValidationCommand(request);
+    helper.validate(request);
+
+    assertTrue(false);
+  }
+
+  @Test
+  @SuppressWarnings(""unchecked"")
+  public void testRecommend_returnsCommandResult() throws MpackAdvisorException, IOException {
+    Configuration configuration = mock(Configuration.class);
+    when(configuration.getRecommendationsArtifactsRolloverMax()).thenReturn(100);
+    MpackAdvisorRunner maRunner = mock(MpackAdvisorRunner.class);
+    AmbariMetaInfo metaInfo = mock(AmbariMetaInfo.class);
+    ServiceInfo service = mock(ServiceInfo.class);
+    when(metaInfo.getService(Mockito.anyString(), Mockito.anyString(), Mockito.anyString())).thenReturn(service);
+    when(service.getServiceAdvisorType()).thenReturn(ServiceInfo.ServiceAdvisorType.PYTHON);
+    MpackAdvisorHelper helper = mpackAdvisorHelperSpy(configuration, maRunner, metaInfo);
+
+    MpackAdvisorCommand<MpackRecommendationResponse> command = mock(MpackAdvisorCommand.class);
+    MpackRecommendationResponse expected = mock(MpackRecommendationResponse.class);
+    MpackAdvisorRequest.MpackAdvisorRequestType requestType = MpackAdvisorRequest.MpackAdvisorRequestType.HOST_GROUPS;
+
+
+    MpackAdvisorRequest request = MpackAdvisorRequest.MpackAdvisorRequestBuilder
+        .forStack()
+        .ofType(requestType)
+        .forMpackInstances(createOdsMpackInstance())
+        .build();
+
+    when(command.invoke(request)).thenReturn(expected);
+    doReturn(command).when(helper).createRecommendationCommand(ServiceInfo.ServiceAdvisorType.PYTHON, request);
+    MpackRecommendationResponse response = helper.recommend(request);
+
+    assertEquals(expected, response);
+  }
+
+  @Test(expected = MpackAdvisorException.class)
+  @SuppressWarnings(""unchecked"")
+  public void testRecommend_commandThrowsException_throwsException() throws MpackAdvisorException,
+      IOException {
+    Configuration configuration = mock(Configuration.class);
+    when(configuration.getRecommendationsArtifactsRolloverMax()).thenReturn(100);
+    MpackAdvisorRunner saRunner = mock(MpackAdvisorRunner.class);
+    AmbariMetaInfo metaInfo = mock(AmbariMetaInfo.class);
+    ServiceInfo service = mock(ServiceInfo.class);
+    when(metaInfo.getService(Mockito.anyString(), Mockito.anyString(), Mockito.anyString())).thenReturn(service);
+    when(service.getServiceAdvisorType()).thenReturn(ServiceInfo.ServiceAdvisorType.PYTHON);
+    MpackAdvisorHelper helper = mpackAdvisorHelperSpy(configuration, saRunner, metaInfo);
+
+    MpackAdvisorCommand<MpackRecommendationResponse> command = mock(MpackAdvisorCommand.class);
+    MpackAdvisorRequest.MpackAdvisorRequestType requestType = MpackAdvisorRequest.MpackAdvisorRequestType.HOST_GROUPS;
+    MpackAdvisorRequest request = MpackAdvisorRequest.MpackAdvisorRequestBuilder.forStack()
+        .ofType(requestType).build();
+
+    when(command.invoke(request)).thenThrow(new MpackAdvisorException(""message""));
+    doReturn(command).when(helper).createRecommendationCommand(ServiceInfo.ServiceAdvisorType.PYTHON, request);
+    helper.recommend(request);
+
+    assertTrue(false);
+  }
+
+  @Test
+  public void testCreateRecommendationCommand_returnsComponentLayoutRecommendationCommand()
+      throws IOException, MpackAdvisorException {
+    Configuration configuration = mock(Configuration.class);
+    when(configuration.getRecommendationsArtifactsRolloverMax()).thenReturn(100);
+    MpackAdvisorRunner maRunner = mock(MpackAdvisorRunner.class);
+    AmbariMetaInfo metaInfo = mock(AmbariMetaInfo.class);
+    ServiceInfo service = mock(ServiceInfo.class);
+    when(metaInfo.getService(Mockito.anyString(), Mockito.anyString(), Mockito.anyString())).thenReturn(service);
+    when(service.getServiceAdvisorType()).thenReturn(ServiceInfo.ServiceAdvisorType.PYTHON);
+    MpackAdvisorHelper helper = new MpackAdvisorHelper(configuration, maRunner, metaInfo, null);
+    MpackAdvisorRequest.MpackAdvisorRequestType requestType = MpackAdvisorRequest.MpackAdvisorRequestType.HOST_GROUPS;
+    MpackAdvisorRequest request = MpackAdvisorRequest.MpackAdvisorRequestBuilder.forStack()
+        .ofType(requestType).build();
+
+    MpackAdvisorCommand<MpackRecommendationResponse> command = null;
+    try {
+      command = helper
+          .createRecommendationCommand(ServiceInfo.ServiceAdvisorType.PYTHON, request);
+    } catch (MpackAdvisorException e) {
+      e.printStackTrace();","[{'comment': ""Shouldn't catch `MpackAdvisorException`, let the test error with it, instead of `NullPointerException` below."", 'commenter': 'adoroszlai'}, {'comment': 'Removed', 'commenter': 'swapanshridhar'}]"
1646,ambari-server/src/test/java/org/apache/ambari/server/api/services/mpackadvisor/MpackAdvisorHelperTest.java,"@@ -0,0 +1,240 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.api.services.mpackadvisor;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+import static org.mockito.Mockito.doReturn;
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.spy;
+import static org.mockito.Mockito.when;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collection;
+
+import org.apache.ambari.server.api.services.AmbariMetaInfo;
+import org.apache.ambari.server.api.services.mpackadvisor.MpackAdvisorRequest.MpackAdvisorRequestType;
+import org.apache.ambari.server.api.services.mpackadvisor.commands.MpackAdvisorCommand;
+import org.apache.ambari.server.api.services.mpackadvisor.commands.MpackComponentLayoutRecommendationCommand;
+import org.apache.ambari.server.api.services.mpackadvisor.commands.MpackComponentLayoutValidationCommand;
+import org.apache.ambari.server.api.services.mpackadvisor.commands.MpackConfigurationValidationCommand;
+import org.apache.ambari.server.api.services.mpackadvisor.recommendations.MpackRecommendationResponse;
+import org.apache.ambari.server.api.services.mpackadvisor.validations.MpackValidationResponse;
+import org.apache.ambari.server.configuration.Configuration;
+import org.apache.ambari.server.state.ServiceInfo;
+import org.apache.ambari.server.topology.MpackInstance;
+import org.apache.ambari.server.topology.ServiceInstance;
+import org.junit.Test;
+import org.mockito.Mockito;
+
+/**
+ * MpackAdvisorHelper unit tests.
+ */
+public class MpackAdvisorHelperTest {
+  @Test
+  @SuppressWarnings(""unchecked"")
+  public void testValidate_returnsCommandResult() throws MpackAdvisorException, IOException {
+    Configuration configuration = mock(Configuration.class);
+    when(configuration.getRecommendationsArtifactsRolloverMax()).thenReturn(100);
+    MpackAdvisorRunner maRunner = mock(MpackAdvisorRunner.class);
+    AmbariMetaInfo metaInfo = mock(AmbariMetaInfo.class);
+    ServiceInfo service = mock(ServiceInfo.class);
+    when(metaInfo.getService(Mockito.anyString(), Mockito.anyString(), Mockito.anyString())).thenReturn(service);
+    when(service.getServiceAdvisorType()).thenReturn(ServiceInfo.ServiceAdvisorType.PYTHON);
+    MpackAdvisorHelper helper = mpackAdvisorHelperSpy(configuration, maRunner, metaInfo);
+
+    MpackAdvisorCommand<MpackValidationResponse> command = mock(MpackAdvisorCommand.class);
+    MpackValidationResponse expected = mock(MpackValidationResponse.class);
+
+    MpackAdvisorRequestType requestType = MpackAdvisorRequestType.HOST_GROUPS;
+    MpackAdvisorRequest request = MpackAdvisorRequest.MpackAdvisorRequestBuilder.forStack()
+        .ofType(requestType).build();
+
+    when(command.invoke(request)).thenReturn(expected);
+    doReturn(command).when(helper).createValidationCommand(request);
+    MpackValidationResponse response = helper.validate(request);
+    assertEquals(expected, response);
+  }
+
+  @Test(expected = MpackAdvisorException.class)
+  @SuppressWarnings(""unchecked"")
+  public void testValidate_commandThrowsException_throwsException() throws MpackAdvisorException,
+      IOException {
+    Configuration configuration = mock(Configuration.class);
+    when(configuration.getRecommendationsArtifactsRolloverMax()).thenReturn(100);
+    MpackAdvisorRunner maRunner = mock(MpackAdvisorRunner.class);
+    AmbariMetaInfo metaInfo = mock(AmbariMetaInfo.class);
+    ServiceInfo service = mock(ServiceInfo.class);
+    when(metaInfo.getService(Mockito.anyString(), Mockito.anyString(), Mockito.anyString())).thenReturn(service);
+    when(service.getServiceAdvisorType()).thenReturn(ServiceInfo.ServiceAdvisorType.PYTHON);
+    MpackAdvisorHelper helper = mpackAdvisorHelperSpy(configuration, maRunner, metaInfo);
+
+    MpackAdvisorCommand<MpackValidationResponse> command = mock(MpackAdvisorCommand.class);
+    MpackAdvisorRequest.MpackAdvisorRequestType requestType = MpackAdvisorRequest.MpackAdvisorRequestType.HOST_GROUPS;
+    MpackAdvisorRequest request = MpackAdvisorRequest.MpackAdvisorRequestBuilder.forStack().ofType(requestType).build();
+
+    when(command.invoke(request)).thenThrow(new MpackAdvisorException(""message""));
+    doReturn(command).when(helper).createValidationCommand(request);
+    helper.validate(request);
+
+    assertTrue(false);
+  }
+
+  @Test
+  @SuppressWarnings(""unchecked"")
+  public void testRecommend_returnsCommandResult() throws MpackAdvisorException, IOException {
+    Configuration configuration = mock(Configuration.class);
+    when(configuration.getRecommendationsArtifactsRolloverMax()).thenReturn(100);
+    MpackAdvisorRunner maRunner = mock(MpackAdvisorRunner.class);
+    AmbariMetaInfo metaInfo = mock(AmbariMetaInfo.class);
+    ServiceInfo service = mock(ServiceInfo.class);
+    when(metaInfo.getService(Mockito.anyString(), Mockito.anyString(), Mockito.anyString())).thenReturn(service);
+    when(service.getServiceAdvisorType()).thenReturn(ServiceInfo.ServiceAdvisorType.PYTHON);
+    MpackAdvisorHelper helper = mpackAdvisorHelperSpy(configuration, maRunner, metaInfo);
+
+    MpackAdvisorCommand<MpackRecommendationResponse> command = mock(MpackAdvisorCommand.class);
+    MpackRecommendationResponse expected = mock(MpackRecommendationResponse.class);
+    MpackAdvisorRequest.MpackAdvisorRequestType requestType = MpackAdvisorRequest.MpackAdvisorRequestType.HOST_GROUPS;
+
+
+    MpackAdvisorRequest request = MpackAdvisorRequest.MpackAdvisorRequestBuilder
+        .forStack()
+        .ofType(requestType)
+        .forMpackInstances(createOdsMpackInstance())
+        .build();
+
+    when(command.invoke(request)).thenReturn(expected);
+    doReturn(command).when(helper).createRecommendationCommand(ServiceInfo.ServiceAdvisorType.PYTHON, request);
+    MpackRecommendationResponse response = helper.recommend(request);
+
+    assertEquals(expected, response);
+  }
+
+  @Test(expected = MpackAdvisorException.class)
+  @SuppressWarnings(""unchecked"")
+  public void testRecommend_commandThrowsException_throwsException() throws MpackAdvisorException,
+      IOException {
+    Configuration configuration = mock(Configuration.class);
+    when(configuration.getRecommendationsArtifactsRolloverMax()).thenReturn(100);
+    MpackAdvisorRunner saRunner = mock(MpackAdvisorRunner.class);
+    AmbariMetaInfo metaInfo = mock(AmbariMetaInfo.class);
+    ServiceInfo service = mock(ServiceInfo.class);
+    when(metaInfo.getService(Mockito.anyString(), Mockito.anyString(), Mockito.anyString())).thenReturn(service);
+    when(service.getServiceAdvisorType()).thenReturn(ServiceInfo.ServiceAdvisorType.PYTHON);
+    MpackAdvisorHelper helper = mpackAdvisorHelperSpy(configuration, saRunner, metaInfo);
+
+    MpackAdvisorCommand<MpackRecommendationResponse> command = mock(MpackAdvisorCommand.class);
+    MpackAdvisorRequest.MpackAdvisorRequestType requestType = MpackAdvisorRequest.MpackAdvisorRequestType.HOST_GROUPS;
+    MpackAdvisorRequest request = MpackAdvisorRequest.MpackAdvisorRequestBuilder.forStack()
+        .ofType(requestType).build();
+
+    when(command.invoke(request)).thenThrow(new MpackAdvisorException(""message""));
+    doReturn(command).when(helper).createRecommendationCommand(ServiceInfo.ServiceAdvisorType.PYTHON, request);
+    helper.recommend(request);
+
+    assertTrue(false);
+  }
+
+  @Test
+  public void testCreateRecommendationCommand_returnsComponentLayoutRecommendationCommand()
+      throws IOException, MpackAdvisorException {
+    Configuration configuration = mock(Configuration.class);
+    when(configuration.getRecommendationsArtifactsRolloverMax()).thenReturn(100);
+    MpackAdvisorRunner maRunner = mock(MpackAdvisorRunner.class);
+    AmbariMetaInfo metaInfo = mock(AmbariMetaInfo.class);
+    ServiceInfo service = mock(ServiceInfo.class);
+    when(metaInfo.getService(Mockito.anyString(), Mockito.anyString(), Mockito.anyString())).thenReturn(service);
+    when(service.getServiceAdvisorType()).thenReturn(ServiceInfo.ServiceAdvisorType.PYTHON);
+    MpackAdvisorHelper helper = new MpackAdvisorHelper(configuration, maRunner, metaInfo, null);
+    MpackAdvisorRequest.MpackAdvisorRequestType requestType = MpackAdvisorRequest.MpackAdvisorRequestType.HOST_GROUPS;
+    MpackAdvisorRequest request = MpackAdvisorRequest.MpackAdvisorRequestBuilder.forStack()
+        .ofType(requestType).build();
+
+    MpackAdvisorCommand<MpackRecommendationResponse> command = null;
+    try {
+      command = helper
+          .createRecommendationCommand(ServiceInfo.ServiceAdvisorType.PYTHON, request);
+    } catch (MpackAdvisorException e) {
+      e.printStackTrace();
+    }
+
+    assertEquals(MpackComponentLayoutRecommendationCommand.class, command.getClass());
+  }
+
+  @Test
+  public void testCreateValidationCommand_returnsComponentLayoutValidationCommand()
+      throws IOException, MpackAdvisorException {
+    Configuration configuration = mock(Configuration.class);
+    when(configuration.getRecommendationsArtifactsRolloverMax()).thenReturn(100);
+    MpackAdvisorRunner saRunner = mock(MpackAdvisorRunner.class);
+    AmbariMetaInfo metaInfo = mock(AmbariMetaInfo.class);
+    ServiceInfo service = mock(ServiceInfo.class);
+    when(metaInfo.getService(Mockito.anyString(), Mockito.anyString(), Mockito.anyString())).thenReturn(service);
+    when(service.getServiceAdvisorType()).thenReturn(ServiceInfo.ServiceAdvisorType.PYTHON);
+    MpackAdvisorHelper helper = new MpackAdvisorHelper(configuration, saRunner, metaInfo, null);
+    MpackAdvisorRequest.MpackAdvisorRequestType requestType = MpackAdvisorRequest.MpackAdvisorRequestType.HOST_GROUPS;
+    MpackAdvisorRequest request = MpackAdvisorRequest.MpackAdvisorRequestBuilder.forStack()
+        .ofType(requestType).forMpackInstances(createOdsMpackInstance()).build();
+
+    MpackAdvisorCommand<MpackValidationResponse> command = helper.createValidationCommand(request);
+
+    assertEquals(MpackComponentLayoutValidationCommand.class, command.getClass());
+  }
+
+
+  @Test
+  public void testCreateValidationCommand_returnsConfigurationValidationCommand()
+      throws IOException, MpackAdvisorException {
+    Configuration configuration = mock(Configuration.class);
+    when(configuration.getRecommendationsArtifactsRolloverMax()).thenReturn(100);
+    MpackAdvisorRunner saRunner = mock(MpackAdvisorRunner.class);
+    AmbariMetaInfo metaInfo = mock(AmbariMetaInfo.class);
+    ServiceInfo service = mock(ServiceInfo.class);
+    when(metaInfo.getService(Mockito.anyString(), Mockito.anyString(), Mockito.anyString())).thenReturn(service);
+    when(service.getServiceAdvisorType()).thenReturn(ServiceInfo.ServiceAdvisorType.PYTHON);
+    MpackAdvisorHelper helper = new MpackAdvisorHelper(configuration, saRunner, metaInfo, null);
+    MpackAdvisorRequest.MpackAdvisorRequestType requestType = MpackAdvisorRequest.MpackAdvisorRequestType.CONFIGURATIONS;
+    MpackAdvisorRequest request = MpackAdvisorRequest.MpackAdvisorRequestBuilder.forStack()
+        .forMpackInstances(createOdsMpackInstance()).ofType(requestType).build();
+
+    MpackAdvisorCommand<MpackValidationResponse> command = helper.createValidationCommand(request);
+
+    assertEquals(MpackConfigurationValidationCommand.class, command.getClass());
+  }
+
+  /* Helper function to create ODS Mpack Instance
+   */
+  public static Collection<MpackInstance> createOdsMpackInstance() {
+
+    return new ArrayList<MpackInstance>() {{
+      add(new MpackInstance(""ODS-DEFAULT"", ""ODS"", ""1.0.0-b340"",
+          new ArrayList<ServiceInstance>() {
+            {
+              add(new ServiceInstance(""HBASE"", ""HBASE"", null));
+              add(new ServiceInstance(""HBASE_CLIENTS"", ""HBASE_CLIENTS"", null));
+              add(new ServiceInstance(""HADOOP_CLIENTS"", ""HADOOP_CLIENTS"", null));
+            }}));
+    }};","[{'comment': 'Can simplify using `ImmutableList` instead of anonymous subclass of `ArrayList`.', 'commenter': 'adoroszlai'}, {'comment': 'Done.', 'commenter': 'swapanshridhar'}]"
1646,ambari-server/src/test/java/org/apache/ambari/server/api/services/mpackadvisor/commands/MpackConfigurationRecommendationCommandTest.java,"@@ -0,0 +1,102 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.api.services.mpackadvisor.commands;
+
+import static org.easymock.EasyMock.expect;
+import static org.easymock.EasyMock.replay;
+import static org.mockito.Mockito.mock;
+
+import java.io.File;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.ambari.server.api.services.AmbariMetaInfo;
+import org.apache.ambari.server.api.services.mpackadvisor.MpackAdvisorException;
+import org.apache.ambari.server.api.services.mpackadvisor.MpackAdvisorHelperTest;
+import org.apache.ambari.server.api.services.mpackadvisor.MpackAdvisorRequest;
+import org.apache.ambari.server.api.services.mpackadvisor.MpackAdvisorRunner;
+import org.apache.ambari.server.state.ServiceInfo;
+import org.easymock.EasyMock;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+import org.junit.rules.TemporaryFolder;
+
+public class MpackConfigurationRecommendationCommandTest {
+  private TemporaryFolder temp = new TemporaryFolder();
+
+  @Before
+  public void setUp() throws IOException {
+    temp.create();
+  }
+
+  @After
+  public void tearDown() throws IOException {
+    temp.delete();
+  }
+
+  @Test(expected = Test.None.class /* no exception expected */)","[{'comment': ""`(expected = Test.None.class /* no exception expected */)` is unnecessary, as that's the default behavior."", 'commenter': 'adoroszlai'}, {'comment': 'Removed.', 'commenter': 'swapanshridhar'}]"
1646,ambari-server/src/test/java/org/apache/ambari/server/api/services/mpackadvisor/MpackAdvisorHelperTest.java,"@@ -0,0 +1,240 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.api.services.mpackadvisor;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+import static org.mockito.Mockito.doReturn;
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.spy;
+import static org.mockito.Mockito.when;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collection;
+
+import org.apache.ambari.server.api.services.AmbariMetaInfo;
+import org.apache.ambari.server.api.services.mpackadvisor.MpackAdvisorRequest.MpackAdvisorRequestType;
+import org.apache.ambari.server.api.services.mpackadvisor.commands.MpackAdvisorCommand;
+import org.apache.ambari.server.api.services.mpackadvisor.commands.MpackComponentLayoutRecommendationCommand;
+import org.apache.ambari.server.api.services.mpackadvisor.commands.MpackComponentLayoutValidationCommand;
+import org.apache.ambari.server.api.services.mpackadvisor.commands.MpackConfigurationValidationCommand;
+import org.apache.ambari.server.api.services.mpackadvisor.recommendations.MpackRecommendationResponse;
+import org.apache.ambari.server.api.services.mpackadvisor.validations.MpackValidationResponse;
+import org.apache.ambari.server.configuration.Configuration;
+import org.apache.ambari.server.state.ServiceInfo;
+import org.apache.ambari.server.topology.MpackInstance;
+import org.apache.ambari.server.topology.ServiceInstance;
+import org.junit.Test;
+import org.mockito.Mockito;
+
+/**
+ * MpackAdvisorHelper unit tests.
+ */
+public class MpackAdvisorHelperTest {
+  @Test
+  @SuppressWarnings(""unchecked"")
+  public void testValidate_returnsCommandResult() throws MpackAdvisorException, IOException {
+    Configuration configuration = mock(Configuration.class);
+    when(configuration.getRecommendationsArtifactsRolloverMax()).thenReturn(100);
+    MpackAdvisorRunner maRunner = mock(MpackAdvisorRunner.class);
+    AmbariMetaInfo metaInfo = mock(AmbariMetaInfo.class);
+    ServiceInfo service = mock(ServiceInfo.class);
+    when(metaInfo.getService(Mockito.anyString(), Mockito.anyString(), Mockito.anyString())).thenReturn(service);
+    when(service.getServiceAdvisorType()).thenReturn(ServiceInfo.ServiceAdvisorType.PYTHON);
+    MpackAdvisorHelper helper = mpackAdvisorHelperSpy(configuration, maRunner, metaInfo);
+
+    MpackAdvisorCommand<MpackValidationResponse> command = mock(MpackAdvisorCommand.class);
+    MpackValidationResponse expected = mock(MpackValidationResponse.class);
+
+    MpackAdvisorRequestType requestType = MpackAdvisorRequestType.HOST_GROUPS;
+    MpackAdvisorRequest request = MpackAdvisorRequest.MpackAdvisorRequestBuilder.forStack()
+        .ofType(requestType).build();
+
+    when(command.invoke(request)).thenReturn(expected);
+    doReturn(command).when(helper).createValidationCommand(request);
+    MpackValidationResponse response = helper.validate(request);
+    assertEquals(expected, response);
+  }
+
+  @Test(expected = MpackAdvisorException.class)
+  @SuppressWarnings(""unchecked"")
+  public void testValidate_commandThrowsException_throwsException() throws MpackAdvisorException,
+      IOException {
+    Configuration configuration = mock(Configuration.class);
+    when(configuration.getRecommendationsArtifactsRolloverMax()).thenReturn(100);
+    MpackAdvisorRunner maRunner = mock(MpackAdvisorRunner.class);
+    AmbariMetaInfo metaInfo = mock(AmbariMetaInfo.class);
+    ServiceInfo service = mock(ServiceInfo.class);
+    when(metaInfo.getService(Mockito.anyString(), Mockito.anyString(), Mockito.anyString())).thenReturn(service);
+    when(service.getServiceAdvisorType()).thenReturn(ServiceInfo.ServiceAdvisorType.PYTHON);
+    MpackAdvisorHelper helper = mpackAdvisorHelperSpy(configuration, maRunner, metaInfo);
+
+    MpackAdvisorCommand<MpackValidationResponse> command = mock(MpackAdvisorCommand.class);
+    MpackAdvisorRequest.MpackAdvisorRequestType requestType = MpackAdvisorRequest.MpackAdvisorRequestType.HOST_GROUPS;
+    MpackAdvisorRequest request = MpackAdvisorRequest.MpackAdvisorRequestBuilder.forStack().ofType(requestType).build();
+
+    when(command.invoke(request)).thenThrow(new MpackAdvisorException(""message""));
+    doReturn(command).when(helper).createValidationCommand(request);
+    helper.validate(request);
+
+    assertTrue(false);
+  }
+
+  @Test
+  @SuppressWarnings(""unchecked"")
+  public void testRecommend_returnsCommandResult() throws MpackAdvisorException, IOException {
+    Configuration configuration = mock(Configuration.class);
+    when(configuration.getRecommendationsArtifactsRolloverMax()).thenReturn(100);
+    MpackAdvisorRunner maRunner = mock(MpackAdvisorRunner.class);
+    AmbariMetaInfo metaInfo = mock(AmbariMetaInfo.class);
+    ServiceInfo service = mock(ServiceInfo.class);
+    when(metaInfo.getService(Mockito.anyString(), Mockito.anyString(), Mockito.anyString())).thenReturn(service);
+    when(service.getServiceAdvisorType()).thenReturn(ServiceInfo.ServiceAdvisorType.PYTHON);
+    MpackAdvisorHelper helper = mpackAdvisorHelperSpy(configuration, maRunner, metaInfo);
+
+    MpackAdvisorCommand<MpackRecommendationResponse> command = mock(MpackAdvisorCommand.class);
+    MpackRecommendationResponse expected = mock(MpackRecommendationResponse.class);
+    MpackAdvisorRequest.MpackAdvisorRequestType requestType = MpackAdvisorRequest.MpackAdvisorRequestType.HOST_GROUPS;
+
+
+    MpackAdvisorRequest request = MpackAdvisorRequest.MpackAdvisorRequestBuilder
+        .forStack()
+        .ofType(requestType)
+        .forMpackInstances(createOdsMpackInstance())
+        .build();
+
+    when(command.invoke(request)).thenReturn(expected);
+    doReturn(command).when(helper).createRecommendationCommand(ServiceInfo.ServiceAdvisorType.PYTHON, request);
+    MpackRecommendationResponse response = helper.recommend(request);
+
+    assertEquals(expected, response);
+  }
+
+  @Test(expected = MpackAdvisorException.class)
+  @SuppressWarnings(""unchecked"")
+  public void testRecommend_commandThrowsException_throwsException() throws MpackAdvisorException,
+      IOException {
+    Configuration configuration = mock(Configuration.class);
+    when(configuration.getRecommendationsArtifactsRolloverMax()).thenReturn(100);
+    MpackAdvisorRunner saRunner = mock(MpackAdvisorRunner.class);
+    AmbariMetaInfo metaInfo = mock(AmbariMetaInfo.class);
+    ServiceInfo service = mock(ServiceInfo.class);
+    when(metaInfo.getService(Mockito.anyString(), Mockito.anyString(), Mockito.anyString())).thenReturn(service);
+    when(service.getServiceAdvisorType()).thenReturn(ServiceInfo.ServiceAdvisorType.PYTHON);
+    MpackAdvisorHelper helper = mpackAdvisorHelperSpy(configuration, saRunner, metaInfo);","[{'comment': 'There is much duplication in these test methods.  Can you please try to reduce by extracting common setup?', 'commenter': 'adoroszlai'}, {'comment': 'Done. Added to @Before.', 'commenter': 'swapanshridhar'}]"
1652,ambari-agent/src/main/python/ambari_agent/AmbariConfig.py,"@@ -211,6 +221,45 @@ def extensions_dir(self):
   def host_scripts_dir(self):
     return os.path.join(self.cache_dir, FileCache.HOST_SCRIPTS_CACHE_DIRECTORY)
 
+  @property
+  def command_file_retention_policy(self):
+    """"""
+    Returns the Agent's command file retention policy.  This policy indicates what to do with the
+    command-*.json and status_command.json files after they are done being used to execute commands
+    from the Ambari server.
+
+    Possible policy values are:
+
+    * keep - Keep all command-*.json files
+    * remove - Remove command-*.json files if the operation was successful
+    * remove_on_success - Remove all command-*.json files when no longer needed","[{'comment': 'It seems the descriptions are swapped', 'commenter': 'smolnar82'}, {'comment': 'ugh.. thanks for noticing. ', 'commenter': 'rlevas'}]"
1652,ambari-agent/src/main/python/ambari_agent/CustomServiceOrchestrator.py,"@@ -456,6 +459,43 @@ def runCommand(self, command_header, tmpoutfile, tmperrfile, forced_command_name
       if incremented_commands_for_component:
         self.commands_for_component_in_progress[cluster_id][command['role']] -= 1
 
+      # Conditionally remove the command-*.json file if it exists
+      if os.path.exists(json_path):
+        command_file_retention_policy = self.config.command_file_retention_policy
+
+        if command_file_retention_policy == self.config.COMMAND_FILE_RETENTION_POLICY_REMOVE:
+          remove_command_file = True
+          logger.info(
+            'Removing %s due to the command_file_retention_policy, %s',
+            json_path, command_file_retention_policy
+          )
+        elif command_file_retention_policy == self.config.COMMAND_FILE_RETENTION_POLICY_REMOVE_ON_SUCCESS:
+          if ret and ('exitcode' in ret):
+            exit_code = ret['exitcode']
+            if exit_code == 0:
+              remove_command_file = True
+              logger.info(
+                'Removing %s due to the command_file_retention_policy, %s, and exit code, %d',
+                json_path, command_file_retention_policy, exit_code
+              )
+            else:
+              remove_command_file = False
+              logger.info(
+                'Not removing %s due to the command_file_retention_policy, %s, and exit code, %d',
+                json_path, command_file_retention_policy, exit_code
+              )
+          else:
+            remove_command_file = False
+            logger.info(
+              'Not Removing %s due to the command_file_retention_policy, %s, and a missing exit code value',
+              json_path, command_file_retention_policy
+            )
+        else:
+          remove_command_file = False","[{'comment': 'To follow the above pattern you may add logging here too about the command file is not being removed due to retention policy.', 'commenter': 'smolnar82'}, {'comment': ""Do we need to log this, though? I'd say only log the removals."", 'commenter': 'jonathan-hurley'}, {'comment': ""I agree, I think just logging removals (which is different the today's behavior) is better... else the log will get really big during normal operations.  \r\n\r\nMaybe the logging should be turned down for simple removals (is that possible in Python?) and generally logged for mis-fires - failure to remove due to error condition when the policy is remove_on_success. "", 'commenter': 'rlevas'}, {'comment': ""I'm convinced"", 'commenter': 'smolnar82'}, {'comment': 'Should we really flood ambari-agent.log with this? Since this is a default behaviour.', 'commenter': 'aonishuk'}, {'comment': 'No, we should not; it was only a question and I got the answer for it...', 'commenter': 'smolnar82'}]"
1652,ambari-agent/src/main/python/ambari_agent/CustomServiceOrchestrator.py,"@@ -456,6 +459,43 @@ def runCommand(self, command_header, tmpoutfile, tmperrfile, forced_command_name
       if incremented_commands_for_component:
         self.commands_for_component_in_progress[cluster_id][command['role']] -= 1
 
+      # Conditionally remove the command-*.json file if it exists
+      if os.path.exists(json_path):
+        command_file_retention_policy = self.config.command_file_retention_policy
+
+        if command_file_retention_policy == self.config.COMMAND_FILE_RETENTION_POLICY_REMOVE:
+          remove_command_file = True
+          logger.info(
+            'Removing %s due to the command_file_retention_policy, %s',
+            json_path, command_file_retention_policy
+          )
+        elif command_file_retention_policy == self.config.COMMAND_FILE_RETENTION_POLICY_REMOVE_ON_SUCCESS:
+          if ret and ('exitcode' in ret):
+            exit_code = ret['exitcode']
+            if exit_code == 0:
+              remove_command_file = True
+              logger.info(
+                'Removing %s due to the command_file_retention_policy, %s, and exit code, %d',
+                json_path, command_file_retention_policy, exit_code
+              )
+            else:
+              remove_command_file = False
+              logger.info(
+                'Not removing %s due to the command_file_retention_policy, %s, and exit code, %d',
+                json_path, command_file_retention_policy, exit_code","[{'comment': ""I don't think we need to log this behavior. "", 'commenter': 'jonathan-hurley'}, {'comment': 'really.. I was thinking that the was the only logging we really needed since it was an unexpected or undesired scenario. ', 'commenter': 'rlevas'}, {'comment': 'How about I change the logging to be `debug` rather than `info`?', 'commenter': 'rlevas'}, {'comment': ""I suppose that's fine if you think it's information that could be needed. On a normal basis, I would say it's not, but we can leave it as debug. "", 'commenter': 'jonathan-hurley'}, {'comment': 'Oh, did I make a mistake where I put my comment - I only wanted the normal ""keep"" behavior logging removed.', 'commenter': 'jonathan-hurley'}, {'comment': 'There is no logging for the normal keep path.\r\n', 'commenter': 'rlevas'}]"
1652,ambari-agent/src/main/python/ambari_agent/CustomServiceOrchestrator.py,"@@ -456,6 +462,43 @@ def runCommand(self, command_header, tmpoutfile, tmperrfile, forced_command_name
       if incremented_commands_for_component:
         self.commands_for_component_in_progress[cluster_id][command['role']] -= 1
 
+      # Conditionally remove the command-*.json file if it exists","[{'comment': 'Would be nice to extract these 35 lines to a new method.', 'commenter': 'adoroszlai'}, {'comment': 'Agreed... fixed. Thanks. ', 'commenter': 'rlevas'}]"
1653,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/UpgradeResourceProvider.java,"@@ -857,6 +857,8 @@ major stack versions (e.g., HDP 2.2 -> 2.3), and then set config changes
               Map<String, String> requestProperties = new HashMap<>();
               requestProperties.put(SupportedCustomOperation.REGENERATE_KEYTABS.name().toLowerCase(), ""missing"");
               requestProperties.put(KerberosHelper.ALLOW_RETRY, Boolean.TRUE.toString().toLowerCase());
+              requestProperties.put(KerberosHelper.DIRECTIVE_IGNORE_CONFIGS,","[{'comment': 'Unfortunately, this will prevent any changes to the configurations related to new Kerberos identities, which will cause failures if new configurations are expected. \r\n', 'commenter': 'rlevas'}, {'comment': 'I think a better change would be to remove any static configuration changes from the Kerberos descriptor, add them to service advisors, getServiceConfigurationRecommendationsForKerberos method for the relevant services.', 'commenter': 'rlevas'}]"
1657,ambari-server/src/main/java/org/apache/ambari/server/configuration/Configuration.java,"@@ -2261,6 +2261,15 @@
   public static final ConfigurationProperty<String> VIEWS_HTTP_CACHE_CONTROL_HEADER_VALUE = new ConfigurationProperty<>(
       ""views.http.cache-control"", ""no-store"");
 
+  /**
+   * The value that is additional classpath for the views. It will take comma separated paths. If the individual path is jar
+   * it will be included otherwise if it is a directory then all the files inside it will be included in the classpath. Directories
+   * WILL NOT BE navigated recursively
+   */
+  @Markdown(description = ""Additional class path added to each Ambari View. Semi colon separated jars or directories"")","[{'comment': 'Classpath value separator is contradictory: description says ""semi colon separated"", but comment says ""comma separated"".', 'commenter': 'adoroszlai'}, {'comment': 'correct to comma separated.', 'commenter': 'nitirajrathore'}]"
1657,ambari-server/src/main/java/org/apache/ambari/server/configuration/Configuration.java,"@@ -3739,6 +3748,19 @@ public String getViewsCacheControlHTTPResponseHeader() {
     return getProperty(VIEWS_HTTP_CACHE_CONTROL_HEADER_VALUE);
   }
 
+  /**
+   * Get the commann separated additional classpath, that should be added to view's classloader.","[{'comment': 'typo: ""commann""', 'commenter': 'adoroszlai'}, {'comment': 'correct spelling', 'commenter': 'nitirajrathore'}]"
1657,ambari-server/src/main/java/org/apache/ambari/server/view/ViewExtractor.java,"@@ -64,11 +65,12 @@
    * @param viewArchive  the view archive file
    * @param archiveDir   the view archive directory
    *
+   * @param viewsAdditionalClasspath","[{'comment': '`@param` without description is invalid, please either add description or remove the tag.', 'commenter': 'adoroszlai'}, {'comment': 'added description', 'commenter': 'nitirajrathore'}]"
1657,ambari-server/src/main/java/org/apache/ambari/server/view/ViewExtractor.java,"@@ -201,9 +203,35 @@ private ClassLoader getArchiveClassLoader(ViewConfig viewConfig, File archiveDir
       urlList.add(classesDir.toURI().toURL());
     }
 
+        // include libs in additional classpath
+    for (File file : viewsAdditionalClasspath) {
+      if (file.isDirectory()) {
+        // add all files inside this dir.
+        addDirToClasspath(urlList, file);
+      } else if (file.isFile()) {
+        urlList.add(file.toURI().toURL());
+      }
+    }
+
     // include any libraries in the lib directory
     String libPath = archivePath + File.separator + ARCHIVE_LIB_DIR;
-    File   libDir  = archiveUtility.getFile(libPath);
+    File libDir = archiveUtility.getFile(libPath);
+    addDirToClasspath(urlList, libDir);
+
+    // include the archive directory
+    urlList.add(archiveDir.toURI().toURL());
+
+    LOG.trace(""classpath for view {} is : {}"", viewConfig.getName(), urlList);
+    return new ViewClassLoader(viewConfig, urlList.toArray(new URL[urlList.size()]));
+  }
+
+  /**
+   * Add all the files in libDir to urlList ignoring directories.
+   * @param urlList
+   * @param libDir
+   * @throws MalformedURLException","[{'comment': '`@param`/`@throws` without description is invalid, please either add descriptions or remove all 3 tags.', 'commenter': 'adoroszlai'}, {'comment': 'added description for @param and removed @throws', 'commenter': 'nitirajrathore'}]"
1706,ambari-server/src/main/java/org/apache/ambari/server/orm/dao/ClusterDAO.java,"@@ -294,7 +294,7 @@ public ClusterConfigEntity findEnabledConfigByType(long clusterId, String type)
     return daoUtils.selectOne(query);
   }
 
-  /**
+  /**8","[{'comment': '8?', 'commenter': 'jonathan-hurley'}, {'comment': 'Typo, sorry, I will fix it.', 'commenter': 'scottduan'}]"
1706,ambari-server/src/test/java/org/apache/ambari/server/agent/TestHeartbeatHandler.java,"@@ -1317,25 +1317,14 @@ public void testComponents() throws Exception {
     expected.setComponents(dummyComponents);
 
     Cluster cluster = heartbeatTestHelper.getDummyCluster();
-    Service service = EasyMock.createNiceMock(Service.class);
-    expect(service.getName()).andReturn(""HDFS"").atLeastOnce();
-
+    Service service = cluster.getService(""HDFS"");
     Map<String, ServiceComponent> componentMap = new HashMap<>();
     ServiceComponent nnComponent = EasyMock.createNiceMock(ServiceComponent.class);
-    expect(nnComponent.getName()).andReturn(""NAMENODE"").atLeastOnce();
-    expect(nnComponent.getStackId()).andReturn(dummyStackId).atLeastOnce();
+    expect(nnComponent.getName()).andReturn(""NAMENODE"").anyTimes();
+    expect(nnComponent.getStackId()).andReturn(service.getStackId()).anyTimes();
+    replay(nnComponent);
     componentMap.put(""NAMENODE"", nnComponent);
-
-    expect(service.getServiceComponents()).andReturn(componentMap).atLeastOnce();
-    expect(service.getServiceId()).andReturn(1L).atLeastOnce();
-    expect(service.getServiceType()).andReturn(""HDFS"").atLeastOnce();
-    expect(service.getStackId()).andReturn(dummyStackId).atLeastOnce();
-
-    ActionManager am = actionManagerTestHelper.getMockActionManager();
-
-    replay(service, nnComponent, am);
-
-    cluster.addService(service);
+    service.addServiceComponent(nnComponent);
 
     HeartBeatHandler handler = heartbeatTestHelper.getHeartBeatHandler(am);","[{'comment': '```\r\n[ERROR] COMPILATION ERROR : \r\n[INFO] -------------------------------------------------------------\r\n[ERROR] /home/jenkins/jenkins-slave/workspace/Ambari-Github-PullRequest-Builder/ambari-server/src/test/java/org/apache/ambari/server/agent/TestHeartbeatHandler.java:[1329,72] cannot find symbol\r\n  symbol:   variable am\r\n  location: class org.apache.ambari.server.agent.TestHeartbeatHandler\r\n```', 'commenter': 'adoroszlai'}, {'comment': 'ok, I knew it. When I resolved conflicts online, I accidentally removed \r\nActionManager am = actionManagerTestHelper.getMockActionManager();\r\nI will resubmit it.', 'commenter': 'scottduan'}]"
1769,ambari-logsearch/ambari-logsearch-web/src/app/components/service-logs-table/service-logs-table.component.less,"@@ -110,6 +110,10 @@
     background: none transparent;
   }
 
+  table thead th {","[{'comment': 'better to be merged into the upper `table` selector block (the same for `table td` below as well)', 'commenter': 'aBabiichuk'}]"
1876,ambari-server/src/main/java/org/apache/ambari/server/checks/DatabaseConsistencyCheckHelper.java,"@@ -1190,6 +1189,64 @@ static void checkServiceConfigs()  {
 
   }
 
+  static Map<Long, ConfigGroup> collectConfigGroupsWithoutServiceName() {
+    Map<Long, ConfigGroup> configGroupMap = new HashMap<>();
+    Clusters clusters = injector.getInstance(Clusters.class);
+    Map<String, Cluster> clusterMap = clusters.getClusters();
+
+    if (MapUtils.isEmpty(clusterMap))
+      return configGroupMap;
+
+    for (Cluster cluster : clusterMap.values()) {
+      Map<Long, ConfigGroup> configGroups = cluster.getConfigGroups();
+
+      if (MapUtils.isEmpty(configGroups))
+        continue;
+
+      for (ConfigGroup configGroup : configGroups.values()) {
+        if (StringUtils.isEmpty(configGroup.getServiceName())) {
+          configGroupMap.put(configGroup.getId(), configGroup);
+        }
+      }
+    }
+
+    return configGroupMap;
+  }
+
+  static void checkConfigGroupsHasServiceName() {
+    Map<Long, ConfigGroup> configGroupMap = collectConfigGroupsWithoutServiceName();
+    if (MapUtils.isEmpty(configGroupMap))
+      return;
+    StringBuilder output = new StringBuilder(""[(ConfigGroup) => "");
+
+    for (ConfigGroup configGroup : configGroupMap.values()) {
+      configGroupMap.put(configGroup.getId(), configGroup);
+      output.append(""( "");
+      output.append(configGroup.getName());
+      output.append("" ), "");
+    }
+
+    output.replace(output.lastIndexOf("",""), output.length(), ""]"");
+    warning(""You have config groups present in the database with no "" +","[{'comment': 'Should add message to backup the database before running autofix (generally useful comment)', 'commenter': 'swagle'}]"
1876,ambari-server/src/main/java/org/apache/ambari/server/checks/DatabaseConsistencyCheckHelper.java,"@@ -1190,6 +1189,64 @@ static void checkServiceConfigs()  {
 
   }
 
+  static Map<Long, ConfigGroup> collectConfigGroupsWithoutServiceName() {
+    Map<Long, ConfigGroup> configGroupMap = new HashMap<>();
+    Clusters clusters = injector.getInstance(Clusters.class);
+    Map<String, Cluster> clusterMap = clusters.getClusters();
+
+    if (MapUtils.isEmpty(clusterMap))
+      return configGroupMap;
+
+    for (Cluster cluster : clusterMap.values()) {
+      Map<Long, ConfigGroup> configGroups = cluster.getConfigGroups();
+
+      if (MapUtils.isEmpty(configGroups))
+        continue;
+
+      for (ConfigGroup configGroup : configGroups.values()) {
+        if (StringUtils.isEmpty(configGroup.getServiceName())) {
+          configGroupMap.put(configGroup.getId(), configGroup);
+        }
+      }
+    }
+
+    return configGroupMap;
+  }
+
+  static void checkConfigGroupsHasServiceName() {
+    Map<Long, ConfigGroup> configGroupMap = collectConfigGroupsWithoutServiceName();
+    if (MapUtils.isEmpty(configGroupMap))
+      return;
+    StringBuilder output = new StringBuilder(""[(ConfigGroup) => "");
+
+    for (ConfigGroup configGroup : configGroupMap.values()) {
+      configGroupMap.put(configGroup.getId(), configGroup);
+      output.append(""( "");
+      output.append(configGroup.getName());
+      output.append("" ), "");
+    }
+
+    output.replace(output.lastIndexOf("",""), output.length(), ""]"");
+    warning(""You have config groups present in the database with no "" +
+            "" service name, {}. Run --auto-fix-database to fix "" +
+            ""this automatically."", output.toString());
+  }
+
+  @Transactional
+  static void fixConfigGroupServiceNames() {
+    Map<Long, ConfigGroup> configGroupMap = collectConfigGroupsWithoutServiceName();
+
+    if (!MapUtils.isEmpty(configGroupMap)) {
+      for (Map.Entry<Long, ConfigGroup> configGroupEntry : configGroupMap.entrySet()) {
+        Long id = configGroupEntry.getKey();
+        ConfigGroup configGroup = configGroupEntry.getValue();
+        LOG.info(""Setting service name of config group {} with id {} to {}"",
+                configGroup.getName(), id, configGroup.getTag());
+        configGroup.setServiceName(configGroup.getTag());","[{'comment': 'Seems like this is only set on the business object does underlying method set it on the entity and persist the change?', 'commenter': 'swagle'}, {'comment': '@swagle \r\nConfigGroupImpl.setServiceName() \r\ncalls\r\nconfigGroupDAO.merge(configGroupEntity);\r\nafter setting the service name.\r\nI also checked the database in my vagrant env after upgrading to 2.6 and run the auto-fix. The service_name was populated.', 'commenter': 'kasakrisz'}, {'comment': ""Are you sure that `configGroup.getTag()` is always the service name? That's not enforced anywhere, so I'm not so sure we can make this assumption."", 'commenter': 'jonathan-hurley'}, {'comment': 'Yes we only had tag as a field in configGroup schema, UI always saved tag as serviceName because of the way configs are layed out (always tied to a service). Some time back we added this field, I believe when service config version table was added for consistency. It is always the same if config group is created using the UI! Using the API can have different consequences so as additional check can be done to make sure tag = serviceName before setting it.', 'commenter': 'swagle'}]"
1876,ambari-server/src/main/java/org/apache/ambari/server/checks/DatabaseConsistencyCheckHelper.java,"@@ -1190,6 +1189,64 @@ static void checkServiceConfigs()  {
 
   }
 
+  static Map<Long, ConfigGroup> collectConfigGroupsWithoutServiceName() {
+    Map<Long, ConfigGroup> configGroupMap = new HashMap<>();
+    Clusters clusters = injector.getInstance(Clusters.class);
+    Map<String, Cluster> clusterMap = clusters.getClusters();
+
+    if (MapUtils.isEmpty(clusterMap))
+      return configGroupMap;
+
+    for (Cluster cluster : clusterMap.values()) {
+      Map<Long, ConfigGroup> configGroups = cluster.getConfigGroups();
+
+      if (MapUtils.isEmpty(configGroups))
+        continue;
+
+      for (ConfigGroup configGroup : configGroups.values()) {
+        if (StringUtils.isEmpty(configGroup.getServiceName())) {
+          configGroupMap.put(configGroup.getId(), configGroup);
+        }
+      }
+    }
+
+    return configGroupMap;
+  }
+
+  static void checkConfigGroupsHasServiceName() {
+    Map<Long, ConfigGroup> configGroupMap = collectConfigGroupsWithoutServiceName();
+    if (MapUtils.isEmpty(configGroupMap))
+      return;
+    StringBuilder output = new StringBuilder(""[(ConfigGroup) => "");
+
+    for (ConfigGroup configGroup : configGroupMap.values()) {
+      configGroupMap.put(configGroup.getId(), configGroup);","[{'comment': ""Aren't these values already in the map?"", 'commenter': 'jonathan-hurley'}]"
1876,ambari-server/src/main/java/org/apache/ambari/server/checks/DatabaseConsistencyCheckHelper.java,"@@ -1190,6 +1189,64 @@ static void checkServiceConfigs()  {
 
   }
 
+  static Map<Long, ConfigGroup> collectConfigGroupsWithoutServiceName() {
+    Map<Long, ConfigGroup> configGroupMap = new HashMap<>();
+    Clusters clusters = injector.getInstance(Clusters.class);
+    Map<String, Cluster> clusterMap = clusters.getClusters();
+
+    if (MapUtils.isEmpty(clusterMap))
+      return configGroupMap;
+
+    for (Cluster cluster : clusterMap.values()) {
+      Map<Long, ConfigGroup> configGroups = cluster.getConfigGroups();
+
+      if (MapUtils.isEmpty(configGroups))
+        continue;
+
+      for (ConfigGroup configGroup : configGroups.values()) {
+        if (StringUtils.isEmpty(configGroup.getServiceName())) {
+          configGroupMap.put(configGroup.getId(), configGroup);
+        }
+      }
+    }
+
+    return configGroupMap;
+  }
+
+  static void checkConfigGroupsHasServiceName() {
+    Map<Long, ConfigGroup> configGroupMap = collectConfigGroupsWithoutServiceName();
+    if (MapUtils.isEmpty(configGroupMap))
+      return;
+    StringBuilder output = new StringBuilder(""[(ConfigGroup) => "");
+
+    for (ConfigGroup configGroup : configGroupMap.values()) {
+      configGroupMap.put(configGroup.getId(), configGroup);
+      output.append(""( "");
+      output.append(configGroup.getName());
+      output.append("" ), "");
+    }
+
+    output.replace(output.lastIndexOf("",""), output.length(), ""]"");
+    warning(""You have config groups present in the database with no "" +
+            "" service name, {}. Run --auto-fix-database to fix "" +
+            ""this automatically."", output.toString());
+  }
+
+  @Transactional
+  static void fixConfigGroupServiceNames() {","[{'comment': 'Documentation.', 'commenter': 'jonathan-hurley'}]"
1876,ambari-server/src/main/java/org/apache/ambari/server/checks/DatabaseConsistencyCheckHelper.java,"@@ -1190,6 +1189,64 @@ static void checkServiceConfigs()  {
 
   }
 
+  static Map<Long, ConfigGroup> collectConfigGroupsWithoutServiceName() {","[{'comment': 'Documentation.', 'commenter': 'jonathan-hurley'}]"
1876,ambari-server/src/main/java/org/apache/ambari/server/checks/DatabaseConsistencyCheckHelper.java,"@@ -1190,6 +1189,82 @@ static void checkServiceConfigs()  {
 
   }
 
+  /**
+   * This method collects the ConfigGroups with empty or null service name field
+   */
+  static Map<Long, ConfigGroup> collectConfigGroupsWithoutServiceName() {
+    Map<Long, ConfigGroup> configGroupMap = new HashMap<>();
+    Clusters clusters = injector.getInstance(Clusters.class);
+    Map<String, Cluster> clusterMap = clusters.getClusters();
+
+    if (MapUtils.isEmpty(clusterMap))
+      return configGroupMap;
+
+    for (Cluster cluster : clusterMap.values()) {
+      Map<Long, ConfigGroup> configGroups = cluster.getConfigGroups();
+
+      if (MapUtils.isEmpty(configGroups))
+        continue;
+
+      for (ConfigGroup configGroup : configGroups.values()) {
+        if (StringUtils.isEmpty(configGroup.getServiceName())) {
+          configGroupMap.put(configGroup.getId(), configGroup);
+        }
+      }
+    }
+
+    return configGroupMap;
+  }
+
+  /**
+   * This method checks if there are any ConfigGroup with empty or null service name field
+   */
+  static void checkConfigGroupsHasServiceName() {
+    Map<Long, ConfigGroup> configGroupMap = collectConfigGroupsWithoutServiceName();
+    if (MapUtils.isEmpty(configGroupMap))","[{'comment': 'Please add curly braces', 'commenter': 'benyoka'}]"
1876,ambari-server/src/main/java/org/apache/ambari/server/checks/DatabaseConsistencyCheckHelper.java,"@@ -1190,6 +1189,82 @@ static void checkServiceConfigs()  {
 
   }
 
+  /**
+   * This method collects the ConfigGroups with empty or null service name field
+   */
+  static Map<Long, ConfigGroup> collectConfigGroupsWithoutServiceName() {
+    Map<Long, ConfigGroup> configGroupMap = new HashMap<>();
+    Clusters clusters = injector.getInstance(Clusters.class);
+    Map<String, Cluster> clusterMap = clusters.getClusters();
+
+    if (MapUtils.isEmpty(clusterMap))
+      return configGroupMap;
+
+    for (Cluster cluster : clusterMap.values()) {
+      Map<Long, ConfigGroup> configGroups = cluster.getConfigGroups();
+
+      if (MapUtils.isEmpty(configGroups))
+        continue;
+
+      for (ConfigGroup configGroup : configGroups.values()) {
+        if (StringUtils.isEmpty(configGroup.getServiceName())) {
+          configGroupMap.put(configGroup.getId(), configGroup);
+        }
+      }
+    }
+
+    return configGroupMap;
+  }
+
+  /**
+   * This method checks if there are any ConfigGroup with empty or null service name field
+   */
+  static void checkConfigGroupsHasServiceName() {
+    Map<Long, ConfigGroup> configGroupMap = collectConfigGroupsWithoutServiceName();
+    if (MapUtils.isEmpty(configGroupMap))
+      return;
+
+    StringBuilder output = new StringBuilder(""[(ConfigGroup) => "");
+
+    for (ConfigGroup configGroup : configGroupMap.values()) {
+      output.append(""( "");
+      output.append(configGroup.getName());
+      output.append("" ), "");
+    }
+","[{'comment': 'You could use a Guava Joiner (pre Java 8) or String.join (java 8+) here.', 'commenter': 'benyoka'}]"
1890,ambari-server/src/main/java/org/apache/ambari/server/state/ServiceImpl.java,"@@ -713,20 +720,16 @@ public boolean isSsoIntegrationDesired() {
   }
 
   public boolean isSsoIntegrationEnabled() {
-    return ssoIntegrationSupported && ssoEnabledConfigValid() && ""true"".equalsIgnoreCase(ssoEnabledConfigValue());
-  }
-
-  private boolean ssoEnabledConfigValid() {
-    return ssoEnabledConfiguration != null && ssoEnabledConfiguration.split(""/"").length == 2;
+    try {
+      final Predicate ssoEnabledPredicate = ssoEnabledConfiguration == null ? null : PredicateUtils.fromJSON(ssoEnabledConfiguration);","[{'comment': 'This does not seem to be backwards compatible with the existing implementation.  If it is not, there may be issues with existing mpacks that support this feature. \r\n\r\nI suggest using a different property for the predicate declaration so that old way can be deprecated and write a message to the log if encountered. ', 'commenter': 'rlevas'}, {'comment': 'ok; will submit the change soon', 'commenter': 'smolnar82'}]"
1890,ambari-server/src/main/java/org/apache/ambari/server/state/SingleSignOnInfo.java,"@@ -48,7 +56,7 @@
   /**
    * The configuration that can be used to determine if SSO integration has been enabled.
    * <p>
-   * It is expected that this value is in the form of <code>configuration-type/property_name</code>
+   * It is expected that this value is in the form of a valid JSON predicate ({@link PredicateUtils#fromJSON(String)}","[{'comment': 'For backward compatibility, we cannot expect this. \r\n', 'commenter': 'rlevas'}]"
1890,ambari-server/src/main/java/org/apache/ambari/server/state/SingleSignOnInfo.java,"@@ -111,7 +119,7 @@ public void setSupported(Boolean supported) {
   /**
    * Gets the configuration specification that can be used to determine if SSO has been enabled or not.
    *
-   * @return a configuration specification (config-type/property_name)
+   * @return a configuration specification (JSON predicate)
    */
   public String getEnabledConfiguration() {","[{'comment': 'A separate property can return the Predicate, rather than _compiling_ it over and over again. ', 'commenter': 'rlevas'}]"
1890,ambari-server/src/main/java/org/apache/ambari/server/state/SingleSignOnInfo.java,"@@ -32,7 +34,14 @@
  * <pre>
  *   &lt;sso&gt;
  *     &lt;supported&gt;true&lt;/supported&gt;
- *     &lt;enabledConfiguration&gt;config-type/sso.enabled.property_name&lt;/enabledConfiguration&gt;
+ *     &lt;ssoEnabledTest&gt;
+ *         {
+ *           ""equals"": [
+ *             ""config-type/sso.enabled.property_name"",
+ *             ""true""
+ *           ]
+ *         }
+ *     &lt;/enabledConfiguration&gt;","[{'comment': '""enabledConfiguration"" -> ""ssoEnabledTest""', 'commenter': 'rlevas'}]"
1915,ambari-server/src/main/java/org/apache/ambari/server/checks/DatabaseConsistencyCheckHelper.java,"@@ -1147,6 +1159,248 @@ static void checkServiceConfigs()  {
 
   }
 
+  /**
+   * This method collects the ConfigGroups with empty or null service name field
+   */
+  static Map<Long, ConfigGroup> collectConfigGroupsWithoutServiceName() {
+    Map<Long, ConfigGroup> configGroupMap = new HashMap<>();
+    Clusters clusters = injector.getInstance(Clusters.class);
+    Map<String, Cluster> clusterMap = clusters.getClusters();
+
+    if (MapUtils.isEmpty(clusterMap))
+      return configGroupMap;
+
+    for (Cluster cluster : clusterMap.values()) {
+      Map<Long, ConfigGroup> configGroups = cluster.getConfigGroups();
+
+      if (MapUtils.isEmpty(configGroups))
+        continue;
+","[{'comment': 'Could you please add curly braces?', 'commenter': 'benyoka'}]"
1915,ambari-server/src/main/java/org/apache/ambari/server/checks/DatabaseConsistencyCheckHelper.java,"@@ -1147,6 +1159,248 @@ static void checkServiceConfigs()  {
 
   }
 
+  /**
+   * This method collects the ConfigGroups with empty or null service name field
+   */
+  static Map<Long, ConfigGroup> collectConfigGroupsWithoutServiceName() {
+    Map<Long, ConfigGroup> configGroupMap = new HashMap<>();
+    Clusters clusters = injector.getInstance(Clusters.class);
+    Map<String, Cluster> clusterMap = clusters.getClusters();
+
+    if (MapUtils.isEmpty(clusterMap))
+      return configGroupMap;
+
+    for (Cluster cluster : clusterMap.values()) {
+      Map<Long, ConfigGroup> configGroups = cluster.getConfigGroups();
+
+      if (MapUtils.isEmpty(configGroups))
+        continue;
+
+      for (ConfigGroup configGroup : configGroups.values()) {
+        if (StringUtils.isEmpty(configGroup.getServiceName())) {
+          configGroupMap.put(configGroup.getId(), configGroup);
+        }
+      }
+    }
+
+    return configGroupMap;
+  }
+
+  /**
+   * This method checks if there are any ConfigGroup with empty or null service name field
+   */
+  static void checkConfigGroupsHasServiceName() {
+    Map<Long, ConfigGroup> configGroupMap = collectConfigGroupsWithoutServiceName();
+    if (MapUtils.isEmpty(configGroupMap))
+      return;
+
+    StringBuilder output = new StringBuilder(""[(ConfigGroup) => "");
+
+    for (ConfigGroup configGroup : configGroupMap.values()) {
+      output.append(""( "");
+      output.append(configGroup.getName());
+      output.append("" ), "");
+    }
+
+    output.replace(output.lastIndexOf("",""), output.length(), ""]"");","[{'comment': 'You could use a Guava Joiner (works with Java 7 too) or String.join (Java 8+) here.', 'commenter': 'benyoka'}]"
1915,ambari-server/src/main/java/org/apache/ambari/server/checks/DatabaseConsistencyCheckHelper.java,"@@ -1147,6 +1159,248 @@ static void checkServiceConfigs()  {
 
   }
 
+  /**
+   * This method collects the ConfigGroups with empty or null service name field
+   */
+  static Map<Long, ConfigGroup> collectConfigGroupsWithoutServiceName() {
+    Map<Long, ConfigGroup> configGroupMap = new HashMap<>();
+    Clusters clusters = injector.getInstance(Clusters.class);
+    Map<String, Cluster> clusterMap = clusters.getClusters();
+
+    if (MapUtils.isEmpty(clusterMap))
+      return configGroupMap;
+
+    for (Cluster cluster : clusterMap.values()) {
+      Map<Long, ConfigGroup> configGroups = cluster.getConfigGroups();
+
+      if (MapUtils.isEmpty(configGroups))
+        continue;
+
+      for (ConfigGroup configGroup : configGroups.values()) {
+        if (StringUtils.isEmpty(configGroup.getServiceName())) {
+          configGroupMap.put(configGroup.getId(), configGroup);
+        }
+      }
+    }
+
+    return configGroupMap;
+  }
+
+  /**
+   * This method checks if there are any ConfigGroup with empty or null service name field
+   */
+  static void checkConfigGroupsHasServiceName() {
+    Map<Long, ConfigGroup> configGroupMap = collectConfigGroupsWithoutServiceName();
+    if (MapUtils.isEmpty(configGroupMap))
+      return;
+
+    StringBuilder output = new StringBuilder(""[(ConfigGroup) => "");
+
+    for (ConfigGroup configGroup : configGroupMap.values()) {
+      output.append(""( "");
+      output.append(configGroup.getName());
+      output.append("" ), "");
+    }
+
+    output.replace(output.lastIndexOf("",""), output.length(), ""]"");
+    warning(""You have config groups present in the database with no "" +
+            ""service name, {}. Run --auto-fix-database to fix "" +
+            ""this automatically. Please backup Ambari Server database before running --auto-fix-database."", output.toString());
+  }
+
+  /**
+   * Fix inconsistencies found by @collectConfigGroupsWithoutServiceName
+   */
+  @Transactional
+  static void fixConfigGroupServiceNames() {
+    Map<Long, ConfigGroup> configGroupMap = collectConfigGroupsWithoutServiceName();
+    if (MapUtils.isEmpty(configGroupMap))
+      return;
+
+    Clusters clusters = injector.getInstance(Clusters.class);
+
+    for (Map.Entry<Long, ConfigGroup> configGroupEntry : configGroupMap.entrySet()) {
+      ConfigGroup configGroup = configGroupEntry.getValue();
+      try {
+        Cluster cluster = clusters.getCluster(configGroup.getClusterName());
+        Map<String, Service> serviceMap = cluster.getServices();
+        if (serviceMap.containsKey(configGroup.getTag())) {
+          LOG.info(""Setting service name of config group {} with id {} to {}"",
+                  configGroup.getName(), configGroupEntry.getKey(), configGroup.getTag());
+          configGroup.setServiceName(configGroup.getTag());
+        }","[{'comment': 'Please consider creating an else case just for logging.', 'commenter': 'benyoka'}]"
1915,ambari-server/src/main/java/org/apache/ambari/server/checks/DatabaseConsistencyCheckHelper.java,"@@ -1147,6 +1159,248 @@ static void checkServiceConfigs()  {
 
   }
 
+  /**
+   * This method collects the ConfigGroups with empty or null service name field
+   */
+  static Map<Long, ConfigGroup> collectConfigGroupsWithoutServiceName() {
+    Map<Long, ConfigGroup> configGroupMap = new HashMap<>();
+    Clusters clusters = injector.getInstance(Clusters.class);
+    Map<String, Cluster> clusterMap = clusters.getClusters();
+
+    if (MapUtils.isEmpty(clusterMap))
+      return configGroupMap;
+
+    for (Cluster cluster : clusterMap.values()) {
+      Map<Long, ConfigGroup> configGroups = cluster.getConfigGroups();
+
+      if (MapUtils.isEmpty(configGroups))
+        continue;
+
+      for (ConfigGroup configGroup : configGroups.values()) {
+        if (StringUtils.isEmpty(configGroup.getServiceName())) {
+          configGroupMap.put(configGroup.getId(), configGroup);
+        }
+      }
+    }
+
+    return configGroupMap;
+  }
+
+  /**
+   * This method checks if there are any ConfigGroup with empty or null service name field
+   */
+  static void checkConfigGroupsHasServiceName() {
+    Map<Long, ConfigGroup> configGroupMap = collectConfigGroupsWithoutServiceName();
+    if (MapUtils.isEmpty(configGroupMap))
+      return;
+
+    StringBuilder output = new StringBuilder(""[(ConfigGroup) => "");
+
+    for (ConfigGroup configGroup : configGroupMap.values()) {
+      output.append(""( "");
+      output.append(configGroup.getName());
+      output.append("" ), "");
+    }
+
+    output.replace(output.lastIndexOf("",""), output.length(), ""]"");
+    warning(""You have config groups present in the database with no "" +
+            ""service name, {}. Run --auto-fix-database to fix "" +
+            ""this automatically. Please backup Ambari Server database before running --auto-fix-database."", output.toString());
+  }
+
+  /**
+   * Fix inconsistencies found by @collectConfigGroupsWithoutServiceName
+   */
+  @Transactional
+  static void fixConfigGroupServiceNames() {
+    Map<Long, ConfigGroup> configGroupMap = collectConfigGroupsWithoutServiceName();
+    if (MapUtils.isEmpty(configGroupMap))
+      return;
+
+    Clusters clusters = injector.getInstance(Clusters.class);
+
+    for (Map.Entry<Long, ConfigGroup> configGroupEntry : configGroupMap.entrySet()) {
+      ConfigGroup configGroup = configGroupEntry.getValue();
+      try {
+        Cluster cluster = clusters.getCluster(configGroup.getClusterName());
+        Map<String, Service> serviceMap = cluster.getServices();
+        if (serviceMap.containsKey(configGroup.getTag())) {
+          LOG.info(""Setting service name of config group {} with id {} to {}"",
+                  configGroup.getName(), configGroupEntry.getKey(), configGroup.getTag());
+          configGroup.setServiceName(configGroup.getTag());
+        }
+      } catch (AmbariException e) {
+        // Ignore if cluster not found
+      }
+    }
+  }
+
+  /**
+   * This method checks if there are any ConfigGroup host mappings with hosts
+   * that are not longer a part of the cluster.
+   */
+  static Map<Long, Set<Long>> checkConfigGroupHostMapping(boolean warnIfFound) {
+    LOG.info(""Checking config group host mappings"");
+    Map<Long, Set<Long>> nonMappedHostIds = new HashMap<>();
+    Clusters clusters = injector.getInstance(Clusters.class);
+    Map<String, Cluster> clusterMap = clusters.getClusters();
+    StringBuilder output = new StringBuilder(""[(ConfigGroup, Service, HostCount) => "");
+
+    if (!MapUtils.isEmpty(clusterMap)) {
+      for (Cluster cluster : clusterMap.values()) {
+        Map<Long, ConfigGroup> configGroups = cluster.getConfigGroups();
+        Map<String, Host> clusterHosts;
+        clusterHosts = clusters.getHostsForCluster(cluster.getClusterName());","[{'comment': 'These two lines could be made one.', 'commenter': 'benyoka'}]"
1915,ambari-server/src/main/java/org/apache/ambari/server/checks/DatabaseConsistencyCheckHelper.java,"@@ -1147,6 +1159,248 @@ static void checkServiceConfigs()  {
 
   }
 
+  /**
+   * This method collects the ConfigGroups with empty or null service name field
+   */
+  static Map<Long, ConfigGroup> collectConfigGroupsWithoutServiceName() {
+    Map<Long, ConfigGroup> configGroupMap = new HashMap<>();
+    Clusters clusters = injector.getInstance(Clusters.class);
+    Map<String, Cluster> clusterMap = clusters.getClusters();
+
+    if (MapUtils.isEmpty(clusterMap))
+      return configGroupMap;
+
+    for (Cluster cluster : clusterMap.values()) {
+      Map<Long, ConfigGroup> configGroups = cluster.getConfigGroups();
+
+      if (MapUtils.isEmpty(configGroups))
+        continue;
+
+      for (ConfigGroup configGroup : configGroups.values()) {
+        if (StringUtils.isEmpty(configGroup.getServiceName())) {
+          configGroupMap.put(configGroup.getId(), configGroup);
+        }
+      }
+    }
+
+    return configGroupMap;
+  }
+
+  /**
+   * This method checks if there are any ConfigGroup with empty or null service name field
+   */
+  static void checkConfigGroupsHasServiceName() {
+    Map<Long, ConfigGroup> configGroupMap = collectConfigGroupsWithoutServiceName();
+    if (MapUtils.isEmpty(configGroupMap))
+      return;
+
+    StringBuilder output = new StringBuilder(""[(ConfigGroup) => "");
+
+    for (ConfigGroup configGroup : configGroupMap.values()) {
+      output.append(""( "");
+      output.append(configGroup.getName());
+      output.append("" ), "");
+    }
+
+    output.replace(output.lastIndexOf("",""), output.length(), ""]"");
+    warning(""You have config groups present in the database with no "" +
+            ""service name, {}. Run --auto-fix-database to fix "" +
+            ""this automatically. Please backup Ambari Server database before running --auto-fix-database."", output.toString());
+  }
+
+  /**
+   * Fix inconsistencies found by @collectConfigGroupsWithoutServiceName
+   */
+  @Transactional
+  static void fixConfigGroupServiceNames() {
+    Map<Long, ConfigGroup> configGroupMap = collectConfigGroupsWithoutServiceName();
+    if (MapUtils.isEmpty(configGroupMap))
+      return;
+
+    Clusters clusters = injector.getInstance(Clusters.class);
+
+    for (Map.Entry<Long, ConfigGroup> configGroupEntry : configGroupMap.entrySet()) {
+      ConfigGroup configGroup = configGroupEntry.getValue();
+      try {
+        Cluster cluster = clusters.getCluster(configGroup.getClusterName());
+        Map<String, Service> serviceMap = cluster.getServices();
+        if (serviceMap.containsKey(configGroup.getTag())) {
+          LOG.info(""Setting service name of config group {} with id {} to {}"",
+                  configGroup.getName(), configGroupEntry.getKey(), configGroup.getTag());
+          configGroup.setServiceName(configGroup.getTag());
+        }
+      } catch (AmbariException e) {
+        // Ignore if cluster not found
+      }
+    }
+  }
+
+  /**
+   * This method checks if there are any ConfigGroup host mappings with hosts
+   * that are not longer a part of the cluster.
+   */
+  static Map<Long, Set<Long>> checkConfigGroupHostMapping(boolean warnIfFound) {
+    LOG.info(""Checking config group host mappings"");
+    Map<Long, Set<Long>> nonMappedHostIds = new HashMap<>();
+    Clusters clusters = injector.getInstance(Clusters.class);
+    Map<String, Cluster> clusterMap = clusters.getClusters();
+    StringBuilder output = new StringBuilder(""[(ConfigGroup, Service, HostCount) => "");
+
+    if (!MapUtils.isEmpty(clusterMap)) {
+      for (Cluster cluster : clusterMap.values()) {
+        Map<Long, ConfigGroup> configGroups = cluster.getConfigGroups();
+        Map<String, Host> clusterHosts;
+        clusterHosts = clusters.getHostsForCluster(cluster.getClusterName());
+
+        if (!MapUtils.isEmpty(configGroups) && !MapUtils.isEmpty(clusterHosts)) {
+          for (ConfigGroup configGroup : configGroups.values()) {
+            // Based on current implementation of ConfigGroupImpl the
+            // host mapping would be loaded only if the host actually exists
+            // in the host table
+            Map<Long, Host> hosts = configGroup.getHosts();
+            boolean addToOutput = false;
+            Set<String> hostnames = new HashSet<>();
+            if (!MapUtils.isEmpty(hosts)) {
+              for (Host host : hosts.values()) {
+                // Lookup by hostname - It does have a unique constraint
+                if (!clusterHosts.containsKey(host.getHostName())) {
+                  Set<Long> hostIds = nonMappedHostIds.get(configGroup.getId());
+                  if (CollectionUtils.isEmpty(hostIds)) {
+                    hostIds = new HashSet<>();","[{'comment': 'You could use Map.computeIfAbsent (Java 8+) to create the collection in the map in case it does not yet exist.', 'commenter': 'benyoka'}]"
1915,ambari-server/src/main/java/org/apache/ambari/server/checks/DatabaseConsistencyCheckHelper.java,"@@ -1147,6 +1159,248 @@ static void checkServiceConfigs()  {
 
   }
 
+  /**
+   * This method collects the ConfigGroups with empty or null service name field
+   */
+  static Map<Long, ConfigGroup> collectConfigGroupsWithoutServiceName() {
+    Map<Long, ConfigGroup> configGroupMap = new HashMap<>();
+    Clusters clusters = injector.getInstance(Clusters.class);
+    Map<String, Cluster> clusterMap = clusters.getClusters();
+
+    if (MapUtils.isEmpty(clusterMap))
+      return configGroupMap;
+
+    for (Cluster cluster : clusterMap.values()) {
+      Map<Long, ConfigGroup> configGroups = cluster.getConfigGroups();
+
+      if (MapUtils.isEmpty(configGroups))
+        continue;
+
+      for (ConfigGroup configGroup : configGroups.values()) {
+        if (StringUtils.isEmpty(configGroup.getServiceName())) {
+          configGroupMap.put(configGroup.getId(), configGroup);
+        }
+      }
+    }
+
+    return configGroupMap;
+  }
+
+  /**
+   * This method checks if there are any ConfigGroup with empty or null service name field
+   */
+  static void checkConfigGroupsHasServiceName() {
+    Map<Long, ConfigGroup> configGroupMap = collectConfigGroupsWithoutServiceName();
+    if (MapUtils.isEmpty(configGroupMap))
+      return;
+
+    StringBuilder output = new StringBuilder(""[(ConfigGroup) => "");
+
+    for (ConfigGroup configGroup : configGroupMap.values()) {
+      output.append(""( "");
+      output.append(configGroup.getName());
+      output.append("" ), "");
+    }
+
+    output.replace(output.lastIndexOf("",""), output.length(), ""]"");
+    warning(""You have config groups present in the database with no "" +
+            ""service name, {}. Run --auto-fix-database to fix "" +
+            ""this automatically. Please backup Ambari Server database before running --auto-fix-database."", output.toString());
+  }
+
+  /**
+   * Fix inconsistencies found by @collectConfigGroupsWithoutServiceName
+   */
+  @Transactional
+  static void fixConfigGroupServiceNames() {
+    Map<Long, ConfigGroup> configGroupMap = collectConfigGroupsWithoutServiceName();
+    if (MapUtils.isEmpty(configGroupMap))
+      return;
+
+    Clusters clusters = injector.getInstance(Clusters.class);
+
+    for (Map.Entry<Long, ConfigGroup> configGroupEntry : configGroupMap.entrySet()) {
+      ConfigGroup configGroup = configGroupEntry.getValue();
+      try {
+        Cluster cluster = clusters.getCluster(configGroup.getClusterName());
+        Map<String, Service> serviceMap = cluster.getServices();
+        if (serviceMap.containsKey(configGroup.getTag())) {
+          LOG.info(""Setting service name of config group {} with id {} to {}"",
+                  configGroup.getName(), configGroupEntry.getKey(), configGroup.getTag());
+          configGroup.setServiceName(configGroup.getTag());
+        }
+      } catch (AmbariException e) {
+        // Ignore if cluster not found
+      }
+    }
+  }
+
+  /**
+   * This method checks if there are any ConfigGroup host mappings with hosts
+   * that are not longer a part of the cluster.
+   */
+  static Map<Long, Set<Long>> checkConfigGroupHostMapping(boolean warnIfFound) {
+    LOG.info(""Checking config group host mappings"");
+    Map<Long, Set<Long>> nonMappedHostIds = new HashMap<>();
+    Clusters clusters = injector.getInstance(Clusters.class);
+    Map<String, Cluster> clusterMap = clusters.getClusters();
+    StringBuilder output = new StringBuilder(""[(ConfigGroup, Service, HostCount) => "");
+
+    if (!MapUtils.isEmpty(clusterMap)) {
+      for (Cluster cluster : clusterMap.values()) {
+        Map<Long, ConfigGroup> configGroups = cluster.getConfigGroups();
+        Map<String, Host> clusterHosts;
+        clusterHosts = clusters.getHostsForCluster(cluster.getClusterName());
+
+        if (!MapUtils.isEmpty(configGroups) && !MapUtils.isEmpty(clusterHosts)) {
+          for (ConfigGroup configGroup : configGroups.values()) {
+            // Based on current implementation of ConfigGroupImpl the
+            // host mapping would be loaded only if the host actually exists
+            // in the host table
+            Map<Long, Host> hosts = configGroup.getHosts();
+            boolean addToOutput = false;
+            Set<String> hostnames = new HashSet<>();
+            if (!MapUtils.isEmpty(hosts)) {
+              for (Host host : hosts.values()) {
+                // Lookup by hostname - It does have a unique constraint
+                if (!clusterHosts.containsKey(host.getHostName())) {
+                  Set<Long> hostIds = nonMappedHostIds.get(configGroup.getId());
+                  if (CollectionUtils.isEmpty(hostIds)) {
+                    hostIds = new HashSet<>();
+                    nonMappedHostIds.put(configGroup.getId(), hostIds);
+                  }
+                  hostIds.add(host.getHostId());
+                  hostnames.add(host.getHostName());
+                  addToOutput = true;
+                }
+              }
+            }
+            if (addToOutput) {
+              output.append(""( "");
+              output.append(configGroup.getName());
+              output.append("", "");
+              output.append(configGroup.getTag());
+              output.append("", "");
+              output.append(hostnames);
+              output.append("" ), "");
+            }
+          }
+        }
+      }
+    }
+    if (!MapUtils.isEmpty(nonMappedHostIds) && warnIfFound) {
+      output.replace(output.lastIndexOf("",""), output.length(), ""]"");
+      warning(""You have config group host mappings with hosts that are no "" +
+        ""longer associated with the cluster, {}. Run --auto-fix-database to "" +
+        ""fix this automatically. Alternatively, you can remove this mapping "" +
+        ""from the UI. Please backup Ambari Server database before running --auto-fix-database."", output.toString());
+    }
+
+    return nonMappedHostIds;
+  }
+
+  static Map<Long, ConfigGroup> checkConfigGroupsForDeletedServices(boolean warnIfFound) {
+    Map<Long, ConfigGroup> configGroupMap = new HashMap<>();
+    Clusters clusters = injector.getInstance(Clusters.class);
+    Map<String, Cluster> clusterMap = clusters.getClusters();
+    StringBuilder output = new StringBuilder(""[(ConfigGroup, Service) => "");
+
+    if (!MapUtils.isEmpty(clusterMap)) {
+      for (Cluster cluster : clusterMap.values()) {
+        Map<Long, ConfigGroup> configGroups = cluster.getConfigGroups();
+        Map<String, Service> services = cluster.getServices();
+
+        if (!MapUtils.isEmpty(configGroups)) {
+          for (ConfigGroup configGroup : configGroups.values()) {
+            if (!services.containsKey(configGroup.getServiceName())) {
+              configGroupMap.put(configGroup.getId(), configGroup);
+              output.append(""( "");
+              output.append(configGroup.getName());
+              output.append("", "");
+              output.append(configGroup.getServiceName());
+              output.append("" ), "");
+            }
+          }
+        }
+      }
+    }
+
+    if (warnIfFound && !configGroupMap.isEmpty()) {
+      output.replace(output.lastIndexOf("",""), output.length(), ""]"");
+      warning(""You have config groups present in the database with no "" +
+        ""corresponding service found, {}. Run --auto-fix-database to fix "" +
+          ""this automatically. Please backup Ambari Server database before running --auto-fix-database."", output.toString());
+    }
+
+    return configGroupMap;
+  }","[{'comment': 'Looks like this case is not handled:\r\n- A config group has its service name in the tag field and the service is deleted. Neither checkConfigGroupsHasServiceName() nor checkConfigGroupsForDeletedServices() detects it.', 'commenter': 'benyoka'}, {'comment': 'I had two config groups before upgrade from 2.5:\r\nI upgraded to 2.6 than 2.7 and deleted the service ambari_infra_solr. The configgroup table looks like:\r\n\r\ngroup_name|tag|service_name\r\n----------|---|--------------\r\ncfg1|ZOOKEEPER|(null)\t\r\ncfg_infra|AMBARI_INFRA_SOLR|(null)\t\r\n\r\nWhen stating ambari server both methods warns:\r\n\r\n    WARN - You have config groups present in the database with no service name, [(ConfigGroup) => ( cfg1 ), ( cfg_infra )]\r\n    WARN - You have config groups present in the database with no corresponding service found, [(ConfigGroup, Service) => ( cfg1, null ), ( cfg_infra, null )]\r\n\r\nAfter restarting ambari-server using `--auto-fix-database`\r\n\r\ngroup_name|tag|service_name\r\n----------|---|--------------\r\ncfg1|ZOOKEEPER|ZOOKEEPER\t\r\ncfg_infra|AMBARI_INFRA_SOLR|(null)\t\r\n\r\n    WARN - You have config groups present in the database with no service name, [(ConfigGroup) => ( cfg_infra )]\r\n    WARN - You have config groups present in the database with no corresponding service found, [(ConfigGroup, Service) => ( cfg_infra, null )]', 'commenter': 'kasakrisz'}]"
1962,ambari-logsearch/ambari-logsearch-server/src/main/java/org/apache/ambari/logsearch/rest/error/GeneralExceptionMapper.java,"@@ -0,0 +1,65 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.ambari.logsearch.rest.error;
+
+import java.util.Map;
+
+import javax.inject.Named;
+import javax.ws.rs.core.Response;
+import javax.ws.rs.ext.ExceptionMapper;
+import javax.ws.rs.ext.Provider;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import org.springframework.web.bind.MethodArgumentNotValidException;
+
+import com.fasterxml.jackson.core.JsonParseException;
+import com.fasterxml.jackson.databind.JsonMappingException;
+import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException;
+import com.google.common.collect.Maps;
+
+@Named
+@Provider
+public class GeneralExceptionMapper extends ExceptionMapperBase implements ExceptionMapper<Throwable> {
+  private static final Logger LOG = LoggerFactory.getLogger(GeneralExceptionMapper.class);
+
+  private static final Map<Class, Response.Status> exceptionStatusCodeMap = Maps.newHashMap();
+
+  static {
+    exceptionStatusCodeMap.put(MethodArgumentNotValidException.class, Response.Status.BAD_REQUEST);
+    exceptionStatusCodeMap.put(JsonMappingException.class, Response.Status.BAD_REQUEST);
+    exceptionStatusCodeMap.put(JsonParseException.class, Response.Status.BAD_REQUEST);
+    exceptionStatusCodeMap.put(UnrecognizedPropertyException.class, Response.Status.BAD_REQUEST);","[{'comment': 'probably you can include more stuff, like take a look at some web filters (like we have if config api is not supported, or collections are not ready yet, there is a VResponse object created ... so the goal would be to get rid of that.', 'commenter': 'oleewere'}]"
1962,ambari-logsearch/ambari-logsearch-server/src/main/java/org/apache/ambari/logsearch/rest/error/ExceptionMapperBase.java,"@@ -0,0 +1,55 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.ambari.logsearch.rest.error;
+
+import javax.ws.rs.core.MediaType;
+import javax.ws.rs.core.Response;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public abstract class ExceptionMapperBase {
+
+  private static final Logger LOG = LoggerFactory.getLogger(ExceptionMapperBase.class);
+
+  protected Response toErrorResponse(Response.Status status, Throwable throwable) {
+    LOG.error(""REST Exception occurred:"", throwable);
+
+    return Response.status(status).entity(new StatusMessage(throwable.getMessage(), status.getStatusCode()))
+            .type(MediaType.APPLICATION_JSON_TYPE).build();
+  }
+
+  public static class StatusMessage {
+    private String message;","[{'comment': 'we should include more fields here, at least a detailed message as well. (or a type for later usages)', 'commenter': 'oleewere'}]"
1962,ambari-logsearch/ambari-logsearch-server/src/main/java/org/apache/ambari/logsearch/conf/LogSearchJerseyResourceConfig.java,"@@ -18,19 +18,18 @@
  */
 package org.apache.ambari.logsearch.conf;
 
+import javax.ws.rs.ApplicationPath;
+
 import org.apache.ambari.logsearch.rest.ServiceLogsResource;
-import org.glassfish.jersey.jackson.JacksonFeature;
 import org.glassfish.jersey.server.ResourceConfig;
 import org.glassfish.jersey.servlet.ServletProperties;
 
-import javax.ws.rs.ApplicationPath;
-
 @ApplicationPath(""/api/v1"")
 public class LogSearchJerseyResourceConfig extends ResourceConfig {
 
   public LogSearchJerseyResourceConfig() {
     packages(ServiceLogsResource.class.getPackage().getName());
-    register(JacksonFeature.class);
+    register(com.fasterxml.jackson.jaxrs.json.JacksonJaxbJsonProvider.class);","[{'comment': 'after that changed, every feature will work the same way? (lke if they . throwing an another exeption or the simple use cases)', 'commenter': 'oleewere'}, {'comment': 'I checked the source of JacksonFeature and finally it registers JacksonJaxbJsonProvider too. But it also adds exception mappers for JsonMappingException and JsonParseException resulting a text/plain error response. This is what we do not want so we can provide our own exception mappers.', 'commenter': 'kasakrisz'}]"
1967,ambari-infra/pom.xml,"@@ -99,6 +99,28 @@
   <build>
     <pluginManagement>
       <plugins>
+       <plugin>
+        <groupId>org.codehaus.mojo</groupId>
+        <artifactId>rpm-maven-plugin</artifactId>
+        <version>2.0.1</version>","[{'comment': 'can you use 2.1.4 version here (and for logsearch)? as that one is used in the assembly module', 'commenter': 'oleewere'}]"
2047,README.md,"@@ -0,0 +1,19 @@
+# Apache Ambari
+
+Apache Ambari is a tool for provisioning, managing, and monitoring Apache Hadoop clusters. Ambari consists of a set of RESTful APIs and a browser-based management interface.
+
+## Getting Started
+
+https://cwiki.apache.org/confluence/display/AMBARI/Quick+Start+Guide","[{'comment': 'later, that would be nice if the README.md itself would contain these descriptions that the wiki have', 'commenter': 'oleewere'}]"
2060,ambari-server/src/main/java/org/apache/ambari/server/state/cluster/ClusterImpl.java,"@@ -2786,6 +2808,8 @@ public void onClusterProvisioned(ClusterProvisionedEvent event) {
             e.getKey(), Collections.emptyMap(), e.getValue(),
             ""internal"", ""Removing temporary configurations after successful deployment""
           );
+          setBlueprintProvisioningState(BlueprintProvisioningState.FINISHED);
+          metadataHolder.updateData(controller.getClusterMetadataOnConfigsUpdate(this));","[{'comment': 'This should be outside of the `for` loop, as there may be more than one set of temporary properties.\r\n\r\nCode: Ideally the logic from `onClusterProvisionStarted` should be extracted to a parametrized method, and called with different states from here and from that method.', 'commenter': 'adoroszlai'}]"
2060,ambari-server/src/main/java/org/apache/ambari/server/state/cluster/ClusterImpl.java,"@@ -966,6 +972,22 @@ public void setProvisioningState(State provisioningState) {
     clusterEntity = clusterDAO.merge(clusterEntity);
   }
 
+  @Override
+  public BlueprintProvisioningState getBlueprintProvisioningState() {
+    BlueprintProvisioningState blueprintProvisioningState = null;
+    ClusterEntity clusterEntity = getClusterEntity();
+    blueprintProvisioningState = clusterEntity.getBlueprintProvisioningState();","[{'comment': ""Code: I think the local variable `blueprintProvisioningState` is unnecessary, value can be returned directly.  At the least the variable shouldn't be pre-declared with `null` value."", 'commenter': 'adoroszlai'}]"
2111,ambari-common/src/main/python/resource_management/libraries/execution_command/execution_command.py,"@@ -77,12 +80,18 @@ def get_value(self, query_string, default_value=None):
   def get_module_configs(self):
     return self.__module_configs
 
+  def get_stack_settings(self):
+    return self.__stack_settings
+
+  def get_cluster_settings(self):
+    return self.__cluster_settings
+
   def get_module_name(self):
     """"""
     Retrieve service type from command.json, eg. 'zookeeper', 'hdfs'
     :return: service type
     """"""
-    return self.__get_value(""serviceType"")
+    return self.__get_value(""serviceName"")","[{'comment': 'How will this work with custom service names, eg. `""ZK1""`?', 'commenter': 'adoroszlai'}, {'comment': 'Do we have serviceType in command.json? I cannot find it any more, could you give me a latest command.json which includes this entry? Thanks.', 'commenter': 'scottduan'}]"
2111,ambari-common/src/main/python/resource_management/libraries/execution_command/cluster_settings.py,"@@ -0,0 +1,126 @@
+#!/usr/bin/env python
+""""""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an ""AS IS"" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+""""""
+
+__all__ = [""ClusterSettings""]
+
+class ClusterSettings(object):
+  """"""
+  This class maps to ""/clusterSettings"" in command.json which includes cluster setting information of a cluster
+  """"""
+
+  def __init__(self, clusterSettings):
+    self.__cluster_settings = clusterSettings
+
+  def __get_value(self, key):
+    """"""
+    Get corresponding value from the key
+    :param key:
+    :return: value if key exist else None
+    """"""
+    return self.__cluster_settings.get(key)
+
+  def is_cluster_security_enabled(self):
+    """"""
+    Check cluster security enabled or not
+    :return: ""True"" or ""False"" string
+    """"""
+    security_enabled = self.__get_value(""security_enabled"")
+    return True if security_enabled.lower() == ""true"" else False","[{'comment': ""The doc comment shouldn't say the method returns a `string` when returned value is really a `bool`.  (Same issue in other methods below...)"", 'commenter': 'adoroszlai'}, {'comment': 'I will change the doc. I think the libraray should return the correct type, the original code makes the caller to call lower() and compare it with ""true"" or ""false"" string which is error prone.', 'commenter': 'scottduan'}]"
2111,ambari-common/src/main/python/resource_management/libraries/execution_command/cluster_settings.py,"@@ -0,0 +1,126 @@
+#!/usr/bin/env python
+""""""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an ""AS IS"" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+""""""
+
+__all__ = [""ClusterSettings""]
+
+class ClusterSettings(object):
+  """"""
+  This class maps to ""/clusterSettings"" in command.json which includes cluster setting information of a cluster
+  """"""
+
+  def __init__(self, clusterSettings):
+    self.__cluster_settings = clusterSettings
+
+  def __get_value(self, key):
+    """"""
+    Get corresponding value from the key
+    :param key:
+    :return: value if key exist else None
+    """"""
+    return self.__cluster_settings.get(key)
+
+  def is_cluster_security_enabled(self):
+    """"""
+    Check cluster security enabled or not
+    :return: ""True"" or ""False"" string
+    """"""
+    security_enabled = self.__get_value(""security_enabled"")
+    return True if security_enabled.lower() == ""true"" else False
+
+  def get_recovery_max_count(self):
+    """"""
+    Retrieve cluster recovery count
+    :return: String, need to convert to int
+    """"""
+    return int(self.__get_value(""recovery_max_count""))
+
+  def check_recovery_enabled(self):
+    """"""
+    Check if the cluster can be enabled or not
+    :return: ""True"" or ""False"" string
+    """"""
+    recovery_enabled =  self.__get_value(""recovery_enabled"")
+    return True if recovery_enabled.lower() == ""true"" else False
+
+  def get_recovery_type(self):
+    """"""
+    Retrieve cluster recovery type
+    :return: recovery type, i.e ""AUTO_START""
+    """"""
+    return self.__get_value(""recovery_type"")
+
+  def get_kerberos_domain(self):
+    """"""
+    Retrieve kerberos domain
+    :return: String as kerberos domain
+    """"""
+    return self.__get_value(""kerberos_domain"")
+
+  def get_smokeuser(self):
+    """"""
+    Retrieve smokeuser
+    :return: smkeuser string
+    """"""
+    return self.__get_value(""smokeuser"")
+
+  def get_user_group(self):
+    """"""
+    Retrieve cluster usergroup
+    :return: usergroup string
+    """"""
+    return self.__get_value(""user_group"")
+
+  def get_repo_suse_rhel_template(self):
+    """"""
+    Retrieve template of suse and rhel repo
+    :return: template string
+    """"""
+    return self.__get_value(""repo_suse_rhel_template"")
+
+  def get_repo_ubuntu_template(self):
+    """"""
+    Retrieve template of ubuntu repo
+    :return: template string
+    """"""
+    return self.__get_value(""repo_ubuntu_template"")
+
+  def check_override_uid(self):
+    """"""
+    Check if override_uid is true or false
+    :return: ""true"" or ""false"" string
+    """"""
+    override_uid =  self.__get_value(""override_uid"")
+    return True if override_uid.lower() == ""true"" else False
+
+  def check_sysprep_skip_copy_fast_jar_hdfs(self):
+    """"""
+    Check sysprep_skip_copy_fast_jar_hdfs is true or false
+    :return: ""true"" or ""false"" string
+    """"""
+    skip = self.__get_value(""sysprep_skip_copy_fast_jar_hdfs"")
+    return True if skip.lower() == ""true"" else False
+
+  def check_sysprep_skip_setup_jce(self):
+    skip = self.__get_value(""sysprep_skip_setup_jce"")
+    return True if skip.lower() == ""true"" else False
+
+  def check_ignore_groupsusers_create(self):
+    ignored = self.__get_value(""ignore_groupsusers_create"")
+    return True if ignored.lower() == ""true"" else False","[{'comment': 'Why\r\n\r\n```\r\nreturn True if ignored.lower() == ""true"" else False\r\n```\r\n\r\ninstead of simply\r\n\r\n```\r\nreturn ignored.lower() == ""true""\r\n```\r\n\r\n?  (Same issue in other methods above.)', 'commenter': 'adoroszlai'}]"
2111,ambari-common/src/main/python/resource_management/libraries/execution_command/cluster_settings.py,"@@ -0,0 +1,126 @@
+#!/usr/bin/env python
+""""""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an ""AS IS"" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+""""""
+
+__all__ = [""ClusterSettings""]
+
+class ClusterSettings(object):
+  """"""
+  This class maps to ""/clusterSettings"" in command.json which includes cluster setting information of a cluster
+  """"""
+
+  def __init__(self, clusterSettings):
+    self.__cluster_settings = clusterSettings
+
+  def __get_value(self, key):
+    """"""
+    Get corresponding value from the key
+    :param key:
+    :return: value if key exist else None
+    """"""
+    return self.__cluster_settings.get(key)
+
+  def is_cluster_security_enabled(self):
+    """"""
+    Check cluster security enabled or not
+    :return: ""True"" or ""False"" string
+    """"""
+    security_enabled = self.__get_value(""security_enabled"")
+    return True if security_enabled.lower() == ""true"" else False
+
+  def get_recovery_max_count(self):
+    """"""
+    Retrieve cluster recovery count
+    :return: String, need to convert to int
+    """"""
+    return int(self.__get_value(""recovery_max_count""))
+
+  def check_recovery_enabled(self):
+    """"""
+    Check if the cluster can be enabled or not
+    :return: ""True"" or ""False"" string
+    """"""
+    recovery_enabled =  self.__get_value(""recovery_enabled"")
+    return True if recovery_enabled.lower() == ""true"" else False
+
+  def get_recovery_type(self):
+    """"""
+    Retrieve cluster recovery type
+    :return: recovery type, i.e ""AUTO_START""
+    """"""
+    return self.__get_value(""recovery_type"")
+
+  def get_kerberos_domain(self):
+    """"""
+    Retrieve kerberos domain
+    :return: String as kerberos domain
+    """"""
+    return self.__get_value(""kerberos_domain"")
+
+  def get_smokeuser(self):
+    """"""
+    Retrieve smokeuser
+    :return: smkeuser string
+    """"""
+    return self.__get_value(""smokeuser"")
+
+  def get_user_group(self):
+    """"""
+    Retrieve cluster usergroup
+    :return: usergroup string
+    """"""
+    return self.__get_value(""user_group"")
+
+  def get_repo_suse_rhel_template(self):
+    """"""
+    Retrieve template of suse and rhel repo
+    :return: template string
+    """"""
+    return self.__get_value(""repo_suse_rhel_template"")
+
+  def get_repo_ubuntu_template(self):
+    """"""
+    Retrieve template of ubuntu repo
+    :return: template string
+    """"""
+    return self.__get_value(""repo_ubuntu_template"")
+
+  def check_override_uid(self):
+    """"""
+    Check if override_uid is true or false
+    :return: ""true"" or ""false"" string
+    """"""
+    override_uid =  self.__get_value(""override_uid"")
+    return True if override_uid.lower() == ""true"" else False
+
+  def check_sysprep_skip_copy_fast_jar_hdfs(self):
+    """"""
+    Check sysprep_skip_copy_fast_jar_hdfs is true or false
+    :return: ""true"" or ""false"" string
+    """"""
+    skip = self.__get_value(""sysprep_skip_copy_fast_jar_hdfs"")
+    return True if skip.lower() == ""true"" else False
+
+  def check_sysprep_skip_setup_jce(self):
+    skip = self.__get_value(""sysprep_skip_setup_jce"")
+    return True if skip.lower() == ""true"" else False","[{'comment': ""These `.lower()` calls will fail if the setting is not present with:\r\n\r\n```\r\nAttributeError: 'NoneType' object has no attribute 'lower'\r\n```"", 'commenter': 'adoroszlai'}]"
2111,ambari-common/src/main/python/resource_management/libraries/execution_command/stack_settings.py,"@@ -0,0 +1,109 @@
+#!/usr/bin/env python
+""""""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an ""AS IS"" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+""""""
+
+__all__ = [""StackSettings""]
+
+class StackSettings(object):
+  
+  # Stack related configs from stack's stack_settings.json
+  # TODO: these strings may be removed later
+  STACK_NAME_SETTING = ""stack_name""","[{'comment': 'Most of these constants are unused.  Why do we need them?', 'commenter': 'adoroszlai'}, {'comment': 'Not sure, so temporarily keep them here, will remove them later.', 'commenter': 'scottduan'}]"
2165,ambari-server/src/main/java/org/apache/ambari/server/configuration/AmbariServerConfigurationKey.java,"@@ -73,6 +73,7 @@
   REFERRAL_HANDLING(AmbariServerConfigurationCategory.LDAP_CONFIGURATION, ""ambari.ldap.advanced.referrals"", PLAINTEXT, ""follow"", ""Determines whether to follow LDAP referrals to other URLs when the LDAP controller doesn't have the requested object.""),
   PAGINATION_ENABLED(AmbariServerConfigurationCategory.LDAP_CONFIGURATION, ""ambari.ldap.advanced.pagination_enabled"", PLAINTEXT, ""true"", ""Determines whether results from LDAP are paginated when requested.""),
   COLLISION_BEHAVIOR(AmbariServerConfigurationCategory.LDAP_CONFIGURATION, ""ambari.ldap.advance.collision_behavior"", PLAINTEXT, ""convert"", ""Determines how to handle username collision while updating from LDAP.""),
+  DISABLE_ENDPOINT_IDENTIFICATION(AmbariServerConfigurationCategory.LDAP_CONFIGURATION, ""ambari.ldap.advance.disable_endpoint_identification"", PLAINTEXT, ""false"", ""Determines whether to disable endpoint identification (hostname verification) during SSL handshake while updating from LDAP.""),","[{'comment': ""Is it 'advance' or 'advanced' ?"", 'commenter': 'kasakrisz'}, {'comment': ""This should be 'advanced'. Just like the one before (collision behavior); let me change it..."", 'commenter': 'smolnar82'}, {'comment': ""+1 for 'advanced'. Looks like we have an issue with 'ambari.ldap.advance.collision_behavior' in the line above.  Fixin that will be a bummer. \r\n"", 'commenter': 'rlevas'}]"
2165,ambari-server/src/main/python/ambari_server/setupSecurity.py,"@@ -93,6 +93,7 @@
 LDAP_MGR_PASSWORD_FILENAME = ""ldap-password.dat""
 LDAP_ANONYMOUS_BIND=""ambari.ldap.connectivity.anonymous_bind""
 LDAP_USE_SSL=""ambari.ldap.connectivity.use_ssl""
+LDAP_DISABLE_ENDPOINT_IDENTIFICATION = ""ambari.ldap.advance.disable_endpoint_identification""","[{'comment': ""'advance' -> 'advanced'"", 'commenter': 'rlevas'}]"
2198,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/RequestResourceProvider.java,"@@ -324,6 +340,22 @@ public RequestStatusResponse invoke() throws AmbariException, AuthorizationExcep
     return resources;
   }
 
+  private void authorizeGetResources() throws SystemException {
+    final Set<RoleAuthorization> requiredAuthorizations = Sets.newHashSet();
+    for (String allowedPermissionName : ALLOWED_PERMISSION_NAMES) {
+      PermissionEntity permission = permissionDao.findByName(allowedPermissionName);
+      if (permission != null) {
+        for (RoleAuthorizationEntity roleAuthorization : permission.getAuthorizations()) {
+          requiredAuthorizations.add(RoleAuthorization.translate(roleAuthorization.getAuthorizationId()));
+        }
+      }
+    }
+
+    if (!AuthorizationHelper.isAuthorized(null, null, requiredAuthorizations)) {","[{'comment': 'Rather than testing authorization using roles, we test authorization using privileges (or authorizations - RoleAuthorization).  Privileges are assigned to roles and therefore new roles can be added without needing to update the code.\r\n\r\nThere are two cases here... one for Ambari-level requests and one for cluster-level requests.   Ambari-level requests have no resource associated with them, where as cluster-level requests have a cluster resource associated with them. \r\n\r\nFor the **Ambari-level** case, the a new authorization should be created... `AMBARI.VIEW_STATUS_INFO`. This privilege should be added to the AMBARI.ADMINISTRATOR role. \r\n\r\nOnce this is done, the authorization check would look something like:\r\n\r\n```\r\nAuthorizationHelper.isAuthorized(ResourceType.AMBARI, null, RoleAuthorization.AMBARI_VIEW_STATUS_INFO)\r\n```\r\n\r\nFor the **cluster-level** case, the following set of (existing) authorizations should be used... `CLUSTER.VIEW_STATUS_INFO`, `SERVICE.VIEW_STATUS_INFO`, and `HOST.VIEW_STATUS_INFO`. \r\n\r\nOnce this is done, the authorization check would look something like:\r\n\r\n```\r\nAuthorizationHelper.isAuthorized(ResourceType.CLUSTER, <cluster resource id>, Enum.setOf(RoleAuthorization.CLUSTER_VIEW_STATUS_INFO, RoleAuthorization.SERVICE_VIEW_STATUS_INFO, and RoleAuthorization.HOST_VIEW_STATUS_INFO))\r\n```\r\n', 'commenter': 'rlevas'}, {'comment': ""Got it; I'll update the code and submit a new PR as soon as I can"", 'commenter': 'smolnar82'}, {'comment': ""We have some mechanism in AbstractAuthorizedResourceProvider (and this class extends it) to check if get/create/delete/etc is allowed. The setRequiredGetAuthorizations() can be used to set the required role authorizations to access the resource. Can't we use that mechanism instead of checking this manually?"", 'commenter': 'zeroflag'}, {'comment': '@zeroflag , This is one of those odd cases where the authorization logic depends on the content of the request. So we need to first look to see if the request is for a cluster-level request or an Ambari-level request before forming and executing the authorization check.  \r\n\r\nIf this was not the case, then you are correct, we could use the mechanism you suggested. ', 'commenter': 'rlevas'}, {'comment': '@rlevas I see, thanks for the explanation.', 'commenter': 'zeroflag'}]"
2198,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/RequestResourceProvider.java,"@@ -324,6 +329,23 @@ public RequestStatusResponse invoke() throws AmbariException, AuthorizationExcep
     return resources;
   }
 
+  private void authorizeGetResources(String clusterName) throws NoSuchParentResourceException, AuthorizationException {
+    final boolean ambariLevelRequest = StringUtils.isBlank(clusterName);
+    final ResourceType resourceType = ambariLevelRequest ? ResourceType.AMBARI : ResourceType.CLUSTER;
+    Long resourceId;
+    try {
+      resourceId = ambariLevelRequest ? null : getClusterResourceId(clusterName);
+    } catch (AmbariException e) {
+      throw new NoSuchParentResourceException(""Error while fetching cluster resource ID"", e);
+    }
+    final Set<RoleAuthorization> requiredAuthorizations = ambariLevelRequest ? Sets.newHashSet(AMBARI_VIEW_STATUS_INFO)","[{'comment': '`EnumSet.of` is probably cheaper than `Sets.newHashSet`, but no matter.  All is good. \r\n', 'commenter': 'rlevas'}]"
2203,ambari-server/src/main/resources/common-services/ZOOKEEPER/3.4.5/package/templates/zookeeper_client_jaas.conf.j2,"@@ -19,5 +19,6 @@
 Client {
 com.sun.security.auth.module.Krb5LoginModule required
 useKeyTab=false
-useTicketCache=true;
+useTicketCache=true","[{'comment': 'I am not sure if this is the correct solution here.  I will help to investigate. \r\n', 'commenter': 'rlevas'}, {'comment': 'at least the service check passes with this change. There are also stack changes at `hdp_ambari_definitions`, but I did not publish the patch yet', 'commenter': 'Unknown'}, {'comment': '@dlysnichenko After playing with ZK service check for a while, I found that the solution is to add the following to the JAVA command that executes the ZK client:\r\n\r\n```\r\n -Dzookeeper.sasl.client.username=<principal name>\r\n```\r\n\r\nI think the easiest approach is to add this to the `CLIENT_JVMFLAGS` env in `zookeeper-env/content`.  \r\n\r\nFor example:\r\n```\r\nexport CLIENT_JVMFLAGS=""$CLIENT_JVMFLAGS -Djava.security.auth.login.config={{zk_client_jaas_file}} -Dzookeeper.sasl.client.username={{zk_principal_name}}""\r\n```\r\n\r\nThe `<principal name>` value is the first component of the ZK principal.  If the ZK principal is ""my_zk/c7401.ambari.apache.org@EXAMPLE.COM"", then the value that needs to be set for principal name is ""my_zk"".  Once you do this, you can view the KDC log and see entries like (in the case of an MIT KDC) during the service check. \r\n\r\n```\r\nAug 30 13:23:53 c7401.ambari.apache.org krb5kdc[14444](info): TGS_REQ (4 etypes {18 17 16 23}) 192.168.74.101: ISSUE: authtime 1535635426, etypes {rep=18 tkt=23 ses=18}, ambari-qa-c1@EXAMPLE.COM for my_zk/c7401.ambari.apache.org@EXAMPLE.COM\r\n```\r\n\r\nMy guess is that this will help any process that sources /etc/zookeeper/conf/zookeeper-env.sh before executing the ZK client. \r\n', 'commenter': 'rlevas'}, {'comment': '@rlevas ,\r\nThank you for hint, will update the patch', 'commenter': 'Unknown'}]"
2203,ambari-server/src/main/resources/common-services/ZOOKEEPER/3.4.5/package/scripts/params_linux.py,"@@ -82,6 +82,7 @@
 zoo_cfg_properties_map_length = len(zoo_cfg_properties_map)
 
 zk_principal_name = default(""/configurations/zookeeper-env/zookeeper_principal_name"", ""zookeeper@EXAMPLE.COM"")","[{'comment': 'I know this was not changed in this patch, however the default value is incorrect. It should be ""`zookeeper/_HOST@EXAMPLE.COM`"".    How does this affect the `split` in the next line.  I assume `zk_principal_user` would wind up being ""zookeeper/_HOST@EXAMPLE.COM"" rather than ""zookeeper"".  This this should be unlikely since ""/configurations/zookeeper-env/zookeeper_principal_name"" should be set appropriately. ', 'commenter': 'rlevas'}, {'comment': 'good catch, fixed', 'commenter': 'Unknown'}]"
2251,ambari-logsearch/ambari-logsearch-web/src/app/modules/shared/components/modal-dialog/modal-dialog.component.html,"@@ -0,0 +1,37 @@
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one or more
+  contributor license agreements.  See the NOTICE file distributed with
+  this work for additional information regarding copyright ownership.
+  The ASF licenses this file to You under the Apache License, Version 2.0
+  (the ""License""); you may not use this file except in compliance with
+  the License.  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an ""AS IS"" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+
+<div [class]=""(extraCssClass || '') + ' modal'"" [class.show]=""visible"">
+  <div *ngIf=""showBackdrop"" class=""modal-backdrop"" (click)=""onBackdropClick($event)""></div>
+  <div class=""modal-dialog"" role=""document"">
+    <div class=""modal-content"">
+      <div class=""modal-header"">","[{'comment': ""If there's neither title nor header content nor close button provided, there will be an empty area displayed (whis is the padding of empty `modal-header`). Don't think it's expected"", 'commenter': 'aBabiichuk'}]"
2251,ambari-logsearch/ambari-logsearch-web/src/app/modules/shared/components/modal-dialog/modal-dialog.component.html,"@@ -0,0 +1,37 @@
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one or more
+  contributor license agreements.  See the NOTICE file distributed with
+  this work for additional information regarding copyright ownership.
+  The ASF licenses this file to You under the Apache License, Version 2.0
+  (the ""License""); you may not use this file except in compliance with
+  the License.  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an ""AS IS"" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+
+<div [class]=""(extraCssClass || '') + ' modal'"" [class.show]=""visible"">
+  <div *ngIf=""showBackdrop"" class=""modal-backdrop"" (click)=""onBackdropClick($event)""></div>
+  <div class=""modal-dialog"" role=""document"">
+    <div class=""modal-content"">
+      <div class=""modal-header"">
+        <h5 *ngIf=""title"" class=""modal-title"">{{title | translate}}</h5>
+        <ng-content select=""header""></ng-content>
+        <button *ngIf=""showCloseBtn"" type=""button"" class=""close"" data-dismiss=""modal"" aria-label=""Close"" (click)=""onCloseBtnClick($event)"">
+          <span aria-hidden=""true"">&times;</span>
+        </button>
+      </div>
+      <div class=""modal-body"">
+        <ng-content select=""main""></ng-content>
+      </div>
+      <div class=""modal-footer"">","[{'comment': 'I guess the same is relevant for footer as well', 'commenter': 'aBabiichuk'}]"
2256,ambari-server/src/main/resources/scripts/post-user-creation-hook.sh,"@@ -82,21 +82,32 @@ echo ""Processing post user creation hook payload ...""
 JSON_INPUT=""$CSV_FILE.json""
 echo ""Generating json file $JSON_INPUT ...""
 
+REALM=${HDFS_PRINCIPAL##*@}
+
 echo ""["" | cat > ""$JSON_INPUT""
 while read -r LINE
 do
   USR_NAME=$(echo ""$LINE"" | awk -F, '{print $1}')
+
+  if [ ""$SECURITY_TYPE"" ==  ""KERBEROS"" ]
+  then
+    OWNER=""${USR_NAME}@${REALM}""","[{'comment': 'I believe that this is incorrect and maybe the issue is not really as indicated in the description.  This is what would happen assuming my user kinit-ed as rlevas@EXAMPLE.COM and the auth-to-local rules translated that principal to ""rlevas"" - which is the default behavior when Ambari manages the auth-to-local rules and the configured realm is EXAMPLE.COM:\r\n\r\n```\r\n[rlevas@c7401 ~]$ hdfs dfs -ls /user\r\nFound 4 items\r\ndrwxrwx---   - ambari-qa          hdfs          0 2018-08-03 21:27 /user/ambari-qa\r\ndrwxrwxr-x   - oozie              hdfs          0 2018-08-03 23:16 /user/oozie\r\ndrwx------   - rlevas             hdfs          0 2018-09-06 13:39 /user/rlevas\r\ndrwx------   - rlevas@EXAMPLE.COM hdfs          0 2018-09-06 13:43 /user/rlevas_princ\r\n[rlevas@c7401 ~]$ hdfs dfs -ls /user/rlevas_princ\r\nls: Permission denied: user=rlevas, access=READ_EXECUTE, inode=""/user/rlevas_princ"":rlevas@EXAMPLE.COM:hdfs:drwx------\r\n```', 'commenter': 'rlevas'}, {'comment': ""Robert is right, using the kerberos REALM (like EXAMPLE.COM) won't solve all customer scenarios. The user principal name (upn) can be different from the realm.\r\n"", 'commenter': 'ivannp'}]"
2259,ambari-server/src/main/java/org/apache/ambari/server/api/services/mpackadvisor/MpackAdvisorBlueprintProcessor.java,"@@ -0,0 +1,334 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.api.services.mpackadvisor;
+
+import static java.util.stream.Collectors.toMap;
+import static java.util.stream.Collectors.toSet;
+import static org.apache.ambari.server.utils.ExceptionUtils.unchecked;
+
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.ambari.server.api.services.AdvisorBlueprintProcessor;
+import org.apache.ambari.server.api.services.AmbariMetaInfo;
+import org.apache.ambari.server.api.services.mpackadvisor.recommendations.MpackRecommendationResponse;
+import org.apache.ambari.server.api.services.stackadvisor.StackAdvisorBlueprintProcessor;
+import org.apache.ambari.server.controller.internal.ConfigurationTopologyException;
+import org.apache.ambari.server.state.ComponentInfo;
+import org.apache.ambari.server.state.ServiceInfo;
+import org.apache.ambari.server.state.StackId;
+import org.apache.ambari.server.state.ValueAttributesInfo;
+import org.apache.ambari.server.topology.AdvisedConfiguration;
+import org.apache.ambari.server.topology.ClusterTopology;
+import org.apache.ambari.server.topology.Component;
+import org.apache.ambari.server.topology.ConfigRecommendationStrategy;
+import org.apache.ambari.server.topology.Configuration;
+import org.apache.ambari.server.topology.HostGroup;
+import org.apache.ambari.server.topology.HostGroupInfo;
+import org.apache.ambari.server.topology.MpackInstance;
+import org.apache.ambari.server.topology.ServiceInstance;
+import org.apache.commons.lang3.tuple.Pair;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.base.Preconditions;
+import com.google.common.base.Predicates;
+import com.google.common.collect.ImmutableSet;
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+import com.google.common.collect.Sets;
+import com.google.inject.Singleton;
+
+/**
+ * Generate advised configurations for blueprint cluster provisioning by the mpack advisor.
+ */
+@Singleton
+public class MpackAdvisorBlueprintProcessor implements AdvisorBlueprintProcessor {
+
+  private static final Logger LOG = LoggerFactory.getLogger(StackAdvisorBlueprintProcessor.class);
+
+  private static MpackAdvisorHelper mpackAdvisorHelper;
+
+  static final String RECOMMENDATION_FAILED = ""Configuration recommendation failed."";
+  static final String INVALID_RESPONSE = ""Configuration recommendation returned with invalid response."";
+  private static AmbariMetaInfo metaInfo;
+
+  public static void init(MpackAdvisorHelper instance, AmbariMetaInfo ambariMetaInfo) {
+    mpackAdvisorHelper = instance;
+    metaInfo = ambariMetaInfo;
+  }
+
+  private static final Map<String, String> userContext;
+  static
+  {
+    userContext = new HashMap<>();
+    userContext.put(""operation"", ""ClusterCreate"");
+  }
+
+  /**
+   * Recommend configurations by the mpack advisor, then store the results in cluster topology.
+   * @param clusterTopology cluster topology instance
+   * @param userProvidedConfigurations User configurations of cluster provided in Blueprint + Cluster template
+   */
+  public void adviseConfiguration(ClusterTopology clusterTopology, Map<String, Map<String, String>> userProvidedConfigurations) throws ConfigurationTopologyException {
+    MpackAdvisorRequest request = createMpackAdvisorRequest(clusterTopology, MpackAdvisorRequest.MpackAdvisorRequestType.CONFIGURATIONS);
+    try {
+      MpackRecommendationResponse response = mpackAdvisorHelper.recommend(request);
+      addAllAdvisedConfigurationsToTopology(response, clusterTopology, userProvidedConfigurations);
+    } catch (MpackAdvisorException e) {
+      throw new ConfigurationTopologyException(RECOMMENDATION_FAILED, e);
+    } catch (IllegalArgumentException e) {
+      throw new ConfigurationTopologyException(INVALID_RESPONSE, e);
+    }
+  }
+
+  private MpackAdvisorRequest createMpackAdvisorRequest(ClusterTopology clusterTopology,
+                                                        MpackAdvisorRequest.MpackAdvisorRequestType requestType) {
+    Map<String, Set<Component>> hgComponentsMap = gatherHostGroupComponents(clusterTopology);
+    Map<String, Set<String>> hgHostsMap = gatherHostGroupBindings(clusterTopology);
+    Map<String, Map<String, Set<String>>> mpackComponentsHostsMap = gatherMackComponentsHostsMap(hgComponentsMap,
+      hgHostsMap, clusterTopology.getMpacks());
+    Map<String, Set<String>> mpackComponentsMap = mpackComponentsHostsMap.entrySet().stream().collect(toMap(
+      Map.Entry::getKey,
+      componentHostMap -> Sets.newHashSet(componentHostMap.getValue().keySet())
+    ));
+    Set<MpackInstance> mpacks = copyAndEnrichMpackInstances(clusterTopology, mpackComponentsMap);
+    Configuration configuration = clusterTopology.getConfiguration();
+    return MpackAdvisorRequest.MpackAdvisorRequestBuilder
+      .forStack()
+      .forMpackInstances(mpacks)
+      .forHosts(gatherHosts(clusterTopology))
+      .forHostsGroupBindings(gatherHostGroupBindings(clusterTopology))
+      .forComponentHostsMap(getHostgroups(clusterTopology))
+      .withMpacksToComponentsHostsMap(mpackComponentsHostsMap)
+      .withConfigurations(calculateConfigs(configuration))
+      .withUserContext(userContext)
+      .ofType(requestType)
+      .build();
+  }
+
+  private Set<MpackInstance> copyAndEnrichMpackInstances(ClusterTopology topology,
+                                                         Map<String, Set<String>> mpackComponentsMap) {
+    // Copy mpacks
+    Set<MpackInstance> mpacks = topology.getMpacks().stream().map(MpackInstance::copy).collect(toSet());
+    // Add missing service instances
+    for (MpackInstance mpack: mpacks) {
+      Set<String> mpackComponents = mpackComponentsMap.get(mpack.getMpackType()); // TODO: this should be getMpackName() once mpack advisor fixed
+      for (ServiceInfo serviceInfo : unchecked(() -> metaInfo.getStack(mpack.getStackId())).getServices()) {
+        boolean serviceUsedInBlueprint =
+          serviceInfo.getComponents().stream().filter(comp -> mpackComponents.contains(comp.getName())).findAny().isPresent();
+        // TODO: we will will have to check for existing service instances once multi-service will be enabled and
+        // mpack instances in blueprints may contain service instances
+        if (serviceUsedInBlueprint) {
+          mpack.getServiceInstances().add(new ServiceInstance(serviceInfo.getName(), serviceInfo.getName(), null, mpack));
+        }
+      }
+    }
+    return mpacks;
+  }
+
+  private Collection<MpackRecommendationResponse.HostGroup> getHostgroups(ClusterTopology topology) {
+    // TODO: this will need to rewritten for true multi-everything (multiple mpacks of the same type/version under
+    //  different names)
+    Map<StackId, String> mpackNameByStackId = topology.getMpacks().stream().collect(
+      toMap(
+        MpackInstance::getStackId,
+        MpackInstance::getMpackName
+      ));
+
+    topology.getComponentsByHostgroup().entrySet().stream().collect(
+      toMap(
+  //      Map.Entry::getKey,
+        e -> e.getKey(),
+        components ->
+          components.getValue().stream()
+            .map(comp ->
+              MpackRecommendationResponse.HostGroup.createComponent(comp.componentName(),
+                mpackNameByStackId.get(comp.stackId()),
+                comp.serviceName().orElseGet(() -> comp.serviceType())))
+            .collect(toSet())
+      )
+    );
+
+    return null;","[{'comment': ""Shouldn't this method return the map created by the previous statement, instead of `null`?"", 'commenter': 'adoroszlai'}, {'comment': 'I think this is from an intermediate commit.', 'commenter': 'benyoka'}]"
2259,ambari-server/src/main/java/org/apache/ambari/server/api/services/mpackadvisor/MpackAdvisorBlueprintProcessor.java,"@@ -0,0 +1,334 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.api.services.mpackadvisor;
+
+import static java.util.stream.Collectors.toMap;
+import static java.util.stream.Collectors.toSet;
+import static org.apache.ambari.server.utils.ExceptionUtils.unchecked;
+
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.ambari.server.api.services.AdvisorBlueprintProcessor;
+import org.apache.ambari.server.api.services.AmbariMetaInfo;
+import org.apache.ambari.server.api.services.mpackadvisor.recommendations.MpackRecommendationResponse;
+import org.apache.ambari.server.api.services.stackadvisor.StackAdvisorBlueprintProcessor;
+import org.apache.ambari.server.controller.internal.ConfigurationTopologyException;
+import org.apache.ambari.server.state.ComponentInfo;
+import org.apache.ambari.server.state.ServiceInfo;
+import org.apache.ambari.server.state.StackId;
+import org.apache.ambari.server.state.ValueAttributesInfo;
+import org.apache.ambari.server.topology.AdvisedConfiguration;
+import org.apache.ambari.server.topology.ClusterTopology;
+import org.apache.ambari.server.topology.Component;
+import org.apache.ambari.server.topology.ConfigRecommendationStrategy;
+import org.apache.ambari.server.topology.Configuration;
+import org.apache.ambari.server.topology.HostGroup;
+import org.apache.ambari.server.topology.HostGroupInfo;
+import org.apache.ambari.server.topology.MpackInstance;
+import org.apache.ambari.server.topology.ServiceInstance;
+import org.apache.commons.lang3.tuple.Pair;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.base.Preconditions;
+import com.google.common.base.Predicates;
+import com.google.common.collect.ImmutableSet;
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+import com.google.common.collect.Sets;
+import com.google.inject.Singleton;
+
+/**
+ * Generate advised configurations for blueprint cluster provisioning by the mpack advisor.
+ */
+@Singleton
+public class MpackAdvisorBlueprintProcessor implements AdvisorBlueprintProcessor {
+
+  private static final Logger LOG = LoggerFactory.getLogger(StackAdvisorBlueprintProcessor.class);
+
+  private static MpackAdvisorHelper mpackAdvisorHelper;
+
+  static final String RECOMMENDATION_FAILED = ""Configuration recommendation failed."";
+  static final String INVALID_RESPONSE = ""Configuration recommendation returned with invalid response."";
+  private static AmbariMetaInfo metaInfo;
+
+  public static void init(MpackAdvisorHelper instance, AmbariMetaInfo ambariMetaInfo) {
+    mpackAdvisorHelper = instance;
+    metaInfo = ambariMetaInfo;
+  }
+
+  private static final Map<String, String> userContext;
+  static
+  {
+    userContext = new HashMap<>();
+    userContext.put(""operation"", ""ClusterCreate"");
+  }
+
+  /**
+   * Recommend configurations by the mpack advisor, then store the results in cluster topology.
+   * @param clusterTopology cluster topology instance
+   * @param userProvidedConfigurations User configurations of cluster provided in Blueprint + Cluster template
+   */
+  public void adviseConfiguration(ClusterTopology clusterTopology, Map<String, Map<String, String>> userProvidedConfigurations) throws ConfigurationTopologyException {
+    MpackAdvisorRequest request = createMpackAdvisorRequest(clusterTopology, MpackAdvisorRequest.MpackAdvisorRequestType.CONFIGURATIONS);
+    try {
+      MpackRecommendationResponse response = mpackAdvisorHelper.recommend(request);
+      addAllAdvisedConfigurationsToTopology(response, clusterTopology, userProvidedConfigurations);
+    } catch (MpackAdvisorException e) {
+      throw new ConfigurationTopologyException(RECOMMENDATION_FAILED, e);
+    } catch (IllegalArgumentException e) {
+      throw new ConfigurationTopologyException(INVALID_RESPONSE, e);
+    }
+  }
+
+  private MpackAdvisorRequest createMpackAdvisorRequest(ClusterTopology clusterTopology,
+                                                        MpackAdvisorRequest.MpackAdvisorRequestType requestType) {
+    Map<String, Set<Component>> hgComponentsMap = gatherHostGroupComponents(clusterTopology);
+    Map<String, Set<String>> hgHostsMap = gatherHostGroupBindings(clusterTopology);
+    Map<String, Map<String, Set<String>>> mpackComponentsHostsMap = gatherMackComponentsHostsMap(hgComponentsMap,
+      hgHostsMap, clusterTopology.getMpacks());
+    Map<String, Set<String>> mpackComponentsMap = mpackComponentsHostsMap.entrySet().stream().collect(toMap(
+      Map.Entry::getKey,
+      componentHostMap -> Sets.newHashSet(componentHostMap.getValue().keySet())
+    ));
+    Set<MpackInstance> mpacks = copyAndEnrichMpackInstances(clusterTopology, mpackComponentsMap);
+    Configuration configuration = clusterTopology.getConfiguration();
+    return MpackAdvisorRequest.MpackAdvisorRequestBuilder
+      .forStack()
+      .forMpackInstances(mpacks)
+      .forHosts(gatherHosts(clusterTopology))
+      .forHostsGroupBindings(gatherHostGroupBindings(clusterTopology))
+      .forComponentHostsMap(getHostgroups(clusterTopology))
+      .withMpacksToComponentsHostsMap(mpackComponentsHostsMap)
+      .withConfigurations(calculateConfigs(configuration))
+      .withUserContext(userContext)
+      .ofType(requestType)
+      .build();
+  }
+
+  private Set<MpackInstance> copyAndEnrichMpackInstances(ClusterTopology topology,
+                                                         Map<String, Set<String>> mpackComponentsMap) {
+    // Copy mpacks
+    Set<MpackInstance> mpacks = topology.getMpacks().stream().map(MpackInstance::copy).collect(toSet());
+    // Add missing service instances
+    for (MpackInstance mpack: mpacks) {
+      Set<String> mpackComponents = mpackComponentsMap.get(mpack.getMpackType()); // TODO: this should be getMpackName() once mpack advisor fixed
+      for (ServiceInfo serviceInfo : unchecked(() -> metaInfo.getStack(mpack.getStackId())).getServices()) {
+        boolean serviceUsedInBlueprint =
+          serviceInfo.getComponents().stream().filter(comp -> mpackComponents.contains(comp.getName())).findAny().isPresent();
+        // TODO: we will will have to check for existing service instances once multi-service will be enabled and
+        // mpack instances in blueprints may contain service instances
+        if (serviceUsedInBlueprint) {
+          mpack.getServiceInstances().add(new ServiceInstance(serviceInfo.getName(), serviceInfo.getName(), null, mpack));
+        }
+      }
+    }
+    return mpacks;
+  }
+
+  private Collection<MpackRecommendationResponse.HostGroup> getHostgroups(ClusterTopology topology) {
+    // TODO: this will need to rewritten for true multi-everything (multiple mpacks of the same type/version under
+    //  different names)
+    Map<StackId, String> mpackNameByStackId = topology.getMpacks().stream().collect(
+      toMap(
+        MpackInstance::getStackId,
+        MpackInstance::getMpackName
+      ));
+
+    topology.getComponentsByHostgroup().entrySet().stream().collect(
+      toMap(
+  //      Map.Entry::getKey,
+        e -> e.getKey(),
+        components ->
+          components.getValue().stream()
+            .map(comp ->
+              MpackRecommendationResponse.HostGroup.createComponent(comp.componentName(),
+                mpackNameByStackId.get(comp.stackId()),
+                comp.serviceName().orElseGet(() -> comp.serviceType())))
+            .collect(toSet())
+      )
+    );
+
+    return null;
+  }
+
+  private Map<String, Set<String>> gatherHostGroupBindings(ClusterTopology clusterTopology) {
+    Map<String, Set<String>> hgBindngs = Maps.newHashMap();","[{'comment': 'Typo in `hgBindngs`, should be `hgBindings`.', 'commenter': 'adoroszlai'}, {'comment': 'Thanks.', 'commenter': 'benyoka'}]"
2259,ambari-server/src/main/java/org/apache/ambari/server/configuration/Configuration.java,"@@ -3368,6 +3377,10 @@ public String getMpackAdvisorScript() {
     return getProperty(MPACK_ADVISOR_SCRIPT);
   }
 
+  public boolean isUseLegacyStackAdvisor() {","[{'comment': 'The name `shouldUseLegacyStackAdvisor` would be more natural instead of `isUseLegacyStackAdvisor`.  The latter may be OK for a JavaBean.', 'commenter': 'adoroszlai'}, {'comment': 'Ok.', 'commenter': 'benyoka'}]"
2259,ambari-server/src/main/java/org/apache/ambari/server/topology/ClusterTopology.java,"@@ -148,6 +148,8 @@
    */
   Stream<ResolvedComponent> getComponents();
 
+  Map<String, Set<ResolvedComponent>> getComponentsByHostgroup();","[{'comment': '`Group` with capital G in `getComponentsByHostGroup` would be more in line with other methods in this interface.', 'commenter': 'adoroszlai'}, {'comment': 'Ok', 'commenter': 'benyoka'}]"
2259,ambari-server/src/main/java/org/apache/ambari/server/utils/ExceptionUtils.java,"@@ -0,0 +1,41 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.utils;
+
+import java.io.IOException;
+import java.io.UncheckedIOException;
+
+public class ExceptionUtils {
+
+  public interface ThrowingLambda<T extends Exception, R> {
+    R doIt() throws T;","[{'comment': 'Since `T` is never used (see `? extends` in `unchecked`), I think we could get rid of this custom interface in favor of [`Callable`](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/Callable.html).', 'commenter': 'adoroszlai'}, {'comment': 'Good catch', 'commenter': 'benyoka'}]"
2259,ambari-server/src/main/java/org/apache/ambari/server/utils/ExceptionUtils.java,"@@ -0,0 +1,41 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.utils;
+
+import java.io.IOException;
+import java.io.UncheckedIOException;
+
+public class ExceptionUtils {
+
+  public interface ThrowingLambda<T extends Exception, R> {
+    R doIt() throws T;
+  }
+
+  public static <R> R unchecked(ThrowingLambda<? extends Exception, R> throwingLambda) {","[{'comment': 'Please add Javadoc.', 'commenter': 'adoroszlai'}, {'comment': 'Added', 'commenter': 'benyoka'}]"
2259,ambari-server/src/main/java/org/apache/ambari/server/api/services/mpackadvisor/MpackAdvisorBlueprintProcessor.java,"@@ -0,0 +1,334 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.api.services.mpackadvisor;
+
+import static java.util.stream.Collectors.toMap;
+import static java.util.stream.Collectors.toSet;
+import static org.apache.ambari.server.utils.ExceptionUtils.unchecked;
+
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.ambari.server.api.services.AdvisorBlueprintProcessor;
+import org.apache.ambari.server.api.services.AmbariMetaInfo;
+import org.apache.ambari.server.api.services.mpackadvisor.recommendations.MpackRecommendationResponse;
+import org.apache.ambari.server.api.services.stackadvisor.StackAdvisorBlueprintProcessor;
+import org.apache.ambari.server.controller.internal.ConfigurationTopologyException;
+import org.apache.ambari.server.state.ComponentInfo;
+import org.apache.ambari.server.state.ServiceInfo;
+import org.apache.ambari.server.state.StackId;
+import org.apache.ambari.server.state.ValueAttributesInfo;
+import org.apache.ambari.server.topology.AdvisedConfiguration;
+import org.apache.ambari.server.topology.ClusterTopology;
+import org.apache.ambari.server.topology.Component;
+import org.apache.ambari.server.topology.ConfigRecommendationStrategy;
+import org.apache.ambari.server.topology.Configuration;
+import org.apache.ambari.server.topology.HostGroup;
+import org.apache.ambari.server.topology.HostGroupInfo;
+import org.apache.ambari.server.topology.MpackInstance;
+import org.apache.ambari.server.topology.ServiceInstance;
+import org.apache.commons.lang3.tuple.Pair;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.base.Preconditions;
+import com.google.common.base.Predicates;
+import com.google.common.collect.ImmutableSet;
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+import com.google.common.collect.Sets;
+import com.google.inject.Singleton;
+
+/**
+ * Generate advised configurations for blueprint cluster provisioning by the mpack advisor.
+ */
+@Singleton
+public class MpackAdvisorBlueprintProcessor implements AdvisorBlueprintProcessor {
+
+  private static final Logger LOG = LoggerFactory.getLogger(StackAdvisorBlueprintProcessor.class);
+
+  private static MpackAdvisorHelper mpackAdvisorHelper;
+
+  static final String RECOMMENDATION_FAILED = ""Configuration recommendation failed."";
+  static final String INVALID_RESPONSE = ""Configuration recommendation returned with invalid response."";
+  private static AmbariMetaInfo metaInfo;
+
+  public static void init(MpackAdvisorHelper instance, AmbariMetaInfo ambariMetaInfo) {
+    mpackAdvisorHelper = instance;
+    metaInfo = ambariMetaInfo;
+  }
+
+  private static final Map<String, String> userContext;
+  static
+  {
+    userContext = new HashMap<>();
+    userContext.put(""operation"", ""ClusterCreate"");
+  }
+
+  /**
+   * Recommend configurations by the mpack advisor, then store the results in cluster topology.
+   * @param clusterTopology cluster topology instance
+   * @param userProvidedConfigurations User configurations of cluster provided in Blueprint + Cluster template","[{'comment': 'I think this doc (with the exception of ""mpack"") belongs to the interface being implemented.', 'commenter': 'adoroszlai'}, {'comment': 'Ok', 'commenter': 'benyoka'}]"
2259,ambari-server/src/main/java/org/apache/ambari/server/api/services/mpackadvisor/MpackAdvisorBlueprintProcessor.java,"@@ -0,0 +1,334 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.api.services.mpackadvisor;
+
+import static java.util.stream.Collectors.toMap;
+import static java.util.stream.Collectors.toSet;
+import static org.apache.ambari.server.utils.ExceptionUtils.unchecked;
+
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.ambari.server.api.services.AdvisorBlueprintProcessor;
+import org.apache.ambari.server.api.services.AmbariMetaInfo;
+import org.apache.ambari.server.api.services.mpackadvisor.recommendations.MpackRecommendationResponse;
+import org.apache.ambari.server.api.services.stackadvisor.StackAdvisorBlueprintProcessor;
+import org.apache.ambari.server.controller.internal.ConfigurationTopologyException;
+import org.apache.ambari.server.state.ComponentInfo;
+import org.apache.ambari.server.state.ServiceInfo;
+import org.apache.ambari.server.state.StackId;
+import org.apache.ambari.server.state.ValueAttributesInfo;
+import org.apache.ambari.server.topology.AdvisedConfiguration;
+import org.apache.ambari.server.topology.ClusterTopology;
+import org.apache.ambari.server.topology.Component;
+import org.apache.ambari.server.topology.ConfigRecommendationStrategy;
+import org.apache.ambari.server.topology.Configuration;
+import org.apache.ambari.server.topology.HostGroup;
+import org.apache.ambari.server.topology.HostGroupInfo;
+import org.apache.ambari.server.topology.MpackInstance;
+import org.apache.ambari.server.topology.ServiceInstance;
+import org.apache.commons.lang3.tuple.Pair;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.base.Preconditions;
+import com.google.common.base.Predicates;
+import com.google.common.collect.ImmutableSet;
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+import com.google.common.collect.Sets;
+import com.google.inject.Singleton;
+
+/**
+ * Generate advised configurations for blueprint cluster provisioning by the mpack advisor.
+ */
+@Singleton
+public class MpackAdvisorBlueprintProcessor implements AdvisorBlueprintProcessor {
+
+  private static final Logger LOG = LoggerFactory.getLogger(StackAdvisorBlueprintProcessor.class);
+
+  private static MpackAdvisorHelper mpackAdvisorHelper;
+
+  static final String RECOMMENDATION_FAILED = ""Configuration recommendation failed."";
+  static final String INVALID_RESPONSE = ""Configuration recommendation returned with invalid response."";
+  private static AmbariMetaInfo metaInfo;
+
+  public static void init(MpackAdvisorHelper instance, AmbariMetaInfo ambariMetaInfo) {
+    mpackAdvisorHelper = instance;
+    metaInfo = ambariMetaInfo;
+  }
+
+  private static final Map<String, String> userContext;
+  static
+  {
+    userContext = new HashMap<>();
+    userContext.put(""operation"", ""ClusterCreate"");","[{'comment': 'Can be replaced with an `ImmutableMap`, initialized at the field declaration?', 'commenter': 'adoroszlai'}, {'comment': 'Ok', 'commenter': 'benyoka'}]"
2259,ambari-server/src/main/java/org/apache/ambari/server/api/services/mpackadvisor/MpackAdvisorBlueprintProcessor.java,"@@ -0,0 +1,334 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.api.services.mpackadvisor;
+
+import static java.util.stream.Collectors.toMap;
+import static java.util.stream.Collectors.toSet;
+import static org.apache.ambari.server.utils.ExceptionUtils.unchecked;
+
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.ambari.server.api.services.AdvisorBlueprintProcessor;
+import org.apache.ambari.server.api.services.AmbariMetaInfo;
+import org.apache.ambari.server.api.services.mpackadvisor.recommendations.MpackRecommendationResponse;
+import org.apache.ambari.server.api.services.stackadvisor.StackAdvisorBlueprintProcessor;
+import org.apache.ambari.server.controller.internal.ConfigurationTopologyException;
+import org.apache.ambari.server.state.ComponentInfo;
+import org.apache.ambari.server.state.ServiceInfo;
+import org.apache.ambari.server.state.StackId;
+import org.apache.ambari.server.state.ValueAttributesInfo;
+import org.apache.ambari.server.topology.AdvisedConfiguration;
+import org.apache.ambari.server.topology.ClusterTopology;
+import org.apache.ambari.server.topology.Component;
+import org.apache.ambari.server.topology.ConfigRecommendationStrategy;
+import org.apache.ambari.server.topology.Configuration;
+import org.apache.ambari.server.topology.HostGroup;
+import org.apache.ambari.server.topology.HostGroupInfo;
+import org.apache.ambari.server.topology.MpackInstance;
+import org.apache.ambari.server.topology.ServiceInstance;
+import org.apache.commons.lang3.tuple.Pair;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.base.Preconditions;
+import com.google.common.base.Predicates;
+import com.google.common.collect.ImmutableSet;
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+import com.google.common.collect.Sets;
+import com.google.inject.Singleton;
+
+/**
+ * Generate advised configurations for blueprint cluster provisioning by the mpack advisor.
+ */
+@Singleton
+public class MpackAdvisorBlueprintProcessor implements AdvisorBlueprintProcessor {
+
+  private static final Logger LOG = LoggerFactory.getLogger(StackAdvisorBlueprintProcessor.class);
+
+  private static MpackAdvisorHelper mpackAdvisorHelper;
+
+  static final String RECOMMENDATION_FAILED = ""Configuration recommendation failed."";
+  static final String INVALID_RESPONSE = ""Configuration recommendation returned with invalid response."";","[{'comment': 'These constants could be extracted to the interface to avoid duplication.', 'commenter': 'adoroszlai'}, {'comment': 'Ok', 'commenter': 'benyoka'}]"
2259,ambari-server/src/main/java/org/apache/ambari/server/api/services/mpackadvisor/MpackAdvisorBlueprintProcessor.java,"@@ -0,0 +1,334 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.ambari.server.api.services.mpackadvisor;
+
+import static java.util.stream.Collectors.toMap;
+import static java.util.stream.Collectors.toSet;
+import static org.apache.ambari.server.utils.ExceptionUtils.unchecked;
+
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.ambari.server.api.services.AdvisorBlueprintProcessor;
+import org.apache.ambari.server.api.services.AmbariMetaInfo;
+import org.apache.ambari.server.api.services.mpackadvisor.recommendations.MpackRecommendationResponse;
+import org.apache.ambari.server.api.services.stackadvisor.StackAdvisorBlueprintProcessor;
+import org.apache.ambari.server.controller.internal.ConfigurationTopologyException;
+import org.apache.ambari.server.state.ComponentInfo;
+import org.apache.ambari.server.state.ServiceInfo;
+import org.apache.ambari.server.state.StackId;
+import org.apache.ambari.server.state.ValueAttributesInfo;
+import org.apache.ambari.server.topology.AdvisedConfiguration;
+import org.apache.ambari.server.topology.ClusterTopology;
+import org.apache.ambari.server.topology.Component;
+import org.apache.ambari.server.topology.ConfigRecommendationStrategy;
+import org.apache.ambari.server.topology.Configuration;
+import org.apache.ambari.server.topology.HostGroup;
+import org.apache.ambari.server.topology.HostGroupInfo;
+import org.apache.ambari.server.topology.MpackInstance;
+import org.apache.ambari.server.topology.ServiceInstance;
+import org.apache.commons.lang3.tuple.Pair;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.base.Preconditions;
+import com.google.common.base.Predicates;
+import com.google.common.collect.ImmutableSet;
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+import com.google.common.collect.Sets;
+import com.google.inject.Singleton;
+
+/**
+ * Generate advised configurations for blueprint cluster provisioning by the mpack advisor.
+ */
+@Singleton
+public class MpackAdvisorBlueprintProcessor implements AdvisorBlueprintProcessor {
+
+  private static final Logger LOG = LoggerFactory.getLogger(StackAdvisorBlueprintProcessor.class);
+
+  private static MpackAdvisorHelper mpackAdvisorHelper;
+
+  static final String RECOMMENDATION_FAILED = ""Configuration recommendation failed."";
+  static final String INVALID_RESPONSE = ""Configuration recommendation returned with invalid response."";
+  private static AmbariMetaInfo metaInfo;
+
+  public static void init(MpackAdvisorHelper instance, AmbariMetaInfo ambariMetaInfo) {
+    mpackAdvisorHelper = instance;
+    metaInfo = ambariMetaInfo;
+  }
+
+  private static final Map<String, String> userContext;
+  static
+  {
+    userContext = new HashMap<>();
+    userContext.put(""operation"", ""ClusterCreate"");
+  }
+
+  /**
+   * Recommend configurations by the mpack advisor, then store the results in cluster topology.
+   * @param clusterTopology cluster topology instance
+   * @param userProvidedConfigurations User configurations of cluster provided in Blueprint + Cluster template
+   */
+  public void adviseConfiguration(ClusterTopology clusterTopology, Map<String, Map<String, String>> userProvidedConfigurations) throws ConfigurationTopologyException {
+    MpackAdvisorRequest request = createMpackAdvisorRequest(clusterTopology, MpackAdvisorRequest.MpackAdvisorRequestType.CONFIGURATIONS);
+    try {
+      MpackRecommendationResponse response = mpackAdvisorHelper.recommend(request);
+      addAllAdvisedConfigurationsToTopology(response, clusterTopology, userProvidedConfigurations);
+    } catch (MpackAdvisorException e) {
+      throw new ConfigurationTopologyException(RECOMMENDATION_FAILED, e);
+    } catch (IllegalArgumentException e) {
+      throw new ConfigurationTopologyException(INVALID_RESPONSE, e);
+    }
+  }
+
+  private MpackAdvisorRequest createMpackAdvisorRequest(ClusterTopology clusterTopology,
+                                                        MpackAdvisorRequest.MpackAdvisorRequestType requestType) {
+    Map<String, Set<Component>> hgComponentsMap = gatherHostGroupComponents(clusterTopology);
+    Map<String, Set<String>> hgHostsMap = gatherHostGroupBindings(clusterTopology);
+    Map<String, Map<String, Set<String>>> mpackComponentsHostsMap = gatherMackComponentsHostsMap(hgComponentsMap,
+      hgHostsMap, clusterTopology.getMpacks());
+    Map<String, Set<String>> mpackComponentsMap = mpackComponentsHostsMap.entrySet().stream().collect(toMap(
+      Map.Entry::getKey,
+      componentHostMap -> Sets.newHashSet(componentHostMap.getValue().keySet())
+    ));
+    Set<MpackInstance> mpacks = copyAndEnrichMpackInstances(clusterTopology, mpackComponentsMap);
+    Configuration configuration = clusterTopology.getConfiguration();
+    return MpackAdvisorRequest.MpackAdvisorRequestBuilder
+      .forStack()
+      .forMpackInstances(mpacks)
+      .forHosts(gatherHosts(clusterTopology))
+      .forHostsGroupBindings(gatherHostGroupBindings(clusterTopology))
+      .forComponentHostsMap(getHostgroups(clusterTopology))
+      .withMpacksToComponentsHostsMap(mpackComponentsHostsMap)
+      .withConfigurations(calculateConfigs(configuration))
+      .withUserContext(userContext)
+      .ofType(requestType)
+      .build();
+  }
+
+  private Set<MpackInstance> copyAndEnrichMpackInstances(ClusterTopology topology,
+                                                         Map<String, Set<String>> mpackComponentsMap) {
+    // Copy mpacks
+    Set<MpackInstance> mpacks = topology.getMpacks().stream().map(MpackInstance::copy).collect(toSet());
+    // Add missing service instances
+    for (MpackInstance mpack: mpacks) {
+      Set<String> mpackComponents = mpackComponentsMap.get(mpack.getMpackType()); // TODO: this should be getMpackName() once mpack advisor fixed
+      for (ServiceInfo serviceInfo : unchecked(() -> metaInfo.getStack(mpack.getStackId())).getServices()) {
+        boolean serviceUsedInBlueprint =
+          serviceInfo.getComponents().stream().filter(comp -> mpackComponents.contains(comp.getName())).findAny().isPresent();
+        // TODO: we will will have to check for existing service instances once multi-service will be enabled and
+        // mpack instances in blueprints may contain service instances
+        if (serviceUsedInBlueprint) {
+          mpack.getServiceInstances().add(new ServiceInstance(serviceInfo.getName(), serviceInfo.getName(), null, mpack));
+        }
+      }
+    }
+    return mpacks;
+  }
+
+  private Collection<MpackRecommendationResponse.HostGroup> getHostgroups(ClusterTopology topology) {
+    // TODO: this will need to rewritten for true multi-everything (multiple mpacks of the same type/version under
+    //  different names)
+    Map<StackId, String> mpackNameByStackId = topology.getMpacks().stream().collect(
+      toMap(
+        MpackInstance::getStackId,
+        MpackInstance::getMpackName
+      ));
+
+    topology.getComponentsByHostgroup().entrySet().stream().collect(
+      toMap(
+  //      Map.Entry::getKey,
+        e -> e.getKey(),
+        components ->
+          components.getValue().stream()
+            .map(comp ->
+              MpackRecommendationResponse.HostGroup.createComponent(comp.componentName(),
+                mpackNameByStackId.get(comp.stackId()),
+                comp.serviceName().orElseGet(() -> comp.serviceType())))
+            .collect(toSet())
+      )
+    );
+
+    return null;
+  }
+
+  private Map<String, Set<String>> gatherHostGroupBindings(ClusterTopology clusterTopology) {
+    Map<String, Set<String>> hgBindngs = Maps.newHashMap();
+    for (Map.Entry<String, HostGroupInfo> hgEnrty: clusterTopology.getHostGroupInfo().entrySet()) {
+      hgBindngs.put(hgEnrty.getKey(), Sets.newCopyOnWriteArraySet(hgEnrty.getValue().getHostNames()));
+    }
+    return hgBindngs;
+  }
+
+  private Map<String, Set<Component>> gatherHostGroupComponents(ClusterTopology clusterTopology) {
+    Map<String, Set<Component>> hgComponentsMap = Maps.newHashMap();
+    for (Map.Entry<String, HostGroup> hgEnrty: clusterTopology.getBlueprint().getHostGroups().entrySet()) {
+      hgComponentsMap.put(hgEnrty.getKey(), Sets.newCopyOnWriteArraySet(hgEnrty.getValue().getComponents()));
+    }
+    return hgComponentsMap;
+  }
+
+  private Map<String, Map<String, Map<String, String>>> calculateConfigs(Configuration configuration) {
+    Map<String, Map<String, Map<String, String>>> result = Maps.newHashMap();
+    Map<String, Map<String, String>> fullProperties = configuration.getFullProperties();
+    for (Map.Entry<String, Map<String, String>> siteEntry : fullProperties.entrySet()) {
+      Map<String, Map<String, String>> propsMap = Maps.newHashMap();
+      propsMap.put(""properties"", siteEntry.getValue());
+      result.put(siteEntry.getKey(), propsMap);
+    }
+    return result;
+  }
+
+  private Map<String, Map<String, Set<String>>> gatherMackComponentsHostsMap(Map<String, Set<Component>> hostGroupComponents,
+                                                                Map<String, Set<String>> hostGroupHosts,
+                                                                Set<MpackInstance> mpacks) {
+
+    // Calculate helper maps
+    Map<String, Set<String>> mpackComponents = mpacks.stream()
+      .collect(toMap(
+        MpackInstance::getMpackType, // TODO: this is supposed to be MpackInstance::getMpackName, fix mpack advisor
+        mpack -> getStackComponents(mpack.getStackId())
+      ));
+
+    Map<String, Set<String>> componentNameToMpacks = mpackComponents.entrySet().stream()
+      .flatMap( mpc -> mpc.getValue().stream().map(component -> Pair.of(component, ImmutableSet.of(mpc.getKey()))) )
+      .collect(toMap(
+        Pair::getLeft,
+        Pair::getRight,
+        (set1, set2) -> ImmutableSet.copyOf(Sets.union(set1, set2))
+      ));
+
+    Map<String, Set<Component>> hostComponents = hostGroupComponents.entrySet().stream()
+      .flatMap( hgc -> hostGroupHosts.get(hgc.getKey()).stream().map(host -> Pair.of(host, hgc.getValue())))
+      .collect(toMap(Pair::getLeft, Pair::getRight));
+
+    // Calculate map to return: mpack -> component -> host
+    Map<String, Map<String, Set<String>>> mpackComponentHostsMap = new HashMap<>();
+
+    hostComponents.entrySet().forEach( entry -> {
+      String hostName = entry.getKey();
+      Set<Component> components = entry.getValue();
+      components.forEach( component -> {
+        Set<String> mpacksForComponent = getMpacksForComponent(component, componentNameToMpacks);
+        mpacksForComponent.forEach(mpack -> {","[{'comment': ""The way this class associates components to mpacks (and services in `copyAndEnrichMpackInstances`) is a bit redundant, since it's already performed by [`StackComponentResolver`](https://github.com/apache/ambari/blob/branch-feature-AMBARI-14714/ambari-server/src/main/java/org/apache/ambari/server/topology/StackComponentResolver.java) and the result is stored in [`ResolvedComponent`](https://github.com/apache/ambari/blob/branch-feature-AMBARI-14714/ambari-server/src/main/java/org/apache/ambari/server/topology/ResolvedComponent.java) instances.  If some information is missing, I think it should be added there to avoid duplicated, and potentially different, implementations."", 'commenter': 'adoroszlai'}, {'comment': ""Good catch. I've rewritten the logic to leverage ResolvedComponent's."", 'commenter': 'benyoka'}]"
2259,ambari-server/src/main/java/org/apache/ambari/server/api/services/mpackadvisor/MpackAdvisorBlueprintProcessor.java,"@@ -118,20 +109,30 @@ private MpackAdvisorRequest createMpackAdvisorRequest(ClusterTopology clusterTop
       .build();
   }
 
-  private Set<MpackInstance> copyAndEnrichMpackInstances(ClusterTopology topology,
-                                                         Map<String, Set<String>> mpackComponentsMap) {
+  private Set<MpackInstance> copyAndEnrichMpackInstances(ClusterTopology topology) {
     // Copy mpacks
     Set<MpackInstance> mpacks = topology.getMpacks().stream().map(MpackInstance::copy).collect(toSet());
+
     // Add missing service instances
+    Map<StackId, Set<String>> mpackServices = topology.getComponents().collect(toMap(
+      ResolvedComponent::stackId,
+      comp -> ImmutableSet.of(comp.serviceInfo().getName()),
+      (set1, set2) -> ImmutableSet.copyOf(Sets.union(set1, set2))","[{'comment': 'Can be done less expensively (and perhaps more simply) by using built-ins:\r\n\r\n```\r\nMap<StackId, Set<String>> mpackServices = topology.getComponents().collect(\r\n  groupingBy(ResolvedComponent::stackId,\r\n    mapping(comp -> comp.serviceInfo().getName(), toSet())));\r\n```', 'commenter': 'adoroszlai'}]"
2339,ambari-logsearch/ambari-logsearch-web/src/assets/i18n/en.json,"@@ -47,7 +47,13 @@
   ""filter.users"": ""Users"",
 
   ""filter.capture"": ""Capture"",
+  ""filter.captureSpanshot"": ""Snapshot"",
+  ""filter.refreshingLogListIn"": ""Refreshing log list in..."",
+  ""filter.capture.min"": ""Min"",
+  ""filter.capture.sec"": ""Sec"",
   ""filter.capture.triggeringRefresh"": ""Triggering auto-refresh in {{remainingSeconds}} sec"",
+  ""filter.youAreInSpanshotView"": ""You are in spanshot view"",","[{'comment': ""Typo: should be 'snapshot', not 'spanshot'"", 'commenter': 'aBabiichuk'}]"
2339,ambari-logsearch/ambari-logsearch-web/src/app/components/logs-container/logs-container.component.html,"@@ -65,3 +63,22 @@
   <log-context *ngIf=""isServiceLogContextView"" [id]=""activeLog.id"" [hostName]=""activeLog.host_name""
                [componentName]=""activeLog.component_name""></log-context>
 </div>
+<modal-dialog
+  title=""{{'filter.capture' | translate}}""
+  class=""capture-dialog""
+  [visible]=""autoRefreshRemainingSeconds""
+  (onCloseRequest)=""cancelRequest()"">","[{'comment': ""`cancelRequest` is not defined anywhere. Also, based on this method's name, I guess it should cancel the future request with new filter. As far as I understand, that's not what user would expect when he/she just wants to close the modal."", 'commenter': 'aBabiichuk'}]"
2355,ambari-server/src/main/java/org/apache/ambari/server/topology/AmbariContext.java,"@@ -710,26 +712,21 @@ private void createConfigGroupsAndRegisterHost(ClusterTopology topology, String
     // iterate over topo host group configs which were defined in
     for (Map.Entry<String, Map<String, String>> entry : userProvidedGroupProperties.entrySet()) {
       String type = entry.getKey();
-      String service = stack.getServicesForConfigType(type)
-        .filter(each -> topology.getServiceTypes().contains(each))
-        .findFirst()
-        // TODO check if this is required at all (might be handled by the ""orphan"" removal)
-        // TODO move this validation earlier
-        .orElseThrow(() -> new IllegalArgumentException(""Specified configuration type is not associated with any service in the blueprint: "" + type));
-
-      Config config = configFactory.createReadOnly(type, groupName, entry.getValue(), null);
-      //todo: attributes
-      Map<String, Config> serviceConfigs = groupConfigs.get(service);
-      if (serviceConfigs == null) {
-        serviceConfigs = new HashMap<>();
-        groupConfigs.put(service, serviceConfigs);
-      }
-      serviceConfigs.put(type, config);
+      List<Pair<StackId, String>> stackServices = stack.getStackServicesForConfigType(type).
+        filter(each -> topology.getServiceTypes().contains(each.getValue())).collect(Collectors.toList());
+      Preconditions.checkArgument(!stackServices.isEmpty(), new IllegalArgumentException(""Specified configuration type is not associated with any service in the blueprint: "" + type));","[{'comment': 'Passing the message to `checkArgument` would be enough, no need to create the exception.  Something like:\r\n\r\n```\r\ncheckArgument(!stackServices.isEmpty(),\r\n  ""Specified configuration type is not associated with any service in the blueprint: %s"",\r\n  type));\r\n```', 'commenter': 'adoroszlai'}, {'comment': 'Thanks.', 'commenter': 'benyoka'}]"
2359,ambari-server/src/main/java/org/apache/ambari/server/events/publishers/AgentCommandsPublisher.java,"@@ -244,9 +244,12 @@ public static KerberosCommandParameterProcessor getInstance(String command, Clus
       KerberosServerAction.KerberosCommandParameters kerberosCommandParameters = new KerberosServerAction.KerberosCommandParameters(executionCommand);
 
       try {
-        Map<String, ? extends Collection<String>> serviceComponentFilter = getServiceComponentFilter(kerberosCommandParameters.getServiceComponentFilter());
+        Map<String, Collection<String>> serviceComponentFilter = getServiceComponentFilter(kerberosCommandParameters.getServiceComponentFilter());
 
         Set<ResolvedKerberosKeytab> keytabsToInject = kerberosKeytabController.getFilteredKeytabs(serviceComponentFilter, kerberosCommandParameters.getHostFilter(), kerberosCommandParameters.getIdentityFilter());
+        if (serviceComponentFilter != null) {
+          keytabsToInject.addAll(kerberosKeytabController.findKeytabsForServiceIdentities(executionCommand.getClusterName(), serviceComponentFilter));","[{'comment': 'It seems like this will negate any host filter.  Have you tested regenerating keytab files for a specific host?  Also, does this break any operations related to cleaning up identities?\r\n\r\nWhen I was testing this for a different patch, I believe that the `kerberosKeytabController.getFilteredKeytabs` call above found both service and user (headless) principals related to a given service. \r\n', 'commenter': 'rlevas'}, {'comment': 'This is true - `kerberosKeytabController.getFilteredKeytabs` finds both service and user principals - only if the given service has any components requiring the user principal on that host.\r\nI can reproduce this issue with 2.7.1.0-163 (see description above)\r\n\r\nThis was not the case in the cluster where QE found the issue: they created the cluster with BP - topology validation was switched off - with a host group where `Druid Historical` was present but no `HDFS Client`', 'commenter': 'smolnar82'}, {'comment': 'See`org.apache.ambari.server.orm.dao.KerberosKeytabPrincipalDAO.findByFilter(KerberosKeytabPrincipalFilter filter)`: since the given filter is populated with `service name = HDFS` the underlying DB query will find only the principals for the `Datanode` (which does not need the HDFS user principal)', 'commenter': 'smolnar82'}, {'comment': 'Which means that we may convert everything in the service name filter to the principal filter and do not check the service name at all.', 'commenter': 'smolnar82'}]"
2359,ambari-server/src/main/java/org/apache/ambari/server/serveraction/kerberos/KerberosServerAction.java,"@@ -449,13 +448,20 @@ protected CommandReport processIdentities(Map<String, Object> requestSharedDataC
       }
 
       try {
-        final Map<String, ? extends Collection<String>> serviceComponentFilter = (pruneServiceFilter())
+        final Map<String, Collection<String>> serviceComponentFilter = (pruneServiceFilter())
             ? kerberosKeytabController.adjustServiceComponentFilter(clusters.getCluster(getClusterName()), true, getServiceComponentFilter())
             : getServiceComponentFilter();
-        final Collection<KerberosIdentityDescriptor> serviceIdentities = serviceComponentFilter == null ? null : calculateServiceIdentities(getClusterName(), serviceComponentFilter);
-        for (ResolvedKerberosKeytab rkk : kerberosKeytabController.getFilteredKeytabs(serviceComponentFilter, getHostFilter(), getIdentityFilter())) {
+        final Collection<KerberosIdentityDescriptor> serviceIdentities = serviceComponentFilter == null ? null : kerberosKeytabController.calculateServiceIdentities(getClusterName(), serviceComponentFilter);
+        final Set<ResolvedKerberosKeytab> filteredKeytabs = kerberosKeytabController.getFilteredKeytabs(serviceComponentFilter, getHostFilter(), getIdentityFilter());","[{'comment': 'It is not clear why this is not pulling back all of the relevant `ResolvedKerberosKeytab` items.  If the Kerberos descriptor for TEZ_CLIENT indicates that it needs the HDFS headless keytab file, there should be a set of records in the Ambari database mapping the HDFS principal name and keytab file to TEZ_CLIENT.   If so, then this filter should return the correct items. If not, maybe the problem is there. ', 'commenter': 'rlevas'}, {'comment': ""```\r\nSELECT * FROM kerberos_keytab_principal inner join kkp_mapping_service using (kkp_id) where principal_name='hdfs-c1@EXAMPLE.COM';\r\n```\r\n```\r\n59\t/etc/security/keytabs/hdfs.headless.keytab\thdfs-c1@EXAMPLE.COM\t1\t1\tHDFS\tNAMENODE\r\n59\t/etc/security/keytabs/hdfs.headless.keytab\thdfs-c1@EXAMPLE.COM\t1\t1\tHDFS\tHDFS_CLIENT\r\n59\t/etc/security/keytabs/hdfs.headless.keytab\thdfs-c1@EXAMPLE.COM\t1\t1\tMAPREDUCE2\tHISTORYSERVER\r\n59\t/etc/security/keytabs/hdfs.headless.keytab\thdfs-c1@EXAMPLE.COM\t1\t1\tTEZ\tTEZ_CLIENT\r\n59\t/etc/security/keytabs/hdfs.headless.keytab\thdfs-c1@EXAMPLE.COM\t1\t1\tYARN\tAPP_TIMELINE_SERVER\r\n59\t/etc/security/keytabs/hdfs.headless.keytab\thdfs-c1@EXAMPLE.COM\t1\t1\tYARN\tTIMELINE_READER\r\n```"", 'commenter': 'rlevas'}, {'comment': ""Because it simply queries the DB (where we have no information about the Kerberos descriptor unless the user submitted one) based on the service filter we passed (i.e HDFS):\r\n```\r\nSELECT kkp.*, h.host_name, kkpm.service_name, kkpm.component_name\r\nFROM kerberos_keytab_principal kkp, hosts h, kkp_mapping_service kkpm\r\nWHERE kkp.host_id = h.host_id \r\nAND kkp.kkp_id = kkpm.kkp_id\r\nAND kkpm.service_name = 'HDFS' \r\nAND h.host_name = 'c7403.ambari.apache.org'\r\nORDER BY h.host_name, kkpm.service_name, kkpm.component_name;\r\n```\r\nReturns:\r\n\r\nkkp_id|keytab_path|principal_name|host_id|is_distributed|host_name|service_name|component_name\r\n-|-|-|-|-|-|-|-\r\n6|/etc/security/keytabs/spnego.service.keytab|HTTP/c7403.ambari.apache.org@AMBARI.APACHE.ORG|3|1|c7403.ambari.apache.org|HDFS|DATANODE\r\n30|/etc/security/keytabs/smokeuser.headless.keytab|ambari-qa-cluster1@AMBARI.APACHE.ORG|3|1|c7403.ambari.apache.org|HDFS|DATANODE\r\n37|/etc/security/keytabs/dn.service.keytab|dn/c7403.ambari.apache.org@AMBARI.APACHE.ORG|3|1|c7403.ambari.apache.org|HDFS|DATANODE\r\n\r\nThis is why it would only regenerate DN's keytab.\r\n\r\nHowever we know that other service components needs the headless keytab:\r\n```\r\nSELECT kkp.*, h.host_name, kkpm.service_name, kkpm.component_name\r\nFROM kerberos_keytab_principal kkp, hosts h, kkp_mapping_service kkpm\r\nWHERE kkp.host_id = h.host_id \r\nAND kkp.kkp_id = kkpm.kkp_id\r\nAND kkp.principal_name = 'hdfs-cluster1@AMBARI.APACHE.ORG' \r\nAND h.host_name = 'c7403.ambari.apache.org'\r\nORDER BY h.host_name, kkpm.service_name, kkpm.component_name\r\n```\r\nReturns:\r\n\r\nkkp_id|keytab_path|principal_name|host_id|is_distributed|host_name|service_name|component_name\r\n-|-|-|-|-|-|-|-\r\n15|/etc/security/keytabs/hdfs.headless.keytab|hdfs-cluster1@AMBARI.APACHE.ORG|3|1|c7403.ambari.apache.org|TEZ|TEZ_CLIENT\r\n15|/etc/security/keytabs/hdfs.headless.keytab|hdfs-cluster1@AMBARI.APACHE.ORG|3|1|c7403.ambari.apache.org|AMBARI_METRICS|METRICS_COLLECTOR\r\n\r\nThis is why we need to add other services; let me think it over again; there may be a more elegant and easy way to solve this issue."", 'commenter': 'smolnar82'}]"
2359,ambari-server/src/main/java/org/apache/ambari/server/serveraction/kerberos/stageutils/KerberosKeytabController.java,"@@ -54,6 +58,13 @@
   @Inject
   private KerberosKeytabPrincipalDAO kerberosKeytabPrincipalDAO;
 
+  //TODO: due to circular dependencies in Guice this field cannot be injected with Guice's @Inject annotation; for now we should statically inject in AmbariServer","[{'comment': 'This seems pretty bad.. ', 'commenter': 'rlevas'}, {'comment': 'How about injecting a Provider of KerberosHelper? That would allow lazy injection', 'commenter': 'Unknown'}, {'comment': ""That would also work (as described [here](https://github.com/google/guice/wiki/CyclicDependencies#break-the-cycle-with-a-provider)); but - IMO - Guice providers are just hiding cyclic dependencies which is the real issue here.\r\nIn this case it's not better or worse than the solution I applied (we have plenty of these in the code anyway); one advantage would be is to not add another static injection.\r\n\r\nLong story short: I'll submit a new patch in 5-10 mins."", 'commenter': 'smolnar82'}]"
2376,ambari-server/src/main/resources/Ambari-DDL-SQLServer-CREATE.sql,"@@ -589,15 +589,19 @@ CREATE TABLE configgroup (
   CONSTRAINT FK_configgroup_cluster_id FOREIGN KEY (cluster_id) REFERENCES clusters (cluster_id));
 
 CREATE TABLE confgroupclusterconfigmapping (
+  id BIGINT NOT NULL,
   config_group_id BIGINT NOT NULL,
   cluster_id BIGINT NOT NULL,
   config_type VARCHAR(255) NOT NULL,
+  service_id BIGINT,
   version_tag VARCHAR(255) NOT NULL,
   user_name VARCHAR(255) DEFAULT '_db',
   create_timestamp BIGINT NOT NULL,
-  CONSTRAINT PK_confgroupclustercfgmapping PRIMARY KEY CLUSTERED (config_group_id, cluster_id, config_type),
+  CONSTRAINT PK_confgroupclustercfgmapping PRIMARY KEY (id),
+  CONSTRAINT UQ_cgccm_cgid_cid_ctype_sid UNIQUE (config_group_id, cluster_id, config_type, service_id),
+  CONSTRAINT FK_cgccm_service FOREIGN KEY (service_id) REFERENCES clusterservices (id),
   CONSTRAINT FK_cgccm_gid FOREIGN KEY (config_group_id) REFERENCES configgroup (group_id),
-  CONSTRAINT FK_confg FOREIGN KEY (cluster_id, config_type, version_tag) REFERENCES clusterconfig (cluster_id, type_name, version_tag));
+  CONSTRAINT FK_confg FOREIGN KEY (version_tag, config_type, service_id, cluster_id) REFERENCES clusterconfig (version_tag, type_name, service_id, cluster_id));","[{'comment': ""Shouldn't `FK_confg` match the columns of `UQ_svc_id_config_type_tag` in order?  If the goal is to make all DDLs consistent, then can we keep/apply the order: `cluster_id, service_id, type_name, version_tag`?"", 'commenter': 'adoroszlai'}, {'comment': 'Ok', 'commenter': 'benyoka'}]"
2376,ambari-server/src/main/resources/Ambari-DDL-Derby-CREATE.sql,"@@ -179,7 +179,7 @@ CREATE TABLE clusterconfig (
   CONSTRAINT FK_clusterconfig_cluster_id FOREIGN KEY (cluster_id) REFERENCES clusters (cluster_id),
   CONSTRAINT FK_clusterconfig_stack_id FOREIGN KEY (stack_id) REFERENCES stack(stack_id),
   CONSTRAINT FK_clusterconfig_service_id FOREIGN KEY (service_id) REFERENCES clusterservices(id),
-  CONSTRAINT UQ_config_type_tag UNIQUE (version_tag, type_name, cluster_id),
+  CONSTRAINT UQ_svc_id_config_type_tag UNIQUE (cluster_id, service_id, type_name, version_tag),
   CONSTRAINT UQ_config_type_version UNIQUE (cluster_id, type_name, version));","[{'comment': ""Shouldn't `service_id` be included in `UQ_config_type_version` constraints as well?"", 'commenter': 'adoroszlai'}, {'comment': 'Good catch.', 'commenter': 'benyoka'}]"
2376,ambari-server/src/main/resources/Ambari-DDL-SQLAnywhere-CREATE.sql,"@@ -1314,6 +1318,7 @@ INSERT INTO ambari_sequences(sequence_name, sequence_value) values ('hostcompone
 INSERT INTO ambari_sequences(sequence_name, sequence_value) values ('mpack_inst_svc_id_seq', 0);
 INSERT INTO ambari_sequences(sequence_name, sequence_value) values ('mpack_instance_id_seq', 0);
 INSERT INTO ambari_sequences(sequence_name, sequence_value) values ('hostgroup_component_id_seq', 0);
+INSERT INTO ambari_sequences(sequence_name, sequence_value) values ('cnfgrpclstrcnfigmpg_id_seq', 0);","[{'comment': 'This new sequence `cnfgrpclstrcnfigmpg_id_seq` is missing from `ambari-server/src/main/resources/Ambari-DDL-SQLServer-CREATE.sql`.', 'commenter': 'adoroszlai'}, {'comment': 'Ok', 'commenter': 'benyoka'}]"
2376,ambari-server/src/main/java/org/apache/ambari/server/orm/entities/ConfigGroupConfigMappingEntity.java,"@@ -19,36 +19,52 @@
 
 import javax.persistence.Column;
 import javax.persistence.Entity;
+import javax.persistence.GeneratedValue;
+import javax.persistence.GenerationType;
 import javax.persistence.Id;
-import javax.persistence.IdClass;
 import javax.persistence.JoinColumn;
 import javax.persistence.JoinColumns;
 import javax.persistence.ManyToOne;
 import javax.persistence.NamedQueries;
 import javax.persistence.NamedQuery;
 import javax.persistence.Table;
+import javax.persistence.TableGenerator;
 
 @Entity
 @Table(name = ""confgroupclusterconfigmapping"")
-@IdClass(ConfigGroupConfigMappingEntityPK.class)
+@TableGenerator(name = ""confgroupclusterconfigmapping_id_generator"",
+  table = ""ambari_sequences"", pkColumnName = ""sequence_name"", valueColumnName = ""sequence_value"",
+  pkColumnValue = ""cnfgrpclstrcnfigmpg_id_seq"", initialValue = 1
+)
 @NamedQueries({
   @NamedQuery(name = ""configsByGroup"", query =
   ""SELECT configs FROM ConfigGroupConfigMappingEntity configs "" +
     ""WHERE configs.configGroupId=:groupId"")
 })
 public class ConfigGroupConfigMappingEntity {","[{'comment': 'Can you add the unique constraints defined in the SQL to this class?', 'commenter': 'jonathan-hurley'}]"
2411,ambari-server/src/main/java/org/apache/ambari/server/orm/dao/HostRoleCommandDAO.java,"@@ -286,30 +286,49 @@ public HostRoleCommandDAO(
   public HostRoleCommandEntity findByPK(long taskId) {
     return entityManagerProvider.get().find(HostRoleCommandEntity.class, taskId);
   }
+  
+  @RequiresSession
+  public HostRoleCommandEntity findByPK(long taskId, boolean refresh) {
+    final HostRoleCommandEntity hostRoleCommand = entityManagerProvider.get().find(HostRoleCommandEntity.class, taskId);
+    if (refresh) {
+      entityManagerProvider.get().refresh(hostRoleCommand);
+    }
+    return hostRoleCommand;
+  }
 
   @RequiresSession
   public List<HostRoleCommandEntity> findByPKs(Collection<Long> taskIds) {
+    return findByPKs(taskIds, false);
+  }
+
+  @RequiresSession
+  public List<HostRoleCommandEntity> findByPKs(Collection<Long> taskIds, boolean refresh) {
     if (taskIds == null || taskIds.isEmpty()) {
       return Collections.emptyList();
     }
 
-    TypedQuery<HostRoleCommandEntity> query = entityManagerProvider.get().createQuery(
+    final List<HostRoleCommandEntity> result = new ArrayList<>();
+    final TypedQuery<HostRoleCommandEntity> query = entityManagerProvider.get().createQuery(
       ""SELECT task FROM HostRoleCommandEntity task WHERE task.taskId IN ?1 "" +
         ""ORDER BY task.taskId"",
       HostRoleCommandEntity.class);
 
     if (taskIds.size() > configuration.getTaskIdListLimit()) {
-      List<HostRoleCommandEntity> result = new ArrayList<>();
-
-      List<List<Long>> lists = Lists.partition(new ArrayList<>(taskIds), configuration.getTaskIdListLimit());
+      final List<List<Long>> lists = Lists.partition(new ArrayList<>(taskIds), configuration.getTaskIdListLimit());
       for (List<Long> list : lists) {
         result.addAll(daoUtils.selectList(query, list));
       }
+    } else {
+      result.addAll(daoUtils.selectList(query, taskIds));
+    }
 
-      return result;
+    if (refresh) {
+      for (HostRoleCommandEntity hostRoleCommand : result) {
+        entityManagerProvider.get().refresh(hostRoleCommand);","[{'comment': ""I'm worried about the refresh calls here. In large requests (such as those for upgrades), there could be 10's of 1000's of HRCs. Calling refresh on each one will incur DB hits and potentially cause the web client to timeout on things like Pause/Downgrade.\r\n\r\nI think we need to do some more investigation of this code before it gets checked in. '\r\n\r\nIn general, I think that requiring Java-level locks around JPA means that something is wrong with our JPA design and that's where it should be fixed. `refresh()` is dangerous in that it makes it look like we're fixing a problem, yet we're introducing much larger ones that only customers see."", 'commenter': 'jonathan-hurley'}, {'comment': 'Thanks for your comment @jonathan-hurley \r\nI updated the code to avoid calling refresh within findByPKs; please review', 'commenter': 'smolnar82'}, {'comment': 'Additional note: `ActionDBAccessor.updateHostRoleStates(Collection<CommandReport>) ` - where all this happens - is only invoked on the path of command report processing (from `HeartbeatProcessor`); it does not affect upgrades.', 'commenter': 'smolnar82'}]"
2411,ambari-server/src/main/java/org/apache/ambari/server/actionmanager/ActionDBAccessorImpl.java,"@@ -153,6 +155,8 @@
   private Cache<Long, HostRoleCommand> hostRoleCommandCache;
   private long cacheLimit; //may be exceeded to store tasks from one request
 
+  private final ReadWriteLock hrcOperationsLock = new ReentrantReadWriteLock();","[{'comment': 'It would be great to add some comment here describing the cases when this lock should be used. The variable name does not say too much', 'commenter': 'Unknown'}, {'comment': 'Added', 'commenter': 'smolnar82'}]"
2424,ambari-server/src/main/java/org/apache/ambari/server/scheduler/ExecutionScheduleManager.java,"@@ -427,8 +444,64 @@ protected String getJobName(Long executionId, Long orderId) {
    */
   public void updateBatchSchedule(RequestExecution requestExecution)
     throws AmbariException {
+    BatchRequest lastCompletedBatchRequest = calculateLastCompletedBatch(requestExecution);
+    //TODO check if last completed was last at all.
+
+    if (requestExecution.getStatus().equals(SCHEDULED.name())) {
+      long startingBatchId = lastCompletedBatchRequest == null? 1L : lastCompletedBatchRequest.getOrderId() + 1;
+      scheduleBatch(requestExecution, startingBatchId);
+    } else if (requestExecution.getStatus().equals(PAUSED.name()) ||
+               requestExecution.getStatus().equals(ABORTED.name())) {
+      //TODO delete all or only pending?
+      //TODO delete only trigger?
+      deleteAllJobs(requestExecution);
+      long possibleBatchToAbort = lastCompletedBatchRequest == null? 1L : lastCompletedBatchRequest.getOrderId() + 1;
+      //TODO abort only in_progress?
+      Collection<Long> requestIDsToAbort = requestExecution.getBatchRequestRequestsIDs(possibleBatchToAbort);
+      //TODO move to the separate method
+      for (Long requestId : requestIDsToAbort) {
+        //might be null if the request is for not long running job
+        if (requestId == null) continue;
+
+        StrBuilder sb = new StrBuilder();
+        sb.append(DEFAULT_API_PATH)
+          .append(""/clusters/"")
+          .append(requestExecution.getClusterName())
+          .append(""/requests/"")
+          .append(requestId);
+
+        String body = ""{\""Requests\"":{\""request_status\"":\""ABORTED\"",\""abort_reason\"":\""Request schedule status changed to "" + requestExecution.getStatus() + ""\""}}"";
+        //TODO race condition : request during sending the abort, what should we do?
+        performApiRequest(sb.toString(), body, ""PUT"", requestExecution.getAuthenticatedUserId());
+      }
+    }
+  }
 
-    // TODO: Support delete and update if no jobs are running
+  /**
+   * Iterate through the batches and find the last one with completed status, if none was completed return null
+   * @param requestExecution
+   * @return
+   */
+  private BatchRequest calculateLastCompletedBatch(RequestExecution requestExecution) {
+    BatchRequest result = null;
+    Batch batch = requestExecution.getBatch();
+    if (batch != null) {
+      List<BatchRequest> batchRequests = batch.getBatchRequests();
+      if (batchRequests != null) {
+        Collections.sort(batchRequests);
+
+        ListIterator<BatchRequest> iterator = batchRequests.listIterator();","[{'comment': 'Why iterate? Can we not do this with 1 SQL query?', 'commenter': 'swagle'}, {'comment': ""we'd want to find the next one after the last completed, so we have to iterate, changed the logic in the latest patch a bit"", 'commenter': 'd0zen1'}]"
2424,ambari-server/src/main/java/org/apache/ambari/server/scheduler/ExecutionScheduleManager.java,"@@ -427,8 +444,64 @@ protected String getJobName(Long executionId, Long orderId) {
    */
   public void updateBatchSchedule(RequestExecution requestExecution)
     throws AmbariException {
+    BatchRequest lastCompletedBatchRequest = calculateLastCompletedBatch(requestExecution);
+    //TODO check if last completed was last at all.
+
+    if (requestExecution.getStatus().equals(SCHEDULED.name())) {
+      long startingBatchId = lastCompletedBatchRequest == null? 1L : lastCompletedBatchRequest.getOrderId() + 1;
+      scheduleBatch(requestExecution, startingBatchId);
+    } else if (requestExecution.getStatus().equals(PAUSED.name()) ||
+               requestExecution.getStatus().equals(ABORTED.name())) {
+      //TODO delete all or only pending?
+      //TODO delete only trigger?
+      deleteAllJobs(requestExecution);
+      long possibleBatchToAbort = lastCompletedBatchRequest == null? 1L : lastCompletedBatchRequest.getOrderId() + 1;","[{'comment': 'This is dangerous because the user is allowed to submit orderIds: [2,4,10,15]', 'commenter': 'swagle'}, {'comment': 'already fixed it locally, will commit with latest patch', 'commenter': 'd0zen1'}]"
2424,ambari-server/src/main/java/org/apache/ambari/server/scheduler/ExecutionScheduleManager.java,"@@ -427,8 +444,64 @@ protected String getJobName(Long executionId, Long orderId) {
    */
   public void updateBatchSchedule(RequestExecution requestExecution)
     throws AmbariException {
+    BatchRequest lastCompletedBatchRequest = calculateLastCompletedBatch(requestExecution);
+    //TODO check if last completed was last at all.
+
+    if (requestExecution.getStatus().equals(SCHEDULED.name())) {
+      long startingBatchId = lastCompletedBatchRequest == null? 1L : lastCompletedBatchRequest.getOrderId() + 1;
+      scheduleBatch(requestExecution, startingBatchId);
+    } else if (requestExecution.getStatus().equals(PAUSED.name()) ||
+               requestExecution.getStatus().equals(ABORTED.name())) {
+      //TODO delete all or only pending?
+      //TODO delete only trigger?
+      deleteAllJobs(requestExecution);
+      long possibleBatchToAbort = lastCompletedBatchRequest == null? 1L : lastCompletedBatchRequest.getOrderId() + 1;
+      //TODO abort only in_progress?
+      Collection<Long> requestIDsToAbort = requestExecution.getBatchRequestRequestsIDs(possibleBatchToAbort);
+      //TODO move to the separate method
+      for (Long requestId : requestIDsToAbort) {
+        //might be null if the request is for not long running job
+        if (requestId == null) continue;
+
+        StrBuilder sb = new StrBuilder();
+        sb.append(DEFAULT_API_PATH)
+          .append(""/clusters/"")
+          .append(requestExecution.getClusterName())
+          .append(""/requests/"")
+          .append(requestId);
+
+        String body = ""{\""Requests\"":{\""request_status\"":\""ABORTED\"",\""abort_reason\"":\""Request schedule status changed to "" + requestExecution.getStatus() + ""\""}}"";
+        //TODO race condition : request during sending the abort, what should we do?
+        performApiRequest(sb.toString(), body, ""PUT"", requestExecution.getAuthenticatedUserId());","[{'comment': 'This is flaky, we do not need to use API endpoints to cancel request, just get the ResourceProvider for Requests and use PropertyHelpers to achieve the same. It makes sense for unit testing as well. Also please add unit tests around this. ', 'commenter': 'swagle'}]"
2424,ambari-server/src/main/java/org/apache/ambari/server/scheduler/AbstractLinearExecutionJob.java,"@@ -125,6 +129,19 @@ public void execute(JobExecutionContext context) throws JobExecutionException {
       return;
     }
 
+    String status = null;
+    try {
+      status = executionScheduleManager.getBatchRequestStatus(jobDataMap.getLong(BATCH_REQUEST_EXECUTION_ID_KEY), jobDataMap.getString(BATCH_REQUEST_CLUSTER_NAME_KEY));
+    } catch (AmbariException e) {
+      LOG.warn(""Unable to define the status of batch request : "", e);
+    }
+
+    if(ABORTED.name().equals(status) || PAUSED.name().equals(status)) {
+      LOG.debug(""The linear job chain was paused or aborted, not triggering the next one"");","[{'comment': 'Lets make this info, not too many of these will populate the log, right?', 'commenter': 'swagle'}, {'comment': 'agree', 'commenter': 'd0zen1'}]"
2424,ambari-server/src/main/java/org/apache/ambari/server/scheduler/ExecutionScheduleManager.java,"@@ -275,13 +290,42 @@ public boolean continueOnMisfire(JobExecutionContext jobExecutionContext) {
     return true;
   }
 
+
+  private long getFirstJobOrderId(RequestExecution requestExecution) throws AmbariException {
+    Long firstBatchOrderId = null;
+    Batch batch = requestExecution.getBatch();
+    if (batch != null) {
+      List<BatchRequest> batchRequests = batch.getBatchRequests();
+      if (batchRequests != null) {
+        Collections.sort(batchRequests);
+        ListIterator<BatchRequest> iterator = batchRequests.listIterator();
+        firstBatchOrderId = iterator.next().getOrderId();
+      }
+    }
+    if (firstBatchOrderId == null) {
+      throw new AmbariException(""Can't schedule RequestExecution with no batches"");","[{'comment': 'Can you add comment how this could ever happen?', 'commenter': 'swagle'}, {'comment': ""If the user creates a request schedule with no batches, this shouldn't ever happen under the normal circumstances, the check is just to make sure we deal with the situation"", 'commenter': 'd0zen1'}]"
2424,ambari-server/src/main/java/org/apache/ambari/server/scheduler/ExecutionScheduleManager.java,"@@ -364,7 +413,31 @@ public void scheduleBatch(RequestExecution requestExecution)
     }
   }
 
-  private JobDetail persistBatch(RequestExecution requestExecution)
+  private Integer countFailedTasksBeforeStartingBatch(RequestExecution requestExecution, long startingBatchOrderId) throws AmbariException {
+    int result = 0;
+    Batch batch = requestExecution.getBatch();
+    if (batch != null) {
+      List<BatchRequest> batchRequests = batch.getBatchRequests();
+      if (batchRequests != null) {
+        Collections.sort(batchRequests);
+        for (BatchRequest batchRequest : batchRequests) {
+          if (batchRequest.getOrderId() >= startingBatchOrderId) break;
+
+          if (batchRequest.getRequestId() != null) {
+            BatchRequestResponse batchRequestResponse = getBatchRequestResponse(batchRequest.getRequestId(), requestExecution.getClusterName());
+            if (batchRequestResponse != null) {
+               result += batchRequestResponse.getFailedTaskCount() +
+                         batchRequestResponse.getAbortedTaskCount() +
+                         batchRequestResponse.getTimedOutTaskCount();
+            }
+          }
+        }
+      }
+    }
+    return result;
+  }
+
+  private JobDetail persistBatch(RequestExecution requestExecution, long startingBatchOrderId)","[{'comment': 'Comments to help understand current logic', 'commenter': 'swagle'}]"
2424,ambari-server/src/main/java/org/apache/ambari/server/scheduler/ExecutionScheduleManager.java,"@@ -421,14 +496,60 @@ protected String getJobName(Long executionId, Long orderId) {
   }
 
   /**
-   * Delete and re-create all jobs and triggers
-   * Update schedule for a batch
+   * Pause/resume/abort request schedule and related jobs and triggers
    * @param requestExecution
    */
   public void updateBatchSchedule(RequestExecution requestExecution)
     throws AmbariException {
+    BatchRequest activeBatch = calculateActiveBatch(requestExecution);
+    if (activeBatch == null) {
+      LOG.warn(""Ignoring RequestExecution status update since all batches has been executed"");
+      return;
+    }
+    if (requestExecution.getStatus().equals(SCHEDULED.name())) {
+      scheduleBatch(requestExecution, activeBatch.getOrderId());
+    } else if (requestExecution.getStatus().equals(PAUSED.name()) ||
+               requestExecution.getStatus().equals(ABORTED.name())) {
+      LOG.info(""Request execution status changed to "" + requestExecution.getStatus() + "" for request schedule ""
+        + requestExecution.getId() + "". Deleting related jobs."");
+      deleteJobs(requestExecution, activeBatch.getOrderId());
+      Collection<Long> requestIDsToAbort = requestExecution.getBatchRequestRequestsIDs(activeBatch.getOrderId());
+      for (Long requestId : requestIDsToAbort) {
+        //might be null if the request is for not long running job
+        if (requestId == null) continue;
+        abortRequestById(requestExecution, requestId);
+      }
+    }
+  }
 
-    // TODO: Support delete and update if no jobs are running
+  /**
+   * Iterate through the batches and find the first one with not completed status, if all were completed return null
+   * @param requestExecution
+   * @return
+   */
+  private BatchRequest calculateActiveBatch(RequestExecution requestExecution) {
+    BatchRequest result = null;
+    Batch batch = requestExecution.getBatch();
+    if (batch != null) {
+      List<BatchRequest> batchRequests = batch.getBatchRequests();
+      if (batchRequests != null) {
+        Collections.sort(batchRequests);
+        ListIterator<BatchRequest> iterator = batchRequests.listIterator();
+        do {
+          result = iterator.next();
+        } while (iterator.hasNext() &&
+                 HostRoleStatus.getCompletedStates().contains(HostRoleStatus.valueOf(result.getStatus())) &&
+                 !HostRoleStatus.ABORTED.name().equals(result.getStatus()));","[{'comment': 'No check needed for PAUSE state?', 'commenter': 'swagle'}, {'comment': ""This is the HostRoleStatus of request that doesn't have the PAUSED state. We need to track all completed states ignoring ABORTED."", 'commenter': 'd0zen1'}]"
2424,ambari-server/src/main/java/org/apache/ambari/server/scheduler/ExecutionScheduleManager.java,"@@ -566,6 +702,33 @@ public BatchRequestResponse getBatchRequestResponse(Long requestId, String clust
 
   }
 
+  protected RequestStatus abortRequestById(RequestExecution requestExecution, Long requestId) throws AmbariException {
+    LOG.debug(""Aborting request "" + requestId);
+    ResourceProvider provider =
+        ambariContext.getClusterController().ensureResourceProvider(Resource.Type.Request);","[{'comment': 'AmbariContext is blueprint specific. Use AbstractControllerResourceProvider.getResourceProvider instead.', 'commenter': 'swagle'}]"
2424,ambari-server/src/main/java/org/apache/ambari/server/state/scheduler/RequestExecutionImpl.java,"@@ -452,6 +483,14 @@ public void updateBatchRequest(long batchId,
       }
     }
 
+    // Rare race condition when batch request finished during pausing the request execution,
+    //in this case the job details will be deleted,
+    //so we mark it as not completed because otherwise the job detail will be lost
+    //and the whole Request Execution status will not be set to COMPLETED at the end.
+    if (Status.PAUSED.name().equals(getStatus()) && HostRoleStatus.COMPLETED.name().equals(batchRequestResponse.getStatus())) {","[{'comment': 'Good catch.', 'commenter': 'swagle'}]"
2427,ambari-server/src/main/python/ambari_server/serverSetup.py,"@@ -1272,6 +1272,17 @@ def setup_jce_policy(args):
   print 'NOTE: Restart Ambari Server to apply changes' + \
         ' (""ambari-server restart|stop|start"")'
 
+def get_java_major_version(cmd_out):
+  version_short = re.split(""[java|openjdk|.*] version"", cmd_out)[1].split("" "")[1][1:-1]
+  if re.match(""1\.8.*"", version_short): # 1.8.0_112","[{'comment': ""While Ambari does not support Java 1.7 and earlier, the function should work for those, too.  Otherwise setup will fail with exception (and so does unit test).\r\n\r\n```\r\nFatal exception: Running java version check command failed: list index out of range. Exiting., exit code 1'\r\n```"", 'commenter': 'adoroszlai'}]"
2427,ambari-server/src/main/python/ambari_server/serverSetup.py,"@@ -80,7 +80,7 @@
 JDK_PROMPT = ""[{0}] {1}\n""
 JDK_VALID_CHOICES = ""^[{0}{1:d}]$""
 
-JDK_VERSION_CHECK_CMD = """"""{0} -version 2>&1 | grep -i version | sed 's/.*version "".*\.\(.*\)\..*""/\\1/; 1q' 2>&1""""""
+JDK_VERSION_CHECK_CMD = """"""{0} -version 2>&1 | grep -i version 2>&1""""""","[{'comment': 'Looks like a corresponding change is needed in the unit test.  These mock return values should include the full line of output with version instead of only the number:\r\n\r\nhttps://github.com/apache/ambari/blob/44436e1d5e0d28e07285c7b506cf596748ce85fc/ambari-server/src/test/python/TestAmbariServer.py#L3153\r\n\r\nhttps://github.com/apache/ambari/blob/44436e1d5e0d28e07285c7b506cf596748ce85fc/ambari-server/src/test/python/TestAmbariServer.py#L3162\r\n\r\neg.\r\n\r\n```\r\n    p.communicate.return_value = (\'java version ""1.7.0_191""\', None)\r\n```\r\n\r\nand\r\n\r\n```\r\n    p.communicate.return_value = (\'openjdk version ""1.8.0_161""\', None)\r\n```', 'commenter': 'adoroszlai'}]"
2427,ambari-server/src/main/python/ambari_server/serverSetup.py,"@@ -1292,7 +1303,7 @@ def check_ambari_java_version_is_valid(java_home, java_bin, min_version, propert
       err = ""Checking JDK version command returned with exit code %s"" % process.returncode
       raise FatalException(process.returncode, err)
     else:
-      actual_jdk_version = int(out)
+      actual_jdk_version = int(get_java_major_version(out))","[{'comment': ""I think `STACK_JAVA_VERSION` should be set to `actual_jdk_version` instead of `out`, since `out` now has more content than just the major version number:\r\n\r\nhttps://github.com/apache/ambari/blob/44436e1d5e0d28e07285c7b506cf596748ce85fc/ambari-server/src/main/python/ambari_server/serverSetup.py#L1299\r\n\r\n(Sorry I hadn't noticed this earlier.)"", 'commenter': 'adoroszlai'}]"
2427,ambari-server/src/main/python/ambari_server/serverSetup.py,"@@ -1307,7 +1307,7 @@ def check_ambari_java_version_is_valid(java_home, java_bin, min_version, propert
       print 'JDK version found: {0}'.format(actual_jdk_version)
       if actual_jdk_version < min_version:
         print 'Minimum JDK version is {0} for Ambari. Setup JDK again only for Ambari Server.'.format(min_version)
-        properties.process_pair(STACK_JAVA_VERSION, out)
+        properties.process_pair(STACK_JAVA_VERSION, get_java_major_version(out))","[{'comment': 'I would prefer storing the value returned by `get_java_major_version` instead of calling it twice.', 'commenter': 'adoroszlai'}]"
2428,ambari-common/src/main/python/resource_management/libraries/execution_command/cluster_settings.py,"@@ -0,0 +1,150 @@
+#!/usr/bin/env python
+""""""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an ""AS IS"" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+""""""
+
+__all__ = [""ClusterSettings""]
+
+class ClusterSettings(object):
+    """"""
+    This class maps to ""configurations->cluster-env"" in command.json which includes cluster setting information of a cluster
+    """"""
+
+    def __init__(self, clusterSettings):
+        self.__cluster_settings = clusterSettings
+
+    def __get_value(self, key):
+        """"""
+        Get corresponding value from the key
+        :param key:
+        :return: value if key exist else None
+        """"""
+        return self.__cluster_settings.get(key)
+
+    def is_cluster_security_enabled(self):
+        """"""
+        Check cluster security enabled or not
+        :return: True or False
+        """"""
+        security_enabled = self.__get_value(""security_enabled"")
+        return True if security_enabled and security_enabled.lower() == ""true"" else False","[{'comment': 'These kinds of logical expressions should be simplified:\r\n\r\n```\r\nreturn True if security_enabled and security_enabled.lower() == ""true"" else False\r\n```\r\n\r\nto:\r\n\r\n```\r\nreturn security_enabled and security_enabled.lower() == ""true""\r\n```\r\n', 'commenter': 'adoroszlai'}, {'comment': 'Done.', 'commenter': 'swapanshridhar'}, {'comment': 'I meant all such `return True if ... else False` statements, not only this one.', 'commenter': 'adoroszlai'}]"
2428,ambari-common/src/main/python/resource_management/libraries/execution_command/cluster_settings.py,"@@ -0,0 +1,150 @@
+#!/usr/bin/env python
+""""""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an ""AS IS"" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+""""""
+
+__all__ = [""ClusterSettings""]
+
+class ClusterSettings(object):
+    """"""
+    This class maps to ""configurations->cluster-env"" in command.json which includes cluster setting information of a cluster
+    """"""
+
+    def __init__(self, clusterSettings):
+        self.__cluster_settings = clusterSettings
+
+    def __get_value(self, key):
+        """"""
+        Get corresponding value from the key
+        :param key:
+        :return: value if key exist else None
+        """"""
+        return self.__cluster_settings.get(key)
+
+    def is_cluster_security_enabled(self):
+        """"""
+        Check cluster security enabled or not
+        :return: True or False
+        """"""
+        security_enabled = self.__get_value(""security_enabled"")
+        return True if security_enabled and security_enabled.lower() == ""true"" else False
+
+    def get_recovery_max_count(self):
+        """"""
+        Retrieve cluster recovery count
+        :return: String, need to convert to int
+        """"""
+        return int(self.__get_value(""recovery_max_count""))
+
+    def check_recovery_enabled(self):
+        """"""
+        Check if the cluster can be enabled or not
+        :return: True or False
+        """"""
+        recovery_enabled =  self.__get_value(""recovery_enabled"")
+        return True if recovery_enabled and recovery_enabled.lower() == ""true"" else False
+
+    def get_recovery_type(self):
+        """"""
+        Retrieve cluster recovery type
+        :return: recovery type, i.e ""AUTO_START""
+        """"""
+        return self.__get_value(""recovery_type"")
+
+    def get_kerberos_domain(self):
+        """"""
+        Retrieve kerberos domain
+        :return: String as kerberos domain
+        """"""
+        return self.__get_value(""kerberos_domain"")
+
+    def get_smokeuser(self):
+        """"""
+        Retrieve smokeuser
+        :return: smkeuser string
+        """"""
+        return self.__get_value(""smokeuser"")
+
+    def get_user_group(self):
+        """"""
+        Retrieve cluster usergroup
+        :return: usergroup string
+        """"""
+        return self.__get_value(""user_group"")
+
+    def get_repo_suse_rhel_template(self):
+        """"""
+        Retrieve template of suse and rhel repo
+        :return: template string
+        """"""
+        return self.__get_value(""repo_suse_rhel_template"")
+
+    def get_repo_ubuntu_template(self):
+        """"""
+        Retrieve template of ubuntu repo
+        :return: template string
+        """"""
+        return self.__get_value(""repo_ubuntu_template"")
+
+    def check_override_uid(self):
+        """"""
+        Check if override_uid is true or false
+        :return: True or False
+        """"""
+        override_uid =  self.__get_value(""override_uid"")
+        return True if override_uid and override_uid.lower() == ""true"" else False
+
+    def check_sysprep_skip_copy_fast_jar_hdfs(self):","[{'comment': 'Should there be a function for the new `sysprep_skip_lzo_package_operations` flag, too?', 'commenter': 'adoroszlai'}, {'comment': 'Added.', 'commenter': 'swapanshridhar'}, {'comment': 'Thanks.', 'commenter': 'adoroszlai'}]"
2429,ambari-server/src/test/java/org/apache/ambari/server/controller/internal/PreUpgradeCheckResourceProviderTest.java,"@@ -34,6 +34,7 @@
 
 import javax.persistence.EntityManager;
 
+import org.apache.ambari.metrics.sink.relocated.curator.shaded.com.google.common.collect.Lists;","[{'comment': 'Wuuuuut :)', 'commenter': 'ncole'}, {'comment': '```\r\n[ERROR] ... Illegal import - org.apache.ambari.metrics.sink.relocated.curator.shaded.com.google.common.collect.Lists. [IllegalImport]\r\n```', 'commenter': 'adoroszlai'}]"
2429,ambari-server-spi/src/main/java/org/apache/ambari/spi/ClusterInformation.java,"@@ -0,0 +1,140 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.spi;
+
+import java.util.Map;
+import java.util.Set;
+
+import com.google.common.collect.Maps;
+import com.google.common.collect.Sets;
+
+/**
+ * The {@link ClusterInformation} class is used to pass the state of the cluster
+ * as simple primitive values and collections. It contains the following types of information:
+ * <ul>
+ * <li>The name of a cluster
+ * <li>The current desired configurations of a cluster
+ * <li>The hosts where services and components are installed
+ * <li>The security state of the cluster
+ * </ul>
+ */
+public class ClusterInformation {
+
+  /**
+   * The cluster's current configurations.
+   */
+  private final Map<String, Map<String, String>> m_configurations;
+
+  /**
+   * The name of the cluster.
+   */
+  private final String m_clusterName;
+
+  /**
+   * {@code true} if the cluster is kerberized.
+   */
+  private final boolean m_isKerberized;","[{'comment': ""I prefer 'isKerberosEnabled' "", 'commenter': 'rlevas'}, {'comment': 'Will do.', 'commenter': 'jonathan-hurley'}]"
2429,ambari-server-spi/src/main/java/org/apache/ambari/spi/ClusterInformation.java,"@@ -0,0 +1,140 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.spi;
+
+import java.util.Map;
+import java.util.Set;
+
+import com.google.common.collect.Maps;
+import com.google.common.collect.Sets;
+
+/**
+ * The {@link ClusterInformation} class is used to pass the state of the cluster
+ * as simple primitive values and collections. It contains the following types of information:
+ * <ul>
+ * <li>The name of a cluster
+ * <li>The current desired configurations of a cluster
+ * <li>The hosts where services and components are installed
+ * <li>The security state of the cluster
+ * </ul>
+ */
+public class ClusterInformation {
+
+  /**
+   * The cluster's current configurations.
+   */
+  private final Map<String, Map<String, String>> m_configurations;
+
+  /**
+   * The name of the cluster.
+   */
+  private final String m_clusterName;
+
+  /**
+   * {@code true} if the cluster is kerberized.
+   */
+  private final boolean m_isKerberized;
+
+  /**
+   * A simple representation of the cluster topology where the key is the
+   * combination of service/component and the value is the set of hosts.
+   */
+  private final Map<String, Set<String>> m_topology;
+
+  /**
+   * Constructor.
+   *
+   * @param clusterName
+   *          the name of the cluster.
+   * @param isKerberized
+   *          {@code true} if the cluster is Kerberized.
+   * @param configurations
+   *          a mapping of configuration type (such as foo-site) to the specific
+   *          configurations (such as http.port : 8080).
+   * @param topology
+   *          a mapping of the cluster topology where the key is a combination
+   *          of service / component and the value is the hosts where it is
+   *          installed.
+   */
+  public ClusterInformation(String clusterName, boolean isKerberized,
+      Map<String, Map<String, String>> configurations, Map<String, Set<String>> topology) {
+    m_configurations = configurations;
+    m_clusterName = clusterName;
+    m_isKerberized = isKerberized;
+    m_topology = topology;
+  }
+
+  /**
+   * Gets the cluster name.
+   *
+   * @return the cluster name.
+   */
+  public String getClusterName() {
+    return m_clusterName;
+  }
+
+  /**
+   * Gets whether the cluster is Kerberized.
+   *
+   * @return {@code true} if the cluster is Kerberized.
+   */
+  public boolean isKerberized() {
+    return m_isKerberized;
+  }
+
+  /**
+   * Gets any hosts where the matching service and component are installed.
+   *
+   * @param serviceName
+   *          the service name
+   * @param componentName
+   *          the component name
+   * @return the set of hosts where the component is installed, or an empty set.
+   */
+  public Set<String> getHosts(String serviceName, String componentName) {
+    Set<String> hosts = m_topology.get(serviceName + ""/"" + componentName);
+    if (null == hosts) {
+      hosts = Sets.newHashSet();","[{'comment': ""Wouldn't `new HashSet<>()` or even `Collections.emptySet()` be better here since there is not need to have Google collections as a dependency?"", 'commenter': 'rlevas'}, {'comment': ""Yeah, it's an interesting discussion. I think that since Guava is being phased out by newer JDKs, we can rely less on it."", 'commenter': 'jonathan-hurley'}]"
2429,ambari-server/src/main/resources/stacks/HDP/2.6/upgrades/config-upgrade.xml,"@@ -0,0 +1,778 @@
+<?xml version=""1.0""?>","[{'comment': 'Did you mean to add this file (and others) here?  I thought these were in a different repo now.', 'commenter': 'rlevas'}, {'comment': 'EWWWWW! Nice catch. Let me fix it.', 'commenter': 'jonathan-hurley'}]"
2429,ambari-server/src/main/resources/stacks/HDP/3.0,"@@ -0,0 +1 @@
+/Users/jhurley/src/hwx/hdp_ambari_definitions/src/main/resources/stacks/HDP/3.0","[{'comment': 'This seems accidental... yes? ', 'commenter': 'rlevas'}, {'comment': 'Totally ... part of testing involved getting the old packs on my box.', 'commenter': 'jonathan-hurley'}]"
2430,ambari-server/src/main/java/org/apache/ambari/server/security/encryption/EncryptionUtils.java,"@@ -0,0 +1,126 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p/>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p/>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.security.encryption;
+
+import java.io.UnsupportedEncodingException;
+import java.nio.charset.StandardCharsets;
+
+import org.apache.commons.codec.binary.Base64;
+
+import com.google.inject.Singleton;
+
+@Singleton
+public class EncryptionUtils {
+
+  private static final String BASE_64_FIELD_DELIMITER = ""::"";
+  private static final String UTF_8_CHARSET = StandardCharsets.UTF_8.name();
+
+  private MasterKeyService environmentMasterKeyService;
+
+  /**
+   * Encrypts the given text using the master key found in the environment
+   * 
+   * @param toBeEncrypted
+   *          the text to be encrypted
+   * @return the String representation of the encrypted text
+   * @throws Exception
+   *           in case any error happened during the encryption process
+   */
+  public String encrypt(String toBeEncrypted) throws Exception {
+    return encrypt(toBeEncrypted, (String) null);
+  }
+
+  /**
+   * Encrypts the given text using the given master key
+   * 
+   * @param toBeEncrypted
+   *          the text to be encrypted
+   * @param masterKey
+   *          the mater key to be used for encryption
+   * @return the String representation of the encrypted text
+   * @throws Exception
+   *           in case any error happened during the encryption process
+   */
+  public String encrypt(String toBeEncrypted, String masterKey) throws Exception {
+    return encrypt(toBeEncrypted, getMasterKeyService(masterKey));
+  }
+
+  private MasterKeyService getMasterKeyService(String masterKey) {
+    if (masterKey == null) {
+      initEnvironmentMasterKeyService();
+      if (!environmentMasterKeyService.isMasterKeyInitialized()) {
+        throw new SecurityException(""You are trying to use a persisted master key but its initialization has been failed!"");
+      }
+      return environmentMasterKeyService;
+    }
+    return new MasterKeyServiceImpl(masterKey);
+  }
+
+  private void initEnvironmentMasterKeyService() {
+    if (environmentMasterKeyService == null) {
+      environmentMasterKeyService = new MasterKeyServiceImpl();","[{'comment': 'should not we inject the instance that is used throughout application?', 'commenter': 'Unknown'}, {'comment': ""It's not (yet) used by Guice."", 'commenter': 'smolnar82'}]"
2430,ambari-server/src/main/java/org/apache/ambari/server/security/encryption/EncryptionUtils.java,"@@ -0,0 +1,126 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p/>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p/>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.security.encryption;
+
+import java.io.UnsupportedEncodingException;
+import java.nio.charset.StandardCharsets;
+
+import org.apache.commons.codec.binary.Base64;
+
+import com.google.inject.Singleton;
+
+@Singleton
+public class EncryptionUtils {
+
+  private static final String BASE_64_FIELD_DELIMITER = ""::"";
+  private static final String UTF_8_CHARSET = StandardCharsets.UTF_8.name();
+
+  private MasterKeyService environmentMasterKeyService;
+
+  /**
+   * Encrypts the given text using the master key found in the environment
+   * 
+   * @param toBeEncrypted
+   *          the text to be encrypted
+   * @return the String representation of the encrypted text
+   * @throws Exception
+   *           in case any error happened during the encryption process
+   */
+  public String encrypt(String toBeEncrypted) throws Exception {
+    return encrypt(toBeEncrypted, (String) null);
+  }
+
+  /**
+   * Encrypts the given text using the given master key
+   * 
+   * @param toBeEncrypted
+   *          the text to be encrypted
+   * @param masterKey
+   *          the mater key to be used for encryption","[{'comment': 'mater -> master', 'commenter': 'rlevas'}]"
2430,ambari-server/src/main/java/org/apache/ambari/server/security/encryption/EncryptionUtils.java,"@@ -0,0 +1,126 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p/>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p/>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.security.encryption;
+
+import java.io.UnsupportedEncodingException;
+import java.nio.charset.StandardCharsets;
+
+import org.apache.commons.codec.binary.Base64;
+
+import com.google.inject.Singleton;
+
+@Singleton
+public class EncryptionUtils {
+
+  private static final String BASE_64_FIELD_DELIMITER = ""::"";
+  private static final String UTF_8_CHARSET = StandardCharsets.UTF_8.name();
+
+  private MasterKeyService environmentMasterKeyService;
+
+  /**
+   * Encrypts the given text using the master key found in the environment
+   * 
+   * @param toBeEncrypted
+   *          the text to be encrypted
+   * @return the String representation of the encrypted text
+   * @throws Exception
+   *           in case any error happened during the encryption process
+   */
+  public String encrypt(String toBeEncrypted) throws Exception {
+    return encrypt(toBeEncrypted, (String) null);
+  }
+
+  /**
+   * Encrypts the given text using the given master key
+   * 
+   * @param toBeEncrypted
+   *          the text to be encrypted
+   * @param masterKey
+   *          the mater key to be used for encryption
+   * @return the String representation of the encrypted text
+   * @throws Exception
+   *           in case any error happened during the encryption process
+   */
+  public String encrypt(String toBeEncrypted, String masterKey) throws Exception {","[{'comment': 'The caller should pass in the _key_ to use... not the ""master key"" which is the key known to the Ambari server.  Maybe rename this to ""key"".  It is possible that some other key will be used to encrypt the `toBeEncrypted` string.  We should not tie this utility class directly to Ambari\'s master key. ', 'commenter': 'rlevas'}, {'comment': 'ok', 'commenter': 'smolnar82'}]"
2430,ambari-server/src/main/java/org/apache/ambari/server/security/encryption/EncryptionUtils.java,"@@ -0,0 +1,126 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p/>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p/>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.security.encryption;
+
+import java.io.UnsupportedEncodingException;
+import java.nio.charset.StandardCharsets;
+
+import org.apache.commons.codec.binary.Base64;
+
+import com.google.inject.Singleton;
+
+@Singleton
+public class EncryptionUtils {","[{'comment': 'missing JavaDoc\r\n\r\nAlso, how about splitting this into EncryptionUtils (interface) and EncryptionUtilsImpl to avoid potential circular dependencies. ', 'commenter': 'rlevas'}, {'comment': ""fine by me; in this case I'd rename it to `EncryptionService[Impl`]"", 'commenter': 'smolnar82'}]"
2430,ambari-server/src/main/java/org/apache/ambari/server/security/encryption/EncryptionUtils.java,"@@ -0,0 +1,126 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p/>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p/>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.security.encryption;
+
+import java.io.UnsupportedEncodingException;
+import java.nio.charset.StandardCharsets;
+
+import org.apache.commons.codec.binary.Base64;
+
+import com.google.inject.Singleton;
+
+@Singleton
+public class EncryptionUtils {
+
+  private static final String BASE_64_FIELD_DELIMITER = ""::"";
+  private static final String UTF_8_CHARSET = StandardCharsets.UTF_8.name();
+
+  private MasterKeyService environmentMasterKeyService;","[{'comment': ""Why do we need to cache this?  It we don't then the class is stateless."", 'commenter': 'rlevas'}, {'comment': ""To avoid reading the master key file every time we want to encrypt/decrypt without custom key (i.e. using Ambari's master key in the environment)"", 'commenter': 'smolnar82'}]"
2430,ambari-server/src/main/java/org/apache/ambari/server/security/encryption/EncryptionUtils.java,"@@ -0,0 +1,126 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p/>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p/>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.security.encryption;
+
+import java.io.UnsupportedEncodingException;
+import java.nio.charset.StandardCharsets;
+
+import org.apache.commons.codec.binary.Base64;
+
+import com.google.inject.Singleton;
+
+@Singleton
+public class EncryptionUtils {
+
+  private static final String BASE_64_FIELD_DELIMITER = ""::"";
+  private static final String UTF_8_CHARSET = StandardCharsets.UTF_8.name();
+
+  private MasterKeyService environmentMasterKeyService;
+
+  /**
+   * Encrypts the given text using the master key found in the environment
+   * 
+   * @param toBeEncrypted
+   *          the text to be encrypted
+   * @return the String representation of the encrypted text
+   * @throws Exception
+   *           in case any error happened during the encryption process
+   */
+  public String encrypt(String toBeEncrypted) throws Exception {
+    return encrypt(toBeEncrypted, (String) null);
+  }
+
+  /**
+   * Encrypts the given text using the given master key
+   * 
+   * @param toBeEncrypted
+   *          the text to be encrypted
+   * @param masterKey
+   *          the mater key to be used for encryption
+   * @return the String representation of the encrypted text
+   * @throws Exception
+   *           in case any error happened during the encryption process
+   */
+  public String encrypt(String toBeEncrypted, String masterKey) throws Exception {
+    return encrypt(toBeEncrypted, getMasterKeyService(masterKey));
+  }
+
+  private MasterKeyService getMasterKeyService(String masterKey) {
+    if (masterKey == null) {
+      initEnvironmentMasterKeyService();
+      if (!environmentMasterKeyService.isMasterKeyInitialized()) {
+        throw new SecurityException(""You are trying to use a persisted master key but its initialization has been failed!"");
+      }
+      return environmentMasterKeyService;
+    }
+    return new MasterKeyServiceImpl(masterKey);
+  }
+
+  private void initEnvironmentMasterKeyService() {
+    if (environmentMasterKeyService == null) {
+      environmentMasterKeyService = new MasterKeyServiceImpl();
+    }
+  }
+
+  String encrypt(String toBeEncrypted, MasterKeyService masterKeyService) throws Exception {
+    final AESEncryptor aes = new AESEncryptor(new String(masterKeyService.getMasterSecret()));
+    final EncryptionResult encryptionResult = aes.encrypt(toBeEncrypted);
+    return encodeEncryptionResult(encryptionResult);
+  }
+
+  private String encodeEncryptionResult(EncryptionResult encryptionResult) throws UnsupportedEncodingException {","[{'comment': 'To be more configurable, we should allow the caller to specify the encoding... Base64 or BinHex.  ', 'commenter': 'rlevas'}, {'comment': 'ok', 'commenter': 'smolnar82'}]"
2430,ambari-server/src/main/java/org/apache/ambari/server/security/encryption/MasterKeyServiceImpl.java,"@@ -42,7 +41,7 @@
   private static final Logger LOG = LoggerFactory.getLogger(MasterKeyServiceImpl.class);
   private static final String MASTER_PASSPHRASE = ""masterpassphrase"";
   private static final String MASTER_PERSISTENCE_TAG_PREFIX = ""#1.0# "";
-  private static final AESEncryptor aes = new AESEncryptor(MASTER_PASSPHRASE);
+  private final EncryptionUtils encryptionUtils = new EncryptionUtils();","[{'comment': 'We do not need to do this if EncryptionUtils is stateless. \r\n', 'commenter': 'rlevas'}]"
2430,ambari-server/src/main/java/org/apache/ambari/server/security/encryption/MasterKeyServiceImpl.java,"@@ -302,11 +275,7 @@ private void initializeFromFile(File masterFile) throws Exception {
       List<String> lines = FileUtils.readLines(masterFile, ""UTF8"");
       String tag = lines.get(0);
       LOG.info(""Loading from persistent master: "" + tag);
-      String line = new String(Base64.decodeBase64(lines.get(1)));","[{'comment': 'do we have a string that is encoded  twice into Base64?', 'commenter': 'Unknown'}, {'comment': ""yes; this is how it's working for a long time...I kept it as is to support backward compatibility"", 'commenter': 'smolnar82'}]"
2430,ambari-server/src/main/java/org/apache/ambari/server/security/encryption/EncryptionServiceImpl.java,"@@ -0,0 +1,112 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p/>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p/>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.security.encryption;
+
+import java.io.UnsupportedEncodingException;
+import java.nio.charset.StandardCharsets;
+
+import org.apache.ambari.server.utils.TextEncoding;
+import org.apache.commons.codec.binary.Base64;
+import org.apache.commons.codec.binary.Hex;
+
+import com.google.inject.Singleton;
+
+@Singleton
+public class EncryptionServiceImpl implements EncryptionService {","[{'comment': ""How about naming it AESEncryptionService instead of EncryptionServiceImpl? I'm not a big fan of the *Impl convention which assumes only one implementation. I would rather name it after some unique property of the specific implementation. As I see this implementation always uses AES so why not put this into the name."", 'commenter': 'zeroflag'}, {'comment': 'Agreed; uploaded a new patch.', 'commenter': 'smolnar82'}]"
2444,ambari-server/src/main/resources/stacks/HDP/2.3/services/stack_advisor.py,"@@ -26,6 +26,11 @@
 
 # Local Imports
 
+try:
+  from stack_advisor_hdp22 import *","[{'comment': 'Explicitly import HDP2.2 stack advisor from stacks/HDP/2.6/stack-advisors/stack_advisor_hdp22.py', 'commenter': 'jayush'}]"
2444,ambari-server/src/main/java/org/apache/ambari/server/state/ServiceInfo.java,"@@ -469,6 +471,56 @@ public void setProperties(List<PropertyInfo> properties) {
     this.properties = properties;
   }
 
+  @Override
+  public Object clone() throws CloneNotSupportedException {
+    ServiceInfo clone = (ServiceInfo) super.clone();","[{'comment': ""I don't see where you're cloning and need this method."", 'commenter': 'ncole'}, {'comment': '@ncole I clone the service info in StackMerger tool to create the flattened ServiceInfo.  ', 'commenter': 'jayush'}]"
2444,ambari-server/src/main/java/org/apache/ambari/server/stack/StackManager.java,"@@ -380,6 +403,9 @@ public ExtensionInfo getExtension(String name, String version) {
    * @return true if all of the repo update tasks have completed; false otherwise
    */
   public boolean haveAllRepoUrlsBeenResolved() {
+    if(stackContext == null) {","[{'comment': ""Should never be null.  How is that happening?  Even if there's no directory for a stack (how?) the context should be available."", 'commenter': 'ncole'}]"
2444,ambari-server/src/main/resources/common-services/AMBARI_INFRA_SOLR/0.1.0/service_advisor.py,"@@ -25,13 +25,19 @@
 SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
 STACKS_DIR = os.path.join(SCRIPT_DIR, '../../../stacks/')
 PARENT_FILE = os.path.join(STACKS_DIR, 'service_advisor.py')
+ABSOLUTE_STACKS_DIR = os.path.join(SCRIPT_DIR, '/var/lib/ambari-server/resources/stacks/')","[{'comment': 'This feels dangerous.  The resources dir can be set in `ambari.properties` and can\'t be relied on like this.  You can resolve the ""absolute"" directory using python magic `os.path.abspath` with `STACKS_DIR`', 'commenter': 'ncole'}, {'comment': 'Yes, this will break anyone who uses this property (like I do).', 'commenter': 'jonathan-hurley'}, {'comment': 'Fixed it.', 'commenter': 'jayush'}]"
2444,ambari-server/src/main/resources/common-services/AMBARI_METRICS/0.1.0/service_advisor.py,"@@ -32,13 +32,19 @@
 SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
 STACKS_DIR = os.path.join(SCRIPT_DIR, '../../../stacks/')
 PARENT_FILE = os.path.join(STACKS_DIR, 'service_advisor.py')
+ABSOLUTE_STACKS_DIR = os.path.join(SCRIPT_DIR, '/var/lib/ambari-server/resources/stacks/')","[{'comment': 'See above', 'commenter': 'ncole'}]"
2444,ambari-server/src/main/java/org/apache/ambari/server/state/ServiceInfo.java,"@@ -59,7 +59,7 @@
 
 @XmlAccessorType(XmlAccessType.FIELD)
 @JsonFilter(""propertiesfilter"")
-public class ServiceInfo implements Validable {
+public class ServiceInfo implements Validable, Cloneable {","[{'comment': ""Hmmm - I always disliked `Cloneable` b/c it's very easy to miss changes made to the class. What about something like `SerializationUtils#clone` from `commons.lang`?"", 'commenter': 'jonathan-hurley'}, {'comment': 'Hmm, but this will require all stack objects to be serializable. For now I will stick with what I have and if required will look at this later.', 'commenter': 'jayush'}]"
2444,ambari-server/src/main/java/org/apache/ambari/server/stack/StackManager.java,"@@ -188,6 +188,29 @@ public StackManager(@Assisted(""stackRoot"") File stackRoot,
     populateDB(stackDao, extensionDao);
   }
 
+  /***
+   *  Constructor. Initialize StackManager for merging service definitions and creating merged stack definition
+   * @param stackRoot
+   * @param commonServicesRoot
+   */
+  public StackManager(File stackRoot, File commonServicesRoot, boolean validate) throws AmbariException {","[{'comment': ""I feel it's dangerous to use a class which is supposed to be `@Inject` without injection. It's very possible that the addition of fields in the Injected constructor could cause NPEs when the utility goes to use this constructor. "", 'commenter': 'jonathan-hurley'}]"
2456,ambari-web/app/views/common/configs/widgets/slider_config_widget_view.js,"@@ -693,6 +692,7 @@ App.SliderConfigWidgetView = App.ConfigWidgetView.extend({
       unitLabel = units[unitLabelIndex];
       valueLabel = this._extraRound(tick);
     }
+    var separator = (separator && unitLabel) ? separator : '';
     label = valueLabel + separator + unitLabel;","[{'comment': ""It is not a good practice to create a shadow variable in the context. Instead you can create a new variable (eg.: `localSeparator = separator;` or even better to skip the value assignments by doing the following:\r\n```\r\nlabel = valueLabel + ((separator && unitLabel) ? separator : '') + unitLabel;\r\n```"", 'commenter': 'tobias-istvan'}]"
2456,ambari-web/app/views/common/configs/widgets/slider_config_widget_view.js,"@@ -653,15 +653,15 @@ App.SliderConfigWidgetView = App.ConfigWidgetView.extend({
         var min = this.get('parseFunction')(this.getValueAttributeByGroup('minimum'));
         if (configValue < min) {
           min = this.widgetValueByConfigAttributes(min);
-          this.updateWarningsForCompatibilityWithWidget(Em.I18n.t('config.warnMessage.outOfBoundaries.less').format(min + this.get('unitLabel')));
+          this.updateWarningsForCompatibilityWithWidget(Em.I18n.t('config.warnMessage.outOfBoundaries.less').format(this.formatTickLabel(min, ' ')));","[{'comment': ""Is there a guarantee that value and unit won't be placed on different lines under the slider?"", 'commenter': 'aBabiichuk'}, {'comment': 'This patch is focusing on the warning labels\r\n<img width=""520"" alt=""screen shot 2018-10-15 at 9 09 10 am"" src=""https://user-images.githubusercontent.com/33458261/47013099-5e9c0800-d146-11e8-8d2a-9f51626ed36e.png"">\r\nand the Config warning page:\r\n<img width=""1370"" alt=""screen shot 2018-10-15 at 9 09 32 am"" src=""https://user-images.githubusercontent.com/33458261/47013213-b0449280-d146-11e8-85f5-5584066e0efe.png"">\r\n\r\nThe label in the slider was changed by this PR #2399 \r\nI checked that the ticker labels and the slider has the attribute `white-space: nowrap;`', 'commenter': 'kasakrisz'}]"
2456,ambari-web/app/views/common/configs/widgets/slider_config_widget_view.js,"@@ -680,7 +680,6 @@ App.SliderConfigWidgetView = App.ConfigWidgetView.extend({
    */
   formatTickLabel: function (tick, separator) {","[{'comment': 'Are all `formatTickLabel` usages handled in this PR?', 'commenter': 'aBabiichuk'}]"
2463,ambari-server/src/main/python/ambari_server/setupSecurity.py,"@@ -821,7 +821,7 @@ def setup_ldap(options):
 
   ssl_truststore_type_default = get_value_from_properties(properties, SSL_TRUSTSTORE_TYPE_PROPERTY, ""jks"")
   ssl_truststore_path_default = get_value_from_properties(properties, SSL_TRUSTSTORE_PATH_PROPERTY)
-  disable_endpoint_identification_default = get_value_from_properties(properties, LDAP_DISABLE_ENDPOINT_IDENTIFICATION, ""False"")
+  disable_endpoint_identification_default = get_value_from_properties(properties, LDAP_DISABLE_ENDPOINT_IDENTIFICATION, ""True"")","[{'comment': 'This should remain false. By default endpoint identification should apply while the client is negotiating with the server during SSL handshake to make sure the client can trust the server.\r\nThe issue, why this change has been added previously, has been occurred only with OpenJDK (i.e. openjdk-1.8.0.181-3.b13.el7_5.x86_64). In this case customers can change this flag and disable endpoint identification (which is less secure but they trust the server they are connecting to)', 'commenter': 'smolnar82'}]"
2489,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/BlueprintExportType.java,"@@ -0,0 +1,164 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.controller.internal;
+
+import java.util.Collection;
+import java.util.Map;
+import java.util.Objects;
+import java.util.Optional;
+
+import org.apache.ambari.server.topology.Configuration;
+import org.apache.commons.lang3.StringUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.collect.ImmutableSet;
+
+/**
+ * Handles most of type-specific behavior for blueprint export.
+ */
+public enum BlueprintExportType {
+  /**
+   * The exported blueprint contains all properties of all config types
+   * for services present in the cluster.
+   */
+  FULL {
+    @Override
+    public Configuration filter(Configuration actualConfig, Configuration defaultConfig) {
+      // no-op
+      return actualConfig;
+    }
+
+    @Override
+    public boolean include(String value, String defaultValue) {
+      return true;
+    }
+
+    @Override
+    public boolean include(Collection<?> collection) {
+      return true;
+    }
+
+    @Override
+    public boolean include(Map<?, ?> map) {
+      return true;
+    }
+  },
+
+  /**
+   * The exported blueprint contains only the properties that do not match default values
+   * as defined in the stack.  Empty lists/maps are also omitted.
+   */
+  MINIMAL {
+    @Override
+    public Configuration filter(Configuration actualConfig, Configuration defaultConfig) {
+      for (Map.Entry<String, Map<String, String>> configTypeEntry : ImmutableSet.copyOf(actualConfig.getProperties().entrySet())) {
+        String configType = configTypeEntry.getKey();
+        Map<String, String> properties = configTypeEntry.getValue();
+        for (Map.Entry<String, String> propertyEntry : ImmutableSet.copyOf(properties.entrySet())) {
+          String propertyName = propertyEntry.getKey();
+          String propertyValue = propertyEntry.getValue();
+          String defaultValue = defaultConfig.getPropertyValue(configType, propertyName);
+          if (include(propertyValue, defaultValue))  {
+            LOG.debug(""Including {}/{} in exported blueprint, as default value and actual value differ:\n{}\nvs\n{}"", configType, propertyName, defaultValue, propertyValue);
+          } else {
+            LOG.debug(""Omitting {}/{} from exported blueprint, as it has the default value of {}"", configType, propertyName, propertyValue);
+            actualConfig.removeProperty(configType, propertyName);
+          }
+        }
+        if (properties.isEmpty()) {
+          actualConfig.getProperties().remove(configType);
+        }
+      }
+
+      for (Map.Entry<String, Map<String, Map<String, String>>> configTypeEntry : ImmutableSet.copyOf(actualConfig.getAttributes().entrySet())) {
+        String configType = configTypeEntry.getKey();
+        Map<String, Map<String, String>> attributes = configTypeEntry.getValue();
+        for (Map.Entry<String, Map<String, String>> attributeEntry : ImmutableSet.copyOf(attributes.entrySet())) {
+          String attributeName = attributeEntry.getKey();
+          Map<String, String> properties = attributeEntry.getValue();
+          for (Map.Entry<String, String> propertyEntry : ImmutableSet.copyOf(properties.entrySet())) {
+            String propertyName = propertyEntry.getKey();
+            String attributeValue = propertyEntry.getValue();
+            String defaultValue = defaultConfig.getAttributeValue(configType, propertyName, attributeName);
+            if (include(attributeValue, defaultValue))  {
+              LOG.debug(""Including {}/{}/{} in exported blueprint, as default value and actual value differ:\n{}\nvs\n{}"", configType, attributeName, propertyName, defaultValue, attributeValue);
+            } else {
+              LOG.debug(""Omitting {}/{}/{} from exported blueprint, as it has the default value of {}"", configType, attributeName, propertyName, attributeValue);
+              properties.remove(propertyName);
+            }
+          }
+          if (properties.isEmpty()) {
+            attributes.remove(attributeName);
+          }
+        }
+        if (attributes.isEmpty()) {
+          actualConfig.getAttributes().remove(configType);
+        }
+      }
+
+      return actualConfig;
+    }
+
+    @Override
+    public boolean include(String value, String defaultValue) {
+      return value != null && (
+        defaultValue == null ||
+          !Objects.equals(StringUtils.trim(defaultValue), StringUtils.trim(value))","[{'comment': 'This trimming strategy is not always applicable, e.g. it is not for password. See https://github.com/apache/ambari/blob/4839a6f6df8bf741d3acc77ebadbe0300c4e3270/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/BlueprintConfigurationProcessor.java#L783 for more.\r\n\r\n(Unless passwords are already substituted in this phase)', 'commenter': 'benyoka'}, {'comment': ""Passwords are substituted, and they have no valid default value anyway, so the filter is not applicable.\r\n\r\nNote that the export includes actual value of properties, not the trimmed one.\r\n\r\nYou are right that in some very special cases this naive trimming could result in a property being exported despite matching the default value when compared using the more sophisticated trimming strategy.  I think currently only properties of type `directories` with multiple default values would be affected, for which I didn't find any example.\r\n\r\nI'll investigate using the trimming strategies in a follow-up task, if that's OK with you."", 'commenter': 'adoroszlai'}]"
2489,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/ExportBlueprintRequest.java,"@@ -175,16 +173,20 @@ private void createConfiguration(TreeNode<Resource> clusterNode) {
       ExportedConfiguration configuration = new ExportedConfiguration(config);
       DesiredConfig desiredConfig = (DesiredConfig) desiredConfigMap.get(configuration.getType());
       if (desiredConfig != null && desiredConfig.getTag().equals(configuration.getTag())) {
+        Map<String, String> configProperties = configuration.getProperties();
+        properties.put(configuration.getType(), configProperties);
 
-        properties.put(configuration.getType(), configuration.getProperties());
-        attributes.put(configuration.getType(), configuration.getPropertyAttributes());
+        Map<String, Map<String, String>> configAttributes = configuration.getPropertyAttributes();
+        attributes.put(configuration.getType(), configAttributes);","[{'comment': 'What is the meaning of the two variable declarations here?', 'commenter': 'benyoka'}, {'comment': ""It's just leftover from my previous approach to filtering.  Removed."", 'commenter': 'adoroszlai'}]"
2489,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/BlueprintExportType.java,"@@ -0,0 +1,164 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.controller.internal;
+
+import java.util.Collection;
+import java.util.Map;
+import java.util.Objects;
+import java.util.Optional;
+
+import org.apache.ambari.server.topology.Configuration;
+import org.apache.commons.lang3.StringUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.collect.ImmutableSet;
+
+/**
+ * Handles most of type-specific behavior for blueprint export.
+ */
+public enum BlueprintExportType {
+  /**
+   * The exported blueprint contains all properties of all config types
+   * for services present in the cluster.
+   */
+  FULL {
+    @Override
+    public Configuration filter(Configuration actualConfig, Configuration defaultConfig) {
+      // no-op
+      return actualConfig;
+    }
+
+    @Override
+    public boolean include(String value, String defaultValue) {
+      return true;
+    }
+
+    @Override
+    public boolean include(Collection<?> collection) {
+      return true;
+    }
+
+    @Override
+    public boolean include(Map<?, ?> map) {
+      return true;
+    }
+  },
+
+  /**
+   * The exported blueprint contains only the properties that do not match default values
+   * as defined in the stack.  Empty lists/maps are also omitted.
+   */
+  MINIMAL {
+    @Override
+    public Configuration filter(Configuration actualConfig, Configuration defaultConfig) {
+      for (Map.Entry<String, Map<String, String>> configTypeEntry : ImmutableSet.copyOf(actualConfig.getProperties().entrySet())) {
+        String configType = configTypeEntry.getKey();
+        Map<String, String> properties = configTypeEntry.getValue();
+        for (Map.Entry<String, String> propertyEntry : ImmutableSet.copyOf(properties.entrySet())) {
+          String propertyName = propertyEntry.getKey();
+          String propertyValue = propertyEntry.getValue();
+          String defaultValue = defaultConfig.getPropertyValue(configType, propertyName);
+          if (include(propertyValue, defaultValue))  {
+            LOG.debug(""Including {}/{} in exported blueprint, as default value and actual value differ:\n{}\nvs\n{}"", configType, propertyName, defaultValue, propertyValue);
+          } else {
+            LOG.debug(""Omitting {}/{} from exported blueprint, as it has the default value of {}"", configType, propertyName, propertyValue);
+            actualConfig.removeProperty(configType, propertyName);
+          }
+        }
+        if (properties.isEmpty()) {
+          actualConfig.getProperties().remove(configType);
+        }
+      }
+
+      for (Map.Entry<String, Map<String, Map<String, String>>> configTypeEntry : ImmutableSet.copyOf(actualConfig.getAttributes().entrySet())) {
+        String configType = configTypeEntry.getKey();
+        Map<String, Map<String, String>> attributes = configTypeEntry.getValue();
+        for (Map.Entry<String, Map<String, String>> attributeEntry : ImmutableSet.copyOf(attributes.entrySet())) {
+          String attributeName = attributeEntry.getKey();
+          Map<String, String> properties = attributeEntry.getValue();
+          for (Map.Entry<String, String> propertyEntry : ImmutableSet.copyOf(properties.entrySet())) {
+            String propertyName = propertyEntry.getKey();
+            String attributeValue = propertyEntry.getValue();
+            String defaultValue = defaultConfig.getAttributeValue(configType, propertyName, attributeName);
+            if (include(attributeValue, defaultValue))  {
+              LOG.debug(""Including {}/{}/{} in exported blueprint, as default value and actual value differ:\n{}\nvs\n{}"", configType, attributeName, propertyName, defaultValue, attributeValue);
+            } else {
+              LOG.debug(""Omitting {}/{}/{} from exported blueprint, as it has the default value of {}"", configType, attributeName, propertyName, attributeValue);
+              properties.remove(propertyName);
+            }
+          }
+          if (properties.isEmpty()) {
+            attributes.remove(attributeName);
+          }
+        }
+        if (attributes.isEmpty()) {
+          actualConfig.getAttributes().remove(configType);
+        }
+      }
+
+      return actualConfig;
+    }
+
+    @Override
+    public boolean include(String value, String defaultValue) {
+      return value != null && (
+        defaultValue == null ||
+          !Objects.equals(StringUtils.trim(defaultValue), StringUtils.trim(value))
+      );
+    }
+
+    @Override
+    public boolean include(Collection<?> collection) {
+      return collection != null && !collection.isEmpty();
+    }
+
+    @Override
+    public boolean include(Map<?, ?> map) {
+      return map != null && !map.isEmpty();
+    }
+  },
+  ;
+
+  public abstract Configuration filter(Configuration actualConfig, Configuration defaultConfig);","[{'comment': 'The filter method is called twice from _ClusterBlueprintRenderer::createBlueprintResource_\r\n\r\n1. once _via createClusterTopology -> ExportBlueprintRequest::new -> ExportBlueprintRequest::createConfiguration_\r\n1. once _via BlueprintConfigurationProcessor::doUpdateForBlueprintExport_\r\n\r\nDo we need both?', 'commenter': 'benyoka'}, {'comment': 'ExportBlueprintRequest seemed the  place to handle it.  But some conversion logic is only possible in BlueprintConfigurationProcessor, so I added it there, too.  It turns out that ExportBlueprintRequest no longer needs to filter, so I removed it from there.', 'commenter': 'adoroszlai'}]"
2586,ambari-project/pom.xml,"@@ -29,8 +29,8 @@
     <skipPythonTests>false</skipPythonTests>
     <solr.version>5.5.2</solr.version>
     <ambari.dir>${project.parent.basedir}</ambari.dir>
-    <powermock.version>1.6.3</powermock.version>
-    <jetty.version>9.4.12.v20180830</jetty.version>
+    <powermock.version>2.0.0-beta.5</powermock.version>
+    <jetty.version>9.4.11.v20180605</jetty.version>","[{'comment': ""`jetty.version` shouldn't be reverted."", 'commenter': 'adoroszlai'}]"
2586,pom.xml,"@@ -163,7 +163,7 @@
         <plugin>
           <groupId>org.apache.maven.plugins</groupId>
           <artifactId>maven-surefire-plugin</artifactId>
-          <version>2.20</version>
+          <version>2.22.0</version>","[{'comment': 'How about `2.22.1` instead?\r\n\r\nhttps://issues.apache.org/jira/browse/SUREFIRE-1562', 'commenter': 'adoroszlai'}]"
2586,ambari-server/src/test/java/org/apache/ambari/server/state/CheckHelperTest.java,"@@ -137,10 +137,6 @@ public void testPreUpgradeCheckNotApplicable() throws Exception {
 
     m_services.add(""KAFKA"");
 
-    Mockito.when(cluster.getServices()).thenReturn(new HashMap<>());","[{'comment': '```\r\n[ERROR] CheckHelperTest.java:22:8: Unused import - java.util.HashMap. [UnusedImports]\r\n```', 'commenter': 'adoroszlai'}]"
2589,ambari-server/src/main/java/org/apache/ambari/server/security/encryption/AESEncryptionService.java,"@@ -76,9 +78,11 @@ private final String getAmbariMasterKey() {
 
   private void initEnvironmentMasterKeyService() {
     if (environmentMasterKeyService == null) {
-      environmentMasterKeyService = new MasterKeyServiceImpl();
-      if (!environmentMasterKeyService.isMasterKeyInitialized()) {
-        throw new SecurityException(""You are trying to use a persisted master key but its initialization has been failed!"");
+      Configuration configuration = new Configuration();","[{'comment': ""Wouldn't it be better to use an injected version of this?"", 'commenter': 'rlevas'}]"
2589,ambari-server/src/main/java/org/apache/ambari/server/security/encryption/AESEncryptionService.java,"@@ -76,9 +78,11 @@ private final String getAmbariMasterKey() {
 
   private void initEnvironmentMasterKeyService() {
     if (environmentMasterKeyService == null) {
-      environmentMasterKeyService = new MasterKeyServiceImpl();
-      if (!environmentMasterKeyService.isMasterKeyInitialized()) {
-        throw new SecurityException(""You are trying to use a persisted master key but its initialization has been failed!"");
+      Configuration configuration = new Configuration();
+      try {
+        environmentMasterKeyService = MasterKeyServiceImpl.getMasterKeyService(null, configuration.getMasterKeyLocation(), configuration.isMasterKeyPersisted());","[{'comment': 'What if Configuration is injected into MasterKeyServiceImpl? In theory this service needs to know where to look after the master key location and related stuff.', 'commenter': 'smolnar82'}]"
2589,ambari-server/src/main/java/org/apache/ambari/server/security/encryption/SensitiveDataEncryption.java,"@@ -0,0 +1,133 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.security.encryption;
+
+import java.util.Collection;
+import java.util.Map;
+
+import org.apache.ambari.server.audit.AuditLoggerModule;
+import org.apache.ambari.server.controller.AmbariManagementController;
+import org.apache.ambari.server.controller.ControllerModule;
+import org.apache.ambari.server.ldap.LdapModule;
+import org.apache.ambari.server.orm.DBAccessor;
+import org.apache.ambari.server.state.Cluster;
+import org.apache.ambari.server.state.Clusters;
+import org.apache.ambari.server.state.Config;
+import org.apache.ambari.server.utils.EventBusSynchronizer;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.inject.Guice;
+import com.google.inject.Inject;
+import com.google.inject.Injector;
+import com.google.inject.persist.PersistService;
+
+public class SensitiveDataEncryption {
+  private static final Logger LOG = LoggerFactory.getLogger
+      (SensitiveDataEncryption.class);
+
+  private PersistService persistService;","[{'comment': 'These fields should be final fields', 'commenter': 'smolnar82'}]"
2589,ambari-server/src/main/java/org/apache/ambari/server/security/encryption/SensitiveDataEncryption.java,"@@ -0,0 +1,133 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.security.encryption;
+
+import java.util.Collection;
+import java.util.Map;
+
+import org.apache.ambari.server.audit.AuditLoggerModule;
+import org.apache.ambari.server.controller.AmbariManagementController;
+import org.apache.ambari.server.controller.ControllerModule;
+import org.apache.ambari.server.ldap.LdapModule;
+import org.apache.ambari.server.orm.DBAccessor;
+import org.apache.ambari.server.state.Cluster;
+import org.apache.ambari.server.state.Clusters;
+import org.apache.ambari.server.state.Config;
+import org.apache.ambari.server.utils.EventBusSynchronizer;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.inject.Guice;
+import com.google.inject.Inject;
+import com.google.inject.Injector;
+import com.google.inject.persist.PersistService;
+
+public class SensitiveDataEncryption {
+  private static final Logger LOG = LoggerFactory.getLogger
+      (SensitiveDataEncryption.class);
+
+  private PersistService persistService;
+  private DBAccessor dbAccessor;
+  private Injector injector;
+
+
+  @Inject
+  public SensitiveDataEncryption(DBAccessor dbAccessor,
+                                 Injector injector,
+                                 PersistService persistService) {
+    this.dbAccessor = dbAccessor;
+    this.injector = injector;
+    this.persistService = persistService;
+  }
+
+  /**
+   * Extension of main controller module
+   */
+  public static class EncryptionHelperControllerModule extends ControllerModule {","[{'comment': 'This class is used by the main method only; we may change it to private', 'commenter': 'smolnar82'}]"
2589,ambari-server/src/main/java/org/apache/ambari/server/security/encryption/SensitiveDataEncryption.java,"@@ -0,0 +1,133 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.security.encryption;
+
+import java.util.Collection;
+import java.util.Map;
+
+import org.apache.ambari.server.audit.AuditLoggerModule;
+import org.apache.ambari.server.controller.AmbariManagementController;
+import org.apache.ambari.server.controller.ControllerModule;
+import org.apache.ambari.server.ldap.LdapModule;
+import org.apache.ambari.server.orm.DBAccessor;
+import org.apache.ambari.server.state.Cluster;
+import org.apache.ambari.server.state.Clusters;
+import org.apache.ambari.server.state.Config;
+import org.apache.ambari.server.utils.EventBusSynchronizer;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.inject.Guice;
+import com.google.inject.Inject;
+import com.google.inject.Injector;
+import com.google.inject.persist.PersistService;
+
+public class SensitiveDataEncryption {
+  private static final Logger LOG = LoggerFactory.getLogger
+      (SensitiveDataEncryption.class);
+
+  private PersistService persistService;
+  private DBAccessor dbAccessor;
+  private Injector injector;
+
+
+  @Inject
+  public SensitiveDataEncryption(DBAccessor dbAccessor,
+                                 Injector injector,
+                                 PersistService persistService) {
+    this.dbAccessor = dbAccessor;
+    this.injector = injector;
+    this.persistService = persistService;
+  }
+
+  /**
+   * Extension of main controller module
+   */
+  public static class EncryptionHelperControllerModule extends ControllerModule {
+
+    public EncryptionHelperControllerModule() throws Exception {
+    }
+
+    @Override
+    protected void configure() {
+      super.configure();
+      EventBusSynchronizer.synchronizeAmbariEventPublisher(binder());
+    }
+  }
+
+  /**
+   * Extension of audit logger module
+   */
+  public static class EncryptionHelperAuditModule extends AuditLoggerModule {","[{'comment': 'This class is used by the main method only; we may change it to private', 'commenter': 'smolnar82'}]"
2589,ambari-server/src/main/java/org/apache/ambari/server/security/encryption/SensitiveDataEncryption.java,"@@ -0,0 +1,133 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.security.encryption;
+
+import java.util.Collection;
+import java.util.Map;
+
+import org.apache.ambari.server.audit.AuditLoggerModule;
+import org.apache.ambari.server.controller.AmbariManagementController;
+import org.apache.ambari.server.controller.ControllerModule;
+import org.apache.ambari.server.ldap.LdapModule;
+import org.apache.ambari.server.orm.DBAccessor;
+import org.apache.ambari.server.state.Cluster;
+import org.apache.ambari.server.state.Clusters;
+import org.apache.ambari.server.state.Config;
+import org.apache.ambari.server.utils.EventBusSynchronizer;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.inject.Guice;
+import com.google.inject.Inject;
+import com.google.inject.Injector;
+import com.google.inject.persist.PersistService;
+
+public class SensitiveDataEncryption {
+  private static final Logger LOG = LoggerFactory.getLogger
+      (SensitiveDataEncryption.class);
+
+  private PersistService persistService;
+  private DBAccessor dbAccessor;
+  private Injector injector;
+
+
+  @Inject
+  public SensitiveDataEncryption(DBAccessor dbAccessor,
+                                 Injector injector,
+                                 PersistService persistService) {
+    this.dbAccessor = dbAccessor;
+    this.injector = injector;
+    this.persistService = persistService;
+  }
+
+  /**
+   * Extension of main controller module
+   */
+  public static class EncryptionHelperControllerModule extends ControllerModule {
+
+    public EncryptionHelperControllerModule() throws Exception {
+    }
+
+    @Override
+    protected void configure() {
+      super.configure();
+      EventBusSynchronizer.synchronizeAmbariEventPublisher(binder());
+    }
+  }
+
+  /**
+   * Extension of audit logger module
+   */
+  public static class EncryptionHelperAuditModule extends AuditLoggerModule {
+
+    public EncryptionHelperAuditModule() throws Exception {","[{'comment': 'The parent class does not define any constructor; we do not need the default one here either', 'commenter': 'smolnar82'}]"
2589,ambari-server/src/main/java/org/apache/ambari/server/security/encryption/SensitiveDataEncryption.java,"@@ -0,0 +1,133 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.security.encryption;
+
+import java.util.Collection;
+import java.util.Map;
+
+import org.apache.ambari.server.audit.AuditLoggerModule;
+import org.apache.ambari.server.controller.AmbariManagementController;
+import org.apache.ambari.server.controller.ControllerModule;
+import org.apache.ambari.server.ldap.LdapModule;
+import org.apache.ambari.server.orm.DBAccessor;
+import org.apache.ambari.server.state.Cluster;
+import org.apache.ambari.server.state.Clusters;
+import org.apache.ambari.server.state.Config;
+import org.apache.ambari.server.utils.EventBusSynchronizer;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.inject.Guice;
+import com.google.inject.Inject;
+import com.google.inject.Injector;
+import com.google.inject.persist.PersistService;
+
+public class SensitiveDataEncryption {","[{'comment': 'Could you please write some lines of JavaDoc about the purpose of this class?', 'commenter': 'smolnar82'}]"
2589,ambari-server/src/main/java/org/apache/ambari/server/security/encryption/SensitiveDataEncryption.java,"@@ -0,0 +1,133 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.security.encryption;
+
+import java.util.Collection;
+import java.util.Map;
+
+import org.apache.ambari.server.audit.AuditLoggerModule;
+import org.apache.ambari.server.controller.AmbariManagementController;
+import org.apache.ambari.server.controller.ControllerModule;
+import org.apache.ambari.server.ldap.LdapModule;
+import org.apache.ambari.server.orm.DBAccessor;
+import org.apache.ambari.server.state.Cluster;
+import org.apache.ambari.server.state.Clusters;
+import org.apache.ambari.server.state.Config;
+import org.apache.ambari.server.utils.EventBusSynchronizer;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.inject.Guice;
+import com.google.inject.Inject;
+import com.google.inject.Injector;
+import com.google.inject.persist.PersistService;
+
+public class SensitiveDataEncryption {
+  private static final Logger LOG = LoggerFactory.getLogger
+      (SensitiveDataEncryption.class);
+
+  private PersistService persistService;
+  private DBAccessor dbAccessor;
+  private Injector injector;
+
+
+  @Inject
+  public SensitiveDataEncryption(DBAccessor dbAccessor,
+                                 Injector injector,
+                                 PersistService persistService) {
+    this.dbAccessor = dbAccessor;
+    this.injector = injector;
+    this.persistService = persistService;
+  }
+
+  /**
+   * Extension of main controller module
+   */
+  public static class EncryptionHelperControllerModule extends ControllerModule {
+
+    public EncryptionHelperControllerModule() throws Exception {
+    }
+
+    @Override
+    protected void configure() {
+      super.configure();
+      EventBusSynchronizer.synchronizeAmbariEventPublisher(binder());
+    }
+  }
+
+  /**
+   * Extension of audit logger module
+   */
+  public static class EncryptionHelperAuditModule extends AuditLoggerModule {
+
+    public EncryptionHelperAuditModule() throws Exception {
+    }
+
+    @Override
+    protected void configure() {
+      super.configure();
+    }
+
+  }
+
+  public void startPersistenceService() {
+    persistService.start();
+  }
+
+  public void stopPersistenceService() {
+    persistService.stop();
+  }
+
+  public static void main(String[] args) {
+    if (args.length < 1 || !""encryption"".equals(args[0]) || !""decryption"".equals(args[0] )){
+      LOG.error(""Expect encryption/decryption action parameter"");
+      return;","[{'comment': 'We may exit here with -1.', 'commenter': 'smolnar82'}]"
2589,ambari-server/src/main/java/org/apache/ambari/server/state/Config.java,"@@ -102,4 +106,14 @@
    * @return the cluster where this config belongs to
    */
   Cluster getCluster();
+
+  /**
+   * Encrypt and persist the configuration.
+   */
+  void encryptSensitiveDataAndSave(Encryptor<Config> configPropertiesEncryptor);
+
+  /**
+   * Decrypt and persist the configuration.
+   */
+  void decryptSensitiveDataAndSave(Encryptor<Config> configPropertiesEncryptor);","[{'comment': ""I'm not sure if we need this new API.\r\nWithin SensitiveDataEncryption you have everything you need to encrypt/decrypt. For instance:\r\n              if (encrypt) {\r\n                configEncryptor.encryptSensitiveData(config)\r\n              } else {\r\n                configEncryptor.decryptSensitiveData(config)\r\n              }\r\n              config.save()\r\n\r\nWhat do you think?"", 'commenter': 'smolnar82'}]"
2589,ambari-server/src/main/java/org/apache/ambari/server/security/encryption/MasterKeyServiceImpl.java,"@@ -243,7 +284,7 @@ private static void protectAccess(File file) throws AmbariException {
     }
   }
 
-  private String readMasterKey() {
+  private void InitializeFromEnv() {","[{'comment': ""Note the capital 'I' at the end; should be 'i' to follow camelCase"", 'commenter': 'smolnar82'}]"
2589,ambari-server/src/main/java/org/apache/ambari/server/state/Config.java,"@@ -22,6 +22,10 @@
 import java.util.Map;
 import java.util.Set;
 
+import org.apache.ambari.server.security.encryption.Encryptor;
+
+import com.google.inject.persist.Transactional;
+","[{'comment': 'Please remove unused imports', 'commenter': 'smolnar82'}]"
2613,ambari-common/src/main/python/resource_management/libraries/script/config_dictionary.py,"@@ -18,13 +18,38 @@
 limitations under the License.
 '''
 from resource_management.core.exceptions import Fail
+from ambari_pycryptodome.Crypto.Cipher import AES
+from ambari_pycryptodome.Crypto.Protocol.KDF import PBKDF2
+from ambari_pycryptodome.Crypto.Hash import SHA1, HMAC
+import os
 
 IMMUTABLE_MESSAGE = """"""Configuration dictionary is immutable!
 
 For adding dynamic properties to xml files please use {{varible_from_params}} substitutions.
 Lookup xml files for {{ for examples. 
 """"""
 
+PBKDF2_PRF = lambda p, s: HMAC.new(p, s, SHA1).digest()
+
+def decrypt(encrypted_value, encryption_key):
+  salt, iv, data = [each.decode('hex') for each in encrypted_value.decode('hex').split('::')]
+  cipher = AES.new(
+    PBKDF2(password=encryption_key, salt=salt, count=65536, dkLen=16, prf=PBKDF2_PRF),
+    AES.MODE_CBC,
+    iv=iv)
+  return cipher.decrypt(data)
+
+def is_encrypted(value):
+  return isinstance(value, basestring) and value.startswith('${enc=aes256_hex, value=')
+
+def encrypted_value(value):
+  return value.split('value=')[1][:-1]
+
+def encryption_key():
+  if 'AGENT_ENCRYPTION_KEY' not in os.environ:
+    raise RuntimeError('Missing encryption key: AGENT_ENCRYPTION_KEY is not defined.')","[{'comment': ""```suggestion\r\n    raise RuntimeError('Missing encryption key: AGENT_ENCRYPTION_KEY is not defined at environment.')\r\n```"", 'commenter': 'Unknown'}]"
2613,ambari-server/src/main/java/org/apache/ambari/server/security/encryption/Encryptor.java,"@@ -36,8 +35,20 @@
    * 
    * @param decryptible
    *          to be decrypted
-   * @return the decrypted value
    */
   void decryptSensitiveData(T decryptible);
 
+  /**
+   * @return the default encryption key used by this encryptor
+   */
+  String getEncryptionKey();
+
+  Encryptor NONE = new Encryptor() {
+    @Override
+    public void encryptSensitiveData(Object data) { }
+    @Override
+    public void decryptSensitiveData(Object decryptible) { }
+    @Override
+    public String getEncryptionKey() { return """"; }","[{'comment': 'how about using null instead of empty string, so we get an NPE if this key is attempted to really be used somewhere?', 'commenter': 'Unknown'}, {'comment': ""it's intentionally an empty string so when encryption is not enabled the salt string will be empty "", 'commenter': 'zeroflag'}]"
2613,ambari-server/src/test/java/org/apache/ambari/server/agent/TestHeartbeatHandler.java,"@@ -989,7 +980,7 @@ public void testRecoveryStatusReports() throws Exception {
           add(command);
         }}).anyTimes();
     replay(am);
-    HeartBeatHandler handler = new HeartBeatHandler(fsm, am, injector);
+    HeartBeatHandler handler = new HeartBeatHandler(fsm, am, Encryptor.NONE, injector);","[{'comment': 'how about adding some method like getDefaultHeartBeatHandler(am) so we call it instead of a copy paste with 4 params, 3 of which do not really matter?', 'commenter': 'Unknown'}, {'comment': 'yeah. good idea, originally i was going to do that but i forgot', 'commenter': 'zeroflag'}]"
2613,start-build-env.sh,"@@ -28,7 +28,7 @@ cd ""$(dirname ""$0"")""
 : ${AMBARI_DIR:=$(pwd -P)}
 
 # Maven version
-: ${MAVEN_VERSION:=3.3.9}","[{'comment': 'Is it really meant to be included into a patch?', 'commenter': 'Unknown'}, {'comment': ""originally i wanted it in a different patch, but it's not really a problem if it goes with this one"", 'commenter': 'zeroflag'}]"
2613,ambari-common/src/main/python/resource_management/libraries/script/config_dictionary.py,"@@ -18,13 +18,35 @@
 limitations under the License.
 '''
 from resource_management.core.exceptions import Fail
+import ambari_pyaes
+from ambari_pbkdf2.pbkdf2 import PBKDF2
+import os
 
 IMMUTABLE_MESSAGE = """"""Configuration dictionary is immutable!
 
 For adding dynamic properties to xml files please use {{varible_from_params}} substitutions.
 Lookup xml files for {{ for examples. 
 """"""
 
+PBKDF2_PRF = lambda p, s: HMAC.new(p, s, SHA1).digest()
+
+def decrypt(encrypted_value, encryption_key):
+  salt, iv, data = [each.decode('hex') for each in encrypted_value.decode('hex').split('::')]
+  key = PBKDF2(encryption_key, salt, iterations=65536).read(16)
+  aes = ambari_pyaes.AESModeOfOperationCBC(key, iv=iv)
+  return ambari_pyaes.util.strip_PKCS7_padding(aes.decrypt(data))
+
+def is_encrypted(value):
+  return isinstance(value, basestring) and value.startswith('${enc=aes256_hex, value=')","[{'comment': 'Ideally this is not hardcoded and the encryption and encoding types are parse out and used to determine the implementations to use.   But for now this should be ok. ', 'commenter': 'rlevas'}, {'comment': 'Yes we do the same on the server side. I added  a comment to warn about this.', 'commenter': 'zeroflag'}]"
2613,ambari-common/src/main/python/resource_management/libraries/script/config_dictionary.py,"@@ -52,8 +74,10 @@ def __getitem__(self, name):
       value = super(ConfigDictionary, self).__getitem__(name)
     except KeyError:
       return UnknownConfiguration(name)
-      
-    
+
+    if is_encrypted(value):
+      return decrypt(encrypted_value(value), encryption_key())","[{'comment': 'why not set `value` to the decrypted value, allowing the rest of the logic to occur?  ', 'commenter': 'rlevas'}, {'comment': 'ok, i modified it', 'commenter': 'zeroflag'}]"
2613,ambari-server/src/main/java/org/apache/ambari/server/agent/stomp/AgentConfigsHolder.java,"@@ -16,12 +16,16 @@
  * limitations under the License.
  */
 package org.apache.ambari.server.agent.stomp;
+
 import java.util.List;
 import java.util.stream.Collectors;
 
+import javax.inject.Named;","[{'comment': ""Shouldn't this be `import com.google.inject.name.Named;`"", 'commenter': 'rlevas'}, {'comment': 'This is incorrect.\r\n@zeroflag I made a mistake in `ConfigImpl` in my previous change to import `javax.inject.Named` instead of `com.google.inject.name.Named`. Could you please fix it here and there too? Thanks!', 'commenter': 'smolnar82'}, {'comment': 'fixed', 'commenter': 'zeroflag'}]"
2623,ambari-server/src/main/java/org/apache/ambari/server/topology/BlueprintValidatorImpl.java,"@@ -282,24 +283,34 @@ public void validateRequiredProperties() throws InvalidTopologyException, GPLLic
               componentName, new Cardinality(""1+""), autoDeployInfo);
 
           resolved = missingDependencyInfo.isEmpty();
+          if (dependencyType.equals(""exclusive"")) {
+            resolved = !resolved;
+          }
         } else if (dependencyScope.equals(""host"")) {
-          if (group.getComponentNames().contains(componentName) || (autoDeployInfo != null && autoDeployInfo.isEnabled())) {
-            resolved = true;
-            group.addComponent(componentName);
+          //TODO check if the group might not contain all the component at this point
+          if (dependencyType.equals(""exclusive"")) {
+            if (!group.getComponentNames().contains(componentName)) {
+              resolved = true;
+            }
+          } else {
+            if (group.getComponentNames().contains(componentName) || (autoDeployInfo != null && autoDeployInfo.isEnabled())) {
+              resolved = true;
+            }
           }
+          group.addComponent(componentName);","[{'comment': 'The component should be added to the group only for inclusive type dependency.  This is basically ""auto-deploy"" happening.', 'commenter': 'adoroszlai'}, {'comment': 'Thanks for noticing', 'commenter': 'd0zen1'}]"
2623,ambari-server/src/main/java/org/apache/ambari/server/topology/BlueprintValidatorImpl.java,"@@ -282,24 +283,34 @@ public void validateRequiredProperties() throws InvalidTopologyException, GPLLic
               componentName, new Cardinality(""1+""), autoDeployInfo);
 
           resolved = missingDependencyInfo.isEmpty();
+          if (dependencyType.equals(""exclusive"")) {
+            resolved = !resolved;
+          }
         } else if (dependencyScope.equals(""host"")) {
-          if (group.getComponentNames().contains(componentName) || (autoDeployInfo != null && autoDeployInfo.isEnabled())) {
-            resolved = true;
-            group.addComponent(componentName);
+          //TODO check if the group might not contain all the component at this point","[{'comment': 'Yes, auto-deployed components may not be there yet.  So probably all auto-deploy should be performed first, then check for exclusivity.', 'commenter': 'adoroszlai'}]"
2623,ambari-server/src/main/java/org/apache/ambari/server/controller/AmbariManagementControllerImpl.java,"@@ -751,6 +752,37 @@ public synchronized void createHostComponents(Set<ServiceComponentHostRequest> r
       throw new DuplicateResourceException(msg + names);
     }
 
+    //TODO move to separate method
+    for (String clusterName : hostComponentNames.keySet()) {
+      for (String serviceName : hostComponentNames.get(clusterName).keySet()) {
+        for (String componentName : hostComponentNames.get(clusterName).get(serviceName).keySet()) {
+          Set<String> hostnames = hostComponentNames.get(clusterName).get(serviceName).get(componentName);","[{'comment': 'Iterate over `entrySet()` instead of using `keySet()` + `get()`.', 'commenter': 'adoroszlai'}]"
2623,ambari-server/src/main/java/org/apache/ambari/server/controller/AmbariManagementControllerImpl.java,"@@ -751,6 +752,37 @@ public synchronized void createHostComponents(Set<ServiceComponentHostRequest> r
       throw new DuplicateResourceException(msg + names);
     }
 
+    //TODO move to separate method
+    for (String clusterName : hostComponentNames.keySet()) {
+      for (String serviceName : hostComponentNames.get(clusterName).keySet()) {
+        for (String componentName : hostComponentNames.get(clusterName).get(serviceName).keySet()) {
+          Set<String> hostnames = hostComponentNames.get(clusterName).get(serviceName).get(componentName);
+          if (hostnames != null && !hostnames.isEmpty()) {
+            ServiceComponent sc = clusters.getCluster(clusterName).getService(serviceName).getServiceComponent(componentName);
+            StackId stackId = sc.getDesiredStackId();
+            List<DependencyInfo> dependencyInfos = ambariMetaInfo.getComponentDependencies(stackId.getStackName(),
+                                                               stackId.getStackVersion(), serviceName, componentName);
+            for (DependencyInfo dependencyInfo : dependencyInfos) {
+              if (""host"".equals(dependencyInfo.getScope()) && ""exclusive"".equals(dependencyInfo.getType())) {
+                Service depSevice = clusters.getCluster(clusterName).getService(dependencyInfo.getServiceName());","[{'comment': 'typo: `depSevice` -> `depService`', 'commenter': 'adoroszlai'}]"
2623,ambari-server/src/main/java/org/apache/ambari/server/controller/AmbariManagementControllerImpl.java,"@@ -751,6 +752,37 @@ public synchronized void createHostComponents(Set<ServiceComponentHostRequest> r
       throw new DuplicateResourceException(msg + names);
     }
 
+    //TODO move to separate method
+    for (String clusterName : hostComponentNames.keySet()) {
+      for (String serviceName : hostComponentNames.get(clusterName).keySet()) {
+        for (String componentName : hostComponentNames.get(clusterName).get(serviceName).keySet()) {
+          Set<String> hostnames = hostComponentNames.get(clusterName).get(serviceName).get(componentName);
+          if (hostnames != null && !hostnames.isEmpty()) {
+            ServiceComponent sc = clusters.getCluster(clusterName).getService(serviceName).getServiceComponent(componentName);
+            StackId stackId = sc.getDesiredStackId();
+            List<DependencyInfo> dependencyInfos = ambariMetaInfo.getComponentDependencies(stackId.getStackName(),
+                                                               stackId.getStackVersion(), serviceName, componentName);
+            for (DependencyInfo dependencyInfo : dependencyInfos) {
+              if (""host"".equals(dependencyInfo.getScope()) && ""exclusive"".equals(dependencyInfo.getType())) {
+                Service depSevice = clusters.getCluster(clusterName).getService(dependencyInfo.getServiceName());
+                if (depSevice != null) {
+                  ServiceComponent dependentSC = depSevice.getServiceComponent(dependencyInfo.getComponentName());
+                  if (dependentSC != null) {
+                    for (String host : dependentSC.getServiceComponentHosts().keySet()) {
+                      if (hostnames.contains(host)) {","[{'comment': ""I'd prefer using set operations instead of a loop: may be a bit faster, but more importantly, the error message can include all hosts with the problem, so it can be fixed in one go."", 'commenter': 'adoroszlai'}]"
2623,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/StackDependencyResourceProvider.java,"@@ -81,6 +83,7 @@
       .put(Resource.Type.StackService, DEPENDENT_SERVICE_NAME_ID)
       .put(Resource.Type.StackServiceComponent, DEPENDENT_COMPONENT_NAME_ID)
       .put(Resource.Type.StackServiceComponentDependency, COMPONENT_NAME_ID)
+      .put(Resource.Type.StackServiceComponentDependencyType, TYPE_ID)","[{'comment': ""I'm not sure what these key properties are used for, but: should type really be a part of the key?  I would think each (service+component, dependent service+component) pair to be unique, regardless of type.  No pair should have both inclusive and exclusive dependency at the same time."", 'commenter': 'adoroszlai'}]"
2637,ambari-server/src/main/java/org/apache/ambari/server/api/services/stackadvisor/recommendations/RecommendationResponse.java,"@@ -163,6 +163,24 @@ public void setProperties(Map<String, String> properties) {
     public void setPropertyAttributes(Map<String, ValueAttributesInfo> propertyAttributes) {
       this.propertyAttributes = propertyAttributes;
     }
+
+    @Override
+    public boolean equals(Object o) {
+      if (this == o) return true;
+      if (o == null || getClass() != o.getClass()) return false;
+
+      BlueprintConfigurations that = (BlueprintConfigurations) o;
+
+      if (properties != null ? !properties.equals(that.properties) : that.properties != null) return false;
+      return propertyAttributes != null ? propertyAttributes.equals(that.propertyAttributes) : that.propertyAttributes == null;","[{'comment': 'Simplify using `Objects.equals`.', 'commenter': 'adoroszlai'}]"
2637,ambari-server/src/main/java/org/apache/ambari/server/api/services/stackadvisor/recommendations/RecommendationResponse.java,"@@ -268,6 +286,27 @@ public void setConfigurations(Map<String, BlueprintConfigurations> configuration
     public void setDependentConfigurations(Map<String, BlueprintConfigurations> dependentConfigurations) {
       this.dependentConfigurations = dependentConfigurations;
     }
+
+    @Override
+    public boolean equals(Object o) {
+      if (this == o) return true;
+      if (o == null || getClass() != o.getClass()) return false;
+
+      ConfigGroup that = (ConfigGroup) o;
+
+      if (hosts != null ? !hosts.equals(that.hosts) : that.hosts != null) return false;
+      if (configurations != null ? !configurations.equals(that.configurations) : that.configurations != null)
+        return false;
+      return dependentConfigurations != null ? dependentConfigurations.equals(that.dependentConfigurations) : that.dependentConfigurations == null;
+    }
+
+    @Override
+    public int hashCode() {
+      int result = hosts != null ? hosts.hashCode() : 0;
+      result = 31 * result + (configurations != null ? configurations.hashCode() : 0);
+      result = 31 * result + (dependentConfigurations != null ? dependentConfigurations.hashCode() : 0);","[{'comment': 'Simplify using `Objects.hash`.', 'commenter': 'adoroszlai'}]"
2642,ambari-server/src/main/java/org/apache/ambari/server/controller/AmbariManagementControllerImpl.java,"@@ -2095,6 +2099,40 @@ private synchronized RequestStatusResponse updateCluster(ClusterRequest request,
     }
   }
 
+  /**
+   * validate cluster name character and length requirements and throw IllegalArgumentException if not valid.
+   * <p>
+   * Character Requirements
+   * <p>
+   * A through Z
+   * a through z
+   * 0 through 9
+   * _ (underscore)
+   * - (dash)
+   * Length Requirements
+   * <p>
+   * Minimum: 1 character
+   * Maximum: 100 characters
+   * @see AmbariManagementControllerImpl#CLUSTER_NAME_VALIDATION_REGEXP
+   *
+   * @param clusterName name to validate
+   * @throws IllegalArgumentException if validation result
+   */
+  public static void validateClusterName(String clusterName) {
+    if (clusterName == null) {
+      throw new IllegalArgumentException(""Invalid arguments, cluster name should not be null"");
+    }
+    if (clusterName.isEmpty()) {
+      throw new IllegalArgumentException(""Invalid arguments, cluster name should not be empty"");
+    }
+    Pattern clusterNamePtrn = Pattern.compile(CLUSTER_NAME_VALIDATION_REGEXP);","[{'comment': ""Rather than recompile this, wouldn't it be better to compile this once into a static final property?"", 'commenter': 'rlevas'}, {'comment': '+1', 'commenter': 'smolnar82'}]"
2646,ambari-server/src/main/java/org/apache/ambari/server/state/quicklinksprofile/DefaultQuickLinkVisibilityController.java,"@@ -57,6 +68,39 @@ public DefaultQuickLinkVisibilityController(QuickLinksProfile profile) throws Qu
     if (filterCount == 0) {
       throw new QuickLinksProfileEvaluationException(""At least one filter must be defined."");
     }
+
+    // compute url overrides
+    profile.getFilters().stream()
+      .filter(f -> f instanceof LinkNameFilter && ((LinkNameFilter)f).getLinkUrl() != null)
+      .findAny()
+      .ifPresent(f -> LOG.warn(""Link url overrides only work on service and component levels. Global filter: {}"", f));
+    for (Service service : profile.getServices()) {
+      urlOverrides.putAll(getUrlOverrides(service.getName(), service.getFilters()));
+
+      for (Component component : service.getComponents()) {
+        Map<Pair<String, String>, String> componentUrlOverrides = getUrlOverrides(service.getName(), component.getFilters());
+        Sets.SetView<Pair<String, String>> duplicateOverrides = Sets.difference(urlOverrides.keySet(), componentUrlOverrides.keySet());
+        if (!duplicateOverrides.isEmpty()) {
+          LOG.warn(""Duplicate url overrides in quick links profile: {}"", duplicateOverrides);
+        }
+        urlOverrides.putAll(componentUrlOverrides);
+      }
+    }
+  }
+
+  private Map<Pair<String, String>, String> getUrlOverrides(String serviceName, Collection<Filter> filters) {
+    return filters.stream()
+      .filter( f -> f instanceof LinkNameFilter && null != ((LinkNameFilter)f).getLinkUrl() )
+      .map( f -> {
+        LinkNameFilter lnf = (LinkNameFilter)f;","[{'comment': 'Can you please extract the following logic (to `LinkNameFilter`)?  It would allow simplifying the streams in `getUrlOverrides` and the controller constructor.\r\n\r\n```\r\n  public static Stream<LinkNameFilter> onlyLinkNameFilters(Stream<? extends Filter> streamOfFilters) {\r\n    return streamOfFilters\r\n      .filter(f -> f instanceof LinkNameFilter)\r\n      .map(f -> (LinkNameFilter) f);\r\n  }\r\n```', 'commenter': 'adoroszlai'}]"
2646,ambari-server/src/main/java/org/apache/ambari/server/state/quicklinksprofile/DefaultQuickLinkVisibilityController.java,"@@ -57,6 +68,39 @@ public DefaultQuickLinkVisibilityController(QuickLinksProfile profile) throws Qu
     if (filterCount == 0) {
       throw new QuickLinksProfileEvaluationException(""At least one filter must be defined."");
     }
+
+    // compute url overrides
+    profile.getFilters().stream()
+      .filter(f -> f instanceof LinkNameFilter && ((LinkNameFilter)f).getLinkUrl() != null)
+      .findAny()
+      .ifPresent(f -> LOG.warn(""Link url overrides only work on service and component levels. Global filter: {}"", f));","[{'comment': 'Is it possible to have multiple global link URL filters?  If so, should it warn about a random one, or about all of them?', 'commenter': 'adoroszlai'}, {'comment': 'Changed logic to report all.', 'commenter': 'benyoka'}]"
2646,ambari-server/src/main/java/org/apache/ambari/server/state/quicklinksprofile/DefaultQuickLinkVisibilityController.java,"@@ -57,6 +68,39 @@ public DefaultQuickLinkVisibilityController(QuickLinksProfile profile) throws Qu
     if (filterCount == 0) {
       throw new QuickLinksProfileEvaluationException(""At least one filter must be defined."");
     }
+
+    // compute url overrides
+    profile.getFilters().stream()
+      .filter(f -> f instanceof LinkNameFilter && ((LinkNameFilter)f).getLinkUrl() != null)
+      .findAny()
+      .ifPresent(f -> LOG.warn(""Link url overrides only work on service and component levels. Global filter: {}"", f));
+    for (Service service : profile.getServices()) {
+      urlOverrides.putAll(getUrlOverrides(service.getName(), service.getFilters()));
+
+      for (Component component : service.getComponents()) {
+        Map<Pair<String, String>, String> componentUrlOverrides = getUrlOverrides(service.getName(), component.getFilters());
+        Sets.SetView<Pair<String, String>> duplicateOverrides = Sets.difference(urlOverrides.keySet(), componentUrlOverrides.keySet());","[{'comment': '```suggestion\r\n        Set<Pair<String, String>> duplicateOverrides = Sets.difference(urlOverrides.keySet(), componentUrlOverrides.keySet());\r\n```', 'commenter': 'adoroszlai'}, {'comment': 'Also it needs to be intersection, not difference', 'commenter': 'benyoka'}]"
2646,ambari-server/src/main/java/org/apache/ambari/server/state/quicklinksprofile/DefaultQuickLinkVisibilityController.java,"@@ -57,6 +68,39 @@ public DefaultQuickLinkVisibilityController(QuickLinksProfile profile) throws Qu
     if (filterCount == 0) {
       throw new QuickLinksProfileEvaluationException(""At least one filter must be defined."");
     }
+
+    // compute url overrides
+    profile.getFilters().stream()
+      .filter(f -> f instanceof LinkNameFilter && ((LinkNameFilter)f).getLinkUrl() != null)
+      .findAny()
+      .ifPresent(f -> LOG.warn(""Link url overrides only work on service and component levels. Global filter: {}"", f));
+    for (Service service : profile.getServices()) {
+      urlOverrides.putAll(getUrlOverrides(service.getName(), service.getFilters()));
+
+      for (Component component : service.getComponents()) {
+        Map<Pair<String, String>, String> componentUrlOverrides = getUrlOverrides(service.getName(), component.getFilters());
+        Sets.SetView<Pair<String, String>> duplicateOverrides = Sets.difference(urlOverrides.keySet(), componentUrlOverrides.keySet());
+        if (!duplicateOverrides.isEmpty()) {
+          LOG.warn(""Duplicate url overrides in quick links profile: {}"", duplicateOverrides);
+        }
+        urlOverrides.putAll(componentUrlOverrides);
+      }
+    }
+  }
+
+  private Map<Pair<String, String>, String> getUrlOverrides(String serviceName, Collection<Filter> filters) {
+    return filters.stream()
+      .filter( f -> f instanceof LinkNameFilter && null != ((LinkNameFilter)f).getLinkUrl() )
+      .map( f -> {
+        LinkNameFilter lnf = (LinkNameFilter)f;
+        return Pair.of(Pair.of(serviceName, lnf.getLinkName()), lnf.getLinkUrl());
+      })
+      .collect( toMap(Pair::getKey, Pair::getValue) );
+  }
+
+  @Override
+  public java.util.Optional<String> getUrlOverride(@Nonnull String service, @Nonnull Link quickLink) {","[{'comment': '```suggestion\r\n  public Optional<String> getUrlOverride(@Nonnull String service, @Nonnull Link quickLink) {\r\n```', 'commenter': 'adoroszlai'}, {'comment': 'Thanks. It is a leftover from Guava Optional -> Java 8 Optional migration.', 'commenter': 'benyoka'}]"
2646,ambari-server/src/main/java/org/apache/ambari/server/state/quicklinksprofile/DefaultQuickLinkVisibilityController.java,"@@ -87,127 +131,52 @@ private int size(@Nullable Collection<?> collection) {
 
   private Optional<Boolean> evaluateComponentRules(@Nonnull String service, @Nonnull Link quickLink) {
     if (null == quickLink.getComponentName()) {
-      return Optional.absent();
+      return Optional.empty();
     }
     else {
       FilterEvaluator componentEvaluator = componentRules.get(ServiceComponent.of(service, quickLink.getComponentName()));
-      return componentEvaluator != null ? componentEvaluator.isVisible(quickLink) : Optional.absent();
+      return componentEvaluator != null ? componentEvaluator.isVisible(quickLink) : Optional.empty();
     }
   }
 
   private Optional<Boolean> evaluateServiceRules(@Nonnull String service, @Nonnull Link quickLink) {
     return serviceRules.containsKey(service) ?
-        serviceRules.get(service).isVisible(quickLink) : Optional.absent();
+        serviceRules.get(service).isVisible(quickLink) : Optional.empty();
   }
 
   static <T> List<T> nullToEmptyList(@Nullable List<T> items) {
     return items != null ? items : Collections.emptyList();
   }
-}
-
-/**
- * Groups quicklink filters that are on the same level (e.g. a global evaluator or an evaluator for the ""HDFS"" service,
- * etc.). The evaluator pick the most applicable filter for a given quick link. If no applicable filter is found, it
- * returns {@link Optional#absent()}.
- * <p>
- *   Filter evaluation order is the following:
- *   <ol>
- *     <li>First, link name filters are evaluated. These match links by name.</li>
- *     <li>If there is no matching link name filter, link attribute filters are evaluated next. ""Hide"" type filters
- *     take precedence to ""show"" type filters.</li>
- *     <li>Finally, the match-all filter is evaluated, provided it exists.</li>
- *   </ol>
- * </p>
- */
-class FilterEvaluator {
-  private final Map<String, Boolean> linkNameFilters = new HashMap<>();
-  private final Set<String> showAttributes = new HashSet<>();
-  private final Set<String> hideAttributes = new HashSet<>();
-  private Optional<Boolean> acceptAllFilter = Optional.absent();
-
-  FilterEvaluator(List<Filter> filters) throws QuickLinksProfileEvaluationException {
-    for (Filter filter: DefaultQuickLinkVisibilityController.nullToEmptyList(filters)) {
-      if (filter instanceof LinkNameFilter) {
-        String linkName = ((LinkNameFilter)filter).getLinkName();
-        if (linkNameFilters.containsKey(linkName) && linkNameFilters.get(linkName) != filter.isVisible()) {
-          throw new QuickLinksProfileEvaluationException(""Contradicting filters for link name ["" + linkName + ""]"");
-        }
-        linkNameFilters.put(linkName, filter.isVisible());
-      }
-      else if (filter instanceof LinkAttributeFilter) {
-        String linkAttribute = ((LinkAttributeFilter)filter).getLinkAttribute();
-        if (filter.isVisible()) {
-          showAttributes.add(linkAttribute);
-        }
-        else {
-          hideAttributes.add(linkAttribute);
-        }
-        if (showAttributes.contains(linkAttribute) && hideAttributes.contains(linkAttribute)) {
-          throw new QuickLinksProfileEvaluationException(""Contradicting filters for link attribute ["" + linkAttribute + ""]"");
-        }
-      }
-      // If none of the above, it is an accept-all filter. We expect only one of this type for an Evaluator
-      else {
-        if (acceptAllFilter.isPresent() && !acceptAllFilter.get().equals(filter.isVisible())) {
-          throw new QuickLinksProfileEvaluationException(""Contradicting accept-all filters."");
-        }
-        acceptAllFilter = Optional.of(filter.isVisible());
-      }
-    }
-  }
 
   /**
-   * @param quickLink the link to evaluate
-   * @return Three way evaluation result, which can be one of these:
-   *    show: Optional.of(true), hide: Optional.of(false), don't know: absent optional
+   * Simple value class encapsulating a link name an component name.
    */
-  Optional<Boolean> isVisible(Link quickLink) {
-    // process first priority filters based on link name
-    if (linkNameFilters.containsKey(quickLink.getName())) {
-      return Optional.of(linkNameFilters.get(quickLink.getName()));
+  static class ServiceComponent {","[{'comment': 'Class comment seems to conflict with code (""link name"" vs. ""service"").\r\n\r\nCan this class be replaced with `Pair`?', 'commenter': 'adoroszlai'}]"
2646,ambari-server/src/main/java/org/apache/ambari/server/state/quicklinksprofile/QuickLinksProfileParser.java,"@@ -91,20 +91,26 @@ public String encode(QuickLinksProfile profile) throws IOException {
   @Override
   public Filter deserialize (JsonParser parser, DeserializationContext context) throws IOException, JsonProcessingException {
     ObjectMapper mapper = (ObjectMapper) parser.getCodec();
-    ObjectNode root = (ObjectNode) mapper.readTree(parser);
+    ObjectNode root = mapper.readTree(parser);
     Class<? extends Filter> filterClass = null;
     List<String> invalidAttributes = new ArrayList<>();
-    for (String fieldName: ImmutableList.copyOf(root.getFieldNames())) {
+    for (String fieldName: ImmutableList.copyOf(root.fieldNames())) {
       switch(fieldName) {
         case LinkAttributeFilter.LINK_ATTRIBUTE:
           if (null != filterClass) {
-            throw new JsonParseException(PARSE_ERROR_MESSAGE_AMBIGUOUS_FILTER, parser.getCurrentLocation());
+            throw new JsonParseException(parser, PARSE_ERROR_MESSAGE_AMBIGUOUS_FILTER, parser.getCurrentLocation());
           }
           filterClass = LinkAttributeFilter.class;
           break;
         case LinkNameFilter.LINK_NAME:
-          if (null != filterClass) {
-            throw new JsonParseException(PARSE_ERROR_MESSAGE_AMBIGUOUS_FILTER, parser.getCurrentLocation());
+          if (null != filterClass && !filterClass.equals(LinkNameFilter.class)) {
+            throw new JsonParseException(parser, PARSE_ERROR_MESSAGE_AMBIGUOUS_FILTER, parser.getCurrentLocation());
+          }
+          filterClass = LinkNameFilter.class;
+          break;","[{'comment': '`LINK_NAME` and `LINK_URL` have identical branches.  Can you please add both labels to the same branch instead of duplicating the code?', 'commenter': 'adoroszlai'}]"
2646,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/QuickLinkArtifactResourceProvider.java,"@@ -206,13 +206,14 @@ public RequestStatus deleteResources(Request request, Predicate predicate) throw
    * @param serviceName the name of the service
    * @param serviceQuickLinks the links
    */
-  private void setVisibility(String serviceName, List<QuickLinksConfigurationInfo> serviceQuickLinks) {
+  private void setVisibilityAndOverrides(String serviceName, List<QuickLinksConfigurationInfo> serviceQuickLinks) {
     QuickLinkVisibilityController visibilityController = getManagementController().getQuicklinkVisibilityController();
 
     for(QuickLinksConfigurationInfo configurationInfo: serviceQuickLinks) {
       for (QuickLinks links: configurationInfo.getQuickLinksConfigurationMap().values()) {
         for(Link link: links.getQuickLinksConfiguration().getLinks()) {
           link.setVisible(visibilityController.isVisible(serviceName, link));
+          visibilityController.getUrlOverride(serviceName, link).ifPresent( url -> link.setUrl(url) );","[{'comment': '```suggestion\r\n          visibilityController.getUrlOverride(serviceName, link).ifPresent(link::setUrl);\r\n```', 'commenter': 'adoroszlai'}]"
2683,ambari-server/src/main/python/ambari-server.py,"@@ -600,11 +601,20 @@ def init_setup_sso_options(parser):
   parser.add_option('--ambari-admin-username', default=None, help=""Ambari administrator username for accessing Ambari's REST API"", dest=""ambari_admin_username"")
   parser.add_option('--ambari-admin-password', default=None, help=""Ambari administrator password for accessing Ambari's REST API"", dest=""ambari_admin_password"")
 
+
 @OsFamilyFuncImpl(OsFamilyImpl.DEFAULT)
 def init_pam_setup_parser_options(parser):
   parser.add_option('--pam-config-file', default=None, help=""Path to the PAM configuration file"", dest=""pam_config_file"")
   parser.add_option('--pam-auto-create-groups', default=None, help=""Automatically create groups for authenticated users [true/false]"", dest=""pam_auto_create_groups"")
 
+
+@OsFamilyFuncImpl(OsFamilyImpl.DEFAULT)
+def init_tproxy_setup_parser_options(parser):
+  parser.add_option('--ambari-admin-username', default=None, help=""Ambari administrator username for accessing Ambari's REST API"", dest=""ambari_admin_username"")
+  parser.add_option('--ambari-admin-password', default=None, help=""Ambari administrator password for accessing Ambari's REST API"", dest=""ambari_admin_password"")
+  parser.add_option('--tproxy-enabled', default=None, help=""Indicates whether to enable/disable Trusted Proxy Support"", dest=""tproxy_enabled"")
+","[{'comment': 'Maybe allow for one proxy user to be set up via the CLI args?\r\n- `--tproxy-proxy-user`\r\n- `--tproxy-proxyuser-hosts`\r\n- `--tproxy-proxyuser-users`\r\n- `--tproxy-proxyuser-groups`\r\n\r\nExample:\r\n```\r\nambari-server setup-trusted-proxy -ambari-admin-username=admin --tproxy-enabled=true --tproxy-proxy-user=knox --tproxy-proxyuser-hosts=""c7401.ambari.apache.org"" --tproxy-proxyuser-users=""*"" --tproxy-proxyuser-users=""users""\r\n```\r\n', 'commenter': 'rlevas'}, {'comment': ""I'm OK with this; but we may go further. What if we supported the following:\r\n\r\n```\r\n--tproxy-PROXYUSER-hosts\r\n--tproxy-PROXYUSER-users\r\n--tproxy-PROXYUSER-groups\r\n```\r\n\r\nAnd we would check if there is at least any of `proxy-X-[hosts|users|groups]` is defined, then we should have the rest (i.e `--proxy-X-hosts` and `proxy-X-groups` are defined but there is no `proxy-X-users` then we fail the operation).\r\n\r\nMoreover: if there is at least one full configuration of a proxyuser set using these options there should be no more proxy users asked in interactively\r\n\r\nThis way we allow end-users to define any number of proxy user configurations without user interaction (thus this tool would be 100% scriptable)."", 'commenter': 'smolnar82'}, {'comment': '@smolnar82 , that works for me. Your way is more flexible. :)', 'commenter': 'rlevas'}, {'comment': '@rlevas \r\n\r\nAs discussed over Zoom the above recommended approach does not really work due to CLI option limitations.  Instead the following CLI option will be defined: `--tproxy-configuration-file-path`; if this is provided a file must exists on the referenced path and must contain the Trusted Proxy configurations in JSON format.\r\n\r\nFor instance:\r\n```\r\n--tproxy-configuration-file-path=/my/path/tproxy.configs\r\n\r\n$ cat /my/path/tproxy.configs\r\n[\r\n  {\r\n    ""name""   : ""knox"",\r\n    ""hosts""  : ""host1"",\r\n    ""users""  : ""user1"",\r\n    ""groups"" : ""group1""\r\n  },\r\n  {\r\n    ""name""   : ""admin"",\r\n    ""hosts""  : ""host2"",\r\n    ""users""  : ""user2, user3"",\r\n    ""groups"" : ""group2""\r\n  }\r\n]\r\n```\r\n\r\n', 'commenter': 'smolnar82'}, {'comment': 'Nice... but maybe change `name` to `proxyuser`?', 'commenter': 'rlevas'}, {'comment': 'Sure', 'commenter': 'smolnar82'}]"
2683,ambari-server/sbin/ambari-server,"@@ -188,6 +188,10 @@ case ""${1:-}"" in
         echo -e ""Setting up SSO authentication properties...""
         $PYTHON ""$AMBARI_PYTHON_EXECUTABLE"" ""$@""
         ;;
+  setup-tproxy)","[{'comment': 'Change `setup-tproxy` to `setup-trusted-proxy`', 'commenter': 'rlevas'}, {'comment': 'ok', 'commenter': 'smolnar82'}]"
2683,ambari-server/sbin/ambari-server,"@@ -210,7 +214,7 @@ case ""${1:-}"" in
         ;;
   *)
         echo ""Usage: $AMBARI_EXECUTABLE
-        {start|stop|reset|restart|upgrade|status|upgradestack|setup|setup-jce|setup-ldap|sync-ldap|set-current|setup-security|refresh-stack-hash|backup|restore|update-host-names|check-database|enable-stack|setup-sso|db-purge-history|install-mpack|uninstall-mpack|upgrade-mpack|setup-kerberos|setup-pam|migrate-ldap-pam} [options]
+        {start|stop|reset|restart|upgrade|status|upgradestack|setup|setup-jce|setup-ldap|sync-ldap|set-current|setup-security|refresh-stack-hash|backup|restore|update-host-names|check-database|enable-stack|setup-sso|setup-tproxy|db-purge-history|install-mpack|uninstall-mpack|upgrade-mpack|setup-kerberos|setup-pam|migrate-ldap-pam} [options]","[{'comment': 'Change `setup-tproxy` to `setup-trusted-proxy`', 'commenter': 'rlevas'}]"
2683,ambari-server/src/main/python/ambari_server/setupActions.py,"@@ -50,3 +50,4 @@
 PAM_SETUP_ACTION = ""setup-pam""
 MIGRATE_LDAP_PAM_ACTION = ""migrate-ldap-pam""
 KERBEROS_SETUP_ACTION = ""setup-kerberos""
+SETUP_TPROXY_ACTION = ""setup-tproxy""","[{'comment': 'Change `setup-tproxy` to `setup-trusted-proxy`', 'commenter': 'rlevas'}]"
2683,ambari-server/src/main/python/ambari_server/setupTrustedProxy.py,"@@ -0,0 +1,154 @@
+#!/usr/bin/env python
+
+'''
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an ""AS IS"" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+'''
+
+import re
+import urllib2
+
+from ambari_commons.exceptions import FatalException, NonFatalException
+from ambari_commons.logging_utils import get_silent, print_info_msg
+from ambari_server.serverConfiguration import get_ambari_properties
+from ambari_server.serverUtils import is_server_runing, get_ambari_admin_username_password_pair, \
+  get_cluster_name, perform_changes_via_rest_api, get_json_via_rest_api, get_value_from_dictionary
+from ambari_server.setupSecurity import REGEX_TRUE_FALSE
+from ambari_server.userInput import get_validated_string_input, get_YN_input
+
+
+PROXYUSER_PLACEHOLDER = ""$PROXY_USER""
+TPROXY_SUPPORT_ENABLED = ""ambari.tproxy.authentication.enabled""
+PROXYUSER_HOSTS = ""ambari.tproxy.proxyuser."" + PROXYUSER_PLACEHOLDER + "".hosts""
+PROXYUSER_USERS = ""ambari.tproxy.proxyuser."" + PROXYUSER_PLACEHOLDER + "".users""
+PROXYUSER_GROUPS = ""ambari.tproxy.proxyuser."" + PROXYUSER_PLACEHOLDER + "".groups""
+
+TPROXY_CONFIG_API_ENTRYPOINT = 'services/AMBARI/components/AMBARI_SERVER/configurations/tproxy-configuration'
+
+REGEX_ANYTHING = "".*""
+WILDCARD_FOR_ALL = ""*""
+
+
+def get_trusted_proxy_properties(ambari_properties, admin_login, admin_password):
+  print_info_msg(""Fetching Trusted Proxy configuration from DB"")
+
+  try:
+    response_code, json_data = get_json_via_rest_api(ambari_properties, admin_login, admin_password, TPROXY_CONFIG_API_ENTRYPOINT)
+  except urllib2.HTTPError as http_error:
+    if http_error.code == 404:
+      # This means that there is no Trusted Proxy configuration in the database yet -> we can not fetch the property (but this is NOT an error)
+      json_data = None
+    else:
+      raise http_error
+
+  if json_data and 'Configuration' in json_data and 'properties' in json_data['Configuration']:
+    return json_data['Configuration']['properties']
+  else:
+    return {}
+
+
+def populate_tproxy_configuration_property(properties, tproxy_user_name, property_name, question_text_qualifier):
+  resolved_property_name = property_name.replace(PROXYUSER_PLACEHOLDER, tproxy_user_name)
+  resolved_property_value = get_value_from_dictionary(properties, resolved_property_name, WILDCARD_FOR_ALL)
+  resolved_property_value = get_validated_string_input(""Allowed {0} for {1} ({2})? "".format(question_text_qualifier, tproxy_user_name, resolved_property_value), resolved_property_value, REGEX_ANYTHING, ""Invalid input"", False)
+  properties[resolved_property_name] = resolved_property_value
+
+
+def add_new_trusted_proxy_config(properties):
+  tproxy_user_name = get_validated_string_input(""The proxy user's (local) username? "", None, REGEX_ANYTHING, ""Invalid Trusted Proxy User Name"", False, allowEmpty=False)
+  populate_tproxy_configuration_property(properties, tproxy_user_name, PROXYUSER_HOSTS, ""hosts"")
+  populate_tproxy_configuration_property(properties, tproxy_user_name, PROXYUSER_USERS, ""users"")
+  populate_tproxy_configuration_property(properties, tproxy_user_name, PROXYUSER_GROUPS, ""groups"")
+  return get_YN_input(""Add another proxy user [y/n] (n)? "", False)
+
+
+def update_tproxy_conf(ambari_properties, tproxy_configuration_properties, admin_login, admin_password):
+  request_data = {
+    ""Configuration"": {
+      ""category"": ""tproxy-configuration"",
+      ""properties"": {
+      }
+    }
+  }
+  request_data['Configuration']['properties'] = tproxy_configuration_properties
+  perform_changes_via_rest_api(ambari_properties, admin_login, admin_password, TPROXY_CONFIG_API_ENTRYPOINT, 'PUT', request_data)
+
+
+def remove_tproxy_conf(ambari_properties, admin_login, admin_password):
+  perform_changes_via_rest_api(ambari_properties, admin_login, admin_password, TPROXY_CONFIG_API_ENTRYPOINT, 'DELETE')
+
+
+def validate_options(options):
+  errors = []
+  if options.tproxy_enabled and not re.match(REGEX_TRUE_FALSE, options.tproxy_enabled):
+    errors.append(""--tproxy-enabled should be to either 'true' or 'false'"")
+
+  if len(errors) > 0:
+    error_msg = ""The following errors occurred while processing your request: {0}""
+    raise FatalException(1, error_msg.format(str(errors)))
+
+
+def setup_trusted_proxy(options):
+  print_info_msg(""Setup Trusted Proxy"")
+
+  server_status, pid = is_server_runing()
+  if not server_status:
+    err = 'Ambari Server is not running.'
+    raise FatalException(1, err)
+
+  if not get_silent():
+    validate_options(options)
+
+    ambari_properties = get_ambari_properties()
+
+    admin_login, admin_password = get_ambari_admin_username_password_pair(options)
+    properties = get_trusted_proxy_properties(ambari_properties, admin_login, admin_password)
+
+    if not options.tproxy_enabled:
+      tproxy_support_enabled = get_value_from_dictionary(properties, TPROXY_SUPPORT_ENABLED)
+
+      if tproxy_support_enabled:
+        if tproxy_support_enabled and 'true' == tproxy_support_enabled:
+          tproxy_status = ""enabled""
+        else:
+          tproxy_status = ""disabled""
+      else:
+        tproxy_status = ""not configured""
+      print_info_msg(""\nTrusted Proxy support is currently %s\n"" % tproxy_status)
+
+      if tproxy_status == ""enabled"":
+        enable_tproxy = not get_YN_input(""Do you want to disable Trusted Proxy support [y/n] (n)? "", False)
+      elif get_YN_input(""Do you want to configure Trusted Proxy Support [y/n] (y)? "", True):
+        enable_tproxy = True
+      else:
+        return False
+    else:
+      enable_tproxy = options.tproxy_enabled == 'true'
+
+    if enable_tproxy:
+      properties[TPROXY_SUPPORT_ENABLED] = ""true""
+      add_new_trusted_proxy = add_new_trusted_proxy_config(properties)
+      while add_new_trusted_proxy:
+        add_new_trusted_proxy = add_new_trusted_proxy_config(properties)
+
+      update_tproxy_conf(ambari_properties, properties, admin_login, admin_password)
+    else:
+      remove_tproxy_conf(ambari_properties, admin_login, admin_password)
+
+  else:
+    warning = ""setup-tproxy is not enabled in silent mode.""","[{'comment': 'Change `setup-tproxy` to `setup-trusted-proxy`', 'commenter': 'rlevas'}]"
2683,ambari-server/src/test/python/TestSetupTrustedProxy.py,"@@ -0,0 +1,230 @@
+'''
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an ""AS IS"" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+'''
+
+import os
+import platform
+import sys
+import unittest
+import StringIO
+
+from mock.mock import patch, MagicMock
+
+from only_for_platform import os_distro_value
+from ambari_commons import os_utils
+from urllib2 import HTTPError
+
+import shutil
+
+# Mock classes for reading from a file
+class MagicFile(object):
+  def __init__(self, data):
+    self.data = data
+
+  def read(self):
+    return self.data
+
+  def __exit__(self, exc_type, exc_val, exc_tb):
+    pass
+
+  def __enter__(self):
+    return self
+pass
+
+project_dir = os.path.join(os.path.abspath(os.path.dirname(__file__)),os.path.normpath(""../../../../""))
+shutil.copyfile(project_dir+""/ambari-server/conf/unix/ambari.properties"", ""/tmp/ambari.properties"")
+
+# We have to use this import HACK because the filename contains a dash
+_search_file = os_utils.search_file
+
+def search_file_proxy(filename, searchpatch, pathsep=os.pathsep):
+  global _search_file
+  if ""ambari.properties"" in filename:
+    return ""/tmp/ambari.properties""
+  return _search_file(filename, searchpatch, pathsep)
+
+os_utils.search_file = search_file_proxy
+
+with patch.object(platform, ""linux_distribution"", return_value = MagicMock(return_value=('Redhat', '7.4', 'Final'))):
+  with patch(""os.path.isdir"", return_value = MagicMock(return_value=True)):
+    with patch(""os.access"", return_value = MagicMock(return_value=True)):
+      with patch.object(os_utils, ""parse_log4j_file"", return_value={'ambari.log.dir': '/var/log/ambari-server'}):
+        with patch(""platform.linux_distribution"", return_value = os_distro_value):
+          with patch(""os.symlink""):
+            with patch(""glob.glob"", return_value = ['/etc/init.d/postgresql-9.3']):
+              _ambari_server_ = __import__('ambari-server')
+              with patch(""__builtin__.open""):
+                from ambari_commons.exceptions import FatalException, NonFatalException
+                from ambari_server.properties import Properties
+                from ambari_server.setupTrustedProxy import setup_trusted_proxy, PROXYUSER_PLACEHOLDER, TPROXY_SUPPORT_ENABLED, PROXYUSER_HOSTS, PROXYUSER_USERS, PROXYUSER_GROUPS
+
+class TestSetupTrustedProxy(unittest.TestCase):
+
+  @patch(""ambari_server.setupTrustedProxy.is_server_runing"")
+  def test_tproxy_setup_should_fail_if_server_is_not_running(self, is_server_runing_mock):
+    out = StringIO.StringIO()
+    sys.stdout = out
+
+    is_server_runing_mock.return_value = (False, 0)
+    options = self._create_empty_options_mock()
+
+    try:
+      setup_trusted_proxy(options)
+      self.fail(""Should fail with non-fatal exception"")
+    except FatalException as e:
+      self.assertTrue(""Ambari Server is not running"" in e.reason)
+      pass
+
+    sys.stdout = sys.__stdout__
+    pass
+
+
+  @patch(""ambari_server.setupTrustedProxy.get_silent"")
+  @patch(""ambari_server.setupTrustedProxy.is_server_runing"")
+  def test_silent_mode_is_not_allowed(self, is_server_runing_mock, get_silent_mock):
+    out = StringIO.StringIO()
+    sys.stdout = out
+
+    is_server_runing_mock.return_value = (True, 0)
+    get_silent_mock.return_value = True
+    options = self._create_empty_options_mock()
+
+    try:
+      setup_trusted_proxy(options)
+      self.fail(""Should fail with fatal exception"")
+    except NonFatalException as e:
+      self.assertTrue(""setup-tproxy is not enabled in silent mode."" in e.reason)","[{'comment': 'Change `setup-tproxy` to `setup-trusted-proxy`', 'commenter': 'rlevas'}]"
2683,ambari-server/src/main/python/ambari_server/setupTrustedProxy.py,"@@ -0,0 +1,154 @@
+#!/usr/bin/env python
+
+'''
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an ""AS IS"" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+'''
+
+import re
+import urllib2
+
+from ambari_commons.exceptions import FatalException, NonFatalException
+from ambari_commons.logging_utils import get_silent, print_info_msg
+from ambari_server.serverConfiguration import get_ambari_properties
+from ambari_server.serverUtils import is_server_runing, get_ambari_admin_username_password_pair, \
+  get_cluster_name, perform_changes_via_rest_api, get_json_via_rest_api, get_value_from_dictionary
+from ambari_server.setupSecurity import REGEX_TRUE_FALSE
+from ambari_server.userInput import get_validated_string_input, get_YN_input
+
+
+PROXYUSER_PLACEHOLDER = ""$PROXY_USER""
+TPROXY_SUPPORT_ENABLED = ""ambari.tproxy.authentication.enabled""
+PROXYUSER_HOSTS = ""ambari.tproxy.proxyuser."" + PROXYUSER_PLACEHOLDER + "".hosts""
+PROXYUSER_USERS = ""ambari.tproxy.proxyuser."" + PROXYUSER_PLACEHOLDER + "".users""
+PROXYUSER_GROUPS = ""ambari.tproxy.proxyuser."" + PROXYUSER_PLACEHOLDER + "".groups""
+
+TPROXY_CONFIG_API_ENTRYPOINT = 'services/AMBARI/components/AMBARI_SERVER/configurations/tproxy-configuration'
+
+REGEX_ANYTHING = "".*""
+WILDCARD_FOR_ALL = ""*""
+
+
+def get_trusted_proxy_properties(ambari_properties, admin_login, admin_password):
+  print_info_msg(""Fetching Trusted Proxy configuration from DB"")
+
+  try:
+    response_code, json_data = get_json_via_rest_api(ambari_properties, admin_login, admin_password, TPROXY_CONFIG_API_ENTRYPOINT)
+  except urllib2.HTTPError as http_error:
+    if http_error.code == 404:
+      # This means that there is no Trusted Proxy configuration in the database yet -> we can not fetch the property (but this is NOT an error)
+      json_data = None
+    else:
+      raise http_error
+
+  if json_data and 'Configuration' in json_data and 'properties' in json_data['Configuration']:","[{'comment': ""Consider using a_dict.get(key, default) instead of the _in_ checks \r\n\r\nFor example: \r\n\r\n```python\r\njson_data.get('Configuration', {}).get('properties', {})\r\n```\r\n"", 'commenter': 'zeroflag'}]"
2683,ambari-server/src/main/python/ambari_server/setupTrustedProxy.py,"@@ -0,0 +1,154 @@
+#!/usr/bin/env python
+
+'''
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an ""AS IS"" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+'''
+
+import re
+import urllib2
+
+from ambari_commons.exceptions import FatalException, NonFatalException
+from ambari_commons.logging_utils import get_silent, print_info_msg
+from ambari_server.serverConfiguration import get_ambari_properties
+from ambari_server.serverUtils import is_server_runing, get_ambari_admin_username_password_pair, \
+  get_cluster_name, perform_changes_via_rest_api, get_json_via_rest_api, get_value_from_dictionary
+from ambari_server.setupSecurity import REGEX_TRUE_FALSE
+from ambari_server.userInput import get_validated_string_input, get_YN_input
+
+
+PROXYUSER_PLACEHOLDER = ""$PROXY_USER""","[{'comment': 'If the placeholder should be always replaced in the end I would use format strings (either with %s or {some_name} + .format(..) ) here.', 'commenter': 'zeroflag'}]"
2683,ambari-server/src/main/python/ambari_server/setupTrustedProxy.py,"@@ -0,0 +1,154 @@
+#!/usr/bin/env python
+
+'''
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an ""AS IS"" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+'''
+
+import re
+import urllib2
+
+from ambari_commons.exceptions import FatalException, NonFatalException
+from ambari_commons.logging_utils import get_silent, print_info_msg
+from ambari_server.serverConfiguration import get_ambari_properties
+from ambari_server.serverUtils import is_server_runing, get_ambari_admin_username_password_pair, \
+  get_cluster_name, perform_changes_via_rest_api, get_json_via_rest_api, get_value_from_dictionary
+from ambari_server.setupSecurity import REGEX_TRUE_FALSE
+from ambari_server.userInput import get_validated_string_input, get_YN_input
+
+
+PROXYUSER_PLACEHOLDER = ""$PROXY_USER""
+TPROXY_SUPPORT_ENABLED = ""ambari.tproxy.authentication.enabled""
+PROXYUSER_HOSTS = ""ambari.tproxy.proxyuser."" + PROXYUSER_PLACEHOLDER + "".hosts""
+PROXYUSER_USERS = ""ambari.tproxy.proxyuser."" + PROXYUSER_PLACEHOLDER + "".users""
+PROXYUSER_GROUPS = ""ambari.tproxy.proxyuser."" + PROXYUSER_PLACEHOLDER + "".groups""
+
+TPROXY_CONFIG_API_ENTRYPOINT = 'services/AMBARI/components/AMBARI_SERVER/configurations/tproxy-configuration'
+
+REGEX_ANYTHING = "".*""
+WILDCARD_FOR_ALL = ""*""
+
+
+def get_trusted_proxy_properties(ambari_properties, admin_login, admin_password):
+  print_info_msg(""Fetching Trusted Proxy configuration from DB"")
+
+  try:
+    response_code, json_data = get_json_via_rest_api(ambari_properties, admin_login, admin_password, TPROXY_CONFIG_API_ENTRYPOINT)
+  except urllib2.HTTPError as http_error:
+    if http_error.code == 404:
+      # This means that there is no Trusted Proxy configuration in the database yet -> we can not fetch the property (but this is NOT an error)
+      json_data = None
+    else:
+      raise http_error
+
+  if json_data and 'Configuration' in json_data and 'properties' in json_data['Configuration']:
+    return json_data['Configuration']['properties']
+  else:
+    return {}
+
+
+def populate_tproxy_configuration_property(properties, tproxy_user_name, property_name, question_text_qualifier):
+  resolved_property_name = property_name.replace(PROXYUSER_PLACEHOLDER, tproxy_user_name)
+  resolved_property_value = get_value_from_dictionary(properties, resolved_property_name, WILDCARD_FOR_ALL)
+  resolved_property_value = get_validated_string_input(""Allowed {0} for {1} ({2})? "".format(question_text_qualifier, tproxy_user_name, resolved_property_value), resolved_property_value, REGEX_ANYTHING, ""Invalid input"", False)
+  properties[resolved_property_name] = resolved_property_value
+
+
+def add_new_trusted_proxy_config(properties):
+  tproxy_user_name = get_validated_string_input(""The proxy user's (local) username? "", None, REGEX_ANYTHING, ""Invalid Trusted Proxy User Name"", False, allowEmpty=False)
+  populate_tproxy_configuration_property(properties, tproxy_user_name, PROXYUSER_HOSTS, ""hosts"")
+  populate_tproxy_configuration_property(properties, tproxy_user_name, PROXYUSER_USERS, ""users"")
+  populate_tproxy_configuration_property(properties, tproxy_user_name, PROXYUSER_GROUPS, ""groups"")
+  return get_YN_input(""Add another proxy user [y/n] (n)? "", False)
+
+
+def update_tproxy_conf(ambari_properties, tproxy_configuration_properties, admin_login, admin_password):
+  request_data = {
+    ""Configuration"": {
+      ""category"": ""tproxy-configuration"",
+      ""properties"": {
+      }
+    }
+  }
+  request_data['Configuration']['properties'] = tproxy_configuration_properties
+  perform_changes_via_rest_api(ambari_properties, admin_login, admin_password, TPROXY_CONFIG_API_ENTRYPOINT, 'PUT', request_data)
+
+
+def remove_tproxy_conf(ambari_properties, admin_login, admin_password):
+  perform_changes_via_rest_api(ambari_properties, admin_login, admin_password, TPROXY_CONFIG_API_ENTRYPOINT, 'DELETE')
+
+
+def validate_options(options):
+  errors = []
+  if options.tproxy_enabled and not re.match(REGEX_TRUE_FALSE, options.tproxy_enabled):
+    errors.append(""--tproxy-enabled should be to either 'true' or 'false'"")
+
+  if len(errors) > 0:
+    error_msg = ""The following errors occurred while processing your request: {0}""
+    raise FatalException(1, error_msg.format(str(errors)))
+
+
+def setup_trusted_proxy(options):
+  print_info_msg(""Setup Trusted Proxy"")
+
+  server_status, pid = is_server_runing()
+  if not server_status:
+    err = 'Ambari Server is not running.'
+    raise FatalException(1, err)
+
+  if not get_silent():
+    validate_options(options)
+
+    ambari_properties = get_ambari_properties()
+
+    admin_login, admin_password = get_ambari_admin_username_password_pair(options)
+    properties = get_trusted_proxy_properties(ambari_properties, admin_login, admin_password)
+
+    if not options.tproxy_enabled:
+      tproxy_support_enabled = get_value_from_dictionary(properties, TPROXY_SUPPORT_ENABLED)
+
+      if tproxy_support_enabled:
+        if tproxy_support_enabled and 'true' == tproxy_support_enabled:","[{'comment': 'why do we need to check the tproxy_support_enabled once again here?', 'commenter': 'zeroflag'}, {'comment': 'At this stage `tproxy_support_enabled` is a String and can be undefined.', 'commenter': 'smolnar82'}, {'comment': ""But we're checking in line 123 and in line 124 too.\r\n\r\n```python\r\n  if tproxy_support_enabled:\r\n    if tproxy_support_enabled and ..:\r\n```"", 'commenter': 'zeroflag'}]"
2683,ambari-server/src/main/python/ambari_server/setupTrustedProxy.py,"@@ -0,0 +1,154 @@
+#!/usr/bin/env python
+
+'''
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an ""AS IS"" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+'''
+
+import re
+import urllib2
+
+from ambari_commons.exceptions import FatalException, NonFatalException
+from ambari_commons.logging_utils import get_silent, print_info_msg
+from ambari_server.serverConfiguration import get_ambari_properties
+from ambari_server.serverUtils import is_server_runing, get_ambari_admin_username_password_pair, \
+  get_cluster_name, perform_changes_via_rest_api, get_json_via_rest_api, get_value_from_dictionary
+from ambari_server.setupSecurity import REGEX_TRUE_FALSE
+from ambari_server.userInput import get_validated_string_input, get_YN_input
+
+
+PROXYUSER_PLACEHOLDER = ""$PROXY_USER""
+TPROXY_SUPPORT_ENABLED = ""ambari.tproxy.authentication.enabled""
+PROXYUSER_HOSTS = ""ambari.tproxy.proxyuser."" + PROXYUSER_PLACEHOLDER + "".hosts""
+PROXYUSER_USERS = ""ambari.tproxy.proxyuser."" + PROXYUSER_PLACEHOLDER + "".users""
+PROXYUSER_GROUPS = ""ambari.tproxy.proxyuser."" + PROXYUSER_PLACEHOLDER + "".groups""
+
+TPROXY_CONFIG_API_ENTRYPOINT = 'services/AMBARI/components/AMBARI_SERVER/configurations/tproxy-configuration'
+
+REGEX_ANYTHING = "".*""
+WILDCARD_FOR_ALL = ""*""
+
+
+def get_trusted_proxy_properties(ambari_properties, admin_login, admin_password):
+  print_info_msg(""Fetching Trusted Proxy configuration from DB"")
+
+  try:
+    response_code, json_data = get_json_via_rest_api(ambari_properties, admin_login, admin_password, TPROXY_CONFIG_API_ENTRYPOINT)
+  except urllib2.HTTPError as http_error:
+    if http_error.code == 404:","[{'comment': 'how about using named constants? \r\nSee that table after `and also the following constants for integer status codes:` at https://docs.python.org/2/library/httplib.html', 'commenter': 'Unknown'}, {'comment': 'Thanks, I was not aware of that module; will replace it', 'commenter': 'smolnar82'}]"
2683,ambari-server/src/main/python/ambari-server.py,"@@ -600,11 +602,25 @@ def init_setup_sso_options(parser):
   parser.add_option('--ambari-admin-username', default=None, help=""Ambari administrator username for accessing Ambari's REST API"", dest=""ambari_admin_username"")
   parser.add_option('--ambari-admin-password', default=None, help=""Ambari administrator password for accessing Ambari's REST API"", dest=""ambari_admin_password"")
 
+
 @OsFamilyFuncImpl(OsFamilyImpl.DEFAULT)
 def init_pam_setup_parser_options(parser):
   parser.add_option('--pam-config-file', default=None, help=""Path to the PAM configuration file"", dest=""pam_config_file"")
   parser.add_option('--pam-auto-create-groups', default=None, help=""Automatically create groups for authenticated users [true/false]"", dest=""pam_auto_create_groups"")
 
+
+@OsFamilyFuncImpl(OsFamilyImpl.DEFAULT)
+def init_tproxy_setup_parser_options(parser):
+  parser.add_option('--ambari-admin-username', default=None, help=""Ambari administrator username for accessing Ambari's REST API"", dest=""ambari_admin_username"")
+  parser.add_option('--ambari-admin-password', default=None, help=""Ambari administrator password for accessing Ambari's REST API"", dest=""ambari_admin_password"")
+  parser.add_option('--tproxy-enabled', default=None, help=""Indicates whether to enable/disable Trusted Proxy Support"", dest=""tproxy_enabled"")
+  parser.add_option('--tproxy-configuration-file-path', default=None,
+                    help=""The path where the Trusted Proxy configuration is located. The content is expected to be in JSON format."" \
+                    ""Sample configuration:[{\""name\"": \""knox\"", \""hosts\"": \""host1\"", \""users\"": \""user1, user2\"", \""groups\"": \""group1\""}]"",","[{'comment': 'Sample configuration reads ""name"" rather than ""proxyuser"".', 'commenter': 'rlevas'}, {'comment': 'Done', 'commenter': 'smolnar82'}]"
2694,ambari-server/src/main/resources/Ambari-DDL-Derby-CREATE.sql,"@@ -106,7 +106,7 @@ CREATE TABLE clusterconfig (
 CREATE TABLE ambari_configuration (
   category_name VARCHAR(100) NOT NULL,
   property_name VARCHAR(100) NOT NULL,
-  property_value VARCHAR(2048),
+  property_value VARCHAR(4000) NOT NULL,","[{'comment': 'If we are going with the `TEXT` type for Postgres, would it be better to go the `TEXT` route for all DB flavors to keep things consistent?', 'commenter': 'rlevas'}, {'comment': ""Makes sense. This would mean:\r\n- CLOB for Oracle\r\n- LONGTEXT for MySQL/MariaDB\r\n\r\nI'm not sure if we need to do this for unsupported DBs at all..."", 'commenter': 'smolnar82'}, {'comment': ""However, given that we need to update the upgrade class too I'd change it like set the length in Postgres to 4k too.\r\n\r\nAny objection?"", 'commenter': 'smolnar82'}, {'comment': 'Nope.. no objection. ', 'commenter': 'rlevas'}, {'comment': 'Done', 'commenter': 'smolnar82'}, {'comment': 'The field is set to nullable in the upgrade catalog, but not null in the table definitions.', 'commenter': 'adoroszlai'}, {'comment': 'Thanks.', 'commenter': 'smolnar82'}, {'comment': 'Fixed', 'commenter': 'smolnar82'}, {'comment': 'Ignore this; I made the change is UC270', 'commenter': 'smolnar82'}, {'comment': 'Fixed now', 'commenter': 'smolnar82'}]"
2694,ambari-server/src/main/java/org/apache/ambari/server/upgrade/UpgradeCatalog270.java,"@@ -975,7 +975,7 @@ protected void addAmbariConfigurationTable() throws SQLException {
     List<DBAccessor.DBColumnInfo> columns = new ArrayList<>();
     columns.add(new DBAccessor.DBColumnInfo(AMBARI_CONFIGURATION_CATEGORY_NAME_COLUMN, String.class, 100, null, false));
     columns.add(new DBAccessor.DBColumnInfo(AMBARI_CONFIGURATION_PROPERTY_NAME_COLUMN, String.class, 100, null, false));
-    columns.add(new DBAccessor.DBColumnInfo(AMBARI_CONFIGURATION_PROPERTY_VALUE_COLUMN, String.class, 2048, null, true));
+    columns.add(new DBAccessor.DBColumnInfo(AMBARI_CONFIGURATION_PROPERTY_VALUE_COLUMN, String.class, 4000, null, true));","[{'comment': 'I think we need a column definition update in `UpgradeCatalog280`, otherwise installations already on 2.7.0+ will not be changed when upgrading to 2.8.', 'commenter': 'adoroszlai'}, {'comment': ""+1 for making the change in `UpgradeCatalog280`.  There is no need to do it in 'UpgradeCatalog270`."", 'commenter': 'rlevas'}, {'comment': 'Fixed', 'commenter': 'smolnar82'}]"
2704,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/UnitUpdater.java,"@@ -65,6 +66,36 @@ public static String updateForClusterCreate(Stack stack, String serviceName, Str
     }
   }
 
+  public static void updateUnits(Configuration configuration, Stack stack) {
+    for (UnitValidatedProperty p : UnitValidatedProperty.ALL) {
+      if (configuration.isPropertySet(p.getConfigType(), p.getPropertyName())) {
+        String value = configuration.getPropertyValue(p.getConfigType(), p.getPropertyName());
+        String updatedValue = updateForClusterCreate(stack, p.getServiceName(), p.getConfigType(), p.getPropertyName(), value);
+        configuration.setProperty(p.getConfigType(), p.getPropertyName(), updatedValue);
+      }
+    }
+  }
+
+  public static void removeUnits(Configuration configuration) {
+    for (UnitValidatedProperty p : UnitValidatedProperty.ALL) {
+      if (configuration.isPropertySet(p.getConfigType(), p.getPropertyName())) {
+        String value = configuration.getPropertyValue(p.getConfigType(), p.getPropertyName());
+        String updatedValue = removeUnit(value);
+        configuration.setProperty(p.getConfigType(), p.getPropertyName(), updatedValue);","[{'comment': 'probably should add an if hasUnits() check before updating.', 'commenter': 'benyoka'}]"
2704,ambari-server/src/main/java/org/apache/ambari/server/topology/addservice/AddServiceInfo.java,"@@ -85,8 +100,12 @@ public Configuration getConfig() {
     return config;
   }
 
+  public Optional<LayoutRecommendationInfo> getRecommendationInfo() {
+    return recommendationInfo;
+  }
+
   /**
-   * Creates a descriptive label to be displayed in the UI.
+   * Creates a descriptive label to be displayed in the UI.[","[{'comment': 'Remove added character', 'commenter': 'benyoka'}]"
2704,ambari-server/src/test/java/org/apache/ambari/server/testutils/TestCollectionUtils.java,"@@ -0,0 +1,50 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.testutils;
+
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.Map;
+
+/**
+ * Utilities for collections used in unit tests.
+ */
+public class TestCollectionUtils {
+
+  /**
+   * A simple (but not production quality) way to create mutable hashmaps for unit tests
+   * @param firstKey the first key in the map
+   * @param firstValue the first value in the map
+   * @param others further keys and values
+   * @param <K> key type
+   * @param <V> value type
+   * @return the map
+   */
+  @SuppressWarnings(""unchecked"")
+  public static <K, V> Map<K, V> map(K firstKey, V firstValue, Object... others) {
+    Map<K, V> map = new HashMap<>();
+    map.put(firstKey, firstValue);
+    Iterator iterator = Arrays.asList(others).iterator();
+    while (iterator.hasNext()) {
+      map.put((K)iterator.next(), (V)iterator.next());","[{'comment': 'How about using `new HashMap<>(ImmutableMap.builder().put(...).putAll(...).etc().build())` instead of adding this method?', 'commenter': 'adoroszlai'}, {'comment': 'It is fine for simple maps, however, becomes difficult to read when nested maps are created.\r\n\r\nE.g. compare the readability of this:\r\n\r\n```java\r\nmap(\r\n  ""oozie-env"", \r\n  map(\r\n    ""oozie_heapsize"", ""1024"",\r\n    ""oozie_permsize"", ""256""),\r\n  ""core-site"", \r\n  map(\r\n    ""fs.trash.interval"", ""360""));\r\n```\r\n\r\nwith this:\r\n\r\n```java\r\nnew HashMap<>(ImmutableMap.of(\r\n  ""oozie-env"", \r\n  new HashMap<>(ImmutableMap.of((\r\n    ""oozie_heapsize"", ""1024"",\r\n    ""oozie_permsize"", ""256"")),\r\n  ""core-site"", \r\n  new HashMap<>(ImmutableMap.of((\r\n    ""fs.trash.interval"", ""360""))));\r\n```\r\n', 'commenter': 'benyoka'}]"
2704,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/UnitUpdater.java,"@@ -65,6 +66,36 @@ public static String updateForClusterCreate(Stack stack, String serviceName, Str
     }
   }
 
+  public static void updateUnits(Configuration configuration, Stack stack) {
+    for (UnitValidatedProperty p : UnitValidatedProperty.ALL) {
+      if (configuration.isPropertySet(p.getConfigType(), p.getPropertyName())) {
+        String value = configuration.getPropertyValue(p.getConfigType(), p.getPropertyName());
+        String updatedValue = updateForClusterCreate(stack, p.getServiceName(), p.getConfigType(), p.getPropertyName(), value);
+        configuration.setProperty(p.getConfigType(), p.getPropertyName(), updatedValue);
+      }
+    }
+  }
+
+  public static void removeUnits(Configuration configuration) {","[{'comment': 'This method is almost exactly the same as `updateUnits`, except the call that updates the value.  There must be a way to extract the common stuff.', 'commenter': 'adoroszlai'}]"
2704,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/UnitUpdater.java,"@@ -65,6 +66,36 @@ public static String updateForClusterCreate(Stack stack, String serviceName, Str
     }
   }
 
+  public static void updateUnits(Configuration configuration, Stack stack) {
+    for (UnitValidatedProperty p : UnitValidatedProperty.ALL) {
+      if (configuration.isPropertySet(p.getConfigType(), p.getPropertyName())) {
+        String value = configuration.getPropertyValue(p.getConfigType(), p.getPropertyName());
+        String updatedValue = updateForClusterCreate(stack, p.getServiceName(), p.getConfigType(), p.getPropertyName(), value);
+        configuration.setProperty(p.getConfigType(), p.getPropertyName(), updatedValue);
+      }
+    }
+  }
+
+  public static void removeUnits(Configuration configuration) {
+    for (UnitValidatedProperty p : UnitValidatedProperty.ALL) {
+      if (configuration.isPropertySet(p.getConfigType(), p.getPropertyName())) {
+        String value = configuration.getPropertyValue(p.getConfigType(), p.getPropertyName());
+        String updatedValue = removeUnit(value);
+        configuration.setProperty(p.getConfigType(), p.getPropertyName(), updatedValue);
+      }
+    }
+  }
+
+  public static String removeUnit(String origValue) {","[{'comment': '`updateForBlueprintExport` already implements unit removal, can you please try to reuse that?', 'commenter': 'adoroszlai'}]"
2704,ambari-server/src/main/java/org/apache/ambari/server/topology/addservice/RequestValidator.java,"@@ -194,9 +195,12 @@ void validateConfiguration() {
     }
 
     Configuration clusterConfig = getClusterDesiredConfigs();
-    clusterConfig.setParentConfiguration(state.getStack().getValidDefaultConfig());
+    if (request.getRecommendationStrategy().shouldUseStackDefaults()) {
+      clusterConfig.setParentConfiguration(state.getStack().getDefaultConfig());","[{'comment': 'Stack defaults should be used even for `NEVER_APPLY`.  This is how it works for blueprints, too.  Otherwise one has to specify every config property explicitly.\r\n\r\nThe recommendation strategy is for stack advisor, not stack defaults.', 'commenter': 'adoroszlai'}, {'comment': ""Agreed, I've also commented on this above.  "", 'commenter': 'rnettleton'}]"
2704,ambari-server/src/main/java/org/apache/ambari/server/state/ValueAttributesInfo.java,"@@ -396,6 +400,19 @@ public int hashCode() {
     return result;
   }
 
+  public Map<String, String> toMap(Optional<ObjectMapper> mapper) {
+    Map<String, String> map =
+      mapper.orElseGet(() -> new ObjectMapper()).convertValue(this, new TypeReference<Map<String, String>>(){});","[{'comment': '```suggestion\r\n      mapper.orElseGet(ObjectMapper::new).convertValue(this, new TypeReference<Map<String, String>>(){});\r\n```', 'commenter': 'adoroszlai'}]"
2704,ambari-server/src/main/java/org/apache/ambari/server/state/ValueAttributesInfo.java,"@@ -396,6 +400,19 @@ public int hashCode() {
     return result;
   }
 
+  public Map<String, String> toMap(Optional<ObjectMapper> mapper) {
+    Map<String, String> map =
+      mapper.orElseGet(() -> new ObjectMapper()).convertValue(this, new TypeReference<Map<String, String>>(){});
+    if ( !Boolean.parseBoolean(map.get(""keyStore"")) ) { // keyStore is declared as a primitive value instead of Boolean -> treat false as unset
+      map.remove(""keyStore"");
+    }
+    return map;
+  }
+
+  public static ValueAttributesInfo fromMap(Map<String, String> attributes, Optional<ObjectMapper> mapper) {
+    return mapper.orElseGet(() -> new ObjectMapper()).convertValue(attributes, ValueAttributesInfo.class);","[{'comment': '```suggestion\r\n    return mapper.orElseGet(ObjectMapper::new).convertValue(attributes, ValueAttributesInfo.class);\r\n```', 'commenter': 'adoroszlai'}]"
2704,ambari-server/src/main/java/org/apache/ambari/server/topology/addservice/AddServiceInfo.java,"@@ -38,18 +39,32 @@
   private final Map<String, Map<String, Set<String>>> newServices;
   private final RequestStageContainer stages;
   private final Configuration config;
-
-  public AddServiceInfo(AddServiceRequest request, String clusterName, Stack stack, Configuration config, RequestStageContainer stages, Map<String, Map<String, Set<String>>> newServices) {
+  private final Optional<LayoutRecommendationInfo> recommendationInfo;
+
+  public AddServiceInfo(AddServiceRequest request,
+                        String clusterName,
+                        Stack stack,
+                        Configuration config,
+                        RequestStageContainer stages,
+                        Map<String, Map<String,
+                        Set<String>>> newServices,
+                        Optional<LayoutRecommendationInfo> recommendationInfo) {
     this.request = request;
     this.clusterName = clusterName;
     this.stack = stack;
     this.newServices = newServices;
     this.stages = stages;
     this.config = config;
+    this.recommendationInfo = null != recommendationInfo ? recommendationInfo : Optional.empty();","[{'comment': 'I think the field and constructor parameter should be of type `LayoutRecommendationInfo`, not `Optional< LayoutRecommendationInfo>`, and `getRecommendationInfo()` should wrap it in `Optional.ofNullable()`.  Makes the constructor a bit simpler, and gets rid of warnings in IDEA.', 'commenter': 'adoroszlai'}]"
2704,ambari-server/src/main/java/org/apache/ambari/server/topology/ConfigRecommendationStrategy.java,"@@ -19,24 +19,51 @@
 package org.apache.ambari.server.topology;
 
 public enum ConfigRecommendationStrategy {
+
   /**
    *  Configuration recommendations are always applied, overriding stack defaults and
    *  configuration defined by the user in the Blueprint and/or Cluster Creation Template.
    */
-  ALWAYS_APPLY,
+  ALWAYS_APPLY(true, true, true),
   /**
    * Configuration recommendations are ignored with this option, both for stack defaults
    * and configuration defined by the user in the Blueprint and/or Cluster Creation Template.
    */
-  NEVER_APPLY,
+  NEVER_APPLY(false, false, false),
   /**
    *  Configuration recommendations are always applied for properties listed as stack defaults,
    *  but not for configurations defined by the user in the Blueprint and/or Cluster Creation Template.
    */
-  ONLY_STACK_DEFAULTS_APPLY,
+  ONLY_STACK_DEFAULTS_APPLY(false, true, false),
   /**
    *  Configuration recommendations are always applied, overriding stack defaults but they don't
    *  override configuration defined by the user in the Blueprint and/or Cluster Creation Template.
    */
-  ALWAYS_APPLY_DONT_OVERRIDE_CUSTOM_VALUES;
+  ALWAYS_APPLY_DONT_OVERRIDE_CUSTOM_VALUES(true, true, false);
+
+  public static ConfigRecommendationStrategy getDefault() {
+    return NEVER_APPLY;
+  }
+
+  private final boolean useStackAdvisor;
+  private final boolean useStackDefaults;
+  private final boolean overrideCustomValues;
+
+  public boolean shouldUseStackAdvisor() {
+    return useStackAdvisor;
+  }
+
+  public boolean shouldUseStackDefaults() {","[{'comment': ""I agree with @adoroszlai 's comments.  This config strategy should never provide a way to control if the stack defaults are to be used.  Defaults should always be applied, and this strategy should provide a way to control how the stack advisor's output is applied. \r\n\r\n"", 'commenter': 'rnettleton'}]"
2704,ambari-server/src/main/java/org/apache/ambari/server/controller/AddServiceRequest.java,"@@ -118,7 +118,7 @@ private AddServiceRequest(
     Configuration configuration
   ) {
     this.operationType = null != operationType ? operationType : OperationType.ADD_SERVICE;
-    this.recommendationStrategy = null != recommendationStrategy ? recommendationStrategy : ConfigRecommendationStrategy.NEVER_APPLY;
+    this.recommendationStrategy = null != recommendationStrategy ? recommendationStrategy : ConfigRecommendationStrategy.getDefault();","[{'comment': 'I think its great to have a getDefault method here, but I\'m concerned about using the same default as a Blueprints deployment. \r\n\r\nWhen the StackAdvisor integration was introduced in earlier versions of Ambari Blueprints, we decided to use ""NEVER_APPLY"" as the default strategy, in order to preserve backwards compatibility for existing users of Blueprints. \r\n\r\nFor the ""Add Service"" API, I think we should consider a different default, perhaps ""ALWAYS_APPLY_DONT_OVERRIDE_CUSTOM_VALUES"" as the default for the Add Service implementation, but keep ""NEVER_APPLY"" as the default for Blueprints.  ', 'commenter': 'rnettleton'}]"
2704,ambari-server/src/main/java/org/apache/ambari/server/topology/addservice/StackAdvisorAdapter.java,"@@ -113,47 +115,128 @@ AddServiceInfo recommendLayout(AddServiceInfo info) {
         response.getRecommendations().getBlueprint().getHostgroupComponentMap(),
         info.getStack()::getServiceForComponent);
 
-      Set<ValidationResponse.ValidationItem> validationItems = validateRecommendedLayout(info.getStack().getStackId(),
-        recommendedLayout,
-        response.getRecommendations().getBlueprintClusterBinding().getHostgroupHostMap());
-      if (!validationItems.isEmpty()) {
-        LOG.warn(""Issues found during recommended topology validation:\n{}"", Joiner.on('\n').join(validationItems));
-      }
-
-      // Keep the recommendations for new services only
-      keepNewServicesOnly(recommendedLayout, info.newServices());
+      // Validate layout
+      Map<String, Set<String>> recommendedComponentHosts = getComponentHostMap(recommendedLayout);
+      StackAdvisorRequest validationRequest = request.builder()
+        .forHostsGroupBindings(response.getRecommendations().getBlueprintClusterBinding().getHostgroupHostMap())
+        .withComponentHostsMap(recommendedComponentHosts)
+        .forHostComponents(getHostComponentMap(recommendedComponentHosts)).build();
+      validate(validationRequest);
 
-      return info.withNewServices(recommendedLayout);
+      Map<String,Map<String,Set<String>>> newServiceRecommendations = keepNewServicesOnly(recommendedLayout, info.newServices());
+      LayoutRecommendationInfo recommendationInfo = new LayoutRecommendationInfo(
+        response.getRecommendations().getBlueprintClusterBinding().getHostgroupHostMap(),
+        recommendedLayout);
+      return info.withLayoutRecommendation(newServiceRecommendations, recommendationInfo);
     }
     catch (AmbariException|StackAdvisorException ex) {
       throw new IllegalArgumentException(""Layout recommendation failed."", ex);
     }
   }
 
-  Set<ValidationResponse.ValidationItem> validateRecommendedLayout(StackId stackId,
-                                                              Map<String,Map<String,Set<String>>> recommendedLayout,
-                                                              Map<String, Set<String>> recommendedHostgroups) throws StackAdvisorException {
-    Map<String, Set<String>> componentsToHosts = getComponentHostMap(recommendedLayout);
-    Map<String, Set<String>> hostsToComponents = getHostComponentMap(componentsToHosts);
-    List<String> hosts = ImmutableList.copyOf(hostsToComponents.keySet());
-
-    StackAdvisorRequest request = StackAdvisorRequest.StackAdvisorRequestBuilder
-      .forStack(stackId)
-      .ofType(StackAdvisorRequest.StackAdvisorRequestType.HOST_GROUPS)
-      .forHosts(hosts)
-      .forServices(recommendedLayout.keySet())
-      .forHostComponents(hostsToComponents)
-      .forHostsGroupBindings(recommendedHostgroups)
-      .withComponentHostsMap(componentsToHosts)
-      .withGPLLicenseAccepted(serverConfig.getGplLicenseAccepted())
-      .build();
-    ValidationResponse response = stackAdvisorHelper.validate(request);
-
-    return response.getItems();
+  AddServiceInfo recommendConfigurations(AddServiceInfo info) {
+    Configuration config = info.getConfig();
+
+    if (info.getRequest().getRecommendationStrategy().shouldUseStackAdvisor()) {
+      // Reuse information from layout recommendation.
+      // Layout recommendation is currently mandatory. When it will be optional, this will have to be rewritten to
+      // compute this information if missing
+      LayoutRecommendationInfo layoutInfo =
+        info.getRecommendationInfo().orElseThrow(() -> new IllegalStateException(""Config recommendation must happen after layout recommendation""));","[{'comment': 'Layout recommendation is no longer mandatory, it is skipped with `""components""`-only input (#2714).', 'commenter': 'adoroszlai'}]"
2743,ambari-web/app/controllers/wizard/step1_controller.js,"@@ -194,11 +194,13 @@ App.WizardStep1Controller = Em.Controller.extend({
   onNetworkIssuesExist: function() {
     if (this.get('networkIssuesExist')) {
       this.get('content.stacks').forEach(function (stack) {
-        stack.setProperties({
-          usePublicRepo: false,
-          useLocalRepo: true
-        });
-        stack.cleanReposBaseUrls();
+       if(stack.get('useLocalRepo') != true){","[{'comment': ""Better to check for `stack.get('useLocalRepo') !== true` or `!stack.get('useLocalRepo')` condition"", 'commenter': 'aBabiichuk'}, {'comment': 'done\r\n', 'commenter': 'Akhilsnaik'}]"
2832,ambari-server/src/main/java/org/apache/ambari/server/controller/AmbariServer.java,"@@ -633,6 +634,7 @@ private ServerConnector createSelectChannelConnectorForAgent(Server server, int
 
 
       HttpConfiguration https_config = new HttpConfiguration();
+      https_config.addCustomizer(new SecureRequestCustomizer());","[{'comment': 'Does this correct the issue? I thought that the `SecureRequestCustomizer` will need to be configured to send or not send the header depending on the configuration of Ambari. \r\n', 'commenter': 'rlevas'}, {'comment': 'Customizer is already removed for API server.\r\nThis PR returns it for agent server, as some properties configured by it seems to be required for agent communication.', 'commenter': 'mpapirkovskyy'}, {'comment': 'awesome.. thanks for the clarification. ', 'commenter': 'rlevas'}]"
2834,ambari-infra/pom.xml,"@@ -25,7 +25,7 @@
 
   <properties>
     <jdk.version>1.8</jdk.version>
-    <solr.version>7.5.0</solr.version>
+    <solr.version>7.7.0</solr.version>","[{'comment': 'can you search for other occurrences 7.5.0 solr version? (also in migration scripts / containers etc.) also you can update this for ambari-logsearch', 'commenter': 'oleewere'}, {'comment': 'done', 'commenter': 'kasakrisz'}]"
2841,ambari-web/app/views/main/service/info/summary/hdfs/widgets.js,"@@ -77,7 +77,7 @@ App.HDFSSummaryWidgetsView = Em.View.extend(App.NameNodeWidgetMixin, App.HDFSSum
 
   dfsTotalFilesValue: Em.computed.getByKey('model.dfsTotalFilesValues', 'hostName'),
 
-  dfsTotalFiles: Em.computed.formatUnavailable('model.dfsTotalFilesValue'),
+  dfsTotalFiles: Em.computed.formatUnavailable('dfsTotalFilesValue'),","[{'comment': 'This change would break the functionality because `dfsTotalFilesValue` is taken from `model`, not from the current view object', 'commenter': 'aBabiichuk'}, {'comment': '@aBabiichuk   i didnt see the model holding a value  :  dfsTotalFilesValue\r\n\r\nalso i see here : https://github.com/apache/ambari/blob/7c68f8d4b9ef32dcd8666786bfbf51ad79b68859/ambari-web/app/views/main/service/info/summary/hdfs/widgets.js#L72 that other variables dfsMissingBlocks are taking value from this current View object only. i followed the trend. \r\n\r\nit works in my cluster correctly and data in shown in widget. can you please see if model actually contains this variable to be referenced here.\r\n', 'commenter': 'Akhilsnaik'}, {'comment': ""Sorry, I was confused by single `s` letter. `dfsTotalFilesValues` is defined in `model`, and then mapped to `dfsTotalFilesValue` in the view, so you're correct"", 'commenter': 'aBabiichuk'}]"
2846,ambari-server/conf/unix/log4j.properties,"@@ -79,7 +79,7 @@ log4j.appender.eclipselink.layout=org.apache.log4j.PatternLayout
 log4j.appender.eclipselink.layout.ConversionPattern=%m%n
 
 # Jersey
-log4j.logger.org.glassfish.jersey=WARN,file
+log4j.logger.com.sun.jersey=WARN,file","[{'comment': 'You should probably add a new line for `com.sun.jersey` since some of the views use `org.glassfish.jersey`', 'commenter': 'rlevas'}, {'comment': 'Hi, @rlevas . I have added the `org.glassfish.jersey`  back. Please review it again.', 'commenter': 'coder-chenzhi'}]"
2873,ambari-server/src/main/java/org/apache/ambari/server/stack/UpdateActiveRepoVersionOnStartup.java,"@@ -88,10 +88,14 @@ public void process() throws AmbariException {
 
           StackId stackId = repositoryVersion.getStackId();
           StackInfo stack = stackManager.getStack(stackId.getStackName(), stackId.getStackVersion());
-
-          if (updateRepoVersion(stack, repositoryVersion)) {
-            repositoryVersionDao.merge(repositoryVersion);
-          }
+        
+          if(stack !=null) {
+            if (updateRepoVersion(stack, repositoryVersion)) {
+              repositoryVersionDao.merge(repositoryVersion);
+            }
+          } else {
+            LOG.error(String.format(""Check if Stack %s  version %s is present in file system"",  stackId.getStackName(), stackId.getStackVersion()));","[{'comment': 'I think an exception should be thrown here.  Isn\'t is really bad if the stack is `null`?\r\n\r\n```\r\nthrow new AmbariException(""Stack {} version {} was not found in file system"",  stackId.getStackName(), stackId.getStackVersion())\r\n```', 'commenter': 'rlevas'}, {'comment': ""@rlevas  thanks for reviewing it. In subsequent calls there is an stack exception if there is no nullpointerexception here. here is the sample log.\r\n-------------------\r\n2019-03-18 16:49:10,611 ERROR [main] UpdateActiveRepoVersionOnStartup:97 - Check if Stack HDP  version 3.1 is present in file system\r\n2019-03-18 16:49:10,615  INFO [main] CertificateManager:75 - Initialization of root certificate\r\n2019-03-18 16:49:10,616  INFO [main] CertificateManager:77 - Certificate exists:true\r\n2019-03-18 16:49:10,616  INFO [main] KerberosChecker:128 - Skipping Ambari Server Kerberos credentials check.\r\n2019-03-18 16:49:10,960  INFO [main] ContextLoader:304 - Root WebApplicationContext: initialization started\r\n2019-03-18 16:49:10,961  INFO [main] AnnotationConfigWebApplicationContext:583 - Refreshing Root WebApplicationContext: startup date [Mon Mar 18 16:49:10 UTC 2019]; parent: org.springframework.context.support.ClassPathXmlApplicationContext@1b5f960a\r\n2019-03-18 16:49:10,983  INFO [main] AnnotationConfigWebApplicationContext:208 - Registering annotated classes: [class org.apache.ambari.server.configuration.spring.ApiSecurityConfig]\r\n2019-03-18 16:49:11,052  INFO [main] AmbariManagementControllerImpl:427 - Initializing the AmbariManagementControllerImpl\r\n2019-03-18 16:49:11,128  INFO [main] SingleFileWatch:78 - Starting SingleFileWatcher:ambari.properties\r\n2019-03-18 16:49:11,131  INFO [main] AmbariServer:786 - Jetty is configuring ambari-client-thread with 2 reserved acceptors/selectors and a total pool size of 25 for 32 processors.\r\n2019-03-18 16:49:11,149  INFO [main] AmbariServer:786 - Jetty is configuring qtp-ambari-agent with 4 reserved acceptors/selectors and a total pool size of 25 for 32 processors.\r\n2019-03-18 16:49:11,228  INFO [main] ClustersImpl:281 - Initializing cluster and host data.\r\n2019-03-18 16:49:11,260 ERROR [main] ClusterImpl:389 - Service config versioning disabled due to exception:\r\norg.apache.ambari.server.ParentObjectNotFoundException: Parent Stack Version resource doesn't exist.  Stack data, Stack HDP 3.1 is not found in Ambari metainfo.  Stack data, Stack HDP 3.1 is not found in Ambari metainfo\r\n        at org.apache.ambari.server.api.services.AmbariMetaInfo.getServices(AmbariMetaInfo.java:512)\r\n        at org.apache.ambari.server.state.cluster.ClusterImpl.collectServiceConfigTypesMapping(ClusterImpl.java:387)\r\n        at org.apache.ambari.server.state.cluster.ClusterImpl.loadServiceConfigTypes(ClusterImpl.java:370)\r\n        at org.apache.ambari.server.state.cluster.ClusterImpl.loadStackVersion(ClusterImpl.java:2660)\r\n        at org.apache.ambari.server.state.cluster.ClusterImpl.<init>(ClusterImpl.java:348)\r\n        at org.apache.ambari.server.state.cluster.ClusterImpl$$EnhancerByGuice$$d2f9db14.<init>(<generated>)\r\n        at org.apache.ambari.server.state.cluster.ClusterImpl$$EnhancerByGuice$$d2f9db14$$FastClassByGuice$$838904f0.newInstance(<generated>)\r\n        at com.google.inject.internal.ProxyFactory$ProxyConstructor.newInstance(ProxyFactory.java:265)\r\n        at com.google.inject.internal.ConstructorInjector.provision(ConstructorInjector.java:111)\r\n        at com.google.inject.internal.ConstructorInjector.construct(ConstructorInjector.java:90)\r\n        at com.google.inject.internal.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:268)\r\n        at com.google.inject.internal.InjectorImpl$2$1.call(InjectorImpl.java:1019)\r\n        at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1085)\r\n        at com.google.inject.internal.InjectorImpl$2.get(InjectorImpl.java:1015)\r\n        at com.google.inject.assistedinject.FactoryProvider2.invoke(FactoryProvider2.java:776)\r\n        at com.sun.proxy.$Proxy11.create(Unknown Source)\r\n        at org.apache.ambari.server.state.cluster.ClustersImpl.loadClustersAndHosts(ClustersImpl.java:300)\r\n        at org.apache.ambari.server.state.cluster.ClustersImpl.safelyLoadClustersAndHosts(ClustersImpl.java:266)\r\n        at org.apache.ambari.server.state.cluster.ClustersImpl.getClustersByName(ClustersImpl.java:182)\r\n        at org.apache.ambari.server.state.cluster.ClustersImpl.getClusters(ClustersImpl.java:677)\r\n        at org.apache.ambari.server.checks.DatabaseConsistencyCheckHelper.collectConfigGroupsWithoutServiceName(DatabaseConsistencyCheckHelper.java:1168)\r\n        at org.apache.ambari.server.checks.DatabaseConsistencyCheckHelper.checkConfigGroupsHasServiceName(DatabaseConsistencyCheckHelper.java:1194)\r\n        at org.apache.ambari.server.checks.DatabaseConsistencyCheckHelper.runAllDBChecks(DatabaseConsistencyCheckHelper.java:203)\r\n        at org.apache.ambari.server.controller.AmbariServer.runDatabaseConsistencyCheck(AmbariServer.java:717)\r\n        at org.apache.ambari.server.controller.AmbariServer.run(AmbariServer.java:339)\r\n        at org.apache.ambari.server.controller.AmbariServer.main(AmbariServer.java:1108)\r\nCaused by: org.apache.ambari.server.StackAccessException: Stack data, Stack HDP 3.1 is not found in Ambari metainfo\r\n        at org.apache.ambari.server.api.services.AmbariMetaInfo.getStack(AmbariMetaInfo.java:645)\r\n        at org.apache.ambari.server.api.services.AmbariMetaInfo.getServices(AmbariMetaInfo.java:510)\r\n        ... 25 more\r\n-------------------\r\n\r\nStill do you think throwing Ambari exception is optimal? "", 'commenter': 'apappu'}, {'comment': 'I am not sure.  Maybe @jonathan-hurley, @ncole, or @dlysnichenko can comment.  ', 'commenter': 'rlevas'}, {'comment': 'The DB is in a bad state if you have services attached to a cluster with a non-existent stack.  That is the problem that should be solved.', 'commenter': 'ncole'}, {'comment': ""I would tend to agree - we shouldn't be skipping this b/c of `null` ... there's an underlying problem causing it. Should we throw an exception here and stop the `ambari-server start` command?"", 'commenter': 'jonathan-hurley'}, {'comment': '@ncole thanks for replying - usually we have seen these kind of issues where customers install/upgrade/remove HDF mpacks - if there is no corresponding mpack/stack they it fails with this exception - so throwing error would give some indication that stack definition is not present in Ambari.', 'commenter': 'apappu'}, {'comment': 'In that case, I agree that an exception should be thrown instead.', 'commenter': 'ncole'}, {'comment': 'sure - will make the code changes and re-attach it. thanks.', 'commenter': 'apappu'}, {'comment': '@ncole , @rlevas I made the code changes to incorporate ambariexception and and tested again - Can you please review and merge', 'commenter': 'apappu'}]"
2873,ambari-server/src/main/java/org/apache/ambari/server/stack/UpdateActiveRepoVersionOnStartup.java,"@@ -88,10 +88,15 @@ public void process() throws AmbariException {
 
           StackId stackId = repositoryVersion.getStackId();
           StackInfo stack = stackManager.getStack(stackId.getStackName(), stackId.getStackVersion());
-
-          if (updateRepoVersion(stack, repositoryVersion)) {
-            repositoryVersionDao.merge(repositoryVersion);
-          }
+        
+          if(stack !=null) {
+            if (updateRepoVersion(stack, repositoryVersion)) {
+              repositoryVersionDao.merge(repositoryVersion);
+            }
+          } else {
+            LOG.error(String.format(""Check if Stack %s  version %s is present in file system"",  stackId.getStackName(), stackId.getStackVersion()));
+            throw new AmbariException(String.format(""Stack %s version %s  was not found in file system"",  stackId.getStackName(), stackId.getStackVersion()));","[{'comment': 'Change this to: \r\n`Stack %s was not found on the file system. In the event that it was removed, please ensure that it exists before starting Ambari Server.`', 'commenter': 'jonathan-hurley'}, {'comment': '> Change this to:\r\n> `Stack %s was not found on the file system. In the event that it was removed, please ensure that it exists before starting Ambari Server.`\r\n\r\nResolved.', 'commenter': 'apappu'}]"
2873,ambari-server/src/main/java/org/apache/ambari/server/stack/UpdateActiveRepoVersionOnStartup.java,"@@ -88,10 +88,15 @@ public void process() throws AmbariException {
 
           StackId stackId = repositoryVersion.getStackId();
           StackInfo stack = stackManager.getStack(stackId.getStackName(), stackId.getStackVersion());
-
-          if (updateRepoVersion(stack, repositoryVersion)) {
-            repositoryVersionDao.merge(repositoryVersion);
-          }
+        
+          if(stack !=null) {","[{'comment': 'Formatting:\r\n`if (stack != null)`', 'commenter': 'jonathan-hurley'}, {'comment': 'Addressed', 'commenter': 'apappu'}]"
2873,ambari-server/src/main/java/org/apache/ambari/server/stack/UpdateActiveRepoVersionOnStartup.java,"@@ -88,10 +88,16 @@ public void process() throws AmbariException {
 
           StackId stackId = repositoryVersion.getStackId();
           StackInfo stack = stackManager.getStack(stackId.getStackName(), stackId.getStackVersion());
-
-          if (updateRepoVersion(stack, repositoryVersion)) {
-            repositoryVersionDao.merge(repositoryVersion);
-          }
+        
+          if (stack !=null) {
+            if (updateRepoVersion(stack, repositoryVersion)) {
+              repositoryVersionDao.merge(repositoryVersion);
+            }
+          } else {
+            LOG.error(String.format(""Check if Stack %s  version %s is present in file system"",  stackId.getStackName(), stackId.getStackVersion()));","[{'comment': ""No need to log here if you're throwing and error below."", 'commenter': 'jonathan-hurley'}]"
2874,ambari-server/src/main/java/org/apache/ambari/server/stack/UpdateActiveRepoVersionOnStartup.java,"@@ -89,8 +89,12 @@ public void process() throws AmbariException {
           StackId stackId = repositoryVersion.getStackId();
           StackInfo stack = stackManager.getStack(stackId.getStackName(), stackId.getStackVersion());
 
-          if (updateRepoVersion(stack, repositoryVersion)) {
-            repositoryVersionDao.merge(repositoryVersion);
+          if(stack !=null) {
+            if (updateRepoVersion(stack, repositoryVersion)) {
+              repositoryVersionDao.merge(repositoryVersion);
+            }
+          } else {
+            LOG.error(String.format(""Check if Stack %s  version %s is present in file system"",  stackId.getStackName(), stackId.getStackVersion()));","[{'comment': 'I think an exception should be thrown here.  Isn\'t is really bad if the stack is `null`?\r\n\r\n```\r\nthrow new AmbariException(""Stack {} version {} was not found in file system"",  stackId.getStackName(), stackId.getStackVersion())\r\n```', 'commenter': 'rlevas'}, {'comment': 'I think an exception should be thrown here.  Isn\'t is really bad if the stack is `null`?\r\n\r\n```\r\nthrow new AmbariException(""Stack {} version {} was not found in file system"",  stackId.getStackName(), stackId.getStackVersion())\r\n```', 'commenter': 'rlevas'}]"
2899,ambari-server/src/main/java/org/apache/ambari/server/audit/event/LoginAuditEvent.java,"@@ -62,15 +62,14 @@ private LoginAuditEventBuilder() {
     protected void buildAuditMessage(StringBuilder builder) {
       super.buildAuditMessage(builder);
 
-      builder.append("", Operation(User login), Roles("").append(System.lineSeparator());
+      builder.append("", Operation(User login), Roles("");
 
       if (roles != null && !roles.isEmpty()) {
         List<String> lines = new LinkedList<>();
         for (Map.Entry<String, List<String>> entry : roles.entrySet()) {
           lines.add(""    "" + entry.getKey() + "": "" + StringUtils.join(entry.getValue(), "", ""));
         }
-        builder.append(StringUtils.join(lines, System.lineSeparator()));
-        builder.append(System.lineSeparator());
+        builder.append(StringUtils.join(lines, "" , ""));","[{'comment': 'This may make for a confusing list of values...\r\n```\r\nentry1: value1a, value1b,   entry2: entry2a...  \r\n```\r\n\r\nMaybe use a `;` instead...\r\n```\r\nentry1: value1a, value1b;   entry2: entry2a...  \r\n```\r\n', 'commenter': 'rlevas'}]"
2899,ambari-server/src/main/java/org/apache/ambari/server/audit/event/LoginAuditEvent.java,"@@ -62,15 +62,14 @@ private LoginAuditEventBuilder() {
     protected void buildAuditMessage(StringBuilder builder) {
       super.buildAuditMessage(builder);
 
-      builder.append("", Operation(User login), Roles("").append(System.lineSeparator());
+      builder.append("", Operation(User login), Roles("");
 
       if (roles != null && !roles.isEmpty()) {
         List<String> lines = new LinkedList<>();
         for (Map.Entry<String, List<String>> entry : roles.entrySet()) {
           lines.add(""    "" + entry.getKey() + "": "" + StringUtils.join(entry.getValue(), "", ""));","[{'comment': 'The extra spaces seem unnecessary if the entries are to be place on a single line...\r\n\r\n```\r\n    entry1: value1a, value1b,    entry2: entry2a...  \r\n```\r\n\r\nIt should be \r\n\r\n```\r\nentry1: value1a, value1b, entry2: entry2a...  \r\n```\r\n\r\nNote: I have a comment on the separator for the entries below....', 'commenter': 'rlevas'}]"
2899,ambari-server/src/main/java/org/apache/ambari/server/audit/event/request/AddRepositoryVersionRequestAuditEvent.java,"@@ -88,18 +88,15 @@ protected void buildAuditMessage(StringBuilder builder) {
         .append(repoVersion)
         .append(""), Repositories("");
 
-      if (!repos.isEmpty()) {
-        builder.append(System.lineSeparator());
-      }
 
       for (Map.Entry<String, List<Map<String, String>>> repo : repos.entrySet()) {
         builder.append(""Operating system: "").append(repo.getKey());
-        builder.append(System.lineSeparator());
+        builder.append("" ( "");
         for (Map<String, String> properties : repo.getValue()) {
           builder.append(""    Repository ID("").append(properties.get(""repo_id""));","[{'comment': 'Remove unnecessary spaces...', 'commenter': 'rlevas'}]"
2899,ambari-server/src/main/java/org/apache/ambari/server/audit/event/request/ChangeRepositoryVersionRequestAuditEvent.java,"@@ -89,18 +89,16 @@ protected void buildAuditMessage(StringBuilder builder) {
         .append(repoVersion)
         .append(""), Repositories("");
 
-      if (!repos.isEmpty()) {
-        builder.append(System.lineSeparator());
-      }
+
 
       for (Map.Entry<String, List<Map<String, String>>> repo : repos.entrySet()) {
         builder.append(""Operating system: "").append(repo.getKey());
-        builder.append(System.lineSeparator());
+        builder.append("" ( "");
         for (Map<String, String> properties : repo.getValue()) {
           builder.append(""    Repository ID("").append(properties.get(""repo_id""));","[{'comment': 'Remove unnecessary spaces...', 'commenter': 'rlevas'}]"
2899,ambari-server/src/main/java/org/apache/ambari/server/audit/event/request/ClusterPrivilegeChangeRequestAuditEvent.java,"@@ -99,7 +97,7 @@ protected void buildAuditMessage(StringBuilder builder) {
         }
       }
 
-      builder.append(StringUtils.join(lines, System.lineSeparator()));
+      builder.append(StringUtils.join(lines, "" , ""));","[{'comment': 'the separator should be `;` since `,` is used to separate the values for each entry. ', 'commenter': 'rlevas'}]"
2899,ambari-server/src/main/java/org/apache/ambari/server/audit/event/request/ViewPrivilegeChangeRequestAuditEvent.java,"@@ -119,9 +117,7 @@ protected void buildAuditMessage(StringBuilder builder) {
           lines.add(""  Roles: "" + StringUtils.join(roles.get(role), "", ""));
         }
       }
-
-      builder.append(StringUtils.join(lines, System.lineSeparator()));
-
+      builder.append(StringUtils.join(lines, "" , ""));","[{'comment': 'the separator should be `;` since `,` is used to separate the values for each entry.\r\n', 'commenter': 'rlevas'}]"
2942,ambari-server/src/main/java/org/apache/ambari/server/audit/event/request/ClusterPrivilegeChangeRequestAuditEvent.java,"@@ -80,28 +80,32 @@ protected void buildAuditMessage(StringBuilder builder) {
       roleSet.addAll(roles.keySet());
 
       builder.append("", Roles("");
-      if (!users.isEmpty() || !groups.isEmpty()|| !roles.isEmpty()) {
-        builder.append(System.lineSeparator());
-      }
+
 
       List<String> lines = new LinkedList<>();
+      List<String> tmpLines = null;
+
 
       for (String role : roleSet) {
-        lines.add(role + "": "");
+        tmpLines = new LinkedList<>();
+        lines.add(role + "": ["");
         if (users.get(role) != null && !users.get(role).isEmpty()) {
-          lines.add(""  Users: "" + StringUtils.join(users.get(role), "", ""));
+          tmpLines.add(""  Users: "" + StringUtils.join(users.get(role), "", ""));
         }
         if (groups.get(role) != null && !groups.get(role).isEmpty()) {
-          lines.add(""  Groups: "" + StringUtils.join(groups.get(role), "", ""));
+          tmpLines.add(""  Groups: "" + StringUtils.join(groups.get(role), "", ""));
         }
         if (roles.get(role) != null && !roles.get(role).isEmpty()) {
-          lines.add(""  Roles: "" + StringUtils.join(roles.get(role), "", ""));
+          tmpLines.add(""  Roles: "" + StringUtils.join(roles.get(role), "", ""));
         }
+        lines.add(StringUtils.join(tmpLines, "";""));
+        lines.add(""] "");
       }
 
-      builder.append(StringUtils.join(lines, System.lineSeparator()));
+      builder.append(StringUtils.join(lines, """"));","[{'comment': 'Now that there is no separator between lines, each line can be `append`ed to the `builder` in the loop, and `lines` is no longer needed.  `tmpLines` can be renamed to `lines`.\r\n\r\nSame applies to `ViewPrivilegeChangeRequestAuditEvent.java`.', 'commenter': 'adoroszlai'}, {'comment': 'tmpLines is needed since there is comma separator for those items. ', 'commenter': 'apappu'}, {'comment': 'Yes, `tmpLines` is needed, but `lines` is not.', 'commenter': 'adoroszlai'}, {'comment': 'there is a comma separator for tmpLines and no separator for lines. so it is needed. ', 'commenter': 'apappu'}]"
2942,ambari-server/src/main/java/org/apache/ambari/server/audit/event/request/ClusterPrivilegeChangeRequestAuditEvent.java,"@@ -80,28 +80,32 @@ protected void buildAuditMessage(StringBuilder builder) {
       roleSet.addAll(roles.keySet());
 
       builder.append("", Roles("");
-      if (!users.isEmpty() || !groups.isEmpty()|| !roles.isEmpty()) {
-        builder.append(System.lineSeparator());
-      }
+
 
       List<String> lines = new LinkedList<>();
+      List<String> tmpLines = null;","[{'comment': 'Please move `tmpLines` declaration inside the loop.', 'commenter': 'adoroszlai'}, {'comment': 'Taken care', 'commenter': 'apappu'}, {'comment': ""Can you please clarify how it's taken care of?"", 'commenter': 'adoroszlai'}, {'comment': 'Declared outside the loop and initialized inside the loop. \r\n\r\n`    List<String> tmpLines = null;\r\n      for (String role : roleSet) {\r\n        tmpLines = new LinkedList<>();`', 'commenter': 'apappu'}, {'comment': 'Declared outside but initialized inside for loop\r\n`    List<String> tmpLines = null;\r\n    for (String role : roleSet) {\r\n        tmpLines = new LinkedList<>();`', 'commenter': 'apappu'}, {'comment': 'Since the variable is only used inside the loop, there is no need to declare it outside.', 'commenter': 'adoroszlai'}, {'comment': 'moved inside', 'commenter': 'apappu'}]"
2942,ambari-server/src/main/java/org/apache/ambari/server/audit/event/request/ChangeRepositoryVersionRequestAuditEvent.java,"@@ -89,18 +89,16 @@ protected void buildAuditMessage(StringBuilder builder) {
         .append(repoVersion)
         .append(""), Repositories("");
 
-      if (!repos.isEmpty()) {
-        builder.append(System.lineSeparator());
-      }
+
 
       for (Map.Entry<String, List<Map<String, String>>> repo : repos.entrySet()) {
         builder.append(""Operating system: "").append(repo.getKey());
-        builder.append(System.lineSeparator());
+        builder.append(""("");
         for (Map<String, String> properties : repo.getValue()) {
           builder.append(""    Repository ID("").append(properties.get(""repo_id""));","[{'comment': 'I think the indentation can be removed:\r\n\r\n```suggestion\r\n           builder.append(""Repository ID("").append(properties.get(""repo_id""));\r\n```', 'commenter': 'adoroszlai'}, {'comment': 'Taken care', 'commenter': 'apappu'}]"
2942,ambari-server/src/main/java/org/apache/ambari/server/audit/event/request/ClusterPrivilegeChangeRequestAuditEvent.java,"@@ -80,28 +80,32 @@ protected void buildAuditMessage(StringBuilder builder) {
       roleSet.addAll(roles.keySet());
 
       builder.append("", Roles("");
-      if (!users.isEmpty() || !groups.isEmpty()|| !roles.isEmpty()) {
-        builder.append(System.lineSeparator());
-      }
+
 
       List<String> lines = new LinkedList<>();
+      List<String> tmpLines = null;
+
 
       for (String role : roleSet) {
-        lines.add(role + "": "");
+        tmpLines = new LinkedList<>();
+        lines.add(role + "": ["");
         if (users.get(role) != null && !users.get(role).isEmpty()) {
-          lines.add(""  Users: "" + StringUtils.join(users.get(role), "", ""));
+          tmpLines.add(""  Users: "" + StringUtils.join(users.get(role), "", ""));","[{'comment': '```suggestion\r\n          tmpLines.add(""Users: "" + StringUtils.join(users.get(role), "", ""));\r\n```', 'commenter': 'adoroszlai'}, {'comment': 'Taken care', 'commenter': 'apappu'}]"
2942,ambari-server/src/main/java/org/apache/ambari/server/audit/event/LoginAuditEvent.java,"@@ -62,15 +62,14 @@ private LoginAuditEventBuilder() {
     protected void buildAuditMessage(StringBuilder builder) {
       super.buildAuditMessage(builder);
 
-      builder.append("", Operation(User login), Roles("").append(System.lineSeparator());
+      builder.append("", Operation(User login), Roles("");
 
       if (roles != null && !roles.isEmpty()) {
         List<String> lines = new LinkedList<>();
         for (Map.Entry<String, List<String>> entry : roles.entrySet()) {
-          lines.add(""    "" + entry.getKey() + "": "" + StringUtils.join(entry.getValue(), "", ""));
+          lines.add(""("" + ""    "" + entry.getKey() + "": "" + StringUtils.join(entry.getValue(), "", "") + "")"");","[{'comment': '```suggestion\r\n          lines.add(""("" + entry.getKey() + "": "" + StringUtils.join(entry.getValue(), "", "") + "")"");\r\n```', 'commenter': 'adoroszlai'}, {'comment': 'Taken care', 'commenter': 'apappu'}]"
2942,ambari-server/src/main/java/org/apache/ambari/server/audit/event/request/ClusterPrivilegeChangeRequestAuditEvent.java,"@@ -80,28 +80,32 @@ protected void buildAuditMessage(StringBuilder builder) {
       roleSet.addAll(roles.keySet());
 
       builder.append("", Roles("");
-      if (!users.isEmpty() || !groups.isEmpty()|| !roles.isEmpty()) {
-        builder.append(System.lineSeparator());
-      }
+
 
       List<String> lines = new LinkedList<>();
+      List<String> tmpLines = null;
+
 
       for (String role : roleSet) {
-        lines.add(role + "": "");
+        tmpLines = new LinkedList<>();
+        lines.add(role + "": ["");
         if (users.get(role) != null && !users.get(role).isEmpty()) {
-          lines.add(""  Users: "" + StringUtils.join(users.get(role), "", ""));
+          tmpLines.add(""  Users: "" + StringUtils.join(users.get(role), "", ""));
         }
         if (groups.get(role) != null && !groups.get(role).isEmpty()) {
-          lines.add(""  Groups: "" + StringUtils.join(groups.get(role), "", ""));
+          tmpLines.add(""  Groups: "" + StringUtils.join(groups.get(role), "", ""));","[{'comment': '```suggestion\r\n          tmpLines.add(""Groups: "" + StringUtils.join(groups.get(role), "", ""));\r\n```', 'commenter': 'adoroszlai'}]"
2942,ambari-server/src/main/java/org/apache/ambari/server/audit/event/request/ClusterPrivilegeChangeRequestAuditEvent.java,"@@ -80,28 +80,32 @@ protected void buildAuditMessage(StringBuilder builder) {
       roleSet.addAll(roles.keySet());
 
       builder.append("", Roles("");
-      if (!users.isEmpty() || !groups.isEmpty()|| !roles.isEmpty()) {
-        builder.append(System.lineSeparator());
-      }
+
 
       List<String> lines = new LinkedList<>();
+      List<String> tmpLines = null;
+
 
       for (String role : roleSet) {
-        lines.add(role + "": "");
+        tmpLines = new LinkedList<>();
+        lines.add(role + "": ["");
         if (users.get(role) != null && !users.get(role).isEmpty()) {
-          lines.add(""  Users: "" + StringUtils.join(users.get(role), "", ""));
+          tmpLines.add(""  Users: "" + StringUtils.join(users.get(role), "", ""));
         }
         if (groups.get(role) != null && !groups.get(role).isEmpty()) {
-          lines.add(""  Groups: "" + StringUtils.join(groups.get(role), "", ""));
+          tmpLines.add(""  Groups: "" + StringUtils.join(groups.get(role), "", ""));
         }
         if (roles.get(role) != null && !roles.get(role).isEmpty()) {
-          lines.add(""  Roles: "" + StringUtils.join(roles.get(role), "", ""));
+          tmpLines.add(""  Roles: "" + StringUtils.join(roles.get(role), "", ""));","[{'comment': '```suggestion\r\n          tmpLines.add(""Roles: "" + StringUtils.join(roles.get(role), "", ""));\r\n```', 'commenter': 'adoroszlai'}, {'comment': 'Taken care', 'commenter': 'apappu'}]"
2942,ambari-server/src/main/java/org/apache/ambari/server/audit/event/LoginAuditEvent.java,"@@ -62,15 +62,14 @@ private LoginAuditEventBuilder() {
     protected void buildAuditMessage(StringBuilder builder) {
       super.buildAuditMessage(builder);
 
-      builder.append("", Operation(User login), Roles("").append(System.lineSeparator());
+      builder.append("", Operation(User login), Roles("");
 
       if (roles != null && !roles.isEmpty()) {
         List<String> lines = new LinkedList<>();
         for (Map.Entry<String, List<String>> entry : roles.entrySet()) {
-          lines.add(""    "" + entry.getKey() + "": "" + StringUtils.join(entry.getValue(), "", ""));
+          lines.add(""("" +entry.getKey() + "": "" + StringUtils.join(entry.getValue(), "", "") + "")"");
         }
-        builder.append(StringUtils.join(lines, System.lineSeparator()));
-        builder.append(System.lineSeparator());
+        builder.append(StringUtils.join(lines, "")""));","[{'comment': 'I think `)` as separator between roles would be pretty confusing.', 'commenter': 'adoroszlai'}, {'comment': '@adoroszlai in line 72 it is not a separator between the roles . it is only closing the opened bracket. line 70 adds the comma as separator between the roles.\r\n\r\nthis is how the actual audit log entry would look alike,\r\n`2019-04-26T17:06:17.451Z, User(test), RemoteIp(10.22.8.5), Operation(User login), Roles(c2111: Cluster Administrator ,Files View: View User ,YARN Queue Manager: View User), Status(Success)`\r\n\r\n', 'commenter': 'apappu'}, {'comment': ""No, the opening bracket is closed in line 74.  Here's an example with multiple roles:\r\n\r\n```\r\n2019-04-26T17:35:57.685Z, User(test), RemoteIp(172.27.89.142), Operation(User login), Roles((Files View: View User))(cl1: Service Administrator)), Status(Success)\r\n```"", 'commenter': 'adoroszlai'}, {'comment': 'thanks for pointing out @adoroszlai - I have corrected that . latest logs shows like this\r\n\r\n`2019-04-26T21:08:18.862Z, User(test), RemoteIp(10.22.8.5), Operation(User login), Roles(c2111: Cluster Administrator,Files View: View User,YARN Queue Manager: View User), Status(Success)`\r\n`2019-04-26T21:08:36.964Z, User(admin), RemoteIp(10.22.8.5), Operation(User login), Roles(Ambari: Ambari Administrator), Status(Success)`', 'commenter': 'apappu'}, {'comment': '@adoroszlai  @apappu  if this Conversation is resolved , will you mark as resolve conversation ', 'commenter': 'Akhilsnaik'}]"
2986,contrib/fast-hdfs-resource/src/main/java/org/apache/ambari/fast_hdfs_resource/Runner.java,"@@ -127,6 +128,25 @@ public static void main(String[] args)
           }
 
           if (resource.getAction().equals(""create"")) {
+            if (""file"".equals(resource.getType())) {
+              // Check if the file already exists with same timestamp","[{'comment': 'Comment says ""same timestamp"", but code checks file size.', 'commenter': 'adoroszlai'}, {'comment': '@adoroszlai , \r\nI have correct it. I see python code(case of webHDFS , it checks for filesize) i made the same logic in case of DFS also .', 'commenter': 'Akhilsnaik'}]"
2986,contrib/fast-hdfs-resource/src/main/java/org/apache/ambari/fast_hdfs_resource/Runner.java,"@@ -127,6 +128,25 @@ public static void main(String[] args)
           }
 
           if (resource.getAction().equals(""create"")) {
+            if (""file"".equals(resource.getType())) {
+              // Check if the file already exists with same timestamp
+              // or not
+              FileStatus status = dfs.getFileStatus(pathHadoop);
+              long fileLength = status.getLen();
+              File f = new File(resource.getSource());
+              if (null == f || !f.exists()) {","[{'comment': 'These checks should precede `dfs.getFileStatus` call, since target file size is not important if the operation is being skipped due to non-existent source file.', 'commenter': 'adoroszlai'}, {'comment': '@adoroszlai  ,\r\n I have implemented the review comments .\r\n', 'commenter': 'Akhilsnaik'}, {'comment': 'Thanks!', 'commenter': 'adoroszlai'}]"
3013,ambari-server/src/test/java/org/apache/ambari/server/controller/BackgroundCustomCommandExecutionTest.java,"@@ -206,6 +216,90 @@ private void setOsFamily(Host host, String osFamily, String osVersion) {
     host.setHostAttributes(hostAttributes);
   }
 
+
+  @SuppressWarnings(""serial"")
+  @Test
+  public void testUpdateHBaseReplicationCustomCommand()
+          throws AuthorizationException, AmbariException, IllegalAccessException,
+          NoSuchFieldException {
+    createClusterFixture();
+    Map<String, String> requestProperties = new HashMap<String, String>() {
+      {
+        put(REQUEST_CONTEXT_PROPERTY, ""Enable Cross Cluster HBase Replication"");
+        put(""command"", ""UPDATE_REPLICATION"");
+        put(""parameters"", UPDATE_REPLICATION_PARAMS);
+      }
+    };
+    ExecuteActionRequest actionRequest = new ExecuteActionRequest(""c1"",
+            ""UPDATE_REPLICATION"", new HashMap<>(), false);
+    actionRequest.getResourceFilters().add(new RequestResourceFilter(""HBASE"", ""HBASE_MASTER"",
+            Collections.singletonList(""c7007"")));
+
+    controller.createAction(actionRequest, requestProperties);
+
+    Mockito.verify(am, Mockito.times(1))
+            .sendActions(requestCapture.capture(), any(ExecuteActionRequest.class));
+
+    Request request = requestCapture.getValue();
+    Assert.assertNotNull(request);
+    Assert.assertNotNull(request.getStages());
+    Assert.assertEquals(1, request.getStages().size());
+    Stage stage = request.getStages().iterator().next();
+
+    Assert.assertEquals(1, stage.getHosts().size());
+
+    List<ExecutionCommandWrapper> commands = stage.getExecutionCommands(""c7007"");
+    Assert.assertEquals(1, commands.size());
+    ExecutionCommand command = commands.get(0).getExecutionCommand();
+    Assert.assertEquals(AgentCommandType.EXECUTION_COMMAND, command.getCommandType());
+    Assert.assertEquals(""UPDATE_REPLICATION"", command.getCommandParams().get(""custom_command""));
+
+  }
+
+
+
+  @SuppressWarnings(""serial"")
+  @Test
+  public void testStopHBaseReplicationCustomCommand()
+          throws AuthorizationException, AmbariException, IllegalAccessException,
+          NoSuchFieldException {
+    createClusterFixture();
+    Map<String, String> requestProperties = new HashMap<String, String>() {
+      {
+        put(REQUEST_CONTEXT_PROPERTY, ""Disable Cross Cluster HBase Replication"");
+        put(""command"", ""STOP_REPLICATION"");
+        put(""parameters"", STOP_REPLICATION_PARAMS);
+      }
+    };
+    ExecuteActionRequest actionRequest = new ExecuteActionRequest(""c1"",
+            ""STOP_REPLICATION"", new HashMap<>(), false);
+    actionRequest.getResourceFilters().add(new RequestResourceFilter(""HBASE"", ""HBASE_MASTER"",
+            Collections.singletonList(""c7007"")));
+
+    controller.createAction(actionRequest, requestProperties);
+
+    Mockito.verify(am, Mockito.times(1))
+            .sendActions(requestCapture.capture(), any(ExecuteActionRequest.class));
+
+    Request request = requestCapture.getValue();
+    Assert.assertNotNull(request);
+    Assert.assertNotNull(request.getStages());
+    Assert.assertEquals(1, request.getStages().size());
+    Stage stage = request.getStages().iterator().next();
+
+    Assert.assertEquals(1, stage.getHosts().size());
+
+    List<ExecutionCommandWrapper> commands = stage.getExecutionCommands(""c7007"");
+    Assert.assertEquals(1, commands.size());
+    ExecutionCommand command = commands.get(0).getExecutionCommand();
+    Assert.assertEquals(AgentCommandType.EXECUTION_COMMAND, command.getCommandType());
+    Assert.assertEquals(""STOP_REPLICATION"", command.getCommandParams().get(""custom_command""));
+","[{'comment': 'please remove empty lines which r not required', 'commenter': 'hapylestat'}]"
3013,ambari-server/src/test/java/org/apache/ambari/server/controller/BackgroundCustomCommandExecutionTest.java,"@@ -206,6 +216,90 @@ private void setOsFamily(Host host, String osFamily, String osVersion) {
     host.setHostAttributes(hostAttributes);
   }
 
+
+  @SuppressWarnings(""serial"")
+  @Test
+  public void testUpdateHBaseReplicationCustomCommand()
+          throws AuthorizationException, AmbariException, IllegalAccessException,
+          NoSuchFieldException {
+    createClusterFixture();
+    Map<String, String> requestProperties = new HashMap<String, String>() {
+      {
+        put(REQUEST_CONTEXT_PROPERTY, ""Enable Cross Cluster HBase Replication"");
+        put(""command"", ""UPDATE_REPLICATION"");
+        put(""parameters"", UPDATE_REPLICATION_PARAMS);
+      }
+    };
+    ExecuteActionRequest actionRequest = new ExecuteActionRequest(""c1"",
+            ""UPDATE_REPLICATION"", new HashMap<>(), false);
+    actionRequest.getResourceFilters().add(new RequestResourceFilter(""HBASE"", ""HBASE_MASTER"",
+            Collections.singletonList(""c7007"")));
+
+    controller.createAction(actionRequest, requestProperties);
+
+    Mockito.verify(am, Mockito.times(1))
+            .sendActions(requestCapture.capture(), any(ExecuteActionRequest.class));
+
+    Request request = requestCapture.getValue();
+    Assert.assertNotNull(request);
+    Assert.assertNotNull(request.getStages());
+    Assert.assertEquals(1, request.getStages().size());
+    Stage stage = request.getStages().iterator().next();
+
+    Assert.assertEquals(1, stage.getHosts().size());
+
+    List<ExecutionCommandWrapper> commands = stage.getExecutionCommands(""c7007"");
+    Assert.assertEquals(1, commands.size());
+    ExecutionCommand command = commands.get(0).getExecutionCommand();
+    Assert.assertEquals(AgentCommandType.EXECUTION_COMMAND, command.getCommandType());
+    Assert.assertEquals(""UPDATE_REPLICATION"", command.getCommandParams().get(""custom_command""));
+","[{'comment': 'unused empty lines', 'commenter': 'hapylestat'}]"
3013,ambari-server/src/test/java/org/apache/ambari/server/controller/BackgroundCustomCommandExecutionTest.java,"@@ -206,6 +216,90 @@ private void setOsFamily(Host host, String osFamily, String osVersion) {
     host.setHostAttributes(hostAttributes);
   }
 
+
+  @SuppressWarnings(""serial"")
+  @Test
+  public void testUpdateHBaseReplicationCustomCommand()
+          throws AuthorizationException, AmbariException, IllegalAccessException,
+          NoSuchFieldException {
+    createClusterFixture();
+    Map<String, String> requestProperties = new HashMap<String, String>() {
+      {
+        put(REQUEST_CONTEXT_PROPERTY, ""Enable Cross Cluster HBase Replication"");
+        put(""command"", ""UPDATE_REPLICATION"");
+        put(""parameters"", UPDATE_REPLICATION_PARAMS);
+      }
+    };
+    ExecuteActionRequest actionRequest = new ExecuteActionRequest(""c1"",
+            ""UPDATE_REPLICATION"", new HashMap<>(), false);
+    actionRequest.getResourceFilters().add(new RequestResourceFilter(""HBASE"", ""HBASE_MASTER"",
+            Collections.singletonList(""c7007"")));
+
+    controller.createAction(actionRequest, requestProperties);
+
+    Mockito.verify(am, Mockito.times(1))
+            .sendActions(requestCapture.capture(), any(ExecuteActionRequest.class));
+
+    Request request = requestCapture.getValue();
+    Assert.assertNotNull(request);
+    Assert.assertNotNull(request.getStages());
+    Assert.assertEquals(1, request.getStages().size());
+    Stage stage = request.getStages().iterator().next();
+
+    Assert.assertEquals(1, stage.getHosts().size());
+
+    List<ExecutionCommandWrapper> commands = stage.getExecutionCommands(""c7007"");
+    Assert.assertEquals(1, commands.size());
+    ExecutionCommand command = commands.get(0).getExecutionCommand();
+    Assert.assertEquals(AgentCommandType.EXECUTION_COMMAND, command.getCommandType());
+    Assert.assertEquals(""UPDATE_REPLICATION"", command.getCommandParams().get(""custom_command""));
+
+  }
+
+","[{'comment': 'unused empty lines', 'commenter': 'hapylestat'}]"
3017,ambari-server/src/main/java/org/apache/ambari/server/security/encryption/CertificateUtils.java,"@@ -18,35 +18,16 @@
 package org.apache.ambari.server.security.encryption;
 
 import java.io.ByteArrayInputStream;
-import java.io.File;
-import java.io.IOException;
 import java.io.UnsupportedEncodingException;
 import java.security.cert.CertificateException;
 import java.security.cert.CertificateFactory;
 import java.security.cert.X509Certificate;
 import java.security.interfaces.RSAPublicKey;
 
-import org.apache.commons.io.FileUtils;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
 /**
  * Utility class containing methods to works with certificates
  */
 public class CertificateUtils {
-  private static final Logger LOG = LoggerFactory.getLogger(CertificateUtils.class);
-
-  /**
-   * Get RSA public key from X.509 certificate file
-   * @param filePath path to certificate file
-   * @return RSA public key
-   * @throws IOException
-   * @throws CertificateException
-   */
-  public static RSAPublicKey getPublicKeyFromFile(String filePath) throws IOException, CertificateException {","[{'comment': 'why are we deleting this function ?\r\n', 'commenter': 'Akhilsnaik'}, {'comment': 'Thanks for pointing this out @Akhilsnaik \r\nThis function is not being used anywhere in the code. Only getPublicKeyFromString is being used.', 'commenter': 'virajjasani'}]"
3027,contrib/views/hive20/src/main/java/org/apache/ambari/view/hive20/resources/browser/DDLProxy.java,"@@ -173,6 +173,11 @@ public TableMeta getTableProperties(ViewContext context, ConnectionConfig connec
       public boolean apply(@Nullable DatabaseInfo input) {
         return input.getName().equalsIgnoreCase(databaseId);
       }
+/*","[{'comment': 'remove commented test code', 'commenter': 'mpapirkovskyy'}]"
3027,contrib/views/hive20/src/main/java/org/apache/ambari/view/hive20/resources/browser/DDLProxy.java,"@@ -222,6 +222,11 @@ private TableResponse transformToTableResponse(TableInfo tableInfo, String datab
       public boolean apply(@Nullable TableInfo input) {
         return input.getName().equalsIgnoreCase(tableName);
       }
+
+      /*@Override
+      public boolean test(@NullableDecl String input) {
+        return false;
+      }*/","[{'comment': 'one more please', 'commenter': 'mpapirkovskyy'}, {'comment': 'one more commented code block', 'commenter': 'mpapirkovskyy'}, {'comment': 'this is same as one above :) ', 'commenter': 'hapylestat'}]"
3038,ambari-server/src/main/java/org/apache/ambari/server/controller/ControllerModule.java,"@@ -640,25 +640,27 @@ private void installFactories() {
     // the dispatch factory
     for (BeanDefinition beanDefinition : beanDefinitions) {
       String className = beanDefinition.getBeanClassName();
-      Class<?> clazz = ClassUtils.resolveClassName(className,
-          ClassUtils.getDefaultClassLoader());
-
-      try {
-        NotificationDispatcher dispatcher;
-        if (clazz.equals(AmbariSNMPDispatcher.class)) {
-          dispatcher = (NotificationDispatcher) clazz.getConstructor(Integer.class).newInstance(configuration.getAmbariSNMPUdpBindPort());
-        } else if (clazz.equals(SNMPDispatcher.class)) {
-          dispatcher = (NotificationDispatcher) clazz.getConstructor(Integer.class).newInstance(configuration.getSNMPUdpBindPort());
-        } else {
-          dispatcher = (NotificationDispatcher) clazz.newInstance();
+      if (className != null) {","[{'comment': 'Can we add some log message, in case we got  className with null value?', 'commenter': 'hapylestat'}, {'comment': 'Thanks @hapylestat for the review. Updated the patch based on the comment, please let me know if this looks good to you.\r\nThe message is logged here: https://github.com/apache/ambari/pull/3038/files#diff-76a5e296845577ffc0f1a86bb38ee177R665', 'commenter': 'virajjasani'}, {'comment': 'Updated trunk PR: #3037 as well', 'commenter': 'virajjasani'}, {'comment': '@hapylestat @dlysnichenko @jonathan-hurley  could you please help me trigger a build for this PR?', 'commenter': 'virajjasani'}, {'comment': 'Please review @dlysnichenko @hapylestat\r\nPR for trunk: #3037 ', 'commenter': 'virajjasani'}, {'comment': 'Please review @mpapirkovskyy ', 'commenter': 'virajjasani'}]"
3046,ambari-server/src/main/java/org/apache/ambari/server/serveraction/kerberos/CreateKeytabFilesServerAction.java,"@@ -231,6 +242,9 @@ protected CommandReport processIdentity(ResolvedKerberosPrincipal resolvedPrinci
                       commandReport = createCommandReport(1, HostRoleStatus.FAILED, ""{}"", actionLog.getStdOut(), actionLog.getStdErr());
                     } else {
                       try {
+                        if(regenerateKeytabs) {
+                          Keytab keytab = createKeytab(resolvedPrincipal.getPrincipal(), password, keyNumber, operationHandler, visitedPrincipalKeys != null, true, actionLog);","[{'comment': ""we can't create here a keytab as `password` would be always `null` and keytab would be invalid at the end"", 'commenter': 'hapylestat'}, {'comment': 'I guess the proper place to do the change is `CreatePrincipalsServerAction`, there is a place like: \r\n```\r\n      } else if (!StringUtils.isEmpty(kerberosPrincipalEntity.getCachedKeytabPath())) {\r\n        // This principal has been processed and a keytab file has been cached for it... do not process it.\r\n        processPrincipal = false;\r\n      } else {\r\n```\r\nif we change it to something like: \r\n```\r\n      } else if (!StringUtils.isEmpty(kerberosPrincipalEntity.getCachedKeytabPath())) {\r\n        File file = new File(kerberosPrincipalEntity.getCachedKeytabPath());\r\n\r\n        // Process the principal if the cache is missing, otherwise skip it\r\n        processPrincipal = !file.exists();\r\n      } else {\r\n```\r\nIt would process with the the proper keytab re-generation', 'commenter': 'hapylestat'}, {'comment': '@hapylestat I thought changing CreateKeytab operation code is more relevant - in the above recommendation we are touching the CreatePrincipleAction right.', 'commenter': 'apappu'}, {'comment': ""> we can't create here a keytab as `password` would be always `null` and keytab would be invalid at the end\r\n\r\nLook like we don't need password here - see https://github.com/apache/ambari/blob/branch-2.7/ambari-server/src/main/java/org/apache/ambari/server/serveraction/kerberos/KDCKerberosOperationHandler.java#L177"", 'commenter': 'apappu'}, {'comment': 'for Kerberos yes, but check please code for non-Kerberos handlers', 'commenter': 'hapylestat'}]"
3046,ambari-server/src/main/java/org/apache/ambari/server/serveraction/kerberos/CreateKeytabFilesServerAction.java,"@@ -217,6 +217,17 @@ protected CommandReport processIdentity(ResolvedKerberosPrincipal resolvedPrinci
                 KerberosPrincipalEntity principalEntity = kerberosPrincipalDAO.find(resolvedPrincipal.getPrincipal());
                 String cachedKeytabPath = (principalEntity == null) ? null : principalEntity.getCachedKeytabPath();
 
+		//This is to hanlde regenerate keytabs for missing hosts if keytab file does not exist in the cache dir.                
+		if(!regenerateKeytabs) {","[{'comment': 'no need to check for `regenerateKeytabs`, it is enough to check for the cached keytab and regenerate it if needed from already created keytab.', 'commenter': 'hapylestat'}, {'comment': '@hapylestat If regenerateKeytabs is already true and then don\'t need to check the file exist - since this operation is already slow - adding ""if(!regenerateKeytabs)"" check would avoid running file existance check again for keytabs which is not needed.', 'commenter': 'apappu'}]"
3046,ambari-server/src/main/java/org/apache/ambari/server/serveraction/kerberos/CreateKeytabFilesServerAction.java,"@@ -217,6 +217,17 @@ protected CommandReport processIdentity(ResolvedKerberosPrincipal resolvedPrinci
                 KerberosPrincipalEntity principalEntity = kerberosPrincipalDAO.find(resolvedPrincipal.getPrincipal());
                 String cachedKeytabPath = (principalEntity == null) ? null : principalEntity.getCachedKeytabPath();
 
+		//This is to hanlde regenerate keytabs for missing hosts if keytab file does not exist in the cache dir.                
+		if(!regenerateKeytabs) {
+                  if(cachedKeytabPath != null) {
+                    File file = new File(cachedKeytabPath);
+                    if(!file.exists()) {
+                      regenerateKeytabs = true;
+                      LOG.info(""Regenerating the keytab  since  "" + cachedKeytabPath + "" is not present"");","[{'comment': 'please use formatting like `LOG.info(""something {} important"", importantVar);`', 'commenter': 'hapylestat'}, {'comment': 'sure - will take care of that.', 'commenter': 'apappu'}]"
3097,ambari-agent/src/main/python/ambari_agent/ActionQueue.py,"@@ -37,6 +38,12 @@
 
 MAX_SYMBOLS_PER_LOG_MESSAGE = 7900
 
+PASSWORD_REPLACEMENT = '********'","[{'comment': 'In most other places we use [PROTECTED] placeholder so I would do the same here.', 'commenter': 'aonishuk'}]"
3097,ambari-agent/src/main/python/ambari_agent/ActionQueue.py,"@@ -37,6 +38,12 @@
 
 MAX_SYMBOLS_PER_LOG_MESSAGE = 7900
 
+PASSWORD_REPLACEMENT = '********'
+
+def sanitize(text):","[{'comment': 'Maybe call this something like hide_passwords or similar. Just to make it more clear on what it does.', 'commenter': 'aonishuk'}]"
3097,ambari-agent/src/main/python/ambari_agent/ActionQueue.py,"@@ -37,6 +38,12 @@
 
 MAX_SYMBOLS_PER_LOG_MESSAGE = 7900
 
+PASSWORD_REPLACEMENT = '********'
+
+def sanitize(text):
+  """""" Replaces the matching passwords with **** in the given text """"""
+  return None if text is None else re.sub(r""('\S*password':\s*u?')(\S+)(')"", r'\1{}\3'.format(PASSWORD_REPLACEMENT), text)","[{'comment': 'The regex should be pre-compiled. To increase performance.', 'commenter': 'aonishuk'}]"
3110,ambari-metrics/ambari-metrics-kafka-sink/src/main/java/org/apache/hadoop/metrics2/sink/kafka/JvmMetricSet.java,"@@ -0,0 +1,200 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.metrics2.sink.kafka;
+
+import java.lang.management.ManagementFactory;
+import java.lang.management.MemoryMXBean;
+import java.lang.management.RuntimeMXBean;
+import java.lang.management.ThreadMXBean;
+import java.util.AbstractMap;
+import java.util.Arrays;
+import java.util.List;
+import java.util.Map;
+import java.util.stream.Collectors;
+import java.util.stream.Stream;
+import com.yammer.metrics.core.Gauge;
+import com.yammer.metrics.core.MetricName;
+import com.yammer.metrics.util.RatioGauge;
+
+public class JvmMetricSet {
+
+  private static final String MEMORY = ""memory"";
+  private static final String THREADS = ""threads"";
+  private static final String RUNTIME = ""runtime"";
+
+  private static final JvmMetricSet INSTANCE = new JvmMetricSet();
+
+
+  public static JvmMetricSet getInstance() {
+    return INSTANCE;
+  }
+
+  private final MemoryMXBean memoryMXBean;
+  private final ThreadMXBean threadMXBean;
+  private final RuntimeMXBean runtimeMXBean;
+
+  private static class JvmMetric {
+    private final MetricName metricName;
+    private final Gauge<?> metric;
+
+    JvmMetric(MetricName metricName, Gauge<?> metric) {
+      this.metricName = metricName;
+      this.metric = metric;
+    }
+
+    MetricName getMetricName() {
+      return metricName;
+    }
+
+    Gauge<?> getMetric() {
+      return metric;
+    }
+  }
+
+
+  private JvmMetricSet() {
+    this(ManagementFactory.getMemoryMXBean(), ManagementFactory.getThreadMXBean(),
+      ManagementFactory.getRuntimeMXBean());
+
+  }
+
+  private JvmMetricSet(MemoryMXBean memoryMXBean, ThreadMXBean threadMXBean, RuntimeMXBean runtimeMXBean) {
+    this.memoryMXBean = memoryMXBean;
+    this.threadMXBean = threadMXBean;
+    this.runtimeMXBean = runtimeMXBean;
+  }
+
+  public Map<MetricName, Gauge<?>> getJvmMetrics() {
+    return Stream.concat(
+      getMemoryUsageMetrics().stream(),
+      Stream.concat(
+        getThreadMetrics().stream(),
+        Stream.of(getRuntimeMetrics())
+      ))
+      .collect(Collectors.toMap(JvmMetric::getMetricName, JvmMetric::getMetric));
+  }
+
+  private List<JvmMetric> getMemoryUsageMetrics() {
+
+    return Stream.of(
+      new AbstractMap.SimpleEntry<>(""heap_usage"", memoryMXBean.getHeapMemoryUsage()),
+      new AbstractMap.SimpleEntry<>(""non_heap_usage"", memoryMXBean.getNonHeapMemoryUsage()))
+      .map(entry ->
+        new JvmMetric(
+          MetricNameBuilder.builder().type(MEMORY).name(entry.getKey()).build(),
+          new RatioGauge() {
+
+            @Override
+            protected double getNumerator() {
+              return entry.getValue().getUsed();
+            }
+
+            @Override
+            protected double getDenominator() {
+              return entry.getValue().getMax();
+            }
+          }
+        ))
+      .collect(Collectors.toList());
+
+  }
+
+  private List<JvmMetric> getThreadMetrics() {
+
+    return
+      Stream.concat(
+        Stream.of(
+          new JvmMetric(
+            MetricNameBuilder.builder().type(THREADS).name(""thread_count"").build(),
+            new Gauge<Integer>() {
+              @Override
+              public Integer value() {
+                return threadMXBean.getThreadCount();
+              }
+            }
+          ),
+          new JvmMetric(
+            MetricNameBuilder.builder().type(THREADS).name(""daemon_thread_count"").build(),
+            new Gauge<Integer>() {
+              @Override
+              public Integer value() {
+                return threadMXBean.getDaemonThreadCount();
+              }
+            }
+          )),
+        Stream
+          .of(Thread.State.RUNNABLE, Thread.State.BLOCKED, Thread.State.TIMED_WAITING, Thread.State.TERMINATED)
+          .map(state -> new JvmMetric(
+            MetricNameBuilder.builder().type(THREADS).name(getThreadMetricNameByState(state)).build(),
+            new Gauge<Long>() {
+              @Override
+              public Long value() {
+                return getThreadCountByState(state);
+              }
+            }
+          )))
+        .collect(Collectors.toList());
+  }
+
+  private String getThreadMetricNameByState(Thread.State state) {
+    String name = ""thread-states."";
+    switch (state) {
+      case BLOCKED:
+        name += ""blocked"";
+        break;
+      case RUNNABLE:
+        name += ""runnable"";
+        break;
+      case TIMED_WAITING:
+        name += ""timed_waiting"";
+        break;
+      case TERMINATED:
+        name += ""terminated"";
+        break;
+      case NEW:
+        name += ""new"";
+        break;
+      case WAITING:
+        name += ""waiting"";
+        break;
+    }
+    return name;","[{'comment': 'Why not replace this huge  case with hard-code to `state.name().toLowerCase()` ?\r\n\r\ni.e. `return String.format(""thread-states.%s"", state.name().toLowerCase());`  with adding `@Nonnull` for state argument', 'commenter': 'hapylestat'}, {'comment': 'make sense', 'commenter': 'sziszo'}]"
3163,ambari-server/src/main/java/org/apache/ambari/server/api/handlers/DeleteHandler.java,"@@ -83,8 +83,8 @@ protected ResultMetadata convert(RequestStatusMetaData requestStatusMetaData) {
       return null;
     }
 
-    if (requestStatusMetaData.getClass() != DeleteStatusMetaData.class) {
-      throw new IllegalArgumentException(""RequestStatusDetails is not of type DeleteStatusDetails"");
+    if (!DeleteStatusMetaData.class.isInstance(requestStatusMetaData)) {","[{'comment': 'please, use the form like `!(requestStatusMetaData instanceof DeleteStatusMetaData)`', 'commenter': 'hapylestat'}, {'comment': '+1. done thanks', 'commenter': 'praveenkjvs'}]"
3163,ambari-server/src/main/java/org/apache/ambari/server/api/handlers/DeleteHandler.java,"@@ -83,8 +83,8 @@ protected ResultMetadata convert(RequestStatusMetaData requestStatusMetaData) {
       return null;
     }
 
-    if (requestStatusMetaData.getClass() != DeleteStatusMetaData.class) {
-      throw new IllegalArgumentException(""RequestStatusDetails is not of type DeleteStatusDetails"");
+    if (!DeleteStatusMetaData.class.isInstance(requestStatusMetaData)) {
+      throw new IllegalArgumentException(""RequestStatusDetails is not of type DeleteStatusDetails. requestStatusMetaData.getClass= "" + requestStatusMetaData.getClass());","[{'comment': 'Exception message is not very much descriptive, let\'s make it more clear: \r\n`String.format(""Wrong status details class received -  expecting: %s; actual: %s"", <class name>, <class name>)`', 'commenter': 'hapylestat'}, {'comment': 'Done. Thanks', 'commenter': 'praveenkjvs'}]"
3163,ambari-server/src/main/java/org/apache/ambari/server/controller/AmbariManagementControllerImpl.java,"@@ -3605,12 +3605,13 @@ private void checkIfHostComponentsInDeleteFriendlyState(ServiceComponentHostRequ
     ServiceComponentHost componentHost = component.getServiceComponentHost(request.getHostname());
 
     if (!componentHost.canBeRemoved()) {
-      throw new AmbariException(""Host Component cannot be removed""
+      throw new AmbariException(""Host Component cannot be removede. Component in undeletable state.""","[{'comment': ""Let's replace with something like  `Current host component state prohibiting component removal.`"", 'commenter': 'hapylestat'}, {'comment': 'Done. Thanks,', 'commenter': 'praveenkjvs'}]"
3163,ambari-server/src/main/java/org/apache/ambari/server/state/ServiceComponentImpl.java,"@@ -558,14 +558,15 @@ public void deleteServiceComponentHosts(String hostname, DeleteHostComponentStat
       ServiceComponentHost sch = getServiceComponentHost(hostname);
       LOG.info(""Deleting servicecomponenthost for cluster"" + "", clusterName="" + getClusterName()
           + "", serviceName="" + getServiceName() + "", componentName="" + getName()
-          + "", recoveryEnabled="" + isRecoveryEnabled() + "", hostname="" + sch.getHostName());
+          + "", recoveryEnabled="" + isRecoveryEnabled() + "", hostname="" + sch.getHostName() + "", cuurentState="" + sch.getState());","[{'comment': '`cuurentState` => `state`', 'commenter': 'hapylestat'}]"
3163,ambari-server/src/main/java/org/apache/ambari/server/state/ServiceComponentImpl.java,"@@ -558,14 +558,15 @@ public void deleteServiceComponentHosts(String hostname, DeleteHostComponentStat
       ServiceComponentHost sch = getServiceComponentHost(hostname);
       LOG.info(""Deleting servicecomponenthost for cluster"" + "", clusterName="" + getClusterName()
           + "", serviceName="" + getServiceName() + "", componentName="" + getName()
-          + "", recoveryEnabled="" + isRecoveryEnabled() + "", hostname="" + sch.getHostName());
+          + "", recoveryEnabled="" + isRecoveryEnabled() + "", hostname="" + sch.getHostName() + "", cuurentState="" + sch.getState());
       if (!sch.canBeRemoved()) {
-        throw new AmbariException(""Could not delete hostcomponent from cluster""
+        throw new AmbariException(""Could not delete hostcomponent from cluster, component in undeletable state""
             + "", clusterName="" + getClusterName()
             + "", serviceName="" + getServiceName()
             + "", componentName="" + getName()
             + "", recoveryEnabled="" + isRecoveryEnabled()
-            + "", hostname="" + sch.getHostName());
+            + "", hostname="" + sch.getHostName()
+            + "", currentState="" + sch.getState());","[{'comment': 'While the components have several states like desired and current, the current status is something what anyone will see by default and is used as generic `state`\r\n\r\n`currentState` => `state`', 'commenter': 'hapylestat'}, {'comment': 'Done. Thanks,', 'commenter': 'praveenkjvs'}]"
3163,ambari-server/src/main/java/org/apache/ambari/server/controller/AmbariManagementControllerImpl.java,"@@ -3605,12 +3605,13 @@ private void checkIfHostComponentsInDeleteFriendlyState(ServiceComponentHostRequ
     ServiceComponentHost componentHost = component.getServiceComponentHost(request.getHostname());
 
     if (!componentHost.canBeRemoved()) {
-      throw new AmbariException(""Host Component cannot be removed""
+      throw new AmbariException(""Host Component cannot be removede. Component in undeletable state.""
               + "", clusterName="" + request.getClusterName()
               + "", serviceName="" + request.getServiceName()
               + "", componentName="" + request.getComponentName()
               + "", hostname="" + request.getHostname()
-              + "", request="" + request);
+              + "", request="" + request
+              + "", currentState="" + componentHost.getState());","[{'comment': '`currentState` => `state`', 'commenter': 'hapylestat'}, {'comment': 'Done.. Thanks,', 'commenter': 'praveenkjvs'}]"
3163,ambari-admin/src/main/resources/ui/admin-web/bower.json,"@@ -19,8 +19,5 @@
     ""chai"": ""1.8.0"",
     ""mocha"": ""1.14.0"",
     ""sinon"": ""1.10.3""
-  },","[{'comment': 'not sure, why this file were modified in scope of current task', 'commenter': 'hapylestat'}, {'comment': 'that was by mistake. My bad.\r\n\r\nThanks for taking this up.\r\n\r\n', 'commenter': 'praveenkjvs'}]"
3174,ambari-common/src/main/python/ambari_commons/os_linux.py,"@@ -84,3 +84,7 @@ def os_set_open_files_limit(maxOpenFiles):
 
 def os_getpass(prompt):
   return getpass.unix_getpass(prompt)
+
+def os_is_service_exist(serviceName):
+  status = os.system(""service %s status >/dev/null 2>&1"" % serviceName)","[{'comment': '`service` command is kinda outdated and belongs to the `systemv`,  the thing which should be used - systemd, if system is controlled by it: `systemctl list-units --full -all | grep -Fq ""$SERVICENAME.service""`\r\n\r\nThe problem is that service is not even preinstalled now in a fresh linux distros and may fail.   So we need to check, if system is systemd enabled, use `systectl`, and if system is old - use service', 'commenter': 'hapylestat'}, {'comment': ""ok, I'll fix it."", 'commenter': 'sziszo'}]"
3174,ambari-server/src/main/python/ambari_server/dbConfiguration_linux.py,"@@ -385,12 +385,24 @@ class PGConfig(LinuxDBMSConfig):
     PG_HBA_RELOAD_CMD = AMBARI_SUDO_BINARY + "" %s reload %s"" % (SERVICE_CMD, PG_SERVICE_NAME)
   else:
     SERVICE_CMD = ""/usr/bin/env service""
-    PG_ST_CMD = ""%s %s status"" % (SERVICE_CMD, PG_SERVICE_NAME)
     if os.path.isfile(""/usr/bin/postgresql-setup""):
         PG_INITDB_CMD = ""/usr/bin/postgresql-setup initdb""
     else:
+      if OSCheck.is_suse_family() and not is_service_exist(PG_SERVICE_NAME):
+        versioned_script_paths = glob.glob(""/usr/pgsql-*/bin/postgresql*-setup"")
+        if versioned_script_paths:
+          for versioned_script_path in versioned_script_paths:
+            pgsql_version = re.search(r'pgsql-([0-9]+\.?[0-9]+)', versioned_script_path).group(1)
+            pgsql_service_file_name = ""postgresql-%s"" % pgsql_version
+            if is_service_exist(pgsql_service_file_name):","[{'comment': 'i wonder what happen if there would be several services like postgres-9.5, postgres-9.6 and so on.   The glob may return result in random order and we will basically pick a random service instance. Should we decide, that we will pick latest version or first version and verify which of the services r in the enabled state?', 'commenter': 'hapylestat'}, {'comment': ""Good question, I've been thinking about it. I think we should let the users to decide that. If there are multiple service enabled then the user must specify which one to use, otherwise one of them will be chosen arbitrarily. \r\nSo I just tried to follow the way we use at Redhat."", 'commenter': 'sziszo'}, {'comment': ""well, for the usual installation we may ask user .. but for the silent, where no user interaction allowed - let's pick the latest enabled version. It should be reasonable choice"", 'commenter': 'hapylestat'}, {'comment': ""ok then I'll change it "", 'commenter': 'sziszo'}]"
3174,ambari-common/src/main/python/ambari_commons/os_linux.py,"@@ -84,3 +84,11 @@ def os_set_open_files_limit(maxOpenFiles):
 
 def os_getpass(prompt):
   return getpass.unix_getpass(prompt)
+
+def os_is_service_exist(serviceName):
+  systemManager = os.popen('ps --no-headers -o comm 1').read().strip()","[{'comment': 'we can use this way to detect systemd: https://www.freedesktop.org/software/systemd/man/sd_booted.html\r\n\r\ni.e. check existence of /run/systemd/system/', 'commenter': 'hapylestat'}, {'comment': 'Ok', 'commenter': 'sziszo'}]"
3182,ambari-server/src/main/java/org/apache/ambari/server/api/services/users/AuthService.java,"@@ -0,0 +1,88 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.api.services.users;
+
+import java.util.Collections;
+
+import javax.ws.rs.POST;
+import javax.ws.rs.Path;
+import javax.ws.rs.Produces;
+import javax.ws.rs.core.Context;
+import javax.ws.rs.core.HttpHeaders;
+import javax.ws.rs.core.Response;
+import javax.ws.rs.core.UriInfo;
+
+import org.apache.ambari.server.api.resources.ResourceInstance;
+import org.apache.ambari.server.api.services.BaseService;
+import org.apache.ambari.server.api.services.Request;
+import org.apache.ambari.server.controller.spi.Resource;
+import org.apache.http.HttpStatus;
+
+import io.swagger.annotations.Api;
+import io.swagger.annotations.ApiImplicitParam;
+import io.swagger.annotations.ApiImplicitParams;
+import io.swagger.annotations.ApiOperation;
+import io.swagger.annotations.ApiResponse;
+import io.swagger.annotations.ApiResponses;
+
+/**
+ * Service responsible for auth requests.
+ */
+@Path(""/auth"")
+@Api(value = ""Auth"", description = ""Endpoint for authentication operations"")
+public class AuthService extends BaseService {
+
+  private static final String AUTH_USERS_REQUEST_TYPE = ""org.apache.ambari.server.controller.AuthRequestCreateAuthSwagger"";
+
+  /**
+   * Creates an auth response.
+   * Handles: POST /auth
+   *
+   * @param headers http headers
+   * @param ui      uri info
+   * @return information regarding the requested user and related info
+   */
+  @POST
+  @Produces(""text/plain"")
+  @ApiOperation(value = ""Get auth info for one or more users in a single request"")
+  @ApiImplicitParams({
+          @ApiImplicitParam(dataType = AUTH_USERS_REQUEST_TYPE, paramType = PARAM_TYPE_BODY, allowMultiple = true)
+  })
+  @ApiResponses({
+          @ApiResponse(code = HttpStatus.SC_CREATED, message = MSG_SUCCESSFUL_OPERATION),
+          @ApiResponse(code = HttpStatus.SC_ACCEPTED, message = MSG_REQUEST_ACCEPTED),
+          @ApiResponse(code = HttpStatus.SC_BAD_REQUEST, message = MSG_INVALID_ARGUMENTS),
+          @ApiResponse(code = HttpStatus.SC_NOT_FOUND, message = MSG_RESOURCE_NOT_FOUND),
+          @ApiResponse(code = HttpStatus.SC_CONFLICT, message = MSG_RESOURCE_ALREADY_EXISTS),
+          @ApiResponse(code = HttpStatus.SC_UNAUTHORIZED, message = MSG_NOT_AUTHENTICATED),
+          @ApiResponse(code = HttpStatus.SC_FORBIDDEN, message = MSG_PERMISSION_DENIED),
+          @ApiResponse(code = HttpStatus.SC_INTERNAL_SERVER_ERROR, message = MSG_SERVER_ERROR),
+  })
+  public Response getUsersViaPost(String body, @Context HttpHeaders headers, @Context UriInfo ui) {","[{'comment': 'I have some doubts about  HttpStatus.SC_CREATED, HttpStatus.SC_CONFLICT statuses, mostly they belongs to post request by CRUD logic, however we using here POST request not to create something .. but to login the user.\r\nHttpStatus.SC_ACCEPTED - here is not used, as we r not doing here any async jobs', 'commenter': 'hapylestat'}]"
3182,ambari-server/src/main/java/org/apache/ambari/server/api/services/users/AuthService.java,"@@ -0,0 +1,88 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.api.services.users;
+
+import java.util.Collections;
+
+import javax.ws.rs.POST;
+import javax.ws.rs.Path;
+import javax.ws.rs.Produces;
+import javax.ws.rs.core.Context;
+import javax.ws.rs.core.HttpHeaders;
+import javax.ws.rs.core.Response;
+import javax.ws.rs.core.UriInfo;
+
+import org.apache.ambari.server.api.resources.ResourceInstance;
+import org.apache.ambari.server.api.services.BaseService;
+import org.apache.ambari.server.api.services.Request;
+import org.apache.ambari.server.controller.spi.Resource;
+import org.apache.http.HttpStatus;
+
+import io.swagger.annotations.Api;
+import io.swagger.annotations.ApiImplicitParam;
+import io.swagger.annotations.ApiImplicitParams;
+import io.swagger.annotations.ApiOperation;
+import io.swagger.annotations.ApiResponse;
+import io.swagger.annotations.ApiResponses;
+
+/**
+ * Service responsible for auth requests.
+ */
+@Path(""/auth"")
+@Api(value = ""Auth"", description = ""Endpoint for authentication operations"")
+public class AuthService extends BaseService {
+
+  private static final String AUTH_USERS_REQUEST_TYPE = ""org.apache.ambari.server.controller.AuthRequestCreateAuthSwagger"";
+
+  /**
+   * Creates an auth response.
+   * Handles: POST /auth
+   *
+   * @param headers http headers
+   * @param ui      uri info
+   * @return information regarding the requested user and related info
+   */
+  @POST
+  @Produces(""text/plain"")
+  @ApiOperation(value = ""Get auth info for one or more users in a single request"")","[{'comment': ""let's describe this as User authorization request"", 'commenter': 'hapylestat'}]"
3182,ambari-server/src/main/java/org/apache/ambari/server/controller/internal/AuthResourceProvider.java,"@@ -0,0 +1,177 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.ambari.server.controller.internal;
+
+import java.util.HashSet;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.ambari.server.controller.AmbariManagementController;
+import org.apache.ambari.server.controller.predicate.EqualsPredicate;
+import org.apache.ambari.server.controller.spi.NoSuchParentResourceException;
+import org.apache.ambari.server.controller.spi.NoSuchResourceException;
+import org.apache.ambari.server.controller.spi.Predicate;
+import org.apache.ambari.server.controller.spi.Request;
+import org.apache.ambari.server.controller.spi.RequestStatus;
+import org.apache.ambari.server.controller.spi.Resource;
+import org.apache.ambari.server.controller.spi.ResourceAlreadyExistsException;
+import org.apache.ambari.server.controller.spi.ResourcePredicateEvaluator;
+import org.apache.ambari.server.controller.spi.ResourceProvider;
+import org.apache.ambari.server.controller.spi.SystemException;
+import org.apache.ambari.server.controller.spi.UnsupportedPropertyException;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.collect.ImmutableMap;
+import com.google.common.collect.Sets;
+import com.google.inject.assistedinject.Assisted;
+import com.google.inject.assistedinject.AssistedInject;
+
+/**
+ * Resource provider for auth resources.
+ */
+public class AuthResourceProvider extends AbstractControllerResourceProvider implements ResourcePredicateEvaluator {
+
+  private static final Logger LOG = LoggerFactory.getLogger(AuthResourceProvider.class);
+
+  // ----- Property ID constants ---------------------------------------------
+
+  public static final String AUTH_RESOURCE_CATEGORY = ""Auth"";
+
+  public static final String USERNAME_PROPERTY_ID = ""user_name"";
+
+  public static final String AUTH_USERNAME_PROPERTY_ID = AUTH_RESOURCE_CATEGORY + ""/"" + USERNAME_PROPERTY_ID;
+
+  /**
+   * The key property ids for a User resource.
+   */
+  private static Map<Resource.Type, String> keyPropertyIds = ImmutableMap.<Resource.Type, String>builder()
+      .put(Resource.Type.Auth, AUTH_USERNAME_PROPERTY_ID)
+      .build();
+
+  private static Set<String> propertyIds = Sets.newHashSet(
+      AUTH_USERNAME_PROPERTY_ID
+  );
+
+  /**
+   * Create a new resource provider for the given management controller.
+   */
+  @AssistedInject
+  AuthResourceProvider(@Assisted AmbariManagementController managementController) {
+    super(Resource.Type.Auth, propertyIds, keyPropertyIds, managementController);
+  }
+
+  @Override
+  public RequestStatus createResourcesAuthorized(Request request)","[{'comment': 'Can we simplify the current provider: here we will no data and use it as a safe spot to authorize the user with post request.  The user profile could be pulled afterwards using old api endpoint.', 'commenter': 'hapylestat'}]"
3198,ambari-agent/src/main/java/org/apache/ambari/tools/zk/ZkConnection.java,"@@ -41,6 +41,7 @@ public static ZooKeeper open(String serverAddress, int sessionTimeoutMillis, int
   {
     final CountDownLatch connSignal = new CountDownLatch(1);
     ZooKeeper zooKeeper = new ZooKeeper(serverAddress, sessionTimeoutMillis, new Watcher() {","[{'comment': 'can we use here lambda instead without creating anonymous class?', 'commenter': 'hapylestat'}, {'comment': ""Lambda expressions are not supported at language level '7'. ambari-agent is still using 1.7. Need I change that too? pom.xml under ambari-agent:\r\n![image](https://user-images.githubusercontent.com/13834479/84139655-d8eb6f80-aa82-11ea-8cd1-64ddff3a95bd.png)\r\n"", 'commenter': 'rickyma'}, {'comment': 'yep, as starting from Ambari-2.7 and higher, Ambari not supporting Java 7. So we can update language level without problem', 'commenter': 'hapylestat'}]"
3198,ambari-server/src/main/java/org/apache/ambari/server/security/authorization/Users.java,"@@ -1415,6 +1416,7 @@ public void addKerberosAuthentication(UserEntity userEntity, String principalNam
         UserAuthenticationType.KERBEROS,
         principalName,
         new Validator() {
+          @Override","[{'comment': ""let's do it using lambda approach"", 'commenter': 'hapylestat'}, {'comment': ""It's done."", 'commenter': 'rickyma'}]"
3198,ambari-server/src/main/java/org/apache/ambari/server/security/authorization/Users.java,"@@ -1373,6 +1373,7 @@ public void addJWTAuthentication(UserEntity userEntity, String key, boolean pers
         UserAuthenticationType.JWT,
         key,
         new Validator() {","[{'comment': ""let's do it using lambda approach"", 'commenter': 'hapylestat'}, {'comment': ""It's done."", 'commenter': 'rickyma'}]"
3198,ambari-server/src/main/java/org/apache/ambari/server/security/authorization/Users.java,"@@ -1463,6 +1465,7 @@ public void addLocalAuthentication(UserEntity userEntity, String password, boole
         UserAuthenticationType.LOCAL,
         encodedPassword,
         new Validator() {","[{'comment': ""let's do it using lambda approach"", 'commenter': 'hapylestat'}, {'comment': ""It's done."", 'commenter': 'rickyma'}]"
3198,ambari-server/src/main/java/org/apache/ambari/server/security/authorization/Users.java,"@@ -1504,6 +1507,7 @@ public void addPamAuthentication(UserEntity userEntity, String userName, boolean
         UserAuthenticationType.PAM,
         userName,
         new Validator() {","[{'comment': ""let's do it using lambda approach"", 'commenter': 'hapylestat'}, {'comment': ""It's done."", 'commenter': 'rickyma'}]"
3198,ambari-server/src/main/java/org/apache/ambari/server/security/authorization/Users.java,"@@ -1545,6 +1549,7 @@ public void addLdapAuthentication(UserEntity userEntity, String dn, boolean pers
         UserAuthenticationType.LDAP,
         StringUtils.lowerCase(dn), // DNs are case-insensitive and are stored internally as the bytes of lowercase characters
         new Validator() {","[{'comment': ""let's do it using lambda approach"", 'commenter': 'hapylestat'}, {'comment': ""It's done."", 'commenter': 'rickyma'}]"
3198,ambari-server/src/main/java/org/apache/ambari/server/security/authorization/AmbariLdapBindAuthenticator.java,"@@ -290,6 +290,7 @@ private DirContextOperations setAmbariAdminAttr(DirContextOperations user, LdapS
     LOG.debug(""LDAP login - set admin attr filter: {}"", setAmbariAdminAttrFilter);
 
     AttributesMapper attributesMapper = new AttributesMapper() {","[{'comment': ""let's do it using lambda approach"", 'commenter': 'hapylestat'}, {'comment': ""It's done."", 'commenter': 'rickyma'}]"
3198,ambari-server/src/main/java/org/apache/ambari/server/state/fsm/event/Event.java,"@@ -26,5 +26,6 @@
 
   TYPE getType();
   long getTimestamp();
+  @Override","[{'comment': 'why would we override method declared in the interface?  Well, i have no clue if this method declaration makes any sense at all - coz it is present in the extended interface', 'commenter': 'hapylestat'}, {'comment': ""OK, I'll remove it."", 'commenter': 'rickyma'}]"
3198,ambari-server/src/main/java/org/apache/ambari/server/utils/ScheduledExecutorCompletionService.java,"@@ -35,6 +35,7 @@
       super(task, null);
       this.task = task;
     }
+    @Override
     protected void done() { queue.add(task); }","[{'comment': 'can we also reformat the line to have proper formatting?   (not one-liner)\r\n\r\nthanks', 'commenter': 'hapylestat'}, {'comment': ""It's done."", 'commenter': 'rickyma'}]"
3208,ambari-server/src/main/java/org/apache/ambari/server/controller/AmbariHandlerList.java,"@@ -251,6 +251,10 @@ private WebAppContext getHandler(ViewInstanceEntity viewInstanceDefinition)
     webAppContext.addFilter(new FilterHolder(springSecurityFilter), ""/*"", AmbariServer.DISPATCHER_TYPES);
     webAppContext.setAllowNullPathInfo(true);
 
+    if (webAppContext.getErrorHandler() != null) {
+      webAppContext.getErrorHandler().setShowStacks(false);","[{'comment': 'can we make this configurable via introducing some debug property  or add logging to ambari-server.log? \r\n\r\nIn most cases, such kind of errors r not present in the logs and the only source of them r response from the server', 'commenter': 'hapylestat'}, {'comment': 'yeah sure, it makes sense to make it configurable. Let me check if we already have this type of property or if I need to create a new one.', 'commenter': 'sziszo'}, {'comment': ""I couldn't find a config property that I could use so I introduced a new one to display the error stacks. @hapylestat  please review it"", 'commenter': 'sziszo'}]"
3243,ambari-metrics/ambari-metrics-grafana/ambari-metrics/datasource.js,"@@ -273,7 +273,7 @@ define([
             }
             var metricTransform = !target.transform || target.transform === ""none"" ? '' : '._' + target.transform;
             var seriesAggregator = !target.seriesAggregator || target.seriesAggregator === ""none"" ? '' : '&seriesAggregateFunction=' + target.seriesAggregator;
-            var templatedComponent = (_.isEmpty(tComponent)) ? target.app : tComponent;
+            var templatedComponent = (_.isEmpty(target.app)) ? tComponent : target.app;","[{'comment': 'Hi! Can you explain what is the purpose of this change ? Also, can we be sure that it does not cause issue for other dashboards?', 'commenter': 'payert'}, {'comment': 'If the metric name is changed, please update it in the metrics_whitelist files as well.', 'commenter': 'payert'}, {'comment': '@payert  thanks for your review.\r\n1. the metrics I changed are not new metrics, they have been added in metrics_whitelist, please look at: [metrics_whitelist#529](https://github.com/apache/ambari/blob/9cfc3c1ed4be10cdbae6d1bbab85189ee62624f2/ambari-metrics/ambari-metrics-timelineservice/conf/unix/metrics_whitelist#L529)\r\n\r\n2. In Grafana HDFS NameNode Page, The backend query url will be like this:\r\n""/ws/v1/timeline/metrics?metricNames=rpc.rpc.CallQueueLength._avg&hostname=%&**appId=namenode**&instanceId=&startTime=XXX &endTime=XXX"",   \r\n**The parameter appid in query URL is namenode, but the missing metrics must be queried  from `appid=datanode`.**\r\nappId is obtained from templatedComponentas the history code `var templatedComponent = (_.isEmpty(tComponent)) ? target.app : tComponent;` ,   `tComponent` is a global variable and it will never be empty, so the value of appId is always \'namenode\'.  \r\nTherefore I change the way to init templatedComponent to ensure the templatedComponent use the target.app first. \r\n\r\n\r\nAlthough I have checked other metrics and no problem had been found so far. I am also worried that my modification to `datasource.js` will affect other metrics.  Do you have any Suggestions?\r\n\r\n\r\n\r\n\r\n\r\n', 'commenter': 'echohlne'}, {'comment': ""@akiyamaneko I suggest moving the problematic graphs to the Datanode dashboard. After all it seems that those metrics are now fired by the Datanode instead of the Namenode. This way we can eliminate the need of using 'datanode' appID from the Namenode dashboard."", 'commenter': 'payert'}, {'comment': '@payert  thanks for your suggesion, I have move the releated metrics from namenode to datanodes page and munatal test screet shot also update. please help to review again.  \r\n', 'commenter': 'echohlne'}]"
3243,ambari-server/src/main/resources/common-services/AMBARI_METRICS/0.1.0/package/files/grafana-dashboards/HDP/grafana-hdfs-datanodes.json,"@@ -83,19 +83,18 @@
           ""targets"": [
             {
               ""aggregator"": ""avg"",
-              ""app"": ""datanode"",
+              ""app"": ""namenode"",","[{'comment': 'I guess all the ""app"" fields should be ""datanode"" on this dashboard, aren\'t they ? ', 'commenter': 'payert'}]"
3254,ambari-metrics/ambari-metrics-timelineservice/src/main/java/org/apache/ambari/metrics/core/timeline/upgrade/core/MetricsDataMigrationLauncher.java,"@@ -28,17 +28,15 @@
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 
-import java.io.BufferedReader;
-import java.io.FileInputStream;
-import java.io.FileWriter;
-import java.io.IOException;
-import java.io.InputStreamReader;
+import java.io.*;","[{'comment': ""We don't use wildcards in imports:\r\nhttps://cwiki.apache.org/confluence/display/AMBARI/Coding+Guidelines+for+Ambari"", 'commenter': 'dvitiiuk'}, {'comment': 'fixed', 'commenter': 'payert'}]"
3254,ambari-metrics/ambari-metrics-timelineservice/src/main/java/org/apache/ambari/metrics/core/timeline/upgrade/core/AbstractPhoenixMetricsCopier.java,"@@ -53,7 +53,7 @@ public AbstractPhoenixMetricsCopier(String inputTableName, String outputTableNam
   @Override
   public void run(){
     LOG.info(String.format(""Copying %s metrics from %s to %s"", metricNames, inputTable, outputTable));
-    final long startTimer = System.currentTimeMillis();
+    final long st = System.currentTimeMillis();","[{'comment': 'what is st?   can we rename it to something with more meaning like timerStart   and estimatedTime for example to timerDelta', 'commenter': 'hapylestat'}, {'comment': 'no need for final due to  ""effectively final"" concept since java 8', 'commenter': 'hapylestat'}, {'comment': 'fixed', 'commenter': 'payert'}]"
3254,ambari-metrics/ambari-metrics-timelineservice/src/main/java/org/apache/ambari/metrics/core/timeline/upgrade/core/AbstractPhoenixMetricsCopier.java,"@@ -64,15 +64,15 @@ public void run(){
     } catch (SQLException e) {
       LOG.error(e);
     } finally {
-      final long estimatedTime = System.currentTimeMillis() - startTimer;
+      final long estimatedTime = System.currentTimeMillis() - st;","[{'comment': 'no need for final keyword', 'commenter': 'hapylestat'}]"
3254,ambari-metrics/ambari-metrics-timelineservice/src/main/java/org/apache/ambari/metrics/core/timeline/upgrade/core/AbstractPhoenixMetricsCopier.java,"@@ -64,15 +64,15 @@ public void run(){
     } catch (SQLException e) {
       LOG.error(e);
     } finally {
-      final long estimatedTime = System.currentTimeMillis() - startTimer;
+      final long estimatedTime = System.currentTimeMillis() - st;
       LOG.debug(String.format(""Copying took %s seconds from table %s to table %s for metric names %s"", estimatedTime/ 1000.0, inputTable, outputTable, metricNames));
 
       saveMetricsProgress();
     }
   }
 
   private String getMetricNamesLikeClause() {
-    StringBuilder sb = new StringBuilder();
+    StringBuilder sb = new StringBuilder(256);","[{'comment': 'my god....can we compact sb.append clauses below like  \r\n\r\n      sb\r\n        .append(""METRIC_NAME LIKE \'"")\r\n        .append(metricName)\r\n        .append(""\'"");', 'commenter': 'hapylestat'}, {'comment': 'thx in advance', 'commenter': 'hapylestat'}]"
3254,ambari-metrics/ambari-metrics-timelineservice/src/main/java/org/apache/ambari/metrics/core/timeline/upgrade/core/AbstractPhoenixMetricsCopier.java,"@@ -116,18 +116,19 @@ private void saveMetricsProgress() {
       LOG.info(""Skipping metrics progress save as the file is null"");
       return;
     }
-    synchronized (this.processedMetricsFile) {
-      for (String metricName : metricNames) {
-        try {
+
+    for (String metricName : metricNames) {
+      try {
+        synchronized (this.processedMetricsFile) {
           this.processedMetricsFile.append(inputTable).append("":"").append(metricName).append(System.lineSeparator());
-        } catch (IOException e) {
-          LOG.error(e);
         }
+      } catch (IOException e) {
+        LOG.error(e);
       }
     }
   }
 
-  protected String getQueryHint(Long startTime) {
+  protected String getQueryHint(long startTime) {","[{'comment': 'can we compact the sb.append here ', 'commenter': 'hapylestat'}]"
3254,ambari-metrics/ambari-metrics-timelineservice/src/main/java/org/apache/ambari/metrics/core/timeline/upgrade/core/MetricsDataMigrationLauncher.java,"@@ -114,8 +112,29 @@ public MetricsDataMigrationLauncher(String whitelistedFilePath, String processed
     LOG.info(String.format(""Split metric names into %s batches with size of %s"", metricNamesBatches.size(), batchSize));
   }
 
+  private long calculateStartEpochTime(Long startDay) {
+    final long days;
+    if (startDay == null) {
+      LOG.info(""No starting day have been provided."");","[{'comment': '""No starting day have been provided, using default: {DEFAULT_START_DAYS}""', 'commenter': 'hapylestat'}]"
3262,ambari-metrics/ambari-metrics-timelineservice/src/main/java/org/apache/ambari/metrics/core/timeline/aggregators/AggregatorUtils.java,"@@ -50,6 +50,11 @@
       for (Double value : metricValues.values()) {
         // TODO: Some nulls in data - need to investigate null values from host
         if (value != null) {
+          // As the null values does not participate in the sum of current metric, NaN is handled similarly.","[{'comment': 'why to add new `if` condition if all could be combined in to one like: `if (value != null && !value.isNaN()) {`', 'commenter': 'hapylestat'}]"
3275,ambari-server/src/main/java/org/apache/ambari/server/actionmanager/ActionDBAccessorImpl.java,"@@ -218,8 +218,10 @@ public Request getRequest(long requestId) {
 
     // only request commands which actually need to be aborted; requesting all
     // commands here can cause OOM problems during large requests like upgrades
+    List<HostRoleStatus> statusesToAbort = new ArrayList<>(HostRoleStatus.SCHEDULED_STATES);","[{'comment': 'its better to create new constant for SCHEDULED_STATES + SCHEDULED_STATES instead of combining them each time', 'commenter': 'hapylestat'}, {'comment': 'Added and tested the requested changes.', 'commenter': 'dvitiiuk'}]"
3305,ambari-metrics/ambari-metrics-assembly/pom.xml,"@@ -266,10 +266,45 @@
                             <exclude>lib/*tests.jar</exclude>
                             <exclude>lib/findbugs*.jar</exclude>
                             <exclude>lib/jdk.tools*.jar</exclude>
+                            <exclude>lib/hadoop*.jar</exclude>","[{'comment': 'I guess you are excluding here the jars from the hbase/lib that will be replaced later, right? You can add a comment why are those excluded.', 'commenter': 'payert'}]"
3305,ambari-metrics/ambari-metrics-assembly/pom.xml,"@@ -837,13 +894,43 @@
                 <data>
                   <src>${collector.dir}/target/embedded/${hbase.folder}</src>
                   <type>directory</type>
-                  <excludes>bin/**,bin/*,lib/*tests.jar</excludes>
+                  <excludes>bin/**,bin/*,lib/*tests.jar,lib/hadoop*jar,lib/guava-11.0.2.jar,lib/commons-beanutils-core-1.8.0.jar</excludes>","[{'comment': 'Can we add line break after the comma for better readability ?', 'commenter': 'payert'}, {'comment': ""I tried it and unfortunately it doesn't work. The vafer jdeb [documentation](https://github.com/tcurdt/jdeb/blob/master/docs/maven.md)  also includes the following:\r\n\r\n`includes/exludes: \tA comma seperated list of files to include from the directory or tarball`"", 'commenter': 'vakulamariann'}]"
3305,ambari-metrics/ambari-metrics-timelineservice/pom.xml,"@@ -738,6 +737,25 @@
       <version>3.2</version>
       <scope>test</scope>
     </dependency>
+
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-minicluster</artifactId>
+      <version>${hadoop.version}</version>
+    </dependency>
+
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-client</artifactId>
+      <version>${hadoop.version}</version>
+    </dependency>
+
+    <dependency>","[{'comment': 'The commons-beanutils-1.9.3.jar located in ambari-metrics-assembly/target/embedded/hadoop-3.1.1/share/hadoop/common/lib/\r\nMaybe we could use that instead of introducing a new managed dependency here.', 'commenter': 'payert'}, {'comment': 'I see you are right, I will use that instead. ', 'commenter': 'vakulamariann'}]"
3305,ambari-metrics/ambari-metrics-timelineservice/pom.xml,"@@ -738,6 +737,25 @@
       <version>3.2</version>
       <scope>test</scope>
     </dependency>
+
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>","[{'comment': ""Think about reducing the scope of the dependencies maybe to 'provided'.\r\nAlso you could add comments to elaborate on why the dependencies are required."", 'commenter': 'payert'}, {'comment': 'I will delete these because these jars are used only distributed mode. ', 'commenter': 'vakulamariann'}]"
3365,ambari-common/src/main/python/resource_management/libraries/providers/hdfs_resource.py,"@@ -524,29 +524,33 @@ def _create_file(self, target, source=None, mode=""""):
     file_status = self._get_file_status(target) if target!=self.main_resource.resource.target else self.target_status
     mode = """" if not mode else mode
 
+    kwargs = {'permission': mode} if mode else {}
     if file_status:
+      # Target file exists
       if source:
+        # Upload target file
         length = file_status['length']
         local_file_size = os.stat(source).st_size # TODO: os -> sudo
 
         # TODO: re-implement this using checksums
         if local_file_size == length:
           Logger.info(format(""DFS file {target} is identical to {source}, skipping the copying""))
-          return
         elif not self.main_resource.resource.replace_existing_files:
           Logger.info(format(""Not replacing existing DFS file {target} which is different from {source}, due to replace_existing_files=False""))
-          return
+        else:
+          Logger.info(format(""Reupload file {target} in DFS""))
+
+          self.util.run_command(target, 'CREATE', method='PUT', overwrite=True, assertable_result=False, file_to_put=source, **kwargs)
+          # Get file status again after file reupload
+          self.target_status = self._get_file_status(target)","[{'comment': 'We cannot reassign `self.target_status` here.\r\nBecause when we create a directory, the `target_status` represents the directory status, if we reassign this variable here, we will get the latest file status we uploaded', 'commenter': 'kevinw66'}]"
3378,ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/package/scripts/service_check.py,"@@ -81,12 +81,13 @@ def service_check(self, env):
     params.HdfsResource(None, action=""execute"")
 
     if params.has_journalnode_hosts:
+      journalnode_web_path = ""/journalnode.html""","[{'comment': 'Why we use `journalnode.html` instead of `index.html` here?', 'commenter': 'kevinw66'}, {'comment': ""It's also okay if using index.html but in fact index.html will jump to journalnode.html"", 'commenter': 'smallyao'}]"
3378,ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/package/scripts/service_check.py,"@@ -81,12 +81,13 @@ def service_check(self, env):
     params.HdfsResource(None, action=""execute"")
 
     if params.has_journalnode_hosts:
+      journalnode_web_path = ""/journalnode.html""
       if params.security_enabled:
         for host in params.journalnode_hosts:
           if params.https_only:
-            uri = format(""https://{host}:{journalnode_port}"")
+            uri = format(""https://{host}:{journalnode_port}{journalnode_web_path}"")","[{'comment': ""It'll be better if we can put `/` here between `{journalnode_port}` and `{journalnode_web_path}`"", 'commenter': 'kevinw66'}, {'comment': ""Because the variable journalnode_web_path starts with character /,  we don't need  put /  between {journalnode_port} and journalnode_web_path anymore. I will change this in a different way"", 'commenter': 'smallyao'}]"
3378,ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/package/scripts/service_check.py,"@@ -81,12 +81,13 @@ def service_check(self, env):
     params.HdfsResource(None, action=""execute"")
 
     if params.has_journalnode_hosts:
+      journalnode_web_path = ""/journalnode.html""
       if params.security_enabled:
         for host in params.journalnode_hosts:
           if params.https_only:
-            uri = format(""https://{host}:{journalnode_port}"")
+            uri = format(""https://{host}:{journalnode_port}{journalnode_web_path}"")
           else:
-            uri = format(""http://{host}:{journalnode_port}"")
+            uri = format(""http://{host}:{journalnode_port}{journalnode_web_path}"")","[{'comment': 'Same here', 'commenter': 'kevinw66'}]"
3378,ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/package/files/checkWebUI.py,"@@ -67,19 +67,21 @@ def main():
   parser.add_option(""-p"", ""--port"", dest=""port"", help=""Port of WEB UI to check it availability"")
   parser.add_option(""-s"", ""--https"", dest=""https"", help=""\""True\"" if value of dfs.http.policy is \""HTTPS_ONLY\"""")
   parser.add_option(""-o"", ""--protocol"", dest=""protocol"", help=""Protocol to use when executing https request"")
+  parser.add_option(""-a"", ""--path"", dest=""path"", help=""Path of WEB UI"")
 
   (options, args) = parser.parse_args()
   
   hosts = options.hosts.split(',')
   port = options.port
   https = options.https
   protocol = options.protocol
+  path = options.path
 
   for host in hosts:
-    httpCode = make_connection(host, port, https.lower() == ""true"", protocol)
+    httpCode = make_connection(host, port, https.lower() == ""true"", protocol, path)
 
     if httpCode != 200:","[{'comment': 'Hi @smallyao , after I take a deep look at this bug, I found the error only occurs when Kerberos is disabled and HA is enabled, it reports an error when http response code is not 200, but now it returns 302, which I think also should be healthy status.\r\nCan we simply add one more condition here to make 302 passed too?', 'commenter': 'kevinw66'}, {'comment': ""Hi @kevinw66  It's also okay to make http response code 302 pass if just simply resolve this issue.  I will modify it."", 'commenter': 'smallyao'}]"
3381,ambari-common/src/main/python/resource_management/libraries/functions/conf_select.py,"@@ -44,7 +44,8 @@
 
 def _get_cmd(command, package, version):
   conf_selector_path = stack_tools.get_stack_tool_path(stack_tools.CONF_SELECTOR_NAME)
-  return ('ambari-python-wrap', conf_selector_path, command, '--package', package, '--stack-version', version, '--conf-version', '0')
+  # return ('ambari-python-wrap', conf_selector_path, command, '--package', package, '--stack-version', version, '--conf-version', '0')","[{'comment': 'We should delete the code rather than comment it.', 'commenter': 'kevinw66'}]"
3381,ambari-common/src/main/python/resource_management/libraries/functions/stack_select.py,"@@ -242,6 +242,22 @@ def get_packages(scope, service_name = None, component_name = None):
 
   return packages
 
+def setup_stack_symlinks(version):
+    """"""
+  Invokes <stack-selector-tool> set all against a calculated fully-qualified, ""normalized"" version based on a
+  stack version, such as ""2.3"". This should always be called after a component has been
+  installed to ensure that all HDP pointers are correct. The stack upgrade logic does not
+  interact with this since it's done via a custom command and will not trigger this hook.
+  :return:
+  """"""
+    # get the packages which the stack-select tool should be used on
+    stack_packages = get_packages(PACKAGE_SCOPE_INSTALL)
+    if stack_packages is None:
+        return
+
+        # On parallel command execution this should be executed by a single process at a time.
+    for package in stack_packages:
+        select(package, version)","[{'comment': 'Please use the correct indent in comments', 'commenter': 'kevinw66'}]"
3381,ambari-common/src/main/python/resource_management/libraries/script/script.py,"@@ -892,6 +892,9 @@ def install_packages(self, env):
                           str(config['clusterLevelParams']['stack_version']))
       reload_windows_env()
 
+    from resource_management.libraries.functions import stack_select
+    stack_select.setup_stack_symlinks(Script.get_stack_version())","[{'comment': ""We don't need this since `setup_stack_symlinks` will be called in `after hook`."", 'commenter': 'kevinw66'}]"
3381,ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/properties/stack_packages.json,"@@ -0,0 +1,655 @@
+{
+  ""BIGTOP"": {
+    ""stack-select"": {
+      ""HBASE"": {
+        ""HBASE_CLIENT"": {
+          ""STACK-SELECT-PACKAGE"": ""hbase-client"",
+          ""INSTALL"": [
+            ""hbase-client""
+          ],
+          ""PATCH"": [
+            ""hbase-client""
+          ],
+          ""STANDARD"": [
+            ""hbase-client"",
+            ""phoenix-client"",
+            ""hadoop-client""
+          ]
+        },
+        ""HBASE_MASTER"": {
+          ""STACK-SELECT-PACKAGE"": ""hbase-master"",
+          ""INSTALL"": [
+            ""hbase-master""
+          ],
+          ""PATCH"": [
+            ""hbase-master""
+          ],
+          ""STANDARD"": [
+            ""hbase-master""
+          ]
+        },
+        ""HBASE_REGIONSERVER"": {
+          ""STACK-SELECT-PACKAGE"": ""hbase-regionserver"",
+          ""INSTALL"": [
+            ""hbase-regionserver""
+          ],
+          ""PATCH"": [
+            ""hbase-regionserver""
+          ],
+          ""STANDARD"": [
+            ""hbase-regionserver""
+          ]
+        }
+      },
+      ""HDFS"": {
+        ""DATANODE"": {
+          ""STACK-SELECT-PACKAGE"": ""hadoop-hdfs-datanode"",
+          ""INSTALL"": [
+            ""hadoop-hdfs-datanode""
+          ],
+          ""PATCH"": [
+            ""hadoop-hdfs-datanode""
+          ],
+          ""STANDARD"": [
+            ""hadoop-hdfs-datanode""
+          ]
+        },
+        ""HDFS_CLIENT"": {
+          ""STACK-SELECT-PACKAGE"": ""hadoop-hdfs-client"",
+          ""INSTALL"": [
+            ""hadoop-hdfs-client""
+          ],
+          ""PATCH"": [
+            ""hadoop-hdfs-client""
+          ],
+          ""STANDARD"": [
+            ""hadoop-client""
+          ]
+        },
+        ""NAMENODE"": {
+          ""STACK-SELECT-PACKAGE"": ""hadoop-hdfs-namenode"",
+          ""INSTALL"": [
+            ""hadoop-hdfs-namenode""
+          ],
+          ""PATCH"": [
+            ""hadoop-hdfs-namenode""
+          ],
+          ""STANDARD"": [
+            ""hadoop-hdfs-namenode""
+          ]
+        },
+        ""JOURNALNODE"": {
+          ""STACK-SELECT-PACKAGE"": ""hadoop-hdfs-journalnode"",
+          ""INSTALL"": [
+            ""hadoop-hdfs-journalnode""
+          ],
+          ""PATCH"": [
+            ""hadoop-hdfs-journalnode""
+          ],
+          ""STANDARD"": [
+            ""hadoop-hdfs-journalnode""
+          ]
+        },
+        ""SECONDARY_NAMENODE"": {
+          ""STACK-SELECT-PACKAGE"": ""hadoop-hdfs-secondarynamenode"",
+          ""INSTALL"": [
+            ""hadoop-hdfs-secondarynamenode""
+          ],
+          ""PATCH"": [
+            ""hadoop-hdfs-secondarynamenode""
+          ],
+          ""STANDARD"": [
+            ""hadoop-hdfs-secondarynamenode""
+          ]
+        },
+        ""ZKFC"": {
+          ""STACK-SELECT-PACKAGE"": ""hadoop-hdfs-zkfc"",
+          ""INSTALL"": [
+            ""hadoop-hdfs-zkfc""
+          ],
+          ""PATCH"": [
+            ""hadoop-hdfs-zkfc""
+          ],
+          ""STANDARD"": [
+            ""hadoop-hdfs-zkfc""
+          ]
+        }
+      },
+      ""HIVE"": {
+        ""HIVE_METASTORE"": {
+          ""STACK-SELECT-PACKAGE"": ""hive-metastore"",
+          ""INSTALL"": [
+            ""hive-metastore""
+          ],
+          ""PATCH"": [
+          ""hive-metastore""
+          ],
+          ""STANDARD"": [
+          ""hive-metastore""
+          ]
+        },
+          ""HIVE_SERVER"": {
+          ""STACK-SELECT-PACKAGE"": ""hive-server2"",
+          ""INSTALL"": [
+          ""hive-server2""
+          ],
+          ""PATCH"": [
+          ""hive-server2""
+          ],
+          ""STANDARD"": [
+          ""hive-server2""
+          ]
+        },
+          ""HIVE_CLIENT"": {
+          ""STACK-SELECT-PACKAGE"": ""hive-client"",
+          ""INSTALL"": [
+          ""hive-client""
+          ],
+          ""PATCH"": [
+          ""hive-client""
+          ],
+          ""STANDARD"": [
+          ""hadoop-client""
+          ]
+        }
+        },
+          ""KAFKA"": {
+          ""KAFKA_BROKER"": {
+          ""STACK-SELECT-PACKAGE"": ""kafka-broker"",
+          ""INSTALL"": [
+          ""kafka-broker""
+          ],
+          ""PATCH"": [
+          ""kafka-broker""
+          ],
+          ""STANDARD"": [
+          ""kafka-broker""
+          ]
+        }
+        },
+          ""MAPREDUCE2"": {
+          ""HISTORYSERVER"": {
+          ""STACK-SELECT-PACKAGE"": ""hadoop-mapreduce-historyserver"",
+          ""INSTALL"": [
+          ""hadoop-mapreduce-historyserver""
+          ],
+          ""PATCH"": [
+          ""hadoop-mapreduce-historyserver""
+          ],
+          ""STANDARD"": [
+          ""hadoop-mapreduce-historyserver""
+          ]
+        },
+          ""MAPREDUCE2_CLIENT"": {
+          ""STACK-SELECT-PACKAGE"": ""hadoop-mapreduce-client"",
+          ""INSTALL"": [
+          ""hadoop-mapreduce-client""
+          ],
+          ""PATCH"": [
+          ""hadoop-mapreduce-client""
+          ],
+          ""STANDARD"": [
+          ""hadoop-client""
+          ]
+        }
+        },
+          ""OOZIE"": {
+          ""OOZIE_CLIENT"": {
+          ""STACK-SELECT-PACKAGE"": ""oozie-client"",
+          ""INSTALL"": [
+          ""oozie-client""
+          ],
+          ""PATCH"": [
+          ""oozie-client""
+          ],
+          ""STANDARD"": [
+          ""oozie-client""
+          ]
+        },
+          ""OOZIE_SERVER"": {
+          ""STACK-SELECT-PACKAGE"": ""oozie-server"",
+          ""INSTALL"": [
+          ""oozie-client"",
+          ""oozie-server""
+          ],
+          ""PATCH"": [
+          ""oozie-server"",
+          ""oozie-client""
+          ],
+          ""STANDARD"": [
+          ""oozie-client"",
+          ""oozie-server""
+          ]
+        }
+        },
+          ""RANGER"": {
+          ""RANGER_ADMIN"": {
+          ""STACK-SELECT-PACKAGE"": ""ranger-admin"",
+          ""INSTALL"": [
+          ""ranger-admin""
+          ],
+          ""PATCH"": [
+          ""ranger-admin""
+          ],
+          ""STANDARD"": [
+          ""ranger-admin""
+          ]
+        },
+          ""RANGER_TAGSYNC"": {
+          ""STACK-SELECT-PACKAGE"": ""ranger-tagsync"",
+          ""INSTALL"": [
+          ""ranger-tagsync""
+          ],
+          ""PATCH"": [
+          ""ranger-tagsync""
+          ],
+          ""STANDARD"": [
+          ""ranger-tagsync""
+          ]
+        },
+          ""RANGER_USERSYNC"": {
+          ""STACK-SELECT-PACKAGE"": ""ranger-usersync"",
+          ""INSTALL"": [
+          ""ranger-usersync""
+          ],
+          ""PATCH"": [
+          ""ranger-usersync""
+          ],
+          ""STANDARD"": [
+          ""ranger-usersync""
+          ]
+        }
+        },
+          ""RANGER_KMS"": {
+          ""RANGER_KMS_SERVER"": {
+          ""STACK-SELECT-PACKAGE"": ""ranger-kms"",
+          ""INSTALL"": [
+          ""ranger-kms""
+          ],
+          ""PATCH"": [
+          ""ranger-kms""
+          ],
+          ""STANDARD"": [
+          ""ranger-kms""
+          ]
+        }
+        },
+          ""SPARK"": {
+          ""SPARK_CLIENT"": {
+          ""STACK-SELECT-PACKAGE"": ""spark-client"",
+          ""INSTALL"": [
+          ""spark-client""
+          ],
+          ""PATCH"": [
+          ""spark-client""
+          ],
+          ""STANDARD"": [
+          ""spark-client""
+          ]
+        },
+          ""SPARK_JOBHISTORYSERVER"": {
+          ""STACK-SELECT-PACKAGE"": ""spark-historyserver"",
+          ""INSTALL"": [
+          ""spark-historyserver""
+          ],
+          ""PATCH"": [
+          ""spark-historyserver""
+          ],
+          ""STANDARD"": [
+          ""spark-historyserver""
+          ]
+        },
+          ""SPARK_THRIFTSERVER"": {
+          ""STACK-SELECT-PACKAGE"": ""spark-thriftserver"",
+          ""INSTALL"": [
+          ""spark-thriftserver""
+          ],
+          ""PATCH"": [
+          ""spark-thriftserver""
+          ],
+          ""STANDARD"": [
+          ""spark-thriftserver""
+          ]
+        }
+        },
+          ""FLINK"": {
+          ""FLINK_CLIENT"": {
+          ""STACK-SELECT-PACKAGE"": ""flink-client"",
+          ""INSTALL"": [
+          ""flink-client""
+          ],
+          ""PATCH"": [
+          ""flink-client""
+          ],
+          ""STANDARD"": [
+          ""flink-client""
+          ]
+        },
+          ""FLINK_JOBHISTORYSERVER"": {
+          ""STACK-SELECT-PACKAGE"": ""flink-historyserver"",
+          ""INSTALL"": [
+          ""flink-historyserver""
+          ],
+          ""PATCH"": [
+          ""flink-historyserver""
+          ],
+          ""STANDARD"": [
+          ""flink-historyserver""
+          ]
+        }
+        },
+          ""SOLR"": {
+          ""SOLR_SERVER"": {
+          ""STACK-SELECT-PACKAGE"": ""solr-server"",
+          ""INSTALL"": [
+          ""solr-server""
+          ],
+          ""PATCH"": [
+          ""solr-server""
+          ],
+          ""STANDARD"": [
+          ""solr-server""
+          ]
+        }
+        },
+          ""SUPERSET"": {
+          ""SUPERSET"": {
+          ""STACK-SELECT-PACKAGE"": ""superset"",
+          ""INSTALL"": [
+          ""superset""
+          ],
+          ""PATCH"": [
+          ""superset""
+          ],
+          ""STANDARD"": [
+          ""superset""
+          ]
+        }
+        },
+          ""TEZ"": {
+          ""TEZ_CLIENT"": {
+          ""STACK-SELECT-PACKAGE"": ""tez-client"",
+          ""INSTALL"": [
+          ""tez-client""
+          ],
+          ""PATCH"": [
+          ""tez-client""
+          ],
+          ""STANDARD"": [
+          ""hadoop-client""
+          ]
+        }
+        },
+          ""YARN"": {
+          ""NODEMANAGER"": {
+          ""STACK-SELECT-PACKAGE"": ""hadoop-yarn-nodemanager"",
+          ""INSTALL"": [
+          ""hadoop-yarn-nodemanager""
+          ],
+          ""PATCH"": [
+          ""hadoop-yarn-nodemanager""
+          ],
+          ""STANDARD"": [
+          ""hadoop-yarn-nodemanager""
+          ]
+        },
+          ""RESOURCEMANAGER"": {
+          ""STACK-SELECT-PACKAGE"": ""hadoop-yarn-resourcemanager"",
+          ""INSTALL"": [
+          ""hadoop-yarn-resourcemanager""
+          ],
+          ""PATCH"": [
+          ""hadoop-yarn-resourcemanager""
+          ],
+          ""STANDARD"": [
+          ""hadoop-yarn-resourcemanager""
+          ]
+        },
+          ""YARN_CLIENT"": {
+          ""STACK-SELECT-PACKAGE"": ""hadoop-yarn-client"",
+          ""INSTALL"": [
+          ""hadoop-yarn-client""
+          ],
+          ""PATCH"": [
+          ""hadoop-yarn-client""
+          ],
+          ""STANDARD"": [
+          ""hadoop-client""
+          ]
+        }
+        },
+          ""ZEPPELIN"": {
+          ""ZEPPELIN_MASTER"": {
+          ""STACK-SELECT-PACKAGE"": ""zeppelin-server"",
+          ""INSTALL"": [
+          ""zeppelin-server""
+          ],
+          ""PATCH"": [
+          ""zeppelin-server""
+          ],
+          ""STANDARD"": [
+          ""zeppelin-server""
+          ]
+        }
+        },
+          ""ZOOKEEPER"": {
+          ""ZOOKEEPER_CLIENT"": {
+          ""STACK-SELECT-PACKAGE"": ""zookeeper-client"",
+          ""INSTALL"": [
+          ""zookeeper-client""
+          ],
+          ""PATCH"": [
+          ""zookeeper-client""
+          ],
+          ""STANDARD"": [
+          ""zookeeper-client""
+          ]
+        },
+          ""ZOOKEEPER_SERVER"": {
+          ""STACK-SELECT-PACKAGE"": ""zookeeper-server"",
+          ""INSTALL"": [
+          ""zookeeper-server""
+          ],
+          ""PATCH"": [
+          ""zookeeper-server""
+          ],
+          ""STANDARD"": [
+          ""zookeeper-server""
+          ]
+        }
+        }
+        },
+          ""conf-select"": {
+          ""hadoop"": [
+          {
+          ""conf_dir"": ""/etc/hadoop/conf"",
+          ""current_dir"": ""{0}/current/hadoop-client/conf"",
+          ""component"": ""hadoop-client""
+        }
+          ],
+          ""hbase"": [
+          {
+          ""conf_dir"": ""/etc/hbase/conf"",
+          ""current_dir"": ""{0}/current/hbase-client/conf"",
+          ""component"": ""hbase-client""
+        }
+          ],
+          ""hive"": [
+          {
+          ""conf_dir"": ""/etc/hive/conf"",
+          ""current_dir"": ""{0}/current/hive-client/conf"",
+          ""component"": ""hive-client""
+        }
+          ],
+          ""hive-hcatalog"": [
+          {
+          ""conf_dir"": ""/etc/hive-webhcat/conf"",
+          ""prefix"": ""/etc/hive-webhcat"",
+          ""current_dir"": ""{0}/current/hive-webhcat/etc/webhcat"",
+          ""component"": ""hive-webhcat""
+        },
+          {
+          ""conf_dir"": ""/etc/hive-hcatalog/conf"",
+          ""prefix"": ""/etc/hive-hcatalog"",
+          ""current_dir"": ""{0}/current/hive-webhcat/etc/hcatalog"",
+          ""component"": ""hive-webhcat""
+        }
+          ],
+          ""kafka"": [
+          {
+          ""conf_dir"": ""/etc/kafka/conf"",
+          ""current_dir"": ""{0}/current/kafka-broker/conf"",
+          ""component"": ""kafka-broker""
+        }
+          ],
+          ""oozie"": [
+          {
+          ""conf_dir"": ""/etc/oozie/conf"",
+          ""current_dir"": ""{0}/current/oozie-client/conf"",
+          ""component"": ""oozie-client""
+        }
+          ],
+          ""phoenix"": [
+          {
+          ""conf_dir"": ""/etc/phoenix/conf"",
+          ""current_dir"": ""{0}/current/phoenix-client/conf"",
+          ""component"": ""phoenix-client""
+        }
+          ],
+          ""ranger-admin"": [
+          {
+          ""conf_dir"": ""/etc/ranger/admin/conf"",
+          ""current_dir"": ""{0}/current/ranger-admin/conf"",
+          ""component"": ""ranger-admin""
+        }
+          ],
+          ""ranger-kms"": [
+          {
+          ""conf_dir"": ""/etc/ranger/kms/conf"",
+          ""current_dir"": ""{0}/current/ranger-kms/conf"",
+          ""component"": ""ranger-kms""
+        }
+          ],
+          ""ranger-tagsync"": [
+          {
+          ""conf_dir"": ""/etc/ranger/tagsync/conf"",
+          ""current_dir"": ""{0}/current/ranger-tagsync/conf"",
+          ""component"": ""ranger-tagsync""
+        }
+          ],
+          ""ranger-usersync"": [
+          {
+          ""conf_dir"": ""/etc/ranger/usersync/conf"",
+          ""current_dir"": ""{0}/current/ranger-usersync/conf"",
+          ""component"": ""ranger-usersync""
+        }
+          ],
+          ""spark"": [
+          {
+          ""conf_dir"": ""/etc/spark/conf"",
+          ""current_dir"": ""{0}/current/spark-client/conf"",
+          ""component"": ""spark-client""
+        }
+          ],
+          ""flink"": [
+          {
+          ""conf_dir"": ""/etc/flink/conf"",
+          ""current_dir"": ""{0}/current/flink-client/conf"",
+          ""component"": ""flink-client""
+        }
+          ],
+          ""solr"": [
+          {
+          ""conf_dir"": ""/etc/solr/conf"",
+          ""current_dir"": ""{0}/current/solr-server/conf"",
+          ""component"": ""solr-server""
+        }
+          ],
+          ""superset"": [
+          {
+          ""conf_dir"": ""/etc/superset/conf"",
+          ""current_dir"": ""{0}/current/superset/conf"",
+          ""component"": ""superset""
+        }
+          ],
+          ""tez"": [
+          {
+          ""conf_dir"": ""/etc/tez/conf"",
+          ""current_dir"": ""{0}/current/tez-client/conf"",
+          ""component"": ""tez-client""
+        }
+          ],
+          ""zeppelin"": [
+          {
+          ""conf_dir"": ""/etc/zeppelin/conf"",
+          ""current_dir"": ""{0}/current/zeppelin-server/conf"",
+          ""component"": ""zeppelin-server""
+        }
+          ],
+          ""zookeeper"": [
+          {
+          ""conf_dir"": ""/etc/zookeeper/conf"",
+          ""current_dir"": ""{0}/current/zookeeper-client/conf"",
+          ""component"": ""zookeeper-client""
+        }
+          ]
+        },
+          ""conf-select-patching"": {
+          ""HBASE"": {
+          ""packages"": [""hbase""]
+        },
+          ""HDFS"": {
+          ""packages"": []
+        },
+          ""HIVE"": {
+          ""packages"": [""hive"", ""hive-hcatalog""]
+        },
+          ""KAFKA"": {
+          ""packages"": [""kafka""]
+        },
+          ""MAPREDUCE2"": {
+          ""packages"": []
+        },
+          ""OOZIE"": {
+          ""packages"": [""oozie""]
+        },
+          ""RANGER"": {
+          ""packages"": [""ranger-admin"", ""ranger-usersync"", ""ranger-tagsync""]
+        },
+          ""RANGER_KMS"": {
+          ""packages"": [""ranger-kms""]
+        },
+          ""SPARK"": {
+          ""packages"": [""spark""]
+        },
+          ""FLINK"": {
+          ""packages"": [""flink""]
+        },
+          ""SOLR"": {
+          ""packages"": [""solr""]
+        },
+          ""SUPERSET"": {
+          ""packages"": [""superset""]
+        },
+          ""TEZ"": {
+          ""packages"": [""tez""]
+        },
+          ""YARN"": {
+          ""packages"": []
+        },
+          ""ZEPPELIN"": {
+          ""packages"": [""zeppelin""]
+        },
+          ""ZOOKEEPER"": {
+          ""packages"": [""zookeeper""]
+        }
+        },
+          ""upgrade-dependencies"" : {
+          ""ATLAS"": [""STORM""],
+          ""HIVE"": [""TEZ"", ""MAPREDUCE2"", ""SQOOP""],
+          ""TEZ"": [""HIVE""],
+          ""MAPREDUCE2"": [""HIVE""],
+          ""OOZIE"": [""MAPREDUCE2""]
+        }
+        }
+        }","[{'comment': '`Oozie`, `Storm`, `Sqoop` and `Superset` can be deleted, also use correct indent here.', 'commenter': 'kevinw66'}]"
3421,ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SOLR/package/scripts/params.py,"@@ -84,6 +92,10 @@ def get_name_from_principal(principal):
 # Only supporting SolrCloud mode - so hardcode those options
 solr_cloudmode = 'true'
 solr_dir = '/usr/lib/solr'
+
+if stack_version_formatted and check_stack_feature(StackFeature.ROLLING_UPGRADE, stack_version_formatted):
+  solr_dir = format(""{stack_root}/current/solr-server"")","[{'comment': 'Can we use `component_directory` here? Like [Zookeeper](https://github.com/apache/ambari/blob/trunk/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/ZOOKEEPER/package/scripts/params_linux.py#L55)?', 'commenter': 'kevinw66'}, {'comment': ""That's a good reminder, but I found `stack_packages.json` has the map relation, should we obtain it from here?\r\ne.g.\r\n![1666225375739](https://user-images.githubusercontent.com/16263438/196828556-1973491c-6c00-4f77-81c2-b391bf6859f9.png)\r\n"", 'commenter': 'timyuer'}]"
3590,ambari-server/src/main/java/org/apache/ambari/server/checks/InstallPackagesCheck.java,"@@ -78,18 +78,23 @@ public UpgradeCheckResult perform(UpgradeCheckRequest request) throws AmbariExce
 
     final StackId targetStackId = new StackId(repositoryVersion.getStackId());
 
-    if (!repositoryVersion.getVersion().matches(""^\\d+(\\.\\d+)*\\-\\d+$"")) {
-      String message = MessageFormat.format(
-          ""The Repository Version {0} for Stack {1} must contain a \""-\"" followed by a build number. ""
-              + ""Make sure that another registered repository does not have the same repo URL or ""
-              + ""shares the same build number. Next, try reinstalling the Repository Version."",
-          repositoryVersion.getVersion(), targetStackId.getStackVersion());
-
-      result.getFailedOn().add(""Repository Version "" + repositoryVersion.getVersion());
-      result.setStatus(UpgradeCheckStatus.FAIL);
-      result.setFailReason(message);
-      return result;
-    }
+    /**
+     * Because the version for BigTop is BIGTOP-3.2.0,
+     * But the version for HDP is HDP-x.x.x.x-x,
+     * So to be compatible with both HDP and BIGTOP, comment out this.
+     */
+    // if (!repositoryVersion.getVersion().matches(""^\\d+(\\.\\d+)*\\-\\d+$"")) {
+    //   String message = MessageFormat.format(
+    //       ""The Repository Version {0} for Stack {1} must contain a \""-\"" followed by a build number. ""
+    //           + ""Make sure that another registered repository does not have the same repo URL or ""
+    //           + ""shares the same build number. Next, try reinstalling the Repository Version."",
+    //       repositoryVersion.getVersion(), targetStackId.getStackVersion());
+
+    //   result.getFailedOn().add(""Repository Version "" + repositoryVersion.getVersion());
+    //   result.setStatus(UpgradeCheckStatus.FAIL);
+    //   result.setFailReason(message);
+    //   return result;
+    // }","[{'comment': 'We can just delete the code', 'commenter': 'kevinw66'}, {'comment': 'Update as comment.', 'commenter': 'timyuer'}]"
