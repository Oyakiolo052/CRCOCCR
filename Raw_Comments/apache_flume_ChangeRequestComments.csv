Pull,Path,Diff_hunk,Comment
91,flume-ng-doc/sphinx/FlumeUserGuide.rst,"@@ -3920,7 +3920,7 @@ Property Name          Default      Description
 **type**               --           The component type name has to be ``remove_header``
 withName               --           Name of the header to remove
 fromList               --           List of headers to remove, separated with the separator specified with from.list.separator
-fromListSeparator    \s*,\s*      Regular expression used to separate multiple header names in the list specified using from.list
+fromListSeparator      \s*,\s*      Regular expression used to separate multiple header names in the list specified using from.list","[{'comment': '`\\\\s*,\\\\s*` should be in the default column, `\\s*,\\s*` will be translated to `s*,s*`. (Double backslash is needed to display a single one)', 'commenter': 'adenes'}]"
92,flume-ng-channels/flume-kafka-channel/src/main/java/org/apache/flume/channel/kafka/KafkaChannel.java,"@@ -564,14 +564,22 @@ protected void doCommit() throws InterruptedException {
                   ex);
         }
       } else {
+        //event taken ensures that we have collected events in this transaction","[{'comment': 'nit: missing space after //', 'commenter': 'simonati'}]"
92,flume-ng-channels/flume-kafka-channel/src/main/java/org/apache/flume/channel/kafka/KafkaChannel.java,"@@ -564,14 +564,22 @@ protected void doCommit() throws InterruptedException {
                   ex);
         }
       } else {
+        //event taken ensures that we have collected events in this transaction
+        // before committing
         if (consumerAndRecords.get().failedEvents.isEmpty() && eventTaken) {","[{'comment': 'Naive question: could you please tell why the else path is not needed. Condition to the if statement suggests that we silently ignore some failure scenario.', 'commenter': 'simonati'}, {'comment': 'See comment', 'commenter': 'jholoman'}]"
92,flume-ng-channels/flume-kafka-channel/src/main/java/org/apache/flume/channel/kafka/KafkaChannel.java,"@@ -677,34 +685,45 @@ private Event deserializeValue(byte[] value, boolean parseAsFlumeEvent) throws I
     }
 
     void poll() {
+      logger.trace(""polling with timeout: {}ms channel-{}"", pollTimeout, channelUUID);
       this.records = consumer.poll(pollTimeout);
       this.recordIterator = records.iterator();
-      logger.trace(""polling"");
+      logger.debug(""returned {} records from last poll channel-{}"", records.count(), channelUUID);
     }
 
     void commitOffsets() {
       this.consumer.commitSync(offsets);
     }
 
-    // This will reset the latest assigned partitions to the last committed offsets;
+    String getOffsetMapString() {
+      StringBuilder sb = new StringBuilder();
+      sb.append(""Current offsets map: "");
+      for (TopicPartition tp : offsets.keySet()) {
+        sb.append(""p"").append(tp.partition()).append(""-"")
+            .append(offsets.get(tp).offset()).append("" "");
+      }
+      return sb.toString();
+    }
 
-    public void printCurrentAssignment() {
+    // This prints the current committed offsets when debug is enabled
+    String getCommittedOffsetsString() {
       StringBuilder sb = new StringBuilder();
+      sb.append(""Channel "").append(this.uuid)
+          .append("" committed: "");
       for (TopicPartition tp : this.consumer.assignment()) {
         try {
-          sb.append(""Committed: ["").append(tp).append("","")
+          sb.append(""["").append(tp).append("","")
               .append(this.consumer.committed(tp).offset())
-              .append("","").append(this.consumer.committed(tp).metadata()).append(""]"");","[{'comment': 'is consumer.committed(tp).metadata() not useful anymore?', 'commenter': 'simonati'}, {'comment': ""I haven't found a good use for it, and it's just polluting the logs. "", 'commenter': 'jholoman'}, {'comment': 'Thanks for the explanation. Fine by me.', 'commenter': 'simonati'}]"
92,flume-ng-channels/flume-kafka-channel/src/main/java/org/apache/flume/channel/kafka/KafkaChannel.java,"@@ -677,34 +685,45 @@ private Event deserializeValue(byte[] value, boolean parseAsFlumeEvent) throws I
     }
 
     void poll() {
+      logger.trace(""polling with timeout: {}ms channel-{}"", pollTimeout, channelUUID);
       this.records = consumer.poll(pollTimeout);
       this.recordIterator = records.iterator();
-      logger.trace(""polling"");
+      logger.debug(""returned {} records from last poll channel-{}"", records.count(), channelUUID);
     }
 
     void commitOffsets() {
       this.consumer.commitSync(offsets);
     }
 
-    // This will reset the latest assigned partitions to the last committed offsets;
+    String getOffsetMapString() {
+      StringBuilder sb = new StringBuilder();
+      sb.append(""Current offsets map: "");
+      for (TopicPartition tp : offsets.keySet()) {
+        sb.append(""p"").append(tp.partition()).append(""-"")
+            .append(offsets.get(tp).offset()).append("" "");
+      }
+      return sb.toString();
+    }
 
-    public void printCurrentAssignment() {
+    // This prints the current committed offsets when debug is enabled
+    String getCommittedOffsetsString() {","[{'comment': 'I think this should be private', 'commenter': 'simonati'}]"
92,flume-ng-channels/flume-kafka-channel/src/main/java/org/apache/flume/channel/kafka/KafkaChannel.java,"@@ -677,34 +685,45 @@ private Event deserializeValue(byte[] value, boolean parseAsFlumeEvent) throws I
     }
 
     void poll() {
+      logger.trace(""polling with timeout: {}ms channel-{}"", pollTimeout, channelUUID);
       this.records = consumer.poll(pollTimeout);
       this.recordIterator = records.iterator();
-      logger.trace(""polling"");
+      logger.debug(""returned {} records from last poll channel-{}"", records.count(), channelUUID);
     }
 
     void commitOffsets() {
       this.consumer.commitSync(offsets);
     }
 
-    // This will reset the latest assigned partitions to the last committed offsets;
+    String getOffsetMapString() {","[{'comment': 'I think this should be private', 'commenter': 'simonati'}, {'comment': 'Agree', 'commenter': 'jholoman'}]"
92,flume-ng-channels/flume-kafka-channel/src/main/java/org/apache/flume/channel/kafka/KafkaChannel.java,"@@ -564,14 +564,22 @@ protected void doCommit() throws InterruptedException {
                   ex);
         }
       } else {
+        //event taken ensures that we have collected events in this transaction
+        // before committing
         if (consumerAndRecords.get().failedEvents.isEmpty() && eventTaken) {
+          logger.trace(""about to commit batch."");
           long startTime = System.nanoTime();
           consumerAndRecords.get().commitOffsets();
           long endTime = System.nanoTime();
           counter.addToKafkaCommitTimer((endTime - startTime) / (1000 * 1000));
-          consumerAndRecords.get().printCurrentAssignment();
+          counter.addToEventTakeSuccessCount(Long.valueOf(events.get().size()));","[{'comment': 'nit: could you please remove Long.valueOf? it is not needed (here and in the doRollback function)', 'commenter': 'simonati'}, {'comment': 'Sure', 'commenter': 'jholoman'}, {'comment': 'This needs to be moved back out of the if statement.', 'commenter': 'jholoman'}]"
92,flume-ng-channels/flume-kafka-channel/src/main/java/org/apache/flume/channel/kafka/KafkaChannel.java,"@@ -497,21 +495,18 @@ protected Event doTake() throws InterruptedException {
             e = deserializeValue(record.value(), parseAsFlumeEvent);
             TopicPartition tp = new TopicPartition(record.topic(), record.partition());
             OffsetAndMetadata oam = new OffsetAndMetadata(record.offset() + 1, batchUUID);
+            //we should refactor this into a method in the subclass
             consumerAndRecords.get().offsets.put(tp, oam);
 
-            if (logger.isTraceEnabled()) {","[{'comment': 'Have you measuered the performance impact of this change? Based on general guidance this should be the other way around except perf measurement shows otherwise. I would like to ask you moving back the log level checking condition if you do relatively heavy computation for calculating the argument list for logging function calls on critical paths (doTake is considered as critical path)\r\n\r\nPlease see: http://logging.apache.org/log4j/1.2/manual.html#performance', 'commenter': 'simonati'}, {'comment': ""My understanding is that using the {} means they aren't evaluated if the logging mode doesn't match. Am I wrong about this?\r\n\r\nhttp://www.slf4j.org/faq.html#logging_performance"", 'commenter': 'jholoman'}, {'comment': ""You can save the cost of string concatenation with {} if log level doesn't match (which is already a gain). In java before calling any function, its argument list is evaluated. So JVM will execute consumerAndRecords.get().getOffsetMapString() statement before invoking logger.trace(arg1, arg2) to make sure arg2 is computed so that it can be passed into."", 'commenter': 'simonati'}, {'comment': ""You can save the cost of string concatenation with {} if log level doesn't match (which is already a gain). In java before calling any function, its argument list is evaluated. So JVM will execute consumerAndRecords.get().getOffsetMapString() statement before invoking logger.trace(arg1, arg2) to make sure arg2 is computed so that it can be passed into."", 'commenter': 'simonati'}]"
92,flume-ng-channels/flume-kafka-channel/src/main/java/org/apache/flume/channel/kafka/KafkaChannel.java,"@@ -154,8 +155,7 @@ public void stop() {
     producer.close();
     counter.stop();
     super.stop();
-    logger.info(""Kafka channel {} stopped. Metrics: {}"", getName(),","[{'comment': 'I would like to ask you to revert this line. I consider shutdown metrics to be quite useful.', 'commenter': 'simonati'}, {'comment': 'They are already printed in the Flume Agent log when the channel closes:\r\n\r\norg.apache.flume.instrumentation.MonitoredCounterGroup: Shutdown Metric for type: CHANNEL, name: kafka-channel-1. channel.start.time == 1480359313437\r\n\r\nSo this was redundant.', 'commenter': 'jholoman'}, {'comment': 'Indeed. conter.stop() logs this data at info level already. ', 'commenter': 'simonati'}]"
92,flume-ng-channels/flume-kafka-channel/src/main/java/org/apache/flume/channel/kafka/KafkaChannel.java,"@@ -497,21 +495,18 @@ protected Event doTake() throws InterruptedException {
             e = deserializeValue(record.value(), parseAsFlumeEvent);
             TopicPartition tp = new TopicPartition(record.topic(), record.partition());
             OffsetAndMetadata oam = new OffsetAndMetadata(record.offset() + 1, batchUUID);
+            //we should refactor this into a method in the subclass","[{'comment': 'nit: missing whitespace after //', 'commenter': 'simonati'}]"
92,flume-ng-channels/flume-kafka-channel/src/main/java/org/apache/flume/channel/kafka/KafkaChannel.java,"@@ -482,11 +483,8 @@ protected Event doTake() throws InterruptedException {
       if (!consumerAndRecords.get().failedEvents.isEmpty()) {
         e = consumerAndRecords.get().failedEvents.removeFirst();
       } else {
-
-        if (logger.isDebugEnabled()) {","[{'comment': 'Please see my comment on logging performance.', 'commenter': 'simonati'}]"
92,flume-ng-channels/flume-kafka-channel/src/main/java/org/apache/flume/channel/kafka/KafkaChannel.java,"@@ -497,21 +495,18 @@ protected Event doTake() throws InterruptedException {
             e = deserializeValue(record.value(), parseAsFlumeEvent);
             TopicPartition tp = new TopicPartition(record.topic(), record.partition());
             OffsetAndMetadata oam = new OffsetAndMetadata(record.offset() + 1, batchUUID);
+            //we should refactor this into a method in the subclass
             consumerAndRecords.get().offsets.put(tp, oam);
 
-            if (logger.isTraceEnabled()) {
-              logger.trace(""Took offset: {}"", consumerAndRecords.get().offsets.toString());
-            }
+            logger.trace(""Committted Offsets: {}"", consumerAndRecords.get().getOffsetMapString());
 
             //Add the key to the header
             if (record.key() != null) {
               e.getHeaders().put(KEY_HEADER, record.key());
             }
 
-            if (logger.isDebugEnabled()) {
-              logger.debug(""Processed output from partition {} offset {}"",
-                           record.partition(), record.offset());
-            }
+            logger.debug(""Processed output from partition {} offset {}"",
+                record.partition(), record.offset());","[{'comment': 'This looks good as partition() and offset() only returns a value and performs no computation.', 'commenter': 'simonati'}]"
92,flume-ng-channels/flume-kafka-channel/src/main/java/org/apache/flume/channel/kafka/KafkaChannel.java,"@@ -564,15 +560,22 @@ protected void doCommit() throws InterruptedException {
                   ex);
         }
       } else {
+        // event taken ensures that we have collected events in this transaction
+        // before committing
         if (consumerAndRecords.get().failedEvents.isEmpty() && eventTaken) {
+          logger.trace(""About to commit batch."");
           long startTime = System.nanoTime();
           consumerAndRecords.get().commitOffsets();
           long endTime = System.nanoTime();
           counter.addToKafkaCommitTimer((endTime - startTime) / (1000 * 1000));
-          consumerAndRecords.get().printCurrentAssignment();
+          logger.debug(""{}"", consumerAndRecords.get().getCommittedOffsetsString());","[{'comment': 'Please see my comment on logging performance.', 'commenter': 'simonati'}]"
92,flume-ng-channels/flume-kafka-channel/src/main/java/org/apache/flume/channel/kafka/KafkaChannel.java,"@@ -373,6 +372,7 @@ private void migrateOffsets() {
   }
 
   private void decommissionConsumerAndRecords(ConsumerAndRecords c) {
+    c.consumer.wakeup();","[{'comment': ""What is the intention here?\r\nFrom the javadoc of KafkaConsumer.close() it seems that wakeup won't interrupt the stop. So I would assume something else should have been interrupted by this statement. In that case wouldn't be the desired order: wakeup first then close?\r\n\r\n> org.apache.kafka.clients.consumer.KafkaConsumer\r\n> public void close()\r\n> Close the consumer, waiting indefinitely for any needed cleanup. If auto-commit is enabled, this will commit the current offsets. Note that wakeup() cannot be use to interrupt close. "", 'commenter': 'simonati'}, {'comment': 'The intention is to break out of the blocking thread in poll(), before closing the consumer.', 'commenter': 'jholoman'}, {'comment': 'Please ignore my comment I misread this change. It is completely fine in the way you done it originally.', 'commenter': 'simonati'}, {'comment': 'Please ignore my prev comment I misread this change. Sorry for that. It is completely fine in the way you done it originally.', 'commenter': 'simonati'}]"
92,flume-ng-channels/flume-kafka-channel/src/main/java/org/apache/flume/channel/kafka/KafkaChannel.java,"@@ -564,15 +560,22 @@ protected void doCommit() throws InterruptedException {
                   ex);
         }
       } else {
+        // event taken ensures that we have collected events in this transaction
+        // before committing
         if (consumerAndRecords.get().failedEvents.isEmpty() && eventTaken) {
+          logger.trace(""About to commit batch."");
           long startTime = System.nanoTime();
           consumerAndRecords.get().commitOffsets();
           long endTime = System.nanoTime();
           counter.addToKafkaCommitTimer((endTime - startTime) / (1000 * 1000));
-          consumerAndRecords.get().printCurrentAssignment();
+          logger.debug(""{}"", consumerAndRecords.get().getCommittedOffsetsString());
+        }
+
+        int takes = events.get().size();","[{'comment': 'Have you considered moving this into the \r\n> if (consumerAndRecords.get().failedEvents.isEmpty() && eventTaken) {\r\n\r\nblock?', 'commenter': 'simonati'}, {'comment': ""Maybe? I don't have a strong opinion on it and would prefer to make as few changes as possible. "", 'commenter': 'jholoman'}]"
105,flume-ng-sinks/flume-ng-kafka-sink/src/main/java/org/apache/flume/sink/kafka/KafkaSinkConstants.java,"@@ -33,7 +33,10 @@ Licensed to the Apache Software Foundation (ASF) under one or more
       KAFKA_PREFIX + CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG;
 
   public static final String KEY_HEADER = ""key"";
-  public static final String TOPIC_HEADER = ""topic"";
+  public static final String TOPIC_DEFAULT_HEADER_NAME = ""topic"";","[{'comment': 'Please rename it to DEFAULT_<< property name >> to follow convention.', 'commenter': 'simonati'}, {'comment': 'Also I think TOPIC_OVERRIDE_HEADER and DEFAULT_TOPIC_OVERRIDE_HEADER naming might be better. Would you consider changing it?', 'commenter': 'simonati'}, {'comment': 'Done', 'commenter': 'tmgstevens'}]"
105,flume-ng-sinks/flume-ng-kafka-sink/src/main/java/org/apache/flume/sink/kafka/KafkaSink.java,"@@ -171,8 +173,8 @@ public Status process() throws EventDeliveryException {
         byte[] eventBody = event.getBody();
         Map<String, String> headers = event.getHeaders();
 
-        eventTopic = headers.get(TOPIC_HEADER);
-        if (eventTopic == null) {
+        eventTopic = headers.get(topicHeader);","[{'comment': 'Would you mind rephrasing it to \r\n```java\r\nif (allowTopicHeader) {\r\n    eventTopic = headers.get(topicHeader);  \r\n    if (eventTopic == null) {\r\n       log.debug(""usage of topicheader was configured but header was not available {}"", info about the event); \r\n       eventTopic = topic;\r\n    }\r\n} else {\r\n    eventTopic = topic;\r\n}\r\n```', 'commenter': 'simonati'}, {'comment': 'Done', 'commenter': 'tmgstevens'}]"
105,flume-ng-sinks/flume-ng-kafka-sink/src/main/java/org/apache/flume/sink/kafka/KafkaSink.java,"@@ -116,6 +115,9 @@ Licensed to the Apache Software Foundation (ASF) under one or more
   private boolean useAvroEventFormat;
   private String partitionHeader = null;
   private Integer staticPartitionId = null;
+  private boolean allowTopicHeader;","[{'comment': 'I think allowTopicOverride would be more self describing name.', 'commenter': 'simonati'}, {'comment': 'Done', 'commenter': 'tmgstevens'}]"
105,flume-ng-sinks/flume-ng-kafka-sink/src/main/java/org/apache/flume/sink/kafka/KafkaSinkConstants.java,"@@ -33,7 +33,10 @@ Licensed to the Apache Software Foundation (ASF) under one or more
       KAFKA_PREFIX + CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG;
 
   public static final String KEY_HEADER = ""key"";
-  public static final String TOPIC_HEADER = ""topic"";
+  public static final String TOPIC_DEFAULT_HEADER_NAME = ""topic"";
+  public static final String TOPIC_HEADER_NAME = ""topicHeader"";
+  public static final String ALLOW_TOPIC_HEADER = ""allowTopicHeader"";","[{'comment': 'please consider my naming comment from above (ALLOW_TOPIC_OVERRIDE)', 'commenter': 'simonati'}, {'comment': 'Done', 'commenter': 'tmgstevens'}]"
105,flume-ng-sources/flume-kafka-source/src/main/java/org/apache/flume/source/kafka/KafkaSourceConstants.java,"@@ -49,9 +49,13 @@
   public static final String OLD_GROUP_ID = ""groupId"";
 
   // flume event headers
-  public static final String TOPIC_HEADER = ""topic"";
+  public static final String TOPIC_HEADER_DEFAULT = ""topic"";","[{'comment': 'Please use naming convention for default values by prefixing DEFAULT in the name instead of postfixing: DEFAULT_TOPIC_HEADER', 'commenter': 'simonati'}, {'comment': 'Done', 'commenter': 'tmgstevens'}]"
105,flume-ng-sources/flume-kafka-source/src/main/java/org/apache/flume/source/kafka/KafkaSourceConstants.java,"@@ -49,9 +49,13 @@
   public static final String OLD_GROUP_ID = ""groupId"";
 
   // flume event headers
-  public static final String TOPIC_HEADER = ""topic"";
+  public static final String TOPIC_HEADER_DEFAULT = ""topic"";
   public static final String KEY_HEADER = ""key"";
   public static final String TIMESTAMP_HEADER = ""timestamp"";
   public static final String PARTITION_HEADER = ""partition"";
 
+  public static final String SET_TOPIC_HEADER = ""setTopicHeader"";
+  public static final boolean SET_TOPIC_HEADER_DEFAULT = true;","[{'comment': 'DEFAULT_SET_TOPIC_HEADER like above', 'commenter': 'simonati'}, {'comment': 'Done', 'commenter': 'tmgstevens'}]"
105,flume-ng-sources/flume-kafka-source/src/main/java/org/apache/flume/source/kafka/KafkaSourceConstants.java,"@@ -49,9 +49,13 @@
   public static final String OLD_GROUP_ID = ""groupId"";
 
   // flume event headers
-  public static final String TOPIC_HEADER = ""topic"";
+  public static final String TOPIC_HEADER_DEFAULT = ""topic"";
   public static final String KEY_HEADER = ""key"";
   public static final String TIMESTAMP_HEADER = ""timestamp"";
   public static final String PARTITION_HEADER = ""partition"";
 
+  public static final String SET_TOPIC_HEADER = ""setTopicHeader"";
+  public static final boolean SET_TOPIC_HEADER_DEFAULT = true;
+  public static final String TOPIC_HEADER_CONF = ""topicHeader"";","[{'comment': 'You may simply use TOPIC_HEADER instead.', 'commenter': 'simonati'}, {'comment': ""I think that's a different setting"", 'commenter': 'tmgstevens'}, {'comment': ""Isn't it similar concept to the one in Sink? If not then would you mind updating the name of the DEFAULT constant to be consistent? \r\n(I see that KEY_HEADER looks confusing then but I guess that can be fixed in a followup commit)"", 'commenter': 'simonati'}, {'comment': 'Fixed.', 'commenter': 'tmgstevens'}]"
105,flume-ng-sources/flume-kafka-source/src/test/java/org/apache/flume/source/kafka/TestKafkaSource.java,"@@ -655,6 +655,76 @@ public void testDefaultSettingsOnReConfigure() throws Exception {
     Assert.assertNull(kafkaSource.getConsumerProps().getProperty(sampleConsumerProp));
   }
 
+  /**
+   * Tests the availability (or not) of the topic header in the output events,
+   * based on the configuration parameters added in FLUME-3046
+   * @throws InterruptedException
+   * @throws EventDeliveryException
+   */
+  @Test
+  public void testTopicHeaderSetNotSet() throws InterruptedException, EventDeliveryException {
+    context.put(TOPICS, topic0);
+    kafkaSource.configure(context);
+    kafkaSource.start();
+
+    Thread.sleep(500L);","[{'comment': 'What is the purpose of having Thread.sleep  between startup of source and produce? Is it really needed?', 'commenter': 'simonati'}, {'comment': 'This gives time for Kafka to process the message - avoids flakiness. There is precedent in other tests for this.', 'commenter': 'tmgstevens'}, {'comment': 'As discussed offline an async call is involved so keeping this.', 'commenter': 'simonati'}]"
105,flume-ng-sources/flume-kafka-source/src/test/java/org/apache/flume/source/kafka/TestKafkaSource.java,"@@ -655,6 +655,76 @@ public void testDefaultSettingsOnReConfigure() throws Exception {
     Assert.assertNull(kafkaSource.getConsumerProps().getProperty(sampleConsumerProp));
   }
 
+  /**
+   * Tests the availability (or not) of the topic header in the output events,
+   * based on the configuration parameters added in FLUME-3046
+   * @throws InterruptedException
+   * @throws EventDeliveryException
+   */
+  @Test
+  public void testTopicHeaderSetNotSet() throws InterruptedException, EventDeliveryException {
+    context.put(TOPICS, topic0);
+    kafkaSource.configure(context);
+    kafkaSource.start();
+
+    Thread.sleep(500L);
+
+    kafkaServer.produce(topic0, """", ""hello, world"");
+
+    Thread.sleep(500L);","[{'comment': 'What is the purpose of having Thread.sleep  between produce and process? Is it really needed?', 'commenter': 'simonati'}, {'comment': 'This gives time for Kafka to process the message - avoids flakiness. There is precedent in other tests for this.', 'commenter': 'tmgstevens'}, {'comment': 'As discussed offline an async call is involved so keeping this.', 'commenter': 'simonati'}]"
105,flume-ng-sources/flume-kafka-source/src/test/java/org/apache/flume/source/kafka/TestKafkaSource.java,"@@ -655,6 +655,76 @@ public void testDefaultSettingsOnReConfigure() throws Exception {
     Assert.assertNull(kafkaSource.getConsumerProps().getProperty(sampleConsumerProp));
   }
 
+  /**
+   * Tests the availability (or not) of the topic header in the output events,
+   * based on the configuration parameters added in FLUME-3046
+   * @throws InterruptedException
+   * @throws EventDeliveryException
+   */
+  @Test
+  public void testTopicHeaderSetNotSet() throws InterruptedException, EventDeliveryException {","[{'comment': 'Would you mind splitting it up to 3 tests which are indeed test slightly different things?\r\n\r\n1. header override was not configured --> expect that there is a header called `topic` and its value is `test0`\r\n2. header override was not configured only the header name --> expect that newly created header is `customTopicHeader`<->`test0`\r\n3. header override is disabled --> expect `topic`<->`test0` and `customTopicHeader`<->`null`', 'commenter': 'simonati'}]"
105,flume-ng-sinks/flume-ng-kafka-sink/src/test/java/org/apache/flume/sink/kafka/TestKafkaSink.java,"@@ -293,9 +293,9 @@ public void testTopicFromConfHeader() throws UnsupportedEncodingException {
   public void testTopicNotFromConfHeader() throws UnsupportedEncodingException {
     Sink kafkaSink = new KafkaSink();
     Context context = prepareDefaultContext();
-    context.put(KafkaSinkConstants.ALLOW_TOPIC_HEADER, ""false"");
-    context.put(KafkaSinkConstants.TOPIC_HEADER_NAME, ""foo"");
-    context.put(KafkaSinkConstants.TOPIC_HEADER_NAME, ""bar"");
+    context.put(KafkaSinkConstants.ALLOW_TOPIC_OVERRIDE_HEADER, ""false"");
+    context.put(KafkaSinkConstants.TOPIC_OVERRIDE_HEADER, ""foo"");
+    context.put(KafkaSinkConstants.TOPIC_OVERRIDE_HEADER, ""bar"");","[{'comment': 'nit: this line of code is not needed', 'commenter': 'simonati'}, {'comment': 'Fixed', 'commenter': 'tmgstevens'}]"
105,flume-ng-sinks/flume-ng-kafka-sink/src/main/java/org/apache/flume/sink/kafka/KafkaSink.java,"@@ -173,10 +173,18 @@ public Status process() throws EventDeliveryException {
         byte[] eventBody = event.getBody();
         Map<String, String> headers = event.getHeaders();
 
-        eventTopic = headers.get(topicHeader);
-        if (eventTopic == null || !allowTopicHeader) {
+        if (allowTopicOverride) {
+          eventTopic = headers.get(topicHeader);
+          if (eventTopic == null) {
+            logger.debug(""{} was set to true but header {} was null. Producing to {}"" + ","[{'comment': 'Thanks for the updated log message. I hope this will help troubleshooters.', 'commenter': 'simonati'}]"
106,flume-ng-auth/src/test/java/org/apache/flume/auth/TestFlumeAuthenticator.java,"@@ -110,6 +111,46 @@ public void testFlumeLogin() throws IOException {
     }
   }
 
+  /**
+   * Test whether the exception raised in the <code>PrivilegedExceptionAction</code> gets
+   * propagated as-is from {@link KerberosAuthenticator#execute(PrivilegedExceptionAction)}.
+   */
+  @Test(expected = IOException.class)
+  public void testKerberosAuthenticatorExceptionInExecute() throws Exception {
+    String principal = flumePrincipal;
+    String keytab = flumeKeytab.getAbsolutePath();
+
+    // Clear the previous statically stored logged in credentials
+    FlumeAuthenticationUtil.clearCredentials();","[{'comment': 'I think this cleanup should be part of tearDown. (Here and in other tests as well.)', 'commenter': 'simonati'}, {'comment': 'Thanks @simonati for the comment, I pushed a new commit to refactor this.', 'commenter': 'adenes'}]"
108,flume-ng-node/src/main/java/org/apache/flume/node/Application.java,"@@ -83,16 +84,25 @@ public synchronized void start() {
   }
 
   @Subscribe
-  public synchronized void handleConfigurationEvent(MaterializedConfiguration conf) {
-    stopAllComponents();
-    startAllComponents(conf);
+  public void handleConfigurationEvent(MaterializedConfiguration conf) {
+    if (stopping) {
+      logger.info(""Will not handle Configuration Event while stopping"");
+      return;
+    }
+    synchronized (this) {
+      stopAllComponents();
+      startAllComponents(conf);
+      
+    }
   }
 
   public synchronized void stop() {
+    stopping = true;
     supervisor.stop();
     if (monitorServer != null) {
       monitorServer.stop();
     }
+    stopping = false;","[{'comment': ""In general it's a good practice to wrap this to a `finally` block. In this particular case it mightn't be necessary as this is only called from the shutdown hook, so in fact it doesn't matter whether this is set to `false`. But in that case do we really need this line, wdyt?"", 'commenter': 'adenes'}, {'comment': ""Thanks for your remark. I'm afraid I will have to rework my change a bit, because this single boolean flag will not entirely prevent the issue. "", 'commenter': 'andrasbeni'}]"
108,flume-ng-node/src/main/java/org/apache/flume/node/Application.java,"@@ -65,6 +68,7 @@
   private final LifecycleSupervisor supervisor;
   private MaterializedConfiguration materializedConfiguration;
   private MonitorService monitorServer;
+  private volatile Lock lock = new ReentrantLock();","[{'comment': ""I'd choose a more descriptive name for this, e.g. `lifecycleLock`. I suppose the `volatile` just remained from the previous change, a `final` would be better here :)"", 'commenter': 'adenes'}]"
111,flume-ng-channels/flume-kafka-channel/src/test/java/org/apache/flume/channel/kafka/ChannelCallbackTest.java,"@@ -0,0 +1,66 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.flume.channel.kafka;
+
+import org.apache.kafka.clients.producer.RecordMetadata;
+import org.apache.kafka.common.TopicPartition;
+import org.apache.log4j.Level;
+import org.apache.log4j.Logger;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+public class ChannelCallbackTest {","[{'comment': 'Could you please rename this to `TestChannelCallback` to conform to the current naming conventions?', 'commenter': 'adenes'}]"
111,flume-ng-channels/flume-kafka-channel/src/test/java/org/apache/flume/channel/kafka/ChannelCallbackTest.java,"@@ -0,0 +1,66 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.flume.channel.kafka;
+
+import org.apache.kafka.clients.producer.RecordMetadata;
+import org.apache.kafka.common.TopicPartition;
+import org.apache.log4j.Level;
+import org.apache.log4j.Logger;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+public class ChannelCallbackTest {
+
+  //note that this is log4j backing the slf4j logger
+  //slf4j does not give an API to change log level at runtime
+  private static final Logger CHANNEL_BACKING_LOGGER = Logger.getLogger(ChannelCallback.class);
+
+  Level originalLevel;
+
+  @Before
+  public void setup() {
+    originalLevel = CHANNEL_BACKING_LOGGER.getLevel();
+  }
+
+  @After
+  public void cleanup() {
+    CHANNEL_BACKING_LOGGER.setLevel(originalLevel);
+  }
+
+  private final ChannelCallback channelCallback = new ChannelCallback(0, System.currentTimeMillis());
+  private final RecordMetadata metadata
+          = new RecordMetadata(new TopicPartition(""TEST-TOPIC"", 0), 0, 0);
+  private final Exception exception = new Exception(""TEST"");
+
+  @Test
+  public void onCompletion() throws Exception {","[{'comment': ""Could you please add some comment here, maybe with a reference to the Jira issue? Knowing the background helps to understand the test and that the expected behaviour is not to throw an exception but without that knowledge it's quite baffling.\r\nIt would be even better to wrap the `onCompletion` calls to a `try-catch` block and in case of an (unwanted) NPEx fail the test with an `Assert.fail()`.\r\n\r\nPlease also rename the test method name to `testOnCompletion` as most of the test methods in Flume have the `test` prefix. (Or to some more self-explaining like `testOnCompletionLoggingNoException` - could be better but this is the best I could come up with)"", 'commenter': 'adenes'}]"
111,flume-ng-channels/flume-kafka-channel/src/test/java/org/apache/flume/channel/kafka/ChannelCallbackTest.java,"@@ -0,0 +1,66 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.flume.channel.kafka;
+
+import org.apache.kafka.clients.producer.RecordMetadata;
+import org.apache.kafka.common.TopicPartition;
+import org.apache.log4j.Level;
+import org.apache.log4j.Logger;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+public class ChannelCallbackTest {
+
+  //note that this is log4j backing the slf4j logger
+  //slf4j does not give an API to change log level at runtime
+  private static final Logger CHANNEL_BACKING_LOGGER = Logger.getLogger(ChannelCallback.class);
+
+  Level originalLevel;","[{'comment': 'minor: this can be private', 'commenter': 'adenes'}]"
111,flume-ng-channels/flume-kafka-channel/src/test/java/org/apache/flume/channel/kafka/ChannelCallbackTest.java,"@@ -0,0 +1,66 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.flume.channel.kafka;
+
+import org.apache.kafka.clients.producer.RecordMetadata;
+import org.apache.kafka.common.TopicPartition;
+import org.apache.log4j.Level;
+import org.apache.log4j.Logger;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+public class ChannelCallbackTest {
+
+  //note that this is log4j backing the slf4j logger
+  //slf4j does not give an API to change log level at runtime
+  private static final Logger CHANNEL_BACKING_LOGGER = Logger.getLogger(ChannelCallback.class);
+
+  Level originalLevel;
+
+  @Before
+  public void setup() {
+    originalLevel = CHANNEL_BACKING_LOGGER.getLevel();
+  }
+
+  @After
+  public void cleanup() {
+    CHANNEL_BACKING_LOGGER.setLevel(originalLevel);
+  }
+
+  private final ChannelCallback channelCallback = new ChannelCallback(0, System.currentTimeMillis());","[{'comment': 'minor: could you please move these 3 variables up to the top of the class?', 'commenter': 'adenes'}]"
125,flume-ng-sinks/flume-ng-kafka-sink/src/main/java/org/apache/flume/sink/kafka/KafkaSink.java,"@@ -453,7 +453,9 @@ public void onCompletion(RecordMetadata metadata, Exception exception) {
 
     if (logger.isDebugEnabled()) {
       long eventElapsedTime = System.currentTimeMillis() - startTime;
-      logger.debug(""Acked message partition:{} ofset:{}"",  metadata.partition(), metadata.offset());
+      if (metadata != null) {
+        logger.debug(""Acked message partition:{} ofset:{}"", metadata.partition(), metadata.offset());","[{'comment': 'This line is longer than 100 characters which brakes the checkstyle verification, could you please fix it? Thank you.', 'commenter': 'adenes'}, {'comment': 'My pleasure', 'commenter': 'loleek'}, {'comment': 'Done.Thanks for you review.', 'commenter': 'loleek'}]"
129,flume-ng-sinks/flume-hdfs-sink/src/test/java/org/apache/flume/sink/hdfs/TestHDFSEventSink.java,"@@ -1545,4 +1549,61 @@ private void testRetryRename(boolean closeSucceed)
     Assert.assertEquals(6, totalRenameAttempts);
 
   }
+
+  /**
+   * BucketWriter.append() can throw a BucketClosedException when called from
+   * HDFSEventSink.process() due to a race condition between HDFSEventSink.process() and the
+   * BucketWriter's close threads.
+   * This test case tests whether if this happens the newly created BucketWriter will be flushed.
+   * For more details see FLUME-3085
+   */
+  @Test
+  public void testFlushedIfAppendFailedWithBucketClosedException() throws Exception {
+    sink = new HDFSEventSink() {
+      @Override
+      BucketWriter initializeBucketWriter(String realPath, String realName, String lookupPath,
+                                          HDFSWriter hdfsWriter, WriterCallback closeCallback) {
+        BucketWriter bw = Mockito.spy(super.initializeBucketWriter(realPath, realName, lookupPath,
+            hdfsWriter, closeCallback));
+        try {
+          // create mock BucketWriters where the first append() succeeds but the
+          // the second call throws a BucketClosedException
+          Mockito.doCallRealMethod()
+              .doThrow(BucketClosedException.class)
+              .when(bw).append(Mockito.any(Event.class));
+        } catch (IOException | InterruptedException e) {
+          Assert.fail(""This shouldn't happen, as append() is called during mocking."");
+        }
+        return bw;
+      }
+    };
+
+    Context context = new Context(ImmutableMap.of(""hdfs.path"", testPath));
+    Configurables.configure(sink, context);
+
+    MemoryChannel channel = new MemoryChannel();
+    Configurables.configure(channel, new Context());
+
+    sink.setChannel(channel);
+    sink.start();
+
+    Transaction txn = channel.getTransaction();
+    txn.begin();
+    channel.put(EventBuilder.withBody(""test"".getBytes()));
+    channel.put(EventBuilder.withBody(""test"".getBytes()));
+    txn.commit();
+    txn.close();
+
+    sink.process();
+
+    // After processing the events the only BucketWriter in the sfWriters map is
+    // the one which was created after the first one threw the BucketClosedException.
+    // It is expected that its flush() method was called exactly once.
+    Map<String, BucketWriter> bucketWriterMap = sink.getSfWriters();","[{'comment': 'Instead of accessing sfwriters you can collect the created BucketWriters from your test extension of HDFSEventSink. This way you could check that whatever bucketwriter was created is actually flushed/closed. sfwriters list might not have reference to every created bucketwriter (lookup path is common so the newly created bw kicks out the closed one on line 406). ', 'commenter': 'simonati'}]"
129,flume-ng-sinks/flume-hdfs-sink/src/main/java/org/apache/flume/sink/hdfs/HDFSEventSink.java,"@@ -415,6 +410,9 @@ public void run(String bucketPath) {
           }
           bucketWriter.append(event);
         }
+
+        // track the buckets getting written in this transaction
+        writers.add(bucketWriter);","[{'comment': ""Looks good to also capture the bucketwriter created on line 406. Since the original (line 389 or 393) one threw a closed exception that means that we don't have to flush the original one only the newly created one (close implicitly contains flush as well)"", 'commenter': 'simonati'}, {'comment': 'nit: Can we keep the if-statement? I suppose add() on the same element multiple times would just replace the previous element in the set and end up as a no-op, but it seems clearer to me to keep the ""contains"" there, especially now that you\'ve switched to a hash-based container and it would be an O(1) operation, e.g.\r\n\r\n```\r\nif (!writers.contains(bucketWriter)) {\t\t\r\n  writers.add(bucketWriter);\t\t\r\n}\r\n```', 'commenter': 'mpercy'}, {'comment': 'sure, will update the code', 'commenter': 'adenes'}]"
129,flume-ng-sinks/flume-hdfs-sink/src/test/java/org/apache/flume/sink/hdfs/TestHDFSEventSink.java,"@@ -1545,4 +1549,61 @@ private void testRetryRename(boolean closeSucceed)
     Assert.assertEquals(6, totalRenameAttempts);
 
   }
+
+  /**
+   * BucketWriter.append() can throw a BucketClosedException when called from
+   * HDFSEventSink.process() due to a race condition between HDFSEventSink.process() and the
+   * BucketWriter's close threads.
+   * This test case tests whether if this happens the newly created BucketWriter will be flushed.
+   * For more details see FLUME-3085
+   */
+  @Test
+  public void testFlushedIfAppendFailedWithBucketClosedException() throws Exception {
+    sink = new HDFSEventSink() {
+      @Override
+      BucketWriter initializeBucketWriter(String realPath, String realName, String lookupPath,
+                                          HDFSWriter hdfsWriter, WriterCallback closeCallback) {
+        BucketWriter bw = Mockito.spy(super.initializeBucketWriter(realPath, realName, lookupPath,
+            hdfsWriter, closeCallback));
+        try {
+          // create mock BucketWriters where the first append() succeeds but the
+          // the second call throws a BucketClosedException
+          Mockito.doCallRealMethod()
+              .doThrow(BucketClosedException.class)
+              .when(bw).append(Mockito.any(Event.class));
+        } catch (IOException | InterruptedException e) {
+          Assert.fail(""This shouldn't happen, as append() is called during mocking."");
+        }
+        return bw;
+      }
+    };
+
+    Context context = new Context(ImmutableMap.of(""hdfs.path"", testPath));
+    Configurables.configure(sink, context);
+
+    MemoryChannel channel = new MemoryChannel();
+    Configurables.configure(channel, new Context());
+
+    sink.setChannel(channel);
+    sink.start();
+
+    Transaction txn = channel.getTransaction();","[{'comment': 'nit: I would have stubbed the channel.take() instead of juggling around with this transaction.', 'commenter': 'simonati'}]"
129,flume-ng-sinks/flume-hdfs-sink/src/test/java/org/apache/flume/sink/hdfs/TestHDFSEventSink.java,"@@ -1545,4 +1553,62 @@ private void testRetryRename(boolean closeSucceed)
     Assert.assertEquals(6, totalRenameAttempts);
 
   }
+
+  /**
+   * BucketWriter.append() can throw a BucketClosedException when called from
+   * HDFSEventSink.process() due to a race condition between HDFSEventSink.process() and the
+   * BucketWriter's close threads.
+   * This test case tests whether if this happens the newly created BucketWriter will be flushed.
+   * For more details see FLUME-3085
+   */
+  @Test
+  public void testFlushedIfAppendFailedWithBucketClosedException() throws Exception {
+    final Set<BucketWriter> bucketWriters = new HashSet<>();
+    sink = new HDFSEventSink() {
+      @Override
+      BucketWriter initializeBucketWriter(String realPath, String realName, String lookupPath,
+                                          HDFSWriter hdfsWriter, WriterCallback closeCallback) {
+        BucketWriter bw = Mockito.spy(super.initializeBucketWriter(realPath, realName, lookupPath,
+            hdfsWriter, closeCallback));
+        try {
+          // create mock BucketWriters where the first append() succeeds but the
+          // the second call throws a BucketClosedException
+          Mockito.doCallRealMethod()
+              .doThrow(BucketClosedException.class)
+              .when(bw).append(Mockito.any(Event.class));
+        } catch (IOException | InterruptedException e) {
+          Assert.fail(""This shouldn't happen, as append() is called during mocking."");
+        }
+        bucketWriters.add(bw);
+        return bw;
+      }
+    };
+
+    Context context = new Context(ImmutableMap.of(""hdfs.path"", testPath));
+    Configurables.configure(sink, context);
+
+    Channel channel = Mockito.spy(new MemoryChannel());","[{'comment': ""Ahh I see. MockChannel is in flume-ng-core test folder thus cannot be used as is. And a simple Mockito.mock(MemoryChannel.class) is not enough as beside take(), also getTransaction has to be implemented. It might need a later refactoring to move this Mock implementations to a shared module but for now I'm fine with this."", 'commenter': 'simonati'}]"
129,flume-ng-sinks/flume-hdfs-sink/src/main/java/org/apache/flume/sink/hdfs/HDFSEventSink.java,"@@ -354,9 +354,9 @@ static CompressionCodec getCodec(String codecName) {
   public Status process() throws EventDeliveryException {
     Channel channel = getChannel();
     Transaction transaction = channel.getTransaction();
-    List<BucketWriter> writers = Lists.newArrayList();
     transaction.begin();
     try {
+      Set<BucketWriter> writers = new LinkedHashSet<>();","[{'comment': ""nit: I don't think we need ordering, how about just HashSet<>()?"", 'commenter': 'mpercy'}, {'comment': ""I'm not sure but I vaguely remember some issue regarding to the ordering (cc. @simonati) so I would keep the `LinkedHashSet`. On the other hand not tests break with `HashSet`."", 'commenter': 'adenes'}, {'comment': 'OK', 'commenter': 'mpercy'}]"
129,flume-ng-sinks/flume-hdfs-sink/src/test/java/org/apache/flume/sink/hdfs/TestHDFSEventSink.java,"@@ -1545,4 +1553,62 @@ private void testRetryRename(boolean closeSucceed)
     Assert.assertEquals(6, totalRenameAttempts);
 
   }
+
+  /**
+   * BucketWriter.append() can throw a BucketClosedException when called from
+   * HDFSEventSink.process() due to a race condition between HDFSEventSink.process() and the
+   * BucketWriter's close threads.
+   * This test case tests whether if this happens the newly created BucketWriter will be flushed.
+   * For more details see FLUME-3085
+   */
+  @Test
+  public void testFlushedIfAppendFailedWithBucketClosedException() throws Exception {
+    final Set<BucketWriter> bucketWriters = new HashSet<>();
+    sink = new HDFSEventSink() {
+      @Override
+      BucketWriter initializeBucketWriter(String realPath, String realName, String lookupPath,
+                                          HDFSWriter hdfsWriter, WriterCallback closeCallback) {
+        BucketWriter bw = Mockito.spy(super.initializeBucketWriter(realPath, realName, lookupPath,
+            hdfsWriter, closeCallback));
+        try {
+          // create mock BucketWriters where the first append() succeeds but the
+          // the second call throws a BucketClosedException","[{'comment': ""I don't see any logic here to make this throw only on the second call. It seems like it would throw on every call. Maybe I'm missing something."", 'commenter': 'mpercy'}, {'comment': ""Chaining the `do*` calls in Mockito's Stubber class ensures that the first call will call the real method and the second call throws the exception. \r\nFor more details see the Stubber's javadoc: https://static.javadoc.io/org.mockito/mockito-core/1.9.0/org/mockito/stubbing/Stubber.html"", 'commenter': 'adenes'}]"
129,flume-ng-sinks/flume-hdfs-sink/src/test/java/org/apache/flume/sink/hdfs/TestHDFSEventSink.java,"@@ -1545,4 +1553,62 @@ private void testRetryRename(boolean closeSucceed)
     Assert.assertEquals(6, totalRenameAttempts);
 
   }
+
+  /**
+   * BucketWriter.append() can throw a BucketClosedException when called from
+   * HDFSEventSink.process() due to a race condition between HDFSEventSink.process() and the
+   * BucketWriter's close threads.
+   * This test case tests whether if this happens the newly created BucketWriter will be flushed.
+   * For more details see FLUME-3085
+   */
+  @Test
+  public void testFlushedIfAppendFailedWithBucketClosedException() throws Exception {
+    final Set<BucketWriter> bucketWriters = new HashSet<>();
+    sink = new HDFSEventSink() {
+      @Override
+      BucketWriter initializeBucketWriter(String realPath, String realName, String lookupPath,
+                                          HDFSWriter hdfsWriter, WriterCallback closeCallback) {
+        BucketWriter bw = Mockito.spy(super.initializeBucketWriter(realPath, realName, lookupPath,
+            hdfsWriter, closeCallback));
+        try {
+          // create mock BucketWriters where the first append() succeeds but the
+          // the second call throws a BucketClosedException
+          Mockito.doCallRealMethod()
+              .doThrow(BucketClosedException.class)
+              .when(bw).append(Mockito.any(Event.class));
+        } catch (IOException | InterruptedException e) {
+          Assert.fail(""This shouldn't happen, as append() is called during mocking."");
+        }
+        bucketWriters.add(bw);
+        return bw;
+      }
+    };
+
+    Context context = new Context(ImmutableMap.of(""hdfs.path"", testPath));
+    Configurables.configure(sink, context);
+
+    Channel channel = Mockito.spy(new MemoryChannel());
+    Configurables.configure(channel, new Context());
+
+    final Iterator<Event> events = Iterators.forArray(
+        EventBuilder.withBody(""test1"".getBytes()), EventBuilder.withBody(""test2"".getBytes()));
+    Mockito.doAnswer(new Answer() {
+      @Override
+      public Object answer(InvocationOnMock invocation) throws Throwable {
+        return events.hasNext() ? events.next() : null;
+      }
+    }).when(channel).take();
+
+    sink.setChannel(channel);
+    sink.start();
+
+    sink.process();
+
+    // It is expected that flush() method was called exactly once for every BucketWriter
+    for (BucketWriter bw : bucketWriters) {","[{'comment': 'I think it would be good to add some more assertions: how many events were taken from the channel, how many events were successfully written to HDFS, and how many bucketWriters there are.', 'commenter': 'mpercy'}, {'comment': 'ok, will do.', 'commenter': 'adenes'}]"
137,flume-ng-sinks/flume-ng-kafka-sink/src/test/java/org/apache/flume/sink/kafka/TestKafkaSink.java,"@@ -239,6 +240,42 @@ public void testTopicAndKeyFromHeader() throws UnsupportedEncodingException {
                  new String((byte[]) fetchedMsg.key(), ""UTF-8""));
   }
 
+  @Test
+  public void testReplaceSubStringOfTopicWithHeaders() throws UnsupportedEncodingException {
+    Sink kafkaSink = new KafkaSink();
+    Context context = prepareDefaultContext();
+    context.put(TOPIC_CONFIG, TestConstants.HEADER_TOPIC);
+    Configurables.configure(kafkaSink, context);
+    Channel memoryChannel = new MemoryChannel();
+    Configurables.configure(memoryChannel, context);
+    kafkaSink.setChannel(memoryChannel);
+    kafkaSink.start();
+
+    String msg = ""test-replace-substring-of-topic-with-headers"";
+    Map<String, String> headers = new HashMap<String, String>();","[{'comment': 'Nit: explicit type argument can be replaced by `<>`. Or `Collections.singletonMap()` might be even better.', 'commenter': 'adenes'}]"
141,flume-ng-core/src/main/java/org/apache/flume/source/AvroSource.java,"@@ -300,28 +306,30 @@ public ChannelPipeline getPipeline() throws Exception {
   public void stop() {
     logger.info(""Avro source {} stopping: {}"", getName(), this);
 
-    server.close();
+    if (server != null) {
+      server.close();
+      try {
+        server.join();
+        server = null;
+      } catch (InterruptedException e) {
+        logger.info(""Avro source "" + getName() + "": Interrupted while waiting "" +","[{'comment': 'I think the`Thread.currentThread().interrupt();` should be called here to set the interrupt flag of the thread.', 'commenter': 'adenes'}, {'comment': 'Sure thing. ', 'commenter': 'simonati'}]"
141,flume-ng-core/src/test/java/org/apache/flume/source/TestAvroSource.java,"@@ -97,20 +96,19 @@ public void testLifecycle() throws InterruptedException {
     boolean bound = false;
 
     for (int i = 0; i < 100 && !bound; i++) {
-      try {
-        Context context = new Context();
-
-        context.put(""port"", String.valueOf(selectedPort = 41414 + i));
-        context.put(""bind"", ""0.0.0.0"");
 
-        Configurables.configure(source, context);
+      Context context = new Context();
 
+      context.put(""port"", String.valueOf(selectedPort = 41414 + i));
+      context.put(""bind"", ""0.0.0.0"");
+      // Invalid configuration may throw a FlumeException which has to be expected in the callers","[{'comment': 'Could you please explain what this comment means? Or is it related to the changes you made, i.e. removing the configuration part from the `try-catch` block?', 'commenter': 'adenes'}, {'comment': 'Yes it is related to that change. Can you please recommend a better wording?', 'commenter': 'simonati'}, {'comment': ""I'm not sure it is needed here at all, without knowing the previous version of the code it cannot be understood. I'd simply remove it."", 'commenter': 'adenes'}, {'comment': 'With one exception I removed this comment. In that occurrence the helper function is called by different tests and some of the tests expect a FlumeException to be thrown caused by invalid configuration. It could have saved me some time during implementation that `configure()` can also raise a FlumeException so I would like to make life of future devs a bit easier. Also I rephrased a little the comment.\r\n', 'commenter': 'simonati'}]"
141,flume-ng-core/src/test/java/org/apache/flume/source/TestAvroSource.java,"@@ -129,6 +127,64 @@ public void testLifecycle() throws InterruptedException {
   }
 
   @Test
+  public void testSourceStoppedOnFlumeExceptionIfPortUsed()
+      throws InterruptedException, IOException {
+    final String loopbackIPv4 = ""127.0.0.1"";
+    final int port = 10500;
+
+    // create a dummy socket bound to a known port.
+    try (ServerSocketChannel dummyServerSocket = ServerSocketChannel.open()) {
+      dummyServerSocket.socket().setReuseAddress(true);
+      dummyServerSocket.socket().bind(new InetSocketAddress(loopbackIPv4, port));
+
+      Context context = new Context();
+      context.put(""port"", String.valueOf(port));
+      context.put(""bind"", loopbackIPv4);
+      // Invalid configuration may throw a FlumeException which has to be expected in the callers
+      Configurables.configure(source, context);
+      try {
+        source.start();
+        Assert.fail(""Expected an exception during startup caused by binding on a used port"");
+      } catch (FlumeException e) {
+        logger.info(""Received an expected exception."", e);
+        Assert.assertTrue(""Expected a bind related root cause"",","[{'comment': ""Wouldn't it be useful to do an `instanceof` check for `e.getCause()`?"", 'commenter': 'adenes'}, {'comment': 'I think that should be considered as internal implementation detail (the situation that we use Netty so it throws a netty related exception should not be considered by any user of the AvroSource). I think the ""API"" here is a FlumeException with the relevant message.', 'commenter': 'simonati'}, {'comment': ""I kind of agree, but with the current implementation (throwing `new FlumeException(nce)`) the message itself is an implementation detail as well, because it comes directly from the netty code.\r\nI'd suggest to explicitly define the message when throwing the `FlumeException`. Wdyt?"", 'commenter': 'adenes'}, {'comment': ""> I'd suggest to explicitly define the message\r\n\r\nGood point. "", 'commenter': 'simonati'}, {'comment': 'Does this sound good?\r\n\r\nthrow new FlumeException(""Failed to set up server socket"", nce);', 'commenter': 'simonati'}, {'comment': 'yes, thx', 'commenter': 'adenes'}]"
141,flume-ng-core/src/test/java/org/apache/flume/source/TestAvroSource.java,"@@ -129,6 +127,64 @@ public void testLifecycle() throws InterruptedException {
   }
 
   @Test
+  public void testSourceStoppedOnFlumeExceptionIfPortUsed()
+      throws InterruptedException, IOException {
+    final String loopbackIPv4 = ""127.0.0.1"";
+    final int port = 10500;
+
+    // create a dummy socket bound to a known port.
+    try (ServerSocketChannel dummyServerSocket = ServerSocketChannel.open()) {
+      dummyServerSocket.socket().setReuseAddress(true);
+      dummyServerSocket.socket().bind(new InetSocketAddress(loopbackIPv4, port));
+
+      Context context = new Context();
+      context.put(""port"", String.valueOf(port));
+      context.put(""bind"", loopbackIPv4);
+      // Invalid configuration may throw a FlumeException which has to be expected in the callers
+      Configurables.configure(source, context);
+      try {
+        source.start();
+        Assert.fail(""Expected an exception during startup caused by binding on a used port"");
+      } catch (FlumeException e) {
+        logger.info(""Received an expected exception."", e);
+        Assert.assertTrue(""Expected a bind related root cause"",
+            e.getMessage().contains(""Failed to bind to""));
+      }
+    }
+    // As port is already in use, an exception is thrown and the source is stopped
+    // cleaning up the opened sockets during source.start().
+    Assert.assertEquals(""Server is stopped"", LifecycleState.STOP,
+            source.getLifecycleState());
+  }
+
+  @Test
+  public void testInvalidAddress()
+      throws InterruptedException, IOException {
+    final String invalidHost = ""invalid.host"";
+    final int port = 10501;
+
+    Context context = new Context();
+    context.put(""port"", String.valueOf(port));
+    context.put(""bind"", invalidHost);
+    // Invalid configuration may throw a FlumeException which has to be expected in the callers
+    Configurables.configure(source, context);
+
+    try {
+      source.start();
+      Assert.fail(""Expected an exception during startup caused by binding on a invalid host"");
+    } catch (FlumeException e) {
+      logger.info(""Received an expected exception."", e);
+      Assert.assertTrue(""Expected a bind related root cause"",","[{'comment': 'Same as above (line 150)', 'commenter': 'adenes'}, {'comment': 'I think the same answer applies. ', 'commenter': 'simonati'}]"
143,flume-ng-core/src/main/java/org/apache/flume/source/AvroSource.java,"@@ -292,12 +287,7 @@ private ChannelPipelineFactory initChannelPipelineFactory() {
         keystorePassword, keystoreType, enableIpFilter,
         patternRuleConfigDefinition);
     } else {
-      pipelineFactory = new ChannelPipelineFactory() {
-        @Override
-        public ChannelPipeline getPipeline() throws Exception {
-          return Channels.pipeline();
-        }
-      };
+      pipelineFactory = () -> Channels.pipeline();","[{'comment': 'This can be replaced by method reference, i.e. `pipelineFactory = Channels::pipeline`', 'commenter': 'adenes'}]"
147,flume-ng-core/src/main/java/org/apache/flume/source/MultiportSyslogTCPSource.java,"@@ -137,6 +139,9 @@ public void configure(Context context) {
     portHeader = context.getString(
             SyslogSourceConfigurationConstants.CONFIG_PORT_HEADER);
 
+    ipHeader = context.getString(
+            SyslogSourceConfigurationConstants.CONFIG_IP_HEADER);","[{'comment': ""The existing port header contains a _local_ port, while the new port header is the _remote_ IP, so they belong to two different network interfaces.\r\nTo make it clear, I would rename the new header to 'sourceIpHeader' or 'senderIpHeader'.\r\n\r\n(The local IP could also be stored in a header for the case when the Flume host contains multiple network cards, and this IP would belong to the existing port header and could be called ipHeader simply.)"", 'commenter': 'turcsanyip'}]"
147,flume-ng-core/src/main/java/org/apache/flume/source/MultiportSyslogTCPSource.java,"@@ -287,6 +294,16 @@ public void messageReceived(IoSession session, Object message) {
         decoder = portCharsets.get(port).get();
       }
 
+      //get ip from session
+      String ip = """";
+      SocketAddress socket = session.getRemoteAddress();","[{'comment': 'It is an _address_ not a _socket_, so _(***)address_ name would be better for this variable. ', 'commenter': 'turcsanyip'}, {'comment': 'This could be checked and cast to an InetSocketAddress and then getAddress().getHostAddress() to retrieve the IP.\r\neven better, based on a flag we could retrieve the hostname as IP addresses tend to be temporary nowadays.', 'commenter': 'szaboferee'}, {'comment': '@szaboferee I agree\r\nThis is how the ""port"" is retrieved\r\n290:      int port = ((InetSocketAddress) session.getLocalAddress()).getPort();\r\nThis seems much safer to me:\r\n      String hostName = ((InetSocketAddress) session.getRemoteAddress()).getHostName();\r\n      String ipAddress = ((InetSocketAddress) session.getRemoteAddress()).getAddress().getHostAddress();', 'commenter': 'majorendre'}]"
147,flume-ng-core/src/main/java/org/apache/flume/source/SyslogSourceConfigurationConstants.java,"@@ -74,6 +74,9 @@
   public static final String CONFIG_KEEP_FIELDS_TIMESTAMP = ""timestamp"";
   public static final String CONFIG_KEEP_FIELDS_HOSTNAME = ""hostname"";
 
+  public static final String CONFIG_IP_HEADER = ""ipHeader"";
+  public static final String DEFAULT_IP_HEADER = ""ip"";","[{'comment': 'This is only used in tests so it should be defined there and not in the production code', 'commenter': 'szaboferee'}]"
147,flume-ng-doc/sphinx/FlumeUserGuide.rst,"@@ -1609,6 +1610,7 @@ keepFields            none              Setting this to 'all' will preserve the
                                         fields can be included: priority, version,
                                         timestamp, hostname. The values 'true' and 'false'
                                         have been deprecated in favor of 'all' and 'none'.
+ipHeader              --                If specified, the IP will be stored in the header of each event using the header name specified here. This allows for interceptors and channel selectors to customize routing logic based on the incoming host.","[{'comment': '""the IP will be stored"": please specify which IP\r\n""incoming host"": it might be better to use _source_ or _sender_ instead of incoming', 'commenter': 'turcsanyip'}]"
147,flume-ng-core/src/test/java/org/apache/flume/source/TestMultiportSyslogTCPSource.java,"@@ -383,4 +386,84 @@ public void testPortCharsetHandling() throws UnknownHostException, Exception {
     Assert.assertNull(evt.getHeaders().get(SyslogUtils.EVENT_STATUS));
   }
 
+  /**
+   * Test event with a given ip header.
+   */
+  @Test
+  public void testIpHeader() throws UnknownHostException, IOException {
+    final String TEST_IP_HEADER = ""testIpHeader"";
+
+    MultiportSyslogTCPSource source = new MultiportSyslogTCPSource();
+    Channel channel = new MemoryChannel();
+
+    Context channelContext = new Context();
+    channelContext.put(""capacity"", String.valueOf(100));
+    channelContext.put(""transactionCapacity"", String.valueOf(100));
+    Configurables.configure(channel, channelContext);
+
+    List<Channel> channels = Lists.newArrayList();
+    channels.add(channel);
+
+    ChannelSelector rcs = new ReplicatingChannelSelector();
+    rcs.setChannels(channels);
+
+    source.setChannelProcessor(new ChannelProcessor(rcs));
+    Context context = new Context();
+    StringBuilder ports = new StringBuilder();
+    context.put(SyslogSourceConfigurationConstants.CONFIG_PORTS,
+            String.valueOf(getFreePort()));
+    context.put(SyslogSourceConfigurationConstants.CONFIG_IP_HEADER,
+            TEST_IP_HEADER);
+
+    source.configure(context);
+    source.start();
+
+    //create a socket to send a test event
+    Socket syslogSocket;
+    syslogSocket = new Socket(
+            InetAddress.getLocalHost(), getFreePort());","[{'comment': 'this breaks as the client tries to connect to a new random port and not to the port the server is listening on. ', 'commenter': 'szaboferee'}, {'comment': 'This does not work with the current trunk version.\r\nPlease rebase the PR onto trunk and fix the test failure.', 'commenter': 'turcsanyip'}]"
153,flume-ng-sources/flume-jms-source/pom.xml,"@@ -70,10 +70,15 @@ limitations under the License.
       <scope>test</scope>
     </dependency>
 
+  <dependency>","[{'comment': 'nit: could you please fix the indentation here?', 'commenter': 'adenes'}]"
155,pom.xml,"@@ -100,7 +100,8 @@ limitations under the License.
     <mvn-surefire-plugin.version>2.14.1</mvn-surefire-plugin.version>
     <mvn-surefire-plugin-old.version>2.12.3</mvn-surefire-plugin-old.version>
     <mvn-surefire-plugin-old-morphline.version>2.12.4</mvn-surefire-plugin-old-morphline.version>
-    <netty.version>3.9.4.Final</netty.version>
+    <netty.version>3.10.6.Final</netty.version>
+    <netty-all.version>4.1.14.Final</netty-all.version>","[{'comment': 'Today the latest version is 4.1.17.final\r\nDo we have a reason to use the older one?', 'commenter': 'mcsanady'}, {'comment': 'Thanks for pointing it out. I will update it', 'commenter': 'szaboferee'}]"
158,flume-ng-core/src/main/java/org/apache/flume/tools/FlumeBeanConfigurator.java,"@@ -0,0 +1,142 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.flume.tools;
+
+import java.lang.reflect.Field;
+import java.lang.reflect.Method;
+import java.util.Map;
+
+import org.apache.flume.Context;
+import org.apache.flume.conf.ConfigurationException;
+import org.slf4j.Logger;
+
+/**
+ * Utility class to enable runtime configuration of Java objects using provided
+ * Flume context objects (or equivalent). The methods use reflection to identify
+ * Fields on the configurable object and then looks for matching properties in
+ * the provided properties bundle.
+ *
+ */
+public class FlumeBeanConfigurator {","[{'comment': ""I'm a little bit concerned about this FlumeBeanConfigurator class, because if it is not used with caution it could expose setters we might would not like to expose. I see why it is easier to use a util like this with the jetty configuration. We should consider moving this somewhere so only the HttpSource  could see and use it.\r\n\r\nAlso some question that could be documented is :\r\nWhen the user sets both `keystorePassword` and `SslContextFactory.KeyStorePassword` which one will be set?\r\n\r\n"", 'commenter': 'szaboferee'}, {'comment': ""Clarified the ``keystore`` bit in the docs and actually changed the order of precedence.\r\n\r\nRegarding the ``FlumeBeanConfigurator`` I'm afraid I don't agree that we could expose anything that wasn't already exposed. The Configurator only uses public setters, so cannot set any property that wasn't already available to be set - developers obviously need to use it with their eyes open.\r\n\r\nI've added a set of Unit tests to test the functionality of this, including testing that private setters cannot be modified. Hopefully this satisfies your concerns."", 'commenter': 'tmgstevens'}]"
158,flume-ng-core/src/main/java/org/apache/flume/tools/FlumeBeanConfigurator.java,"@@ -0,0 +1,142 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.flume.tools;
+
+import java.lang.reflect.Method;
+import java.util.Map;
+
+import org.apache.flume.Context;
+import org.apache.flume.conf.ConfigurationException;
+import org.slf4j.Logger;
+
+/**
+ * Utility class to enable runtime configuration of Java objects using provided
+ * Flume context objects (or equivalent). The methods use reflection to identify
+ * Fields on the configurable object and then looks for matching properties in
+ * the provided properties bundle.
+ *
+ */
+public class FlumeBeanConfigurator {
+
+  /**
+   * Utility method that will set properties on a Java bean (<code>Object configurable</code>)
+   * based on the provided <code>properties</code> bundle.
+   * Logs using the provided <code>logger</code>. If there is a type issue, or an access problem
+   * then a <code>ConfigurationException</code> will be thrown.
+   *
+   * @param configurable Any properties must be modifiable via setter methods.
+   * @param properties Map&lt;String, String&gt;
+   * @param logger org.slf4j.Logger
+   * @throws ConfigurationException
+   */
+  public static void setConfigurationFields(Object configurable, Map<String, String> properties,","[{'comment': ""Could you please remove the `logger` parameter of this method?\r\nI think it's better to have a private logger as all the logged exceptions are propagated to the caller so they have the chance to log the errors with their own loggers.\r\nPlus if a caller doesn't have a logger then it can't call this method. Or - which is even worse - can try to call it with `null` logger and then bump into unexpected NPExceptions."", 'commenter': 'adenes'}, {'comment': 'Perhaps mark as @InterfaceAudience.Private and @InterfaceStability.Evolving for a release', 'commenter': 'mpercy'}, {'comment': 'Done', 'commenter': 'tmgstevens'}]"
158,flume-ng-core/src/test/java/org/apache/flume/source/http/FlumeHttpServletRequestWrapper.java,"@@ -52,273 +63,282 @@ public FlumeHttpServletRequestWrapper(String data) throws UnsupportedEncodingExc
     this(data, ""UTF-8"");
   }
 
-  @Override","[{'comment': 'These `@Override`s are not needed to be removed as these methods still override the ones from the parent class. Could you please put them back + also add them to the newly added methods? Thanks.', 'commenter': 'adenes'}, {'comment': 'Done', 'commenter': 'tmgstevens'}]"
158,flume-ng-core/src/main/java/org/apache/flume/tools/FlumeBeanConfigurator.java,"@@ -0,0 +1,142 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.flume.tools;
+
+import java.lang.reflect.Method;
+import java.util.Map;
+
+import org.apache.flume.Context;
+import org.apache.flume.conf.ConfigurationException;
+import org.slf4j.Logger;
+
+/**
+ * Utility class to enable runtime configuration of Java objects using provided
+ * Flume context objects (or equivalent). The methods use reflection to identify
+ * Fields on the configurable object and then looks for matching properties in
+ * the provided properties bundle.
+ *
+ */
+public class FlumeBeanConfigurator {
+
+  /**
+   * Utility method that will set properties on a Java bean (<code>Object configurable</code>)
+   * based on the provided <code>properties</code> bundle.
+   * Logs using the provided <code>logger</code>. If there is a type issue, or an access problem
+   * then a <code>ConfigurationException</code> will be thrown.
+   *
+   * @param configurable Any properties must be modifiable via setter methods.
+   * @param properties Map&lt;String, String&gt;
+   * @param logger org.slf4j.Logger
+   * @throws ConfigurationException
+   */
+  public static void setConfigurationFields(Object configurable, Map<String, String> properties,
+          Logger logger) throws ConfigurationException {
+    Class<?> clazz = configurable.getClass();
+
+    for (Method method : clazz.getMethods()) {
+      String methodName = method.getName();
+      if (methodName.startsWith(""set"") && method.getParameterTypes().length == 1) {
+        String fieldName = methodName.substring(3);","[{'comment': 'If this is to be case sensitive, it should probably lowercase the first letter so that the property fooBar maps to setFooBar() which would be consistent with most of the rest of Flume.', 'commenter': 'mpercy'}, {'comment': 'Done', 'commenter': 'tmgstevens'}]"
158,flume-ng-core/src/main/java/org/apache/flume/tools/FlumeBeanConfigurator.java,"@@ -0,0 +1,142 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.flume.tools;
+
+import java.lang.reflect.Method;
+import java.util.Map;
+
+import org.apache.flume.Context;
+import org.apache.flume.conf.ConfigurationException;
+import org.slf4j.Logger;
+
+/**
+ * Utility class to enable runtime configuration of Java objects using provided
+ * Flume context objects (or equivalent). The methods use reflection to identify
+ * Fields on the configurable object and then looks for matching properties in
+ * the provided properties bundle.
+ *
+ */
+public class FlumeBeanConfigurator {
+
+  /**
+   * Utility method that will set properties on a Java bean (<code>Object configurable</code>)
+   * based on the provided <code>properties</code> bundle.
+   * Logs using the provided <code>logger</code>. If there is a type issue, or an access problem
+   * then a <code>ConfigurationException</code> will be thrown.
+   *
+   * @param configurable Any properties must be modifiable via setter methods.
+   * @param properties Map&lt;String, String&gt;
+   * @param logger org.slf4j.Logger
+   * @throws ConfigurationException
+   */
+  public static void setConfigurationFields(Object configurable, Map<String, String> properties,
+          Logger logger) throws ConfigurationException {
+    Class<?> clazz = configurable.getClass();
+
+    for (Method method : clazz.getMethods()) {
+      String methodName = method.getName();
+      if (methodName.startsWith(""set"") && method.getParameterTypes().length == 1) {
+        String fieldName = methodName.substring(3);
+
+        String value = properties.get(fieldName);
+        if (value != null) {
+
+          Class<?> fieldType = method.getParameterTypes()[0];;
+          try {
+            if (fieldType.equals(String.class)) {
+              method.invoke(configurable, value);
+            } else if (fieldType.equals(boolean.class)) {
+              method.invoke(configurable, Boolean.parseBoolean(value));
+            } else if (fieldType.equals(short.class)) {
+              method.invoke(configurable, Short.parseShort(value));
+            } else if (fieldType.equals(long.class)) {
+              method.invoke(configurable, Long.parseLong(value));
+            } else if (fieldType.equals(float.class)) {
+              method.invoke(configurable, Float.parseFloat(value));
+            } else if (fieldType.equals(int.class)) {
+              method.invoke(configurable, Integer.parseInt(value));
+            } else if (fieldType.equals(double.class)) {
+              method.invoke(configurable, Double.parseDouble(value));
+            } else if (fieldType.equals(char.class)) {
+              method.invoke(configurable, value.charAt(0));
+            } else if (fieldType.equals(byte.class)) {
+              method.invoke(configurable, Byte.parseByte(value));
+            } else if (fieldType.equals(String[].class)) {
+              method.invoke(configurable, (Object)value.split(""\\s+""));
+            } else {
+              logger.error(""Unable to set "" + fieldName + "" as it is an unsupported type: ""","[{'comment': 'Should this just throw?', 'commenter': 'mpercy'}]"
158,flume-ng-core/src/main/java/org/apache/flume/tools/FlumeBeanConfigurator.java,"@@ -0,0 +1,142 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.flume.tools;
+
+import java.lang.reflect.Method;
+import java.util.Map;
+
+import org.apache.flume.Context;
+import org.apache.flume.conf.ConfigurationException;
+import org.slf4j.Logger;
+
+/**
+ * Utility class to enable runtime configuration of Java objects using provided
+ * Flume context objects (or equivalent). The methods use reflection to identify
+ * Fields on the configurable object and then looks for matching properties in
+ * the provided properties bundle.
+ *
+ */
+public class FlumeBeanConfigurator {
+
+  /**
+   * Utility method that will set properties on a Java bean (<code>Object configurable</code>)
+   * based on the provided <code>properties</code> bundle.
+   * Logs using the provided <code>logger</code>. If there is a type issue, or an access problem
+   * then a <code>ConfigurationException</code> will be thrown.
+   *
+   * @param configurable Any properties must be modifiable via setter methods.
+   * @param properties Map&lt;String, String&gt;
+   * @param logger org.slf4j.Logger
+   * @throws ConfigurationException
+   */
+  public static void setConfigurationFields(Object configurable, Map<String, String> properties,
+          Logger logger) throws ConfigurationException {
+    Class<?> clazz = configurable.getClass();
+
+    for (Method method : clazz.getMethods()) {","[{'comment': ""Should we do this the opposite way, where we try to call a setter for each property in the map, or that isn't going to be viable?"", 'commenter': 'mpercy'}, {'comment': ""Would be nicer, but we'd need to infer the type because there isn't a Class.getMethod(String name) method."", 'commenter': 'tmgstevens'}]"
158,flume-ng-core/src/test/java/org/apache/flume/source/http/FlumeHttpServletRequestWrapper.java,"@@ -52,273 +63,282 @@ public FlumeHttpServletRequestWrapper(String data) throws UnsupportedEncodingExc
     this(data, ""UTF-8"");
   }
 
-  @Override
   public String getAuthType() {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
   public Cookie[] getCookies() {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
   public long getDateHeader(String name) {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
   public String getHeader(String name) {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
-  public Enumeration getHeaders(String name) {
+  public Enumeration<String> getHeaders(String name) {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
-  public Enumeration getHeaderNames() {
+  public Enumeration<String> getHeaderNames() {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
   public int getIntHeader(String name) {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
   public String getMethod() {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
   public String getPathInfo() {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
   public String getPathTranslated() {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
   public String getContextPath() {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
   public String getQueryString() {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
   public String getRemoteUser() {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
   public boolean isUserInRole(String role) {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
   public Principal getUserPrincipal() {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
   public String getRequestedSessionId() {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
   public String getRequestURI() {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
   public StringBuffer getRequestURL() {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
   public String getServletPath() {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
   public HttpSession getSession(boolean create) {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
   public HttpSession getSession() {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
   public boolean isRequestedSessionIdValid() {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
   public boolean isRequestedSessionIdFromCookie() {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
   public boolean isRequestedSessionIdFromURL() {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
   public boolean isRequestedSessionIdFromUrl() {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
   public Object getAttribute(String name) {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
-  public Enumeration getAttributeNames() {
+  public Enumeration<String> getAttributeNames() {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
   public String getCharacterEncoding() {
     return charset;
   }
 
-  @Override
   public void setCharacterEncoding(String env) throws UnsupportedEncodingException {
     this.charset = env;
   }
 
-  @Override
   public int getContentLength() {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
   public String getContentType() {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
   public ServletInputStream getInputStream() throws IOException {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
   public String getParameter(String name) {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
-  public Enumeration getParameterNames() {
+  public Enumeration<String> getParameterNames() {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
   public String[] getParameterValues(String name) {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
-  public Map getParameterMap() {
+  public Map<String,String[]> getParameterMap() {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
   public String getProtocol() {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
   public String getScheme() {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
   public String getServerName() {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
   public int getServerPort() {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
   public BufferedReader getReader() throws IOException {
     return reader;
   }
 
-  @Override
   public String getRemoteAddr() {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
   public String getRemoteHost() {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
   public void setAttribute(String name, Object o) {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
   public void removeAttribute(String name) {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
   public Locale getLocale() {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
-  public Enumeration getLocales() {
+  public Enumeration<Locale> getLocales() {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
   public boolean isSecure() {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
   public RequestDispatcher getRequestDispatcher(String path) {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
   public String getRealPath(String path) {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
   public int getRemotePort() {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override
   public String getLocalName() {
     throw new UnsupportedOperationException(""Not supported yet."");
   }
 
-  @Override","[{'comment': 'Why remove all these `@Override`s? There is no longer an inheritance relationship?', 'commenter': 'mpercy'}]"
158,flume-ng-doc/sphinx/FlumeUserGuide.rst,"@@ -1719,25 +1719,42 @@ unavailable status.
 All events sent in one post request are considered to be one batch and
 inserted into the channel in one transaction.
 
-=================  ============================================  =====================================================================================
-Property Name      Default                                       Description
-=================  ============================================  =====================================================================================
-**type**                                                         The component type name, needs to be ``http``
-**port**           --                                            The port the source should bind to.
-bind               0.0.0.0                                       The hostname or IP address to listen on
-handler            ``org.apache.flume.source.http.JSONHandler``  The FQCN of the handler class.
-handler.*          --                                            Config parameters for the handler
-selector.type      replicating                                   replicating or multiplexing
-selector.*                                                       Depends on the selector.type value
-interceptors       --                                            Space-separated list of interceptors
+This source is based on Jetty 9.4 and offers the ability to set additional
+Jetty-specific parameters which will be passed directly to the Jetty components.
+
+====================  ============================================  =====================================================================================
+Property Name         Default                                       Description
+====================  ============================================  =====================================================================================
+**type**                                                            The component type name, needs to be ``http``
+**port**              --                                            The port the source should bind to.
+bind                  0.0.0.0                                       The hostname or IP address to listen on
+handler               ``org.apache.flume.source.http.JSONHandler``  The FQCN of the handler class.
+handler.*             --                                            Config parameters for the handler
+selector.type         replicating                                   replicating or multiplexing
+selector.*                                                          Depends on the selector.type value
+interceptors          --                                            Space-separated list of interceptors
 interceptors.*
-enableSSL          false                                         Set the property true, to enable SSL. *HTTP Source does not support SSLv3.*
-excludeProtocols   SSLv3                                         Space-separated list of SSL/TLS protocols to exclude. SSLv3 is always excluded.
-keystore                                                         Location of the keystore includng keystore file name
-keystorePassword                                                 Keystore password
+enableSSL             false                                         Set the property true, to enable SSL. *HTTP Source does not support SSLv3.*
+excludeProtocols      SSLv3                                         Space-separated list of SSL/TLS protocols to exclude. SSLv3 is always excluded.
+keystore                                                            Location of the keystore includng keystore file name
+keystorePassword                                                    Keystore password
+QueuedThreadPool.*                                                  Jetty specific settings to be set on org.eclipse.jetty.util.thread.QueuedThreadPool.","[{'comment': ""This is clearly a highly flexible approach, but do we really want to adopt all public methods of Jetty being configurable via Flume? Flume has traditionally been nearly 100% backwards compatible and if we directly expose Jetty public methods to users then are we writing checks we can't cash later on? Say if Jetty changes some method names in a future major version that we would like to upgrade to."", 'commenter': 'mpercy'}, {'comment': ""Note the concern, but we've got plenty of users requesting additional configuration. It feels wrong to copy the params in one-by-one as we'll forever be behind the curve. Do you have any thoughts as to how we can improve the configurability? \r\n\r\nI can only think of shading the three relevant classes in a test and specifically including a JUnit to test that properties don't disappear."", 'commenter': 'tmgstevens'}]"
178,flume-ng-sinks/flume-ng-hbase-sink/src/main/java/org/apache/flume/sink/hbase/AsyncHBaseSink.java,"@@ -422,6 +425,19 @@ public void configure(Context context) {
         context.getInteger(HBaseSinkConfigurationConstants.CONFIG_MAX_CONSECUTIVE_FAILS,
                            HBaseSinkConfigurationConstants.DEFAULT_MAX_CONSECUTIVE_FAILS);
 
+
+    ImmutableMap<String, String> asyncProperties","[{'comment': 'Could you please use standard `Map` instead of the `ImmutableMap`?', 'commenter': 'adenes'}]"
181,flume-ng-sinks/flume-hive-sink/metastore_db/README_DO_NOT_TOUCH_FILES.txt,"@@ -0,0 +1,9 @@
+","[{'comment': 'Hi @rgoers , thanks for the pull request\r\nI am wondering how this metastore_db folder is necessary to upgrade the log4j version.', 'commenter': 'szaboferee'}, {'comment': 'This is some leftover from a failed test run. Could you please remove it so we could review the real changes?', 'commenter': 'szaboferee'}, {'comment': ""I didn't modify those files so I'm not sure how they got in the commit. Out of curiosity, is there a reason they aren't copied to the target directory if the tests are going to modify them? Maven convention is that only items in the target directory should be modified by the build so I don't have my gitignore set up to skip these files."", 'commenter': 'rgoers'}, {'comment': 'I will revert those files as soon as I can.', 'commenter': 'rgoers'}]"
181,conf/log4j.properties,"@@ -1,68 +0,0 @@
-#","[{'comment': 'I was wondering why were you changing log4j.properties to XML.\r\nAre there benefits I am not aware of?\r\n\r\nThere might be systems where flume was integrated with log4j.properties format. What is the precedence if there are both .properties and .xml?\r\n', 'commenter': 'szaboferee'}, {'comment': ""Log4j 2 doesn't support the log4j 1 properties format. Even if the log4j 2 properties format is used it is still different than the log4j 1 format. I prefer the XML format."", 'commenter': 'rgoers'}, {'comment': 'What happens if there is a log4j.properties (log4j 2 format) and a log4j.xml file on the classpath at the same time?', 'commenter': 'szaboferee'}, {'comment': 'Log4j uses an @Order annotation on the ConfigurationFactories to determine which factories get precedence. Properties have the highest, followed by yaml and then XML. So if a properties file is present it will be used instead of the xml file.', 'commenter': 'rgoers'}, {'comment': 'Great, thanks for the information.', 'commenter': 'szaboferee'}]"
181,flume-ng-clients/flume-ng-log4jappender/pom.xml,"@@ -67,6 +67,7 @@ limitations under the License.
     <dependency>
       <groupId>log4j</groupId>
       <artifactId>log4j</artifactId>
+      <version>1.2.17</version>","[{'comment': ""Wouldn't be better to have this in the parent dependencyManagement as there are transitive dependencies that are still using log4j 1.x? "", 'commenter': 'szaboferee'}, {'comment': 'No. All references to log4j 1 have been removed except for this one instance.', 'commenter': 'rgoers'}, {'comment': 'All references in flume however, some dependencies of flume has it as a transitive dependency.\r\nFor example hive-shmis, hadoop-common, kafka_2.10.\r\n\r\nWe should harmonize the versions for these dependencies as well to avoid version mismatches. ', 'commenter': 'szaboferee'}, {'comment': 'I went through and excluded all the transitive dependencies to log4j in the Flume pom.xml files that I could find. They have been replaced with references to log4j-1.2-api.', 'commenter': 'rgoers'}, {'comment': ""Seeing the output of `mvn dependency:tree` there are a few places where it looks like you might have missed it. Could you please double check it to see if I am right? \r\n\r\nWouldn't it be easier to use the dependencyManagement in parent pom to exclude the old version?"", 'commenter': 'szaboferee'}, {'comment': 'It might. I will take a look again.', 'commenter': 'rgoers'}]"
181,flume-ng-sinks/flume-http-sink/src/main/java/org/apache/flume/sink/http/HttpSink.java,"@@ -27,7 +27,8 @@
 import org.apache.flume.conf.Configurable;
 import org.apache.flume.instrumentation.SinkCounter;
 import org.apache.flume.sink.AbstractSink;
-import org.apache.log4j.Logger;
+import org.apache.logging.log4j.LogManager;","[{'comment': 'I would prefer the slf4j logger here because that way the user could decide to use a different logger implementation.\r\n(despite the fact that the original code already used log4j)', 'commenter': 'szaboferee'}, {'comment': 'Because it was using log4j I converted it to use log4j...', 'commenter': 'rgoers'}, {'comment': 'Flume components are using slf4j. I guess this log4j logger in the HttpSink was overlooked at the review and that is why it is not an slf4j logger. It would be nice if it could be consistent. ', 'commenter': 'szaboferee'}, {'comment': ""I have no problem if that change is made but it doesn't necessarily have to happen as part of this PR."", 'commenter': 'rgoers'}, {'comment': 'I agree. I will make a different PR about it. Thank you.', 'commenter': 'szaboferee'}]"
182,flume-ng-sinks/flume-ng-morphline-solr-sink/pom.xml,"@@ -32,7 +32,7 @@ limitations under the License.
     <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
     <solr.version>${solr-global.version}</solr.version>
     <solr.expected.version>${solr-global.version}</solr.expected.version> <!-- sanity check to verify we actually run against the expected version rather than some outdated version -->
-    <surefire.version>${mvn-surefire-plugin-old-morphline.version}</surefire.version>
+    <surefire.version>${mvn-surefire-plugin.version}</surefire.version>","[{'comment': 'I would remove this property and also the version tag from the part:\r\n`        <groupId>org.apache.maven.plugins</groupId>\r\n        <artifactId>maven-surefire-plugin</artifactId>\r\n        <version>${surefire.version}</version>`\r\n\r\nso the plugin could use the pluginManagement settings from the parent pom', 'commenter': 'szaboferee'}, {'comment': '`<plugin>\r\n        <groupId>org.apache.maven.plugins</groupId>\r\n        <artifactId>maven-surefire-plugin</artifactId>\r\n        <configuration>\r\n          <argLine>-Dtests.locale=en_us</argLine>\r\n          <redirectTestOutputToFile>true</redirectTestOutputToFile>\r\n        </configuration>\r\n      </plugin>`', 'commenter': 'mcsanady'}]"
185,flume-ng-sinks/flume-ng-kafka-sink/src/test/java/org/apache/flume/sink/kafka/util/TestUtil.java,"@@ -57,6 +57,7 @@ private void init() {
     // get the localhost.
     try {
       hostname = InetAddress.getLocalHost().getHostName();
+      hostname = ""localhost"";","[{'comment': 'Please remove this. This is probably due to an incorrect /etc/hosts file in your test environment', 'commenter': 'tmgstevens'}]"
185,flume-ng-sources/flume-kafka-source/pom.xml,"@@ -83,9 +83,8 @@
     </dependency>
     <dependency>
       <groupId>org.apache.kafka</groupId>
-      <artifactId>kafka_2.10</artifactId>
-      <classifier>test</classifier>
-      <scope>test</scope>","[{'comment': 'test scope should remain for this - we only need full Kafka for the tests, right?', 'commenter': 'tmgstevens'}]"
185,flume-ng-sources/flume-kafka-source/src/test/java/org/apache/flume/source/kafka/TestKafkaSource.java,"@@ -46,6 +46,7 @@
 import org.apache.kafka.common.security.JaasUtils;
 import org.junit.After;
 import org.junit.Before;
+import org.junit.Ignore;","[{'comment': 'What is this for?', 'commenter': 'tmgstevens'}]"
185,flume-ng-sources/flume-kafka-source/src/test/java/org/apache/flume/source/kafka/TestKafkaSource.java,"@@ -20,7 +20,7 @@
 import com.google.common.base.Charsets;
 import com.google.common.collect.Lists;
 import junit.framework.Assert;
-import kafka.common.TopicExistsException;
+import org.apache.kafka.common.errors.TopicExistsException;","[{'comment': 'What is this for?', 'commenter': 'tmgstevens'}]"
185,flume-ng-sources/flume-kafka-source/src/main/java/org/apache/flume/source/kafka/KafkaSource.java,"@@ -57,16 +46,31 @@
 import org.apache.kafka.clients.consumer.OffsetAndMetadata;
 import org.apache.kafka.common.PartitionInfo;
 import org.apache.kafka.common.TopicPartition;
-import org.apache.kafka.common.protocol.SecurityProtocol;
 import org.apache.kafka.common.security.JaasUtils;
+import org.apache.kafka.common.security.auth.SecurityProtocol;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
-
-import com.google.common.base.Optional;
 import scala.Option;
 
-import static org.apache.flume.source.kafka.KafkaSourceConstants.*;
-import static scala.collection.JavaConverters.asJavaListConverter;
+import java.io.ByteArrayInputStream;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Properties;
+import java.util.UUID;
+import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.regex.Pattern;
+
+import static org.apache.flume.source.kafka.KafkaSourceConstants.DEFAULT_GROUP_ID;","[{'comment': 'What are these imports for? Please review.', 'commenter': 'tmgstevens'}, {'comment': 'Sorry its my IntelliJ settings, we have a policy not to use wildcard imports.  I can revert it back if it is not acceptable.  Please advise.', 'commenter': 'vegiapparao'}, {'comment': 'I think our style guide allows wildcard imports for static fields. Please try to ensure that non-stylistic changes get brought in when submitting a patch.', 'commenter': 'tmgstevens'}, {'comment': 'Import style changes have been reverted.', 'commenter': 'vegiapparao'}]"
185,flume-ng-sdk/src/test/java/org/apache/flume/event/TestEventBuilder.java,"@@ -19,14 +19,14 @@
 
 package org.apache.flume.event;
 
-import java.util.HashMap;
-import java.util.Map;
-
 import org.apache.flume.Event;
 import org.apache.flume.FlumeException;
 import org.junit.Assert;
 import org.junit.Test;
 
+import java.util.HashMap;
+import java.util.Map;
+","[{'comment': 'These changes are redundant', 'commenter': 'tmgstevens'}]"
185,flume-ng-channels/flume-kafka-channel/src/test/resources/kafka-server.properties,"@@ -18,20 +18,24 @@
 ############################# Server Basics #############################
 
 # The id of the broker. This must be set to a unique integer for each broker.
-broker.id=0
+broker.id=-1
 
 ############################# Socket Server Settings #############################
 
 # The port the socket server listens on
 port=9092
 
+listeners = PLAINTEXT://0.0.0.0:9092
+
 # Hostname the broker will bind to. If not set, the server will bind to all interfaces
 #host.name=localhost
 
 # Hostname the broker will advertise to producers and consumers. If not set, it uses the
 # value for ""host.name"" if configured.  Otherwise, it will use the value returned from
 # java.net.InetAddress.getCanonicalHostName().
 #advertised.host.name=<hostname routable by clients>
+advertised.listeners=PLAINTEXT://localhost:9092
+advertised.host.name = localhost","[{'comment': ""We shouldn't need these changes"", 'commenter': 'tmgstevens'}]"
185,flume-ng-channels/flume-kafka-channel/src/main/java/org/apache/flume/channel/kafka/KafkaChannel.java,"@@ -76,8 +77,32 @@
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicReference;
 
-import static org.apache.flume.channel.kafka.KafkaChannelConfiguration.*;","[{'comment': 'I believe our style guide allows wildcard imports for static fields', 'commenter': 'tmgstevens'}, {'comment': 'Reverted the change', 'commenter': 'vegiapparao'}]"
185,flume-ng-channels/flume-kafka-channel/src/main/java/org/apache/flume/channel/kafka/KafkaChannel.java,"@@ -296,8 +321,9 @@ private synchronized ConsumerAndRecords createConsumerAndRecords() {
       KafkaConsumer<String, byte[]> consumer = new KafkaConsumer<String, byte[]>(consumerProps);
       ConsumerAndRecords car = new ConsumerAndRecords(consumer, channelUUID);
       logger.info(""Created new consumer to connect to Kafka"");
-      car.consumer.subscribe(Arrays.asList(topic.get()),","[{'comment': ""I'm not sure why you've changed this."", 'commenter': 'tmgstevens'}, {'comment': ""Why are we adding in a dangling ChannelRebalanceListener. Can we be sure that this doesn't introduce bugs?"", 'commenter': 'tmgstevens'}, {'comment': '@tmgstevens, references to ChannelRebalanceListener has been removed.  There was a compilation error when I first started making changes and hence I added `ChannelRebalanceListener`. ', 'commenter': 'vegiapparao'}]"
185,flume-ng-channels/flume-kafka-channel/src/test/java/org/apache/flume/channel/kafka/TestBasicFunctionality.java,"@@ -49,6 +49,7 @@ public void testProps() throws Exception {
     Context context = new Context();
     context.put(""kafka.producer.some-parameter"", ""1"");
     context.put(""kafka.consumer.another-parameter"", ""1"");
+    context.put(""offsets.topic.replication.factor"", ""1"");","[{'comment': 'What does this change bring? Did we need this to get the test working?', 'commenter': 'tmgstevens'}, {'comment': 'Removed it now.  Initially, the tests failed with an error and an online search suggested to have this property in kafka-server.properties.  ', 'commenter': 'vegiapparao'}, {'comment': ""The tests intermittently fail with this error.\r\n`Number of alive brokers '1' does not meet the required replication factor '3' for the offsets topic (configured via 'offsets.topic.replication.factor'). This error can be ignored if the cluster is starting up and not all brokers are up yet`.  The tests would pass consistently by having the property `offsets.topic.replication.factor = 1` in `kafka-server.properties`.  "", 'commenter': 'vegiapparao'}]"
185,flume-ng-channels/flume-kafka-channel/src/test/java/org/apache/flume/channel/kafka/TestKafkaChannelBase.java,"@@ -120,6 +122,7 @@ Context prepareDefaultContext(boolean parseAsFlume) {
     Context context = new Context();
     context.put(BOOTSTRAP_SERVERS_CONFIG, testUtil.getKafkaServerUrl());
     context.put(PARSE_AS_FLUME_EVENT, String.valueOf(parseAsFlume));
+    context.put(""offsets.topic.replication.factor"", ""1"");","[{'comment': 'What does this change bring? Did we need this to get the test working?', 'commenter': 'tmgstevens'}, {'comment': 'Removed it now.', 'commenter': 'vegiapparao'}]"
185,flume-ng-channels/flume-kafka-channel/src/test/java/org/apache/flume/channel/kafka/TestOffsetsAndMigration.java,"@@ -121,6 +121,7 @@ private void doTestMigrateZookeeperOffsets(boolean hasZookeeperOffsets, boolean
     Context context = prepareDefaultContext(false);
     context.put(ZOOKEEPER_CONNECT_FLUME_KEY, testUtil.getZkUrl());
     context.put(GROUP_ID_FLUME, group);
+    context.put(""offsets.topic.replication.factor"", ""1"");","[{'comment': 'What does this change bring? Did we need this to get the test working?', 'commenter': 'tmgstevens'}, {'comment': 'Removed it now.', 'commenter': 'vegiapparao'}]"
185,flume-ng-sources/flume-kafka-source/src/main/java/org/apache/flume/source/kafka/KafkaSource.java,"@@ -16,21 +16,10 @@
  */
 package org.apache.flume.source.kafka;
 
-import java.io.ByteArrayInputStream;","[{'comment': 'Please tidy this commit to avoid reformatting changes (for example reordering of imports)', 'commenter': 'tmgstevens'}]"
185,flume-ng-sources/flume-kafka-source/src/main/java/org/apache/flume/source/kafka/KafkaSource.java,"@@ -464,12 +468,17 @@ private String lookupBootstrap(String zookeeperConnect, SecurityProtocol securit
     ZkUtils zkUtils = ZkUtils.apply(zookeeperConnect, ZK_SESSION_TIMEOUT, ZK_CONNECTION_TIMEOUT,
         JaasUtils.isZkSecurityEnabled());
     try {
-      List<BrokerEndPoint> endPoints =
-          asJavaListConverter(zkUtils.getAllBrokerEndPointsForChannel(securityProtocol)).asJava();
+      List<Broker> brokers = seqAsJavaList(zkUtils.getAllBrokersInCluster());
+
       List<String> connections = new ArrayList<>();
-      for (BrokerEndPoint endPoint : endPoints) {
-        connections.add(endPoint.connectionString());
+
+      for (Broker broker : brokers) {
+        List<EndPoint> endPointList = seqAsJavaList(broker.endPoints());
+        for (EndPoint endPoint : endPointList) {
+          connections.add(endPoint.host() + "":"" + endPoint.port());","[{'comment': ""I'm intrigued by this change - is endPoint.connectionString no longer supplied? It seems the old version would have been better."", 'commenter': 'tmgstevens'}, {'comment': '`endPoint.connectingString` does exist in the new library, but the newer one returns additional text.  This is evident from the test `TestKafkaSource#testBootstrapLookup`.  The test fails with the following output.  \r\n\r\nExpected :localhost:65069\r\nActual   :PLAINTEXT://localhost:65069\r\n', 'commenter': 'vegiapparao'}]"
185,flume-ng-sources/flume-kafka-source/src/test/java/org/apache/flume/source/kafka/KafkaSourceEmbeddedKafka.java,"@@ -67,6 +68,7 @@ public KafkaSourceEmbeddedKafka(Properties properties) {
     Properties props = new Properties();
     props.put(""zookeeper.connect"",zookeeper.getConnectString());
     props.put(""broker.id"",""1"");
+    props.put(""offsets.topic.replication.factor"", ""1"");","[{'comment': ""Please remove if not req'd"", 'commenter': 'tmgstevens'}, {'comment': 'Removed', 'commenter': 'vegiapparao'}]"
185,flume-ng-sources/flume-kafka-source/src/test/java/org/apache/flume/source/kafka/TestKafkaSource.java,"@@ -69,14 +69,14 @@
 import static org.apache.flume.source.kafka.KafkaSourceConstants.BATCH_SIZE;
 import static org.apache.flume.source.kafka.KafkaSourceConstants.BOOTSTRAP_SERVERS;
 import static org.apache.flume.source.kafka.KafkaSourceConstants.DEFAULT_AUTO_COMMIT;
+import static org.apache.flume.source.kafka.KafkaSourceConstants.DEFAULT_TOPIC_HEADER;","[{'comment': 'Please avoid reordering imports', 'commenter': 'tmgstevens'}]"
185,flume-shared/flume-shared-kafka-test/src/main/java/org/apache/flume/shared/kafka/test/KafkaPartitionTestUtil.java,"@@ -199,7 +200,9 @@ public static void checkResultsAgainstSkew(PartitionTestScenario scenario,
 
       TopicPartition partition = new TopicPartition(topic, i);
 
-      consumer.assign(Arrays.asList(partition));","[{'comment': 'What does this change bring?', 'commenter': 'tmgstevens'}, {'comment': '`KafkaConsume.assign` signature has been changed in 0.10.0.0\r\n0.9.0.1 => `public void assign(java.util.List<org.apache.kafka.common.TopicPartition>);`\r\n0.10.0.0 => `public void assign(java.util.Collection<org.apache.kafka.common.TopicPartition>);`', 'commenter': 'vegiapparao'}]"
197,flume-ng-configfilters/flume-ng-external-process-config-filter/src/main/java/org/apache/flume/configfilter/ExternalProcessConfigFilter.java,"@@ -0,0 +1,98 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.flume.configfilter;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import java.io.InputStream;
+import java.nio.charset.Charset;
+import java.nio.charset.UnsupportedCharsetException;
+import java.util.Arrays;
+import java.util.Map;
+import java.util.Scanner;
+
+public class ExternalProcessConfigFilter extends AbstractConfigFilter {
+
+  private static final Logger LOGGER = LoggerFactory.getLogger(ExternalProcessConfigFilter.class);
+
+  private static final String COMMAND_KEY = ""command"";
+  private static final String CHARSET_KEY = ""charset"";
+  private static final String CHARSET_DEFAULT = ""UTF-8"";
+
+  Charset charset;
+  private String command;
+
+  @Override
+  public String filter(String key) {
+    try {
+      return execCommand(key);
+    } catch (InterruptedException | IllegalStateException | IOException ex) {
+      LOGGER.error(""Error while reading value for key {}: "", key, ex);
+    }
+    return null;
+  }
+
+  @Override
+  public void initializeWithConfiguration(Map<String, String> configuration) {
+    String charsetName = configuration.getOrDefault(CHARSET_KEY, CHARSET_DEFAULT);
+    try {
+      charset = Charset.forName(charsetName);
+    } catch (UnsupportedCharsetException ex) {
+      throw new RuntimeException(""Unsupported charset: "" + charsetName, ex);
+    }
+
+    command = configuration.get(COMMAND_KEY);
+    if (command == null) {
+      throw new IllegalArgumentException(COMMAND_KEY + "" must be set for "" +
+          ""ExternalProcessConfigFilter"");
+    }
+
+  }
+
+  private String execCommand(String key) throws IOException, InterruptedException {
+    String[] split = command.split(""\\s+"");
+    int newLength = split.length + 1;
+    String[] commandParts = Arrays.copyOf(split, newLength);
+    commandParts[newLength - 1] = key;
+    Process p = Runtime.getRuntime().exec(commandParts);
+    p.waitFor();
+    if (p.exitValue() != 0) {
+      String stderr;
+      try {
+        stderr = getResultFromStream(p.getErrorStream());
+      } catch (Throwable t) {
+        stderr = null;
+      }
+      throw new IllegalStateException(
+          String.format(""Process (%s) exited with non-zero (%s) status code. Sterr: %s"",
+              this.command, p.exitValue(), stderr));
+    }
+
+
+    return getResultFromStream(p.getInputStream());
+  }
+
+  private String getResultFromStream(InputStream inputStream) {
+    try (Scanner scanner = new Scanner(inputStream, charset.name())) {
+      return scanner.useDelimiter(""\\A"").next().trim();","[{'comment': 'Why is the `useDelimiter(""\\\\A"")` needed here?\r\nOr why not use the good old `BufferedReader.readLine()`?', 'commenter': 'adenes'}, {'comment': 'using the delimiter \\A (begining of the string) lets scanner to read the whole input and not just one line. Scanner also has readLine();\r\n\r\nI can change it to readLine if it makes more sense to just read the first line of the input.', 'commenter': 'szaboferee'}]"
197,flume-ng-configuration/src/main/java/org/apache/flume/conf/FlumeConfiguration.java,"@@ -348,75 +381,135 @@ private boolean isValid() {
       sinkgroupSet = validateGroups(sinkSet);
 
       // If no sources or sinks are present, then this is invalid
-      if (sourceSet.size() == 0 && sinkSet.size() == 0) {
-        logger.warn(""Agent configuration for '"" + agentName
-            + ""' has no sources or sinks. Will be marked invalid."");
-        errorList.add(new FlumeConfigurationError(agentName,
-            BasicConfigurationConstants.CONFIG_SOURCES,
-            FlumeConfigurationErrorType.PROPERTY_VALUE_NULL,
-            ErrorOrWarning.ERROR));
-        errorList.add(new FlumeConfigurationError(agentName,
-            BasicConfigurationConstants.CONFIG_SINKS,
-            FlumeConfigurationErrorType.PROPERTY_VALUE_NULL,
-            ErrorOrWarning.ERROR));
+      if (sourceSet.isEmpty() && sinkSet.isEmpty()) {
+        LOGGER.warn(
+            ""Agent configuration for '{}' has no sources or sinks. Will be marked invalid."",
+            agentName
+        );
+        addError(CONFIG_SOURCES, PROPERTY_VALUE_NULL, ERROR);
+        addError(CONFIG_SINKS, PROPERTY_VALUE_NULL, ERROR);
         return false;
       }
 
       // Now rewrite the sources/sinks/channels
 
-      this.sources = getSpaceDelimitedList(sourceSet);
-      this.channels = getSpaceDelimitedList(channelSet);
-      this.sinks = getSpaceDelimitedList(sinkSet);
-      this.sinkgroups = getSpaceDelimitedList(sinkgroupSet);
+      this.configFilters = getSpaceDelimitedList(configFilterSet);
+      sources = getSpaceDelimitedList(sourceSet);
+      channels = getSpaceDelimitedList(channelSet);
+      sinks = getSpaceDelimitedList(sinkSet);
+      sinkgroups = getSpaceDelimitedList(sinkgroupSet);
 
-      if (logger.isDebugEnabled() && LogPrivacyUtil.allowLogPrintConfig()) {
-        logger.debug(""Post validation configuration for {}"", agentName);
-        logger.debug(this.getPostvalidationConfig());
+      if (LOGGER.isDebugEnabled() && LogPrivacyUtil.allowLogPrintConfig()) {
+        LOGGER.debug(""Post validation configuration for {}"", agentName);
+        LOGGER.debug(getPostvalidationConfig());
       }
 
       return true;
     }
 
-    private ChannelType getKnownChannel(String type) {
-      ChannelType[] values = ChannelType.values();
-      for (ChannelType value : values) {
-        if (value.toString().equalsIgnoreCase(type)) return value;
+    private void runFiltersThroughConfigs() {
+      runFiltersOnContextMaps(
+          sourceContextMap,
+          channelContextMap,
+          sinkContextMap,
+          sinkGroupContextMap
+      );
+    }
 
-        String channel = value.getChannelClassName();
+    private void runFiltersOnContextMaps(Map<String, Context>... maps) {
+      for (Map<String, Context> map: maps) {
+        for (Context c : map.values()) {
+          for (Entry<String, String> k : c.getParameters().entrySet()) {
+            c.put(k.getKey(), filterValue(k.getValue()));
+          }
+        }
+      }
+    }
 
-        if (channel != null && channel.equalsIgnoreCase(type)) return value;
+    private void createConfigFilters() {
+      for (String name: configFilterSet) {
+        Context context = configFilterContextMap.get(name);
+        ComponentConfiguration componentConfiguration = configFilterConfigMap.get(name);
+        try {
+
+          if (context != null) {
+            ConfigFilter configFilter = ConfigFilterFactory.create(
+                name, context.getString(BasicConfigurationConstants.CONFIG_TYPE)
+            );
+            configFilter.initializeWithConfiguration(context.getParameters());
+            configFiltersInstances.add(configFilter);
+          } else if (componentConfiguration != null) {
+            ConfigFilter configFilter = ConfigFilterFactory.create(
+                componentConfiguration.getComponentName(), componentConfiguration.getType()
+            );
+            configFiltersInstances.add(configFilter);
+          }
+        } catch (Exception e) {
+          LOGGER.error(""Error while creating config filter {}"", name, e);
+        }
+      }
+    }
 
+    private String filterValue(String value) {
+      for (ConfigFilter configFilter : configFiltersInstances) {
+        //JAVA EL expression style ${myFilterName['my_key']} or
+        //JAVA EL expression style ${myFilterName[""my_key""]} or
+        //JAVA EL expression style ${myFilterName[my_key]}
+        List<String> delimiters = Arrays.asList(""'"", ""\"""", """");
+        for (String delimiter: delimiters) {
+          String prefix = ""${"" + configFilter.getName() + ""["" + delimiter;
+          String suffix = delimiter + ""]}"";
+          boolean matchesWithDelimiter = value.startsWith(prefix) && value.endsWith(suffix);
+
+          if (matchesWithDelimiter) {
+            String filteredValue = configFilter.filter(value.substring(
+                configFilter.getName().length() + delimiter.length() + 3,","[{'comment': 'This can be simplified to `prefix.length()`', 'commenter': 'adenes'}, {'comment': 'BTW, why not regexp with capture group and back reference for the delimiter?', 'commenter': 'adenes'}, {'comment': 'will change to regex and make it more flexible. I chose not to use regex, because it is always slower than simple string operations. Fortunately, it has to run only when config changes and only as many times as many lines the config has. A user has to have a large configuration to feel the difference, so It is ok to change it to regex.', 'commenter': 'szaboferee'}]"
197,flume-ng-configuration/src/main/java/org/apache/flume/conf/FlumeConfiguration.java,"@@ -348,75 +381,135 @@ private boolean isValid() {
       sinkgroupSet = validateGroups(sinkSet);
 
       // If no sources or sinks are present, then this is invalid
-      if (sourceSet.size() == 0 && sinkSet.size() == 0) {
-        logger.warn(""Agent configuration for '"" + agentName
-            + ""' has no sources or sinks. Will be marked invalid."");
-        errorList.add(new FlumeConfigurationError(agentName,
-            BasicConfigurationConstants.CONFIG_SOURCES,
-            FlumeConfigurationErrorType.PROPERTY_VALUE_NULL,
-            ErrorOrWarning.ERROR));
-        errorList.add(new FlumeConfigurationError(agentName,
-            BasicConfigurationConstants.CONFIG_SINKS,
-            FlumeConfigurationErrorType.PROPERTY_VALUE_NULL,
-            ErrorOrWarning.ERROR));
+      if (sourceSet.isEmpty() && sinkSet.isEmpty()) {
+        LOGGER.warn(
+            ""Agent configuration for '{}' has no sources or sinks. Will be marked invalid."",
+            agentName
+        );
+        addError(CONFIG_SOURCES, PROPERTY_VALUE_NULL, ERROR);
+        addError(CONFIG_SINKS, PROPERTY_VALUE_NULL, ERROR);
         return false;
       }
 
       // Now rewrite the sources/sinks/channels
 
-      this.sources = getSpaceDelimitedList(sourceSet);
-      this.channels = getSpaceDelimitedList(channelSet);
-      this.sinks = getSpaceDelimitedList(sinkSet);
-      this.sinkgroups = getSpaceDelimitedList(sinkgroupSet);
+      this.configFilters = getSpaceDelimitedList(configFilterSet);
+      sources = getSpaceDelimitedList(sourceSet);
+      channels = getSpaceDelimitedList(channelSet);
+      sinks = getSpaceDelimitedList(sinkSet);
+      sinkgroups = getSpaceDelimitedList(sinkgroupSet);
 
-      if (logger.isDebugEnabled() && LogPrivacyUtil.allowLogPrintConfig()) {
-        logger.debug(""Post validation configuration for {}"", agentName);
-        logger.debug(this.getPostvalidationConfig());
+      if (LOGGER.isDebugEnabled() && LogPrivacyUtil.allowLogPrintConfig()) {
+        LOGGER.debug(""Post validation configuration for {}"", agentName);
+        LOGGER.debug(getPostvalidationConfig());
       }
 
       return true;
     }
 
-    private ChannelType getKnownChannel(String type) {
-      ChannelType[] values = ChannelType.values();
-      for (ChannelType value : values) {
-        if (value.toString().equalsIgnoreCase(type)) return value;
+    private void runFiltersThroughConfigs() {
+      runFiltersOnContextMaps(
+          sourceContextMap,
+          channelContextMap,
+          sinkContextMap,
+          sinkGroupContextMap
+      );
+    }
 
-        String channel = value.getChannelClassName();
+    private void runFiltersOnContextMaps(Map<String, Context>... maps) {
+      for (Map<String, Context> map: maps) {
+        for (Context c : map.values()) {
+          for (Entry<String, String> k : c.getParameters().entrySet()) {
+            c.put(k.getKey(), filterValue(k.getValue()));
+          }
+        }
+      }
+    }
 
-        if (channel != null && channel.equalsIgnoreCase(type)) return value;
+    private void createConfigFilters() {
+      for (String name: configFilterSet) {
+        Context context = configFilterContextMap.get(name);
+        ComponentConfiguration componentConfiguration = configFilterConfigMap.get(name);
+        try {
+
+          if (context != null) {
+            ConfigFilter configFilter = ConfigFilterFactory.create(
+                name, context.getString(BasicConfigurationConstants.CONFIG_TYPE)
+            );
+            configFilter.initializeWithConfiguration(context.getParameters());
+            configFiltersInstances.add(configFilter);
+          } else if (componentConfiguration != null) {
+            ConfigFilter configFilter = ConfigFilterFactory.create(
+                componentConfiguration.getComponentName(), componentConfiguration.getType()
+            );
+            configFiltersInstances.add(configFilter);
+          }
+        } catch (Exception e) {
+          LOGGER.error(""Error while creating config filter {}"", name, e);
+        }
+      }
+    }
 
+    private String filterValue(String value) {
+      for (ConfigFilter configFilter : configFiltersInstances) {
+        //JAVA EL expression style ${myFilterName['my_key']} or
+        //JAVA EL expression style ${myFilterName[""my_key""]} or
+        //JAVA EL expression style ${myFilterName[my_key]}
+        List<String> delimiters = Arrays.asList(""'"", ""\"""", """");
+        for (String delimiter: delimiters) {
+          String prefix = ""${"" + configFilter.getName() + ""["" + delimiter;
+          String suffix = delimiter + ""]}"";
+          boolean matchesWithDelimiter = value.startsWith(prefix) && value.endsWith(suffix);
+
+          if (matchesWithDelimiter) {
+            String filteredValue = configFilter.filter(value.substring(","[{'comment': ""Should we catch & log `Exception`s (or `Throwable`s) here instead of relying on the implementations? What'd happen if there is an unexpected exception in the `filter()` method?"", 'commenter': 'adenes'}, {'comment': 'I will add a try catch here and the value will not be replaced if an exception has been thrown', 'commenter': 'szaboferee'}]"
197,flume-ng-configfilters/flume-ng-external-process-config-filter/pom.xml,"@@ -0,0 +1,89 @@
+<?xml version=""1.0"" encoding=""UTF-8""?>
+<!--
+Licensed to the Apache Software Foundation (ASF) under one or more
+contributor license agreements.  See the NOTICE file distributed with
+this work for additional information regarding copyright ownership.
+The ASF licenses this file to You under the Apache License, Version 2.0
+(the ""License""); you may not use this file except in compliance with
+the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an ""AS IS"" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+-->
+<project xmlns=""http://maven.apache.org/POM/4.0.0""
+         xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
+         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
+    <parent>
+        <artifactId>flume-ng-configfilters</artifactId>
+        <groupId>org.apache.flume</groupId>
+        <version>1.9.0-SNAPSHOT</version>
+    </parent>
+    <modelVersion>4.0.0</modelVersion>
+
+    <artifactId>flume-ng-external-process-config-filter</artifactId>
+    <name>Flume NG External Process Config Filter</name>
+
+    <dependencies>
+        <dependency>
+            <groupId>org.slf4j</groupId>
+            <artifactId>slf4j-api</artifactId>
+        </dependency>
+        <dependency>
+            <groupId>org.apache.flume</groupId>
+            <artifactId>flume-ng-config-filter-api</artifactId>
+        </dependency>
+        <dependency>
+            <groupId>junit</groupId>
+            <artifactId>junit</artifactId>
+            <version>4.8.2</version>
+            <scope>test</scope>
+        </dependency>
+    </dependencies>
+ <profiles>
+     <!-- workaround fro test resource permission errors -->
+     <profile>
+         <id>unix</id>
+         <activation>
+             <os>
+                 <family>unix</family>
+             </os>
+         </activation>
+         <build>
+             <plugins>
+                 <plugin>
+                     <groupId>org.codehaus.mojo</groupId>
+                     <artifactId>exec-maven-plugin</artifactId>
+                     <executions>
+                         <execution>
+                             <id>fix-resource-permissions</id>
+                             <goals>
+                                 <goal>exec</goal>
+                             </goals>
+                             <phase>process-test-resources</phase>
+                             <configuration>
+                                 <executable>/bin/sh</executable>
+                                 <arguments>
+                                     <argument>-c</argument>
+                                     <argument>
+                                         set -x
+
+                                         SRC=""${basedir}/src/test/resources""
+                                         DST=""${project.build.directory}/test-classes""
+
+                                         find ""$$SRC"" -printf ""%P\0"" | xargs --verbose -0 -I {} chmod --reference=""$$SRC/{}"" -f ""$$DST/{}""","[{'comment': '--verbose switch does not work on macos', 'commenter': 'mcsanady'}, {'comment': 'thanks!', 'commenter': 'szaboferee'}]"
201,flume-ng-core/src/main/java/org/apache/flume/source/ExecSource.java,"@@ -333,7 +333,7 @@ public void run() {
           while ((line = reader.readLine()) != null) {
             sourceCounter.incrementEventReceivedCount();
             synchronized (eventList) {
-              eventList.add(EventBuilder.withBody(line.getBytes(charset)));
+              eventList.add(EventBuilder.withBody(line.getBytes(""UTF-8"")));","[{'comment': 'The `charset` variable you just removed contains the configurable charset which is `UTF-8` by default if you have not changed it in your configuration.\r\n\r\nDid this change actually fix your issue? ', 'commenter': 'szaboferee'}, {'comment': 'Yes, it is solved.\r\nLook roughly at the data flow:\r\nIf I specify a1.sources.r1.charset = GBK in the source configuration, the whole process of the data is encoded as follows:\r\nExecSource read data (GBK) -->> Data write eventList use encoding (GBK) --->> RegexExtractorInterceptor.java line 130 decoding (UTF-8, garbled) --->> elasticsearchSink default decoding (UTF-8 and no configuration items, Garbled)\r\nSo, I think that after reading out the data according to the configuration code, it should use the default UTF-8 encoding (getBytes()) instead of the configured encoding (GBK), otherwise the subsequent decoding (UTF- 8) Will be garbled.\r\n----------------------------------------------------------------------\r\n\r\n\r\nsourcea1.sources.r1.charset = GBK\r\nexecSourceGBK>> eventListGBK >> RegexExtractorInterceptor.java130UTF-8>> elasticsearchSinkUTF-8\r\nUTF-8getBytesGBKUTF-8', 'commenter': 'xvjie'}, {'comment': 'I see. Thanks for the clarification. In this case, the charset change should be either in the elasticsearchsink or should be a new config parameter like `targerCharset`', 'commenter': 'szaboferee'}]"
202,flume-ng-sinks/flume-hdfs-sink/src/main/java/org/apache/flume/sink/hdfs/BucketWriter.java,"@@ -114,6 +114,7 @@
   // reopened. Not ideal, but avoids internals of owners
   protected boolean closed = false;
   AtomicInteger renameTries = new AtomicInteger(0);
+  AtomicInteger closeTries = new AtomicInteger(0);","[{'comment': 'This line seems redundant.', 'commenter': 'jojochuang'}, {'comment': 'Agree, this is an unused variable.', 'commenter': 'majorendre'}]"
202,flume-ng-sinks/flume-hdfs-sink/src/test/java/org/apache/flume/sink/hdfs/MockHDFSWriter.java,"@@ -85,6 +101,15 @@ public void sync() throws IOException {
 
   public void close() throws IOException {
     filesClosed++;
+    currentCloseAttempts.incrementAndGet();
+    logger.info(""Attempting to close: '"" + currentCloseAttempts + ""' of '"" +
+        numberOfRetriesRequired + ""'"");
+    if (currentCloseAttempts.get() >= numberOfRetriesRequired || numberOfRetriesRequired == 0) {
+      logger.info(""closing file"");
+
+    } else {
+      throw new IOException(""MockIOException"");","[{'comment': 'This message should log the reason an exception is thrown.', 'commenter': 'jojochuang'}]"
202,flume-ng-sinks/flume-hdfs-sink/src/test/java/org/apache/flume/sink/hdfs/TestBucketWriter.java,"@@ -450,6 +456,39 @@ public void SequenceFileRenameRetryCoreTest(int numberOfRetriesRequired, boolean
                       bucketWriter.renameTries.get() == numberOfRetriesRequired);
   }
 
+  public void SequenceFileCloseRetryCoreTest(int numberOfRetriesRequired)
+      throws Exception {
+    String hdfsPath = ""file:///tmp/flume-test."" +
+        Calendar.getInstance().getTimeInMillis() +
+        ""."" + Thread.currentThread().getId();
+
+    Context context = new Context();
+    Configuration conf = new Configuration();
+    FileSystem fs = FileSystem.get(conf);
+    Path dirPath = new Path(hdfsPath);
+    fs.delete(dirPath, true);
+    fs.mkdirs(dirPath);
+    context.put(""hdfs.path"", hdfsPath);
+    context.put(""hdfs.closeTries"", String.valueOf(numberOfRetriesRequired));
+    context.put(""hdfs.rollCount"", ""1"");
+    context.put(""hdfs.retryInterval"", ""1"");
+    context.put(""hdfs.callTimeout"", Long.toString(1000));
+    MockHDFSWriter mockHDFSWriter = new MockHDFSWriter(numberOfRetriesRequired);
+    BucketWriter bucketWriter = new BucketWriter(
+        0, 0, 1, 1, ctx, hdfsPath, hdfsPath, ""singleBucket"", "".tmp"", null, null,
+        null, mockHDFSWriter, timedRollerPool, proxy,
+        new SinkCounter(""test-bucket-writer-"" + System.currentTimeMillis()), 0, null, null, 30000,
+        Executors.newSingleThreadExecutor(), 1, numberOfRetriesRequired);
+
+    Event event = EventBuilder.withBody(""test"", Charsets.UTF_8);
+    bucketWriter.append(event);
+    bucketWriter.close(false);
+    TimeUnit.SECONDS.sleep(numberOfRetriesRequired + 2);
+    Assert.assertTrue(""Expected "" + numberOfRetriesRequired + "" "" +","[{'comment': 'This is good, but it would be even better to use Assert.assertEquals()', 'commenter': 'jojochuang'}]"
203,flume-ng-core/src/test/java/org/apache/flume/source/shaded/guava/TestRateLimiter.java,"@@ -0,0 +1,512 @@
+package org.apache.flume.source.shaded.guava;","[{'comment': 'some tests are commented out, if they are not needed, I believe they could be removed', 'commenter': 'szaboferee'}, {'comment': 'Fixed', 'commenter': 'tmgstevens'}]"
203,flume-ng-core/src/main/java/org/apache/flume/source/StressSource.java,"@@ -89,6 +91,13 @@ protected void doConfigure(Context context) throws FlumeException {
     /* Size of events to be generated. */
     int size = context.getInteger(""size"", 500);
 
+    int rateLimit = context.getInteger(""maxEventsPerSecond"", 0);","[{'comment': 'Could you please update the documentation with the new parameter?', 'commenter': 'szaboferee'}, {'comment': 'Absolutely, thought I had!!', 'commenter': 'tmgstevens'}]"
203,flume-ng-core/src/main/java/org/apache/flume/source/StressSource.java,"@@ -123,6 +132,9 @@ protected Status doProcess() throws EventDeliveryException {
       lastSent = batchSize;
 
       if (batchSize == 1) {
+        if (limiter != null) {","[{'comment': ""Is it intentional to not having a rate limit with batchSize larger than 1?\r\n`limiter.acquire(batchSize)` would be kind of what we need if I'm right"", 'commenter': 'szaboferee'}, {'comment': ""That is a VERY good spot - thanks @szaboferee for being awake when reviewing!!!!!!\r\n\r\nIt's also slightly trickier than batchSize because it might be the last batch, although perhaps I didn't need to make it quite that complex. Anyhow, done."", 'commenter': 'tmgstevens'}]"
203,flume-ng-core/src/test/java/org/apache/flume/source/TestStressSource.java,"@@ -82,6 +82,49 @@ public void testMaxTotalEvents() throws InterruptedException,
     }
     verify(mockProcessor, times(35)).processEvent(getEvent(source));
   }
+  
+  @Test
+  public void testRateLimitedEvents() throws InterruptedException,","[{'comment': 'This test method covers 2 test cases: rate limited events and non rate limited events.\r\nIt would be more straightforward to separate these cases into 2 test methods (1 test case - 1 test method with appropriate names describing the cases).', 'commenter': 'turcsanyip'}, {'comment': 'Done. In fact, with the batching changes, now broken into four tests', 'commenter': 'tmgstevens'}]"
214,flume-ng-doc/sphinx/FlumeUserGuide.rst,"@@ -1047,6 +1048,11 @@ ignorePattern             ^$              Regular expression specifying which fi
                                           the file is ignored.
 trackerDir                .flumespool     Directory to store metadata related to processing of files.
                                           If this path is not an absolute path, then it is interpreted as relative to the spoolDir.
+trackingPolicy            rename          The tracking policy defines how file processign is tracked. It can be ""rename"" or
+                                          ""tracker_dir"". This parameter is only effective if the deletePolicy is ""never"".
+                                          ""rename"" - After processing files they get renamed according to the fileSuffix parameter.
+                                          ""tracker_dir"" - Files are not renemad but a new empty file is created in the trackerDir.","[{'comment': 'typo: renamed', 'commenter': 'turcsanyip'}]"
214,flume-ng-doc/sphinx/FlumeUserGuide.rst,"@@ -1047,6 +1048,11 @@ ignorePattern             ^$              Regular expression specifying which fi
                                           the file is ignored.
 trackerDir                .flumespool     Directory to store metadata related to processing of files.
                                           If this path is not an absolute path, then it is interpreted as relative to the spoolDir.
+trackingPolicy            rename          The tracking policy defines how file processign is tracked. It can be ""rename"" or","[{'comment': 'type: processing', 'commenter': 'turcsanyip'}, {'comment': 'I need a spell checker', 'commenter': 'majorendre'}]"
214,flume-ng-core/src/test/java/org/apache/flume/client/avro/TestReliableSpoolingFileEventReader.java,"@@ -602,11 +603,40 @@ private void templateTestForLargeNumberOfFiles(ConsumeOrder order, Comparator<Lo
     }
   }
 
+  private void createMultilevelFiles(File dir, int currDepth, int maxDepth, int dirNum, int fileNum,
+                                     Map<Long, List<String>> expected, MutableLong id) throws IOException {
+    if (currDepth == maxDepth) {
+      createFiles(dir, fileNum, expected, id);
+    } else {
+      for (int i = 0; i < dirNum; i++) {
+        File nextDir = new File(dir, ""dir-"" + i);
+        nextDir.mkdirs();
+        createMultilevelFiles(nextDir, currDepth + 1, maxDepth, dirNum, fileNum, expected, id);
+      }
+    }
+  }
+
+  private void createFiles(File dir, int N, Map<Long, List<String>> expected, MutableLong id) throws IOException {","[{'comment': '_fileNum_ would be better instead of _N_', 'commenter': 'turcsanyip'}]"
214,flume-ng-core/src/test/java/org/apache/flume/client/avro/TestReliableSpoolingFileEventReader.java,"@@ -500,22 +486,39 @@ public void testConsumeFileYoungestWithLexicographicalComparision()
     Assert.assertEquals(expected, actual);
   }
 
-  @Test public void testLargeNumberOfFilesOLDEST() throws IOException {    
-    templateTestForLargeNumberOfFiles(ConsumeOrder.OLDEST, null, 1000);
+  @Test public void testLargeNumberOfFilesOLDEST() throws IOException {
+    templateTestForRecursiveDirs(ConsumeOrder.OLDEST, null, 3, 3, 37, TrackingPolicy.RENAME);
   }
 
-  @Test public void testLargeNumberOfFilesYOUNGEST() throws IOException {    
-    templateTestForLargeNumberOfFiles(ConsumeOrder.YOUNGEST, new Comparator<Long>() {
+  @Test public void testLargeNumberOfFilesYOUNGEST() throws IOException {
+    templateTestForRecursiveDirs(ConsumeOrder.YOUNGEST, new Comparator<Long>() {
 
       @Override
       public int compare(Long o1, Long o2) {
         return o2.compareTo(o1);
       }
-    }, 1000);
+    }, 3, 3, 37, TrackingPolicy.RENAME);
+  }
+
+  @Test public void testLargeNumberOfFilesRANDOM() throws IOException {
+    templateTestForRecursiveDirs(ConsumeOrder.RANDOM, null, 3, 3, 37, TrackingPolicy.RENAME);
   }
 
-  @Test public void testLargeNumberOfFilesRANDOM() throws IOException {    
-    templateTestForLargeNumberOfFiles(ConsumeOrder.RANDOM, null, 1000);
+  @Test public void testLargeNumberOfFilesOLDESTTrackerDir() throws IOException {
+    templateTestForRecursiveDirs(ConsumeOrder.OLDEST, null, 3, 3, 10, TrackingPolicy.TRACKER_DIR);
+  }
+
+  @Test public void testLargeNumberOfFilesYOUNGESTTrackerDir() throws IOException {
+    templateTestForRecursiveDirs(ConsumeOrder.YOUNGEST, new Comparator<Long>() {","[{'comment': 'Comparator.reverseOrder() would be more compact (the same for line 494).', 'commenter': 'turcsanyip'}, {'comment': 'This is since: 1.8. It would make back porting difficult.', 'commenter': 'majorendre'}, {'comment': 'Maybe we dont want to backport to java 1.7', 'commenter': 'majorendre'}]"
214,flume-ng-core/src/main/java/org/apache/flume/client/avro/ReliableSpoolingFileEventReader.java,"@@ -269,6 +292,38 @@ public FileVisitResult visitFile(Path candidate, BasicFileAttributes attrs)
     return candidateFiles;
   }
 
+  private Set<Path> getTrackerDirCompletedFiles() throws IOException {
+    final Set<Path> completedFiles = new HashSet<>();
+    if (TrackingPolicy.TRACKER_DIR != trackingPolicy) {
+      return completedFiles;
+    }
+
+    Path trackerDirPath = Paths.get(trackerDirectory.getPath());","[{'comment': 'trackerDirectory.toPath() would be simpler. The same for line 545.', 'commenter': 'turcsanyip'}, {'comment': 'thanks for spotting', 'commenter': 'majorendre'}]"
222,flume-ng-core/src/test/java/org/apache/flume/source/TestAvroSource.java,"@@ -64,9 +67,16 @@
 import org.junit.Assert;
 import org.junit.Before;
 import org.junit.Test;
+import org.mockito.Mockito;
+import org.mockito.internal.util.reflection.Whitebox;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import static org.mockito.Matchers.any;
+import static org.mockito.Matchers.anyListOf;
+import static org.mockito.Mockito.doThrow;
+import static org.mockito.Mockito.when;","[{'comment': 'unused import', 'commenter': 'szaboferee'}]"
229,flume-ng-sinks/flume-hdfs-sink/src/test/java/org/apache/flume/sink/hdfs/TestBucketWriter.java,"@@ -411,6 +411,12 @@ public void testSequenceFileRenameRetries() throws Exception {
     SequenceFileRenameRetryCoreTest(2, false);
   }
 
+  @Test
+  public void testSequenceFileSystemCloseRetries() throws Exception {","[{'comment': ""'System' does not mean anything in this context, please remove it."", 'commenter': 'turcsanyip'}]"
229,flume-ng-sinks/flume-hdfs-sink/src/test/java/org/apache/flume/sink/hdfs/TestBucketWriter.java,"@@ -450,6 +456,39 @@ public void SequenceFileRenameRetryCoreTest(int numberOfRetriesRequired, boolean
                       bucketWriter.renameTries.get() == numberOfRetriesRequired);
   }
 
+  private void SequenceFileCloseRetryCoreTest(int numberOfRetriesRequired)","[{'comment': 'Please use lower case method names.\r\nCould you please correct the original method where it was copied from?', 'commenter': 'turcsanyip'}]"
229,flume-ng-sinks/flume-hdfs-sink/src/main/java/org/apache/flume/sink/hdfs/BucketWriter.java,"@@ -293,7 +287,7 @@ public Void call() throws Exception {
               bucketPath, rollInterval);
           try {
             // Roll the file and remove reference from sfWriters map.
-            close(true);
+            close(true, false);","[{'comment': 'the second parameter can be removed', 'commenter': 'szaboferee'}]"
229,flume-ng-sinks/flume-hdfs-sink/src/main/java/org/apache/flume/sink/hdfs/BucketWriter.java,"@@ -313,51 +307,87 @@ public Void call() throws Exception {
    * method will not cause the bucket writer to be dereferenced from the HDFS
    * sink that owns it. This method should be used only when size or count
    * based rolling closes this file.
-   * @throws IOException On failure to rename if temp file exists.
-   * @throws InterruptedException
    */
-  public void close() throws IOException, InterruptedException {
-    close(false);
+  public void close() throws InterruptedException {
+    close(false, false);","[{'comment': 'the second parameter can be removed\r\n\r\n', 'commenter': 'szaboferee'}]"
229,flume-ng-sinks/flume-hdfs-sink/src/main/java/org/apache/flume/sink/hdfs/BucketWriter.java,"@@ -313,51 +307,87 @@ public Void call() throws Exception {
    * method will not cause the bucket writer to be dereferenced from the HDFS
    * sink that owns it. This method should be used only when size or count
    * based rolling closes this file.
-   * @throws IOException On failure to rename if temp file exists.
-   * @throws InterruptedException
    */
-  public void close() throws IOException, InterruptedException {
-    close(false);
+  public void close() throws InterruptedException {
+    close(false, false);
   }
 
   private CallRunner<Void> createCloseCallRunner() {
     return new CallRunner<Void>() {
-      private final HDFSWriter localWriter = writer;
       @Override
       public Void call() throws Exception {
-        localWriter.close(); // could block
+        writer.close(); // could block
         return null;
       }
     };
   }
 
-  private Callable<Void> createScheduledRenameCallable() {
+  private class CloseCallable implements Callable<Void> {
+    private final String path = bucketPath;
+    private int closeTries = 0;
 
-    return new Callable<Void>() {
-      private final String path = bucketPath;
-      private final String finalPath = targetPath;
-      private FileSystem fs = fileSystem;
-      private int renameTries = 1; // one attempt is already done
+    @Override
+    public Void call() throws Exception {
+      close(false);
+      return null;
+    }
 
-      @Override
-      public Void call() throws Exception {
-        if (renameTries >= maxRenameTries) {
-          LOG.warn(""Unsuccessfully attempted to rename "" + path + "" "" +
-              maxRenameTries + "" times. File may still be open."");
-          return null;
+    /**
+     * Tries to close the writer. Repeats the close if the maximum number
+     * of retries is not reached or an immediate close is not reuqested.
+     * If all close attempts were unsuccessful we try to recover the lease.
+     * @param immediate An immediate close is required
+     */
+    public void close(boolean immediate) {
+      closeTries++;
+      boolean shouldRetry = closeTries < maxRetries || !immediate;","[{'comment': 'if immediate is false, souldRetry is always true so it will retry endlessly.', 'commenter': 'szaboferee'}, {'comment': 'Thanks a lot for spotting this. Updated the unit test to discover such errors.', 'commenter': 'majorendre'}]"
229,flume-ng-sinks/flume-hdfs-sink/src/test/java/org/apache/flume/sink/hdfs/TestBucketWriter.java,"@@ -447,6 +453,39 @@ public void SequenceFileRenameRetryCoreTest(int numberOfRetriesRequired, boolean
                       bucketWriter.renameTries.get() == numberOfRetriesRequired);
   }
 
+  private void sequenceFileCloseRetryCoreTest(int numberOfRetriesRequired)
+      throws Exception {
+    String hdfsPath = ""file:///tmp/flume-test."" +
+        Calendar.getInstance().getTimeInMillis() +
+        ""."" + Thread.currentThread().getId();
+
+    Context context = new Context();
+    Configuration conf = new Configuration();
+    FileSystem fs = FileSystem.get(conf);
+    Path dirPath = new Path(hdfsPath);
+    fs.delete(dirPath, true);
+    fs.mkdirs(dirPath);
+    context.put(""hdfs.path"", hdfsPath);
+    context.put(""hdfs.closeTries"", String.valueOf(numberOfRetriesRequired));
+    context.put(""hdfs.rollCount"", ""1"");
+    context.put(""hdfs.retryInterval"", ""1"");
+    context.put(""hdfs.callTimeout"", Long.toString(1000));
+    MockHDFSWriter mockHDFSWriter = new MockHDFSWriter(numberOfRetriesRequired);
+    BucketWriter bucketWriter = new BucketWriter(
+        0, 0, 1, 1, ctx, hdfsPath, hdfsPath, ""singleBucket"", "".tmp"", null, null,
+        null, mockHDFSWriter, timedRollerPool, proxy,
+        new SinkCounter(""test-bucket-writer-"" + System.currentTimeMillis()), 0, null, null, 30000,
+        Executors.newSingleThreadExecutor(), 1, numberOfRetriesRequired);
+
+    Event event = EventBuilder.withBody(""test"", Charsets.UTF_8);
+    bucketWriter.append(event);
+    bucketWriter.close(false);
+    TimeUnit.SECONDS.sleep(numberOfRetriesRequired + 2);
+    Assert.assertEquals(""Expected "" + numberOfRetriesRequired + "" "" +
+        ""but got "" + mockHDFSWriter.currentCloseAttempts,
+        mockHDFSWriter.currentCloseAttempts.get(), numberOfRetriesRequired);
+  }
+","[{'comment': 'Could you modify the test to detect the endless retry attempts?', 'commenter': 'szaboferee'}, {'comment': '@szaboferee Thanks for your findings.', 'commenter': 'majorendre'}]"
230,flume-ng-core/src/main/java/org/apache/flume/source/SslContextAwareAbstractSource.java,"@@ -0,0 +1,200 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with this
+ * work for additional information regarding copyright ownership. The ASF
+ * licenses this file to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+ * License for the specific language governing permissions and limitations under
+ * the License.
+ */
+
+package org.apache.flume.source;
+
+import java.io.FileInputStream;
+import java.security.KeyStore;
+import java.util.Arrays;
+import java.util.LinkedHashSet;
+import java.util.Objects;
+import java.util.Optional;
+import java.util.Set;
+import java.util.function.Supplier;
+import java.util.stream.Stream;
+
+import javax.net.ssl.KeyManagerFactory;
+import javax.net.ssl.SSLContext;
+import javax.net.ssl.SSLEngine;
+import javax.net.ssl.SSLParameters;
+
+import org.apache.flume.Context;
+import org.apache.flume.FlumeException;
+import org.apache.flume.util.SSLUtil;
+
+public abstract class SslContextAwareAbstractSource extends AbstractSource {
+  private static final String SSL_ENABLED_KEY = ""ssl"";
+  private static final boolean SSL_ENABLED_DEFAULT_VALUE = false;
+  private static final String KEYSTORE_KEY = ""keystore"";
+  private static final String KEYSTORE_PASSWORD_KEY = ""keystore-password"";
+  private static final String KEYSTORE_TYPE_KEY = ""keystore-type"";
+  private static final String KEYSTORE_TYPE_DEFAULT_VALUE = ""JKS"";
+
+  private static final String EXCLUDE_PROTOCOLS = ""exclude-protocols"";
+  private static final String INCLUDE_PROTOCOLS = ""include-protocols"";
+
+  private static final String EXCLUDE_CIPHER_SUITES = ""exclude-cipher-suites"";
+  private static final String INCLUDE_CIPHER_SUITES = ""include-cipher-suites"";
+
+  private String keystore;
+  private String keystorePassword;
+  private String keystoreType;
+  private boolean sslEnabled = false;
+  private final Set<String> excludeProtocols = new LinkedHashSet<>(Arrays.asList(""SSLv3""));
+  private final Set<String> includeProtocols = new LinkedHashSet<>();
+  private final Set<String> excludeCipherSuites = new LinkedHashSet<>();
+  private final Set<String> includeCipherSuites = new LinkedHashSet<>();
+
+
+  public String getKeystore() {
+    return keystore;
+  }
+
+  public String getKeystorePassword() {
+    return keystorePassword;
+  }
+
+  public String getKeystoreType() {
+    return keystoreType;
+  }
+
+  public Set<String> getExcludeProtocols() {
+    return excludeProtocols;
+  }
+
+  public Set<String> getIncludeProtocols() {
+    return includeProtocols;
+  }
+
+  public Set<String> getExcludeCipherSuites() {
+    return excludeCipherSuites;
+  }
+
+  public Set<String> getIncludeCipherSuites() {
+    return includeCipherSuites;
+  }
+
+  public boolean isSslEnabled() {
+    return sslEnabled;
+  }
+
+  protected void configureSsl(Context context) {
+    sslEnabled = context.getBoolean(SSL_ENABLED_KEY, SSL_ENABLED_DEFAULT_VALUE);
+    keystore = context.getString(KEYSTORE_KEY, SSLUtil.getGlobalKeystorePath());
+    keystorePassword = context.getString(
+        KEYSTORE_PASSWORD_KEY, SSLUtil.getGlobalKeystorePassword());
+    keystoreType = context.getString(
+        KEYSTORE_TYPE_KEY, SSLUtil.getGlobalKeystoreType(KEYSTORE_TYPE_DEFAULT_VALUE));
+
+    Optional.ofNullable(context.getString(EXCLUDE_PROTOCOLS)).ifPresent(
+        s -> excludeProtocols.addAll(Arrays.asList(s.split("" ""))));
+    Optional.ofNullable(context.getString(INCLUDE_PROTOCOLS)).ifPresent(
+        s -> includeProtocols.addAll(Arrays.asList(s.split("" ""))));
+
+    Optional.ofNullable(context.getString(EXCLUDE_CIPHER_SUITES)).ifPresent(
+        s -> excludeCipherSuites.addAll(Arrays.asList(s.split("" ""))));
+    Optional.ofNullable(context.getString(INCLUDE_CIPHER_SUITES)).ifPresent(
+        s -> includeCipherSuites.addAll(Arrays.asList(s.split("" ""))));
+
+    if (sslEnabled) {
+      Objects.requireNonNull(keystore,
+          KEYSTORE_KEY + "" must be specified when SSL is enabled"");
+      Objects.requireNonNull(keystorePassword,
+          KEYSTORE_PASSWORD_KEY + "" must be specified when SSL is enabled"");
+      try {
+        KeyStore ks = KeyStore.getInstance(keystoreType);
+        ks.load(new FileInputStream(keystore), keystorePassword.toCharArray());
+      } catch (Exception ex) {
+        throw new FlumeException(
+          ""Source "" + getName() + "" configured with invalid keystore: "" + keystore, ex);
+      }
+    }
+  }
+
+  private Optional<SSLContext> getSslContext() {
+    if (sslEnabled) {
+      try {
+        KeyStore ks = KeyStore.getInstance(keystoreType);
+        ks.load(new FileInputStream(keystore), keystorePassword.toCharArray());
+
+        // can be set with ""ssl.KeyManagerFactory.algorithm""
+        String algorithm = KeyManagerFactory.getDefaultAlgorithm();
+        // Set up key manager factory to use our key store
+        KeyManagerFactory kmf = KeyManagerFactory.getInstance(algorithm);
+        kmf.init(ks, keystorePassword.toCharArray());
+
+        SSLContext serverContext = SSLContext.getInstance(""TLS"");
+        serverContext.init(kmf.getKeyManagers(), null, null);
+
+        return Optional.of(serverContext);
+      } catch (Exception e) {
+        throw new Error(""Failed to initialize the server-side SSLContext"", e);
+      }
+    } else {
+      return Optional.empty();
+    }
+  }
+
+  private Optional<SSLEngine> getSslEngine(boolean useClientMode) {
+    return getSslContext().map(sslContext -> {
+      SSLEngine sslEngine = sslContext.createSSLEngine();
+      sslEngine.setUseClientMode(useClientMode);
+      sslEngine.setEnabledProtocols(
+          getEnabledProtocols(sslEngine.getEnabledProtocols()));
+      sslEngine.setEnabledCipherSuites(
+          getEnabledCipherSuites(sslEngine.getEnabledCipherSuites()));
+      return sslEngine;
+    });
+  }
+
+  protected Supplier<Optional<SSLContext>> getSslContextSupplier() {
+    return this::getSslContext;
+  }
+
+
+  protected Supplier<Optional<SSLEngine>> getSslEngineSupplier(boolean useClientMode) {
+    return () -> getSslEngine(useClientMode);
+  }
+
+  protected SSLParameters getSslParametersForContext(SSLContext sslContext) {
+    SSLParameters sslParameters = sslContext.getDefaultSSLParameters();
+
+    return getSSLParametersWithEnabledProtocolsAndCipherSuites(sslParameters);
+  }
+
+  protected SSLParameters getSSLParametersWithEnabledProtocolsAndCipherSuites(
+      SSLParameters sslParameters) {
+    return new SSLParameters(
+        getEnabledCipherSuites(sslParameters.getCipherSuites()),
+        getEnabledProtocols(sslParameters.getProtocols())
+    );
+  }
+
+  private String[] getEnabledProtocols(String[] enabledProtocols) {","[{'comment': ""The method name is a bit misleading because the input parameter is also 'enabled protocols'.\r\nI would rename it to getFilteredProtocols (+ getFilteredCipherSuites)."", 'commenter': 'turcsanyip'}]"
230,flume-ng-core/src/main/java/org/apache/flume/source/SslContextAwareAbstractSource.java,"@@ -0,0 +1,200 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with this
+ * work for additional information regarding copyright ownership. The ASF
+ * licenses this file to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+ * License for the specific language governing permissions and limitations under
+ * the License.
+ */
+
+package org.apache.flume.source;
+
+import java.io.FileInputStream;
+import java.security.KeyStore;
+import java.util.Arrays;
+import java.util.LinkedHashSet;
+import java.util.Objects;
+import java.util.Optional;
+import java.util.Set;
+import java.util.function.Supplier;
+import java.util.stream.Stream;
+
+import javax.net.ssl.KeyManagerFactory;
+import javax.net.ssl.SSLContext;
+import javax.net.ssl.SSLEngine;
+import javax.net.ssl.SSLParameters;
+
+import org.apache.flume.Context;
+import org.apache.flume.FlumeException;
+import org.apache.flume.util.SSLUtil;
+
+public abstract class SslContextAwareAbstractSource extends AbstractSource {
+  private static final String SSL_ENABLED_KEY = ""ssl"";
+  private static final boolean SSL_ENABLED_DEFAULT_VALUE = false;
+  private static final String KEYSTORE_KEY = ""keystore"";
+  private static final String KEYSTORE_PASSWORD_KEY = ""keystore-password"";
+  private static final String KEYSTORE_TYPE_KEY = ""keystore-type"";
+  private static final String KEYSTORE_TYPE_DEFAULT_VALUE = ""JKS"";
+
+  private static final String EXCLUDE_PROTOCOLS = ""exclude-protocols"";
+  private static final String INCLUDE_PROTOCOLS = ""include-protocols"";
+
+  private static final String EXCLUDE_CIPHER_SUITES = ""exclude-cipher-suites"";
+  private static final String INCLUDE_CIPHER_SUITES = ""include-cipher-suites"";
+
+  private String keystore;
+  private String keystorePassword;
+  private String keystoreType;
+  private boolean sslEnabled = false;
+  private final Set<String> excludeProtocols = new LinkedHashSet<>(Arrays.asList(""SSLv3""));
+  private final Set<String> includeProtocols = new LinkedHashSet<>();
+  private final Set<String> excludeCipherSuites = new LinkedHashSet<>();
+  private final Set<String> includeCipherSuites = new LinkedHashSet<>();
+
+
+  public String getKeystore() {
+    return keystore;
+  }
+
+  public String getKeystorePassword() {
+    return keystorePassword;
+  }
+
+  public String getKeystoreType() {
+    return keystoreType;
+  }
+
+  public Set<String> getExcludeProtocols() {
+    return excludeProtocols;
+  }
+
+  public Set<String> getIncludeProtocols() {
+    return includeProtocols;
+  }
+
+  public Set<String> getExcludeCipherSuites() {
+    return excludeCipherSuites;
+  }
+
+  public Set<String> getIncludeCipherSuites() {
+    return includeCipherSuites;
+  }
+
+  public boolean isSslEnabled() {
+    return sslEnabled;
+  }
+
+  protected void configureSsl(Context context) {
+    sslEnabled = context.getBoolean(SSL_ENABLED_KEY, SSL_ENABLED_DEFAULT_VALUE);
+    keystore = context.getString(KEYSTORE_KEY, SSLUtil.getGlobalKeystorePath());
+    keystorePassword = context.getString(
+        KEYSTORE_PASSWORD_KEY, SSLUtil.getGlobalKeystorePassword());
+    keystoreType = context.getString(
+        KEYSTORE_TYPE_KEY, SSLUtil.getGlobalKeystoreType(KEYSTORE_TYPE_DEFAULT_VALUE));
+
+    Optional.ofNullable(context.getString(EXCLUDE_PROTOCOLS)).ifPresent(
+        s -> excludeProtocols.addAll(Arrays.asList(s.split("" ""))));
+    Optional.ofNullable(context.getString(INCLUDE_PROTOCOLS)).ifPresent(
+        s -> includeProtocols.addAll(Arrays.asList(s.split("" ""))));
+
+    Optional.ofNullable(context.getString(EXCLUDE_CIPHER_SUITES)).ifPresent(
+        s -> excludeCipherSuites.addAll(Arrays.asList(s.split("" ""))));
+    Optional.ofNullable(context.getString(INCLUDE_CIPHER_SUITES)).ifPresent(
+        s -> includeCipherSuites.addAll(Arrays.asList(s.split("" ""))));
+
+    if (sslEnabled) {
+      Objects.requireNonNull(keystore,
+          KEYSTORE_KEY + "" must be specified when SSL is enabled"");
+      Objects.requireNonNull(keystorePassword,
+          KEYSTORE_PASSWORD_KEY + "" must be specified when SSL is enabled"");
+      try {
+        KeyStore ks = KeyStore.getInstance(keystoreType);
+        ks.load(new FileInputStream(keystore), keystorePassword.toCharArray());
+      } catch (Exception ex) {
+        throw new FlumeException(
+          ""Source "" + getName() + "" configured with invalid keystore: "" + keystore, ex);
+      }
+    }
+  }
+
+  private Optional<SSLContext> getSslContext() {
+    if (sslEnabled) {
+      try {
+        KeyStore ks = KeyStore.getInstance(keystoreType);
+        ks.load(new FileInputStream(keystore), keystorePassword.toCharArray());
+
+        // can be set with ""ssl.KeyManagerFactory.algorithm""
+        String algorithm = KeyManagerFactory.getDefaultAlgorithm();
+        // Set up key manager factory to use our key store
+        KeyManagerFactory kmf = KeyManagerFactory.getInstance(algorithm);
+        kmf.init(ks, keystorePassword.toCharArray());
+
+        SSLContext serverContext = SSLContext.getInstance(""TLS"");
+        serverContext.init(kmf.getKeyManagers(), null, null);
+
+        return Optional.of(serverContext);
+      } catch (Exception e) {
+        throw new Error(""Failed to initialize the server-side SSLContext"", e);
+      }
+    } else {
+      return Optional.empty();
+    }
+  }
+
+  private Optional<SSLEngine> getSslEngine(boolean useClientMode) {
+    return getSslContext().map(sslContext -> {
+      SSLEngine sslEngine = sslContext.createSSLEngine();
+      sslEngine.setUseClientMode(useClientMode);
+      sslEngine.setEnabledProtocols(
+          getEnabledProtocols(sslEngine.getEnabledProtocols()));
+      sslEngine.setEnabledCipherSuites(
+          getEnabledCipherSuites(sslEngine.getEnabledCipherSuites()));
+      return sslEngine;
+    });
+  }
+
+  protected Supplier<Optional<SSLContext>> getSslContextSupplier() {
+    return this::getSslContext;
+  }
+
+
+  protected Supplier<Optional<SSLEngine>> getSslEngineSupplier(boolean useClientMode) {
+    return () -> getSslEngine(useClientMode);
+  }
+
+  protected SSLParameters getSslParametersForContext(SSLContext sslContext) {
+    SSLParameters sslParameters = sslContext.getDefaultSSLParameters();
+
+    return getSSLParametersWithEnabledProtocolsAndCipherSuites(sslParameters);
+  }
+
+  protected SSLParameters getSSLParametersWithEnabledProtocolsAndCipherSuites(","[{'comment': 'The method name is a bit complicated. On the other hand, the name of the previous method does not describe exactly what it does (following the same logic that should be getSSLParametersWithEnabledProtocolsAndCipherSuitesForContext).\r\nSo it is hard to follow what these methods do.\r\n\r\nI would simply implement the overloaded version of getEnabledProtocols / getEnabledCipherSuites with SSLParameters input parameter and extract the filtered lists from that object.', 'commenter': 'turcsanyip'}]"
230,flume-ng-sdk/src/main/java/org/apache/flume/api/NettyAvroRpcClient.java,"@@ -96,7 +99,10 @@
   private String truststore;
   private String truststorePassword;
   private String truststoreType;
-  private final List<String> excludeProtocols = new LinkedList<String>();
+  private final Set<String> excludeProtocols = new LinkedHashSet<>(Arrays.asList(""SSLv3""));","[{'comment': 'I think these fields and also some logic (eg. parseList()) could moved to AbstractRPCClient.', 'commenter': 'turcsanyip'}]"
230,flume-ng-core/src/test/java/org/apache/flume/source/TestMultiportSyslogTCPSource.java,"@@ -155,6 +163,103 @@ private static final int getFreePort() throws IOException {
     return portList;
   }
 
+  private List<Integer> testNPortsSSL(MultiportSyslogTCPSource source, Channel channel,","[{'comment': ""This and testNPorts methods have a lot of common code. As far as I see, testNPortsSSL contains the same code as  testNPorts with some additional logic.\r\nWouldn't it be possible to consolidate this code duplication?"", 'commenter': 'turcsanyip'}]"
230,flume-ng-core/src/main/java/org/apache/flume/source/AvroSource.java,"@@ -455,52 +411,19 @@ private PatternRule generateRule(String patternRuleDefinition) throws FlumeExcep
       implements ChannelPipelineFactory {
 
     private boolean enableCompression;
-    private boolean enableSsl;
-    private String keystore;
-    private String keystorePassword;
-    private String keystoreType;
 
     private boolean enableIpFilter;
     private String patternRuleConfigDefinition;
+    private Supplier<Optional<SSLEngine>> sslEngineSupplier;
 
-    public AdvancedChannelPipelineFactory(boolean enableCompression,
-        boolean enableSsl, String keystore, String keystorePassword,
-        String keystoreType, boolean enableIpFilter,
-        String patternRuleConfigDefinition) {
+    public AdvancedChannelPipelineFactory(boolean enableCompression, boolean enableIpFilter,
+        String patternRuleConfigDefinition, Supplier<Optional<SSLEngine>> sslEngineProvider) {
       this.enableCompression = enableCompression;
-      this.enableSsl = enableSsl;
-      this.keystore = keystore;
-      this.keystorePassword = keystorePassword;
-      this.keystoreType = keystoreType;
       this.enableIpFilter = enableIpFilter;
       this.patternRuleConfigDefinition = patternRuleConfigDefinition;
+      this.sslEngineSupplier = sslEngineProvider;","[{'comment': 'sslEngineProvider => sslEngineSupplier', 'commenter': 'turcsanyip'}]"
276,flume-ng-channels/flume-kafka-channel/src/main/java/org/apache/flume/channel/kafka/KafkaChannel.java,"@@ -347,13 +351,16 @@ private void migrateOffsets() {
 
   private Map<TopicPartition, OffsetAndMetadata> getKafkaOffsets(
       KafkaConsumer<String, byte[]> client) {
-    Map<TopicPartition, OffsetAndMetadata> offsets = new HashMap<>();
+    Map<TopicPartition, OffsetAndMetadata> offsets = null;","[{'comment': 'This will cause a NullPointerException on line 345.', 'commenter': 'szaboferee'}, {'comment': ""It could only happen if the topic gets deleted while the offset migration is running.\r\nJust for sure, I've added the null checks."", 'commenter': 'turcsanyip'}]"
276,flume-ng-sources/flume-kafka-source/src/main/java/org/apache/flume/source/kafka/KafkaSource.java,"@@ -597,13 +601,16 @@ private void migrateOffsets(String topicStr) {
 
   private Map<TopicPartition, OffsetAndMetadata> getKafkaOffsets(
       KafkaConsumer<String, byte[]> client, String topicStr) {
-    Map<TopicPartition, OffsetAndMetadata> offsets = new HashMap<>();
+    Map<TopicPartition, OffsetAndMetadata> offsets = null;","[{'comment': 'This will cause a NullPointerException on line 596.\r\n\r\n', 'commenter': 'szaboferee'}]"
310,flume-ng-core/src/main/java/org/apache/flume/SinkRunner.java,"@@ -166,6 +166,13 @@ public void run() {
           } catch (InterruptedException ex) {
             Thread.currentThread().interrupt();
           }
+        } catch (Throwable th){","[{'comment': 'In general, catching `Error` is to be avoided: https://docs.oracle.com/javase/8/docs/api/java/lang/Error.html . Catching the superclass `Throwable` falls under the same category.\r\n\r\nCan you think of any alternative ways of improvement?', 'commenter': 'bessbd'}, {'comment': ""@lightzhao, OutOfMemoryError means Flume has run out of memory. Can you increase the amount of memory you give to Flume instead?\r\n\r\nYou also may want to consider more carefully tuning the byteCapacity of your memory channel. See https://flume.apache.org/FlumeUserGuide.html#memory-channel\r\n\r\nEither way, I don't think this patch is a good way to work around your OOME problem.\r\n\r\nEdit: For some reason I thought you were using the MemoryChannel. Looking again, I guess not. Can you simply give Flume more memory? It seems you are running too close to the memory limit of what you give the JVM via -Xmx."", 'commenter': 'mpercy'}]"
371,flume-ng-sinks/flume-ng-kafka-sink/src/test/java/org/apache/flume/sink/kafka/util/TestUtil.java,"@@ -39,6 +39,9 @@ Licensed to the Apache Software Foundation (ASF) under one or more
 import java.util.Properties;
 import java.util.concurrent.TimeUnit;
 
+import static org.apache.kafka.common.config.SslConfigs.*;","[{'comment': 'Please avoid * imports', 'commenter': 'tmgstevens'}]"
371,flume-ng-sources/flume-kafka-source/src/test/java/org/apache/flume/source/kafka/TestKafkaSource.java,"@@ -74,20 +74,8 @@
 import java.util.Properties;
 import java.util.regex.Pattern;
 
-import static org.apache.flume.source.kafka.KafkaSourceConstants.AVRO_EVENT;
-import static org.apache.flume.source.kafka.KafkaSourceConstants.BATCH_DURATION_MS;
-import static org.apache.flume.source.kafka.KafkaSourceConstants.BATCH_SIZE;
-import static org.apache.flume.source.kafka.KafkaSourceConstants.BOOTSTRAP_SERVERS;
-import static org.apache.flume.source.kafka.KafkaSourceConstants.DEFAULT_AUTO_COMMIT;
-import static org.apache.flume.source.kafka.KafkaSourceConstants.KAFKA_CONSUMER_PREFIX;
-import static org.apache.flume.source.kafka.KafkaSourceConstants.OLD_GROUP_ID;
-import static org.apache.flume.source.kafka.KafkaSourceConstants.PARTITION_HEADER;
-import static org.apache.flume.source.kafka.KafkaSourceConstants.TIMESTAMP_HEADER;
-import static org.apache.flume.source.kafka.KafkaSourceConstants.TOPIC;
-import static org.apache.flume.source.kafka.KafkaSourceConstants.TOPICS;
-import static org.apache.flume.source.kafka.KafkaSourceConstants.TOPICS_REGEX;
-import static org.apache.flume.source.kafka.KafkaSourceConstants.DEFAULT_TOPIC_HEADER;
-import static org.apache.flume.source.kafka.KafkaSourceConstants.ZOOKEEPER_CONNECT_FLUME_KEY;
+import static org.apache.flume.source.kafka.KafkaSourceConstants.*;","[{'comment': 'Please revert this change. Our coding guidelines avoid importing .*', 'commenter': 'tmgstevens'}]"
