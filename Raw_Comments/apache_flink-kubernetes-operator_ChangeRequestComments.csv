Pull,Path,Diff_hunk,Comment
1,examples/basic-checkpoint-ha.yaml,"@@ -0,0 +1,61 @@
+################################################################################
+#  Licensed to the Apache Software Foundation (ASF) under one
+#  or more contributor license agreements.  See the NOTICE file
+#  distributed with this work for additional information
+#  regarding copyright ownership.  The ASF licenses this file
+#  to you under the Apache License, Version 2.0 (the
+#  ""License""); you may not use this file except in compliance
+#  with the License.  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an ""AS IS"" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+# limitations under the License.
+################################################################################
+
+apiVersion: flink.apache.org/v1alpha1
+kind: FlinkDeployment
+metadata:
+  namespace: default
+  name: basic-checkpoint-ha-example
+spec:
+  image: flink:1.14.3
+  flinkVersion: 1.14.3
+  flinkConfiguration:
+    taskmanager.numberOfTaskSlots: ""2""","[{'comment': 'Task Manager already has slot configuration. Is Flink Configuration redundant? Or can we remove the slot configuration in Taskmanager?', 'commenter': 'Grypse'}, {'comment': 'Good catch, the config is definitely redundant here in the yamls and I will clean it up.\r\n\r\n', 'commenter': 'gyfora'}]"
1,README.md,"@@ -1,2 +1,69 @@
 # flink-kubernetes-operator
-Apache Flink Kubernetes Operator
+Temporary repository for Flink Kubernetes Operator. The content will be moved to OSS repo once created an IPR. Check [FLIP-212](https://cwiki.apache.org/confluence/display/FLINK/FLIP-212%3A+Introduce+Flink+Kubernetes+Operator) further info.
+
+## Installation
+
+The operator is managed helm chart. To install run:
+```
+ cd helm/flink-operator
+ helm install flink-operator .
+```
+
+In order to use the webhook for FlinkDeployment validation, you must install the cert-manager on the Kubernetes cluster:
+```
+kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.7.1/cert-manager.yaml
+```
+
+The webhook can be disabled during helm install by passing the `--set webhook.create=false` parameter or editing the `values.yaml` directly.
+
+## User Guide
+### Create a new Flink deployment
+The flink-operator will watch the CRD resources and submit a new Flink deployment once the CR it applied.","[{'comment': '`... is applied` ?', 'commenter': 'martin-g'}]"
1,README.md,"@@ -1,2 +1,69 @@
 # flink-kubernetes-operator
-Apache Flink Kubernetes Operator
+Temporary repository for Flink Kubernetes Operator. The content will be moved to OSS repo once created an IPR. Check [FLIP-212](https://cwiki.apache.org/confluence/display/FLINK/FLIP-212%3A+Introduce+Flink+Kubernetes+Operator) further info.
+
+## Installation
+
+The operator is managed helm chart. To install run:
+```
+ cd helm/flink-operator
+ helm install flink-operator .
+```
+
+In order to use the webhook for FlinkDeployment validation, you must install the cert-manager on the Kubernetes cluster:
+```
+kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.7.1/cert-manager.yaml
+```
+
+The webhook can be disabled during helm install by passing the `--set webhook.create=false` parameter or editing the `values.yaml` directly.
+
+## User Guide
+### Create a new Flink deployment
+The flink-operator will watch the CRD resources and submit a new Flink deployment once the CR it applied.
+```
+kubectl create -f examples/basic.yaml
+```
+
+### Delete a Flink deployment
+```
+kubectl delete -f create/basic.yaml
+
+OR
+
+kubectl delete flinkdep {dep_name}
+```
+
+### Get/List Flink deployments
+Get all the Flink deployments running in the K8s cluster
+```
+kubectl get flinkdep
+```
+Describe a specific Flink deployment to show the status(including job status, savepoint, ect.)","[{'comment': 's/ect/etc/', 'commenter': 'martin-g'}]"
1,README.md,"@@ -1,2 +1,69 @@
 # flink-kubernetes-operator
-Apache Flink Kubernetes Operator
+Temporary repository for Flink Kubernetes Operator. The content will be moved to OSS repo once created an IPR. Check [FLIP-212](https://cwiki.apache.org/confluence/display/FLINK/FLIP-212%3A+Introduce+Flink+Kubernetes+Operator) further info.
+
+## Installation
+
+The operator is managed helm chart. To install run:
+```
+ cd helm/flink-operator
+ helm install flink-operator .
+```
+
+In order to use the webhook for FlinkDeployment validation, you must install the cert-manager on the Kubernetes cluster:
+```
+kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.7.1/cert-manager.yaml
+```
+
+The webhook can be disabled during helm install by passing the `--set webhook.create=false` parameter or editing the `values.yaml` directly.
+
+## User Guide
+### Create a new Flink deployment
+The flink-operator will watch the CRD resources and submit a new Flink deployment once the CR it applied.
+```
+kubectl create -f examples/basic.yaml
+```
+
+### Delete a Flink deployment
+```
+kubectl delete -f create/basic.yaml
+
+OR
+
+kubectl delete flinkdep {dep_name}
+```
+
+### Get/List Flink deployments
+Get all the Flink deployments running in the K8s cluster
+```
+kubectl get flinkdep
+```
+Describe a specific Flink deployment to show the status(including job status, savepoint, ect.)
+```
+kubectl describe flinkdep {dep_name}
+```
+## Developer Guide
+
+### Building docker images
+```
+docker build . -t <repo>/flink-java-operator:latest
+docker push <repo>flink-java-operator:latest
+helm install flink-operator . --set image.repository=<repo> --set image.tag=latest
+```
+### Running the operator locally
+You can run or debug the `FlinkOperator` from your preferred IDE. The operator itself is accessing the deployed Flink clusters through the REST interface. When running locally the `rest.port` and `rest.address` Flink configuration parameters must be modified to a locally accessible value.
+
+When using `minikube tunnel` the rest service is exposed on `localhost:8081`
+```
+> minikube tunnel
+
+> kubectl get services
+NAME                         TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE
+basic-session-example        ClusterIP      None           <none>        6123/TCP,6124/TCP   14h
+basic-session-example-rest   LoadBalancer   10.96.36.250   127.0.0.1     8081:30572/TCP      14h
+```
+The operator pics up the default log and flink configurations from `/opt/flink/conf`. You can put the rest configuration parameters here:","[{'comment': 's/pics/picks/', 'commenter': 'martin-g'}]"
1,pom.xml,"@@ -0,0 +1,286 @@
+<?xml version=""1.0"" encoding=""UTF-8""?>
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+  http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+<project xmlns=""http://maven.apache.org/POM/4.0.0""
+         xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
+         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
+
+    <parent>
+      <groupId>org.apache</groupId>
+      <artifactId>apache</artifactId>
+      <version>20</version>
+    </parent>
+
+    <modelVersion>4.0.0</modelVersion>
+
+    <groupId>org.apache.flink</groupId>
+    <artifactId>flink-kubernetes-operator-parent</artifactId>
+    <version>1.0-SNAPSHOT</version>
+
+    <name>Flink Kubernetes: </name>
+    <packaging>pom</packaging>
+    <url>https://flink.apache.org</url>
+    <inceptionYear>2014</inceptionYear>
+
+    <licenses>
+      <license>
+        <name>The Apache Software License, Version 2.0</name>
+        <url>https://www.apache.org/licenses/LICENSE-2.0.txt</url>
+        <distribution>repo</distribution>
+      </license>
+    </licenses>
+
+    <modules>
+      <module>flink-kubernetes-operator</module>
+      <module>flink-kubernetes-webhook</module>
+    </modules>
+
+    <properties>
+        <maven.compiler.source>1.8</maven.compiler.source>
+        <maven.compiler.target>1.8</maven.compiler.target>
+        <maven-assembly-plugin.version>3.3.0</maven-assembly-plugin.version>
+        <maven-surefire-plugin.version>3.0.0-M4</maven-surefire-plugin.version>
+        <maven-failsafe-plugin.version>3.0.0-M4</maven-failsafe-plugin.version>
+        <maven-resources-plugin.version>3.2.0</maven-resources-plugin.version>
+
+        <operator.sdk.version>2.0.1</operator.sdk.version>
+        <fabric8.version>5.12.1</fabric8.version>
+        <lombok.version>1.18.22</lombok.version>
+
+        <scala.version>2.12</scala.version>
+        <flink.version>1.14.3</flink.version>
+        <flink.shaded.version>15.0</flink.shaded.version>
+
+        <slf4j.version>1.7.15</slf4j.version>
+        <log4j.version>2.17.1</log4j.version>
+
+        <spotless.version>2.4.2</spotless.version>
+        <awaitility.version>4.1.0</awaitility.version>
+        <it.skip>true</it.skip>
+        <mockito.version>2.21.0</mockito.version>","[{'comment': 'Why such old Mockito version ?', 'commenter': 'martin-g'}, {'comment': ""Not sure but let's upgrade it, I agree :) "", 'commenter': 'gyfora'}]"
1,pom.xml,"@@ -0,0 +1,285 @@
+<?xml version=""1.0"" encoding=""UTF-8""?>
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+  http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+<project xmlns=""http://maven.apache.org/POM/4.0.0""
+         xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
+         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
+
+    <parent>
+      <groupId>org.apache</groupId>
+      <artifactId>apache</artifactId>
+      <version>20</version>
+    </parent>","[{'comment': 'Should we use `flink-parent` here so we can inherit the same quality plugins etc?', 'commenter': 'dannycranmer'}, {'comment': 'Does that mean that plugins, profiles etc defined in flink-parent are autmatically enabled here also? If that is the case that might make our life more difficult in the long run\r\n\r\nLooking at the statefun project that also does not use the flink parent: https://github.com/apache/flink-statefun/blob/master/pom.xml\r\n\r\n', 'commenter': 'gyfora'}, {'comment': ""We don't have to inherit Flink parent, but we could if there is value. On one hand there may be plugin setups that we may benefit from, on the other hand we may inherit things that we don't need or that conflict. I would propose to analyze that outside of this PR. "", 'commenter': 'tweise'}]"
1,flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/reconciler/JobReconcilerTest.java,"@@ -0,0 +1,100 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.reconciler;
+
+import org.apache.flink.api.common.JobID;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.kubernetes.operator.TestUtils;
+import org.apache.flink.kubernetes.operator.crd.FlinkDeployment;
+import org.apache.flink.kubernetes.operator.crd.spec.UpgradeMode;
+import org.apache.flink.kubernetes.operator.crd.status.JobStatus;
+import org.apache.flink.kubernetes.operator.service.FlinkService;
+import org.apache.flink.kubernetes.operator.utils.FlinkUtils;
+import org.apache.flink.runtime.jobgraph.SavepointConfigOptions;
+
+import io.fabric8.kubernetes.client.KubernetesClient;
+import org.junit.jupiter.api.Test;
+import org.mockito.ArgumentCaptor;
+import org.mockito.Mockito;
+
+import java.util.Optional;
+
+import static org.junit.jupiter.api.Assertions.assertEquals;
+import static org.mockito.ArgumentMatchers.any;
+import static org.mockito.ArgumentMatchers.eq;
+import static org.mockito.Mockito.times;
+
+/** @link JobStatusObserver unit tests */
+public class JobReconcilerTest {
+
+    public static final String JOB_NAME = ""test1"";
+    public static final String JOB_ID = ""fd72014d4c864993a2e5a9287b4a9c5d"";
+
+    private FlinkService flinkService = Mockito.mock(FlinkService.class);
+
+    @Test
+    public void testUpgrade() throws Exception {
+        KubernetesClient kubernetesClient = Mockito.mock(KubernetesClient.class);
+        JobReconciler reconciler = new JobReconciler(kubernetesClient, flinkService);
+        FlinkDeployment deployment = TestUtils.buildApplicationCluster();
+        Configuration config = FlinkUtils.getEffectiveConfig(deployment);
+
+        reconciler.reconcile(""test"", deployment, config);
+        Mockito.verify(flinkService, times(1)).submitApplicationCluster(eq(deployment), eq(config));
+        Mockito.clearInvocations(flinkService);
+        deployment.getStatus().setSpec(deployment.getSpec());
+
+        JobStatus jobStatus = new JobStatus();
+        jobStatus.setJobName(JOB_NAME);
+        jobStatus.setJobId(JOB_ID);
+        jobStatus.setState(""RUNNING"");
+
+        deployment.getStatus().setJobStatus(jobStatus);
+
+        // Test stateless upgrade
+        FlinkDeployment statelessUpgrade = TestUtils.clone(deployment);
+        statelessUpgrade.getSpec().getJob().setUpgradeMode(UpgradeMode.STATELESS);
+        statelessUpgrade.getSpec().getFlinkConfiguration().put(""new"", ""conf"");
+        reconciler.reconcile(""test"", statelessUpgrade, config);
+        Mockito.verify(flinkService, times(1))
+                .cancelJob(eq(JobID.fromHexString(JOB_ID)), eq(UpgradeMode.STATELESS), eq(config));
+
+        Mockito.verify(flinkService, times(1))","[{'comment': ""Flink code style instructs us to [avoid using Mockito](https://flink.apache.org/contributing/code-style-and-quality-common.html#avoid-mockito---use-reusable-test-implementations). I am not sure whether these rules apply to other repos, but I assume so. Let's clarify this"", 'commenter': 'dannycranmer'}, {'comment': ""That's a very good point, I think the codestyle definitely applies here as well. I will try to see how we can change those tests to not use mockito"", 'commenter': 'gyfora'}]"
1,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/FlinkUtils.java,"@@ -0,0 +1,229 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.utils;
+
+import org.apache.flink.configuration.ConfigConstants;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.configuration.CoreOptions;
+import org.apache.flink.configuration.DeploymentOptions;
+import org.apache.flink.configuration.GlobalConfiguration;
+import org.apache.flink.configuration.JobManagerOptions;
+import org.apache.flink.configuration.PipelineOptions;
+import org.apache.flink.configuration.TaskManagerOptions;
+import org.apache.flink.kubernetes.configuration.KubernetesConfigOptions;
+import org.apache.flink.kubernetes.configuration.KubernetesDeploymentTarget;
+import org.apache.flink.kubernetes.operator.crd.FlinkDeployment;
+import org.apache.flink.kubernetes.operator.crd.spec.FlinkDeploymentSpec;
+import org.apache.flink.util.StringUtils;
+
+import com.fasterxml.jackson.databind.JsonNode;
+import com.fasterxml.jackson.databind.ObjectMapper;
+import com.fasterxml.jackson.databind.node.ArrayNode;
+import com.fasterxml.jackson.databind.node.ObjectNode;
+import io.fabric8.kubernetes.api.model.Pod;
+import io.fabric8.kubernetes.client.KubernetesClient;
+import io.fabric8.kubernetes.client.internal.SerializationUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.File;
+import java.io.IOException;
+import java.net.URI;
+import java.nio.file.Files;
+import java.util.Collections;
+import java.util.Iterator;
+
+/** Flink Utility methods used by the operator. */
+public class FlinkUtils {
+
+    private static final Logger LOG = LoggerFactory.getLogger(FlinkUtils.class);
+    private static final ObjectMapper MAPPER = new ObjectMapper();
+
+    public static Configuration getEffectiveConfig(FlinkDeployment flinkApp) {
+        String namespace = flinkApp.getMetadata().getNamespace();
+        String clusterId = flinkApp.getMetadata().getName();
+        FlinkDeploymentSpec spec = flinkApp.getSpec();
+
+        try {
+            String flinkConfDir = System.getenv().get(ConfigConstants.ENV_FLINK_CONF_DIR);
+            Configuration effectiveConfig =
+                    flinkConfDir != null
+                            ? GlobalConfiguration.loadConfiguration(flinkConfDir)
+                            : new Configuration();
+
+            effectiveConfig.setString(KubernetesConfigOptions.NAMESPACE, namespace);
+            effectiveConfig.setString(KubernetesConfigOptions.CLUSTER_ID, clusterId);
+
+            if (spec.getIngressDomain() != null) {
+                effectiveConfig.set(
+                        KubernetesConfigOptions.REST_SERVICE_EXPOSED_TYPE,
+                        KubernetesConfigOptions.ServiceExposedType.ClusterIP);
+            }
+
+            if (spec.getJob() != null) {
+                effectiveConfig.set(
+                        DeploymentOptions.TARGET, KubernetesDeploymentTarget.APPLICATION.getName());
+            } else {
+                effectiveConfig.set(
+                        DeploymentOptions.TARGET, KubernetesDeploymentTarget.SESSION.getName());
+            }
+
+            if (!StringUtils.isNullOrWhitespaceOnly(spec.getImage())) {
+                effectiveConfig.set(KubernetesConfigOptions.CONTAINER_IMAGE, spec.getImage());
+            }
+
+            if (!StringUtils.isNullOrWhitespaceOnly(spec.getImagePullPolicy())) {
+                effectiveConfig.set(
+                        KubernetesConfigOptions.CONTAINER_IMAGE_PULL_POLICY,
+                        KubernetesConfigOptions.ImagePullPolicy.valueOf(spec.getImagePullPolicy()));
+            }
+
+            if (spec.getFlinkConfiguration() != null && !spec.getFlinkConfiguration().isEmpty()) {
+                spec.getFlinkConfiguration().forEach(effectiveConfig::setString);
+            }
+
+            // Pod template
+            if (spec.getPodTemplate() != null) {
+                effectiveConfig.set(
+                        KubernetesConfigOptions.KUBERNETES_POD_TEMPLATE,
+                        createTempFile(spec.getPodTemplate()));
+            }
+
+            if (spec.getJobManager() != null) {
+                if (spec.getJobManager().getResource() != null) {
+                    effectiveConfig.setString(
+                            JobManagerOptions.TOTAL_PROCESS_MEMORY.key(),
+                            spec.getJobManager().getResource().getMemory());
+                    effectiveConfig.set(
+                            KubernetesConfigOptions.JOB_MANAGER_CPU,
+                            spec.getJobManager().getResource().getCpu());
+                }
+
+                if (spec.getJobManager().getPodTemplate() != null) {
+                    effectiveConfig.set(
+                            KubernetesConfigOptions.JOB_MANAGER_POD_TEMPLATE,
+                            createTempFile(
+                                    mergePodTemplates(
+                                            spec.getPodTemplate(),
+                                            spec.getJobManager().getPodTemplate())));
+                }
+            }
+
+            if (spec.getTaskManager() != null) {
+                if (spec.getTaskManager().getTaskSlots() > 0) {
+                    effectiveConfig.set(
+                            TaskManagerOptions.NUM_TASK_SLOTS,
+                            spec.getTaskManager().getTaskSlots());
+                }
+
+                if (spec.getTaskManager().getResource() != null) {
+                    effectiveConfig.setString(
+                            TaskManagerOptions.TOTAL_PROCESS_MEMORY.key(),
+                            spec.getTaskManager().getResource().getMemory());
+                    effectiveConfig.set(
+                            KubernetesConfigOptions.TASK_MANAGER_CPU,
+                            spec.getTaskManager().getResource().getCpu());
+                }
+
+                if (spec.getTaskManager().getPodTemplate() != null) {
+                    effectiveConfig.set(
+                            KubernetesConfigOptions.TASK_MANAGER_POD_TEMPLATE,
+                            createTempFile(
+                                    mergePodTemplates(
+                                            spec.getPodTemplate(),
+                                            spec.getTaskManager().getPodTemplate())));
+                }
+            }
+
+            if (spec.getJob() != null) {
+                final URI uri = new URI(spec.getJob().getJarURI());
+                effectiveConfig.set(
+                        PipelineOptions.JARS, Collections.singletonList(uri.toString()));
+
+                if (spec.getJob().getParallelism() > 0) {
+                    effectiveConfig.set(
+                            CoreOptions.DEFAULT_PARALLELISM, spec.getJob().getParallelism());
+                }
+            }
+
+            return effectiveConfig;
+        } catch (Exception e) {
+            throw new RuntimeException(""Failed to load configuration"", e);
+        }
+    }","[{'comment': 'This method is very long, suggest splitting it out into smaller methods', 'commenter': 'dannycranmer'}, {'comment': ""I completely agree, I would not jump on this straight away but will add a ticket for this so we don't forget."", 'commenter': 'gyfora'}, {'comment': 'https://issues.apache.org/jira/browse/FLINK-26163\r\n', 'commenter': 'gyfora'}]"
1,pom.xml,"@@ -0,0 +1,285 @@
+<?xml version=""1.0"" encoding=""UTF-8""?>
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+  http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+<project xmlns=""http://maven.apache.org/POM/4.0.0""
+         xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
+         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
+
+    <parent>
+      <groupId>org.apache</groupId>
+      <artifactId>apache</artifactId>
+      <version>20</version>
+    </parent>
+
+    <modelVersion>4.0.0</modelVersion>
+
+    <groupId>org.apache.flink</groupId>
+    <artifactId>flink-kubernetes-operator-parent</artifactId>
+    <version>1.0-SNAPSHOT</version>
+
+    <name>Flink Kubernetes: </name>
+    <packaging>pom</packaging>
+    <url>https://flink.apache.org</url>
+    <inceptionYear>2014</inceptionYear>
+
+    <licenses>
+      <license>
+        <name>The Apache Software License, Version 2.0</name>
+        <url>https://www.apache.org/licenses/LICENSE-2.0.txt</url>
+        <distribution>repo</distribution>
+      </license>
+    </licenses>
+
+    <modules>
+      <module>flink-kubernetes-operator</module>
+      <module>flink-kubernetes-webhook</module>
+    </modules>
+
+    <properties>
+        <maven.compiler.source>11</maven.compiler.source>
+        <maven.compiler.target>11</maven.compiler.target>
+        <maven-assembly-plugin.version>3.3.0</maven-assembly-plugin.version>
+        <maven-surefire-plugin.version>3.0.0-M4</maven-surefire-plugin.version>
+        <maven-failsafe-plugin.version>3.0.0-M4</maven-failsafe-plugin.version>
+        <maven-resources-plugin.version>3.2.0</maven-resources-plugin.version>
+
+        <operator.sdk.version>2.1.1</operator.sdk.version>
+        <fabric8.version>5.12.1</fabric8.version>
+        <lombok.version>1.18.22</lombok.version>
+
+        <scala.version>2.12</scala.version>
+        <flink.version>1.14.3</flink.version>
+        <flink.shaded.version>15.0</flink.shaded.version>","[{'comment': 'Why are we using a newer version for shaded libs?', 'commenter': 'dannycranmer'}, {'comment': 'Flink 1.15 already uses this version, I think by the time we have a first release here that is going to be the current version :)', 'commenter': 'gyfora'}, {'comment': 'Besides it being the next version, the dependencies do not need to line up exactly with Flink core. It is conceivable that in the future they will diverge in other cases also.', 'commenter': 'tweise'}]"
1,helm/flink-operator/templates/webhook.yaml,"@@ -0,0 +1,103 @@
+################################################################################
+#  Licensed to the Apache Software Foundation (ASF) under one
+#  or more contributor license agreements.  See the NOTICE file
+#  distributed with this work for additional information
+#  regarding copyright ownership.  The ASF licenses this file
+#  to you under the Apache License, Version 2.0 (the
+#  ""License""); you may not use this file except in compliance
+#  with the License.  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an ""AS IS"" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+# limitations under the License.
+################################################################################
+---
+{{- if .Values.webhook.create }}
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: flink-operator-webhook-service
+  namespace: {{ .Values.operatorNamespace.name }}
+spec:
+  ports:
+  - port: 443
+    targetPort: 9443
+  selector:
+    app.kubernetes.io/name: {{ include ""flink-operator.name"" . }}
+---
+{{- if .Values.webhook.keystore.useDefaultPassword }}
+apiVersion: v1
+kind: Secret
+metadata:
+  name: flink-operator-webhook-secret
+  namespace: {{ .Values.operatorNamespace.name }}
+type: Opaque
+data:
+  password: cGFzc3dvcmQxMjM0
+{{- end }}
+---
+apiVersion: cert-manager.io/v1
+kind: Certificate
+metadata:
+  name: flink-operator-serving-cert
+  namespace: {{ .Values.operatorNamespace.name }}
+spec:
+  dnsNames:
+  - flink-operator-webhook-service.default.svc
+  - flink-operator-webhook-service.default.svc.cluster.local
+  keystores:
+    pkcs12:
+      create: true
+      passwordSecretRef:
+      {{- if .Values.webhook.keystore.useDefaultPassword }}
+        name: flink-operator-webhook-secret
+        key: password
+      {{- else }}
+        {{- with .Values.webhook.keystore.passwordSecretRef }}
+          {{- toYaml . | nindent 8 }}
+        {{- end }}
+      {{- end }}
+  issuerRef:
+    kind: Issuer
+    name: flink-operator-selfsigned-issuer
+  commonName: FlinkDeployment Validator
+  secretName: webhook-server-cert
+---
+apiVersion: cert-manager.io/v1
+kind: Issuer
+metadata:
+  name: flink-operator-selfsigned-issuer
+  namespace: {{ .Values.operatorNamespace.name }}
+spec:
+  selfSigned: {}
+---
+apiVersion: admissionregistration.k8s.io/v1
+kind: ValidatingWebhookConfiguration
+metadata:
+  annotations:
+    cert-manager.io/inject-ca-from: default/flink-operator-serving-cert
+  name: flink-operator-validating-webhook-configuration","[{'comment': 'Just want to check if we should add `namespace` for `flink-operator-validating-webhook-configuration` and change `default/flink-operator-serving-cert` to `{{ .Values.operatorNamespace.name }}/flink-operator-serving-cert`', 'commenter': 'bgeng777'}, {'comment': ""The webhook config does not have a namespace (it's global), but you are absolutely right about the cert name, that is a mistake. Fixing now"", 'commenter': 'gyfora'}]"
1,helm/flink-operator/templates/webhook.yaml,"@@ -0,0 +1,103 @@
+################################################################################
+#  Licensed to the Apache Software Foundation (ASF) under one
+#  or more contributor license agreements.  See the NOTICE file
+#  distributed with this work for additional information
+#  regarding copyright ownership.  The ASF licenses this file
+#  to you under the Apache License, Version 2.0 (the
+#  ""License""); you may not use this file except in compliance
+#  with the License.  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an ""AS IS"" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+# limitations under the License.
+################################################################################
+---
+{{- if .Values.webhook.create }}
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: flink-operator-webhook-service
+  namespace: {{ .Values.operatorNamespace.name }}
+spec:
+  ports:
+  - port: 443
+    targetPort: 9443
+  selector:
+    app.kubernetes.io/name: {{ include ""flink-operator.name"" . }}
+---
+{{- if .Values.webhook.keystore.useDefaultPassword }}
+apiVersion: v1
+kind: Secret
+metadata:
+  name: flink-operator-webhook-secret
+  namespace: {{ .Values.operatorNamespace.name }}
+type: Opaque
+data:
+  password: cGFzc3dvcmQxMjM0
+{{- end }}
+---
+apiVersion: cert-manager.io/v1
+kind: Certificate
+metadata:
+  name: flink-operator-serving-cert
+  namespace: {{ .Values.operatorNamespace.name }}
+spec:
+  dnsNames:
+  - flink-operator-webhook-service.default.svc
+  - flink-operator-webhook-service.default.svc.cluster.local","[{'comment': 'must be:\r\nflink-operator-webhook-service.{{ .Values.operatorNamespace.name }}.svc\r\nflink-operator-webhook-service.{{ .Values.operatorNamespace.name }}.svc.cluster.local', 'commenter': 'morhidi'}, {'comment': 'Good catch, fixed ðŸ‘ ', 'commenter': 'gyfora'}]"
1,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/crd/status/JobStatus.java,"@@ -0,0 +1,36 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.crd.status;
+
+import lombok.AllArgsConstructor;
+import lombok.Builder;
+import lombok.Data;
+import lombok.NoArgsConstructor;
+
+/** Status of an individual job within the Flink deployment. */
+@Data
+@NoArgsConstructor
+@AllArgsConstructor
+@Builder
+public class JobStatus {
+    private String jobName;
+    private String jobId;
+    private String state;","[{'comment': 'Should `state` field fall into one enum value of `JobState`? I find the usages of this field include `jobStatus.setState(""suspended"");` in `JobReconciler.java` and `jobStatus.setState(""RUNNING"");` in `JobReconcilerTest.java`. The mix use of upper/lower case may be somehow misleading.', 'commenter': 'bgeng777'}, {'comment': 'Good point, I think this could be mapped to the JobState enum, I will fix this tomorrow', 'commenter': 'gyfora'}, {'comment': 'Opened a jira ticket for this', 'commenter': 'gyfora'}]"
1,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/crd/spec/JobSpec.java,"@@ -0,0 +1,41 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.crd.spec;
+
+import lombok.AllArgsConstructor;
+import lombok.Builder;
+import lombok.Data;
+import lombok.EqualsAndHashCode;
+import lombok.NoArgsConstructor;
+
+/** Flink job spec. */
+@Data
+@NoArgsConstructor
+@AllArgsConstructor
+@Builder
+public class JobSpec {
+    private String jarURI;
+    private int parallelism;
+    private String entryClass;","[{'comment': 'A small improvement: `entryClassName` may be more accurate as in our upstream flink code, this field is named as `applicationClassName`.', 'commenter': 'bgeng777'}, {'comment': 'in the rest api it is simply called `entryClass` (https://nightlies.apache.org/flink/flink-docs-master/docs/ops/rest_api/#jars-jarid-run), if others also feel strongly about this we can change it, but if not I would prefer to keep it as is', 'commenter': 'gyfora'}, {'comment': '+1 for keeping as is', 'commenter': 'tweise'}]"
9,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/FlinkUtils.java,"@@ -55,109 +43,19 @@
     private static final ObjectMapper MAPPER = new ObjectMapper();
 
     public static Configuration getEffectiveConfig(FlinkDeployment flinkApp) {
-        String namespace = flinkApp.getMetadata().getNamespace();
-        String clusterId = flinkApp.getMetadata().getName();
-        FlinkDeploymentSpec spec = flinkApp.getSpec();
-
         try {
-            String flinkConfDir = System.getenv().get(ConfigConstants.ENV_FLINK_CONF_DIR);
-            Configuration effectiveConfig = loadConfiguration(flinkConfDir);
-
-            effectiveConfig.setString(KubernetesConfigOptions.NAMESPACE, namespace);
-            effectiveConfig.setString(KubernetesConfigOptions.CLUSTER_ID, clusterId);
-
-            if (spec.getIngressDomain() != null) {
-                effectiveConfig.set(
-                        KubernetesConfigOptions.REST_SERVICE_EXPOSED_TYPE,
-                        KubernetesConfigOptions.ServiceExposedType.ClusterIP);
-            }
-
-            if (spec.getJob() != null) {
-                effectiveConfig.set(
-                        DeploymentOptions.TARGET, KubernetesDeploymentTarget.APPLICATION.getName());
-            } else {
-                effectiveConfig.set(
-                        DeploymentOptions.TARGET, KubernetesDeploymentTarget.SESSION.getName());
-            }
-
-            if (!StringUtils.isNullOrWhitespaceOnly(spec.getImage())) {
-                effectiveConfig.set(KubernetesConfigOptions.CONTAINER_IMAGE, spec.getImage());
-            }
-
-            if (!StringUtils.isNullOrWhitespaceOnly(spec.getImagePullPolicy())) {
-                effectiveConfig.set(
-                        KubernetesConfigOptions.CONTAINER_IMAGE_PULL_POLICY,
-                        KubernetesConfigOptions.ImagePullPolicy.valueOf(spec.getImagePullPolicy()));
-            }
-
-            if (spec.getFlinkConfiguration() != null && !spec.getFlinkConfiguration().isEmpty()) {
-                spec.getFlinkConfiguration().forEach(effectiveConfig::setString);
-            }
-
-            // Pod template
-            if (spec.getPodTemplate() != null) {
-                effectiveConfig.set(
-                        KubernetesConfigOptions.KUBERNETES_POD_TEMPLATE,
-                        createTempFile(spec.getPodTemplate()));
-            }
-
-            if (spec.getJobManager() != null) {
-                if (spec.getJobManager().getResource() != null) {
-                    effectiveConfig.setString(
-                            JobManagerOptions.TOTAL_PROCESS_MEMORY.key(),
-                            spec.getJobManager().getResource().getMemory());
-                    effectiveConfig.set(
-                            KubernetesConfigOptions.JOB_MANAGER_CPU,
-                            spec.getJobManager().getResource().getCpu());
-                }
-
-                if (spec.getJobManager().getPodTemplate() != null) {
-                    effectiveConfig.set(
-                            KubernetesConfigOptions.JOB_MANAGER_POD_TEMPLATE,
-                            createTempFile(
-                                    mergePodTemplates(
-                                            spec.getPodTemplate(),
-                                            spec.getJobManager().getPodTemplate())));
-                }
-            }
-
-            if (spec.getTaskManager() != null) {
-                if (spec.getTaskManager().getTaskSlots() > 0) {
-                    effectiveConfig.set(
-                            TaskManagerOptions.NUM_TASK_SLOTS,
-                            spec.getTaskManager().getTaskSlots());
-                }
-
-                if (spec.getTaskManager().getResource() != null) {
-                    effectiveConfig.setString(
-                            TaskManagerOptions.TOTAL_PROCESS_MEMORY.key(),
-                            spec.getTaskManager().getResource().getMemory());
-                    effectiveConfig.set(
-                            KubernetesConfigOptions.TASK_MANAGER_CPU,
-                            spec.getTaskManager().getResource().getCpu());
-                }
-
-                if (spec.getTaskManager().getPodTemplate() != null) {
-                    effectiveConfig.set(
-                            KubernetesConfigOptions.TASK_MANAGER_POD_TEMPLATE,
-                            createTempFile(
-                                    mergePodTemplates(
-                                            spec.getPodTemplate(),
-                                            spec.getTaskManager().getPodTemplate())));
-                }
-            }
-
-            if (spec.getJob() != null) {
-                final URI uri = new URI(spec.getJob().getJarURI());
-                effectiveConfig.set(
-                        PipelineOptions.JARS, Collections.singletonList(uri.toString()));
-
-                if (spec.getJob().getParallelism() > 0) {
-                    effectiveConfig.set(
-                            CoreOptions.DEFAULT_PARALLELISM, spec.getJob().getParallelism());
-                }
-            }
-
+            final Configuration effectiveConfig =
+                    new FlinkConfigBuilder(flinkApp)
+                            .applyFlinkConfiguration()
+                            .applyImage()
+                            .applyImagePullPolicy()
+                            .applyCommonPodTemplate()
+                            .applyIngressDomain()
+                            .applyJobManagerSpec()
+                            .applyTaskManagerSpec()
+                            .applyJobOrSessionSpec()
+                            .build();","[{'comment': 'This could be part of the FlinkConfigBuilder as a static method.\r\n\r\n```\r\npublic static Configuration buildFor(FlinkDeployment dep) {...}\r\n```', 'commenter': 'gyfora'}, {'comment': 'Thanks for the advice. Fixed in a02f7185edfdac9e71e470f46b69d5f89c4dacab', 'commenter': 'bgeng777'}]"
9,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/FlinkUtils.java,"@@ -172,7 +70,7 @@ public static Configuration loadConfiguration(String confDir) {
         return configuration;
     }
 
-    private static String createTempFile(Pod podTemplate) throws IOException {
+    public static String createTempFile(Pod podTemplate) throws IOException {","[{'comment': 'We could move this to the FlinkConfigbuilder and keep as private', 'commenter': 'gyfora'}, {'comment': 'Moved in a02f7185edfdac9e71e470f46b69d5f89c4dacab', 'commenter': 'bgeng777'}]"
9,flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/utils/FlinkConfigBuilderTest.java,"@@ -0,0 +1,175 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.utils;
+
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.configuration.CoreOptions;
+import org.apache.flink.configuration.DeploymentOptions;
+import org.apache.flink.configuration.JobManagerOptions;
+import org.apache.flink.configuration.MemorySize;
+import org.apache.flink.configuration.PipelineOptions;
+import org.apache.flink.configuration.TaskManagerOptions;
+import org.apache.flink.kubernetes.configuration.KubernetesConfigOptions;
+import org.apache.flink.kubernetes.configuration.KubernetesDeploymentTarget;
+import org.apache.flink.kubernetes.operator.TestUtils;
+import org.apache.flink.kubernetes.operator.crd.FlinkDeployment;
+
+import com.fasterxml.jackson.databind.ObjectMapper;
+import com.fasterxml.jackson.dataformat.yaml.YAMLFactory;
+import io.fabric8.kubernetes.api.model.Container;
+import io.fabric8.kubernetes.api.model.Pod;
+import org.junit.Assert;
+import org.junit.jupiter.api.BeforeAll;
+import org.junit.jupiter.api.Test;
+
+import java.io.File;
+import java.util.ArrayList;
+import java.util.Arrays;
+
+import static org.apache.flink.kubernetes.operator.TestUtils.IMAGE;
+import static org.apache.flink.kubernetes.operator.TestUtils.IMAGE_POLICY;
+import static org.apache.flink.kubernetes.operator.TestUtils.SAMPLE_JAR;
+import static org.apache.flink.kubernetes.operator.TestUtils.SERVICE_ACCOUNT;
+
+/** FlinkConfigBuilderTest. */
+public class FlinkConfigBuilderTest {
+    private static final ObjectMapper OBJECT_MAPPER = new ObjectMapper(new YAMLFactory());
+    private static FlinkDeployment flinkDeployment;
+
+    @BeforeAll
+    public static void prepareFlinkDeployment() {
+        flinkDeployment = TestUtils.buildApplicationCluster();
+        Container container0 = new Container();
+        container0.setName(""container0"");
+        Pod pod0 =
+                TestUtils.getTestPod(
+                        ""pod0 hostname"", ""pod0 api version"", Arrays.asList(container0));
+        Pod pod1 = TestUtils.getTestPod(""pod1 hostname"", ""pod1 api version"", new ArrayList<>());
+        Pod pod2 = TestUtils.getTestPod(""pod2 hostname"", ""pod2 api version"", new ArrayList<>());
+
+        flinkDeployment.getSpec().setPodTemplate(pod0);
+        flinkDeployment.getSpec().setIngressDomain(""test.com"");
+        flinkDeployment.getSpec().getJobManager().setPodTemplate(pod1);
+        flinkDeployment.getSpec().getTaskManager().setPodTemplate(pod2);
+        flinkDeployment.getSpec().getJob().setParallelism(2);
+    }
+
+    @Test
+    public void testApplyImage() {
+        Configuration configuration = new FlinkConfigBuilder(flinkDeployment).applyImage().build();
+        System.out.println(configuration);","[{'comment': 'remove sysout please', 'commenter': 'gyfora'}, {'comment': 'Fixed in a02f7185edfdac9e71e470f46b69d5f89c4dacab', 'commenter': 'bgeng777'}]"
18,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/EnvUtils.java,"@@ -0,0 +1,60 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.utils;
+
+/** Util to get value from environments. */
+public class EnvUtils {
+
+    public static final String ENV_FLINK_OPERATOR_CONF_DIR = ""FLINK_OPERATOR_CONF_DIR"";
+    public static final String ENV_FLINK_CONF_DIR = ""FLINK_CONF_DIR"";
+    public static final String ENV_WEBHOOK_KEYSTORE_FILE = ""WEBHOOK_KEYSTORE_FILE"";
+    public static final String ENV_WEBHOOK_KEYSTORE_PASSWORD = ""WEBHOOK_KEYSTORE_PASSWORD"";
+    public static final String ENV_WEBHOOK_KEYSTORE_TYPE = ""WEBHOOK_KEYSTORE_TYPE"";
+    public static final String ENV_WEBHOOK_SERVER_PORT = ""WEBHOOK_SERVER_PORT"";
+    public static final String ENV_WATCHED_NAMESPACES = ""FLINK_OPERATOR_WATCH_NAMESPACES"";
+    public static final String ENV_HOSTNAME = ""HOSTNAME"";
+    public static final String ENV_OPERATOR_NAME = ""OPERATOR_NAME"";
+    public static final String ENV_OPERATOR_NAMESPACE = ""OPERATOR_NAMESPACE"";
+
+    /**
+     * Get the value provided by environments.
+     *
+     * @param key the target key
+     * @param required whether to verify the key is exists.
+     * @return the value or default value provided by environments.
+     */
+    public static String get(String key, boolean required) {","[{'comment': 'We could probably use two separate methods get() and getRequired() here.', 'commenter': 'morhidi'}, {'comment': 'LGTM otherwise', 'commenter': 'morhidi'}, {'comment': 'I added separate `get` method without `required` parameter ', 'commenter': 'Aitozi'}, {'comment': 'nit: I also lean to `getRequired()`.', 'commenter': 'wangyang0918'}, {'comment': 'updated', 'commenter': 'Aitozi'}]"
18,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/EnvUtils.java,"@@ -0,0 +1,70 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.utils;
+
+/** Util to get value from environments. */
+public class EnvUtils {
+
+    public static final String ENV_FLINK_OPERATOR_CONF_DIR = ""FLINK_OPERATOR_CONF_DIR"";
+    public static final String ENV_FLINK_CONF_DIR = ""FLINK_CONF_DIR"";
+    public static final String ENV_WEBHOOK_KEYSTORE_FILE = ""WEBHOOK_KEYSTORE_FILE"";
+    public static final String ENV_WEBHOOK_KEYSTORE_PASSWORD = ""WEBHOOK_KEYSTORE_PASSWORD"";
+    public static final String ENV_WEBHOOK_KEYSTORE_TYPE = ""WEBHOOK_KEYSTORE_TYPE"";
+    public static final String ENV_WEBHOOK_SERVER_PORT = ""WEBHOOK_SERVER_PORT"";
+    public static final String ENV_WATCHED_NAMESPACES = ""FLINK_OPERATOR_WATCH_NAMESPACES"";
+    public static final String ENV_HOSTNAME = ""HOSTNAME"";
+    public static final String ENV_OPERATOR_NAME = ""OPERATOR_NAME"";
+    public static final String ENV_OPERATOR_NAMESPACE = ""OPERATOR_NAMESPACE"";
+
+    /**
+     * Get the value provided by environments.
+     *
+     * @param key the target key
+     * @return the value value provided by environments.
+     */
+    public static String get(String key) {
+        return getRequired(key, false);
+    }
+
+    /**
+     * Get the value provided by environments.
+     *
+     * @param key the target key
+     * @param required whether to verify the key is exists.
+     * @return the value provided by environments.
+     */
+    public static String getRequired(String key, boolean required) {","[{'comment': 'Hmm. We will not need the second argument `required` now. Right?', 'commenter': 'wangyang0918'}, {'comment': 'getRequired should not have a boolean parameter, it is always true (its in the method name)', 'commenter': 'gyfora'}]"
18,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/EnvUtils.java,"@@ -0,0 +1,70 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.utils;
+
+/** Util to get value from environments. */
+public class EnvUtils {
+
+    public static final String ENV_FLINK_OPERATOR_CONF_DIR = ""FLINK_OPERATOR_CONF_DIR"";
+    public static final String ENV_FLINK_CONF_DIR = ""FLINK_CONF_DIR"";
+    public static final String ENV_WEBHOOK_KEYSTORE_FILE = ""WEBHOOK_KEYSTORE_FILE"";
+    public static final String ENV_WEBHOOK_KEYSTORE_PASSWORD = ""WEBHOOK_KEYSTORE_PASSWORD"";
+    public static final String ENV_WEBHOOK_KEYSTORE_TYPE = ""WEBHOOK_KEYSTORE_TYPE"";
+    public static final String ENV_WEBHOOK_SERVER_PORT = ""WEBHOOK_SERVER_PORT"";
+    public static final String ENV_WATCHED_NAMESPACES = ""FLINK_OPERATOR_WATCH_NAMESPACES"";
+    public static final String ENV_HOSTNAME = ""HOSTNAME"";
+    public static final String ENV_OPERATOR_NAME = ""OPERATOR_NAME"";
+    public static final String ENV_OPERATOR_NAMESPACE = ""OPERATOR_NAMESPACE"";
+
+    /**
+     * Get the value provided by environments.
+     *
+     * @param key the target key
+     * @return the value value provided by environments.
+     */
+    public static String get(String key) {
+        return getRequired(key, false);
+    }
+
+    /**
+     * Get the value provided by environments.
+     *
+     * @param key the target key
+     * @param required whether to verify the key is exists.
+     * @return the value provided by environments.
+     */
+    public static String getRequired(String key, boolean required) {
+        String value = System.getenv().get(key);
+        if (value == null && required) {","[{'comment': 'Do we also need to check the empty string here?', 'commenter': 'wangyang0918'}]"
18,flink-kubernetes-webhook/src/main/java/org/apache/flink/kubernetes/operator/admission/FlinkOperatorWebhook.java,"@@ -117,26 +111,19 @@ protected void initChannel(SocketChannel ch) {
     }
 
     private static SslContext createSslContext() throws Exception {
-        String keystorePath = System.getenv(WEBHOOK_KEYSTORE_FILE);
-        String keystorePassword = System.getenv(WEBHOOK_KEYSTORE_PASSWORD);
-        String keystoreType = System.getenv(WEBHOOK_KEYSTORE_TYPE);
+        String keystorePath = EnvUtils.get(EnvUtils.ENV_WEBHOOK_KEYSTORE_FILE);
 
         if (StringUtils.isEmpty(keystorePath)) {
             LOG.info(
                     ""No keystore path is defined in ""
-                            + WEBHOOK_KEYSTORE_FILE
+                            + EnvUtils.ENV_WEBHOOK_KEYSTORE_FILE
                             + "", running without ssl"");
             return null;
         }
 
-        if (StringUtils.isAnyEmpty(keystorePassword, keystoreType)) {
-            throw new RuntimeException(
-                    WEBHOOK_KEYSTORE_PASSWORD
-                            + "" and ""
-                            + WEBHOOK_KEYSTORE_TYPE
-                            + "" must no be empty"");
-        }
-
+        String keystorePassword =
+                EnvUtils.getRequired(EnvUtils.ENV_WEBHOOK_KEYSTORE_PASSWORD, true);","[{'comment': '`getRequired` does not check the empty string.', 'commenter': 'wangyang0918'}]"
21,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/controller/FlinkDeploymentController.java,"@@ -109,6 +119,22 @@ public DeleteControl cleanup(FlinkDeployment flinkApp, Context context) {
         } catch (Exception e) {
             throw new ReconciliationException(e);
         }
+
+        if (!jobManagerDeployments.containsKey(flinkApp)) {
+            return context.getSecondaryResource(Deployment.class)
+                    .map(
+                            deployment -> {
+                                LOG.info(
+                                        ""JobManager deployment {} in namespace {} is ready"",
+                                        flinkApp.getMetadata().getName(),
+                                        flinkApp.getMetadata().getNamespace());
+                                jobManagerDeployments.put(flinkApp, deployment);","[{'comment': ""Shouldn't we check and put into the jobManagerDeployments befre we try to observe? It seems now we introduce an extra refesh between every reconcile and observe."", 'commenter': 'gyfora'}, {'comment': 'We do check before we observe. The deployment check is executed once, it does not introduce an extra refresh. It also works when the operator is restarted and finds existing deployments.', 'commenter': 'tweise'}, {'comment': ""Oh I think I see what you mean. Do you mean that we check after the initial reconcolitiation? (right after the job was deployed).\r\n\r\nMy suggestion was to move before line 108 just to be safe, but maybe it's not necessary"", 'commenter': 'gyfora'}, {'comment': 'The current implementation checks job status only after the deployment was observed. I think my earlier reply was misleading.', 'commenter': 'tweise'}, {'comment': 'Note that that there is other logic in `JobStatusObserver` that will exit when not ready to watch, confusingly returning true. I did not want to touch that in the PR. My goal is to implement the terminal error check when the deployment never shows up, for example due to invalid service account.', 'commenter': 'tweise'}]"
21,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/controller/FlinkDeploymentController.java,"@@ -88,17 +92,23 @@ public DeleteControl cleanup(FlinkDeployment flinkApp, Context context) {
                 operatorNamespace,
                 kubernetesClient,
                 true);
+        jobManagerDeployments.remove(flinkApp);","[{'comment': 'Should we also remove from this map when we cancel the flink job in the Flink service?', 'commenter': 'gyfora'}, {'comment': 'Otherwise after an upgrade the observer might hit the same issue', 'commenter': 'gyfora'}, {'comment': 'Good catch, will look into that tomorrow.', 'commenter': 'tweise'}, {'comment': 'done', 'commenter': 'tweise'}]"
21,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/controller/FlinkDeploymentController.java,"@@ -109,6 +119,22 @@ public DeleteControl cleanup(FlinkDeployment flinkApp, Context context) {
         } catch (Exception e) {
             throw new ReconciliationException(e);
         }
+
+        if (!jobManagerDeployments.containsKey(flinkApp)) {","[{'comment': 'Maybe we could extract the checking and loading of the deployment from the secondaryResource into a new method in the controller to keep the main reconcile flow as simple as possible', 'commenter': 'gyfora'}, {'comment': 'We could also move the whole logic into JobStatusObserver passing the context object.', 'commenter': 'morhidi'}, {'comment': 'It would scatter the resource watching logic. I would prefer to keep it here since it needs to work in tandem with `prepareEventSources`', 'commenter': 'tweise'}, {'comment': 'I also agree with @morhidi that this probably fits best into the observer itself because that is the component responsible for checking the running jobs already. The observer could also prepare the event sources that we would just pass in the controller.', 'commenter': 'gyfora'}, {'comment': 'Refactored the logic and `JobStatusObserver` can be removed / consolidated with JobReconciler in a follow-up.', 'commenter': 'tweise'}]"
21,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/controller/FlinkDeploymentController.java,"@@ -144,7 +170,16 @@ private void updateForReconciliationError(FlinkDeployment flinkApp, String err)
         //        return List.of(new PerResourcePollingEventSource<>(
         //                new FlinkResourceSupplier, context.getPrimaryCache(), POLL_PERIOD,
         //                FlinkApplication.class));","[{'comment': 'I think we can remove these comments now that we have some logic here :)', 'commenter': 'gyfora'}]"
21,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/controller/FlinkDeploymentController.java,"@@ -109,6 +118,19 @@ public DeleteControl cleanup(FlinkDeployment flinkApp, Context context) {
         } catch (Exception e) {
             throw new ReconciliationException(e);
         }
+
+        if (!jobManagerDeployments.contains(flinkApp.getMetadata().getSelfLink())) {
+            Optional<Deployment> deployment = context.getSecondaryResource(Deployment.class);
+            if (deployment.isPresent()) {
+                LOG.info(
+                        ""JobManager deployment {} in namespace {} is ready"",
+                        flinkApp.getMetadata().getName(),
+                        flinkApp.getMetadata().getNamespace());
+                jobManagerDeployments.add(flinkApp.getMetadata().getSelfLink());
+                // reschedule for immediate job status check
+                return UpdateControl.updateStatus(flinkApp).rescheduleAfter(0);","[{'comment': ""We will still need a delay here as even after the pod is ready it still takes a bit for the REST server to listen at the port. This is how I currently see the job status check still hitting a timeout. BTW @gyfora job status checks blocking the operator threads isn't so nice! "", 'commenter': 'tweise'}, {'comment': 'Try @ControllerConfiguration(generationAwareEventProcessing = true)', 'commenter': 'morhidi'}, {'comment': 'In this case, the reconcile loop will kick off after 60 seconds. Can be a simple workaround until we find a solution for this', 'commenter': 'morhidi'}, {'comment': 'could we move this whole thing into a `boolean checkAndCacheDeployment(...)` -or something similar- method?', 'commenter': 'gyfora'}, {'comment': 'I moved it into into separate method for now. Will see how it can be arranged better when I have the error/timeout check in place. ', 'commenter': 'tweise'}, {'comment': 'The annotation does not seem to change anything wrt the original error. There is a separate reschedule interval for this condition, which absent of more sophisticated readiness check I have set to 5s for now - that seems to avoids the timeout exception. ', 'commenter': 'tweise'}]"
21,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/controller/FlinkDeploymentController.java,"@@ -114,6 +126,39 @@ public DeleteControl cleanup(FlinkDeployment flinkApp, Context context) {
         } catch (Exception e) {
             throw new ReconciliationException(e);
         }
+
+        return checkDeployment(flinkApp, context);
+    }
+
+    private UpdateControl<FlinkDeployment> checkDeployment(
+            FlinkDeployment flinkApp, Context context) {
+        if (!jobManagerDeployments.contains(flinkApp.getMetadata().getSelfLink())) {","[{'comment': 'Why do we use `selfLink` here? I am afraid the `selfLink` field does not always exist.', 'commenter': 'wangyang0918'}, {'comment': 'It is a unique reference that avoids caching entire resource objects and duplicate key construction. Why should the field not exist? Did you encounter it somewhere?', 'commenter': 'tweise'}]"
21,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/controller/FlinkDeploymentController.java,"@@ -145,11 +190,16 @@ private void updateForReconciliationError(FlinkDeployment flinkApp, String err)
     @Override
     public List<EventSource> prepareEventSources(
             EventSourceContext<FlinkDeployment> eventSourceContext) {
-        // TODO: start status updated
-        //        return List.of(new PerResourcePollingEventSource<>(
-        //                new FlinkResourceSupplier, context.getPrimaryCache(), POLL_PERIOD,
-        //                FlinkApplication.class));
-        return Collections.emptyList();
+        // reconcile when job manager deployment and REST API are ready
+        SharedIndexInformer<Deployment> deploymentInformer =
+                kubernetesClient
+                        .apps()
+                        .deployments()
+                        .inAnyNamespace()","[{'comment': 'Maybe we do not need to watch all the namespaces if `FLINK_OPERATOR_WATCH_NAMESPACES` configured.', 'commenter': 'wangyang0918'}, {'comment': 'I would prefer that. Need to figure out how to get the controller config from the context and set multiple namespaces.', 'commenter': 'tweise'}, {'comment': ""It's possible to reconstruct the config like this:\r\n```\r\n        FlinkControllerConfig config = new FlinkControllerConfig(this);\r\n        Set<String> namespaces = config.getNamespaces();\r\n```\r\nBut that should not be necessary. @gyfora maybe?"", 'commenter': 'tweise'}, {'comment': 'Not sure if there is a nicer way (i need to check the operator sdk code) but you could add a setConfiguration method to the FlinkOperator and call it from the constructor of the FlinkControllerConfig', 'commenter': 'gyfora'}]"
21,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/controller/FlinkDeploymentController.java,"@@ -145,11 +190,16 @@ private void updateForReconciliationError(FlinkDeployment flinkApp, String err)
     @Override
     public List<EventSource> prepareEventSources(
             EventSourceContext<FlinkDeployment> eventSourceContext) {
-        // TODO: start status updated
-        //        return List.of(new PerResourcePollingEventSource<>(
-        //                new FlinkResourceSupplier, context.getPrimaryCache(), POLL_PERIOD,
-        //                FlinkApplication.class));
-        return Collections.emptyList();
+        // reconcile when job manager deployment and REST API are ready
+        SharedIndexInformer<Deployment> deploymentInformer =
+                kubernetesClient
+                        .apps()
+                        .deployments()
+                        .inAnyNamespace()
+                        .withLabel(""type"", ""flink-native-kubernetes"")","[{'comment': 'Using constants in `org.apache.flink.kubernetes.utils.Constants` could make us aware of upstream changes.', 'commenter': 'wangyang0918'}]"
21,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/controller/FlinkDeploymentController.java,"@@ -114,6 +126,39 @@ public DeleteControl cleanup(FlinkDeployment flinkApp, Context context) {
         } catch (Exception e) {
             throw new ReconciliationException(e);
         }
+
+        return checkDeployment(flinkApp, context);
+    }
+
+    private UpdateControl<FlinkDeployment> checkDeployment(
+            FlinkDeployment flinkApp, Context context) {
+        if (!jobManagerDeployments.contains(flinkApp.getMetadata().getSelfLink())) {
+            Optional<Deployment> deployment = context.getSecondaryResource(Deployment.class);
+            if (deployment.isPresent()) {
+                DeploymentStatus status = deployment.get().getStatus();
+                DeploymentSpec spec = deployment.get().getSpec();
+                if (status != null
+                        && status.getAvailableReplicas() != null
+                        && spec.getReplicas().intValue() == status.getReplicas()
+                        && spec.getReplicas().intValue() == status.getAvailableReplicas()) {
+                    LOG.info(
+                            ""JobManager deployment {} in namespace {} is ready"",
+                            flinkApp.getMetadata().getName(),
+                            flinkApp.getMetadata().getNamespace());
+                    jobManagerDeployments.add(flinkApp.getMetadata().getSelfLink());","[{'comment': 'Do we need to remove the current flinkApp from cache `jobManagerDeployments` when the replicas is not enough? For example, the JobManager crashed for a while.', 'commenter': 'wangyang0918'}, {'comment': 'Maybe, the job status check would fail in that case and we could handle it there. We can address this as follow-up. There is a lot of work that needs to go into the reconciliation logic based on what I noticed while working on this PR.', 'commenter': 'tweise'}]"
23,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/crd/status/JobStatus.java,"@@ -30,7 +32,7 @@
 public class JobStatus {
     private String jobName;
     private String jobId;
-    private String state;","[{'comment': 'Why are we renaming this and not other fields? I agree that it is not super consistent we \r\njobName, jobId, state, updateTime, savepointLocation\r\n\r\nWouldnt it be better to have simply name, id, state ...', 'commenter': 'gyfora'}, {'comment': 'Maybe this could be a separate JIRA discussion to improve field naming in the spec/status and do multiple changes together to simplify testing (redeploying CRD etc.)\r\n', 'commenter': 'gyfora'}, {'comment': ""So to summarize, my suggestion is to not change the name of the field in this PR, but instead let's open a ticket where we gather some improvements to the spec/status instead."", 'commenter': 'gyfora'}, {'comment': 'Sure. It makes sense to me to leave the name change to another ticket. The renaming has been rolled back.\r\nThe reason of renaming here is to follow the naming convention of `org.apache.flink.runtime.client.JobStatusMessage` of flink. We could discuss if such change is necessary later.', 'commenter': 'bgeng777'}]"
23,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/observer/JobStatusObserver.java,"@@ -95,7 +95,7 @@ private JobStatus mergeJobStatus(
         if (newStatus == null) {
             newStatus = createJobStatus(newJob);
         } else {
-            newStatus.setState(newJob.getJobState().name());
+            newStatus.setState(JobState.valueOf(newJob.getJobState().name()));","[{'comment': 'This will give an ugly error if the job is not in a running state. Not sure how to go around this or even if it makes sense to use the operator jobstate enum here after all', 'commenter': 'gyfora'}, {'comment': ""Hi @gyfora thanks for sharing this point and I think about it as well and I totally agree that there is some error handle missing here.\r\nI discuss it a little in the [jira](https://issues.apache.org/jira/browse/FLINK-26178). I think using the job state enum should be reasonable as this state itself is an enum. I believe the key is [FLINK-26139](https://issues.apache.org/jira/browse/FLINK-26139) has not been resolved properly which makes the error handle in this part is misleading. I would spend some time on FLINK-26139, hoping to propose a state machine to describe the job state transition later.\r\n\r\nThis PR seems to be done a little early. We may finish it after we resolve FLINK-26139. What's your advice?"", 'commenter': 'bgeng777'}, {'comment': 'Yea I agree we need to first figure out how exactly we want to handle the state transitions and react to different job states. I am sorry to have wasted your time with this ticket, I should have thought more about it before opening it.', 'commenter': 'gyfora'}, {'comment': ""Oh, it doesn't matter. The work is not a waste at all, I may just need to refine it once we achieve consensus on the state transition. Thanks a lot for the above review and I will close this PR and may reopen it when [FLINK-26139](https://issues.apache.org/jira/browse/FLINK-26139) is done."", 'commenter': 'bgeng777'}]"
31,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/BaseReconciler.java,"@@ -41,17 +43,32 @@
 
     private static final Logger LOG = LoggerFactory.getLogger(BaseReconciler.class);
 
-    public static final int REFRESH_SECONDS = 60;
-    public static final int PORT_READY_DELAY_SECONDS = 10;
+    public final int reconcileDelayInSec;
+    public final int portCheckIntervalInSec;","[{'comment': ""Maybe these could be captured into it's own configclass for easy access and that way it's going to be much easier to extend in the future"", 'commenter': 'gyfora'}, {'comment': 'Agree, will follow your suggestion', 'commenter': 'Aitozi'}]"
31,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/JobManagerDeploymentStatus.java,"@@ -41,14 +38,15 @@
     /** JobManager deployment not found, probably not started or killed by user. */
     MISSING;
 
-    public UpdateControl<FlinkDeployment> toUpdateControl(FlinkDeployment flinkDeployment) {
+    public UpdateControl<FlinkDeployment> toUpdateControl(
+            FlinkDeployment flinkDeployment, int reconcileDelayInSec, int portCheckIntervalInSec) {","[{'comment': ""If we have a dedicated configclass for all these values, then it's enough to pass that and can be changed/extended later"", 'commenter': 'gyfora'}]"
31,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/OperatorConfigOptions.java,"@@ -0,0 +1,41 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.config;
+
+import org.apache.flink.configuration.ConfigOption;
+import org.apache.flink.configuration.ConfigOptions;
+
+/** This class holds configuration constants used by flink operator. */
+public class OperatorConfigOptions {
+
+    public static final ConfigOption<Integer> OPERATOR_RECONCILER_RESCHEDULE_DELAY_IN_SEC =
+            ConfigOptions.key(""operator.reconciler.reschedule.delay.sec"")
+                    .intType()
+                    .defaultValue(60)
+                    .withDescription(
+                            ""The delay in second for the controller to reschedule the reconcile process"");
+
+    public static final ConfigOption<Integer> OPERATOR_RECONCILER_PORT_CHECK_INTERVAL_IN_SEC =
+            ConfigOptions.key(""operator.reconciler.port-check.interval.sec"")","[{'comment': 'I think they both should be called interval for consistency. Could we change this in all places to make it consistent please?', 'commenter': 'gyfora'}, {'comment': ""also since the checking logic will be moved to the observer, let's call it `operator.observer....`"", 'commenter': 'gyfora'}]"
37,flink-kubernetes-webhook/pom.xml,"@@ -61,41 +62,4 @@ under the License.
         </dependency>
     </dependencies>
 
-    <build>
-        <plugins>
-            <plugin>
-                <groupId>org.apache.maven.plugins</groupId>
-                <artifactId>maven-shade-plugin</artifactId>","[{'comment': 'I think we probably need to keep the shade plugin to include the netty classes in our jar. \r\n\r\nBut we can definitely remove the `flink-core` dependency and the exclusions for the `flink-kubernetes-operator`', 'commenter': 'gyfora'}]"
37,docker-entrypoint.sh,"@@ -31,7 +31,8 @@ elif [ ""$1"" = ""operator"" ]; then
 elif [ ""$1"" = ""webhook"" ]; then
     echo ""Starting Webhook""
 
-    exec java -jar $LOG_CONFIG /$WEBHOOK_JAR
+    # Adds the operator shaded jar on the classpath when the webhook starts
+    exec java -classpath $LOG_CONFIG /$OPERATOR_JAR -jar $LOG_CONFIG /$WEBHOOK_JAR","[{'comment': 'we have `$LOG_CONFIG` twice now, that looks like a mistake', 'commenter': 'gyfora'}, {'comment': '@gyfora, sorry for the mistake. I have removed the unused first `$LOG_CONFIG`.', 'commenter': 'SteNicholas'}, {'comment': ""No need to be sorry, I didn't mean it in a wrong way :) just wanted to save you some debugging time"", 'commenter': 'gyfora'}, {'comment': 'Also I am not sure if you can use both -classpath and -jar together, maybe you need only classpath + main class instead', 'commenter': 'gyfora'}]"
37,docker-entrypoint.sh,"@@ -31,7 +31,8 @@ elif [ ""$1"" = ""operator"" ]; then
 elif [ ""$1"" = ""webhook"" ]; then
     echo ""Starting Webhook""
 
-    exec java -jar $LOG_CONFIG /$WEBHOOK_JAR
+    # Adds the operator shaded jar on the classpath when the webhook starts
+    exec java -Xbootclasspath/a:/$OPERATOR_JAR -jar $LOG_CONFIG /$WEBHOOK_JAR","[{'comment': 'did you try:\r\n```\r\nexec java $LOG_CONFIG -classpath /$OPERATOR_JAR:/$WEBHOOK_JAR org.apache.flink.kubernetes.operator.admission.FlinkOperatorWebhook\r\n```', 'commenter': 'gyfora'}, {'comment': 'you should be able to run the webhook on minikube and test there', 'commenter': 'gyfora'}]"
41,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/CompositeReconciler.java,"@@ -0,0 +1,86 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.reconciler;
+
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.kubernetes.operator.config.FlinkOperatorConfiguration;
+import org.apache.flink.kubernetes.operator.crd.FlinkDeployment;
+import org.apache.flink.kubernetes.operator.service.FlinkService;
+
+import io.fabric8.kubernetes.client.KubernetesClient;
+import io.javaoperatorsdk.operator.api.reconciler.Context;
+import io.javaoperatorsdk.operator.api.reconciler.DeleteControl;
+import io.javaoperatorsdk.operator.api.reconciler.UpdateControl;
+
+import java.util.Map;
+import java.util.concurrent.ConcurrentHashMap;
+
+/** The composite reconciler which will use the target reconciler based on the app mode. */
+public class CompositeReconciler implements FlinkReconciler {
+
+    private final Map<Mode, FlinkReconciler> reconcilerMap = new ConcurrentHashMap<>();
+    private final KubernetesClient kubernetesClient;
+    private final FlinkService flinkService;
+    private final FlinkOperatorConfiguration operatorConfiguration;
+    private final FlinkReconcilerFactory factory;
+
+    public CompositeReconciler(
+            KubernetesClient kubernetesClient,
+            FlinkService flinkService,
+            FlinkOperatorConfiguration operatorConfiguration,
+            FlinkReconcilerFactory factory) {
+        this.kubernetesClient = kubernetesClient;
+        this.flinkService = flinkService;
+        this.operatorConfiguration = operatorConfiguration;
+        this.factory = factory;
+    }
+
+    @Override
+    public UpdateControl<FlinkDeployment> reconcile(
+            String operatorNamespace,
+            FlinkDeployment flinkApp,
+            Context context,
+            Configuration effectiveConfig)
+            throws Exception {
+        Mode mode = inferMode(flinkApp);
+        return getOrCreateReconciler(mode)
+                .reconcile(operatorNamespace, flinkApp, context, effectiveConfig);
+    }
+
+    @Override
+    public DeleteControl cleanup(
+            String operatorNamespace, FlinkDeployment flinkApp, Configuration effectiveConfig) {
+        Mode mode = inferMode(flinkApp);
+        return getOrCreateReconciler(mode).cleanup(operatorNamespace, flinkApp, effectiveConfig);
+    }
+
+    private FlinkReconciler getOrCreateReconciler(Mode mode) {
+        return reconcilerMap.computeIfAbsent(
+                mode,
+                m -> factory.create(kubernetesClient, flinkService, operatorConfiguration, mode));
+    }
+
+    private Mode inferMode(FlinkDeployment flinkApp) {","[{'comment': ""I don't really see the value of having a Mode enum, it just adds unnecessary complexity, this simple if branch could be part of the factory logic, then we wouldnt be restricted to hardcoded set of modes"", 'commenter': 'gyfora'}, {'comment': 'It acts the lookup key in the reconcileMap, By this we can choose the target reconciler respect to the `FlinkDeploymentApp` mode. My thought is not have to create `Reconciler` each loop, So i use a KeyMap to store it.', 'commenter': 'Aitozi'}, {'comment': 'the factory can still distinghuish the reconcilers and cache them, but this should be an implementation detail of the factory at this point', 'commenter': 'gyfora'}, {'comment': 'Basically your CompositeReconciler + factory logic should be The Factory :) ', 'commenter': 'gyfora'}, {'comment': 'Get it, thanks, I will change the implementation', 'commenter': 'Aitozi'}]"
41,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/FlinkReconciler.java,"@@ -0,0 +1,45 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.reconciler;
+
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.kubernetes.operator.crd.FlinkDeployment;
+
+import io.javaoperatorsdk.operator.api.reconciler.Context;
+import io.javaoperatorsdk.operator.api.reconciler.DeleteControl;
+import io.javaoperatorsdk.operator.api.reconciler.UpdateControl;
+
+/**
+ * The internal interface of reconciler, It aligns to the {@link
+ * io.javaoperatorsdk.operator.api.reconciler.Reconciler}.
+ */
+public interface FlinkReconciler {
+
+    UpdateControl<FlinkDeployment> reconcile(
+            String operatorNamespace,
+            FlinkDeployment flinkApp,
+            Context context,
+            Configuration effectiveConfig)
+            throws Exception;
+
+    default DeleteControl cleanup(","[{'comment': 'Why does this have a default basically empty implementaiton? I think we should keep it abstract (required)', 'commenter': 'gyfora'}, {'comment': 'This aligns to the `io.javaoperatorsdk.operator.api.reconciler.Reconciler`, It provides the `default` cleanup. I will remove it.', 'commenter': 'Aitozi'}]"
41,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/CompositeReconciler.java,"@@ -0,0 +1,86 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.reconciler;
+
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.kubernetes.operator.config.FlinkOperatorConfiguration;
+import org.apache.flink.kubernetes.operator.crd.FlinkDeployment;
+import org.apache.flink.kubernetes.operator.service.FlinkService;
+
+import io.fabric8.kubernetes.client.KubernetesClient;
+import io.javaoperatorsdk.operator.api.reconciler.Context;
+import io.javaoperatorsdk.operator.api.reconciler.DeleteControl;
+import io.javaoperatorsdk.operator.api.reconciler.UpdateControl;
+
+import java.util.Map;
+import java.util.concurrent.ConcurrentHashMap;
+
+/** The composite reconciler which will use the target reconciler based on the app mode. */
+public class CompositeReconciler implements FlinkReconciler {","[{'comment': 'I think if we get rid of the mode we dont even need the CompositeReconciler and use the ReconcilerFactory in place of it.', 'commenter': 'gyfora'}, {'comment': 'Get it, I will simplify it.', 'commenter': 'Aitozi'}]"
41,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/FlinkReconcilerFactory.java,"@@ -0,0 +1,74 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.reconciler;
+
+import org.apache.flink.kubernetes.operator.config.FlinkOperatorConfiguration;
+import org.apache.flink.kubernetes.operator.crd.FlinkDeployment;
+import org.apache.flink.kubernetes.operator.service.FlinkService;
+
+import io.fabric8.kubernetes.client.KubernetesClient;
+
+import java.util.Map;
+import java.util.concurrent.ConcurrentHashMap;
+
+/** The factory to create reconciler based on app mode. */
+public class FlinkReconcilerFactory {
+
+    private final KubernetesClient kubernetesClient;
+    private final FlinkService flinkService;
+    private final FlinkOperatorConfiguration operatorConfiguration;
+    private final Map<Mode, FlinkReconciler> reconcilerMap;
+
+    public FlinkReconcilerFactory(
+            KubernetesClient kubernetesClient,
+            FlinkService flinkService,
+            FlinkOperatorConfiguration operatorConfiguration) {
+        this.kubernetesClient = kubernetesClient;
+        this.flinkService = flinkService;
+        this.operatorConfiguration = operatorConfiguration;
+        this.reconcilerMap = new ConcurrentHashMap<>();
+    }
+
+    public FlinkReconciler getOrCreate(FlinkDeployment flinkApp) {","[{'comment': 'Since we have only 2 reconcilers and no resource implications, I would prefer to instantiate and populate the map in the constructor.\r\n\r\nThis way we can avoid any errors after the reconciler has started', 'commenter': 'gyfora'}, {'comment': 'I lean to keep lazy instantiate behavior, If we instantiate and populate the map in the constructor it will lose the meaning of the `ReconcilerFactory`, It will act like the previous with two pre-instantiated reconciler.', 'commenter': 'Aitozi'}, {'comment': 'Ok we can keep it like this now :)', 'commenter': 'gyfora'}, {'comment': ""Ditto, `Flink` in the name doesn't resonate. I would say either just `ReconcilerFactory` or `FlinkDeploymentReconcilerFactory`"", 'commenter': 'tweise'}, {'comment': 'renamed to `ReconcilerFactory`', 'commenter': 'Aitozi'}]"
41,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/FlinkReconciler.java,"@@ -0,0 +1,43 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.reconciler;
+
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.kubernetes.operator.crd.FlinkDeployment;
+
+import io.javaoperatorsdk.operator.api.reconciler.Context;
+import io.javaoperatorsdk.operator.api.reconciler.DeleteControl;
+import io.javaoperatorsdk.operator.api.reconciler.UpdateControl;
+
+/**
+ * The internal interface of reconciler, It aligns to the {@link
+ * io.javaoperatorsdk.operator.api.reconciler.Reconciler}.","[{'comment': 'This is a bit different from the operator SDK reconciler as in our case we have validator, observer and reconciler so it is only a subset of the responsitiblities.', 'commenter': 'gyfora'}, {'comment': 'removed the extra comments.', 'commenter': 'Aitozi'}]"
41,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/FlinkReconcilerFactory.java,"@@ -0,0 +1,74 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.reconciler;
+
+import org.apache.flink.kubernetes.operator.config.FlinkOperatorConfiguration;
+import org.apache.flink.kubernetes.operator.crd.FlinkDeployment;
+import org.apache.flink.kubernetes.operator.service.FlinkService;
+
+import io.fabric8.kubernetes.client.KubernetesClient;
+
+import java.util.Map;
+import java.util.concurrent.ConcurrentHashMap;
+
+/** The factory to create reconciler based on app mode. */
+public class FlinkReconcilerFactory {
+
+    private final KubernetesClient kubernetesClient;
+    private final FlinkService flinkService;
+    private final FlinkOperatorConfiguration operatorConfiguration;
+    private final Map<Mode, FlinkReconciler> reconcilerMap;
+
+    public FlinkReconcilerFactory(
+            KubernetesClient kubernetesClient,
+            FlinkService flinkService,
+            FlinkOperatorConfiguration operatorConfiguration) {
+        this.kubernetesClient = kubernetesClient;
+        this.flinkService = flinkService;
+        this.operatorConfiguration = operatorConfiguration;
+        this.reconcilerMap = new ConcurrentHashMap<>();
+    }
+
+    public FlinkReconciler getOrCreate(FlinkDeployment flinkApp) {
+        return reconcilerMap.computeIfAbsent(
+                getMode(flinkApp),
+                mode -> {
+                    switch (mode) {
+                        case SESSION:
+                            return new SessionReconciler(
+                                    kubernetesClient, flinkService, operatorConfiguration);
+                        case JOB:
+                            return new JobReconciler(
+                                    kubernetesClient, flinkService, operatorConfiguration);
+                        default:
+                            throw new UnsupportedOperationException(
+                                    String.format(""Unsupported running mode: %s"", mode));
+                    }
+                });
+    }
+
+    private Mode getMode(FlinkDeployment flinkApp) {
+        return flinkApp.getSpec().getJob() != null ? Mode.JOB : Mode.SESSION;
+    }
+
+    enum Mode {
+        JOB,","[{'comment': 'Instead of JOB we could call it APPLICATION to better represent the deployment mode', 'commenter': 'gyfora'}, {'comment': 'renamed', 'commenter': 'Aitozi'}]"
41,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/FlinkReconcilerFactory.java,"@@ -0,0 +1,74 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.reconciler;
+
+import org.apache.flink.kubernetes.operator.config.FlinkOperatorConfiguration;
+import org.apache.flink.kubernetes.operator.crd.FlinkDeployment;
+import org.apache.flink.kubernetes.operator.service.FlinkService;
+
+import io.fabric8.kubernetes.client.KubernetesClient;
+
+import java.util.Map;
+import java.util.concurrent.ConcurrentHashMap;
+
+/** The factory to create reconciler based on app mode. */
+public class FlinkReconcilerFactory {
+
+    private final KubernetesClient kubernetesClient;
+    private final FlinkService flinkService;
+    private final FlinkOperatorConfiguration operatorConfiguration;
+    private final Map<Mode, FlinkReconciler> reconcilerMap;
+
+    public FlinkReconcilerFactory(
+            KubernetesClient kubernetesClient,
+            FlinkService flinkService,
+            FlinkOperatorConfiguration operatorConfiguration) {
+        this.kubernetesClient = kubernetesClient;
+        this.flinkService = flinkService;
+        this.operatorConfiguration = operatorConfiguration;
+        this.reconcilerMap = new ConcurrentHashMap<>();
+    }
+
+    public FlinkReconciler getOrCreate(FlinkDeployment flinkApp) {
+        return reconcilerMap.computeIfAbsent(
+                getMode(flinkApp),
+                mode -> {
+                    switch (mode) {
+                        case SESSION:
+                            return new SessionReconciler(
+                                    kubernetesClient, flinkService, operatorConfiguration);
+                        case APPLICATION:
+                            return new JobReconciler(
+                                    kubernetesClient, flinkService, operatorConfiguration);
+                        default:","[{'comment': 'The default branch could be removed because the return value of the `getMode` method only has `APPLICATION` or `SESSION` mode, no other mode here.', 'commenter': 'SteNicholas'}, {'comment': 'Yes, But the `default` branch is forced. So I throw an exception there.', 'commenter': 'Aitozi'}]"
41,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/FlinkReconcilerFactory.java,"@@ -0,0 +1,74 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.reconciler;
+
+import org.apache.flink.kubernetes.operator.config.FlinkOperatorConfiguration;
+import org.apache.flink.kubernetes.operator.crd.FlinkDeployment;
+import org.apache.flink.kubernetes.operator.service.FlinkService;
+
+import io.fabric8.kubernetes.client.KubernetesClient;
+
+import java.util.Map;
+import java.util.concurrent.ConcurrentHashMap;
+
+/** The factory to create reconciler based on app mode. */
+public class FlinkReconcilerFactory {
+
+    private final KubernetesClient kubernetesClient;
+    private final FlinkService flinkService;
+    private final FlinkOperatorConfiguration operatorConfiguration;
+    private final Map<Mode, FlinkReconciler> reconcilerMap;
+
+    public FlinkReconcilerFactory(
+            KubernetesClient kubernetesClient,
+            FlinkService flinkService,
+            FlinkOperatorConfiguration operatorConfiguration) {
+        this.kubernetesClient = kubernetesClient;
+        this.flinkService = flinkService;
+        this.operatorConfiguration = operatorConfiguration;
+        this.reconcilerMap = new ConcurrentHashMap<>();
+    }
+
+    public FlinkReconciler getOrCreate(FlinkDeployment flinkApp) {
+        return reconcilerMap.computeIfAbsent(
+                getMode(flinkApp),
+                mode -> {
+                    switch (mode) {
+                        case SESSION:
+                            return new SessionReconciler(","[{'comment': 'IMO, the `FlinkReconciler` could add an interface like `getMode`, and register the mapping to the `FlinkReconcilerFactory`. Thus, the `getOrCreate` could return the reconciler by the mode.', 'commenter': 'SteNicholas'}, {'comment': 'I have considered this way, But I do not quite see the usage of the `getMode`. So I lean to keep the current way, we just have to maintain the instantiation in the `ReconcilerFactory`, and instantiate at need.', 'commenter': 'Aitozi'}]"
41,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/FlinkReconciler.java,"@@ -0,0 +1,40 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.reconciler;
+
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.kubernetes.operator.crd.FlinkDeployment;
+
+import io.javaoperatorsdk.operator.api.reconciler.Context;
+import io.javaoperatorsdk.operator.api.reconciler.DeleteControl;
+import io.javaoperatorsdk.operator.api.reconciler.UpdateControl;
+
+/** The internal interface of reconciler. */
+public interface FlinkReconciler {","[{'comment': 'Is it better to naming the interface to `Reconciler`? Because the `BaseReconciler` is the implementation of this interface. The `FlinkReconciler` interface naming is not so good to me.', 'commenter': 'SteNicholas'}, {'comment': '+1, I think `Flink` is redundant', 'commenter': 'tweise'}, {'comment': 'will rename it', 'commenter': 'Aitozi'}]"
41,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/FlinkReconciler.java,"@@ -0,0 +1,40 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.reconciler;
+
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.kubernetes.operator.crd.FlinkDeployment;
+
+import io.javaoperatorsdk.operator.api.reconciler.Context;
+import io.javaoperatorsdk.operator.api.reconciler.DeleteControl;
+import io.javaoperatorsdk.operator.api.reconciler.UpdateControl;
+
+/** The internal interface of reconciler. */
+public interface FlinkReconciler {
+
+    UpdateControl<FlinkDeployment> reconcile(","[{'comment': 'Please add the comment of the interface.', 'commenter': 'SteNicholas'}, {'comment': 'updated', 'commenter': 'Aitozi'}]"
41,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/FlinkReconciler.java,"@@ -0,0 +1,40 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.reconciler;
+
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.kubernetes.operator.crd.FlinkDeployment;
+
+import io.javaoperatorsdk.operator.api.reconciler.Context;
+import io.javaoperatorsdk.operator.api.reconciler.DeleteControl;
+import io.javaoperatorsdk.operator.api.reconciler.UpdateControl;
+
+/** The internal interface of reconciler. */
+public interface FlinkReconciler {
+
+    UpdateControl<FlinkDeployment> reconcile(
+            String operatorNamespace,
+            FlinkDeployment flinkApp,
+            Context context,
+            Configuration effectiveConfig)
+            throws Exception;
+
+    DeleteControl cleanup(","[{'comment': 'Ditto.', 'commenter': 'SteNicholas'}]"
44,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/FlinkOperator.java,"@@ -27,33 +27,43 @@
 import org.apache.flink.kubernetes.operator.reconciler.SessionReconciler;
 import org.apache.flink.kubernetes.operator.service.FlinkService;
 import org.apache.flink.kubernetes.operator.utils.FlinkUtils;
+import org.apache.flink.kubernetes.operator.utils.javaoperatorsdk.ConfigurationServiceDecorator;
 import org.apache.flink.kubernetes.operator.validation.DefaultDeploymentValidator;
 import org.apache.flink.kubernetes.operator.validation.FlinkDeploymentValidator;
 
 import io.fabric8.kubernetes.client.DefaultKubernetesClient;
+import io.fabric8.kubernetes.client.KubernetesClient;
 import io.javaoperatorsdk.operator.Operator;
+import io.javaoperatorsdk.operator.api.config.ConfigurationService;
 import io.javaoperatorsdk.operator.config.runtime.DefaultConfigurationService;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+
 /** Main Class for Flink native k8s operator. */
 public class FlinkOperator {
     private static final Logger LOG = LoggerFactory.getLogger(FlinkOperator.class);
 
-    public static void main(String... args) {
+    final Operator operator;","[{'comment': ""let's keep this private final and add a protected getter with the @VisibleForTesting annotation"", 'commenter': 'gyfora'}, {'comment': 'Fixed now.', 'commenter': 'kelemensanyi'}]"
44,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/FlinkOperator.java,"@@ -78,12 +88,27 @@ public static void main(String... args) {
                         jobReconciler,
                         sessionReconciler);
 
-        FlinkControllerConfig controllerConfig = new FlinkControllerConfig(controller);
+        controllerConfig = new FlinkControllerConfig(controller);
         controller.setControllerConfig(controllerConfig);
         controllerConfig.setConfigurationService(configurationService);
+    }
 
+    public void run() {
         operator.register(controller, controllerConfig);
         operator.installShutdownHook();
         operator.start();
     }
+
+    protected ConfigurationService getConfigurationService() {
+        return new ConfigurationServiceDecorator(DefaultConfigurationService.instance()) {
+            @Override
+            public ExecutorService getExecutorService() {
+                return Executors.newCachedThreadPool();","[{'comment': 'Should this always be unlimited size? Maybe we should make this configurable, what do you think', 'commenter': 'gyfora'}, {'comment': ""The only case I can see when I would set a limit is when I would be getting out of memory. In such a case by setting a limit we could keep the operator running.\r\n\r\nI've added a new config option for that: `RECONCILIATION_MAX_PARALLELISM`"", 'commenter': 'kelemensanyi'}]"
44,flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/FlinkOperatorTest.java,"@@ -0,0 +1,64 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator;
+
+import org.junit.jupiter.api.Assertions;
+import org.junit.jupiter.api.Test;
+
+import java.util.Collections;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicInteger;
+import java.util.stream.IntStream;
+
+/** @link FlinkOperator unit tests. */
+public class FlinkOperatorTest {
+
+    @Test
+    public void testExecutorServiceCanRun100ThreadsParallel() throws Exception {","[{'comment': 'Seems like the test is testing that cached executor service works that it should. This feels pretty unnecessary, it would be enough I think to assert that the executor service that we get back from the operator is the expected type/config.\r\n\r\nEven better would be to actually start the operator and try to submit multiple different deployment (different names) in parallel and see that it handles it.', 'commenter': 'gyfora'}, {'comment': ""I originally opted for the functional test, because\r\n- the `ExecutorService` interface does not allow to discover its configuration and\r\n- the Controller delegated the construction of the executor service to the standard library, which might return another type of class in future releases.\r\n\r\nFunctional testing is also not an ideal solution, since we can not test if the pool is really unbounded and as you suggested we might do a bit more testing than it is strictly necessary.\r\n\r\nI don't have a strong opinion here, so why not do the simplest then: I've changed the tests as you've suggested. ðŸ‘ "", 'commenter': 'kelemensanyi'}]"
44,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/FlinkOperator.java,"@@ -74,12 +88,43 @@ public static void main(String... args) {
                         reconcilerFactory,
                         observerFactory);
 
-        FlinkControllerConfig controllerConfig = new FlinkControllerConfig(controller);
+        controllerConfig = new FlinkControllerConfig(controller);
         controller.setControllerConfig(controllerConfig);
         controllerConfig.setConfigurationService(configurationService);
+    }
 
+    public void run() {
         operator.register(controller, controllerConfig);
         operator.installShutdownHook();
         operator.start();
     }
+
+    protected ConfigurationService getConfigurationService() {
+        return new ConfigurationServiceDecorator(DefaultConfigurationService.instance()) {
+            @Override
+            public ExecutorService getExecutorService() {
+                int maxPoolSize = getReconciliationMaxPoolSize();
+                return new ThreadPoolExecutor(
+                        0, maxPoolSize, 60L, TimeUnit.SECONDS, new SynchronousQueue<>());
+            }
+
+            private int getReconciliationMaxPoolSize() {
+                int value =
+                        EnvUtils.getPositiveOrMinusOneInt(
+                                EnvUtils.ENV_RECONCILIATION_MAX_PARALLELISM,
+                                Integer.MAX_VALUE,
+                                ""Specify a positive number or -1 for infinite parallelism."");","[{'comment': 'I think it would be a bit nicer and (a lot simpler) to add this to `OperatorConfigOptions` and expose it through the `FlinkOperatorConfiguration` class instead of the env variable. \r\n\r\nWe have some env variable based configs still in a few places but I think for things like this the operator config is much better.', 'commenter': 'gyfora'}, {'comment': 'No worries. :) Fixed now, please have another look.', 'commenter': 'kelemensanyi'}]"
44,helm/flink-operator/values.yaml,"@@ -61,6 +61,7 @@ operatorConfiguration:
     metrics.reporter.slf4j.interval: 5 MINUTE
 
     operator.reconciler.reschedule.interval.sec: 15
+    operator.reconciliation.max.parallelism: -1","[{'comment': ""we do not need to add this here, it's already in the defaults"", 'commenter': 'gyfora'}, {'comment': ""That's removed now."", 'commenter': 'kelemensanyi'}]"
44,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/FlinkOperator.java,"@@ -30,47 +31,55 @@
 import org.apache.flink.kubernetes.operator.validation.FlinkDeploymentValidator;
 
 import io.fabric8.kubernetes.client.DefaultKubernetesClient;
+import io.fabric8.kubernetes.client.KubernetesClient;
 import io.javaoperatorsdk.operator.Operator;
 import io.javaoperatorsdk.operator.api.config.ConfigurationService;
 import io.javaoperatorsdk.operator.api.config.ConfigurationServiceOverrider;
 import io.javaoperatorsdk.operator.config.runtime.DefaultConfigurationService;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import java.util.concurrent.Executors;
+
 /** Main Class for Flink native k8s operator. */
 public class FlinkOperator {
     private static final Logger LOG = LoggerFactory.getLogger(FlinkOperator.class);
 
-    public static void main(String... args) {
+    private final Operator operator;
+    private final FlinkDeploymentController controller;
+    private final FlinkControllerConfig controllerConfig;
+
+    public FlinkOperator() {
 
         LOG.info(""Starting Flink Kubernetes Operator"");
         DefaultConfig defaultConfig = FlinkUtils.loadDefaultConfig();
         OperatorMetricUtils.initOperatorMetrics(defaultConfig.getOperatorConfig());
 
-        DefaultKubernetesClient client = new DefaultKubernetesClient();
+        KubernetesClient client = new DefaultKubernetesClient();
         String namespace = client.getNamespace();
         if (namespace == null) {
             namespace = ""default"";
         }
 
-        ConfigurationService configurationService =
-                new ConfigurationServiceOverrider(DefaultConfigurationService.instance())
-                        .checkingCRDAndValidateLocalModel(false)
-                        .build();
-
-        Operator operator = new Operator(client, configurationService);
-
         FlinkOperatorConfiguration operatorConfiguration =
                 FlinkOperatorConfiguration.fromConfiguration(defaultConfig.getOperatorConfig());
 
+        ConfigurationServiceOverrider configOverrider =
+                new ConfigurationServiceOverrider(DefaultConfigurationService.instance())
+                        .checkingCRDAndValidateLocalModel(false);
+        configOverrider = setupExecutorService(configOverrider, operatorConfiguration);
+        ConfigurationService configurationService = configOverrider.build();","[{'comment': 'this is getting long we can move it into a dedicated method or one of the util classes and probably merge with `setupExecutorService`', 'commenter': 'gyfora'}, {'comment': 'Fixed.', 'commenter': 'kelemensanyi'}]"
44,flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/FlinkOperatorTest.java,"@@ -0,0 +1,50 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator;
+
+import org.apache.flink.kubernetes.operator.config.FlinkOperatorConfiguration;
+import org.apache.flink.kubernetes.operator.utils.FlinkUtils;
+
+import org.junit.jupiter.api.Assertions;
+import org.junit.jupiter.api.Test;
+
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.ThreadPoolExecutor;
+
+/** @link FlinkOperator unit tests. */
+public class FlinkOperatorTest {
+
+    @Test
+    public void testExecutorServiceUsesReconciliationMaxParallelismFromConfig() {
+        final var defaultConfig = FlinkUtils.loadDefaultConfig();
+        final var operatorConfig =","[{'comment': ""We can remove the `final` keyword from everywhere in this method it's not really necessary and just adds more code"", 'commenter': 'gyfora'}, {'comment': 'Fixed.', 'commenter': 'kelemensanyi'}]"
44,flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/FlinkOperatorTest.java,"@@ -0,0 +1,50 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator;
+
+import org.apache.flink.kubernetes.operator.config.FlinkOperatorConfiguration;
+import org.apache.flink.kubernetes.operator.utils.FlinkUtils;
+
+import org.junit.jupiter.api.Assertions;
+import org.junit.jupiter.api.Test;
+
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.ThreadPoolExecutor;
+
+/** @link FlinkOperator unit tests. */
+public class FlinkOperatorTest {
+
+    @Test
+    public void testExecutorServiceUsesReconciliationMaxParallelismFromConfig() {
+        final var defaultConfig = FlinkUtils.loadDefaultConfig();
+        final var operatorConfig =
+                FlinkOperatorConfiguration.fromConfiguration(defaultConfig.getOperatorConfig());
+        final int maxParallelism = operatorConfig.getReconcilerMaxParallelism();
+        final int threadCount = maxParallelism == -1 ? Integer.MAX_VALUE : maxParallelism;
+        System.out.println(threadCount);","[{'comment': 'pls remove print', 'commenter': 'gyfora'}, {'comment': 'Fixed.', 'commenter': 'kelemensanyi'}]"
44,flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/FlinkOperatorTest.java,"@@ -0,0 +1,50 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator;
+
+import org.apache.flink.kubernetes.operator.config.FlinkOperatorConfiguration;
+import org.apache.flink.kubernetes.operator.utils.FlinkUtils;
+
+import org.junit.jupiter.api.Assertions;
+import org.junit.jupiter.api.Test;
+
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.ThreadPoolExecutor;
+
+/** @link FlinkOperator unit tests. */
+public class FlinkOperatorTest {
+
+    @Test
+    public void testExecutorServiceUsesReconciliationMaxParallelismFromConfig() {
+        final var defaultConfig = FlinkUtils.loadDefaultConfig();","[{'comment': 'you should probably create the flink config directly by instantiating a `new Configuration` that will avoid env dependent test behavior and you can easily test the 2 expected behaviour modes here', 'commenter': 'gyfora'}, {'comment': 'Good point. Fixed now.', 'commenter': 'kelemensanyi'}]"
45,flink-kubernetes-webhook/src/main/java/org/apache/flink/kubernetes/operator/admission/AdmissionHandler.java,"@@ -66,15 +70,26 @@ public AdmissionHandler(Validator<GenericKubernetesResource> validator) {
 
     @Override
     protected void channelRead0(ChannelHandlerContext ctx, HttpRequest httpRequest) {
-        final ByteBuf msgContent = ((FullHttpRequest) httpRequest).content();
-        AdmissionReview review;
-        try {
-            InputStream in = new ByteBufInputStream(msgContent);
-            review = objectMapper.readValue(in, AdmissionReview.class);
-            AdmissionReview response = validatingController.handle(review);
-            sendResponse(ctx, objectMapper.writeValueAsString(response));
-        } catch (Exception e) {
-            sendError(ctx, ExceptionUtils.getStackTrace(e));
+        QueryStringDecoder decoder = new QueryStringDecoder(httpRequest.uri());
+        String path = decoder.path();
+        if (""/validate"".equals(path)) {
+            final ByteBuf msgContent = ((FullHttpRequest) httpRequest).content();
+            AdmissionReview review;
+            try {
+                InputStream in = new ByteBufInputStream(msgContent);
+                review = objectMapper.readValue(in, AdmissionReview.class);
+                AdmissionReview response = validatingController.handle(review);
+                sendResponse(ctx, objectMapper.writeValueAsString(response));
+            } catch (Exception e) {
+                sendError(ctx, ExceptionUtils.getStackTrace(e));
+            }
+        } else {
+            String error =
+                    String.format(
+                            ""Webhook accepts the illegal request path '%s', only accepts the request path '/validate'."",","[{'comment': ""Could we please change this to: \r\n```\r\nIllegal path requested: '%s'. Only /validate is accepted.\r\n```"", 'commenter': 'gyfora'}, {'comment': '@gyfora , of course yes. The above error message looks good to me.', 'commenter': 'SteNicholas'}]"
45,flink-kubernetes-webhook/src/main/java/org/apache/flink/kubernetes/operator/admission/AdmissionHandler.java,"@@ -66,15 +70,25 @@ public AdmissionHandler(Validator<GenericKubernetesResource> validator) {
 
     @Override
     protected void channelRead0(ChannelHandlerContext ctx, HttpRequest httpRequest) {
-        final ByteBuf msgContent = ((FullHttpRequest) httpRequest).content();
-        AdmissionReview review;
-        try {
-            InputStream in = new ByteBufInputStream(msgContent);
-            review = objectMapper.readValue(in, AdmissionReview.class);
-            AdmissionReview response = validatingController.handle(review);
-            sendResponse(ctx, objectMapper.writeValueAsString(response));
-        } catch (Exception e) {
-            sendError(ctx, ExceptionUtils.getStackTrace(e));
+        QueryStringDecoder decoder = new QueryStringDecoder(httpRequest.uri());
+        String path = decoder.path();
+        if (""/validate"".equals(path)) {","[{'comment': 'nit: could make `/validate` static and reuse it in the error msg :)', 'commenter': 'Aitozi'}, {'comment': 'nit: could make `/validate` static and reuse it in the error msg :)', 'commenter': 'Aitozi'}]"
50,helm/flink-operator/values.yaml,"@@ -18,9 +18,6 @@
 
 ---
 
-operatorNamespace:
-  name: default","[{'comment': ""We should document this in the README so that it's clear how to select the namespace :) at least one example would be good"", 'commenter': 'gyfora'}, {'comment': '> I think the change makes sense but could you please open a JIRA ticket first?\r\n\r\nSure, will do~', 'commenter': 'haoxins'}, {'comment': 'https://issues.apache.org/jira/browse/FLINK-26594', 'commenter': 'haoxins'}, {'comment': 'perfect, thank you :) ', 'commenter': 'gyfora'}]"
50,README.md,"@@ -7,7 +7,7 @@ A Kubernetes operator for Apache Flink, implemented in Java. See [FLIP-212](http
 The operator is managed helm chart. To install run:
 ```
  cd helm/flink-operator
- helm install flink-operator .
+ helm install flink-operator . --namespace flink-operator --create-namespace","[{'comment': 'Is it the intention to make these parameters mandatory?', 'commenter': 'tweise'}]"
50,helm/flink-operator/templates/serviceaccount.yaml,"@@ -22,7 +22,7 @@ apiVersion: v1
 kind: ServiceAccount
 metadata:
   name: {{ include ""flink-operator.serviceAccountName"" . }}
-  namespace: {{ .Values.operatorNamespace.name }}
+  namespace: {{ Release.Namespace }}","[{'comment': 'I assume this should be `.Release.Namespace`.', 'commenter': 'mbalassi'}, {'comment': 'yeah, thanks~', 'commenter': 'haoxins'}]"
52,helm/flink-operator/values.yaml,"@@ -55,10 +55,32 @@ webhook:
   #   name: jks-password-secret
   #   key: password-key
 
-imagePullSecrets: []
-nameOverride: """"
-fullnameOverride: """"
+operatorConfiguration:
+  create: true
+  flink-conf.yaml: |+
+    metrics.reporter.slf4j.factory.class: org.apache.flink.metrics.slf4j.Slf4jReporterFactory
+    metrics.reporter.slf4j.interval: 5 MINUTE
+
+    operator.reconciler.reschedule.interval.sec: 15
+    operator.observer.progress-check.interval.sec: 5
+  log4j2.properties: |+
+    rootLogger.level = DEBUG
+  # Set append to false to replace configuration files
+  # append: true","[{'comment': 'Looks better :) I suggest to move this property under `create: true`. I missed it at first sight.', 'commenter': 'morhidi'}]"
56,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/observer/Observer.java,"@@ -75,12 +80,15 @@ private boolean isApplicationClusterReady(FlinkDeployment dep) {
     }
 
     private void observeJmDeployment(
-            FlinkDeployment flinkApp, Context context, Configuration effectiveConfig) {
+            FlinkDeployment flinkApp,
+            Context context,
+            Configuration effectiveConfig,
+            boolean checkOnReady) {
         FlinkDeploymentStatus deploymentStatus = flinkApp.getStatus();
         JobManagerDeploymentStatus previousJmStatus =
                 deploymentStatus.getJobManagerDeploymentStatus();
 
-        if (JobManagerDeploymentStatus.READY == previousJmStatus) {
+        if (JobManagerDeploymentStatus.READY == previousJmStatus && !checkOnReady) {","[{'comment': 'Rather than adding the flag, this can be moved into observe.', 'commenter': 'tweise'}, {'comment': ""@tweise +1, the branch that the REST server to be ready is only covered in `observe` method. The exception like `TimeoutException` doesn't need to call `isJobManagerPortReady` to check."", 'commenter': 'SteNicholas'}]"
56,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/observer/Observer.java,"@@ -127,14 +136,26 @@ private void observeJmDeployment(
                     return;
                 }
             }
+            // checking the pod is expensive; we only do it when the deployment isn't ready
+            PodList jmPods = flinkService.getJmPodList(flinkApp, effectiveConfig);
+            for (Pod pod : jmPods.getItems()) {
+                for (ContainerStatus cs : pod.getStatus().getContainerStatuses()) {
+                    ContainerStateWaiting csw = cs.getState().getWaiting();
+                    if (""CrashLoopBackOff"".equals(csw.getReason())) {","[{'comment': 'Could this check be `!StringUtils.isEmpty(csw.getReason()) \xa0&& csw.getReason().toLowerCase().contains(""CrashLoopBackOff"".toLowerCase())`?', 'commenter': 'SteNicholas'}]"
56,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/observer/Observer.java,"@@ -96,23 +104,24 @@ private void observeJmDeployment(
             if (status != null
                     && status.getAvailableReplicas() != null
                     && spec.getReplicas().intValue() == status.getReplicas()
-                    && spec.getReplicas().intValue() == status.getAvailableReplicas()
-                    && flinkService.isJobManagerPortReady(effectiveConfig)) {
-
-                // typically it takes a few seconds for the REST server to be ready
+                    && spec.getReplicas().intValue() == status.getAvailableReplicas()) {","[{'comment': 'Could we please move these 4 conditions into a `deploymentExists` or something similar method to simplify the flow?', 'commenter': 'gyfora'}]"
56,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/observer/Observer.java,"@@ -96,23 +104,24 @@ private void observeJmDeployment(
             if (status != null
                     && status.getAvailableReplicas() != null
                     && spec.getReplicas().intValue() == status.getReplicas()
-                    && spec.getReplicas().intValue() == status.getAvailableReplicas()
-                    && flinkService.isJobManagerPortReady(effectiveConfig)) {
-
-                // typically it takes a few seconds for the REST server to be ready
+                    && spec.getReplicas().intValue() == status.getAvailableReplicas()) {
+                if (flinkService.isJobManagerPortReady(effectiveConfig)) {","[{'comment': ""I don't really see the point in moving this into a nested if branch with fall-through logic. I think the simple if-else before was correct."", 'commenter': 'gyfora'}]"
56,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/observer/Observer.java,"@@ -96,23 +104,24 @@ private void observeJmDeployment(
             if (status != null
                     && status.getAvailableReplicas() != null
                     && spec.getReplicas().intValue() == status.getReplicas()
-                    && spec.getReplicas().intValue() == status.getAvailableReplicas()
-                    && flinkService.isJobManagerPortReady(effectiveConfig)) {
-
-                // typically it takes a few seconds for the REST server to be ready
+                    && spec.getReplicas().intValue() == status.getAvailableReplicas()) {
+                if (flinkService.isJobManagerPortReady(effectiveConfig)) {
+                    // typically it takes a few seconds for the REST server to be ready
+                    LOG.info(
+                            ""JobManager deployment {} in namespace {} port ready, waiting for the REST API..."",
+                            flinkApp.getMetadata().getName(),
+                            flinkApp.getMetadata().getNamespace());
+                    deploymentStatus.setJobManagerDeploymentStatus(
+                            JobManagerDeploymentStatus.DEPLOYED_NOT_READY);
+                    return;
+                }
+            } else {","[{'comment': 'We could also remove the else branch here (as we return early in other cases) to flatten the logic a bit to make it more readable. ', 'commenter': 'gyfora'}]"
56,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/observer/Observer.java,"@@ -127,14 +136,26 @@ private void observeJmDeployment(
                     return;
                 }
             }
+            // checking the pod is expensive; we only do it when the deployment isn't ready
+            PodList jmPods = flinkService.getJmPodList(flinkApp, effectiveConfig);
+            for (Pod pod : jmPods.getItems()) {
+                for (ContainerStatus cs : pod.getStatus().getContainerStatuses()) {
+                    ContainerStateWaiting csw = cs.getState().getWaiting();
+                    if (""CrashLoopBackOff"".equals(csw.getReason())) {
+                        LOG.warn(""JobManager pod fails: {} {}"", csw.getReason(), csw.getMessage());
+                        return;","[{'comment': 'Could we move this check to a separate method?', 'commenter': 'gyfora'}]"
58,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/exception/DeploymentFailedException.java,"@@ -26,6 +26,7 @@
 /** Exception to signal terminal deployment failure. */
 public class DeploymentFailedException extends RuntimeException {
     public static final String COMPONENT_JOBMANAGER = ""JobManagerDeployment"";
+    private static final long serialVersionUID = -1070179896083579221L;","[{'comment': 'Why did you add the serialVersionUIDs?', 'commenter': 'gyfora'}, {'comment': 'The IDE complains this ', 'commenter': 'Aitozi'}, {'comment': 'nit: we could always set the `serialVersionUIDs` to `1L` when introduce a serializable class. BTW, do we really need to serialize `DeploymentFailedException`?\r\n\r\nhttps://flink.apache.org/contributing/code-style-and-quality-java.html', 'commenter': 'wangyang0918'}, {'comment': 'All the `Exception` class are `Serializable`, So we should add `serialVersionUIDs` for them. I will keep in mind to set the `serialVersionUIDs` to `1L` when introduce serializable class next time.', 'commenter': 'Aitozi'}]"
58,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/observer/JobManagerDeploymentStatus.java,"@@ -45,7 +46,9 @@
     ERROR;
 
     public UpdateControl<FlinkDeployment> toUpdateControl(
-            FlinkDeployment flinkDeployment, FlinkOperatorConfiguration operatorConfiguration) {
+            FlinkDeployment originalCopy,","[{'comment': 'Maybe instead of adding `originalCopy` here we could simply change this method to return the reschedule delay only. I think that would fit better in the current flow. Then we can use the `ReconciliationUtils.toUpdateControl` in the controller\r\n\r\nWhat do you think?', 'commenter': 'gyfora'}, {'comment': 'Agree, I will improve it ', 'commenter': 'Aitozi'}]"
58,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/ReconciliationUtils.java,"@@ -74,4 +77,22 @@ public static void updateForReconciliationError(FlinkDeployment flinkApp, String
             throw new IllegalStateException(e);
         }
     }
+
+    public static UpdateControl<FlinkDeployment> toUpdateControl(
+            FlinkDeployment originalCopy, FlinkDeployment current) {
+        UpdateControl<FlinkDeployment> updateControl;
+        if (!Objects.equals(originalCopy.getSpec(), current.getSpec())) {
+            throw new UnsupportedOperationException(
+                    ""The spec changed during reconcile is not supported."");","[{'comment': 'I think this is a valid safeguard but I think the error should be something like:\r\n```\r\nDetected spec change after reconcile, this probably indicates a bug.\r\n```', 'commenter': 'gyfora'}]"
58,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/observer/JobManagerDeploymentStatus.java,"@@ -44,7 +41,7 @@
     /** Deployment in terminal error, requires spec change for reconciliation to continue. */
     ERROR;
 
-    public UpdateControl<FlinkDeployment> toUpdateControl(
+    public Time rescheduleAfter(","[{'comment': 'I think we should use the built in java Duration class instead of the Flink Time', 'commenter': 'gyfora'}, {'comment': 'Fixed', 'commenter': 'Aitozi'}]"
62,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/FlinkService.java,"@@ -129,6 +132,26 @@ public boolean isJobManagerPortReady(Configuration config) {
         return false;
     }
 
+    public boolean isJobManagerServing(Configuration config) {
+        try (ClusterClient<String> clusterClient = getClusterClient(config)) {
+            clusterClient.getWebInterfaceURL();
+            URL url =
+                    new URL(
+                            clusterClient.getWebInterfaceURL()
+                                    + JobsOverviewHeaders.getInstance().getTargetRestEndpointURL());
+            // TODO: add support for https if necessary
+            HttpURLConnection connection = (HttpURLConnection) url.openConnection();
+            connection.setRequestMethod(""GET"");
+            connection.connect();
+            if (connection.getResponseCode() == HttpURLConnection.HTTP_OK) {
+                return true;
+            }","[{'comment': 'Why did you chose to call the rest api this way instead of through the clusterclient directly?', 'commenter': 'gyfora'}, {'comment': 'Thanks for this catch. I neglect that the `clusterClient.listJobs()` will actually send reqesut the same with this raw http blocking call. Have updated to use the clusterClient.', 'commenter': 'bgeng777'}]"
62,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/FlinkOperatorConfiguration.java,"@@ -38,7 +38,7 @@ public static FlinkOperatorConfiguration fromConfiguration(Configuration operato
 
         int restApiReadyDelaySeconds =
                 operatorConfig.getInteger(
-                        OperatorConfigOptions.OPERATOR_OBSERVER_REST_READY_DELAY_IN_SEC);
+                        OperatorConfigOptions.OPERATOR_OBSERVER_PROGRESS_CHECK_INTERVAL_IN_SEC);","[{'comment': 'we can simply use `progressCheckIntervalSeconds` and remove `restApiReadyDelaySeconds` from here', 'commenter': 'gyfora'}, {'comment': 'I totally agree with your point. I believe `restApiReadyDelaySeconds` field is not necessary any more. I am currently working on fixing the CI issue and will take your advice asap.', 'commenter': 'bgeng777'}, {'comment': '`restApiReadyDelaySeconds` removed.', 'commenter': 'bgeng777'}]"
62,.github/workflows/ci.yml,"@@ -52,7 +52,7 @@ jobs:
           export SHELL=/bin/bash
           export DOCKER_BUILDKIT=1
           eval $(minikube -p minikube docker-env)
-          docker build -f ./Dockerfile -t flink-kubernetes-operator:ci-latest .
+          docker build -f ./Dockerfile -t flink-kubernetes-operator:ci-latest --progress plain .","[{'comment': 'We could keep this to make the debugging failed CI easier.', 'commenter': 'wangyang0918'}]"
62,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/FlinkService.java,"@@ -129,6 +129,16 @@ public boolean isJobManagerPortReady(Configuration config) {
         return false;
     }
 
+    public boolean isJobManagerServing(Configuration config) {
+        try (ClusterClient<String> clusterClient = getClusterClient(config)) {
+            clusterClient.listJobs().get(10, TimeUnit.SECONDS);
+            return true;
+        } catch (Exception ignored) {
+        }","[{'comment': 'Does this need to log the exception for listing the jobs of the cluster client?', 'commenter': 'SteNicholas'}, {'comment': 'The `observeJmDeployment()` method will output log WARN msg when the rest service is not working. \r\nAdding it here seems to be redundant?', 'commenter': 'bgeng777'}]"
62,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/FlinkService.java,"@@ -129,6 +129,16 @@ public boolean isJobManagerPortReady(Configuration config) {
         return false;
     }
 
+    public boolean isJobManagerServing(Configuration config) {
+        try (ClusterClient<String> clusterClient = getClusterClient(config)) {
+            clusterClient.listJobs().get(10, TimeUnit.SECONDS);
+            return true;","[{'comment': ""Does this have another way to check the JobManager is serving? The exception way doesn't look good to me."", 'commenter': 'SteNicholas'}, {'comment': 'I do agree the try catch solution is not elegant but this pr aims at verifying the REST service of JM.\r\nFor network issue, to my best knowledge, try catch is the most widely used choice. Do you have any suggestion?', 'commenter': 'bgeng777'}, {'comment': '@bgeng777, what about using the `clusterClient#getWebInterfaceURL` to check the serving?', 'commenter': 'SteNicholas'}, {'comment': ""`getWebInterfaceURL` is neat but one drawback of `clusterClient#getWebInterfaceURL` is that it only returns the current web url(i.e. address of the leader of jm) but does not guarantee the JM can actually work. In `isJobManagerPortReady`, we currently have already used `getWebInterfaceURL` and will 'telnet' the port in the url. In the jira, we think it may not be enough and that's why I finally choose the `listJob` call."", 'commenter': 'bgeng777'}, {'comment': 'The `getWebInterfaceURL` might not work in the current situation because we are using `StandaloneClientHAServices` when creating the rest cluster client.', 'commenter': 'wangyang0918'}, {'comment': ""Please move the timeout into the config, the previous delay was also configurable. Did you check how the timeout behaves? If REST API is not available when the call is made and then becomes available after 5s, will the list call still fail after waiting for 10s? I believe that's what I had seen when testing this."", 'commenter': 'tweise'}, {'comment': 'hi @tweise, I check the flink code and find the underlying impl of `listJob` is [retriable](https://github.com/apache/flink/blob/46bb03848ea67fa7b8952f757d57a2cda6ab16aa/flink-clients/src/main/java/org/apache/flink/client/program/rest/RestClusterClient.java#L833). The retry delay is controlled by the flink config `rest.retry.delay` (default val is 3s). For your case, if the initial call falis, it should retry each 3s until achieving the max retry attempts(default is 20). So do you mean you meet unexpected failure of this list call? Any steps that I can follow to repeat it? \r\n\r\nFor the configurable timeout, in fact, I reuse the `listJobs()` in our `FlinkService`. I want to know if you are meaning that we should make both of them wait a configurable timeout. thanks.\r\n', 'commenter': 'bgeng777'}, {'comment': 'Hi @tweise , I tried following steps to test the actual behavior of `clusterClient.listJobs()`:\r\n1. Apply the `basic-session-example` yaml and wait for everything to be fine\r\n2. Run ` kubectl scale deployment basic-session-example --replicas=0` so JM port and rest call are both unavailable\r\n3. Rescale the replicas to 1 quickly when the `isJobManagerServing` is waiting.\r\nI keep monitoring the operator log (I added some local dummy debug info)\r\n<img width=""1339"" alt=""image"" src=""https://user-images.githubusercontent.com/80749729/158746332-dff95fec-2701-4b35-b39e-3eb60d647c64.png"">\r\n\r\nFrom the log, I find that the rest call is sent at 13:56:05(In the k8s log, it is 05:56:05 due to time lag) and keeps waiting. Then when JM is started at 13:56:07 and everything becomes ok again around 13:56:13, the rest call succeeds. So I believe the answer to ""will the list call still fail after waiting for 10s"" is that it will not fail due to the retry.', 'commenter': 'bgeng777'}]"
62,.github/workflows/ci.yml,"@@ -52,7 +52,7 @@ jobs:
           export SHELL=/bin/bash
           export DOCKER_BUILDKIT=1
           eval $(minikube -p minikube docker-env)
-          docker build -f ./Dockerfile -t flink-kubernetes-operator:ci-latest .
+          docker build --progress=plain --no-cache -f ./Dockerfile -t flink-kubernetes-operator:ci-latest .","[{'comment': 'Are these changes unrelated?', 'commenter': 'tweise'}, {'comment': 'You are right, they are unrelated. You can check the commit history. It is a hotfix used for helping debug the github CI problem. Yang and I think it may be useful for our further development so I leave it here. Do you have any suggestion?', 'commenter': 'bgeng777'}]"
62,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/observer/BaseObserver.java,"@@ -62,8 +62,22 @@ protected void observeJmDeployment(
         JobManagerDeploymentStatus previousJmStatus =
                 deploymentStatus.getJobManagerDeploymentStatus();
 
+        if (JobManagerDeploymentStatus.READY == previousJmStatus) {
+            if (!flinkService.isJobManagerServing(effectiveConfig)) {
+                deploymentStatus.setJobManagerDeploymentStatus(
+                        JobManagerDeploymentStatus.DEPLOYED_NOT_READY);
+                logger.warn(""The job manager is currently not ready for serving."");
+            }
+            return;
+        }","[{'comment': ""This is not really your fault but now the JobObserver won't call observe if the job is already in a ready state (while the sessionobserver always does). I think we need to remove the if logic in the jobObserver and just have your change"", 'commenter': 'gyfora'}, {'comment': 'you can probably reproduce the problem by adding a test case to the JobObserverTest', 'commenter': 'gyfora'}, {'comment': 'Thanks for pointing this out. I remove the `observeJmDeployment` call in `observeFlinkJobStatus`. Let me know if that is our expected change.', 'commenter': 'bgeng777'}, {'comment': ""don't remove that :) I am talking about:\r\nhttps://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/observer/JobObserver.java#L50\r\n\r\nand please add a testcase similar to what you added for the session observer so we can guard against future regressions here"", 'commenter': 'gyfora'}, {'comment': 'Got it. Will add it now', 'commenter': 'bgeng777'}, {'comment': '+1 for adding a test to cover this.', 'commenter': 'wangyang0918'}, {'comment': 'Add tests for `observeApplicationCluster()` to cover the case that JM is ready and later becomes unavailable and finally recovers. Let me know if it is enough.', 'commenter': 'bgeng777'}]"
62,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/observer/BaseObserver.java,"@@ -62,8 +62,22 @@ protected void observeJmDeployment(
         JobManagerDeploymentStatus previousJmStatus =
                 deploymentStatus.getJobManagerDeploymentStatus();
 
+        if (JobManagerDeploymentStatus.READY == previousJmStatus) {
+            if (!flinkService.isJobManagerServing(effectiveConfig)) {
+                deploymentStatus.setJobManagerDeploymentStatus(
+                        JobManagerDeploymentStatus.DEPLOYED_NOT_READY);
+                logger.warn(""The job manager is currently not ready for serving."");
+            }
+            return;
+        }
+
         if (JobManagerDeploymentStatus.DEPLOYED_NOT_READY == previousJmStatus) {
-            deploymentStatus.setJobManagerDeploymentStatus(JobManagerDeploymentStatus.READY);
+            // check if the JM is ready for accepting job submission
+            if (flinkService.isJobManagerServing(effectiveConfig)) {
+                deploymentStatus.setJobManagerDeploymentStatus(JobManagerDeploymentStatus.READY);
+            } else {
+                logger.info(""The job manager has not been ready for serving yet..."");
+            }","[{'comment': ""Thank you @bgeng777 for adding the extra tests. Before merging this I would like to clear up the check flow a little because what you and what @tweise has done conflicts a little.\r\n\r\nI think we should not `return` from the observe method if `flinkService.isJobManagerServing(effectiveConfig)` is false. In those cases we should always check the deployment too which happen later down in this method.\r\n\r\nThis is the whole idea with @tweise 's recent change here: https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/observer/JobObserver.java#L74\r\n\r\nWith your change @bgeng777 we can probably get rid of the observe call there but we still need to validate the deployment if the lest api does not respond."", 'commenter': 'gyfora'}, {'comment': 'Also we should use the same **configurable** timeout in both `flinkservice.listJobs` and `flinkservice. isJobManagerServing ` (probably the isJobManagerServing could call listJobs internally) because if the hardcoded 10 seconds fail for any reason the whole logic breaks', 'commenter': 'gyfora'}, {'comment': ""I think we are pretty close to getting this right, let's cover these few corner cases and we are going to be in a very good shape :)"", 'commenter': 'gyfora'}, {'comment': 'Thanks a lot for the detailed explanation! @gyfora \r\n1. After checking [FLINK-26473](https://issues.apache.org/jira/browse/FLINK-26473) and again, I agree with cancelling the `return` to make it fall back to the deployment. When the rest service is unavailabe, it is reasonable to check the deployment to find if anything bad happens. I will update the logic here.\r\n\r\n2. Adding configurable timeout as @tweise and you suggest makes sense to me. As we achieve consensus on setting the same config for both methods, reusing the same logic is absolutely better.', 'commenter': 'bgeng777'}, {'comment': ""I have updated for above changes and enhanced current tests to cover the case when falling to check the jm deployment. \r\nOne thing I am not so sure is that I reuse the `OPERATOR_OBSERVER_PROGRESS_CHECK_INTERVAL_IN_SEC` for the timeout of `clusterClient.listJobs()`. AFAIK, the rest call is also a kind of 'progress'. Is there any strong point I ignore to introduce a new config for the rest call timeout?\r\n cc @gyfora @wangyang0918 @tweise \r\n"", 'commenter': 'bgeng777'}]"
80,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/FlinkOperatorConfiguration.java,"@@ -30,30 +30,33 @@
 @Value
 public class FlinkOperatorConfiguration {
 
-    int reconcileIntervalSeconds;
-    int progressCheckIntervalSeconds;
-    int restApiReadyDelaySeconds;
-    int savepointTriggerGracePeriodSeconds;
+    long reconcileIntervalSeconds;","[{'comment': 'We can keep the `Duration` here, let it to be converted when used.', 'commenter': 'Aitozi'}, {'comment': 'By this we can also save the `seconds` in variable name.', 'commenter': 'Aitozi'}]"
80,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/FlinkOperatorConfiguration.java,"@@ -24,36 +24,33 @@
 
 import lombok.Value;
 
+import java.time.Duration;
 import java.util.Set;
 
 /** Configuration class for operator. */
 @Value
 public class FlinkOperatorConfiguration {
 
-    int reconcileIntervalSeconds;
-    int progressCheckIntervalSeconds;
-    int restApiReadyDelaySeconds;
-    int savepointTriggerGracePeriodSeconds;
+    Duration reconcileIntervalSeconds;","[{'comment': 'If these are `Duration` types now please remove the `Seconds` suffix from their name too.', 'commenter': 'mbalassi'}]"
80,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/FlinkOperatorConfiguration.java,"@@ -24,36 +24,33 @@
 
 import lombok.Value;
 
+import java.time.Duration;
 import java.util.Set;
 
 /** Configuration class for operator. */
 @Value
 public class FlinkOperatorConfiguration {
 
-    int reconcileIntervalSeconds;
-    int progressCheckIntervalSeconds;
-    int restApiReadyDelaySeconds;
-    int savepointTriggerGracePeriodSeconds;
+    Duration reconcileIntervalSeconds;
+    Duration progressCheckIntervalSeconds;
+    Duration restApiReadyDelaySeconds;
+    Duration savepointTriggerGracePeriodSeconds;
     String flinkServiceHostOverride;
     Set<String> watchedNamespaces;
 
     public static FlinkOperatorConfiguration fromConfiguration(Configuration operatorConfig) {
-        int reconcileIntervalSeconds =
-                operatorConfig.getInteger(
-                        OperatorConfigOptions.OPERATOR_RECONCILER_RESCHEDULE_INTERVAL_IN_SEC);
+        Duration reconcileIntervalSeconds =","[{'comment': 'please remove the Seconds suffix', 'commenter': 'mbalassi'}]"
80,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/observer/JobManagerDeploymentStatus.java,"@@ -44,7 +44,7 @@
 
     public Duration rescheduleAfter(
             FlinkDeployment flinkDeployment, FlinkOperatorConfiguration operatorConfiguration) {
-        int rescheduleAfterSec;
+        Duration rescheduleAfterSec;","[{'comment': 'please remove the Seconds suffix', 'commenter': 'mbalassi'}]"
81,docs/content/docs/operations/ingress.md,"@@ -24,4 +24,36 @@ specific language governing permissions and limitations
 under the License.
 -->
 
+# Accessing Flinkâ€™s Web UI
+
+The Flink Kubernetes Operator, by default, does not change the way the native kubernetes integration [exposes](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/resource-providers/native_kubernetes/#accessing-flinks-web-ui) the Flink Web UI.
+
 # Ingress
+The Operator also supports creating an optional Ingress entry for the Web UI. Ingress generation can be turned on by defining the `ingressDomain` variable in the FlinkDeployment:
+
+```yaml
+apiVersion: flink.apache.org/v1alpha1
+kind: FlinkDeployment
+metadata:
+  namespace: default
+  name: basic-ingress
+spec:
+  image: flink:1.14.3
+  flinkVersion: v1_14
+  ingressDomain: flink.k8s.io
+  ...
+```
+
+The Operator takes the `ingressDomain`, `name` and `namespace` values from the deployment and creates an Ingress using the template `{{name}}.{{namespace}}.{{ingressDomain}}` to generate the host. This requires that anything `*.flink.k8s.io` should be routed to the Ingress controller on the Kubernetes cluster.","[{'comment': 'Please add the following link to `Ingress` in this sentence:\r\n\r\nhttps://kubernetes.io/docs/concepts/services-networking/ingress/', 'commenter': 'mbalassi'}, {'comment': 'Added, thanks @mbalassi ', 'commenter': 'morhidi'}]"
81,docs/content/docs/operations/ingress.md,"@@ -24,4 +24,36 @@ specific language governing permissions and limitations
 under the License.
 -->
 
+# Accessing Flinkâ€™s Web UI
+
+The Flink Kubernetes Operator, by default, does not change the way the native kubernetes integration [exposes](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/resource-providers/native_kubernetes/#accessing-flinks-web-ui) the Flink Web UI.
+
 # Ingress
+The Operator also supports creating an optional Ingress entry for the Web UI. Ingress generation can be turned on by defining the `ingressDomain` variable in the FlinkDeployment:
+
+```yaml
+apiVersion: flink.apache.org/v1alpha1
+kind: FlinkDeployment
+metadata:
+  namespace: default
+  name: basic-ingress
+spec:
+  image: flink:1.14.3
+  flinkVersion: v1_14
+  ingressDomain: flink.k8s.io
+  ...
+```
+
+The Operator takes the `ingressDomain`, `name` and `namespace` values from the deployment and creates an Ingress using the template `{{name}}.{{namespace}}.{{ingressDomain}}` to generate the host. This requires that anything `*.flink.k8s.io` should be routed to the Ingress controller on the Kubernetes cluster.
+
+```shell
+kubectl get ingress
+NAME             CLASS   HOSTS                                 ADDRESS        PORTS   AGE
+basic-ingress    nginx   basic-ingress.default.flink.k8s.io    192.168.49.2   80      30m
+basic-ingress2   nginx   basic-ingress2.default.flink.k8s.io   192.168.49.2   80      3m39s
+","[{'comment': 'Remove 2 empty lines from code snippet', 'commenter': 'mbalassi'}, {'comment': 'Removed, thanks', 'commenter': 'morhidi'}]"
85,flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/reconciler/JobReconcilerTest.java,"@@ -199,6 +200,38 @@ public void triggerSavepoint() throws Exception {
         assertNull(spDeployment.getStatus().getJobStatus().getSavepointInfo().getTriggerId());
     }
 
+    @Test
+    public void triggerRestart() throws Exception {
+        Context context = TestUtils.createContextWithReadyJobManagerDeployment();
+        TestingFlinkService flinkService = new TestingFlinkService();
+
+        JobReconciler reconciler = new JobReconciler(null, flinkService, operatorConfiguration);
+        FlinkDeployment deployment = TestUtils.buildApplicationCluster();
+        Configuration config = FlinkUtils.getEffectiveConfig(deployment, new Configuration());
+
+        reconciler.reconcile(deployment, context, config);
+        List<Tuple2<String, JobStatusMessage>> runningJobs = flinkService.listJobs();
+        verifyAndSetRunningJobsToStatus(deployment, runningJobs);
+        long jobStartTime = runningJobs.get(0).f1.getStartTime();
+
+        // Test restart job
+        FlinkDeployment restartJob = ReconciliationUtils.clone(deployment);
+        restartJob.getSpec().setRestartNonce(1L);
+        reconciler.reconcile(restartJob, context, config);
+
+        runningJobs = flinkService.listJobs();
+        assertEquals(1, runningJobs.size());
+        long newJobStartTime = runningJobs.get(0).f1.getStartTime();
+        Assertions.assertTrue(newJobStartTime > jobStartTime);","[{'comment': 'nit: it looks that in the file, we have already imported `org.junit.jupiter.api.Assertions.assertEquals` and `org.junit.jupiter.api.Assertions.assertTrue`. Maybe we can use these asserts without `Assertion.` prefix.', 'commenter': 'bgeng777'}, {'comment': 'Good catch, removed.', 'commenter': 'Aitozi'}]"
85,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/JobReconciler.java,"@@ -114,6 +119,14 @@ public void reconcile(FlinkDeployment flinkApp, Context context, Configuration e
         }
     }
 
+    private void restart(FlinkDeployment flinkApp, Configuration effectiveConfig) throws Exception {
+        LOG.info(""Restart application cluster now."");
+        String jobIdString = flinkApp.getStatus().getJobStatus().getJobId();
+        JobID jobID = jobIdString != null ? JobID.fromHexString(jobIdString) : null;
+        flinkService.cancelJob(jobID, UpgradeMode.LAST_STATE, effectiveConfig);","[{'comment': 'IIUC, reason for using the `flinkService.cancelJob` with `LAST_STATE` mode is to delete the cluster directly?\r\nBut in `LAST_STATE` mode, we will not delete HA config. Is that expected? \r\n', 'commenter': 'bgeng777'}, {'comment': 'Yes. IMO, the restart semantic will let the job keep running from the latest checkpoint. So the HA config is retained.', 'commenter': 'Aitozi'}, {'comment': 'I recheck the JIRA, the restart should be a shortcut of `suspend -> delete current deployment -> restart job from latest state in a new deployment"", right? If that is the case, I think your code works fine.', 'commenter': 'bgeng777'}, {'comment': ""Yes, It's exactly as you said"", 'commenter': 'Aitozi'}]"
85,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/JobReconciler.java,"@@ -114,6 +119,14 @@ public void reconcile(FlinkDeployment flinkApp, Context context, Configuration e
         }
     }
 
+    private void restart(FlinkDeployment flinkApp, Configuration effectiveConfig) throws Exception {
+        LOG.info(""Restart application cluster now."");
+        String jobIdString = flinkApp.getStatus().getJobStatus().getJobId();
+        JobID jobID = jobIdString != null ? JobID.fromHexString(jobIdString) : null;","[{'comment': 'Since we always `cancelJob` with `LAST_STATE`, do we really need to get the `jobID` here?', 'commenter': 'wangyang0918'}, {'comment': 'not necessary, will replace it with `null`.', 'commenter': 'Aitozi'}]"
85,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/ReconciliationUtils.java,"@@ -65,6 +65,22 @@ public static void updateForReconciliationError(FlinkDeployment flinkApp, String
         reconciliationStatus.setError(err);
     }
 
+    public static boolean triggerRestart(FlinkDeployment flinkApp) {
+        Long restartNonce = flinkApp.getSpec().getRestartNonce();
+        boolean everReconcileSucceed =","[{'comment': 'IIRC, the `triggerRestart` is never called with `lastReconciledSpec` is null since `JobReconciler#reconcile()` will directly return in such case.', 'commenter': 'wangyang0918'}, {'comment': 'oh, yes. Thanks for point out.', 'commenter': 'Aitozi'}]"
85,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/JobReconciler.java,"@@ -202,17 +200,19 @@ private void printCancelLogs(UpgradeMode upgradeMode) {
                 effectiveConfig);
     }
 
-    private void suspendJob(
+    private JobState suspendJob(
             FlinkDeployment flinkApp, UpgradeMode upgradeMode, Configuration effectiveConfig)
             throws Exception {
         final Optional<String> savepointOpt =
                 internalSuspendJob(flinkApp, upgradeMode, effectiveConfig);
 
         JobStatus jobStatus = flinkApp.getStatus().getJobStatus();
-        jobStatus.setState(JobState.SUSPENDED.name());","[{'comment': 'I wonder do we need to set the `JobStatus#state` to `SUSPEND` here? One is the observed state, one is the desired state. Maybe we could just clear the state and let the next reconcile to sync the state by observer? ', 'commenter': 'Aitozi'}, {'comment': 'I prefer to update the status as early as possible since we are pretty sure the job has already been cancelled.', 'commenter': 'wangyang0918'}, {'comment': 'Get it', 'commenter': 'Aitozi'}]"
85,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/validation/DefaultDeploymentValidator.java,"@@ -245,6 +241,19 @@
 
         JobSpec oldJob = oldSpec.getJob();
         JobSpec newJob = newSpec.getJob();
+
+        if (lastReconciledSpec(deployment) != null
+                && ReconciliationUtils.triggerRestart(deployment)) {
+            if (newJob.getState() != JobState.RUNNING) {
+                return Optional.of(
+                        String.format(""Cannot restart job to %s state"", newJob.getState()));
+            }
+            if (oldJob.getState() != JobState.RUNNING) {
+                return Optional.of(
+                        String.format(""Cannot restart job in %s state"", oldJob.getState()));
+            }
+        }","[{'comment': 'I think we can remove these checks together with the `ReconciliationUtils.triggerRestart(deployment))` I dont think we are achieving anything and we actually restrict cases when the user want to restart a failing job', 'commenter': 'gyfora'}, {'comment': 'This validation is try to guard that job should target `RUNNING` state and also in `RUNNING` state when user try to use the restartNonce. The `RUNNING` state is the desire state of the job not the observed state here. Otherwise, what will we do if user changes the `restartNonce` when the job is in `SUSPENDED` state ? ', 'commenter': 'Aitozi'}, {'comment': 'nothing will happen, but why would anything happen anyways :) without the validaiton the restartNonce will behave like any other spec change', 'commenter': 'gyfora'}, {'comment': 'Instead of adding this specific validation for the `restartNonce` field we could discuss whether we should allow spec changes in suspended state. I think it is fine', 'commenter': 'gyfora'}, {'comment': '> but why would anything happen anyways\r\n\r\nI think the `restartNonce` have the semantic of bring the job to a `start/running` status somehow? If not go with this check it will work like ask job to restart once, but it still suspend :).\r\n\r\n> we could discuss whether we should allow spec changes in suspended state\r\n\r\nIf JobState changes from suspended to suspended, with other spec changes, It will not take effect, I think it will not bring bad impact to allow the change. The problem of the `restartNonce` field change in suspended state is only the problem of the semantic, as I said first.', 'commenter': 'Aitozi'}, {'comment': ""I think the way you implemented it the restartNonce will not change the desired state. And I think it's good like this. This is just a field to trigger restart without having to change the actual spec."", 'commenter': 'gyfora'}, {'comment': 'About the removal of the validation, I want to keep it to Monday to wait for more inputs. Not insist on it, Just want to hear more opinions on it :)', 'commenter': 'Aitozi'}, {'comment': ""All I am trying to say, is that the way it is implemented, changing the `restartNonce` field works as any other spec change and it triggers a simple upgrade of the deplyoment. (this is perfectly good)\r\n\r\nGiven this, I don't see the point in introducing restrictions on this field as we also dont have restrictions on when an upgrade can be triggered. I just want to keep it conistent and simple.\r\n\r\nWe can of course wait for others to chime in here as well:\r\n@wangyang0918 @morhidi @tweise "", 'commenter': 'gyfora'}, {'comment': 'I lean to make `restartNonce` working as a normal fields and do not have special validation.', 'commenter': 'wangyang0918'}, {'comment': '@gyfora @wangyang0918 Thanks for your suggestion, I have removed the validation accordingly ', 'commenter': 'Aitozi'}]"
86,docs/content/docs/development/guide.md,"@@ -93,6 +93,22 @@ rest.port: 8081
 rest.address: localhost
 ```
 
+### Uninstalling the operator locally
+```bash
+helm uninstall flink-operator
+```
+
+### CRD Generating and Upgrading 
+
+By default, the CRD is generated by [Fabric8 CRDGenerator](https://github.com/fabric8io/kubernetes-client/blob/master/doc/CRD-generator.md), when building from source.","[{'comment': 'Please update:\r\n... generated by _the_ Fabric8 ...', 'commenter': 'mbalassi'}, {'comment': 'fixed', 'commenter': 'Aitozi'}]"
86,docs/content/docs/development/guide.md,"@@ -93,6 +93,22 @@ rest.port: 8081
 rest.address: localhost
 ```
 
+### Uninstalling the operator locally
+```bash
+helm uninstall flink-operator
+```
+
+### CRD Generating and Upgrading 
+
+By default, the CRD is generated by [Fabric8 CRDGenerator](https://github.com/fabric8io/kubernetes-client/blob/master/doc/CRD-generator.md), when building from source.
+When installing flink-operator first time, the CRD will apply to the kubernetes cluster automatically. But it will not be removed or upgraded when re-install the flink-operator, referring to helm [doc](https://helm.sh/docs/chart_best_practices/custom_resource_definitions/). ","[{'comment': 'Instead of:\r\nthe flink-operator, referring to helm doc\r\nPlease use:\r\nthe flink-operator as described in the relevant helm documentation', 'commenter': 'mbalassi'}, {'comment': 'Please update:\r\nWhen installing flink-operator _for the_ first time...\r\n', 'commenter': 'mbalassi'}, {'comment': 'fixed', 'commenter': 'Aitozi'}]"
86,docs/content/docs/development/guide.md,"@@ -93,6 +93,22 @@ rest.port: 8081
 rest.address: localhost
 ```
 
+### Uninstalling the operator locally
+```bash
+helm uninstall flink-operator
+```
+
+### CRD Generating and Upgrading 
+
+By default, the CRD is generated by the [Fabric8 CRDGenerator](https://github.com/fabric8io/kubernetes-client/blob/master/doc/CRD-generator.md), when building from source.
+When installing flink-operator for the first time, the CRD will apply to the kubernetes cluster automatically. But it will not be removed or upgraded when re-install the flink-operator, as described in the relevant helm [documentation](https://helm.sh/docs/chart_best_practices/custom_resource_definitions/). ","[{'comment': 'Should be: \r\nthe CRD will **be applied** to the kubernetes\r\nupgraded when **re-installing** the flink-operator\r\n', 'commenter': 'gyfora'}, {'comment': 'Fixed', 'commenter': 'Aitozi'}]"
86,docs/content/docs/development/guide.md,"@@ -93,6 +93,22 @@ rest.port: 8081
 rest.address: localhost
 ```
 
+### Uninstalling the operator locally
+```bash
+helm uninstall flink-operator
+```
+
+### CRD Generating and Upgrading ","[{'comment': 'Should be: **Generating and Upgrading the CRD**', 'commenter': 'gyfora'}, {'comment': 'Fixed', 'commenter': 'Aitozi'}]"
91,helm/flink-operator/templates/flink-operator.yaml,"@@ -67,6 +67,8 @@ spec:
               value: -Dlog4j.configurationFile=/opt/flink-operator/conf/log4j2.properties
             - name: FLINK_OPERATOR_WATCH_NAMESPACES
               value: {{ join "","" .Values.watchNamespaces  }}
+            - name: JAVA_OPERATOR_SDK_CHECK_CRD","[{'comment': 'Then ENV doest not work. They only support java properties now.\r\n\r\nhttps://github.com/java-operator-sdk/java-operator-sdk/pull/1016/files', 'commenter': 'wangyang0918'}]"
109,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/JobReconciler.java,"@@ -82,7 +83,11 @@ public void reconcile(FlinkDeployment flinkApp, Context context, Configuration e
         }
 
         boolean specChanged = !flinkApp.getSpec().equals(lastReconciledSpec);
-        if (specChanged && readyForSpecChanges(flinkApp)) {
+        if (specChanged) {
+            if (!readyForSpecChanges(flinkApp)) {","[{'comment': 'Could we please rename this method and changed the logged line? `readyForSpecChanges` is a little confusing.\r\n\r\nWe could call it `inUpgradeableState` that would describe the logic better', 'commenter': 'gyfora'}, {'comment': 'Make sense.', 'commenter': 'wangyang0918'}]"
109,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/JobReconciler.java,"@@ -114,6 +119,9 @@ public void reconcile(FlinkDeployment flinkApp, Context context, Configuration e
     }
 
     private boolean readyForSpecChanges(FlinkDeployment deployment) {
+        if (isUpgradeModeChangedToLastState(deployment)) {
+            return isJobRunning(deployment);","[{'comment': 'I think we could allow upgrading to last-state even SUSPENDED.  So we could simply change the IF branch at  line 125 to:\r\n\r\n```\r\nif (deployment.getSpec().getJob().getUpgradeMode() != UpgradeMode.SAVEPOINT && !isUpgradeModeChangedToLastState(deployment))\r\n```\r\n\r\nThis way if previous job was with savepoint we would pick up the last savepoint  otherwise we either have state or not depending on the HA setting. Which would be fair for a suspended job.', 'commenter': 'gyfora'}, {'comment': 'I think you are right. We should allow upgrade to last-state even SUSPENDED.', 'commenter': 'wangyang0918'}]"
109,flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/reconciler/JobReconcilerTest.java,"@@ -143,6 +144,10 @@ public void testUpgradeModeChangeFromSavepointToLastState() throws Exception {
         deployment.getSpec().getJob().setState(JobState.RUNNING);
         deployment.getSpec().setImage(""new-image-2"");
 
+        // Wait ready for spec changes
+        deployment.getStatus().setJobManagerDeploymentStatus(JobManagerDeploymentStatus.READY);
+        deployment.getStatus().getJobStatus().setState(JobState.RUNNING.name());
+","[{'comment': 'This should not be necessary, in this test we simulate the SUSPENDED -> RUNNING', 'commenter': 'gyfora'}, {'comment': 'good point.', 'commenter': 'wangyang0918'}]"
109,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/ReconciliationUtils.java,"@@ -118,6 +122,29 @@ public static void updateForReconciliationError(FlinkDeployment flinkApp, String
         return updateControl.rescheduleAfter(rescheduleAfter.toMillis());
     }
 
+    public static boolean isUpgradeModeChangedToLastStateAndHADisabledPreviously(
+            FlinkDeployment flinkApp) {
+        final FlinkDeploymentSpec lastReconciledSpec =
+                Preconditions.checkNotNull(
+                        flinkApp.getStatus().getReconciliationStatus().getLastReconciledSpec());
+        if (lastReconciledSpec.getJob() == null || flinkApp.getSpec().getJob() == null) {","[{'comment': 'These will never be null for jobs so I think we can remove the check.', 'commenter': 'gyfora'}]"
109,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/ReconciliationUtils.java,"@@ -118,6 +122,29 @@ public static void updateForReconciliationError(FlinkDeployment flinkApp, String
         return updateControl.rescheduleAfter(rescheduleAfter.toMillis());
     }
 
+    public static boolean isUpgradeModeChangedToLastStateAndHADisabledPreviously(
+            FlinkDeployment flinkApp) {
+        final FlinkDeploymentSpec lastReconciledSpec =
+                Preconditions.checkNotNull(
+                        flinkApp.getStatus().getReconciliationStatus().getLastReconciledSpec());
+        if (lastReconciledSpec.getJob() == null || flinkApp.getSpec().getJob() == null) {
+            return false;
+        }
+        if (lastReconciledSpec.getFlinkConfiguration() == null
+                || lastReconciledSpec.getFlinkConfiguration().isEmpty()) {
+            return false;","[{'comment': 'If upgrade mode is last state these will never be null due to validation, so we can remove the check', 'commenter': 'gyfora'}]"
109,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/validation/DefaultDeploymentValidator.java,"@@ -241,6 +243,19 @@
             return Optional.of(""Cannot switch from job to session cluster"");
         }
 
+        if (StringUtils.isNullOrWhitespaceOnly(
+                        newSpec.getFlinkConfiguration()
+                                .get(CheckpointingOptions.SAVEPOINT_DIRECTORY.key()))
+                && deployment.getStatus().getJobManagerDeploymentStatus()","[{'comment': 'While it might be correct wouldnâ€™t it be clearer to check the last reconciled job state == RUNNING?', 'commenter': 'gyfora'}, {'comment': 'Imagine the current `JobManagerDeploymentStatus` is `ERROR`, and `isUpgradeModeChangedToLastStateAndHADisabledPreviously()` is true, the savepoint path also needs to be configured. Right? Because it might turn into `READY` in the following reconciliation.', 'commenter': 'wangyang0918'}]"
112,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/controller/FlinkSessionJobController.java,"@@ -0,0 +1,122 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.controller;
+
+import org.apache.flink.kubernetes.operator.config.DefaultConfig;
+import org.apache.flink.kubernetes.operator.config.FlinkOperatorConfiguration;
+import org.apache.flink.kubernetes.operator.crd.FlinkSessionJob;
+import org.apache.flink.kubernetes.operator.exception.ReconciliationException;
+import org.apache.flink.kubernetes.operator.observer.Observer;
+import org.apache.flink.kubernetes.operator.reconciler.Reconciler;
+import org.apache.flink.kubernetes.operator.reconciler.ReconciliationUtils;
+import org.apache.flink.kubernetes.operator.validation.InternalValidator;
+
+import io.fabric8.kubernetes.client.KubernetesClient;
+import io.javaoperatorsdk.operator.api.reconciler.Context;
+import io.javaoperatorsdk.operator.api.reconciler.ControllerConfiguration;
+import io.javaoperatorsdk.operator.api.reconciler.DeleteControl;
+import io.javaoperatorsdk.operator.api.reconciler.ErrorStatusHandler;
+import io.javaoperatorsdk.operator.api.reconciler.RetryInfo;
+import io.javaoperatorsdk.operator.api.reconciler.UpdateControl;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.Optional;
+
+/** Controller that runs the main reconcile loop for {@link FlinkSessionJob}. */
+@ControllerConfiguration
+public class FlinkSessionJobController
+        implements io.javaoperatorsdk.operator.api.reconciler.Reconciler<FlinkSessionJob>,
+                ErrorStatusHandler<FlinkSessionJob> {
+
+    private static final Logger LOG = LoggerFactory.getLogger(FlinkSessionJobController.class);
+    private final KubernetesClient kubernetesClient;
+
+    private final InternalValidator<FlinkSessionJob> validator;
+    private final Reconciler<FlinkSessionJob> reconciler;
+    private final Observer<FlinkSessionJob> observer;
+    private final DefaultConfig defaultConfig;
+    private final FlinkOperatorConfiguration operatorConfiguration;
+
+    private FlinkControllerConfig<FlinkSessionJob> controllerConfig;
+
+    public FlinkSessionJobController(
+            DefaultConfig defaultConfig,
+            FlinkOperatorConfiguration operatorConfiguration,
+            KubernetesClient kubernetesClient,
+            InternalValidator<FlinkSessionJob> validator,
+            Reconciler<FlinkSessionJob> reconciler,
+            Observer<FlinkSessionJob> observer) {
+        this.defaultConfig = defaultConfig;
+        this.operatorConfiguration = operatorConfiguration;
+        this.kubernetesClient = kubernetesClient;
+        this.validator = validator;
+        this.reconciler = reconciler;
+        this.observer = observer;
+    }
+
+    @Override
+    public UpdateControl<FlinkSessionJob> reconcile(
+            FlinkSessionJob flinkSessionJob, Context context) {
+        FlinkSessionJob originalCopy = ReconciliationUtils.clone(flinkSessionJob);
+        LOG.info(""Starting reconciliation"");
+        Optional<String> validationError = validator.validate(flinkSessionJob);
+        if (validationError.isPresent()) {
+            LOG.error(""Validation failed: "" + validationError.get());
+            ReconciliationUtils.updateForReconciliationError(
+                    flinkSessionJob, validationError.get());
+            return ReconciliationUtils.toUpdateControl(originalCopy, flinkSessionJob);
+        }
+
+        try {
+            // TODO refactor the reconciler interface to return UpdateControl directly","[{'comment': 'I dont think the reconciler should return the updatecontrol, at least we have changed this a few times already before to the current state where we keep the logic in the ReconciliationUtils.', 'commenter': 'gyfora'}, {'comment': 'IMO, The `ReconciliationUtils` is a tool which can be invoked in the controller or reconciler. \r\nI want to return the updatecontrol from the reconciler because reconciler can directly decide the updatecontrol. Currently, `FlinkDeploymentController` infer the updatecontrol from the flinkApp, If we push this into the reconciler, it will be more clear. BTW, the `io.javaoperatorsdk.operator.api.reconciler.Reconciler#reconcile` also return the updatecontrol.', 'commenter': 'Aitozi'}, {'comment': 'Okay but letâ€™s do this in a follow up ticket , I would like to minimize impact on the existing logic', 'commenter': 'gyfora'}, {'comment': 'This can wait until after the release I think :)', 'commenter': 'gyfora'}, {'comment': 'OK', 'commenter': 'Aitozi'}, {'comment': 'I also proposal a change to the `Reconciler` and `Observer` interface method as below: \r\n\r\n```\r\npublic interface Reconciler<CR, CTX extends ReconcilerContext<CR>> {\r\n\r\n    UpdateControl<CR> reconcile(CR cr, CTX context) throws Exception;\r\n\r\n    DeleteControl cleanup(CR cr, CTX ctx);\r\n}\r\n```\r\nand \r\n\r\n```\r\n    public ReconcilerContext(CR customResource, Context context, Configuration effectiveConfig) {\r\n        this.customResource = customResource;\r\n        this.context = context;\r\n        this.effectiveConfig = effectiveConfig;\r\n    }\r\n```\r\n\r\nBy this we can extend the `ReconcilerContext` to ship with other resource objects. for example , In session job: we need to fetch the `FlinkDeployment` to get the jobmanager status and the effective config. The `FlinkDeployment` object will be used in the observer and reconciler component.  After this change, we can extend the reconciler easily. \r\n\r\nWhat do you think about this? @gyfora \r\n\r\n', 'commenter': 'Aitozi'}, {'comment': 'I think this is a good idea and will make the code cleaner. I wonder if it makes sense to restrict the context to contain 1 specific resource, what if we need 2? We can iterate more on this design after the release.', 'commenter': 'gyfora'}, {'comment': 'The `1 specific resource` is the target CR event. And the other resources can be added as new field in the subclass.', 'commenter': 'Aitozi'}]"
112,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/observer/BaseObserver.java,"@@ -45,7 +45,7 @@
 import java.util.Optional;
 
 /** The base observer. */
-public abstract class BaseObserver implements Observer {
+public abstract class BaseObserver implements Observer<FlinkDeployment> {","[{'comment': 'We could rename this BaseDeploymentObserver/AbstractDeploymentObserver and move deployment related stuff under a deployment package , similarly we could have a `job` package for sessionjobs', 'commenter': 'gyfora'}, {'comment': 'Agree, I will do it', 'commenter': 'Aitozi'}, {'comment': 'Two more question here:\r\n\r\n- should we also move the `JobManagerDeploymentStatus` under the crd, since it is part of the crd.\r\n- should we also rename the old `JobObserver` and `JobReconciler` to `ApplicationObserver` or `ApplicationJobObserver` and so on I prefer the latter', 'commenter': 'Aitozi'}, {'comment': '1. Yes, lets move it to the `status` package\r\n2. I think it should be called `ApplicationReconciler`/`ApplicationObserver` as it observes/reconciles both job and deployment.', 'commenter': 'gyfora'}]"
112,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/BaseReconciler.java,"@@ -28,7 +28,7 @@
 import io.javaoperatorsdk.operator.api.reconciler.DeleteControl;
 
 /** BaseReconciler with functionality that is common to job and session modes. */
-public abstract class BaseReconciler implements Reconciler {
+public abstract class BaseReconciler implements Reconciler<FlinkDeployment> {","[{'comment': 'As with the BaseObserver this could be called BaseDeploymentReconcer / AbstractDeploymentReconciler and moved to a separate package', 'commenter': 'gyfora'}]"
112,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/FlinkSessionJobReconciler.java,"@@ -0,0 +1,125 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.reconciler;","[{'comment': 'this could go into a `job`/`sessionjob` package', 'commenter': 'gyfora'}]"
112,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/FlinkSessionJobReconciler.java,"@@ -0,0 +1,125 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.reconciler;
+
+import org.apache.flink.api.common.JobID;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.kubernetes.operator.config.FlinkOperatorConfiguration;
+import org.apache.flink.kubernetes.operator.crd.FlinkDeployment;
+import org.apache.flink.kubernetes.operator.crd.FlinkSessionJob;
+import org.apache.flink.kubernetes.operator.crd.spec.FlinkSessionJobSpec;
+import org.apache.flink.kubernetes.operator.observer.JobManagerDeploymentStatus;
+import org.apache.flink.kubernetes.operator.service.FlinkService;
+import org.apache.flink.kubernetes.operator.utils.FlinkUtils;
+
+import io.fabric8.kubernetes.client.KubernetesClient;
+import io.javaoperatorsdk.operator.api.reconciler.Context;
+import io.javaoperatorsdk.operator.api.reconciler.DeleteControl;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.Optional;
+
+/** The reconciler for the {@link FlinkSessionJob}. */
+public class FlinkSessionJobReconciler implements Reconciler<FlinkSessionJob> {
+
+    private static final Logger LOG = LoggerFactory.getLogger(JobReconciler.class);
+
+    private final FlinkOperatorConfiguration operatorConfiguration;
+    private final KubernetesClient kubernetesClient;
+    private final FlinkService flinkService;
+
+    public FlinkSessionJobReconciler(
+            KubernetesClient kubernetesClient,
+            FlinkService flinkService,
+            FlinkOperatorConfiguration operatorConfiguration) {
+        this.kubernetesClient = kubernetesClient;
+        this.flinkService = flinkService;
+        this.operatorConfiguration = operatorConfiguration;
+    }
+
+    @Override
+    public void reconcile(
+            FlinkSessionJob flinkSessionJob, Context context, Configuration defaultConfig)
+            throws Exception {
+
+        FlinkSessionJobSpec lastReconciledSpec =
+                flinkSessionJob.getStatus().getReconciliationStatus().getLastReconciledSpec();
+
+        if (lastReconciledSpec == null) {
+            deployFlinkJob(flinkSessionJob, defaultConfig);
+        }
+        // TODO reconcile other spec change.
+    }
+
+    @Override
+    public DeleteControl cleanup(FlinkSessionJob sessionJob, Configuration defaultConfig) {
+        Optional<FlinkDeployment> flinkDepOptional = getFlinkDeployment(sessionJob);
+
+        if (flinkDepOptional.isPresent()) {
+            Configuration effectiveConfig =
+                    FlinkUtils.getEffectiveConfig(flinkDepOptional.get(), defaultConfig);
+            String jobID = sessionJob.getStatus().getJobStatus().getJobId();
+            if (jobID != null) {
+                try {
+                    flinkService.cancelSessionJob(JobID.fromHexString(jobID), effectiveConfig);
+                } catch (Exception e) {
+                    LOG.error(""Failed to cancel job."", e);
+                }
+            }
+        } else {
+            LOG.info(""Session cluster has disappeared."");
+        }
+        return DeleteControl.defaultDelete();
+    }
+
+    private void deployFlinkJob(FlinkSessionJob sessionJob, Configuration defaultConfig)
+            throws Exception {
+        Optional<FlinkDeployment> flinkDepOptional = getFlinkDeployment(sessionJob);
+        if (flinkDepOptional.isPresent()
+                && (flinkDepOptional.get().getStatus().getJobManagerDeploymentStatus()
+                        == JobManagerDeploymentStatus.READY)) {
+            Configuration effectiveConfig =
+                    FlinkUtils.getEffectiveConfig(flinkDepOptional.get(), defaultConfig);
+            flinkService.submitJobToSessionCluster(sessionJob, effectiveConfig);
+            ReconciliationUtils.updateForSpecReconciliationSuccess(sessionJob);
+        } else {
+            LOG.info(""Session cluster has not been ready"");","[{'comment': '`Session cluster deployment is not in READY state`', 'commenter': 'gyfora'}]"
112,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/FlinkSessionJobReconciler.java,"@@ -0,0 +1,125 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.reconciler;
+
+import org.apache.flink.api.common.JobID;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.kubernetes.operator.config.FlinkOperatorConfiguration;
+import org.apache.flink.kubernetes.operator.crd.FlinkDeployment;
+import org.apache.flink.kubernetes.operator.crd.FlinkSessionJob;
+import org.apache.flink.kubernetes.operator.crd.spec.FlinkSessionJobSpec;
+import org.apache.flink.kubernetes.operator.observer.JobManagerDeploymentStatus;
+import org.apache.flink.kubernetes.operator.service.FlinkService;
+import org.apache.flink.kubernetes.operator.utils.FlinkUtils;
+
+import io.fabric8.kubernetes.client.KubernetesClient;
+import io.javaoperatorsdk.operator.api.reconciler.Context;
+import io.javaoperatorsdk.operator.api.reconciler.DeleteControl;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.Optional;
+
+/** The reconciler for the {@link FlinkSessionJob}. */
+public class FlinkSessionJobReconciler implements Reconciler<FlinkSessionJob> {
+
+    private static final Logger LOG = LoggerFactory.getLogger(JobReconciler.class);
+
+    private final FlinkOperatorConfiguration operatorConfiguration;
+    private final KubernetesClient kubernetesClient;
+    private final FlinkService flinkService;
+
+    public FlinkSessionJobReconciler(
+            KubernetesClient kubernetesClient,
+            FlinkService flinkService,
+            FlinkOperatorConfiguration operatorConfiguration) {
+        this.kubernetesClient = kubernetesClient;
+        this.flinkService = flinkService;
+        this.operatorConfiguration = operatorConfiguration;
+    }
+
+    @Override
+    public void reconcile(
+            FlinkSessionJob flinkSessionJob, Context context, Configuration defaultConfig)
+            throws Exception {
+
+        FlinkSessionJobSpec lastReconciledSpec =
+                flinkSessionJob.getStatus().getReconciliationStatus().getLastReconciledSpec();
+
+        if (lastReconciledSpec == null) {
+            deployFlinkJob(flinkSessionJob, defaultConfig);
+        }
+        // TODO reconcile other spec change.","[{'comment': 'We could at least log something here for the users for the time being', 'commenter': 'gyfora'}]"
112,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/FlinkSessionJobReconciler.java,"@@ -0,0 +1,125 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.reconciler;
+
+import org.apache.flink.api.common.JobID;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.kubernetes.operator.config.FlinkOperatorConfiguration;
+import org.apache.flink.kubernetes.operator.crd.FlinkDeployment;
+import org.apache.flink.kubernetes.operator.crd.FlinkSessionJob;
+import org.apache.flink.kubernetes.operator.crd.spec.FlinkSessionJobSpec;
+import org.apache.flink.kubernetes.operator.observer.JobManagerDeploymentStatus;
+import org.apache.flink.kubernetes.operator.service.FlinkService;
+import org.apache.flink.kubernetes.operator.utils.FlinkUtils;
+
+import io.fabric8.kubernetes.client.KubernetesClient;
+import io.javaoperatorsdk.operator.api.reconciler.Context;
+import io.javaoperatorsdk.operator.api.reconciler.DeleteControl;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.Optional;
+
+/** The reconciler for the {@link FlinkSessionJob}. */
+public class FlinkSessionJobReconciler implements Reconciler<FlinkSessionJob> {
+
+    private static final Logger LOG = LoggerFactory.getLogger(JobReconciler.class);
+
+    private final FlinkOperatorConfiguration operatorConfiguration;
+    private final KubernetesClient kubernetesClient;
+    private final FlinkService flinkService;
+
+    public FlinkSessionJobReconciler(
+            KubernetesClient kubernetesClient,
+            FlinkService flinkService,
+            FlinkOperatorConfiguration operatorConfiguration) {
+        this.kubernetesClient = kubernetesClient;
+        this.flinkService = flinkService;
+        this.operatorConfiguration = operatorConfiguration;
+    }
+
+    @Override
+    public void reconcile(
+            FlinkSessionJob flinkSessionJob, Context context, Configuration defaultConfig)
+            throws Exception {
+
+        FlinkSessionJobSpec lastReconciledSpec =
+                flinkSessionJob.getStatus().getReconciliationStatus().getLastReconciledSpec();
+
+        if (lastReconciledSpec == null) {
+            deployFlinkJob(flinkSessionJob, defaultConfig);
+        }
+        // TODO reconcile other spec change.
+    }
+
+    @Override
+    public DeleteControl cleanup(FlinkSessionJob sessionJob, Configuration defaultConfig) {
+        Optional<FlinkDeployment> flinkDepOptional = getFlinkDeployment(sessionJob);
+
+        if (flinkDepOptional.isPresent()) {
+            Configuration effectiveConfig =
+                    FlinkUtils.getEffectiveConfig(flinkDepOptional.get(), defaultConfig);
+            String jobID = sessionJob.getStatus().getJobStatus().getJobId();
+            if (jobID != null) {
+                try {
+                    flinkService.cancelSessionJob(JobID.fromHexString(jobID), effectiveConfig);
+                } catch (Exception e) {
+                    LOG.error(""Failed to cancel job."", e);
+                }
+            }
+        } else {
+            LOG.info(""Session cluster has disappeared."");","[{'comment': '`Session cluster deployment not available`', 'commenter': 'gyfora'}]"
112,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/FlinkService.java,"@@ -116,6 +136,82 @@ public void submitSessionCluster(FlinkDeployment deployment, Configuration conf)
         LOG.info(""Session cluster successfully deployed"");
     }
 
+    public void submitJobToSessionCluster(FlinkSessionJob sessionJob, Configuration conf)
+            throws Exception {
+        JarRunResponseBody jarRunResponseBody =
+                jarUpload(sessionJob, conf)
+                        .handle(
+                                (response, throwable) -> {
+                                    if (throwable != null) {
+                                        LOG.error(""Failed to upload user jar."", throwable);
+                                        throw new FlinkRuntimeException(throwable);
+                                    } else {
+                                        return jarRun(sessionJob, response, conf);
+                                    }
+                                })
+                        .get();
+
+        String jobID = jarRunResponseBody.getJobId().toHexString();
+        LOG.info(""Submitted job: {} to session cluster."", jobID);
+        sessionJob.getStatus().setJobStatus(JobStatus.builder().jobId(jobID).build());
+    }
+
+    private JarRunResponseBody jarRun(
+            FlinkSessionJob sessionJob, JarUploadResponseBody response, Configuration conf) {
+        final String jarId =
+                response.getFilename().substring(response.getFilename().lastIndexOf(""/"") + 1);
+        // we generate jobID in advance to help deduplicate job submission.
+        JobID jobID = new JobID();
+        try {
+            final RestClusterClient<String> clusterClient =","[{'comment': 'should use `try (RestClusterClient<String> clusterClient = ...) { } ..` pattern', 'commenter': 'gyfora'}]"
112,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/validation/InternalValidator.java,"@@ -17,18 +17,20 @@
 
 package org.apache.flink.kubernetes.operator.validation;
 
-import org.apache.flink.kubernetes.operator.crd.FlinkDeployment;
-
 import java.util.Optional;
 
-/** Validator for {@link FlinkDeployment} resources. */
-public interface FlinkDeploymentValidator {
+/**
+ * Validator for custom resources.
+ *
+ * @param <CR> The custom resource to be validated.
+ */
+public interface InternalValidator<CR> {","[{'comment': 'Maybe `FlinkResourceValidator` would be a vbetter name ', 'commenter': 'gyfora'}]"
112,helm/flink-operator/templates/flink-operator.yaml,"@@ -123,6 +127,12 @@ spec:
                 path: flink-conf.yaml
               - key: log4j-console.properties
                 path: log4j-console.properties
+        {{- if .Values.hostPath.create }}
+        - name: {{ .Values.hostPath.name }}
+          hostPath:
+            path: {{ .Values.hostPath.hostPath }}
+            type: DirectoryOrCreate
+        {{- end }}","[{'comment': 'This logic is minikube specific right? Maybe it would be better to allow the user to define a volume-mount directly. That way they can use hostPath / persistentvolumeclaim etc.', 'commenter': 'gyfora'}]"
112,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/FlinkService.java,"@@ -116,6 +136,83 @@ public void submitSessionCluster(FlinkDeployment deployment, Configuration conf)
         LOG.info(""Session cluster successfully deployed"");
     }
 
+    public void submitJobToSessionCluster(FlinkSessionJob sessionJob, Configuration conf)
+            throws Exception {
+        JarRunResponseBody jarRunResponseBody =
+                jarUpload(sessionJob, conf)
+                        .handle(
+                                (response, throwable) -> {
+                                    if (throwable != null) {
+                                        LOG.error(""Failed to upload user jar."", throwable);
+                                        throw new FlinkRuntimeException(throwable);
+                                    } else {
+                                        return jarRun(sessionJob, response, conf);
+                                    }
+                                })
+                        .get();
+
+        String jobID = jarRunResponseBody.getJobId().toHexString();
+        LOG.info(""Submitted job: {} to session cluster."", jobID);
+        sessionJob.getStatus().setJobStatus(JobStatus.builder().jobId(jobID).build());
+    }
+
+    private JarRunResponseBody jarRun(
+            FlinkSessionJob sessionJob, JarUploadResponseBody response, Configuration conf) {
+        final String jarId =
+                response.getFilename().substring(response.getFilename().lastIndexOf(""/"") + 1);
+        // we generate jobID in advance to help deduplicate job submission.
+        JobID jobID = new JobID();
+        try (final RestClusterClient<String> clusterClient =
+                (RestClusterClient<String>) getClusterClient(conf)) {
+            final JarRunHeaders headers = JarRunHeaders.getInstance();
+            final JarRunMessageParameters parameters = headers.getUnresolvedMessageParameters();
+            parameters.jarIdPathParameter.resolve(jarId);
+            final JobSpec job = sessionJob.getSpec().getJob();
+            final JarRunRequestBody runRequestBody =
+                    new JarRunRequestBody(
+                            job.getEntryClass(),
+                            null,
+                            job.getArgs() == null ? null : Arrays.asList(job.getArgs()),
+                            job.getParallelism() > 0 ? job.getParallelism() : null,
+                            jobID,
+                            null,
+                            sessionJob.getSpec().getJob().getInitialSavepointPath());
+            LOG.info(""Submitting job: {} to session cluster."", jobID.toHexString());
+            return clusterClient
+                    .sendRequest(headers, parameters, runRequestBody)
+                    .get(1, TimeUnit.MINUTES);
+        } catch (Exception e) {
+            LOG.error(""Failed to submit job to session cluster."", e);
+            throw new FlinkRuntimeException(e);
+        }
+    }
+
+    private CompletableFuture<JarUploadResponseBody> jarUpload(
+            FlinkSessionJob sessionJob, final Configuration conf) throws Exception {
+        Path path = jarResolver.resolve(sessionJob.getSpec().getJob().getJarURI());
+        JarUploadHeaders headers = JarUploadHeaders.getInstance();
+        String clusterId = sessionJob.getSpec().getClusterId();
+        String namespace = sessionJob.getMetadata().getNamespace();","[{'comment': 'nit: we can add some `final` modifiers to make codes nicer.', 'commenter': 'bgeng777'}, {'comment': 'Fixed', 'commenter': 'Aitozi'}, {'comment': ""I know it's a small detail but it's not a good practice to add a bunch of `final` modifiers inside methods @bgeng777 .\r\n\r\nYou can almost add these everywhere if you really wanted to but they are basically boilerplate that do not affect anything performance or otherwise."", 'commenter': 'gyfora'}, {'comment': ""We can leave this now if you want, but let's not use `final` modifiers inside methods unless there is a very good reason for it. Class fields of course are very different and those should be final when possible"", 'commenter': 'gyfora'}, {'comment': 'I see you have this change in your last commit, I think you probably should just revert/drop it', 'commenter': 'gyfora'}, {'comment': ""I'm also torn with it, with some other method have already use this pattern, so I also add here. But I also think it's fussy to add `final` to each local variable. \r\nIMO, the local final variable should introduce in some performance critical path, so I prefer to drop them in these method."", 'commenter': 'Aitozi'}, {'comment': 'Thanks for the clarification. Will consider it more carefully.', 'commenter': 'bgeng777'}]"
112,helm/flink-operator/values.yaml,"@@ -41,11 +41,23 @@ jobServiceAccount:
     ""helm.sh/resource-policy"": keep
   name: ""flink""
 
-hostPath:
+volumeMounts: |-
+  - name: flink-userlib
+    mountPath: /opt/flink/userlib
+
+volumes: |-
+  - name: flink-userlib
+    hostPath:
+      path: /tmp/flink/userlib
+      type: DirectoryOrCreate","[{'comment': ""we could call these `operatorVolumes` and `operatorVolumeMounts` to be a bit more specific. Later we need a more generic way to customize the operator and we can get rid of this but it's out of scope now.\r\n\r\nAlso we should not use strings here and I think the use should be able to specify them in proper yaml format:\r\n```\r\noperatorVolumeMounts:\r\n  - name: flink-userlib\r\n    mountPath: /opt/flink/userlib\r\noperatorVolumes:\r\n  - name: flink-userlib\r\n    hostPath:\r\n      path: /tmp/flink/userlib\r\n      type: DirectoryOrCreate\r\n```\r\n\r\nYou can then in the template use `toYaml` in the template. There should be some other examples already\r\n      "", 'commenter': 'gyfora'}, {'comment': 'Nice suggestion', 'commenter': 'Aitozi'}]"
112,helm/flink-operator/templates/flink-operator.yaml,"@@ -191,3 +183,20 @@ data:
   {{- index (.Values.flinkDefaultConfiguration) ""log4j-console.properties"" | nindent 4 -}}
 {{- end }}
 {{- end }}
+---
+{{- if .Values.pvc.create }}
+apiVersion: v1
+kind: PersistentVolumeClaim
+metadata:
+  name: {{ .Values.pvc.name }}
+  namespace: {{ .Release.Namespace }}
+  labels:
+    {{- include ""flink-operator.labels"" . | nindent 4 }}
+spec:
+  accessModes:
+    - ReadWriteOnce
+  volumeMode: Filesystem
+  resources:
+    requests:
+      storage: {{ .Values.pvc.size }}
+{{- end }}","[{'comment': 'what does this do? how does this relate to the volume mount  (`flink-userlib`) that you have specified in the values yaml?', 'commenter': 'gyfora'}]"
131,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/controller/FlinkDeploymentController.java,"@@ -104,9 +105,30 @@ public DeleteControl cleanup(FlinkDeployment flinkApp, Context context) {
 
     @Override
     public UpdateControl<FlinkDeployment> reconcile(FlinkDeployment flinkApp, Context context) {
-        FlinkDeployment originalCopy = ReconciliationUtils.clone(flinkApp);
         LOG.info(""Starting reconciliation"");
+        FlinkDeployment originalCopy = ReconciliationUtils.clone(flinkApp);
+        FlinkDeploymentSpec lastReconciledSpec =
+                flinkApp.getStatus().getReconciliationStatus().getLastReconciledSpec();
+        if (lastReconciledSpec != null) {
+            Configuration latestValidatedConfig =
+                    FlinkUtils.getEffectiveConfig(
+                            flinkApp.getMetadata(),
+                            lastReconciledSpec,
+                            defaultConfig.getFlinkConfig());
+            try {
+                observerFactory
+                        .getOrCreate(flinkApp)
+                        .observe(flinkApp, context, latestValidatedConfig);
+            } catch (DeploymentFailedException dfe) {
+                handleDeploymentFailed(flinkApp, dfe);
+                LOG.info(""Reconciliation successfully completed"");","[{'comment': 'This log is not right', 'commenter': 'Aitozi'}, {'comment': 'IIUC, when catching a DeploymentFailedException, our current code would log `""Reconciliation successfully completed""`.\r\nI agree we can improve this log msg.\r\n', 'commenter': 'bgeng777'}, {'comment': 'Oh, I ignored it', 'commenter': 'Aitozi'}]"
131,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/controller/FlinkDeploymentController.java,"@@ -104,9 +105,30 @@ public DeleteControl cleanup(FlinkDeployment flinkApp, Context context) {
 
     @Override
     public UpdateControl<FlinkDeployment> reconcile(FlinkDeployment flinkApp, Context context) {
-        FlinkDeployment originalCopy = ReconciliationUtils.clone(flinkApp);
         LOG.info(""Starting reconciliation"");
+        FlinkDeployment originalCopy = ReconciliationUtils.clone(flinkApp);
+        FlinkDeploymentSpec lastReconciledSpec =
+                flinkApp.getStatus().getReconciliationStatus().getLastReconciledSpec();
+        if (lastReconciledSpec != null) {
+            Configuration latestValidatedConfig =","[{'comment': 'nit: maybe called `lastValidatedConfig` better, because it from the `lastReconciledSpec`', 'commenter': 'Aitozi'}, {'comment': 'Agree. Fixed.', 'commenter': 'bgeng777'}]"
131,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/FlinkConfigBuilder.java,"@@ -50,13 +51,20 @@
 
 /** Builder to get effective flink config from {@link FlinkDeployment}. */
 public class FlinkConfigBuilder {
-    private final FlinkDeployment deploy;
+    private final ObjectMeta meta;
     private final FlinkDeploymentSpec spec;
     private final Configuration effectiveConfig;
 
     public FlinkConfigBuilder(FlinkDeployment deploy, Configuration flinkConfig) {
-        this.deploy = deploy;
-        this.spec = this.deploy.getSpec();
+        this.meta = deploy.getMetadata();
+        this.spec = deploy.getSpec();
+        this.effectiveConfig = new Configuration(flinkConfig);
+    }
+
+    public FlinkConfigBuilder(","[{'comment': 'Can we directly pass the namespace and clusterId here? It seems we only use these two fields', 'commenter': 'Aitozi'}, {'comment': ""I considered this solution as well. I am not sure if we will use other fields of the meta in the future. One example could be `annotations`: I notice that in our flink's `KubernetesConfigOptions`, there are option like `JOB_MANAGER_ANNOTATIONS` and `TASK_MANAGER_ANNOTATIONS`. If we have plan to support such fields in our cr, we may need to add back the `ObjectMeta`. WDYT?"", 'commenter': 'bgeng777'}, {'comment': 'Get it', 'commenter': 'Aitozi'}]"
131,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/FlinkConfigBuilder.java,"@@ -50,13 +51,20 @@
 
 /** Builder to get effective flink config from {@link FlinkDeployment}. */
 public class FlinkConfigBuilder {
-    private final FlinkDeployment deploy;
+    private final ObjectMeta meta;
     private final FlinkDeploymentSpec spec;
     private final Configuration effectiveConfig;
 
     public FlinkConfigBuilder(FlinkDeployment deploy, Configuration flinkConfig) {
-        this.deploy = deploy;
-        this.spec = this.deploy.getSpec();
+        this.meta = deploy.getMetadata();","[{'comment': 'this(deploy.getMetadata(), deploy.getSpec(), flinkConfig)', 'commenter': 'Aitozi'}, {'comment': 'Nice catch. Fixed.', 'commenter': 'bgeng777'}]"
131,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/controller/FlinkDeploymentController.java,"@@ -104,9 +105,31 @@ public DeleteControl cleanup(FlinkDeployment flinkApp, Context context) {
 
     @Override
     public UpdateControl<FlinkDeployment> reconcile(FlinkDeployment flinkApp, Context context) {
-        FlinkDeployment originalCopy = ReconciliationUtils.clone(flinkApp);
         LOG.info(""Starting reconciliation"");
+        FlinkDeployment originalCopy = ReconciliationUtils.clone(flinkApp);
+        FlinkDeploymentSpec lastReconciledSpec =
+                flinkApp.getStatus().getReconciliationStatus().getLastReconciledSpec();
+        if (lastReconciledSpec != null) {
+            Configuration lastValidatedConfig =
+                    FlinkUtils.getEffectiveConfig(
+                            flinkApp.getMetadata(),
+                            lastReconciledSpec,
+                            defaultConfig.getFlinkConfig());","[{'comment': 'Maybe this whole logic could be embedded inside the observer. \r\nThat would change it to simply `.observe(flinkApp, context)`\r\n\r\nIt would also simplify the controller loop which is very important', 'commenter': 'gyfora'}, {'comment': 'I think it also fits better into the Observer because using the correct config is actually part of the observation logic ', 'commenter': 'gyfora'}, {'comment': 'Moving the check and generation of `lastValidatedConfig` makes sense. It follows the pattern in our `reconciler#reconcile()` and makes codes much cleaner. \r\nRefactored.', 'commenter': 'bgeng777'}]"
131,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/controller/FlinkDeploymentController.java,"@@ -104,9 +105,31 @@ public DeleteControl cleanup(FlinkDeployment flinkApp, Context context) {
 
     @Override
     public UpdateControl<FlinkDeployment> reconcile(FlinkDeployment flinkApp, Context context) {
-        FlinkDeployment originalCopy = ReconciliationUtils.clone(flinkApp);
         LOG.info(""Starting reconciliation"");
+        FlinkDeployment originalCopy = ReconciliationUtils.clone(flinkApp);
+        FlinkDeploymentSpec lastReconciledSpec =
+                flinkApp.getStatus().getReconciliationStatus().getLastReconciledSpec();
+        if (lastReconciledSpec != null) {
+            Configuration lastValidatedConfig =
+                    FlinkUtils.getEffectiveConfig(
+                            flinkApp.getMetadata(),
+                            lastReconciledSpec,
+                            defaultConfig.getFlinkConfig());
+            try {
+                observerFactory
+                        .getOrCreate(flinkApp)
+                        .observe(flinkApp, context, lastValidatedConfig);
+            } catch (DeploymentFailedException dfe) {","[{'comment': 'We could also keep everything inside the original try-catch block and just do the validation there also. Maybe that will make it look nicer :) ', 'commenter': 'gyfora'}]"
131,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/observer/JobObserver.java,"@@ -46,14 +48,24 @@ public JobObserver(
     }
 
     @Override
-    public void observe(FlinkDeployment flinkApp, Context context, Configuration effectiveConfig) {
+    public void observe(FlinkDeployment flinkApp, Context context, Configuration defaultConfig) {","[{'comment': 'I suggest we not pass the defaultConfig to the `observe` method but pass the `DefaultConfig` to the Observer constructor directly. This way we can avoid accidentally using the wrong config.', 'commenter': 'gyfora'}]"
131,flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/controller/FlinkDeploymentControllerTest.java,"@@ -397,7 +397,45 @@ public void testUpgradeNotReadyCluster() {
         testUpgradeNotReadyCluster(appCluster, false);
     }
 
-    public void testUpgradeNotReadyCluster(FlinkDeployment appCluster, boolean allowUpgrade) {
+    @Test
+    public void verifyReconcileWithBadConfig() throws Exception {","[{'comment': 'This test does not cover that observer should use previous validated config, not the current invalid one. Right?', 'commenter': 'wangyang0918'}, {'comment': 'You are right. Current test only verifies the order of `observe()` and `validate()`.\r\nI enhanced the test to verify the previous validated config is really used by modifying the rest port.', 'commenter': 'bgeng777'}]"
131,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/controller/FlinkDeploymentController.java,"@@ -92,34 +92,33 @@ public FlinkDeploymentController(
     @Override
     public DeleteControl cleanup(FlinkDeployment flinkApp, Context context) {
         LOG.info(""Deleting FlinkDeployment"");
-        Configuration effectiveConfig =
-                FlinkUtils.getEffectiveConfig(flinkApp, defaultConfig.getFlinkConfig());
         try {
-            observerFactory.getOrCreate(flinkApp).observe(flinkApp, context, effectiveConfig);
+            observerFactory.getOrCreate(flinkApp).observe(flinkApp, context);
         } catch (DeploymentFailedException dfe) {
             // ignore during cleanup
         }
+        Configuration effectiveConfig =
+                FlinkUtils.getEffectiveConfig(flinkApp, defaultConfig.getFlinkConfig());
         return reconcilerFactory.getOrCreate(flinkApp).cleanup(flinkApp, effectiveConfig);
     }
 
     @Override
     public UpdateControl<FlinkDeployment> reconcile(FlinkDeployment flinkApp, Context context) {
-        FlinkDeployment originalCopy = ReconciliationUtils.clone(flinkApp);
         LOG.info(""Starting reconciliation"");
-
-        Optional<String> validationError = validator.validate(flinkApp);
-        if (validationError.isPresent()) {
-            LOG.error(""Validation failed: "" + validationError.get());
-            ReconciliationUtils.updateForReconciliationError(flinkApp, validationError.get());
-            return ReconciliationUtils.toUpdateControl(
-                    operatorConfiguration, originalCopy, flinkApp, false);
-        }
-
-        Configuration effectiveConfig =
-                FlinkUtils.getEffectiveConfig(flinkApp, defaultConfig.getFlinkConfig());
-
+        FlinkDeployment originalCopy = ReconciliationUtils.clone(flinkApp);
         try {
-            observerFactory.getOrCreate(flinkApp).observe(flinkApp, context, effectiveConfig);
+            observerFactory.getOrCreate(flinkApp).observe(flinkApp, context);
+
+            Configuration effectiveConfig =
+                    FlinkUtils.getEffectiveConfig(flinkApp, defaultConfig.getFlinkConfig());
+            Optional<String> validationError = validator.validate(flinkApp);","[{'comment': 'You changed the order of validation and getting the effective config. You first need to validate ', 'commenter': 'gyfora'}, {'comment': 'It would also be good to add a simple test to guard this by submitting a a config in the controllertest that would cause an exception in the getEffectiveConfig method.', 'commenter': 'gyfora'}, {'comment': 'Thanks for the catch. I restored the order and enhanced the test to cover the case that `getEffectiveConfig()` will throw exception if we do not validate() first.', 'commenter': 'bgeng777'}]"
131,flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/controller/FlinkDeploymentControllerTest.java,"@@ -397,7 +402,59 @@ public void testUpgradeNotReadyCluster() {
         testUpgradeNotReadyCluster(appCluster, false);
     }
 
-    public void testUpgradeNotReadyCluster(FlinkDeployment appCluster, boolean allowUpgrade) {
+    @Test
+    public void verifyReconcileWithBadConfig() {
+
+        FlinkDeployment appCluster = TestUtils.buildApplicationCluster();
+        UpdateControl<FlinkDeployment> updateControl;
+        // Override rest port, and it should be saved in lastReconciledSpec once a successful
+        // reconcile() finishes.
+        appCluster.getSpec().getFlinkConfiguration().put(RestOptions.PORT.key(), ""8088"");
+        updateControl = testController.reconcile(appCluster, context);
+        assertTrue(updateControl.isUpdateStatus());
+        assertEquals(
+                JobManagerDeploymentStatus.DEPLOYING,
+                appCluster.getStatus().getJobManagerDeploymentStatus());
+
+        // Check when the bad config is applied, observe() will change the cluster state correctly
+        appCluster.getSpec().getJobManager().setReplicas(-1);
+        // Next reconcile will set error msg and observe with previous validated config
+        updateControl = testController.reconcile(appCluster, context);
+        assertEquals(
+                ""JobManager replicas should not be configured less than one."",
+                appCluster.getStatus().getReconciliationStatus().getError());
+        assertTrue(updateControl.isUpdateStatus());
+        assertEquals(
+                JobManagerDeploymentStatus.DEPLOYED_NOT_READY,
+                appCluster.getStatus().getJobManagerDeploymentStatus());
+
+        // Check the exception
+        appCluster.getSpec().getJobManager().setReplicas(1);
+        appCluster.getSpec().getJob().setJarURI(null);
+        assertDoesNotThrow(
+                () -> {
+                    testController.reconcile(appCluster, context);
+                });
+        assertEquals(
+                JobManagerDeploymentStatus.READY,
+                appCluster.getStatus().getJobManagerDeploymentStatus());
+        // Verify the saved rest port in lastReconciledSpec is actually used in observe().
+        // Otherwise, the NaN rest port
+        // would lead to exception
+        appCluster.getSpec().getFlinkConfiguration().put(RestOptions.PORT.key(), ""NaN"");","[{'comment': 'I am afraid we still not verify the observer is using last reconciled config. I suggest to do it in the `TestingFlinkService#listJobs`. To achieve that, we could set a consumer of `TestingFlinkService`, just like `TestingClusterClient`.', 'commenter': 'wangyang0918'}]"
131,flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/controller/FlinkDeploymentControllerTest.java,"@@ -397,7 +402,59 @@ public void testUpgradeNotReadyCluster() {
         testUpgradeNotReadyCluster(appCluster, false);
     }
 
-    public void testUpgradeNotReadyCluster(FlinkDeployment appCluster, boolean allowUpgrade) {
+    @Test
+    public void verifyReconcileWithBadConfig() {
+
+        FlinkDeployment appCluster = TestUtils.buildApplicationCluster();
+        UpdateControl<FlinkDeployment> updateControl;
+        // Override rest port, and it should be saved in lastReconciledSpec once a successful
+        // reconcile() finishes.
+        appCluster.getSpec().getFlinkConfiguration().put(RestOptions.PORT.key(), ""8088"");
+        updateControl = testController.reconcile(appCluster, context);
+        assertTrue(updateControl.isUpdateStatus());
+        assertEquals(
+                JobManagerDeploymentStatus.DEPLOYING,
+                appCluster.getStatus().getJobManagerDeploymentStatus());
+
+        // Check when the bad config is applied, observe() will change the cluster state correctly
+        appCluster.getSpec().getJobManager().setReplicas(-1);
+        // Next reconcile will set error msg and observe with previous validated config
+        updateControl = testController.reconcile(appCluster, context);
+        assertEquals(
+                ""JobManager replicas should not be configured less than one."",
+                appCluster.getStatus().getReconciliationStatus().getError());
+        assertTrue(updateControl.isUpdateStatus());
+        assertEquals(
+                JobManagerDeploymentStatus.DEPLOYED_NOT_READY,
+                appCluster.getStatus().getJobManagerDeploymentStatus());
+
+        // Check the exception
+        appCluster.getSpec().getJobManager().setReplicas(1);
+        appCluster.getSpec().getJob().setJarURI(null);
+        assertDoesNotThrow(","[{'comment': ""What's the purpose of this assert?"", 'commenter': 'wangyang0918'}, {'comment': ""It is for @gyfora's latest [comment](https://github.com/apache/flink-kubernetes-operator/pull/131#discussion_r839643933) to make sure  we first do validation and then get the effective config."", 'commenter': 'bgeng777'}, {'comment': 'I think the test will also fail without `assertDoesNotThrow`. Right?', 'commenter': 'wangyang0918'}, {'comment': 'Not sure if I understand correctly: you are right that the `assertDoesNotThrow` is redundant here. \r\nThe idea of this part is that when `setJarURI(null)`, if we get effective config before validation, the test will fail with exception due to `getEffectiveConfig()`. But as long as it can successfully finish, we guarantee that validation is done before `getEffectiveConfig()`. \r\nI removed extra `assertDoesNotThrow`. Let me know if that is enough.', 'commenter': 'bgeng777'}]"
131,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/Mode.java,"@@ -19,13 +19,33 @@
 package org.apache.flink.kubernetes.operator.config;
 
 import org.apache.flink.kubernetes.operator.crd.FlinkDeployment;
+import org.apache.flink.kubernetes.operator.crd.spec.FlinkDeploymentSpec;
 
 /** The mode of {@link FlinkDeployment}. */
 public enum Mode {
     APPLICATION,
     SESSION;
 
+    /**
+     * Return the mode of the given FlinkDeployment for Observer and Reconciler. Note, switching
+     * mode for an existing deployment is not allowed.
+     *
+     * @param flinkApp given FlinkDeployment
+     * @return Mode
+     */
     public static Mode getMode(FlinkDeployment flinkApp) {
-        return flinkApp.getSpec().getJob() != null ? APPLICATION : SESSION;
+        // Try to use lastReconciledSpec if it exists.
+        // The mode derived from last-reconciled spec or current spec should be same.
+        // If they are different, observation phase will use last-reconciled spec and validation
+        // phase will fail.
+        FlinkDeploymentSpec lastReconciledSpec =
+                flinkApp.getStatus().getReconciliationStatus().getLastReconciledSpec();
+        return lastReconciledSpec == null
+                ? getMode(flinkApp.getSpec())
+                : getMode(flinkApp.getSpec());","[{'comment': 'should this be `getMode(lastReconciledSpec)` here ? Maybe something is wrong with the test you added :)', 'commenter': 'gyfora'}, {'comment': 'hmmm, I made a silly mistake here :(\r\nAfter fixing the Mode bug, I was wondering if there was any other spec needed to be changed to last reconciled spec in Observer. When testing with `DeploymentSpec spec = deployment.get().getSpec();` in `observeJmDeployment`, I somehow broke my previous codes and committed the wrong codes.\r\nIt should be fixed by now.', 'commenter': 'bgeng777'}, {'comment': 'no worries, we have code review for a reason :) ', 'commenter': 'gyfora'}]"
139,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/validation/DefaultDeploymentValidator.java,"@@ -145,10 +146,29 @@
         }
 
         Configuration configuration = Configuration.fromMap(confMap);
-        if (job.getUpgradeMode() == UpgradeMode.LAST_STATE
-                && !FlinkUtils.isKubernetesHAActivated(configuration)) {
-            return Optional.of(
-                    ""Job could not be upgraded with last-state while Kubernetes HA disabled"");
+        if (job.getUpgradeMode() == UpgradeMode.LAST_STATE) {
+            if (!FlinkUtils.isKubernetesHAActivated(configuration)) {
+                return Optional.of(
+                        ""Job could not be upgraded with last-state while Kubernetes HA disabled"");
+            } else if (!configuration.containsKey(
+                            ExecutionCheckpointingOptions.CHECKPOINTING_INTERVAL.key())
+                    || configuration
+                                    .get(ExecutionCheckpointingOptions.CHECKPOINTING_INTERVAL)
+                                    .compareTo(
+                                            configuration.get(
+                                                    ExecutionCheckpointingOptions
+                                                            .MIN_PAUSE_BETWEEN_CHECKPOINTS))","[{'comment': 'I dont really understant the point of this check. Is this necessary?', 'commenter': 'gyfora'}, {'comment': '@gyfora, this check refers to the logic in `CheckpointConfig#setCheckpointInterval` which compares to the `MINIMAL_CHECKPOINT_TIME`, therefore this check is necessary.\r\n@wangyang0918 WDYT?', 'commenter': 'SteNicholas'}]"
139,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/controller/FlinkDeploymentController.java,"@@ -114,6 +115,10 @@ public DeleteControl cleanup(FlinkDeployment flinkApp, Context context) {
                 ReconciliationUtils.updateForReconciliationError(flinkApp, validationError.get());
                 return ReconciliationUtils.toUpdateControl(
                         operatorConfiguration, originalCopy, flinkApp, false);
+            } else if (flinkApp.getSpec()
+                    .getFlinkConfiguration()
+                    .containsKey(ExecutionCheckpointingOptions.CHECKPOINTING_INTERVAL.key())) {
+                FlinkUtils.setCheckpointingInterval(originalCopy.getSpec().getFlinkConfiguration());","[{'comment': 'We should simply add this logic to the FlinkConfigBuilder instead of modifying the spec ', 'commenter': 'gyfora'}]"
139,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/FlinkConfigBuilder.java,"@@ -94,6 +97,17 @@ public FlinkConfigBuilder applyFlinkConfiguration() {
                     REST_SERVICE_EXPOSED_TYPE,
                     KubernetesConfigOptions.ServiceExposedType.ClusterIP);
         }
+
+        // With last-state upgrade mode, set the default value of 'execution.checkpointing.interval'
+        // to 5 minutes when HA is enabled.
+        if (spec.getJob() != null
+                && spec.getJob().getUpgradeMode() == UpgradeMode.LAST_STATE
+                && FlinkUtils.isKubernetesHAActivated(effectiveConfig)","[{'comment': 'The HA config is already validated for last-state mode by the DeploymentValidator, no need to check again here', 'commenter': 'gyfora'}]"
139,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/FlinkConfigBuilder.java,"@@ -94,6 +97,16 @@ public FlinkConfigBuilder applyFlinkConfiguration() {
                     REST_SERVICE_EXPOSED_TYPE,
                     KubernetesConfigOptions.ServiceExposedType.ClusterIP);
         }
+
+        // With last-state upgrade mode, set the default value of 'execution.checkpointing.interval'
+        // to 5 minutes when HA is enabled.
+        if (spec.getJob() != null
+                && spec.getJob().getUpgradeMode() == UpgradeMode.LAST_STATE
+                && !effectiveConfig.contains(
+                        ExecutionCheckpointingOptions.CHECKPOINTING_INTERVAL)) {
+            effectiveConfig.set(
+                    ExecutionCheckpointingOptions.CHECKPOINTING_INTERVAL, Duration.ofMinutes(5));","[{'comment': ""Let's move the 5 minutes into a constant in FlinkConfigBuilder and I think we are good to go :) "", 'commenter': 'gyfora'}]"
141,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/OperatorConfigOptions.java,"@@ -61,12 +62,18 @@
                     .durationType()
                     .defaultValue(Duration.ofSeconds(10))
                     .withDescription(
-                            ""The interval before a savepoint trigger attempt is marked as unsuccessful"");
+                            ""The interval before a savepoint trigger attempt is marked as unsuccessful."");
 
     public static final ConfigOption<Duration> OPERATOR_OBSERVER_FLINK_CLIENT_TIMEOUT =
             ConfigOptions.key(""operator.observer.flink.client.timeout"")
                     .durationType()
                     .defaultValue(Duration.ofSeconds(10))
                     .withDescription(
                             ""The timeout for the observer to wait the flink rest client to return."");
+
+    public static final ConfigOption<String> OPERATOR_DEPLOYMENT_VALIDATOR_TYPE =
+            ConfigOptions.key(""operator.deployment.validator.type"")","[{'comment': 'I think we should support a comma separated list of custom validators: `operator.validation.custom.validators: MyValidator1, MyValidator2`\r\nAnd maybe we could always have the default validator applied', 'commenter': 'gyfora'}, {'comment': 'This will also be impacted by @Aitozi s work on the session controller as the SessionJob will probably have a different validation logic. Maybe we should have one validator interface that validates all the different kinds of resources together, not sure.', 'commenter': 'gyfora'}, {'comment': ""Let's think about this design a little more , looping in @wangyang0918 as well. "", 'commenter': 'gyfora'}, {'comment': 'Yes, I think one common validator interface for different resources better, This have be done partly [here](https://github.com/apache/flink-kubernetes-operator/pull/112/files#diff-943f50a712d95a394ac0abe927611387251f0883c2653218a5c0cf64cd846410). \r\n\r\nMaybe we could provide two options to config the custom validators for session job and deployment then ? ', 'commenter': 'Aitozi'}, {'comment': 'I actually meant the opposite @Aitozi . I think maybe the validator interface should have one explicit method for each relevant resource type. \r\n\r\nvalidateDeployment, validateSessionJob\r\n\r\nThey could have a default empty implementation if we want. This might make it easier to implement custom validators.', 'commenter': 'gyfora'}, {'comment': 'Hah, I know your meaning now, I think it looks good to me. In this way, we can use one option to specify the custom validator', 'commenter': 'Aitozi'}, {'comment': 'It also has the benefit that the user knows what can be validated and what not :)', 'commenter': 'gyfora'}, {'comment': '+1 on the suggested changes by @gyfora ', 'commenter': 'morhidi'}, {'comment': '@gyfora, at present, the implementation has considered the comma separated list and get corresponding validator by the lower case values of the custom validators.', 'commenter': 'SteNicholas'}, {'comment': '? I dont understand sorry.  The configoption should use `.asList()` and allow the user to pass a list of identifiers.', 'commenter': 'gyfora'}, {'comment': ""@gyfora, Oh, sorry to misunderstood your points. But I have considered the above point and confused about the relationship between the validators. If user define two validators Validator1 & Validator2 and the condition is that Validator1 or Validator2, the support a comma separated list of custom validators couldn't cover this condition."", 'commenter': 'SteNicholas'}, {'comment': 'Let meg give you an example to clarify:\r\nThe user provides the list: `MyValidator1, MyValidator2`\r\n\r\nIn this case the validtation would be :\r\n 1. Validate using DefaultValidator -> if error, return\r\n 2. Validate using MyValidator1 -> if error, return\r\n 3. Validate using MyValidator2 -> if error, return\r\n \r\nIf all validators successfully validated, perform reconcile', 'commenter': 'gyfora'}, {'comment': 'Why do we need this config options? Just like Flink plugin mechanism, the name of each plugin(validator) could be simply same as the folder name.\r\n```\r\n/opt/flink-operator/plugins\r\n/opt/flink-operator/plugins/validator-1\r\n/opt/flink-operator/plugins/validator-2\r\n... ...\r\n``` ', 'commenter': 'wangyang0918'}, {'comment': '@wangyang0918 I think the config is still necessary to select from the available validators no?', 'commenter': 'gyfora'}, {'comment': 'So do you mean the scenario that user puts a validator-x in the plugins, but he/she does not want it taken effect?\r\n\r\nWhat I want to say is the validator will always take effect once it is put in the plugins. Same behavior with filesystem plugin.', 'commenter': 'wangyang0918'}, {'comment': 'Yea maybe it is better to enable the validators automatically if they are in the plugins. That will simplify a bunch of things, get rid of the identifier, config options etc.\r\n\r\nGood point', 'commenter': 'gyfora'}]"
141,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/ValidatorUtils.java,"@@ -0,0 +1,70 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.utils;
+
+import org.apache.flink.kubernetes.operator.validation.DefaultDeploymentValidator;
+import org.apache.flink.kubernetes.operator.validation.FlinkDeploymentValidator;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.LinkedList;
+import java.util.List;
+
+/** Validator utilities. */
+public final class ValidatorUtils {
+
+    private static final Logger LOG = LoggerFactory.getLogger(ValidatorUtils.class);
+
+    public static FlinkDeploymentValidator discoverValidator(
+            ClassLoader classLoader, String validatorIdentifier) {
+        final List<FlinkDeploymentValidator> validators = discoverValidators(classLoader);
+        return validators.stream()
+                .filter(
+                        v ->
+                                v.validatorIdentifier()
+                                        .toLowerCase()
+                                        .equals(validatorIdentifier.toLowerCase()))
+                .findFirst()
+                .orElse(DefaultDeploymentValidator.INSTANCE);","[{'comment': 'This should probably throw an error instead of falling back, the user needs to know', 'commenter': 'gyfora'}]"
141,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/validation/FlinkDeploymentValidator.java,"@@ -24,11 +24,20 @@
 /** Validator for {@link FlinkDeployment} resources. */
 public interface FlinkDeploymentValidator {
 
+    /**
+     * Returns a unique identifier among same validator interfaces.
+     *
+     * <p>For consistency, an identifier should be declared as one lower case word (e.g. {@code
+     * default}). If multiple validators exist for different versions, a version should be appended
+     * using ""-"" (e.g. {@code custom-1}).
+     */
+    String validatorIdentifier();","[{'comment': 'This could be simply called getIdentifier()', 'commenter': 'gyfora'}]"
141,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/validation/FlinkDeploymentValidator.java,"@@ -24,11 +24,20 @@
 /** Validator for {@link FlinkDeployment} resources. */
 public interface FlinkDeploymentValidator {","[{'comment': 'Maybe we could also add a method called `configure(Configuration operatorConf)` that could be very useful in practice', 'commenter': 'gyfora'}]"
141,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/ValidatorUtils.java,"@@ -0,0 +1,70 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.utils;
+
+import org.apache.flink.kubernetes.operator.validation.DefaultDeploymentValidator;
+import org.apache.flink.kubernetes.operator.validation.FlinkDeploymentValidator;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.LinkedList;
+import java.util.List;
+
+/** Validator utilities. */
+public final class ValidatorUtils {
+
+    private static final Logger LOG = LoggerFactory.getLogger(ValidatorUtils.class);
+
+    public static FlinkDeploymentValidator discoverValidator(
+            ClassLoader classLoader, String validatorIdentifier) {
+        final List<FlinkDeploymentValidator> validators = discoverValidators(classLoader);
+        return validators.stream()
+                .filter(
+                        v ->
+                                v.validatorIdentifier()
+                                        .toLowerCase()
+                                        .equals(validatorIdentifier.toLowerCase()))
+                .findFirst()
+                .orElse(DefaultDeploymentValidator.INSTANCE);
+    }
+
+    private static List<FlinkDeploymentValidator> discoverValidators(ClassLoader classLoader) {
+        final List<FlinkDeploymentValidator> result = new LinkedList<>();
+        ServiceLoaderUtils.load(FlinkDeploymentValidator.class, classLoader)","[{'comment': 'Actually I am wondering maybe it would be better to use the plugin mechanism here similar to how metrics reporters are loaded:\r\n\r\n```\r\n private static Iterator<MetricReporterFactory> getAllReporterFactories(@Nullable PluginManager pluginManager) {\r\n        Iterator<MetricReporterFactory> factoryIteratorSPI = ServiceLoader.load(MetricReporterFactory.class).iterator();\r\n        Iterator<MetricReporterFactory> factoryIteratorPlugins = pluginManager != null ? pluginManager.load(MetricReporterFactory.class) : Collections.emptyIterator();\r\n        return Iterators.concat(factoryIteratorPlugins, factoryIteratorSPI);\r\n    }\r\n```\r\n\r\n@wangyang0918 @morhidi wdyt?', 'commenter': 'gyfora'}, {'comment': 'I am in favor of reusing the Flink plugin mechanism to load validator. This could avoid potential class conflicts if it is a uber jar without shading.', 'commenter': 'wangyang0918'}]"
141,flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/utils/ValidatorUtilsTest.java,"@@ -0,0 +1,78 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.utils;
+
+import org.apache.flink.configuration.ConfigConstants;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.kubernetes.operator.TestUtils;
+import org.apache.flink.kubernetes.operator.validation.DefaultValidator;
+import org.apache.flink.kubernetes.operator.validation.FlinkResourceValidator;
+import org.apache.flink.kubernetes.operator.validation.TestValidator;
+import org.apache.flink.util.Preconditions;
+
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.rules.TemporaryFolder;
+
+import java.io.File;
+import java.io.IOException;
+import java.nio.file.Files;
+import java.nio.file.Paths;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Set;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+/** Test class for {@link ValidatorUtils}. */
+public class ValidatorUtilsTest {
+
+    @Rule public TemporaryFolder temporaryFolder = new TemporaryFolder();
+
+    private static final String VALIDATOR_NAME = ""test-validator"";
+    private static final String VALIDATOR_JAR = VALIDATOR_NAME + ""-test-jar.jar"";
+
+    @Test
+    public void testDiscoverValidators() throws IOException {
+        File validatorRootFolder = temporaryFolder.newFolder();
+        File testValidatorFolder = new File(validatorRootFolder, VALIDATOR_NAME);
+        Preconditions.checkState(testValidatorFolder.mkdirs());","[{'comment': 'Use assert instead of `Preconditions` in the tests.', 'commenter': 'wangyang0918'}]"
141,flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/utils/ValidatorUtilsTest.java,"@@ -0,0 +1,78 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.utils;
+
+import org.apache.flink.configuration.ConfigConstants;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.kubernetes.operator.TestUtils;
+import org.apache.flink.kubernetes.operator.validation.DefaultValidator;
+import org.apache.flink.kubernetes.operator.validation.FlinkResourceValidator;
+import org.apache.flink.kubernetes.operator.validation.TestValidator;
+import org.apache.flink.util.Preconditions;
+
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.rules.TemporaryFolder;
+
+import java.io.File;
+import java.io.IOException;
+import java.nio.file.Files;
+import java.nio.file.Paths;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Set;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+/** Test class for {@link ValidatorUtils}. */
+public class ValidatorUtilsTest {
+
+    @Rule public TemporaryFolder temporaryFolder = new TemporaryFolder();
+
+    private static final String VALIDATOR_NAME = ""test-validator"";
+    private static final String VALIDATOR_JAR = VALIDATOR_NAME + ""-test-jar.jar"";
+
+    @Test
+    public void testDiscoverValidators() throws IOException {
+        File validatorRootFolder = temporaryFolder.newFolder();
+        File testValidatorFolder = new File(validatorRootFolder, VALIDATOR_NAME);
+        Preconditions.checkState(testValidatorFolder.mkdirs());
+        File testValidatorJar = new File(""target"", VALIDATOR_JAR);
+        Preconditions.checkState(
+                testValidatorJar.exists(),
+                ""Unable to locate jar file for test: "" + testValidatorJar);
+        Files.copy(
+                testValidatorJar.toPath(),
+                Paths.get(testValidatorFolder.toString(), VALIDATOR_JAR));
+        Map<String, String> systemEnv = new HashMap<>(System.getenv());
+        systemEnv.put(ConfigConstants.ENV_FLINK_PLUGINS_DIR, validatorRootFolder.getPath());
+        TestUtils.setEnv(systemEnv);","[{'comment': 'We need to restore the old env after this test. Otherwise, we will have some strange unstable tests in the future.', 'commenter': 'wangyang0918'}]"
141,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/FlinkOperator.java,"@@ -75,10 +77,10 @@ public FlinkOperator(DefaultConfig defaultConfig) {
         this.configurationService = getConfigurationService(operatorConfiguration);
         this.operator = new Operator(client, configurationService);
         this.flinkService = new FlinkService(client, operatorConfiguration);
+        this.validators = ValidatorUtils.discoverValidators(defaultConfig.getFlinkConfig());","[{'comment': 'If will be great if we could log the discovered validator.', 'commenter': 'wangyang0918'}]"
141,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/validation/FlinkResourceValidator.java,"@@ -17,19 +17,20 @@
 
 package org.apache.flink.kubernetes.operator.validation;
 
+import org.apache.flink.core.plugin.Plugin;
 import org.apache.flink.kubernetes.operator.crd.FlinkDeployment;
 import org.apache.flink.kubernetes.operator.crd.FlinkSessionJob;
 
 import java.util.Optional;
 
 /** Validator for different resources. */
-public interface FlinkResourceValidator {
+public interface FlinkResourceValidator extends Plugin {
 
     /**
      * Validate and return optional error.
      *
-     * @param deployment
-     * @return Optional error string, should be present iff validation resulted in an error
+     * @param deployment A Flink application or session cluster deployment.
+     * @return Optional error string, should be present if validation resulted in an error","[{'comment': 'iff -> if, unnecessary change', 'commenter': 'wangyang0918'}]"
141,flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/utils/ValidatorUtilsTest.java,"@@ -0,0 +1,80 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.utils;
+
+import org.apache.flink.configuration.ConfigConstants;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.kubernetes.operator.TestUtils;
+import org.apache.flink.kubernetes.operator.validation.DefaultValidator;
+import org.apache.flink.kubernetes.operator.validation.FlinkResourceValidator;
+import org.apache.flink.kubernetes.operator.validation.TestValidator;
+
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.rules.TemporaryFolder;
+
+import java.io.File;
+import java.io.IOException;
+import java.nio.file.Files;
+import java.nio.file.Paths;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Set;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+/** Test class for {@link ValidatorUtils}. */
+public class ValidatorUtilsTest {
+
+    @Rule public TemporaryFolder temporaryFolder = new TemporaryFolder();
+
+    private static final String VALIDATOR_NAME = ""test-validator"";
+    private static final String VALIDATOR_JAR = VALIDATOR_NAME + ""-test-jar.jar"";
+
+    @Test
+    public void testDiscoverValidators() throws IOException {
+        File validatorRootFolder = temporaryFolder.newFolder();
+        File testValidatorFolder = new File(validatorRootFolder, VALIDATOR_NAME);
+        assertTrue(testValidatorFolder.mkdirs());
+        File testValidatorJar = new File(""target"", VALIDATOR_JAR);
+        assertTrue(testValidatorJar.exists());
+        Files.copy(
+                testValidatorJar.toPath(),
+                Paths.get(testValidatorFolder.toString(), VALIDATOR_JAR));
+        Map<String, String> originalEnv = System.getenv();
+        try {
+            Map<String, String> systemEnv = new HashMap<>(originalEnv);
+            systemEnv.put(ConfigConstants.ENV_FLINK_PLUGINS_DIR, validatorRootFolder.getPath());
+            TestUtils.setEnv(systemEnv);
+            Set<FlinkResourceValidator> validators =
+                    ValidatorUtils.discoverValidators(new Configuration());
+            assertEquals(2, validators.size());
+            validators.forEach(","[{'comment': '```\r\nassertEquals(\r\n        Arrays.asList(DefaultValidator.class.getName(), TestValidator.class.getName()),\r\n        validators.stream()\r\n                .map(v -> v.getClass().getName())\r\n                .collect(Collectors.toList()));\r\n```', 'commenter': 'wangyang0918'}]"
141,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/validation/FlinkResourceValidator.java,"@@ -17,18 +17,19 @@
 
 package org.apache.flink.kubernetes.operator.validation;
 
+import org.apache.flink.core.plugin.Plugin;
 import org.apache.flink.kubernetes.operator.crd.FlinkDeployment;
 import org.apache.flink.kubernetes.operator.crd.FlinkSessionJob;
 
 import java.util.Optional;
 
 /** Validator for different resources. */
-public interface FlinkResourceValidator {
+public interface FlinkResourceValidator extends Plugin {","[{'comment': 'I am not sure whether we need to introduce the `FlinkResourceValidatorFactory`, which actually extends the `Plugin` interface. The advantage is we do not need a default constructor for `FlinkResourceValidator`.\r\n\r\nAnother thing is we might not need to extend `Plugin` interface since we do not call `#configure()`. If some users implement the `#configure()`, it could not work.', 'commenter': 'wangyang0918'}, {'comment': '@wangyang0918, I have added the call of the `configure()` in `ValidatorUtils`. \r\nThe advantage of the introduction of the `FlinkResourceValidatorFactory ` is indeed that we do not need a default constructor for `FlinkResourceValidator`, but still need a default constructor for `FlinkResourceValidatorFactory`. In above comments, @gyfora mentioned that adds a method called `configure(Configuration operatorConf)` to `FlinkResourceValidator`. \r\nTherefore, IMO, the `FlinkResourceValidator` could extend the `Plugin` interface.', 'commenter': 'SteNicholas'}]"
141,flink-kubernetes-operator/src/main/resources/META-INF/services/org.apache.flink.kubernetes.operator.validation.FlinkResourceValidator,"@@ -0,0 +1,16 @@
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the ""License""); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+org.apache.flink.kubernetes.operator.validation.DefaultValidator","[{'comment': ""Wouldn't this cause the DefaultValidator to be loaded twice? I see we add it explicitly in the discoverValidators method"", 'commenter': 'gyfora'}]"
141,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/controller/FlinkSessionJobController.java,"@@ -102,11 +102,17 @@ public void init(FlinkControllerConfig<FlinkSessionJob> config) {
         LOG.info(""Starting reconciliation"");
         FlinkSessionJob originalCopy = ReconciliationUtils.clone(flinkSessionJob);
         observer.observe(flinkSessionJob, context);
-        Optional<String> validationError =
-                validator.validateSessionJob(
-                        flinkSessionJob,
-                        OperatorUtils.getSecondaryResource(
-                                flinkSessionJob, context, operatorConfiguration));
+        Optional<String> validationError = Optional.empty();
+        for (FlinkResourceValidator validator : validators) {
+            if ((validationError =
+                            validator.validateSessionJob(
+                                    flinkSessionJob,
+                                    OperatorUtils.getSecondaryResource(
+                                            flinkSessionJob, context, operatorConfiguration)))
+                    .isPresent()) {
+                break;
+            }
+        }
         if (validationError.isPresent()) {
             LOG.error(""Validation failed: "" + validationError.get());","[{'comment': 'Maybe we should move this to a method to keep the controller loop simple', 'commenter': 'gyfora'}]"
150,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/OperatorConfigOptions.java,"@@ -33,7 +33,7 @@
                     .durationType()
                     .defaultValue(Duration.ofSeconds(60))
                     .withDescription(
-                            ""The interval for the controller to reschedule the reconcile process"");
+                            ""The interval for the controller to reschedule the reconcile process."");","[{'comment': 'I have updated this in #141.', 'commenter': 'SteNicholas'}, {'comment': 'Thanks for the information. As we do not automatically generate documents for now, I will roll back this change and only keep it in our doc website.', 'commenter': 'bgeng777'}]"
150,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/OperatorConfigOptions.java,"@@ -61,7 +61,7 @@
                     .durationType()
                     .defaultValue(Duration.ofSeconds(10))
                     .withDescription(
-                            ""The interval before a savepoint trigger attempt is marked as unsuccessful"");
+                            ""The interval before a savepoint trigger attempt is marked as unsuccessful."");","[{'comment': 'Ditto.', 'commenter': 'SteNicholas'}]"
150,docs/content/docs/deployment/configuration.md,"@@ -0,0 +1,38 @@
+---","[{'comment': 'Could the document of configuration be generated by `OperatorConfigOptions`?', 'commenter': 'SteNicholas'}, {'comment': ""Not very easy to do unfortunately. flink-docs contains the annotaiton processing logic for flink but it's not flexible"", 'commenter': 'gyfora'}]"
153,docker-entrypoint.sh,"@@ -27,12 +27,12 @@ if [ ""$1"" = ""help"" ]; then
 elif [ ""$1"" = ""operator"" ]; then
     echo ""Starting Operator""
 
-    exec java -cp /$FLINK_KUBERNETES_SHADED_JAR:/$OPERATOR_JAR $LOG_CONFIG org.apache.flink.kubernetes.operator.FlinkOperator
+    exec java $JVM_ARGS -cp /$FLINK_KUBERNETES_SHADED_JAR:/$OPERATOR_JAR $LOG_CONFIG org.apache.flink.kubernetes.operator.FlinkOperator","[{'comment': 'I think we should put the JVM_ARGS after the log config, to give the user an opportunity of overwriting every setting', 'commenter': 'gyfora'}, {'comment': 'Good point, fixed', 'commenter': 'Aitozi'}]"
153,helm/flink-kubernetes-operator/values.yaml,"@@ -78,3 +78,8 @@ metrics:
 imagePullSecrets: []
 nameOverride: """"
 fullnameOverride: """"
+
+# Set the jvm start up options for webhook and operator
+jvmArgs:
+  webhook: """"
+  operator: """"","[{'comment': 'This looks pretty awkward. We should start structuring the helm options a bit more. \r\nI will probably open a ticket for this.', 'commenter': 'gyfora'}, {'comment': 'I also feel the helm option is a little mess now ðŸ˜„, do you have some suggestion for this ? Or let it be improved in your ticket?', 'commenter': 'Aitozi'}, {'comment': 'I will try to work on this in another ticket, we need to clean up many things . Itâ€™s ok as it is ', 'commenter': 'gyfora'}]"
155,docs/content/docs/operations/helm.md,"@@ -109,3 +109,24 @@ spec:
       skipCrds: true
 ...
 ```
+
+## Advanced customization techniques
+The Helm chart does not aim to provide configuration options for all the possible deployment scenarios of the Operator. There are use cases for injecting common tools and/or sidecars in most enterprise environments that simply cannot be covered by public Helm charts.
+
+Fortunately, [post rendering](https://helm.sh/docs/topics/advanced/#post-rendering) in Helm gives you the ability to manually manipulate manifests before they are installed on a Kubernetes cluster. This allows users to use tools like [kustomize](https://kustomize.io) to apply configuration changes without the need to fork public charts.
+
+The GitHub repository for the Operator contains a simple [example](https://github.com/apache/flink-kubernetes-operator/tree/main/examples/kustomize) on how to augment the Operator Deployment with an init container using `kustomize`.
+
+```yaml
+initContainers:
+  - name: busybox
+    image: busybox:latest
+    command: [ 'sh','-c','echo Happy Customization!' ]```
+```
+
+You can enable the example using the `--post-renderer` option when installing the Operator with Helm:
+
+```yaml","[{'comment': 'nit: shell', 'commenter': 'mbalassi'}, {'comment': 'fixed, thanks', 'commenter': 'morhidi'}]"
155,docs/content/docs/operations/helm.md,"@@ -109,3 +109,107 @@ spec:
       skipCrds: true
 ...
 ```
+
+## Advanced customization techniques
+The Helm chart does not aim to provide configuration options for all the possible deployment scenarios of the Operator. There are use cases for injecting common tools and/or sidecars in most enterprise environments that simply cannot be covered by public Helm charts.
+
+Fortunately, [post rendering](https://helm.sh/docs/topics/advanced/#post-rendering) in Helm gives you the ability to manually manipulate manifests before they are installed on a Kubernetes cluster. This allows users to use tools like [kustomize](https://kustomize.io) to apply configuration changes without the need to fork public charts.
+
+The GitHub repository for the Operator contains a simple [example](https://github.com/apache/flink-kubernetes-operator/tree/main/examples/kustomize) on how to augment the Operator Deployment with a [fluent-bit](https://docs.fluentbit.io/manual) sidecar container and adjust container resources using `kustomize`.
+
+The example demonstrates that we can still use a `values.yaml` file to override the default Helm values for changing the log configuration, for example:
+
+```yaml
+
+operatorConfiguration:
+  ...
+  log4j2.properties: |+
+    rootLogger.appenderRef.file.ref = LogFile
+    appender.file.name = LogFile
+    appender.file.type = File
+    appender.file.append = false
+    appender.file.fileName = ${sys:log.file}
+    appender.file.layout.type = PatternLayout
+    appender.file.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n
+
+jvmArgs:
+  webhook: ""-Dlog.file=/opt/flink/log/webhook.log -Xms256m -Xmx256m""
+  operator: ""-Dlog.file=/opt/flink/log/operator.log -Xms2048m -Xmx2048m""
+```
+
+But we cannot ingest our fluent-bit sidecar for example unless we patch the deployment using `kustomize`
+
+```yaml
+################################################################################
+#  Licensed to the Apache Software Foundation (ASF) under one
+#  or more contributor license agreements.  See the NOTICE file
+#  distributed with this work for additional information
+#  regarding copyright ownership.  The ASF licenses this file
+#  to you under the Apache License, Version 2.0 (the
+#  ""License""); you may not use this file except in compliance
+#  with the License.  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an ""AS IS"" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+# limitations under the License.
+################################################################################
+
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: not-important
+spec:
+  template:
+    spec:
+      containers:
+        - name: flink-kubernetes-operator
+          volumeMounts:
+            - name: flink-log
+              mountPath: /opt/flink/log
+          resources:
+            requests:
+              memory: ""2.5Gi""
+              cpu: ""1000m""
+            limits:
+              memory: ""2.5Gi""
+              cpu: ""2000m""
+        - name: flink-webhook
+          volumeMounts:
+            - name: flink-log
+              mountPath: /opt/flink/log
+          resources:
+            requests:
+              memory: ""0.5Gi""
+              cpu: ""200m""
+            limits:
+              memory: ""0.5Gi""
+              cpu: ""500m""
+        - name: fluentbit
+          image: fluent/fluent-bit:1.8.12
+          command: [ 'sh','-c','/fluent-bit/bin/fluent-bit -i tail -p path=/opt/flink/log/*.log -p multiline.parser=java -o stdout' ]","[{'comment': 'It seems that the `sh` is not available in the image `fluent/fluent-bit:1.8.12`.', 'commenter': 'wangyang0918'}, {'comment': 'thanks @wangyang0918 bit weird, it worked on my arm64 arch laptop, changed it to the debug version in the example, anyway, to be able to attach a terminal to it. PTAL', 'commenter': 'morhidi'}, {'comment': 'It is strange. I will have a try whether debug version could work.', 'commenter': 'wangyang0918'}, {'comment': 'The flink-kubernetes-operator could run normally now. But the `fluent` does not tail the logs correctly. Do I miss some thing?\r\n\r\n```\r\nwangyang-pc:flink-kubernetes-operator danrtsey.wy$ kubectl logs flink-kubernetes-operator-7c6b75b9bf-nrg9x -c fluentbit\r\nFluent Bit v1.8.12\r\n* Copyright (C) 2019-2021 The Fluent Bit Authors\r\n* Copyright (C) 2015-2018 Treasure Data\r\n* Fluent Bit is a CNCF sub-project under the umbrella of Fluentd\r\n* https://fluentbit.io\r\n\r\n[2022/04/07 07:54:58] [ info] [engine] started (pid=8)\r\n[2022/04/07 07:54:58] [ info] [storage] version=1.1.5, initializing...\r\n[2022/04/07 07:54:58] [ info] [storage] in-memory\r\n[2022/04/07 07:54:58] [ info] [storage] normal synchronization mode, checksum disabled, max_chunks_up=128\r\n[2022/04/07 07:54:58] [ info] [cmetrics] version=0.2.2\r\n[2022/04/07 07:54:58] [ info] [input:tail:tail.0] multiline core started\r\n[2022/04/07 07:54:58] [ info] [sp] stream processor started\r\n```', 'commenter': 'wangyang0918'}, {'comment': 'it takes a few seconds, let me adjust the flush interval', 'commenter': 'morhidi'}, {'comment': 'Could you share me some light why the `kustomize/values.yaml` does not take effect in my environment? Both the logging and the jvm args does not work for me.\r\n\r\n```\r\nhelm install flink-kubernetes-operator helm/flink-kubernetes-operator --post-renderer examples/kustomize/render\r\n```\r\n\r\nBTW, the changes in the `kustomize/patch.yaml` could take effect.', 'commenter': 'wangyang0918'}, {'comment': ""you have to reference it with -f values.yaml (it's just still the standard values override) it has noting to do with customise, I just put it to the example folder\r\n\r\n```helm install flink-kubernetes-operator helm/flink-kubernetes-operator -f examples/kustomize/values.yaml --post-renderer examples/kustomize/render```"", 'commenter': 'morhidi'}, {'comment': 'Aha. So `examples/kustomize/values.yaml` will not be merged with `helm/values.yaml`, but just replace. Right?', 'commenter': 'wangyang0918'}, {'comment': ""correct (actually it's an override in Helm too)"", 'commenter': 'morhidi'}, {'comment': 'bumped the fluent bit version to address the issue appears on latest macs \r\n\r\n```\r\nno matching manifest for linux/arm64/v8 in the manifest list entries.\r\n```', 'commenter': 'morhidi'}]"
160,flink-kubernetes-operator/src/main/resources-filtered/.flink-kubernetes-operator.version.properties,"@@ -0,0 +1,16 @@
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the ""License""); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+project.version=${project.version}","[{'comment': 'Could we also add some git commit information here and print them in the logs?', 'commenter': 'wangyang0918'}]"
160,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/EnvUtils.java,"@@ -68,4 +90,102 @@ public static String getRequired(String key) {
         }
         return value;
     }
+
+    /**
+     * Logs information about the environment, like code revision, current user, Java version, and
+     * JVM parameters.
+     *
+     * @param log The logger to log the information to.
+     * @param componentName The component name to mention in the log.
+     * @param commandLineArgs The arguments accompanying the starting the component.
+     */
+    public static void logEnvironmentInfo(
+            Logger log, String componentName, String[] commandLineArgs) {
+        if (log.isInfoEnabled()) {
+            EnvironmentInformation.RevisionInformation rev = getRevisionInformation();
+            String javaHome = System.getenv(""JAVA_HOME"");
+            String inheritedLogs = System.getenv(""FLINK_INHERITED_LOGS"");
+            String arch = System.getProperty(""os.arch"");
+            long maxHeapMegabytes = getMaxJvmHeapMemory() >>> 20;
+            if (inheritedLogs != null) {
+                log.info(
+                        ""--------------------------------------------------------------------------------"");
+                log.info("" Preconfiguration: "");
+                log.info(inheritedLogs);
+            }
+            log.info(
+                    ""--------------------------------------------------------------------------------"");
+            log.info(
+                    "" Starting ""
+                            + componentName
+                            + "" (Version: ""
+                            + getProjectVersion()
+                            + "", Flink Version: ""
+                            + getVersion()
+                            + "", Scala: ""
+                            + getScalaVersion()
+                            + "", ""
+                            + ""Rev:""
+                            + rev.commitId
+                            + "", ""
+                            + ""Date:""
+                            + rev.commitDate
+                            + "")"");
+            log.info("" OS current user: "" + System.getProperty(""user.name""));
+            log.info("" Current Hadoop/Kerberos user: "" + getHadoopUser());
+            log.info("" JVM: "" + getJvmVersion());
+            log.info("" Arch: "" + arch);
+            log.info("" Maximum heap size: "" + maxHeapMegabytes + "" MiBytes"");
+            log.info("" JAVA_HOME: "" + (javaHome == null ? ""(not set)"" : javaHome));
+            String hadoopVersionString = getHadoopVersionString();","[{'comment': 'We do not need the hadoop related information.', 'commenter': 'wangyang0918'}]"
160,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/EnvUtils.java,"@@ -68,4 +90,102 @@ public static String getRequired(String key) {
         }
         return value;
     }
+
+    /**
+     * Logs information about the environment, like code revision, current user, Java version, and
+     * JVM parameters.
+     *
+     * @param log The logger to log the information to.
+     * @param componentName The component name to mention in the log.
+     * @param commandLineArgs The arguments accompanying the starting the component.
+     */
+    public static void logEnvironmentInfo(
+            Logger log, String componentName, String[] commandLineArgs) {
+        if (log.isInfoEnabled()) {
+            EnvironmentInformation.RevisionInformation rev = getRevisionInformation();
+            String javaHome = System.getenv(""JAVA_HOME"");
+            String inheritedLogs = System.getenv(""FLINK_INHERITED_LOGS"");","[{'comment': 'We also do not need the `FLINK_INHERITED_LOGS` since we will never configure it.', 'commenter': 'wangyang0918'}]"
160,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/EnvUtils.java,"@@ -68,4 +90,102 @@ public static String getRequired(String key) {
         }
         return value;
     }
+
+    /**
+     * Logs information about the environment, like code revision, current user, Java version, and
+     * JVM parameters.
+     *
+     * @param log The logger to log the information to.
+     * @param componentName The component name to mention in the log.
+     * @param commandLineArgs The arguments accompanying the starting the component.
+     */
+    public static void logEnvironmentInfo(
+            Logger log, String componentName, String[] commandLineArgs) {
+        if (log.isInfoEnabled()) {
+            EnvironmentInformation.RevisionInformation rev = getRevisionInformation();","[{'comment': 'I am afraid we still could not get the git revision information correctly.', 'commenter': 'wangyang0918'}]"
160,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/EnvUtils.java,"@@ -68,4 +97,113 @@ public static String getRequired(String key) {
         }
         return value;
     }
+
+    /**
+     * Logs information about the environment, like code revision, current user, Java version, and
+     * JVM parameters.
+     *
+     * @param log The logger to log the information to.
+     * @param componentName The component name to mention in the log.
+     * @param commandLineArgs The arguments accompanying the starting the component.
+     */
+    public static void logEnvironmentInfo(
+            Logger log, String componentName, String[] commandLineArgs) {
+        if (log.isInfoEnabled()) {
+            Properties properties = new Properties();
+            try (InputStream propFile =
+                    EnvUtils.class.getClassLoader().getResourceAsStream(PROP_FILE)) {
+                if (propFile != null) {
+                    properties.load(propFile);
+                }
+            } catch (IOException e) {
+                LOG.info(
+                        ""Cannot determine code revision: Unable to read version property file.: {}"",
+                        e.getMessage());
+            }
+            String javaHome = System.getenv(""JAVA_HOME"");
+            String arch = System.getProperty(""os.arch"");
+            long maxHeapMegabytes = getMaxJvmHeapMemory() >>> 20;
+            log.info(
+                    ""--------------------------------------------------------------------------------"");
+            log.info(
+                    "" Starting ""
+                            + componentName
+                            + "" (Version: ""
+                            + getProperty(properties, ""project.version"", UNKNOWN)
+                            + "", Flink Version: ""
+                            + getVersion()
+                            + "", Scala: ""","[{'comment': 'We do not need to print the scala version. The `flink-kubernetes-operator` does not depends any scala classes and we have already excluded the scala dependencies. Also Flink will be scala free in the later version.', 'commenter': 'wangyang0918'}]"
160,flink-kubernetes-operator/pom.xml,"@@ -282,6 +293,35 @@ under the License.
                     </execution>
                 </executions>
             </plugin>
+            <plugin>
+                <!-- Description: https://github.com/git-commit-id/git-commit-id-maven-plugin
+                    Used to show the git ref when starting the jobManager. -->","[{'comment': '```suggestion\r\n                    Used to show the git ref when starting the Flink Kubernetes operator. -->\r\n```', 'commenter': 'wangyang0918'}]"
162,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/FlinkConfigBuilder.java,"@@ -241,7 +244,9 @@ private static void setResource(
                     isJM
                             ? KubernetesConfigOptions.JOB_MANAGER_CPU
                             : KubernetesConfigOptions.TASK_MANAGER_CPU;
-            effectiveConfig.setString(memoryConfigOption.key(), resource.getMemory());
+            if (!StringUtils.isNullOrWhitespaceOnly(resource.getMemory())) {","[{'comment': 'We should not allow empty settings, this should be validated instead', 'commenter': 'gyfora'}]"
162,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/FlinkConfigBuilder.java,"@@ -184,8 +184,11 @@ public FlinkConfigBuilder applyJobOrSessionSpec() throws URISyntaxException {
         if (spec.getJob() != null) {
             effectiveConfig.set(
                     DeploymentOptions.TARGET, KubernetesDeploymentTarget.APPLICATION.getName());
-            final URI uri = new URI(spec.getJob().getJarURI());
-            effectiveConfig.set(PipelineOptions.JARS, Collections.singletonList(uri.toString()));
+            if (spec.getJob().getJarURI() != null) {
+                effectiveConfig.set(
+                        PipelineOptions.JARS,
+                        Collections.singletonList(new URI(spec.getJob().getJarURI()).toString()));
+            }","[{'comment': ""What's the point of this change?"", 'commenter': 'gyfora'}]"
162,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/validation/DefaultValidator.java,"@@ -58,6 +58,7 @@
     @Override
     public Optional<String> validateDeployment(FlinkDeployment deployment) {
         FlinkDeploymentSpec spec = deployment.getSpec();
+        Map<String, String> effectiveConfig = FlinkUtils.getEffectiveConfig(deployment).toMap();","[{'comment': 'I think we cannot get the `getEffectiveConfig` here as that relies on the FlinkConfigBuilder logic which in turn relies on the validation. \r\n\r\nWe should imply get the defaultConfiguration I think.', 'commenter': 'gyfora'}]"
162,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/FlinkUtils.java,"@@ -61,6 +61,18 @@ public static DefaultConfig loadDefaultConfig() {
         return new DefaultConfig(operatorConfig, flinkConfig);
     }
 
+    public static Configuration getFlinkEffectiveConfig(FlinkDeployment flinkApp) {
+        try {
+            final Configuration effectiveConfig =
+                    FlinkConfigBuilder.buildFlinkFrom(
+                            flinkApp, loadConfiguration(EnvUtils.get(EnvUtils.ENV_FLINK_CONF_DIR)));","[{'comment': 'We should avoid loading the configuration again and again. Best would be to only do it once here or in the validtor', 'commenter': 'gyfora'}]"
162,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/validation/DefaultValidator.java,"@@ -58,6 +58,8 @@ public class DefaultValidator implements FlinkResourceValidator {
     @Override
     public Optional<String> validateDeployment(FlinkDeployment deployment) {
         FlinkDeploymentSpec spec = deployment.getSpec();
+        Map<String, String> effectiveConfig =
+                FlinkUtils.getFlinkEffectiveConfig(deployment).toMap();","[{'comment': 'I think this is still incorrect from a validation perspective ,we should simply make a copy from the default config and put all values.:\r\n\r\n```\r\nConfiguration conf = new Configuration(defaultConf)\r\nconf.putAll(spec.getFlinkConfiguration)\r\n```', 'commenter': 'gyfora'}]"
162,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/validation/DefaultValidator.java,"@@ -56,12 +57,17 @@ public class DefaultValidator implements FlinkResourceValidator {
     private static final Set<String> ALLOWED_LOG_CONF_KEYS =
             Set.of(Constants.CONFIG_FILE_LOG4J_NAME, Constants.CONFIG_FILE_LOGBACK_NAME);
 
-    private static Configuration defaultFlinkConf =
+    @VisibleForTesting
+    static Configuration defaultFlinkConf =
             FlinkUtils.loadConfiguration(EnvUtils.get(EnvUtils.ENV_FLINK_CONF_DIR));
 
     @Override
     public Optional<String> validateDeployment(FlinkDeployment deployment) {
         FlinkDeploymentSpec spec = deployment.getSpec();
+        Map<String, String> effectiveConfig = defaultFlinkConf.toMap();
+        if (spec.getFlinkConfiguration() != null) {
+            effectiveConfig.putAll(spec.getFlinkConfiguration());
+        }
         return firstPresent(
                 validateFlinkVersion(spec.getFlinkVersion()),
                 validateFlinkConfig(spec.getFlinkConfiguration()),","[{'comment': 'It looks like `spec.getFlinkConfiguration()` here should be replaced with `effectiveConfig` as well.', 'commenter': 'bgeng777'}, {'comment': ""@bgeng777, I don't think `spec.getFlinkConfiguration()` should be replaced with `effectiveConfig`. The validation of Flink config validates the user-defined configuration. Validating the `effectiveConfig` which merges the default config and user-defined config is unnecessary."", 'commenter': 'SteNicholas'}, {'comment': 'I agree with your claim of dividing configs into default config and user-defined config.\r\nMy point is that `validateFlinkConfig` actually is to verify if forbidden configs like `KubernetesConfigOptions.NAMESPACE` and `KubernetesConfigOptions.CLUSTER_ID` is defined. I think such validation should be applied to default config as well.', 'commenter': 'bgeng777'}]"
162,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/validation/DefaultValidator.java,"@@ -70,8 +76,8 @@ public Optional<String> validateDeployment(FlinkDeployment deployment) {
                         deployment.getMetadata().getName(),
                         deployment.getMetadata().getNamespace()),
                 validateLogConfig(spec.getLogConfiguration()),
-                validateJobSpec(spec.getJob(), spec.getFlinkConfiguration()),
-                validateJmSpec(spec.getJobManager(), spec.getFlinkConfiguration()),
+                validateJobSpec(spec.getJob(), effectiveConfig),
+                validateJmSpec(spec.getJobManager(), effectiveConfig),
                 validateTmSpec(spec.getTaskManager()),
                 validateSpecChange(deployment));","[{'comment': 'We should also use effective config in `validateSpecChange` to get `SAVEPOINT_DIRECTORY`.', 'commenter': 'wangyang0918'}]"
162,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/validation/DefaultValidator.java,"@@ -56,12 +57,17 @@ public class DefaultValidator implements FlinkResourceValidator {
     private static final Set<String> ALLOWED_LOG_CONF_KEYS =
             Set.of(Constants.CONFIG_FILE_LOG4J_NAME, Constants.CONFIG_FILE_LOGBACK_NAME);
 
-    private static Configuration defaultFlinkConf =
+    @VisibleForTesting
+    static Configuration defaultFlinkConf =","[{'comment': 'I think we could make `defaultFlinkConf` as internal private field and introduce two constructor. \r\n\r\n```\r\n    public DefaultValidator() {\r\n        this(FlinkUtils.loadConfiguration(EnvUtils.get(EnvUtils.ENV_FLINK_CONF_DIR)));\r\n    }\r\n\r\n    public DefaultValidator(Configuration defaultFlinkConf) {\r\n        this.defaultFlinkConf = defaultFlinkConf;\r\n    }\r\n```', 'commenter': 'wangyang0918'}]"
162,flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/validation/DefaultValidatorTest.java,"@@ -274,6 +277,29 @@ public void testValidation() {
         testSuccess(dep -> dep.getSpec().setFlinkVersion(FlinkVersion.v1_15));
     }
 
+    @Test
+    public void testValidationWithDefaultConfig() {","[{'comment': 'We could make `testSuccess` accepts a new created validator with default flink config. Then updating the env will be unnecessary since loading default flink config from env is not what we want to test.\r\n\r\n```\r\n    @Test\r\n    public void testValidationWithDefaultConfig() {\r\n        final Configuration defaultFlinkConfig = new Configuration();\r\n        defaultFlinkConfig.set(\r\n                HighAvailabilityOptions.HA_MODE,\r\n                KubernetesHaServicesFactory.class.getCanonicalName());\r\n        final DefaultValidator validatorWithDefaultConfig =\r\n                new DefaultValidator(defaultFlinkConfig);\r\n        testSuccess(\r\n                dep -> {\r\n                    dep.getSpec().setFlinkConfiguration(new HashMap<>());\r\n                    dep.getSpec().getJob().setUpgradeMode(UpgradeMode.LAST_STATE);\r\n                },\r\n                validatorWithDefaultConfig);\r\n    }\r\n```', 'commenter': 'wangyang0918'}]"
162,flink-kubernetes-webhook/src/test/java/org/apache/flink/kubernetes/operator/admission/AdmissionHandlerTest.java,"@@ -84,7 +86,11 @@ public void testHandleValidateRequestWithoutContent() {
     public void testHandleValidateRequestWithAdmissionReview() throws IOException {
         final EmbeddedChannel embeddedChannel = new EmbeddedChannel(admissionHandler);
         final FlinkDeployment flinkDeployment = new FlinkDeployment();
-        flinkDeployment.setSpec(new FlinkDeploymentSpec());
+        flinkDeployment.setMetadata(","[{'comment': 'Why we have this change?', 'commenter': 'wangyang0918'}]"
162,flink-kubernetes-operator/src/test/resources/test-validation/flink-conf.yaml,"@@ -0,0 +1,19 @@
+################################################################################","[{'comment': 'After the above changes, this file is useless.', 'commenter': 'wangyang0918'}]"
168,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/artifact/FileSystemBasedArtifactFetcher.java,"@@ -0,0 +1,50 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.artifact;
+
+import org.apache.flink.core.fs.FileSystem;
+
+import org.apache.commons.io.FileUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.File;
+
+/** Leverage the flink filesystem plugin to fetch the artifact. */
+public class FileSystemBasedArtifactFetcher implements ArtifactFetcher {
+
+    public static final Logger LOG = LoggerFactory.getLogger(FileSystemBasedArtifactFetcher.class);
+    public static final FileSystemBasedArtifactFetcher INSTANCE =
+            new FileSystemBasedArtifactFetcher();
+
+    @Override
+    public File fetch(String uri, File targetDir) throws Exception {
+        org.apache.flink.core.fs.Path source = new org.apache.flink.core.fs.Path(uri);
+        var start = System.currentTimeMillis();
+        FileSystem fileSystem = source.getFileSystem();
+        String fileName = source.getName();
+        File targetFile = new File(targetDir, fileName);
+        FileUtils.copyToFile(fileSystem.open(source), targetFile);","[{'comment': 'It seems that the opened stream is not closed correctly.', 'commenter': 'wangyang0918'}, {'comment': 'Nice catch, fixed', 'commenter': 'Aitozi'}]"
168,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/artifact/HttpArtifactFetcher.java,"@@ -0,0 +1,48 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.artifact;
+
+import org.apache.commons.io.FileUtils;
+import org.apache.commons.io.FilenameUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.File;
+import java.net.URL;
+
+/** Download the jar from the http resource. */
+public class HttpArtifactFetcher implements ArtifactFetcher {
+
+    public static final Logger LOG = LoggerFactory.getLogger(HttpArtifactFetcher.class);
+    public static final HttpArtifactFetcher INSTANCE = new HttpArtifactFetcher();
+
+    @Override
+    public File fetch(String uri, File targetDir) throws Exception {
+        var start = System.currentTimeMillis();
+        URL url = new URL(uri);
+        String fileName = FilenameUtils.getName(url.getFile());
+        File targetFile = new File(targetDir, fileName);
+        FileUtils.copyToFile(new URL(uri).openStream(), targetFile);","[{'comment': 'Same as above.', 'commenter': 'wangyang0918'}, {'comment': 'fixed', 'commenter': 'Aitozi'}]"
168,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/OperatorConfigOptions.java,"@@ -97,4 +97,10 @@ public class OperatorConfigOptions {
                     .withDescription(
                             ""The timeout for deployments to become ready/stable ""
                                     + ""before being rolled back if rollback is enabled."");
+
+    public static final ConfigOption<String> OPERATOR_USER_JAR_BASE_DIR =","[{'comment': 'We might need to update the docs.', 'commenter': 'wangyang0918'}, {'comment': 'Yes, But the CI seems not find it ðŸ˜„', 'commenter': 'Aitozi'}, {'comment': 'Does the option change have to be copy to `configuration.md` manually ? ', 'commenter': 'Aitozi'}, {'comment': 'Added', 'commenter': 'Aitozi'}, {'comment': 'Not in this PR, but we could improve to generate the document automatically just like in Flink.', 'commenter': 'wangyang0918'}, {'comment': 'OK, I have add the option to the `configuration.md`', 'commenter': 'Aitozi'}, {'comment': '> Not in this PR, but we could improve to generate the document automatically just like in Flink.\r\n\r\nCreated a ticket for this https://issues.apache.org/jira/browse/FLINK-27334', 'commenter': 'Aitozi'}]"
168,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/FlinkService.java,"@@ -204,12 +209,14 @@ private JarRunResponseBody runJar(
         }
     }
 
-    private JarUploadResponseBody uploadJar(FlinkSessionJob sessionJob, Configuration conf)
+    private JarUploadResponseBody uploadJar(
+            FlinkSessionJob sessionJob, FlinkDeployment sessionCluster, Configuration conf)
             throws Exception {
-        Path path = jarResolver.resolve(sessionJob.getSpec().getJob().getJarURI());
+        String targetDir = artifactManager.generateJarDir(sessionCluster, sessionJob);
+        File jarFile = artifactManager.fetch(sessionJob.getSpec().getJob().getJarURI(), targetDir);","[{'comment': 'If the user specified jar is a local file `file:///tmp/a.jar`, do we still need to copy it to `/opt/flink/artifacts`?', 'commenter': 'wangyang0918'}, {'comment': ""I also think about it, The good thing is that we can do no special thing for the local filesystem when `fetch` and `delete` (If no copy, we may can't delete after submit too). Do you think we need to handle the difference now ?"", 'commenter': 'Aitozi'}, {'comment': ""Let's keep the current behavior and do some improvements later if necessary."", 'commenter': 'wangyang0918'}]"
168,flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/artifact/ArtifactManagerTest.java,"@@ -0,0 +1,134 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.artifact;
+
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.kubernetes.operator.TestUtils;
+import org.apache.flink.kubernetes.operator.config.FlinkOperatorConfiguration;
+import org.apache.flink.kubernetes.operator.config.OperatorConfigOptions;
+import org.apache.flink.util.Preconditions;
+
+import com.sun.net.httpserver.HttpExchange;
+import com.sun.net.httpserver.HttpHandler;
+import com.sun.net.httpserver.HttpServer;
+import org.apache.commons.io.FileUtils;
+import org.junit.jupiter.api.Assertions;
+import org.junit.jupiter.api.BeforeEach;
+import org.junit.jupiter.api.Test;
+import org.junit.jupiter.api.io.TempDir;
+
+import java.io.File;
+import java.io.IOException;
+import java.net.HttpURLConnection;
+import java.net.InetSocketAddress;
+import java.net.URL;
+import java.nio.file.Path;
+
+/** Test for {@link ArtifactManager}. */
+public class ArtifactManagerTest {
+
+    @TempDir Path tempDir;
+    private ArtifactManager artifactManager;
+
+    @BeforeEach
+    public void setup() {
+        Configuration configuration = new Configuration();
+        configuration.setString(
+                OperatorConfigOptions.OPERATOR_USER_JAR_BASE_DIR,
+                tempDir.toAbsolutePath().toString());
+        artifactManager =
+                new ArtifactManager(FlinkOperatorConfiguration.fromConfiguration(configuration));
+    }
+
+    @Test
+    public void testGenerateJarDir() {
+        String baseDir =
+                artifactManager.generateJarDir(
+                        TestUtils.buildSessionCluster(), TestUtils.buildSessionJob());
+        String expected =
+                tempDir.toString()
+                        + File.separator
+                        + TestUtils.TEST_NAMESPACE
+                        + File.separator
+                        + TestUtils.TEST_DEPLOYMENT_NAME
+                        + File.separator
+                        + TestUtils.TEST_SESSION_JOB_NAME;
+        Assertions.assertEquals(expected, baseDir);
+    }
+
+    @Test
+    public void testFilesystemFetch() throws Exception {
+        var sourceFile = mockTheJarFile();
+        File file =
+                artifactManager.fetch(
+                        String.format(""file://%s"", sourceFile.getAbsolutePath()),
+                        tempDir.toString());
+        Assertions.assertTrue(file.exists());
+        Assertions.assertEquals(tempDir.toString(), file.getParentFile().toString());
+    }
+
+    @Test
+    public void testHttpFetch() throws Exception {
+        HttpServer httpServer = null;
+        try {
+            httpServer = HttpServer.create(new InetSocketAddress(1234), 0);","[{'comment': 'The test will be unstable when port `1234` is not available or running concurrently.', 'commenter': 'wangyang0918'}, {'comment': 'Fixed by start with retry', 'commenter': 'Aitozi'}]"
168,flink-kubernetes-operator/pom.xml,"@@ -135,6 +141,13 @@ under the License.
             <version>${fabric8.version}</version>
             <scope>test</scope>
         </dependency>
+
+        <dependency>
+            <groupId>org.apache.flink</groupId>
+            <artifactId>flink-test-utils-junit</artifactId>
+            <version>${flink.version}</version>
+            <scope>test</scope>
+        </dependency>","[{'comment': 'It seems that we do not need this dependency.', 'commenter': 'wangyang0918'}, {'comment': 'Nice catch, removed', 'commenter': 'Aitozi'}]"
168,flink-kubernetes-operator/pom.xml,"@@ -75,6 +75,12 @@ under the License.
             <version>${flink.version}</version>
         </dependency>
 
+        <dependency>","[{'comment': 'We might need to update the notice file when introducing new dependency.', 'commenter': 'wangyang0918'}, {'comment': 'The flink-core seems already included. \r\n\r\n```\r\n- org.apache.flink:flink-core:1.14.4\r\n```', 'commenter': 'Aitozi'}, {'comment': 'What I mean is the transitive dependencies? Do they have already included in the NOTICE file?', 'commenter': 'wangyang0918'}, {'comment': 'Since the dependency of `flink-clients` already include the `flink-core`, So I remove the duplicated declaration here', 'commenter': 'Aitozi'}]"
168,flink-kubernetes-operator/pom.xml,"@@ -270,6 +283,13 @@ under the License.
                             <version>${flink.version}</version>
                             <outputDirectory>${plugins.tmp.dir}/flink-metrics-statsd</outputDirectory>
                         </artifactItem>
+                        <artifactItem>","[{'comment': 'It is strange to only add the oss filesystem here. We need to bundle all the internal fs implementation or none.', 'commenter': 'wangyang0918'}, {'comment': 'As discussed in the mail list, I think we should avoid bundle all the fs jars It will make the image bigger and not necessary. I intend to add here as an example for the user to follow up the guide, But I think I can remove it now and clarify how to extend it in the doc.', 'commenter': 'Aitozi'}, {'comment': 'Removed', 'commenter': 'Aitozi'}, {'comment': 'Yes. We could remove it now and document it with `kustomize`.', 'commenter': 'wangyang0918'}, {'comment': 'Get it. I will add it in the followup doc ticket', 'commenter': 'Aitozi'}, {'comment': 'We just could add the document in FLINK-27270.', 'commenter': 'wangyang0918'}, {'comment': 'Yes, I marked on the description', 'commenter': 'Aitozi'}]"
168,docs/content/docs/operations/configuration.md,"@@ -62,3 +62,4 @@ To learn more about metrics and logging configuration please refer to the dedica
 | kubernetes.operator.observer.flink.client.timeout     |     10s    |  Duration    | The timeout for the observer to wait the flink rest client to return.            |
 | kubernetes.operator.reconciler.flink.cancel.job.timeout     |     1min    |  Duration    | The timeout for the reconciler to wait for flink to cancel job.            |
 | kubernetes.operator.reconciler.flink.cluster.shutdown.timeout     |     60s    |  Duration    | The timeout for the reconciler to wait for flink to shutdown cluster.           |
+| kubernetes.operator.user.artifacts.base.dir     |     60s    |  String |     The base dir to put the session job artifacts.           |","[{'comment': 'The default value is not `60s`.', 'commenter': 'wangyang0918'}]"
171,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/FlinkService.java,"@@ -225,7 +225,9 @@ public Optional<String> cancelJob(
     public void stopSessionCluster(
             FlinkDeployment deployment, Configuration conf, boolean deleteHaData) {
         FlinkUtils.deleteCluster(deployment, kubernetesClient, deleteHaData);
-        FlinkUtils.waitForClusterShutdown(kubernetesClient, conf);
+        if (!deleteHaData) {","[{'comment': 'Could we simply remove the `FlinkUtils.waitForClusterShutdown(kubernetesClient, conf)` here and call it directly in the `SessionReconciler#upgradeSessionCluster`?\r\n\r\nFurther more, we may do not need the `FlinkService#stopSessionCluster` and use `FlinkUtils.deleteCluster` instead.', 'commenter': 'wangyang0918'}, {'comment': 'FlinkService#stopSessionCluster has been removed', 'commenter': 'spoon-lz'}]"
171,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/FlinkService.java,"@@ -401,12 +400,6 @@ public Optional<String> cancelSessionJob(
         return savepointOpt;
     }
 
-    public void stopSessionCluster(","[{'comment': 'After you rebase the latest master, I am afraid that we still need the `stopSessionCluster`, which simply call the `FlinkUtils.deleteCluster` in the production code. And we could still keep the testing implementation.', 'commenter': 'wangyang0918'}, {'comment': '`stopSessionCluster` is back now', 'commenter': 'spoon-lz'}]"
178,Dockerfile,"@@ -23,21 +23,23 @@ WORKDIR /app
 ENV SHADED_DIR=flink-kubernetes-shaded
 ENV OPERATOR_DIR=flink-kubernetes-operator
 ENV WEBHOOK_DIR=flink-kubernetes-webhook
+ENV DOCS_DIR=flink-kubernetes-docs
 
 RUN mkdir $OPERATOR_DIR $WEBHOOK_DIR
 
 COPY pom.xml .
 COPY $SHADED_DIR/pom.xml ./$SHADED_DIR/
 COPY $WEBHOOK_DIR/pom.xml ./$WEBHOOK_DIR/
 COPY $OPERATOR_DIR/pom.xml ./$OPERATOR_DIR/
+COPY $DOCS_DIR/pom.xml ./$DOCS_DIR/","[{'comment': 'Do we really need to copy `flink-kubernetes-docs` when building the image?', 'commenter': 'wangyang0918'}, {'comment': '@wangyang0918, only the `pom.xml` of `flink-kubernetes-docs`is copied into Dockerfile, which is introduced as module in `pom.xml` of parent and need to copy.', 'commenter': 'SteNicholas'}]"
178,docs/content/docs/operations/configuration.md,"@@ -52,13 +52,4 @@ To learn more about metrics and logging configuration please refer to the dedica
 
 ## Operator Configuration Reference
 
-| Key  | Default | Type | Description |
-| ---- | ------- | ---- | ----------- |
-| kubernetes.operator.reconciler.reschedule.interval     |    60s     |  Duration    | The interval for the controller to reschedule the reconcile process.            |
-| kubernetes.operator.observer.rest-ready.delay    |  10s       |     Duration |     Final delay before deployment is marked ready after port becomes accessible.        |
-| kubernetes.operator.reconciler.max.parallelism     |     5    |  Integer    |    The maximum number of threads running the reconciliation loop. Use -1 for infinite.         |
-| kubernetes.operator.observer.progress-check.interval     |  10s       |  Duration    |     The interval for observing status for in-progress operations such as deployment and savepoints.        |
-| kubernetes.operator.observer.savepoint.trigger.grace-period     |     10s    |   Duration   |   The interval before a savepoint trigger attempt is marked as unsuccessful.          |
-| kubernetes.operator.observer.flink.client.timeout     |     10s    |  Duration    | The timeout for the observer to wait the flink rest client to return.            |
-| kubernetes.operator.reconciler.flink.cancel.job.timeout     |     1min    |  Duration    | The timeout for the reconciler to wait for flink to cancel job.            |
-| kubernetes.operator.reconciler.flink.cluster.shutdown.timeout     |     60s    |  Duration    | The timeout for the reconciler to wait for flink to shutdown cluster.           |
+{{< generated/kubernetes_operator_config_configuration >}}","[{'comment': 'Could you please verify locally to make sure `configuration` page is normal?', 'commenter': 'wangyang0918'}]"
178,flink-kubernetes-docs/README.md,"@@ -0,0 +1,35 @@
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+
+# Documentation generators
+
+This module contains generators that create HTML files directly from Flink Kubernetes Operator's source code.
+
+## Configuration documentation
+
+The `ConfigOptionsDocGenerator` can be used to generate a reference of `ConfigOptions`. By default, a separate file is generated for each `*Options` class found in `org.apache.flink.kubernetes.operator.docs.configuration`. The `@ConfigGroups` annotation can be used to generate multiple files from a single class.
+
+To integrate an `*Options` class from another package, add another module-package argument pair to `ConfigOptionsDocGenerator#LOCATIONS`.
+
+The files can be generated by running `mvn package -Dgenerate-config-docs -pl flink-kubernetes-docs -nsu -DskipTests`, and can be integrated into the documentation using `{{ include generated/<file-name> >}}`.","[{'comment': 'I believe we need to update the `ci.yml` to cover the new generated doc.', 'commenter': 'wangyang0918'}]"
178,flink-kubernetes-docs/pom.xml,"@@ -0,0 +1,157 @@
+<?xml version=""1.0"" encoding=""UTF-8""?>
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+  http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+<project xmlns=""http://maven.apache.org/POM/4.0.0""
+         xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
+         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
+    <modelVersion>4.0.0</modelVersion>
+
+    <parent>
+        <groupId>org.apache.flink</groupId>
+        <artifactId>flink-kubernetes-operator-parent</artifactId>
+        <version>1.0-SNAPSHOT</version>
+        <relativePath>..</relativePath>
+    </parent>
+
+    <artifactId>flink-kubernetes-docs</artifactId>
+    <name>Flink Kubernetes Docs</name>
+
+    <properties>
+        <generated.docs.dir>./docs/layouts/shortcodes/generated</generated.docs.dir>
+        <assertj.version>3.21.0</assertj.version>
+    </properties>
+
+    <dependencies>
+        <dependency>
+            <groupId>org.apache.flink</groupId>","[{'comment': 'This dependency could be provided.', 'commenter': 'wangyang0918'}]"
178,flink-kubernetes-docs/src/test/java/org/apache/flink/kubernetes/operator/docs/util/TestLoggerExtension.java,"@@ -0,0 +1,80 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.docs.util;","[{'comment': 'We do not need this class in current project.', 'commenter': 'wangyang0918'}]"
178,flink-kubernetes-docs/src/test/resources/META-INF/services/org.junit.jupiter.api.extension.Extension,"@@ -0,0 +1,16 @@
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the ""License""); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+org.apache.flink.kubernetes.operator.docs.util.TestLoggerExtension","[{'comment': 'We do not need this file.', 'commenter': 'wangyang0918'}]"
178,.github/workflows/ci.yml,"@@ -80,6 +80,11 @@ jobs:
           cd flink-kubernetes-webhook
           mvn verify -Dit.skip=false
           cd ..
+      - name: Tests in flink-kubernetes-docs","[{'comment': 'We already have `mvn clean install -DskipTests -Pgenerate-docs` in ci.yml to guard the docs are re-generated accordingly. What I mean is to simply add `-Dgenerate-config-docs` here.\r\nMaybe `-Pgenerate-config-docs` looks better.', 'commenter': 'wangyang0918'}, {'comment': 'Could we simply reuse the existing profile? ', 'commenter': 'gyfora'}, {'comment': 'It also makes sense to reuse the profile. It seems we do not need to differentiate generating CRD docs and config docs.', 'commenter': 'wangyang0918'}]"
178,flink-kubernetes-docs/README.md,"@@ -0,0 +1,35 @@
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+
+# Documentation generators
+
+This module contains generators that create HTML files directly from Flink Kubernetes Operator's source code.
+
+## Configuration documentation
+
+The `ConfigOptionsDocGenerator` can be used to generate a reference of `ConfigOptions`. By default, a separate file is generated for each `*Options` class found in `org.apache.flink.kubernetes.operator.docs.configuration`. The `@ConfigGroups` annotation can be used to generate multiple files from a single class.
+
+To integrate an `*Options` class from another package, add another module-package argument pair to `ConfigOptionsDocGenerator#LOCATIONS`.
+
+The files can be generated by running `mvn package -Dgenerate-docs -pl flink-kubernetes-docs -nsu -DskipTests`, and can be integrated into the documentation using `{{ include generated/<file-name> >}}`.","[{'comment': 'We need to update the docs here.', 'commenter': 'wangyang0918'}]"
187,flink-kubernetes-standalone-cluster/src/main/java/org/apache/flink/kubernetes/operator/standalone/StandaloneKubernetesConfigOptionsInternal.java,"@@ -0,0 +1,49 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.standalone;
+
+import org.apache.flink.configuration.ConfigOption;
+
+import static org.apache.flink.configuration.ConfigOptions.key;
+
+/**
+ * This class holds internal configuration constants used by flink operator when deploying flink
+ * clusters in standalone mode.
+ */
+public class StandaloneKubernetesConfigOptionsInternal {
+    public static final ConfigOption<Integer> KUBERNETES_TASKMANAGER_REPLICAS =
+            key(""kubernetes.internal.taskmanager.replicas"")
+                    .intType()
+                    .defaultValue(1)
+                    .withDescription(
+                            ""Specify how many pods will be in the TaskManager pool. For ""
+                                    + ""standalone kubernetes Flink sessions clusters."");
+
+    public static final ConfigOption<Boolean> APPLICATION_CLUSTER =
+            key(""kubernetes.internal.application.cluster"")
+                    .booleanType()
+                    .defaultValue(false)
+                    .withDescription(
+                            ""Specify whether the cluster will be deployed in application mode"");
+
+    public static final ConfigOption<String> APPLICATION_JAR_MAIN_CLASS =","[{'comment': 'Can we simply reuse the existing flink config option for this?', 'commenter': 'gyfora'}]"
187,flink-kubernetes-standalone-cluster/src/main/java/org/apache/flink/kubernetes/operator/standalone/StandaloneKubernetesConfigOptionsInternal.java,"@@ -0,0 +1,49 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.standalone;
+
+import org.apache.flink.configuration.ConfigOption;
+
+import static org.apache.flink.configuration.ConfigOptions.key;
+
+/**
+ * This class holds internal configuration constants used by flink operator when deploying flink
+ * clusters in standalone mode.
+ */
+public class StandaloneKubernetesConfigOptionsInternal {
+    public static final ConfigOption<Integer> KUBERNETES_TASKMANAGER_REPLICAS =
+            key(""kubernetes.internal.taskmanager.replicas"")
+                    .intType()
+                    .defaultValue(1)
+                    .withDescription(
+                            ""Specify how many pods will be in the TaskManager pool. For ""
+                                    + ""standalone kubernetes Flink sessions clusters."");
+
+    public static final ConfigOption<Boolean> APPLICATION_CLUSTER =","[{'comment': ""If not application then it's a session cluster? Are there other options? Maybe this should be an enum instead"", 'commenter': 'gyfora'}]"
187,flink-kubernetes-standalone-cluster/src/main/java/org/apache/flink/kubernetes/operator/kubeclient/decorators/UserLibMountDecorator.java,"@@ -0,0 +1,81 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.kubeclient.decorators;
+
+import org.apache.flink.kubernetes.kubeclient.FlinkPod;
+import org.apache.flink.kubernetes.kubeclient.decorators.AbstractKubernetesStepDecorator;
+import org.apache.flink.kubernetes.operator.kubeclient.parameters.StandaloneKubernetesJobManagerParameters;
+
+import io.fabric8.kubernetes.api.model.Container;
+import io.fabric8.kubernetes.api.model.ContainerBuilder;
+import io.fabric8.kubernetes.api.model.Pod;
+import io.fabric8.kubernetes.api.model.PodBuilder;
+import io.fabric8.kubernetes.api.model.Volume;
+import io.fabric8.kubernetes.api.model.VolumeBuilder;
+
+import static org.apache.flink.util.Preconditions.checkNotNull;
+
+/**
+ * Mount the Flink User Lib directory to enable Flink to pick up a Jars defined in
+ * pipeline.classpaths. Used for starting standalone application clusters
+ */
+public class UserLibMountDecorator extends AbstractKubernetesStepDecorator {
+    private static final String USER_LIB_VOLUME = ""user-lib-dir"";
+    private static final String USER_LIB_PATH = ""/opt/flink/usrlib"";","[{'comment': ""Wouldn't this simply come from the user provided image?"", 'commenter': 'gyfora'}, {'comment': ""This can come from user provided image but the images in the flink docker don't have this directory created. The reason it needs to be created is that if it isn't created then configuration defined user classpaths are ignored (https://github.com/apache/flink/blob/b904c948fb17f57b81a8e794cc67718c432bbcaa/flink-clients/src/main/java/org/apache/flink/client/program/DefaultPackagedProgramRetriever.java#L119)"", 'commenter': 'usamj'}]"
187,flink-kubernetes-standalone-cluster/pom.xml,"@@ -0,0 +1,87 @@
+<?xml version=""1.0"" encoding=""UTF-8""?>
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+  http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+<project xmlns=""http://maven.apache.org/POM/4.0.0""
+         xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
+         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
+    <modelVersion>4.0.0</modelVersion>
+
+
+    <parent>
+        <artifactId>flink-kubernetes-operator-parent</artifactId>
+        <groupId>org.apache.flink</groupId>
+        <version>1.0-SNAPSHOT</version>
+        <relativePath>..</relativePath>
+    </parent>
+
+
+
+    <artifactId>flink-kubernetes-standalone-cluster</artifactId>","[{'comment': 'Maybe a more suitable name would be simply `flink-kubernetes-standalone`', 'commenter': 'gyfora'}]"
187,flink-kubernetes-standalone-cluster/pom.xml,"@@ -0,0 +1,87 @@
+<?xml version=""1.0"" encoding=""UTF-8""?>
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+  http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+<project xmlns=""http://maven.apache.org/POM/4.0.0""
+         xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
+         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
+    <modelVersion>4.0.0</modelVersion>
+
+
+    <parent>
+        <artifactId>flink-kubernetes-operator-parent</artifactId>
+        <groupId>org.apache.flink</groupId>
+        <version>1.0-SNAPSHOT</version>
+        <relativePath>..</relativePath>
+    </parent>
+
+
+
+    <artifactId>flink-kubernetes-standalone-cluster</artifactId>
+    <name>Flink Kubernetes Standalone Kube Client</name>","[{'comment': 'I think we can simply name this `Flink Kubernetes Standalone`', 'commenter': 'gyfora'}]"
193,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/FlinkConfigBuilder.java,"@@ -223,6 +224,15 @@ protected FlinkConfigBuilder applyJobOrSessionSpec() throws URISyntaxException {
         return this;
     }
 
+    protected FlinkConfigBuilder applyAllowNonRestoredState() {
+        if (spec.getJob() != null) {","[{'comment': 'I think this could easily go into `applyJobOrSessionSpec`', 'commenter': 'gyfora'}, {'comment': 'Fixed', 'commenter': 'Aitozi'}]"
193,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/crd/spec/JobSpec.java,"@@ -65,4 +65,7 @@ public class JobSpec {
 
     /** Upgrade mode of the Flink job. */
     @EqualsAndHashCode.Exclude private UpgradeMode upgradeMode = UpgradeMode.STATELESS;
+
+    /** Allow checkpoint state that cannot be mapped to any job vertex in tasks. */
+    @EqualsAndHashCode.Exclude private boolean allowNonRestoredState = false;","[{'comment': 'I think we should not exclude this from the equals comparison', 'commenter': 'gyfora'}, {'comment': 'I think this option only affect the next upgrade behavior whether to recover from the unknown state mapping and do not have to trigger a direct upgrade after user change this. So I think it should be same with the upgradeMode.', 'commenter': 'Aitozi'}]"
193,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/FlinkConfigBuilder.java,"@@ -216,6 +217,9 @@ protected FlinkConfigBuilder applyJobOrSessionSpec() throws URISyntaxException {
                 effectiveConfig.set(
                         CoreOptions.DEFAULT_PARALLELISM, spec.getJob().getParallelism());
             }
+            if (spec.getJob().isAllowNonRestoredState()) {
+                effectiveConfig.set(SavepointConfigOptions.SAVEPOINT_IGNORE_UNCLAIMED_STATE, true);","[{'comment': ""I think we could make the `allowNonRestoredState` Boolean instead of boolean and set it into the config regardless of the value if it's not null. \r\n\r\nWith the current logic if the user defines true in the flinkConfiguration and sets it to false in the jobspec we would not use it."", 'commenter': 'gyfora'}, {'comment': 'we could also add a test for this to make sure the spec takes precedence', 'commenter': 'gyfora'}, {'comment': 'Good point, fixed', 'commenter': 'Aitozi'}]"
194,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/controller/FlinkDeploymentController.java,"@@ -169,4 +181,14 @@ private Optional<String> validateDeployment(FlinkDeployment deployment) {
         }
         return validationError;
     }
+
+    private void addMetricsIfAbsent(FlinkDeployment flinkApp) {","[{'comment': 'We could call this methid `getMetrics` and simply use it everywhere instead of manuall getting from the map. That would potentially avoid some freak race conditions and null pointers between adding and getting.', 'commenter': 'gyfora'}, {'comment': 'fixed', 'commenter': 'morhidi'}]"
194,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/metrics/FlinkDeploymentMetrics.java,"@@ -0,0 +1,68 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.metrics;
+
+import org.apache.flink.kubernetes.operator.crd.FlinkDeployment;
+import org.apache.flink.kubernetes.operator.crd.status.JobManagerDeploymentStatus;
+import org.apache.flink.metrics.MetricGroup;
+
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.Set;
+import java.util.concurrent.ConcurrentHashMap;
+
+/** FlinkDeployment metrics. */
+public class FlinkDeploymentMetrics {
+
+    private final Map<JobManagerDeploymentStatus, Set<String>> statuses = new HashMap<>();
+    public static final String NS_SCOPE_KEY = ""ns"";
+    public static final String METRIC_GROUP_NAME = ""FlinkDeployment"";
+
+    public FlinkDeploymentMetrics(MetricGroup parentMetricGroup) {
+        MetricGroup flinkDeploymentMetrics = parentMetricGroup.addGroup(METRIC_GROUP_NAME);
+        for (JobManagerDeploymentStatus status : JobManagerDeploymentStatus.values()) {
+            statuses.put(status, ConcurrentHashMap.newKeySet());
+        }
+        for (JobManagerDeploymentStatus status : JobManagerDeploymentStatus.values()) {
+            statuses.put(status, new HashSet<>());
+            MetricGroup metricGroup = flinkDeploymentMetrics.addGroup(status.toString());
+            metricGroup.gauge(""Count"", () -> statuses.get(status).size());
+        }
+        flinkDeploymentMetrics.gauge(
+                ""Count"", () -> statuses.values().stream().mapToInt(Set::size).sum());
+    }
+
+    public void update(FlinkDeployment flinkApp) {
+        remove(flinkApp);
+        statuses.get(flinkApp.getStatus().getJobManagerDeploymentStatus())
+                .add(flinkApp.getMetadata().getName());
+    }
+
+    public void remove(FlinkDeployment flinkApp) {
+        statuses.values()
+                .forEach(
+                        deployments -> {
+                            deployments.remove(flinkApp.getMetadata().getName());
+                        });
+    }
+
+    public boolean isEmpty() {
+        return statuses.values().stream().mapToInt(Set::size).sum() == 0;","[{'comment': 'If you store the Count gauge into a field you can simply read the value here and compare it to 0', 'commenter': 'gyfora'}, {'comment': ""That's a leftover, will clean it up."", 'commenter': 'morhidi'}]"
194,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/controller/FlinkSessionJobController.java,"@@ -266,4 +276,14 @@ private Optional<String> validateSessionJob(FlinkSessionJob sessionJob, Context
         }
         return validationError;
     }
+
+    private void addMetricsIfAbsent(FlinkSessionJob sessionJob) {","[{'comment': 'Same comment as I added to the deployment controller', 'commenter': 'gyfora'}, {'comment': 'fixed', 'commenter': 'morhidi'}]"
194,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/metrics/FlinkDeploymentMetrics.java,"@@ -0,0 +1,64 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.metrics;
+
+import org.apache.flink.kubernetes.operator.crd.FlinkDeployment;
+import org.apache.flink.kubernetes.operator.crd.status.JobManagerDeploymentStatus;
+import org.apache.flink.metrics.MetricGroup;
+
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.Set;
+import java.util.concurrent.ConcurrentHashMap;
+
+/** FlinkDeployment metrics. */
+public class FlinkDeploymentMetrics {","[{'comment': 'Could this extract a `MetricUtils` for the same metrics? The `FlinkDeploymentMetrics ` has certain same code for `FlinkSessionJobMetrics`.', 'commenter': 'SteNicholas'}, {'comment': ""Hi @SteNicholas, Thanks for your feedback. I'm planning to add deployment level metrics too, I'll likely refactor this part later a bit."", 'commenter': 'morhidi'}]"
194,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/metrics/FlinkSessionJobMetrics.java,"@@ -0,0 +1,45 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.metrics;
+
+import org.apache.flink.kubernetes.operator.crd.FlinkSessionJob;
+import org.apache.flink.metrics.MetricGroup;
+
+import java.util.Set;
+import java.util.concurrent.ConcurrentHashMap;
+
+/** FlinkSessionJob metrics. */
+public class FlinkSessionJobMetrics implements CustomResourceMetrics<FlinkSessionJob> {
+
+    private final Set<String> sessionJobs = ConcurrentHashMap.newKeySet();
+    public static final String METRIC_GROUP_NAME = ""FlinkSessionJob"";
+    public static final String NS_SCOPE_KEY = ""watchedns"";","[{'comment': 'We should not define this constant twice (they also mismatch at the moment)', 'commenter': 'gyfora'}, {'comment': ""Correct, that's a leftover from the refactor, give me a sec."", 'commenter': 'morhidi'}]"
197,e2e-tests/data/sessionjob-cr.yaml,"@@ -76,7 +76,7 @@ metadata:
 spec:
   deploymentName: session-cluster-1
   job:
-    jarURI: https://repo1.maven.org/maven2/org/apache/flink/flink-examples-streaming_2.12/1.14.3/flink-examples-streaming_2.12-1.14.3.jar
+    jarURI: https://repo1.maven.org/maven2/org/apache/flink/flink-examples-streaming_2.12/1.13.6/flink-examples-streaming_2.12-1.13.6.jar","[{'comment': 'I prefer to still use the 1.14 by default in the e2e tests.', 'commenter': 'wangyang0918'}]"
197,.github/workflows/ci.yml,"@@ -143,10 +150,13 @@ jobs:
           kubectl get pods
       - name: Run Flink e2e tests
         run: |
+          sed -i ""s/image: flink:.*/image: ${{ matrix.versions.image }}/"" e2e-tests/data/*.yaml
+          sed -i ""s/flinkVersion: .*/flinkVersion: ${{ matrix.versions.flinkVersion }}/"" e2e-tests/data/*.yaml
           ls e2e-tests/test_*.sh | while read script_test;do \","[{'comment': 'I would like to add a `git diff HEAD` here to verify that the `sed` is executed successfully.\r\n', 'commenter': 'wangyang0918'}]"
197,e2e-tests/data/flinkdep-cr.yaml,"@@ -22,8 +22,8 @@ metadata:
   namespace: default
   name: flink-example-statemachine
 spec:
-  image: flink:1.14.3
-  flinkVersion: v1_14
+  image: flink:1.
+  flinkVersion: v1_","[{'comment': 'It is strange that the e2e test does not fail with this typo.', 'commenter': 'wangyang0918'}]"
197,e2e-tests/data/sessionjob-cr.yaml,"@@ -22,8 +22,8 @@ metadata:
   namespace: default
   name: session-cluster-1
 spec:
-  image: flink:1.14.3
-  flinkVersion: v1_14
+  image: flink:1.","[{'comment': 'Same as above.', 'commenter': 'wangyang0918'}, {'comment': 'I got it. It is due to the `sed`.', 'commenter': 'wangyang0918'}]"
201,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/FlinkService.java,"@@ -192,7 +194,8 @@ private JarRunResponseBody runJar(
                             job.getParallelism() > 0 ? job.getParallelism() : null,
                             jobID,
                             job.getAllowNonRestoredState(),
-                            savepoint);
+                            savepoint,
+                            RestoreMode.DEFAULT);","[{'comment': 'Based on the CI we should only set the RestoreMode if version is newer than 1.14 otherwise set null', 'commenter': 'gyfora'}]"
201,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/FlinkService.java,"@@ -349,7 +352,8 @@ public void cancelJob(FlinkDeployment deployment, UpgradeMode upgradeMode) throw
                                             .stopWithSavepoint(
                                                     Preconditions.checkNotNull(jobId),
                                                     false,
-                                                    savepointDirectory)
+                                                    savepointDirectory,
+                                                    SavepointFormatType.DEFAULT)","[{'comment': 'Only set formatType if version newer than 1.14 otherwise set null', 'commenter': 'gyfora'}]"
201,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/FlinkService.java,"@@ -447,7 +451,11 @@ public Optional<String> cancelSessionJob(
                     try {
                         String savepoint =
                                 clusterClient
-                                        .stopWithSavepoint(jobID, false, savepointDirectory)
+                                        .stopWithSavepoint(
+                                                jobID,
+                                                false,
+                                                savepointDirectory,
+                                                SavepointFormatType.DEFAULT)","[{'comment': 'Only set formatType if version newer than 1.14 otherwise set null', 'commenter': 'gyfora'}]"
201,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/FlinkService.java,"@@ -491,7 +499,11 @@ public void triggerSavepoint(
                             .sendRequest(
                                     savepointTriggerHeaders,
                                     savepointTriggerMessageParameters,
-                                    new SavepointTriggerRequestBody(savepointDirectory, false))
+                                    new SavepointTriggerRequestBody(
+                                            savepointDirectory,
+                                            false,
+                                            SavepointFormatType.DEFAULT,","[{'comment': 'Only set formatType if version newer than 1.14 otherwise set null', 'commenter': 'gyfora'}]"
201,.github/workflows/ci.yml,"@@ -41,10 +41,10 @@ jobs:
       - name: Build with Maven
         run: |
           set -o pipefail; mvn clean install -Pgenerate-docs | tee ./mvn.log; set +o pipefail
-          if [[ $(grep -c ""overlapping classes"" ./mvn.log) -gt 0 ]];then
-            echo ""Found overlapping classes, please fix""
-            exit 1
-          fi
+          #if [[ $(grep -c ""overlapping classes"" ./mvn.log) -gt 0 ]];then
+          #  echo ""Found overlapping classes, please fix""
+          #  exit 1
+          #fi","[{'comment': 'It would be nice to specifically ignore only the errors we introduced by those 2 classes and not turn off this completely.', 'commenter': 'gyfora'}]"
201,.github/workflows/ci.yml,"@@ -123,7 +123,7 @@ jobs:
       - name: Build with Maven
         run: |
           set -o pipefail; mvn clean install -DskipTests | tee ./mvn.log; set +o pipefail
-          if [[ $(grep -c ""overlapping classes"" ./mvn.log) -gt 0 ]];then","[{'comment': 'I think overlapping classes are shown per jar. So this change might hide some other overlapping classes in different jars', 'commenter': 'gyfora'}]"
201,flink-kubernetes-operator/src/main/resources/META-INF/NOTICE,"@@ -70,25 +70,25 @@ This project bundles the following dependencies under the Apache Software Licens
 - org.apache.commons:commons-compress:1.21
 - org.apache.commons:commons-lang3:3.12.0
 - org.apache.commons:commons-math3:3.5
-- org.apache.flink:flink-annotations:1.14.4
-- org.apache.flink:flink-clients_2.12:1.14.4
-- org.apache.flink:flink-core:1.14.4
-- org.apache.flink:flink-file-sink-common:1.14.4
-- org.apache.flink:flink-hadoop-fs:1.14.4
-- org.apache.flink:flink-java:1.14.4
-- org.apache.flink:flink-metrics-core:1.14.4
-- org.apache.flink:flink-optimizer:1.14.4
-- org.apache.flink:flink-queryable-state-client-java:1.14.4
-- org.apache.flink:flink-rpc-akka-loader:1.14.4
-- org.apache.flink:flink-rpc-core:1.14.4
-- org.apache.flink:flink-runtime:1.14.4
-- org.apache.flink:flink-shaded-asm-7:7.1-14.0
-- org.apache.flink:flink-shaded-force-shading:14.0
-- org.apache.flink:flink-shaded-guava:30.1.1-jre-14.0
-- org.apache.flink:flink-shaded-jackson:2.12.4-14.0
-- org.apache.flink:flink-shaded-netty:4.1.65.Final-14.0
-- org.apache.flink:flink-shaded-zookeeper-3:3.4.14-14.0
-- org.apache.flink:flink-streaming-java_2.12:1.14.4
+- org.apache.flink:flink-annotations:1.15.0","[{'comment': 'We might need to review the bundled transitive dependencies since we bump the Flink version.', 'commenter': 'wangyang0918'}]"
201,.github/workflows/ci.yml,"@@ -42,8 +42,10 @@ jobs:
         run: |
           set -o pipefail; mvn clean install -Pgenerate-docs | tee ./mvn.log; set +o pipefail
           if [[ $(grep -c ""overlapping classes"" ./mvn.log) -gt 0 ]];then","[{'comment': 'Maybe we could count the overlapping classes like following.\r\n```\r\ncat ./mvn.log | grep -E -v \'flink-runtime-.*.jar, flink-kubernetes-operator-.*.jar define 2 overlapping classes\' | grep -c ""overlapping classes"" -\r\n```', 'commenter': 'wangyang0918'}, {'comment': 'I like this', 'commenter': 'gyfora'}]"
202,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/KubernetesOperatorSessionJobConfigOptions.java,"@@ -0,0 +1,44 @@
+package org.apache.flink.kubernetes.operator.config;
+
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.Map;
+
+/** List supported session job specific configurations */
+public enum KubernetesOperatorSessionJobConfigOptions {
+
+    SESSION_JOB_HTTP_JAR_HEADERS(""kubernetes.operator.user.artifacts.http.header"");","[{'comment': 'I think this all can be simply part of the `KuberenetesOperatorConfigOptions` no need for a new class', 'commenter': 'gyfora'}, {'comment': 'Hi @gyfora, I was thinking if we put this configuration at the `KuberenetesOperatorConfigOptions`, Does that imply you intend to put this header config field as a candidate in `flink-conf.yaml`? In that case, we are assuming the headers are the same for all session jobs within the cluster? I think maybe we can allow users to config different headers for different session jobs within one session cluster.', 'commenter': 'FuyaoLi2017'}, {'comment': ""Hi @gyfora If not, you also intend to keep this new `flinkConfiguration` as job specific. How do we control the behavior in the session cluster since it could have multiple different jobs?\r\nTaking a step back, we can assume the headers are all the same for the same cluster, that will make things simplier, but might NOT be very flexible.\r\ncc @wangyang0918 \r\nSee Yang's comment.\r\nhttps://issues.apache.org/jira/browse/FLINK-27483?focusedCommentId=17532137&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17532137"", 'commenter': 'FuyaoLi2017'}, {'comment': 'I am not suggesting to change the logic, just the location of the constant you defined to not create unnecessary new classes and spread the options too much :)\r\n\r\nThe operator should always use the config defined in the resource (session job/deployment) no difference. We usually apply this logic on top of the default config defined in the operator.', 'commenter': 'gyfora'}, {'comment': 'But I would allow users to put the header conf in the default Flink conf yaml to have some default value applied in case the session job doesnâ€™t want to ovverride it ', 'commenter': 'gyfora'}, {'comment': 'We just need to document clearly which configuration options could take effect in `FlinkSessionJob.spec. flinkConfiguration`.', 'commenter': 'wangyang0918'}]"
202,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/crd/spec/FlinkSessionJobSpec.java,"@@ -38,4 +40,7 @@ public class FlinkSessionJobSpec extends AbstractFlinkSpec {
 
     /** The name of the target session cluster deployment. */
     private String deploymentName;
+
+    /** Session job specific configuration */
+    private Map<String, String> sessionJobFlinkConfiguration;","[{'comment': 'Instead of specificng a new config here, lets move the `flinkConfiguration` field of the `FlinkDeploymentSpec` up to `AbstractFlinkSpec` that way it will be inherited', 'commenter': 'gyfora'}]"
202,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/crd/spec/AbstractFlinkSpec.java,"@@ -40,4 +42,7 @@ public abstract class AbstractFlinkSpec {
      * restart, change the number to anything other than the current value.
      */
     private Long restartNonce;
+
+    /** Flink configuration overrides for the Flink deployment. */","[{'comment': 'nit: the comment should be update: overrides for the Flink deployment or session job.', 'commenter': 'Aitozi'}]"
202,docs/content/docs/operations/configuration.md,"@@ -53,3 +53,11 @@ To learn more about metrics and logging configuration please refer to the dedica
 ## Operator Configuration Reference
 
 {{< generated/kubernetes_operator_config_configuration >}}
+
+## Job Specific Configuration Reference
+Job specific configuration can be configured under `spec.job.flinkConfiguration` and it will override flink configurations defined in `flink-conf.yaml`.","[{'comment': 'do you mean `spec.flinkConfiguration` ?', 'commenter': 'Aitozi'}]"
202,docs/content/docs/operations/configuration.md,"@@ -53,3 +53,11 @@ To learn more about metrics and logging configuration please refer to the dedica
 ## Operator Configuration Reference
 
 {{< generated/kubernetes_operator_config_configuration >}}
+
+## Job Specific Configuration Reference
+Job specific configuration can be configured under `spec.job.flinkConfiguration` and it will override flink configurations defined in `flink-conf.yaml`.
+
+- For application clusters, `spec.job.flinkConfiguration` will be located in `FlinkDeployment` CustomResource.","[{'comment': 'ditto', 'commenter': 'Aitozi'}]"
202,docs/content/docs/operations/configuration.md,"@@ -53,3 +53,11 @@ To learn more about metrics and logging configuration please refer to the dedica
 ## Operator Configuration Reference
 
 {{< generated/kubernetes_operator_config_configuration >}}
+
+## Job Specific Configuration Reference
+Job specific configuration can be configured under `spec.job.flinkConfiguration` and it will override flink configurations defined in `flink-conf.yaml`.
+
+- For application clusters, `spec.job.flinkConfiguration` will be located in `FlinkDeployment` CustomResource.
+- For session clusters, configuring `spec.job.flinkConfiguration` in parent `FlinkDeployment` will be applied to all session jobs within the session cluster.
+You can also configure `spec.job.flinkConfiguration` in `FlinkSessionJob` CustomResource for a specific session job. ","[{'comment': 'A newline here ', 'commenter': 'Aitozi'}, {'comment': 'I added a new line at the end.', 'commenter': 'FuyaoLi2017'}]"
202,docs/layouts/shortcodes/generated/kubernetes_operator_config_configuration.html,"@@ -110,5 +110,11 @@
             <td>String</td>
             <td>The base dir to put the session job artifacts.</td>
         </tr>
+        <tr>
+            <td><h5>kubernetes.operator.user.artifacts.http.header</h5></td>
+            <td style=""word-wrap: break-word;"">(none)</td>
+            <td>Map</td>
+            <td>Custom HTTP header for a Flink job. If configured in cluster level, headers will be applied to all jobs within the cluster. This field can also be configured under spec.job.flinkConfiguration for a specific session job within a session cluster. If configured at session job level, it will override the cluster level configuration. Expected format: headerKey1:headerValue1,headerKey2:headerValue2.</td>","[{'comment': 'ditto', 'commenter': 'Aitozi'}, {'comment': ""This is a generated html page. I guess we don't need to touch it."", 'commenter': 'FuyaoLi2017'}]"
202,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/artifact/HttpArtifactFetcher.java,"@@ -32,12 +37,24 @@ public class HttpArtifactFetcher implements ArtifactFetcher {
     public static final HttpArtifactFetcher INSTANCE = new HttpArtifactFetcher();
 
     @Override
-    public File fetch(String uri, File targetDir) throws Exception {
+    public File fetch(String uri, Configuration flinkConfiguration, File targetDir)
+            throws Exception {
         var start = System.currentTimeMillis();
         URL url = new URL(uri);
+        HttpURLConnection conn = (HttpURLConnection) url.openConnection();
+
+        Map<String, String> clusterLevelHeader =","[{'comment': 'we do not know this header is cluster level or job level here, what about naming it `headers`  directly ?', 'commenter': 'Aitozi'}]"
202,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/sessionjob/FlinkSessionJobReconciler.java,"@@ -83,6 +84,13 @@ public void reconcile(FlinkSessionJob flinkSessionJob, Context context) throws E
 
         Configuration deployedConfig = configManager.getObserveConfig(flinkDepOptional.get());
 
+        // merge session job specific config
+        Map<String, String> sessionJobFlinkConfiguration =","[{'comment': 'nit: maybe we could move this to the `FlinkConfigManager` ', 'commenter': 'Aitozi'}, {'comment': 'I created a `getSessionJobObserveConfig` method in `FlinkConfigManager` and replace this one and a few other places.', 'commenter': 'FuyaoLi2017'}]"
202,docs/content/docs/operations/configuration.md,"@@ -53,3 +53,11 @@ To learn more about metrics and logging configuration please refer to the dedica
 ## Operator Configuration Reference
 
 {{< generated/kubernetes_operator_config_configuration >}}
+
+## Job Specific Configuration Reference
+Job specific configuration can be configured under `spec.job.flinkConfiguration` and it will override flink configurations defined in `flink-conf.yaml`.
+
+- For application clusters, `spec.job.flinkConfiguration` will be located in `FlinkDeployment` CustomResource.
+- For session clusters, configuring `spec.job.flinkConfiguration` in parent `FlinkDeployment` will be applied to all session jobs within the session cluster.
+You can also configure `spec.job.flinkConfiguration` in `FlinkSessionJob` CustomResource for a specific session job. 
+The session job level configuration will override the parent session cluster's Flink configuration.","[{'comment': ""I think we should let user know that the cluster level's flink config will not overridden by the session job's flinkConfiguration actually (I mean the session cluster's config). Personally, I think what sessionjob`spec.flinkConfiguration` defined, is not the **flink** Configuration, just some custom configs. "", 'commenter': 'Aitozi'}, {'comment': ""If we can enumerate the possible configuration options and keys we can simply add a method to the validator that checks that the user doesn't try to set something that won't take effect"", 'commenter': 'gyfora'}, {'comment': 'Added related docs and validator code changes.', 'commenter': 'FuyaoLi2017'}]"
202,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/KubernetesOperatorConfigOptions.java,"@@ -141,4 +142,15 @@ public class KubernetesOperatorConfigOptions {
                     .noDefaultValue()
                     .withDescription(
                             ""Whether to enable recovery of missing/deleted jobmanager deployments. False by default for Flink 1.14, true for newer Flink version."");
+
+    public static final ConfigOption<Map<String, String>> JAR_ARTIFACT_HTTP_HEADER =
+            ConfigOptions.key(""kubernetes.operator.user.artifacts.http.header"")
+                    .mapType()
+                    .noDefaultValue()
+                    .withDescription(
+                            ""Custom HTTP header for a Flink job. If configured in cluster level, headers will be applied to all jobs within""","[{'comment': ""I think the description should detail what the config is good for. Does it affect artifact downloading?\r\n\r\nIt doesn't need to explain how configurations work, we have general docs for that. So I would completely remove this part:\r\n```\r\nIf configured in cluster level, headers will be applied to all jobs within the cluster. This field can also be configured under spec.job.flinkConfiguration for a specific session job within a session cluster. If configured at session job level, it will override the cluster level configuration.\r\n```"", 'commenter': 'gyfora'}]"
202,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/FlinkConfigManager.java,"@@ -133,6 +135,19 @@ public Configuration getObserveConfig(FlinkDeployment deployment) {
         return getConfig(deployment.getMetadata(), ReconciliationUtils.getDeployedSpec(deployment));
     }
 
+    public Configuration getSessionJobObserveConfig(","[{'comment': ""I think it's a bit confusing to call it `getSessionJobObserveConfig`. I would prefer to call it simply `getSessionJobConfig`"", 'commenter': 'gyfora'}]"
202,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/FlinkConfigManager.java,"@@ -133,6 +135,19 @@ public Configuration getObserveConfig(FlinkDeployment deployment) {
         return getConfig(deployment.getMetadata(), ReconciliationUtils.getDeployedSpec(deployment));
     }
 
+    public Configuration getSessionJobObserveConfig(
+            FlinkDeployment deployment, FlinkSessionJob flinkSessionJob) {
+        Configuration sessionJobConfig = getObserveConfig(deployment);
+
+        // merge session job specific config
+        Map<String, String> sessionJobFlinkConfiguration =
+                flinkSessionJob.getSpec().getFlinkConfiguration();
+        if (sessionJobFlinkConfiguration != null && !sessionJobFlinkConfiguration.isEmpty()) {","[{'comment': 'The `!sessionJobFlinkConfiguration.isEmpty()` check is not necessary', 'commenter': 'gyfora'}]"
202,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/validation/DefaultValidator.java,"@@ -404,4 +409,21 @@ private Optional<String> validateServiceAccount(String serviceAccount) {
         }
         return Optional.empty();
     }
+
+    private Optional<String> validateFlinkSessionJobConfig(
+            Map<String, String> flinkSessionJobConfig) {
+        if (flinkSessionJobConfig == null) {
+            return Optional.empty();
+        }
+
+        for (String key : flinkSessionJobConfig.keySet()) {
+            if (!ALLOWED_FLINK_SESSION_JOB_CONF_KEYS.contains(key)) {","[{'comment': 'Could we please add a simple test for this into the validator test?', 'commenter': 'gyfora'}]"
202,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/artifact/FileSystemBasedArtifactFetcher.java,"@@ -33,7 +34,8 @@ public class FileSystemBasedArtifactFetcher implements ArtifactFetcher {
             new FileSystemBasedArtifactFetcher();
 
     @Override
-    public File fetch(String uri, File targetDir) throws Exception {
+    public File fetch(String uri, Configuration flinkConfiguration, File targetDir)","[{'comment': 'Why did you add the config field here? Is it used?', 'commenter': 'gyfora'}, {'comment': ""No, it is not used. I need to change this line since this class overrides the `artifactFetcher` interface. In the future, we might need this configuration? Not sure. Anyways, it doesn't hurt."", 'commenter': 'FuyaoLi2017'}]"
202,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/artifact/HttpArtifactFetcher.java,"@@ -32,12 +37,26 @@ public class HttpArtifactFetcher implements ArtifactFetcher {
     public static final HttpArtifactFetcher INSTANCE = new HttpArtifactFetcher();
 
     @Override
-    public File fetch(String uri, File targetDir) throws Exception {
+    public File fetch(String uri, Configuration flinkConfiguration, File targetDir)
+            throws Exception {
         var start = System.currentTimeMillis();
         URL url = new URL(uri);
+        HttpURLConnection conn = (HttpURLConnection) url.openConnection();
+
+        // merged session job level header and cluster level header, session job level header take
+        // precedence.
+        Map<String, String> headers =
+                flinkConfiguration.get(KubernetesOperatorConfigOptions.JAR_ARTIFACT_HTTP_HEADER);
+
+        if (headers != null && headers.size() > 0) {","[{'comment': 'size check not necessary', 'commenter': 'gyfora'}]"
202,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/FlinkOperatorConfiguration.java,"@@ -41,6 +42,7 @@ public class FlinkOperatorConfiguration {
     Duration flinkCancelJobTimeout;
     Duration flinkShutdownClusterTimeout;
     String artifactsBaseDir;
+    Map<String, String> artifactHttpHeader;","[{'comment': ""We shouldn't add this extra field here, it's not an operator configuration and it's also not used anywhere"", 'commenter': 'gyfora'}]"
202,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/FlinkConfigManager.java,"@@ -133,6 +135,19 @@ public Configuration getObserveConfig(FlinkDeployment deployment) {
         return getConfig(deployment.getMetadata(), ReconciliationUtils.getDeployedSpec(deployment));
     }
 
+    public Configuration getSessionJobObserveConfig(
+            FlinkDeployment deployment, FlinkSessionJob flinkSessionJob) {
+        Configuration sessionJobConfig = getObserveConfig(deployment);
+
+        // merge session job specific config
+        Map<String, String> sessionJobFlinkConfiguration =
+                flinkSessionJob.getSpec().getFlinkConfiguration();
+        if (sessionJobFlinkConfiguration != null && !sessionJobFlinkConfiguration.isEmpty()) {
+            sessionJobFlinkConfiguration.forEach(sessionJobConfig::setString);","[{'comment': 'I am wondering for this particular case, would it make sense to merge the HTTP headers instead of overwriting them if the base config also defined it?', 'commenter': 'gyfora'}, {'comment': 'I guess the logic would be more straight forward if this config in sessionJob directly overrides the parent config. \r\n\r\nIf we do the merging work. Then a user will need to check two CRs instead of one to determine what are the actual headers that is applied during jar fetching during debugging. It could be a little bit confusing.', 'commenter': 'FuyaoLi2017'}]"
204,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/crd/spec/FlinkDeploymentSpec.java,"@@ -45,7 +45,7 @@ public class FlinkDeploymentSpec extends AbstractFlinkSpec {
     private String imagePullPolicy;
 
     /** Kubernetes service used by the Flink deployment. */
-    private String serviceAccount;
+    private String serviceAccount = ""flink"";","[{'comment': 'I suggest to leave it on `null` and throw a validation error if not set. Accidentally using the wrong service account could be bad', 'commenter': 'gyfora'}, {'comment': 'thanks for the suggestion. It makes sense to me to add validation check so that serviceAccount will be a required field.\r\nFor users who use helm and set a different serviceAccount, they are supposed to be aware of using correct service account.\r\nFor users who use helm and use the default serviceAccount, we may add some comments in examples or detailed validation error info to alert users to set the correct serviceAccount(i.e. `flink`).\r\n', 'commenter': 'bgeng777'}]"
204,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/validation/DefaultValidator.java,"@@ -385,4 +386,12 @@ private Optional<String> validateSpecChange(FlinkSessionJob sessionJob) {
 
         return Optional.empty();
     }
+
+    private Optional<String> validateServiceAccount(String serviceAccount) {
+        if (serviceAccount == null) {
+            return Optional.of(
+                    ""serviceAccount must be defined. If you use helm, its value should be the same with the name of jobServiceAccount."");","[{'comment': ""I think it would be better to say `spec.serviceAccount must be defined. ...` so the user knows where exactly it's missing."", 'commenter': 'gyfora'}]"
216,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/KubernetesOperatorConfigOptions.java,"@@ -143,6 +143,18 @@ public class KubernetesOperatorConfigOptions {
                     .withDescription(
                             ""Whether to enable recovery of missing/deleted jobmanager deployments. False by default for Flink 1.14, true for newer Flink version."");
 
+    public static final ConfigOption<Integer> OPERATOR_SAVEPOINT_HISTORY_MAX_COUNT =
+            ConfigOptions.key(""kubernetes.operator.savepoint.history.max.count"")
+                    .intType()
+                    .defaultValue(10)
+                    .withDescription(""Maximum number of savepoint history entries to retain."");
+
+    public static final ConfigOption<Duration> OPERATOR_SAVEPOINT_HISTORY_MAX_AGE =","[{'comment': 'Since we are performing a lazy clean-up, the savepoint may live longer than the max age.', 'commenter': 'wangyang0918'}, {'comment': 'I added this to the doc.', 'commenter': 'tweise'}]"
216,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/observer/SavepointObserver.java,"@@ -131,9 +136,56 @@ private Optional<String> observeTriggeredSavepointProgress(
 
         LOG.info(""Savepoint status updated with latest completed savepoint info"");
         currentSavepointInfo.updateLastSavepoint(savepointFetchResult.getSavepoint());
+        updateSavepointHistory(
+                currentSavepointInfo, savepointFetchResult.getSavepoint(), deployedConfig);","[{'comment': 'I think we should also update the savepoint history when we shut down using savepoint. (Or in case of failure of the shutdown operation as part of the `observeLatestSavepoint`)', 'commenter': 'gyfora'}, {'comment': 'The job needs to be running for savepoint cleanup to succeed.', 'commenter': 'tweise'}, {'comment': 'That makes sense but the savepoint itself could still be recorded in the history and cleaned up later after upgrade ', 'commenter': 'gyfora'}]"
216,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/observer/SavepointObserver.java,"@@ -131,15 +136,72 @@ private Optional<String> observeTriggeredSavepointProgress(
 
         LOG.info(""Savepoint status updated with latest completed savepoint info"");
         currentSavepointInfo.updateLastSavepoint(savepointFetchResult.getSavepoint());
+        updateSavepointHistory(
+                currentSavepointInfo, savepointFetchResult.getSavepoint(), deployedConfig, true);
         return Optional.empty();
     }
 
+    @VisibleForTesting
+    void updateSavepointHistory(
+            SavepointInfo currentSavepointInfo,
+            Savepoint newSavepoint,
+            Configuration deployedConfig,
+            boolean cleanup) {
+        List<Savepoint> savepointHistory = currentSavepointInfo.getSavepointHistory();
+        if (savepointHistory == null) {
+            currentSavepointInfo.setSavepointHistory(savepointHistory = new ArrayList<>());
+        }
+        if (!savepointHistory.isEmpty()) {
+            Savepoint recentSp = savepointHistory.get(savepointHistory.size() - 1);
+            if (recentSp.getLocation().equals(newSavepoint.getLocation())) {
+                return;
+            }
+        }
+        savepointHistory.add(newSavepoint);
+
+        if (!cleanup) {
+            return;
+        }
+
+        // maintain history
+        int maxCount = configManager.getOperatorConfiguration().getSavepointHistoryMaxCount();
+        while (savepointHistory.size() > maxCount) {
+            // remove oldest entries
+            disposeSavepointQuietly(savepointHistory.remove(0), deployedConfig);
+        }
+
+        Duration maxAge = configManager.getOperatorConfiguration().getSavepointHistoryMaxAge();
+        long maxTms = System.currentTimeMillis() - maxAge.toMillis();
+        Iterator<Savepoint> it = savepointHistory.iterator();
+        while (it.hasNext()) {
+            Savepoint sp = it.next();
+            if (sp.getTimeStamp() < maxTms && sp != newSavepoint) {
+                it.remove();
+                disposeSavepointQuietly(sp, deployedConfig);
+            }
+        }
+    }
+
+    private void disposeSavepointQuietly(Savepoint sp, Configuration conf) {
+        try {
+            LOG.info(""Disposing savepoint {}"", sp);
+            flinkService.disposeSavepoint(sp.getLocation(), conf);
+        } catch (Exception e) {
+            // savepoint dispose error should nota affect the deployment
+            LOG.error(""Exception while disposing savepoint {}"", sp.getLocation(), e);
+        }
+    }
+
     private void observeLatestSavepoint(
             SavepointInfo savepointInfo, String jobID, Configuration deployedConfig) {
         try {
             flinkService
                     .getLastCheckpoint(JobID.fromHexString(jobID), deployedConfig)
-                    .ifPresent(savepointInfo::updateLastSavepoint);
+                    .ifPresent(
+                            sp -> {
+                                savepointInfo.updateLastSavepoint(sp);
+                                updateSavepointHistory(savepointInfo, sp, deployedConfig, false);","[{'comment': ""I am afraid this will only work for Flink 1.15 and it's not enough. You need to also add this to `FlinkService#cancelJob` where it would be recorded regardless of Flink version.\r\n\r\nThere is no absolute guarantee that it will be recorded either way but having it in both places eliminates the bulk of the possibilities :)"", 'commenter': 'gyfora'}, {'comment': 'You could cover this quite nicely with a FlinkVersion parameterized tests. The TestingFlinkService behaves according to the Flink version (keeping the Flink deployment around in Finished state after cancel for 1.15 and removing for earlier versions)', 'commenter': 'gyfora'}]"
216,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/FlinkService.java,"@@ -431,11 +431,12 @@ public void cancelJob(FlinkDeployment deployment, UpgradeMode upgradeMode) throw
         }
         deploymentStatus.getJobStatus().setState(JobStatus.FINISHED.name());
         savepointOpt.ifPresent(
-                location ->
-                        deploymentStatus
-                                .getJobStatus()
-                                .getSavepointInfo()
-                                .setLastSavepoint(Savepoint.of(location)));
+                location -> {
+                    Savepoint sp = Savepoint.of(location);
+                    deploymentStatus.getJobStatus().getSavepointInfo().setLastSavepoint(sp);
+                    // this is required for Flink < 1.15
+                    deploymentStatus.getJobStatus().getSavepointInfo().addSavepointToHistory(sp);","[{'comment': 'We also need to do this for `FlinkSessionJobReconciler`.', 'commenter': 'wangyang0918'}, {'comment': 'done!', 'commenter': 'tweise'}]"
216,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/observer/SavepointObserver.java,"@@ -131,15 +135,63 @@ private Optional<String> observeTriggeredSavepointProgress(
 
         LOG.info(""Savepoint status updated with latest completed savepoint info"");
         currentSavepointInfo.updateLastSavepoint(savepointFetchResult.getSavepoint());
+        updateSavepointHistory(
+                currentSavepointInfo, savepointFetchResult.getSavepoint(), deployedConfig, true);
         return Optional.empty();
     }
 
+    @VisibleForTesting
+    void updateSavepointHistory(
+            SavepointInfo currentSavepointInfo,
+            Savepoint newSavepoint,
+            Configuration deployedConfig,
+            boolean cleanup) {
+
+        currentSavepointInfo.addSavepointToHistory(newSavepoint);
+        if (!cleanup) {
+            return;
+        }
+
+        // maintain history
+        List<Savepoint> savepointHistory = currentSavepointInfo.getSavepointHistory();
+        int maxCount = configManager.getOperatorConfiguration().getSavepointHistoryMaxCount();
+        while (savepointHistory.size() > maxCount) {
+            // remove oldest entries
+            disposeSavepointQuietly(savepointHistory.remove(0), deployedConfig);
+        }
+
+        Duration maxAge = configManager.getOperatorConfiguration().getSavepointHistoryMaxAge();
+        long maxTms = System.currentTimeMillis() - maxAge.toMillis();
+        Iterator<Savepoint> it = savepointHistory.iterator();
+        while (it.hasNext()) {
+            Savepoint sp = it.next();
+            if (sp.getTimeStamp() < maxTms && sp != newSavepoint) {
+                it.remove();
+                disposeSavepointQuietly(sp, deployedConfig);
+            }
+        }
+    }
+
+    private void disposeSavepointQuietly(Savepoint sp, Configuration conf) {
+        try {
+            LOG.info(""Disposing savepoint {}"", sp);
+            flinkService.disposeSavepoint(sp.getLocation(), conf);
+        } catch (Exception e) {
+            // savepoint dispose error should nota affect the deployment","[{'comment': 'typo: nota?', 'commenter': 'wangyang0918'}]"
246,helm/flink-kubernetes-operator/templates/flink-operator.yaml,"@@ -34,6 +34,12 @@ spec:
         {{- include ""flink-operator.selectorLabels"" . | nindent 8 }}
       annotations:
         kubectl.kubernetes.io/default-container: {{ .Chart.Name }}
+      {{- $keyExist := .Values.operatorPodDeployment | default dict -}}","[{'comment': 'This could be just `if .Values.operatorPodDeployment.annotations`, right?\r\nWe are using the same convention throughout the helm chart.', 'commenter': 'mbalassi'}, {'comment': 'I could go with .Values.operatorPodDeployment.annotations as well, . but in that case, we need to have the following in the values.yaml, even if we intend not to pass any new annotations.\r\noperatorPodDeployment:\r\n   annotations: {}\r\nApproach that I went with, is to keep ""operatorPodDeployment"" completely optional, as the chart will not fail, even if we don\'t pass ""operatorPodDeployment"" in the values.yaml file.\r\nIf we desire the former approach, I can change it accordingly', 'commenter': 'zeus1ammon'}, {'comment': 'How about using a construct like this?\r\n\r\n```\r\n{{- if index (.Values.defaultConfiguration) ""flink-conf.yaml"" }}\r\n  {{- index (.Values.defaultConfiguration) ""flink-conf.yaml"" | nindent 4 -}}\r\n{{- end }}\r\n```', 'commenter': 'gyfora'}]"
246,helm/flink-kubernetes-operator/values.yaml,"@@ -30,6 +30,9 @@ image:
 rbac:
   create: true
 
+operatorPodDeployment:","[{'comment': 'I find the name confusing. Is it for the pod or the deployment?', 'commenter': 'mbalassi'}, {'comment': 'it is for Pod annotation. I had named it this way, as we have only a helm template for deployment, but we can change it. Please let me know, if there is any suggestion', 'commenter': 'zeus1ammon'}, {'comment': 'Then we could call it simply `operatorPod`', 'commenter': 'gyfora'}]"
252,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/FlinkService.java,"@@ -354,8 +355,54 @@ protected ClusterClient<String> getClusterClient(Configuration config) throws Ex
     }
 
     public void cancelJob(FlinkDeployment deployment, UpgradeMode upgradeMode) throws Exception {
-        var conf = configManager.getObserveConfig(deployment);
-        var deploymentStatus = deployment.getStatus();
+        Optional<String> savepointOpt = Optional.empty();
+
+        Configuration conf = null;
+        FlinkDeploymentStatus deploymentStatus = deployment.getStatus();
+        try {
+            conf = configManager.getObserveConfig(deployment);
+            savepointOpt = cancelJob(deployment, upgradeMode, conf, deploymentStatus);
+        } catch (LastReconciledSpecException e) {
+            LOG.warn(""Last reconciled spec is null"", e);","[{'comment': 'Why would we cancel the job if last reconciled spec is null? that means it was never deployed', 'commenter': 'gyfora'}, {'comment': 'For some reason the deployment was corrupted, and could never finish starting, and it is impossible to delete it, impossible.\r\n\r\nThis happened to me several times, in different namespaces.', 'commenter': 'Miuler'}]"
252,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/exception/LastReconciledSpecException.java,"@@ -0,0 +1,25 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.exception;
+
+/** Exception when last status is null. */
+public class LastReconciledSpecException extends RuntimeException {","[{'comment': 'Seems like you are using exception for flow control instead of explicitly checking the last reconciled spec, can we avoid this?', 'commenter': 'gyfora'}, {'comment': 'In FlinkOperatorConfiguration#getObserveConfig, you already threw a RuntimeException, I only wrapper this exception to detect that it was launched by flink operator.', 'commenter': 'Miuler'}]"
252,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/FlinkService.java,"@@ -354,8 +355,54 @@ protected ClusterClient<String> getClusterClient(Configuration config) throws Ex
     }
 
     public void cancelJob(FlinkDeployment deployment, UpgradeMode upgradeMode) throws Exception {
-        var conf = configManager.getObserveConfig(deployment);
-        var deploymentStatus = deployment.getStatus();
+        Optional<String> savepointOpt = Optional.empty();
+
+        Configuration conf = null;
+        FlinkDeploymentStatus deploymentStatus = deployment.getStatus();
+        try {
+            conf = configManager.getObserveConfig(deployment);
+            savepointOpt = cancelJob(deployment, upgradeMode, conf, deploymentStatus);
+        } catch (LastReconciledSpecException e) {
+            LOG.warn(""Last reconciled spec is null"", e);
+        }
+
+        switch (upgradeMode) {","[{'comment': 'Could you please not refactor the method? Seems like a lot of unnecessary changes, itâ€™s hard to see what actually changed in the logic', 'commenter': 'gyfora'}, {'comment': 'The method would be enormous. It seemed like a good practice to me to separate the behavior of deleting the cluster using the flink client and, on the other hand, deleting the kubernetes deployment. At first it is more complicated but then it becomes easier to read', 'commenter': 'Miuler'}]"
252,flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/controller/FlinkDeploymentControllerTest.java,"@@ -274,6 +278,89 @@ public void verifyInProgressDeploymentWithCrashLoopBackoff() throws Exception {
         validatingResponseProvider.assertValidated();
     }
 
+    @ParameterizedTest
+    @EnumSource(FlinkVersion.class)
+    public void cleanUpDeploymentWithError(FlinkVersion flinkVersion)
+            throws JsonProcessingException {","[{'comment': ""I think this test case is bit of an overkill.\r\nTo reproduce the bug and validate the fix it could simply be replaced with the following 2 lines:\r\n\r\n```\r\n        FlinkDeployment flinkDeployment = TestUtils.buildApplicationCluster();\r\n        testController.cleanup(flinkDeployment, context);\r\n```\r\n\r\nWe don't really need the error state, json etc because thats not related to the actual issue at hand I think. Also the rest of the method tests deployments, upgrades etc that are already tested in other methods in this class."", 'commenter': 'gyfora'}, {'comment': 'ok', 'commenter': 'Miuler'}]"
252,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/FlinkConfigManager.java,"@@ -134,6 +135,11 @@ public Configuration getDeployConfig(ObjectMeta objectMeta, FlinkDeploymentSpec
     public Configuration getObserveConfig(FlinkDeployment deployment) {
         var deployedSpec = ReconciliationUtils.getDeployedSpec(deployment);
         if (deployedSpec == null) {
+            try {
+                LOG.debug(""deployment: {}"", objectMapper.writeValueAsString(deployment));
+            } catch (JsonProcessingException e) {
+                LOG.debug(""deployment: {}"", deployment);
+            }","[{'comment': 'I think this debug log does not add any value here. Could we please remove it?', 'commenter': 'gyfora'}, {'comment': 'Ok', 'commenter': 'Miuler'}]"
252,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/FlinkService.java,"@@ -229,14 +229,12 @@ private JarRunResponseBody runJar(
                                     ? RestoreMode.DEFAULT
                                     : null);
             LOG.info(""Submitting job: {} to session cluster."", jobID.toHexString());
+            var clientTimeout =
+                    configManager.getOperatorConfiguration().getFlinkClientTimeout().toSeconds();
+            LOG.debug(""clientTimeout: {}"", clientTimeout);","[{'comment': 'The client timeout configuration is static and already logged on info level when the operator starts. We should remove the logging here.', 'commenter': 'gyfora'}, {'comment': 'ok', 'commenter': 'Miuler'}, {'comment': ""I have a doubt, what is the properti for this clinetTimeout?\r\n\r\nI add this in the values.yaml\r\n```\r\n  flink-conf.yaml: |+\r\n    # Flink Config Overrides\r\n    client.timeout: 4 MINUTE\r\n```\r\n\r\nand in my pod I see this\r\n```\r\nexec -ti  migration-cosmosdb-wape-5578f9948c-9t6xm  -- bash\r\n\r\nroot@migration-cosmosdb-wape-5578f9948c-9t6xm:/opt/flink# grep time conf/flink-conf.yaml\r\nclient.timeout: 4 MINUTE\r\n```\r\n\r\nbut in my log I see this\r\n```\r\n2022-06-04 12:20:30,505 o.a.f.k.o.c.FlinkConfigManager [INFO ] Updating default configuration to {blob.server.port=6124, taskmanager.memory.process.size=1728m, client.timeout=4 MINUTE, jobmanager.memory.process.size=1600m, jobmanager.rpc.port=6123, taskmanager.rpc.port=6122, queryable-state.proxy.ports=6125, paralle\r\nlism.default=2, taskmanager.numberOfTaskSlots=2, kubernetes.operator.metrics.reporter.slf4j.interval=5 MINUTE, kubernetes.operator.observer.progress-check.interval=5 s, kubernetes.operator.metrics.reporter.slf4j.factory.class=org.apache.flink.metrics.slf4j.Slf4jReporterFactory, kubernetes.operator.reconciler.resched\r\nule.interval=15 s}\r\n...\r\n...\r\n2022-06-04 12:20:30,762 i.j.o.a.c.ExecutorServiceManager [DEBUG] Initialized ExecutorServiceManager executor: class java.util.concurrent.ThreadPoolExecutor, timeout: 10\r\n...\r\n...\r\n2022-06-04 12:28:12,124 o.a.f.k.o.s.FlinkService       [DEBUG][flink-wape-02/migration-cosmosdb-wape-sessionjob] clientTimeout: 10\r\n...\r\n...\r\n2022-06-04 12:26:52,685 i.j.o.p.e.ReconciliationDispatcher [ERROR][flink-wape-02/migration-cosmosdb-wape-sessionjob] Error during event processing ExecutionScope{ resource id: CustomResourceID{name='migration-cosmosdb-wape-sessionjob', namespace='flink-wape-02'}, version: null} failed.\r\norg.apache.flink.kubernetes.operator.exception.ReconciliationException: org.apache.flink.util.FlinkRuntimeException: java.util.concurrent.TimeoutException\r\n    at org.apache.flink.kubernetes.operator.controller.FlinkSessionJobController.reconcile(FlinkSessionJobController.java:117)\r\n    at org.apache.flink.kubernetes.operator.controller.FlinkSessionJobController.reconcile(FlinkSessionJobController.java:59)\r\n    at io.javaoperatorsdk.operator.processing.Controller$2.execute(Controller.java:101)\r\n    at io.javaoperatorsdk.operator.processing.Controller$2.execute(Controller.java:76)\r\n    at io.javaoperatorsdk.operator.api.monitoring.Metrics.timeControllerExecution(Metrics.java:34)\r\n    at io.javaoperatorsdk.operator.processing.Controller.reconcile(Controller.java:75)\r\n    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.reconcileExecution(ReconciliationDispatcher.java:143)\r\n    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleReconcile(ReconciliationDispatcher.java:109)\r\n    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleDispatch(ReconciliationDispatcher.java:74)\r\n    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleExecution(ReconciliationDispatcher.java:50)\r\n    at io.javaoperatorsdk.operator.processing.event.EventProcessor$ControllerExecution.run(EventProcessor.java:349)\r\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n    at java.base/java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.flink.util.FlinkRuntimeException: java.util.concurrent.TimeoutException\r\n    at org.apache.flink.kubernetes.operator.service.FlinkService.runJar(FlinkService.java:240)\r\n    at org.apache.flink.kubernetes.operator.service.FlinkService.submitJobToSessionCluster(FlinkService.java:198)\r\n    at org.apache.flink.kubernetes.operator.reconciler.sessionjob.FlinkSessionJobReconciler.submitAndInitStatus(FlinkSessionJobReconciler.java:164)\r\n    at org.apache.flink.kubernetes.operator.reconciler.sessionjob.FlinkSessionJobReconciler.reconcile(FlinkSessionJobReconciler.java:88)\r\n    at org.apache.flink.kubernetes.operator.reconciler.sessionjob.FlinkSessionJobReconciler.reconcile(FlinkSessionJobReconciler.java:48)\r\n    at org.apache.flink.kubernetes.operator.controller.FlinkSessionJobController.reconcile(FlinkSessionJobController.java:115)\r\n    ... 13 more\r\nCaused by: java.util.concurrent.TimeoutException\r\n    at java.base/java.util.concurrent.CompletableFuture.timedGet(Unknown Source)\r\n    at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)\r\n    at org.apache.flink.kubernetes.operator.service.FlinkService.runJar(FlinkService.java:237)\r\n    ... 18 more\r\n```\r\n\r\nI see my `client.timeout=4 MINUTE` but also a `clientTimeout 10` seconds?"", 'commenter': 'Miuler'}, {'comment': '\r\nAll my pipelines are multiplied by this timeout :(', 'commenter': 'Miuler'}, {'comment': 'We should probably improve these config names but this is currently controlled by:\r\n```\r\nkubernetes.operator.observer.flink.client.timeout\r\n```', 'commenter': 'gyfora'}]"
252,flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/controller/FlinkDeploymentControllerTest.java,"@@ -274,6 +279,15 @@ public void verifyInProgressDeploymentWithCrashLoopBackoff() throws Exception {
         validatingResponseProvider.assertValidated();
     }
 
+    @ParameterizedTest
+    @EnumSource(FlinkVersion.class)
+    public void cleanUpDeploymentWithError(FlinkVersion flinkVersion)
+            throws JsonProcessingException {
+        FlinkDeployment flinkDeployment = TestUtils.buildApplicationCluster();
+        val deleteControl = testController.cleanup(flinkDeployment, context);","[{'comment': 'I think you accidentally used lombok `val` instead of `var`', 'commenter': 'gyfora'}, {'comment': 'Can I use val in the tests? I prefer to use final instead of a mutable variable.', 'commenter': 'Miuler'}, {'comment': 'We generally donâ€™t use this and also avoid final variables in these cases as it doesnâ€™t affect anything only adds boilerplate on these cases.\r\n\r\nalso we should restrict Lombok usage where it adds significant tangible value ', 'commenter': 'gyfora'}, {'comment': 'final variables inside methods are a bit of a controversy in Java. I would say the flink best practice is to only use this in performance critical low level code ', 'commenter': 'gyfora'}, {'comment': 'ok', 'commenter': 'Miuler'}]"
256,docs/content/docs/development/roadmap.md,"@@ -0,0 +1,39 @@
+---
+title: ""RoadMap""
+weight: 1
+type: docs
+aliases:
+- /development/roadmap.html
+---
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+  http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+
+# RoadMap Guide","[{'comment': 'Could be `# Flink Operator Roadmap`', 'commenter': 'gyfora'}]"
256,docs/content/docs/development/roadmap.md,"@@ -0,0 +1,39 @@
+---
+title: ""RoadMap""
+weight: 1
+type: docs
+aliases:
+- /development/roadmap.html
+---
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+  http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+
+# RoadMap Guide
+
+We're here to tell us what's we will do something next","[{'comment': ""Instead this could say:\r\n\r\n```\r\nThis page is intended to present an overview of the general direction of the Flink Kubernetes Operator project, and the larger new features on our radar.\r\n\r\nIt's not a comprehensive list and might be slightly outdated at any given time. Please check JIRA for the actual work items and the Flink mailing lists for feature planning and discussion.\r\n```"", 'commenter': 'gyfora'}]"
260,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/observer/deployment/ApplicationObserver.java,"@@ -82,11 +79,14 @@ protected boolean observeFlinkCluster(
 
         var jobStatus = flinkApp.getStatus().getJobStatus();
 
+        var previousJobState = jobStatus.getState();
         boolean jobFound =
                 jobStatusObserver.observe(
-                        jobStatus,
+                        flinkApp.getStatus(),
                         deployedConfig,
                         new ApplicationObserverContext(flinkApp, context, deployedConfig));
+        EventUtils.generateEventsOnJobStatusChanged(
+                kubernetesClient, previousJobState, jobStatus, flinkApp);","[{'comment': 'I think this method should be called in the JobStatusObserver directly to avoid having it in multiple places. There you already have previousJobState etc.', 'commenter': 'gyfora'}, {'comment': 'I have tried that way, but it relies on the resource object, It will have to touch some more class to pass it. ', 'commenter': 'Aitozi'}, {'comment': ""Wouldn't it be enough to simply change the jobstatus observer to get the full resource object instead of the status only?"", 'commenter': 'gyfora'}, {'comment': 'Good suggestion, I have pushed the commit, PTAL again', 'commenter': 'Aitozi'}]"
260,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/observer/JobStatusObserver.java,"@@ -94,14 +91,64 @@ private void ifRunningMoveToReconciling(JobStatus jobStatus, String previousJobS
     protected abstract void onTimeout(CTX ctx);
 
     /**
-     * Find and update previous job status based on the job list from the cluster and return the
-     * target status.
+     * Filter the target job status message by the job list from the cluster.
      *
-     * @param status the target job status to be updated.
+     * @param status the target job status.
      * @param clusterJobStatuses the candidate cluster jobs.
-     * @return The target status of the job. If no matched job found, {@code Optional.empty()} will
+     * @return The target job status message. If no matched job found, {@code Optional.empty()} will
      *     be returned.
      */
-    protected abstract Optional<String> updateJobStatus(
+    protected abstract Optional<JobStatusMessage> filterTargetJob(
             JobStatus status, List<JobStatusMessage> clusterJobStatuses);
+
+    /**
+     * Update the status in CR according to the cluster job status.
+     *
+     * @param status the target job status
+     * @param clusterJobStatus the status fetch from the cluster.
+     * @param deployedConfig Deployed job config.
+     */
+    private void updateJobStatus(
+            CommonStatus<SPEC> status,
+            JobStatusMessage clusterJobStatus,
+            Configuration deployedConfig) {
+        var jobStatus = status.getJobStatus();
+        var previousJobStatus = jobStatus.getState();
+
+        jobStatus.setState(clusterJobStatus.getJobState().name());
+        jobStatus.setJobName(clusterJobStatus.getJobName());
+        jobStatus.setJobId(clusterJobStatus.getJobId().toHexString());
+        jobStatus.setStartTime(String.valueOf(clusterJobStatus.getStartTime()));
+
+        if (jobStatus.getState().equals(previousJobStatus)) {
+            LOG.info(""Job status ({}) unchanged"", previousJobStatus);
+        } else {
+            jobStatus.setUpdateTime(String.valueOf(System.currentTimeMillis()));
+            LOG.info(
+                    ""Job status successfully updated from {} to {}"",
+                    previousJobStatus,
+                    jobStatus.getState());
+        }
+
+        if (clusterJobStatus.getJobState() == org.apache.flink.api.common.JobStatus.FAILED) {
+            try {
+                var result =
+                        flinkService.requestJobResult(deployedConfig, clusterJobStatus.getJobId());
+                result.getSerializedThrowable()
+                        .ifPresent(
+                                t -> {
+                                    var error = t.getFullStringifiedStackTrace();
+                                    if (error != null && !error.equals(status.getError())) {","[{'comment': '`!error.equals(status.getError())` check not necessary', 'commenter': 'gyfora'}, {'comment': 'This is meant to avoid the duplicated log in operator', 'commenter': 'Aitozi'}, {'comment': 'oh I see, sry', 'commenter': 'gyfora'}]"
260,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/observer/JobStatusObserver.java,"@@ -31,23 +36,29 @@
 import java.util.concurrent.TimeoutException;
 
 /** An observer to observe the job status. */
-public abstract class JobStatusObserver<CTX> {
+public abstract class JobStatusObserver<
+        SPEC extends AbstractFlinkSpec, STATUS extends CommonStatus<SPEC>, CTX> {
 
     private static final Logger LOG = LoggerFactory.getLogger(JobStatusObserver.class);
+    private static final int MAX_ERROR_STRING_LENGTH = 512;
     private final FlinkService flinkService;
+    private final KubernetesClient kubernetesClient;
 
-    public JobStatusObserver(FlinkService flinkService) {
+    public JobStatusObserver(FlinkService flinkService, KubernetesClient client) {
         this.flinkService = flinkService;
+        this.kubernetesClient = client;","[{'comment': 'Sorry to annoy you but the `FlinkService` currently has a `getKubernetesClient` method that we could use to avoid too many changes  :) ', 'commenter': 'gyfora'}, {'comment': ""hah, thanks for remind. I did't notice that just now. "", 'commenter': 'Aitozi'}, {'comment': 'Removed', 'commenter': 'Aitozi'}]"
260,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/observer/JobStatusObserver.java,"@@ -31,23 +36,29 @@
 import java.util.concurrent.TimeoutException;
 
 /** An observer to observe the job status. */
-public abstract class JobStatusObserver<CTX> {
+public abstract class JobStatusObserver<
+        SPEC extends AbstractFlinkSpec, STATUS extends CommonStatus<SPEC>, CTX> {
 
     private static final Logger LOG = LoggerFactory.getLogger(JobStatusObserver.class);
+    private static final int MAX_ERROR_STRING_LENGTH = 512;
     private final FlinkService flinkService;
+    private final KubernetesClient kubernetesClient;
 
-    public JobStatusObserver(FlinkService flinkService) {
+    public JobStatusObserver(FlinkService flinkService, KubernetesClient client) {
         this.flinkService = flinkService;
+        this.kubernetesClient = client;
     }
 
     /**
      * Observe the status of the flink job.
      *
-     * @param jobStatus The job status to be observed.
+     * @param resource The custom resource to be observed.
      * @param deployedConfig Deployed job config.
      * @return If job found return true, otherwise return false.
      */
-    public boolean observe(JobStatus jobStatus, Configuration deployedConfig, CTX ctx) {
+    public boolean observe(
+            AbstractFlinkResource<SPEC, STATUS> resource, Configuration deployedConfig, CTX ctx) {","[{'comment': 'Do we need the new generic parameters? I think AbstractFlinkResource<?, ?> would be enough and we also dont need to change the class signature that way', 'commenter': 'gyfora'}, {'comment': 'Replaced with generic parameters', 'commenter': 'Aitozi'}, {'comment': 'sorry I messed up my comment, what I was trying to say is that we can simply use wildcard parameters:\r\n```\r\nAbstractFlinkResource<?, ?>\r\n```', 'commenter': 'gyfora'}, {'comment': 'thanks, looks more simple', 'commenter': 'Aitozi'}]"
260,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/observer/JobStatusObserver.java,"@@ -94,14 +95,84 @@ private void ifRunningMoveToReconciling(JobStatus jobStatus, String previousJobS
     protected abstract void onTimeout(CTX ctx);
 
     /**
-     * Find and update previous job status based on the job list from the cluster and return the
-     * target status.
+     * Filter the target job status message by the job list from the cluster.
      *
-     * @param status the target job status to be updated.
+     * @param status the target job status.
      * @param clusterJobStatuses the candidate cluster jobs.
-     * @return The target status of the job. If no matched job found, {@code Optional.empty()} will
+     * @return The target job status message. If no matched job found, {@code Optional.empty()} will
      *     be returned.
      */
-    protected abstract Optional<String> updateJobStatus(
+    protected abstract Optional<JobStatusMessage> filterTargetJob(
             JobStatus status, List<JobStatusMessage> clusterJobStatuses);
+
+    /**
+     * Update the status in CR according to the cluster job status.
+     *
+     * @param resource the target custom resource.
+     * @param clusterJobStatus the status fetch from the cluster.
+     * @param deployedConfig Deployed job config.
+     */
+    private <SPEC extends AbstractFlinkSpec, STATUS extends CommonStatus<SPEC>>
+            void updateJobStatus(
+                    AbstractFlinkResource<SPEC, STATUS> resource,
+                    JobStatusMessage clusterJobStatus,
+                    Configuration deployedConfig) {
+        var jobStatus = resource.getStatus().getJobStatus();
+        var previousJobStatus = jobStatus.getState();
+
+        jobStatus.setState(clusterJobStatus.getJobState().name());
+        jobStatus.setJobName(clusterJobStatus.getJobName());
+        jobStatus.setJobId(clusterJobStatus.getJobId().toHexString());
+        jobStatus.setStartTime(String.valueOf(clusterJobStatus.getStartTime()));
+
+        if (jobStatus.getState().equals(previousJobStatus)) {
+            LOG.info(""Job status ({}) unchanged"", previousJobStatus);","[{'comment': 'Should we return after this line? ', 'commenter': 'gyfora'}, {'comment': 'Fixed', 'commenter': 'Aitozi'}]"
264,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/observer/SavepointObserver.java,"@@ -148,13 +149,38 @@ void cleanupSavepointHistory(
 
         // maintain history
         List<Savepoint> savepointHistory = currentSavepointInfo.getSavepointHistory();
-        int maxCount = configManager.getOperatorConfiguration().getSavepointHistoryMaxCount();
+        Integer countThreshold =
+                configManager.getOperatorConfiguration().getSavepointHistoryCountThreshold();
+        int maxCount =
+                deployedConfig.get(
+                        KubernetesOperatorConfigOptions.OPERATOR_SAVEPOINT_HISTORY_MAX_COUNT);
+        if (countThreshold != null && maxCount > countThreshold) {
+            LOG.warn(
+                    ""Maximum number of savepoint history entries to retain changes to {}. The value of '{}' exceeds the value of '{}'"",
+                    countThreshold,
+                    KubernetesOperatorConfigOptions.OPERATOR_SAVEPOINT_HISTORY_MAX_COUNT.key(),
+                    KubernetesOperatorConfigOptions.OPERATOR_SAVEPOINT_HISTORY_COUNT_THRESHOLD
+                            .key());
+            maxCount = countThreshold;
+        }","[{'comment': 'Can we refactor this pattern into a utility method? \r\n```\r\npublic static <T extends Comparable<T>> T getValueWithThreshold(ConfigOption<T> option, ConfigOption<T> threshold, Configuraiton config) {\r\n...\r\n}\r\n```', 'commenter': 'gyfora'}]"
264,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/KubernetesOperatorConfigOptions.java,"@@ -145,12 +145,25 @@ public class KubernetesOperatorConfigOptions {
                     .withDescription(
                             ""Whether to enable recovery of missing/deleted jobmanager deployments."");
 
+    public static final ConfigOption<Integer> OPERATOR_SAVEPOINT_HISTORY_COUNT_THRESHOLD =
+            ConfigOptions.key(""kubernetes.operator.savepoint.history.count.threshold"")
+                    .intType()
+                    .noDefaultValue()
+                    .withDescription(""Number threshold of savepoint history entries to retain."");
+
     public static final ConfigOption<Integer> OPERATOR_SAVEPOINT_HISTORY_MAX_COUNT =
             ConfigOptions.key(""kubernetes.operator.savepoint.history.max.count"")
                     .intType()
                     .defaultValue(10)
                     .withDescription(""Maximum number of savepoint history entries to retain."");
 
+    public static final ConfigOption<Duration> OPERATOR_SAVEPOINT_HISTORY_AGE_THRESHOLD =
+            ConfigOptions.key(""kubernetes.operator.savepoint.history.age.threshold"")
+                    .durationType()
+                    .noDefaultValue()
+                    .withDescription(
+                            ""Age threshold for savepoint history entries to retain. Due to lazy clean-up, the most recent savepoint may live longer than the max age."");
+","[{'comment': 'I think the new config keys should be:\r\n`OPERATOR_SAVEPOINT_HISTORY_MAX_AGE.key() + "".threshold""`\r\n`OPERATOR_SAVEPOINT_HISTORY_MAX_COUNT.key() + "".threshold""`\r\n\r\nTo make this straightforward to users and admins', 'commenter': 'gyfora'}]"
264,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/FlinkOperatorConfiguration.java,"@@ -40,8 +40,7 @@ public class FlinkOperatorConfiguration {
     Duration flinkCancelJobTimeout;
     Duration flinkShutdownClusterTimeout;
     String artifactsBaseDir;
-    int savepointHistoryMaxCount;
-    Duration savepointHistoryMaxAge;
+    Configuration operatorConfig;","[{'comment': 'I would prefer to keep the concrete thershold values as fields here to keep this consistent', 'commenter': 'gyfora'}]"
264,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/ConfigOptionUtils.java,"@@ -0,0 +1,58 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.utils;
+
+import org.apache.flink.configuration.ConfigOption;
+import org.apache.flink.configuration.Configuration;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/** {@link ConfigOption} utilities. */
+public class ConfigOptionUtils {
+
+    private static final Logger LOG = LoggerFactory.getLogger(ConfigOptionUtils.class);
+
+    /**
+     * Gets the value of {@link ConfigOption} with threshold.
+     *
+     * @param configOption The config option.
+     * @param thresholdOption The threshold option.
+     * @param effectiveConfig The effective config.
+     * @param operatorConfig The operator config.
+     * @return The value of {@link ConfigOption} with threshold.
+     */
+    public static <T extends Comparable<T>> T getValueWithThreshold(
+            ConfigOption<T> configOption,
+            ConfigOption<T> thresholdOption,
+            Configuration effectiveConfig,
+            Configuration operatorConfig) {","[{'comment': 'instead of passing the `thresholdOption` + `operatorConfig` we could simply pass the threshold value directly that we get from the operatorconfig to make this simpler', 'commenter': 'gyfora'}]"
264,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/ConfigOptionUtils.java,"@@ -0,0 +1,58 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.utils;
+
+import org.apache.flink.configuration.ConfigOption;
+import org.apache.flink.configuration.Configuration;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/** {@link ConfigOption} utilities. */
+public class ConfigOptionUtils {
+
+    private static final Logger LOG = LoggerFactory.getLogger(ConfigOptionUtils.class);
+
+    /**
+     * Gets the value of {@link ConfigOption} with threshold.
+     *
+     * @param configOption The config option.
+     * @param thresholdOption The threshold option.
+     * @param effectiveConfig The effective config.
+     * @param operatorConfig The operator config.
+     * @return The value of {@link ConfigOption} with threshold.
+     */
+    public static <T extends Comparable<T>> T getValueWithThreshold(
+            ConfigOption<T> configOption,
+            ConfigOption<T> thresholdOption,
+            Configuration effectiveConfig,","[{'comment': 'you could simply call the parameter conf/config this is just a utility method', 'commenter': 'gyfora'}]"
265,flink-kubernetes-webhook/src/main/java/org/apache/flink/kubernetes/operator/admission/Utils.java,"@@ -0,0 +1,34 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.admission;
+
+import com.fasterxml.jackson.core.JsonProcessingException;
+import com.fasterxml.jackson.databind.ObjectMapper;
+import io.fabric8.kubernetes.api.model.HasMetadata;
+
+/** Admission utils. */
+public class Utils {
+
+    private static final ObjectMapper mapper = new ObjectMapper();
+
+    public static <T> T convertToTargetType(HasMetadata resource, Class<T> targetType)","[{'comment': 'I think we donâ€™t need this utility class, ObjectMapper already has a convertValue method that does exactly this.', 'commenter': 'gyfora'}, {'comment': 'ðŸ‘ðŸ», Fixed', 'commenter': 'Aitozi'}]"
265,flink-kubernetes-webhook/src/main/java/org/apache/flink/kubernetes/operator/admission/mutator/FlinkMutator.java,"@@ -0,0 +1,70 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.admission.mutator;
+
+import org.apache.flink.kubernetes.operator.admission.Utils;
+import org.apache.flink.kubernetes.operator.crd.CrdConstants;
+import org.apache.flink.kubernetes.operator.crd.FlinkSessionJob;
+
+import io.fabric8.kubernetes.api.model.HasMetadata;
+import io.javaoperatorsdk.admissioncontroller.NotAllowedException;
+import io.javaoperatorsdk.admissioncontroller.Operation;
+import io.javaoperatorsdk.admissioncontroller.mutation.Mutator;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.HashMap;
+
+/** The default mutator. */
+public class FlinkMutator implements Mutator<HasMetadata> {
+    private static final Logger LOG = LoggerFactory.getLogger(FlinkMutator.class);
+
+    @Override
+    public HasMetadata mutate(HasMetadata resource, Operation operation)
+            throws NotAllowedException {
+        if (operation == Operation.CREATE) {
+            LOG.debug(""Mutating resource {}"", resource);
+
+            if (CrdConstants.KIND_SESSION_JOB.equals(resource.getKind())) {
+                try {
+                    var sessionJob = Utils.convertToTargetType(resource, FlinkSessionJob.class);
+                    patchInternalLabel(sessionJob);
+                    return sessionJob;
+                } catch (Exception e) {
+                    throw new RuntimeException(e);
+                }
+            }
+        }
+        return resource;
+    }
+
+    private void patchInternalLabel(FlinkSessionJob flinkSessionJob) {","[{'comment': 'I think it would be clearer to call it simply `setSessionTargetLabel`', 'commenter': 'gyfora'}]"
265,helm/flink-kubernetes-operator/templates/webhook.yaml,"@@ -109,4 +109,36 @@ webhooks:
         operator: In
         values: [{{- range .Values.watchNamespaces }}{{ . | quote }},{{- end}}]
   {{- end }}
-  {{- end }}
+---","[{'comment': 'I think we should have a separate config flag to enable this. We should handle the validating and mutating webhooks independently of each other as some envs might want one but not the other.', 'commenter': 'gyfora'}, {'comment': 'Agree, will do it ', 'commenter': 'Aitozi'}, {'comment': 'Fixed', 'commenter': 'Aitozi'}]"
268,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/controller/FlinkDeploymentController.java,"@@ -81,59 +80,121 @@ public FlinkDeploymentController(
             ReconcilerFactory reconcilerFactory,
             ObserverFactory observerFactory,
             MetricManager<FlinkDeployment> metricManager,
+            Metrics metrics,
             StatusHelper<FlinkDeploymentStatus> statusHelper) {
         this.configManager = configManager;
         this.kubernetesClient = kubernetesClient;
         this.validators = validators;
         this.reconcilerFactory = reconcilerFactory;
         this.observerFactory = observerFactory;
         this.metricManager = metricManager;
+        this.metrics = metrics;
         this.statusHelper = statusHelper;
     }
 
     @Override
     public DeleteControl cleanup(FlinkDeployment flinkApp, Context context) {","[{'comment': ""I don't really understand this part. This must be handled at JOSDK level, isn't it?\r\nhttps://github.com/java-operator-sdk/java-operator-sdk/blob/main/operator-framework-core/src/main/java/io/javaoperatorsdk/operator/processing/Controller.java#L109"", 'commenter': 'morhidi'}, {'comment': '@morhidi, the `Metrics` should be integrated in the `FlinkDeploymentController` and `FlinkSessionController`, like above `Controller`. Any misunderstanding?', 'commenter': 'SteNicholas'}, {'comment': ""I mean this is done for every controller automatically by the JOSDK, there's no need to do this explicitly. "", 'commenter': 'morhidi'}]"
268,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/controller/FlinkDeploymentController.java,"@@ -81,59 +80,121 @@ public FlinkDeploymentController(
             ReconcilerFactory reconcilerFactory,
             ObserverFactory observerFactory,
             MetricManager<FlinkDeployment> metricManager,
+            Metrics metrics,
             StatusHelper<FlinkDeploymentStatus> statusHelper) {
         this.configManager = configManager;
         this.kubernetesClient = kubernetesClient;
         this.validators = validators;
         this.reconcilerFactory = reconcilerFactory;
         this.observerFactory = observerFactory;
         this.metricManager = metricManager;
+        this.metrics = metrics;
         this.statusHelper = statusHelper;
     }
 
     @Override
     public DeleteControl cleanup(FlinkDeployment flinkApp, Context context) {
-        LOG.info(""Deleting FlinkDeployment"");
-        statusHelper.updateStatusFromCache(flinkApp);
         try {
-            observerFactory.getOrCreate(flinkApp).observe(flinkApp, context);
-        } catch (DeploymentFailedException dfe) {
-            // ignore during cleanup
+            return metrics.timeControllerExecution(
+                    new Metrics.ControllerExecution<>() {
+                        @Override
+                        public String name() {
+                            return ""cleanup"";
+                        }
+
+                        @Override
+                        public String controllerName() {
+                            return FlinkDeploymentController.class.getSimpleName();
+                        }
+
+                        @Override
+                        public String successTypeName(DeleteControl deleteControl) {
+                            return deleteControl.isRemoveFinalizer()
+                                    ? ""delete""
+                                    : ""finalizerNotRemoved"";
+                        }
+
+                        @Override
+                        public DeleteControl execute() {
+                            LOG.info(""Deleting FlinkDeployment"");
+                            statusHelper.updateStatusFromCache(flinkApp);
+                            try {
+                                observerFactory.getOrCreate(flinkApp).observe(flinkApp, context);
+                            } catch (DeploymentFailedException dfe) {
+                                // ignore during cleanup
+                            }
+                            metricManager.onRemove(flinkApp);
+                            statusHelper.removeCachedStatus(flinkApp);
+                            return reconcilerFactory
+                                    .getOrCreate(flinkApp)
+                                    .cleanup(flinkApp, context);
+                        }
+                    });
+        } catch (Exception e) {
+            throw new OperatorException(e);
         }
-        metricManager.onRemove(flinkApp);
-        statusHelper.removeCachedStatus(flinkApp);
-        return reconcilerFactory.getOrCreate(flinkApp).cleanup(flinkApp, context);
     }
 
     @Override
     public UpdateControl<FlinkDeployment> reconcile(FlinkDeployment flinkApp, Context context)","[{'comment': 'ditto: https://github.com/java-operator-sdk/java-operator-sdk/blob/main/operator-framework-core/src/main/java/io/javaoperatorsdk/operator/processing/Controller.java#L151', 'commenter': 'morhidi'}]"
268,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/FlinkOperator.java,"@@ -70,37 +70,36 @@ public class FlinkOperator {
     private final FlinkConfigManager configManager;
     private final Set<FlinkResourceValidator> validators;
     private final MetricGroup metricGroup;
+    private final Metrics metrics;
+
+    private static final String OPERATOR_SDK_GROUP = ""operator.sdk"";
 
     public FlinkOperator(@Nullable Configuration conf) {
         this.client = new DefaultKubernetesClient();
         this.configManager = conf != null ? new FlinkConfigManager(conf) : new FlinkConfigManager();
-        this.operator =
-                new Operator(
-                        client,
-                        getConfigurationServiceOverriderConsumer(
-                                configManager.getOperatorConfiguration()));
-        this.flinkService = new FlinkService(client, configManager);
-        this.validators = ValidatorUtils.discoverValidators(configManager);
         this.metricGroup =
                 OperatorMetricUtils.initOperatorMetrics(configManager.getDefaultConfig());
+        this.metrics = new FlinkOperatorMetrics(metricGroup.addGroup(OPERATOR_SDK_GROUP));
+        this.operator = new Operator(client, getConfigurationServiceOverriderConsumer());
+        this.flinkService = new FlinkService(client, configManager);
+        this.validators = ValidatorUtils.discoverValidators(configManager);
         PluginManager pluginManager =
                 PluginUtils.createPluginManagerFromRootFolder(configManager.getDefaultConfig());
         FileSystem.initialize(configManager.getDefaultConfig(), pluginManager);
     }
 
-    @VisibleForTesting
-    protected static Consumer<ConfigurationServiceOverrider>
-            getConfigurationServiceOverriderConsumer(
-                    FlinkOperatorConfiguration operatorConfiguration) {
+    private Consumer<ConfigurationServiceOverrider> getConfigurationServiceOverriderConsumer() {
         return overrider -> {
-            int parallelism = operatorConfiguration.getReconcilerMaxParallelism();
+            int parallelism =
+                    configManager.getOperatorConfiguration().getReconcilerMaxParallelism();
             if (parallelism == -1) {
                 LOG.info(""Configuring operator with unbounded reconciliation thread pool."");
                 overrider.withExecutorService(Executors.newCachedThreadPool());
             } else {
                 LOG.info(""Configuring operator with {} reconciliation threads."", parallelism);
                 overrider.withConcurrentReconciliationThreads(parallelism);
             }
+            overrider.withMetrics(metrics);","[{'comment': 'I only expected that we would need to implement the Metrics interface and register it here, and the rest is handle by JOSDK framework.', 'commenter': 'morhidi'}, {'comment': '@morhidi, IMO, the `Metrics` should be integrated with overrider here to register the `Metrics` here.', 'commenter': 'SteNicholas'}, {'comment': 'I added the following to the code, and nothing else:\r\n\r\n```\r\noverrider.withMetrics(new Metrics() {\r\n                @Override\r\n                public void receivedEvent(Event event) {\r\n                    LOG.info(""receivedEvent() called"");\r\n                }\r\n\r\n                @Override\r\n                public void reconcileCustomResource(ResourceID resourceID, RetryInfo retryInfo) {\r\n                    LOG.info(""reconcileCustomResource() called"");\r\n                }\r\n\r\n                @Override\r\n                public void failedReconciliation(ResourceID resourceID, Exception exception) {\r\n                    LOG.info(""failedReconciliation() called"");\r\n                }\r\n\r\n                @Override\r\n                public void cleanupDoneFor(ResourceID resourceID) {\r\n                    LOG.info(""cleanupDoneFor() called"");\r\n                }\r\n\r\n                @Override\r\n                public void finishedReconciliation(ResourceID resourceID) {\r\n                    LOG.info(""finishedReconciliation() called"");\r\n                }\r\n\r\n            });\r\n```\r\n\r\nand I see it in the logs, it is being called:\r\n\r\n```\r\n2022-06-14 12:16:07,480 o.a.f.k.o.FlinkOperator        [INFO ] [default.basic-session-cluster] receivedEvent() called\r\n2022-06-14 12:16:07,481 o.a.f.k.o.FlinkOperator        [INFO ] [default.basic-session-job-example] receivedEvent() called\r\n2022-06-14 12:16:07,592 o.a.f.k.o.FlinkOperator        [INFO ] [default.basic-session-job-example] reconcileCustomResource() called\r\n2022-06-14 12:16:07,593 o.a.f.k.o.FlinkOperator        [INFO ] [default.basic-session-job-example] receivedEvent() called\r\n2022-06-14 12:16:07,605 o.a.f.k.o.FlinkOperator        [INFO ] [default.basic-session-job-example2] receivedEvent() called\r\n2022-06-14 12:16:07,607 o.a.f.k.o.FlinkOperator        [INFO ] [default.basic-session-job-example2] reconcileCustomResource() called\r\n2022-06-14 12:16:07,608 o.a.f.k.o.FlinkOperator        [INFO ] [default.basic-session-job-example2] receivedEvent() called\r\n2022-06-14 12:16:07,622 o.a.f.k.o.FlinkOperator        [INFO ] [default.basic-session-job-example] receivedEvent() called\r\n2022-06-14 12:16:07,662 o.a.f.k.o.FlinkOperator        [INFO ] [default.basic-session-job-example] receivedEvent() called\r\n2022-06-14 12:16:07,686 o.a.f.k.o.FlinkOperator        [INFO ] [default.basic-session-job-example2] receivedEvent() called\r\n2022-06-14 12:16:07,778 o.a.f.k.o.FlinkOperator        [INFO ] [default.basic-session-job-example2] receivedEvent() called\r\n2022-06-14 12:16:07,799 o.a.f.k.o.FlinkOperator        [INFO ] [default.basic-session-job-example] receivedEvent() called\r\n```', 'commenter': 'morhidi'}]"
268,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/FlinkOperator.java,"@@ -71,36 +70,34 @@ public class FlinkOperator {
     private final Set<FlinkResourceValidator> validators;
     private final MetricGroup metricGroup;
 
+    private static final String OPERATOR_SDK_GROUP = ""operator.sdk"";
+
     public FlinkOperator(@Nullable Configuration conf) {
         this.client = new DefaultKubernetesClient();
         this.configManager = conf != null ? new FlinkConfigManager(conf) : new FlinkConfigManager();
-        this.operator =
-                new Operator(
-                        client,
-                        getConfigurationServiceOverriderConsumer(
-                                configManager.getOperatorConfiguration()));
-        this.flinkService = new FlinkService(client, configManager);
-        this.validators = ValidatorUtils.discoverValidators(configManager);
         this.metricGroup =
                 OperatorMetricUtils.initOperatorMetrics(configManager.getDefaultConfig());
+        this.operator = new Operator(client, getConfigurationServiceOverriderConsumer());
+        this.flinkService = new FlinkService(client, configManager);
+        this.validators = ValidatorUtils.discoverValidators(configManager);
         PluginManager pluginManager =
                 PluginUtils.createPluginManagerFromRootFolder(configManager.getDefaultConfig());
         FileSystem.initialize(configManager.getDefaultConfig(), pluginManager);
     }
 
-    @VisibleForTesting
-    protected static Consumer<ConfigurationServiceOverrider>
-            getConfigurationServiceOverriderConsumer(
-                    FlinkOperatorConfiguration operatorConfiguration) {
+    private Consumer<ConfigurationServiceOverrider> getConfigurationServiceOverriderConsumer() {
         return overrider -> {
-            int parallelism = operatorConfiguration.getReconcilerMaxParallelism();
+            int parallelism =
+                    configManager.getOperatorConfiguration().getReconcilerMaxParallelism();
             if (parallelism == -1) {
                 LOG.info(""Configuring operator with unbounded reconciliation thread pool."");
                 overrider.withExecutorService(Executors.newCachedThreadPool());
             } else {
                 LOG.info(""Configuring operator with {} reconciliation threads."", parallelism);
                 overrider.withConcurrentReconciliationThreads(parallelism);
             }
+            overrider.withMetrics(
+                    new FlinkOperatorMetrics(metricGroup.addGroup(OPERATOR_SDK_GROUP)));","[{'comment': 'I think this should be configurable: `kubernetes.operator.josdk.metrics.enabled : true/false`', 'commenter': 'gyfora'}]"
268,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/metrics/FlinkOperatorMetrics.java,"@@ -0,0 +1,140 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.metrics;
+
+import org.apache.flink.metrics.Counter;
+import org.apache.flink.metrics.MetricGroup;
+import org.apache.flink.runtime.metrics.MetricRegistry;
+
+import io.javaoperatorsdk.operator.Operator;
+import io.javaoperatorsdk.operator.api.monitoring.Metrics;
+import io.javaoperatorsdk.operator.api.reconciler.RetryInfo;
+import io.javaoperatorsdk.operator.processing.event.Event;
+import io.javaoperatorsdk.operator.processing.event.ResourceID;
+
+import java.util.LinkedList;
+import java.util.List;
+import java.util.Map;
+import java.util.Optional;
+import java.util.concurrent.ConcurrentHashMap;
+
+/**
+ * Implementation of {@link Metrics} to monitor the operations of {@link Operator} and forward
+ * metrics to {@link MetricRegistry}.
+ */
+public class FlinkOperatorMetrics implements Metrics {
+
+    private static final String RECONCILIATIONS = ""reconciliations."";
+    private final MetricGroup metricGroup;
+    private final Map<String, Counter> counters;
+
+    public FlinkOperatorMetrics(MetricGroup metricGroup) {
+        this.metricGroup = metricGroup;
+        this.counters = new ConcurrentHashMap<>();
+    }
+
+    public <T> T timeControllerExecution(ControllerExecution<T> execution) throws Exception {
+        String executionMetric =
+                String.format(
+                        ""controller.%s.execution.%s"", execution.controllerName(), execution.name());
+        counter(executionMetric);
+        try {
+            T result = execution.execute();
+            counter(executionMetric + "".success"");
+            counter(executionMetric + "".success."" + execution.successTypeName(result));
+            return result;
+        } catch (Exception e) {
+            counter(executionMetric + "".failure"");
+            counter(executionMetric + "".failure."" + e.getClass().getSimpleName());
+            throw e;
+        }
+    }
+
+    public void receivedEvent(Event event) {
+        incrementCounter(
+                event.getRelatedCustomResourceID(),
+                ""events.received"",
+                ""event"",
+                event.getClass().getSimpleName());
+    }
+
+    @Override
+    public void cleanupDoneFor(ResourceID resourceID) {
+        incrementCounter(resourceID, ""events.delete"");
+    }
+
+    @Override
+    public void reconcileCustomResource(ResourceID resourceID, RetryInfo retryInfoNullable) {
+        Optional<RetryInfo> retryInfo = Optional.ofNullable(retryInfoNullable);
+        incrementCounter(
+                resourceID,
+                RECONCILIATIONS + ""started"",
+                RECONCILIATIONS + ""retries.number"",
+                """" + retryInfo.map(RetryInfo::getAttemptCount).orElse(0),
+                RECONCILIATIONS + ""retries.last"",
+                """" + retryInfo.map(RetryInfo::isLastAttempt).orElse(true));
+    }
+
+    @Override
+    public void finishedReconciliation(ResourceID resourceID) {
+        incrementCounter(resourceID, RECONCILIATIONS + ""success"");
+    }
+
+    public void failedReconciliation(ResourceID resourceID, Exception exception) {
+        var cause = exception.getCause();
+        if (cause == null) {
+            cause = exception;
+        } else if (cause instanceof RuntimeException) {
+            cause = cause.getCause() != null ? cause.getCause() : cause;
+        }
+        incrementCounter(
+                resourceID,
+                RECONCILIATIONS + ""failed"",
+                ""exception"",
+                cause.getClass().getSimpleName());
+    }
+
+    public <T extends Map<?, ?>> T monitorSizeOf(T map, String name) {
+        metricGroup.gauge(name + "".size"", map::size);
+        return map;
+    }
+
+    private void incrementCounter(ResourceID id, String counterName, String... additionalTags) {","[{'comment': 'I think tags should be metric groups so that this integrates well with the reporters.', 'commenter': 'gyfora'}]"
268,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/KubernetesOperatorConfigOptions.java,"@@ -203,4 +203,11 @@ public class KubernetesOperatorConfigOptions {
                     .defaultValue(false)
                     .withDescription(
                             ""Enables dynamic change of watched/monitored namespaces. Defaults to false"");
+
+    public static final ConfigOption<Boolean> OPERATOR_JOSDK_METRICS_ENABLED =
+            ConfigOptions.key(""kubernetes.operator.josdk.metrics.enabled"")
+                    .booleanType()
+                    .defaultValue(true)
+                    .withDescription(
+                            ""Enables that Metrics of Java Operator SDK forwards metrics to the Flink metric registries . Defaults to true"");","[{'comment': 'Please do not include the default value in the description. It will be part of the HTML generated', 'commenter': 'gyfora'}, {'comment': '@gyfora , config options like `kubernetes.operator.watched.namespaces`, `kubernetes.operator.dynamic.namespaces.enabled` include the default value in the description. Do I need to update these description of these options?', 'commenter': 'SteNicholas'}, {'comment': ""Don't worry about the others for now, Matyas will fix some of those"", 'commenter': 'gyfora'}]"
270,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/validation/DefaultValidator.java,"@@ -100,6 +104,21 @@ private static Optional<String> firstPresent(Optional<String>... errOpts) {
         return Optional.empty();
     }
 
+    private Optional<String> validateDeploymentName(String name) {
+        if (name == null) {
+            return Optional.of(""FlinkDeployment name must be define."");","[{'comment': 'The null check is unnecessary, this will never be null on kubernetes', 'commenter': 'gyfora'}, {'comment': 'Ok, I removed the null check.', 'commenter': 'Grypse'}]"
270,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/validation/DefaultValidator.java,"@@ -100,6 +104,21 @@ private static Optional<String> firstPresent(Optional<String>... errOpts) {
         return Optional.empty();
     }
 
+    private Optional<String> validateDeploymentName(String name) {
+        if (name == null) {
+            return Optional.of(""FlinkDeployment name must be define."");
+        } else {
+            Matcher matcher = DEPLOYMENT_NAME_PATTERN.matcher(name);
+            if (!matcher.matches()) {
+                return Optional.of(
+                        String.format(
+                                ""The FlinkDeployment meta.name: %s is a invalid value, and must start and end with an alphanumeric character (e.g. 'my-name',  or '123-abc', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?'"",","[{'comment': ""Minor correction:\r\n\r\n```\r\nThe FlinkDeployment name: %s is invalid. It must consist of lower case alphanumeric characters or '-', start with an alphabetic character, and end with an alphanumeric character (e.g. 'my-name', or 'abc-123')\r\n```"", 'commenter': 'gyfora'}, {'comment': 'Corrected!', 'commenter': 'Grypse'}, {'comment': 'can you actually take the string that I wrote please? I think you only removed the end but I improved the whole message.\r\nmeta.name -> name\r\na invalid value -> invalid.\r\n\r\nFix example: 123-abc does not match the regex, it is not valid', 'commenter': 'gyfora'}, {'comment': ""I change the message to this:\r\n`The FlinkDeployment name: %s is invalid, must consist of lower case alphanumeric characters or '-', start with an alphabetic character, and end with an alphanumeric character (e.g. 'my-name',  or 'abc-123'), and the length must be no more than 45 characters.`\r\nMore detailed message have been added, including what to start and end with and length requirements.\r\nWhat do u think?"", 'commenter': 'Grypse'}, {'comment': 'Yes please change the message to what you wrote (you did not change it to this :D)', 'commenter': 'gyfora'}, {'comment': ""Can you please just copy-paste this into the message so we can avoid a few other iterations on this? \r\n```\r\nThe FlinkDeployment name: %s is invalid, must consist of lower case alphanumeric characters or '-', start with an alphabetic character, and end with an alphanumeric character (e.g. 'my-name', or 'abc-123'), and the length must be no more than 45 characters.\r\n```"", 'commenter': 'gyfora'}, {'comment': 'Changed.', 'commenter': 'Grypse'}]"
278,flink-kubernetes-mock-shaded/pom.xml,"@@ -0,0 +1,145 @@
+<?xml version=""1.0"" encoding=""UTF-8""?>
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+  http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+<project xmlns=""http://maven.apache.org/POM/4.0.0""
+         xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
+         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
+    <parent>
+        <artifactId>flink-kubernetes-operator-parent</artifactId>
+        <groupId>org.apache.flink</groupId>
+        <version>1.1-SNAPSHOT</version>
+        <relativePath>..</relativePath>
+    </parent>
+
+    <modelVersion>4.0.0</modelVersion>
+
+    <artifactId>flink-kubernetes-mock-shaded</artifactId>
+    <name>Flink Kubernetes Mock Shaded</name>
+    <dependencies>
+        <dependency>
+            <groupId>io.fabric8</groupId>
+            <artifactId>kubernetes-server-mock</artifactId>
+            <version>5.5.0</version>
+            <exclusions>","[{'comment': 'It is a bit unusual to introduce a whole new module for mocking a test dependency. What is the purpose of this? Can we get rid of the module somehow?', 'commenter': 'gyfora'}]"
278,flink-kubernetes-operator/pom.xml,"@@ -143,6 +150,14 @@ under the License.
             <version>${junit.jupiter.version}</version>
             <scope>test</scope>
         </dependency>
+
+        <dependency>
+            <groupId>org.mockito</groupId>
+            <artifactId>mockito-core</artifactId>
+            <version>${mockito.version}</version>
+            <type>jar</type>
+            <scope>test</scope>
+        </dependency>","[{'comment': 'In our current tests we spent considerable effort to avoid using mocks (and Mockito) according to the Flink codestyle: https://flink.apache.org/contributing/code-style-and-quality-common.html\r\n', 'commenter': 'gyfora'}]"
278,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/Mode.java,"@@ -46,6 +49,10 @@ public static Mode getMode(FlinkDeployment flinkApp) {
     }
 
     private static Mode getMode(FlinkDeploymentSpec spec) {
-        return spec.getJob() != null ? APPLICATION : SESSION;
+        KubernetesDeploymentMode deploymentMode = spec.getMode();","[{'comment': 'This could use the `getDeploymentMode` utility to avoid null checks', 'commenter': 'gyfora'}]"
278,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/Mode.java,"@@ -20,11 +20,14 @@
 
 import org.apache.flink.kubernetes.operator.crd.FlinkDeployment;
 import org.apache.flink.kubernetes.operator.crd.spec.FlinkDeploymentSpec;
+import org.apache.flink.kubernetes.operator.crd.spec.KubernetesDeploymentMode;
 
 /** The mode of {@link FlinkDeployment}. */
 public enum Mode {
-    APPLICATION,
-    SESSION;
+    NATIVE_APPLICATION,
+    NATIVE_SESSION,
+    STANDALONE_APPLICATION,
+    STANDALONE_SESSION;","[{'comment': 'Adding these new enums feels a bit forced because everwhere you use it you either check session/application or native/standalone. Maybe we can simply have 2 enums (Mode & KubernetesDeploymentMode)', 'commenter': 'gyfora'}]"
278,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/FlinkConfigBuilder.java,"@@ -216,30 +229,94 @@ protected FlinkConfigBuilder applyTaskManagerSpec() throws IOException {
                     spec.getTaskManager().getPodTemplate(),
                     effectiveConfig,
                     false);
+
+            if (spec.getTaskManager().getReplicas() != null
+                    && spec.getTaskManager().getReplicas() > 0) {
+                effectiveConfig.set(
+                        StandaloneKubernetesConfigOptionsInternal.KUBERNETES_TASKMANAGER_REPLICAS,
+                        spec.getTaskManager().getReplicas());
+            }
+        }
+
+        if (spec.getJob() != null
+                && KubernetesDeploymentMode.getDeploymentMode(spec)
+                        == KubernetesDeploymentMode.STANDALONE) {
+            final int tmTaskSlots = effectiveConfig.get(TaskManagerOptions.NUM_TASK_SLOTS);
+            final int jobParallelism = spec.getJob().getParallelism();
+            if (!effectiveConfig.contains(
+                            StandaloneKubernetesConfigOptionsInternal
+                                    .KUBERNETES_TASKMANAGER_REPLICAS)
+                    && tmTaskSlots > 0
+                    && jobParallelism > 0) {
+                effectiveConfig.set(
+                        StandaloneKubernetesConfigOptionsInternal.KUBERNETES_TASKMANAGER_REPLICAS,
+                        (int) Math.ceil((double) jobParallelism / tmTaskSlots));
+            }","[{'comment': 'I think this logic could be simplified by a combination of the `getParallelism()` and `FlinkUtils#getNumTaskManagers() ` . At the moment it seems a little overly complex :) \r\n\r\n', 'commenter': 'gyfora'}]"
278,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/InternalKubernetesOperatorConfigOptions.java,"@@ -0,0 +1,34 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.config;
+
+import org.apache.flink.configuration.ConfigOption;
+import org.apache.flink.kubernetes.operator.crd.spec.KubernetesDeploymentMode;
+
+import static org.apache.flink.configuration.ConfigOptions.key;
+
+/** This class holds internal configuration constants used by the flink operator. */
+public class InternalKubernetesOperatorConfigOptions {
+
+    public static final ConfigOption<KubernetesDeploymentMode> KUBERNETES_DEPLOYMENT_MODE =
+            key(""kubernetes.operator.internal.deployment-mode"")","[{'comment': 'can we prefix with `$internal.` please?', 'commenter': 'gyfora'}]"
278,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/ReconciliationUtils.java,"@@ -494,4 +500,42 @@ public static <SPEC extends AbstractFlinkSpec> void updateStatusForAlreadyUpgrad
         reconciliationStatus.setLastReconciledSpec(
                 ReconciliationUtils.writeSpecWithMeta(lastSpecWithMeta.f0, lastSpecWithMeta.f1));
     }
+
+    public static boolean shouldRollBackDeployment(
+            ReconciliationStatus<FlinkDeploymentSpec> reconciliationStatus,
+            Configuration configuration,
+            FlinkService flinkService) {
+
+        if (reconciliationStatus.getState() == ReconciliationState.ROLLING_BACK) {
+            return true;
+        }
+
+        if (!configuration.get(KubernetesOperatorConfigOptions.DEPLOYMENT_ROLLBACK_ENABLED)
+                || reconciliationStatus.getState() == ReconciliationState.ROLLED_BACK
+                || reconciliationStatus.isLastReconciledSpecStable()) {
+            return false;
+        }
+
+        var lastStableSpec = reconciliationStatus.deserializeLastStableSpec();
+        if (lastStableSpec != null
+                && lastStableSpec.getJob() != null
+                && lastStableSpec.getJob().getState() == JobState.SUSPENDED) {
+            // Should not roll back to suspended state
+            return false;
+        }
+
+        Duration readinessTimeout =
+                configuration.get(KubernetesOperatorConfigOptions.DEPLOYMENT_READINESS_TIMEOUT);
+        if (!Instant.now()","[{'comment': 'This method/logic has been moved to the `AbstractFlinkResourceReconciler`', 'commenter': 'gyfora'}]"
278,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/AbstractFlinkResourceReconciler.java,"@@ -288,41 +282,8 @@ private boolean checkNewSpecAlreadyDeployed(CR resource, Configuration deployCon
      * @param configuration Flink cluster configuration.
      * @return True if the resource should be rolled back.
      */
-    private boolean shouldRollBack(
-            ReconciliationStatus<SPEC> reconciliationStatus, Configuration configuration) {
-
-        if (reconciliationStatus.getState() == ReconciliationState.ROLLING_BACK) {
-            return true;
-        }
-
-        if (!configuration.get(KubernetesOperatorConfigOptions.DEPLOYMENT_ROLLBACK_ENABLED)
-                || reconciliationStatus.getState() == ReconciliationState.ROLLED_BACK
-                || reconciliationStatus.isLastReconciledSpecStable()) {
-            return false;
-        }","[{'comment': 'Why did you remove this?', 'commenter': 'gyfora'}]"
278,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/AbstractJobReconciler.java,"@@ -206,12 +204,6 @@ protected void rollback(CR resource, Context context, Configuration observeConfi
         reconciliationStatus.setState(ReconciliationState.ROLLED_BACK);
     }
 
-    @Override
-    public boolean reconcileOtherChanges(CR resource, Configuration observeConfig)
-            throws Exception {
-        return SavepointUtils.triggerSavepointIfNeeded(flinkService, resource, observeConfig);
-    }","[{'comment': 'Why did you remove it from here? It applies for both sessionjobs and applications', 'commenter': 'gyfora'}]"
278,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/ApplicationReconciler.java,"@@ -186,24 +192,24 @@ private static void setRandomJobResultStorePath(Configuration effectiveConfig) {
     }
 
     @Override
-    public boolean reconcileOtherChanges(FlinkDeployment deployment, Configuration observeConfig)
-            throws Exception {
-        if (super.reconcileOtherChanges(deployment, observeConfig)) {
+    public boolean reconcileOtherChanges(
+            FlinkDeployment deployment, Context ctx, Configuration observeConfig) throws Exception {
+        if (SavepointUtils.triggerSavepointIfNeeded(flinkService, deployment, observeConfig)) {
             return true;
         }","[{'comment': 'I think we can restore the original behaviour here and call super now', 'commenter': 'gyfora'}]"
278,flink-kubernetes-standalone/pom.xml,"@@ -0,0 +1,89 @@
+<?xml version=""1.0"" encoding=""UTF-8""?>
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+  http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+<project xmlns=""http://maven.apache.org/POM/4.0.0""
+         xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
+         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
+    <modelVersion>4.0.0</modelVersion>
+
+
+    <parent>
+        <artifactId>flink-kubernetes-operator-parent</artifactId>
+        <groupId>org.apache.flink</groupId>
+        <version>1.1-SNAPSHOT</version>
+        <relativePath>..</relativePath>
+    </parent>
+
+
+    <artifactId>flink-kubernetes-standalone</artifactId>
+    <name>Flink Kubernetes Standalone</name>
+    <packaging>jar</packaging>
+
+    <dependencies>
+        <dependency>
+            <groupId>org.apache.flink</groupId>
+            <artifactId>flink-kubernetes</artifactId>
+            <version>${flink.version}</version>
+            <exclusions>
+                <exclusion>
+                    <groupId>*</groupId>
+                    <artifactId>*</artifactId>
+                </exclusion>
+            </exclusions>
+        </dependency>
+
+        <dependency>
+            <groupId>org.apache.flink</groupId>
+            <artifactId>flink-clients</artifactId>
+            <version>${flink.version}</version>
+        </dependency>
+
+        <!-- Test -->
+        <dependency>
+            <groupId>org.junit.jupiter</groupId>
+            <artifactId>junit-jupiter-api</artifactId>
+            <version>${junit.jupiter.version}</version>
+            <scope>test</scope>
+        </dependency>
+
+        <dependency>
+            <groupId>org.hamcrest</groupId>
+            <artifactId>hamcrest-all</artifactId>
+            <version>${hamcrest.version}</version>
+            <scope>test</scope>
+        </dependency>
+
+        <dependency>
+            <artifactId>flink-kubernetes-mock-shaded</artifactId>
+            <groupId>org.apache.flink</groupId>
+            <version>1.1-SNAPSHOT</version>","[{'comment': 'this shuold be `${project.version}`', 'commenter': 'gyfora'}]"
278,flink-kubernetes-shaded/pom.xml,"@@ -69,6 +81,16 @@ under the License.
                                     <shadedPattern>org.apache.flink.kubernetes.shaded.io.fabric8</shadedPattern>
                                 </relocation>
                             </relocations>
+                            <filters>
+                                <filter>
+                                    <artifact>*:*</artifact>
+                                    <excludes>
+                                        <exclude>META-INF/DEPENDENCIES</exclude>
+                                        <exclude>META-INF/LICENSE</exclude>
+                                        <exclude>META-INF/MANIFEST.MF</exclude>
+                                    </excludes>
+                                </filter>
+                            </filters>","[{'comment': 'This will remove the NOTICE / LINCENSE files from the shaded jar which is probably not right from a licensing perspective as we are releasing this artifact. \r\n\r\nWhat do you think @wangyang0918 ', 'commenter': 'gyfora'}]"
278,flink-kubernetes-mock-shaded/pom.xml,"@@ -0,0 +1,146 @@
+<?xml version=""1.0"" encoding=""UTF-8""?>
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+  http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+<project xmlns=""http://maven.apache.org/POM/4.0.0""
+         xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
+         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
+    <parent>
+        <artifactId>flink-kubernetes-operator-parent</artifactId>
+        <groupId>org.apache.flink</groupId>
+        <version>1.1-SNAPSHOT</version>
+        <relativePath>..</relativePath>
+    </parent>
+
+    <modelVersion>4.0.0</modelVersion>
+
+    <artifactId>flink-kubernetes-mock-shaded</artifactId>
+    <name>Flink Kubernetes Mock Shaded</name>
+    <dependencies>","[{'comment': 'I think we need to somehow exclude this from the `mvn deploy` command so that we do not release this artifact. Otherwise we have to also make sure that NOTICES/LICENSING is correct in the jar', 'commenter': 'gyfora'}]"
278,flink-kubernetes-standalone/pom.xml,"@@ -0,0 +1,89 @@
+<?xml version=""1.0"" encoding=""UTF-8""?>
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+  http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+<project xmlns=""http://maven.apache.org/POM/4.0.0""
+         xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
+         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
+    <modelVersion>4.0.0</modelVersion>
+
+
+    <parent>
+        <artifactId>flink-kubernetes-operator-parent</artifactId>
+        <groupId>org.apache.flink</groupId>
+        <version>1.1-SNAPSHOT</version>
+        <relativePath>..</relativePath>
+    </parent>","[{'comment': 'The flink-kubernetes-standalone is currently missing the NOTICE file under src/main/resources...', 'commenter': 'gyfora'}]"
278,flink-kubernetes-standalone/pom.xml,"@@ -0,0 +1,89 @@
+<?xml version=""1.0"" encoding=""UTF-8""?>
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+  http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+<project xmlns=""http://maven.apache.org/POM/4.0.0""
+         xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
+         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
+    <modelVersion>4.0.0</modelVersion>
+
+
+    <parent>
+        <artifactId>flink-kubernetes-operator-parent</artifactId>
+        <groupId>org.apache.flink</groupId>
+        <version>1.1-SNAPSHOT</version>
+        <relativePath>..</relativePath>
+    </parent>
+
+
+    <artifactId>flink-kubernetes-standalone</artifactId>
+    <name>Flink Kubernetes Standalone</name>
+    <packaging>jar</packaging>
+
+    <dependencies>
+        <dependency>
+            <groupId>org.apache.flink</groupId>
+            <artifactId>flink-kubernetes</artifactId>
+            <version>${flink.version}</version>
+            <exclusions>
+                <exclusion>
+                    <groupId>*</groupId>
+                    <artifactId>*</artifactId>
+                </exclusion>
+            </exclusions>
+        </dependency>
+
+        <dependency>
+            <groupId>org.apache.flink</groupId>
+            <artifactId>flink-clients</artifactId>
+            <version>${flink.version}</version>
+        </dependency>
+
+        <!-- Test -->
+        <dependency>
+            <groupId>org.junit.jupiter</groupId>
+            <artifactId>junit-jupiter-api</artifactId>
+            <version>${junit.jupiter.version}</version>
+            <scope>test</scope>
+        </dependency>
+
+        <dependency>
+            <groupId>org.hamcrest</groupId>
+            <artifactId>hamcrest-all</artifactId>
+            <version>${hamcrest.version}</version>
+            <scope>test</scope>
+        </dependency>
+
+        <dependency>
+            <artifactId>flink-kubernetes-mock-shaded</artifactId>
+            <groupId>org.apache.flink</groupId>
+            <version>1.1-SNAPSHOT</version>
+            <scope>test</scope>
+        </dependency>
+
+    </dependencies>
+
+    <build>
+        <plugins>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-surefire-plugin</artifactId>
+                <version>3.0.0-M4</version>
+            </plugin>","[{'comment': 'We seem to have this in basically every pom, could we simply move it to the project parent pom instead?', 'commenter': 'gyfora'}]"
278,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/StandaloneFlinkService.java,"@@ -0,0 +1,168 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.service;
+
+import org.apache.flink.annotation.VisibleForTesting;
+import org.apache.flink.client.deployment.ClusterDeploymentException;
+import org.apache.flink.client.deployment.ClusterSpecification;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.kubernetes.KubernetesClusterClientFactory;
+import org.apache.flink.kubernetes.configuration.KubernetesConfigOptions;
+import org.apache.flink.kubernetes.operator.config.FlinkConfigManager;
+import org.apache.flink.kubernetes.operator.crd.FlinkDeployment;
+import org.apache.flink.kubernetes.operator.crd.spec.JobSpec;
+import org.apache.flink.kubernetes.operator.crd.spec.UpgradeMode;
+import org.apache.flink.kubernetes.operator.crd.status.FlinkDeploymentStatus;
+import org.apache.flink.kubernetes.operator.kubeclient.Fabric8FlinkStandaloneKubeClient;
+import org.apache.flink.kubernetes.operator.kubeclient.FlinkStandaloneKubeClient;
+import org.apache.flink.kubernetes.operator.standalone.KubernetesStandaloneClusterDescriptor;
+import org.apache.flink.kubernetes.operator.utils.StandaloneKubernetesUtils;
+import org.apache.flink.kubernetes.utils.KubernetesUtils;
+import org.apache.flink.util.concurrent.ExecutorThreadFactory;
+
+import io.fabric8.kubernetes.api.model.ObjectMeta;
+import io.fabric8.kubernetes.api.model.PodList;
+import io.fabric8.kubernetes.client.KubernetesClient;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+
+import static org.apache.flink.kubernetes.utils.Constants.LABEL_CONFIGMAP_TYPE_HIGH_AVAILABILITY;
+
+/**
+ * Implementation of {@link FlinkService} submitting and interacting with Standalone Kubernetes
+ * Flink clusters and jobs.
+ */
+public class StandaloneFlinkService extends AbstractFlinkService {
+
+    private static final Logger LOG = LoggerFactory.getLogger(StandaloneFlinkService.class);
+
+    public StandaloneFlinkService(
+            KubernetesClient kubernetesClient, FlinkConfigManager configManager) {
+        super(kubernetesClient, configManager);
+    }
+
+    @Override
+    protected void deployApplicationCluster(JobSpec jobSpec, Configuration conf) throws Exception {
+        LOG.info(""Deploying application cluster"");
+        submitClusterInternal(conf);
+        LOG.info(""Application cluster successfully deployed"");
+    }
+
+    @Override
+    public void submitSessionCluster(Configuration conf) throws Exception {
+        LOG.info(""Deploying session cluster"");
+        submitClusterInternal(conf);
+        LOG.info(""Session cluster successfully deployed"");
+    }
+
+    @Override
+    public void cancelJob(FlinkDeployment deployment, UpgradeMode upgradeMode, Configuration conf)
+            throws Exception {
+        cancelJob(deployment, upgradeMode, conf, true);
+    }
+
+    @Override
+    public void deleteClusterDeployment(
+            ObjectMeta meta, FlinkDeploymentStatus status, boolean deleteHaData) {
+        deleteClusterInternal(meta, deleteHaData);
+    }
+
+    @Override
+    protected PodList getJmPodList(String namespace, String clusterId) {
+        return kubernetesClient
+                .pods()
+                .inNamespace(namespace)
+                .withLabels(StandaloneKubernetesUtils.getJobManagerSelectors(clusterId))
+                .list();
+    }
+
+    @VisibleForTesting
+    protected FlinkStandaloneKubeClient createNamespacedKubeClient(
+            Configuration configuration, String namespace) {
+        final int poolSize =
+                configuration.get(KubernetesConfigOptions.KUBERNETES_CLIENT_IO_EXECUTOR_POOL_SIZE);
+
+        ExecutorService executorService =
+                Executors.newFixedThreadPool(
+                        poolSize,
+                        new ExecutorThreadFactory(""flink-kubeclient-io-for-standalone-service""));
+
+        return new Fabric8FlinkStandaloneKubeClient(
+                configuration,
+                Fabric8FlinkStandaloneKubeClient.createNamespacedKubeClient(namespace),
+                executorService);
+    }
+
+    private void submitClusterInternal(Configuration conf) throws ClusterDeploymentException {
+        final String namespace = conf.get(KubernetesConfigOptions.NAMESPACE);
+
+        FlinkStandaloneKubeClient client = createNamespacedKubeClient(conf, namespace);
+        try (final KubernetesStandaloneClusterDescriptor kubernetesClusterDescriptor =
+                new KubernetesStandaloneClusterDescriptor(conf, client)) {
+            kubernetesClusterDescriptor.deploySessionCluster(getClusterSpecification(conf));","[{'comment': 'Why do we always call `deploySessionCluster` both for session and applicationclusters?\r\n\r\nThere is a deployApplicationCluster method in the clusterdescriptor also.', 'commenter': 'gyfora'}]"
278,flink-kubernetes-mock-shaded/pom.xml,"@@ -0,0 +1,154 @@
+<?xml version=""1.0"" encoding=""UTF-8""?>","[{'comment': 'I completely removed the whole mock shaded module (https://github.com/gyfora/flink-kubernetes-operator/commit/64a1ec539f56b68bf76091aabde56fcf00791467) and everything seem to still work. All tests still pass.\r\n\r\nDo we still need this? What am I missing?', 'commenter': 'gyfora'}, {'comment': 'And it actually fixed some of my other intellij test issues', 'commenter': 'gyfora'}]"
278,flink-kubernetes-standalone/src/main/java/org/apache/flink/kubernetes/operator/kubeclient/factory/StandaloneKubernetesTaskManagerFactory.java,"@@ -0,0 +1,90 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.kubeclient.factory;
+
+import org.apache.flink.kubernetes.kubeclient.FlinkPod;
+import org.apache.flink.kubernetes.kubeclient.decorators.EnvSecretsDecorator;
+import org.apache.flink.kubernetes.kubeclient.decorators.FlinkConfMountDecorator;
+import org.apache.flink.kubernetes.kubeclient.decorators.HadoopConfMountDecorator;
+import org.apache.flink.kubernetes.kubeclient.decorators.KerberosMountDecorator;
+import org.apache.flink.kubernetes.kubeclient.decorators.KubernetesStepDecorator;
+import org.apache.flink.kubernetes.kubeclient.decorators.MountSecretsDecorator;
+import org.apache.flink.kubernetes.operator.kubeclient.decorators.CmdStandaloneTaskManagerDecorator;
+import org.apache.flink.kubernetes.operator.kubeclient.decorators.InitStandaloneTaskManagerDecorator;
+import org.apache.flink.kubernetes.operator.kubeclient.parameters.StandaloneKubernetesTaskManagerParameters;
+import org.apache.flink.kubernetes.operator.utils.StandaloneKubernetesUtils;
+import org.apache.flink.kubernetes.utils.Constants;
+import org.apache.flink.util.Preconditions;
+
+import io.fabric8.kubernetes.api.model.Pod;
+import io.fabric8.kubernetes.api.model.PodBuilder;
+import io.fabric8.kubernetes.api.model.apps.Deployment;
+import io.fabric8.kubernetes.api.model.apps.DeploymentBuilder;
+
+/** Utility class for constructing the TaskManager Deployment when deploying in standalone mode. */
+public class StandaloneKubernetesTaskManagerFactory {
+
+    public static Deployment buildKubernetesTaskManagerDeployment(
+            FlinkPod podTemplate,
+            StandaloneKubernetesTaskManagerParameters kubernetesTaskManagerParameters) {
+        FlinkPod flinkPod = Preconditions.checkNotNull(podTemplate).copy();
+
+        final KubernetesStepDecorator[] stepDecorators =
+                new KubernetesStepDecorator[] {
+                    new InitStandaloneTaskManagerDecorator(kubernetesTaskManagerParameters),
+                    new EnvSecretsDecorator(kubernetesTaskManagerParameters),
+                    new MountSecretsDecorator(kubernetesTaskManagerParameters),
+                    new CmdStandaloneTaskManagerDecorator(kubernetesTaskManagerParameters),
+                    new HadoopConfMountDecorator(kubernetesTaskManagerParameters),
+                    new KerberosMountDecorator(kubernetesTaskManagerParameters),
+                    new FlinkConfMountDecorator(kubernetesTaskManagerParameters)
+                };","[{'comment': ""I noticed that we are missing the following two decorators here:\r\n\r\n```\r\nnew UserLibMountDecorator(kubernetesJobManagerParameters),\r\nnew PodTemplateMountDecorator(kubernetesJobManagerParameters)\r\n```\r\n\r\nWhy don't we have these here? "", 'commenter': 'gyfora'}, {'comment': ""Both are unneeded for TM, looking at it closer `PodTemplateMountDecorator` isn't needed for JM either. I will remove it. "", 'commenter': 'usamj'}, {'comment': ""Hm interesting, do you know why is it used in the Native integration and not here? I don't quite understand the purpose of it. "", 'commenter': 'gyfora'}, {'comment': 'It looks like it mounts the TM pod template into the Pod which I assume is used by the JM to create the TM pods? ', 'commenter': 'usamj'}, {'comment': 'Thatâ€™s right, makes sense :)', 'commenter': 'gyfora'}]"
278,flink-kubernetes-standalone/src/test/java/org/apache/flink/kubernetes/operator/kubeclient/Fabric8FlinkStandaloneKubeClientTest.java,"@@ -0,0 +1,105 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.kubeclient;
+
+import org.apache.flink.client.deployment.ClusterSpecification;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.kubernetes.kubeclient.FlinkPod;
+import org.apache.flink.kubernetes.kubeclient.KubernetesJobManagerSpecification;
+import org.apache.flink.kubernetes.operator.kubeclient.factory.StandaloneKubernetesJobManagerFactory;
+import org.apache.flink.kubernetes.operator.kubeclient.factory.StandaloneKubernetesTaskManagerFactory;
+import org.apache.flink.kubernetes.operator.kubeclient.parameters.StandaloneKubernetesJobManagerParameters;
+import org.apache.flink.kubernetes.operator.kubeclient.parameters.StandaloneKubernetesTaskManagerParameters;
+import org.apache.flink.kubernetes.operator.kubeclient.utils.TestUtils;
+import org.apache.flink.util.concurrent.Executors;
+
+import io.fabric8.kubernetes.api.model.apps.Deployment;
+import io.fabric8.kubernetes.client.NamespacedKubernetesClient;
+import io.fabric8.kubernetes.client.server.mock.EnableKubernetesMockClient;
+import io.fabric8.kubernetes.client.server.mock.KubernetesMockServer;
+import org.junit.jupiter.api.BeforeEach;
+import org.junit.jupiter.api.Test;
+
+import java.util.List;
+
+import static org.junit.jupiter.api.Assertions.assertEquals;
+
+/** @link Fabric8FlinkStandaloneKubeClient unit tests */
+@EnableKubernetesMockClient(crud = true)
+public class Fabric8FlinkStandaloneKubeClientTest {
+    private static final String NAMESPACE = ""test"";
+
+    KubernetesMockServer mockServer;
+    protected NamespacedKubernetesClient kubernetesClient;
+    private FlinkStandaloneKubeClient flinkKubeClient;
+    private StandaloneKubernetesTaskManagerParameters taskManagerParameters;
+    private Deployment tmDeployment;
+    private ClusterSpecification clusterSpecification;
+    private Configuration flinkConfig = new Configuration();
+
+    @BeforeEach
+    public final void setup() {
+        flinkConfig = TestUtils.createTestFlinkConfig();
+        kubernetesClient = mockServer.createClient();
+
+        flinkKubeClient =
+                new Fabric8FlinkStandaloneKubeClient(
+                        flinkConfig, kubernetesClient, Executors.newDirectExecutorService());
+        clusterSpecification = TestUtils.createClusterSpecification();
+
+        taskManagerParameters =
+                new StandaloneKubernetesTaskManagerParameters(flinkConfig, clusterSpecification);
+
+        tmDeployment =
+                StandaloneKubernetesTaskManagerFactory.buildKubernetesTaskManagerDeployment(
+                        new FlinkPod.Builder().build(), taskManagerParameters);
+    }
+
+    @Test
+    public void testCreateTaskManagerDeployment() {
+        flinkKubeClient.createTaskManagerDeployment(tmDeployment);
+
+        final List<Deployment> resultedDeployments =
+                kubernetesClient.apps().deployments().inNamespace(NAMESPACE).list().getItems();
+        assertEquals(1, resultedDeployments.size());
+    }
+
+    @Test
+    public void testStopAndCleanupCluster() throws Exception {
+        flinkConfig = TestUtils.createTestFlinkConfig();
+        ClusterSpecification clusterSpecification = TestUtils.createClusterSpecification();","[{'comment': 'seems like duplicate code, as @BeforeEach already assigns the same values', 'commenter': 'gyfora'}]"
278,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/NativeFlinkService.java,"@@ -0,0 +1,160 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.service;
+
+import org.apache.flink.api.common.JobStatus;
+import org.apache.flink.client.cli.ApplicationDeployer;
+import org.apache.flink.client.deployment.ClusterClientFactory;
+import org.apache.flink.client.deployment.ClusterClientServiceLoader;
+import org.apache.flink.client.deployment.ClusterDescriptor;
+import org.apache.flink.client.deployment.DefaultClusterClientServiceLoader;
+import org.apache.flink.client.deployment.application.ApplicationConfiguration;
+import org.apache.flink.client.deployment.application.cli.ApplicationClusterDeployer;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.kubernetes.operator.config.FlinkConfigManager;
+import org.apache.flink.kubernetes.operator.crd.FlinkDeployment;
+import org.apache.flink.kubernetes.operator.crd.spec.JobSpec;
+import org.apache.flink.kubernetes.operator.crd.spec.UpgradeMode;
+import org.apache.flink.kubernetes.operator.crd.status.FlinkDeploymentStatus;
+import org.apache.flink.kubernetes.operator.crd.status.JobManagerDeploymentStatus;
+import org.apache.flink.kubernetes.utils.KubernetesUtils;
+
+import io.fabric8.kubernetes.api.model.ObjectMeta;
+import io.fabric8.kubernetes.api.model.PodList;
+import io.fabric8.kubernetes.client.KubernetesClient;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import static org.apache.flink.kubernetes.utils.Constants.LABEL_CONFIGMAP_TYPE_HIGH_AVAILABILITY;
+
+/**
+ * Implementation of {@link FlinkService} submitting and interacting with Native Kubernetes Flink
+ * clusters and jobs.
+ */
+public class NativeFlinkService extends AbstractFlinkService {
+
+    private static final Logger LOG = LoggerFactory.getLogger(NativeFlinkService.class);
+
+    public NativeFlinkService(KubernetesClient kubernetesClient, FlinkConfigManager configManager) {
+        super(kubernetesClient, configManager);
+    }
+
+    @Override
+    protected void deployApplicationCluster(JobSpec jobSpec, Configuration conf) throws Exception {
+        final ClusterClientServiceLoader clusterClientServiceLoader =","[{'comment': 'We should add ` LOG.info(""Deploying application cluster"");` in the beginning for consistency', 'commenter': 'gyfora'}]"
278,flink-kubernetes-standalone/src/main/java/org/apache/flink/kubernetes/operator/standalone/KubernetesStandaloneClusterDescriptor.java,"@@ -0,0 +1,257 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.standalone;
+
+import org.apache.flink.client.deployment.ClusterDeploymentException;
+import org.apache.flink.client.deployment.ClusterDescriptor;
+import org.apache.flink.client.deployment.ClusterRetrieveException;
+import org.apache.flink.client.deployment.ClusterSpecification;
+import org.apache.flink.client.deployment.application.ApplicationConfiguration;
+import org.apache.flink.client.program.ClusterClient;
+import org.apache.flink.client.program.ClusterClientProvider;
+import org.apache.flink.client.program.rest.RestClusterClient;
+import org.apache.flink.configuration.BlobServerOptions;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.configuration.HighAvailabilityOptions;
+import org.apache.flink.configuration.JobManagerOptions;
+import org.apache.flink.configuration.RestOptions;
+import org.apache.flink.configuration.TaskManagerOptions;
+import org.apache.flink.kubernetes.KubernetesClusterDescriptor;
+import org.apache.flink.kubernetes.configuration.KubernetesConfigOptions;
+import org.apache.flink.kubernetes.kubeclient.Endpoint;
+import org.apache.flink.kubernetes.kubeclient.FlinkPod;
+import org.apache.flink.kubernetes.kubeclient.KubernetesJobManagerSpecification;
+import org.apache.flink.kubernetes.operator.kubeclient.FlinkStandaloneKubeClient;
+import org.apache.flink.kubernetes.operator.kubeclient.factory.StandaloneKubernetesJobManagerFactory;
+import org.apache.flink.kubernetes.operator.kubeclient.factory.StandaloneKubernetesTaskManagerFactory;
+import org.apache.flink.kubernetes.operator.kubeclient.parameters.StandaloneKubernetesJobManagerParameters;
+import org.apache.flink.kubernetes.operator.kubeclient.parameters.StandaloneKubernetesTaskManagerParameters;
+import org.apache.flink.kubernetes.utils.Constants;
+import org.apache.flink.kubernetes.utils.KubernetesUtils;
+import org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils;
+import org.apache.flink.runtime.highavailability.nonha.standalone.StandaloneClientHAServices;
+import org.apache.flink.runtime.jobmanager.HighAvailabilityMode;
+import org.apache.flink.runtime.rpc.AddressResolution;
+
+import io.fabric8.kubernetes.api.model.apps.Deployment;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import java.util.Optional;
+
+import static org.apache.flink.util.Preconditions.checkNotNull;
+
+/** Standalone Kubernetes specific {@link ClusterDescriptor} implementation. */
+public class KubernetesStandaloneClusterDescriptor extends KubernetesClusterDescriptor {
+
+    private static final Logger LOG =
+            LoggerFactory.getLogger(KubernetesStandaloneClusterDescriptor.class);
+
+    private static final String CLUSTER_DESCRIPTION = ""Standalone Kubernetes cluster"";
+
+    private final Configuration flinkConfig;
+
+    private final FlinkStandaloneKubeClient client;
+
+    private final String clusterId;
+
+    public KubernetesStandaloneClusterDescriptor(
+            Configuration flinkConfig, FlinkStandaloneKubeClient client) {
+        super(flinkConfig, client);
+        this.flinkConfig = checkNotNull(flinkConfig);
+        this.client = checkNotNull(client);
+        this.clusterId =
+                checkNotNull(
+                        flinkConfig.getString(KubernetesConfigOptions.CLUSTER_ID),
+                        ""ClusterId must be specified!"");
+    }
+
+    @Override
+    public String getClusterDescription() {
+        return CLUSTER_DESCRIPTION;
+    }
+
+    @Override
+    public ClusterClientProvider<String> deploySessionCluster(
+            ClusterSpecification clusterSpecification) throws ClusterDeploymentException {
+        flinkConfig.set(
+                StandaloneKubernetesConfigOptionsInternal.CLUSTER_MODE,
+                StandaloneKubernetesConfigOptionsInternal.ClusterMode.SESSION);
+
+        final ClusterClientProvider clusterClientProvider =
+                deployClusterInternal(clusterSpecification);
+
+        try (ClusterClient<String> clusterClient = clusterClientProvider.getClusterClient()) {
+            LOG.info(
+                    ""Created flink session cluster {} successfully, JobManager Web Interface: {}"",
+                    clusterId,
+                    clusterClient.getWebInterfaceURL());
+        }
+        return clusterClientProvider;
+    }
+
+    @Override
+    public ClusterClientProvider<String> deployApplicationCluster(
+            ClusterSpecification clusterSpecification,
+            ApplicationConfiguration applicationConfiguration)
+            throws ClusterDeploymentException {
+        flinkConfig.set(
+                StandaloneKubernetesConfigOptionsInternal.CLUSTER_MODE,
+                StandaloneKubernetesConfigOptionsInternal.ClusterMode.APPLICATION);
+        applicationConfiguration.applyToConfiguration(flinkConfig);
+        final ClusterClientProvider clusterClientProvider =
+                deployClusterInternal(clusterSpecification);
+
+        try (ClusterClient<String> clusterClient = clusterClientProvider.getClusterClient()) {
+            LOG.info(
+                    ""Created flink session cluster {} successfully, JobManager Web Interface: {}"",","[{'comment': 'The log should say `application cluster` instead of `session cluster`', 'commenter': 'gyfora'}]"
292,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/validation/DefaultValidator.java,"@@ -178,7 +178,7 @@ private Optional<String> validateJobSpec(
             return Optional.empty();
         }
 
-        if (job.getJarURI() == null) {
+        if (job.getJarURI() == null || job.getJarURI().equals("""")) {","[{'comment': 'This condition in this `if` check could be replaced by `StringUtils.isEmpty(job.getJarURI())`', 'commenter': 'gyfora'}, {'comment': '`isEmpty` not found, using `isNullOrWhitespaceOnly` instead', 'commenter': 'haoxins'}]"
304,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/FlinkConfigManager.java,"@@ -211,11 +212,28 @@ protected Cache<Key, Configuration> getCache() {
         return cache;
     }
 
+    private static Configuration loadGlobalConfiguration() {
+        return loadGlobalConfiguration(EnvUtils.get(EnvUtils.ENV_DYNAMIC_CONF_DIR));
+    }
+
+    @VisibleForTesting
+    protected static Configuration loadGlobalConfiguration(Optional<String> dynamicConfigDir) {
+        if (dynamicConfigDir.isPresent()) {
+            Configuration dynamicConfigs =
+                    GlobalConfiguration.loadConfiguration(dynamicConfigDir.get());
+            LOG.debug(
+                    ""Loading default configuration with overrides from "" + dynamicConfigDir.get());
+            return GlobalConfiguration.loadConfiguration(dynamicConfigs);
+        }","[{'comment': 'I think we should not call this ""dynamic"" as it confuses things with the actual dynamic reloading of the configuraiton. \r\n\r\nWe could mayve call it `ENV_CONF_OVERRIDE_DIR` and `confOverrideDir` respectively', 'commenter': 'gyfora'}, {'comment': 'makes sense, will use `override` over `dynamic`', 'commenter': 'morhidi'}]"
304,flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/config/FlinkConfigManagerTest.java,"@@ -163,4 +171,19 @@ public void testConfUpdateAndCleanup() {
                         .getObserveConfig(deployment)
                         .get(KubernetesOperatorConfigOptions.OPERATOR_RECONCILE_INTERVAL));
     }
+
+    @Test
+    public void testDynamicConfiguration(@TempDir Path dynamicConfigDir) throws IOException {","[{'comment': 'Similarly to my previous config lets call this `confOverrideDirTest` or something like that', 'commenter': 'gyfora'}]"
306,examples/flink-python-example/Dockerfile,"@@ -0,0 +1,41 @@
+################################################################################
+#  Licensed to the Apache Software Foundation (ASF) under one
+#  or more contributor license agreements.  See the NOTICE file
+#  distributed with this work for additional information
+#  regarding copyright ownership.  The ASF licenses this file
+#  to you under the Apache License, Version 2.0 (the
+#  ""License""); you may not use this file except in compliance
+#  with the License.  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an ""AS IS"" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+# limitations under the License.
+################################################################################
+
+FROM flink:1.15.0
+
+
+RUN sed -i s@/archive.ubuntu.com/@/mirrors.aliyun.com/@g /etc/apt/sources.list && apt-get update -y && \","[{'comment': 'Is this replacement necessary?', 'commenter': 'mbalassi'}, {'comment': 'Thanks for pointing out this! It is not necessary and I add it as my network cannot access ubuntu.com fast enough. Removed in 0f49f02038a37be98778d3f45d1de02ad639ae7c.', 'commenter': 'bgeng777'}]"
306,examples/flink-python-example/README.md,"@@ -0,0 +1,83 @@
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+  http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+
+# Flink Kubernetes Operator Python Example
+
+## Overview
+
+This is an end-to-end example of running Flink Python jobs using the Flink Kubernetes Operator.
+
+
+*What's in this example?*
+
+ 1. Python script of a simple streaming job
+ 2. DockerFile to build custom image with pyflink and python demo
+ 3. Example YAML for submitting the python job using the operator
+
+## How does it work?
+
+Flink supports python job in application mode by utilizing `org.apache.flink.client.python.PythonDriver` class as the ","[{'comment': 'nit: python job -> python jobs', 'commenter': 'mbalassi'}, {'comment': 'fixed in 0f49f02038a37be98778d3f45d1de02ad639ae7c', 'commenter': 'bgeng777'}]"
306,examples/flink-python-example/README.md,"@@ -0,0 +1,83 @@
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+  http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+
+# Flink Kubernetes Operator Python Example
+
+## Overview
+
+This is an end-to-end example of running Flink Python jobs using the Flink Kubernetes Operator.
+
+
+*What's in this example?*
+
+ 1. Python script of a simple streaming job
+ 2. DockerFile to build custom image with pyflink and python demo
+ 3. Example YAML for submitting the python job using the operator
+
+## How does it work?
+
+Flink supports python job in application mode by utilizing `org.apache.flink.client.python.PythonDriver` class as the 
+entry class. In Flink Kubernetes Operator, we can reuse this class to run python jobs as well. ","[{'comment': 'nit: In Flink Kubernetes Operator -> With the Flink Kubernetes Operator ', 'commenter': 'mbalassi'}, {'comment': 'fixed in 0f49f02038a37be98778d3f45d1de02ad639ae7c', 'commenter': 'bgeng777'}]"
306,examples/flink-python-example/README.md,"@@ -0,0 +1,83 @@
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+  http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+
+# Flink Kubernetes Operator Python Example
+
+## Overview
+
+This is an end-to-end example of running Flink Python jobs using the Flink Kubernetes Operator.
+
+
+*What's in this example?*
+
+ 1. Python script of a simple streaming job","[{'comment': 'Why is the Python example only for streaming job? Why not support batch job together?', 'commenter': 'SteNicholas'}, {'comment': 'Here we are just showing a workable example and IMO there is no key difference between running a pyflink batch/streaming job.\r\n\r\nBut you do raise a good point. I have tried batch examples in pyflink as well. And I notice that in [FLINK-27468](https://issues.apache.org/jira/browse/FLINK-27468), we add `effectiveConfig.set(SHUTDOWN_ON_APPLICATION_FINISH, false);` and as a result, when the batch example finishes, the job manager pod will not exit. Such behavior is subtly different the directly running `flink run-application ....` which release JM pods in the end.\r\nOur user story of how to support batch jobs may need more discussion cc@gyfora @wangyang0918  but it may be not in the scope of this PR. ', 'commenter': 'bgeng777'}, {'comment': 'It is reasonable to have the JobManager pod running even after the job reached to a globally terminal state. But maybe we need to have a configurable idle timeout to determine when the JobManager should be cleaned up.', 'commenter': 'wangyang0918'}, {'comment': ""@bgeng777, if the user uses the example to create table environment with `EnvironmentSettings.inBatchMode()`, what's the behavior will happen?"", 'commenter': 'SteNicholas'}, {'comment': '@SteNicholas The job will finish normally but the JM will keep running after the batch job finishes. ', 'commenter': 'bgeng777'}]"
306,examples/flink-python-example/Dockerfile,"@@ -0,0 +1,41 @@
+################################################################################
+#  Licensed to the Apache Software Foundation (ASF) under one
+#  or more contributor license agreements.  See the NOTICE file
+#  distributed with this work for additional information
+#  regarding copyright ownership.  The ASF licenses this file
+#  to you under the Apache License, Version 2.0 (the
+#  ""License""); you may not use this file except in compliance
+#  with the License.  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an ""AS IS"" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+# limitations under the License.
+################################################################################
+
+FROM flink:1.15.0
+
+
+RUN apt-get update -y && \","[{'comment': 'Add the comments `install python3: it has updated Python to 3.9 in Debian 11 and so install Python 3.7 from source`, `it currently only supports Python 3.6, 3.7 and 3.8 in PyFlink officially.` or add the comments for the link: [using-flink-python-on-docker](https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/deployment/resource-providers/standalone/docker/#using-flink-python-on-docker)', 'commenter': 'SteNicholas'}, {'comment': 'Good point. Fixed in df65337a15f05736a0e11034bfa114f84fd68fab', 'commenter': 'bgeng777'}]"
306,examples/flink-python-example/README.md,"@@ -0,0 +1,83 @@
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+  http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+
+# Flink Kubernetes Operator Python Example
+
+## Overview
+
+This is an end-to-end example of running Flink Python jobs using the Flink Kubernetes Operator.
+
+
+*What's in this example?*
+
+ 1. Python script of a simple streaming job
+ 2. DockerFile to build custom image with pyflink and python demo
+ 3. Example YAML for submitting the python job using the operator
+
+## How does it work?
+
+Flink supports Python jobs in application mode by utilizing `org.apache.flink.client.python.PythonDriver` class as the 
+entry class. With the Flink Kubernetes Operator, we can reuse this class to run Python jobs as well. 
+
+The class is packaged in flink-python_${scala_version}-${flink_version}.jar which is in the default Flink image.
+So we do not need to create a new job jar. Instead, we just set `entryClass` of the job crd to 
+`org.apache.flink.client.python.PythonDriver`. After applying the job yaml, the launched job manager pod will run the `main()` 
+method of PythonDriver and parse arguments declared in the `args` field of the job crd.
+
+Note, in `args` field, users must either specify `-py` option or `-pym` option. ","[{'comment': ""Could the document of `args` provide user with [submitting-pyflink-jobs](https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/deployment/cli/#submitting-pyflink-jobs) reference? Otherwise the user doesn't know other PyFlink submission arguments."", 'commenter': 'SteNicholas'}, {'comment': 'Improved in df65337a15f05736a0e11034bfa114f84fd68fab', 'commenter': 'bgeng777'}]"
306,examples/flink-python-example/Dockerfile,"@@ -0,0 +1,43 @@
+################################################################################
+#  Licensed to the Apache Software Foundation (ASF) under one
+#  or more contributor license agreements.  See the NOTICE file
+#  distributed with this work for additional information
+#  regarding copyright ownership.  The ASF licenses this file
+#  to you under the Apache License, Version 2.0 (the
+#  ""License""); you may not use this file except in compliance
+#  with the License.  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an ""AS IS"" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+# limitations under the License.
+################################################################################
+# Check https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/deployment/resource-providers/standalone/docker/#using-flink-python-on-docker for more details
+FROM flink:1.15.0
+
+# install python3: it has updated Python to 3.9 in Debian 11 and so install Python 3.7 from source, \
+# it currently only supports Python 3.6, 3.7 and 3.8 in PyFlink officially.
+
+RUN apt-get update -y && \
+apt-get install -y build-essential libssl-dev zlib1g-dev libbz2-dev libffi-dev && \
+wget https://www.python.org/ftp/python/3.7.9/Python-3.7.9.tgz && \
+tar -xvf Python-3.7.9.tgz && \
+cd Python-3.7.9 && \
+./configure --without-tests --enable-shared && \
+make -j6 && \
+make install && \
+ldconfig /usr/local/lib && \
+cd .. && rm -f Python-3.7.9.tgz && rm -rf Python-3.7.9 && \
+ln -s /usr/local/bin/python3 /usr/local/bin/python && \
+apt-get clean && \
+rm -rf /var/lib/apt/lists/*
+
+# install PyFlink
+RUN pip3 install apache-flink==1.15.0
+
+# ddd python script","[{'comment': 'Typo?', 'commenter': 'wangyang0918'}]"
306,examples/flink-python-example/Dockerfile,"@@ -0,0 +1,43 @@
+################################################################################
+#  Licensed to the Apache Software Foundation (ASF) under one
+#  or more contributor license agreements.  See the NOTICE file
+#  distributed with this work for additional information
+#  regarding copyright ownership.  The ASF licenses this file
+#  to you under the Apache License, Version 2.0 (the
+#  ""License""); you may not use this file except in compliance
+#  with the License.  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an ""AS IS"" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+# limitations under the License.
+################################################################################
+# Check https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/deployment/resource-providers/standalone/docker/#using-flink-python-on-docker for more details
+FROM flink:1.15.0
+
+# install python3: it has updated Python to 3.9 in Debian 11 and so install Python 3.7 from source, \
+# it currently only supports Python 3.6, 3.7 and 3.8 in PyFlink officially.
+
+RUN apt-get update -y && \
+apt-get install -y build-essential libssl-dev zlib1g-dev libbz2-dev libffi-dev && \
+wget https://www.python.org/ftp/python/3.7.9/Python-3.7.9.tgz && \
+tar -xvf Python-3.7.9.tgz && \
+cd Python-3.7.9 && \
+./configure --without-tests --enable-shared && \
+make -j6 && \
+make install && \
+ldconfig /usr/local/lib && \
+cd .. && rm -f Python-3.7.9.tgz && rm -rf Python-3.7.9 && \
+ln -s /usr/local/bin/python3 /usr/local/bin/python && \
+apt-get clean && \
+rm -rf /var/lib/apt/lists/*
+
+# install PyFlink
+RUN pip3 install apache-flink==1.15.0
+
+# ddd python script
+RUN mkdir /opt/flink/usrlib","[{'comment': '```\r\nUSER flink\r\n```\r\nWe need to change the user to flink before the following command.', 'commenter': 'wangyang0918'}]"
306,examples/flink-python-example/python_demo.py,"@@ -0,0 +1,49 @@
+################################################################################
+#  Licensed to the Apache Software Foundation (ASF) under one
+#  or more contributor license agreements.  See the NOTICE file
+#  distributed with this work for additional information
+#  regarding copyright ownership.  The ASF licenses this file
+#  to you under the Apache License, Version 2.0 (the
+#  ""License""); you may not use this file except in compliance
+#  with the License.  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an ""AS IS"" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+# limitations under the License.
+################################################################################
+import logging
+import sys
+
+from pyflink.datastream import StreamExecutionEnvironment
+from pyflink.table import StreamTableEnvironment
+
+
+def python_demo():
+    env = StreamExecutionEnvironment.get_execution_environment()
+    env.set_parallelism(1)
+
+    t_env = StreamTableEnvironment.create(stream_execution_environment=env)
+    t_env.execute_sql(""""""
+    CREATE TABLE orders (
+      order_number BIGINT,
+      price        DECIMAL(32,2),
+      buyer        ROW<first_name STRING, last_name STRING>,
+      order_time   TIMESTAMP(3)
+    ) WITH (
+      'connector' = 'datagen'
+    )"""""")
+
+    t_env.execute_sql(""""""
+        CREATE TABLE print_table WITH ('connector' = 'print')
+          LIKE orders"""""")
+    t_env.execute_sql(""""""
+        INSERT INTO print_table SELECT * FROM orders"""""")
+
+
+if __name__ == '__main__':
+    logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=""%(message)s"")
+    python_demo()","[{'comment': 'We would better have an empty line here.', 'commenter': 'wangyang0918'}]"
307,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/observer/sessionjob/SessionJobObserver.java,"@@ -115,4 +131,62 @@ public void observe(FlinkSessionJob flinkSessionJob, Context context) {
         }
         SavepointUtils.resetTriggerIfJobNotRunning(flinkSessionJob, eventRecorder);
     }
+
+    private void checkIfAlreadyUpgraded(
+            FlinkSessionJob flinkSessionJob, Configuration deployedConfig) {
+        var uid = flinkSessionJob.getMetadata().getUid();
+        Collection<JobStatusMessage> jobStatusMessages;
+        try {
+            jobStatusMessages = flinkService.listJobs(deployedConfig);
+        } catch (Exception e) {
+            throw new RuntimeException(""Failed to list jobs"", e);
+        }
+        var matchedJobs = new ArrayList<JobID>();
+        for (JobStatusMessage jobStatusMessage : jobStatusMessages) {
+            var jobId = jobStatusMessage.getJobId();
+            if (jobId.getLowerPart() == uid.hashCode()
+                    && !jobStatusMessage.getJobState().isGloballyTerminalState()) {
+                matchedJobs.add(jobId);
+            }
+        }
+
+        if (matchedJobs.isEmpty()) {
+            return;
+        } else if (matchedJobs.size() > 1) {
+            // this indicates a bug, which means we have more than one running job for a single
+            // SessionJob CR.
+            throw new RuntimeException(
+                    String.format(
+                            ""Unexpected case: %d job found for the resource with uid: %s"",
+                            matchedJobs.size(), flinkSessionJob.getMetadata().getUid()));
+        } else {
+            var matchedJobID = matchedJobs.get(0);
+            Long upgradeTargetGeneration =
+                    ReconciliationUtils.getUpgradeTargetGeneration(flinkSessionJob);
+            long deployedGeneration = matchedJobID.getUpperPart();
+
+            if (upgradeTargetGeneration == deployedGeneration) {
+                var oldJobID = flinkSessionJob.getStatus().getJobStatus().getJobId();
+                LOG.info(
+                        ""Pending upgrade is already deployed, updating status. Old jobID:{}, new jobID:{}"",
+                        oldJobID,
+                        matchedJobID.toHexString());
+                ReconciliationUtils.updateStatusForAlreadyUpgraded(flinkSessionJob);
+                flinkSessionJob
+                        .getStatus()
+                        .getJobStatus()
+                        .setState(org.apache.flink.api.common.JobStatus.RECONCILING.name());
+                flinkSessionJob
+                        .getStatus()
+                        .getJobStatus()
+                        .setJobId(matchedJobs.get(0).toHexString());
+            } else {
+                LOG.warn(
+                        ""Running job {}'s generation {} doesn't match upgrade target generation {}."",
+                        matchedJobID.toHexString(),
+                        deployedGeneration,
+                        upgradeTargetGeneration);
+            }","[{'comment': 'I think it might make sense to throw an error at this point. Otherwise we get double submission because the new job will have a new jobid due to the different generation.', 'commenter': 'gyfora'}, {'comment': 'I think we have a chance to get here:\r\n\r\n1. The upgrade fails between the `UPGRADING` status recorded and deploy\r\n2. The deployedGeneration will smaller than the targetGeneration\r\n\r\nIn this case, we have to wait for the next turn to deal with reconcileSpecChange. It will cancel the oldJobID in jobStatus and submit with new JobID.', 'commenter': 'Aitozi'}, {'comment': 'For safe, I think we could throw an exception here if the matchedJobID do not equal to the JobID in the JobStatus, too. WDTY ? ', 'commenter': 'Aitozi'}, {'comment': 'Sorry I donâ€™t completely understand.\r\n\r\nwhen we are in UPGRADING state the old job is already suspended. We should never have upgrading state + still the old job running ', 'commenter': 'gyfora'}, {'comment': 'You are right, Updated.', 'commenter': 'Aitozi'}]"
308,docs/content/docs/custom-resource/pod-template.md,"@@ -100,6 +100,66 @@ spec:
     parallelism: 2
 ```
 
+We also provide a way to obtain the jar package remotely, which can avoid you repeating the packaging image every time","[{'comment': 'nit: ""package remotely, which can avoid you repeating the packaging image every time"" -> ""package remotely to avoid repeating the image packaging on each iteration.""', 'commenter': 'mbalassi'}]"
308,docs/content/docs/custom-resource/pod-template.md,"@@ -100,6 +100,66 @@ spec:
     parallelism: 2
 ```
 
+We also provide a way to obtain the jar package remotely, which can avoid you repeating the packaging image every time
+```yaml
+apiVersion: flink.apache.org/v1beta1
+kind: FlinkDeployment
+metadata:
+  namespace: default
+  name: pod-template-example
+spec:
+  image: flink:1.15
+  flinkVersion: v1_15","[{'comment': 'please add `spec.serviceAccount: flink`', 'commenter': 'mbalassi'}]"
308,examples/pod-template-artifact-fetcher.yaml,"@@ -0,0 +1,84 @@
+################################################################################","[{'comment': 'Pls remove this file, this is just a duplicate', 'commenter': 'morhidi'}]"
312,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/metrics/OperatorJosdkMetrics.java,"@@ -43,21 +47,41 @@ public class OperatorJosdkMetrics implements Metrics {
     private static final String RECONCILIATION = ""Reconciliation"";
     private static final String RESOURCE = ""Resource"";
     private static final String EVENT = ""Event"";
+    private static final int WINDOW_SIZE = 1000;
 
     private final KubernetesOperatorMetricGroup operatorMetricGroup;
     private final Configuration conf;
+    private final Clock clock;
 
     private final Map<ResourceID, KubernetesResourceNamespaceMetricGroup> resourceNsMetricGroups =
             new ConcurrentHashMap<>();
     private final Map<ResourceID, KubernetesResourceMetricGroup> resourceMetricGroups =
             new ConcurrentHashMap<>();
 
+    private final Map<String, Histogram> histograms = new ConcurrentHashMap<>();
     private final Map<String, Counter> counters = new ConcurrentHashMap<>();
 
     public OperatorJosdkMetrics(
             KubernetesOperatorMetricGroup operatorMetricGroup, Configuration conf) {
         this.operatorMetricGroup = operatorMetricGroup;
         this.conf = conf;
+        this.clock = SystemClock.getInstance();
+    }
+
+    @Override
+    public <T> T timeControllerExecution(ControllerExecution<T> execution) throws Exception {
+        long startTime = clock.relativeTimeNanos();
+        try {
+            T result = execution.execute();
+            String successType = execution.successTypeName(result);
+            histogram(execution.controllerName(), execution.name(), successType)","[{'comment': 'Instead if using the controllerName directly could we please map this to `FlinkDeployment` / `FlinkSessionJob` based on the name? This would make it consistent with other metrics', 'commenter': 'gyfora'}]"
312,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/metrics/OperatorJosdkMetrics.java,"@@ -113,6 +137,19 @@ public void failedReconciliation(ResourceID resourceID, Exception exception) {
         return map;
     }
 
+    private Histogram histogram(String... names) {
+        MetricGroup group = operatorMetricGroup.addGroup(OPERATOR_SDK_GROUP);
+        for (String name : names) {
+            group = group.addGroup(name);
+        }
+        var finalGroup = group;
+        return histograms.computeIfAbsent(
+                String.join(""."", group.getScopeComponents()),
+                s ->
+                        finalGroup.histogram(","[{'comment': 'Instead of `Nanos` I suggest we use Seconds and name it to `TimeSeconds` to make it consistent with other metrics. ', 'commenter': 'gyfora'}, {'comment': '@gyfora, IMO, the `TimeNanos` makes sense because this use the `clock.relativeTimeNanos` not the seconds. WDYT?', 'commenter': 'SteNicholas'}, {'comment': 'Please convert the nano time to seconds before calling it `TimeSeconds` :D ', 'commenter': 'gyfora'}]"
313,docs/content/docs/operations/olm.md,"@@ -0,0 +1,247 @@
+---
+title: ""OLM""","[{'comment': 'nit: Operator Lifecycle Manager', 'commenter': 'mbalassi'}]"
313,docs/content/docs/operations/olm.md,"@@ -0,0 +1,247 @@
+---
+title: ""OLM""
+weight: 1","[{'comment': 'let us make this 5, so that it gets added right with upgrades', 'commenter': 'mbalassi'}]"
313,docs/content/docs/operations/olm.md,"@@ -0,0 +1,247 @@
+---
+title: ""OLM""
+weight: 1
+type: docs
+aliases:
+- /operations/olm.html
+---
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+  http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+
+# Managing Flink Operator Using OLM
+The Operator Lifecycle Management(OLM) helps install, update, and manage the lifecycle of all Operators in a Kubernetes Clusters. The [Operatorhub](https://operatorhub.io/) contains a catalog of community shared Kubernetes operators which are ready to be deployed using OLM.
+## Prerequisite
+1. Kubernetes Cluster. Use [kind](https://kind.sigs.k8s.io/docs/user/quick-start/) to create cluster locally if needed.","[{'comment': 'Did you test with minikube also? Our user examples use that, would be nice to keep consistent:\r\nhttps://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/try-flink-kubernetes-operator/quick-start/', 'commenter': 'mbalassi'}, {'comment': 'Make sense. Let me verify that.', 'commenter': 'tedhtchang'}]"
313,docs/content/docs/operations/olm.md,"@@ -0,0 +1,247 @@
+---
+title: ""OLM""
+weight: 1
+type: docs
+aliases:
+- /operations/olm.html
+---
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+  http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+
+# Managing Flink Operator Using OLM
+The Operator Lifecycle Management(OLM) helps install, update, and manage the lifecycle of all Operators in a Kubernetes Clusters. The [Operatorhub](https://operatorhub.io/) contains a catalog of community shared Kubernetes operators which are ready to be deployed using OLM.
+## Prerequisite
+1. Kubernetes Cluster. Use [kind](https://kind.sigs.k8s.io/docs/user/quick-start/) to create cluster locally if needed.
+2. Install the `operator-sdk` CLI from [here](https://sdk.operatorframework.io/docs/installation/).
+
+    The CLI binary provides commands to easily install/uninstall OLM in a Kubernetes cluster and also facilitates the development process of OLM bundles.
+## Quick Start
+
+1. Install OLM on your Kubernetes Cluster.
+   ```sh
+   operator-sdk olm install
+   ```
+
+   Once installed, you should see similar message below:
+   ```
+    operator-sdk olm install
+    INFO[0000] Fetching resources for resolved version ""latest""
+    INFO[0018] Creating CRDs and resources
+    INFO[0018]   Creating CustomResourceDefinition ""catalogsources.operators.coreos.com""
+    INFO[0018]   Creating CustomResourceDefinition ""clusterserviceversions.operators.coreos.com""
+    INFO[0018]   Creating CustomResourceDefinition ""installplans.operators.coreos.com""
+    INFO[0019]   Creating CustomResourceDefinition ""olmconfigs.operators.coreos.com""
+    INFO[0019]   Creating CustomResourceDefinition ""operatorconditions.operators.coreos.com""
+    INFO[0019]   Creating CustomResourceDefinition ""operatorgroups.operators.coreos.com""
+    INFO[0019]   Creating CustomResourceDefinition ""operators.operators.coreos.com""
+    INFO[0019]   Creating CustomResourceDefinition ""subscriptions.operators.coreos.com""
+    INFO[0019]   Creating Namespace ""olm""
+    INFO[0019]   Creating Namespace ""operators""
+    INFO[0019]   Creating ServiceAccount ""olm/olm-operator-serviceaccount""
+    INFO[0019]   Creating ClusterRole ""system:controller:operator-lifecycle-manager""
+    INFO[0019]   Creating ClusterRoleBinding ""olm-operator-binding-olm""
+    INFO[0019]   Creating OLMConfig ""cluster""
+    INFO[0022]   Creating Deployment ""olm/olm-operator""
+    INFO[0022]   Creating Deployment ""olm/catalog-operator""
+    INFO[0022]   Creating ClusterRole ""aggregate-olm-edit""
+    INFO[0022]   Creating ClusterRole ""aggregate-olm-view""
+    INFO[0022]   Creating OperatorGroup ""operators/global-operators""
+    INFO[0022]   Creating OperatorGroup ""olm/olm-operators""
+    INFO[0022]   Creating ClusterServiceVersion ""olm/packageserver""
+    INFO[0022]   Creating CatalogSource ""olm/operatorhubio-catalog""
+    INFO[0022] Waiting for deployment/olm-operator rollout to complete
+    INFO[0022]   Waiting for Deployment ""olm/olm-operator"" to rollout: 0 of 1 updated replicas are available
+    INFO[0052]   Deployment ""olm/olm-operator"" successfully rolled out
+    INFO[0052] Waiting for deployment/catalog-operator rollout to complete
+    INFO[0052]   Waiting for Deployment ""olm/catalog-operator"" to rollout: 0 of 1 updated replicas are available
+    INFO[0053]   Deployment ""olm/catalog-operator"" successfully rolled out
+    INFO[0053] Waiting for deployment/packageserver rollout to complete
+    INFO[0053]   Waiting for Deployment ""olm/packageserver"" to rollout: 0 of 2 updated replicas are available
+    INFO[0057]   Deployment ""olm/packageserver"" successfully rolled out
+    INFO[0058] Successfully installed OLM version ""latest""
+
+    NAME                                            NAMESPACE    KIND                        STATUS
+    catalogsources.operators.coreos.com                          CustomResourceDefinition    Installed
+    clusterserviceversions.operators.coreos.com                  CustomResourceDefinition    Installed
+    installplans.operators.coreos.com                            CustomResourceDefinition    Installed
+    olmconfigs.operators.coreos.com                              CustomResourceDefinition    Installed
+    operatorconditions.operators.coreos.com                      CustomResourceDefinition    Installed
+    operatorgroups.operators.coreos.com                          CustomResourceDefinition    Installed
+    operators.operators.coreos.com                               CustomResourceDefinition    Installed
+    subscriptions.operators.coreos.com                           CustomResourceDefinition    Installed
+    olm                                                          Namespace                   Installed
+    operators                                                    Namespace                   Installed
+    olm-operator-serviceaccount                     olm          ServiceAccount              Installed
+    system:controller:operator-lifecycle-manager                 ClusterRole                 Installed
+    olm-operator-binding-olm                                     ClusterRoleBinding          Installed
+    cluster                                                      OLMConfig                   Installed
+    olm-operator                                    olm          Deployment                  Installed
+    catalog-operator                                olm          Deployment                  Installed
+    aggregate-olm-edit                                           ClusterRole                 Installed
+    aggregate-olm-view                                           ClusterRole                 Installed
+    global-operators                                operators    OperatorGroup               Installed
+    olm-operators                                   olm          OperatorGroup               Installed
+    packageserver                                   olm          ClusterServiceVersion       Installed
+    operatorhubio-catalog                           olm          CatalogSource               Installed
+    ```
+
+2. Install Flink Operator
+    ```sh
+    cat << EOF | kubectl apply -f -
+    apiVersion: operators.coreos.com/v1alpha2
+    kind: OperatorGroup
+    metadata:
+      name: default-og
+      namespace: default
+    ---
+    apiVersion: operators.coreos.com/v1alpha1
+    kind: Subscription
+    metadata:
+      name: flink-operator
+      namespace: default
+    spec:
+      channel: alpha
+      name: flink-kubernetes-operator
+      source: operatorhubio-catalog
+      sourceNamespace: olm
+    EOF
+    ```
+    Watch the installation process
+    ```sh
+    kubectl get csv --watch
+    ```
+
+    When installed successfully you should see
+    ```
+    NAME                               DISPLAY                     VERSION   REPLACES   PHASE
+    flink-kubernetes-operator.v1.0.1   Flink Kubernetes Operator   1.0.1                Succeeded
+    ```
+3. (Optional) [Submit a Flink Job](https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/try-flink-kubernetes-operator/quick-start/#submitting-a-flink-job)
+
+
+# OLM Bundle Development Process
+The https://github.com/k8s-operatorhub/community-operators repo is the canonical source of the community operators which can be deployed by OLM. Developers may share their operators by submitting OLM bundles. The steps below demonstrate the process of updating, testing the OLM bundle locally, and submit a PR.
+
+1. Download the [opm cli](https://github.com/operator-framework/operator-registry/releases). This binary is used to build a catalog image.
+    ```sh
+    # using Mac as an example
+    chomod +x <pathto>/darwin-amd64-opm
+    mv <pathto>/darwin-amd64-opm /usr/local/bin/opm
+    ```
+
+2. Modify the exiting bundle
+    ```
+    git clone https://github.com/k8s-operatorhub/community-operators.git
+    cd community-operators/operators/flink-kubernetes-operator
+    ```
+    Flink operator is packaged as [bundle format](https://github.com/operator-framework/community-operators/blob/master/docs/testing-operators.md#bundle-format-supported-with-0140-or-newer). The top level folders are operator versions containing the manifests and metadata folders.","[{'comment': 'nit: packaged as a bundle', 'commenter': 'mbalassi'}]"
313,docs/content/docs/operations/olm.md,"@@ -0,0 +1,247 @@
+---
+title: ""OLM""
+weight: 1
+type: docs
+aliases:
+- /operations/olm.html
+---
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+  http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+
+# Managing Flink Operator Using OLM
+The Operator Lifecycle Management(OLM) helps install, update, and manage the lifecycle of all Operators in a Kubernetes Clusters. The [Operatorhub](https://operatorhub.io/) contains a catalog of community shared Kubernetes operators which are ready to be deployed using OLM.
+## Prerequisite
+1. Kubernetes Cluster. Use [kind](https://kind.sigs.k8s.io/docs/user/quick-start/) to create cluster locally if needed.
+2. Install the `operator-sdk` CLI from [here](https://sdk.operatorframework.io/docs/installation/).
+
+    The CLI binary provides commands to easily install/uninstall OLM in a Kubernetes cluster and also facilitates the development process of OLM bundles.
+## Quick Start
+
+1. Install OLM on your Kubernetes Cluster.
+   ```sh
+   operator-sdk olm install
+   ```
+
+   Once installed, you should see similar message below:
+   ```
+    operator-sdk olm install
+    INFO[0000] Fetching resources for resolved version ""latest""
+    INFO[0018] Creating CRDs and resources
+    INFO[0018]   Creating CustomResourceDefinition ""catalogsources.operators.coreos.com""
+    INFO[0018]   Creating CustomResourceDefinition ""clusterserviceversions.operators.coreos.com""
+    INFO[0018]   Creating CustomResourceDefinition ""installplans.operators.coreos.com""
+    INFO[0019]   Creating CustomResourceDefinition ""olmconfigs.operators.coreos.com""
+    INFO[0019]   Creating CustomResourceDefinition ""operatorconditions.operators.coreos.com""
+    INFO[0019]   Creating CustomResourceDefinition ""operatorgroups.operators.coreos.com""
+    INFO[0019]   Creating CustomResourceDefinition ""operators.operators.coreos.com""
+    INFO[0019]   Creating CustomResourceDefinition ""subscriptions.operators.coreos.com""
+    INFO[0019]   Creating Namespace ""olm""
+    INFO[0019]   Creating Namespace ""operators""
+    INFO[0019]   Creating ServiceAccount ""olm/olm-operator-serviceaccount""
+    INFO[0019]   Creating ClusterRole ""system:controller:operator-lifecycle-manager""
+    INFO[0019]   Creating ClusterRoleBinding ""olm-operator-binding-olm""
+    INFO[0019]   Creating OLMConfig ""cluster""
+    INFO[0022]   Creating Deployment ""olm/olm-operator""
+    INFO[0022]   Creating Deployment ""olm/catalog-operator""
+    INFO[0022]   Creating ClusterRole ""aggregate-olm-edit""
+    INFO[0022]   Creating ClusterRole ""aggregate-olm-view""
+    INFO[0022]   Creating OperatorGroup ""operators/global-operators""
+    INFO[0022]   Creating OperatorGroup ""olm/olm-operators""
+    INFO[0022]   Creating ClusterServiceVersion ""olm/packageserver""
+    INFO[0022]   Creating CatalogSource ""olm/operatorhubio-catalog""
+    INFO[0022] Waiting for deployment/olm-operator rollout to complete
+    INFO[0022]   Waiting for Deployment ""olm/olm-operator"" to rollout: 0 of 1 updated replicas are available
+    INFO[0052]   Deployment ""olm/olm-operator"" successfully rolled out
+    INFO[0052] Waiting for deployment/catalog-operator rollout to complete
+    INFO[0052]   Waiting for Deployment ""olm/catalog-operator"" to rollout: 0 of 1 updated replicas are available
+    INFO[0053]   Deployment ""olm/catalog-operator"" successfully rolled out
+    INFO[0053] Waiting for deployment/packageserver rollout to complete
+    INFO[0053]   Waiting for Deployment ""olm/packageserver"" to rollout: 0 of 2 updated replicas are available
+    INFO[0057]   Deployment ""olm/packageserver"" successfully rolled out
+    INFO[0058] Successfully installed OLM version ""latest""
+
+    NAME                                            NAMESPACE    KIND                        STATUS
+    catalogsources.operators.coreos.com                          CustomResourceDefinition    Installed
+    clusterserviceversions.operators.coreos.com                  CustomResourceDefinition    Installed
+    installplans.operators.coreos.com                            CustomResourceDefinition    Installed
+    olmconfigs.operators.coreos.com                              CustomResourceDefinition    Installed
+    operatorconditions.operators.coreos.com                      CustomResourceDefinition    Installed
+    operatorgroups.operators.coreos.com                          CustomResourceDefinition    Installed
+    operators.operators.coreos.com                               CustomResourceDefinition    Installed
+    subscriptions.operators.coreos.com                           CustomResourceDefinition    Installed
+    olm                                                          Namespace                   Installed
+    operators                                                    Namespace                   Installed
+    olm-operator-serviceaccount                     olm          ServiceAccount              Installed
+    system:controller:operator-lifecycle-manager                 ClusterRole                 Installed
+    olm-operator-binding-olm                                     ClusterRoleBinding          Installed
+    cluster                                                      OLMConfig                   Installed
+    olm-operator                                    olm          Deployment                  Installed
+    catalog-operator                                olm          Deployment                  Installed
+    aggregate-olm-edit                                           ClusterRole                 Installed
+    aggregate-olm-view                                           ClusterRole                 Installed
+    global-operators                                operators    OperatorGroup               Installed
+    olm-operators                                   olm          OperatorGroup               Installed
+    packageserver                                   olm          ClusterServiceVersion       Installed
+    operatorhubio-catalog                           olm          CatalogSource               Installed
+    ```
+
+2. Install Flink Operator
+    ```sh
+    cat << EOF | kubectl apply -f -
+    apiVersion: operators.coreos.com/v1alpha2
+    kind: OperatorGroup
+    metadata:
+      name: default-og
+      namespace: default
+    ---
+    apiVersion: operators.coreos.com/v1alpha1
+    kind: Subscription
+    metadata:
+      name: flink-operator
+      namespace: default
+    spec:
+      channel: alpha
+      name: flink-kubernetes-operator
+      source: operatorhubio-catalog
+      sourceNamespace: olm
+    EOF
+    ```
+    Watch the installation process
+    ```sh
+    kubectl get csv --watch
+    ```
+
+    When installed successfully you should see
+    ```
+    NAME                               DISPLAY                     VERSION   REPLACES   PHASE
+    flink-kubernetes-operator.v1.0.1   Flink Kubernetes Operator   1.0.1                Succeeded
+    ```
+3. (Optional) [Submit a Flink Job](https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/try-flink-kubernetes-operator/quick-start/#submitting-a-flink-job)
+
+
+# OLM Bundle Development Process
+The https://github.com/k8s-operatorhub/community-operators repo is the canonical source of the community operators which can be deployed by OLM. Developers may share their operators by submitting OLM bundles. The steps below demonstrate the process of updating, testing the OLM bundle locally, and submit a PR.
+
+1. Download the [opm cli](https://github.com/operator-framework/operator-registry/releases). This binary is used to build a catalog image.
+    ```sh
+    # using Mac as an example
+    chomod +x <pathto>/darwin-amd64-opm
+    mv <pathto>/darwin-amd64-opm /usr/local/bin/opm
+    ```
+
+2. Modify the exiting bundle
+    ```
+    git clone https://github.com/k8s-operatorhub/community-operators.git
+    cd community-operators/operators/flink-kubernetes-operator
+    ```
+    Flink operator is packaged as [bundle format](https://github.com/operator-framework/community-operators/blob/master/docs/testing-operators.md#bundle-format-supported-with-0140-or-newer). The top level folders are operator versions containing the manifests and metadata folders.
+    ```
+    tree flink-kubernetes-operator/
+    â”œâ”€â”€ 1.0.1
+    â”‚Â Â  â”œâ”€â”€ bundle.Dockerfile
+    â”‚Â Â  â”œâ”€â”€ manifests
+    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ flink-keystore.secret.yaml
+    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ flink-kubernetes-operator.clusterserviceversion.yaml
+    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ flink-kubernetes-operator.configmap.yaml
+    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ flinkdeployments.flink.apache.org-v1.yml
+    â”‚Â Â  â”‚Â Â  â””â”€â”€ flinksessionjobs.flink.apache.org-v1.yml
+    â”‚Â Â  â””â”€â”€ metadata
+    â”‚Â Â      â””â”€â”€ annotations.yaml
+    â””â”€â”€ ci.yaml
+    ```
+    The manifests folder must have one ClusterServiceVersion(CSV) and CustomResourceDefiniton(CRD) manifests. The CSV
+    is used to define deployments, RBACs, required CRDs, and WebhookDefinitions, etc. Applying the CSV will automatically generate resources that are necessary to run the operator. A list of of [supported resources](https://olm.operatorframework.io/docs/tasks/creating-operator-manifests/#packaging-additional-objects-alongside-an-operator), such as configmap and secret, may also be included in the manifests folder. See [CSV how-to](https://olm.operatorframework.io/docs/tasks/creating-operator-manifests/#writing-your-operator-manifests) for detail.
+
+3. Test locally
+
+    Before submitting any changes, do the following steps to validate and test operator deployment locally.
+
+    To validate bundle format:
+    ```sh
+    cd community-operators/operators/flink-kubernetes-operator/1.0.1/
+    operator-sdk bundle validate ./ --select-optional name=operatorhub
+    ```
+    You should see the following if the bundle is valid:
+    ```
+    INFO[0000] All validation tests have completed successfully
+    ```
+
+    Build bundle image and publish
+    ```sh
+    export namespace=<your namespace>
+    docker build -f bundle.Dockerfile -t docker.io/${namespace}/my-flink-operator-bundle:1.0.1
+    docker push docker.io/${namespace}/my-flink-operator-bundle:1.0.1
+    ```
+
+    Build catalog image and publish
+    ```
+    opm index add --bundles docker.io/${namespace}/my-flink-operator-bundle:1.0.1 --tag docker.io/${namespace}/my-flink-catalog:latest
+    docker push docker.io/${namespace}/my-flink-catalog:latest
+    ```
+
+    Deploy your own catalog service into the cluster
+    ```sh
+    cat <<EOF | kubectl apply -f -
+    apiVersion: operators.coreos.com/v1alpha1
+    kind: CatalogSource
+    metadata:
+      name: my-flink-catalog
+      namespace: default
+    spec:
+      displayName: â€œmy flink catalogâ€
+      sourceType: grpc
+      image: docker.io/${namespace}/my-flink-catalog:latest
+    EOF
+    ```
+    Once it is running you should see messages similar to
+    ```
+    kubectl get catsrc
+    NAME                     READY   STATUS    RESTARTS   AGE
+    my-flink-catalog-7vcjr   1/1     Running   0          29s
+    ```
+    Install the operator by subscribing it from your catalog service
+    ```sh
+    cat <<EOF | kubectl apply -f -
+    apiVersion: operators.coreos.com/v1alpha2
+    kind: OperatorGroup
+    metadata:
+      name: default-og
+      namespace: default
+    ---
+    apiVersion: operators.coreos.com/v1alpha1
+    kind: Subscription
+    metadata:
+      name: my-flink-operator-subscription
+      namespace: default
+    spec:
+      channel: alpha
+      name: flink-kubernetes-operator
+      source: my-flink-catalog
+      sourceNamespace: default
+    EOF
+    ```
+    Once installed, you should see
+    ```
+    kubectl get csv
+    NAME                               DISPLAY                     VERSION   REPLACES   PHASE
+    flink-kubernetes-operator.v1.0.1   Flink Kubernetes Operator   1.0.1                Succeeded
+    ```
+    Proceed to  [submit a Flink Job](https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/try-flink-kubernetes-operator/quick-start/#submitting-a-flink-job) and verify logs.","[{'comment': 'nit: extra space in `to  submit` ', 'commenter': 'mbalassi'}]"
317,examples/README.md,"@@ -0,0 +1,175 @@
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+  http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+
+# Flink Kubernetes Operator Examples
+
+## Overview
+
+This directory contains few examples for Flink Kubernetes Operator.","[{'comment': 'nit: for _the_ Flink Kubernetes', 'commenter': 'mbalassi'}, {'comment': 'Fixed.\r\nThanks!', 'commenter': 'pvary'}]"
317,examples/README.md,"@@ -0,0 +1,175 @@
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+  http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+
+# Flink Kubernetes Operator Examples
+
+## Overview
+
+This directory contains few examples for Flink Kubernetes Operator.
+These examples should only serve as starting points when familiarizing yourself with the
+Flink Kubernetes Operator and users are expected to extend these based on their production needs.
+
+## Usage
+
+Please refer to the [Quick Start](https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/try-flink-kubernetes-operator/quick-start)
+to run the examples.","[{'comment': '... for setting up your environment to run the examples.', 'commenter': 'mbalassi'}, {'comment': 'Fixed', 'commenter': 'pvary'}]"
317,examples/README.md,"@@ -0,0 +1,175 @@
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+  http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+
+# Flink Kubernetes Operator Examples
+
+## Overview
+
+This directory contains few examples for Flink Kubernetes Operator.
+These examples should only serve as starting points when familiarizing yourself with the
+Flink Kubernetes Operator and users are expected to extend these based on their production needs.
+
+## Usage
+
+Please refer to the [Quick Start](https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/try-flink-kubernetes-operator/quick-start)
+to run the examples.
+
+## Examples
+
+### Basic Application Deployment example
+
+This is a simple deployment defined by a minimal deployment file.
+The configuration contains the following:
+- Defines the job to run
+- Assigns the resources available for the job
+- Defines the parallelism used
+
+To run the job submit the yaml file using kubectl:
+```bash
+cd examples","[{'comment': 'Since this Readme is in `examples` you can omit this imho.', 'commenter': 'mbalassi'}, {'comment': 'Also feel free to omit it in later occurances up to this level of the directory structure.', 'commenter': 'mbalassi'}, {'comment': 'Removed', 'commenter': 'pvary'}]"
317,examples/README.md,"@@ -0,0 +1,175 @@
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+  http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+
+# Flink Kubernetes Operator Examples
+
+## Overview
+
+This directory contains few examples for Flink Kubernetes Operator.
+These examples should only serve as starting points when familiarizing yourself with the
+Flink Kubernetes Operator and users are expected to extend these based on their production needs.
+
+## Usage
+
+Please refer to the [Quick Start](https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/try-flink-kubernetes-operator/quick-start)
+to run the examples.
+
+## Examples
+
+### Basic Application Deployment example
+
+This is a simple deployment defined by a minimal deployment file.
+The configuration contains the following:
+- Defines the job to run
+- Assigns the resources available for the job
+- Defines the parallelism used
+
+To run the job submit the yaml file using kubectl:
+```bash
+cd examples
+kubectl apply -f basic.yaml
+```
+
+### Basic Session Deployment example
+
+This example shows how to create a basic Session Cluster and then how to submit specific jobs to this cluster if needed.
+
+#### Without jobs 
+
+The Flink Deployment could be created without any jobs.
+In this case the Flink jobs could be created later by submitting the jobs
+separately.
+
+The Flink Deployment configuration contains the following:
+- The name of the Flink Deployment
+- The resources available for the Flink Deployment
+
+The Flink Deployment configuration does __NOT__ contain the following:
+- The job to run
+- Any job specific configurations
+
+To create a Flink Deployment with the specific resources without any jobs run the following command:
+```bash
+cd examples
+kubectl apply -f basic-session-deployment-only.yaml
+```
+
+##### Adding jobs
+
+For an existing Flink Deployment another configuration could be used to create new jobs.
+This configuration should contain the following:
+- The Flink Deployment to use
+- The job to run
+- Any job specific configurations
+
+If the Flink Deployment is created by `basic-session-deployment-only.yaml` new job could be added
+by the following command:
+```bash
+cd examples
+kubectl apply -f basic-session-job-only.yaml
+```
+
+#### Creating Deployment and Jobs together
+
+Alternatively the Flink Deployment and the Flink Session Job configurations can be submitted together.
+
+To try out this run the following command:
+```bash
+cd examples
+kubectl apply -f basic-session-deployment-and-job.yaml
+```
+
+### SQL runner
+
+For running Flink SQL scripts check this [example](https://github.com/apache/flink-kubernetes-operator/tree/main/examples/flink-sql-runner-example).","[{'comment': 'Please use a relative link, so that the user does not accidentally change branches:\r\nhttps://github.blog/2013-01-31-relative-links-in-markup-files/', 'commenter': 'mbalassi'}, {'comment': 'Done, and fixed when used absolute link to the `kustomize` example as well', 'commenter': 'pvary'}]"
317,examples/README.md,"@@ -0,0 +1,167 @@
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+  http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+
+# Flink Kubernetes Operator Examples
+
+## Overview
+
+This directory contains few examples for the Flink Kubernetes Operator.
+These examples should only serve as starting points when familiarizing yourself with the
+Flink Kubernetes Operator and users are expected to extend these based on their production needs.
+
+## Usage
+
+Please refer to the [Quick Start](https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/try-flink-kubernetes-operator/quick-start)
+for setting up your environment to run the examples.
+
+## Examples
+
+### Basic Application Deployment example
+
+This is a simple deployment defined by a minimal deployment file.
+The configuration contains the following:
+- Defines the job to run
+- Assigns the resources available for the job
+- Defines the parallelism used
+
+To run the job submit the yaml file using kubectl:
+```bash
+kubectl apply -f basic.yaml
+```
+
+### Basic Session Deployment example
+
+This example shows how to create a basic Session Cluster and then how to submit specific jobs to this cluster if needed.
+
+#### Without jobs 
+
+The Flink Deployment could be created without any jobs.
+In this case the Flink jobs could be created later by submitting the jobs
+separately.
+
+The Flink Deployment configuration contains the following:
+- The name of the Flink Deployment
+- The resources available for the Flink Deployment
+
+The Flink Deployment configuration does __NOT__ contain the following:
+- The job to run
+- Any job specific configurations
+
+To create a Flink Deployment with the specific resources without any jobs run the following command:
+```bash
+kubectl apply -f basic-session-deployment-only.yaml
+```
+
+##### Adding jobs
+
+For an existing Flink Deployment another configuration could be used to create new jobs.
+This configuration should contain the following:
+- The Flink Deployment to use
+- The job to run
+- Any job specific configurations
+
+If the Flink Deployment is created by `basic-session-deployment-only.yaml` new job could be added
+by the following command:
+```bash
+kubectl apply -f basic-session-job-only.yaml
+```
+
+#### Creating Deployment and Jobs together
+
+Alternatively the Flink Deployment and the Flink Session Job configurations can be submitted together.
+
+To try out this run the following command:
+```bash
+kubectl apply -f basic-session-deployment-and-job.yaml
+```
+
+### SQL runner
+
+For running Flink SQL scripts check this [example](flink-sql-runner-example/README.md).
+","[{'comment': 'Hi @pvary, the python example has been merged as well and it would be great if we can add it in this doc like SQL runner.', 'commenter': 'bgeng777'}, {'comment': 'Thanks for the heads up @bgeng777!\r\nAdded the link to the python example', 'commenter': 'pvary'}]"
317,examples/README.md,"@@ -0,0 +1,171 @@
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+  http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+
+# Flink Kubernetes Operator Examples
+
+## Overview
+
+This directory contains few examples for the Flink Kubernetes Operator.
+These examples should only serve as starting points when familiarizing yourself with the
+Flink Kubernetes Operator and users are expected to extend these based on their production needs.
+
+## Usage
+
+Please refer to the [Quick Start](https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/try-flink-kubernetes-operator/quick-start)
+for setting up your environment to run the examples.
+
+## Examples
+
+### Basic Application Deployment example
+
+This is a simple deployment defined by a minimal deployment file.
+The configuration contains the following:
+- Defines the job to run
+- Assigns the resources available for the job
+- Defines the parallelism used
+
+To run the job submit the yaml file using kubectl:
+```bash
+kubectl apply -f basic.yaml
+```
+
+### Basic Session Deployment example
+
+This example shows how to create a basic Session Cluster and then how to submit specific jobs to this cluster if needed.
+
+#### Without jobs 
+
+The Flink Deployment could be created without any jobs.
+In this case the Flink jobs could be created later by submitting the jobs
+separately.
+
+The Flink Deployment configuration contains the following:
+- The name of the Flink Deployment
+- The resources available for the Flink Deployment
+
+The Flink Deployment configuration does __NOT__ contain the following:
+- The job to run
+- Any job specific configurations
+
+To create a Flink Deployment with the specific resources without any jobs run the following command:
+```bash
+kubectl apply -f basic-session-deployment-only.yaml
+```
+
+##### Adding jobs
+
+For an existing Flink Deployment another configuration could be used to create new jobs.
+This configuration should contain the following:
+- The Flink Deployment to use
+- The job to run
+- Any job specific configurations
+
+If the Flink Deployment is created by `basic-session-deployment-only.yaml` new job could be added
+by the following command:
+```bash
+kubectl apply -f basic-session-job-only.yaml
+```
+
+#### Creating Deployment and Jobs together
+
+Alternatively the Flink Deployment and the Flink Session Job configurations can be submitted together.
+
+To try out this run the following command:
+```bash
+kubectl apply -f basic-session-deployment-and-job.yaml
+```
+
+### SQL runner
+
+For running Flink SQL scripts check this [example](flink-sql-runner-example/README.md).
+
+### Python example
+
+For running Flink Python jobs check this [example](flink-python-example/README.md).
+
+### Advanced examples
+
+There are typical requirements for production systems and the examples below contain configuration files
+showing how to archive some of these.
+
+#### Custom logging
+
+This example adds specific logging configuration for the Flink Deployment using the
+`logConfiguration` property. To try out this run the following command:
+```bash
+kubectl apply -f custom-logging.yaml
+```
+
+#### Pod templates
+
+Flink Kubernetes Operator provides the possibility to simplify the deployment descriptors by using
+[Pod Templates](https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/custom-resource/pod-template/).
+
+This example shows how these templates are created and used. To try out this run the following command:
+```bash
+kubectl apply -f pod-template.yaml
+```
+
+#### Ingress
+
+Flink's Web UI access can be configured by the
+[Ingress](https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/operations/ingress/)
+entries.
+
+##### Basic Ingress example
+
+Simple domain based ingress configuration could be tried out by running this command:
+```bash
+kubectl apply -f basic-ingress.yaml
+```
+
+##### Advanced Ingress example
+
+It is possible to generate path based routing. To try out this run the following command:
+```bash
+kubectl apply -f advanced-ingress.yaml
+```
+
+#### High availability
+
+Basic example to configure Flink Deployments in
+[HA mode](https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/deployment/ha/overview/).
+The example shows how to set savepoint directory, checkpoint directory and HA. To try out this run the following command:
+```bash
+kubectl apply -f basic-checkpoint-ha.yaml
+```","[{'comment': 'I would not call this ""advanced"" as this is a very general requirement. We should have this near the top and also I would prefer to call it `Checkpointing & High Availability`', 'commenter': 'gyfora'}, {'comment': 'We could put it even directly under `### Basic Application Deployment example`', 'commenter': 'gyfora'}, {'comment': 'Thanks for the insight!\r\nDone', 'commenter': 'pvary'}]"
337,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/FlinkOperatorConfiguration.java,"@@ -129,6 +131,14 @@ public static FlinkOperatorConfiguration fromConfiguration(Configuration operato
         String labelSelector =
                 operatorConfig.getString(KubernetesOperatorConfigOptions.OPERATOR_LABEL_SELECTOR);
 
+        SavepointFormatType savepointFormatType =
+                SavepointFormatType.valueOf(
+                        operatorConfig
+                                .getString(
+                                        KubernetesOperatorConfigOptions
+                                                .OPERATOR_SAVEPOINT_FORMAT_TYPE)
+                                .toUpperCase());
+","[{'comment': 'This setting should not be part of the `FlinkOperatorConfiguration` is it is intended to be set on a per resource basis', 'commenter': 'gyfora'}]"
337,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/KubernetesOperatorConfigOptions.java,"@@ -280,4 +281,12 @@ public class KubernetesOperatorConfigOptions {
                     .defaultValue(true)
                     .withDescription(
                             ""Enables last-state fallback for savepoint upgrade mode. When the job is not running thus savepoint cannot be triggered but HA metadata is available for last state restore the operator can initiate the upgrade process when the flag is enabled."");
+
+    @Documentation.Section(SECTION_DYNAMIC)
+    public static final ConfigOption<String> OPERATOR_SAVEPOINT_FORMAT_TYPE =","[{'comment': 'Should this be enum type instead?', 'commenter': 'gyfora'}]"
337,flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/service/NativeFlinkServiceTest.java,"@@ -180,8 +186,10 @@ public void testCancelJobWithLastStateUpgradeMode() throws Exception {
                         .get());
     }
 
-    @Test
-    public void testTriggerSavepoint() throws Exception {
+    @ParameterizedTest
+    @MethodSource(""provideSavepointFormatType"")
+    public void testTriggerSavepoint(Configuration configuration, FlinkConfigManager configManager)
+            throws Exception {","[{'comment': 'These tests do not actually test that the correct savepoint format type is used during savepointing.\r\n\r\nWe should remove these parameterized tests and simply validate that if the user configures the savepoint type the rest api is called with that type in the FlinkService', 'commenter': 'gyfora'}]"
337,flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/service/NativeFlinkServiceTest.java,"@@ -337,6 +339,77 @@ public void testEffectiveStatus() {
                 AbstractFlinkService.getEffectiveStatus(allFinished));
     }
 
+    @Test
+    public void testNativeSavepointFormat() throws Exception {
+        final TestingClusterClient<String> testingClusterClient =
+                new TestingClusterClient<>(configuration, TestUtils.TEST_DEPLOYMENT_NAME);
+        final String savepointPath = ""file:///path/of/svp"";
+        final CompletableFuture<Tuple3<JobID, String, Boolean>> savepointFeature =
+                new CompletableFuture<>();
+        configuration.set(CheckpointingOptions.SAVEPOINT_DIRECTORY, savepointPath);
+        testingClusterClient.setRequestProcessor(
+                (headers, parameters, requestBody) -> {
+                    savepointFeature.complete(
+                            new Tuple3<>(
+                                    ((SavepointTriggerMessageParameters) parameters)
+                                            .jobID.getValue(),
+                                    ((SavepointTriggerRequestBody) requestBody)
+                                            .getTargetDirectory()
+                                            .get(),
+                                    ((SavepointTriggerRequestBody) requestBody).isCancelJob()));","[{'comment': 'should we also capture the savepoint format? I think thats what we want to test here', 'commenter': 'gyfora'}]"
337,flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/service/NativeFlinkServiceTest.java,"@@ -337,6 +339,77 @@ public void testEffectiveStatus() {
                 AbstractFlinkService.getEffectiveStatus(allFinished));
     }
 
+    @Test
+    public void testNativeSavepointFormat() throws Exception {
+        final TestingClusterClient<String> testingClusterClient =
+                new TestingClusterClient<>(configuration, TestUtils.TEST_DEPLOYMENT_NAME);
+        final String savepointPath = ""file:///path/of/svp"";
+        final CompletableFuture<Tuple3<JobID, String, Boolean>> savepointFeature =
+                new CompletableFuture<>();
+        configuration.set(CheckpointingOptions.SAVEPOINT_DIRECTORY, savepointPath);
+        testingClusterClient.setRequestProcessor(
+                (headers, parameters, requestBody) -> {
+                    savepointFeature.complete(
+                            new Tuple3<>(
+                                    ((SavepointTriggerMessageParameters) parameters)
+                                            .jobID.getValue(),
+                                    ((SavepointTriggerRequestBody) requestBody)
+                                            .getTargetDirectory()
+                                            .get(),
+                                    ((SavepointTriggerRequestBody) requestBody).isCancelJob()));
+                    return CompletableFuture.completedFuture(new TriggerResponse(new TriggerId()));
+                });
+        final CompletableFuture<Tuple3<JobID, Boolean, String>> stopWithSavepointFuture =
+                new CompletableFuture<>();
+        testingClusterClient.setStopWithSavepointFunction(","[{'comment': 'We should improve this to also capture the format type so we can test it.', 'commenter': 'gyfora'}]"
337,flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/service/NativeFlinkServiceTest.java,"@@ -337,6 +339,77 @@ public void testEffectiveStatus() {
                 AbstractFlinkService.getEffectiveStatus(allFinished));
     }
 
+    @Test
+    public void testNativeSavepointFormat() throws Exception {
+        final TestingClusterClient<String> testingClusterClient =
+                new TestingClusterClient<>(configuration, TestUtils.TEST_DEPLOYMENT_NAME);
+        final String savepointPath = ""file:///path/of/svp"";
+        final CompletableFuture<Tuple3<JobID, String, Boolean>> savepointFeature =
+                new CompletableFuture<>();
+        configuration.set(CheckpointingOptions.SAVEPOINT_DIRECTORY, savepointPath);
+        testingClusterClient.setRequestProcessor(
+                (headers, parameters, requestBody) -> {
+                    savepointFeature.complete(
+                            new Tuple3<>(
+                                    ((SavepointTriggerMessageParameters) parameters)
+                                            .jobID.getValue(),
+                                    ((SavepointTriggerRequestBody) requestBody)
+                                            .getTargetDirectory()
+                                            .get(),
+                                    ((SavepointTriggerRequestBody) requestBody).isCancelJob()));
+                    return CompletableFuture.completedFuture(new TriggerResponse(new TriggerId()));
+                });
+        final CompletableFuture<Tuple3<JobID, Boolean, String>> stopWithSavepointFuture =
+                new CompletableFuture<>();
+        testingClusterClient.setStopWithSavepointFunction(
+                (id, advanceToEndOfEventTime, savepointDir) -> {
+                    stopWithSavepointFuture.complete(
+                            new Tuple3<>(id, advanceToEndOfEventTime, savepointDir));
+                    return CompletableFuture.completedFuture(savepointPath);
+                });
+
+        final FlinkService flinkService = createFlinkService(testingClusterClient);
+
+        final JobID jobID = JobID.generate();
+        final FlinkDeployment deployment = TestUtils.buildApplicationCluster();
+        deployment
+                .getSpec()
+                .getFlinkConfiguration()
+                .put(CheckpointingOptions.SAVEPOINT_DIRECTORY.key(), savepointPath);
+        deployment.getStatus().setJobManagerDeploymentStatus(JobManagerDeploymentStatus.READY);
+        JobStatus jobStatus = deployment.getStatus().getJobStatus();
+        jobStatus.setJobId(jobID.toHexString());
+        jobStatus.setState(org.apache.flink.api.common.JobStatus.RUNNING.name());
+        ReconciliationUtils.updateStatusForDeployedSpec(deployment, new Configuration());
+
+        jobStatus.setJobId(jobID.toString());
+        deployment.getStatus().setJobStatus(jobStatus);
+        flinkService.triggerSavepoint(
+                deployment.getStatus().getJobStatus().getJobId(),
+                SavepointTriggerType.MANUAL,
+                deployment.getStatus().getJobStatus().getSavepointInfo(),
+                new Configuration(configuration)
+                        .set(
+                                KubernetesOperatorConfigOptions.OPERATOR_SAVEPOINT_FORMAT_TYPE,
+                                SavepointFormatType.NATIVE));
+        assertTrue(savepointFeature.isDone());
+        assertEquals(jobID, savepointFeature.get().f0);
+        assertEquals(savepointPath, savepointFeature.get().f1);
+        assertFalse(savepointFeature.get().f2);
+
+        flinkService.cancelJob(
+                deployment,
+                UpgradeMode.SAVEPOINT,
+                new Configuration(configManager.getObserveConfig(deployment))
+                        .set(
+                                KubernetesOperatorConfigOptions.OPERATOR_SAVEPOINT_FORMAT_TYPE,
+                                SavepointFormatType.NATIVE));
+        assertTrue(stopWithSavepointFuture.isDone());
+        assertEquals(jobID, stopWithSavepointFuture.get().f0);
+        assertFalse(stopWithSavepointFuture.get().f1);
+        assertEquals(savepointPath, stopWithSavepointFuture.get().f2);","[{'comment': 'these are nice but would be better to test the format also', 'commenter': 'gyfora'}]"
337,flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/service/NativeFlinkServiceTest.java,"@@ -337,6 +339,77 @@ public void testEffectiveStatus() {
                 AbstractFlinkService.getEffectiveStatus(allFinished));
     }
 
+    @Test
+    public void testNativeSavepointFormat() throws Exception {
+        final TestingClusterClient<String> testingClusterClient =
+                new TestingClusterClient<>(configuration, TestUtils.TEST_DEPLOYMENT_NAME);
+        final String savepointPath = ""file:///path/of/svp"";
+        final CompletableFuture<Tuple3<JobID, String, Boolean>> savepointFeature =
+                new CompletableFuture<>();
+        configuration.set(CheckpointingOptions.SAVEPOINT_DIRECTORY, savepointPath);
+        testingClusterClient.setRequestProcessor(
+                (headers, parameters, requestBody) -> {
+                    savepointFeature.complete(
+                            new Tuple3<>(
+                                    ((SavepointTriggerMessageParameters) parameters)
+                                            .jobID.getValue(),
+                                    ((SavepointTriggerRequestBody) requestBody)
+                                            .getTargetDirectory()
+                                            .get(),
+                                    ((SavepointTriggerRequestBody) requestBody).isCancelJob()));
+                    return CompletableFuture.completedFuture(new TriggerResponse(new TriggerId()));
+                });
+        final CompletableFuture<Tuple3<JobID, Boolean, String>> stopWithSavepointFuture =
+                new CompletableFuture<>();
+        testingClusterClient.setStopWithSavepointFunction(
+                (id, advanceToEndOfEventTime, savepointDir) -> {
+                    stopWithSavepointFuture.complete(
+                            new Tuple3<>(id, advanceToEndOfEventTime, savepointDir));
+                    return CompletableFuture.completedFuture(savepointPath);
+                });
+
+        final FlinkService flinkService = createFlinkService(testingClusterClient);
+
+        final JobID jobID = JobID.generate();
+        final FlinkDeployment deployment = TestUtils.buildApplicationCluster();
+        deployment
+                .getSpec()
+                .getFlinkConfiguration()
+                .put(CheckpointingOptions.SAVEPOINT_DIRECTORY.key(), savepointPath);
+        deployment.getStatus().setJobManagerDeploymentStatus(JobManagerDeploymentStatus.READY);
+        JobStatus jobStatus = deployment.getStatus().getJobStatus();
+        jobStatus.setJobId(jobID.toHexString());
+        jobStatus.setState(org.apache.flink.api.common.JobStatus.RUNNING.name());
+        ReconciliationUtils.updateStatusForDeployedSpec(deployment, new Configuration());
+
+        jobStatus.setJobId(jobID.toString());
+        deployment.getStatus().setJobStatus(jobStatus);
+        flinkService.triggerSavepoint(
+                deployment.getStatus().getJobStatus().getJobId(),
+                SavepointTriggerType.MANUAL,
+                deployment.getStatus().getJobStatus().getSavepointInfo(),
+                new Configuration(configuration)
+                        .set(
+                                KubernetesOperatorConfigOptions.OPERATOR_SAVEPOINT_FORMAT_TYPE,
+                                SavepointFormatType.NATIVE));
+        assertTrue(savepointFeature.isDone());
+        assertEquals(jobID, savepointFeature.get().f0);
+        assertEquals(savepointPath, savepointFeature.get().f1);
+        assertFalse(savepointFeature.get().f2);","[{'comment': 'these are nice but would be better to test the format also', 'commenter': 'gyfora'}]"
337,flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/service/NativeFlinkServiceTest.java,"@@ -337,6 +339,77 @@ public void testEffectiveStatus() {
                 AbstractFlinkService.getEffectiveStatus(allFinished));
     }
 
+    @Test
+    public void testNativeSavepointFormat() throws Exception {
+        final TestingClusterClient<String> testingClusterClient =
+                new TestingClusterClient<>(configuration, TestUtils.TEST_DEPLOYMENT_NAME);
+        final String savepointPath = ""file:///path/of/svp"";
+        final CompletableFuture<Tuple3<JobID, String, Boolean>> savepointFeature =","[{'comment': 'typo: `savepointFeature` -> `savepointFuture`', 'commenter': 'gyfora'}]"
351,docs/content/docs/custom-resource/reference.md,"@@ -174,6 +174,79 @@ This page serves as a full reference for FlinkDeployment custom resource definit
 | LAST_STATE | Job is upgraded using any latest checkpoint or savepoint available. |
 | STATELESS | Job is upgraded with empty state. |
 
+### Diff
+**Class**: org.apache.flink.kubernetes.operator.crd.spec.diff.Diff
+
+**Description**: Contains the differences between two {@link Diffable} class fields.
+
+ <p>Inspired by:
+ https://github.com/apache/commons-lang/blob/master/src/main/java/org/apache/commons/lang3/builder/Diff.java
+
+| Parameter | Type | Docs |
+| ----------| ---- | ---- |
+| serialVersionUID | long |  |
+| fieldName | @lombok.NonNull java.lang.String |  |
+| left | T |  |
+| right | T |  |
+| type | @lombok.NonNull org.apache.flink.kubernetes.operator.crd.spec.diff.DiffType |  |
+","[{'comment': 'These Diff related classes should not be part of the CRD reference. We should either move them to another package or change the generator logic.', 'commenter': 'gyfora'}]"
351,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/crd/spec/diff/Diff.java,"@@ -0,0 +1,40 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.crd.spec.diff;
+
+import org.apache.flink.annotation.Experimental;
+
+import lombok.NonNull;
+import lombok.Value;
+
+/**
+ * Contains the differences between two {@link Diffable} class fields.
+ *
+ * <p>Inspired by:
+ * https://github.com/apache/commons-lang/blob/master/src/main/java/org/apache/commons/lang3/builder/Diff.java
+ */
+@Experimental
+@Value
+public class Diff<T> {
+    private static final long serialVersionUID = 1L;
+
+    @NonNull private final String fieldName;
+    private final T left;
+    private final T right;
+    @NonNull private final DiffType type;","[{'comment': '`private final` can be removed if `@Value` is used', 'commenter': 'gyfora'}]"
351,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/crd/spec/diff/DiffResult.java,"@@ -0,0 +1,102 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.crd.spec.diff;
+
+import org.apache.flink.annotation.Experimental;
+
+import lombok.NonNull;
+import org.apache.commons.lang3.builder.ToStringBuilder;
+import org.apache.commons.lang3.builder.ToStringStyle;
+
+import java.util.Collections;
+import java.util.List;
+
+/**
+ * Contains a collection of the differences between two {@link Diffable} objects.
+ *
+ * <p>Inspired by:
+ * https://github.com/apache/commons-lang/blob/master/src/main/java/org/apache/commons/lang3/builder/DiffResult.java
+ */
+@Experimental
+public class DiffResult<T> {
+
+    public static final String OBJECTS_SAME_STRING = """";
+
+    private final List<Diff<?>> diffList;
+    private final T left;
+    private final T right;
+    private final DiffType type;
+
+    DiffResult(@NonNull T left, @NonNull T right, @NonNull List<Diff<?>> diffList) {
+        this.left = left;
+        this.right = right;
+        this.diffList = diffList;
+        this.type = getSpechChangeType(diffList);
+    }
+
+    public T getLeft() {
+        return this.left;
+    }
+
+    public T getRight() {
+        return this.right;
+    }
+
+    public List<Diff<?>> getDiffs() {
+        return Collections.unmodifiableList(diffList);
+    }
+
+    public int getNumberOfDiffs() {
+        return diffList.size();
+    }
+
+    public DiffType getType() {
+        return type;
+    }","[{'comment': 'We could use the `@Getter` for this', 'commenter': 'gyfora'}]"
351,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/crd/spec/diff/DiffResult.java,"@@ -0,0 +1,102 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.crd.spec.diff;
+
+import org.apache.flink.annotation.Experimental;
+
+import lombok.NonNull;
+import org.apache.commons.lang3.builder.ToStringBuilder;
+import org.apache.commons.lang3.builder.ToStringStyle;
+
+import java.util.Collections;
+import java.util.List;
+
+/**
+ * Contains a collection of the differences between two {@link Diffable} objects.
+ *
+ * <p>Inspired by:
+ * https://github.com/apache/commons-lang/blob/master/src/main/java/org/apache/commons/lang3/builder/DiffResult.java
+ */
+@Experimental
+public class DiffResult<T> {
+
+    public static final String OBJECTS_SAME_STRING = """";
+
+    private final List<Diff<?>> diffList;
+    private final T left;
+    private final T right;
+    private final DiffType type;
+
+    DiffResult(@NonNull T left, @NonNull T right, @NonNull List<Diff<?>> diffList) {
+        this.left = left;
+        this.right = right;
+        this.diffList = diffList;
+        this.type = getSpechChangeType(diffList);
+    }
+
+    public T getLeft() {
+        return this.left;
+    }
+
+    public T getRight() {
+        return this.right;
+    }
+
+    public List<Diff<?>> getDiffs() {
+        return Collections.unmodifiableList(diffList);
+    }
+
+    public int getNumberOfDiffs() {
+        return diffList.size();
+    }
+
+    public DiffType getType() {
+        return type;
+    }
+
+    @Override
+    public String toString() {
+        if (diffList.isEmpty()) {
+            return OBJECTS_SAME_STRING;
+        }
+
+        final ToStringBuilder lhsBuilder =
+                new ToStringBuilder(left, ToStringStyle.SHORT_PREFIX_STYLE);
+        final ToStringBuilder rhsBuilder =
+                new ToStringBuilder(right, ToStringStyle.SHORT_PREFIX_STYLE);
+
+        diffList.forEach(
+                diff -> {
+                    lhsBuilder.append(diff.getFieldName(), diff.getLeft());
+                    rhsBuilder.append(diff.getFieldName(), diff.getRight());
+                });
+
+        return String.format(""%s differs from %s"", lhsBuilder.build(), rhsBuilder.build());
+    }
+
+    private DiffType getSpechChangeType(List<Diff<?>> diffs) {","[{'comment': 'should be static', 'commenter': 'gyfora'}]"
351,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/FlinkService.java,"@@ -93,4 +93,6 @@ void triggerSavepoint(
     PodList getJmPodList(FlinkDeployment deployment, Configuration conf);
 
     void waitForClusterShutdown(Configuration conf);
+
+    boolean reactiveScale(ObjectMeta meta, JobSpec jobSpec, Configuration conf);","[{'comment': ""I think we should simply call this `scale` . This operation does not to be reactive, that's basically just one way to implement this for standalone."", 'commenter': 'gyfora'}, {'comment': 'It could also return false by default to avoid having to implement it for non standalone clusters.', 'commenter': 'gyfora'}]"
351,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/AbstractFlinkResourceReconciler.java,"@@ -122,16 +123,23 @@ public final void reconcile(CR cr, Context<?> ctx) throws Exception {
                 cr.getStatus().getReconciliationStatus().deserializeLastReconciledSpec();
         SPEC currentDeploySpec = cr.getSpec();
 
+        var specDiff = currentDeploySpec.diff(lastReconciledSpec);
+
+        var flinkService = getFlinkService(cr, ctx);
+
         boolean specChanged =
-                reconciliationStatus.getState() == ReconciliationState.UPGRADING
-                        || !currentDeploySpec.equals(lastReconciledSpec);
+                DiffType.IGNORE != specDiff.getType()
+                        || reconciliationStatus.getState() == ReconciliationState.UPGRADING;
+
         var observeConfig = getObserveConfig(cr, ctx);
-        var flinkService = getFlinkService(cr, ctx);
+
         if (specChanged) {
+
             if (checkNewSpecAlreadyDeployed(cr, deployConfig)) {
                 return;
             }
-            LOG.info(MSG_SPEC_CHANGED);
+            LOG.info(String.format(MSG_SPEC_CHANGED, specDiff.getType()));
+            LOG.info(specDiff.toString());","[{'comment': 'We should log this in a single line', 'commenter': 'gyfora'}, {'comment': 'fixed', 'commenter': 'morhidi'}]"
351,flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/reconciler/diff/SpecDiffTest.java,"@@ -0,0 +1,171 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.reconciler.diff;
+
+import org.apache.flink.configuration.CoreOptions;
+import org.apache.flink.kubernetes.operator.TestUtils;
+import org.apache.flink.kubernetes.operator.crd.spec.AbstractFlinkSpec;
+import org.apache.flink.kubernetes.operator.crd.spec.FlinkDeploymentSpec;
+import org.apache.flink.kubernetes.operator.crd.spec.FlinkSessionJobSpec;
+import org.apache.flink.kubernetes.operator.crd.spec.UpgradeMode;
+import org.apache.flink.kubernetes.operator.reconciler.ReconciliationUtils;
+
+import org.junit.jupiter.params.ParameterizedTest;
+import org.junit.jupiter.params.provider.Arguments;
+import org.junit.jupiter.params.provider.MethodSource;
+
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.stream.Stream;
+
+import static org.apache.flink.kubernetes.operator.config.KubernetesOperatorConfigOptions.OPERATOR_RECONCILE_INTERVAL;
+import static org.apache.flink.kubernetes.operator.metrics.KubernetesOperatorMetricOptions.SCOPE_NAMING_KUBERNETES_OPERATOR;
+import static org.junit.jupiter.api.Assertions.assertEquals;
+import static org.junit.jupiter.params.provider.Arguments.arguments;
+
+/** Spec diff test. */
+public class SpecDiffTest {
+
+    @ParameterizedTest
+    @MethodSource(""applicationTestParams"")
+    public void testIgnore(AbstractFlinkSpec left, AbstractFlinkSpec right) {
+        var diff = left.diff(right);
+        assertEquals(DiffType.IGNORE, diff.getType());
+        assertEquals(0, diff.getNumDiffs());
+        if (right.getJob() != null) {
+            right.getJob().setUpgradeMode(UpgradeMode.LAST_STATE);
+            right.getJob().setAllowNonRestoredState(true);
+            right.getJob().setInitialSavepointPath(""local:///tmp"");
+            right.getJob().setSavepointTriggerNonce(123L);
+            diff = left.diff(right);
+            assertEquals(DiffType.IGNORE, diff.getType());
+            assertEquals(4, diff.getNumDiffs());
+        }
+    }
+
+    @ParameterizedTest
+    @MethodSource(""applicationTestParams"")
+    public void testScale(AbstractFlinkSpec left, AbstractFlinkSpec right) {
+        var diff = left.diff(right);
+        assertEquals(DiffType.IGNORE, diff.getType());
+        assertEquals(0, diff.getNumDiffs());
+
+        if (right.getJob() != null) {
+            right.getJob().setParallelism(100);
+            diff = left.diff(right);
+            assertEquals(DiffType.SCALE, diff.getType());
+            assertEquals(1, diff.getNumDiffs());
+
+            right.getJob().setUpgradeMode(UpgradeMode.LAST_STATE);
+            right.getJob().setAllowNonRestoredState(true);
+            right.getJob().setInitialSavepointPath(""local:///tmp"");
+            right.getJob().setSavepointTriggerNonce(123L);
+            diff = left.diff(right);
+            assertEquals(DiffType.SCALE, diff.getType());
+            assertEquals(5, diff.getNumDiffs());
+        }
+    }
+
+    @ParameterizedTest
+    @MethodSource(""applicationTestParams"")
+    public void testReconcile(AbstractFlinkSpec left, AbstractFlinkSpec right) {
+        var diff = left.diff(right);
+        assertEquals(DiffType.IGNORE, diff.getType());
+        assertEquals(0, diff.getNumDiffs());
+
+        if (left instanceof FlinkDeploymentSpec) {
+            ((FlinkDeploymentSpec) right).setImage(""flink:greatest"");
+            assertEquals(DiffType.UPGRADE, left.diff(right).getType());
+            diff = left.diff(right);
+            assertEquals(DiffType.UPGRADE, diff.getType());
+            assertEquals(1, diff.getNumDiffs());
+        }
+","[{'comment': ""It's a bit strange to use `ParameterizedTest` here because you actually have 3 different inner ifs for the 3 different params. I would personally prefer to have single test cases because the parameterization just breaks up the logic  (hides what is tested) and its harder to understand."", 'commenter': 'gyfora'}, {'comment': 'Dropped the `ParameterizedTest` approach', 'commenter': 'morhidi'}]"
351,flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/reconciler/diff/SpecDiffTest.java,"@@ -0,0 +1,171 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.reconciler.diff;
+
+import org.apache.flink.configuration.CoreOptions;
+import org.apache.flink.kubernetes.operator.TestUtils;
+import org.apache.flink.kubernetes.operator.crd.spec.AbstractFlinkSpec;
+import org.apache.flink.kubernetes.operator.crd.spec.FlinkDeploymentSpec;
+import org.apache.flink.kubernetes.operator.crd.spec.FlinkSessionJobSpec;
+import org.apache.flink.kubernetes.operator.crd.spec.UpgradeMode;
+import org.apache.flink.kubernetes.operator.reconciler.ReconciliationUtils;
+
+import org.junit.jupiter.params.ParameterizedTest;
+import org.junit.jupiter.params.provider.Arguments;
+import org.junit.jupiter.params.provider.MethodSource;
+
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.stream.Stream;
+
+import static org.apache.flink.kubernetes.operator.config.KubernetesOperatorConfigOptions.OPERATOR_RECONCILE_INTERVAL;
+import static org.apache.flink.kubernetes.operator.metrics.KubernetesOperatorMetricOptions.SCOPE_NAMING_KUBERNETES_OPERATOR;
+import static org.junit.jupiter.api.Assertions.assertEquals;
+import static org.junit.jupiter.params.provider.Arguments.arguments;
+
+/** Spec diff test. */
+public class SpecDiffTest {
+
+    @ParameterizedTest
+    @MethodSource(""applicationTestParams"")
+    public void testIgnore(AbstractFlinkSpec left, AbstractFlinkSpec right) {
+        var diff = left.diff(right);
+        assertEquals(DiffType.IGNORE, diff.getType());
+        assertEquals(0, diff.getNumDiffs());
+        if (right.getJob() != null) {
+            right.getJob().setUpgradeMode(UpgradeMode.LAST_STATE);
+            right.getJob().setAllowNonRestoredState(true);
+            right.getJob().setInitialSavepointPath(""local:///tmp"");
+            right.getJob().setSavepointTriggerNonce(123L);
+            diff = left.diff(right);
+            assertEquals(DiffType.IGNORE, diff.getType());
+            assertEquals(4, diff.getNumDiffs());
+        }
+    }
+
+    @ParameterizedTest
+    @MethodSource(""applicationTestParams"")
+    public void testScale(AbstractFlinkSpec left, AbstractFlinkSpec right) {
+        var diff = left.diff(right);
+        assertEquals(DiffType.IGNORE, diff.getType());
+        assertEquals(0, diff.getNumDiffs());
+
+        if (right.getJob() != null) {
+            right.getJob().setParallelism(100);
+            diff = left.diff(right);
+            assertEquals(DiffType.SCALE, diff.getType());
+            assertEquals(1, diff.getNumDiffs());
+
+            right.getJob().setUpgradeMode(UpgradeMode.LAST_STATE);
+            right.getJob().setAllowNonRestoredState(true);
+            right.getJob().setInitialSavepointPath(""local:///tmp"");
+            right.getJob().setSavepointTriggerNonce(123L);
+            diff = left.diff(right);
+            assertEquals(DiffType.SCALE, diff.getType());
+            assertEquals(5, diff.getNumDiffs());
+        }
+    }
+
+    @ParameterizedTest
+    @MethodSource(""applicationTestParams"")
+    public void testReconcile(AbstractFlinkSpec left, AbstractFlinkSpec right) {
+        var diff = left.diff(right);
+        assertEquals(DiffType.IGNORE, diff.getType());
+        assertEquals(0, diff.getNumDiffs());
+
+        if (left instanceof FlinkDeploymentSpec) {
+            ((FlinkDeploymentSpec) right).setImage(""flink:greatest"");
+            assertEquals(DiffType.UPGRADE, left.diff(right).getType());
+            diff = left.diff(right);
+            assertEquals(DiffType.UPGRADE, diff.getType());
+            assertEquals(1, diff.getNumDiffs());
+        }
+
+        if (left instanceof FlinkSessionJobSpec) {
+            ((FlinkSessionJobSpec) right).setDeploymentName(""does-non-exist"");
+            assertEquals(DiffType.UPGRADE, left.diff(right).getType());
+            diff = left.diff(right);
+            assertEquals(DiffType.UPGRADE, diff.getType());
+            assertEquals(1, diff.getNumDiffs());
+        }
+
+        if (right.getJob() != null) {
+            right.getJob().setParallelism(100);
+            right.getJob().setUpgradeMode(UpgradeMode.LAST_STATE);
+            assertEquals(DiffType.UPGRADE, left.diff(right).getType());
+            diff = left.diff(right);
+            assertEquals(DiffType.UPGRADE, diff.getType());
+            assertEquals(3, diff.getNumDiffs());
+        }","[{'comment': 'It would also be nice to include 1-2 more complex diff scenarios such us some inner array of the podTemplate (like env variables) have changed so that we get full confidence in the logic we copied', 'commenter': 'gyfora'}, {'comment': 'Added some advanced diffs. Please note that we do a deep diff on Diffable objects only, the rest is just plain equals.', 'commenter': 'morhidi'}]"
356,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/AbstractJobReconciler.java,"@@ -107,12 +107,18 @@ protected void reconcileSpecChange(
 
         JobState currentJobState = lastReconciledSpec.getJob().getState();
         JobState desiredJobState = currentDeploySpec.getJob().getState();
+        // check upgrade mode compatibility if deployment was ever stable
+        // an initially unsuccessful deployment can be reverted
+        var hasStableSpec =
+                resource.getStatus().getReconciliationStatus().getLastStableSpec() != null;","[{'comment': 'I think this check is too weak. You might have completed checkpoints already at this point (if the job went into a restart loop after some checkpoints but before the operator considered it to be stable).\r\n\r\nAlso as we improve stability conditions it might become even weaker in the future', 'commenter': 'gyfora'}]"
356,flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/reconciler/deployment/ApplicationReconcilerTest.java,"@@ -149,6 +149,9 @@ public void testUpgrade(FlinkVersion flinkVersion) throws Exception {
         deployment.getSpec().setRestartNonce(100L);
         flinkService.setHaDataAvailable(false);
         deployment.getStatus().getJobStatus().setState(""RECONCILING"");
+        // enforcement of savepoint upgrade from stable spec
+        var reconStatus = deployment.getStatus().getReconciliationStatus();
+        reconStatus.setLastStableSpec(reconStatus.getLastReconciledSpec());","[{'comment': 'I think with the right logic these tests changes might not be necessary anymore.', 'commenter': 'gyfora'}]"
356,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/ApplicationReconciler.java,"@@ -98,12 +99,39 @@ protected Optional<UpgradeMode> getAvailableUpgradeMode(
                                 .OPERATOR_JOB_UPGRADE_LAST_STATE_FALLBACK_ENABLED)
                 && FlinkUtils.isKubernetesHAActivated(deployConfig)
                 && FlinkUtils.isKubernetesHAActivated(observeConfig)
-                && flinkService.isHaMetadataAvailable(deployConfig)
                 && !flinkVersionChanged(
                         ReconciliationUtils.getDeployedSpec(deployment), deployment.getSpec())) {
-            LOG.info(
-                    ""Job is not running but HA metadata is available for last state restore, ready for upgrade"");
-            return Optional.of(UpgradeMode.LAST_STATE);
+
+            if (!flinkService.isHaMetadataAvailable(deployConfig)) {
+                if (deployment.getStatus().getReconciliationStatus().getLastStableSpec() == null) {","[{'comment': 'Would be nice to encapsulate the logic inside this branch into a method/utility, there are quite a few things happening in there.', 'commenter': 'gyfora'}, {'comment': 'done', 'commenter': 'tweise'}]"
356,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/ApplicationReconciler.java,"@@ -98,12 +99,39 @@ protected Optional<UpgradeMode> getAvailableUpgradeMode(
                                 .OPERATOR_JOB_UPGRADE_LAST_STATE_FALLBACK_ENABLED)
                 && FlinkUtils.isKubernetesHAActivated(deployConfig)
                 && FlinkUtils.isKubernetesHAActivated(observeConfig)
-                && flinkService.isHaMetadataAvailable(deployConfig)
                 && !flinkVersionChanged(
                         ReconciliationUtils.getDeployedSpec(deployment), deployment.getSpec())) {
-            LOG.info(
-                    ""Job is not running but HA metadata is available for last state restore, ready for upgrade"");
-            return Optional.of(UpgradeMode.LAST_STATE);
+
+            if (!flinkService.isHaMetadataAvailable(deployConfig)) {
+                if (deployment.getStatus().getReconciliationStatus().getLastStableSpec() == null) {
+                    // initial deployment failure, reset to allow for spec change to proceed
+                    flinkService.deleteClusterDeployment(
+                            deployment.getMetadata(), deployment.getStatus(), false);
+                    flinkService.waitForClusterShutdown(deployConfig);
+                    // in case the deployment succeeded between check and delete, fall through to
+                    // the upgrade path
+                    if (!flinkService.isHaMetadataAvailable(deployConfig)) {","[{'comment': 'I think there should be an `else` branch which returns LAST_STATE (if metadata was actually present after shutdown)', 'commenter': 'gyfora'}, {'comment': 'Indeed, and that also made it easier to move the logic to a separate method.', 'commenter': 'tweise'}]"
356,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/ApplicationReconciler.java,"@@ -98,12 +99,39 @@ protected Optional<UpgradeMode> getAvailableUpgradeMode(
                                 .OPERATOR_JOB_UPGRADE_LAST_STATE_FALLBACK_ENABLED)
                 && FlinkUtils.isKubernetesHAActivated(deployConfig)
                 && FlinkUtils.isKubernetesHAActivated(observeConfig)
-                && flinkService.isHaMetadataAvailable(deployConfig)
                 && !flinkVersionChanged(
                         ReconciliationUtils.getDeployedSpec(deployment), deployment.getSpec())) {
-            LOG.info(
-                    ""Job is not running but HA metadata is available for last state restore, ready for upgrade"");
-            return Optional.of(UpgradeMode.LAST_STATE);
+
+            if (!flinkService.isHaMetadataAvailable(deployConfig)) {
+                if (deployment.getStatus().getReconciliationStatus().getLastStableSpec() == null) {
+                    // initial deployment failure, reset to allow for spec change to proceed
+                    flinkService.deleteClusterDeployment(
+                            deployment.getMetadata(), deployment.getStatus(), false);
+                    flinkService.waitForClusterShutdown(deployConfig);
+                    // in case the deployment succeeded between check and delete, fall through to
+                    // the upgrade path
+                    if (!flinkService.isHaMetadataAvailable(deployConfig)) {
+                        LOG.info(
+                                ""Job never entered stable state. Clearing previous spec to reset for initial deploy"");
+                        // TODO: lastSpecWithMeta.f1.isFirstDeployment() is false","[{'comment': 'Would be nice to clear up this TODO ', 'commenter': 'gyfora'}, {'comment': ""I think there could be something wrong with the first deployment logic (or I don't understand it). But it would be better to deal with that outside of this PR."", 'commenter': 'tweise'}]"
357,examples/basic-checkpoint-ha.yaml,"@@ -58,3 +58,4 @@ spec:
     upgradeMode: savepoint
     state: running
     savepointTriggerNonce: 0
+  mode: native","[{'comment': 'Can you add the missing newlines for the modified examples please?', 'commenter': 'gyfora'}, {'comment': 'Added!', 'commenter': 'Grypse'}]"
357,examples/basic-ingress.yaml,"@@ -39,3 +39,4 @@ spec:
   job:
     jarURI: local:///opt/flink/examples/streaming/StateMachineExample.jar
     parallelism: 2
+  mode: native","[{'comment': 'missing newline at end', 'commenter': 'gyfora'}, {'comment': 'Added!', 'commenter': 'Grypse'}]"
357,examples/basic-session-deployment-only.yaml,"@@ -34,3 +34,4 @@ spec:
     resource:
       memory: ""2048m""
       cpu: 1
+  mode: native","[{'comment': 'missing newline at end', 'commenter': 'gyfora'}, {'comment': 'Added newline to the end.', 'commenter': 'Grypse'}]"
357,examples/custom-logging.yaml,"@@ -63,3 +63,4 @@ spec:
       logger.zookeeper.level = INFO
       logger.netty.name = org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline
       logger.netty.level = OFF
+  mode: native","[{'comment': 'missing newline at end', 'commenter': 'gyfora'}, {'comment': 'Added newline to the end.', 'commenter': 'Grypse'}]"
357,examples/pod-template.yaml,"@@ -82,3 +82,4 @@ spec:
     jarURI: local:///opt/flink/downloads/flink-examples-streaming.jar
     entryClass: org.apache.flink.streaming.examples.statemachine.StateMachineExample
     parallelism: 2
+  mode: native","[{'comment': 'missing newline at end', 'commenter': 'gyfora'}, {'comment': 'Added newline to the end.', 'commenter': 'Grypse'}]"
357,examples/standalone_pvc.yaml,"@@ -0,0 +1,81 @@
+################################################################################
+#  Licensed to the Apache Software Foundation (ASF) under one
+#  or more contributor license agreements.  See the NOTICE file
+#  distributed with this work for additional information
+#  regarding copyright ownership.  The ASF licenses this file
+#  to you under the Apache License, Version 2.0 (the
+#  ""License""); you may not use this file except in compliance
+#  with the License.  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an ""AS IS"" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+# limitations under the License.
+################################################################################
+
+kind: FlinkDeployment
+metadata:
+  namespace: default
+  name: basic-example
+spec:
+  image: flink:1.15.1
+  flinkVersion: v1_15
+  flinkConfiguration:
+    taskmanager.numberOfTaskSlots: ""2""
+  serviceAccount: flink
+  jobManager:
+    replicas: 1
+    resource:
+      memory: ""2048m""
+      cpu: 1
+    volumeClaimTemplates:
+      - metadata:
+          name: log
+        spec:
+          accessModes: [ ""ReadWriteOnce"" ]
+          ### Please override with custom supported storageClassName
+          storageClassName: ""alicloud-local-lvm""
+          resources:
+            requests:
+              storage: 10Gi
+    podTemplate:
+      apiVersion: v1
+      kind: Pod
+      metadata:
+        name: job-manager-pod-template
+      spec:
+        containers:
+          - name: flink-main-container
+            volumeMounts:
+              - name: log
+                mountPath: /opt/flink/log
+  taskManager:
+    replicas: 1
+    resource:
+      memory: ""2048m""
+      cpu: 1
+    volumeClaimTemplates:
+      - metadata:
+          name: log
+        spec:
+          accessModes: [ ""ReadWriteOnce"" ]
+          ### Please override with custom supported storageClassName
+          storageClassName: ""alicloud-local-lvm""
+          resources:
+            requests:
+              storage: 10Gi
+    podTemplate:
+      apiVersion: v1
+      kind: Pod
+      metadata:
+        name: task-manager-pod-template
+      spec:
+        containers:
+          - name: flink-main-container
+            volumeMounts:
+              - name: log
+                mountPath: /opt/flink/log
+  mode: standalone","[{'comment': 'missing newline at end', 'commenter': 'gyfora'}, {'comment': 'Added newline to the end.', 'commenter': 'Grypse'}]"
357,examples/standalone_pvc.yaml,"@@ -0,0 +1,81 @@
+################################################################################
+#  Licensed to the Apache Software Foundation (ASF) under one
+#  or more contributor license agreements.  See the NOTICE file
+#  distributed with this work for additional information
+#  regarding copyright ownership.  The ASF licenses this file
+#  to you under the Apache License, Version 2.0 (the
+#  ""License""); you may not use this file except in compliance
+#  with the License.  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an ""AS IS"" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+# limitations under the License.
+################################################################################
+
+kind: FlinkDeployment
+metadata:
+  namespace: default
+  name: basic-example
+spec:
+  image: flink:1.15.1
+  flinkVersion: v1_15
+  flinkConfiguration:
+    taskmanager.numberOfTaskSlots: ""2""
+  serviceAccount: flink
+  jobManager:
+    replicas: 1
+    resource:
+      memory: ""2048m""
+      cpu: 1
+    volumeClaimTemplates:
+      - metadata:
+          name: log
+        spec:
+          accessModes: [ ""ReadWriteOnce"" ]
+          ### Please override with custom supported storageClassName
+          storageClassName: ""alicloud-local-lvm""","[{'comment': 'Can we remove the reference to `alicloud` ? ', 'commenter': 'gyfora'}, {'comment': 'Just removed `alicloud` reference.', 'commenter': 'Grypse'}]"
357,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/FlinkConfigBuilder.java,"@@ -417,6 +421,33 @@ private static String createTempFile(Pod podTemplate) throws IOException {
         return tmp.getAbsolutePath();
     }
 
+    private static void setPvcTemplate(
+            List<PersistentVolumeClaim> persistentVolumeClaims,
+            Configuration effectiveConfig,
+            boolean isJm)
+            throws IOException {
+        if (persistentVolumeClaims != null) {
+            final ConfigOption<String> pvcConfigOptions =
+                    isJm
+                            ? StandaloneKubernetesConfigOptionsInternal.JOB_MANAGER_PVC_TEMPLATE
+                            : StandaloneKubernetesConfigOptionsInternal.TASK_MANAGER_PVC_TEMPLATE;
+            effectiveConfig.setString(
+                    pvcConfigOptions, String.join("","", createPvcTempFile(persistentVolumeClaims)));
+        }
+    }
+
+    private static List<String> createPvcTempFile(List<PersistentVolumeClaim> pvcTemplates)","[{'comment': 'We should add the newly created files to the list of cleaned up files under `cleanupTmpFiles` in this class', 'commenter': 'gyfora'}, {'comment': 'Added the newly created files to the list of cleaned up files to the method of `cleanupTmpFiles `', 'commenter': 'Grypse'}]"
357,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/FlinkUtils.java,"@@ -187,6 +187,18 @@ public static void setGenerationAnnotation(Configuration conf, Long generation)
         conf.set(KubernetesConfigOptions.JOB_MANAGER_ANNOTATIONS, labels);
     }
 
+    public static void setTaskmanagerGenerationAnnotation(Configuration conf, Long generation) {
+        if (generation == null) {
+            return;
+        }
+        var labels =
+                new HashMap<>(
+                        conf.getOptional(KubernetesConfigOptions.TASK_MANAGER_ANNOTATIONS)
+                                .orElse(Collections.emptyMap()));
+        labels.put(CR_GENERATION_LABEL, generation.toString());
+        conf.set(KubernetesConfigOptions.TASK_MANAGER_ANNOTATIONS, labels);
+    }","[{'comment': 'I feel this method duplicates too much code, we could simply have a generic method that takes the TASK_MANAGER/JOBMANAGER_ANNOTATIONS config option as param to factor out the logic', 'commenter': 'gyfora'}, {'comment': 'I have taken the TASK_MANAGER/JOBMANAGER_ANNOTATIONS config option as param to refactor annotation generation', 'commenter': 'Grypse'}]"
357,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/observer/deployment/AbstractDeploymentObserver.java,"@@ -141,41 +148,19 @@ protected void observeJmDeployment(
             return;
         }
 
-        Optional<Deployment> deployment = context.getSecondaryResource(Deployment.class);
-        if (deployment.isPresent()) {
-            DeploymentStatus status = deployment.get().getStatus();
-            DeploymentSpec spec = deployment.get().getSpec();
-            if (status != null
-                    && status.getAvailableReplicas() != null
-                    && spec.getReplicas().intValue() == status.getReplicas()
-                    && spec.getReplicas().intValue() == status.getAvailableReplicas()
-                    && flinkService.isJobManagerPortReady(effectiveConfig)) {
-
-                // typically it takes a few seconds for the REST server to be ready
-                logger.info(
-                        ""JobManager deployment port is ready, waiting for the Flink REST API..."");
-                deploymentStatus.setJobManagerDeploymentStatus(
-                        JobManagerDeploymentStatus.DEPLOYED_NOT_READY);
+        if (KubernetesDeploymentMode.NATIVE.equals(flinkApp.getSpec().getMode())) {
+            Optional<Deployment> deployment = context.getSecondaryResource(Deployment.class);
+            if (deployment.isPresent()) {
+                jmDeploymentObserver(deployment.get(), effectiveConfig, deploymentStatus, flinkApp);
                 return;","[{'comment': 'Why do we use StatefulSets for JobManager also? Seems to be an unnecessary complication without much benefits.', 'commenter': 'gyfora'}, {'comment': 'With StatefulSet for JobManager has the following advantagesï¼š\r\n\r\n1. Avoid random string suffix of JM Pod, more intuitive identification of JM.\r\n2. There is no need to manually create the PVC each time, which is very convenient when you need to create many flink clusters in production.\r\n3. When JM replicas is greater than 1, the one-to-one correspondence between PVC and JM can be guaranteed.', 'commenter': 'Grypse'}, {'comment': 'If these are the only advantages I would prefer Deployment:\r\n 1. Native integration has the random suffix, not a problem you can select by label if necessary\r\n 2. Not sure why you need a PVC for the JM\r\n 3. There is not a good reason to have more than one 1 replica and even then PVC is not necessary anyways\r\n \r\n @wangyang0918 ', 'commenter': 'gyfora'}, {'comment': 'Also looping @tweise here', 'commenter': 'gyfora'}, {'comment': 'Another benefit:\r\nwhen using a system with StatefulSet, allowing a simple HA solution like using a filesystem with PVC(such as SSD) can recover faster than remote storage(such as HDFS or OSS).', 'commenter': 'Grypse'}, {'comment': ""It's very important to run JM with PVC for working with work-dir[1] to fast recovery.\r\n\r\n1. https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/deployment/resource-providers/standalone/working_directory/"", 'commenter': 'Grypse'}, {'comment': 'I think you can have PVC with the current podTemplate also right?\r\n\r\nAlso the JM doesnâ€™t access much data during recovery , so even if it has to fetch it from remote storage it wouldnâ€™t really have a big impact.', 'commenter': 'gyfora'}, {'comment': 'Sorry for join the discussion so late.\r\n\r\nI share the same concern with @gyfora and @usamj . Having a PVC for JobManager does not take enough benefits now. IIRC, we does not have any state in the JobManager which needs to be recovered after failover. Using the StatefulSet also makes the JobManager has risk that could not be started if the previous one terminated too slow or stucked.', 'commenter': 'wangyang0918'}]"
360,examples/kubernetes-client-examples/pom.xml,"@@ -0,0 +1,49 @@
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+  http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+<project xmlns=""http://maven.apache.org/POM/4.0.0""
+         xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
+         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
+    <modelVersion>4.0.0</modelVersion>
+
+    <parent>
+        <groupId>org.apache.flink</groupId>
+        <artifactId>flink-kubernetes-operator-parent</artifactId>
+        <version>1.2-SNAPSHOT</version>
+        <relativePath>../..</relativePath>
+    </parent>
+
+    <artifactId>kubernetes-client-examples</artifactId>
+    <name>Flink Kubernetes client code Example</name>
+
+    <dependencies>
+        <dependency>
+            <groupId>io.fabric8</groupId>
+            <artifactId>kubernetes-client</artifactId>
+            <version>5.12.3</version>
+            <scope>compile</scope>
+        </dependency>
+        <dependency>
+            <groupId>org.apache.flink</groupId>
+            <artifactId>flink-kubernetes-operator</artifactId>
+            <version>1.2-SNAPSHOT</version>
+            <scope>compile</scope>
+        </dependency>
+    </dependencies>
+
+</project>","[{'comment': 'missing new line', 'commenter': 'morhidi'}, {'comment': 'added\r\n', 'commenter': 'gauravmiglanid11'}]"
360,examples/kubernetes-client-examples/src/main/java/org/apache/flink/examples/Basic.java,"@@ -0,0 +1,80 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.examples;
+
+import org.apache.flink.kubernetes.operator.crd.FlinkDeployment;
+import org.apache.flink.kubernetes.operator.crd.spec.FlinkDeploymentSpec;
+import org.apache.flink.kubernetes.operator.crd.spec.FlinkVersion;
+import org.apache.flink.kubernetes.operator.crd.spec.JobManagerSpec;
+import org.apache.flink.kubernetes.operator.crd.spec.JobSpec;
+import org.apache.flink.kubernetes.operator.crd.spec.Resource;
+import org.apache.flink.kubernetes.operator.crd.spec.TaskManagerSpec;
+import org.apache.flink.kubernetes.operator.crd.spec.UpgradeMode;
+
+import io.fabric8.kubernetes.api.model.ObjectMeta;
+import io.fabric8.kubernetes.client.KubernetesClient;
+import io.fabric8.kubernetes.client.KubernetesClientException;
+
+import java.util.Map;
+import java.util.Optional;
+
+import static java.util.Map.entry;
+
+/** client code for ../basic.yaml. */
+public class Basic {
+    public static void main(String[] args) {
+        FlinkDeployment flinkDeployment = new FlinkDeployment();
+        flinkDeployment.setApiVersion(""flink.apache.org/v1beta1"");
+        flinkDeployment.setKind(""FlinkDeployment"");
+        ObjectMeta objectMeta = new ObjectMeta();
+        objectMeta.setName(""advanced-ingress"");","[{'comment': 'this is not an advanced-ingress example, rather a basic', 'commenter': 'morhidi'}, {'comment': 'done', 'commenter': 'gauravmiglanid11'}]"
360,examples/kubernetes-client-examples/src/main/java/org/apache/flink/examples/Basic.java,"@@ -0,0 +1,80 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.examples;
+
+import org.apache.flink.kubernetes.operator.crd.FlinkDeployment;
+import org.apache.flink.kubernetes.operator.crd.spec.FlinkDeploymentSpec;
+import org.apache.flink.kubernetes.operator.crd.spec.FlinkVersion;
+import org.apache.flink.kubernetes.operator.crd.spec.JobManagerSpec;
+import org.apache.flink.kubernetes.operator.crd.spec.JobSpec;
+import org.apache.flink.kubernetes.operator.crd.spec.Resource;
+import org.apache.flink.kubernetes.operator.crd.spec.TaskManagerSpec;
+import org.apache.flink.kubernetes.operator.crd.spec.UpgradeMode;
+
+import io.fabric8.kubernetes.api.model.ObjectMeta;
+import io.fabric8.kubernetes.client.KubernetesClient;
+import io.fabric8.kubernetes.client.KubernetesClientException;
+
+import java.util.Map;
+import java.util.Optional;
+
+import static java.util.Map.entry;
+
+/** client code for ../basic.yaml. */
+public class Basic {
+    public static void main(String[] args) {
+        FlinkDeployment flinkDeployment = new FlinkDeployment();
+        flinkDeployment.setApiVersion(""flink.apache.org/v1beta1"");
+        flinkDeployment.setKind(""FlinkDeployment"");
+        ObjectMeta objectMeta = new ObjectMeta();
+        objectMeta.setName(""advanced-ingress"");
+        flinkDeployment.setMetadata(objectMeta);
+        FlinkDeploymentSpec flinkDeploymentSpec = new FlinkDeploymentSpec();
+        flinkDeploymentSpec.setFlinkVersion(FlinkVersion.v1_15);
+        flinkDeploymentSpec.setImage(""flink:1.15"");
+        Map<String, String> flinkConfiguration =
+                Map.ofEntries(entry(""taskmanager.numberOfTaskSlots"", ""2""));
+        flinkDeploymentSpec.setFlinkConfiguration(flinkConfiguration);
+        flinkDeployment.setSpec(flinkDeploymentSpec);
+        flinkDeploymentSpec.setServiceAccount(""flink"");
+        JobManagerSpec jobManagerSpec = new JobManagerSpec();
+        jobManagerSpec.setResource(new Resource(1.0, ""2048m""));
+        flinkDeploymentSpec.setJobManager(jobManagerSpec);
+        TaskManagerSpec taskManagerSpec = new TaskManagerSpec();
+        taskManagerSpec.setResource(new Resource(1.0, ""2048m""));
+        flinkDeploymentSpec.setTaskManager(taskManagerSpec);
+        flinkDeployment
+                .getSpec()
+                .setJob(
+                        JobSpec.builder()
+                                .jarURI(
+                                        ""local:///opt/flink/examples/streaming/StateMachineExample.jar"")
+                                .parallelism(2)
+                                .upgradeMode(UpgradeMode.STATELESS)
+                                .build());
+
+        try (KubernetesClient kubernetesClient =","[{'comment': 'The example should work as is with minikube, using: KubernetesClient kubernetesClient = new DefaultKubernetesClient(); \r\n\r\nIt is fine to add a utility explaining how to connect to a remote Kube API server, but please explain, how to get the token, cert, etc.', 'commenter': 'morhidi'}]"
360,examples/kubernetes-client-examples/pom.xml,"@@ -0,0 +1,49 @@
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+  http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+<project xmlns=""http://maven.apache.org/POM/4.0.0""
+         xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
+         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
+    <modelVersion>4.0.0</modelVersion>
+
+    <parent>
+        <groupId>org.apache.flink</groupId>
+        <artifactId>flink-kubernetes-operator-parent</artifactId>
+        <version>1.2-SNAPSHOT</version>
+        <relativePath>../..</relativePath>
+    </parent>
+
+    <artifactId>kubernetes-client-examples</artifactId>
+    <name>Flink Kubernetes client code Example</name>
+
+    <dependencies>
+        <dependency>","[{'comment': 'fabric8 is coming with the operator, no need for this dep here', 'commenter': 'morhidi'}]"
360,examples/kubernetes-client-examples/README.MD,"@@ -0,0 +1,48 @@
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+  http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+
+# Flink Kubernetes Client Code Example
+
+## Overview
+
+This is an end-to-end example of running Flink Job via code using the Flink Kubernetes Operator.
+
+It is only intended to serve as a showcase of how Flink Operator classes can be used to submit flink job the Flink Kubernetes Operator.
+
+
+*What's in this example?*
+
+1. Code approach to the simple basic.yaml","[{'comment': 'wording: Sample code for submitting an application similar to `examples/basic.yaml` programatically.', 'commenter': 'morhidi'}]"
360,examples/kubernetes-client-examples/README.MD,"@@ -0,0 +1,48 @@
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+  http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+
+# Flink Kubernetes Client Code Example
+
+## Overview
+
+This is an end-to-end example of running Flink Job via code using the Flink Kubernetes Operator.
+
+It is only intended to serve as a showcase of how Flink Operator classes can be used to submit flink job the Flink Kubernetes Operator.","[{'comment': 'nit: Second sentence is probably redundant here', 'commenter': 'morhidi'}]"
370,flink-kubernetes-standalone/src/main/java/org/apache/flink/kubernetes/operator/kubeclient/parameters/StandaloneKubernetesJobManagerParameters.java,"@@ -87,4 +88,8 @@ public Boolean getAllowNonRestoredState() {
         }
         return null;
     }
+
+    public Boolean isPipelineClasspathDefined() {
+        return flinkConfig.contains(PipelineOptions.CLASSPATHS);
+    }","[{'comment': ""I don't really understand this check. Why can't we always mount the usrlib dir? What if PipelineOptions.CLASSPATHS points to a directory outside usrlib?"", 'commenter': 'gyfora'}, {'comment': ""> Why can't we always mount the usrlib dir?\r\n\r\nGreat question! Check the end of my PR description for more context. It turns out, when [StandaloneApplicationClusterEntryPoint](https://github.com/apache/flink/blob/40d50f177c8ac63057dea2c755173a0dc138ede5/flink-container/src/main/java/org/apache/flink/container/entrypoint/StandaloneApplicationClusterEntryPoint.java#L104) starts, it tries to find the `usrlib` folder. And if it's not null [it'll be used for loading the classes, not the system classpath](https://github.com/apache/flink/blob/40d50f177c8ac63057dea2c755173a0dc138ede5/flink-clients/src/main/java/org/apache/flink/client/program/DefaultPackagedProgramRetriever.java#L143-L147). So, we simply can't have a `usrlib` folder when we want to load classes from `/opt/flink/lib`.\r\n\r\n> What if PipelineOptions.CLASSPATHS points to a directory outside usrlib?\r\n\r\nExactly. I don't know the context and all the use-cases, but in my mind `UserLibMountDecorator` shouldn't be part of the operator - let the user create/mount the right folder and place jars there. But deleting it in the context of this PR is a big change IMO, so added a check based on the comment on the `UserLibMountDecorator` class: `Mount the Flink User Lib directory to enable Flink to pick up a Jars defined in pipeline.classpaths.`. Which is think is misleading. WDYT?"", 'commenter': 'sap1ens'}, {'comment': 'I think without the `UserLibMountDecorator` it would be more cumbersome for the user to use the standalone mode. @usamj could probably provide more context here.', 'commenter': 'gyfora'}, {'comment': 'As you mentioned without the decorator the user would have to perform more work to use the Operator.\r\n\r\nI feel ease of use of the operator is important which is why I added this responsibility to the operator and believe that it should stay.', 'commenter': 'usamj'}, {'comment': 'I agree that we should keep it, but did you look at the changes @usamj ? The questions is whether the change makes sense or not to support this feature', 'commenter': 'gyfora'}]"
370,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/validation/DefaultValidator.java,"@@ -180,10 +180,6 @@ private Optional<String> validateJobSpec(
             return Optional.empty();
         }
 
-        if (StringUtils.isNullOrWhitespaceOnly(job.getJarURI())) {
-            return Optional.of(""Jar URI must be defined"");
-        }","[{'comment': 'You removed validation of JarURI for both FlinkDeployment and FlinkSessionJob but at this moment the FlinkSessionJob still requires it (otherwise I think we might get a NPE somewhere else). This is why I thought it would be simplest to include the `no-op` jar changes also in this PR \r\n\r\ncc @jeesmon ', 'commenter': 'gyfora'}, {'comment': ""@gyfora @sap1ens What I did in our side was:\r\n\r\n* Created a new maven module `flink-kubernetes-noop` to create a noop jar. I believe Gyula had mentioned we may be able to use some maven magic to generate this noop jar without adding another module, but I didn't invest time to look at it yet. I was trying to keep this change separate with minimal diff from upstream.\r\n* Updated Dockerfile to copy noop jar to $FLINK_HOME/lib folder. We use a different Dockerfile in our side as we have to use photon based base image.\r\n\r\nAs we are using helm to deploy FlinkSessionJob CR, I didn't make any operator code change to use noop jar when jarURI is not present. Instead in helm template of FlinkSessionJob I set jarURI to `file:///opt/flink/lib/noop-0.0.1.jar` if jarURI is absent and entryClass is present in values.yaml.\r\n\r\nIf the above path is still beneficial, I can create a PR. But please include changes as part of this PR instead of what I outlined above if it fits better with this PR."", 'commenter': 'jeesmon'}, {'comment': ""I don't think we need a new maven module, we could simply commit the empty jar into the project itself and avoid the complications :) "", 'commenter': 'gyfora'}, {'comment': ""Hey @jeesmon, I've tweaked `AbstractFlinkService` to pass an empty noop jar, however now I'm seeing an exception on the JobManager side: \r\n\r\n```\r\nCaused by: java.lang.ClassNotFoundException: org.apache.flink.streaming.examples.statemachine.StateMachineExample\r\n\tat java.net.URLClassLoader.findClass(Unknown Source) ~[?:?]\r\n\tat java.lang.ClassLoader.loadClass(Unknown Source) ~[?:?]\r\n\tat org.apache.flink.util.FlinkUserCodeClassLoader.loadClassWithoutExceptionHandling(FlinkUserCodeClassLoader.java:68) ~[flink-dist-1.15.2.jar:1.15.2]\r\n\tat org.apache.flink.util.ChildFirstClassLoader.loadClassWithoutExceptionHandling(ChildFirstClassLoader.java:65) ~[flink-dist-1.15.2.jar:1.15.2]\r\n\tat org.apache.flink.util.FlinkUserCodeClassLoader.loadClass(FlinkUserCodeClassLoader.java:52) ~[flink-dist-1.15.2.jar:1.15.2]\r\n\tat java.lang.ClassLoader.loadClass(Unknown Source) ~[?:?]\r\n\tat org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.loadClass(FlinkUserCodeClassLoaders.java:172) ~[flink-dist-1.15.2.jar:1.15.2]\r\n\tat java.lang.Class.forName0(Native Method) ~[?:?]\r\n\tat java.lang.Class.forName(Unknown Source) ~[?:?]\r\n\tat org.apache.flink.client.program.PackagedProgram.loadMainClass(PackagedProgram.java:479) ~[flink-dist-1.15.2.jar:1.15.2]\r\n\tat org.apache.flink.client.program.PackagedProgram.<init>(PackagedProgram.java:153) ~[flink-dist-1.15.2.jar:1.15.2]\r\n\tat org.apache.flink.client.program.PackagedProgram.<init>(PackagedProgram.java:65) ~[flink-dist-1.15.2.jar:1.15.2]\r\n\tat org.apache.flink.client.program.PackagedProgram$Builder.build(PackagedProgram.java:691) ~[flink-dist-1.15.2.jar:1.15.2]\r\n\tat org.apache.flink.runtime.webmonitor.handlers.utils.JarHandlerUtils$JarHandlerContext.toPackagedProgram(JarHandlerUtils.java:182) ~[flink-dist-1.15.2.jar:1.15.2]\r\n```\r\n\r\nMy job config is the following:\r\n\r\n```\r\napiVersion: flink.apache.org/v1beta1\r\nkind: FlinkSessionJob\r\nmetadata:\r\n  name: basic-session-job-only-example-noop2\r\nspec:\r\n  deploymentName: basic-session-deployment-only-example\r\n  job:\r\n    entryClass: org.apache.flink.streaming.examples.statemachine.StateMachineExample\r\n    parallelism: 4\r\n    upgradeMode: stateless\r\n```\r\n\r\nI've manually copied the jar file with the StateMachineExample class to the /opt/flink/lib folder.\r\n\r\nAfter analyzing the code it looks like Flink creates and utilizes a special classloader that only uses the uploaded jar and nothing else. Of course, it can't find anything since the jar is empty.\r\n\r\nWhat do I miss? "", 'commenter': 'sap1ens'}, {'comment': 'For me your code change seemed to work with the following base image:\r\n\r\n```\r\nFROM flink:1.15.1\r\nRUN cp /opt/flink/examples/streaming/StateMachineExample.jar /opt/flink/lib/.\r\n```\r\n\r\nSessionJob:\r\n```\r\napiVersion: flink.apache.org/v1beta1\r\nkind: FlinkSessionJob\r\nmetadata:\r\n  name: basic-session-job-only-example\r\nspec:\r\n  deploymentName: basic-session-deployment-only-example\r\n  job:\r\n    parallelism: 4\r\n    upgradeMode: stateless\r\n    entryClass: org.apache.flink.streaming.examples.statemachine.StateMachineExample\r\n```', 'commenter': 'gyfora'}, {'comment': ""Thanks @gyfora for confirming. That's exactly what I did when I tested. I was going to test it again this morning but you already did it :) "", 'commenter': 'jeesmon'}, {'comment': 'Alright, I added the jar after starting the container, thanks @jeesmon for telling me that :) \r\n\r\nThanks for checking!', 'commenter': 'sap1ens'}]"
370,flink-kubernetes-standalone/src/main/java/org/apache/flink/kubernetes/operator/kubeclient/parameters/StandaloneKubernetesJobManagerParameters.java,"@@ -87,4 +88,8 @@ public Boolean getAllowNonRestoredState() {
         }
         return null;
     }
+
+    public Boolean isPipelineClasspathDefined() {","[{'comment': 'this method should return `boolean` instead', 'commenter': 'gyfora'}, {'comment': 'Resolved', 'commenter': 'sap1ens'}]"
375,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/AbstractFlinkService.java,"@@ -165,7 +165,8 @@ public void submitApplicationCluster(
         if (requireHaMetadata) {
             validateHaMetadataExists(conf);
         }
-        deployApplicationCluster(jobSpec, conf);
+
+        deployApplicationCluster(jobSpec, removeOperatorConfigs(conf));","[{'comment': 'I think you might have forgotten adding it for session cluster deployments.', 'commenter': 'gyfora'}, {'comment': 'Do you mean submitSessionCluster in NativeFlinkService? kubernetesClusterDescriptor.deploySessionCluster takes cluster specification as input. It already helped to filter unnecessary configs?', 'commenter': 'HuangZhenQiu'}, {'comment': 'Yes I think this needs to be added to the `submitSessionCluster` implementations for both the native and standalone service.\r\n\r\nA proper test would actually validate all deployment modes and make sure the config is filtered :\r\n  - Native\r\n    - Application\r\n    - Session\r\n  - Standalone\r\n    - Application\r\n    - Session\r\n  \r\n  + SessionJob\r\n\r\n', 'commenter': 'gyfora'}, {'comment': 'Thanks for the suggestions. Test cases are added in the latest revision.', 'commenter': 'HuangZhenQiu'}]"
375,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/AbstractFlinkService.java,"@@ -182,7 +184,13 @@ public JobID submitJobToSessionCluster(
             throws Exception {
         // we generate jobID in advance to help deduplicate job submission.
         var jobID = FlinkUtils.generateSessionJobFixedJobID(meta);
-        runJar(spec.getJob(), jobID, uploadJar(meta, spec, conf), conf, savepoint);
+        Configuration runtimeConfig = removeOperatorConfigs(conf);
+        runJar(
+                spec.getJob(),
+                jobID,
+                uploadJar(meta, spec, runtimeConfig),
+                runtimeConfig,
+                savepoint);","[{'comment': ""I looked at the code and I think for sessionJobs we should not remove any configs. When we submit the jar through the rest api the configuration is not passed. \r\n\r\nRemoving operator configs here can actually break some artifact fetcher mechanisms within the operator here.\r\n\r\nWe don't have very good test coverage it seems that would catch this problem. I will open some tickets"", 'commenter': 'gyfora'}, {'comment': 'Thanks for the suggestion. Revised acccordingly.', 'commenter': 'HuangZhenQiu'}]"
375,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/AbstractFlinkService.java,"@@ -624,7 +626,7 @@ protected ClusterClient<String> getClusterClient(Configuration conf) throws Exce
                 conf, clusterId, (c, e) -> new StandaloneClientHAServices(restServerAddress));
     }
 
-    private JarRunResponseBody runJar(
+    protected JarRunResponseBody runJar(","[{'comment': ""please undo this change I think it's not necessary anymore"", 'commenter': 'gyfora'}, {'comment': 'Done', 'commenter': 'HuangZhenQiu'}]"
375,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/AbstractFlinkService.java,"@@ -666,7 +668,7 @@ private JarRunResponseBody runJar(
         }
     }
 
-    private JarUploadResponseBody uploadJar(
+    protected JarUploadResponseBody uploadJar(","[{'comment': ""please undo this change I think it's not necessary anymore"", 'commenter': 'gyfora'}, {'comment': 'Done', 'commenter': 'HuangZhenQiu'}]"
376,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/FlinkConfigManager.java,"@@ -157,7 +158,14 @@ public Configuration getObserveConfig(FlinkDeployment deployment) {
             throw new RuntimeException(
                     ""Cannot create observe config before first deployment, this indicates a bug."");
         }
-        return getConfig(deployment.getMetadata(), deployedSpec);
+        var conf = getConfig(deployment.getMetadata(), deployedSpec);
+        if (deployment.getSpec().getFlinkConfiguration() != null) {
+            var deployConfig = Configuration.fromMap(deployment.getSpec().getFlinkConfiguration());
+            deployConfig
+                    .getOptional(SAVEPOINT_DIRECTORY)
+                    .ifPresent(dir -> conf.set(SAVEPOINT_DIRECTORY, dir));","[{'comment': 'Maybe we could factor this out to a static utility like:\r\n```\r\napplyConfigsFromCurrentSpec(ConfigOption... configs)\r\n```', 'commenter': 'gyfora'}, {'comment': 'Added.', 'commenter': 'gaborgsomogyi'}]"
376,flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/TestUtils.java,"@@ -137,6 +137,36 @@ public static FlinkDeployment buildApplicationCluster(FlinkVersion version) {
         return buildApplicationCluster(TEST_DEPLOYMENT_NAME, TEST_NAMESPACE, version);
     }
 
+    public static FlinkDeployment buildApplicationCluster(","[{'comment': ""If this is only used in one test class let's keep it there otherwise it gets a bit confusing"", 'commenter': 'gyfora'}, {'comment': 'Moved back to test.', 'commenter': 'gaborgsomogyi'}]"
376,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/FlinkConfigManager.java,"@@ -177,6 +180,17 @@ private void addOperatorConfigsFromSpec(AbstractFlinkSpec spec, Configuration co
         }
     }
 
+    @SuppressWarnings(""unchecked"")
+    private void applyConfigsFromCurrentSpec(","[{'comment': 'This method should internally call the `addOperatorConfigsFromSpec` so we keep things tidy and logically together.', 'commenter': 'gyfora'}, {'comment': 'Moved.', 'commenter': 'gaborgsomogyi'}]"
377,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/AbstractFlinkResourceReconciler.java,"@@ -426,4 +431,17 @@ protected boolean flinkVersionChanged(SPEC oldSpec, SPEC newSpec) {
         }
         return false;
     }
+
+    private void setOwnerReference(CR owner, Configuration deployConfig) {
+        final Map<String, String> ownerReference =
+                Map.of(
+                        ""apiVersion"", owner.getApiVersion(),
+                        ""kind"", owner.getKind(),
+                        ""name"", owner.getMetadata().getName(),
+                        ""uid"", owner.getMetadata().getUid(),
+                        ""blockOwnerDeletion"", ""false"",
+                        ""controller"", ""false"");
+        deployConfig.set(
+                KubernetesConfigOptions.JOB_MANAGER_OWNER_REFERENCE, List.of(ownerReference));
+    }","[{'comment': ""Feels like the `FlinkConfigBuilder` might be a better place for this, then we can be sure that it's universally set in a single place."", 'commenter': 'gyfora'}, {'comment': 'Or the `getDeployConfig` method of the `FlinkConfigManager` it might be easier to add there :)', 'commenter': 'gyfora'}, {'comment': '@gyfora \r\nI tried it but, the problem is with the apiVersion and kind\r\n\r\nIs there a good way to add the apiVersion and kind of CustomResources, if not in the `AbstractFlinkResourceReconciler`?\r\n', 'commenter': 'zezaeoh'}, {'comment': 'We could probably change the information passed to getDeployConfig if necessary. Pass the whole CR + spec instead of meta + spec', 'commenter': 'gyfora'}, {'comment': 'but I dont have a better idea', 'commenter': 'gyfora'}]"
377,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/AbstractFlinkResourceReconciler.java,"@@ -97,6 +100,8 @@ public final void reconcile(CR cr, Context<?> ctx) throws Exception {
             return;
         }
 
+        setOwnerReference(cr, deployConfig);
+","[{'comment': 'I think we should call this method from inside the `deploy` method otherwise the ownerreference might not be set correctly during rollbacks, restarts etc', 'commenter': 'gyfora'}, {'comment': 'You are right, it might be have too broad impact to call this method from there :)', 'commenter': 'zezaeoh'}]"
379,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/StandaloneFlinkService.java,"@@ -66,14 +68,14 @@ public StandaloneFlinkService(
     @Override
     protected void deployApplicationCluster(JobSpec jobSpec, Configuration conf) throws Exception {
         LOG.info(""Deploying application cluster"");
-        submitClusterInternal(conf, Mode.APPLICATION);
+        submitClusterInternal(conf, Mode.APPLICATION, Optional.of(jobSpec));","[{'comment': 'Instead of passing the jobSpec through we should stick to the current model of passing the args through conf.\r\n\r\nIn `FlinkConfigBuilder` we can take the args from the JobSpec and set them to the flink config `kubernetes.jobmanager.entrypoint.args` (already exists).\r\n\r\nThis can then be accessed through the `StandaloneKubernetesJobManagerParameters` in `CmdStandaloneJobManagerDecorator` to add the args', 'commenter': 'usamj'}, {'comment': 'Thanks for the feedback! That makes more sense', 'commenter': 'avocadomaster'}]"
379,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/StandaloneFlinkService.java,"@@ -115,7 +117,7 @@ protected FlinkStandaloneKubeClient createNamespacedKubeClient(
                 executorService);
     }
 
-    private void submitClusterInternal(Configuration conf, Mode mode)
+    private void submitClusterInternal(Configuration conf, Mode mode, Optional<JobSpec> jobSpec)","[{'comment': 'We should use a `@Nullable` arg here instead of an Optional, as per Flink [coding standards](https://flink.apache.org/contributing/code-style-and-quality-java.html#java-optional)', 'commenter': 'dannycranmer'}]"
379,flink-kubernetes-standalone/src/main/java/org/apache/flink/kubernetes/operator/kubeclient/decorators/CmdStandaloneJobManagerDecorator.java,"@@ -85,6 +85,12 @@ private List<String> getApplicationClusterArgs() {
             args.add(allowNonRestoredState.toString());
         }
 
+        mainContainer.getArgs().stream()
+                .forEach(
+                        arg -> {
+                            args.add(arg);
+                        });
+
         return args;","[{'comment': 'How about `args.addAll(mainContainer.getArgs())`?', 'commenter': 'dannycranmer'}]"
379,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/FlinkConfigBuilder.java,"@@ -269,6 +270,12 @@ protected FlinkConfigBuilder applyJobOrSessionSpec() throws URISyntaxException {
                 effectiveConfig.set(
                         ApplicationConfiguration.APPLICATION_MAIN_CLASS, jobSpec.getEntryClass());
             }
+","[{'comment': 'Can we add a test for this, perhaps in `FlinkConfigBuilderTest`?', 'commenter': 'usamj'}, {'comment': 'âœ… ', 'commenter': 'avocadomaster'}]"
394,docs/content/docs/custom-resource/reference.md,"@@ -225,6 +225,16 @@ This page serves as a full reference for FlinkDeployment custom resource definit
 | error | java.lang.String | Error information about the FlinkDeployment/FlinkSessionJob. |
 | reconciliationStatus | org.apache.flink.kubernetes.operator.crd.status.FlinkSessionJobReconciliationStatus | Status of the last reconcile operation. |
 
+### JobHealthInfo
+**Class**: org.apache.flink.kubernetes.operator.crd.status.JobHealthInfo","[{'comment': 'We should move this class to another package, so that the reference generator does not pick it up. This object is not directly part of the status (only in a serialized form)', 'commenter': 'gyfora'}, {'comment': 'Moved.', 'commenter': 'gaborgsomogyi'}]"
394,docs/content/docs/custom-resource/reference.md,"@@ -251,6 +261,7 @@ This page serves as a full reference for FlinkDeployment custom resource definit
 | startTime | java.lang.String | Start time of the job. |
 | updateTime | java.lang.String | Update time of the job. |
 | savepointInfo | org.apache.flink.kubernetes.operator.crd.status.SavepointInfo | Information about pending and last savepoint for the job. |
+| jobHealthInfo | java.lang.String | Information about job health. |","[{'comment': 'I have been thinking a bit about this, we currently have a `clusterInfo` field in the `FlinkDeploymentStatus` we could simply use that map to store this information. \r\n\r\nI think originally it was designed with some flexibility in mind so we can store different metadata. cc @morhidi ', 'commenter': 'gyfora'}, {'comment': ""That's a key-value store which is designed to store config info:\r\n```\r\n    /** Config information from running clusters. */\r\n    private Map<String, String> clusterInfo = new HashMap<>();\r\n```\r\nFor example one key is `DashboardConfiguration.FIELD_NAME_FLINK_VERSION`.\r\nI can put it there but at the first glance it's not intended to store structures like this.\r\nAre we sure that we want to do that?\r\n"", 'commenter': 'gaborgsomogyi'}, {'comment': 'Well we added this so technically we are free to add anything to it. Currently contains version info but we might as well put the health info there too. \r\n\r\nThis approach has 0 risk if we later want to change/remove the health info. Whereas if you introduce a new field to the CRD that is there to stay forever due to backward compatibility', 'commenter': 'gyfora'}, {'comment': 'OK, moved the health info to the `clusterInfo` map.', 'commenter': 'gaborgsomogyi'}]"
394,docs/layouts/shortcodes/generated/kubernetes_operator_config_configuration.html,"@@ -80,6 +80,24 @@
             <td>Boolean</td>
             <td>Whether to enable recovery of missing/deleted jobmanager deployments.</td>
         </tr>
+        <tr>
+            <td><h5>kubernetes.operator.job.health-check.enabled</h5></td>
+            <td style=""word-wrap: break-word;"">false</td>
+            <td>Boolean</td>
+            <td>Whether to enable health check for jobs.</td>
+        </tr>
+        <tr>
+            <td><h5>kubernetes.operator.job.restart-check.duration-window</h5></td>
+            <td style=""word-wrap: break-word;"">2 min</td>
+            <td>Duration</td>
+            <td>The duration of the time window where job restart count measured.</td>
+        </tr>
+        <tr>
+            <td><h5>kubernetes.operator.job.restart-check.threshold</h5></td>","[{'comment': 'There is some inconsistency in the naming here, `health-check` vs `restart-check`. \r\nMaybe it could be:\r\n```\r\nkubernetes.operator.job.health-check.enabled\r\nkubernetes.operator.job.health-check.window\r\nkubernetes.operator.job.health-check.restarts.threshold / kubernetes.operator.job.health-check.num- restarts.threshold\r\n\r\n```', 'commenter': 'gyfora'}, {'comment': 'Good point, renamed to:\r\n```\r\nkubernetes.operator.job.health-check.enabled\r\nkubernetes.operator.job.health-check.restarts.window\r\nkubernetes.operator.job.health-check.restarts.threshold\r\n```\r\n', 'commenter': 'gaborgsomogyi'}]"
394,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/observer/deployment/AbstractDeploymentObserver.java,"@@ -116,13 +116,10 @@ public void observe(FlinkDeployment flinkApp, Context<?> context) {
     }
 
     private void observeClusterInfo(FlinkDeployment flinkApp, Configuration configuration) {
-        if (!flinkApp.getStatus().getClusterInfo().isEmpty()) {","[{'comment': 'Important change that now we fetch `ClusterInfo` all the time (now `ClusterInfo` contains other things like healt info).', 'commenter': 'gaborgsomogyi'}]"
394,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/observer/deployment/AbstractDeploymentObserver.java,"@@ -116,13 +116,10 @@ public void observe(FlinkDeployment flinkApp, Context<?> context) {
     }
 
     private void observeClusterInfo(FlinkDeployment flinkApp, Configuration configuration) {
-        if (!flinkApp.getStatus().getClusterInfo().isEmpty()) {
-            return;
-        }
         try {
             Map<String, String> clusterInfo = flinkService.getClusterInfo(configuration);
-            flinkApp.getStatus().setClusterInfo(clusterInfo);
-            logger.debug(""ClusterInfo: {}"", clusterInfo);
+            flinkApp.getStatus().getClusterInfo().putAll(clusterInfo);","[{'comment': 'Important change that now we merge fetched info in order to keep previously set health info or other upcoming things.', 'commenter': 'gaborgsomogyi'}]"
394,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/observer/deployment/AbstractDeploymentObserver.java,"@@ -139,8 +135,6 @@ protected void observeJmDeployment(
             return;
         }
 
-        flinkApp.getStatus().setClusterInfo(new HashMap<>());","[{'comment': 'This was blocking the reconciler to get valid health info in case of the cluster was in restarting state.', 'commenter': 'gaborgsomogyi'}]"
394,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/KubernetesOperatorConfigOptions.java,"@@ -315,4 +315,28 @@ public static String operatorConfigKey(String key) {
                     .intType()
                     .defaultValue(8085)
                     .withDescription(""The port the health probe will use to expose the status."");
+
+    @Documentation.Section(SECTION_SYSTEM)","[{'comment': 'I think these configs should be in `SECTION_DYNAMIC` as it can be defined on a per resource layer by the user.', 'commenter': 'gyfora'}, {'comment': 'Fixed.', 'commenter': 'gaborgsomogyi'}]"
394,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/ApplicationReconciler.java,"@@ -272,6 +286,46 @@ private void recoverJmDeployment(
         restoreJob(deployment, specToRecover, deployment.getStatus(), ctx, observeConfig, true);
     }
 
+    private boolean shouldRestartJobBecauseUnhealthy(
+            FlinkDeployment deployment, Configuration observeConfig) {
+        if (observeConfig.getBoolean(OPERATOR_CLUSTER_HEALTH_CHECK_ENABLED)) {
+            if (jobHealthChecker == null) {
+                jobHealthChecker = new JobHealthChecker(Clock.systemDefaultZone());
+            }
+        } else {
+            if (jobHealthChecker != null) {
+                jobHealthChecker = null;
+            }
+        }","[{'comment': ""It doesn't look correct to manipulate fields based on resource specific configs like health check enabled/or not "", 'commenter': 'gyfora'}, {'comment': 'Refactored fully.', 'commenter': 'gaborgsomogyi'}]"
394,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/ApplicationReconciler.java,"@@ -272,6 +286,46 @@ private void recoverJmDeployment(
         restoreJob(deployment, specToRecover, deployment.getStatus(), ctx, observeConfig, true);
     }
 
+    private boolean shouldRestartJobBecauseUnhealthy(
+            FlinkDeployment deployment, Configuration observeConfig) {
+        if (observeConfig.getBoolean(OPERATOR_CLUSTER_HEALTH_CHECK_ENABLED)) {
+            if (jobHealthChecker == null) {
+                jobHealthChecker = new JobHealthChecker(Clock.systemDefaultZone());
+            }
+        } else {
+            if (jobHealthChecker != null) {
+                jobHealthChecker = null;
+            }
+        }
+
+        boolean result = false;
+
+        if (jobHealthChecker != null) {
+            final String clusterInfoKey = JobHealthInfo.class.getSimpleName();
+            if (deployment.getStatus().getClusterInfo().containsKey(clusterInfoKey)) {
+                LOG.debug(""Cluster info contains job health info"");
+                if (!jobHealthChecker.isJobHealthy(","[{'comment': 'I think there are all kinds of concurrency issues here. I think your code assumes that there is one reconciler per resource, but the reconciler is used concurrently from multiple thread.', 'commenter': 'gyfora'}, {'comment': 'Refactored fully.', 'commenter': 'gaborgsomogyi'}]"
394,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/JobHealthChecker.java,"@@ -0,0 +1,130 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.reconciler.deployment;
+
+import org.apache.flink.annotation.VisibleForTesting;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.kubernetes.operator.health.JobHealthInfo;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import javax.annotation.Nullable;
+
+import java.time.Clock;
+import java.time.Duration;
+
+import static org.apache.flink.kubernetes.operator.config.KubernetesOperatorConfigOptions.OPERATOR_CLUSTER_HEALTH_CHECK_RESTARTS_THRESHOLD;
+import static org.apache.flink.kubernetes.operator.config.KubernetesOperatorConfigOptions.OPERATOR_CLUSTER_HEALTH_CHECK_RESTARTS_WINDOW;
+
+/** Evaluates whether the job is healthy. */
+public class JobHealthChecker {
+
+    private static final Logger LOG = LoggerFactory.getLogger(JobHealthChecker.class);
+
+    private final Clock clock;
+    @VisibleForTesting @Nullable JobHealthInfo lastValidJobHealthInfo;
+
+    public JobHealthChecker(Clock clock) {
+        this.clock = clock;
+    }
+
+    @VisibleForTesting
+    void setLastValidJobHealthInfo(JobHealthInfo lastValidJobHealthInfo) {
+        LOG.debug(""Setting last valid health check info"");
+        this.lastValidJobHealthInfo = lastValidJobHealthInfo;","[{'comment': 'If the isHealthy logic depends only on the last health info, it might be best to compute it at the time when you observe the new one but before you put it in the status. At that point you have the previous and new health info', 'commenter': 'gyfora'}, {'comment': 'This way you would also put the isHealthy info in the status and the reconciler would not need to compute anything', 'commenter': 'gyfora'}, {'comment': 'Refactored fully.', 'commenter': 'gaborgsomogyi'}]"
394,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/health/ClusterHealthInfo.java,"@@ -0,0 +1,78 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.health;
+
+import org.apache.flink.annotation.Experimental;
+
+import com.fasterxml.jackson.core.JsonProcessingException;
+import com.fasterxml.jackson.databind.ObjectMapper;
+import lombok.AllArgsConstructor;
+import lombok.Data;
+import lombok.NoArgsConstructor;
+import lombok.ToString;
+
+import java.time.Clock;
+
+/** Represents information about job health. */
+@Experimental
+@Data
+@AllArgsConstructor
+@NoArgsConstructor
+@ToString
+public class ClusterHealthInfo {","[{'comment': '```\r\n@Data\r\n@NoArgsConstructor\r\n```\r\nShould be enough, the rest comes from Data', 'commenter': 'gyfora'}, {'comment': '`@ToString` dropped, the printout is the same.\r\n`@AllArgsConstructor` is must because otherwise `new ClusterHealthInfo(clock.millis(), numRestarts, true);` fails to compile', 'commenter': 'gaborgsomogyi'}]"
394,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/observer/deployment/ApplicationObserver.java,"@@ -32,15 +33,21 @@
 
 import io.javaoperatorsdk.operator.api.reconciler.Context;
 
+import javax.annotation.Nullable;
+
 import java.util.List;
 import java.util.Optional;
 
+import static org.apache.flink.kubernetes.operator.config.KubernetesOperatorConfigOptions.OPERATOR_CLUSTER_HEALTH_CHECK_ENABLED;
+
 /** The observer of {@link org.apache.flink.kubernetes.operator.config.Mode#APPLICATION} cluster. */
 public class ApplicationObserver extends AbstractDeploymentObserver {
 
     private final SavepointObserver<FlinkDeployment, FlinkDeploymentStatus> savepointObserver;
     private final JobStatusObserver<FlinkDeployment, ApplicationObserverContext> jobStatusObserver;
 
+    @Nullable private ClusterHealthObserver clusterHealthObserver;","[{'comment': 'Not `@Nullable` ', 'commenter': 'gyfora'}, {'comment': 'Fixed.', 'commenter': 'gaborgsomogyi'}]"
395,docs/content/docs/custom-resource/overview.md,"@@ -149,6 +149,26 @@ For Session clusters the operator only provides very basic management and monito
  - Monitor overall cluster health
  - Stop / Delete Session cluster
 
+### Cluster Deployment Modes","[{'comment': 'This reads ok, however as a consumer of this documentation I might want to know when to pick Native vs Standalone or vice versa. Can we add some basic use-case examples where each mode is useful for?', 'commenter': 'dannycranmer'}]"
407,docs/content/docs/custom-resource/job-management.md,"@@ -241,6 +241,21 @@ In order this feature to work one must enable [recovery of missing job deploymen
 At the moment deployment is considered unhealthy when Flink's restarts count reaches `kubernetes.operator.cluster.health-check.restarts.threshold` (default: `64`)
 within time window of `kubernetes.operator.cluster.health-check.restarts.window` (default: 2 minutes).
 
+## Restart failed job deployments
+
+The operator can restart a failed Flink cluster deployment. This could be useful in cases when the job main task is
+able to reconfigure the job to handle these failures.
+
+For example a job could dynamically create the DAG based on some job configuration which job configuration could
+change over time. When a task detects a record which could not be handled with the current configuration then the task
+should throw a `SuppressRestartsException` to fail the job. If `kubernetes.operator.cluster.restart.failed` is set to 
+`true` (default: `false`) then the operator detects the failed job and restarts it. When the job restarts then it reads
+the new job configuration and creates the new DAG based on this new configuration. The new deployment could handle the
+incoming records and no manual intervention is needed.","[{'comment': ""Let's remove this paragraph, this is very use-case specific."", 'commenter': 'gyfora'}, {'comment': 'Done', 'commenter': 'pvary'}]"
407,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/KubernetesOperatorConfigOptions.java,"@@ -339,4 +339,11 @@ public static String operatorConfigKey(String key) {
                     .withDescription(
                             ""The threshold which is checked against job restart count within a configured window. ""
                                     + ""If the restart count is reaching the threshold then full cluster restart is initiated."");
+
+    @Documentation.Section(SECTION_DYNAMIC)
+    public static final ConfigOption<Boolean> OPERATOR_CLUSTER_RESTART_FAILED =
+            operatorConfig(""cluster.restart.failed"")","[{'comment': 'this should be:\r\n\r\n```\r\njob.restart.failed\r\n```', 'commenter': 'gyfora'}, {'comment': 'Done', 'commenter': 'pvary'}]"
407,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/ApplicationReconciler.java,"@@ -262,26 +264,44 @@ public boolean reconcileOtherChanges(
             return true;
         }
 
+        if (JobStatus.valueOf(deployment.getStatus().getJobStatus().getState()) == JobStatus.FAILED","[{'comment': 'Maybe we could move this logic up one level to the `AbstractJobReconciler` this logic and config makes sense for both Applications and SessionJobs', 'commenter': 'gyfora'}, {'comment': 'Done', 'commenter': 'pvary'}]"
407,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/AbstractJobReconciler.java,"@@ -238,8 +240,34 @@ protected void rollback(CR resource, Context<?> ctx, Configuration observeConfig
     @Override
     public boolean reconcileOtherChanges(
             CR resource, Context<?> context, Configuration observeConfig) throws Exception {
-        return SavepointUtils.triggerSavepointIfNeeded(
-                getFlinkService(resource, context), resource, observeConfig);
+        var jobStatus =
+                org.apache.flink.api.common.JobStatus.valueOf(
+                        resource.getStatus().getJobStatus().getState());
+        if (jobStatus == org.apache.flink.api.common.JobStatus.FAILED
+                && observeConfig.getBoolean(OPERATOR_JOB_RESTART_FAILED)) {
+            LOG.info(""Stopping failed Flink Cluster deployment..."");
+            cancelJob(resource, context, UpgradeMode.LAST_STATE, observeConfig);
+            resource.getStatus().setError("""");
+            resubmitJmDeployment(resource, context, observeConfig, false);
+            return true;
+        } else {
+            return SavepointUtils.triggerSavepointIfNeeded(
+                    getFlinkService(resource, context), resource, observeConfig);
+        }
+    }
+
+    protected void resubmitJmDeployment(
+            CR deployment, Context<?> ctx, Configuration observeConfig, boolean requireHaMetadata)
+            throws Exception {
+        LOG.info(""Resubmitting Flink Cluster deployment..."");
+        SPEC specToRecover = ReconciliationUtils.getDeployedSpec(deployment);
+        restoreJob(
+                deployment,
+                specToRecover,
+                deployment.getStatus(),
+                ctx,
+                observeConfig,
+                requireHaMetadata);","[{'comment': ""The `resubmitJmDeployment` name is not correct here. Maybe it's better to simply inline the `restoreJob` call in the if branch if it's only called in one place"", 'commenter': 'gyfora'}, {'comment': 'Or just rename it to `resubmintJob` and modify the logging accordingly', 'commenter': 'gyfora'}, {'comment': 'I like this. Renamed to `resubmitJob`, and modified the log message accordingly', 'commenter': 'pvary'}]"
407,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/AbstractJobReconciler.java,"@@ -253,4 +281,14 @@ public boolean reconcileOtherChanges(
     protected abstract void cancelJob(
             CR resource, Context<?> ctx, UpgradeMode upgradeMode, Configuration observeConfig)
             throws Exception;
+
+    /**
+     * Removes a failed job.
+     *
+     * @param resource The failed job.
+     * @param observeConfig Observe configuration.
+     * @throws Exception Error during cancellation.
+     */
+    protected abstract void removeFailedJob(
+            CR resource, Context<?> ctx, Configuration observeConfig) throws Exception;","[{'comment': 'I suggest we rename this to `cleanupAfterFailedJob`', 'commenter': 'gyfora'}, {'comment': 'Done', 'commenter': 'pvary'}]"
409,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/crd/spec/AbstractFlinkSpec.java,"@@ -57,6 +57,9 @@ public abstract class AbstractFlinkSpec implements Diffable<AbstractFlinkSpec> {
     })
     private Map<String, String> flinkConfiguration;
 
+    /** Exception specification to include stack trace in CRD. */
+    private boolean stackTraceEnabled;","[{'comment': 'This should be a `ConfigOption` that user can provide in the `flinkConfiguration` section like other things that configure the operator behaviour.', 'commenter': 'gyfora'}, {'comment': 'Thanks for the suggestion, I have moved them to ConfigOption along with new config to limit the stacktrace, throwable size etc...', 'commenter': 'darenwkt'}]"
409,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/FlinkResourceExceptionUtils.java,"@@ -0,0 +1,118 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.utils;
+
+import org.apache.commons.lang3.exception.ExceptionUtils;
+import org.apache.flink.kubernetes.operator.crd.AbstractFlinkResource;
+import org.apache.flink.kubernetes.operator.crd.spec.AbstractFlinkSpec;
+import org.apache.flink.kubernetes.operator.exception.DeploymentFailedException;
+import org.apache.flink.kubernetes.operator.exception.FlinkResourceException;
+import org.apache.flink.kubernetes.operator.exception.ReconciliationException;
+import org.apache.flink.runtime.rest.util.RestClientException;
+
+import com.fasterxml.jackson.core.JsonProcessingException;
+import com.fasterxml.jackson.databind.ObjectMapper;
+
+import java.util.Map;
+import java.util.Optional;
+import java.util.stream.Collectors;
+
+/** Flink Resource Exception utilities. */
+public final class FlinkResourceExceptionUtils {
+
+    public static <R extends AbstractFlinkResource> void updateFlinkResourceException(
+            Throwable throwable, R resource) {
+
+        FlinkResourceException flinkResourceException =
+                getFlinkResourceException(throwable, getStackTraceEnabled(resource));
+
+        try {
+            ((AbstractFlinkResource<?, ?>) resource)
+                    .getStatus()
+                    .setError(convertToJson(flinkResourceException));
+        } catch (Exception e) {
+            // Rollback to setting error string/message to CRD
+            ((AbstractFlinkResource<?, ?>) resource)
+                    .getStatus()
+                    .setError(
+                            (e instanceof ReconciliationException)
+                                    ? e.getCause().toString()
+                                    : e.toString());
+        }
+    }
+
+    private static <R extends AbstractFlinkResource> boolean getStackTraceEnabled(R resource) {
+        return Optional.ofNullable((AbstractFlinkSpec) resource.getSpec())
+                .map(AbstractFlinkSpec::isStackTraceEnabled)
+                .orElse(false);
+    }
+
+    private static FlinkResourceException getFlinkResourceException(
+            Throwable throwable, boolean isStackTraceEnabled) {
+        FlinkResourceException flinkResourceException =
+                convertToFlinkResourceException(throwable, isStackTraceEnabled);
+
+        flinkResourceException.setThrowableList(
+                ExceptionUtils.getThrowableList(
+                                throwable.getCause())
+                        .stream()
+                        .map((t) -> convertToFlinkResourceException(t, false))
+                        .collect(Collectors.toList()));
+
+        return flinkResourceException;
+    }
+
+    private static FlinkResourceException convertToFlinkResourceException(
+            Throwable throwable, boolean isStackTraceEnabled) {
+        FlinkResourceException flinkResourceException =
+                FlinkResourceException.builder()
+                        .type(throwable.getClass().getName())
+                        .message(throwable.getMessage())
+                        .build();
+
+        if (isStackTraceEnabled) {
+            flinkResourceException.setStackTraceElements(throwable.getStackTrace());
+        }
+
+        enrichMetadata(throwable, flinkResourceException);
+
+        return flinkResourceException;
+    }
+
+    private static void enrichMetadata(
+            Throwable throwable, FlinkResourceException flinkResourceException) {
+        if (throwable instanceof RestClientException) {
+            flinkResourceException.setAdditionalMetadata(
+                    Map.of(
+                            ""httpResponseCode"",
+                            ((RestClientException) throwable).getHttpResponseStatus().code()));
+        }
+
+        if (throwable instanceof DeploymentFailedException) {
+            flinkResourceException.setAdditionalMetadata(
+                    Map.of(""reason"", ((DeploymentFailedException) throwable).getReason()));
+        }
+
+        // This section can be extended to enrich more metadata in the future.
+    }
+
+    private static String convertToJson(FlinkResourceException flinkResourceException)
+            throws JsonProcessingException {
+        return new ObjectMapper().writeValueAsString(flinkResourceException);","[{'comment': 'We should re-use a `private static final ObjectMapper`', 'commenter': 'gyfora'}, {'comment': 'Updated, thanks', 'commenter': 'darenwkt'}]"
409,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/FlinkResourceExceptionUtils.java,"@@ -0,0 +1,118 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.utils;
+
+import org.apache.commons.lang3.exception.ExceptionUtils;
+import org.apache.flink.kubernetes.operator.crd.AbstractFlinkResource;
+import org.apache.flink.kubernetes.operator.crd.spec.AbstractFlinkSpec;
+import org.apache.flink.kubernetes.operator.exception.DeploymentFailedException;
+import org.apache.flink.kubernetes.operator.exception.FlinkResourceException;
+import org.apache.flink.kubernetes.operator.exception.ReconciliationException;
+import org.apache.flink.runtime.rest.util.RestClientException;
+
+import com.fasterxml.jackson.core.JsonProcessingException;
+import com.fasterxml.jackson.databind.ObjectMapper;
+
+import java.util.Map;
+import java.util.Optional;
+import java.util.stream.Collectors;
+
+/** Flink Resource Exception utilities. */
+public final class FlinkResourceExceptionUtils {
+
+    public static <R extends AbstractFlinkResource> void updateFlinkResourceException(
+            Throwable throwable, R resource) {
+
+        FlinkResourceException flinkResourceException =
+                getFlinkResourceException(throwable, getStackTraceEnabled(resource));
+
+        try {
+            ((AbstractFlinkResource<?, ?>) resource)
+                    .getStatus()
+                    .setError(convertToJson(flinkResourceException));
+        } catch (Exception e) {
+            // Rollback to setting error string/message to CRD
+            ((AbstractFlinkResource<?, ?>) resource)
+                    .getStatus()
+                    .setError(
+                            (e instanceof ReconciliationException)
+                                    ? e.getCause().toString()
+                                    : e.toString());
+        }
+    }
+
+    private static <R extends AbstractFlinkResource> boolean getStackTraceEnabled(R resource) {
+        return Optional.ofNullable((AbstractFlinkSpec) resource.getSpec())
+                .map(AbstractFlinkSpec::isStackTraceEnabled)
+                .orElse(false);
+    }
+
+    private static FlinkResourceException getFlinkResourceException(
+            Throwable throwable, boolean isStackTraceEnabled) {
+        FlinkResourceException flinkResourceException =
+                convertToFlinkResourceException(throwable, isStackTraceEnabled);
+
+        flinkResourceException.setThrowableList(
+                ExceptionUtils.getThrowableList(
+                                throwable.getCause())
+                        .stream()
+                        .map((t) -> convertToFlinkResourceException(t, false))
+                        .collect(Collectors.toList()));","[{'comment': 'I think this is a bit dangerous, we should have a default limit on these. Maybe throwable list max 2 elements by default, configurable to more or less', 'commenter': 'gyfora'}, {'comment': 'I have added the default limit as ConfigOption and set it to 2', 'commenter': 'darenwkt'}]"
409,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/FlinkResourceExceptionUtils.java,"@@ -0,0 +1,118 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.utils;
+
+import org.apache.commons.lang3.exception.ExceptionUtils;
+import org.apache.flink.kubernetes.operator.crd.AbstractFlinkResource;
+import org.apache.flink.kubernetes.operator.crd.spec.AbstractFlinkSpec;
+import org.apache.flink.kubernetes.operator.exception.DeploymentFailedException;
+import org.apache.flink.kubernetes.operator.exception.FlinkResourceException;
+import org.apache.flink.kubernetes.operator.exception.ReconciliationException;
+import org.apache.flink.runtime.rest.util.RestClientException;
+
+import com.fasterxml.jackson.core.JsonProcessingException;
+import com.fasterxml.jackson.databind.ObjectMapper;
+
+import java.util.Map;
+import java.util.Optional;
+import java.util.stream.Collectors;
+
+/** Flink Resource Exception utilities. */
+public final class FlinkResourceExceptionUtils {
+
+    public static <R extends AbstractFlinkResource> void updateFlinkResourceException(
+            Throwable throwable, R resource) {
+
+        FlinkResourceException flinkResourceException =
+                getFlinkResourceException(throwable, getStackTraceEnabled(resource));
+
+        try {
+            ((AbstractFlinkResource<?, ?>) resource)
+                    .getStatus()
+                    .setError(convertToJson(flinkResourceException));
+        } catch (Exception e) {
+            // Rollback to setting error string/message to CRD
+            ((AbstractFlinkResource<?, ?>) resource)
+                    .getStatus()
+                    .setError(
+                            (e instanceof ReconciliationException)
+                                    ? e.getCause().toString()
+                                    : e.toString());
+        }
+    }
+
+    private static <R extends AbstractFlinkResource> boolean getStackTraceEnabled(R resource) {
+        return Optional.ofNullable((AbstractFlinkSpec) resource.getSpec())
+                .map(AbstractFlinkSpec::isStackTraceEnabled)
+                .orElse(false);
+    }
+
+    private static FlinkResourceException getFlinkResourceException(
+            Throwable throwable, boolean isStackTraceEnabled) {
+        FlinkResourceException flinkResourceException =
+                convertToFlinkResourceException(throwable, isStackTraceEnabled);
+
+        flinkResourceException.setThrowableList(
+                ExceptionUtils.getThrowableList(
+                                throwable.getCause())
+                        .stream()
+                        .map((t) -> convertToFlinkResourceException(t, false))
+                        .collect(Collectors.toList()));
+
+        return flinkResourceException;
+    }
+
+    private static FlinkResourceException convertToFlinkResourceException(
+            Throwable throwable, boolean isStackTraceEnabled) {
+        FlinkResourceException flinkResourceException =
+                FlinkResourceException.builder()
+                        .type(throwable.getClass().getName())
+                        .message(throwable.getMessage())
+                        .build();
+
+        if (isStackTraceEnabled) {
+            flinkResourceException.setStackTraceElements(throwable.getStackTrace());","[{'comment': 'We should have size limits on the stack trace as well, how many lines, total length etc.', 'commenter': 'gyfora'}, {'comment': 'Added ConfigOption to limit stackTrace list size', 'commenter': 'darenwkt'}]"
409,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/FlinkResourceExceptionUtils.java,"@@ -0,0 +1,118 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.utils;
+
+import org.apache.commons.lang3.exception.ExceptionUtils;
+import org.apache.flink.kubernetes.operator.crd.AbstractFlinkResource;
+import org.apache.flink.kubernetes.operator.crd.spec.AbstractFlinkSpec;
+import org.apache.flink.kubernetes.operator.exception.DeploymentFailedException;
+import org.apache.flink.kubernetes.operator.exception.FlinkResourceException;
+import org.apache.flink.kubernetes.operator.exception.ReconciliationException;
+import org.apache.flink.runtime.rest.util.RestClientException;
+
+import com.fasterxml.jackson.core.JsonProcessingException;
+import com.fasterxml.jackson.databind.ObjectMapper;
+
+import java.util.Map;
+import java.util.Optional;
+import java.util.stream.Collectors;
+
+/** Flink Resource Exception utilities. */
+public final class FlinkResourceExceptionUtils {
+
+    public static <R extends AbstractFlinkResource> void updateFlinkResourceException(
+            Throwable throwable, R resource) {
+
+        FlinkResourceException flinkResourceException =
+                getFlinkResourceException(throwable, getStackTraceEnabled(resource));
+
+        try {
+            ((AbstractFlinkResource<?, ?>) resource)
+                    .getStatus()
+                    .setError(convertToJson(flinkResourceException));
+        } catch (Exception e) {
+            // Rollback to setting error string/message to CRD
+            ((AbstractFlinkResource<?, ?>) resource)
+                    .getStatus()
+                    .setError(
+                            (e instanceof ReconciliationException)
+                                    ? e.getCause().toString()
+                                    : e.toString());
+        }
+    }
+
+    private static <R extends AbstractFlinkResource> boolean getStackTraceEnabled(R resource) {
+        return Optional.ofNullable((AbstractFlinkSpec) resource.getSpec())
+                .map(AbstractFlinkSpec::isStackTraceEnabled)
+                .orElse(false);
+    }
+
+    private static FlinkResourceException getFlinkResourceException(
+            Throwable throwable, boolean isStackTraceEnabled) {
+        FlinkResourceException flinkResourceException =
+                convertToFlinkResourceException(throwable, isStackTraceEnabled);
+
+        flinkResourceException.setThrowableList(
+                ExceptionUtils.getThrowableList(
+                                throwable.getCause())
+                        .stream()
+                        .map((t) -> convertToFlinkResourceException(t, false))
+                        .collect(Collectors.toList()));
+
+        return flinkResourceException;
+    }
+
+    private static FlinkResourceException convertToFlinkResourceException(
+            Throwable throwable, boolean isStackTraceEnabled) {
+        FlinkResourceException flinkResourceException =
+                FlinkResourceException.builder()
+                        .type(throwable.getClass().getName())
+                        .message(throwable.getMessage())
+                        .build();
+
+        if (isStackTraceEnabled) {
+            flinkResourceException.setStackTraceElements(throwable.getStackTrace());
+        }
+
+        enrichMetadata(throwable, flinkResourceException);
+
+        return flinkResourceException;
+    }
+
+    private static void enrichMetadata(
+            Throwable throwable, FlinkResourceException flinkResourceException) {
+        if (throwable instanceof RestClientException) {
+            flinkResourceException.setAdditionalMetadata(
+                    Map.of(
+                            ""httpResponseCode"",
+                            ((RestClientException) throwable).getHttpResponseStatus().code()));
+        }
+
+        if (throwable instanceof DeploymentFailedException) {
+            flinkResourceException.setAdditionalMetadata(
+                    Map.of(""reason"", ((DeploymentFailedException) throwable).getReason()));","[{'comment': 'We should add size limit on the reason', 'commenter': 'gyfora'}, {'comment': 'Added size limit as configOption', 'commenter': 'darenwkt'}]"
420,flink-kubernetes-webhook/pom.xml,"@@ -96,6 +96,12 @@ under the License.
             <version>${okhttp.version}</version>
             <scope>test</scope>
         </dependency>
+        <dependency>
+            <groupId>org.mockito</groupId>","[{'comment': ""We're trying to avoid using mockito in the project. We write test classes instead."", 'commenter': 'morhidi'}, {'comment': '@morhidi Understand, I looked at using https://junit-pioneer.org/docs/environment-variables/ which has some nice capabilities to change system env vars, but noticed when running it I get `This extension uses reflection to mutate JDK-internal state, which is fragile` warnings when running in intellij. Do you think it would be ok to add a method to EnvUtils e.g. setEnvironment(Map env) and use this to override System env vars ? Or can you think of something better ?', 'commenter': 'tagarr'}, {'comment': 'Actually noticed there is a method in TestUtils, am I ok to use that ?', 'commenter': 'tagarr'}, {'comment': ""Ah ok, I didn't realize there's `TestUtils.setEnv(Map<String, String> newEnv)`, yes you can use that."", 'commenter': 'morhidi'}]"
420,helm/flink-kubernetes-operator/templates/webhook.yaml,"@@ -85,7 +85,7 @@ metadata:
     cert-manager.io/inject-ca-from: {{ .Release.Namespace }}/flink-operator-serving-cert
   name: flink-operator-{{ .Release.Namespace }}-webhook-configuration
 webhooks:
-- name: flinkoperator.flink.apache.org
+- name: validationwebhook.flink.apache.org","[{'comment': 'Why the change?', 'commenter': 'morhidi'}, {'comment': 'the webhook names need to be unique', 'commenter': 'tagarr'}, {'comment': 'Do you mean the validating and the mutating webhook name clashes?', 'commenter': 'gyfora'}, {'comment': 'If so, why not change both?', 'commenter': 'gyfora'}, {'comment': 'will do', 'commenter': 'tagarr'}]"
420,pom.xml,"@@ -89,6 +89,8 @@ under the License.
         <okhttp.version>4.10.0</okhttp.version>
 
         <snakeyaml.version>1.32</snakeyaml.version>
+
+        <mockito.version>4.8.1</mockito.version>","[{'comment': ""Let's remove mockito from the tests."", 'commenter': 'morhidi'}]"
420,helm/flink-kubernetes-operator/templates/webhook.yaml,"@@ -121,7 +121,7 @@ metadata:
     cert-manager.io/inject-ca-from: {{ .Release.Namespace }}/flink-operator-serving-cert
   name: flink-operator-{{ .Release.Namespace }}-webhook-configuration
 webhooks:
-  - name: flinkoperator.flink.apache.org
+  - name: mutationwebhook.flink.apache.org","[{'comment': 'it has been changed', 'commenter': 'tagarr'}]"
420,helm/flink-kubernetes-operator/templates/flink-operator.yaml,"@@ -79,7 +79,13 @@ spec:
           {{- end }}
           env:
             - name: OPERATOR_NAMESPACE
-              value: {{ .Release.Namespace }}
+              valueFrom:
+                fieldRef:
+                  fieldPath: metadata.namespace
+            - name: WATCH_NAMESPACES
+              valueFrom:
+                fieldRef:
+                  fieldPath: metadata.annotations['olm.targetNamespaces']","[{'comment': ""We should try to generalise this somehow. Having `WATCH_NAMESPACES` as overrides are ok, ingesting them from the 'olm.targetNamespaces' is not."", 'commenter': 'morhidi'}, {'comment': 'I can remove this and work out how to inject it into the csv if you prefer ?', 'commenter': 'tagarr'}, {'comment': ""I don't mind if there's no Helm support for this, if you can injest it using olm."", 'commenter': 'morhidi'}, {'comment': ""Can we remove this part of the logic? \r\n\r\n```\r\n- name: WATCH_NAMESPACES\r\n               valueFrom:\r\n                 fieldRef:\r\n                   fieldPath: metadata.annotations['olm.targetNamespaces']\r\n```"", 'commenter': 'morhidi'}, {'comment': 'done', 'commenter': 'tagarr'}]"
420,helm/flink-kubernetes-operator/templates/flink-operator.yaml,"@@ -79,7 +79,9 @@ spec:
           {{- end }}
           env:
             - name: OPERATOR_NAMESPACE","[{'comment': ""Sorry, I forgot to submit this comment, I'm trying to understand why we need to change this part from:\r\n```\r\n- name: OPERATOR_NAMESPACE\r\n               value: {{ .Release.Namespace }}\r\n```\r\n\r\nto \r\n```\r\n- name: OPERATOR_NAMESPACE\r\n               valueFrom:\r\n                 fieldRef:\r\n                   fieldPath: metadata.namespace\r\n```\r\n\r\nIt doesn't seem to be related to the watch namespaces, otherwise the changes are good.\r\n"", 'commenter': 'morhidi'}, {'comment': '@jbusche @tagarr ??', 'commenter': 'morhidi'}, {'comment': ""So it is a more consistent way of defining the operator namespace and works for all install methods. That's why I made the change."", 'commenter': 'tagarr'}]"
425,docs/content/docs/custom-resource/reference.md,"@@ -67,7 +67,7 @@ This page serves as a full reference for FlinkDeployment custom resource definit
 | Parameter | Type | Docs |
 | ----------| ---- | ---- |
 | job | org.apache.flink.kubernetes.operator.api.spec.JobSpec | Job specification for application deployments/session job. Null for session clusters. |
-| restartNonce | java.lang.Long | Nonce used to manually trigger restart for the cluster/session job. In order to trigger  restart, change the number to anything other than the current value. |
+| restartNonce | java.lang.Long | Nonce used to manually trigger restart for the cluster/session job. In order to trigger restart, change the number to anything other than the current value. |","[{'comment': 'The reference page is generated based on JavaDocs, you need to fix it there, you cannot edit this page directly', 'commenter': 'gyfora'}, {'comment': '> The reference page is generated based on JavaDocs, you need to fix it there, you cannot edit this page directly\r\n\r\nOkay, Thanks for your review ,I got it, and will update it.', 'commenter': 'yangjf2019'}, {'comment': '@gyfora It is a space caused by a line break in the doc. Is there any solution for this? Thank you!', 'commenter': 'yangjf2019'}, {'comment': 'The solution in that case would be improving the doc generator to automatically remove double spaces.', 'commenter': 'gyfora'}, {'comment': 'Okay ,I got it.', 'commenter': 'yangjf2019'}, {'comment': '@gyfora Please take a look again, thanks!', 'commenter': 'yangjf2019'}]"
425,flink-kubernetes-operator-api/pom.xml,"@@ -276,6 +276,14 @@ under the License.
                     </execution>
                 </executions>
             </plugin>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-compiler-plugin</artifactId>
+                <configuration>
+                    <source>11</source>
+                    <target>11</target>
+                </configuration>
+            </plugin>","[{'comment': ""I don't understand why we make this change here."", 'commenter': 'gyfora'}, {'comment': 'I added this code as prompted by the IDE, it may not be required and has been removed.', 'commenter': 'yangjf2019'}, {'comment': 'Executing the mvn command locally verifies that it passes.\r\n\r\n`mvn clean install -Pgenerate-docs`\r\n\r\n<img width=""1766"" alt=""image"" src=""https://user-images.githubusercontent.com/54518670/200539642-305f3458-950f-4f88-be5b-9e976fa4377b.png"">\r\n<img width=""1191"" alt=""image"" src=""https://user-images.githubusercontent.com/54518670/200539773-be41dbbc-507c-4a20-a5a8-7e2cb944299e.png"">\r\n', 'commenter': 'yangjf2019'}, {'comment': 'what if you do git status after running the build?\r\nI understand that it passes but it should not leave any new changes\r\n', 'commenter': 'gyfora'}, {'comment': 'The CI checks that after running the doc generation there are no new changes in git, this is the check that fails', 'commenter': 'gyfora'}, {'comment': 'Or, I turn off the PR and resubmit it again.', 'commenter': 'yangjf2019'}, {'comment': '> there should not be any more changed files after running\r\n> \r\n> ```\r\n> mvn clean install -Pgenerate-docs\r\n> ```\r\n\r\nThis file has been changed.\r\n\r\n`docs/content/docs/custom-resource/reference.md`', 'commenter': 'yangjf2019'}]"
438,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/AbstractFlinkService.java,"@@ -362,11 +362,27 @@ public void cancelSessionJob(
             FlinkSessionJob sessionJob, UpgradeMode upgradeMode, Configuration conf)
             throws Exception {
 
+        // Not allowing the jobs which are already in the completed state.
+        String[] jobStateCannotBeCancelled =
+                new String[] {
+                    JobStatus.CANCELLING.name(),
+                    JobStatus.CANCELED.name(),
+                    JobStatus.FAILING.name(),
+                    JobStatus.FAILED.name(),
+                    JobStatus.FINISHED.name()
+                };
+
         var jobStatus = sessionJob.getStatus().getJobStatus();
+
+        if (Arrays.stream(jobStateCannotBeCancelled).anyMatch(jobStatus.getState()::contains)) {
+            throw new RuntimeException(""Job is Already in "" + jobStatus.getState() + "" state"");
+        }
+","[{'comment': 'This logic is not correct. We should not throw an error, here that would only make the current situation worse.\r\n\r\nplease look at the logic for application clusters:\r\n```\r\nif (ReconciliationUtils.isJobRunning(deploymentStatus)) {\r\n                            LOG.info(""Suspending job with savepoint."");\r\n                            String savepoint =\r\n                                    clusterClient\r\n                                            .stopWithSavepoint(\r\n                                                    Preconditions.checkNotNull(jobId),\r\n                                                    false,\r\n                                                    savepointDirectory,\r\n                                                    conf.get(FLINK_VERSION)\r\n                                                                    .isNewerVersionThan(\r\n                                                                            FlinkVersion.v1_14)\r\n                                                            ? savepointFormatType\r\n                                                            : null)\r\n                                            .get(timeout, TimeUnit.SECONDS);\r\n                            savepointOpt = Optional.of(savepoint);\r\n                            LOG.info(""Job successfully suspended with savepoint {}."", savepoint);\r\n                        } else if (ReconciliationUtils.isJobInTerminalState(deploymentStatus)) {\r\n                            LOG.info(\r\n                                    ""Job is already in terminal state skipping cancel-with-savepoint operation."");\r\n                        } else {\r\n                            throw new RuntimeException(\r\n                                    ""Unexpected non-terminal status: "" + deploymentStatus);\r\n                        }\r\n```\r\nif the job is already in terminal state we **do not** throw error, because the job is already in the state where we want to get to.', 'commenter': 'gyfora'}, {'comment': 'Got your point. I thought of another way to handle it. I will fix this.', 'commenter': 'rgsriram'}]"
438,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/AbstractFlinkService.java,"@@ -391,22 +395,30 @@ public void cancelSessionJob(
                             conf.get(ExecutionCheckpointingOptions.CHECKPOINTING_TIMEOUT)
                                     .getSeconds();
                     try {
-                        String savepoint =
-                                clusterClient
-                                        .stopWithSavepoint(
-                                                jobId,
-                                                false,
-                                                savepointDirectory,
-                                                conf.get(FLINK_VERSION)
-                                                                .isNewerVersionThan(
-                                                                        FlinkVersion.v1_14)
-                                                        ? conf.get(
-                                                                KubernetesOperatorConfigOptions
-                                                                        .OPERATOR_SAVEPOINT_FORMAT_TYPE)
-                                                        : null)
-                                        .get(timeout, TimeUnit.SECONDS);
-                        savepointOpt = Optional.of(savepoint);
-                        LOG.info(""Job successfully suspended with savepoint {}."", savepoint);
+                        if (ReconciliationUtils.isJobRunning(sessionJobStatus)) {
+                            String savepoint =
+                                    clusterClient
+                                            .stopWithSavepoint(
+                                                    jobId,
+                                                    false,
+                                                    savepointDirectory,
+                                                    conf.get(FLINK_VERSION)
+                                                                    .isNewerVersionThan(
+                                                                            FlinkVersion.v1_14)
+                                                            ? conf.get(
+                                                                    KubernetesOperatorConfigOptions
+                                                                            .OPERATOR_SAVEPOINT_FORMAT_TYPE)
+                                                            : null)
+                                            .get(timeout, TimeUnit.SECONDS);
+                            savepointOpt = Optional.of(savepoint);
+                            LOG.info(""Job successfully suspended with savepoint {}."", savepoint);
+                        } else if (ReconciliationUtils.isJobInTerminalState(sessionJobStatus)) {
+                            LOG.info(
+                                    ""Job is already in terminal state skipping cancel-with-savepoint operation."");
+                        } else {
+                            throw new RuntimeException(
+                                    ""Unexpected non-terminal status: "" + sessionJobStatus);
+                        }","[{'comment': 'any reason why the logging and exception is not present for the STATELESS branch?', 'commenter': 'gyfora'}, {'comment': 'I think it could be moved simply before the switch statement to apply uniformly', 'commenter': 'gyfora'}]"
438,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/AbstractFlinkService.java,"@@ -362,11 +362,21 @@ public void cancelSessionJob(
             FlinkSessionJob sessionJob, UpgradeMode upgradeMode, Configuration conf)
             throws Exception {
 
-        var jobStatus = sessionJob.getStatus().getJobStatus();
+        var sessionJobStatus = sessionJob.getStatus();
+        var jobStatus = sessionJobStatus.getJobStatus();
         var jobIdString = jobStatus.getJobId();
         Preconditions.checkNotNull(jobIdString, ""The job to be suspend should not be null"");
         var jobId = JobID.fromHexString(jobIdString);
         Optional<String> savepointOpt = Optional.empty();
+
+        if (ReconciliationUtils.isJobInTerminalState(sessionJobStatus)) {
+            LOG.info(""Job is already in terminal state."");","[{'comment': 'Something is wrong with your tests because this code is clearly not correct. after logging you can either `return` or put the entire try block in the `else` branch.\r\n\r\nIn any case it would be good to update the test to actually test for this otherwise we might introduce a regression easily later.', 'commenter': 'gyfora'}, {'comment': ""Yes, you're right. Sorry, I accidentally pushed before that. I noticed it. I ran test cases now. Fixing the testcase."", 'commenter': 'rgsriram'}, {'comment': 'I found the following behaviour. When the job is in Terminal State (COMPLETED/FINISHED/FAILED). There is no exception. It goes to a terminal state and continues to cancel the job. Ideally, which is not expected. I checked behaviour with both STATELESS and STATEFUL jobs. I attached only the STATELESS job behaviour in the below image for reference. BTW, I used print statements to find both job ids are equal.  I have added those as `assert` checks in the test case.\r\n\r\n<img width=""1661"" alt=""Screenshot 2022-11-16 at 10 09 08 AM"" src=""https://user-images.githubusercontent.com/7067975/202087450-061f087a-5a21-498e-a2c7-38e24bc108ad.png"">\r\n\r\nComing to the actual error reported in the ticket ""Job not found"". It happens only in the case when the job is not present in the session jobs list. I have tested using the following test case. \r\n\r\n```\r\n@Test\r\npublic void testCancelTheDeletedSessionJob() throws Exception {\r\n    FlinkSessionJob sessionJob = TestUtils.buildSessionJob();\r\n    var readyContext = TestUtils.createContextWithReadyFlinkDeployment();\r\n    reconciler.reconcile(sessionJob, readyContext);\r\n    assertEquals(1, flinkService.listJobs().size());\r\n    verifyAndSetRunningJobsToStatus(\r\n            sessionJob,\r\n            JobState.RUNNING,\r\n            org.apache.flink.api.common.JobStatus.RECONCILING.name(),\r\n            null,\r\n            flinkService.listJobs());\r\n\r\n    var statelessSessionJob = ReconciliationUtils.clone(sessionJob);\r\n    var jobConfig = flinkService.listJobs().get(0).f2;\r\n\r\n    // JobID must be equal.\r\n    assertEquals(\r\n            statelessSessionJob.getStatus().getJobStatus().getJobId(),\r\n            flinkService.listJobs().get(0).f1.getJobId().toHexString());\r\n\r\n    statelessSessionJob.getSpec().getJob().setUpgradeMode(UpgradeMode.STATELESS);\r\n    statelessSessionJob.getSpec().getJob().setParallelism(2);\r\n    // job suspended first\r\n    reconciler.reconcile(statelessSessionJob, readyContext);\r\n    assertEquals(\r\n            org.apache.flink.api.common.JobStatus.FINISHED,\r\n            flinkService.listJobs().get(0).f1.getJobState());\r\n    verifyJobState(statelessSessionJob, JobState.SUSPENDED, ""FINISHED"");\r\n\r\n    // Test - cancel the deleted session job\r\n    flinkService.listJobs().remove(0);\r\n\r\n    flinkService.cancelSessionJob(\r\n                            statelessSessionJob, UpgradeMode.STATELESS, jobConfig);\r\n}\r\n```\r\n\r\nI got the below exception. I have not included this test case. I felt it is not required.\r\n\r\n \r\n<img width=""1657"" alt=""Screenshot 2022-11-16 at 9 48 17 AM"" src=""https://user-images.githubusercontent.com/7067975/202088349-e7831c43-fffa-4e14-8a43-ee1c90921418.png"">\r\n\r\nIMO - The exception is correct because the job itself is not present. But I think we can find the job if it exists before cancelling it. If the job does not exist we can throw an error, it cuts the actual execution logic.\r\n\r\n\r\n', 'commenter': 'rgsriram'}, {'comment': ""Please beware that what you wrote is based on the `TestingFlinkService` which is basically a mock that intents to emulate the Flink client behaviour. \r\n\r\nThe fact that you don't get certain errors that are explained the ticket does not mean that it works correctly only that the `TestingFlinkService` implementation is incorrect. It is best to test it manually in minikube to reproduce the problem, modify the testing service to represent the correct behaviour then write some tests and assert that you can reproduce the problem first.\r\n\r\nThen once you fix it you will know that it worked"", 'commenter': 'gyfora'}, {'comment': ""I have validated the error through the minikube. Following are my observations.\r\n1. Suspending the job using Custom Resource is working fine. (Any mode)\r\n2. If we cancel the job via flink cli, we are getting the error mentioned in the ticket. That is coming from the flink code. one example,\r\n\r\n- [Dispatcher.java#stopWithSavepointAndGetLocation](https://github.com/apache/flink/blob/5d66e82915eace9342c175163b17f610bfbf7fa4/flink-runtime/src/main/java/org/apache/flink/runtime/dispatcher/Dispatcher.java#L976)\r\n- [FlinkJobNotExceptionNotFound](https://github.com/apache/flink/blob/99c2a415e9eeefafacf70762b6f54070f7911ceb/flink-runtime/src/main/java/org/apache/flink/runtime/messages/FlinkJobNotFoundException.java#L29).\r\n\r\nCorrect me If I am wrong, this is something we can't control via operator code right?. We need to modify the cli and flink session cluster code in flink.\r\n"", 'commenter': 'rgsriram'}, {'comment': ""If the job is cancelled via the CLI why isn't it observed in a terminal state? I don't really think that should give us any error if the operator works correctly."", 'commenter': 'gyfora'}, {'comment': ""Yes, You're right. \r\n\r\nLet me put it clearly, there are the following scenarios.\r\n1. Cancel the same job via cli more than 1 time-> Throws an error which is out of the scope of the operator. (IMO)\r\n2. Cancelled the job via cli and suspended the same job via operator -> No error.\r\n\r\n"", 'commenter': 'rgsriram'}, {'comment': 'Ah ok, then I misunderstood what you wrote earlier a little :)\r\n\r\nyea that sounds good!', 'commenter': 'gyfora'}]"
438,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/AbstractFlinkService.java,"@@ -362,11 +362,22 @@ public void cancelSessionJob(
             FlinkSessionJob sessionJob, UpgradeMode upgradeMode, Configuration conf)
             throws Exception {
 
-        var jobStatus = sessionJob.getStatus().getJobStatus();
+        var sessionJobStatus = sessionJob.getStatus();
+        var jobStatus = sessionJobStatus.getJobStatus();
         var jobIdString = jobStatus.getJobId();
         Preconditions.checkNotNull(jobIdString, ""The job to be suspend should not be null"");
         var jobId = JobID.fromHexString(jobIdString);
         Optional<String> savepointOpt = Optional.empty();
+
+        if (ReconciliationUtils.isJobInTerminalState(sessionJobStatus)) {
+            LOG.info(""Job is already in terminal state. JobID {}"", jobId.toHexString());
+            return;
+        } else if (!ReconciliationUtils.isJobRunning(sessionJobStatus)) {","[{'comment': 'I think we donâ€™t need this branch here, this would prevent us from suspending failing jobs', 'commenter': 'gyfora'}, {'comment': 'I have re-written the logic and added test cases accordingly. Please check.', 'commenter': 'rgsriram'}, {'comment': '@gaborgsomogyi - Please kindly check.', 'commenter': 'rgsriram'}, {'comment': '@gyfora - The build pipeline is failing with the below error. At this place: https://github.com/apache/flink-kubernetes-operator/actions/runs/3574448062/jobs/6009735670. \r\n\r\nCould you please help me here? Not sure if re-triggering solves it. \r\n\r\n```\r\nflinkdeployment.flink.apache.org ""session-cluster-1"" deleted\r\nflinksessionjob.flink.apache.org ""flink-example-statemachine"" deleted\r\npersistentvolumeclaim ""session-cluster-1-pvc"" deleted\r\ningressclass.networking.k8s.io ""nginx"" deleted\r\nNo resources found\r\nError: Process completed with exit code 1.\r\n```', 'commenter': 'rgsriram'}, {'comment': 'above issue is resolved now.', 'commenter': 'rgsriram'}]"
438,flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/reconciler/sessionjob/SessionJobReconcilerTest.java,"@@ -232,6 +232,218 @@ public void testStatelessUpgrade() throws Exception {
                 flinkService.listJobs());
     }
 
+    @Test","[{'comment': 'I think this can be compacted such a way but that said in my other comment these are passing all the time no matter if your fix is there or not:\r\n```\r\n    @ParameterizedTest\r\n    @EnumSource(org.apache.flink.api.common.JobStatus.class)\r\n    public void testCancelStatelessSessionJob(org.apache.flink.api.common.JobStatus fromJobStatus) throws Exception {\r\n        FlinkSessionJob sessionJob = TestUtils.buildSessionJob();\r\n\r\n        var readyContext = TestUtils.createContextWithReadyFlinkDeployment();\r\n\r\n        reconciler.reconcile(sessionJob, readyContext);\r\n        assertEquals(1, flinkService.listJobs().size());\r\n        verifyAndSetRunningJobsToStatus(\r\n                sessionJob,\r\n                JobState.RUNNING,\r\n                org.apache.flink.api.common.JobStatus.RECONCILING.name(),\r\n                null,\r\n                flinkService.listJobs());\r\n\r\n        var statelessSessionJob = ReconciliationUtils.clone(sessionJob);\r\n        var jobConfig = flinkService.listJobs().get(0).f2;\r\n\r\n        // JobID must be equal.\r\n        assertEquals(\r\n                statelessSessionJob.getStatus().getJobStatus().getJobId(),\r\n                flinkService.listJobs().get(0).f1.getJobId().toHexString());\r\n\r\n        statelessSessionJob.getSpec().getJob().setUpgradeMode(UpgradeMode.STATELESS);\r\n        statelessSessionJob.getSpec().getJob().setParallelism(2);\r\n        reconciler.reconcile(statelessSessionJob, readyContext);\r\n\r\n        statelessSessionJob\r\n                .getStatus()\r\n                .getJobStatus()\r\n                .setState(fromJobStatus.name());\r\n        flinkService.cancelSessionJob(statelessSessionJob, UpgradeMode.STATELESS, jobConfig);\r\n        assertEquals(\r\n                org.apache.flink.api.common.JobStatus.FINISHED,\r\n                flinkService.listJobs().get(0).f1.getJobState());\r\n        verifyJobState(statelessSessionJob, JobState.SUSPENDED, ""FINISHED"");\r\n    }\r\n```\r\n', 'commenter': 'gaborgsomogyi'}]"
438,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/AbstractFlinkService.java,"@@ -362,25 +362,31 @@ public void cancelSessionJob(
             FlinkSessionJob sessionJob, UpgradeMode upgradeMode, Configuration conf)
             throws Exception {
 
-        var jobStatus = sessionJob.getStatus().getJobStatus();
+        var sessionJobStatus = sessionJob.getStatus();
+        var jobStatus = sessionJobStatus.getJobStatus();
         var jobIdString = jobStatus.getJobId();
         Preconditions.checkNotNull(jobIdString, ""The job to be suspend should not be null"");
         var jobId = JobID.fromHexString(jobIdString);
         Optional<String> savepointOpt = Optional.empty();
+
+        LOG.debug(""Current Job State, {}"", jobStatus.getState());
+
         try (ClusterClient<String> clusterClient = getClusterClient(conf)) {
             final String clusterId = clusterClient.getClusterId();
             switch (upgradeMode) {
                 case STATELESS:
-                    LOG.info(""Cancelling job."");
-                    clusterClient
-                            .cancel(jobId)
-                            .get(
-                                    configManager
-                                            .getOperatorConfiguration()
-                                            .getFlinkCancelJobTimeout()
-                                            .toSeconds(),
-                                    TimeUnit.SECONDS);
-                    LOG.info(""Job successfully cancelled."");
+                    if (ReconciliationUtils.isJobRunning(sessionJobStatus)) {","[{'comment': ""I've just removed this if condition and the tests are still passing. Something is wrong."", 'commenter': 'gaborgsomogyi'}]"
438,flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/reconciler/sessionjob/SessionJobReconcilerTest.java,"@@ -232,6 +232,218 @@ public void testStatelessUpgrade() throws Exception {
                 flinkService.listJobs());
     }
 
+    @Test
+    public void testCancelStatelessSessionJob() throws Exception {
+        FlinkSessionJob sessionJob = TestUtils.buildSessionJob();
+
+        var readyContext = TestUtils.createContextWithReadyFlinkDeployment();
+
+        reconciler.reconcile(sessionJob, readyContext);
+        assertEquals(1, flinkService.listJobs().size());
+        verifyAndSetRunningJobsToStatus(
+                sessionJob,
+                JobState.RUNNING,
+                org.apache.flink.api.common.JobStatus.RECONCILING.name(),
+                null,
+                flinkService.listJobs());
+
+        var statelessSessionJob = ReconciliationUtils.clone(sessionJob);
+        var jobConfig = flinkService.listJobs().get(0).f2;
+
+        // JobID must be equal.
+        assertEquals(
+                statelessSessionJob.getStatus().getJobStatus().getJobId(),
+                flinkService.listJobs().get(0).f1.getJobId().toHexString());
+
+        // Case RUNNING -> FINISHED
+        statelessSessionJob.getSpec().getJob().setUpgradeMode(UpgradeMode.STATELESS);
+        statelessSessionJob.getSpec().getJob().setParallelism(2);
+        flinkService.cancelSessionJob(statelessSessionJob, UpgradeMode.STATELESS, jobConfig);
+        reconciler.reconcile(statelessSessionJob, readyContext);
+        assertEquals(
+                org.apache.flink.api.common.JobStatus.FINISHED,
+                flinkService.listJobs().get(0).f1.getJobState());
+        verifyJobState(statelessSessionJob, JobState.SUSPENDED, ""FINISHED"");
+
+        // Case FAILING -> FINISHED
+        statelessSessionJob
+                .getStatus()
+                .getJobStatus()
+                .setState(org.apache.flink.api.common.JobStatus.FAILING.name());
+        flinkService.cancelSessionJob(statelessSessionJob, UpgradeMode.STATELESS, jobConfig);
+        assertEquals(
+                org.apache.flink.api.common.JobStatus.FINISHED,
+                flinkService.listJobs().get(0).f1.getJobState());
+        verifyJobState(statelessSessionJob, JobState.SUSPENDED, ""FINISHED"");
+
+        // Case CANCELLING -> FINISHED
+        statelessSessionJob
+                .getStatus()
+                .getJobStatus()
+                .setState(org.apache.flink.api.common.JobStatus.CANCELLING.name());
+        flinkService.cancelSessionJob(statelessSessionJob, UpgradeMode.STATELESS, jobConfig);
+        verifyJobState(statelessSessionJob, JobState.SUSPENDED, ""FINISHED"");
+
+        // Case RESTARTING -> FINISHED
+        statelessSessionJob
+                .getStatus()
+                .getJobStatus()
+                .setState(org.apache.flink.api.common.JobStatus.RESTARTING.name());
+        flinkService.cancelSessionJob(statelessSessionJob, UpgradeMode.STATELESS, jobConfig);
+        verifyJobState(statelessSessionJob, JobState.SUSPENDED, ""FINISHED"");
+
+        // Case FAILED -> FINISHED
+        statelessSessionJob
+                .getStatus()
+                .getJobStatus()
+                .setState(org.apache.flink.api.common.JobStatus.FAILED.name());
+        flinkService.cancelSessionJob(statelessSessionJob, UpgradeMode.STATELESS, jobConfig);
+        verifyJobState(statelessSessionJob, JobState.SUSPENDED, ""FINISHED"");
+
+        // Case FINISHED -> FINISHED
+        statelessSessionJob
+                .getStatus()
+                .getJobStatus()
+                .setState(org.apache.flink.api.common.JobStatus.FINISHED.name());
+        flinkService.cancelSessionJob(statelessSessionJob, UpgradeMode.STATELESS, jobConfig);
+        verifyJobState(statelessSessionJob, JobState.SUSPENDED, ""FINISHED"");
+
+        // Case CANCELLED -> FINISHED
+        statelessSessionJob
+                .getStatus()
+                .getJobStatus()
+                .setState(org.apache.flink.api.common.JobStatus.CANCELED.name());
+        flinkService.cancelSessionJob(statelessSessionJob, UpgradeMode.STATELESS, jobConfig);
+        verifyJobState(statelessSessionJob, JobState.SUSPENDED, ""FINISHED"");
+    }
+
+    @Test
+    public void testCancelStatefulSessionJob() throws Exception {","[{'comment': ""* Here the same, test parts which are not expecting an exception are passing w/o the fix.\r\n* Please split the test per source state and don't create 10+ assertions within a single test.\r\nEither one can do parameterized test like I've suggested before or one can extract the common functionality into a function. An example can be found [here](https://github.com/apache/flink-kubernetes-operator/blob/80e5f3f4add50355de6964c1466ba558d46bc79d/flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/reconciler/deployment/ApplicationReconcilerUpgradeModeTest.java#L86-L102)."", 'commenter': 'gaborgsomogyi'}, {'comment': 'Hi Gabor, \r\n\r\nThis is expected for a stateless session job like in the application mode. In application mode, it will throw an error if it can not gracefully shut down the stateless job. By any chance did you mean that?  I also thought about it but it wasn\'t there initially. I think it should be added.\r\n\r\n```\r\n          case STATELESS:\r\n                    if (ReconciliationUtils.isJobRunning(deployment.getStatus())) {\r\n                        LOG.info(""Job is running, cancelling job."");\r\n                        try {\r\n                            clusterClient\r\n                                    .cancel(Preconditions.checkNotNull(jobId))\r\n                                    .get(\r\n                                            configManager\r\n                                                    .getOperatorConfiguration()\r\n                                                    .getFlinkCancelJobTimeout()\r\n                                                    .toSeconds(),\r\n                                            TimeUnit.SECONDS);\r\n                            LOG.info(""Job successfully cancelled."");\r\n                        } catch (Exception e) {\r\n                            LOG.error(""Could not shut down cluster gracefully, deleting..."", e);\r\n                        }\r\n                    }\r\n                    deleteClusterDeployment(deployment.getMetadata(), deploymentStatus, true);\r\n                    break;\r\n```\r\n\r\nStateful job is failing as expected.\r\n\r\n<img width=""1668"" alt=""Screenshot 2022-11-30 at 6 39 54 PM"" src=""https://user-images.githubusercontent.com/7067975/204805994-0bebe2fd-daba-4c2f-a922-c716f50a98a3.png"">\r\n\r\nAnd I agree with you on splitting the test cases into multiple small functions.\r\n\r\n```\r\nRUNNING JOB -> Savepoint Cancel -> OK\r\nTERMINAL JOB -> Savepoint Cancel -> OK\r\nNOT RUNNING or TERMINAL -> Savepoint Cancel -> EXCEPTION\r\n\r\nRUNNING JOB -> Stateless Cancel -> OK\r\nTERMINAL JOB -> Stateless Cancel -> OK\r\nNOT RUNNING or TERMINAL -> Stateless Cancel -> OK\r\n```\r\n Above is the core logic for the test cases. Please feel free to correct me if I am wrong in any place.\r\n', 'commenter': 'rgsriram'}, {'comment': 'Thanks for your valuable time in reviewing :)', 'commenter': 'rgsriram'}]"
464,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/controller/FlinkDeploymentController.java,"@@ -139,10 +142,13 @@ public UpdateControl<FlinkDeployment> reconcile(FlinkDeployment flinkApp, Contex
                 configManager.getOperatorConfiguration(), flinkApp, previousDeployment, true);
     }
 
-    private void handleDeploymentFailed(FlinkDeployment flinkApp, DeploymentFailedException dfe) {
+    private void handleDeploymentFailed(
+            FlinkDeployment flinkApp, DeploymentFailedException dfe, boolean updateJobStatus) {
         LOG.error(""Flink Deployment failed"", dfe);
-        flinkApp.getStatus().setJobManagerDeploymentStatus(JobManagerDeploymentStatus.ERROR);
-        flinkApp.getStatus().getJobStatus().setState(JobStatus.RECONCILING.name());
+        if (updateJobStatus) {","[{'comment': 'These should be two separate methods, the update status flag does not have any well defined meaning I feel', 'commenter': 'gyfora'}]"
464,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/exception/RecoverableDeploymentFailureException.java,"@@ -0,0 +1,25 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.exception;
+
+/** Exception to signal non-terminal deployment failure. */
+public class RecoverableDeploymentFailureException extends DeploymentFailedException {","[{'comment': 'Letâ€™s not extend DeploymentFailedException , the deployment does not fail in these cases (the jobmanager deployment I mean)\r\n\r\nwe could simply call FlinkResourceException or something more generic ', 'commenter': 'gyfora'}]"
464,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/ApplicationReconciler.java,"@@ -125,7 +125,7 @@ protected Optional<UpgradeMode> getAvailableUpgradeMode(
 
         if (status.getJobManagerDeploymentStatus() == JobManagerDeploymentStatus.MISSING
                 || status.getJobManagerDeploymentStatus() == JobManagerDeploymentStatus.ERROR) {
-            throw new DeploymentFailedException(
+            throw new RecoveryFailureException(
                     ""JobManager deployment is missing and HA data is not available to make stateful upgrades. ""
                             + ""It is possible that the job has finished or terminally failed, or the configmaps have been deleted. ""
                             + ""Manual restore required."",","[{'comment': 'How true is this? `Manual restore required.`', 'commenter': 'gaborgsomogyi'}, {'comment': 'This almost certainly means that the operator cannot restore this anymore. The user needs to delete the CR and recover using initialSavepoontPath at this stage', 'commenter': 'gyfora'}, {'comment': 'There could be some error cases where we still recover I guess , but we would have to check. In that case that would recover automatically. But missing I think is final in this case ', 'commenter': 'gyfora'}]"
481,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/SessionReconciler.java,"@@ -96,6 +96,19 @@ protected void reconcileSpecChange(
             Configuration deployConfig,
             DiffType type)
             throws Exception {
+        if (type == DiffType.SCALE) {","[{'comment': 'If this logic is the same as the one in the AbstracJobReconciler we should move it instead to the shared parent class (AbstractFlinkResourceRecnciler)\r\n\r\n', 'commenter': 'gyfora'}]"
481,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/StandaloneFlinkService.java,"@@ -171,11 +170,6 @@ protected void deleteClusterInternal(ObjectMeta meta, boolean deleteHaConfigmaps
 
     @Override
     public boolean scale(ObjectMeta meta, JobSpec jobSpec, Configuration conf) {
-        if (conf.get(JobManagerOptions.SCHEDULER_MODE) == null) {","[{'comment': 'This check was supposed to check whether reactive scheduler is enabled for applications, we canâ€™t simply remove it.\r\n\r\nMaybe we are lacking test coverage for thisâ€¦\r\n\r\nIn any case it checks not null which already looks incorrect. @morhidi you worked on this in the past what do you think?', 'commenter': 'gyfora'}, {'comment': 'Even if there is no scheduler mode enabled , i.e if the scheduler mode is null, still we should be able to scale the cluster i.e even if the adaptive or reactive mode is not enabled and only scaling change should allow to only scale. But let me know your thoughts @gyfora , @morhidi ', 'commenter': 'swathic95'}, {'comment': ""In this case, we are calculating the desired replicas as \r\n```\r\nvar desiredReplicas =\r\n                conf.get(StandaloneKubernetesConfigOptionsInternal.KUBERNETES_TASKMANAGER_REPLICAS);\r\n```\r\nWhich is actually ```kubernetes.internal.taskmanager.replicas``` which is created whenever there is a CR update while building the task manager. Here, this value would be equal to the following:\r\n1) If taskmanager replicas are present, KUBERNETES_TASKMANAGER_REPLICAS = task_manager_replicas configured. [Code Reference to 1](https://github.com/apache/flink-kubernetes-operator/blob/dc173cbccd5f9a90fe573ce01790684253dcc0a9/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/FlinkConfigBuilder.java#L233) \r\n2) Else, if a job is running, and KUBERNETES_TASKMANAGER_REPLICAS is not yet set, only then it calculates the parallelism = task_slots * task_manager_replicas or parallelism configured ( if no TM replicas ) and [Code reference to 2 ](https://github.com/apache/flink-kubernetes-operator/blob/dc173cbccd5f9a90fe573ce01790684253dcc0a9/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/FlinkConfigBuilder.java#L247)\r\n```KUBERNETES_TASKMANAGER_REPLICAS =  (parallelism + taskSlots - 1) / taskSlots;```\r\n[Internal task manager calc for 2 ]( https://github.com/apache/flink-kubernetes-operator/blob/dc173cbccd5f9a90fe573ce01790684253dcc0a9/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/FlinkUtils.java#L174)\r\n\r\nWe'll always fall to 1st category as the user has updated the task manager replicas manually. \r\n\r\n\r\n@gyfora , @morhidi  if you think, we have other scenarios which might affect it,let me know, I'll create another diffType ( SCALE_REPLICAS ) which will be used only by task manager replicas change and add a separate logic to scale based on the desired TM replicas and not ```kubernetes.internal.taskmanager.replicas```\r\n"", 'commenter': 'swathic95'}, {'comment': ""That's fine but what I am saying is that the StandaloneFlinkService#scale method is used by both Application and Session cluster deployments. You cannot simply scale an application cluster by adding more replicas unless reactive scheduler is configured for the job. \r\n\r\nSo the check that you removed is neeeded for the application logic to work. \r\nAnd it should be improved to:\r\n```\r\nif (conf.get(JobManagerOptions.SCHEDULER_MODE) == REACTIVE) {\r\n            return false;\r\n        }\r\n```\r\n"", 'commenter': 'gyfora'}, {'comment': 'As you can see your change actually breaks the test case for this:\r\n```\r\nStandaloneFlinkServiceTest.testReactiveScale:180 expected: <false> but was: <true>\r\n```\r\n\r\nYou need to add new tests in this class to make sure this works well for sessions too and still preserve the existing behaviour for Application deployments', 'commenter': 'gyfora'}, {'comment': 'Understood your concern, will make changes to make it work for both session and application mode', 'commenter': 'swathic95'}, {'comment': 'Addressed the comment where it scales in appmode only if parallelism changes, and if there is a replica change when the reactive scheduler is not enabled, the scaling is not successful. Added test cases as well in both session and app mode for different scenarios. \r\nCurrently scaling can happen due to the following in standalone clusters :\r\n1) Session mode - replica change\r\n2) App mode enabled with scheduler configured', 'commenter': 'swathic95'}]"
481,flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/service/StandaloneFlinkServiceTest.java,"@@ -177,11 +177,123 @@ public void testReactiveScale() throws Exception {
                 .getSpec()
                 .getFlinkConfiguration()
                 .remove(JobManagerOptions.SCHEDULER_MODE.key());
-        assertFalse(
+        assertTrue(","[{'comment': 'Please do not change the expected value here. Your change broke the test, you should restore the behaviour not adapt the test to the broken change :) ', 'commenter': 'gyfora'}]"
481,flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/service/StandaloneFlinkServiceTest.java,"@@ -177,11 +177,123 @@ public void testReactiveScale() throws Exception {
                 .getSpec()
                 .getFlinkConfiguration()
                 .remove(JobManagerOptions.SCHEDULER_MODE.key());
-        assertFalse(
+        assertTrue(
+                flinkStandaloneService.scale(
+                        flinkDeployment.getMetadata(),
+                        flinkDeployment.getSpec().getJob(),
+                        buildConfig(flinkDeployment, configuration)));
+    }
+
+    @Test
+    public void testTMReplicaScale() throws Exception {
+        var flinkDeployment = TestUtils.buildApplicationCluster();","[{'comment': 'would be nice to test this with a session cluster resource where `job = null`.\r\nYou could use `TestUtils.buildSessionCluster()` instead\r\n', 'commenter': 'gyfora'}]"
481,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/AbstractFlinkResourceReconciler.java,"@@ -316,6 +320,35 @@ private boolean checkNewSpecAlreadyDeployed(CR resource, Configuration deployCon
         return false;
     }
 
+    /**
+     * Scale the cluster whenever there is a scaling change, based on the task manager replica
+     * update or the parallelism in case of scheduler mode.
+     *
+     * @param cr Resource being reconciled.
+     * @param ctx Reconciliation context.
+     * @param observeConfig Observe configuration.
+     * @param deployConfig Configuration to be deployed.
+     * @param diffType Spec change type.
+     * @throws Exception
+     */
+    private void scaleCluster(
+            CR cr,
+            Context<?> ctx,
+            Configuration observeConfig,
+            Configuration deployConfig,
+            DiffType diffType)
+            throws Exception {
+        boolean scaled =
+                getFlinkService(cr, ctx)
+                        .scale(cr.getMetadata(), cr.getSpec().getJob(), deployConfig);
+        if (scaled) {
+            LOG.info(""Scaling succeeded"");
+            ReconciliationUtils.updateStatusForDeployedSpec(cr, deployConfig);
+            return;
+        }
+        reconcileSpecChange(cr, ctx, observeConfig, deployConfig, diffType);","[{'comment': ""it might be better to return a boolean from this method so we don't need to call `reconcileSpecChange` from two places."", 'commenter': 'gyfora'}]"
481,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/AbstractFlinkResourceReconciler.java,"@@ -316,6 +319,38 @@ private boolean checkNewSpecAlreadyDeployed(CR resource, Configuration deployCon
         return false;
     }
 
+    /**
+     * Scale the cluster whenever there is a scaling change, based on the task manager replica
+     * update or the parallelism in case of scheduler mode.
+     *
+     * @param cr Resource being reconciled.
+     * @param ctx Reconciliation context.
+     * @param observeConfig Observe configuration.
+     * @param deployConfig Configuration to be deployed.
+     * @param diffType Spec change type.
+     * @throws Exception","[{'comment': 'missing `@return` javadoc', 'commenter': 'gyfora'}]"
481,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/StandaloneFlinkService.java,"@@ -171,7 +171,11 @@ protected void deleteClusterInternal(ObjectMeta meta, boolean deleteHaConfigmaps
 
     @Override
     public boolean scale(ObjectMeta meta, JobSpec jobSpec, Configuration conf) {
-        if (conf.get(JobManagerOptions.SCHEDULER_MODE) == null) {
+        if (conf.get(JobManagerOptions.SCHEDULER_MODE) == null
+                && conf.get(StandaloneKubernetesConfigOptionsInternal.CLUSTER_MODE)
+                        .equals(
+                                StandaloneKubernetesConfigOptionsInternal.ClusterMode
+                                        .APPLICATION)) {","[{'comment': ""Instead of checking the ClusterMode, it's simpler to check `jobSpec != null`, and also please explicitly check the `SCHEDULER_MODE != REACTIVE` instead of just checking null"", 'commenter': 'gyfora'}]"
481,flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/service/StandaloneFlinkServiceTest.java,"@@ -184,6 +184,125 @@ public void testReactiveScale() throws Exception {
                         buildConfig(flinkDeployment, configuration)));
     }
 
+    @Test
+    public void testTMReplicaScaleApplication() throws Exception {","[{'comment': 'I think the new tests are good enough and you can simply remove the old unit test. That is covered in the one you added', 'commenter': 'gyfora'}]"
487,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/validation/DefaultValidator.java,"@@ -241,15 +245,35 @@ private Optional<String> validateJobSpec(
     }
 
     private Optional<String> validateJmSpec(JobManagerSpec jmSpec, Map<String, String> confMap) {
+        Configuration conf = Configuration.fromMap(confMap);
+        var jmMemoryDefined =
+                jmSpec != null
+                        && jmSpec.getResource() != null
+                        && !StringUtils.isNullOrWhitespaceOnly(jmSpec.getResource().getMemory());
+        Optional<String> jmMemoryValidation =
+                jmMemoryDefined ? Optional.empty() : validateJmMemoryConfig(conf);
+
         if (jmSpec == null) {
-            return Optional.empty();
+            return jmMemoryValidation;
         }
 
         return firstPresent(
+                jmMemoryValidation,
                 validateResources(""JobManager"", jmSpec.getResource()),
                 validateJmReplicas(jmSpec.getReplicas(), confMap));
     }
 
+    private Optional<String> validateJmMemoryConfig(Configuration conf) {
+        try {
+            JobManagerProcessUtils.processSpecFromConfigWithNewOptionToInterpretLegacyHeap(","[{'comment': 'Why do we use this method vs using `processSpecFromConfig`?', 'commenter': 'gyfora'}, {'comment': '- `processSpecFromConfigWithNewOptionToInterpretLegacyHeap` internally calls `processSpecFromConfig` - [JobManagerProcessUtils.java#L73-L74](https://github.com/apache/flink/blob/75a92efd7b35501698e5de253e5231d680830c16/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/JobManagerProcessUtils.java#L73-L74)\r\n- Also, `processSpecFromConfig` is not `public`, but package private - [JobManagerProcessUtils.java#L82](https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/JobManagerProcessUtils.java#L82)', 'commenter': 'nowke'}]"
487,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/validation/DefaultValidator.java,"@@ -241,15 +245,35 @@ private Optional<String> validateJobSpec(
     }
 
     private Optional<String> validateJmSpec(JobManagerSpec jmSpec, Map<String, String> confMap) {
+        Configuration conf = Configuration.fromMap(confMap);
+        var jmMemoryDefined =
+                jmSpec != null
+                        && jmSpec.getResource() != null
+                        && !StringUtils.isNullOrWhitespaceOnly(jmSpec.getResource().getMemory());
+        Optional<String> jmMemoryValidation =
+                jmMemoryDefined ? Optional.empty() : validateJmMemoryConfig(conf);
+
         if (jmSpec == null) {
-            return Optional.empty();
+            return jmMemoryValidation;
         }
 
         return firstPresent(
+                jmMemoryValidation,
                 validateResources(""JobManager"", jmSpec.getResource()),
                 validateJmReplicas(jmSpec.getReplicas(), confMap));
     }
 
+    private Optional<String> validateJmMemoryConfig(Configuration conf) {
+        try {
+            JobManagerProcessUtils.processSpecFromConfigWithNewOptionToInterpretLegacyHeap(
+                    conf, JobManagerOptions.JVM_HEAP_MEMORY);
+        } catch (IllegalConfigurationException e) {","[{'comment': 'I think we should catch any exception here in case Flink throws something else.', 'commenter': 'gyfora'}]"
487,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/validation/DefaultValidator.java,"@@ -261,16 +285,40 @@ private Optional<String> validateJmReplicas(int replicas, Map<String, String> co
         return Optional.empty();
     }
 
-    private Optional<String> validateTmSpec(TaskManagerSpec tmSpec) {
+    private Optional<String> validateTmSpec(TaskManagerSpec tmSpec, Map<String, String> confMap) {
+        Configuration conf = Configuration.fromMap(confMap);
+
+        var tmMemoryDefined =
+                tmSpec != null
+                        && tmSpec.getResource() != null
+                        && !StringUtils.isNullOrWhitespaceOnly(tmSpec.getResource().getMemory());
+        Optional<String> tmMemoryConfigValidation =
+                tmMemoryDefined ? Optional.empty() : validateTmMemoryConfig(conf);
+
         if (tmSpec == null) {
-            return Optional.empty();
+            return tmMemoryConfigValidation;
         }
 
+        return firstPresent(
+                tmMemoryConfigValidation,
+                validateResources(""TaskManager"", tmSpec.getResource()),
+                validateTmReplicas(tmSpec));
+    }
+
+    private Optional<String> validateTmMemoryConfig(Configuration conf) {
+        try {
+            TaskExecutorProcessUtils.processSpecFromConfig(conf);
+        } catch (IllegalConfigurationException e) {","[{'comment': 'I think we should catch any exception here in case Flink throws something else.', 'commenter': 'gyfora'}]"
487,flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/validation/DefaultValidatorTest.java,"@@ -248,13 +252,40 @@ public void testValidationWithoutDefaultConfig() {
         testError(
                 dep -> dep.getSpec().getJobManager().getResource().setMemory(""invalid""),
                 ""JobManager resource memory parse error"");
-
         testError(
                 dep -> dep.getSpec().getTaskManager().getResource().setMemory(null),
-                ""TaskManager resource memory must be defined"");
+                ""TaskManager memory configuration failed"");","[{'comment': 'I think the previous error message was much more informative for the user. \r\n""TaskManager memory configuration failed"" is pretty hard to understand.', 'commenter': 'gyfora'}, {'comment': ""It is thrown by `IllegalConfigurationException` - [JobManagerProcessUtils.java#L7](https://github.com/apache/flink/blob/5c3658aa06b79e8039043145560a1ad2bcce68b0/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/JobManagerProcessUtils.java#L78)\r\n\r\nFull error message is very descriptive, example:\r\n\r\n```\r\nTaskManager memory configuration failed: Either required fine-grained memory \r\n(taskmanager.memory.task.heap.size and taskmanager.memory.managed.size), \r\nor Total Flink Memory size (Key: 'taskmanager.memory.flink.size' , default: null (fallback keys: [])), \r\nor Total Process Memory size (Key: 'taskmanager.memory.process.size' , \r\ndefault: null (fallback keys: [])) need to be configured explicitly.\r\n```"", 'commenter': 'nowke'}, {'comment': 'In this case I would expect an error instructing the user to set `spect.taskmanager.resource.memory` instead of the flink configs directly.', 'commenter': 'gyfora'}, {'comment': ""Changed it to `TaskManager resource memory must be defined using 'spec.taskManager.resource.memory'`. "", 'commenter': 'nowke'}]"
507,flink-kubernetes-operator-api/src/main/java/org/apache/flink/kubernetes/operator/api/docs/CrdReferenceDoclet.java,"@@ -210,7 +233,7 @@ public Void scan(Element e, Integer depth) {
                 case ENUM_CONSTANT:
                     out.println(
                             ""| ""
-                                    + e
+                                    + getJsonPropValueOfEnum(e)","[{'comment': 'I think this logic should be applied to all fields, not only enums. Currently only enums are affected but we could define @JsonProperty annotations on other fields in the future', 'commenter': 'gyfora'}]"
509,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/ScalingExecutor.java,"@@ -116,6 +133,20 @@ public boolean scaleResource(
         return true;
     }
 
+    private static String scalingReport(Map<JobVertexID, ScalingSummary> scalingSummaries) {","[{'comment': '```suggestion\r\n    private static String stringifiedScalingReport(Map<JobVertexID, ScalingSummary> scalingSummaries) {\r\n```', 'commenter': 'mxm'}]"
509,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/ScalingExecutor.java,"@@ -99,14 +103,27 @@ public boolean scaleResource(
             return false;
         }
 
+        var scalingReport = scalingReport(scalingSummaries);
+        eventRecorder.triggerEvent(
+                resource,
+                EventRecorder.Type.Normal,
+                EventRecorder.Reason.ScalingReport,
+                EventRecorder.Component.Operator,
+                scalingReport);
+
+        if (!conf.get(SCALING_ENABLED)) {
+            return false;
+        }","[{'comment': 'Moving this condition here will generate scaling events even though the scaling is disabled. I think this is not expected.', 'commenter': 'mxm'}, {'comment': 'I think that is completely expected. This will make events useful in case the user prefers to trigger actions manually. For instance when trying out the autoscaler initially.\r\n\r\nWhen scaling actions are disabled events are actually actionable to the user this way', 'commenter': 'gyfora'}, {'comment': 'Absolutely, I think this is a great addition, but only if we inform the users that the scaling wasnâ€™t actually executed. We can include this in the event message. Otherwise I foresee confusion around whether the scaling was executed or not.', 'commenter': 'mxm'}, {'comment': 'I hear you, how about this:\r\n```\r\n2023-01-25 13:16:04,184 o.a.f.k.o.u.EventCollector     [INFO ] >>> Event  | Info    | SCALINGREPORT   | Scaling vertices: Vertex ID 611fc81374e72d3d19e2d14196df735c | Parallelism 1 -> 2 | Processing capacity 100,00 -> 157,00 | Target data rate 110,00\r\n```\r\nvs\r\n\r\n```\r\n2023-01-25 13:16:04,274 o.a.f.k.o.u.EventCollector     [INFO ] >>> Event  | Info    | SCALINGREPORT   | Recommended parallelism change: Vertex ID b6844a4ed9bebdbbd33e4ebc1662745e | Parallelism 1 -> 2 | Processing capacity 100,00 -> 157,00 | Target data rate 110,00\r\n```', 'commenter': 'morhidi'}, {'comment': 'That is awesome! :)', 'commenter': 'mxm'}]"
509,examples/autoscaling/src/main/java/autoscaling/AutoscalingExample.java,"@@ -18,23 +18,38 @@
 
 package autoscaling;
 
+import org.apache.flink.api.common.functions.RichMapFunction;
 import org.apache.flink.streaming.api.datastream.DataStream;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
+import org.apache.flink.streaming.api.functions.sink.SinkFunction;
 
 /** Autoscaling Example. */
 public class AutoscalingExample {
     public static void main(String[] args) throws Exception {
         var env = StreamExecutionEnvironment.getExecutionEnvironment();
-        DataStream<Long> stream = env.fromSequence(Long.MIN_VALUE, Long.MAX_VALUE);
+        long numIterations = Long.parseLong(args[0]);
+        DataStream<Long> stream =
+                env.fromSequence(Long.MIN_VALUE, Long.MAX_VALUE).filter(i -> System.nanoTime() > 1);
         stream =
-                stream.shuffle()
+                stream.keyBy(s -> ""s"")","[{'comment': 'I think it may be better to keep to `shuffle()` here, I was just testing some skew scenarios and forgot this in.', 'commenter': 'gyfora'}]"
513,docs/layouts/shortcodes/generated/dynamic_section.html,"@@ -8,6 +8,18 @@
         </tr>
     </thead>
     <tbody>
+        <tr>
+            <td><h5>kubernetes.operator.cluster.health-check.completed-checkpoints.enabled</h5></td>
+            <td style=""word-wrap: break-word;"">false</td>
+            <td>Boolean</td>
+            <td>Whether to enable completed checkpoint health check for clusters.</td>
+        </tr>
+        <tr>
+            <td><h5>kubernetes.operator.cluster.health-check.completed-checkpoints.window</h5></td>","[{'comment': 'maybe it would be better to call it `checkpoint-progress`, and make it more explicit in the description that we record unhealthy state if no checkpoints were taken at all in the check window', 'commenter': 'gyfora'}, {'comment': 'Changed + fixed other issues in the doc.', 'commenter': 'gaborgsomogyi'}]"
513,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/KubernetesOperatorConfigOptions.java,"@@ -379,6 +379,24 @@ public static String operatorConfigKey(String key) {
                             ""The threshold which is checked against job restart count within a configured window. ""
                                     + ""If the restart count is reaching the threshold then full cluster restart is initiated."");
 
+    @Documentation.Section(SECTION_DYNAMIC)
+    public static final ConfigOption<Boolean>
+            OPERATOR_CLUSTER_HEALTH_CHECK_CHECKPOINT_PROGRESS_ENABLED =
+                    operatorConfig(""cluster.health-check.checkpoint-progress.enabled"")
+                            .booleanType()
+                            .defaultValue(false)
+                            .withDescription(
+                                    ""Whether to enable checkpoint progress health check for clusters."");
+
+    @Documentation.Section(SECTION_DYNAMIC)
+    public static final ConfigOption<Duration>
+            OPERATOR_CLUSTER_HEALTH_CHECK_CHECKPOINT_PROGRESS_WINDOW =
+                    operatorConfig(""cluster.health-check.checkpoint-progress.window"")
+                            .durationType()
+                            .defaultValue(Duration.ofMinutes(5))
+                            .withDescription(
+                                    ""The duration of the time window where job completed checkpoint count measured. This must be bigger than checkpointing interval."");","[{'comment': 'This description should be something like:\r\n```\r\nIf no checkpoints are completed within the defined time window, the job is considered unhealthy. This must be bigger than checkpointing interval.\r\n```', 'commenter': 'gyfora'}, {'comment': 'Changed.', 'commenter': 'gaborgsomogyi'}]"
513,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/observer/ClusterHealthEvaluator.java,"@@ -75,72 +77,139 @@ public void evaluate(
             var lastValidClusterHealthInfo = getLastValidClusterHealthInfo(clusterInfo);
             if (lastValidClusterHealthInfo == null) {
                 LOG.debug(""No last valid health info, skipping health check"");
+                observedClusterHealthInfo.setNumRestartsEvaluationTimeStamp(
+                        observedClusterHealthInfo.getTimeStamp());
+                observedClusterHealthInfo.setNumCompletedCheckpointsIncreasedTimeStamp(
+                        observedClusterHealthInfo.getTimeStamp());
                 setLastValidClusterHealthInfo(clusterInfo, observedClusterHealthInfo);
             } else if (observedClusterHealthInfo.getTimeStamp()
                     < lastValidClusterHealthInfo.getTimeStamp()) {
                 String msg =
                         ""Observed health info timestamp is less than the last valid health info timestamp, this indicates a bug..."";
                 LOG.error(msg);
                 throw new IllegalStateException(msg);
-            } else if (observedClusterHealthInfo.getNumRestarts()
-                    < lastValidClusterHealthInfo.getNumRestarts()) {
-                LOG.debug(
-                        ""Observed health info number of restarts is less than the last valid health info number of restarts, skipping health check"");
-                setLastValidClusterHealthInfo(clusterInfo, observedClusterHealthInfo);
             } else {
-                boolean isHealthy = true;
-
                 LOG.debug(""Valid health info exist, checking cluster health"");
                 LOG.debug(""Last valid health info: {}"", lastValidClusterHealthInfo);
                 LOG.debug(""Observed health info: {}"", observedClusterHealthInfo);
 
-                var timestampDiffMs =
-                        observedClusterHealthInfo.getTimeStamp()
-                                - lastValidClusterHealthInfo.getTimeStamp();
-                LOG.debug(
-                        ""Time difference between health infos: {}"",
-                        Duration.ofMillis(timestampDiffMs));
-
-                var restartCheckWindow =
-                        configuration.get(OPERATOR_CLUSTER_HEALTH_CHECK_RESTARTS_WINDOW);
-                var restartCheckWindowMs = restartCheckWindow.toMillis();
-                double countMultiplier = (double) restartCheckWindowMs / (double) timestampDiffMs;
-                // If the 2 health info timestamp difference is within the window then no
-                // scaling needed
-                if (countMultiplier > 1) {
-                    countMultiplier = 1;
-                }
-                long numRestarts =
-                        (long)
-                                ((double)
-                                                (observedClusterHealthInfo.getNumRestarts()
-                                                        - lastValidClusterHealthInfo
-                                                                .getNumRestarts())
-                                        * countMultiplier);
-                LOG.debug(
-                        ""Calculated restart count for {} window: {}"",
-                        restartCheckWindow,
-                        numRestarts);
-
-                if (lastValidClusterHealthInfo.getTimeStamp()
-                        < clock.millis() - restartCheckWindowMs) {
-                    LOG.debug(""Last valid health info timestamp is outside of the window"");
-                    setLastValidClusterHealthInfo(clusterInfo, observedClusterHealthInfo);
-                }
-
-                var restartThreshold =
-                        configuration.get(OPERATOR_CLUSTER_HEALTH_CHECK_RESTARTS_THRESHOLD);
-                if (numRestarts > restartThreshold) {
-                    LOG.info(""Restart count hit threshold: {}"", restartThreshold);
-                    setLastValidClusterHealthInfo(clusterInfo, observedClusterHealthInfo);
-                    isHealthy = false;
-                }
-
-                // Update the health flag
-                lastValidClusterHealthInfo = getLastValidClusterHealthInfo(clusterInfo);
+                boolean isHealthy =
+                        evaluateRestarts(
+                                configuration,
+                                clusterInfo,
+                                lastValidClusterHealthInfo,
+                                observedClusterHealthInfo);
+                isHealthy &=","[{'comment': ""let's use\r\n```\r\nisHealthy = evaluteRestarts(...) && evaluateCheckpoints(...)\r\n```\r\nfor clarity"", 'commenter': 'gyfora'}, {'comment': 'Changed.', 'commenter': 'gaborgsomogyi'}]"
513,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/observer/ClusterHealthEvaluator.java,"@@ -75,72 +77,139 @@ public void evaluate(
             var lastValidClusterHealthInfo = getLastValidClusterHealthInfo(clusterInfo);
             if (lastValidClusterHealthInfo == null) {
                 LOG.debug(""No last valid health info, skipping health check"");","[{'comment': 'I think we should clear all health info (restarts, checkpoint progress) during an upgrade/suspend, so we guarantee that it is reinitialized after the jm starts after the upgrade.', 'commenter': 'gyfora'}, {'comment': 'Added.', 'commenter': 'gaborgsomogyi'}]"
513,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/ApplicationReconciler.java,"@@ -145,6 +147,15 @@ public void deploy(
         var relatedResource = ctx.getResource();
         var status = relatedResource.getStatus();
         var flinkService = ctx.getFlinkService();
+
+        if (spec.getJob().getState().equals(JobState.SUSPENDED)
+                || status.getReconciliationStatus()
+                        .getState()
+                        .equals(ReconciliationState.UPGRADING)) {","[{'comment': 'Do we need any of these checks? If we are deploying a job then the previous health info should probably be cleared always', 'commenter': 'gyfora'}, {'comment': ""Well, such case we're more on the safe side so not needed. Removed."", 'commenter': 'gaborgsomogyi'}]"
519,docs/content/docs/operations/helm.md,"@@ -74,7 +74,8 @@ The configurable parameters of the Helm chart and which default values as detail
 | operatorPod.dnsPolicy | DNS policy to be used by the operator pod. | |
 | operatorPod.dnsConfig | DNS configuration to be used by the operator pod. | |
 | operatorPod.nodeSelector | Custom nodeSelector to be added to the operator pod. | |
-| operatorPod.resources | Custom resources block to be added to the operator pod. | |
+| operatorPod.resources | Custom resources block to be added to the operator pod on main container. | |
+| operatorPod.flinkWebhookResources | Custom resources block to be added to the operator pod on flink-webhook container. | |","[{'comment': 'How about `webhook.resurces` instead? It is a bit more concise and would make sense to group it like that?', 'commenter': 'mbalassi'}, {'comment': 'Maybe there should be new key `webhookPod` to be consistent with `operatorPod`? Or jus use `operatorPod.resources` a some configuration for webhook container is used from `operatorPod` key?', 'commenter': 'retrry'}, {'comment': 'I vote for `webhookPod` for the sake of consistency', 'commenter': 'morhidi'}, {'comment': 'Should `webhookPod.dnsConfig`be created for consistency, as currently webhook container config uses `operatorPod.dnsConfig`?', 'commenter': 'retrry'}, {'comment': 'Let us do `webhookPod` then and include the `dnsConfig` please.', 'commenter': 'mbalassi'}, {'comment': 'cc @Vince-Chenal ', 'commenter': 'mbalassi'}, {'comment': ""In fact there are two containers within the same pod, that's why I decided to keep it within operatorPod field.\r\n`operatorPod.resources` is already confusing as we don't know which container it configures.\r\nwhat do you think?"", 'commenter': 'Vince-Chenal'}, {'comment': 'I moved it to `.Values.operatorPod.webhook.resources`, does it sound ok for you ?', 'commenter': 'Vince-Chenal'}]"
524,helm/flink-kubernetes-operator/crds/flinkdeployments.flink.apache.org-v1.yml,"@@ -9243,6 +9242,19 @@ spec:
                 type: object
               error:
                 type: string
+              lifecycleState:
+                enum:
+                - CREATED
+                - SUSPENDED
+                - UPGRADING
+                - DEPLOYED
+                - STABLE
+                - ROLLING_BACK
+                - ROLLED_BACK
+                - FAILED
+                - terminal","[{'comment': 'What is this exactly? :)', 'commenter': 'morhidi'}, {'comment': 'For some reason the fabric8 crd gen includes all the fields from the `ResourceLifecycleState`, and this is why `terminal` and `description` also appear. This is not good I agree, maybe some annotation needed above those fields to ignore those by the crd generator. I can take a look at those tomorrow afternoon (CET). However, maybe that is not required to commit the automated generated yaml files, am I right?', 'commenter': 'zch93'}, {'comment': '`@JsonIgnore` is the answer:\r\nhttps://github.com/fabric8io/kubernetes-client/blob/master/doc/CRD-generator.md', 'commenter': 'mbalassi'}]"
524,flink-kubernetes-operator-api/src/main/java/org/apache/flink/kubernetes/operator/api/status/CommonStatus.java,"@@ -43,14 +43,16 @@
     /** Error information about the FlinkDeployment/FlinkSessionJob. */
     private String error;
 
+    @PrinterColumn(name = ""Resource Lifecycle State"")","[{'comment': 'Let us call this simply `Lifecycle State`, resource is redundant.', 'commenter': 'mbalassi'}]"
540,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/sessionjob/SessionJobReconciler.java,"@@ -100,6 +106,26 @@ public DeleteControl cleanupInternal(FlinkResourceContext<FlinkSessionJob> ctx)
             if (jobID != null) {
                 try {
                     cancelJob(ctx, UpgradeMode.STATELESS);
+                } catch (ExecutionException e) {
+                    final var cause = e.getCause();
+
+                    if (cause instanceof FlinkJobNotFoundException) {
+                        LOG.error(""Job {} not found in the Flink cluster."", jobID, e);
+                        return DeleteControl.defaultDelete();
+                    }
+
+                    if (cause instanceof FlinkJobTerminatedWithoutCancellationException) {
+                        LOG.error(""Job {} already terminated without cancellation."", jobID, e);
+                        return DeleteControl.defaultDelete();
+                    }
+
+                    final var delay = 10_000L;","[{'comment': 'What about allow users to define that value via Flink configurations? ', 'commenter': 'tamirsagi'}, {'comment': 'good point ~', 'commenter': 'haoxins'}, {'comment': 'We could also use the KubernetesOperatorConfigOptions.OPERATOR_OBSERVER_PROGRESS_CHECK_INTERVAL that we use for similar purposes. That also defaults to 10s', 'commenter': 'gyfora'}, {'comment': 'done', 'commenter': 'haoxins'}]"
540,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/sessionjob/SessionJobReconciler.java,"@@ -24,30 +24,41 @@
 import org.apache.flink.kubernetes.operator.api.spec.UpgradeMode;
 import org.apache.flink.kubernetes.operator.api.status.FlinkSessionJobStatus;
 import org.apache.flink.kubernetes.operator.api.status.JobManagerDeploymentStatus;
+import org.apache.flink.kubernetes.operator.config.FlinkConfigManager;
 import org.apache.flink.kubernetes.operator.controller.FlinkResourceContext;
 import org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractJobReconciler;
 import org.apache.flink.kubernetes.operator.reconciler.deployment.NoopJobAutoscalerFactory;
 import org.apache.flink.kubernetes.operator.utils.EventRecorder;
 import org.apache.flink.kubernetes.operator.utils.StatusRecorder;
+import org.apache.flink.runtime.messages.FlinkJobNotFoundException;
+import org.apache.flink.runtime.messages.FlinkJobTerminatedWithoutCancellationException;
 
 import io.fabric8.kubernetes.client.KubernetesClient;
 import io.javaoperatorsdk.operator.api.reconciler.DeleteControl;
+import io.javaoperatorsdk.operator.api.reconciler.UpdateControl;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 import java.util.Optional;
+import java.util.concurrent.ExecutionException;
 
 /** The reconciler for the {@link FlinkSessionJob}. */
 public class SessionJobReconciler
         extends AbstractJobReconciler<FlinkSessionJob, FlinkSessionJobSpec, FlinkSessionJobStatus> {
 
     private static final Logger LOG = LoggerFactory.getLogger(SessionJobReconciler.class);
 
+    private UpdateControl updateControl;","[{'comment': ""Can you please remove this field? I think it's unused"", 'commenter': 'gyfora'}, {'comment': 'done', 'commenter': 'haoxins'}]"
546,flink-kubernetes-operator-autoscaler/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/metrics/MetricAggregator.java,"@@ -34,7 +33,11 @@ public enum MetricAggregator {
         this.getter = getter;
     }
 
-    public Optional<Double> get(AggregatedMetric metric) {
-        return Optional.ofNullable(metric).map(getter).filter(d -> !d.isNaN());
+    public double get(AggregatedMetric metric) {
+        if (metric != null) {
+            return getter.apply(metric);
+        } else {
+            return Double.NaN;
+        }","[{'comment': 'This change here could be the reason why the lag growth rate may be nan after this.\r\nBefore this commit after your pending record related change the lag would be 0 , and now this may return Double.nan.', 'commenter': 'gyfora'}, {'comment': 'We discussed that this may delay decisions in case of NaNs but should not cause issues.', 'commenter': 'mxm'}]"
549,docs/layouts/shortcodes/generated/dynamic_section.html,"@@ -86,6 +86,12 @@
             <td>Duration</td>
             <td>Interval at which periodic savepoints will be triggered. The triggering schedule is not guaranteed, savepoints will be triggered as part of the regular reconcile loop.</td>
         </tr>
+        <tr>
+            <td><h5>kubernetes.operator.rescaling.cluster-cooldown</h5></td>
+            <td style=""word-wrap: break-word;"">1 min</td>
+            <td>Duration</td>","[{'comment': 'The default value should be 0 I think to preserve the current behavior as this does not affect all kinds of Kubernetes clusters .\r\n\r\nmany are actually fixed sized', 'commenter': 'gyfora'}, {'comment': 'This one is tricky. I would rather like to be conservative here because concurrent scaling operations can create a lot of unexpected churn in the cluster. Users have seen cost increase from resource spikes which can paralyze the entire cluster and lead to adding more k8s nodes (if the cluster autoscaler is active). Users may always change this default. \r\n\r\nBeyond this, we may add something like a concurrency parameter which allows to define how many pipelines are allowed to scale before the cool down. Additionally, checking the actual allocatable resources before scaling is something we need to look into.', 'commenter': 'mxm'}, {'comment': ""I personally would not enforce any cross deployment timeout / cooldown by default. I don't think it's intuitive why something on one deployment should have an effect on others. Also this is specific to EKS not a general problem.\r\n\r\nWhile the lack of this config may cause problems on EKS over time, it causes immediate weird behaviour on every operator that is using the autoscaler for multiple jobs. Imagine having 60 jobs, it will take an hour to scale once. Also some jobs may never be scaled because there is no faire queing etc."", 'commenter': 'gyfora'}, {'comment': ""I'm ok with not enforcing a default. Fairness is not a big concern because there is an additional per-pipeline stabilization and metric collection phase which will allow other pipelines to be scaled."", 'commenter': 'mxm'}]"
558,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/AbstractFlinkService.java,"@@ -627,14 +637,42 @@ public Map<String, String> getClusterInfo(Configuration conf) throws Exception {
                                             .toSeconds(),
                                     TimeUnit.SECONDS);
 
-            runtimeVersion.put(
+            clusterInfo.put(
                     DashboardConfiguration.FIELD_NAME_FLINK_VERSION,
                     dashboardConfiguration.getFlinkVersion());
-            runtimeVersion.put(
+            clusterInfo.put(
                     DashboardConfiguration.FIELD_NAME_FLINK_REVISION,
                     dashboardConfiguration.getFlinkRevision());
         }
-        return runtimeVersion;
+
+        // JobManager resource usage can be deduced from the CR
+        var jmParameters =
+                new KubernetesJobManagerParameters(
+                        conf, new KubernetesClusterClientFactory().getClusterSpecification(conf));
+        var jmTotalCpu =
+                jmParameters.getJobManagerCPU()
+                        * jmParameters.getJobManagerCPULimitFactor()
+                        * jmParameters.getReplicas();
+        var jmTotalMemory =
+                Math.round(
+                        jmParameters.getJobManagerMemoryMB()
+                                * Math.pow(1024, 2)
+                                * jmParameters.getJobManagerMemoryLimitFactor()
+                                * jmParameters.getReplicas());
+
+        // TaskManager resource usage is best gathered from the REST API to get current replicas","[{'comment': ""If fractional values are used for the CPU, there will be a difference between retrieving it from Flink REST and Kubernetes CR. Flink uses `Hardware.getNumberCPUCores()` under the hood to retrieve this value, not sure exactly how that works, but it's definitely an integer in the end :D \r\n\r\nThis will lead to weird scenarios where if you have 3 JM and 3 TM replicas, all with `.5` CPU shares, the result will be `4.5` as total CPUs.\r\n\r\nAn easy solution might be to just retrieve the number of TMs and multiply it with the CPU defined in the CR."", 'commenter': 'mateczagany'}, {'comment': 'Good catch @mateczagany. I had this suspicion in the back of my mind, that the CPU consumption might be overreported, but the way we pass the values to the taskmanagers via `flink-kubernetes` (which does have proper fractional values) convinced me that it should be ok. I will dive a bit deeper into this and come back.', 'commenter': 'mbalassi'}, {'comment': 'There is a limit factor for TaskManager cores that Flink allows to be configured on top of the resources defined on the Kubernestes level, similarly to have I calculated the JobManager resources. I setup an example to validate your suggestion where I have one JM and TM each, with 0.5 cpus configured in the resources field each. The cpu limit factors are 1.0. We end up with 1.5 cpus (0.5 for the JM accurately reported and 1.0 for the TM).\r\n\r\n```\r\n  jobManager:\r\n    replicas: 1\r\n    resource:\r\n      cpu: 0.5\r\n      memory: 2048m\r\n  serviceAccount: flink\r\n  taskManager:\r\n    resource:\r\n      cpu: 0.5\r\n      memory: 2048m\r\nstatus:\r\n  clusterInfo:\r\n    flink-revision: DeadD0d0 @ 1970-01-01T01:00:00+01:00\r\n    flink-version: 1.16.1\r\n    tm-cpu-limit-factor: ""1.0""\r\n    jm-cpu-limit-factor: ""1.0""\r\n    total-cpu: ""1.5""\r\n    total-memory: ""4294967296""\r\n  jobManagerDeploymentStatus: READY\r\n```\r\n\r\nIt is a bit of a tough problem, because the Flink UI also shows 1 core for the TM (using the same value that we get from the REST API).\r\n\r\n<img width=""1403"" alt=""Screenshot 2023-03-31 at 12 08 26"" src=""https://user-images.githubusercontent.com/5990983/229091963-f5e9a985-2ebe-4518-9623-6a4d4da9ad3c.png"">\r\n\r\nSo ultimately we have to decide whether to stick with Flink or with Kubernetes, I am leaning towards the latter (with calculating in the limit factor, but avoiding the rounding).', 'commenter': 'mbalassi'}, {'comment': ""I tried to implement the same logic for `tmTotalCpu` as what you did the with `jmTotalCpu`, and I think it should be valid: `tmCpuRequest * tmCpuLimitFactor * numberOfTaskManagers`\r\n\r\n`tmCpuRequest` and `tmCpuLimitFactor` are accessible the same way as for the JM. Just retrieve `kubernetes.taskmanager.cpu` and `kubernetes.taskmanager.cpu.limit-factor` from the Flink config.\r\n\r\nI'm not sure about `numberOfTaskManagers`, in my test I just downloaded the number of TMs from the Flink REST API, maybe we could just use `FlinkUtils#getNumTaskManagers` instead.\r\n\r\nCode:\r\n```\r\nvar tmTotalCpu =\r\n        tmHardwareDesc.get().count()\r\n                * conf.getDouble(KubernetesConfigOptions.TASK_MANAGER_CPU)\r\n                * conf.getDouble(KubernetesConfigOptions.TASK_MANAGER_CPU_LIMIT_FACTOR);\r\n```\r\n\r\nLimit factors:\r\n```\r\nkubernetes.taskmanager.cpu.limit-factor = 1.3\r\nkubernetes.jobmanager.cpu.limit-factor = 1.3\r\n```\r\n\r\nResult:\r\n```\r\nJob Manager:\r\n  Replicas:            2\r\n  Resource:\r\n    Cpu:          0.5\r\n    Memory:       1g\r\nTask Manager:\r\n  Replicas:            2\r\n  Resource:\r\n    Cpu:     0.5\r\n    Memory:  1g\r\nStatus:\r\n  Cluster Info:\r\n    Total - Cpu:                  2.6\r\n    Total - Memory:               4294967296\r\n```\r\n\r\nDo you think this could work?"", 'commenter': 'mateczagany'}, {'comment': ""Thanks @mateczagany, this approach looks good. If you have the bandwidth would you mind pushing your suggestions to this PR branch so that the commit can be attributed to you? ðŸ˜ I have invited you as a collaborator to my fork, you might need to accept that.\r\n\r\nI would ask the following if you have the time:\r\n\r\n1. Get resource configuration from the config as you suggested uniformly for JMs and TMs\r\n2. Get JM replicas from config, TM replicas from the REST API (we are trying to be careful with the TM replicas because we foresee that we might be changing things dynamically there via the autoscaler soon)\r\n3. Add a test to `FlinkDeploymentMetricsTest` that verifies that given that the `status.clusterInfo` is properly filled out we fill out the metrics properly.\r\n\r\nCurrently we do not have meaningful test for creating the clusterInfo and since we are relying on the application's REST API I do not see an easy way of testing it properly, so I would accept this change without that (but it might merit a separate JIRA)."", 'commenter': 'mbalassi'}, {'comment': ""I've pushed your requests and also extracted the logic to a new method so we could test it more easily without needing REST API, I just wasn't sure where to place the test, I'm not that familiar with the project structure yet :D \r\n\r\nIf you think the PR looks ok, please let me know where you think I should write a test `AbstractFlinkService#calculateClusterResourceMetrics`, and I will do that as well."", 'commenter': 'mateczagany'}]"
558,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/AbstractFlinkService.java,"@@ -645,10 +641,19 @@ public Map<String, String> getClusterInfo(Configuration conf) throws Exception {
                     dashboardConfiguration.getFlinkRevision());
         }
 
-        // JobManager resource usage can be deduced from the CR
-        var jmParameters =
-                new KubernetesJobManagerParameters(
-                        conf, new KubernetesClusterClientFactory().getClusterSpecification(conf));
+        clusterInfo.putAll(
+                calculateClusterResourceMetrics(
+                        conf, getTaskManagersInfo(conf).getTaskManagerInfos().size()));
+
+        return clusterInfo;
+    }
+
+    private HashMap<String, String> calculateClusterResourceMetrics(","[{'comment': 'nit: maybe call this `calculateClusterResourceUsage` or `calculateClusterResourceFootprint`, since technically not the metrics yet.', 'commenter': 'mbalassi'}, {'comment': ""You're right! I've also moved the method to two separate methods in `FlinkUtils` and will add tests tomorrow if this seems okay. This will result in duplicated code, but I think it improves the code, also easier to re-use and test this way.\r\n\r\nI will add tests for the two new methods tomorrow."", 'commenter': 'mateczagany'}, {'comment': 'I have added the new tests in FlinkUtils and rebased to main', 'commenter': 'mateczagany'}]"
558,flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/metrics/FlinkDeploymentMetricsTest.java,"@@ -187,6 +193,66 @@ public void testMetricsMultiNamespace() {
         }
     }
 
+    @Test
+    public void testResourceMetrics() {
+        var namespace1 = ""ns1"";
+        var namespace2 = ""ns2"";
+        var deployment1 = TestUtils.buildApplicationCluster(""deployment1"", namespace1);
+        var deployment2 = TestUtils.buildApplicationCluster(""deployment2"", namespace1);
+        var deployment3 = TestUtils.buildApplicationCluster(""deployment3"", namespace2);
+
+        deployment1
+                .getStatus()
+                .getClusterInfo()
+                .putAll(
+                        Map.of(
+                                AbstractFlinkService.FIELD_NAME_TOTAL_CPU, ""5"",","[{'comment': 'Could you please add a test that has unexpected values (null, empty string etc) - given that the status field could be (but not expected to be) modified externally we want to make sure that the operator logic does not fail on that (This is why I used `NumberUtils` in the implementation).', 'commenter': 'mbalassi'}, {'comment': ""I've added the tests, also added a check to convert `Infinity` and `NaN` values to 0 instead."", 'commenter': 'mateczagany'}]"
561,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/validation/DefaultValidator.java,"@@ -366,6 +368,18 @@ private Optional<String> validateResources(String component, Resource resource)
             return Optional.of(component + "" resource memory parse error: "" + iae.getMessage());
         }
 
+        String storage = resource.getEphemeralStorage();","[{'comment': 'This will not catch misconfigured storage resources if memory is not defined.', 'commenter': 'morhidi'}, {'comment': 'Got catch. Thanks!', 'commenter': 'HuangZhenQiu'}]"
561,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/FlinkConfigBuilder.java,"@@ -409,15 +419,44 @@ private static void setPodTemplate(
             effectiveConfig.setString(
                     podConfigOption,
                     createTempFile(
-                            mergePodTemplates(
-                                    basicPod,
-                                    appendPod,
-                                    effectiveConfig.get(
-                                            KubernetesOperatorConfigOptions
-                                                    .POD_TEMPLATE_MERGE_BY_NAME))));
+                            applyResourceToPodTemplate(
+                                    mergePodTemplates(","[{'comment': 'I think we should not nest these calls but do one after the other for readability', 'commenter': 'gyfora'}, {'comment': 'Updated', 'commenter': 'HuangZhenQiu'}]"
561,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/FlinkConfigBuilder.java,"@@ -409,15 +419,44 @@ private static void setPodTemplate(
             effectiveConfig.setString(
                     podConfigOption,
                     createTempFile(
-                            mergePodTemplates(
-                                    basicPod,
-                                    appendPod,
-                                    effectiveConfig.get(
-                                            KubernetesOperatorConfigOptions
-                                                    .POD_TEMPLATE_MERGE_BY_NAME))));
+                            applyResourceToPodTemplate(
+                                    mergePodTemplates(
+                                            basicPod,
+                                            appendPod,
+                                            effectiveConfig.get(
+                                                    KubernetesOperatorConfigOptions
+                                                            .POD_TEMPLATE_MERGE_BY_NAME)),
+                                    resource)));
         }
     }
 
+    private static Pod applyResourceToPodTemplate(Pod podTemplate, Resource resource) {
+        if (resource != null
+                && !StringUtils.isNullOrWhitespaceOnly(resource.getEphemeralStorage())
+                && podTemplate != null
+                && podTemplate.getSpec() != null) {","[{'comment': 'This will ignore the ephemeralStorage setting if the user did not specify any podTemplate (or spec in the podTemplate), that is probably incorrect.', 'commenter': 'gyfora'}, {'comment': 'We need some more test coverage for different configs:\r\n 1. Ephemeral storage settings without any podtemplate \r\n 2. Ephemeral storage without tm/jm podTemplate \r\n 3. Ephemeral storage without spec in podTemplate', 'commenter': 'gyfora'}, {'comment': 'Added different scenarios in FlinkConfigBuilderTest', 'commenter': 'HuangZhenQiu'}]"
561,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/FlinkConfigBuilder.java,"@@ -409,15 +419,44 @@ private static void setPodTemplate(
             effectiveConfig.setString(
                     podConfigOption,
                     createTempFile(
-                            mergePodTemplates(
-                                    basicPod,
-                                    appendPod,
-                                    effectiveConfig.get(
-                                            KubernetesOperatorConfigOptions
-                                                    .POD_TEMPLATE_MERGE_BY_NAME))));
+                            applyResourceToPodTemplate(
+                                    mergePodTemplates(
+                                            basicPod,
+                                            appendPod,
+                                            effectiveConfig.get(
+                                                    KubernetesOperatorConfigOptions
+                                                            .POD_TEMPLATE_MERGE_BY_NAME)),
+                                    resource)));
         }
     }
 
+    private static Pod applyResourceToPodTemplate(Pod podTemplate, Resource resource) {
+        if (resource != null
+                && !StringUtils.isNullOrWhitespaceOnly(resource.getEphemeralStorage())
+                && podTemplate != null
+                && podTemplate.getSpec() != null) {
+            for (Container container : podTemplate.getSpec().getContainers()) {","[{'comment': 'Should this apply for all containers or only to the main jm/tm container? `Constants.MAIN_CONTAINER_NAME` \r\nI think only to the main one.', 'commenter': 'gyfora'}, {'comment': 'I was confused about the requirement.  It makes more sense to only apply it to flink main container.', 'commenter': 'HuangZhenQiu'}]"
561,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/FlinkConfigBuilder.java,"@@ -391,31 +399,87 @@ private static void setResource(
     }
 
     private static void setPodTemplate(
-            Pod basicPod, Pod appendPod, Configuration effectiveConfig, boolean isJM)
+            Pod basicPod,
+            Pod appendPod,
+            Resource resource,
+            Configuration effectiveConfig,
+            boolean isJM)
             throws IOException {
 
-        if (basicPod == null && appendPod == null) {
-            return;
-        }
-
         // Avoid to create temporary pod template files for JobManager and TaskManager if it is not
         // configured explicitly via .spec.JobManagerSpec.podTemplate or
         // .spec.TaskManagerSpec.podTemplate.
+        final ConfigOption<String> podConfigOption =
+                isJM
+                        ? KubernetesConfigOptions.JOB_MANAGER_POD_TEMPLATE
+                        : KubernetesConfigOptions.TASK_MANAGER_POD_TEMPLATE;
+
+        Pod basicPodWithResource = applyResourceToPodTemplate(basicPod, resource);
+
         if (appendPod != null) {
-            final ConfigOption<String> podConfigOption =
-                    isJM
-                            ? KubernetesConfigOptions.JOB_MANAGER_POD_TEMPLATE
-                            : KubernetesConfigOptions.TASK_MANAGER_POD_TEMPLATE;
-            effectiveConfig.setString(
-                    podConfigOption,
-                    createTempFile(
-                            mergePodTemplates(
-                                    basicPod,
-                                    appendPod,
-                                    effectiveConfig.get(
-                                            KubernetesOperatorConfigOptions
-                                                    .POD_TEMPLATE_MERGE_BY_NAME))));
+            Pod mergedPodTemplate =
+                    mergePodTemplates(
+                            basicPodWithResource,
+                            appendPod,
+                            effectiveConfig.get(
+                                    KubernetesOperatorConfigOptions.POD_TEMPLATE_MERGE_BY_NAME));
+            effectiveConfig.setString(podConfigOption, createTempFile(mergedPodTemplate));
+        } else {
+            effectiveConfig.setString(podConfigOption, createTempFile(basicPodWithResource));
+        }
+    }
+
+    private static Pod applyResourceToPodTemplate(Pod podTemplate, Resource resource) {
+        if (resource == null
+                || StringUtils.isNullOrWhitespaceOnly(resource.getEphemeralStorage())) {
+            return podTemplate;
         }
+
+        if (podTemplate == null || podTemplate.getSpec() == null) {
+            PodSpec spec = new PodSpec();
+            spec.getContainers()
+                    .add(
+                            decorateContainerWithEphemeralStorage(
+                                    new Container(), resource.getEphemeralStorage()));
+            Pod newPodTemplate = new Pod();
+            newPodTemplate.setSpec(spec);
+            return newPodTemplate;","[{'comment': 'If only the spec is null, this logic overwrites the entire pod, that sounds incorrect. Imagine the user only set the metadata. Please add a test to guard against this.', 'commenter': 'gyfora'}, {'comment': 'Thanks for pointing it out. Added the testApplyResourceToPodTemplate function in FlinkConfigBuilderTest to cover it. ', 'commenter': 'HuangZhenQiu'}]"
561,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/FlinkConfigBuilder.java,"@@ -391,31 +399,87 @@ private static void setResource(
     }
 
     private static void setPodTemplate(
-            Pod basicPod, Pod appendPod, Configuration effectiveConfig, boolean isJM)
+            Pod basicPod,
+            Pod appendPod,
+            Resource resource,
+            Configuration effectiveConfig,
+            boolean isJM)
             throws IOException {
 
-        if (basicPod == null && appendPod == null) {
-            return;
-        }
-
         // Avoid to create temporary pod template files for JobManager and TaskManager if it is not
         // configured explicitly via .spec.JobManagerSpec.podTemplate or
         // .spec.TaskManagerSpec.podTemplate.
+        final ConfigOption<String> podConfigOption =
+                isJM
+                        ? KubernetesConfigOptions.JOB_MANAGER_POD_TEMPLATE
+                        : KubernetesConfigOptions.TASK_MANAGER_POD_TEMPLATE;
+
+        Pod basicPodWithResource = applyResourceToPodTemplate(basicPod, resource);","[{'comment': ""Honestly I don't think this logic will work like this due to how Pod merging works by default. I recommend testing the following example which is likely to be broken currently:\r\n\r\n1. Top level (common) pod template empty.\r\n2. TM/JM level pod Template define 2 containers: [init, main-container] , in this order\r\n3. Define ephemeral storage\r\n\r\nI think the result will not be what you expect.\r\n"", 'commenter': 'gyfora'}, {'comment': 'Yes, you are right. I think the best way to add the feature without breaking existing supported functionality is that:\r\n\r\nAlways merge first as long as not both basicPod and appendPod are null, then apply resource to pod.\r\n', 'commenter': 'HuangZhenQiu'}]"
563,flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/utils/EventUtilsTest.java,"@@ -72,6 +72,7 @@ public void accept(Event event) {
                         .withName(eventName)
                         .get();
         Assertions.assertEquals(event.getMetadata().getUid(), eventConsumed.getMetadata().getUid());","[{'comment': 'This is redundant with the assertions below', 'commenter': 'morhidi'}]"
563,flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/controller/FlinkDeploymentControllerTest.java,"@@ -217,9 +227,10 @@ public void verifyBasicReconcileLoop(FlinkVersion flinkVersion) throws Exception
 
     @Test
     public void verifyFailedDeployment() throws Exception {
+
         var submittedEventValidatingResponseProvider =
                 new TestUtils.ValidatingResponseProvider<>(
-                        new EventBuilder().withNewMetadata().endMetadata().build(),
+                        mockedEvent,","[{'comment': ""good enough, but can't we send back the the event we received?"", 'commenter': 'morhidi'}, {'comment': ""I tried but the the `TestUtils.ValidatingResponseProvider` class requires the event to be returned at construction time, and I couldn't find a way to intercept the `createOrReplace` call inside `EventUtils.createOrUpdateEvent`"", 'commenter': 'rodmeneses'}]"
565,docs/layouts/shortcodes/generated/dynamic_section.html,"@@ -98,6 +98,12 @@
             <td>Boolean</td>
             <td>Configure the array merge behaviour during pod merging. Arrays can be either merged by position or name matching.</td>
         </tr>
+        <tr>
+            <td><h5>kubernetes.operator.savepoint.cleaner.enabled</h5></td>","[{'comment': 'Instead of â€œcleanerâ€ I think it should be â€¦cleanup.enabled', 'commenter': 'gyfora'}, {'comment': 'Renamed to `cleanup` in latest commit', 'commenter': 'darenwkt'}]"
565,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/FlinkOperatorConfiguration.java,"@@ -59,10 +59,11 @@ public class FlinkOperatorConfiguration {
     Duration flinkCancelJobTimeout;
     Duration flinkShutdownClusterTimeout;
     String artifactsBaseDir;
+    boolean savepointCleanerEnabled;","[{'comment': 'I think this should not be part of the operator configuration (which are system wide configs) but instead should be configurable on a per resource level (simply get it based on the observeconfig)', 'commenter': 'gyfora'}, {'comment': 'Moved to ObserverConfig', 'commenter': 'darenwkt'}]"
565,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/observer/SavepointObserver.java,"@@ -142,6 +142,11 @@ void cleanupSavepointHistory(
             SavepointInfo currentSavepointInfo,
             Configuration observeConfig) {
 
+        if (!configManager.getOperatorConfiguration().isSavepointCleanerEnabled()) {
+            LOG.info(""Operator savepoint cleaner is not enabled, do not clean savepointHistory"");
+            return;
+        }","[{'comment': 'I think we need to remove from the savepoint history / status even if the cleanup is disabled otherwise the status will eventually grow too big and break (size limit is 1 mb)', 'commenter': 'gyfora'}, {'comment': 'Agreed, I have updated the logic to remove from savepoint history/status even if cleanup is disabled. This ensures savepointHistory in CR is within maxCount and maxAge. The only difference is cleanup disabled will not send `disposeSavepoint` REST call to JM.', 'commenter': 'darenwkt'}]"
613,flink-kubernetes-operator-autoscaler/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/JobAutoScalerImpl.java,"@@ -143,4 +163,18 @@ private AutoscalerFlinkMetrics getOrInitAutoscalerFlinkMetrics(
                         new AutoscalerFlinkMetrics(
                                 ctx.getResourceMetricGroup().addGroup(""AutoScaler"")));
     }
+
+    private void resetRecommendedParallelisms(
+            Map<ResourceID, Map<JobVertexID, Integer>> recommendedParallelisms,
+            Map<JobVertexID, Map<ScalingMetric, EvaluatedScalingMetric>> lastEvaluatedMetrics,
+            ResourceID resourceID) {
+        var parallelisms = new HashMap<JobVertexID, Integer>();
+        lastEvaluatedMetrics.forEach(
+                (jobVertexID, map) -> {
+                    parallelisms.put(
+                            jobVertexID, (int) map.get(ScalingMetric.PARALLELISM).getCurrent());
+                });
+        recommendedParallelisms.put(resourceID, parallelisms);
+        LOG.debug(""Recommended parallelisms are reset to current {}"", parallelisms);
+    }","[{'comment': 'This could be removed and unified with the `updateRecommendedParallelisms` method (make it static utility maybe?) as the resetting is the same as updating with empty scaling summaries.', 'commenter': 'gyfora'}]"
613,flink-kubernetes-operator-autoscaler/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/JobAutoScalerImpl.java,"@@ -99,22 +104,37 @@ public boolean scale(FlinkResourceContext<? extends AbstractFlinkResource<?, ?>>
 
             var collectedMetrics =
                     metricsCollector.updateMetrics(
-                            resource, autoScalerInfo, ctx.getFlinkService(), conf);
+                            resource,
+                            autoScalerInfo,
+                            ctx.getFlinkService(),
+                            conf,
+                            recommendedParallelisms);
 
             LOG.debug(""Evaluating scaling metrics for {}"", collectedMetrics);
             var evaluatedMetrics = evaluator.evaluate(conf, collectedMetrics);
             LOG.debug(""Scaling metrics evaluated: {}"", evaluatedMetrics);
             lastEvaluatedMetrics.put(resourceId, evaluatedMetrics);
-            flinkMetrics.registerScalingMetrics(() -> lastEvaluatedMetrics.get(resourceId));
+
+            flinkMetrics.registerEvaluatedScalingMetrics(
+                    () -> lastEvaluatedMetrics.get(resourceId));
+            flinkMetrics.registerRecommendedParallelismMetrics(
+                    () -> recommendedParallelisms.get(resourceId));","[{'comment': 'I think we should pass these together because they both need to be provided', 'commenter': 'gyfora'}]"
613,flink-kubernetes-operator-autoscaler/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/JobAutoScalerImpl.java,"@@ -99,22 +104,37 @@ public boolean scale(FlinkResourceContext<? extends AbstractFlinkResource<?, ?>>
 
             var collectedMetrics =
                     metricsCollector.updateMetrics(
-                            resource, autoScalerInfo, ctx.getFlinkService(), conf);
+                            resource,
+                            autoScalerInfo,
+                            ctx.getFlinkService(),
+                            conf,
+                            recommendedParallelisms);
 
             LOG.debug(""Evaluating scaling metrics for {}"", collectedMetrics);
             var evaluatedMetrics = evaluator.evaluate(conf, collectedMetrics);
             LOG.debug(""Scaling metrics evaluated: {}"", evaluatedMetrics);
             lastEvaluatedMetrics.put(resourceId, evaluatedMetrics);
-            flinkMetrics.registerScalingMetrics(() -> lastEvaluatedMetrics.get(resourceId));
+
+            flinkMetrics.registerEvaluatedScalingMetrics(
+                    () -> lastEvaluatedMetrics.get(resourceId));
+            flinkMetrics.registerRecommendedParallelismMetrics(
+                    () -> recommendedParallelisms.get(resourceId));
 
             if (!collectedMetrics.isFullyCollected()) {
                 // We have done an upfront evaluation, but we are not ready for scaling.
+                resetRecommendedParallelisms(recommendedParallelisms, evaluatedMetrics, resourceId);
                 autoScalerInfo.replaceInKubernetes(kubernetesClient);
                 return false;
             }
 
             var specAdjusted =
-                    scalingExecutor.scaleResource(resource, autoScalerInfo, conf, evaluatedMetrics);
+                    scalingExecutor.scaleResource(
+                            resource,
+                            autoScalerInfo,
+                            conf,
+                            evaluatedMetrics,
+                            recommendedParallelisms);","[{'comment': 'Not something that should be fixed in this PR but we should consider a refactor by introducing some sort of autoscaler resource context to avoid so many params', 'commenter': 'gyfora'}]"
613,flink-kubernetes-operator-autoscaler/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/ScalingMetricCollector.java,"@@ -100,7 +100,9 @@ public CollectedMetricHistory updateMetrics(
             cleanup(cr);
             metricHistory.clear();
             metricCollectionStartTs = now;
+            cleanupRecommendedParallelisms(recommendedParallelisms, resourceID);
         }
+        var topology = getJobTopology(flinkService, cr, conf, autoscalerInfo);","[{'comment': 'Moving the topology assignment is not necessary right? (I made changes to this logic in my open PR so it would be better to not move this if not necessary)', 'commenter': 'gyfora'}, {'comment': 'It was necessary in certain cases it fetched the new job graph and then viped it out with the metric history.', 'commenter': 'morhidi'}]"
613,flink-kubernetes-operator-autoscaler/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/AutoscalerFlinkMetrics.java,"@@ -45,7 +43,13 @@ public class AutoscalerFlinkMetrics {
 
     private final MetricGroup metricGroup;
 
-    private final Set<JobVertexID> vertexMetrics = new HashSet<>();
+    private final Map<JobVertexID, MetricGroup> vertexMetricGroups = new ConcurrentHashMap<>();
+    private final Map<Tuple2<JobVertexID, ScalingMetric>, MetricGroup> scalingMetricGroups =
+            new ConcurrentHashMap<>();
+    private final Map<Tuple2<JobVertexID, ScalingMetric>, Gauge<Double>> currentScalingMetrics =
+            new ConcurrentHashMap<>();
+    private final Map<Tuple2<JobVertexID, ScalingMetric>, Gauge<Double>> averageScalingMetrics =
+            new ConcurrentHashMap<>();","[{'comment': 'Please revert back to using non-current hash map. This is not accessed concurrently.', 'commenter': 'mxm'}]"
613,flink-kubernetes-operator-autoscaler/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/AutoscalerFlinkMetrics.java,"@@ -54,51 +58,122 @@ public AutoscalerFlinkMetrics(MetricGroup metricGroup) {
         this.metricGroup = metricGroup;
     }
 
-    public void registerScalingMetrics(
+    public void registerEvaluatedScalingMetrics(
             Supplier<Map<JobVertexID, Map<ScalingMetric, EvaluatedScalingMetric>>>
                     currentVertexMetrics) {
         currentVertexMetrics
                 .get()
                 .forEach(
-                        (jobVertexID, evaluated) -> {
-                            if (!vertexMetrics.add(jobVertexID)) {
-                                return;
-                            }
-                            LOG.info(""Registering scaling metrics for job vertex {}"", jobVertexID);
-                            var jobVertexMg =
-                                    metricGroup.addGroup(""jobVertexID"", jobVertexID.toHexString());
-
-                            evaluated.forEach(
-                                    (sm, esm) -> {
-                                        var smGroup = jobVertexMg.addGroup(sm.name());
-
-                                        smGroup.gauge(
-                                                ""Current"",
-                                                () ->
-                                                        Optional.ofNullable(
-                                                                        currentVertexMetrics.get())
-                                                                .map(m -> m.get(jobVertexID))
-                                                                .map(
-                                                                        metrics ->
-                                                                                metrics.get(sm)
-                                                                                        .getCurrent())
-                                                                .orElse(null));
-
-                                        if (sm.isCalculateAverage()) {
-                                            smGroup.gauge(
-                                                    ""Average"",
-                                                    () ->
-                                                            Optional.ofNullable(
-                                                                            currentVertexMetrics
-                                                                                    .get())
-                                                                    .map(m -> m.get(jobVertexID))
-                                                                    .map(
-                                                                            metrics ->
-                                                                                    metrics.get(sm)
-                                                                                            .getAverage())
-                                                                    .orElse(null));
-                                        }
+                        (jobVertexID, evaluatedScalingMetrics) -> {
+                            var jobVertexMg = getJobVertexMetricGroup(jobVertexID);
+                            evaluatedScalingMetrics.forEach(
+                                    (scalingMetric, esm) -> {
+                                        var smg =
+                                                getScalingMetricGroup(
+                                                        jobVertexID, jobVertexMg, scalingMetric);
+                                        registerScalingMetric(
+                                                currentVertexMetrics,
+                                                jobVertexID,
+                                                scalingMetric,
+                                                smg);
                                     });
                         });
     }
+
+    public void registerRecommendedParallelismMetrics(
+            Supplier<Map<JobVertexID, Integer>> recommendedParallelisms) {
+
+        if (recommendedParallelisms == null || recommendedParallelisms.get() == null) {
+            return;
+        }
+        recommendedParallelisms
+                .get()
+                .forEach(
+                        (jobVertexID, parallelism) -> {
+                            var jobVertexMg = getJobVertexMetricGroup(jobVertexID);
+                            var smg =
+                                    getScalingMetricGroup(
+                                            jobVertexID, jobVertexMg, RECOMMENDED_PARALLELISM);
+
+                            currentScalingMetrics.computeIfAbsent(
+                                    Tuple2.of(jobVertexID, RECOMMENDED_PARALLELISM),
+                                    key ->
+                                            smg.gauge(
+                                                    ""Current"",
+                                                    () ->
+                                                            getCurrentValue(
+                                                                    recommendedParallelisms,
+                                                                    jobVertexID)));
+                        });
+    }","[{'comment': 'Why does this require a separate method? The recommended parallelism should just be exposed like any other evaluated scaling metric.', 'commenter': 'mxm'}]"
613,flink-kubernetes-operator-autoscaler/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/AutoscalerFlinkMetrics.java,"@@ -54,51 +58,122 @@ public AutoscalerFlinkMetrics(MetricGroup metricGroup) {
         this.metricGroup = metricGroup;
     }
 
-    public void registerScalingMetrics(
+    public void registerEvaluatedScalingMetrics(
             Supplier<Map<JobVertexID, Map<ScalingMetric, EvaluatedScalingMetric>>>
                     currentVertexMetrics) {
         currentVertexMetrics
                 .get()
                 .forEach(
-                        (jobVertexID, evaluated) -> {
-                            if (!vertexMetrics.add(jobVertexID)) {
-                                return;
-                            }
-                            LOG.info(""Registering scaling metrics for job vertex {}"", jobVertexID);
-                            var jobVertexMg =
-                                    metricGroup.addGroup(""jobVertexID"", jobVertexID.toHexString());
-
-                            evaluated.forEach(
-                                    (sm, esm) -> {
-                                        var smGroup = jobVertexMg.addGroup(sm.name());
-
-                                        smGroup.gauge(
-                                                ""Current"",
-                                                () ->
-                                                        Optional.ofNullable(
-                                                                        currentVertexMetrics.get())
-                                                                .map(m -> m.get(jobVertexID))
-                                                                .map(
-                                                                        metrics ->
-                                                                                metrics.get(sm)
-                                                                                        .getCurrent())
-                                                                .orElse(null));
-
-                                        if (sm.isCalculateAverage()) {
-                                            smGroup.gauge(
-                                                    ""Average"",
-                                                    () ->
-                                                            Optional.ofNullable(
-                                                                            currentVertexMetrics
-                                                                                    .get())
-                                                                    .map(m -> m.get(jobVertexID))
-                                                                    .map(
-                                                                            metrics ->
-                                                                                    metrics.get(sm)
-                                                                                            .getAverage())
-                                                                    .orElse(null));
-                                        }","[{'comment': ""As far as I understand, this should have been a separate commit or PR. The refactoring wasn't necessary to make this work."", 'commenter': 'mxm'}]"
613,flink-kubernetes-operator-autoscaler/src/test/java/org/apache/flink/kubernetes/operator/autoscaler/BacklogBasedScalingTest.java,"@@ -1,437 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the ""License""); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an ""AS IS"" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.kubernetes.operator.autoscaler;
-
-import org.apache.flink.api.common.JobID;
-import org.apache.flink.api.common.JobStatus;
-import org.apache.flink.configuration.Configuration;
-import org.apache.flink.kubernetes.operator.OperatorTestBase;
-import org.apache.flink.kubernetes.operator.TestUtils;
-import org.apache.flink.kubernetes.operator.api.FlinkDeployment;
-import org.apache.flink.kubernetes.operator.autoscaler.config.AutoScalerOptions;
-import org.apache.flink.kubernetes.operator.autoscaler.metrics.FlinkMetric;
-import org.apache.flink.kubernetes.operator.autoscaler.topology.JobTopology;
-import org.apache.flink.kubernetes.operator.autoscaler.topology.VertexInfo;
-import org.apache.flink.kubernetes.operator.config.FlinkConfigManager;
-import org.apache.flink.kubernetes.operator.reconciler.ReconciliationUtils;
-import org.apache.flink.kubernetes.operator.utils.EventCollector;
-import org.apache.flink.kubernetes.operator.utils.EventRecorder;
-import org.apache.flink.runtime.jobgraph.JobVertexID;
-import org.apache.flink.runtime.rest.messages.job.metrics.AggregatedMetric;
-
-import io.fabric8.kubernetes.api.model.HasMetadata;
-import io.fabric8.kubernetes.client.KubernetesClient;
-import io.fabric8.kubernetes.client.server.mock.EnableKubernetesMockClient;
-import io.javaoperatorsdk.operator.processing.event.ResourceID;
-import lombok.Getter;
-import org.jetbrains.annotations.NotNull;
-import org.junit.jupiter.api.BeforeEach;
-import org.junit.jupiter.api.Test;
-
-import java.time.Clock;
-import java.time.Duration;
-import java.time.Instant;
-import java.time.ZoneId;
-import java.util.Map;
-import java.util.Set;
-import java.util.stream.Collectors;
-
-import static org.junit.jupiter.api.Assertions.assertEquals;
-import static org.junit.jupiter.api.Assertions.assertFalse;
-import static org.junit.jupiter.api.Assertions.assertTrue;
-
-/** Test for scaling metrics collection logic. */
-@EnableKubernetesMockClient(crud = true)
-public class BacklogBasedScalingTest extends OperatorTestBase {
-
-    @Getter private KubernetesClient kubernetesClient;
-
-    private ScalingMetricEvaluator evaluator;
-    private TestingMetricsCollector metricsCollector;
-    private ScalingExecutor scalingExecutor;
-
-    private FlinkDeployment app;
-    private JobVertexID source1, sink;
-
-    private JobAutoScalerImpl autoscaler;
-
-    private EventCollector eventCollector = new EventCollector();
-
-    @BeforeEach
-    public void setup() {
-        evaluator = new ScalingMetricEvaluator();
-        scalingExecutor =
-                new ScalingExecutor(
-                        kubernetesClient,
-                        new EventRecorder(kubernetesClient, new EventCollector()));
-
-        app = TestUtils.buildApplicationCluster();
-        app.getMetadata().setGeneration(1L);
-        app.getStatus().getJobStatus().setJobId(new JobID().toHexString());
-        kubernetesClient.resource(app).createOrReplace();
-
-        source1 = new JobVertexID();
-        sink = new JobVertexID();
-
-        metricsCollector =
-                new TestingMetricsCollector(
-                        new JobTopology(
-                                new VertexInfo(source1, Set.of(), 1, 720),
-                                new VertexInfo(sink, Set.of(source1), 1, 720)));
-
-        var defaultConf = new Configuration();
-        defaultConf.set(AutoScalerOptions.AUTOSCALER_ENABLED, true);
-        defaultConf.set(AutoScalerOptions.STABILIZATION_INTERVAL, Duration.ZERO);
-        defaultConf.set(AutoScalerOptions.RESTART_TIME, Duration.ofSeconds(1));
-        defaultConf.set(AutoScalerOptions.CATCH_UP_DURATION, Duration.ofSeconds(2));
-        defaultConf.set(AutoScalerOptions.SCALING_ENABLED, true);
-        defaultConf.set(AutoScalerOptions.MAX_SCALE_DOWN_FACTOR, 1.);
-        defaultConf.set(AutoScalerOptions.MAX_SCALE_UP_FACTOR, (double) Integer.MAX_VALUE);
-        defaultConf.set(AutoScalerOptions.TARGET_UTILIZATION, 0.8);
-        defaultConf.set(AutoScalerOptions.TARGET_UTILIZATION_BOUNDARY, 0.1);
-        defaultConf.set(AutoScalerOptions.SCALE_UP_GRACE_PERIOD, Duration.ZERO);
-
-        configManager = new FlinkConfigManager(defaultConf);
-        ReconciliationUtils.updateStatusForDeployedSpec(
-                app, configManager.getDeployConfig(app.getMetadata(), app.getSpec()));
-        app.getStatus().getJobStatus().setState(JobStatus.RUNNING.name());
-
-        autoscaler =
-                new JobAutoScalerImpl(
-                        kubernetesClient,
-                        metricsCollector,
-                        evaluator,
-                        scalingExecutor,
-                        new EventRecorder(kubernetesClient, eventCollector));
-
-        // Reset custom window size to default
-        metricsCollector.setTestMetricWindowSize(null);
-    }
-
-    @Test
-    public void test() throws Exception {
-        var ctx = createAutoscalerTestContext();
-
-        /* Test scaling up. */
-        var now = Instant.ofEpochMilli(0);
-        setClocksTo(now);
-        redeployJob(now);
-        // Adjust metric window size, so we can fill the metric window with two metrics
-        metricsCollector.setTestMetricWindowSize(Duration.ofSeconds(1));
-        metricsCollector.setCurrentMetrics(
-                Map.of(
-                        source1,
-                        Map.of(
-                                FlinkMetric.BUSY_TIME_PER_SEC,
-                                new AggregatedMetric("""", Double.NaN, 850., Double.NaN, Double.NaN),
-                                FlinkMetric.NUM_RECORDS_OUT_PER_SEC,
-                                new AggregatedMetric("""", Double.NaN, Double.NaN, Double.NaN, 500.),
-                                FlinkMetric.NUM_RECORDS_IN_PER_SEC,
-                                new AggregatedMetric("""", Double.NaN, Double.NaN, Double.NaN, 500.),
-                                FlinkMetric.PENDING_RECORDS,
-                                new AggregatedMetric(
-                                        """", Double.NaN, Double.NaN, Double.NaN, 2000.)),
-                        sink,
-                        Map.of(
-                                FlinkMetric.BUSY_TIME_PER_SEC,
-                                new AggregatedMetric("""", Double.NaN, 850., Double.NaN, Double.NaN),
-                                FlinkMetric.NUM_RECORDS_IN_PER_SEC,
-                                new AggregatedMetric(
-                                        """", Double.NaN, Double.NaN, Double.NaN, 500.))));
-
-        autoscaler.scale(getResourceContext(app, ctx));
-        assertEquals(
-                1, AutoScalerInfo.forResource(app, kubernetesClient).getMetricHistory().size());
-        assertFlinkMetricsCount(0, 0, ctx);
-
-        now = now.plus(Duration.ofSeconds(1));
-        setClocksTo(now);
-        autoscaler.scale(getResourceContext(app, ctx));
-        assertEquals(
-                2, AutoScalerInfo.forResource(app, kubernetesClient).getMetricHistory().size());
-
-        var scaledParallelism = ScalingExecutorTest.getScaledParallelism(app);
-        assertEquals(4, scaledParallelism.get(source1));
-        assertEquals(4, scaledParallelism.get(sink));
-        assertFlinkMetricsCount(1, 0, ctx);
-
-        /* Test stability while processing pending records. */
-
-        // Update topology to reflect updated parallelisms
-        metricsCollector.setJobTopology(
-                new JobTopology(
-                        new VertexInfo(source1, Set.of(), 4, 24),
-                        new VertexInfo(sink, Set.of(source1), 4, 720)));
-        metricsCollector.setCurrentMetrics(
-                Map.of(
-                        source1,
-                        Map.of(
-                                FlinkMetric.BUSY_TIME_PER_SEC,
-                                new AggregatedMetric("""", Double.NaN, 1000., Double.NaN, Double.NaN),
-                                FlinkMetric.NUM_RECORDS_OUT_PER_SEC,
-                                new AggregatedMetric("""", Double.NaN, Double.NaN, Double.NaN, 1800.),
-                                FlinkMetric.NUM_RECORDS_IN_PER_SEC,
-                                new AggregatedMetric("""", Double.NaN, Double.NaN, Double.NaN, 1800.),
-                                FlinkMetric.PENDING_RECORDS,
-                                new AggregatedMetric(
-                                        """", Double.NaN, Double.NaN, Double.NaN, 2500.)),
-                        sink,
-                        Map.of(
-                                FlinkMetric.BUSY_TIME_PER_SEC,
-                                new AggregatedMetric("""", Double.NaN, 1000., Double.NaN, Double.NaN),
-                                FlinkMetric.NUM_RECORDS_IN_PER_SEC,
-                                new AggregatedMetric(
-                                        """", Double.NaN, Double.NaN, Double.NaN, 1800.))));
-        now = now.plusSeconds(1);
-        setClocksTo(now);
-        // Redeploying which erases metric history
-        redeployJob(now);
-        // Adjust metric window size, so we can fill the metric window with three metrics
-        metricsCollector.setTestMetricWindowSize(Duration.ofSeconds(2));
-
-        autoscaler.scale(getResourceContext(app, ctx));
-        assertFlinkMetricsCount(1, 0, ctx);
-        assertEquals(
-                1, AutoScalerInfo.forResource(app, kubernetesClient).getMetricHistory().size());
-        scaledParallelism = ScalingExecutorTest.getScaledParallelism(app);
-        assertEquals(4, scaledParallelism.get(source1));
-        assertEquals(4, scaledParallelism.get(sink));
-
-        metricsCollector.setCurrentMetrics(
-                Map.of(
-                        source1,
-                        Map.of(
-                                FlinkMetric.BUSY_TIME_PER_SEC,
-                                new AggregatedMetric("""", Double.NaN, 1000., Double.NaN, Double.NaN),
-                                FlinkMetric.NUM_RECORDS_OUT_PER_SEC,
-                                new AggregatedMetric("""", Double.NaN, Double.NaN, Double.NaN, 1800.),
-                                FlinkMetric.NUM_RECORDS_IN_PER_SEC,
-                                new AggregatedMetric("""", Double.NaN, Double.NaN, Double.NaN, 1800.),
-                                FlinkMetric.PENDING_RECORDS,
-                                new AggregatedMetric(
-                                        """", Double.NaN, Double.NaN, Double.NaN, 1200.)),
-                        sink,
-                        Map.of(
-                                FlinkMetric.BUSY_TIME_PER_SEC,
-                                new AggregatedMetric("""", Double.NaN, 1000., Double.NaN, Double.NaN),
-                                FlinkMetric.NUM_RECORDS_IN_PER_SEC,
-                                new AggregatedMetric(
-                                        """", Double.NaN, Double.NaN, Double.NaN, 1800.))));
-
-        now = now.plus(Duration.ofSeconds(1));
-        setClocksTo(now);
-        autoscaler.scale(getResourceContext(app, ctx));
-        assertFlinkMetricsCount(1, 0, ctx);
-        assertEquals(
-                2, AutoScalerInfo.forResource(app, kubernetesClient).getMetricHistory().size());
-        scaledParallelism = ScalingExecutorTest.getScaledParallelism(app);
-        assertEquals(4, scaledParallelism.get(source1));
-        assertEquals(4, scaledParallelism.get(sink));
-
-        /* Test scaling down. */
-
-        // We have finally caught up to our original lag, time to scale down
-        metricsCollector.setCurrentMetrics(
-                Map.of(
-                        source1,
-                        Map.of(
-                                FlinkMetric.BUSY_TIME_PER_SEC,
-                                new AggregatedMetric("""", Double.NaN, 600., Double.NaN, Double.NaN),
-                                FlinkMetric.NUM_RECORDS_OUT_PER_SEC,
-                                new AggregatedMetric("""", Double.NaN, Double.NaN, Double.NaN, 800.),
-                                FlinkMetric.NUM_RECORDS_IN_PER_SEC,
-                                new AggregatedMetric("""", Double.NaN, Double.NaN, Double.NaN, 800.),
-                                FlinkMetric.PENDING_RECORDS,
-                                new AggregatedMetric("""", Double.NaN, Double.NaN, Double.NaN, 0.)),
-                        sink,
-                        Map.of(
-                                FlinkMetric.BUSY_TIME_PER_SEC,
-                                new AggregatedMetric("""", Double.NaN, 600., Double.NaN, Double.NaN),
-                                FlinkMetric.NUM_RECORDS_IN_PER_SEC,
-                                new AggregatedMetric(
-                                        """", Double.NaN, Double.NaN, Double.NaN, 800.))));
-        now = now.plus(Duration.ofSeconds(1));
-        setClocksTo(now);
-        autoscaler.scale(getResourceContext(app, ctx));
-        assertFlinkMetricsCount(2, 0, ctx);
-        assertEquals(
-                3, AutoScalerInfo.forResource(app, kubernetesClient).getMetricHistory().size());
-
-        scaledParallelism = ScalingExecutorTest.getScaledParallelism(app);
-        assertEquals(2, scaledParallelism.get(source1));
-        assertEquals(2, scaledParallelism.get(sink));
-
-        metricsCollector.setJobTopology(
-                new JobTopology(
-                        new VertexInfo(source1, Set.of(), 2, 24),
-                        new VertexInfo(sink, Set.of(source1), 2, 720)));
-
-        /* Test stability while processing backlog. */
-
-        metricsCollector.setCurrentMetrics(
-                Map.of(
-                        source1,
-                        Map.of(
-                                FlinkMetric.BUSY_TIME_PER_SEC,
-                                new AggregatedMetric("""", Double.NaN, 1000., Double.NaN, Double.NaN),
-                                FlinkMetric.NUM_RECORDS_OUT_PER_SEC,
-                                new AggregatedMetric("""", Double.NaN, Double.NaN, Double.NaN, 900.),
-                                FlinkMetric.NUM_RECORDS_IN_PER_SEC,
-                                new AggregatedMetric("""", Double.NaN, Double.NaN, Double.NaN, 900.),
-                                FlinkMetric.PENDING_RECORDS,
-                                new AggregatedMetric("""", Double.NaN, Double.NaN, Double.NaN, 500.)),
-                        sink,
-                        Map.of(
-                                FlinkMetric.BUSY_TIME_PER_SEC,
-                                new AggregatedMetric("""", Double.NaN, 1000., Double.NaN, Double.NaN),
-                                FlinkMetric.NUM_RECORDS_IN_PER_SEC,
-                                new AggregatedMetric(
-                                        """", Double.NaN, Double.NaN, Double.NaN, 900.))));
-        now = now.plus(Duration.ofSeconds(1));
-        setClocksTo(now);
-        app.getStatus().getJobStatus().setStartTime(String.valueOf(now.toEpochMilli()));
-        autoscaler.scale(getResourceContext(app, ctx));
-        assertFlinkMetricsCount(2, 1, ctx);
-        scaledParallelism = ScalingExecutorTest.getScaledParallelism(app);
-        assertEquals(2, scaledParallelism.get(source1));
-        assertEquals(2, scaledParallelism.get(sink));
-
-        metricsCollector.setCurrentMetrics(
-                Map.of(
-                        source1,
-                        Map.of(
-                                FlinkMetric.BUSY_TIME_PER_SEC,
-                                new AggregatedMetric("""", Double.NaN, 1000., Double.NaN, Double.NaN),
-                                FlinkMetric.NUM_RECORDS_OUT_PER_SEC,
-                                new AggregatedMetric("""", Double.NaN, Double.NaN, Double.NaN, 900.),
-                                FlinkMetric.NUM_RECORDS_IN_PER_SEC,
-                                new AggregatedMetric("""", Double.NaN, Double.NaN, Double.NaN, 900.),
-                                FlinkMetric.PENDING_RECORDS,
-                                new AggregatedMetric("""", Double.NaN, Double.NaN, Double.NaN, 100.)),
-                        sink,
-                        Map.of(
-                                FlinkMetric.BUSY_TIME_PER_SEC,
-                                new AggregatedMetric("""", Double.NaN, 1000., Double.NaN, Double.NaN),
-                                FlinkMetric.NUM_RECORDS_IN_PER_SEC,
-                                new AggregatedMetric(
-                                        """", Double.NaN, Double.NaN, Double.NaN, 900.))));
-        now = now.plus(Duration.ofSeconds(1));
-        setClocksTo(now);
-        autoscaler.scale(getResourceContext(app, ctx));
-        assertFlinkMetricsCount(2, 2, ctx);
-
-        scaledParallelism = ScalingExecutorTest.getScaledParallelism(app);
-        assertEquals(2, scaledParallelism.get(source1));
-        assertEquals(2, scaledParallelism.get(sink));
-
-        metricsCollector.setCurrentMetrics(
-                Map.of(
-                        source1,
-                        Map.of(
-                                FlinkMetric.BUSY_TIME_PER_SEC,
-                                new AggregatedMetric("""", Double.NaN, 500., Double.NaN, Double.NaN),
-                                FlinkMetric.NUM_RECORDS_OUT_PER_SEC,
-                                new AggregatedMetric("""", Double.NaN, Double.NaN, Double.NaN, 500.),
-                                FlinkMetric.NUM_RECORDS_IN_PER_SEC,
-                                new AggregatedMetric("""", Double.NaN, Double.NaN, Double.NaN, 500.),
-                                FlinkMetric.PENDING_RECORDS,
-                                new AggregatedMetric("""", Double.NaN, Double.NaN, Double.NaN, 0.)),
-                        sink,
-                        Map.of(
-                                FlinkMetric.BUSY_TIME_PER_SEC,
-                                new AggregatedMetric("""", Double.NaN, 500., Double.NaN, Double.NaN),
-                                FlinkMetric.NUM_RECORDS_IN_PER_SEC,
-                                new AggregatedMetric(
-                                        """", Double.NaN, Double.NaN, Double.NaN, 500.))));
-        now = now.plus(Duration.ofSeconds(1));
-        setClocksTo(now);
-        autoscaler.scale(getResourceContext(app, ctx));
-        assertFlinkMetricsCount(2, 3, ctx);
-        scaledParallelism = ScalingExecutorTest.getScaledParallelism(app);
-        assertEquals(2, scaledParallelism.get(source1));
-        assertEquals(2, scaledParallelism.get(sink));
-    }
-
-    @Test
-    public void testMetricsPersistedAfterRedeploy() {
-        var ctx = createAutoscalerTestContext();
-        var now = Instant.ofEpochMilli(0);
-        setClocksTo(now);
-        app.getStatus().getJobStatus().setUpdateTime(String.valueOf(now.toEpochMilli()));
-        metricsCollector.setCurrentMetrics(
-                Map.of(
-                        source1,
-                        Map.of(
-                                FlinkMetric.NUM_RECORDS_OUT_PER_SEC,
-                                new AggregatedMetric("""", Double.NaN, Double.NaN, Double.NaN, 500.),
-                                FlinkMetric.NUM_RECORDS_IN_PER_SEC,
-                                new AggregatedMetric("""", Double.NaN, Double.NaN, Double.NaN, 500.)),
-                        sink,
-                        Map.of(
-                                FlinkMetric.NUM_RECORDS_IN_PER_SEC,
-                                new AggregatedMetric(
-                                        """", Double.NaN, Double.NaN, Double.NaN, 500.))));
-
-        autoscaler.scale(getResourceContext(app, ctx));
-        assertFalse(AutoScalerInfo.forResource(app, kubernetesClient).getMetricHistory().isEmpty());
-    }
-
-    @Test
-    public void testEventOnError() {
-        // Invalid config
-        app.getSpec()
-                .getFlinkConfiguration()
-                .put(""kubernetes.operator.job.autoscaler.enabled"", ""3"");
-        autoscaler.scale(getResourceContext(app, createAutoscalerTestContext()));
-
-        var event = eventCollector.events.poll();
-        assertTrue(eventCollector.events.isEmpty());
-        assertEquals(EventRecorder.Reason.AutoscalerError.toString(), event.getReason());
-        assertTrue(event.getMessage().startsWith(""Could not parse""));
-    }
-
-    private void redeployJob(Instant now) {
-        app.getStatus().getJobStatus().setUpdateTime(String.valueOf(now.toEpochMilli()));
-    }
-
-    private void setClocksTo(Instant time) {
-        var clock = Clock.fixed(time, ZoneId.systemDefault());
-        metricsCollector.setClock(clock);
-        scalingExecutor.setClock(clock);
-    }
-
-    @NotNull
-    private TestUtils.TestingContext<HasMetadata> createAutoscalerTestContext() {
-        return new TestUtils.TestingContext<>() {
-            public <T1> Set<T1> getSecondaryResources(Class<T1> aClass) {
-                return (Set)
-                        kubernetesClient.configMaps().inAnyNamespace().list().getItems().stream()
-                                .collect(Collectors.toSet());
-            }
-        };
-    }
-
-    private void assertFlinkMetricsCount(
-            int scalingCount, int balancedCount, TestUtils.TestingContext<HasMetadata> ctx) {
-        AutoscalerFlinkMetrics autoscalerFlinkMetrics =
-                autoscaler.flinkMetrics.get(
-                        ResourceID.fromResource(getResourceContext(app, ctx).getResource()));
-        assertEquals(scalingCount, autoscalerFlinkMetrics.numScalings.getCount());
-        assertEquals(balancedCount, autoscalerFlinkMetrics.numBalanced.getCount());
-    }
-}","[{'comment': 'Can we keep this test class and keep the refactoring to a minimum in this feature PR?', 'commenter': 'mxm'}]"
613,flink-kubernetes-operator-autoscaler/src/test/java/org/apache/flink/kubernetes/operator/autoscaler/JobAutoScalerImplTest.java,"@@ -18,59 +18,537 @@
 package org.apache.flink.kubernetes.operator.autoscaler;
 
 import org.apache.flink.api.common.JobID;
+import org.apache.flink.api.common.JobStatus;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.kubernetes.operator.OperatorTestBase;
 import org.apache.flink.kubernetes.operator.TestUtils;
 import org.apache.flink.kubernetes.operator.api.FlinkDeployment;
+import org.apache.flink.kubernetes.operator.autoscaler.config.AutoScalerOptions;
+import org.apache.flink.kubernetes.operator.autoscaler.metrics.FlinkMetric;
+import org.apache.flink.kubernetes.operator.autoscaler.topology.JobTopology;
+import org.apache.flink.kubernetes.operator.autoscaler.topology.VertexInfo;
 import org.apache.flink.kubernetes.operator.config.FlinkConfigManager;
-import org.apache.flink.kubernetes.operator.controller.FlinkResourceContext;
 import org.apache.flink.kubernetes.operator.reconciler.ReconciliationUtils;
+import org.apache.flink.kubernetes.operator.utils.EventCollector;
+import org.apache.flink.kubernetes.operator.utils.EventRecorder;
+import org.apache.flink.runtime.jobgraph.JobVertexID;
+import org.apache.flink.runtime.rest.messages.job.metrics.AggregatedMetric;
 
+import io.fabric8.kubernetes.api.model.HasMetadata;
 import io.fabric8.kubernetes.client.KubernetesClient;
 import io.fabric8.kubernetes.client.server.mock.EnableKubernetesMockClient;
 import io.javaoperatorsdk.operator.processing.event.ResourceID;
 import lombok.Getter;
+import org.jetbrains.annotations.NotNull;
 import org.junit.jupiter.api.Assertions;
 import org.junit.jupiter.api.BeforeEach;
 import org.junit.jupiter.api.Test;
 
-import static org.apache.flink.kubernetes.operator.autoscaler.config.AutoScalerOptions.AUTOSCALER_ENABLED;
+import java.time.Clock;
+import java.time.Duration;
+import java.time.Instant;
+import java.time.ZoneId;
+import java.util.Map;
+import java.util.Set;
+import java.util.stream.Collectors;
+
 import static org.junit.jupiter.api.Assertions.assertEquals;
+import static org.junit.jupiter.api.Assertions.assertFalse;
+import static org.junit.jupiter.api.Assertions.assertNull;
+import static org.junit.jupiter.api.Assertions.assertTrue;
 
-/** Tests for JobAutoScalerImpl. */
+/** Test for {@link JobAutoScalerImpl }. */
 @EnableKubernetesMockClient(crud = true)
 public class JobAutoScalerImplTest extends OperatorTestBase {
 
     @Getter private KubernetesClient kubernetesClient;
 
+    private ScalingMetricEvaluator evaluator;
+    private TestingMetricsCollector metricsCollector;
+    private ScalingExecutor scalingExecutor;
+
     private FlinkDeployment app;
+    private JobVertexID source1, sink;
+
+    private JobAutoScalerImpl autoscaler;
+
+    private EventCollector eventCollector = new EventCollector();
 
     @BeforeEach
     public void setup() {
+        evaluator = new ScalingMetricEvaluator();
+        scalingExecutor =
+                new ScalingExecutor(
+                        kubernetesClient,
+                        new EventRecorder(kubernetesClient, new EventCollector()));
+
         app = TestUtils.buildApplicationCluster();
         app.getMetadata().setGeneration(1L);
         app.getStatus().getJobStatus().setJobId(new JobID().toHexString());
         kubernetesClient.resource(app).createOrReplace();
 
+        source1 = new JobVertexID();
+        sink = new JobVertexID();
+
+        metricsCollector =
+                new TestingMetricsCollector(
+                        new JobTopology(
+                                new VertexInfo(source1, Set.of(), 1, 720),
+                                new VertexInfo(sink, Set.of(source1), 1, 720)));
+
         var defaultConf = new Configuration();
-        defaultConf.set(AUTOSCALER_ENABLED, true);
+        defaultConf.set(AutoScalerOptions.AUTOSCALER_ENABLED, true);
+        defaultConf.set(AutoScalerOptions.STABILIZATION_INTERVAL, Duration.ZERO);
+        defaultConf.set(AutoScalerOptions.RESTART_TIME, Duration.ofSeconds(1));
+        defaultConf.set(AutoScalerOptions.CATCH_UP_DURATION, Duration.ofSeconds(2));
+        defaultConf.set(AutoScalerOptions.SCALING_ENABLED, true);
+        defaultConf.set(AutoScalerOptions.MAX_SCALE_DOWN_FACTOR, 1.);
+        defaultConf.set(AutoScalerOptions.MAX_SCALE_UP_FACTOR, (double) Integer.MAX_VALUE);
+        defaultConf.set(AutoScalerOptions.TARGET_UTILIZATION, 0.8);
+        defaultConf.set(AutoScalerOptions.TARGET_UTILIZATION_BOUNDARY, 0.1);
+        defaultConf.set(AutoScalerOptions.SCALE_UP_GRACE_PERIOD, Duration.ZERO);
+
         configManager = new FlinkConfigManager(defaultConf);
         ReconciliationUtils.updateStatusForDeployedSpec(
                 app, configManager.getDeployConfig(app.getMetadata(), app.getSpec()));
+        app.getStatus().getJobStatus().setState(JobStatus.RUNNING.name());
+
+        autoscaler =
+                new JobAutoScalerImpl(
+                        kubernetesClient,
+                        metricsCollector,
+                        evaluator,
+                        scalingExecutor,
+                        new EventRecorder(kubernetesClient, eventCollector));
+
+        // Reset custom window size to default
+        metricsCollector.setTestMetricWindowSize(null);
+    }
+
+    @Test
+    public void testBacklogBasedScaling() throws Exception {","[{'comment': ""Is this a new test or an existing one? It's hard to review when there are many unrelated changes."", 'commenter': 'mxm'}]"
613,flink-kubernetes-operator-autoscaler/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/AutoscalerFlinkMetrics.java,"@@ -54,51 +58,122 @@ public AutoscalerFlinkMetrics(MetricGroup metricGroup) {
         this.metricGroup = metricGroup;
     }
 
-    public void registerScalingMetrics(
+    public void registerEvaluatedScalingMetrics(
             Supplier<Map<JobVertexID, Map<ScalingMetric, EvaluatedScalingMetric>>>
                     currentVertexMetrics) {
         currentVertexMetrics
                 .get()
                 .forEach(
-                        (jobVertexID, evaluated) -> {
-                            if (!vertexMetrics.add(jobVertexID)) {
-                                return;
-                            }
-                            LOG.info(""Registering scaling metrics for job vertex {}"", jobVertexID);
-                            var jobVertexMg =
-                                    metricGroup.addGroup(""jobVertexID"", jobVertexID.toHexString());
-
-                            evaluated.forEach(
-                                    (sm, esm) -> {
-                                        var smGroup = jobVertexMg.addGroup(sm.name());
-
-                                        smGroup.gauge(
-                                                ""Current"",
-                                                () ->
-                                                        Optional.ofNullable(
-                                                                        currentVertexMetrics.get())
-                                                                .map(m -> m.get(jobVertexID))
-                                                                .map(
-                                                                        metrics ->
-                                                                                metrics.get(sm)
-                                                                                        .getCurrent())
-                                                                .orElse(null));
-
-                                        if (sm.isCalculateAverage()) {
-                                            smGroup.gauge(
-                                                    ""Average"",
-                                                    () ->
-                                                            Optional.ofNullable(
-                                                                            currentVertexMetrics
-                                                                                    .get())
-                                                                    .map(m -> m.get(jobVertexID))
-                                                                    .map(
-                                                                            metrics ->
-                                                                                    metrics.get(sm)
-                                                                                            .getAverage())
-                                                                    .orElse(null));
-                                        }
+                        (jobVertexID, evaluatedScalingMetrics) -> {
+                            var jobVertexMg = getJobVertexMetricGroup(jobVertexID);
+                            evaluatedScalingMetrics.forEach(
+                                    (scalingMetric, esm) -> {
+                                        var smg =
+                                                getScalingMetricGroup(
+                                                        jobVertexID, jobVertexMg, scalingMetric);
+                                        registerScalingMetric(
+                                                currentVertexMetrics,
+                                                jobVertexID,
+                                                scalingMetric,
+                                                smg);
                                     });
                         });
     }
+
+    public void registerRecommendedParallelismMetrics(","[{'comment': '(1) Special casing for the PR feature in new component', 'commenter': 'mxm'}]"
613,flink-kubernetes-operator-autoscaler/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/JobAutoScalerImpl.java,"@@ -143,4 +163,18 @@ private AutoscalerFlinkMetrics getOrInitAutoscalerFlinkMetrics(
                         new AutoscalerFlinkMetrics(
                                 ctx.getResourceMetricGroup().addGroup(""AutoScaler"")));
     }
+
+    private void resetRecommendedParallelisms(","[{'comment': '(2) Special casing for the PR feature in new component', 'commenter': 'mxm'}]"
613,flink-kubernetes-operator-autoscaler/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/ScalingExecutor.java,"@@ -269,6 +273,25 @@ private void setVertexParallelismOverrides(
         resource.getSpec().setFlinkConfiguration(flinkConf.toMap());
     }
 
+    private void updateRecommendedParallelisms(","[{'comment': '(3) Special casing for the PR feature in new component', 'commenter': 'mxm'}]"
613,flink-kubernetes-operator-autoscaler/src/test/java/org/apache/flink/kubernetes/operator/autoscaler/MetricsCollectionAndEvaluationTest.java,"@@ -68,6 +69,12 @@ public class MetricsCollectionAndEvaluationTest {
 
     private final AutoScalerInfo scalingInfo = new AutoScalerInfo(new HashMap<>());
 
+    private final Map<ResourceID, Map<JobVertexID, Integer>> lastScalingSummaries =
+            new ConcurrentHashMap<>();
+
+    private final Map<ResourceID, Map<JobVertexID, Integer>> recommendedParallelisms =
+            new ConcurrentHashMap<>();","[{'comment': 'Test is not concurrent, hence normal HashMap would suffice and would be advisable to use here.', 'commenter': 'mxm'}]"
613,flink-kubernetes-operator-autoscaler/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/ScalingMetricCollector.java,"@@ -441,6 +443,17 @@ public void cleanup(AbstractFlinkResource<?, ?> cr) {
         topologies.remove(resourceId);
     }
 
+    private void cleanupRecommendedParallelisms(","[{'comment': '(4) Special casing for the PR feature in new component', 'commenter': 'mxm'}]"
620,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/KubernetesOperatorConfigOptions.java,"@@ -244,6 +245,14 @@ public static String operatorConfigKey(String key) {
                     .withDescription(
                             ""Maximum number of throwable to be included in CR status error field."");
 
+    @Documentation.Section(SECTION_DYNAMIC)
+    public static final ConfigOption<Map<String, String>> OPERATOR_EXCEPTION_LABEL_MAPPER =
+            operatorConfig(""exception.metadata.mapper"")","[{'comment': 'The config option should also be renamed to `exception.label.mapper` and then please regenerate the docs.', 'commenter': 'gyfora'}]"
620,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/KubernetesOperatorConfigOptions.java,"@@ -244,6 +245,14 @@ public static String operatorConfigKey(String key) {
                     .withDescription(
                             ""Maximum number of throwable to be included in CR status error field."");
 
+    @Documentation.Section(SECTION_DYNAMIC)
+    public static final ConfigOption<Map<String, String>> OPERATOR_EXCEPTION_LABEL_MAPPER =
+            operatorConfig(""exception.metadata.mapper"")
+                    .mapType()
+                    .defaultValue(new HashMap<>())
+                    .withDescription(
+                            ""Key-Value pair where key is the REGEX to filter through the exception messages and value is the string to be included in CR status error metadata field if the REGEX matches. Expected format: headerKey1:headerValue1,headerKey2:headerValue2."");","[{'comment': 'Please update the description to refer to label instead of metadata', 'commenter': 'gyfora'}]"
620,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/FlinkResourceExceptionUtils.java,"@@ -29,14 +29,18 @@
 import com.fasterxml.jackson.databind.ObjectMapper;
 import org.apache.commons.lang3.exception.ExceptionUtils;
 
+import java.util.ArrayList;
+import java.util.HashMap;
 import java.util.Map;
 import java.util.Optional;
+import java.util.regex.Pattern;
 import java.util.stream.Collectors;
 
 /** Flink Resource Exception utilities. */
 public final class FlinkResourceExceptionUtils {
 
     private static final ObjectMapper objectMapper = new ObjectMapper();
+    public static final String LABEL_MAPPER_IDENTIFIER = ""labels"";","[{'comment': 'Could we simply call this constant `LABELS` or `LABELS_KEY?`', 'commenter': 'gyfora'}]"
620,helm/flink-kubernetes-operator/conf/flink-conf.yaml,"@@ -43,6 +43,7 @@ parallelism.default: 1
 # kubernetes.operator.exception.stacktrace.max.length: 2048
 # kubernetes.operator.exception.field.max.length: 2048
 # kubernetes.operator.exception.throwable.list.max.count: 2
+# kubernetes.operator.exception.metadata.mapper: Job has already been submitted:duplicatedJobFound,Server returned HTTP response code:httpResponseCodeFound","[{'comment': 'Please update this as well after renaming the config key', 'commenter': 'gyfora'}]"
620,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/FlinkResourceExceptionUtils.java,"@@ -129,26 +144,56 @@ public static Optional<String> getSubstringWithMaxLength(String str, int limit)
     private static void enrichMetadata(
             Throwable throwable,
             FlinkResourceException flinkResourceException,
-            int lengthThreshold) {
+            int lengthThreshold,
+            Map<String, String> labelMapper) {
+        if (flinkResourceException.getAdditionalMetadata() == null) {
+            flinkResourceException.setAdditionalMetadata(new HashMap<>());
+        }
+
         if (throwable instanceof RestClientException) {
-            flinkResourceException.setAdditionalMetadata(
-                    Map.of(
+            flinkResourceException
+                    .getAdditionalMetadata()
+                    .put(
                             ""httpResponseCode"",
-                            ((RestClientException) throwable).getHttpResponseStatus().code()));
+                            ((RestClientException) throwable).getHttpResponseStatus().code());
         }
 
         if (throwable instanceof DeploymentFailedException) {
             getSubstringWithMaxLength(
                             ((DeploymentFailedException) throwable).getReason(), lengthThreshold)
                     .ifPresent(
                             reason ->
-                                    flinkResourceException.setAdditionalMetadata(
-                                            Map.of(""reason"", reason)));
+                                    flinkResourceException
+                                            .getAdditionalMetadata()
+                                            .put(""reason"", reason));
         }
 
+        labelMapper
+                .entrySet()
+                .forEach(
+                        (entry) -> {
+                            Pattern pattern = Pattern.compile(entry.getKey());
+
+                            org.apache.flink.util.ExceptionUtils.findThrowable(
+                                            throwable, t -> pattern.matcher(t.getMessage()).find())","[{'comment': 'Can you please add a test for Exceptions with null message? (such as a nullpointer) I think this code would fail', 'commenter': 'gyfora'}, {'comment': 'Hi Gyula, thanks for catching this, I have fixed the code and added the test, thank you', 'commenter': 'darenwkt'}]"
621,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/NativeFlinkService.java,"@@ -93,6 +93,12 @@ protected PodList getJmPodList(String namespace, String clusterId) {
                 .list();
     }
 
+    @Override
+    protected PodList getTmPodList(String namespace, String clusterId) {
+        // Native mode does not manage TaskManager
+        return null;","[{'comment': 'Maybe return empty PodList to save the null check in the service?', 'commenter': 'gyfora'}]"
621,flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/service/StandaloneFlinkServiceTest.java,"@@ -99,9 +99,12 @@ public void testDeleteClusterDeploymentWithHADelete() throws Exception {
         List<Deployment> deployments = kubernetesClient.apps().deployments().list().getItems();
         assertEquals(2, deployments.size());
 
+        var requestsBeforeDelete = mockServer.getRequestCount();
         flinkStandaloneService.deleteClusterDeployment(
                 flinkDeployment.getMetadata(), flinkDeployment.getStatus(), configuration, true);
 
+        assertEquals(6, mockServer.getRequestCount() - requestsBeforeDelete);","[{'comment': ""I don't really feel that this adequately tests the functionality. We should make sure that the deletion is blocked until the service returns empty list right?"", 'commenter': 'gyfora'}]"
621,flink-kubernetes-operator/pom.xml,"@@ -198,6 +198,13 @@ under the License.
             <artifactId>junit-jupiter-params</artifactId>
             <scope>test</scope>
         </dependency>
+
+        <dependency>
+            <groupId>org.mockito</groupId>
+            <artifactId>mockito-core</artifactId>
+            <version>${mockito.version}</version>
+            <scope>test</scope>
+        </dependency>","[{'comment': 'Please avoid using Mockito as per the Flink coding guidelines: https://flink.apache.org/how-to-contribute/code-style-and-quality-common/#avoid-mockito---use-reusable-test-implementations\r\n\r\nI suggest overriding the getPodList method of the standalone flink service by hand or using the TestingFlinkService.', 'commenter': 'gyfora'}]"
622,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/ReconciliationUtils.java,"@@ -281,6 +284,17 @@ public static boolean isUpgradeModeChangedToLastStateAndHADisabledPreviously(
                 && !HighAvailabilityMode.isHighAvailabilityModeActivated(observeConfig);
     }
 
+    public static <SPEC extends AbstractFlinkSpec> SPEC getLastSpec(
+            AbstractFlinkResource<SPEC, ?> deployment) {
+        var reconciliationStatus = deployment.getStatus().getReconciliationStatus();
+        var reconciliationState = reconciliationStatus.getState();
+        if (reconciliationState != ReconciliationState.ROLLED_BACK) {
+            return reconciliationStatus.deserializeLastReconciledSpec();
+        } else {
+            return reconciliationStatus.deserializeLastRollbackSpec();","[{'comment': 'We should use `lastStableSpec` and avoid introducing a new status field. The lastStableSpec is meant exactly for rollback purposes.', 'commenter': 'gyfora'}, {'comment': ""I rely on the `lastStableSpec` to do the rollback.\r\nIt is indeed the spec used to roll back to a working state.\r\nBut once the job is rolled back I need to be able to detect changes to the current spec (the one which has lead to the rollback). As the job spec has not been updated we need to be able to compare it with a specific stored spec from which we will be able to do comparison and find changes\r\nI can't rely on the `lastReconcileSpec` or `lastStableSpec` as they are not align with the job spec.\r\nThis is why I introduce this `lastRollbackSpec` which contains the job spec leading to failure."", 'commenter': 'ashangit'}, {'comment': 'You should always compare to the `lastReconciledSpec` I think that was the last spec that was reconciled and submitted by the user.', 'commenter': 'gyfora'}, {'comment': 'As long as you do not update the `lastReconciledSpec` during the rollback the logic should be good', 'commenter': 'gyfora'}, {'comment': 'The lastReconcileSpec is updated now as I rely on same code path than the upgrade one.\r\nI will take a look to not update it but it looks quite strange to me to finally not have the lastReconcile being aligned with the ""current"" running spec', 'commenter': 'ashangit'}, {'comment': '`lastReconciled` spec is not `current` spec. Current spec is already in the spec, lastReconciled is what was last reconciled by the operator based on what the user provided. Its sole purpose is to be able to do the above mentioned comparison. We should try not to change the semantics if possible.', 'commenter': 'gyfora'}]"
622,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/controller/FlinkDeploymentController.java,"@@ -140,6 +141,12 @@ public UpdateControl<FlinkDeployment> reconcile(FlinkDeployment flinkApp, Contex
                         previousDeployment,
                         false);
             }
+            // Rely on the last stable spec if rolling back
+            if (flinkApp.getStatus().getReconciliationStatus().getState()
+                    == ReconciliationState.ROLLING_BACK) {
+                flinkApp.setSpec(
+                        flinkApp.getStatus().getReconciliationStatus().deserializeLastStableSpec());","[{'comment': 'We might want to do a spec diff check there with the current spec vs the last reconciled spec. It could happen that we initiated the rollback (setting the state to ROLLING_BACK) and in the meantime the user actually submitted a new spec.\r\n\r\nWith the current logic the new spec would be first ignored and we perform a rollback and after that we reconcile the new spec. This is not entirely incorrect but it will lead to extra time and extra redeployments.', 'commenter': 'gyfora'}, {'comment': ""I've updated the PR to take in account this case.\r\n\r\nI also set the state to Upgrading if the there is a spec change as we don't want to apply specific code path related to roll back mechanism: https://github.com/apache/flink-kubernetes-operator/pull/622/files#diff-29ea38a50cac5b4432dd0969bc3e2177e29a5507f8c7bb01b80f605a8740de41R146\r\n\r\nI had here to somehow exclude a specific spec diff when it is due to job.state as we can override this job.state in https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/ReconciliationUtils.java#L358"", 'commenter': 'ashangit'}, {'comment': 'My change is bad for now there are still some cases with job.state diff and some usually ignore changed (for ex on `kubernetes.operator.deployment.rollback.enabled`) that are seen as an upgrade change now and lead to an upgrade instead of roll abck', 'commenter': 'ashangit'}, {'comment': 'I will take a look, it could be related to the latest changes we have made to the master regarding the rescaling logic.', 'commenter': 'gyfora'}, {'comment': ""For this issue it is clearly on my side.\r\nI should have found a good way to detect the case I'm currently rebasing from last master (few conflict to resolve) and will push it"", 'commenter': 'ashangit'}, {'comment': 'Rebased + updated with [working solution](https://github.com/apache/flink-kubernetes-operator/pull/622/files#diff-29ea38a50cac5b4432dd0969bc3e2177e29a5507f8c7bb01b80f605a8740de41R157)', 'commenter': 'ashangit'}, {'comment': ""I'm wondering if this is safe to decide to go to upgrade mode if we are rolling back.\r\n\r\nFor ex. if we initiate roll back with HA metadata not having being recreated we will have the job in not `RUNNING` state (should be in `RECONCILING` state). The operator will failed to deploy as the HA metadata won't be available and job won't be running (https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/ApplicationReconciler.java#L88)"", 'commenter': 'ashangit'}, {'comment': 'The getAvailableUpgrade mode method would only return save point if the job is either already shut down / terminally failed or is running and can be upgraded with the save point. So if it returns SAVEPOINT it should be safe to call cancel with savepoint.', 'commenter': 'gyfora'}, {'comment': 'You are right in a rollback scenario the job is definitely not in running. So the only time we will get back save point if there is no JM meta and then the getAvailableUpgradeMode will actually first delete the JM deployment and return SAVEPOINT', 'commenter': 'gyfora'}, {'comment': 'so I believe we should not need any additional logic to change what was returned ', 'commenter': 'gyfora'}]"
622,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/ReconciliationUtils.java,"@@ -125,12 +126,19 @@ private static <SPEC extends AbstractFlinkSpec> void updateStatusForSpecReconcil
         // Clear errors
         status.setError(null);
         reconciliationStatus.setReconciliationTimestamp(clock.instant().toEpochMilli());
-        reconciliationStatus.setState(
-                upgrading ? ReconciliationState.UPGRADING : ReconciliationState.DEPLOYED);
+        var state = upgrading ? ReconciliationState.UPGRADING : ReconciliationState.DEPLOYED;
+        if (status.getReconciliationStatus().getState() == ReconciliationState.ROLLING_BACK) {
+            state = upgrading ? ReconciliationState.ROLLING_BACK : ReconciliationState.ROLLED_BACK;
+        }
+        reconciliationStatus.setState(state);
 
+        var clonedSpec = ReconciliationUtils.clone(spec);
+        if (status.getReconciliationStatus().getState() == ReconciliationState.ROLLING_BACK
+                || status.getReconciliationStatus().getState() == ReconciliationState.ROLLED_BACK) {
+            clonedSpec = reconciliationStatus.deserializeLastReconciledSpec();
+        }","[{'comment': 'Both of these if branches have some redundant code, setting the state/clonedSpec variables twice instead of once in case of redeployments. ', 'commenter': 'gyfora'}]"
622,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/AbstractJobReconciler.java,"@@ -113,9 +113,22 @@ protected boolean reconcileSpecChange(FlinkResourceContext<CR> ctx, Configuratio
                     EventRecorder.Reason.Suspended,
                     EventRecorder.Component.JobManagerDeployment,
                     MSG_SUSPENDED);
+
+            UpgradeMode upgradeMode = availableUpgradeMode.getUpgradeMode().get();
+
             // We must record the upgrade mode used to the status later
-            currentDeploySpec.getJob().setUpgradeMode(availableUpgradeMode.getUpgradeMode().get());
-            cancelJob(ctx, availableUpgradeMode.getUpgradeMode().get());
+            currentDeploySpec.getJob().setUpgradeMode(upgradeMode);
+
+            // We must use LAST_STATE mode when rolling back from
+            // SAVEPOINT. But we don't want to set upgrade mode to LAST_STATE
+            // as we will rely on SAVEPOINT restoreJob mechanism
+            if (upgradeMode == UpgradeMode.SAVEPOINT
+                    && status.getReconciliationStatus().getState()
+                            == ReconciliationState.ROLLING_BACK) {
+                upgradeMode = UpgradeMode.LAST_STATE;
+            }","[{'comment': ""Can you please explain what's going on here? I don't really understand why we change from savepoint -> last-state for the cancel operation specifically."", 'commenter': 'gyfora'}, {'comment': ""From discussion we had in https://issues.apache.org/jira/browse/FLINK-32012 we decided that it would not be fine to call cancel operation with `SAVEPOINT` upgradeMode as we could not take a savepoint and don't want to remove the HA metadata\r\n\r\nDo I miss something?"", 'commenter': 'ashangit'}]"
622,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/AbstractFlinkResourceReconciler.java,"@@ -138,14 +138,32 @@ public void reconcile(FlinkResourceContext<CR> ctx) throws Exception {
                 DiffType.IGNORE != diffType
                         || reconciliationStatus.getState() == ReconciliationState.UPGRADING;
 
+        if (!specChanged && reconciliationStatus.getState() == ReconciliationState.ROLLING_BACK) {
+            // Rely on the last stable spec if rolling back and no change in the spec
+            cr.setSpec(cr.getStatus().getReconciliationStatus().deserializeLastStableSpec());
+            specChanged = true;
+        } else if (specChanged
+                && reconciliationStatus.getState() == ReconciliationState.ROLLING_BACK) {
+            // Spec has changed while rolling back we should apply new spec and move to upgrading
+            // state
+            // Don't take in account changes on job.state as it could be overriden to running if the
+            // current spec is not valid
+            lastReconciledSpec.getJob().setState(currentDeploySpec.getJob().getState());
+            var specDiffRollingBack =
+                    new ReflectiveDiffBuilder<>(
+                                    ctx.getDeploymentMode(), lastReconciledSpec, currentDeploySpec)
+                            .build();
+            if (DiffType.IGNORE != specDiffRollingBack.getType()) {
+                reconciliationStatus.setState(ReconciliationState.UPGRADING);
+            }
+        }","[{'comment': 'It would be great to encapsulate this into a method that mutates the CR/status fields and returns the value of specChanged variable that we should set afterwards. \r\n\r\nIt could be something like:\r\n\r\n```\r\nif (reconciliationStatus.getState() == ReconciliationState.ROLLING_BACK) {\r\n  specChanged = prepareCrForRollback(...)\r\n}\r\n```\r\n', 'commenter': 'gyfora'}]"
625,flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/utils/FlinkUtilsTest.java,"@@ -86,6 +89,43 @@ public void testMergePods() {
         assertEquals(pod2.getSpec().getContainers(), mergedPod.getSpec().getContainers());
     }
 
+    @Test
+    public void testAddStartupProbe() {
+        Pod pod = new Pod();
+        FlinkUtils.addStartupProbe(pod);
+
+        Probe expectedProbe = new Probe();
+        expectedProbe.setPeriodSeconds(1);
+        expectedProbe.setFailureThreshold(Integer.MAX_VALUE);
+        expectedProbe.setHttpGet(new HTTPGetAction());
+        expectedProbe.getHttpGet().setPort(new IntOrString(""rest""));
+        expectedProbe.getHttpGet().setPath(""/config"");
+
+        assertEquals(1, pod.getSpec().getContainers().size());
+        assertEquals(Constants.MAIN_CONTAINER_NAME, pod.getSpec().getContainers().get(0).getName());
+        assertEquals(expectedProbe, pod.getSpec().getContainers().get(0).getStartupProbe());
+
+        FlinkUtils.addStartupProbe(pod);
+
+        assertEquals(1, pod.getSpec().getContainers().size());
+        assertEquals(Constants.MAIN_CONTAINER_NAME, pod.getSpec().getContainers().get(0).getName());
+        assertEquals(expectedProbe, pod.getSpec().getContainers().get(0).getStartupProbe());
+
+        // Custom startup probe
+        pod.getSpec().getContainers().get(0).setStartupProbe(new Probe());
+        assertEquals(1, pod.getSpec().getContainers().size());","[{'comment': ""Don't you need to call `FlinkUtils.addStartupProbe(pod);` after adding this startup probe and then indeed check if the startup probe has not been overwritten?"", 'commenter': 'ashangit'}, {'comment': 'good catch :) ', 'commenter': 'gyfora'}]"
634,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/KubernetesOperatorConfigOptions.java,"@@ -525,4 +525,12 @@ public static String operatorConfigKey(String key) {
                     .enumType(DeletionPropagation.class)
                     .defaultValue(DeletionPropagation.FOREGROUND)
                     .withDescription(""JM/TM Deployment deletion propagation."");
+
+    @Documentation.Section(SECTION_DYNAMIC)
+    public static final ConfigOption<Boolean> SAVEPOINT_ON_DELETION =
+            operatorConfig(""savepoint.on.deletion"")","[{'comment': 'We did not manage to have very consistent config names so far but maybe `job.savepoint-on-deletion` would be a bit better :) ', 'commenter': 'gyfora'}]"
634,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/AbstractJobReconciler.java,"@@ -292,6 +294,14 @@ protected void resubmitJob(FlinkResourceContext<CR> ctx, boolean requireHaMetada
         restoreJob(ctx, specToRecover, ctx.getObserveConfig(), requireHaMetadata);
     }
 
+    protected void cancelJob(FlinkResourceContext<CR> ctx) throws Exception {
+        UpgradeMode upgradeMode =
+                configManager.getOperatorConfiguration().isSavepointOnDeletion()","[{'comment': ""I have recently reworked the operatorConfig handling to allow for more flexible default value handling. The ctx now provides a method to get the operator config directly so you don't need the configmanager.\r\n\r\n`ctx.getOperatorConfig()`"", 'commenter': 'gyfora'}]"
634,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/AbstractJobReconciler.java,"@@ -292,6 +294,14 @@ protected void resubmitJob(FlinkResourceContext<CR> ctx, boolean requireHaMetada
         restoreJob(ctx, specToRecover, ctx.getObserveConfig(), requireHaMetadata);
     }
 
+    protected void cancelJob(FlinkResourceContext<CR> ctx) throws Exception {","[{'comment': 'Do we need to introduce a new method? I would prefer to keep the existing ones because the third method with the same name is getting a bit confusing', 'commenter': 'gyfora'}]"
648,flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/reconciler/deployment/ApplicationReconcilerTest.java,"@@ -1070,4 +1070,33 @@ public void testUpgradeReconciledGeneration() throws Exception {
                         .getMetadata()
                         .getGeneration());
     }
+
+    @ParameterizedTest
+    @MethodSource(""org.apache.flink.kubernetes.operator.TestUtils#flinkVersions"")
+    public void testSubmitAndDrainOnCleanUpWithSavepoint(FlinkVersion flinkVersion)
+            throws Exception {
+        var conf = configManager.getDefaultConfig();
+        conf.set(KubernetesOperatorConfigOptions.SAVEPOINT_ON_DELETION, true);
+        conf.set(KubernetesOperatorConfigOptions.DRAIN_ON_SAVEPOINT_DELETION, true);
+        configManager.updateDefaultConfig(conf);
+
+        FlinkDeployment deployment = TestUtils.buildApplicationCluster(flinkVersion);
+
+        // session ready
+        reconciler.reconcile(deployment, TestUtils.createContextWithReadyFlinkDeployment());
+        verifyAndSetRunningJobsToStatus(deployment, flinkService.listJobs());
+
+        // clean up
+        assertEquals(
+                null, deployment.getStatus().getJobStatus().getSavepointInfo().getLastSavepoint());
+        reconciler.cleanup(deployment, TestUtils.createContextWithReadyFlinkDeployment());
+        assertEquals(
+                ""savepoint_0"",
+                deployment
+                        .getStatus()
+                        .getJobStatus()
+                        .getSavepointInfo()
+                        .getLastSavepoint()
+                        .getLocation());
+    }","[{'comment': ""These tests you added don't actually cover that draining was used (the proper client method was called).\r\n\r\nI suggest removing what you added and replacing with 2 tests in the `AbstractFlinkServiceTests` there you can test the changed service methods directly and make sure that the Flink client is called with the proper argument."", 'commenter': 'gyfora'}, {'comment': 'Replaced unit tests to AbstractFlinkServiceTests', 'commenter': 'mananmangal'}]"
648,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/FlinkOperatorConfiguration.java,"@@ -185,6 +186,9 @@ public static FlinkOperatorConfiguration fromConfiguration(Configuration operato
         boolean savepointOnDeletion =
                 operatorConfig.get(KubernetesOperatorConfigOptions.SAVEPOINT_ON_DELETION);
 
+        boolean drainJobOnSavepointDeletion =
+                operatorConfig.get(KubernetesOperatorConfigOptions.DRAIN_ON_SAVEPOINT_DELETION);","[{'comment': 'Similar to this fix PR: https://github.com/apache/flink-kubernetes-operator/pull/659\r\n\r\nThese configs should not be part of the `FlinkOperatorConfiguration` which will prevent it for setting it for a per-resource level. It should be accessed from the observeConfig directly.', 'commenter': 'gyfora'}, {'comment': 'Got it!\r\nUpdated the PR to reflect these changes.', 'commenter': 'mananmangal'}]"
648,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/KubernetesOperatorConfigOptions.java,"@@ -566,4 +566,12 @@ public static String operatorConfigKey(String key) {
                     .defaultValue(false)
                     .withDescription(
                             ""Indicate whether a savepoint must be taken when deleting a FlinkDeployment or FlinkSessionJob."");
+
+    @Documentation.Section(SECTION_DYNAMIC)
+    public static final ConfigOption<Boolean> DRAIN_ON_SAVEPOINT_DELETION =
+            operatorConfig(""job.drain-on-savepoint-deletion"")
+                    .booleanType()
+                    .defaultValue(false)
+                    .withDescription(
+                            ""Indicate whether a job should be drained before deleting a FlinkDeployment or FlinkSessionJob, only if savepoint on deletion is enabled."");","[{'comment': '`Indicate whether a job should be drained before deleting a FlinkDeployment or FlinkSessionJob, only if savepoint on deletion is enabled.` -> `Indicate whether the job should be drained when stopping with savepoint.`', 'commenter': 'gyfora'}, {'comment': 'updated the description.', 'commenter': 'mananmangal'}]"
655,examples/autoscaling/src/main/java/autoscaling/LoadSimulationPipeline.java,"@@ -0,0 +1,184 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package autoscaling;
+
+import org.apache.flink.api.common.functions.RichFlatMapFunction;
+import org.apache.flink.api.java.utils.ParameterTool;
+import org.apache.flink.streaming.api.datastream.DataStream;
+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
+import org.apache.flink.streaming.api.functions.sink.DiscardingSink;
+import org.apache.flink.streaming.api.functions.source.SourceFunction;
+import org.apache.flink.util.Collector;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.time.Duration;
+import java.time.temporal.ChronoUnit;
+
+/**
+ * Example pipeline which simulates load fluctuating load from zero to a defined max, and
+ * vice-versa. The goal is to simulate a fluctuating traffic pattern which traverses all possible
+ * stages between peak and zero load. The load curve is computed using a sine function.
+ *
+ * <p>The pipeline has defaults but can be parameterized as follows:
+ *
+ * <pre>
+ *  repeatsAfterMs => The period length after which the initial load will be reached again.
+ *  maxLoadPerTask => Each task's max load is presented by a double which is similar to the Unix CPU load
+ *                    in the sense that at least maxLoad amount of subtasks are needed to sustain the load.
+ *                    For example, a max load of 1 represents 100% load on a single subtask, 50% load on two subtasks.
+ *                    Similarly, a max load of 2 represents 100% load on two tasks, 50% load on 4 subtasks.
+ *
+ *                    Multiple tasks and branches can be defined to test Flink Autoscaling. The format is as follows:
+ *                       maxLoadTask1Branch1[;maxLoadTask2Branch1...[\n maxLoadTask1Branch2[;maxLoadTask2Branch2...]...]
+ *
+ *                    A concrete example: ""1;2;4\n4;2;1""
+ *                      Two branches are created with three tasks each. On the first branch, the tasks have
+ *                      a load of 1, 2, and 3 respectively. On the second branch, the tasks have the load reversed.
+ *                      This means, that at peak Flink Autoscaling at target utilization of 0.5, the parallelisms of
+ *                      the tasks will be 2, 4, 8 for branch one and vise-versa for branch two.
+ * </pre>
+ */
+public class LoadSimulationPipeline {
+
+    private static final Logger LOG = LoggerFactory.getLogger(LoadSimulationPipeline.class);
+
+    public static void main(String[] args) throws Exception {
+        var env = StreamExecutionEnvironment.getExecutionEnvironment();
+        env.disableOperatorChaining();
+
+        var arguments = ParameterTool.fromArgs(args);
+        String maxLoadPerTask =
+                arguments.get(""maxLoadPerTask"", ""1;2;4;8;16;\n16;8;4;2;1\n8;4;16;1;2"");
+        long repeatsAfterMs =
+                Duration.ofMinutes(arguments.getLong(""repeatsAfterMinutes"", 60)).toMillis();
+        int samplingIntervalMs = arguments.getInt(""samplingIntervalMs"", 1_000);
+
+        for (String branch : maxLoadPerTask.split(""\n"")) {
+            String[] taskLoads = branch.split("";"");
+
+            DataStream<Long> stream =
+                    env.addSource(new ImpulseSource(samplingIntervalMs)).name(""ImpulseSource"");
+
+            for (String load : taskLoads) {
+                double maxLoad = Double.parseDouble(load);
+                stream =
+                        stream.shuffle()
+                                .flatMap(
+                                        new LoadSimulationFn(
+                                                maxLoad, repeatsAfterMs, samplingIntervalMs))
+                                .name(""MaxLoad: "" + maxLoad)
+                                .broadcast();
+            }
+
+            stream.addSink(new DiscardingSink<>());
+        }
+
+        env.execute(
+                ""Load Simulation (repeats after ""
+                        + Duration.of(repeatsAfterMs, ChronoUnit.MILLIS)
+                        + "")"");
+    }
+
+    private static class ImpulseSource implements SourceFunction<Long> {
+        private final int maxSleepTimeMs;
+        boolean canceled;","[{'comment': 'has to be `volatile`', 'commenter': 'gyfora'}, {'comment': ""I'm not sure about that. This will still stop, there can be a small delay though."", 'commenter': 'mxm'}, {'comment': 'Without volatile itâ€™s not guaranteed to ever stop I think ', 'commenter': 'gyfora'}]"
655,examples/autoscaling/src/main/java/autoscaling/LoadSimulationPipeline.java,"@@ -0,0 +1,184 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package autoscaling;
+
+import org.apache.flink.api.common.functions.RichFlatMapFunction;
+import org.apache.flink.api.java.utils.ParameterTool;
+import org.apache.flink.streaming.api.datastream.DataStream;
+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
+import org.apache.flink.streaming.api.functions.sink.DiscardingSink;
+import org.apache.flink.streaming.api.functions.source.SourceFunction;
+import org.apache.flink.util.Collector;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.time.Duration;
+import java.time.temporal.ChronoUnit;
+
+/**
+ * Example pipeline which simulates load fluctuating load from zero to a defined max, and
+ * vice-versa. The goal is to simulate a fluctuating traffic pattern which traverses all possible
+ * stages between peak and zero load. The load curve is computed using a sine function.
+ *
+ * <p>The pipeline has defaults but can be parameterized as follows:
+ *
+ * <pre>
+ *  repeatsAfterMs => The period length after which the initial load will be reached again.
+ *  maxLoadPerTask => Each task's max load is presented by a double which is similar to the Unix CPU load
+ *                    in the sense that at least maxLoad amount of subtasks are needed to sustain the load.
+ *                    For example, a max load of 1 represents 100% load on a single subtask, 50% load on two subtasks.
+ *                    Similarly, a max load of 2 represents 100% load on two tasks, 50% load on 4 subtasks.
+ *
+ *                    Multiple tasks and branches can be defined to test Flink Autoscaling. The format is as follows:
+ *                       maxLoadTask1Branch1[;maxLoadTask2Branch1...[\n maxLoadTask1Branch2[;maxLoadTask2Branch2...]...]
+ *
+ *                    A concrete example: ""1;2;4\n4;2;1""
+ *                      Two branches are created with three tasks each. On the first branch, the tasks have
+ *                      a load of 1, 2, and 3 respectively. On the second branch, the tasks have the load reversed.
+ *                      This means, that at peak Flink Autoscaling at target utilization of 0.5, the parallelisms of
+ *                      the tasks will be 2, 4, 8 for branch one and vise-versa for branch two.
+ * </pre>
+ */
+public class LoadSimulationPipeline {
+
+    private static final Logger LOG = LoggerFactory.getLogger(LoadSimulationPipeline.class);
+
+    public static void main(String[] args) throws Exception {
+        var env = StreamExecutionEnvironment.getExecutionEnvironment();
+        env.disableOperatorChaining();
+
+        var arguments = ParameterTool.fromArgs(args);
+        String maxLoadPerTask =
+                arguments.get(""maxLoadPerTask"", ""1;2;4;8;16;\n16;8;4;2;1\n8;4;16;1;2"");
+        long repeatsAfterMs =
+                Duration.ofMinutes(arguments.getLong(""repeatsAfterMinutes"", 60)).toMillis();
+        int samplingIntervalMs = arguments.getInt(""samplingIntervalMs"", 1_000);
+
+        for (String branch : maxLoadPerTask.split(""\n"")) {
+            String[] taskLoads = branch.split("";"");
+
+            DataStream<Long> stream =
+                    env.addSource(new ImpulseSource(samplingIntervalMs)).name(""ImpulseSource"");
+
+            for (String load : taskLoads) {
+                double maxLoad = Double.parseDouble(load);
+                stream =
+                        stream.shuffle()
+                                .flatMap(
+                                        new LoadSimulationFn(
+                                                maxLoad, repeatsAfterMs, samplingIntervalMs))
+                                .name(""MaxLoad: "" + maxLoad)
+                                .broadcast();
+            }
+
+            stream.addSink(new DiscardingSink<>());
+        }
+
+        env.execute(
+                ""Load Simulation (repeats after ""
+                        + Duration.of(repeatsAfterMs, ChronoUnit.MILLIS)
+                        + "")"");
+    }
+
+    private static class ImpulseSource implements SourceFunction<Long> {
+        private final int maxSleepTimeMs;
+        boolean canceled;
+
+        public ImpulseSource(int samplingInterval) {
+            this.maxSleepTimeMs = samplingInterval / 10;
+        }
+
+        @Override
+        public void run(SourceContext<Long> sourceContext) throws Exception {
+            while (!canceled) {
+                sourceContext.collect(42L);","[{'comment': 'should collect under checkpoint lock', 'commenter': 'gyfora'}, {'comment': 'The lock is there to ensure correctness while checkpointing. What is being checkpointed? This is not stateful function, neither are the other operators.', 'commenter': 'mxm'}, {'comment': 'If checkpointing is off this doesnâ€™t matter much ', 'commenter': 'gyfora'}]"
655,examples/autoscaling/src/main/java/autoscaling/LoadSimulationPipeline.java,"@@ -0,0 +1,184 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package autoscaling;
+
+import org.apache.flink.api.common.functions.RichFlatMapFunction;
+import org.apache.flink.api.java.utils.ParameterTool;
+import org.apache.flink.streaming.api.datastream.DataStream;
+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
+import org.apache.flink.streaming.api.functions.sink.DiscardingSink;
+import org.apache.flink.streaming.api.functions.source.SourceFunction;
+import org.apache.flink.util.Collector;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.time.Duration;
+import java.time.temporal.ChronoUnit;
+
+/**
+ * Example pipeline which simulates load fluctuating load from zero to a defined max, and","[{'comment': '`load fluctuating load` -> `fluctuating load`', 'commenter': 'gyfora'}]"
655,examples/autoscaling/autoscaling-canary.yaml,"@@ -0,0 +1,56 @@
+################################################################################
+#  Licensed to the Apache Software Foundation (ASF) under one
+#  or more contributor license agreements.  See the NOTICE file
+#  distributed with this work for additional information
+#  regarding copyright ownership.  The ASF licenses this file
+#  to you under the Apache License, Version 2.0 (the
+#  ""License""); you may not use this file except in compliance
+#  with the License.  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an ""AS IS"" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+# limitations under the License.
+################################################################################
+
+apiVersion: flink.apache.org/v1beta1
+kind: FlinkDeployment
+metadata:
+  name: autoscaling-canary","[{'comment': 'I wonder if the name canary may be confused by the actual canary functionality in the operator. Maybe we could simply name it example2 or periodic load or something like that.', 'commenter': 'gyfora'}, {'comment': 'Sure, we can rename to `autoscaling-dynamic`.', 'commenter': 'mxm'}]"
655,examples/autoscaling/autoscaling-canary.yaml,"@@ -0,0 +1,56 @@
+################################################################################
+#  Licensed to the Apache Software Foundation (ASF) under one
+#  or more contributor license agreements.  See the NOTICE file
+#  distributed with this work for additional information
+#  regarding copyright ownership.  The ASF licenses this file
+#  to you under the Apache License, Version 2.0 (the
+#  ""License""); you may not use this file except in compliance
+#  with the License.  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an ""AS IS"" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+# limitations under the License.
+################################################################################
+
+apiVersion: flink.apache.org/v1beta1
+kind: FlinkDeployment
+metadata:
+  name: autoscaling-canary
+spec:
+  image: autoscaling-example
+  flinkVersion: v1_18
+  flinkConfiguration:
+    kubernetes.operator.job.autoscaler.enabled: ""true""
+    kubernetes.operator.job.autoscaler.stabilization.interval: ""1m""
+    kubernetes.operator.job.autoscaler.metrics.window: ""15m""
+    kubernetes.operator.job.autoscaler.target.utilization: ""0.5""
+    kubernetes.operator.job.autoscaler.target.utilization.boundary: ""0.3""
+    pipeline.max-parallelism: ""32""","[{'comment': 'should we enable the adaptive scheduler?', 'commenter': 'gyfora'}, {'comment': 'We can but it is not essential for this pipeline.', 'commenter': 'mxm'}]"
662,flink-kubernetes-operator-api/src/main/java/org/apache/flink/kubernetes/operator/api/listener/FlinkResourceListener.java,"@@ -45,8 +44,6 @@ public interface FlinkResourceListener extends Plugin {
     interface ResourceContext<R extends AbstractFlinkResource<?, ?>> {
         R getFlinkResource();
 
-        KubernetesClient getKubernetesClient();","[{'comment': 'I think we should not touch this here, I think the FlinkResourceListener interfaces should be reworked but probably not as part of this ticket.', 'commenter': 'gyfora'}, {'comment': 'Thanks for the comment, have addressed it in latest commit', 'commenter': 'darenwkt'}]"
672,flink-kubernetes-operator-autoscaler/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/JobAutoScalerImpl.java,"@@ -74,6 +76,36 @@ public JobAutoScalerImpl(
         this.infoManager = new AutoscalerInfoManager();
     }
 
+    @Override
+    public void scale(FlinkResourceContext<?> ctx) {
+        var conf = ctx.getObserveConfig();
+        var resource = ctx.getResource();
+        var resourceId = ResourceID.fromResource(resource);
+        var autoscalerMetrics = getOrInitAutoscalerFlinkMetrics(ctx, resourceId);
+
+        try {
+            if (resource.getSpec().getJob() == null || !conf.getBoolean(AUTOSCALER_ENABLED)) {
+                LOG.debug(""Autoscaler is disabled"");
+                return;
+            }
+
+            // Initialize metrics only if autoscaler is enabled
+            var status = resource.getStatus();
+            if (status.getLifecycleState() != ResourceLifecycleState.STABLE
+                    || !status.getJobStatus().getState().equals(JobStatus.RUNNING.name())) {
+                LOG.info(""Autoscaler is waiting for RUNNING job state"");
+                lastEvaluatedMetrics.remove(resourceId);
+                return;
+            }
+
+            updateParallelismOverrides(ctx, conf, resource, resourceId, autoscalerMetrics);
+        } catch (Throwable e) {
+            onError(ctx, resource, autoscalerMetrics, e);
+        } finally {
+            applyParallelismOverrides(ctx);","[{'comment': 'At first sight, this looks like the overrides will get applied, even if the autoscaler is disabled. There is another check though that prevents this here: https://github.com/apache/flink-kubernetes-operator/pull/672/files?diff=unified&w=1#diff-7df0c6b50a32c0055e6a1dcfcf9ab25cddb2a245b2125119fd9b57d65918698dR128 (line 128)\r\n\r\nA bit confusing. See other comment line 88.', 'commenter': 'mxm'}, {'comment': "" I will move the disabled check in front of the try block which will clear the overrides and simply return. That way we won't actually call this method if it's disabled."", 'commenter': 'gyfora'}, {'comment': 'good point', 'commenter': 'gyfora'}]"
672,flink-kubernetes-operator-autoscaler/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/JobAutoScalerImpl.java,"@@ -140,86 +171,73 @@ public void applyParallelismOverrides(FlinkResourceContext<?> ctx) {
                         ConfigurationUtils.convertValue(userOverrides, String.class));
     }
 
-    @Override
-    public boolean scale(FlinkResourceContext<?> ctx) {
+    private boolean updateParallelismOverrides(","[{'comment': 'Could this return value can be removed?', 'commenter': '1996fanrui'}, {'comment': 'good catch', 'commenter': 'gyfora'}]"
672,flink-kubernetes-operator-autoscaler/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/JobAutoScalerImpl.java,"@@ -74,6 +76,36 @@ public JobAutoScalerImpl(
         this.infoManager = new AutoscalerInfoManager();
     }
 
+    @Override
+    public void scale(FlinkResourceContext<?> ctx) {
+        var conf = ctx.getObserveConfig();
+        var resource = ctx.getResource();
+        var resourceId = ResourceID.fromResource(resource);
+        var autoscalerMetrics = getOrInitAutoscalerFlinkMetrics(ctx, resourceId);
+
+        try {
+            if (resource.getSpec().getJob() == null || !conf.getBoolean(AUTOSCALER_ENABLED)) {
+                LOG.debug(""Autoscaler is disabled"");
+                return;
+            }
+
+            // Initialize metrics only if autoscaler is enabled
+            var status = resource.getStatus();
+            if (status.getLifecycleState() != ResourceLifecycleState.STABLE
+                    || !status.getJobStatus().getState().equals(JobStatus.RUNNING.name())) {
+                LOG.info(""Autoscaler is waiting for RUNNING job state"");
+                lastEvaluatedMetrics.remove(resourceId);
+                return;
+            }
+
+            updateParallelismOverrides(ctx, conf, resource, resourceId, autoscalerMetrics);","[{'comment': '```suggestion\r\n            runScalingLogic(ctx, conf, resource, resourceId, autoscalerMetrics);\r\n```', 'commenter': 'mxm'}]"
672,flink-kubernetes-operator-autoscaler/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/JobAutoScalerImpl.java,"@@ -74,6 +76,36 @@ public JobAutoScalerImpl(
         this.infoManager = new AutoscalerInfoManager();
     }
 
+    @Override
+    public void scale(FlinkResourceContext<?> ctx) {
+        var conf = ctx.getObserveConfig();
+        var resource = ctx.getResource();
+        var resourceId = ResourceID.fromResource(resource);
+        var autoscalerMetrics = getOrInitAutoscalerFlinkMetrics(ctx, resourceId);
+","[{'comment': 'An alternative would be to apply the current overrides here and the new overrides after the scaling. That would get rid of the finally block.', 'commenter': 'mxm'}, {'comment': 'I think the `finally` block is fairly clean and makes it obvious that overrides are applied always, exactly once at the very end ', 'commenter': 'gyfora'}, {'comment': ""Fair. Let's keep it like it is then."", 'commenter': 'mxm'}]"
672,flink-kubernetes-operator-autoscaler/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/JobAutoScalerImpl.java,"@@ -74,6 +76,36 @@ public JobAutoScalerImpl(
         this.infoManager = new AutoscalerInfoManager();
     }
 
+    @Override
+    public void scale(FlinkResourceContext<?> ctx) {
+        var conf = ctx.getObserveConfig();
+        var resource = ctx.getResource();
+        var resourceId = ResourceID.fromResource(resource);
+        var autoscalerMetrics = getOrInitAutoscalerFlinkMetrics(ctx, resourceId);
+
+        try {
+            if (resource.getSpec().getJob() == null || !conf.getBoolean(AUTOSCALER_ENABLED)) {
+                LOG.debug(""Autoscaler is disabled"");","[{'comment': 'Would reset the overrides here instead of buried in the method called by the finally block.', 'commenter': 'mxm'}, {'comment': 'That would make sense, I agree to remove the duplicate logic / checking. I tried to make the least amount of changes before the review :) ', 'commenter': 'gyfora'}]"
677,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/state/AutoScalerStateStore.java,"@@ -0,0 +1,58 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.autoscaler.state;
+
+import org.apache.flink.annotation.Experimental;
+import org.apache.flink.autoscaler.JobAutoScalerContext;
+
+/**
+ * The state store is responsible for store all states during scaling.
+ *
+ * @param <KEY> The job key.
+ * @param <Context> Instance of JobAutoScalerContext.
+ */
+@Experimental
+public interface AutoScalerStateStore<KEY, Context extends JobAutoScalerContext<KEY>> {
+
+    void storeScalingHistory(Context jobContext, String scalingHistory);","[{'comment': 'Hi @gyfora  @mxm , because of some offline comments and ease of review, I split the decoupling into the FLINK-33097 and FLINK-33098. Of course, if you think they should be merged to one PR. I will go ahead at this PR with multiple commits.\r\n\r\n@Samrat002 provided 2 improvements about the `StateStore`.\r\n\r\n## 1. Using the structured class instead of `String`\r\n\r\nThe structured class is clearer than String.\r\n\r\nFirst of all, we define the `SerializableState` interface to abstract the `serialize` and `deserialize`.\r\n\r\n```\r\n    interface SerializableState<State extends SerializableState> {\r\n\r\n        String serialize();\r\n\r\n        State deserialize(String serializedResult);\r\n    }\r\n```\r\n\r\nAnd then, define a `ScalingHistory` class, it implement the `SerializableState`.\r\n\r\n\r\n## 2. Using the `byte[]` instead of `String` as the serialized result\r\n\r\nReason: In the future there may be some complex state objects that cannot be serialized to String.\r\n\r\n\r\nHi @Samrat002 , please correct me if my description is wrong, thanks~', 'commenter': '1996fanrui'}, {'comment': ""> improvement1\r\n\r\nThe improvement1 sounds make sense to me, and I need to improve the `SerializableState`. The current `deserialize` method needs to create a object first, and then call `object.deserialize(serializedResult)`. In general, the serialize is a separate object.\r\n\r\nI'm afraid whether it's too complex If we introduce 2 class for each state.\r\n\r\n> improvement2\r\n\r\nFor improvement2, my concern is the serialized type is changed, and all old jobs cannot be compatible directly. \r\n\r\nThe compatibility of `byte[]` must be stronger than String, but the benefits it brings are uncertain (because there may not be classes that can only be serialized into `byte[]` in the future).\r\n\r\nThe negative impact is certain, and it will bring additional migration costs to historical users."", 'commenter': '1996fanrui'}, {'comment': '\r\nMuch appreciated, @1996fanrui , for summarizing our discussion.\r\n\r\nI would like to add one more point: the `AutoScalerEventHandler` currently offers two types, namely `warning` and `normal`. In my opinion, it would be beneficial to include an `Error` or `Fatal` option as part of the interface. This flexibility would allow different users implementing this autoscaler module to define and use these options according to their specific requirements.\r\n\r\nI would definitely appreciate hearing the thoughts of @mxm  and @gyfora  regarding the adoption of structured classes over string and the proposal to introduce a new type called Fatal in `AutoScalerEventHandler`, as described in this thread.', 'commenter': 'Samrat002'}, {'comment': 'Hi @Samrat002 , thanks for your feedback!\r\n\r\nI have updated these method parameters of `AutoScalerStateStore` to the specific class instead of String, such as: `Map<JobVertexID, SortedMap<Instant, ScalingSummary>> scalingHistory`.\r\n\r\n```\r\npublic interface AutoScalerStateStore<KEY, Context extends JobAutoScalerContext<KEY>> {\r\n\r\n    void storeScalingHistory(\r\n            Context jobContext,\r\n            Map<JobVertexID, SortedMap<Instant, ScalingSummary>> scalingHistory);\r\n\r\n    Optional<Map<JobVertexID, SortedMap<Instant, ScalingSummary>>> getScalingHistory(\r\n            Context jobContext);\r\n\r\n    void removeScalingHistory(Context jobContext);\r\n}\r\n```\r\n\r\nThe PR has been updated as well.\r\n\r\nDo you think is it ok? It means the state store is responsible for how to serialize and deserialize, for example:\r\n\r\n- The default `KubernetesAutoScalerStateStore` will serialize all states to String inside of `KubernetesAutoScalerStateStore`\r\n- As you mentioned before: if there is any complex type in the future. Each state store to determine how to serialize them.\r\n\r\nAlso, let me add a reason why update these parameters here:\r\n\r\nCurrently, all states are stored at ConfigMap, and it has size limitation. The size limitation should just work with `KubernetesAutoScalerStateStore`, and size limitation is a part of serialization. So we should move the serialization and deserialization in the `AutoScalerStateStore`.', 'commenter': '1996fanrui'}]"
677,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/JobAutoScalerContext.java,"@@ -0,0 +1,44 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.autoscaler;
+
+import org.apache.flink.annotation.Experimental;
+import org.apache.flink.api.common.JobID;
+import org.apache.flink.client.program.rest.RestClusterClient;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.metrics.MetricGroup;
+
+/**
+ * The job autoscaler context, it includes all details related to the current job.
+ *
+ * @param <KEY> The job key.
+ */
+@Experimental
+public interface JobAutoScalerContext<KEY> {
+
+    /** The identifier of each flink job. */
+    KEY getJobKey();
+
+    JobID getJobID();
+
+    Configuration getConfiguration();
+
+    MetricGroup getMetricGroup();
+
+    RestClusterClient<String> getRestClusterClient() throws Exception;","[{'comment': ""Sorry for jumping in out of nowhere. But I started looking into the FLIP-334. I should have participated in the discussion earlier but missed it. Anyway, I am wondering why we're relying on `RestClusterClient` here? Shouldn't we use the `ClusterClient` interface, instead, to keep it generic and improve testability?"", 'commenter': 'XComp'}, {'comment': "">  I am wondering why we're relying on `RestClusterClient` here? Shouldn't we use the `ClusterClient` interface, instead, to keep it generic and improve testability?\r\n\r\nAfter I check, `ClusterClient` has only some commonly used interfaces, such as: `submitJob`, `getJobStatus`, `cancelJob` and a series of savepoint releated interfaces, etc.\r\n\r\nAutoscaler is using a couple of interfaces that request rest api, such as: \r\n\r\n- [getJobTopology](https://github.com/apache/flink-kubernetes-operator/blob/662fa612a8ab352e43ab8a99fa61aadfbe41e4d7/flink-kubernetes-operator-autoscaler/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/ScalingMetricCollector.java#L106) \r\n- [getJobDetailsInfo](https://github.com/apache/flink-kubernetes-operator/blob/662fa612a8ab352e43ab8a99fa61aadfbe41e4d7/flink-kubernetes-operator-autoscaler/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/ScalingMetricCollector.java#L97)\r\n- [Query generic metrics ](https://github.com/apache/flink-kubernetes-operator/blob/662fa612a8ab352e43ab8a99fa61aadfbe41e4d7/flink-kubernetes-operator-autoscaler/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/ScalingMetricCollector.java#L433)\r\n\r\nAlso, the new `rescale rest api (FLIP-291)`[1] will be introduced in this FLIP. That's why we using the `RestClusterClient` here. \r\n\r\nThese information cannot be fetched by `ClusterClient`, right?\r\n\r\nPlease correct me if I misunderstand here, thanks a lot.\r\n\r\n[1] https://cwiki.apache.org/confluence/display/FLINK/FLIP-291%3A+Externalized+Declarative+Resource+Management"", 'commenter': '1996fanrui'}]"
677,flink-autoscaler/pom.xml,"@@ -45,6 +45,32 @@ under the License.
             <scope>provided</scope>
         </dependency>
 
+        <dependency>
+            <groupId>org.projectlombok</groupId>
+            <artifactId>lombok</artifactId>
+            <version>${lombok.version}</version>
+            <scope>provided</scope>
+        </dependency>
+
+        <dependency>
+            <groupId>org.junit.jupiter</groupId>
+            <artifactId>junit-jupiter-params</artifactId>
+            <scope>test</scope>
+        </dependency>
+
+        <!-- TODO FLINK-33098: These jackson dependencies can be replaced with flink shaded jackson. It can be done","[{'comment': 'The shaded Jackson dependencies for 2.14.2 are already published (see [org.apache.flink:flink-shaded-jackson:2.14.2-17.0](https://mvnrepository.com/artifact/org.apache.flink/flink-shaded-jackson/2.14.2-17.0)). You should be able to use it even without waiting for Flink 1.18 to be released. ...or am I missing something here? :thinking: ', 'commenter': 'XComp'}, {'comment': 'The `flink-autoscaler` depends on the `flink-runtime 1.17.1`, and  `flink-runtime 1.17.1` depends on the `flink-shaded-jackson 2.13.4-16.1`.\r\n\r\nIf `flink-autoscaler` depends on the `flink-shaded-jackson 2.14.2-17.0` directly, the `2.13.4-16.1` and `2.14.2-17.0` may be conflicted, right?\r\n\r\nSo here has 2 temporary solutions:\r\n\r\n- Soluiton1: Using the unified jackson dependence of `flink-kubernetes-operator` (Current PR is it.)\r\n- Solution2: `flink-autoscaler` depends on the `flink-shaded-jackson 2.14.2-17.0` directly, and exclude `flink-shaded-jackson` from `flink-runtime`.\r\n\r\nIn the short term, all of them are fine for me.\r\n\r\nHi @gyfora @mxm , you are the expert of `flink-kubernetes-operator`, could the whole `flink-kubernetes-operator` use the `flink-shaded-jackson 2.14.2-17.0`? \r\n\r\nIf yes, this problem is easy to solve.', 'commenter': '1996fanrui'}, {'comment': 'I think we should use the shaded dependency that comes with the Flink client version we are depending on. That is the simplest way to avoid any potential issues :) ', 'commenter': 'gyfora'}]"
677,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/AbstractFlinkResourceReconciler.java,"@@ -174,6 +182,32 @@ public void reconcile(FlinkResourceContext<CR> ctx) throws Exception {
         }
     }
 
+    private void scaling(FlinkResourceContext<CR> ctx) throws Exception {
+        KubernetesJobAutoScalerContext autoScalerContext = ctx.getJobAutoScalerContext();
+
+        if (autoscalerDisabled(ctx)) {
+            autoScalerContext.getConfiguration().set(AUTOSCALER_ENABLED, false);
+            resourceScaler.scale(autoScalerContext);
+            return;
+        }
+        if (waitingForRunning(ctx.getResource().getStatus())) {
+            LOG.info(""Autoscaler is waiting for  stable, running state"");
+            resourceScaler.cleanup(autoScalerContext.getJobKey());
+            return;","[{'comment': ""Previously the logic was to still apply the parallelism overrides to the resource even though the resource is not running, but that's not the case here anymore. \r\n\r\nThis will cause the in-place scaling to fail because during the next reconciliation the spec will not contain the parallelism overrides as the job is not in a running state."", 'commenter': 'mateczagany'}, {'comment': 'Thanks for pointing it out.\r\n\r\nIf the `applyParallelismOverrides` is expected here, we can call the related code here. The `applyParallelismOverrides` just calls the `stateStore.getParallelismOverrides(ctx)` and `scalingRealizer.realize(ctx, userOverrides);`, and `stateStore` and `scalingRealizer` can be reached here. \r\n\r\nI plan to extract the `org.apache.flink.autoscaler.JobAutoScalerImpl#applyParallelismOverrides` to a static method, WDYT?', 'commenter': '1996fanrui'}]"
677,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/ConfigMapStore.java,"@@ -0,0 +1,176 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.autoscaler;
+
+import org.apache.flink.annotation.VisibleForTesting;
+import org.apache.flink.kubernetes.utils.Constants;
+import org.apache.flink.util.Preconditions;
+
+import io.fabric8.kubernetes.api.model.ConfigMap;
+import io.fabric8.kubernetes.api.model.HasMetadata;
+import io.fabric8.kubernetes.api.model.ObjectMeta;
+import io.fabric8.kubernetes.client.KubernetesClient;
+import io.javaoperatorsdk.operator.processing.event.ResourceID;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import javax.annotation.Nonnull;
+
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Optional;
+import java.util.concurrent.ConcurrentHashMap;
+
+/** The config map store, it's responsible for store/get/remove the state with String type. */
+public class ConfigMapStore {
+
+    private static final Logger LOG = LoggerFactory.getLogger(KubernetesAutoScalerStateStore.class);
+
+    private static final String LABEL_COMPONENT_AUTOSCALER = ""autoscaler"";
+
+    private final KubernetesClient kubernetesClient;
+
+    /**
+     * The cache for each resourceId may be in three situations: 1. The resourceId isn't exist :
+     * ConfigMap isn't loaded from kubernetes, or it's removed. 2. The resourceId is exist, and
+     * value is the Optional.empty() : We have loaded the ConfigMap from kubernetes, but the
+     * ConfigMap isn't created at kubernetes side. 3. The resourceId is exist, and the Optional
+     * isn't empty : We have loaded the ConfigMap from kubernetes, it may be not same with
+     * kubernetes side due to it's not flushed after updating.
+     */
+    private final ConcurrentHashMap<ResourceID, Optional<ConfigMap>> cache =
+            new ConcurrentHashMap<>();
+
+    public ConfigMapStore(KubernetesClient kubernetesClient) {
+        this.kubernetesClient = kubernetesClient;
+    }
+
+    protected void putSerializedState(
+            KubernetesJobAutoScalerContext jobContext, String key, String value) {
+        getOrCreateState(jobContext).put(key, value);
+    }
+
+    protected Optional<String> getSerializedState(
+            KubernetesJobAutoScalerContext jobContext, String key) {
+        return getConfigMap(jobContext).map(configMap -> configMap.getData().get(key));
+    }
+
+    protected void removeSerializedState(KubernetesJobAutoScalerContext jobContext, String key) {
+        getConfigMap(jobContext)
+                .ifPresentOrElse(
+                        configMap -> configMap.getData().remove(key),
+                        () -> {
+                            throw new IllegalStateException(
+                                    ""The configMap isn't created, so the remove is unavailable."");
+                        });
+    }
+
+    public void flush(KubernetesJobAutoScalerContext jobContext) {
+        Optional<ConfigMap> configMapOpt = cache.get(jobContext.getJobKey());
+        Preconditions.checkState(","[{'comment': ""This will fail during the autoscale stabilization period as we won't collect any metrics there. I think we could just simply log here and not throw an error."", 'commenter': 'mateczagany'}, {'comment': ""Some callers call the `flush` in the end even if the state isn't updated. So you suggestion is easy to caller.\r\n\r\nUpdated the prod code, and I will add more tests for it later due to I'm on vacation, thanks~"", 'commenter': '1996fanrui'}]"
677,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/AbstractJobAutoScalerContext.java,"@@ -0,0 +1,80 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.autoscaler;
+
+import org.apache.flink.api.common.JobID;
+import org.apache.flink.client.program.rest.RestClusterClient;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.metrics.MetricGroup;
+import org.apache.flink.util.function.SupplierWithException;
+
+/**
+ * The abstract job autoscaler context.
+ *
+ * @param <KEY> The job key.
+ */
+public abstract class AbstractJobAutoScalerContext<KEY> implements JobAutoScalerContext<KEY> {","[{'comment': ""I would simply rename this as `JobAutoScalerContext<KEY>`, delete constructor , make all fields simply final and put `@Value` on it, to generate all the getters, toString etc.\r\n\r\nWe don't need to have interfaces for everything, too much boilerplate"", 'commenter': 'gyfora'}]"
677,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/DefaultJobAutoScalerContext.java,"@@ -0,0 +1,36 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.autoscaler;
+
+import org.apache.flink.api.common.JobID;
+import org.apache.flink.client.program.rest.RestClusterClient;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.metrics.MetricGroup;
+import org.apache.flink.util.function.SupplierWithException;
+
+/** The default job autoscaler context, the jobKey is JobID. */
+public class DefaultJobAutoScalerContext extends AbstractJobAutoScalerContext<JobID> {","[{'comment': ""Let's simply delete the class and simply have a non-abstract `JobAutoscalerContext<JobId>`"", 'commenter': 'gyfora'}]"
677,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/JobAutoScalerContext.java,"@@ -0,0 +1,44 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.autoscaler;
+
+import org.apache.flink.annotation.Experimental;
+import org.apache.flink.api.common.JobID;
+import org.apache.flink.client.program.rest.RestClusterClient;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.metrics.MetricGroup;
+
+/**
+ * The job autoscaler context, it includes all details related to the current job.
+ *
+ * @param <KEY> The job key.
+ */
+@Experimental
+public interface JobAutoScalerContext<KEY> {","[{'comment': ""Let's delete the interface for now (see my previous comment) we can re-add this later if necessary but I doubt it."", 'commenter': 'gyfora'}]"
677,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/state/HeapedAutoScalerStateStore.java,"@@ -0,0 +1,114 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.autoscaler.state;
+
+import org.apache.flink.autoscaler.JobAutoScalerContext;
+import org.apache.flink.autoscaler.ScalingSummary;
+import org.apache.flink.autoscaler.metrics.CollectedMetrics;
+import org.apache.flink.runtime.jobgraph.JobVertexID;
+
+import java.time.Instant;
+import java.util.Map;
+import java.util.Optional;
+import java.util.SortedMap;
+import java.util.concurrent.ConcurrentHashMap;
+
+/**
+ * The state store based on the Java Heap.
+ *
+ * @param <KEY>
+ * @param <Context>
+ */
+public class HeapedAutoScalerStateStore<KEY, Context extends JobAutoScalerContext<KEY>>","[{'comment': ""Let's rename this `HeapedAutoScalerStateStore` -> `InMemoryAutoScalerStateStore`"", 'commenter': 'gyfora'}]"
677,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/config/AutoScalerOptions.java,"@@ -166,6 +197,13 @@ private static ConfigOptions.OptionBuilder autoScalerConfig(String key) {
                     .stringType()
                     .asList()
                     .defaultValues()
+                    .withDeprecatedKeys(deprecatedOperatorConfigKey(""vertex.exclude.ids""))
                     .withDescription(
                             ""A (semicolon-separated) list of vertex ids in hexstring for which to disable scaling. Caution: For non-sink vertices this will still scale their downstream operators until https://issues.apache.org/jira/browse/FLINK-31215 is implemented."");
+
+    public static final ConfigOption<Duration> FLINK_CLIENT_TIMEOUT =
+            autoScalerConfig(""flink.client.timeout"")
+                    .durationType()
+                    .defaultValue(Duration.ofSeconds(10))
+                    .withDescription(""The timeout for waiting the flink rest client to return."");","[{'comment': 'Do we need to expose this as a configuration or can we just use the default? \r\n\r\nI think it is better to let the user configure standard Flink configs as listed here: https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#client-timeout', 'commenter': 'mxm'}]"
677,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/config/AutoScalerOptions.java,"@@ -15,149 +15,180 @@
  * limitations under the License.
  */
 
-package org.apache.flink.kubernetes.operator.autoscaler.config;
+package org.apache.flink.autoscaler.config;
 
+import org.apache.flink.autoscaler.metrics.MetricAggregator;
 import org.apache.flink.configuration.ConfigOption;
 import org.apache.flink.configuration.ConfigOptions;
-import org.apache.flink.kubernetes.operator.autoscaler.metrics.MetricAggregator;
 
 import java.time.Duration;
 import java.util.List;
 
-import static org.apache.flink.kubernetes.operator.config.KubernetesOperatorConfigOptions.operatorConfig;
-
 /** Config options related to the autoscaler module. */
 public class AutoScalerOptions {
 
+    public static final String DEPRECATED_K8S_OP_CONF_PREFIX = ""kubernetes.operator."";
+    public static final String AUTOSCALER_CONF_PREFIX = ""job.autoscaler."";
+
+    private static String deprecatedOperatorConfigKey(String key) {
+        return DEPRECATED_K8S_OP_CONF_PREFIX + AUTOSCALER_CONF_PREFIX + key;
+    }
+
+    private static String autoScalerConfigKey(String key) {
+        return AUTOSCALER_CONF_PREFIX + key;
+    }
+
     private static ConfigOptions.OptionBuilder autoScalerConfig(String key) {
-        return operatorConfig(""job.autoscaler."" + key);
+        return ConfigOptions.key(autoScalerConfigKey(key));
     }
 
     public static final ConfigOption<Boolean> AUTOSCALER_ENABLED =
             autoScalerConfig(""enabled"")
                     .booleanType()
                     .defaultValue(false)
+                    .withDeprecatedKeys(deprecatedOperatorConfigKey(""enabled""))","[{'comment': 'This might confuse some existing users because the deprecated keys will not appear on the configuration page. Can we add a note on the configuration page that we renamed the configuration prefix?', 'commenter': 'mxm'}]"
677,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/event/AutoScalerEventHandler.java,"@@ -15,22 +15,28 @@
  * limitations under the License.
  */
 
-package org.apache.flink.kubernetes.operator.reconciler.deployment;
+package org.apache.flink.autoscaler.event;
 
-import org.apache.flink.kubernetes.operator.controller.FlinkResourceContext;
-import org.apache.flink.kubernetes.operator.utils.EventRecorder;
+import org.apache.flink.annotation.Experimental;
+import org.apache.flink.autoscaler.JobAutoScalerContext;
 
-/** An autoscaler implementation which does nothing. */
-public class NoopJobAutoscalerFactory implements JobAutoScalerFactory, JobAutoScaler {
+import javax.annotation.Nullable;
 
-    @Override
-    public JobAutoScaler create(EventRecorder eventRecorder) {
-        return this;
-    }
+/**
+ * Handler all loggable events during scaling.
+ *
+ * @param <KEY> The job key.
+ * @param <Context> Instance of JobAutoScalerContext.
+ */
+@Experimental
+public interface AutoScalerEventHandler<KEY, Context extends JobAutoScalerContext<KEY>> {
 
-    @Override
-    public void scale(FlinkResourceContext<?> ctx) {}
+    void handleEvent(
+            Context context, Type type, String reason, String message, @Nullable String messageKey);
 
-    @Override
-    public void cleanup(FlinkResourceContext<?> ctx) {}
+    /** The type of the events. */
+    enum Type {
+        Normal,
+        Warning","[{'comment': 'What about `Error`?', 'commenter': 'mxm'}]"
677,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/AutoscalerLoader.java,"@@ -17,41 +17,37 @@
 
 package org.apache.flink.kubernetes.operator.utils;
 
-import org.apache.flink.kubernetes.operator.reconciler.deployment.JobAutoScalerFactory;
-import org.apache.flink.kubernetes.operator.reconciler.deployment.NoopJobAutoscalerFactory;
-import org.apache.flink.util.Preconditions;
-
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import java.util.ServiceLoader;
-
-/** Loads the active Autoscaler implementation from the classpath. */
+import org.apache.flink.autoscaler.JobAutoScaler;
+import org.apache.flink.autoscaler.JobAutoScalerImpl;
+import org.apache.flink.autoscaler.RestApiMetricsCollector;
+import org.apache.flink.autoscaler.ScalingExecutor;
+import org.apache.flink.autoscaler.ScalingMetricEvaluator;
+import org.apache.flink.kubernetes.operator.autoscaler.ConfigMapStore;
+import org.apache.flink.kubernetes.operator.autoscaler.KubernetesAutoScalerEventHandler;
+import org.apache.flink.kubernetes.operator.autoscaler.KubernetesAutoScalerStateStore;
+import org.apache.flink.kubernetes.operator.autoscaler.KubernetesJobAutoScalerContext;
+import org.apache.flink.kubernetes.operator.autoscaler.KubernetesScalingRealizer;
+
+import io.fabric8.kubernetes.client.KubernetesClient;
+import io.javaoperatorsdk.operator.processing.event.ResourceID;
+
+/** Loads the Autoscaler implementation. */
 public class AutoscalerLoader {","[{'comment': ""Given that there isn't anything dynamically loaded anymore, this class should be removed. Or renamed to `AutoscalerFactory`."", 'commenter': 'mxm'}]"
677,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/JobAutoScalerImpl.java,"@@ -0,0 +1,203 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.autoscaler;
+
+import org.apache.flink.annotation.VisibleForTesting;
+import org.apache.flink.autoscaler.config.AutoScalerOptions;
+import org.apache.flink.autoscaler.event.AutoScalerEventHandler;
+import org.apache.flink.autoscaler.metrics.AutoscalerFlinkMetrics;
+import org.apache.flink.autoscaler.metrics.EvaluatedScalingMetric;
+import org.apache.flink.autoscaler.metrics.ScalingMetric;
+import org.apache.flink.autoscaler.realizer.ScalingRealizer;
+import org.apache.flink.autoscaler.state.AutoScalerStateStore;
+import org.apache.flink.configuration.PipelineOptions;
+import org.apache.flink.runtime.jobgraph.JobVertexID;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Optional;
+import java.util.concurrent.ConcurrentHashMap;
+
+import static org.apache.flink.autoscaler.config.AutoScalerOptions.AUTOSCALER_ENABLED;
+import static org.apache.flink.autoscaler.metrics.AutoscalerFlinkMetrics.initRecommendedParallelism;
+import static org.apache.flink.autoscaler.metrics.AutoscalerFlinkMetrics.resetRecommendedParallelism;
+
+/** The default implementation of {@link JobAutoScaler}. */
+public class JobAutoScalerImpl<KEY, Context extends JobAutoScalerContext<KEY>>
+        implements JobAutoScaler<KEY, Context> {
+
+    private static final Logger LOG = LoggerFactory.getLogger(JobAutoScalerImpl.class);
+
+    @VisibleForTesting protected static final String AUTOSCALER_ERROR = ""AutoscalerError"";","[{'comment': ""Can we inline this constant? This isn't used in tests either."", 'commenter': 'mxm'}]"
677,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/JobAutoScalerImpl.java,"@@ -0,0 +1,203 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.autoscaler;
+
+import org.apache.flink.annotation.VisibleForTesting;
+import org.apache.flink.autoscaler.config.AutoScalerOptions;
+import org.apache.flink.autoscaler.event.AutoScalerEventHandler;
+import org.apache.flink.autoscaler.metrics.AutoscalerFlinkMetrics;
+import org.apache.flink.autoscaler.metrics.EvaluatedScalingMetric;
+import org.apache.flink.autoscaler.metrics.ScalingMetric;
+import org.apache.flink.autoscaler.realizer.ScalingRealizer;
+import org.apache.flink.autoscaler.state.AutoScalerStateStore;
+import org.apache.flink.configuration.PipelineOptions;
+import org.apache.flink.runtime.jobgraph.JobVertexID;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Optional;
+import java.util.concurrent.ConcurrentHashMap;
+
+import static org.apache.flink.autoscaler.config.AutoScalerOptions.AUTOSCALER_ENABLED;
+import static org.apache.flink.autoscaler.metrics.AutoscalerFlinkMetrics.initRecommendedParallelism;
+import static org.apache.flink.autoscaler.metrics.AutoscalerFlinkMetrics.resetRecommendedParallelism;
+
+/** The default implementation of {@link JobAutoScaler}. */
+public class JobAutoScalerImpl<KEY, Context extends JobAutoScalerContext<KEY>>
+        implements JobAutoScaler<KEY, Context> {
+
+    private static final Logger LOG = LoggerFactory.getLogger(JobAutoScalerImpl.class);
+
+    @VisibleForTesting protected static final String AUTOSCALER_ERROR = ""AutoscalerError"";
+
+    private final ScalingMetricCollector<KEY, Context> metricsCollector;
+    private final ScalingMetricEvaluator evaluator;
+    private final ScalingExecutor<KEY, Context> scalingExecutor;
+    private final AutoScalerEventHandler<KEY, Context> eventHandler;
+    private final ScalingRealizer<KEY, Context> scalingRealizer;
+    private final AutoScalerStateStore<KEY, Context> stateStore;
+
+    @VisibleForTesting
+    final Map<KEY, Map<JobVertexID, Map<ScalingMetric, EvaluatedScalingMetric>>>
+            lastEvaluatedMetrics = new ConcurrentHashMap<>();
+
+    @VisibleForTesting
+    final Map<KEY, AutoscalerFlinkMetrics> flinkMetrics = new ConcurrentHashMap<>();
+
+    public JobAutoScalerImpl(
+            ScalingMetricCollector<KEY, Context> metricsCollector,
+            ScalingMetricEvaluator evaluator,
+            ScalingExecutor<KEY, Context> scalingExecutor,
+            AutoScalerEventHandler<KEY, Context> eventHandler,
+            ScalingRealizer<KEY, Context> scalingRealizer,
+            AutoScalerStateStore<KEY, Context> stateStore) {
+        this.metricsCollector = metricsCollector;
+        this.evaluator = evaluator;
+        this.scalingExecutor = scalingExecutor;
+        this.eventHandler = eventHandler;
+        this.scalingRealizer = scalingRealizer;
+        this.stateStore = stateStore;
+    }
+
+    @Override
+    public void scale(Context ctx) throws Exception {
+        var autoscalerMetrics = getOrInitAutoscalerFlinkMetrics(ctx);
+
+        try {
+            runScalingLogic(ctx, autoscalerMetrics);
+        } catch (Throwable e) {
+            onError(ctx, autoscalerMetrics, e);
+        } finally {
+            applyParallelismOverrides(ctx);
+        }
+    }
+
+    @Override
+    public void cleanup(KEY jobKey) {
+        LOG.info(""Cleaning up autoscaling meta data"");
+        metricsCollector.cleanup(jobKey);
+        lastEvaluatedMetrics.remove(jobKey);
+        flinkMetrics.remove(jobKey);
+        stateStore.removeInfoFromCache(jobKey);
+    }
+
+    private void clearParallelismOverrides(Context ctx) throws Exception {
+        var parallelismOverrides = stateStore.getParallelismOverrides(ctx);
+        if (parallelismOverrides.isPresent()) {
+            stateStore.removeParallelismOverrides(ctx);
+            stateStore.flush(ctx);
+        }
+    }
+
+    @VisibleForTesting
+    protected Optional<Map<String, String>> getParallelismOverrides(Context ctx) throws Exception {
+        return stateStore.getParallelismOverrides(ctx);
+    }
+
+    /**
+     * If there are any parallelism overrides by the {@link JobAutoScaler} apply them to the
+     * scalingRealizer.
+     *
+     * @param ctx Job context
+     */
+    @VisibleForTesting
+    protected void applyParallelismOverrides(Context ctx) throws Exception {
+        var overridesOpt = getParallelismOverrides(ctx);
+        if (overridesOpt.isEmpty() || overridesOpt.get().isEmpty()) {
+            return;
+        }
+        Map<String, String> overrides = overridesOpt.get();
+        LOG.debug(""Applying parallelism overrides: {}"", overrides);
+
+        var conf = ctx.getConfiguration();
+        var userOverrides = new HashMap<>(conf.get(PipelineOptions.PARALLELISM_OVERRIDES));
+        var exclusions = conf.get(AutoScalerOptions.VERTEX_EXCLUDE_IDS);
+
+        overrides.forEach(
+                (k, v) -> {
+                    // Respect user override for excluded vertices
+                    if (exclusions.contains(k)) {
+                        userOverrides.putIfAbsent(k, v);
+                    } else {
+                        userOverrides.put(k, v);
+                    }
+                });
+        scalingRealizer.realize(ctx, userOverrides);
+    }
+
+    private void runScalingLogic(Context ctx, AutoscalerFlinkMetrics autoscalerMetrics)
+            throws Exception {
+
+        var conf = ctx.getConfiguration();
+        if (!conf.getBoolean(AUTOSCALER_ENABLED)) {
+            LOG.debug(""Autoscaler is disabled"");
+            clearParallelismOverrides(ctx);
+            return;
+        }
+
+        var collectedMetrics = metricsCollector.updateMetrics(ctx, stateStore);
+
+        if (collectedMetrics.getMetricHistory().isEmpty()) {
+            stateStore.flush(ctx);
+            return;
+        }
+        LOG.debug(""Collected metrics: {}"", collectedMetrics);
+
+        var evaluatedMetrics = evaluator.evaluate(conf, collectedMetrics);
+        LOG.debug(""Evaluated metrics: {}"", evaluatedMetrics);
+        lastEvaluatedMetrics.put(ctx.getJobKey(), evaluatedMetrics);
+
+        initRecommendedParallelism(evaluatedMetrics);
+        autoscalerMetrics.registerScalingMetrics(
+                collectedMetrics.getJobTopology().getVerticesInTopologicalOrder(),
+                () -> lastEvaluatedMetrics.get(ctx.getJobKey()));
+
+        if (!collectedMetrics.isFullyCollected()) {
+            // We have done an upfront evaluation, but we are not ready for scaling.
+            resetRecommendedParallelism(evaluatedMetrics);
+            stateStore.flush(ctx);
+            return;
+        }
+
+        var parallelismChanged = scalingExecutor.scaleResource(ctx, evaluatedMetrics);
+
+        if (parallelismChanged) {
+            autoscalerMetrics.incrementScaling();
+        } else {
+            autoscalerMetrics.incrementBalanced();
+        }
+
+        stateStore.flush(ctx);","[{'comment': 'Can we run the flush after calling this method right after line 85?', 'commenter': 'mxm'}]"
677,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/JobVertexScaler.java,"@@ -38,42 +36,42 @@
 import java.util.Map;
 import java.util.SortedMap;
 
-import static org.apache.flink.kubernetes.operator.autoscaler.config.AutoScalerOptions.MAX_SCALE_DOWN_FACTOR;
-import static org.apache.flink.kubernetes.operator.autoscaler.config.AutoScalerOptions.MAX_SCALE_UP_FACTOR;
-import static org.apache.flink.kubernetes.operator.autoscaler.config.AutoScalerOptions.SCALE_UP_GRACE_PERIOD;
-import static org.apache.flink.kubernetes.operator.autoscaler.config.AutoScalerOptions.TARGET_UTILIZATION;
-import static org.apache.flink.kubernetes.operator.autoscaler.config.AutoScalerOptions.VERTEX_MAX_PARALLELISM;
-import static org.apache.flink.kubernetes.operator.autoscaler.config.AutoScalerOptions.VERTEX_MIN_PARALLELISM;
-import static org.apache.flink.kubernetes.operator.autoscaler.metrics.ScalingMetric.EXPECTED_PROCESSING_RATE;
-import static org.apache.flink.kubernetes.operator.autoscaler.metrics.ScalingMetric.MAX_PARALLELISM;
-import static org.apache.flink.kubernetes.operator.autoscaler.metrics.ScalingMetric.PARALLELISM;
-import static org.apache.flink.kubernetes.operator.autoscaler.metrics.ScalingMetric.TRUE_PROCESSING_RATE;
+import static org.apache.flink.autoscaler.config.AutoScalerOptions.MAX_SCALE_DOWN_FACTOR;
+import static org.apache.flink.autoscaler.config.AutoScalerOptions.MAX_SCALE_UP_FACTOR;
+import static org.apache.flink.autoscaler.config.AutoScalerOptions.SCALE_UP_GRACE_PERIOD;
+import static org.apache.flink.autoscaler.config.AutoScalerOptions.TARGET_UTILIZATION;
+import static org.apache.flink.autoscaler.config.AutoScalerOptions.VERTEX_MAX_PARALLELISM;
+import static org.apache.flink.autoscaler.config.AutoScalerOptions.VERTEX_MIN_PARALLELISM;
+import static org.apache.flink.autoscaler.metrics.ScalingMetric.EXPECTED_PROCESSING_RATE;
+import static org.apache.flink.autoscaler.metrics.ScalingMetric.MAX_PARALLELISM;
+import static org.apache.flink.autoscaler.metrics.ScalingMetric.PARALLELISM;
+import static org.apache.flink.autoscaler.metrics.ScalingMetric.TRUE_PROCESSING_RATE;
 
 /** Component responsible for computing vertex parallelism based on the scaling metrics. */
-public class JobVertexScaler {
+public class JobVertexScaler<KEY, Context extends JobAutoScalerContext<KEY>> {
 
     private static final Logger LOG = LoggerFactory.getLogger(JobVertexScaler.class);
 
+    @VisibleForTesting public static final String INEFFECTIVE_SCALING = ""IneffectiveScaling"";","[{'comment': ""Can we inline this? It isn't used in tests either."", 'commenter': 'mxm'}]"
677,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/ScalingMetricEvaluator.java,"@@ -154,7 +155,7 @@ private Map<ScalingMetric, EvaluatedScalingMetric> evaluateMetrics(
     }
 
     @VisibleForTesting
-    protected static void computeProcessingRateThresholds(
+    public static void computeProcessingRateThresholds(","[{'comment': ""Any reason for changing the visibility here? I couldn't find why this was required."", 'commenter': 'mxm'}]"
677,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/metrics/ScalingHistoryUtils.java,"@@ -0,0 +1,117 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.autoscaler.metrics;
+
+import org.apache.flink.autoscaler.JobAutoScalerContext;
+import org.apache.flink.autoscaler.ScalingSummary;
+import org.apache.flink.autoscaler.config.AutoScalerOptions;
+import org.apache.flink.autoscaler.state.AutoScalerStateStore;
+import org.apache.flink.runtime.jobgraph.JobVertexID;
+
+import javax.annotation.Nonnull;
+
+import java.time.Instant;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Set;
+import java.util.SortedMap;
+import java.util.TreeMap;
+
+/** The utils for scaling history. */
+public class ScalingHistoryUtils {
+
+    public static <KEY, Context extends JobAutoScalerContext<KEY>> void addToScalingHistoryAndStore(
+            AutoScalerStateStore<KEY, Context> stateStore,
+            Context context,
+            Instant now,
+            Map<JobVertexID, ScalingSummary> summaries)
+            throws Exception {
+        addToScalingHistoryAndStore(
+                stateStore,
+                context,
+                getTrimmedScalingHistory(stateStore, context, now),
+                now,
+                summaries);
+    }
+
+    public static <KEY, Context extends JobAutoScalerContext<KEY>> void addToScalingHistoryAndStore(
+            AutoScalerStateStore<KEY, Context> stateStore,
+            Context context,
+            Map<JobVertexID, SortedMap<Instant, ScalingSummary>> scalingHistory,
+            Instant now,
+            Map<JobVertexID, ScalingSummary> summaries)
+            throws Exception {
+
+        summaries.forEach(
+                (id, summary) ->
+                        scalingHistory.computeIfAbsent(id, j -> new TreeMap<>()).put(now, summary));
+        stateStore.storeScalingHistory(context, scalingHistory);
+    }
+
+    public static <KEY, Context extends JobAutoScalerContext<KEY>> void updateVertexList(
+            AutoScalerStateStore<KEY, Context> stateStore,
+            Context ctx,
+            Instant now,
+            Set<JobVertexID> vertexSet)
+            throws Exception {
+        Map<JobVertexID, SortedMap<Instant, ScalingSummary>> trimmedScalingHistory =
+                getTrimmedScalingHistory(stateStore, ctx, now);
+
+        if (trimmedScalingHistory.keySet().removeIf(v -> !vertexSet.contains(v))) {
+            stateStore.storeScalingHistory(ctx, trimmedScalingHistory);
+        }
+    }
+
+    @Nonnull
+    public static <KEY, Context extends JobAutoScalerContext<KEY>>
+            Map<JobVertexID, SortedMap<Instant, ScalingSummary>> getTrimmedScalingHistory(
+                    AutoScalerStateStore<KEY, Context> autoScalerStateStore,
+                    Context context,
+                    Instant now)
+                    throws Exception {
+        var conf = context.getConfiguration();
+        return autoScalerStateStore
+                .getScalingHistory(context)
+                .map(
+                        scalingHistory -> {
+                            var entryIt = scalingHistory.entrySet().iterator();
+                            while (entryIt.hasNext()) {
+                                var entry = entryIt.next();
+                                // Limit how long past scaling decisions are remembered
+                                entry.setValue(
+                                        entry.getValue()
+                                                .tailMap(
+                                                        now.minus(
+                                                                conf.get(
+                                                                        AutoScalerOptions
+                                                                                .VERTEX_SCALING_HISTORY_AGE))));","[{'comment': 'I think this has caused issue in the pasts. It is better to either use a new TreeMap (versus a subset) or clear old entries like this:\r\n\r\n```java\r\n                                                .headMap(\r\n                                                        now.minus(\r\n                                                                conf.get(\r\n                                                                        AutoScalerOptions\r\n                                                                                .VERTEX_SCALING_HISTORY_AGE)))\r\n                                                        .clear();\r\n```', 'commenter': 'mxm'}]"
677,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/state/AutoScalerStateStore.java,"@@ -0,0 +1,73 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.autoscaler.state;
+
+import org.apache.flink.annotation.Experimental;
+import org.apache.flink.autoscaler.JobAutoScalerContext;
+import org.apache.flink.autoscaler.ScalingSummary;
+import org.apache.flink.autoscaler.metrics.CollectedMetrics;
+import org.apache.flink.runtime.jobgraph.JobVertexID;
+
+import java.time.Instant;
+import java.util.Map;
+import java.util.Optional;
+import java.util.SortedMap;
+
+/**
+ * The state store is responsible for store all states during scaling.","[{'comment': '```suggestion\r\n * The state store is responsible for storing all state during scaling.\r\n```', 'commenter': 'mxm'}]"
677,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/state/AutoScalerStateStore.java,"@@ -0,0 +1,73 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.autoscaler.state;
+
+import org.apache.flink.annotation.Experimental;
+import org.apache.flink.autoscaler.JobAutoScalerContext;
+import org.apache.flink.autoscaler.ScalingSummary;
+import org.apache.flink.autoscaler.metrics.CollectedMetrics;
+import org.apache.flink.runtime.jobgraph.JobVertexID;
+
+import java.time.Instant;
+import java.util.Map;
+import java.util.Optional;
+import java.util.SortedMap;
+
+/**
+ * The state store is responsible for store all states during scaling.
+ *
+ * @param <KEY> The job key.
+ * @param <Context> Instance of JobAutoScalerContext.
+ */
+@Experimental
+public interface AutoScalerStateStore<KEY, Context extends JobAutoScalerContext<KEY>> {
+
+    void storeScalingHistory(
+            Context jobContext, Map<JobVertexID, SortedMap<Instant, ScalingSummary>> scalingHistory)
+            throws Exception;
+
+    Optional<Map<JobVertexID, SortedMap<Instant, ScalingSummary>>> getScalingHistory(
+            Context jobContext) throws Exception;
+
+    void removeScalingHistory(Context jobContext) throws Exception;
+
+    void storeEvaluatedMetrics(
+            Context jobContext, SortedMap<Instant, CollectedMetrics> evaluatedMetrics)
+            throws Exception;
+
+    Optional<SortedMap<Instant, CollectedMetrics>> getEvaluatedMetrics(Context jobContext)
+            throws Exception;
+
+    void removeEvaluatedMetrics(Context jobContext) throws Exception;
+
+    void storeParallelismOverrides(Context jobContext, Map<String, String> parallelismOverrides)
+            throws Exception;
+
+    Optional<Map<String, String>> getParallelismOverrides(Context jobContext) throws Exception;
+
+    void removeParallelismOverrides(Context jobContext) throws Exception;
+
+    /**
+     * The flush is needed because we just save data in cache for all store methods, and flush these
+     * data to the physical storage after the flush method is called to improve the performance.","[{'comment': '```suggestion\r\n     * Flushing is needed because we just save data in cache for all store methods. \r\n     * For less write operations, we flush the cached data to the physical storage \r\n     * only after all operations have been performed.\r\n```', 'commenter': 'mxm'}]"
677,flink-autoscaler/src/test/java/org/apache/flink/autoscaler/JobVertexScalerTest.java,"@@ -41,159 +37,104 @@
 import java.util.Map;
 import java.util.TreeMap;
 
-import static org.apache.flink.kubernetes.operator.autoscaler.JobVertexScaler.INNEFFECTIVE_MESSAGE_FORMAT;
+import static org.apache.flink.autoscaler.JobVertexScaler.INEFFECTIVE_SCALING;
+import static org.apache.flink.autoscaler.JobVertexScaler.INNEFFECTIVE_MESSAGE_FORMAT;
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.assertj.core.api.Assertions.assertThatExceptionOfType;
 import static org.junit.jupiter.api.Assertions.assertEquals;
 import static org.junit.jupiter.api.Assertions.assertFalse;
 import static org.junit.jupiter.api.Assertions.assertTrue;
 
 /** Test for vertex parallelism scaler logic. */
-@EnableKubernetesMockClient(crud = true)
 public class JobVertexScalerTest {
 
-    private JobVertexScaler vertexScaler;
-    private Configuration conf;
-
-    private KubernetesClient kubernetesClient;
-    private EventCollector eventCollector;
-
-    private FlinkDeployment flinkDep;
+    private EventCollector<JobID, JobAutoScalerContext<JobID>> eventCollector;
+    private JobVertexScaler<JobID, JobAutoScalerContext<JobID>> vertexScaler;
+    private JobAutoScalerContext<JobID> context;
 
     @BeforeEach
     public void setup() {
-        flinkDep = TestUtils.buildApplicationCluster();
-        kubernetesClient.resource(flinkDep).createOrReplace();
-        eventCollector = new EventCollector();
-        vertexScaler = new JobVertexScaler(new EventRecorder(eventCollector));
-        conf = new Configuration();
+        eventCollector = new EventCollector<>();
+        vertexScaler = new JobVertexScaler<>(eventCollector);
+        var conf = new Configuration();
         conf.set(AutoScalerOptions.MAX_SCALE_DOWN_FACTOR, 1.);
         conf.set(AutoScalerOptions.MAX_SCALE_UP_FACTOR, (double) Integer.MAX_VALUE);
         conf.set(AutoScalerOptions.CATCH_UP_DURATION, Duration.ZERO);
+        JobID jobID = new JobID();
+        context =
+                new JobAutoScalerContext<>(
+                        jobID, jobID, conf, new UnregisteredMetricsGroup(), null);
     }
 
     @Test
     public void testParallelismScaling() {
         var op = new JobVertexID();
-        conf.set(AutoScalerOptions.TARGET_UTILIZATION, 1.);
+        context.getConfiguration().set(AutoScalerOptions.TARGET_UTILIZATION, 1.);","[{'comment': 'Can we re-add `conf` above and initialize it with `context.getconfiguration()`?', 'commenter': 'mxm'}]"
677,flink-autoscaler/src/test/java/org/apache/flink/autoscaler/RecommendedParallelismTest.java,"@@ -217,73 +198,38 @@ public void endToEnd() throws Exception {
 
         now = now.plus(Duration.ofSeconds(10));
         setClocksTo(now);
-        restart(now);
-
-        // after restart while the job is not running the evaluated metrics are gone
-        autoscaler.scale(getResourceContext(app, ctx));
-        assertEquals(3, getOrCreateInfo(app, kubernetesClient).getMetricHistory().size());
-        assertNull(autoscaler.lastEvaluatedMetrics.get(ResourceID.fromResource(app)));
-        scaledParallelism = ScalingExecutorTest.getScaledParallelism(kubernetesClient, app);
-        assertEquals(4, scaledParallelism.get(source));
-        assertEquals(4, scaledParallelism.get(sink));
-","[{'comment': 'Why did we remove this check?', 'commenter': 'mxm'}]"
682,flink-kubernetes-operator-autoscaler/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/validation/AutoScalerValidator.java,"@@ -0,0 +1,103 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.autoscaler.validation;
+
+import org.apache.flink.configuration.ConfigOption;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.kubernetes.operator.api.FlinkDeployment;
+import org.apache.flink.kubernetes.operator.api.FlinkSessionJob;
+import org.apache.flink.kubernetes.operator.api.spec.FlinkDeploymentSpec;
+import org.apache.flink.kubernetes.operator.autoscaler.config.AutoScalerOptions;
+import org.apache.flink.kubernetes.operator.utils.FlinkUtils;
+import org.apache.flink.kubernetes.operator.validation.FlinkResourceValidator;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.Optional;
+
+/** Validator implementation for the AutoScaler from {@link Configuration}. */
+public class AutoScalerValidator implements FlinkResourceValidator {
+
+    private static final Logger LOG = LoggerFactory.getLogger(FlinkUtils.class);
+
+    @Override
+    public Optional<String> validateSessionJob(
+            FlinkSessionJob sessionJob, Optional<FlinkDeployment> session) {
+        LOG.info(""AutoScaler Configurations validator {}"", sessionJob);
+        var spec = sessionJob.getSpec();
+        if (spec.getFlinkConfiguration() != null) {
+            var flinkConfiguration = Configuration.fromMap(spec.getFlinkConfiguration());
+            return validateAutoScalerFlinkConfiguration(flinkConfiguration);
+        }
+        return Optional.empty();
+    }
+
+    @Override
+    public Optional<String> validateDeployment(FlinkDeployment deployment) {
+        LOG.info(""AutoScaler Configurations validator {}"", deployment);
+        FlinkDeploymentSpec spec = deployment.getSpec();
+        if (spec.getFlinkConfiguration() != null) {
+            var flinkConfiguration = Configuration.fromMap(spec.getFlinkConfiguration());
+            return validateAutoScalerFlinkConfiguration(flinkConfiguration);
+        }
+        return Optional.empty();
+    }
+
+    private Optional<String> validateAutoScalerFlinkConfiguration(
+            Configuration flinkConfiguration) {
+        return firstPresent(
+                validateNumber(
+                        flinkConfiguration, AutoScalerOptions.MAX_SCALE_DOWN_FACTOR, 0.0d, 1.0d),
+                validateNumber(
+                        flinkConfiguration, AutoScalerOptions.MAX_SCALE_UP_FACTOR, 1.0d, 100000.0d),","[{'comment': ""It is possible that some users have already set this to a higher value than 100.000 in their setup when they don't want to limit the scale-up, wouldn't it make sense to set the max value of this config to `Double.MAX_VALUE`? Just to make sure we don't introduce breaking changes in these scenarios."", 'commenter': 'mateczagany'}, {'comment': ""I agree let's not validate numbers that do not actually cause problems"", 'commenter': 'gyfora'}, {'comment': 'Fixed', 'commenter': 'srpraneeth'}]"
682,flink-kubernetes-operator-autoscaler/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/validation/AutoScalerValidator.java,"@@ -0,0 +1,103 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.autoscaler.validation;
+
+import org.apache.flink.configuration.ConfigOption;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.kubernetes.operator.api.FlinkDeployment;
+import org.apache.flink.kubernetes.operator.api.FlinkSessionJob;
+import org.apache.flink.kubernetes.operator.api.spec.FlinkDeploymentSpec;
+import org.apache.flink.kubernetes.operator.autoscaler.config.AutoScalerOptions;
+import org.apache.flink.kubernetes.operator.utils.FlinkUtils;
+import org.apache.flink.kubernetes.operator.validation.FlinkResourceValidator;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.Optional;
+
+/** Validator implementation for the AutoScaler from {@link Configuration}. */
+public class AutoScalerValidator implements FlinkResourceValidator {
+
+    private static final Logger LOG = LoggerFactory.getLogger(FlinkUtils.class);
+
+    @Override
+    public Optional<String> validateSessionJob(
+            FlinkSessionJob sessionJob, Optional<FlinkDeployment> session) {
+        LOG.info(""AutoScaler Configurations validator {}"", sessionJob);
+        var spec = sessionJob.getSpec();
+        if (spec.getFlinkConfiguration() != null) {
+            var flinkConfiguration = Configuration.fromMap(spec.getFlinkConfiguration());
+            return validateAutoScalerFlinkConfiguration(flinkConfiguration);
+        }
+        return Optional.empty();
+    }
+
+    @Override
+    public Optional<String> validateDeployment(FlinkDeployment deployment) {
+        LOG.info(""AutoScaler Configurations validator {}"", deployment);
+        FlinkDeploymentSpec spec = deployment.getSpec();
+        if (spec.getFlinkConfiguration() != null) {
+            var flinkConfiguration = Configuration.fromMap(spec.getFlinkConfiguration());
+            return validateAutoScalerFlinkConfiguration(flinkConfiguration);
+        }
+        return Optional.empty();
+    }
+
+    private Optional<String> validateAutoScalerFlinkConfiguration(
+            Configuration flinkConfiguration) {
+        return firstPresent(
+                validateNumber(
+                        flinkConfiguration, AutoScalerOptions.MAX_SCALE_DOWN_FACTOR, 0.0d, 1.0d),","[{'comment': ""Let's not validate the max value here, only that is not negative."", 'commenter': 'gyfora'}, {'comment': 'Fixed', 'commenter': 'srpraneeth'}]"
682,flink-kubernetes-operator-autoscaler/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/validation/AutoScalerValidator.java,"@@ -0,0 +1,103 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.autoscaler.validation;
+
+import org.apache.flink.configuration.ConfigOption;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.kubernetes.operator.api.FlinkDeployment;
+import org.apache.flink.kubernetes.operator.api.FlinkSessionJob;
+import org.apache.flink.kubernetes.operator.api.spec.FlinkDeploymentSpec;
+import org.apache.flink.kubernetes.operator.autoscaler.config.AutoScalerOptions;
+import org.apache.flink.kubernetes.operator.utils.FlinkUtils;
+import org.apache.flink.kubernetes.operator.validation.FlinkResourceValidator;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.Optional;
+
+/** Validator implementation for the AutoScaler from {@link Configuration}. */
+public class AutoScalerValidator implements FlinkResourceValidator {
+
+    private static final Logger LOG = LoggerFactory.getLogger(FlinkUtils.class);
+
+    @Override
+    public Optional<String> validateSessionJob(
+            FlinkSessionJob sessionJob, Optional<FlinkDeployment> session) {
+        LOG.info(""AutoScaler Configurations validator {}"", sessionJob);
+        var spec = sessionJob.getSpec();
+        if (spec.getFlinkConfiguration() != null) {
+            var flinkConfiguration = Configuration.fromMap(spec.getFlinkConfiguration());
+            return validateAutoScalerFlinkConfiguration(flinkConfiguration);
+        }
+        return Optional.empty();
+    }
+
+    @Override
+    public Optional<String> validateDeployment(FlinkDeployment deployment) {
+        LOG.info(""AutoScaler Configurations validator {}"", deployment);
+        FlinkDeploymentSpec spec = deployment.getSpec();
+        if (spec.getFlinkConfiguration() != null) {
+            var flinkConfiguration = Configuration.fromMap(spec.getFlinkConfiguration());
+            return validateAutoScalerFlinkConfiguration(flinkConfiguration);
+        }
+        return Optional.empty();
+    }
+
+    private Optional<String> validateAutoScalerFlinkConfiguration(
+            Configuration flinkConfiguration) {
+        return firstPresent(
+                validateNumber(
+                        flinkConfiguration, AutoScalerOptions.MAX_SCALE_DOWN_FACTOR, 0.0d, 1.0d),
+                validateNumber(
+                        flinkConfiguration, AutoScalerOptions.MAX_SCALE_UP_FACTOR, 1.0d, 100000.0d),
+                validateNumber(","[{'comment': ""Same as before let's not validate the max"", 'commenter': 'gyfora'}, {'comment': 'Fixed', 'commenter': 'srpraneeth'}]"
682,flink-kubernetes-operator-autoscaler/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/validation/AutoScalerValidator.java,"@@ -0,0 +1,103 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.autoscaler.validation;
+
+import org.apache.flink.configuration.ConfigOption;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.kubernetes.operator.api.FlinkDeployment;
+import org.apache.flink.kubernetes.operator.api.FlinkSessionJob;
+import org.apache.flink.kubernetes.operator.api.spec.FlinkDeploymentSpec;
+import org.apache.flink.kubernetes.operator.autoscaler.config.AutoScalerOptions;
+import org.apache.flink.kubernetes.operator.utils.FlinkUtils;
+import org.apache.flink.kubernetes.operator.validation.FlinkResourceValidator;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.Optional;
+
+/** Validator implementation for the AutoScaler from {@link Configuration}. */
+public class AutoScalerValidator implements FlinkResourceValidator {
+
+    private static final Logger LOG = LoggerFactory.getLogger(FlinkUtils.class);
+
+    @Override
+    public Optional<String> validateSessionJob(
+            FlinkSessionJob sessionJob, Optional<FlinkDeployment> session) {
+        LOG.info(""AutoScaler Configurations validator {}"", sessionJob);
+        var spec = sessionJob.getSpec();
+        if (spec.getFlinkConfiguration() != null) {
+            var flinkConfiguration = Configuration.fromMap(spec.getFlinkConfiguration());
+            return validateAutoScalerFlinkConfiguration(flinkConfiguration);
+        }
+        return Optional.empty();
+    }
+
+    @Override
+    public Optional<String> validateDeployment(FlinkDeployment deployment) {
+        LOG.info(""AutoScaler Configurations validator {}"", deployment);
+        FlinkDeploymentSpec spec = deployment.getSpec();
+        if (spec.getFlinkConfiguration() != null) {
+            var flinkConfiguration = Configuration.fromMap(spec.getFlinkConfiguration());
+            return validateAutoScalerFlinkConfiguration(flinkConfiguration);
+        }
+        return Optional.empty();
+    }
+
+    private Optional<String> validateAutoScalerFlinkConfiguration(
+            Configuration flinkConfiguration) {
+        return firstPresent(
+                validateNumber(
+                        flinkConfiguration, AutoScalerOptions.MAX_SCALE_DOWN_FACTOR, 0.0d, 1.0d),
+                validateNumber(
+                        flinkConfiguration, AutoScalerOptions.MAX_SCALE_UP_FACTOR, 1.0d, 100000.0d),
+                validateNumber(
+                        flinkConfiguration, AutoScalerOptions.TARGET_UTILIZATION, 0.0d, 1.0d),
+                validateNumber(
+                        flinkConfiguration,
+                        AutoScalerOptions.TARGET_UTILIZATION_BOUNDARY,","[{'comment': ""Same as before let's not validate the max"", 'commenter': 'gyfora'}, {'comment': 'Fixed', 'commenter': 'srpraneeth'}]"
682,flink-kubernetes-operator-autoscaler/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/validation/AutoScalerValidator.java,"@@ -0,0 +1,109 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.autoscaler.validation;
+
+import org.apache.flink.configuration.ConfigOption;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.kubernetes.operator.api.FlinkDeployment;
+import org.apache.flink.kubernetes.operator.api.FlinkSessionJob;
+import org.apache.flink.kubernetes.operator.api.spec.FlinkDeploymentSpec;
+import org.apache.flink.kubernetes.operator.autoscaler.config.AutoScalerOptions;
+import org.apache.flink.kubernetes.operator.utils.FlinkUtils;
+import org.apache.flink.kubernetes.operator.validation.FlinkResourceValidator;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.Optional;
+
+/** Validator implementation for the AutoScaler from {@link Configuration}. */
+public class AutoScalerValidator implements FlinkResourceValidator {
+
+    private static final Logger LOG = LoggerFactory.getLogger(FlinkUtils.class);
+
+    @Override
+    public Optional<String> validateSessionJob(
+            FlinkSessionJob sessionJob, Optional<FlinkDeployment> session) {
+        LOG.info(""AutoScaler Configurations validator {}"", sessionJob);
+        var spec = sessionJob.getSpec();
+        if (spec.getFlinkConfiguration() != null) {
+            var flinkConfiguration = Configuration.fromMap(spec.getFlinkConfiguration());
+            return validateAutoScalerFlinkConfiguration(flinkConfiguration);
+        }
+        return Optional.empty();
+    }
+
+    @Override
+    public Optional<String> validateDeployment(FlinkDeployment deployment) {
+        LOG.info(""AutoScaler Configurations validator {}"", deployment);
+        FlinkDeploymentSpec spec = deployment.getSpec();
+        if (spec.getFlinkConfiguration() != null) {
+            var flinkConfiguration = Configuration.fromMap(spec.getFlinkConfiguration());
+            return validateAutoScalerFlinkConfiguration(flinkConfiguration);
+        }
+        return Optional.empty();
+    }
+
+    private Optional<String> validateAutoScalerFlinkConfiguration(
+            Configuration flinkConfiguration) {
+        return firstPresent(
+                validateNumber(flinkConfiguration, AutoScalerOptions.MAX_SCALE_DOWN_FACTOR, 0.0d),
+                validateNumber(flinkConfiguration, AutoScalerOptions.MAX_SCALE_UP_FACTOR, 1.0d),
+                validateNumber(flinkConfiguration, AutoScalerOptions.TARGET_UTILIZATION, 0.0d),
+                validateNumber(
+                        flinkConfiguration,
+                        AutoScalerOptions.TARGET_UTILIZATION_BOUNDARY,
+                        0.0d,
+                        1.0d));","[{'comment': 'I think we should not validate the max here.', 'commenter': 'gyfora'}, {'comment': 'Fixed', 'commenter': 'srpraneeth'}]"
682,flink-kubernetes-operator-autoscaler/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/validation/AutoScalerValidator.java,"@@ -0,0 +1,109 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.autoscaler.validation;
+
+import org.apache.flink.configuration.ConfigOption;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.kubernetes.operator.api.FlinkDeployment;
+import org.apache.flink.kubernetes.operator.api.FlinkSessionJob;
+import org.apache.flink.kubernetes.operator.api.spec.FlinkDeploymentSpec;
+import org.apache.flink.kubernetes.operator.autoscaler.config.AutoScalerOptions;
+import org.apache.flink.kubernetes.operator.utils.FlinkUtils;
+import org.apache.flink.kubernetes.operator.validation.FlinkResourceValidator;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.Optional;
+
+/** Validator implementation for the AutoScaler from {@link Configuration}. */
+public class AutoScalerValidator implements FlinkResourceValidator {
+
+    private static final Logger LOG = LoggerFactory.getLogger(FlinkUtils.class);
+
+    @Override
+    public Optional<String> validateSessionJob(
+            FlinkSessionJob sessionJob, Optional<FlinkDeployment> session) {
+        LOG.info(""AutoScaler Configurations validator {}"", sessionJob);
+        var spec = sessionJob.getSpec();
+        if (spec.getFlinkConfiguration() != null) {
+            var flinkConfiguration = Configuration.fromMap(spec.getFlinkConfiguration());
+            return validateAutoScalerFlinkConfiguration(flinkConfiguration);
+        }
+        return Optional.empty();
+    }
+
+    @Override
+    public Optional<String> validateDeployment(FlinkDeployment deployment) {
+        LOG.info(""AutoScaler Configurations validator {}"", deployment);
+        FlinkDeploymentSpec spec = deployment.getSpec();
+        if (spec.getFlinkConfiguration() != null) {
+            var flinkConfiguration = Configuration.fromMap(spec.getFlinkConfiguration());
+            return validateAutoScalerFlinkConfiguration(flinkConfiguration);
+        }
+        return Optional.empty();
+    }
+
+    private Optional<String> validateAutoScalerFlinkConfiguration(
+            Configuration flinkConfiguration) {
+        return firstPresent(
+                validateNumber(flinkConfiguration, AutoScalerOptions.MAX_SCALE_DOWN_FACTOR, 0.0d),
+                validateNumber(flinkConfiguration, AutoScalerOptions.MAX_SCALE_UP_FACTOR, 1.0d),
+                validateNumber(flinkConfiguration, AutoScalerOptions.TARGET_UTILIZATION, 0.0d),
+                validateNumber(
+                        flinkConfiguration,
+                        AutoScalerOptions.TARGET_UTILIZATION_BOUNDARY,
+                        0.0d,
+                        1.0d));
+    }
+
+    private <T extends Number> Optional<String> validateNumber(
+            Configuration flinkConfiguration,
+            ConfigOption<T> autoScalerConfig,
+            Double min,
+            Double max) {
+        var configValue = flinkConfiguration.get(autoScalerConfig);","[{'comment': 'This line may throw an exception if the config value is not actually a number, we should catch and turn it into a validation error probably.', 'commenter': 'gyfora'}, {'comment': 'The IllegalArgEx is caught and handled as a validation error. ', 'commenter': 'srpraneeth'}]"
682,flink-kubernetes-operator-autoscaler/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/validation/AutoScalerValidator.java,"@@ -0,0 +1,109 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.autoscaler.validation;
+
+import org.apache.flink.configuration.ConfigOption;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.kubernetes.operator.api.FlinkDeployment;
+import org.apache.flink.kubernetes.operator.api.FlinkSessionJob;
+import org.apache.flink.kubernetes.operator.api.spec.FlinkDeploymentSpec;
+import org.apache.flink.kubernetes.operator.autoscaler.config.AutoScalerOptions;
+import org.apache.flink.kubernetes.operator.utils.FlinkUtils;
+import org.apache.flink.kubernetes.operator.validation.FlinkResourceValidator;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.Optional;
+
+/** Validator implementation for the AutoScaler from {@link Configuration}. */
+public class AutoScalerValidator implements FlinkResourceValidator {
+
+    private static final Logger LOG = LoggerFactory.getLogger(FlinkUtils.class);
+
+    @Override
+    public Optional<String> validateSessionJob(
+            FlinkSessionJob sessionJob, Optional<FlinkDeployment> session) {
+        LOG.info(""AutoScaler Configurations validator {}"", sessionJob);
+        var spec = sessionJob.getSpec();
+        if (spec.getFlinkConfiguration() != null) {
+            var flinkConfiguration = Configuration.fromMap(spec.getFlinkConfiguration());
+            return validateAutoScalerFlinkConfiguration(flinkConfiguration);
+        }
+        return Optional.empty();
+    }
+
+    @Override
+    public Optional<String> validateDeployment(FlinkDeployment deployment) {
+        LOG.info(""AutoScaler Configurations validator {}"", deployment);","[{'comment': 'I think we should only validate these configs if the autoscaler itself is enabled. It would be a bit strange to get a validation error if you are not even using the autoscaler :) ', 'commenter': 'gyfora'}, {'comment': 'For this you may also need the default config, you can take a look at how to get that based on the `DefaultValidator`', 'commenter': 'gyfora'}, {'comment': 'I have added a check to validate only if the autoscaler is enabled. I added unit test and tested through e2e test to make sure that it works. \r\n\r\nCan you please explain what is default config for. \r\nI see that in the validator utils the default config is plugged in through a FlinkConfigManager (through constructor in DefaultValidator) but that may not be straight forward in case of AutoScalerValidator as AutoScalerValidator is not present in the operator module. It is discovered through ServiceLoader instead from the autoscaler module at runtime. \r\n\r\nI added a UT to make sure that with out the default config still the flinkConfiguration.get would get me the default value for the autoscaler configurations. \r\nLet me know if you still think I should add the default config I can look into it. \r\n\r\n', 'commenter': 'srpraneeth'}]"
682,e2e-tests/data/autoscaler.yaml,"@@ -41,6 +41,13 @@ spec:
     kubernetes.operator.job.autoscaler.stabilization.interval: ""5s""
     kubernetes.operator.job.autoscaler.metrics.window: ""1m""
 
+#    Invalid Validations for testing autoscaler configurations
+  #    kubernetes.operator.job.autoscaler.scale-down.max-factor: ""-0.6""
+  #    kubernetes.operator.job.autoscaler.scale-up.max-factor: ""100001.0""
+  #    pipeline.max-parallelism: ""2""
+  #    kubernetes.operator.job.autoscaler.target.utilization: ""10.7""","[{'comment': 'we should update this as the large values are not invalid anymore right?', 'commenter': 'gyfora'}, {'comment': 'Updated. Thanks for catching this.. ', 'commenter': 'srpraneeth'}]"
682,flink-kubernetes-operator-autoscaler/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/validation/AutoScalerValidator.java,"@@ -0,0 +1,109 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.autoscaler.validation;
+
+import org.apache.flink.configuration.ConfigOption;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.kubernetes.operator.api.FlinkDeployment;
+import org.apache.flink.kubernetes.operator.api.FlinkSessionJob;
+import org.apache.flink.kubernetes.operator.api.spec.FlinkDeploymentSpec;
+import org.apache.flink.kubernetes.operator.autoscaler.config.AutoScalerOptions;
+import org.apache.flink.kubernetes.operator.utils.FlinkUtils;
+import org.apache.flink.kubernetes.operator.validation.FlinkResourceValidator;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.Optional;
+
+/** Validator implementation for the AutoScaler from {@link Configuration}. */
+public class AutoScalerValidator implements FlinkResourceValidator {
+
+    private static final Logger LOG = LoggerFactory.getLogger(FlinkUtils.class);
+
+    @Override
+    public Optional<String> validateSessionJob(
+            FlinkSessionJob sessionJob, Optional<FlinkDeployment> session) {
+        LOG.info(""AutoScaler Configurations validator {}"", sessionJob);
+        var spec = sessionJob.getSpec();
+        if (spec.getFlinkConfiguration() != null) {
+            var flinkConfiguration = Configuration.fromMap(spec.getFlinkConfiguration());
+            return validateAutoScalerFlinkConfiguration(flinkConfiguration);
+        }
+        return Optional.empty();
+    }
+
+    @Override
+    public Optional<String> validateDeployment(FlinkDeployment deployment) {
+        LOG.info(""AutoScaler Configurations validator {}"", deployment);
+        FlinkDeploymentSpec spec = deployment.getSpec();
+        if (spec.getFlinkConfiguration() != null) {
+            var flinkConfiguration = Configuration.fromMap(spec.getFlinkConfiguration());
+            return validateAutoScalerFlinkConfiguration(flinkConfiguration);
+        }
+        return Optional.empty();
+    }
+
+    private Optional<String> validateAutoScalerFlinkConfiguration(
+            Configuration flinkConfiguration) {
+        return firstPresent(
+                validateNumber(flinkConfiguration, AutoScalerOptions.MAX_SCALE_DOWN_FACTOR, 0.0d),
+                validateNumber(flinkConfiguration, AutoScalerOptions.MAX_SCALE_UP_FACTOR, 1.0d),","[{'comment': 'I think this may be incorrect, MAX_SCALE_UP factor can be anything positive. Probably best to simplify all validations as ""non-negative"" ', 'commenter': 'gyfora'}, {'comment': 'Done.. Fixed.. Now all the above options are just validated for the non negative number. \r\n\r\n|| Rule      | Affecting Property | MinValue | MaxValue | \r\n|-| ----------- | ----------- | ----------- | ----------- | \r\n|1| validateMaxScaleDownFactor | kubernetes.operator.job.autoscaler.scale-down.max-factor | 0.0d | None |\r\n|2| validateMaxScaleUpFactor   | kubernetes.operator.job.autoscaler.scale-up.max-factor | 0.0d | None |\r\n|3| validateTargetUtilization   | kubernetes.operator.job.autoscaler.target.utilization | 0.0d | None | \r\n|4| validateTargetUtilizationBoundary   | kubernetes.operator.job.autoscaler.target.utilization.boundary | 0.0d | None | ', 'commenter': 'srpraneeth'}]"
683,flink-kubernetes-operator-autoscaler/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/config/AutoScalerOptions.java,"@@ -168,4 +168,10 @@ private static ConfigOptions.OptionBuilder autoScalerConfig(String key) {
                     .defaultValues()
                     .withDescription(
                             ""A (semicolon-separated) list of vertex ids in hexstring for which to disable scaling. Caution: For non-sink vertices this will still scale their downstream operators until https://issues.apache.org/jira/browse/FLINK-31215 is implemented."");
+
+    public static final ConfigOption<Long> SCALING_REPORT_INTERVAL =
+            autoScalerConfig(""scaling.report.interval"")
+                    .longType()","[{'comment': 'This should be a `Duration` type config', 'commenter': 'gyfora'}, {'comment': 'updated.', 'commenter': 'clarax'}]"
683,flink-kubernetes-operator-autoscaler/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/ScalingExecutor.java,"@@ -94,14 +94,15 @@ public boolean scaleResource(
         var scalingEnabled = conf.get(SCALING_ENABLED);
 
         var scalingReport = scalingReport(scalingSummaries, scalingEnabled);
-        eventRecorder.triggerEvent(
+        eventRecorder.triggerEventByInterval(","[{'comment': 'We should only control the events by interval if `scalingEnabled == false` (in advisor mode). Regular scaling should always trigger the event', 'commenter': 'gyfora'}, {'comment': 'updated.', 'commenter': 'clarax'}]"
685,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/event/AutoScalerEventHandler.java,"@@ -33,19 +35,15 @@
 @Experimental
 public interface AutoScalerEventHandler<KEY, Context extends JobAutoScalerContext<KEY>> {
 
-    /**
-     * Handle the event.
-     *
-     * @param interval When interval is great than 0, events that repeat within the interval will be
-     *     ignored.
-     */
+    /** Handle the event. */","[{'comment': ""I think the entire eventHandler interface is getting pretty weird. It was already not very nice before this change but now it's even worse because we are leaking too many internals of the operator event triggering mechanism. \r\n\r\nI think we need to break up the handleEvent into two new methods.\r\n\r\n```\r\n// Would be the current logic but without the interval logic\r\nhandleGenericAutoscalerEvent(...)\r\n\r\n// Handle scaling events separately. Then the operator can hide the complex interval/label triggering logic\r\nhandleScalingEvent(Context,  Map<JobVertexID, ScalingSummary> scalingSummaries, scalingEnabled)\r\n```"", 'commenter': 'gyfora'}, {'comment': 'And we could even have a default implementation within the interface itself to turn it into a generic event in case we are not running on the operator', 'commenter': 'gyfora'}, {'comment': 'Thank you for calling out. I was not sure if I should have a separate method, not sure what this handler interfaceshould be handing. We can move the logic of handleGenericAutoscalerEven to the interface and make it an abstract class. What do you think? @gyfora ', 'commenter': 'clarax'}, {'comment': ""I don't understand the suggestion @clarax what do you think my proposal? Have those 2 methods in the interface and add a default implementation for `handleScalingEvent`. \r\n\r\n"", 'commenter': 'gyfora'}, {'comment': 'Updated for two methods.  I don\'t understand ""a default implementation within the interface itself to turn it into a generic event in case we are not running on the operator"". I made the second method a generic method for any suppression callers want to use.\r\n', 'commenter': 'clarax'}, {'comment': ""The reason for removing the interval from this interface is because interval just works for kubernetesEventHandler, right? \r\n\r\nI wanna whether it's needed for Standalone Autoscaler, if it's needed, we should keep the `interval` here.\r\n\r\nActually, I prefer keep it first. And removing it if we are sure it's really not needed in the future. WDYT? cc @gyfora \r\n\r\nAlso, if the interval is not needed here. `AutoScalerOptions#SCALING_REPORT_INTERVAL` should be moved into `flink-kubernetes-operator` module.\r\n"", 'commenter': '1996fanrui'}, {'comment': ""I agree, we can keep the interval as a fixed parameter to the reporter methods. That way we don't need custom config handling in the operator. "", 'commenter': 'gyfora'}, {'comment': ""Interval is needed for all AutoScalerHandler. The reason to remove it from parameter list  is context contains config which defines the interval. I think this is the advantage using a context to wrap up all info to pass so we don't need to use so many parameters in the method. The same reason we don't need to pass scalingEnabled in handleScalingEvent. Thank you for constructing the context during refactoring. @1996fanrui "", 'commenter': 'clarax'}, {'comment': ""I still think it's better to pass Interval / scalingEnabled directly. They are part of the intended behaviour of the method. If we don't pass it, we rely on implementers to figure out how exactly to get this from the config. It would be super easy to miss and come up with an incorrect implementation "", 'commenter': 'gyfora'}, {'comment': 'Updated the PR with the change. Added comments to explain too.', 'commenter': 'clarax'}]"
685,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/KubernetesAutoScalerEventHandler.java,"@@ -43,26 +52,71 @@ public void handleEvent(
             String reason,
             String message,
             @Nullable String messageKey,
-            @Nullable Duration interval) {
-        if (interval == null) {
-            eventRecorder.triggerEvent(
-                    context.getResource(),
-                    EventRecorder.Type.valueOf(type.name()),
-                    reason,
-                    message,
-                    EventRecorder.Component.Operator,
-                    messageKey,
-                    context.getKubernetesClient());
+            @Nonnull Duration interval) {","[{'comment': ""`interval` isn't used here, so it doesn't take effect. \r\n\r\nFrom this implementation, the interface deserves to be discussed how to support interval capabilities more reasonably."", 'commenter': '1996fanrui'}, {'comment': ""Hi Fanrui, KubernetesAutoScalerEventHandler doesn't support interval based dedupe. As discussed, we'd like to keep it in the interface method signature but the actually implementation can choose to ignore it. It is possible we expand the support for those event too as we need for scaling."", 'commenter': 'clarax'}]"
685,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/KubernetesAutoScalerEventHandler.java,"@@ -43,26 +52,71 @@ public void handleEvent(
             String reason,
             String message,
             @Nullable String messageKey,
-            @Nullable Duration interval) {
-        if (interval == null) {
-            eventRecorder.triggerEvent(
-                    context.getResource(),
-                    EventRecorder.Type.valueOf(type.name()),
-                    reason,
-                    message,
-                    EventRecorder.Component.Operator,
-                    messageKey,
-                    context.getKubernetesClient());
+            @Nonnull Duration interval) {
+        eventRecorder.triggerEvent(
+                context.getResource(),
+                EventRecorder.Type.valueOf(type.name()),
+                reason,
+                message,
+                EventRecorder.Component.Operator,
+                messageKey,
+                context.getKubernetesClient());
+    }
+
+    @Override
+    public void handleScalingEvent(
+            KubernetesJobAutoScalerContext context,
+            Map<JobVertexID, ScalingSummary> scalingSummaries,
+            boolean scaled,
+            @Nonnull Duration interval) {
+        if (scaled) {
+            AutoScalerEventHandler.super.handleScalingEvent(
+                    context, scalingSummaries, scaled, interval);
         } else {
-            eventRecorder.triggerEventByInterval(
+            var conf = context.getConfiguration();
+            var scalingReport = AutoScalerEventHandler.scalingReport(scalingSummaries, scaled);
+            var labels = Map.of(PARALLELISM_MAP_KEY, getParallelismHashCode(scalingSummaries));","[{'comment': ""Hi @clarax ,IIUC, the current strategy is: when the scaled is disabled, we want to remove duplicates based on `getParallelismHashCode(scalingSummaries)` and interval, right?\r\n\r\nIf yes, how about we don't change the `AutoScalerEventHandler` and consider `getParallelismHashCode(scalingSummaries)` as the messageKey? \r\n\r\nAnd refactor ingthe `EventUtils`:\r\n\r\n-  Ignoring repeating message based on the `generatedMessageKey` and `interval`\r\n- `generatedMessageKey = messageKey != null ? messageKey : message;`\r\n- For existed event,  when the {@link #MESSAGE_KEY} exists in label, it's messageKey, otherwise the message is messageKey.\r\n\r\nHere is my POC commit[1], it maybe clearer. If the POC has the same effect as the current PR, I like this POC.\r\n\r\nBecause the interface is clearer and  `AutoScalerEventHandler` doesn't care the logic about `handleScalingEvent`.\r\n\r\nPlease correct me if my understanding is wrong, thanks~\r\n\r\n[1] https://github.com/1996fanrui/flink-kubernetes-operator/commit/14e8f189f4d129eba23050ea37fe8c012f0a22d2"", 'commenter': '1996fanrui'}, {'comment': 'cc @gyfora ', 'commenter': '1996fanrui'}, {'comment': ""@1996fanrui The problem with your approach is that it breaks the messageKey logic for scaling events (you set a new key every time the parallelism changes but only when scaling is off)\r\n\r\nThe general problem with the current interface is that it already contains many Kubernetes specific parts (messageKey) but not enough to properly implement new functionality (such as the one we are trying here to deduplicate events based on certain logic, like the actual parallelism overrides)\r\n\r\nIn other interfaces like the StateStore we decided to go with specific methods, and I think that's what we should do here as well. I prefer @clarax s solution as it allows us complete flexibility in the Kubernetes implementation without pushing anything on to the interface.\r\n\r\nTo give a bit more background on the `messageKey` , we use that to define the specific event when triggering in Kubernetes such that we keep the history (last triggered, etc) without always creating new event objects. This is completely independent of the interval / parallelism based triggering that are trying to do here."", 'commenter': 'gyfora'}, {'comment': ""Hi @gyfora , thanks for your explanation!\r\n\r\n> you set a new key every time the parallelism changes but only when scaling is off\r\n \r\nI set it based on the logic of current PR. Do you mean my POC logic isn't exactly same with the current PR? I may have some misunderstand here.\r\n\r\nFYI: I do this POC due to I want to design an interface for support the kubernetes requirement, and let it as generic as possible. If design a generic interface is very hard here, the current design is good to me."", 'commenter': '1996fanrui'}, {'comment': '@1996fanrui in Claras PR the message key is fixed: `ScalingExecutor` in your POC PR the message key is derived from the parallelism, so it changes all the time. They are not the same.', 'commenter': 'gyfora'}, {'comment': 'Hi @1996fanrui, Thanks for the proposed alternative, just adding to @gyfora \'s comment, \r\n""To give a bit more background on the messageKey , we use that to define the specific event when triggering in Kubernetes such that we keep the history (last triggered, etc) without always creating new event objects. This is completely independent of the interval / parallelism based triggering that are trying to do here.\r\n""\r\n\r\nOverriding messageKey will totally break the design of using messageKey to keep history of certain type of events at minimal cost. We keep using the same Kubernetes event object and just need to increment the count of the object and updating the messsages. The dedupe logic by interval is independent where we don\'t even update the kubenetes event object or trigger any operator events. The behaviors of your POC is different from this PR.', 'commenter': 'clarax'}, {'comment': 'Thanks for the clarification!\r\n\r\nSounds good to me.', 'commenter': '1996fanrui'}]"
685,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/EventUtils.java,"@@ -117,53 +119,53 @@ public static boolean createByInterval(
             EventRecorder.Component component,
             Consumer<Event> eventListener,
             @Nullable String messageKey,
-            Duration interval) {
-
+            @Nullable BiPredicate<Map<String, String>, Instant> suppressionPredicate,
+            @Nullable Map<String, String> labels) {
         String eventName =
                 generateEventName(
                         target, type, reason, messageKey != null ? messageKey : message, component);
         Event existing = findExistingEvent(client, target, eventName);
 
         if (existing != null) {
-            if (Objects.equals(existing.getMessage(), message)
-                    && Instant.now()
-                            .isBefore(
-                                    Instant.parse(existing.getLastTimestamp())
-                                            .plusMillis(interval.toMillis()))) {
-                return false;
-            } else {
-                createUpdatedEvent(existing, client, message, eventListener);
+            if (suppressionPredicate != null
+                    && existing.getMetadata() != null
+                    && suppressionPredicate.test(
+                            existing.getMetadata().getLabels(),
+                            Instant.parse(existing.getLastTimestamp()))) {
                 return false;
             }
+            updatedEventWithLabels(existing, client, message, eventListener, labels);
+            return false;
         } else {
-            createNewEvent(
-                    client, target, type, reason, message, component, eventListener, eventName);
+            Event event = buildEvent(target, type, reason, message, component, eventName);
+            setLabels(event, labels);
+            eventListener.accept(client.resource(event).createOrReplace());
             return true;
         }
     }
 
-    private static void createUpdatedEvent(
+    private static void updatedEventWithLabels(
             Event existing,
             KubernetesClient client,
             String message,
-            Consumer<Event> eventListener) {
+            Consumer<Event> eventListener,
+            @Nullable Map<String, String> labels) {
         existing.setLastTimestamp(Instant.now().toString());
         existing.setCount(existing.getCount() + 1);
         existing.setMessage(message);
+        setLabels(existing, labels);
         eventListener.accept(client.resource(existing).createOrReplace());
     }
 
-    private static void createNewEvent(
-            KubernetesClient client,
-            HasMetadata target,
-            EventRecorder.Type type,
-            String reason,
-            String message,
-            EventRecorder.Component component,
-            Consumer<Event> eventListener,
-            String eventName) {
-        Event event = buildEvent(target, type, reason, message, component, eventName);
-        eventListener.accept(client.resource(event).createOrReplace());
+    private static void setLabels(Event existing, @Nullable Map<String, String> labels) {
+        if (existing.getMetadata() == null) {
+            var metaData = new ObjectMeta();
+            metaData.setLabels(labels);
+        } else if (existing.getMetadata().getLabels() == null) {
+            existing.getMetadata().setLabels(labels);
+        } else {
+            existing.getMetadata().setLabels(labels);
+        }","[{'comment': '```suggestion\r\n        } else {\r\n            existing.getMetadata().setLabels(labels);\r\n        }\r\n```\r\n\r\nSame behavior for different code conditions, they can be simplified.', 'commenter': '1996fanrui'}, {'comment': 'Good catch. Will update.', 'commenter': 'clarax'}]"
685,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/EventUtils.java,"@@ -117,53 +119,53 @@ public static boolean createByInterval(
             EventRecorder.Component component,
             Consumer<Event> eventListener,
             @Nullable String messageKey,
-            Duration interval) {
-
+            @Nullable BiPredicate<Map<String, String>, Instant> suppressionPredicate,
+            @Nullable Map<String, String> labels) {
         String eventName =
                 generateEventName(
                         target, type, reason, messageKey != null ? messageKey : message, component);
         Event existing = findExistingEvent(client, target, eventName);
 
         if (existing != null) {
-            if (Objects.equals(existing.getMessage(), message)
-                    && Instant.now()
-                            .isBefore(
-                                    Instant.parse(existing.getLastTimestamp())
-                                            .plusMillis(interval.toMillis()))) {
-                return false;
-            } else {
-                createUpdatedEvent(existing, client, message, eventListener);
+            if (suppressionPredicate != null
+                    && existing.getMetadata() != null
+                    && suppressionPredicate.test(
+                            existing.getMetadata().getLabels(),
+                            Instant.parse(existing.getLastTimestamp()))) {
                 return false;
             }
+            updatedEventWithLabels(existing, client, message, eventListener, labels);
+            return false;
         } else {
-            createNewEvent(
-                    client, target, type, reason, message, component, eventListener, eventName);
+            Event event = buildEvent(target, type, reason, message, component, eventName);
+            setLabels(event, labels);
+            eventListener.accept(client.resource(event).createOrReplace());
             return true;
         }
     }
 
-    private static void createUpdatedEvent(
+    private static void updatedEventWithLabels(
             Event existing,
             KubernetesClient client,
             String message,
-            Consumer<Event> eventListener) {
+            Consumer<Event> eventListener,
+            @Nullable Map<String, String> labels) {
         existing.setLastTimestamp(Instant.now().toString());
         existing.setCount(existing.getCount() + 1);
         existing.setMessage(message);
+        setLabels(existing, labels);
         eventListener.accept(client.resource(existing).createOrReplace());
     }
 
-    private static void createNewEvent(
-            KubernetesClient client,
-            HasMetadata target,
-            EventRecorder.Type type,
-            String reason,
-            String message,
-            EventRecorder.Component component,
-            Consumer<Event> eventListener,
-            String eventName) {
-        Event event = buildEvent(target, type, reason, message, component, eventName);
-        eventListener.accept(client.resource(event).createOrReplace());
+    private static void setLabels(Event existing, @Nullable Map<String, String> labels) {
+        if (existing.getMetadata() == null) {","[{'comment': 'If labels is null, is this `setLabels` necessary? ', 'commenter': '1996fanrui'}, {'comment': ""Currently we don't call with non null value. But it doesn't hurt to allow caller to set it to null as needed. It will be considered as different labels from empty map so we don't accidentally dedupe. "", 'commenter': 'clarax'}]"
685,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/event/AutoScalerEventHandler.java,"@@ -32,20 +40,69 @@
  */
 @Experimental
 public interface AutoScalerEventHandler<KEY, Context extends JobAutoScalerContext<KEY>> {
+    String SCALING_SUMMARY_ENTRY =
+            "" Vertex ID %s | Parallelism %d -> %d | Processing capacity %.2f -> %.2f | Target data rate %.2f"";
+    String SCALING_SUMMARY_HEADER_SCALING_DISABLED = ""Recommended parallelism change:"";
+    String SCALING_SUMMARY_HEADER_SCALING_ENABLED = ""Scaling vertices:"";
+    String SCALING_REPORT_REASON = ""ScalingReport"";
+    String EVENT_MESSAGE_KEY = ""ScalingExecutor"";","[{'comment': 'This should be renamed to `SCALING_REPORT_KEY`', 'commenter': 'gyfora'}, {'comment': 'will update.', 'commenter': 'clarax'}]"
685,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/config/AutoScalerOptions.java,"@@ -201,8 +201,8 @@ private static ConfigOptions.OptionBuilder autoScalerConfig(String key) {
                     .withDescription(
                             ""A (semicolon-separated) list of vertex ids in hexstring for which to disable scaling. Caution: For non-sink vertices this will still scale their downstream operators until https://issues.apache.org/jira/browse/FLINK-31215 is implemented."");
 
-    public static final ConfigOption<Duration> SCALING_REPORT_INTERVAL =
-            autoScalerConfig(""scaling.report.interval"")
+    public static final ConfigOption<Duration> SCALING_EVENT_INTERVAL =
+            autoScalerConfig(""scaling.event.interval"")","[{'comment': 'Docs seem to be inconsistent with this and should regenerated after renaming to `event.interval`', 'commenter': 'gyfora'}, {'comment': 'updated.', 'commenter': 'clarax'}]"
685,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/EventUtils.java,"@@ -197,4 +198,19 @@ private static Event buildEvent(
                 .endMetadata()
                 .build();
     }
+
+    private static boolean intervalCheck(Event existing, @Nullable Duration interval) {
+        return interval != null
+                && Instant.now()
+                        .isBefore(
+                                Instant.parse(existing.getLastTimestamp())
+                                        .plusMillis(interval.toMillis()));
+    }
+
+    private static boolean labelCheck(
+            Event existing, Predicate<Map<String, String>> dedupePredicate) {
+        return dedupePredicate == null
+                || (existing.getMetadata() != null
+                        && dedupePredicate.test(existing.getMetadata().getLabels()));
+    }","[{'comment': 'I may misunderstand something but seems like labels are basically ignored when the `interval == null` . In that case intervalCheck is always false. Is this intentional? ', 'commenter': 'gyfora'}, {'comment': ""when Interval == null, we don't dedupe. This is intentional. It is commented in the interface method param and unit  tested.\r\nbut when the config is null, we use the default value of 30 min. It is also documented and unit tested."", 'commenter': 'clarax'}]"
685,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/config/AutoScalerOptions.java,"@@ -201,8 +201,8 @@ private static ConfigOptions.OptionBuilder autoScalerConfig(String key) {
                     .withDescription(
                             ""A (semicolon-separated) list of vertex ids in hexstring for which to disable scaling. Caution: For non-sink vertices this will still scale their downstream operators until https://issues.apache.org/jira/browse/FLINK-31215 is implemented."");
 
-    public static final ConfigOption<Duration> SCALING_REPORT_INTERVAL =
-            autoScalerConfig(""scaling.report.interval"")
+    public static final ConfigOption<Duration> SCALING_EVENT_INTERVAL =
+            autoScalerConfig(""scaling.event.interval"")
                     .durationType()
                     .defaultValue(Duration.ofSeconds(1800))
                     .withDescription(""Time interval to resend the identical event"");","[{'comment': '```suggestion\r\n                    .withDeprecatedKeys(\r\n                            deprecatedOperatorConfigKey(\r\n                                    ""scaling.event.interval""))\r\n                    .withDescription(""Time interval to resend the identical event"");\r\n```\r\n\r\nAs this comment[1] mentioned: ` all config keys to work with the ""old"" syntax at least in the 1.7.0 release. `, so please update it, thanks!\r\n\r\nSorry for that, I didn\'t support the old key during decoupling autoscaler and kubernetes-operator.\r\n\r\nhttps://github.com/apache/flink-kubernetes-operator/pull/686#discussion_r1369777698', 'commenter': '1996fanrui'}]"
698,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/standalone/JobListFetcher.java,"@@ -0,0 +1,31 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.autoscaler.standalone;
+
+import org.apache.flink.annotation.Experimental;
+import org.apache.flink.autoscaler.JobAutoScalerContext;
+import org.apache.flink.metrics.MetricGroup;
+
+import java.util.List;
+
+/** The JobListFetcher will fetch the jobContext of all jobs. */
+@Experimental
+public interface JobListFetcher<KEY> {
+
+    List<JobAutoScalerContext<KEY>> fetch(MetricGroup metricGroup) throws Exception;","[{'comment': 'Do we really need this interface in the initial version? Can we simply go with the implementation?', 'commenter': 'gyfora'}, {'comment': ""+1, let's just keep the implementation. There are no other consumers for this interface."", 'commenter': 'mxm'}, {'comment': ""Currently, it isn't needed. Could we keep this interfaces if it's not too polluted? Because some reasons:\r\n\r\n1. We will move it to the `flink-autoscaler-standalone` module, so the pollution will be less.\r\n2. We will implement YarnJobFetcher in FLINK-33100\r\n3. `JobListFetcher` is easy to extended for some flink platforms. Most companies will have their own platform for maintenance and management flink jobs, if they use flink on yarn instead of kubernetes. They can use the Standalone Autoscaler directly: \r\n   - Using the `JobListFetcher` to fetch all jobs from platform."", 'commenter': '1996fanrui'}, {'comment': ""And one important reason: \r\n\r\nWhen the job is restarted, the JobID will change. In order to support job non-in-place restart, it's better to use `JobAutoScalerContext<KEY>` instead of `JobAutoScalerContext<JobID>`. In general, other platform has the unique job id or job name, it likes metadata name of kubernetes.\r\n\r\nThe `FlinkClusterJobListFetcher` just support fetch `List<JobAutoScalerContext<JobID>>`, if we just keep `FlinkClusterJobListFetcher`, it's hard to let `StandaloneAutoscaler` generic.\r\n\r\nWDYT?"", 'commenter': '1996fanrui'}]"
698,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/standalone/JobListFetcherFactory.java,"@@ -0,0 +1,30 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.autoscaler.standalone;
+
+import org.apache.flink.annotation.Experimental;
+import org.apache.flink.api.java.utils.ParameterTool;
+
+import java.time.Duration;
+
+/** The factory for {@link JobListFetcher}. */
+@Experimental
+public interface JobListFetcherFactory<KEY> {
+
+    JobListFetcher<KEY> create(ParameterTool parameters, Duration restClientTimeout);","[{'comment': 'Do we really need this factory in the initial version?', 'commenter': 'gyfora'}, {'comment': '+1 We can construct the implementation directly.', 'commenter': 'mxm'}, {'comment': 'I can remove this interface in this PR first, and add it in [FLINK-33100](https://issues.apache.org/jira/browse/FLINK-33100).', 'commenter': '1996fanrui'}]"
698,flink-autoscaler/pom.xml,"@@ -43,14 +43,44 @@ under the License.
             <groupId>org.apache.flink</groupId>
             <artifactId>flink-runtime</artifactId>
             <version>${flink.version}</version>
-            <scope>provided</scope>
         </dependency>
 
         <dependency>
             <groupId>org.apache.flink</groupId>
             <artifactId>flink-clients</artifactId>
             <version>${flink.version}</version>
-            <scope>provided</scope>
+        </dependency>
+
+        <!-- Logging -->
+
+        <dependency>
+            <groupId>org.slf4j</groupId>
+            <artifactId>slf4j-api</artifactId>
+            <version>${slf4j.version}</version>
+        </dependency>
+
+        <dependency>
+            <groupId>org.apache.logging.log4j</groupId>
+            <artifactId>log4j-slf4j-impl</artifactId>
+            <version>${log4j.version}</version>
+        </dependency>
+
+        <dependency>
+            <groupId>org.apache.logging.log4j</groupId>
+            <artifactId>log4j-api</artifactId>
+            <version>${log4j.version}</version>
+        </dependency>
+
+        <dependency>
+            <groupId>org.apache.logging.log4j</groupId>
+            <artifactId>log4j-core</artifactId>
+            <version>${log4j.version}</version>
+        </dependency>
+
+        <dependency>
+            <groupId>org.apache.logging.log4j</groupId>
+            <artifactId>log4j-1.2-api</artifactId>
+            <version>${log4j.version}</version>","[{'comment': 'It would be nice to move these changes to a separate module.', 'commenter': 'mxm'}]"
698,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/event/LoggableEventHandler.java,"@@ -0,0 +1,52 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.autoscaler.event;
+
+import org.apache.flink.autoscaler.JobAutoScalerContext;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import javax.annotation.Nullable;
+
+import java.time.Duration;
+
+/** The loggable autoscaler event handler. */
+public class LoggableEventHandler<KEY, Context extends JobAutoScalerContext<KEY>>
+        implements AutoScalerEventHandler<KEY, Context> {
+
+    private static final Logger LOG = LoggerFactory.getLogger(LoggableEventHandler.class);
+
+    @Override
+    public void handleEvent(
+            Context context,
+            Type type,
+            String reason,
+            String message,
+            @Nullable String messageKey,
+            @Nullable Duration interval) {
+        LOG.info(
+                ""Handle autoscaler event, job key : {}, type : {}, reason : {}, message : {}, messageKey : {}, interval : {}."",","[{'comment': '```suggestion\r\n                ""Autoscaler event, job key : {}, type : {}, reason : {}, message : {}, messageKey : {}, interval : {}."",\r\n```', 'commenter': 'mxm'}]"
698,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/event/LoggableEventHandler.java,"@@ -0,0 +1,52 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.autoscaler.event;
+
+import org.apache.flink.autoscaler.JobAutoScalerContext;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import javax.annotation.Nullable;
+
+import java.time.Duration;
+
+/** The loggable autoscaler event handler. */","[{'comment': '```suggestion\r\n/** Autoscaler event handler which logs events. */\r\n```', 'commenter': 'mxm'}]"
698,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/realizer/RescaleApiScalingRealizer.java,"@@ -0,0 +1,153 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.autoscaler.realizer;
+
+import org.apache.flink.api.common.JobID;
+import org.apache.flink.api.common.JobStatus;
+import org.apache.flink.autoscaler.JobAutoScalerContext;
+import org.apache.flink.autoscaler.config.AutoScalerOptions;
+import org.apache.flink.autoscaler.event.AutoScalerEventHandler;
+import org.apache.flink.client.program.rest.RestClusterClient;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.configuration.JobManagerOptions;
+import org.apache.flink.runtime.jobgraph.JobResourceRequirements;
+import org.apache.flink.runtime.jobgraph.JobVertexID;
+import org.apache.flink.runtime.jobgraph.JobVertexResourceRequirements;
+import org.apache.flink.runtime.rest.messages.EmptyRequestBody;
+import org.apache.flink.runtime.rest.messages.JobMessageParameters;
+import org.apache.flink.runtime.rest.messages.job.JobResourceRequirementsBody;
+import org.apache.flink.runtime.rest.messages.job.JobResourceRequirementsHeaders;
+import org.apache.flink.runtime.rest.messages.job.JobResourcesRequirementsUpdateHeaders;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.time.Duration;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.concurrent.TimeUnit;
+
+/** Rescaling job based on rescale api. */","[{'comment': '```suggestion\r\n/** A ScalingRealizer which uses the Rescale API to apply parallelism changes. */\r\n```', 'commenter': 'mxm'}]"
698,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/realizer/RescaleApiScalingRealizer.java,"@@ -0,0 +1,153 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.autoscaler.realizer;
+
+import org.apache.flink.api.common.JobID;
+import org.apache.flink.api.common.JobStatus;
+import org.apache.flink.autoscaler.JobAutoScalerContext;
+import org.apache.flink.autoscaler.config.AutoScalerOptions;
+import org.apache.flink.autoscaler.event.AutoScalerEventHandler;
+import org.apache.flink.client.program.rest.RestClusterClient;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.configuration.JobManagerOptions;
+import org.apache.flink.runtime.jobgraph.JobResourceRequirements;
+import org.apache.flink.runtime.jobgraph.JobVertexID;
+import org.apache.flink.runtime.jobgraph.JobVertexResourceRequirements;
+import org.apache.flink.runtime.rest.messages.EmptyRequestBody;
+import org.apache.flink.runtime.rest.messages.JobMessageParameters;
+import org.apache.flink.runtime.rest.messages.job.JobResourceRequirementsBody;
+import org.apache.flink.runtime.rest.messages.job.JobResourceRequirementsHeaders;
+import org.apache.flink.runtime.rest.messages.job.JobResourcesRequirementsUpdateHeaders;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.time.Duration;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.concurrent.TimeUnit;
+
+/** Rescaling job based on rescale api. */
+public class RescaleApiScalingRealizer<KEY, Context extends JobAutoScalerContext<KEY>>
+        implements ScalingRealizer<KEY, Context> {
+
+    private static final Logger LOG = LoggerFactory.getLogger(RescaleApiScalingRealizer.class);
+
+    private final AutoScalerEventHandler<KEY, JobAutoScalerContext<KEY>> eventHandler;
+
+    public RescaleApiScalingRealizer(
+            AutoScalerEventHandler<KEY, JobAutoScalerContext<KEY>> eventHandler) {
+        this.eventHandler = eventHandler;
+    }
+
+    @Override
+    public void realize(Context context, Map<String, String> parallelismOverrides) {
+        Configuration conf = context.getConfiguration();
+        if (!conf.get(JobManagerOptions.SCHEDULER)
+                .equals(JobManagerOptions.SchedulerType.Adaptive)) {
+            LOG.debug(""In-place rescaling is only available with the adaptive scheduler"");","[{'comment': 'Should this be logged at WARN? Users might be surprised no scaling is being performed.', 'commenter': 'mxm'}]"
698,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/realizer/RescaleApiScalingRealizer.java,"@@ -0,0 +1,153 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.autoscaler.realizer;
+
+import org.apache.flink.api.common.JobID;
+import org.apache.flink.api.common.JobStatus;
+import org.apache.flink.autoscaler.JobAutoScalerContext;
+import org.apache.flink.autoscaler.config.AutoScalerOptions;
+import org.apache.flink.autoscaler.event.AutoScalerEventHandler;
+import org.apache.flink.client.program.rest.RestClusterClient;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.configuration.JobManagerOptions;
+import org.apache.flink.runtime.jobgraph.JobResourceRequirements;
+import org.apache.flink.runtime.jobgraph.JobVertexID;
+import org.apache.flink.runtime.jobgraph.JobVertexResourceRequirements;
+import org.apache.flink.runtime.rest.messages.EmptyRequestBody;
+import org.apache.flink.runtime.rest.messages.JobMessageParameters;
+import org.apache.flink.runtime.rest.messages.job.JobResourceRequirementsBody;
+import org.apache.flink.runtime.rest.messages.job.JobResourceRequirementsHeaders;
+import org.apache.flink.runtime.rest.messages.job.JobResourcesRequirementsUpdateHeaders;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.time.Duration;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.concurrent.TimeUnit;
+
+/** Rescaling job based on rescale api. */
+public class RescaleApiScalingRealizer<KEY, Context extends JobAutoScalerContext<KEY>>
+        implements ScalingRealizer<KEY, Context> {
+
+    private static final Logger LOG = LoggerFactory.getLogger(RescaleApiScalingRealizer.class);
+
+    private final AutoScalerEventHandler<KEY, JobAutoScalerContext<KEY>> eventHandler;
+
+    public RescaleApiScalingRealizer(
+            AutoScalerEventHandler<KEY, JobAutoScalerContext<KEY>> eventHandler) {
+        this.eventHandler = eventHandler;
+    }
+
+    @Override
+    public void realize(Context context, Map<String, String> parallelismOverrides) {
+        Configuration conf = context.getConfiguration();
+        if (!conf.get(JobManagerOptions.SCHEDULER)
+                .equals(JobManagerOptions.SchedulerType.Adaptive)) {
+            LOG.debug(""In-place rescaling is only available with the adaptive scheduler"");
+            return;
+        }
+
+        JobStatus jobStatus = context.getJobStatus();
+        JobID jobID = context.getJobID();
+        if (jobID == null
+                || jobStatus == null
+                || jobStatus.isGloballyTerminalState()
+                || JobStatus.RECONCILING == jobStatus) {
+            LOG.info(""Job in terminal or reconciling state cannot be scaled in-place"");
+            return;
+        }","[{'comment': 'Why not simply check for ""RUNNING"" state?', 'commenter': 'gyfora'}, {'comment': ""I try to keep the same logic with `NativeFlinkService#supportsInPlaceScaling`[1]. After thinking again, I think it can be simply: just check for `RUNNING` state.\r\n\r\nBecause `JobAutoScalerImpl#scale` won't runScalingLogic  if it isn't `RUNNING`.\r\n\r\n[1] https://github.com/apache/flink-kubernetes-operator/blob/582c07aeec6b4bbcc68e0a85d31000d89dbd1958/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/NativeFlinkService.java#L260"", 'commenter': '1996fanrui'}]"
698,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/realizer/RescaleApiScalingRealizer.java,"@@ -0,0 +1,153 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.autoscaler.realizer;
+
+import org.apache.flink.api.common.JobID;
+import org.apache.flink.api.common.JobStatus;
+import org.apache.flink.autoscaler.JobAutoScalerContext;
+import org.apache.flink.autoscaler.config.AutoScalerOptions;
+import org.apache.flink.autoscaler.event.AutoScalerEventHandler;
+import org.apache.flink.client.program.rest.RestClusterClient;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.configuration.JobManagerOptions;
+import org.apache.flink.runtime.jobgraph.JobResourceRequirements;
+import org.apache.flink.runtime.jobgraph.JobVertexID;
+import org.apache.flink.runtime.jobgraph.JobVertexResourceRequirements;
+import org.apache.flink.runtime.rest.messages.EmptyRequestBody;
+import org.apache.flink.runtime.rest.messages.JobMessageParameters;
+import org.apache.flink.runtime.rest.messages.job.JobResourceRequirementsBody;
+import org.apache.flink.runtime.rest.messages.job.JobResourceRequirementsHeaders;
+import org.apache.flink.runtime.rest.messages.job.JobResourcesRequirementsUpdateHeaders;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.time.Duration;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.concurrent.TimeUnit;
+
+/** Rescaling job based on rescale api. */
+public class RescaleApiScalingRealizer<KEY, Context extends JobAutoScalerContext<KEY>>","[{'comment': 'Please put a note that this is based on code copied from the operator , there is a lot of duplication, so there should be some justification for that.', 'commenter': 'gyfora'}]"
698,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/realizer/RescaleApiScalingRealizer.java,"@@ -0,0 +1,153 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.autoscaler.realizer;
+
+import org.apache.flink.api.common.JobID;
+import org.apache.flink.api.common.JobStatus;
+import org.apache.flink.autoscaler.JobAutoScalerContext;
+import org.apache.flink.autoscaler.config.AutoScalerOptions;
+import org.apache.flink.autoscaler.event.AutoScalerEventHandler;
+import org.apache.flink.client.program.rest.RestClusterClient;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.configuration.JobManagerOptions;
+import org.apache.flink.runtime.jobgraph.JobResourceRequirements;
+import org.apache.flink.runtime.jobgraph.JobVertexID;
+import org.apache.flink.runtime.jobgraph.JobVertexResourceRequirements;
+import org.apache.flink.runtime.rest.messages.EmptyRequestBody;
+import org.apache.flink.runtime.rest.messages.JobMessageParameters;
+import org.apache.flink.runtime.rest.messages.job.JobResourceRequirementsBody;
+import org.apache.flink.runtime.rest.messages.job.JobResourceRequirementsHeaders;
+import org.apache.flink.runtime.rest.messages.job.JobResourcesRequirementsUpdateHeaders;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.time.Duration;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.concurrent.TimeUnit;
+
+/** Rescaling job based on rescale api. */
+public class RescaleApiScalingRealizer<KEY, Context extends JobAutoScalerContext<KEY>>
+        implements ScalingRealizer<KEY, Context> {
+
+    private static final Logger LOG = LoggerFactory.getLogger(RescaleApiScalingRealizer.class);
+
+    private final AutoScalerEventHandler<KEY, JobAutoScalerContext<KEY>> eventHandler;
+
+    public RescaleApiScalingRealizer(
+            AutoScalerEventHandler<KEY, JobAutoScalerContext<KEY>> eventHandler) {
+        this.eventHandler = eventHandler;
+    }
+
+    @Override
+    public void realize(Context context, Map<String, String> parallelismOverrides) {
+        Configuration conf = context.getConfiguration();
+        if (!conf.get(JobManagerOptions.SCHEDULER)
+                .equals(JobManagerOptions.SchedulerType.Adaptive)) {
+            LOG.debug(""In-place rescaling is only available with the adaptive scheduler"");
+            return;
+        }
+
+        JobStatus jobStatus = context.getJobStatus();
+        JobID jobID = context.getJobID();
+        if (jobID == null
+                || jobStatus == null
+                || jobStatus.isGloballyTerminalState()
+                || JobStatus.RECONCILING == jobStatus) {
+            LOG.info(""Job in terminal or reconciling state cannot be scaled in-place"");","[{'comment': ""Should we log at WARNING? This is probably a race condition because the autoscaler wouldn't scale when not in RUNNING state."", 'commenter': 'mxm'}]"
698,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/standalone/StandaloneAutoscalerEntrypoint.java,"@@ -0,0 +1,139 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.autoscaler.standalone;
+
+import org.apache.flink.annotation.Experimental;
+import org.apache.flink.api.java.utils.ParameterTool;
+import org.apache.flink.autoscaler.JobAutoScaler;
+import org.apache.flink.autoscaler.JobAutoScalerContext;
+import org.apache.flink.autoscaler.JobAutoScalerImpl;
+import org.apache.flink.autoscaler.RestApiMetricsCollector;
+import org.apache.flink.autoscaler.ScalingExecutor;
+import org.apache.flink.autoscaler.ScalingMetricEvaluator;
+import org.apache.flink.autoscaler.event.AutoScalerEventHandler;
+import org.apache.flink.autoscaler.event.LoggableEventHandler;
+import org.apache.flink.autoscaler.realizer.RescaleApiScalingRealizer;
+import org.apache.flink.autoscaler.standalone.flinkcluster.FlinkClusterJobListFetcherFactory;
+import org.apache.flink.autoscaler.state.AutoScalerStateStore;
+import org.apache.flink.autoscaler.state.InMemoryAutoScalerStateStore;
+import org.apache.flink.metrics.groups.UnregisteredMetricsGroup;
+import org.apache.flink.util.TimeUtils;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.time.Duration;
+import java.time.Instant;
+import java.util.List;
+
+import static org.apache.flink.autoscaler.config.AutoScalerOptions.FLINK_CLIENT_TIMEOUT;
+
+/** The entrypoint of the standalone autoscaler. */
+@Experimental
+public class StandaloneAutoscalerEntrypoint {
+
+    private static final Logger LOG = LoggerFactory.getLogger(StandaloneAutoscalerEntrypoint.class);
+
+    public static final String SCALING_INTERVAL = ""scalingInterval"";
+    private static final Duration DEFAULT_SCALING_INTERVAL = Duration.ofSeconds(10);
+
+    public static final String JOB_LIST_FETCHER_CLASS_NAME = ""jobListFetcherClassName"";
+    private static final String DEFAULT_JOB_LIST_FETCHER_CLASS_NAME =
+            FlinkClusterJobListFetcherFactory.class.getName();
+
+    // This timeout option is used before the job config is got, such as: listJobs, get
+    // Configuration, etc.
+    public static final String REST_CLIENT_TIMEOUT = ""restClientTimeout"";
+
+    public static <KEY> void main(String[] args) throws Exception {
+        ParameterTool parameters = ParameterTool.fromArgs(args);
+
+        Duration scalingInterval = DEFAULT_SCALING_INTERVAL;
+        if (parameters.get(SCALING_INTERVAL) != null) {
+            scalingInterval = TimeUtils.parseDuration(parameters.get(SCALING_INTERVAL));
+        }
+
+        Duration restClientTimeout = FLINK_CLIENT_TIMEOUT.defaultValue();
+        if (parameters.get(REST_CLIENT_TIMEOUT) != null) {
+            restClientTimeout = TimeUtils.parseDuration(parameters.get(REST_CLIENT_TIMEOUT));
+        }
+
+        // Initialize JobListFetcher and JobAutoScaler.
+        JobListFetcherFactory<KEY> jobListFetcherFactory = getJobListFetcherFactory(parameters);
+        JobListFetcher<KEY> jobListFetcher =
+                jobListFetcherFactory.create(parameters, restClientTimeout);
+
+        JobAutoScaler<KEY, JobAutoScalerContext<KEY>> autoScaler = createJobAutoscaler();
+
+        // Start control loop
+        Instant expectedNextSchedulerTime = Instant.now().plusMillis(scalingInterval.toMillis());
+        while (true) {
+            LOG.info(""Standalone autoscaler starts scaling."");
+            scaling(jobListFetcher, autoScaler);
+
+            waitForNextSchedule(expectedNextSchedulerTime);
+            expectedNextSchedulerTime = Instant.now().plusMillis(scalingInterval.toMillis());
+        }
+    }
+
+    private static void waitForNextSchedule(Instant expectedNextSchedulerTime)
+            throws InterruptedException {
+        if (expectedNextSchedulerTime.isBefore(Instant.now())) {
+            return;
+        }
+        Duration sleepTime = Duration.between(Instant.now(), expectedNextSchedulerTime);
+        Thread.sleep(sleepTime.toMillis());
+    }","[{'comment': 'Can we use a ScheduledExecutorService for that to avoid sleeping etc?', 'commenter': 'gyfora'}]"
698,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/realizer/RescaleApiScalingRealizer.java,"@@ -0,0 +1,153 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.autoscaler.realizer;
+
+import org.apache.flink.api.common.JobID;
+import org.apache.flink.api.common.JobStatus;
+import org.apache.flink.autoscaler.JobAutoScalerContext;
+import org.apache.flink.autoscaler.config.AutoScalerOptions;
+import org.apache.flink.autoscaler.event.AutoScalerEventHandler;
+import org.apache.flink.client.program.rest.RestClusterClient;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.configuration.JobManagerOptions;
+import org.apache.flink.runtime.jobgraph.JobResourceRequirements;
+import org.apache.flink.runtime.jobgraph.JobVertexID;
+import org.apache.flink.runtime.jobgraph.JobVertexResourceRequirements;
+import org.apache.flink.runtime.rest.messages.EmptyRequestBody;
+import org.apache.flink.runtime.rest.messages.JobMessageParameters;
+import org.apache.flink.runtime.rest.messages.job.JobResourceRequirementsBody;
+import org.apache.flink.runtime.rest.messages.job.JobResourceRequirementsHeaders;
+import org.apache.flink.runtime.rest.messages.job.JobResourcesRequirementsUpdateHeaders;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.time.Duration;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.concurrent.TimeUnit;
+
+/** Rescaling job based on rescale api. */
+public class RescaleApiScalingRealizer<KEY, Context extends JobAutoScalerContext<KEY>>
+        implements ScalingRealizer<KEY, Context> {
+
+    private static final Logger LOG = LoggerFactory.getLogger(RescaleApiScalingRealizer.class);
+
+    private final AutoScalerEventHandler<KEY, JobAutoScalerContext<KEY>> eventHandler;
+
+    public RescaleApiScalingRealizer(
+            AutoScalerEventHandler<KEY, JobAutoScalerContext<KEY>> eventHandler) {
+        this.eventHandler = eventHandler;
+    }
+
+    @Override
+    public void realize(Context context, Map<String, String> parallelismOverrides) {
+        Configuration conf = context.getConfiguration();
+        if (!conf.get(JobManagerOptions.SCHEDULER)
+                .equals(JobManagerOptions.SchedulerType.Adaptive)) {
+            LOG.debug(""In-place rescaling is only available with the adaptive scheduler"");
+            return;
+        }
+
+        JobStatus jobStatus = context.getJobStatus();
+        JobID jobID = context.getJobID();
+        if (jobID == null
+                || jobStatus == null
+                || jobStatus.isGloballyTerminalState()
+                || JobStatus.RECONCILING == jobStatus) {
+            LOG.info(""Job in terminal or reconciling state cannot be scaled in-place"");
+            return;
+        }
+
+        Duration flinkRestClientTimeout = conf.get(AutoScalerOptions.FLINK_CLIENT_TIMEOUT);
+
+        try (var client = context.getRestClusterClient()) {
+            var requirements =
+                    new HashMap<>(getVertexResources(client, jobID, flinkRestClientTimeout));
+            boolean parallelismUpdated = false;
+
+            for (Map.Entry<JobVertexID, JobVertexResourceRequirements> entry :
+                    requirements.entrySet()) {
+                var jobVertexId = entry.getKey().toString();
+                var parallelism = entry.getValue().getParallelism();
+                var overrideStr = parallelismOverrides.get(jobVertexId);
+
+                // No overrides for this vertex
+                if (overrideStr == null) {
+                    continue;
+                }
+
+                // We have an override for the vertex
+                int p = Integer.parseInt(overrideStr);
+                var newParallelism = new JobVertexResourceRequirements.Parallelism(1, p);
+                // If the requirements changed we mark this as scaling triggered
+                if (!parallelism.equals(newParallelism)) {
+                    entry.setValue(new JobVertexResourceRequirements(newParallelism));
+                    parallelismUpdated = true;
+                }
+            }
+            if (parallelismUpdated) {
+                updateVertexResources(client, jobID, flinkRestClientTimeout, requirements);
+                eventHandler.handleEvent(
+                        context,
+                        AutoScalerEventHandler.Type.Normal,
+                        ""Scaling"",
+                        String.format(
+                                ""In-place scaling triggered, the new requirements is %s."",
+                                requirements),
+                        null,
+                        null);
+            } else {
+                LOG.info(""Vertex resources requirements already match target, nothing to do..."");
+            }
+        } catch (Exception e) {
+            LOG.warn(""Scaling realize exception."", e);","[{'comment': '```suggestion\r\n            LOG.warn(""Failed to apply parallelism overrides."", e);\r\n```', 'commenter': 'mxm'}]"
698,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/standalone/flinkcluster/FlinkClusterJobListFetcher.java,"@@ -0,0 +1,109 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.autoscaler.standalone.flinkcluster;
+
+import org.apache.flink.api.common.JobID;
+import org.apache.flink.autoscaler.JobAutoScalerContext;
+import org.apache.flink.autoscaler.standalone.JobListFetcher;
+import org.apache.flink.client.program.rest.RestClusterClient;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.metrics.MetricGroup;
+import org.apache.flink.runtime.client.JobStatusMessage;
+import org.apache.flink.runtime.highavailability.nonha.standalone.StandaloneClientHAServices;
+import org.apache.flink.runtime.rest.messages.ConfigurationInfo;
+import org.apache.flink.runtime.rest.messages.EmptyRequestBody;
+import org.apache.flink.runtime.rest.messages.JobMessageParameters;
+import org.apache.flink.runtime.rest.messages.job.JobManagerJobConfigurationHeaders;
+
+import java.time.Duration;
+import java.util.List;
+import java.util.concurrent.TimeUnit;
+import java.util.stream.Collectors;
+
+/** Fetch JobAutoScalerContext based on flink cluster. */
+public class FlinkClusterJobListFetcher implements JobListFetcher<JobID> {
+
+    private final String restServerAddress;
+    private final Duration restClientTimeout;
+
+    public FlinkClusterJobListFetcher(String host, int port, Duration restClientTimeout) {
+        this.restServerAddress = String.format(""http://%s:%s"", host, port);
+        this.restClientTimeout = restClientTimeout;
+    }
+
+    @Override
+    public List<JobAutoScalerContext<JobID>> fetch(MetricGroup metricGroup) throws Exception {","[{'comment': ""I don't think `metricGroup` should be part of the method argument. The `FlinkClusterJobListFetcher` could under the hood simply pass the unregistered metric group."", 'commenter': 'gyfora'}, {'comment': ""Good catch! It can be simply, it can be passed to `FlinkClusterJobListFetcher` by the constructor directly.\r\n\r\nAlso, I don't pass it in this PR, and `new UnregisteredMetricsGroup()` directly when creating the `JobAutoScalerContext`, because we don't support metric reporter for Standalone Autoscaler so far."", 'commenter': '1996fanrui'}]"
698,flink-autoscaler-standalone/src/main/java/org/apache/flink/autoscaler/standalone/StandaloneAutoscalerEntrypoint.java,"@@ -0,0 +1,135 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.autoscaler.standalone;
+
+import org.apache.flink.annotation.Experimental;
+import org.apache.flink.api.java.utils.ParameterTool;
+import org.apache.flink.autoscaler.JobAutoScaler;
+import org.apache.flink.autoscaler.JobAutoScalerContext;
+import org.apache.flink.autoscaler.JobAutoScalerImpl;
+import org.apache.flink.autoscaler.RestApiMetricsCollector;
+import org.apache.flink.autoscaler.ScalingExecutor;
+import org.apache.flink.autoscaler.ScalingMetricEvaluator;
+import org.apache.flink.autoscaler.event.AutoScalerEventHandler;
+import org.apache.flink.autoscaler.standalone.flinkcluster.FlinkClusterJobListFetcher;
+import org.apache.flink.autoscaler.standalone.realizer.RescaleApiScalingRealizer;
+import org.apache.flink.autoscaler.state.AutoScalerStateStore;
+import org.apache.flink.autoscaler.state.InMemoryAutoScalerStateStore;
+import org.apache.flink.util.TimeUtils;
+
+import org.apache.flink.shaded.guava30.com.google.common.util.concurrent.ThreadFactoryBuilder;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.time.Duration;
+import java.util.List;
+import java.util.concurrent.Executors;
+import java.util.concurrent.ScheduledExecutorService;
+import java.util.concurrent.TimeUnit;
+
+import static org.apache.flink.autoscaler.config.AutoScalerOptions.FLINK_CLIENT_TIMEOUT;
+
+/** The entrypoint of the standalone autoscaler. */
+@Experimental
+public class StandaloneAutoscalerEntrypoint {
+
+    private static final Logger LOG = LoggerFactory.getLogger(StandaloneAutoscalerEntrypoint.class);
+
+    public static final String SCALING_INTERVAL = ""scalingInterval"";
+    private static final Duration DEFAULT_SCALING_INTERVAL = Duration.ofSeconds(10);
+
+    // This timeout option is used before the job config is got, such as: listJobs, get
+    // Configuration, etc.
+    public static final String REST_CLIENT_TIMEOUT = ""restClientTimeout"";
+
+    /** Arguments related to {@link FlinkClusterJobListFetcher}. */
+    public static final String FLINK_CLUSTER_HOST = ""flinkClusterHost"";
+
+    private static final String DEFAULT_FLINK_CLUSTER_HOST = ""localhost"";
+
+    public static final String FLINK_CLUSTER_PORT = ""flinkClusterPort"";
+    private static final int DEFAULT_FLINK_CLUSTER_PORT = 8081;
+
+    public static <KEY> void main(String[] args) {","[{'comment': 'If we want users to use this entry point we should add the required fatjar packaging logic to the pom and declare this as the main class.\r\n\r\nPlease also add some minimal readme to this new module to show users how to run the standalone autoscaler .', 'commenter': 'gyfora'}, {'comment': 'If we are adding the fatjar + also include the respective notice file', 'commenter': 'gyfora'}, {'comment': 'Thanks for your reminder!', 'commenter': '1996fanrui'}]"
698,flink-autoscaler-standalone/README.md,"@@ -0,0 +1,79 @@
+# Flink Autoscaler Standalone
+
+## What's the autoscaler standalone?
+
+`Flink Autoscaler Standalone` is an implementation of `Flink Autoscaler`, it runs as 
+a separate java process. It computes the reasonable parallelism of all job vertices 
+by monitoring the metrics, such as: processing rate, busy time, etc. Please see 
+[FLIP-271](https://cwiki.apache.org/confluence/display/FLINK/FLIP-271%3A+Autoscaling) ","[{'comment': 'Here can we link the operator docs autoscaler page instead? Lot of things have changed in the Flip and that page will never be updated.', 'commenter': 'gyfora'}]"
698,flink-autoscaler-standalone/README.md,"@@ -0,0 +1,79 @@
+# Flink Autoscaler Standalone
+
+## What's the autoscaler standalone?
+
+`Flink Autoscaler Standalone` is an implementation of `Flink Autoscaler`, it runs as 
+a separate java process. It computes the reasonable parallelism of all job vertices 
+by monitoring the metrics, such as: processing rate, busy time, etc. Please see 
+[FLIP-271](https://cwiki.apache.org/confluence/display/FLINK/FLIP-271%3A+Autoscaling) 
+for an overview of how autoscaling works.
+
+`Flink Autoscaler Standalone` rescales flink job in-place by rescale api of 
+[FLIP-291](https://cwiki.apache.org/confluence/x/9pRbDg).","[{'comment': 'Here again we should link the docs instead: https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/deployment/elastic_scaling/#externalized-declarative-resource-management', 'commenter': 'gyfora'}]"
698,flink-autoscaler-standalone/README.md,"@@ -0,0 +1,79 @@
+# Flink Autoscaler Standalone
+
+## What's the autoscaler standalone?","[{'comment': 'We should add a paragraph clarifying that this should only be used in non-kubernetes environments. For Kubernetes the Operator provides a full integration that is preferable.', 'commenter': 'gyfora'}]"
698,flink-autoscaler-standalone/pom.xml,"@@ -0,0 +1,207 @@
+<?xml version=""1.0"" encoding=""UTF-8""?>
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+  http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+<project xmlns=""http://maven.apache.org/POM/4.0.0""
+         xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
+         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
+    <modelVersion>4.0.0</modelVersion>
+
+    <parent>
+        <groupId>org.apache.flink</groupId>
+        <artifactId>flink-kubernetes-operator-parent</artifactId>
+        <version>1.7-SNAPSHOT</version>
+        <relativePath>..</relativePath>
+    </parent>
+
+    <artifactId>flink-autoscaler-standalone</artifactId>
+    <name>Flink Autoscaler Standalone</name>
+    <packaging>jar</packaging>
+
+
+    <dependencies>
+        <dependency>
+            <groupId>org.apache.flink</groupId>
+            <artifactId>flink-autoscaler</artifactId>
+            <version>${project.version}</version>
+        </dependency>
+
+        <dependency>
+            <groupId>org.apache.flink</groupId>
+            <artifactId>flink-runtime</artifactId>
+            <version>${flink.version}</version>
+            <exclusions>
+                <exclusion>
+                    <artifactId>flink-rpc-akka-loader</artifactId>
+                    <groupId>org.apache.flink</groupId>
+                </exclusion>
+                <exclusion>
+                    <artifactId>flink-shaded-zookeeper-3</artifactId>
+                    <groupId>org.apache.flink</groupId>
+                </exclusion>
+                <exclusion>
+                    <artifactId>flink-queryable-state-client-java</artifactId>
+                    <groupId>org.apache.flink</groupId>
+                </exclusion>
+                <exclusion>
+                    <artifactId>snappy-java</artifactId>
+                    <groupId>org.xerial.snappy</groupId>
+                </exclusion>
+                <exclusion>
+                    <artifactId>flink-hadoop-fs</artifactId>
+                    <groupId>org.apache.flink</groupId>
+                </exclusion>
+                <exclusion>
+                    <artifactId>commons-compress</artifactId>
+                    <groupId>org.apache.commons</groupId>
+                </exclusion>
+                <exclusion>
+                    <artifactId>commons-text</artifactId>
+                    <groupId>org.apache.commons</groupId>
+                </exclusion>
+                <exclusion>","[{'comment': 'In this entire pom we have groupId / artifactId , in the reverse order. While syntactically correct this is not how it is generally written. Can you please change them?', 'commenter': 'gyfora'}]"
698,flink-autoscaler-standalone/pom.xml,"@@ -0,0 +1,207 @@
+<?xml version=""1.0"" encoding=""UTF-8""?>
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+""License""); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+  http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+<project xmlns=""http://maven.apache.org/POM/4.0.0""
+         xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
+         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
+    <modelVersion>4.0.0</modelVersion>
+
+    <parent>
+        <groupId>org.apache.flink</groupId>
+        <artifactId>flink-kubernetes-operator-parent</artifactId>
+        <version>1.7-SNAPSHOT</version>
+        <relativePath>..</relativePath>
+    </parent>
+
+    <artifactId>flink-autoscaler-standalone</artifactId>
+    <name>Flink Autoscaler Standalone</name>
+    <packaging>jar</packaging>
+
+
+    <dependencies>
+        <dependency>
+            <groupId>org.apache.flink</groupId>
+            <artifactId>flink-autoscaler</artifactId>
+            <version>${project.version}</version>
+        </dependency>
+
+        <dependency>
+            <groupId>org.apache.flink</groupId>
+            <artifactId>flink-runtime</artifactId>
+            <version>${flink.version}</version>
+            <exclusions>
+                <exclusion>
+                    <artifactId>flink-rpc-akka-loader</artifactId>
+                    <groupId>org.apache.flink</groupId>
+                </exclusion>
+                <exclusion>
+                    <artifactId>flink-shaded-zookeeper-3</artifactId>
+                    <groupId>org.apache.flink</groupId>
+                </exclusion>
+                <exclusion>
+                    <artifactId>flink-queryable-state-client-java</artifactId>
+                    <groupId>org.apache.flink</groupId>
+                </exclusion>
+                <exclusion>
+                    <artifactId>snappy-java</artifactId>
+                    <groupId>org.xerial.snappy</groupId>
+                </exclusion>
+                <exclusion>
+                    <artifactId>flink-hadoop-fs</artifactId>
+                    <groupId>org.apache.flink</groupId>
+                </exclusion>
+                <exclusion>
+                    <artifactId>commons-compress</artifactId>
+                    <groupId>org.apache.commons</groupId>
+                </exclusion>
+                <exclusion>
+                    <artifactId>commons-text</artifactId>
+                    <groupId>org.apache.commons</groupId>
+                </exclusion>
+                <exclusion>
+                    <artifactId>lz4-java</artifactId>
+                    <groupId>org.lz4</groupId>
+                </exclusion>
+            </exclusions>
+        </dependency>
+
+        <dependency>
+            <groupId>org.apache.flink</groupId>
+            <artifactId>flink-clients</artifactId>
+            <version>${flink.version}</version>
+            <exclusions>
+                <exclusion>
+                    <artifactId>flink-streaming-java</artifactId>
+                    <groupId>org.apache.flink</groupId>
+                </exclusion>
+                <exclusion>
+                    <artifactId>flink-optimizer</artifactId>
+                    <groupId>org.apache.flink</groupId>
+                </exclusion>
+            </exclusions>
+        </dependency>
+
+        <!-- Logging -->
+
+        <dependency>
+            <groupId>org.slf4j</groupId>
+            <artifactId>slf4j-api</artifactId>
+            <version>${slf4j.version}</version>
+        </dependency>
+
+        <dependency>
+            <groupId>org.apache.logging.log4j</groupId>
+            <artifactId>log4j-slf4j-impl</artifactId>
+            <version>${log4j.version}</version>
+        </dependency>
+
+        <dependency>
+            <groupId>org.apache.logging.log4j</groupId>
+            <artifactId>log4j-api</artifactId>
+            <version>${log4j.version}</version>
+        </dependency>
+
+        <dependency>
+            <groupId>org.apache.logging.log4j</groupId>
+            <artifactId>log4j-core</artifactId>
+            <version>${log4j.version}</version>
+        </dependency>
+
+        <dependency>
+            <groupId>org.apache.logging.log4j</groupId>
+            <artifactId>log4j-1.2-api</artifactId>
+            <version>${log4j.version}</version>
+        </dependency>
+
+        <!-- Test -->
+
+        <dependency>
+            <groupId>org.apache.flink</groupId>
+            <artifactId>flink-autoscaler</artifactId>
+            <version>${project.version}</version>
+            <type>test-jar</type>
+            <classifier>tests</classifier>
+            <scope>test</scope>
+        </dependency>
+
+        <dependency>
+            <groupId>org.junit.jupiter</groupId>
+            <artifactId>junit-jupiter-api</artifactId>
+            <scope>test</scope>
+        </dependency>
+
+        <dependency>
+            <groupId>org.junit.jupiter</groupId>
+            <artifactId>junit-jupiter-params</artifactId>
+            <scope>test</scope>
+        </dependency>
+
+        <dependency>
+            <groupId>org.assertj</groupId>
+            <artifactId>assertj-core</artifactId>
+            <version>${assertj.version}</version>
+            <scope>test</scope>
+        </dependency>
+
+    </dependencies>
+
+    <build>
+        <plugins>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-shade-plugin</artifactId>
+                <executions>
+                    <!-- Run shade goal on package phase -->
+                    <execution>","[{'comment': 'As we are bundling dependencies into a father and we are likely going to publish that to maven (right?) .\r\n\r\nWe need to add a NOTICE file to list the bundled dependencies by license', 'commenter': 'gyfora'}, {'comment': ""Thanks @gyfora  for the quick review! The rest comments are addressed.\r\n\r\n> As we are bundling dependencies into a father and we are likely going to publish that to maven (right?) .\r\n\r\nDuring add the fat jar, I have a question: where do users to download this fat jar? In maven repo or dist?\r\n\r\n> We need to add a NOTICE file to list the bundled dependencies by license\r\n\r\nSorry, I didn't do it before. Is the NOTICE file created by tools or manually? Is there any doc to explain it?"", 'commenter': '1996fanrui'}, {'comment': ""We could simply publish the fat jar to maven, that would be the simplest (instead of the slim jar)\r\n\r\nAs for the notice file creation, I don't know any tools, but I think you will not have too many things in it as it's mostly Flink (that do not need to be listed)"", 'commenter': 'gyfora'}]"
710,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/ConfigMapStore.java,"@@ -67,19 +67,14 @@ protected Optional<String> getSerializedState(
     }
 
     protected void removeSerializedState(KubernetesJobAutoScalerContext jobContext, String key) {
-        getConfigMap(jobContext)
-                .ifPresentOrElse(
-                        configMap -> configMap.getData().remove(key),
-                        () -> {
-                            throw new IllegalStateException(
-                                    ""The configMap isn't created, so the remove is unavailable."");
-                        });
+        getConfigMap(jobContext).ifPresent(configMap -> configMap.getData().remove(key));
     }
 
     public void flush(KubernetesJobAutoScalerContext jobContext) {
         Optional<ConfigMap> configMapOpt = cache.get(jobContext.getJobKey());
         if (configMapOpt == null || configMapOpt.isEmpty()) {
             LOG.debug(""The configMap isn't updated, so skip the flush."");
+            // Do not flush if there are no updates.","[{'comment': ""I'm thinking whether any updating or removeing can be detected, please give me a few times to test it."", 'commenter': '1996fanrui'}, {'comment': 'I write 2 tests:\r\n1. one is `testRemoveAndFlushEveryTime`, it\'s similar to current PR code\r\n2. one is `testQueryAndFlushIfNeeded`, it\'s similar to master code\r\n\r\n- `testRemoveAndFlushEveryTime` will remove and flush every time, and it will call kubernetes every time.\r\n- `testQueryAndFlushIfNeeded` will query from cache, and value is null, so remove and flush isn\'t necessary. So it doens\'t call kubernetes.\r\n\r\nMy question is: the extra call is expected? And I think this PR is fine if it doesn\'t introuduce any extra opration. Please correct my if my understanding is wrong, thanks a lot.\r\n\r\nThese 2 tests can be added to ConfigMapStoreTest, and they can run directly.\r\n\r\n\r\n```    \r\n    @Test\r\n    void testRemoveAndFlushEveryTime() {\r\n        KubernetesJobAutoScalerContext ctx = createContext(""cr1"", kubernetesClient);\r\n        var key = ""key"";\r\n        var value = ""value"";\r\n\r\n        var configMapStore = new ConfigMapStore(kubernetesClient);\r\n        configMapStore.putSerializedState(ctx, key, value);\r\n        assertEquals(2, mockWebServer.getRequestCount());\r\n        configMapStore.flush(ctx);\r\n        assertEquals(3, mockWebServer.getRequestCount());\r\n\r\n        // Get from cache\r\n        assertThat(configMapStore.getSerializedState(ctx, key)).hasValue(value);\r\n        assertEquals(3, mockWebServer.getRequestCount());\r\n\r\n        // Get from kubernetes\r\n        configMapStore = new ConfigMapStore(kubernetesClient);\r\n        assertThat(configMapStore.getSerializedState(ctx, key)).hasValue(value);\r\n        assertEquals(4, mockWebServer.getRequestCount());\r\n\r\n        // Remove the key\r\n        configMapStore.removeSerializedState(ctx, key);\r\n        assertEquals(4, mockWebServer.getRequestCount());\r\n\r\n        configMapStore.flush(ctx);\r\n        assertEquals(5, mockWebServer.getRequestCount());\r\n\r\n        // Every flush will call kubernetes.\r\n        configMapStore.flush(ctx);\r\n        assertEquals(6, mockWebServer.getRequestCount());\r\n        configMapStore.flush(ctx);\r\n        assertEquals(7, mockWebServer.getRequestCount());\r\n        configMapStore.flush(ctx);\r\n        assertEquals(8, mockWebServer.getRequestCount());\r\n    }\r\n\r\n\r\n    @Test\r\n    void testQueryAndFlushIfNeeded() {\r\n        KubernetesJobAutoScalerContext ctx = createContext(""cr1"", kubernetesClient);\r\n        var key = ""key"";\r\n        var value = ""value"";\r\n\r\n        var configMapStore = new ConfigMapStore(kubernetesClient);\r\n        configMapStore.putSerializedState(ctx, key, value);\r\n        assertEquals(2, mockWebServer.getRequestCount());\r\n        configMapStore.flush(ctx);\r\n        assertEquals(3, mockWebServer.getRequestCount());\r\n\r\n        // Get from cache\r\n        assertThat(configMapStore.getSerializedState(ctx, key)).hasValue(value);\r\n        assertEquals(3, mockWebServer.getRequestCount());\r\n\r\n        // Get from kubernetes\r\n        configMapStore = new ConfigMapStore(kubernetesClient);\r\n        assertThat(configMapStore.getSerializedState(ctx, key)).hasValue(value);\r\n        assertEquals(4, mockWebServer.getRequestCount());\r\n\r\n        // Remove the key\r\n        configMapStore.removeSerializedState(ctx, key);\r\n        assertEquals(4, mockWebServer.getRequestCount());\r\n\r\n        configMapStore.flush(ctx);\r\n        assertEquals(5, mockWebServer.getRequestCount());\r\n\r\n        // get is Optional.empty(), so don\'t flush\r\n        assertThat(configMapStore.getSerializedState(ctx, key)).isEmpty();\r\n        assertEquals(5, mockWebServer.getRequestCount());\r\n        assertThat(configMapStore.getSerializedState(ctx, key)).isEmpty();\r\n        assertEquals(5, mockWebServer.getRequestCount());\r\n        assertThat(configMapStore.getSerializedState(ctx, key)).isEmpty();\r\n        assertEquals(5, mockWebServer.getRequestCount());\r\n    }\r\n```', 'commenter': '1996fanrui'}, {'comment': ""We may need to added an extra cache to store the state that last updated. When the flush is called, then compare them. And call kubernetes if they are not same.\r\n\r\nAnd it's better to add a test to check the flush doesn't call kubernetes if state isn't updated.\r\n\r\nAlso, I'm not sure whether this PR is reasonable. After this refactor, the clean up logic is indeed clear. However, these complex operations are moved into state store. When we implememt other persistent state stores in the future, we still need to re-implememt this similar logic again. \r\n\r\nIf this logic can be retained in autoscaler, only one code is fine.\r\n\r\nPlease correct me if I miss some information, thanks"", 'commenter': '1996fanrui'}, {'comment': ""In this case the flush only calls Kubernetes because there is still data in the state store that wasn't removed. Maybe we should add a `clearAll` method to the state store interface and simply call that with the contract that subsequent `clearAll()` and `flush()` actions should be idempotent and ideally should not call Kubernetes again and again"", 'commenter': 'gyfora'}, {'comment': ""Thanks for adding the tests. You are right that the retrieval will always go to Kubernetes. The current implementation has some bugs with the bookkeeping to minimize the number of API calls. It does make more requests than necessary. \r\n\r\nI've rewrote the ConfigMapStore to minimize the number of times we go to Kubernetes. We only retrieve once now on lookup. If the ConfigMap does not exist, we will only create the ConfigMap once we flush back any changes."", 'commenter': 'mxm'}]"
710,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/JobAutoScalerImpl.java,"@@ -121,37 +120,10 @@ public void cleanup(KEY jobKey) {
     }
 
     private void clearStatesAfterAutoscalerDisabled(Context ctx) throws Exception {
-        var needFlush = false;
-        var parallelismOverrides = stateStore.getParallelismOverrides(ctx);
-        if (!parallelismOverrides.isEmpty()) {
-            needFlush = true;
-            stateStore.removeParallelismOverrides(ctx);
-        }
-
-        var collectedMetrics = stateStore.getCollectedMetrics(ctx);
-        if (!collectedMetrics.isEmpty()) {
-            needFlush = true;
-            stateStore.removeCollectedMetrics(ctx);
-        }
-
-        var scalingHistory = stateStore.getScalingHistory(ctx);
-        if (!scalingHistory.isEmpty()) {
-            var trimmedScalingHistory =
-                    trimScalingHistory(clock.instant(), ctx.getConfiguration(), scalingHistory);
-            if (trimmedScalingHistory.isEmpty()) {
-                // All scaling histories are trimmed.
-                needFlush = true;
-                stateStore.removeScalingHistory(ctx);
-            } else if (!scalingHistory.equals(trimmedScalingHistory)) {
-                // Some scaling histories are trimmed.
-                needFlush = true;
-                stateStore.storeScalingHistory(ctx, trimmedScalingHistory);
-            }
-        }
-
-        if (needFlush) {
-            stateStore.flush(ctx);
-        }
+        stateStore.removeParallelismOverrides(ctx);
+        stateStore.removeCollectedMetrics(ctx);
+        stateStore.removeScalingHistory(ctx);
+        stateStore.flush(ctx);","[{'comment': 'can we add a new `clearAll` or `removeAll` method that would guarantee that all state is cleaned and then we can test for subsequent flushing not hitting k8s api', 'commenter': 'gyfora'}, {'comment': 'Done.', 'commenter': 'mxm'}]"
710,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/ConfigMapStore.java,"@@ -44,13 +43,44 @@ public class ConfigMapStore {
 
     private final KubernetesClient kubernetesClient;
 
-    // The cache for each resourceId may be in three states:
-    // 1. The resourceId doesn't exist : ConfigMap isn't loaded from kubernetes, or it's deleted
-    // 2  Exists, Optional.empty() : The ConfigMap doesn't exist in Kubernetes
-    // 3. Exists, Not Empty : We have loaded the ConfigMap from kubernetes, it may not be the same
-    // if not flushed already
-    private final ConcurrentHashMap<ResourceID, Optional<ConfigMap>> cache =
-            new ConcurrentHashMap<>();
+    static class ConfigMapState {","[{'comment': 'Could we please move this class declaration to the end of the enclosing class?', 'commenter': 'gyfora'}, {'comment': 'Sure, moved.', 'commenter': 'mxm'}, {'comment': 'Actually now a top-level class.', 'commenter': 'mxm'}]"
710,flink-autoscaler/src/test/java/org/apache/flink/autoscaler/JobAutoScalerImplTest.java,"@@ -299,60 +296,15 @@ void testAutoscalerDisabled() throws Exception {
         scalingHistory.put(Instant.ofEpochMilli(100), new ScalingSummary());
         scalingHistory.put(Instant.ofEpochMilli(200), new ScalingSummary());
 
-        // Test all scaling aren't expired
-        getInstantScalingSummaryTreeMap(
-                scalingHistory, Clock.fixed(Instant.ofEpochMilli(250), ZoneId.systemDefault()), 2);
+        stateStore.storeScalingHistory(context, Map.of(new JobVertexID(), scalingHistory));
+        assertFalse(stateStore.getScalingHistory(context).isEmpty());
 
-        // Test one scaling aren't expired
-        getInstantScalingSummaryTreeMap(
-                scalingHistory, Clock.fixed(Instant.ofEpochMilli(350), ZoneId.systemDefault()), 1);
-
-        // Test all scaling are expired
-        getInstantScalingSummaryTreeMap(
-                scalingHistory, Clock.fixed(Instant.ofEpochMilli(450), ZoneId.systemDefault()), 0);
-    }
-
-    private void getInstantScalingSummaryTreeMap(
-            SortedMap<Instant, ScalingSummary> scalingHistoryData,
-            Clock clock,
-            int expectedScalingHistorySize)
-            throws Exception {
-        stateStore = new TestingAutoscalerStateStore<>();
         var autoscaler =
                 new JobAutoScalerImpl<>(
                         null, null, null, eventCollector, scalingRealizer, stateStore);
-
-        enrichStateStore(scalingHistoryData);
-        stateStore.flush(context);
-        assertThat(stateStore.getFlushCount()).isEqualTo(1);
-
-        autoscaler.setClock(clock);
         autoscaler.scale(context);
 
-        assertThat(stateStore.getParallelismOverrides(context)).isEmpty();
-        assertThat(stateStore.getCollectedMetrics(context)).isEmpty();
-
-        if (expectedScalingHistorySize > 0) {
-            Map<JobVertexID, SortedMap<Instant, ScalingSummary>> scalingHistory =
-                    stateStore.getScalingHistory(context);
-            assertThat(scalingHistory).isNotEmpty();
-            assertThat(scalingHistory.values())
-                    .allMatch(aa -> aa.size() == expectedScalingHistorySize);
-        } else {
-            assertThat(stateStore.getScalingHistory(context)).isEmpty();
-        }
-        assertThat(stateStore.getFlushCount()).isEqualTo(2);
-    }
-
-    private void enrichStateStore(SortedMap<Instant, ScalingSummary> scalingHistory) {
-        var v1 = new JobVertexID();
-        var v2 = new JobVertexID();
-        stateStore.storeParallelismOverrides(
-                context, Map.of(v1.toString(), ""1"", v2.toString(), ""2""));
-
-        var metricHistory = new TreeMap<Instant, CollectedMetrics>();
-        stateStore.storeCollectedMetrics(context, metricHistory);
-        stateStore.storeScalingHistory(context, Map.of(v1, scalingHistory, v2, scalingHistory));
+        assertTrue(stateStore.getScalingHistory(context).isEmpty());","[{'comment': 'Could we test `ParallelismOverrides` and `CollectedMetrics` as well?', 'commenter': '1996fanrui'}]"
710,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/ConfigMapStore.java,"@@ -44,13 +43,44 @@ public class ConfigMapStore {
 
     private final KubernetesClient kubernetesClient;
 
-    // The cache for each resourceId may be in three states:
-    // 1. The resourceId doesn't exist : ConfigMap isn't loaded from kubernetes, or it's deleted
-    // 2  Exists, Optional.empty() : The ConfigMap doesn't exist in Kubernetes
-    // 3. Exists, Not Empty : We have loaded the ConfigMap from kubernetes, it may not be the same
-    // if not flushed already
-    private final ConcurrentHashMap<ResourceID, Optional<ConfigMap>> cache =
-            new ConcurrentHashMap<>();
+    static class ConfigMapState {
+        private boolean flushed = true;
+        private boolean exists = true;
+
+        @VisibleForTesting ConfigMap configMap;","[{'comment': ""Could we add a setter for `ConfigMap configMap`, and update `exists` inside of this setter. It's safer and clearer for ConfigMapState, and easy to read and understand.\r\n\r\nAnd could `ConfigMapState` as a separate class instead of `inner class of ConfigMapStroe` and mark all fields as private? I see `ConfigMapStroe` update the configMap directly, and it's dangerous in the future if some developers update it without update `flushed` or `exists`. And it's easy to introduce bug in the future.\r\n\r\n\r\nIn short: I prefer `ConfigMapState` to expose the interface rather than the details."", 'commenter': '1996fanrui'}, {'comment': 'The updated code should reflect your request.', 'commenter': 'mxm'}, {'comment': 'Great work, Max! ðŸ‘', 'commenter': '1996fanrui'}]"
710,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/ConfigMapStore.java,"@@ -44,13 +43,44 @@ public class ConfigMapStore {
 
     private final KubernetesClient kubernetesClient;
 
-    // The cache for each resourceId may be in three states:
-    // 1. The resourceId doesn't exist : ConfigMap isn't loaded from kubernetes, or it's deleted
-    // 2  Exists, Optional.empty() : The ConfigMap doesn't exist in Kubernetes
-    // 3. Exists, Not Empty : We have loaded the ConfigMap from kubernetes, it may not be the same
-    // if not flushed already
-    private final ConcurrentHashMap<ResourceID, Optional<ConfigMap>> cache =
-            new ConcurrentHashMap<>();
+    static class ConfigMapState {
+        private boolean flushed = true;
+        private boolean exists = true;
+
+        @VisibleForTesting ConfigMap configMap;
+
+        public Map<String, String> getData() {","[{'comment': '```suggestion\r\n        @VisibleForTesting\r\n        public Map<String, String> getData() {\r\n```\r\n\r\n`getData` is only used for test', 'commenter': '1996fanrui'}]"
710,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/ConfigMapStore.java,"@@ -44,13 +43,44 @@ public class ConfigMapStore {
 
     private final KubernetesClient kubernetesClient;
 
-    // The cache for each resourceId may be in three states:
-    // 1. The resourceId doesn't exist : ConfigMap isn't loaded from kubernetes, or it's deleted
-    // 2  Exists, Optional.empty() : The ConfigMap doesn't exist in Kubernetes
-    // 3. Exists, Not Empty : We have loaded the ConfigMap from kubernetes, it may not be the same
-    // if not flushed already
-    private final ConcurrentHashMap<ResourceID, Optional<ConfigMap>> cache =
-            new ConcurrentHashMap<>();
+    static class ConfigMapState {
+        private boolean flushed = true;
+        private boolean exists = true;
+
+        @VisibleForTesting ConfigMap configMap;
+
+        public Map<String, String> getData() {
+            return Collections.unmodifiableMap(configMap.getData());
+        }
+
+        public void clear() {","[{'comment': ""It's not used, right? Could it be removed?"", 'commenter': '1996fanrui'}, {'comment': 'Maybe we should actually use this instead of calling the separate clear methods. Makes it much ""cleaner""', 'commenter': 'gyfora'}, {'comment': 'Added a method to clear all state.', 'commenter': 'mxm'}]"
710,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/ConfigMapStore.java,"@@ -44,13 +43,44 @@ public class ConfigMapStore {
 
     private final KubernetesClient kubernetesClient;
 
-    // The cache for each resourceId may be in three states:
-    // 1. The resourceId doesn't exist : ConfigMap isn't loaded from kubernetes, or it's deleted
-    // 2  Exists, Optional.empty() : The ConfigMap doesn't exist in Kubernetes
-    // 3. Exists, Not Empty : We have loaded the ConfigMap from kubernetes, it may not be the same
-    // if not flushed already
-    private final ConcurrentHashMap<ResourceID, Optional<ConfigMap>> cache =
-            new ConcurrentHashMap<>();
+    static class ConfigMapState {
+        private boolean flushed = true;
+        private boolean exists = true;
+
+        @VisibleForTesting ConfigMap configMap;
+
+        public Map<String, String> getData() {
+            return Collections.unmodifiableMap(configMap.getData());
+        }
+
+        public void clear() {
+            if (configMap.getData().isEmpty()) {
+                return;
+            }
+            configMap.getData().clear();
+            flushed = false;
+        }
+
+        public void removeKey(String key) {
+            var oldKey = configMap.getData().remove(key);
+            if (oldKey != null) {
+                flushed = false;
+            }
+        }
+
+        public void put(String key, String value) {
+            configMap.getData().put(key, value);
+            flushed = false;
+        }
+    }
+
+    // The cache for each resourceId may be in four states:
+    // 1. No cache entry: ConfigMap isn't loaded from kubernetes, or it's deleted.
+    // 2  Cache entry, not created : The ConfigMap doesn't exist in Kubernetes.
+    // 2  Cache entry, not flushed : The ConfigMap exists in Kubernetes, but it is not updated yet.
+    // 3. Cache entry, flushed and created : We have loaded the ConfigMap from kubernetes, and it's
+    // up-to-date.","[{'comment': ""```suggestion\r\n    // 2. Cache entry, not created : The ConfigMap doesn't exist in Kubernetes.\r\n    // 3. Cache entry, not flushed : The ConfigMap exists in Kubernetes, but it is not updated yet.\r\n    // 4. Cache entry, flushed and created : We have loaded the ConfigMap from kubernetes, and it's\r\n    // up-to-date.\r\n```"", 'commenter': '1996fanrui'}]"
710,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/ConfigMapStore.java,"@@ -63,29 +93,29 @@ protected void putSerializedState(
 
     protected Optional<String> getSerializedState(
             KubernetesJobAutoScalerContext jobContext, String key) {
-        return getConfigMap(jobContext).map(configMap -> configMap.getData().get(key));
+        return Optional.ofNullable(getConfigMap(jobContext).configMap.getData().get(key));
     }
 
     protected void removeSerializedState(KubernetesJobAutoScalerContext jobContext, String key) {
-        getConfigMap(jobContext)
-                .ifPresentOrElse(
-                        configMap -> configMap.getData().remove(key),
-                        () -> {
-                            throw new IllegalStateException(
-                                    ""The configMap isn't created, so the remove is unavailable."");
-                        });
+        getConfigMap(jobContext).removeKey(key);
     }
 
     public void flush(KubernetesJobAutoScalerContext jobContext) {
-        Optional<ConfigMap> configMapOpt = cache.get(jobContext.getJobKey());
-        if (configMapOpt == null || configMapOpt.isEmpty()) {
+        ConfigMapState configMapState = cache.get(jobContext.getJobKey());
+        if (configMapState == null || (configMapState.flushed && configMapState.exists)) {","[{'comment': ""It's better to extract a method `isFlushNeeded` for `ConfigMapState` instead of call `configMapState.flushed && configMapState.exists` directly."", 'commenter': '1996fanrui'}, {'comment': 'This should be nicer now.', 'commenter': 'mxm'}]"
710,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/state/ConfigMapView.java,"@@ -0,0 +1,111 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.autoscaler.state;
+
+import org.apache.flink.annotation.VisibleForTesting;
+import org.apache.flink.util.Preconditions;
+
+import io.fabric8.kubernetes.api.model.ConfigMap;
+import io.fabric8.kubernetes.client.dsl.Resource;
+
+import java.util.Collections;
+import java.util.Map;
+import java.util.function.Function;
+import java.util.function.Supplier;
+
+class ConfigMapView {
+
+    enum State {
+        NEEDS_CREATE,
+        NEEDS_UPDATE,
+        UP_TO_DATE
+    }
+
+    private State state;
+
+    private ConfigMap configMap;
+
+    private final Function<ConfigMap, Resource<ConfigMap>> resourceRetriever;
+
+    public ConfigMapView(
+            Supplier<ConfigMap> configMapRetriever,
+            Supplier<ConfigMap> configMapBuilder,
+            Function<ConfigMap, Resource<ConfigMap>> resourceRetriever) {
+        var existingConfigMap = configMapRetriever.get();
+        if (existingConfigMap != null) {
+            refreshConfigMap(existingConfigMap);
+        } else {
+            this.configMap = configMapBuilder.get();
+            this.state = State.NEEDS_CREATE;","[{'comment': ""I am not super familiar with the `Resource<T>` class but couldn't we use it also to get the `ConfigMap`? Then we could simply use that and don't need the `configMapRetriever`"", 'commenter': 'gyfora'}, {'comment': ""Lookup is performed by name and namespace. Update / Create uses the ConfigMap object which potentially involves version checks. That's how the code currently works. I didn't want to mess with the APIs to avoid introducing regressions."", 'commenter': 'mxm'}, {'comment': 'Updated.', 'commenter': 'mxm'}]"
710,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/state/ConfigMapView.java,"@@ -0,0 +1,111 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.kubernetes.operator.autoscaler.state;
+
+import org.apache.flink.annotation.VisibleForTesting;
+import org.apache.flink.util.Preconditions;
+
+import io.fabric8.kubernetes.api.model.ConfigMap;
+import io.fabric8.kubernetes.client.dsl.Resource;
+
+import java.util.Collections;
+import java.util.Map;
+import java.util.function.Function;
+import java.util.function.Supplier;
+
+class ConfigMapView {
+
+    enum State {
+        NEEDS_CREATE,
+        NEEDS_UPDATE,
+        UP_TO_DATE
+    }
+
+    private State state;
+
+    private ConfigMap configMap;
+
+    private final Function<ConfigMap, Resource<ConfigMap>> resourceRetriever;
+
+    public ConfigMapView(
+            Supplier<ConfigMap> configMapRetriever,
+            Supplier<ConfigMap> configMapBuilder,
+            Function<ConfigMap, Resource<ConfigMap>> resourceRetriever) {
+        var existingConfigMap = configMapRetriever.get();
+        if (existingConfigMap != null) {
+            refreshConfigMap(existingConfigMap);
+        } else {
+            this.configMap = configMapBuilder.get();
+            this.state = State.NEEDS_CREATE;
+        }
+        this.resourceRetriever = resourceRetriever;","[{'comment': 'I think we could further simplify this by keeping only:\r\n```\r\nResource<ConfigMap> resource:\r\nConfigMap configMap;\r\nState state;\r\n```\r\n\r\n\r\nAnd pass those directly in the constructor / through a static create method instead of the Suppliers/Functions which feel a bit unnecessary', 'commenter': 'gyfora'}, {'comment': ""That may be true. Let me try. I didn't want to change the retrieval / create / update logic."", 'commenter': 'mxm'}, {'comment': 'The current state is the following btw:\r\n\r\n```\r\nFunction<ConfigMap, Resource<ConfigMap>> resource:\r\nConfigMap configMap;\r\nState state;\r\n```\r\n\r\nThat occupies less space than what you are suggesting. `resource` is only a pointer to an existing method. In your case it is a reference to a heap object.', 'commenter': 'mxm'}]"
710,flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/state/ConfigMapStore.java,"@@ -44,13 +43,13 @@ public class ConfigMapStore {
 
     private final KubernetesClient kubernetesClient;
 
-    // The cache for each resourceId may be in three states:
-    // 1. The resourceId doesn't exist : ConfigMap isn't loaded from kubernetes, or it's deleted
-    // 2  Exists, Optional.empty() : The ConfigMap doesn't exist in Kubernetes
-    // 3. Exists, Not Empty : We have loaded the ConfigMap from kubernetes, it may not be the same
-    // if not flushed already
-    private final ConcurrentHashMap<ResourceID, Optional<ConfigMap>> cache =
-            new ConcurrentHashMap<>();
+    // The cache for each resourceId may be in four states:
+    // 1. No cache entry: ConfigMap isn't loaded from kubernetes, or it's deleted.
+    // 2. Cache entry, not created : The ConfigMap doesn't exist in Kubernetes.
+    // 3. Cache entry, not flushed : The ConfigMap exists in Kubernetes, but it is not updated yet.
+    // 4. Cache entry, flushed and created : We have loaded the ConfigMap from kubernetes, and it's
+    // up-to-date.","[{'comment': 'Would you mind updating these comments based on the latest `ConfigMapView.State`? For example, using `NEEDS_CREATE` instead of `not created`.', 'commenter': '1996fanrui'}, {'comment': 'Sure. Take a look at https://github.com/apache/flink-kubernetes-operator/pull/710/commits/e6568d958a141e77be54af6c456212faa69869b8. Does that reflect what you had in mind?', 'commenter': 'mxm'}]"
710,flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/autoscaler/state/KubernetesAutoScalerStateStoreTest.java,"@@ -330,4 +331,42 @@ public void testDiscardInvalidHistory() throws Exception {
                                 ctx, KubernetesAutoScalerStateStore.SCALING_HISTORY_KEY))
                 .isEmpty();
     }
+
+    @Test
+    public void testDiscardAllState() {","[{'comment': ""Hi, this test is accessing `configMapStore` instead of `KubernetesAutoScalerStateStore`. \r\n\r\nShould we test `KubernetesAutoScalerStateStore`? If we test `configMapStore`, it's better to move this test to `ConfigMapStoreTest`"", 'commenter': '1996fanrui'}]"
711,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/JobAutoScalerImpl.java,"@@ -159,19 +161,24 @@ private void runScalingLogic(Context ctx, AutoscalerFlinkMetrics autoscalerMetri
             throws Exception {
 
         var collectedMetrics = metricsCollector.updateMetrics(ctx, stateStore);
+        var jobTopology = collectedMetrics.getJobTopology();
 
         if (collectedMetrics.getMetricHistory().isEmpty()) {
             return;
         }
         LOG.debug(""Collected metrics: {}"", collectedMetrics);
 
-        var evaluatedMetrics = evaluator.evaluate(ctx.getConfiguration(), collectedMetrics);
+        var now = clock.instant();
+        // Scaling tracking data contains previous restart times that are taken into account
+        var scalingTracking = getTrimmedScalingTracking(stateStore, ctx, now);
+        var evaluatedMetrics =
+                evaluator.evaluate(ctx.getConfiguration(), collectedMetrics, scalingTracking);","[{'comment': ""Could we treat `scalingTracking` as a metric, just like the other scaling metrics? Apart from being consistent with the current code, this also has some advantages because the scaling time will be reported as a metric out of the box.\r\n\r\nSo basically I'm asking to insert the current rescale time into the `collectedMetrics` map."", 'commenter': 'mxm'}, {'comment': 'How do you propose to make the association between the rescaling decision and the collected metrics record? My understanding is that `updateMetrics` writes collected metrics irrespective from the rescalings, so we need to somehow decide which history entry to associate the restart time with. To be honest this feels like misusing the data structure not for its original purpose and adding complexity that will be hard to interpret when someone reads the code. It is already hard enough to interpret what is going on in the `updateMetrics` method.', 'commenter': 'afedulov'}, {'comment': ""Sorry, small correction: I meant `evaluatedMetrics`, not `collectedMetrics`.\r\n\r\nWe don't need to associate the history entry with any of the metrics used for scaling. We would insert into the evaluated metrics map whatever the current determined rescale time is. This is similar to the MAX_PARALLELISM metric. "", 'commenter': 'mxm'}, {'comment': 'The way the scalingHistory is currently structured, this would currently lead to duplicating the restart time unnecessarily per job vertex.', 'commenter': 'afedulov'}]"
711,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/ScalingRecord.java,"@@ -0,0 +1,35 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.autoscaler;
+
+import lombok.AllArgsConstructor;
+import lombok.Data;
+import lombok.NoArgsConstructor;
+
+import java.time.Instant;
+
+/**
+ * Class for tracking scaling details, including time it took for the job to transition to the
+ * target parallelism.
+ */
+@Data
+@NoArgsConstructor
+@AllArgsConstructor
+public class ScalingRecord {
+    private Instant endTime;
+}","[{'comment': 'Could we just store Instant directly and remove this wrapper class?', 'commenter': 'mxm'}, {'comment': 'My idea was to have structures in place that would be easily extensible. If we are sure we do not not ever need to store anything else here we can store the instant directly. ', 'commenter': 'afedulov'}]"
711,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/ScalingTracking.java,"@@ -0,0 +1,168 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.autoscaler;
+
+import org.apache.flink.annotation.Experimental;
+import org.apache.flink.autoscaler.config.AutoScalerOptions;
+import org.apache.flink.autoscaler.topology.JobTopology;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.runtime.jobgraph.JobVertexID;
+
+import com.fasterxml.jackson.annotation.JsonIgnore;
+import lombok.Builder;
+import lombok.Data;
+import lombok.NoArgsConstructor;
+
+import java.time.Duration;
+import java.time.Instant;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.Optional;
+import java.util.SortedMap;
+import java.util.TreeMap;
+import java.util.stream.Collectors;
+
+/** Stores rescaling related information for the job. */
+@Experimental
+@Data
+@NoArgsConstructor
+@Builder
+public class ScalingTracking {
+
+    /** Details related to recent rescaling operations. */
+    private final TreeMap<Instant, ScalingRecord> scalingRecords = new TreeMap<>();
+
+    public void addScalingRecord(Instant startTimestamp, ScalingRecord scalingRecord) {
+        scalingRecords.put(startTimestamp, scalingRecord);
+    }
+
+    @JsonIgnore
+    public Optional<Entry<Instant, ScalingRecord>> getLatestScalingRecordEntry() {
+        if (!scalingRecords.isEmpty()) {
+            return Optional.of(scalingRecords.lastEntry());
+        } else {
+            return Optional.empty();
+        }
+    }
+
+    /**
+     * Sets the end time for the latest scaling record if its parallelism matches the current job
+     * parallelism.
+     *
+     * @param now The current instant to be set as the end time of the scaling record.
+     * @param jobTopology The current job topology containing details of the job's parallelism.
+     * @param scalingHistory The scaling history.
+     * @return true if the end time is successfully set, false if the end time is already set, the
+     *     latest scaling record cannot be found, or the target parallelism does not match the
+     *     actual parallelism.
+     */
+    public boolean setEndTimeIfTrackedAndParallelismMatches(
+            Instant now,
+            JobTopology jobTopology,
+            Map<JobVertexID, SortedMap<Instant, ScalingSummary>> scalingHistory) {
+        return getLatestScalingRecordEntry()
+                .map(
+                        entry -> {
+                            var value = entry.getValue();
+                            var scalingTimestamp = entry.getKey();
+                            if (value.getEndTime() == null) {
+                                var targetParallelism =
+                                        getTargetParallelismOfScaledVertices(
+                                                scalingTimestamp, scalingHistory);
+                                var actualParallelism = jobTopology.getParallelisms();
+
+                                if (targetParallelismMatchesActual(
+                                        targetParallelism, actualParallelism)) {
+                                    value.setEndTime(now);
+                                    return true;
+                                }
+                            }
+                            return false;
+                        })
+                .orElse(false);
+    }
+
+    private static Map<JobVertexID, Integer> getTargetParallelismOfScaledVertices(
+            Instant scalingTimestamp,
+            Map<JobVertexID, SortedMap<Instant, ScalingSummary>> scalingHistory) {
+        return scalingHistory.entrySet().stream()
+                .filter(entry -> entry.getValue().containsKey(scalingTimestamp))
+                .collect(
+                        Collectors.toMap(
+                                Map.Entry::getKey,
+                                entry ->
+                                        entry.getValue()
+                                                .get(scalingTimestamp)
+                                                .getNewParallelism()));
+    }
+
+    private static boolean targetParallelismMatchesActual(
+            Map<JobVertexID, Integer> targetParallelisms,
+            Map<JobVertexID, Integer> actualParallelisms) {
+        return targetParallelisms.entrySet().stream()
+                .allMatch(
+                        entry -> {
+                            var vertexID = entry.getKey();
+                            var targetParallelism = entry.getValue();
+                            var actualParallelism = actualParallelisms.getOrDefault(vertexID, -1);
+                            return actualParallelism.equals(targetParallelism);","[{'comment': ""This doesn't factor in any excluded vertices through the excluded vertices setting."", 'commenter': 'mxm'}, {'comment': 'Not sure I am missing something, but I thought only vertices which are scaled are entered into the scaling history. Since this is what we build our `targetParallelisms` based on (see `getTargetParallelismOfScaledVertices`) and iterate over  `targetParallelisms`, not the `actualParallelism`, I assumed this already being dealt with.', 'commenter': 'afedulov'}, {'comment': 'I think we only exclude vertices when we generate the `ScalingSummary`. I just want to make sure this logic works, when there are excluded vertices because those would not be scaled and could be set to a different parallelism for other reasons. Just something to double-check.', 'commenter': 'mxm'}, {'comment': 'Looks like this is already taken care of because we do not emit a ScalingSummary for excluded vertices.', 'commenter': 'mxm'}]"
711,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/ScalingTracking.java,"@@ -0,0 +1,168 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.autoscaler;
+
+import org.apache.flink.annotation.Experimental;
+import org.apache.flink.autoscaler.config.AutoScalerOptions;
+import org.apache.flink.autoscaler.topology.JobTopology;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.runtime.jobgraph.JobVertexID;
+
+import com.fasterxml.jackson.annotation.JsonIgnore;
+import lombok.Builder;
+import lombok.Data;
+import lombok.NoArgsConstructor;
+
+import java.time.Duration;
+import java.time.Instant;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.Optional;
+import java.util.SortedMap;
+import java.util.TreeMap;
+import java.util.stream.Collectors;
+
+/** Stores rescaling related information for the job. */
+@Experimental
+@Data
+@NoArgsConstructor
+@Builder
+public class ScalingTracking {
+
+    /** Details related to recent rescaling operations. */
+    private final TreeMap<Instant, ScalingRecord> scalingRecords = new TreeMap<>();
+
+    public void addScalingRecord(Instant startTimestamp, ScalingRecord scalingRecord) {
+        scalingRecords.put(startTimestamp, scalingRecord);
+    }
+
+    @JsonIgnore
+    public Optional<Entry<Instant, ScalingRecord>> getLatestScalingRecordEntry() {
+        if (!scalingRecords.isEmpty()) {
+            return Optional.of(scalingRecords.lastEntry());
+        } else {
+            return Optional.empty();
+        }
+    }
+
+    /**
+     * Sets the end time for the latest scaling record if its parallelism matches the current job
+     * parallelism.
+     *
+     * @param now The current instant to be set as the end time of the scaling record.
+     * @param jobTopology The current job topology containing details of the job's parallelism.
+     * @param scalingHistory The scaling history.
+     * @return true if the end time is successfully set, false if the end time is already set, the
+     *     latest scaling record cannot be found, or the target parallelism does not match the
+     *     actual parallelism.
+     */
+    public boolean setEndTimeIfTrackedAndParallelismMatches(
+            Instant now,
+            JobTopology jobTopology,
+            Map<JobVertexID, SortedMap<Instant, ScalingSummary>> scalingHistory) {
+        return getLatestScalingRecordEntry()
+                .map(
+                        entry -> {
+                            var value = entry.getValue();
+                            var scalingTimestamp = entry.getKey();
+                            if (value.getEndTime() == null) {
+                                var targetParallelism =
+                                        getTargetParallelismOfScaledVertices(
+                                                scalingTimestamp, scalingHistory);
+                                var actualParallelism = jobTopology.getParallelisms();
+
+                                if (targetParallelismMatchesActual(
+                                        targetParallelism, actualParallelism)) {
+                                    value.setEndTime(now);
+                                    return true;
+                                }
+                            }
+                            return false;
+                        })
+                .orElse(false);
+    }
+
+    private static Map<JobVertexID, Integer> getTargetParallelismOfScaledVertices(
+            Instant scalingTimestamp,
+            Map<JobVertexID, SortedMap<Instant, ScalingSummary>> scalingHistory) {
+        return scalingHistory.entrySet().stream()
+                .filter(entry -> entry.getValue().containsKey(scalingTimestamp))
+                .collect(
+                        Collectors.toMap(
+                                Map.Entry::getKey,
+                                entry ->
+                                        entry.getValue()
+                                                .get(scalingTimestamp)
+                                                .getNewParallelism()));
+    }
+
+    private static boolean targetParallelismMatchesActual(
+            Map<JobVertexID, Integer> targetParallelisms,
+            Map<JobVertexID, Integer> actualParallelisms) {
+        return targetParallelisms.entrySet().stream()
+                .allMatch(
+                        entry -> {
+                            var vertexID = entry.getKey();
+                            var targetParallelism = entry.getValue();
+                            var actualParallelism = actualParallelisms.getOrDefault(vertexID, -1);
+                            return actualParallelism.equals(targetParallelism);
+                        });
+    }
+
+    /**
+     * Retrieves the maximum restart time in seconds based on the provided configuration and scaling
+     * records. Defaults to the RESTART_TIME from configuration if the PREFER_TRACKED_RESTART_TIME
+     * option is set to false, or if there are no tracking records available.
+     */
+    public double getMaxRestartTimeSecondsOrDefault(Configuration conf) {
+        long maxRestartTime = -1;
+        long restartTimeFromConfig = conf.get(AutoScalerOptions.RESTART_TIME).toSeconds();
+        if (conf.get(AutoScalerOptions.PREFER_TRACKED_RESTART_TIME)) {
+            for (Map.Entry<Instant, ScalingRecord> entry : scalingRecords.entrySet()) {
+                var startTime = entry.getKey();
+                var endTime = entry.getValue().getEndTime();
+                if (endTime != null) {
+                    var restartTime = Duration.between(startTime, endTime).toSeconds();
+                    maxRestartTime = Math.max(restartTime, maxRestartTime);
+                }
+            }
+        }","[{'comment': ""Just wondering, if we store the max restart time, isn't it enough to store a single value? This would simplify the code."", 'commenter': 'mxm'}, {'comment': 'Do you mean just store maxRestartTime per job instead of the startTime endTime pairs? \r\n1. How are we going to prevent an abnormally large timestamp that happened a month from ""locking"" the restart time forever?\r\n2. How about the idea of adding the exponential moving average we were discussing as a potential improvement?', 'commenter': 'afedulov'}, {'comment': ""If we just store the max rescale time, it is sufficient to use a single value. But, trimming the history won't actually work correctly then. We need to keep all values for this to work correctly. So makes sense as-is."", 'commenter': 'mxm'}]"
711,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/config/AutoScalerOptions.java,"@@ -154,6 +154,15 @@ private static ConfigOptions.OptionBuilder autoScalerConfig(String key) {
                                     + RESTART_TIME.key()
                                     + ""' will act as an upper bound."");
 
+    public static final ConfigOption<Duration> MAX_RESTART_TIME =
+            autoScalerConfig(""restart.time.max"")
+                    .durationType()
+                    .defaultValue(Duration.ofMinutes(30))","[{'comment': ""We have internal overrides for all configuration defaults, but I don't think this overly pessimistic value is a good default for the general public. I would set this to not more than 15 minutes, even that is still conservative."", 'commenter': 'mxm'}, {'comment': 'Done.', 'commenter': 'afedulov'}]"
711,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/ScalingTracking.java,"@@ -142,9 +142,11 @@ public double getMaxRestartTimeSecondsOrDefault(Configuration conf) {
                 }
             }
         }
+        long restartTimeFromConfig = conf.get(AutoScalerOptions.RESTART_TIME).toSeconds();
+        long maxRestartTimeFromConfig = conf.get(AutoScalerOptions.MAX_RESTART_TIME).toSeconds();
         return maxRestartTime == -1
                 ? restartTimeFromConfig","[{'comment': 'I think we should always cap the RESTART_TIME by the configured MAX_RESTART_TIME (if configured).', 'commenter': 'mxm'}, {'comment': ""@gyfora I'd like to hear your thoughts on this"", 'commenter': 'afedulov'}, {'comment': 'Synced with Max offline - we decided to rename the option to `TRACKED_RESTART_TIME_LIMIT` to make the scope clear.', 'commenter': 'afedulov'}, {'comment': 'sounds good!', 'commenter': 'gyfora'}]"
711,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/config/AutoScalerOptions.java,"@@ -142,6 +142,27 @@ private static ConfigOptions.OptionBuilder autoScalerConfig(String key) {
                     .withDescription(
                             ""Expected restart time to be used until the operator can determine it reliably from history."");
 
+    public static final ConfigOption<Boolean> PREFER_TRACKED_RESTART_TIME =
+            autoScalerConfig(""restart.time.tracked.enabled"")
+                    .booleanType()
+                    .defaultValue(false)
+                    .withDescription(
+                            ""Whether to use the actually observed rescaling restart times instead of the fixed '""","[{'comment': '```suggestion\r\n                            ""Whether to use the actual observed rescaling restart times instead of the fixed \'""\r\n```', 'commenter': 'mxm'}]"
711,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/config/AutoScalerOptions.java,"@@ -142,6 +142,27 @@ private static ConfigOptions.OptionBuilder autoScalerConfig(String key) {
                     .withDescription(
                             ""Expected restart time to be used until the operator can determine it reliably from history."");
 
+    public static final ConfigOption<Boolean> PREFER_TRACKED_RESTART_TIME =
+            autoScalerConfig(""restart.time.tracked.enabled"")
+                    .booleanType()
+                    .defaultValue(false)
+                    .withDescription(
+                            ""Whether to use the actually observed rescaling restart times instead of the fixed '""
+                                    + RESTART_TIME.key()
+                                    + ""' configuration. If set to true, the maximum restart duration over a number of ""
+                                    + ""samples will be used. The value of '""
+                                    + RESTART_TIME.key()
+                                    + ""' will act as an upper bound."");
+
+    public static final ConfigOption<Duration> TRACKED_RESTART_TIME_LIMIT =
+            autoScalerConfig(""restart.time.tracked.limit"")
+                    .durationType()
+                    .defaultValue(Duration.ofMinutes(15))
+                    .withDescription(
+                            ""Maximum cap for the calculated restart time when '""","[{'comment': '```suggestion\r\n                            ""Maximum cap for the observed restart time when \'""\r\n```', 'commenter': 'mxm'}]"
711,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/ScalingTracking.java,"@@ -0,0 +1,171 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.autoscaler;
+
+import org.apache.flink.annotation.Experimental;
+import org.apache.flink.autoscaler.config.AutoScalerOptions;
+import org.apache.flink.autoscaler.topology.JobTopology;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.runtime.jobgraph.JobVertexID;
+
+import com.fasterxml.jackson.annotation.JsonIgnore;
+import lombok.Builder;
+import lombok.Data;
+import lombok.NoArgsConstructor;
+
+import java.time.Duration;
+import java.time.Instant;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.Optional;
+import java.util.SortedMap;
+import java.util.TreeMap;
+import java.util.stream.Collectors;
+
+/** Stores rescaling related information for the job. */
+@Experimental
+@Data
+@NoArgsConstructor
+@Builder
+public class ScalingTracking {
+
+    /** Details related to recent rescaling operations. */
+    private final TreeMap<Instant, ScalingRecord> scalingRecords = new TreeMap<>();
+
+    public void addScalingRecord(Instant startTimestamp, ScalingRecord scalingRecord) {
+        scalingRecords.put(startTimestamp, scalingRecord);
+    }
+
+    @JsonIgnore
+    public Optional<Entry<Instant, ScalingRecord>> getLatestScalingRecordEntry() {
+        if (!scalingRecords.isEmpty()) {
+            return Optional.of(scalingRecords.lastEntry());
+        } else {
+            return Optional.empty();
+        }
+    }
+
+    /**
+     * Sets the end time for the latest scaling record if its parallelism matches the current job
+     * parallelism.
+     *
+     * @param now The current instant to be set as the end time of the scaling record.
+     * @param jobTopology The current job topology containing details of the job's parallelism.
+     * @param scalingHistory The scaling history.
+     * @return true if the end time is successfully set, false if the end time is already set, the
+     *     latest scaling record cannot be found, or the target parallelism does not match the
+     *     actual parallelism.
+     */
+    public boolean setEndTimeIfTrackedAndParallelismMatches(
+            Instant now,
+            JobTopology jobTopology,
+            Map<JobVertexID, SortedMap<Instant, ScalingSummary>> scalingHistory) {
+        return getLatestScalingRecordEntry()
+                .map(
+                        entry -> {
+                            var value = entry.getValue();
+                            var scalingTimestamp = entry.getKey();
+                            if (value.getEndTime() == null) {
+                                var targetParallelism =
+                                        getTargetParallelismOfScaledVertices(
+                                                scalingTimestamp, scalingHistory);
+                                var actualParallelism = jobTopology.getParallelisms();
+
+                                if (targetParallelismMatchesActual(
+                                        targetParallelism, actualParallelism)) {
+                                    value.setEndTime(now);
+                                    return true;
+                                }
+                            }
+                            return false;
+                        })
+                .orElse(false);
+    }
+
+    private static Map<JobVertexID, Integer> getTargetParallelismOfScaledVertices(
+            Instant scalingTimestamp,
+            Map<JobVertexID, SortedMap<Instant, ScalingSummary>> scalingHistory) {
+        return scalingHistory.entrySet().stream()
+                .filter(entry -> entry.getValue().containsKey(scalingTimestamp))
+                .collect(
+                        Collectors.toMap(
+                                Map.Entry::getKey,
+                                entry ->
+                                        entry.getValue()
+                                                .get(scalingTimestamp)
+                                                .getNewParallelism()));
+    }
+
+    private static boolean targetParallelismMatchesActual(
+            Map<JobVertexID, Integer> targetParallelisms,
+            Map<JobVertexID, Integer> actualParallelisms) {
+        return targetParallelisms.entrySet().stream()
+                .allMatch(
+                        entry -> {
+                            var vertexID = entry.getKey();
+                            var targetParallelism = entry.getValue();
+                            var actualParallelism = actualParallelisms.getOrDefault(vertexID, -1);
+                            return actualParallelism.equals(targetParallelism);
+                        });
+    }
+
+    /**
+     * Retrieves the maximum restart time in seconds based on the provided configuration and scaling
+     * records. Defaults to the RESTART_TIME from configuration if the PREFER_TRACKED_RESTART_TIME
+     * option is set to false, or if there are no tracking records available. Otherwise, the maximum
+     * observed restart time is capped by the MAX_RESTART_TIME.
+     */
+    public double getMaxRestartTimeSecondsOrDefault(Configuration conf) {","[{'comment': 'I would prefer if we passed `Duration` until we perform any calculation.', 'commenter': 'mxm'}, {'comment': 'https://github.com/apache/flink-kubernetes-operator/pull/711/commits/93ae804e1466fc75009ca10714a65ad984efbd14\r\n', 'commenter': 'afedulov'}]"
711,docs/layouts/shortcodes/generated/auto_scaler_configuration.html,"@@ -80,6 +80,18 @@
             <td>Duration</td>
             <td>Expected restart time to be used until the operator can determine it reliably from history.</td>
         </tr>
+        <tr>
+            <td><h5>job.autoscaler.restart.time.tracked.enabled</h5></td>
+            <td style=""word-wrap: break-word;"">false</td>
+            <td>Boolean</td>
+            <td>Whether to use the actually observed rescaling restart times instead of the fixed 'job.autoscaler.restart.time' configuration. If set to true, the maximum restart duration over a number of samples will be used. The value of 'job.autoscaler.restart.time' will act as an upper bound.</td>
+        </tr>
+        <tr>
+            <td><h5>job.autoscaler.restart.time.tracked.limit</h5></td>","[{'comment': 'Sorry for the late comment, we could consider changing this to `job.autoscaler.restart.time-tracking.enabled` and `job.autoscaler.restart.time-tracking.limit` so that we avoid having another config match a prefix (and therefore make it non yaml compliant) \r\n\r\nNot a huge thing but if we want to support yaml configs like in flink we will have to fix it eventually', 'commenter': 'gyfora'}]"
711,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/ScalingTracking.java,"@@ -0,0 +1,171 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.autoscaler;
+
+import org.apache.flink.annotation.Experimental;
+import org.apache.flink.autoscaler.config.AutoScalerOptions;
+import org.apache.flink.autoscaler.topology.JobTopology;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.runtime.jobgraph.JobVertexID;
+
+import com.fasterxml.jackson.annotation.JsonIgnore;
+import lombok.Builder;
+import lombok.Data;
+import lombok.NoArgsConstructor;
+
+import java.time.Duration;
+import java.time.Instant;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.Optional;
+import java.util.SortedMap;
+import java.util.TreeMap;
+import java.util.stream.Collectors;
+
+/** Stores rescaling related information for the job. */
+@Experimental
+@Data
+@NoArgsConstructor
+@Builder
+public class ScalingTracking {
+
+    /** Details related to recent rescaling operations. */
+    private final TreeMap<Instant, ScalingRecord> scalingRecords = new TreeMap<>();
+
+    public void addScalingRecord(Instant startTimestamp, ScalingRecord scalingRecord) {
+        scalingRecords.put(startTimestamp, scalingRecord);
+    }
+
+    @JsonIgnore
+    public Optional<Entry<Instant, ScalingRecord>> getLatestScalingRecordEntry() {
+        if (!scalingRecords.isEmpty()) {
+            return Optional.of(scalingRecords.lastEntry());
+        } else {
+            return Optional.empty();
+        }
+    }
+
+    /**
+     * Sets the end time for the latest scaling record if its parallelism matches the current job
+     * parallelism.
+     *
+     * @param now The current instant to be set as the end time of the scaling record.
+     * @param jobTopology The current job topology containing details of the job's parallelism.
+     * @param scalingHistory The scaling history.
+     * @return true if the end time is successfully set, false if the end time is already set, the
+     *     latest scaling record cannot be found, or the target parallelism does not match the
+     *     actual parallelism.
+     */
+    public boolean setEndTimeIfTrackedAndParallelismMatches(
+            Instant now,
+            JobTopology jobTopology,
+            Map<JobVertexID, SortedMap<Instant, ScalingSummary>> scalingHistory) {
+        return getLatestScalingRecordEntry()
+                .map(
+                        entry -> {
+                            var value = entry.getValue();
+                            var scalingTimestamp = entry.getKey();
+                            if (value.getEndTime() == null) {
+                                var targetParallelism =
+                                        getTargetParallelismOfScaledVertices(
+                                                scalingTimestamp, scalingHistory);
+                                var actualParallelism = jobTopology.getParallelisms();
+
+                                if (targetParallelismMatchesActual(
+                                        targetParallelism, actualParallelism)) {
+                                    value.setEndTime(now);","[{'comment': 'Should we maybe log this on debug? so we have an overview if we want to debug this?', 'commenter': 'gyfora'}]"
711,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/ScalingTracking.java,"@@ -0,0 +1,171 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.autoscaler;
+
+import org.apache.flink.annotation.Experimental;
+import org.apache.flink.autoscaler.config.AutoScalerOptions;
+import org.apache.flink.autoscaler.topology.JobTopology;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.runtime.jobgraph.JobVertexID;
+
+import com.fasterxml.jackson.annotation.JsonIgnore;
+import lombok.Builder;
+import lombok.Data;
+import lombok.NoArgsConstructor;
+
+import java.time.Duration;
+import java.time.Instant;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.Optional;
+import java.util.SortedMap;
+import java.util.TreeMap;
+import java.util.stream.Collectors;
+
+/** Stores rescaling related information for the job. */
+@Experimental
+@Data
+@NoArgsConstructor
+@Builder
+public class ScalingTracking {
+
+    /** Details related to recent rescaling operations. */
+    private final TreeMap<Instant, ScalingRecord> scalingRecords = new TreeMap<>();","[{'comment': 'I thought we were going to keep a single record and exponential moving avg', 'commenter': 'gyfora'}, {'comment': 'I guess [this](https://github.com/apache/flink-kubernetes-operator/pull/711#issuecomment-1815381974) got buried in the notifications:\r\n```\r\n@gyfora me and Max briefly discussed offline and came to the conclusion that starting with\r\n evaluating the maximum restart time capped by the RESTART_TIME setting is probably \r\ngood enough for the first step. It has the benefit of giving the most ""conservative"" \r\nevaluation and we can add the moving average after some baseline testing. What do you think?\r\n```', 'commenter': 'afedulov'}, {'comment': ""I saw this but this doesn't mention anything about history etc and refers to an offline discussion :) \r\nCombined with the other comment related to the trimming issue (losing the restart info after 24h) I think the exponential moving avg is a simpler and slightly more robust initial approach"", 'commenter': 'gyfora'}]"
711,flink-autoscaler/src/main/java/org/apache/flink/autoscaler/ScalingTracking.java,"@@ -0,0 +1,171 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.autoscaler;
+
+import org.apache.flink.annotation.Experimental;
+import org.apache.flink.autoscaler.config.AutoScalerOptions;
+import org.apache.flink.autoscaler.topology.JobTopology;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.runtime.jobgraph.JobVertexID;
+
+import com.fasterxml.jackson.annotation.JsonIgnore;
+import lombok.Builder;
+import lombok.Data;
+import lombok.NoArgsConstructor;
+
+import java.time.Duration;
+import java.time.Instant;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.Optional;
+import java.util.SortedMap;
+import java.util.TreeMap;
+import java.util.stream.Collectors;
+
+/** Stores rescaling related information for the job. */
+@Experimental
+@Data
+@NoArgsConstructor
+@Builder
+public class ScalingTracking {
+
+    /** Details related to recent rescaling operations. */
+    private final TreeMap<Instant, ScalingRecord> scalingRecords = new TreeMap<>();
+
+    public void addScalingRecord(Instant startTimestamp, ScalingRecord scalingRecord) {
+        scalingRecords.put(startTimestamp, scalingRecord);
+    }
+
+    @JsonIgnore
+    public Optional<Entry<Instant, ScalingRecord>> getLatestScalingRecordEntry() {
+        if (!scalingRecords.isEmpty()) {
+            return Optional.of(scalingRecords.lastEntry());
+        } else {
+            return Optional.empty();
+        }
+    }
+
+    /**
+     * Sets the end time for the latest scaling record if its parallelism matches the current job
+     * parallelism.
+     *
+     * @param now The current instant to be set as the end time of the scaling record.
+     * @param jobTopology The current job topology containing details of the job's parallelism.
+     * @param scalingHistory The scaling history.
+     * @return true if the end time is successfully set, false if the end time is already set, the
+     *     latest scaling record cannot be found, or the target parallelism does not match the
+     *     actual parallelism.
+     */
+    public boolean setEndTimeIfTrackedAndParallelismMatches(
+            Instant now,
+            JobTopology jobTopology,
+            Map<JobVertexID, SortedMap<Instant, ScalingSummary>> scalingHistory) {
+        return getLatestScalingRecordEntry()
+                .map(
+                        entry -> {
+                            var value = entry.getValue();
+                            var scalingTimestamp = entry.getKey();
+                            if (value.getEndTime() == null) {
+                                var targetParallelism =
+                                        getTargetParallelismOfScaledVertices(
+                                                scalingTimestamp, scalingHistory);
+                                var actualParallelism = jobTopology.getParallelisms();
+
+                                if (targetParallelismMatchesActual(
+                                        targetParallelism, actualParallelism)) {
+                                    value.setEndTime(now);
+                                    return true;
+                                }
+                            }
+                            return false;
+                        })
+                .orElse(false);
+    }
+
+    private static Map<JobVertexID, Integer> getTargetParallelismOfScaledVertices(
+            Instant scalingTimestamp,
+            Map<JobVertexID, SortedMap<Instant, ScalingSummary>> scalingHistory) {
+        return scalingHistory.entrySet().stream()
+                .filter(entry -> entry.getValue().containsKey(scalingTimestamp))
+                .collect(
+                        Collectors.toMap(
+                                Map.Entry::getKey,
+                                entry ->
+                                        entry.getValue()
+                                                .get(scalingTimestamp)
+                                                .getNewParallelism()));
+    }
+
+    private static boolean targetParallelismMatchesActual(
+            Map<JobVertexID, Integer> targetParallelisms,
+            Map<JobVertexID, Integer> actualParallelisms) {
+        return targetParallelisms.entrySet().stream()
+                .allMatch(
+                        entry -> {
+                            var vertexID = entry.getKey();
+                            var targetParallelism = entry.getValue();
+                            var actualParallelism = actualParallelisms.getOrDefault(vertexID, -1);
+                            return actualParallelism.equals(targetParallelism);
+                        });
+    }
+
+    /**
+     * Retrieves the maximum restart time in seconds based on the provided configuration and scaling
+     * records. Defaults to the RESTART_TIME from configuration if the PREFER_TRACKED_RESTART_TIME
+     * option is set to false, or if there are no tracking records available. Otherwise, the maximum
+     * observed restart time is capped by the MAX_RESTART_TIME.
+     */
+    public Duration getMaxRestartTimeSecondsOrDefault(Configuration conf) {
+        long maxRestartTime = -1;
+        if (conf.get(AutoScalerOptions.PREFER_TRACKED_RESTART_TIME)) {
+            for (Map.Entry<Instant, ScalingRecord> entry : scalingRecords.entrySet()) {
+                var startTime = entry.getKey();
+                var endTime = entry.getValue().getEndTime();
+                if (endTime != null) {
+                    var restartTime = Duration.between(startTime, endTime).toSeconds();
+                    maxRestartTime = Math.max(restartTime, maxRestartTime);
+                }
+            }
+        }
+        var restartTimeFromConfig = conf.get(AutoScalerOptions.RESTART_TIME);
+        long maxRestartTimeFromConfig =
+                conf.get(AutoScalerOptions.TRACKED_RESTART_TIME_LIMIT).toSeconds();
+        return maxRestartTime == -1
+                ? restartTimeFromConfig
+                : Duration.ofSeconds(Math.min(maxRestartTime, maxRestartTimeFromConfig));
+    }
+
+    /**
+     * Removes records from the internal map that are older than the specified time span and trims
+     * the number of records to the specified maximum count.
+     *
+     * @param keptTimeSpan Duration for how long recent records are to be kept.
+     * @param keptNumRecords The maximum number of recent records to keep.
+     */
+    public void removeOldRecords(Instant now, Duration keptTimeSpan, int keptNumRecords) {
+        var cutoffTime = now.minus(keptTimeSpan);
+
+        // Remove records older than the cutoff time
+        scalingRecords.headMap(cutoffTime).clear();","[{'comment': ""Wouldn't this clear the history if we don't scale for 24 hours? then we fall back to the config ?"", 'commenter': 'gyfora'}, {'comment': 'We should keep at least 1 in the history. But given that scalings do not happen that often the history will always only have 1-2 records only. So EMA may be more robust. cc @mxm ', 'commenter': 'gyfora'}]"
